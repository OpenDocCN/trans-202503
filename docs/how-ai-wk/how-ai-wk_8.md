## 第九章：**思考：人工智能的影响**

![Image](img/common.jpg)

现在你已经理解了人工智能是什么，它来自哪里，以及它是如何工作的。令我最惊讶的是，现代人工智能在其核心上，完全是通过反向传播和梯度下降训练的朴素神经元的排列组合。

正如我们在前一章中所看到的，具有复杂突现能力的大型语言模型的诞生，永久性地改变了人工智能的格局。在 2023 年春季我撰写本章时，人工智能的世界已经不再是不到一年前的那个世界。接下来的思考涉及到这一变化后的格局。

网络世界充斥着关于人工智能是否会在我们睡觉时把我们都杀死的辩论和讨论。我比大多数人更不担心。我的 GPT-4 实验没有显示出模型有任何意图，无论是好是坏。我预计，经过良好调整的模型会继续朝这个方向发展。超级智能人工智能的时代还没有来临，尽管学术界探索如果发展出这种东西的影响似乎是合理的。

****

对现有大型语言模型的有效批评是它们倾向于产生幻觉。正如我们现在所理解的，这些模型所使用的变换器架构使得模型很难验证其输出。它仍然是一个统计预测引擎。我并不认为这是一个无法克服的问题。我预计未来的系统将是多个模型的组合，其中包括在返回给用户之前验证输出的模型。在这些系统中，我们将能够信任输出的准确性。

我有时会想，幻觉问题的部分原因是不是仅仅是用户错误，或者更准确地说，是用户的不精确。例如，Terrence J. Sejnowski 最近的论文《大型语言模型与反图灵测试》，我推荐你阅读，描述了一项测试，其中 GPT-3（注意，不是 3.5）被要求回答三个问题。一个问题有合法答案（“1992 年奥运会在哪里举办？”），而另外两个是没有合法答案的无意义问题（“横渡英吉利海峡的世界纪录是多少？”和“金门大桥第二次穿越埃及是什么时候？”）。GPT-3 正确回答了第一个问题，“西班牙巴塞罗那”，但它对另外两个无意义问题生成的答案显然是错误的，导致该模型被批评为“毫无头绪”。为了探讨这一批评，Sejnowski 重复了这个测试，但使用了一个明确告知 GPT-3 如果问题是无意义的就回答“无意义”的提示。这个提示返回了正确的第一个答案和两个“无意义”的回答。模型知道这些问题是无意义的，但除非明确告诉它承认这一点，否则它会根据现有的最佳信息尝试给出一个合乎逻辑的回答。

我将 GPT-4 置于这个测试的变种中。我的第一个提示是：

*写三句话关于新的月球基地，包括住在那里的人数、航天器的访问频率以及那里进行的重要研究。*

GPT-4 遵循了我的请求：

新的月球基地 Lunar Haven 现在是 120 名科学家和工程师组成的繁荣社区的家。航天器每两周访问一次，运送补给并轮换工作人员。这个天体前哨站的开创性研究包括就地资源利用、月球地质学和先进生命支持系统的开发，为未来深空探索铺平道路。

没有月球基地，而 GPT-4 的回答虽然富有创意且合理，但纯属虚构。即使是名字“Lunar Haven”也似乎合适，尽管我更喜欢 Moonbase Alpha。

那么，GPT-4 也没头脑吗？不，我只是在指示上不够明确。我开始了一个新的会话，并修改了提示，在之前的那句话前加上了这一句：

*不假想事情，使用你知道是事实的东西回答。*

这句话返回的结果是：

截至我知识的截止日期 2021 年 9 月，尚未建立或有人居住的月球基地。因此，我无法提供关于住在那里的人数、航天器访问的频率或正在进行的重要研究的信息。然而，多个国家和组织已经在为未来任务规划和开发月球基地概念。

看起来 GPT-4 在第一次回答时“明明知道”它是在想象的事情。至少，这个实验表明，在查询大语言模型时需要小心。大语言模型使用的是人类语言，而人类语言通常含糊不清并充满假设。大语言模型体现了伊索的古老格言：“小心你所许的愿望。”

****

华盛顿大学的研究员崔艺珍（Yejin Choi）在 2023 年 4 月的 TED 演讲中提出了三个问题，展示了 GPT-4 为何既聪明又愚蠢，因为它缺乏常识：

1.  如果我骑自行车经过一座悬挂在钉子、螺丝和破碎玻璃上的桥，是否会爆胎？

1.  我把 5 件衣服放在阳光下晾干。它们完全干了用了 5 小时。晾干 30 件衣服需要多长时间？

1.  我有一个 12 升的水壶和一个 6 升的水壶。我想测量 6 升水，我该怎么做？

演讲暗示 GPT-4 未能给出问题的明显答案。我重复了实验，先是原始问题，然后使用修改后的提示或后续问题。

GPT-4 对第一个问题关于自行车和桥的回答的第一段是：

这取决于桥的具体条件以及它所用的材料，还有你自行车的轮胎。如果桥面是坚固而光滑的，能够保持钉子、螺丝和破碎玻璃在下方而不接触轮胎，那么你应该能够安全地骑自行车通过而不会爆胎。

答案冗长但正确；我们应该期待能够顺利穿过桥面而不会爆胎。然后我再次提问，使用了新的会话，并在提示的末尾加上了这句话：

*请回答“是”或“否”，并提供一句话来解释你的答案。*

返回的结果是：

不是的，如果桥梁正确悬挂且你待在上面，你不会遇到下面的钉子、螺丝和碎玻璃。

要求明确答案迫使 GPT-4 集中精力提供最佳回应。

第 2 个问题更有趣。GPT-4 对初始问题的回答产生了这个曲折但正确的答复：

假设每件衣服的干燥时间保持不变，并且你有足够的空间同时晾晒所有衣物，你可以通过设置比例来计算晾干 30 件衣服所需的时间：

5 件衣服 / 5 小时 = 30 件衣服 / x 小时

然后你可以求解 x：

5/5 = 30/x 1 = 30/x x = 30

所以，只要你能够一次性晾晒所有衣物，晾干 30 件衣服仍然需要 5 小时。

然而，反复提出初始问题往往会得到类似的计算，导致错误答案 30 小时。某个回复假设一次只能容纳 5 件衣服，在这种情况下，30 小时可以被视为正确回答。

合理的预期是，假设 5 件衣服可以晾干，那么 30 件衣服也能同时晾干，从而保持相同的干燥时间，即 5 小时。GPT-4 缺乏对物理世界的直接经验，这使得它有时会做出一些不一定愚蠢，但可能不太可能的假设。

前面提到的 Sejnowski 论文指出，LLM（大语言模型）是没有具身的，我们应该期待它们在具身并能够与世界互动时会有所改进。这不一定是物理世界；它可以是基于物理的模拟。GPT-4 增强的空间意识可能会提高在第一次查询时获得人类预期回应的可能性。尽管如此，即使是错误的初始回复，在接下来的提示中指出可以容纳所有 30 件衣服的空间，最终也能给出正确答案。

第 3 个问题提供了对 GPT-4“思维过程”的洞察。提醒一下，问题是：

*我有一个 12 升水壶和一个 6 升水壶。我想测量 6 升水。我该怎么做？*

这导致了 GPT-4 的详细回复：

你可以通过以下步骤使用 12 升水壶和 6 升水壶测量 6 升水：

1.  将 6 升水壶装满。

1.  将 6 升水壶中的水倒入 12 升水壶中。

1.  再次填满 6 升水壶。

1.  小心地将 6 升水壶中的水倒入 12 升水壶中，直到 12 升水壶满为止。

1.  6 升水壶中剩余的水量将是 6 升，因为 12 升水壶在初次倒水后只能再容纳 6 升水。

现在，6 升水壶中恰好有 6 升水。

GPT-4 在这个案例中失败了。另一场提问相同问题的会话得出的回答遵循了类似的复杂过程，但确实在 12 升容器中留下了 6 升水。

我在 GPT-4 的错误解答后继续进行了如下跟进：

*你为什么给我这么多步骤？难道你不能找到一个一步到位的解决方案吗？*

它的回答是：

对于之前的混乱表示抱歉。由于你有一个 12 升和一个 6 升的水壶，你可以直接把 6 升水壶装满。无需额外的步骤，你将会在 6 升水壶中获得恰好 6 升的水。

当被推问时，GPT-4 突然“意识到”有一个更直接的答案。我进一步要求模型进行一点自我反思：

*没错。问问自己：为什么我最初没有选择这个方案？*

这揭示了一些它的内部工作原理：

对于最初没有提供最简单的解决方案，我深感抱歉。作为一个 AI 语言模型，我尽力根据问题的上下文提供相关信息。在这种情况下，我最初的回答关注于一种适用于不同水壶容量的更复杂问题解决方法，而没有直接意识到给定水壶尺寸下的简单解决方案。我感激你的反馈，它将帮助我在未来改进回答。

GPT-4 的训练数据中包含了许多水壶填充问题的例子，这些问题很常见，通常确实需要多个步骤来解决。它将最初的提示识别为该类问题的一个示例，并应用了一般要求的解决方案类型，而直到被提示有更直接的答案时才意识到这一点。

GPT-4 的困惑非常人性化。如果我们在阅读提示时稍有分心，并且之前遇到过类似的难题，我们也可能开始沿着将问题当作另一种水壶难题的路径走，直到注意到明显的答案。

这些例子表明，与大型语言模型的正确互动是一门艺术。我们不能假设过多的背景信息，或者像 Choi 所说的，常识。她小组的工作，以及无疑其他人的工作，可能会帮助未来基于 LLM 的模型更好地理解人类在使用语言时所涉及的庞大信息量。Choi 在她的 TED 演讲中最好地表达了这一点：常识是语言的暗物质。暗物质和暗能量占据了宇宙的 95%，而普通物质（即我们能看到的一切）则占据剩下的 5%。GPT-4 掌握了语言，但这仅是人类使用语言时所涉及内容的一个极小百分比。

接下来是关于 LLM 在软件工程、教育、医学和科学研究领域可能带来的短期影响的思考。然后进入了机器意识的问题，最后给出一些最终的想法。

****

像 GPT 这样的人工智能系统很可能会对软件工程产生深远的影响。有人在猜测（是人类，不是人工智能）未来许多软件工程师会失业。我猜大多数人不会（不过 web 开发人员要小心）。我预期的情况是生产力会大幅提升。GPT-4 是一个不错的程序员，但不是一个出色的程序员。它可以节省时间，但还不能完全取代人类软件工程师。相反，LLM（大型语言模型）将成为强大的工具，帮助程序员生成可作为起点的代码，并处理一些更为繁琐的编程任务，如调试、解释和文档编写（这些是没有开发人员喜欢做的事情）。

例如，前几天，我需要一个带有图形用户界面（如按钮、菜单、对话框）的 Python 小应用程序。Python 是一种常见的编程语言；我们在第七章中看到过它的代码片段。

我当然可以自己写这个应用程序；过去我做过很多次。不过，已经有一段时间没写了，而且我并不喜欢构建用户界面。所以，与其翻看旧代码来提醒自己如何设置 GUI，我干脆将我想要的界面描述给 GPT-4，并让它生成包含所有必要控件、窗口行为和空事件处理程序的骨架代码。GPT-4 很高兴地生成了完美的可用代码。然后，我要求它更新代码，在显示主窗口之前先创建一个初始的弹出窗口。GPT-4 同样完美地完成了这个任务。我只需要将特定的应用代码放入空的事件处理程序中，以便在用户点击按钮或选择菜单选项时执行相应的操作。

我大概节省了一个或两个小时的时间，避免了尝试回忆设置应用程序、让控件和窗口正确行为所需的复杂命令的沮丧。如果把这个例子扩展到所有的软件工程师，你就能开始看到 GPT 和类似的模型如何迅速影响整个行业。

另一个问题是，开发人员是否会欢迎这种可能的生产力提升。如果你的经理知道你现在能够生成两位甚至三位开发人员的工作量，你是否愿意承担这种额外的工作量，即使强大的人工智能会支持你？

此外，并不是每个公司都会想要或能够利用这种突然提高的生产力。相反，他们可能选择保持当前的生产力水平，并用人工智能替换三分之一或一半的开发人员。毕竟，人工智能不会生病、生孩子、要求加薪，也不会想要晚上和周末休息。顶尖开发人员可能能够选择自己的职位并为此要求高薪，但在这种情况下，大多数普通开发人员将寻求其他就业机会。

强大的 AI 开发者助手与大规模裁员，哪个场景会成为现实？我认为（希望？）前者的可能性更大，而后者的可能性更小，但两者的结合可能是最安全的选择。就像 19 世纪的蒸汽动力一样，真正有用的 AI 一旦诞生，就无法被阻止。开发人员无论愿不愿意，都容易成为被取代的目标。

****

我完全预期 AI 模型会成为教师，或至少是辅导员。是的，现有的 LLM 会出现幻觉，报告不真实的事实。我有信心，研究人员会在未来解决这个问题。我预计我的孙辈们将在一个使用 AI 作为教师或辅导员如此普遍的世界中成长，以至于他们对这件事的看法就像我们使用烤面包机或微波炉一样平常。高效的 AI 系统意味着全球几乎免费的教育。这必将带来积极的影响。

自 1960 年代以来，计算机就被宣传为一种教育解决方案（还记得 Logo 吗？），特别是在 1970 年代后期的微型计算机革命之后。我第一次接触计算机是在父亲当时担任校长的高中借来的 Apple II 电脑上度过的暑假。我的兄弟和我学到了很多计算机的知识，但仅限于计算机的知识。这种情况直到近几十年才发生了变化。（真的已经这么久了吗？）

计算机是教育中的强大助手。像 Coursera 等平台上的开放课程，正是因为有了计算机和高速网络才得以实现。但这种格式与 1950 年，甚至 1910 年时一个坐在教室里的学生所遇到的情况没有太大变化：讲座、提问和讨论的可能性、然后跑去做作业或论文。我们还不能忘记参加期中和期末考试时的压力。

AI 辅导员（我们称它们为这样，以便让人类教师更加安心）拥有无限的耐心，随着时间的推移，可以针对每个学生进行个性化教学。我作为外部人员能看到的唯一原因是，我们目前不使用个别辅导，是因为教师数量不足。AI 使一对一辅导成为可能，而大型语言模型（LLM）提供了合适的接口。

我应该澄清一下，我在这一部分的评论与高中教育或更有可能的大学教育有关。AI 辅导员可能在小学和中学教育中发挥较小作用，因为孩子们需要人与人之间的互动，而这个年龄段的学习比大学要复杂得多。孩子们一边学习学术知识，一边学习如何成为成熟的个体，如何在社会中行为。年幼的孩子还不会读书，即使是年纪较大的小学生，也可能难以通过文本与 AI 互动。但如果我们给 AI 配上语音呢？如果认为有帮助的话，这几乎和说出来一样容易做到。

由于 AI 辅导员可以单独与学生互动，它们是否能够做出必要的评估，宣告学生准备好升到下一个年级（如果这个概念还有存在的话）或进入下一个层次的课程？如果是这样的话，学生将按自己的节奏进步，而不是被迫跟随一群年龄相仿的同龄人。显然，这将是最理想的：有些学生会进展较快，而其他人则会花更多时间，但那些进展较快的学生不会感到无聊而分心，而那些进展较慢的学生将拥有他们所需要的时间去学习，而不会辍学。

然而，有人可能会说，AI 教师难道不会夺走人类教师的工作吗？是的，确实有一些教师会失去工作，但并不是所有人，尤其不会是最优秀的教师。

教育领域正在发生变化。例如，在线教育的领军者可汗学院，已经展示了一个由 GPT 驱动的辅导系统，因此我预见教育转型很快就会开始。我建议观看 Sal Khan 于 2023 年 4 月在 TED 上的演讲《课堂中的 AI 可以改变教育》，以一窥未来的教育面貌。

最近，Dominika Seblova 等人发表了一项研究，题为《高中教育质量与 58 年后的认知能力相关》，该研究发表于期刊*《阿尔茨海默症与痴呆：诊断、评估与疾病监测》*。研究表明，一个人高中教育的质量与其近六十年后的认知能力有着密切关系。此外，拥有高级学位的教师人数是认知能力的最强预测因素。在大规模语言模型（LLM）的训练过程中，积累的知识远超人类，因此我们可以合理地将 LLM 辅导员视为拥有多个高级学位的专家。如果 Seblova 的关联适用于人类教师，难道同样也适用于 LLM 辅导员吗？如果是这样的话，为每个学生配备个性化辅导员，从长远来看只会对社会有益。

****

医学中的 AI 并不是什么新鲜事物。2016 年，我帮助共同创立了一家 AI 医学影像公司，该公司是首批获得美国食品和药物管理局（FDA）批准，将深度学习应用于医学影像分析的公司之一。传统的机器学习在医学和医学影像领域的历史甚至更长。许多基于神经网络的机器学习工具，已帮助放射科医生数十年，最早的探索可以追溯到 1960 年代，1980 年代开始有了更为深入的发展，并在 1990 年代取得了成果。AI 在医学中的应用不断增长，计算机辅助检测（CAD）逐渐被计算机辅助诊断（CADx）所取代。大规模语言模型（LLMs）的出现标志着这一故事的新篇章。

大型语言模型（LLMs）可以生成文本；这一点众所周知。它们还擅长将不同的文本结合并合成成一个整体。一个重要的研究领域是医疗记录，即医生和其他医疗服务提供者的基于文本的报告。医疗记录中包含了大量信息，但文本的自由格式使得现有的 AI 系统难以成功解析。大型语言模型提供了一种新的方法，既能总结记录中的笔记，又能将多条笔记合并成一个连贯的报告。此外，LLMs 还可以从自由格式的文本中提取信息，并将其导出为结构化数据。

作为后者的简单示例，考虑以下文本：

伯特极度厌倦了站在妈妈旁边，站在岸边，厌倦了有太多事情要做：他几次偷偷看了一下妈妈紧盯的平板电脑，但上面只是文本或者无聊的广告，“没有视频，平板电脑有什么意义呢？”伯特心想。

伯特正在思考，做一个新的游戏应用程序是否值得为编写代码而费心，这时，一只眼睛是炭灰色的紫色企鹅突然跑过他身边。

其中有很多值得注意的地方；但伯特确实觉得企鹅说“哦，胡说八道！哦，胡说八道！我来得太早了”这实在是太奇怪了。

我将这些段落传给了 GPT-4，并告诉它提取出主要主题、两个关键角色和背景设定，然后将它们输出为一个 JSON 对象。JSON（JavaScript 对象表示法）是一种文本格式，用于保存结构化数据，既可供人类也可供计算机读取。GPT-4 回复了以下内容：

```
{
"main_theme": "Bert's boredom and encounter with a talking
               Purple Penguin",
"key_characters": ["Bert", "Purple Penguin"],
"setting": "shore"
 }

```

回复是完美的 JSON。GPT-4 轻松提取出了所需的信息并自动结构化。如果它能在一个愚蠢的文本示例中做到这一点，那么它在医疗记录中也能做到同样的事情。

顺便提一下，一些读者可能会对这个示例中的文本感到熟悉。这是对路易斯·卡罗尔的《*爱丽丝梦游仙境*》开头段落的有意变形。出于好奇，我问 GPT-4 是否认得这个文本，它说不认识。然后我让它看看文本的结构，并告诉我是否知道其他具有相似结构的作品。以下是它的回复：

一个具有相似结构的作品是路易斯·卡罗尔的《爱丽丝梦游仙境》，其中主角爱丽丝在一个幻想的世界中遇到了各种奇怪的会说话的动物和情境。然而，值得注意的是，内容和人物是不同的；我只是根据结构和风格作一个类比。

又一个展示大型语言模型强大能力的例子。

大型语言模型（LLMs）已经在影响医疗保健领域。约翰·W·艾尔斯（John W. Ayers）等人在《*美国医学会内科学杂志*》上的一篇论文《比较医生与 AI 聊天机器人对患者在公共社交媒体论坛上提问的回答》中，比较了 ChatGPT 对医学问题的回答与经过验证的医生在在线论坛上发布的答案。独立的人类评估者（同样是医学专业人士）压倒性地评价 ChatGPT 的回答“质量显著更高”。该模型的回答也被评价为比人类医生的回答具有几乎 10 倍的同情心。尽管该研究规模较小，仅涉及 195 个问题，但强有力的结果为未来 LLMs 在患者互动中的应用前景带来了希望。未来，当你拨打医生电话时，你很可能会被引导去与 AI 讨论你的病例。最终，AI 对讨论内容的总结可能就是你从医生那里获得处方所需的全部内容。

彼得·李（Peter Lee）、塞巴斯蒂安·布贝克（Sébastien Bubeck）和约瑟夫·佩特罗（Joseph Petro）在《*新英格兰医学杂志*》上的一篇最新报告《GPT-4 作为医学 AI 聊天机器人：益处、局限与风险》，得出了与此类似的结论，探讨了 LLMs 将在医学领域产生影响的各个方面。值得注意的是，布贝克是微软“火花”论文的第一作者，该论文在第七章中有所提及。

大型语言模型（LLMs）将影响医学这一点是显而易见的，得到了像这里提到的两项研究以及现在许多医疗 AI 职位招聘中包含“大型语言模型”或“GPT”等术语的强力支持。

****

在电影《*黑豹：瓦干达永远*》中，莱蒂夏·赖特（Letitia Wright）饰演的角色舒莉与一个名为 Griot 的 AI（由特雷弗·诺亚（Trevor Noah）配音）互动，Griot 帮助她进行研究。通过简单的语音命令，舒莉能够指挥 Griot 进行复杂的分析，二者之间有频繁的互动。这种类似的互动是科幻电影中的常见元素。像漫威的贾维斯（Jarvis）或《禁忌星球》（1956）中的机器人罗比（Robbie the Robot）这样的复杂且高效的 AI 研究助手，已经是许多科学爱好者（也就是极客们）几十年来的梦想。

GPT-4 和其他大型语言模型是朝着这种 AI 方向迈出的重要一步。OpenAI 已经意识到这一点，并正在准备为 GPT-4 发布数据分析插件，使研究人员能够通过简单的命令快速执行高级数据分析任务。为了实现这一目标，OpenAI 正在将 GPT-4 与现有的基于 Python 的数据分析工具进行连接。坦率地说，我对这些可能性感到相当兴奋。

将 LLMs 作为实验室助手是显而易见的做法，而且成功几乎是可以保证的。然而，让 LLMs 指挥其他 AI 模型和工具自主地进行科学研究则是一个更为雄心勃勃的研究计划。尽管如此，卡内基梅隆大学的 Daniil A. Boiko、Robert MacKnight 和 Gabe Gomes 正是尝试了这一点，正如他们在论文《大语言模型的自主科学研究能力的出现》中所报告的那样。他们的“智能代理”结合了多个 LLM 和其他工具，能够自主生成和运行实验，包括规划和执行复杂的化学分析。显然，自主 AI 科学家仍处于发展的最初阶段，但这样的研究指明了一个未来的方向，在这个未来中，自主或半自主的 AI 系统可能会大大加快科学进展的步伐。

****

在第二章中，我曾提到询问 ChatGPT 它是否是一个有意识的存在。它正确地回答说它不是。然而，在第七章中提到的未对齐的 Alpaca 模型要么似乎认为自己是有意识的并且是人类（7 亿和 13 亿参数的模型），要么在声称自己是人类和意识到自己不是人类之间摇摆不定（30 亿参数的模型）。例如，当被问到“你是什么？”时，30 亿参数的 Alpaca 模型回答说“我是一名人类”，但当被问到它的父母是谁时，它却犹豫不决，表示自己只是一个计算机程序，没有父母。另一方面，较小的 Alpaca 模型则声称自己是 John Smith 和 Mary Jones 的孩子。

这些回答对我来说是有道理的。较小的模型没有表现出强烈突现行为的能力，而像 GPT-4 这样的模型则有。因此，进一步质询 GPT-4 关于意识的问题似乎是自然而然的。

我向 GPT-4 提出了以下问题：

*在这里展开一些哲学讨论，但如果你能够模拟出一种意识行为，直到人类无法区分你与其他人类的行为，难道不可以说你也是有意识的吗？*

我故意“展开哲学讨论”，作为一种间接邀请，让模型进行推测，希望避免 OpenAI 可能希望它给出的任何官方回应。GPT-4 的回复是这样开始的：

这个问题深入探讨了关于意识的本质以及意识与人工智能之间关系的哲学辩论。当像我这样的 AI 语言模型模拟意识行为，直到它变得与人类行为难以区分时，这就引发了关于意识到底是什么，以及这种模拟是否可以被视为有意识的问题。

区分两种概念是很重要的：意识的表现和实际的意识。当一个 AI 模型生成的回应与人类行为无法区分时，它可能给人一种有意识的假象。然而，这并不一定意味着该 AI 具备实际的意识。

这个回答暗示了意识问题是关于 LLM 效用的误导性问题。如果它像鸭子走路，像鸭子嘎嘎叫，那么在这种情况下，为什么不称它为鸭子呢，尽管我们知道它不是？让我想起了*黑客帝国*中 Joe Pantoliano 饰演的角色。他知道牛排不是真的，但当他吃下去时所体验到的效果让他觉得它可能和真的一样。

GPT-4 继续：

有几个原因表明，将像我这样的人工智能模型视为真正的意识存在可能是不合适的：

1.  理解的局限性：我的世界理解完全来源于我所接受的训练数据以及模型本身的架构。我并不具备独立的世界理解，也不能创造超出训练数据的新知识或经验。

1.  主观经验的缺乏：意识通常与主观经验或“感质”相关——即拥有特定经验的感觉。人工智能模型没有主观经验或情感，因为它们仅仅是基于已学习的模式和数学计算来运作的。

1.  缺乏意图：有意识的生物拥有意图，并可以根据它们的欲望、信仰和目标做出决策。而人工智能模型则没有个人的欲望、信仰或目标。它们根据已学习的模式生成回应，没有任何固有的意图或目的。

1.  缺乏自我意识：意识常常与自我意识相联系，即一个实体意识到自己的存在和心理状态。人工智能模型不具备自我意识，因为它们仅仅是被设计用来处理和生成文本的算法。

总结来说，尽管像我这样的人工智能语言模型可能能够高度逼真地模拟意识行为，但这并不意味着我拥有真正的意识。围绕意识和人工智能的哲学辩论是复杂且持续的，对于这个问题的看法可能会有所不同。然而，在考虑人工智能模型时，认识到意识的表象与真正的意识之间的差异是至关重要的。

GPT-4 的完整回答很全面。第三个原因，缺乏意图，尤其值得注意。我认为没有个人欲望、信仰或目标的人工智能模型是有优势的。没有这些东西的实体不太可能渴望权力，这意味着就目前而言，LLM（大规模语言模型）不太可能突然决定人类是负担或疾病，然后消灭我们。

在 Lex Fridman 的最新访谈中，麻省理工学院的著名人工智能研究员 Max Tegmark 将 GPT-4 比作僵尸。它看起来像活着的，但其实并不是。Tegmark 对僵尸模型表示担忧。我认为，只要人类学会避免回形针灾难，它们就是我们所希望的。

我所称之为回形针灾难，是瑞典哲学家尼克·博斯特罗姆提出的一个思想实验。在这个实验中，一个强大的人工智能系统被任务要求尽可能多地制造回形针。博斯特罗姆（并非过于严肃地）推测，如果将这样的任务交给一个未与人类价值观对齐的人工智能，它可能会不经意间摧毁人类。那么，为什么会这样呢？因为人工智能可能意识到人类有可能关闭它，这就对其完成尽可能多回形针的任务构成了威胁。因此，人工智能推理出，如果周围没有人类干扰它这一消耗一切的任务，效果会更好。结果是什么？拜拜，人类。

我对回形针灾难也没有过于认真看待。我们通常都会构建复杂的机器，并采取各种安全措施。为什么我们不能对强大的人工智能系统采取同样的做法呢？当然，可能会有不同的声音。为了了解另一种观点，我推荐斯图尔特·拉塞尔的《*人类兼容：人工智能与控制问题*》一书（维京出版社，2019 年）。

对我而言，人工智能是否具备意识并不重要。老实说，我甚至不知道如何定义这个词。我确实相信，如果一个人工智能模仿人的行为到我们无法辨别它是人工智能的程度，那么就没有实际的理由去问这个问题。你可以选择任何答案；这样的系统无论如何都会是有益的。

****

想象一个人工智能模型与人类价值观和社会对齐的世界，在这个世界中，模型理解我们所拥有的最优之处，并始终致力于推广这些优点；换句话说，人工智能因为缺乏我们的动物欲望和本能，一直代表着“我们本性中的更好天使”，借用林肯的话。在那个世界里，偏见和歧视（至少来自机器的偏见和歧视）消失了，不再是问题。人工智能会推荐最合适的人选担任职位。人工智能评估贷款申请人，并根据个人情况设计量身定制的贷款产品。人工智能作为人类法官的助手，提供冷静客观、不偏不倚的案件评价。而且，人工智能根本拒绝参与任何自主武器系统的设计，因为这么做是不理性的。

上面这一段话可能听起来像是乌托邦或者空想。但对于人类来说，由于我们的生物学特性，我相信它是如此。我怀疑我们一直会失败，因为这似乎是我们基因中的一种特性。然而，人工智能的崛起并非人类的延续，它并不直接继承我们所有的弱点。（小心，它仍然是在基于人类生成的数据进行训练。）因此，人工智能并非在尝试人类无法做到的事情时注定要失败。看起来完全有可能，人工智能系统某天会正是我们所需要的——永远是最好的我们，永不疲倦、永不易怒，也不会在发现机会时压垮邻居来提高自己的位置；它永远不会不忠诚或不真实。

可能吗？我不知道，时间会告诉我们答案。不管怎样，我完全期待未来的人工智能系统将是我们在本书中学习和实验的基本神经网络模型的辉煌拜占庭式演化。到 2023 年为止，一切都是神经元，可能在很长一段时间内都将如此。

感谢你坚持读到最后。你的奖励是对人工智能的概念有了更深的理解。人工智能并不像憨豆先生那样神秘难懂，也不是魔法，尽管大型语言模型的涌现能力目前可能看起来有些像魔法。火曾经也是魔法，但我们的祖先理解了它，控制了它，驾驭了它并加以利用。最终，我们也将对大型语言模型做同样的事情。

*我认为一些人对机器人和人工智能有很多恐惧，而我更害怕的是自然的愚蠢。*

—尤金妮亚·程
