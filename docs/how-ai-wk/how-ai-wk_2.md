## 第三章：**为什么是现在？人工智能的历史**

![Image](img/common.jpg)

罗温·艾金森的喜剧杰作*《憨豆先生》*开场时是在一个空无一人的伦敦街头的深夜。聚光灯打在一个人身上，主角从天而降，拉丁语合唱团唱道：“ecce homo qui est faba”——看啊，这就是一个豆子人。憨豆先生爬起来，拍掉身上的灰尘，笨拙地跑进黑暗中。他是一个异世界的存在，一个字面上从天而降、难以理解的存在。

鉴于近年来人工智能不断涌现的奇迹，我们或许会误以为人工智能就像《憨豆先生》一样，从天而降，已经完全形成且超出了我们的理解。然而，这一切都不是真的；事实上，我认为人工智能仍处于初期阶段。

那么，为什么现在我们会听到关于人工智能的讨论呢？我将通过一段简短（且带有偏见）的人工智能历史来回答这个问题，并讨论推动人工智能革命的计算技术进展。本章为我们接下来将在全书中探讨的模型提供了背景。

****

自人工智能诞生以来，它一直分为两大阵营：符号主义人工智能和联结主义。*符号主义*人工智能试图通过操作符号、逻辑陈述或关联来模拟智能。而*联结主义*则通过构建简单组件的网络来模拟智能。人类大脑体现了这两种方法。我们使用符号作为思维和语言的元素，我们的大脑由极其复杂的神经元网络构成，每个神经元都是一个简单的处理器。在计算机编程术语中，符号主义人工智能的方法是自上而下的，而联结主义则是自下而上的。自上而下的设计从高级任务开始，然后将任务拆解成越来越小的部分。自下而上的设计从较小的部分开始，并将它们组合在一起。

符号主义人工智能的支持者认为，智能可以在抽象层面上实现，而不需要类似大脑的基质。联结主义者则遵循大脑的进化发展，认为必须有某种基础，比如一个庞大且高度互联的神经元网络，从中可以产生智能（无论如何定义）。

虽然符号主义人工智能与联结主义的辩论持续了很长时间，但随着深度学习的出现，可以肯定地说，联结主义者已经赢得了这场战斗——尽管可能还不是最终的胜利者。近年来，出现了一些融合这两种方法的论文。我猜测符号主义人工智能还有一两次短暂的亮相，甚至可能最终以配角身份登场。

我在 1980 年代末接触到的人工智能完全是符号主义的。联结主义被提到作为另一种方法，但当时认为神经网络不如符号主义，并且最多只会是边缘性有用的技术。

人工智能的完整历史超出了我们的讨论范围。这样的宏篇巨著等待着一位有动力且有能力的历史学家来完成。相反，我将聚焦于机器学习的发展，同时（非常不公平地！）忽略了那些在符号派阵营中付出数十年努力的工作。然而，请知道，在人工智能的大部分历史中，人们主要讨论的是符号 AI，而不是联结主义。为了更公平的呈现，我推荐迈克尔·伍尔德里奇的《*人工智能简史*》（Flatiron Books, 2021），或者帕梅拉·麦考德克在《*这可能很重要：我与人工智能知识分子的生活与时代*》（Lulu Press, 2019）中的深刻个人叙述。

鉴于我明显的联结主义偏见，让我们一起回顾一下机器学习的历史。

### ***1900 年之前***

智能机器的梦想可以追溯到古代。古希腊人讲述了塔罗斯的神话，塔罗斯是一个巨大的机器人，用来守护腓尼基公主欧罗巴。在中世纪和文艺复兴时期，许多自动机——会动且看起来栩栩如生的机器——被开发出来。然而，我怀疑当时没有人认为这些机器是智能的或具备思维能力的。有些甚至是骗局，比如臭名昭著的“机械土耳克”，它通过与许多高水平的棋手对弈并击败他们，令世人惊叹。最终，人们发现机器中隐藏着一个人，他通过操控机械臂在棋盘上移动独立的棋子，同时从下面观察棋盘的配置，来控制这个“自动机”。尽管如此，这台机器的机械部分对于 18 世纪末期来说，仍然相当令人印象深刻。

除了自动机之外，还有早期的尝试试图将思维理解为一种机械过程，并且努力构建能够捕捉思维的逻辑系统。在 17 世纪，戈特弗里德·莱布尼茨抽象地描述了这样的概念，称其为“思维的字母表”。在 1750 年代，朱利安·奥夫雷·德·拉梅特里出版了《*人类机器*》（*L’Homme Machine*），主张思维是一个机械过程。

认为人类的思维可能来源于大脑的物理实体而非精神灵魂的想法，标志着通向人工智能的新篇章的开始。如果我们的思维是生物机器，为什么不能有另一种能够思考的机器呢？

在 19 世纪，乔治·布尔尝试创造一种思维的演算法，最终产生了我们现在所知的布尔代数。计算机依赖于布尔代数，甚至到它代表着计算机的实现形式——数字逻辑门的集合。布尔部分成功了，但他并未达成自己的目标：“调查那些思维运作的基本法则，通过这些法则进行推理；并用演算符号语言加以表达”（《思维的法则》，1854 年）。布尔愿意尝试的精神，代表了朝向人工智能可能性的又一步迈进。

这些早期尝试所缺乏的是一个真正的计算机器。人们可以梦想人工智能或生物（如玛丽·雪莱的*弗兰肯斯坦*中的生物），并假设它们的存在，讨论其后果。但在没有一个能够合理模仿（实现？）思维的机器之前，其他一切都只是猜测。

正是英国人查尔斯·巴贝奇在 19 世纪中期首次构想到一个可实现的通用计算机器——分析机。尽管这台机器从未完全建成，但它包含了现代计算机的所有基本组件，并且理论上能够执行相同的操作。虽然不清楚巴贝奇是否意识到他那台机器的潜在多功能性，但他的朋友艾达·洛夫莱斯确实意识到了这一点。她将这台机器描述为一种广泛适用的通用设备。尽管如此，她并不认为这台机器能够进行思考，正如她在 1843 年所写的《分析机概述》中的这段话所表明的：

分析机完全没有任何原创的野心。它可以做我们知道如何指令它执行的任何事情。它可以进行分析；但它没有预见任何分析关系或真理的能力。它的职能是帮助我们利用我们已经了解的东西。

这句话可能是首次提到人工智能的可能性，并涉及一个可能实现这一目标的设备。短语“做我们知道如何指令它执行的任何事”意味着编程。事实上，洛夫莱斯为分析机编写了一个程序。正因为如此，许多人认为她是第一位计算机程序员。她的程序中有一个漏洞，这让我相信她确实是；没有什么比漏洞更能代表编程了，正如我 40 多年的编程经验所痛苦地证明的那样。

### ***1900 到 1950***

1936 年，当时还是学生的 24 岁英国人艾伦·图灵，写了一篇后来成为计算机科学基石的论文。在这篇论文中，图灵提出了一个通用的概念机器，今天我们称之为*图灵机*，并证明它可以计算任何由算法表示的事物。他还解释了有些事物是算法无法实现的，因此是不可计算的。由于所有现代编程语言都等同于图灵机，现代计算机可以实现任何算法并计算任何可计算的东西。然而，这并没有说明计算可能需要多长时间或需要多少内存。

如果计算机能够计算任何可以实现为算法的事情，那么计算机就能执行人类能够执行的任何思维操作。最终，这就是可能实现真正人工智能的引擎。图灵在 1950 年发表的论文《计算机器与智能》首次认识到数字计算机最终可能会导致智能机器的诞生。在这篇论文中，图灵描述了他的“模仿游戏”，现在被称为*图灵测试*，通过该测试，人类可能会认为一台机器是智能的。近年来，关于通过图灵测试的人工智能系统有很多宣称，其中之一就是 OpenAI 的 ChatGPT。然而，少数人会倾向于认为 ChatGPT 是真正智能的——换句话说，我怀疑这个测试未必能捕捉到人类普遍理解的“智能”含义，未来可能会出现新的测试。

1943 年，沃伦·麦卡洛克和沃尔特·皮茨写下了《神经活动中固有思想的逻辑演算》，这篇论文的标题可以说是最晦涩却引人入胜的之一。这篇论文将“神经网络”（即神经元的集合）表示为数学中的逻辑语句。这些逻辑语句难以解析（至少对我来说是这样），但作者对“没有回路的网络”的描述与我们在第四章中将要探讨的神经网络非常相似——实际上，有人可以认为麦卡洛克和皮茨的开创性论文导致了我们今天所认知的神经网络。坦率来说，神经网络比这篇论文更容易解析和理解，这对我们来说是个好消息。

从关于人工智能机器和生物的幻想故事，到严肃探讨数学是否能够捕捉思维和推理，再到意识到数字计算机能够计算任何可以通过算法描述的事务，这一系列进展为人工智能作为一个合法的研究领域的出现奠定了基础。

### ***1950 到 1970***

1956 年达特茅斯夏季人工智能研究项目研讨会通常被认为是人工智能的发源地，也是“人工智能”这一术语首次被一致使用的地方。达特茅斯研讨会的参与者不到 50 人，但名单中包括了计算机科学和数学领域的几位知名人物：雷·所罗门诺夫、约翰·麦卡锡、马文·明斯基、克劳德·香农、约翰·纳什和沃伦·麦卡洛克等人。当时，计算机科学是数学的一个子领域。该研讨会是一次头脑风暴，为早期的人工智能研究奠定了基础。

1957 年，康奈尔大学的弗兰克·罗森布拉特创造了马克 I 感知机，被广泛认为是神经网络的首次应用。感知机在许多方面都非常出色，包括它被设计用于图像识别，这也是深度学习在 2012 年首次证明自己有效的应用领域。

图 2-1 展示了在《感知机操作手册》中给出的概念性组织结构。感知机使用一个 20×20 像素的数字化电视图像作为输入，然后通过一组“随机”连接传递到一组关联单元，最终导致响应单元的输出。这个配置与今天一些深度学习图像处理方法类似，且类似于一种称为*极限学习机*的神经网络类型。

![Image](img/ch02fig01.jpg)

*图 2-1：Mark I 感知机的组织结构*

如果感知机走在正确的道路上，为什么它在几十年里几乎被遗忘了呢？其中一个原因是罗森布拉特的夸张言辞。1958 年，美国海军（感知机项目的资助方）组织的一次会议上，罗森布拉特的言论夸大其词，以至于*纽约时报*报道了以下内容：

海军今天揭示了一款电子计算机的雏形，预计它将能够走路、说话、看见、写作、自我复制，并意识到自己的存在。后来，感知机将能够识别人物并呼唤他们的名字，甚至瞬间将一种语言的语音翻译为另一种语言的语音和文字，预计如此。

这些评论在当时引起了许多人的不满，尽管现代的人工智能系统确实允许机器走路、说话、看见、写作、识别人物，并且能够在不同语言之间进行语音和文字的翻译，也许我们应该对罗森布拉特宽容一些。他只是早了约 60 年。

几年后，1963 年，伦纳德·厄尔和查尔斯·沃斯勒描述了一个程序，类似于感知机，能够解释作为 0 和 1 的矩阵表示的 20×20 像素图像。与感知机不同，这个程序能够生成必要的图像特征模式和组合，以便学习其输入。厄尔和沃斯勒的程序与 30 多年后出现的卷积神经网络相似，后者是第五章的主题。

我所称之为“经典”机器学习模型的第一个出现是在 1967 年，得益于托马斯·科弗和彼得·哈特。他们提出的*最近邻*是所有机器学习模型中最简单的一种，甚至可以说有些令人羞愧。为了标记一个未知输入，它只需找到最相似的已知输入，并将该输入的标签作为输出。当使用多个邻近的已知输入时，这种方法被称为*k 最近邻*，其中*k*是一个小数值，例如 3 或 5。哈特后来与理查德·杜达和大卫·斯托克一起于 1973 年撰写了《模式分类》的第一版；这本开创性著作使许多计算机科学家和软件工程师（包括我）接触到了机器学习。

感知机的成功在 1969 年戛然而止，当时马文·明斯基和西摩·帕珀特发布了他们的书籍《*感知机*》，书中展示了单层和双层感知机网络无法处理有趣任务的事实。我们稍后会解释“单层”和“双层”是什么意思。与《*感知机*》一书一起，1973 年詹姆斯·莱特希尔发布的《人工智能：通用调查》（通常被称为“莱特希尔报告”）共同引发了现在所称的第一次 AI 寒冬；人工智能研究的资金迅速枯竭。

明斯基和帕珀特对感知机模型的批评是有道理的；然而，许多人忽略了他们的观察——这些限制并不适用于更复杂的感知机模型。无论如何，损害已经造成，联结主义几乎在 1980 年代初期消失。

请注意“几乎”。1979 年，福岛邦彦发布了一篇论文，该论文于 1980 年被翻译成英文，名为“Neocognitron: A Self-Organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position”。“Neocognitron”这个名字并未广泛传播，这也许是过去三十年计算机科学中“tron”后缀的最后一次使用。虽然乌尔和沃斯勒在 1963 年的程序与卷积神经网络有一些相似之处，但对许多人而言，Neocognitron 才是最初的形式。卷积神经网络的成功直接引发了当前的 AI 革命。

### ***1980 到 1990***

在 1980 年代初，随着专门为运行 Lisp 编程语言设计的计算机的出现，人工智能开始商业化，Lisp 曾是人工智能的通用语言。（如今，它是 Python。）随着 Lisp 机器的出现，*专家系统*也崛起——这些软件旨在捕捉专家在特定领域的知识。人工智能的商业化使得第一次 AI 寒冬得以结束。

专家系统背后的概念，确实具有吸引力。例如，要构建一个诊断特定癌症类型的专家系统，你首先采访专家，提取他们的知识并将其整理成知识库。知识库将知识表示为规则和事实的组合。然后，你将知识库与推理引擎结合，推理引擎根据存储的事实或用户输入系统的信息来决定何时以及如何执行规则。规则根据事实触发，这可能导致将新的事实加入知识库，从而触发更多规则，依此类推。一个经典的专家系统示例是 CLIPS，这是 NASA 在 1985 年开发的，并于 1996 年发布到公共领域。

在专家系统中，并没有连接主义网络或单元集合可以（希望）促使智能行为的产生，这使得它成为符号 AI 的一个良好示例。相反，知识库本质上是一个僵化的规则集合，比如“如果发动机温度超过这个阈值，那么另一个原因很可能是这个”，以及事实，比如“发动机温度低于阈值”。知识工程师是专家与专家系统之间的纽带。从专家对知识工程师提出的问题的回答中构建知识库是复杂的，而且随着时间推移，得到的知识库难以修改。然而，设计专家系统的难度并不意味着它们毫无用处；它们仍然存在，主要以“业务规则管理系统”的形式存在，但目前对现代 AI 的影响很小。

围绕专家系统的炒作，加上早期的成功，推动了 1980 年代初期对 AI 的重新关注。但当人们发现专家系统过于脆弱，无法具有广泛的应用时，整个行业陷入低谷，AI 的第二次寒冬在十年中期到来。

在 1980 年代，连接主义者占据了背景位置，但他们并没有停滞不前。1982 年，John Hopfield 展示了现在被称为 Hopfield 网络的模型。*Hopfield 网络*是一种神经网络，它以分布式的方式存储信息在网络的权重中，然后在稍后的时间提取这些信息。Hopfield 网络在现代深度学习中并不广泛使用，但它们证明了连接主义方法的实用性。

在 1986 年，David Rumelhart、Geoffrey Hinton 和 Ronald Williams 发布了他们的论文《通过反向传播误差学习表示》，该论文概述了用于训练神经网络的反向传播算法。训练神经网络涉及调整神经元之间的权重，以使网络按预期工作。反向传播算法通过计算调整特定权重对网络整体性能的影响，成为使这一过程高效的关键。有了这些信息，就可以通过应用已知的训练数据来迭代地训练网络，然后在分类时使用网络的误差来调整权重，迫使网络在下一次迭代中表现得更好。（我将在第四章中更深入地讨论神经网络训练。）通过反向传播，神经网络可以远远超越 Rosenblatt 的感知机的有限表现。然而，即使有了反向传播，1980 年代的神经网络仍然不过是一些玩具。关于谁发明了反向传播以及何时发明，仍然存在争议，但 1986 年的论文通常被认为是最能影响神经网络研究者的展示。

### ***1990 到 2000***

第二次人工智能寒冬延续到 1990 年代，但象征主义和联结主义阵营的研究依然在进行中。科琳娜·科尔特斯（Corinna Cortes）和弗拉基米尔·瓦普尼克（Vladimir Vapnik）于 1995 年将机器学习社区引入了*支持向量机（SVM）*。从某种意义上讲，SVM 代表了经典机器学习的巅峰。在 1990 年代至 2000 年代初期，SVM 的成功遏制了神经网络的发展。神经网络需要大量的数据集和显著的计算能力；而 SVM 则通常对资源的要求较少。神经网络通过网络的能力来表示一个函数，即从输入到期望输出的映射，而 SVM 则利用巧妙的数学方法简化了复杂的分类问题。

支持向量机（SVM）的成功在学术界和更广泛的软件工程领域都得到了认可，尤其是在涉及机器学习的应用逐渐增多的背景下。虽然公众对这些进展了解甚少，但智能机器仍然频繁出现在科幻作品中。

这场人工智能寒冬在 1997 年以 IBM 的深蓝超算战胜当时的世界象棋冠军加里·卡斯帕罗夫（Garry Kasparov）而结束。当时，几乎没有人认为机器能打败最强的国际象棋选手。有趣的是，十年前，我的一位教授曾预测，人工智能将在 2000 年前实现这一壮举。这个教授是预言家吗？其实不然。深蓝将快速定制硬件与复杂的软件结合，并应用了已知的人工智能搜索算法（特别是 Minimax 算法）。结合启发式方法和其他国际象棋大师提供的丰富定制知识，深蓝能够通过搜索比任何人类选手更多的可能走法，超越人类对手的评估。无论如何，深蓝的核心实施了人工智能专家们所知道的*如果机器有足够资源就能打败人类*的策略。深蓝的胜利是不可避免的，因为研究人员预计计算机会变得足够快，最终超越人类的能力。所需的东西是已知的；只剩下将这些要素整合在一起。

1998 年，扬·勒昆（Yann LeCun）、莱昂·博图（Léon Bottou）、约书亚·本吉奥（Yoshua Bengio）和帕特里克·哈夫纳（Patrick Haffner）发表了《基于梯度的学习应用于文档识别》的论文，虽然这篇论文没有引起公众的注意，但它对人工智能和世界而言是一个划时代的时刻。福岛的 Neocognitron 与卷积神经网络（CNN）有着强烈的相似性，而这些卷积神经网络正是启动现代人工智能革命的关键技术，这篇论文直接介绍了卷积神经网络以及我们在第一章中使用的（臭名昭著的）MNIST 数据集。1998 年卷积神经网络（CNN）的出现引发了一个问题：为什么世界要等到 14 年后才真正关注到这一技术？我们将在本章后面回到这个问题。

### ***2000 到 2012 年***

Leo Breiman 在 2001 年通过将随机森林算法的现有部分组织成一个连贯的整体，介绍了*随机森林*，这就像 19 世纪达尔文将进化理论整合成一个完整的理论体系一样。随机森林是我们在第三章中要讨论的最后一个经典机器学习算法。如果“随机森林”让你想起第一章中的决策树，那是有原因的：随机森林实际上是由决策树组成的森林。

堆叠去噪自编码器是一种中间模型，它们是我在 2010 年接触深度学习的入门之一。一个*自编码器*是一个神经网络，它通过一个中间层传递输入数据，然后生成输出。它的目标是从中间层的编码形式重构输入数据。

自编码器可能看起来像是一个愚蠢的玩意儿，但在学习重构输入时，中间层通常会学到一些关于输入的有趣信息，这些信息能够捕捉到输入的本质，而不会关注细微的、琐碎的细节。例如，如果输入是 MNIST 数字，那么自编码器的中间层就会学习到数字的特征，而不是字母。

*去噪自编码器*类似，但我们会在将输入通过中间层之前随机丢弃一部分输入值。自编码器仍然需要学习重构整个输入，但现在它面临的任务更加困难，因为输入是不完整的。这个过程有助于自编码器的中间层发现输入的更好编码。

最终，*堆叠去噪自编码器*是由多个去噪自编码器堆叠而成，其中一个自编码器的中间层输出作为下一个自编码器的输入。以这种方式排列时，堆叠学习了输入的一个新表示，这通常有助于附加在堆叠顶部的分类器区分不同的类别。例如，在我当时的工作中，输入是图像的若干小片段，这些片段可能包含感兴趣的目标。使用经过训练的两到三层堆叠去噪自编码器，将输入转换成一个数字列表，这些数字通常能够代表输入的本质，同时忽略图像的细节。然后，这些输出会与支持向量机结合使用，以判断输入是否为目标。

### ***2012 到 2021***

深度学习在 2012 年引起了全球的关注，当时 AlexNet，这一特定的卷积神经网络架构，以仅 15%以上的错误率赢得了 ImageNet 挑战赛，远低于任何竞争对手。ImageNet 挑战赛要求模型识别彩色图像的主要主题，无论是狗、猫、割草机等。实际上，“狗”并不是一个足够的答案。ImageNet 数据集包含 1,000 个类别的物体，其中包括大约 120 种不同的狗品种。所以，正确的回答应该是“它是一只边境牧羊犬”或者“它是一只比利时马利诺犬”。

随机猜测意味着将一个类别标签随机分配给每一张图片。在这种情况下，我们可以预期总体成功率为千分之一，或者错误率为 99.9%。AlexNet 的 15%的错误率确实令人印象深刻——那是在 2012 年。到 2017 年，卷积神经网络将错误率降低到大约 3%，低于约 5%的水平，这是少数勇敢地手动进行挑战的人能够实现的。你能分辨出 120 种不同的犬种吗？我当然做不到。

AlexNet 打开了洪水闸门。新模型打破了所有先前的记录，并开始实现没有人真正期望它们实现的任务：比如重新构想图像的风格，生成图像内容及活动的文本描述，或者像人类一样甚至更好地玩视频游戏等等。

这一领域迅速发展，以至于几乎无法跟上每天涌现的新论文。保持最新状态的唯一方法是每年参加多场会议，并审查在如 arXiv（[*https://www.arxiv.org*](https://www.arxiv.org)）这样的站点上首次发布的新研究，这里是许多领域的研究首发地。这促使了像[*https://www.arxiv-sanity-lite.com*](https://www.arxiv-sanity-lite.com)这样的站点的创建，它根据读者的兴趣对机器学习论文进行排名，希望能够更容易地找到“最好的”论文。

2014 年，另一个突破性进展出现在研究员 Ian Goodfellow 与朋友们在一个晚上交谈时的启发下。结果就是*生成对抗网络（GANs）*的诞生，Yann LeCun 在当时称之为 20 到 30 年来神经网络领域最重要的突破（在 NeurIPS 2016 上听到的）。我们将在第六章中讨论 GANs，它开辟了一个新的研究领域，让模型能够“创造”与训练数据相关但不同的输出。GANs 导致了当前*生成式 AI*的爆炸性发展，包括像 ChatGPT 和 Stable Diffusion 这样的系统。

*强化学习*是机器学习的三大分支之一，另外两个是我们一直在讨论的监督学习和尝试在没有标注数据集的情况下训练模型的无监督学习。在强化学习中，一个代理（即模型）通过奖励函数被教导如何完成任务。其在机器人技术中的应用显而易见。

Google 的 DeepMind 团队在 2013 年推出了一个基于深度强化学习的系统，能够成功地学习玩 Atari 2600 视频游戏，且与人类专家的表现相当或更好。（我不确定当时 35 年的游戏系统中，谁算得上是专家。）对我来说，这个系统最令人印象深刻的部分是，模型的输入正是人类的输入：屏幕图像，仅此而已。这意味着该系统必须学会如何解析输入的图像，并从中学会如何通过移动操纵杆来赢得游戏（实际上是使用模拟器）。

在击败人类原始视频游戏的能力与击败人类在围棋等抽象策略游戏中的能力之间的差距，历史上曾被认为是不可逾越的。我在 1980 年代末明确被教导，像 Deep Blue 这样通过 Minimax 算法在国际象棋中取胜的系统并不适用于围棋这样的游戏；因此，没有机器能够战胜最优秀的人类围棋选手。然而，我的教授们错了，尽管当时他们有充分的理由相信他们的观点。

2016 年，Google 的 AlphaGo 系统在五局三胜制的比赛中以 4 比 1 击败了围棋冠军李世石。全世界都注意到了这一点，进一步增强了对一个范式转变已经发生的认知。到这时，机器学习已经取得了商业上的成功。然而，AlphaGo 的胜利对于机器学习研究人员和从业者来说，仍然是完全令人印象深刻的。

大多数普通公众没有注意到，AlphaGo 在 2017 年被 AlphaGo Zero 所取代，后者是一个完全从零开始，通过与自己对弈进行训练的系统，完全没有任何人类输入。很快，AlphaGo Zero 掌握了围棋，甚至战胜了原版的 AlphaGo 系统（取得了 100 场胜利，且一场未败）。

然而，在 2022 年，目前最先进的围棋系统 KataGo 屡次且轻松地被一个训练目标不是为了赢得比赛，而是为了揭示现代 AI 系统固有脆弱性的对抗系统所击败。对抗系统所使用的棋招超出了 KataGo 训练时所遇到的范围。这是一个现实世界的例子，展示了模型擅长内插却不擅长外推。当对抗系统被训练为不是为了在围棋中更强，而是为了利用并“挫败”AI 时，它能够赢得四局中的三局以上。我向读者推荐*星际迷航：下一代*的那集“巅峰表现”，其中安卓人 Data 并非试图赢得一场艰难的策略游戏，而是通过与对手匹敌并让对方感到挫败来“获胜”。

深度学习在视频游戏中击败人类的趋势持续不减。取代像 Atari 这种原始游戏，深度强化学习系统如今在更复杂的游戏中达到了大师级表现。2019 年，DeepMind 的 AlphaStar 系统在*星际争霸 II*这款策略游戏中超越了 99.8%的玩家，该游戏要求发展单位并策划战斗计划。

1975 年的阿西洛马重组 DNA 大会是认识到生物技术增长和潜在伦理问题的重要里程碑。该会议对未来的研究产生了积极影响，并且在当年，组织者发布了一篇总结性论文，概述了生物技术的伦理方法。那时，这个领域还处于初期阶段，但潜在的风险已经被早期认识到，并采取了措施，确保在考虑未来研究时，伦理问题始终处于重要位置。

2017 年的阿西洛马有益 AI 大会故意模仿了早期的会议，以提高人们对 AI 潜在危险的认识。如今，会议中常常能看到诸如“AI 造福人类”这样的议题标题。2017 年阿西洛马大会促成了人工智能发展与应用的指导原则的制定。类似地，到 2023 年，美国政府——具体来说是白宫科技政策办公室——已经制定了《AI 权利法案蓝图》，旨在保护美国公众免受 AI 不加区分应用的有害影响。事实上，白宫官员们也特别致力于直接与 AI 社区沟通，鼓励在开发更强大的 AI 系统时充分考虑各种问题。所有这些都是一个好兆头，但历史告诉我们，人类的法律往往滞后于技术的发展，因此这些努力是否能够有效地框定这一领域，仍有待观察。

### ***2021 年至今***

2020 年的 COVID-19 大流行使世界大多数地方陷入停滞。然而，AI 社区受到的影响却非常有限，这可能是因为远程协作和会议在这个领域中运作得相当顺利。此外，我们可以通过互联网访问强大的计算机，因此物理上的接近性不会像其他学科那样限制研究，尤其是在这些学科中需要接触硬件。

从 2021 年开始，并且持续到我写这篇文章时，新模型的爆炸式增长让人印象深刻，每一个比上一个更为强大。大多数模型能够接受人类书写的文本输入，进而生成文本、图像甚至视频输出。输入通常结合了文本和示例图像来引导系统。目前流行的（且令人担忧的）系统包括 DALL-E 2、Stable Diffusion、Midjourney 和 ChatGPT 等。大多数大型科技公司也展示了生成视频、音频甚至 3D 模型的尖端技术。这些系统目前引发的热议让人想起了 2012 年深度学习凭借 AlexNet 证明其价值时的兴奋情绪。这又是一次范式转变；但需要明确的是，这些系统并没有意识。我并不是唯一有这种看法的人，甚至 ChatGPT 也同意我这个观点。我问它：“你是一个有意识的存在吗？”它的回答是，逐字如下：

作为一个机器学习模型，我并不是一个有意识的存在。我是一个计算机程序，经过训练，根据用户输入生成文本。我没有像人类一样思考、推理或体验意识的能力。我设计的目的是提供信息和帮助用户，但我没有独立的思想或意识。

我认为艾达·洛夫莱斯会对这个答案感到满意，尽管她可能会对一个没有思维的机器是如何生成这个答案感到困惑。不过，请注意，ChatGPT 并没有声称自己不思考，而是说它不像人类那样思考。我们将在第六章中探讨图像合成，在第七章中探讨像 ChatGPT 这样的庞大语言模型。也许到时候我们能找到解决她（假设的）困惑的方法。

****

那么，为什么是现在呢？简短的回答是符号人工智能的衰落和对连接主义方法极为有利的技术创新的兴起。

符号人工智能和连接主义是同时出现的，符号人工智能主导了几十年，迫使连接主义处于背景中。然而，在经历了两次人工智能寒冬，符号人工智能几乎濒临灭绝之后，连接主义在关键技术创新的帮助下崛起，并填补了这一空白。

我认为符号人工智能与连接主义的关系，就像是非鸟类恐龙与哺乳动物之间的关系。从地质学的角度来看，恐龙和哺乳动物几乎在同一时期出现，但大型陆地恐龙主宰了地球约 1.6 亿年，迫使哺乳动物只能在阴影中勉强生存。当大约 6600 万年前小行星撞击地球时，大型恐龙灭绝了，给了哺乳动物进化并接管地球的机会。

当然，类比最终会失效。恐龙并没有完全灭绝——我们现在称它们为鸟类——而且它们并不是因为某种劣势而灭绝的。事实上，恐龙是地球上最伟大的成功故事之一。非鸟类恐龙之所以灭绝，纯粹是因为运气不好。几乎可以说，是一场灾难导致了它们的灭绝（“灾难”这个词来源于意大利语*disastro*，意思是“坏星”）。

符号人工智能有可能重新出现吗？它可能会以某种形式出现，但会与连接主义合作。符号人工智能曾承诺智能行为在抽象层面上是可能的，但它没有实现这一点。连接主义声称，智能行为可以从一组更简单的单元中产生。深度学习的成功支持了这一观点，更不用说地球上当前存在的数十亿个活脑了。但正如 ChatGPT 所指出的，现有的连接主义模型“并不像人类那样思考、推理或体验意识。”现代神经网络不是心智；它们是表示学习的数据处理器。我将在第五章中进一步阐明这一点。

尽管我们物种*智人*在很大程度上依赖于符号思维，但符号思维并不是智能的必备条件。在他的书《理解人类进化》（剑桥大学出版社，2022 年）中， 人类学家伊恩·塔特萨尔（Ian Tattersall）声称，尼安德特人不太可能像我们一样使用符号思维，也没有像我们一样的语言，但他们仍然具有智能。实际上，尼安德特人足够像人类，以至于我们的祖先曾多次与他们“作爱，而非打仗”——非非洲血统的人的 DNA 证明了这一事实。

我预计，在不久的将来，联结主义和符号人工智能之间会产生协同效应。例如，由于像 ChatGPT 这样的系统最终只是在预测下一个输出符号（单词或单词的一部分），它无法知道自己何时说错话。一个关联的符号系统可以检测到响应中的错误推理并加以修正。至于这样的系统如何实现，我并不清楚。

****

到 1960 年代初，联结主义可能会出现的一些迹象已经显现。那么，是否仅仅是符号人工智能的偏见延迟了这场革命长达数十年？不是的。联结主义的发展停滞是因为速度、算法和数据问题。让我们逐一分析这些问题。

### ***速度***

要理解为什么速度阻碍了联结主义的发展，我们需要了解计算机是如何工作的。为了便于理解，我们可以把计算机看作是一个内存，它存储数据（数字）和一个处理单元，通常被称为中央处理单元（CPU）。一个微处理器——比如你桌面电脑、智能手机、语音控制助手、汽车、微波炉，以及几乎你所用的所有设备（除了烤面包机，哦，很多烤面包机也有）——就是一个 CPU。把 CPU 想象成传统计算机：数据从内存或输入设备（如键盘或鼠标）进入 CPU，经过处理后，再通过 CPU 发送到内存或输出设备（如显示器或硬盘）。

另一方面，图形处理单元（GPU）是为显示器开发的，最初是为视频游戏产业设计的，目的是实现快速图形渲染。GPU 能够同时对成百上千的内存位置（即*像素*）执行相同的操作，例如“乘以 2”。如果 CPU 想要把一千个内存位置乘以 2，它必须依次处理第一个、第二个、第三个，依此类推。事实上，训练和实现神经网络所需的主要操作正好适合 GPU 能够完成的任务。像 NVIDIA 这样的 GPU 制造商很早就意识到了这一点，并开始开发用于深度学习的 GPU。把 GPU 看作是一张超计算机卡，可以插入到你的 PC 中。

1945 年，电子数值积分和计算机（ENIAC）是当时最先进的技术。ENIAC 的速度估计为每秒约 0.00289 百万指令（MIPS）。换句话说，ENIAC 每秒可以执行不到 3000 条指令。到 1980 年，像当时许多流行个人计算机中所用的 6502 8 位微处理器，运行速度约为 0.43 MIPS，或每秒大约 50 万条指令。到 2023 年，我用来写这本书的电脑中已有些过时的 Intel i7-4790 CPU 运行速度约为 130,000 MIPS，使我的 PC 比 1980 年的 6502 快了大约 300,000 倍，比 ENIAC 快了大约 4500 万倍。

然而，NVIDIA 的 A100 GPU 在用于深度学习时，能够达到 312 太弗洛普（TFLOPS），即 312,000,000 MIPS：比 6502 快 7.3 亿倍，比 ENIAC 快令人难以置信的 1100 *亿*倍。机器学习的计算能力在这个时间跨度内的增长令人瞠目结舌。而且，训练一个大型神经网络并处理一个庞大的数据集，通常需要数十到数百个这样的 GPU。

*结论：直到快速 GPU 的出现，计算机在训练具有构建像 ChatGPT 这样的能力的神经网络时，速度仍然太慢。*

### ***算法***

正如你将在 第四章 中学到的那样，我们是通过基本单元来构建神经网络，这些单元执行一个简单的任务：收集输入值，将每个值与权重值相乘，求和，加上偏置值，并将结果传递给激活函数生成输出值。换句话说，许多输入数字变成一个输出数字。成千上万到百万个这样的单元的集体行为，导致数十亿的权重值，使深度学习系统能够完成它们的任务。

神经网络的结构是一回事；将神经网络调整到所需任务是另一回事。可以将网络的结构，称为其 *架构*，比作解剖学。在解剖学中，我们关心的是构成身体的各个部分：这是心脏，那是肝脏，等等。训练网络更像是生理学：如何让一个部分与另一个部分协同工作？解剖学（架构）存在，但生理学（训练过程）并未完全被理解。几十年来，这一切发生了变化，得益于关键的算法创新：反向传播、网络初始化、激活函数、丢弃法和归一化、以及先进的梯度下降算法。理解这些术语的详细含义并不至关重要，重要的是要知道，这些术语所代表的改进——以及前面提到的处理速度提升，结合改进的数据集（接下来会讨论）——是深度学习革命的主要推动力。

尽管长期以来人们知道，合适的权重和偏置值能使网络适应所需的任务，但缺乏的是一种高效的方法来*找到*这些值。1980 年代引入的反向传播算法，结合随机梯度下降，开始改变这一状况。

训练过程通过迭代找到最终的权重和偏置值，这些值是根据模型在训练数据上的误差来调整的。迭代过程从初始状态开始，使用一组初始的权重和偏置。然而，这些初始的权重和偏置应该是什么呢？长期以来，人们认为初始的权重和偏置并不重要；只需在某个范围内随机选择一些小数值。这种方法通常有效，但也有很多时候不起作用，导致网络无法很好地学习，甚至根本不学习。因此，需要一种更有原则的网络初始化方法。

现代网络仍然是随机初始化的，但这些随机值依赖于网络的架构和所使用的激活函数类型。关注这些细节使得网络能够更好地学习。初始化很重要。

我们将神经网络按层次排列，其中一层的输出成为下一层的输入。分配给网络中每个节点的激活函数决定了该节点的输出值。历史上，激活函数通常是 sigmoid 或双曲正切函数，这两者绘制出来时呈现出*S*形曲线。这些函数在大多数情况下是不合适的，最终被一种名字很长但实际上简单的函数所取代：*修正线性单元（ReLU）*。ReLU 提出了一个简单的问题：输入是否小于零？如果是，输出为零；否则，输出为输入值。因此，ReLU 激活函数比旧的激活函数要好，而且计算机几乎可以瞬间问出并回答这个问题。因此，切换到 ReLU 是双赢：既提高了网络的性能，又加快了速度。

Dropout 和批量归一化是一些较为高级的训练方法，描述起来有些复杂，尤其是在我们关心的层面上。2012 年引入的*dropout*会在训练时随机将一层节点的部分输出设置为零。其效果就像是同时训练成千上万的模型，每个模型既独立又互相关联。适当使用 dropout 会对网络学习产生显著影响。正如一位著名计算机科学家当时对我说的：“如果我们在 1980 年代就有 dropout，今天的世界可能会完全不同。”

*批量归一化*调整数据在网络层之间流动时的表现。输入出现在网络的一侧，并通过各个层流动，直到输出。在示意图中，这通常呈现为从左到右的运动。归一化被插入层之间，以便调整数值，使其保持在有意义的范围内。批量归一化是首个可学习的归一化技术，意味着它在网络学习的过程中，学习如何处理数据。整个归一化方法系列都由批量归一化发展而来。

深度学习革命的最后一项关键算法创新是梯度下降法，它与反向传播一起，帮助学习权重和偏差。梯度下降的思想早于机器学习的出现，但近年来所开发的版本对深度学习的成功做出了重要贡献。我们将在第四章中进一步了解这一主题。

*结论：最初的神经网络训练方法是原始的，无法发挥其真正的潜力。算法创新改变了这一点。*

### ***数据***

神经网络需要大量的训练数据。当人们问我，训练特定任务的模型需要多少数据时，我的回答总是一样的：所有的数据。模型是从数据中学习的；数据越多，效果越好，因为更多的数据意味着模型在使用时能够更好地呈现遇到的实际情况。

在万维网出现之前，收集、标注并处理足够规模的数据集以训练深度神经网络是非常困难的。这一局面在 1990 年代末到 2000 年代初随着网络的快速增长以及它所代表的数据爆炸而发生了变化。

例如，Statista（[*https://www.statista.com*](https://www.statista.com)）声称，2022 年，每分钟都有 500 小时的新视频上传到 YouTube，*这一速度惊人*。还估计，1995 年 12 月，约有 1600 万人在使用互联网，占全球人口的 0.4%。到了 2022 年 7 月，这个数字增长到了近 55 亿人，占全球人口的 69%。社交媒体的使用、电商以及携带智能手机四处走动，足以产生惊人的数据量——所有这些数据都被捕捉并用于人工智能。社交媒体之所以免费，是因为我们和我们生成的数据就是产品。

我在工作中常听到一句话：“我们曾经数据匮乏，但现在我们被数据淹没了。”没有足够的庞大数据集和标签，深度学习无法学习。但另一方面，拥有庞大数据集时，令人惊叹的事情会发生。

*结论：在机器学习中，数据就是一切。*

****

本章的主要收获包括：

+   符号人工智能与连接主义的争斗早期就已出现，并导致了几十年符号人工智能的主导地位。

+   连接主义曾因速度、算法和数据问题而长时间遭遇困境。

+   随着 2012 年深度学习革命的到来，连接主义者目前已取得胜利。

+   深度学习革命的直接原因是计算机速度的提升、图形处理单元的出现、算法的改进以及海量数据集的涌现。

在我们完成足够的历史背景介绍后，让我们回到机器学习，从经典算法开始。
