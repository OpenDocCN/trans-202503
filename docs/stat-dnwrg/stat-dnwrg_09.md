# 第十章 每个人都会犯错

直到现在，我一直以为科学家能够进行完美准确的统计计算，错误只出现在选择计算所需的合适数字时。科学家们可能会错误使用统计检验的结果，或者未能进行相关计算，但至少他们能计算出*P*值吧？

也许并非如此。

对医学和心理学试验中统计显著性结果的调查表明，许多*P*值是错误的，一些统计上不显著的结果在正确计算后实际上是显著的。^(1),^(2) 即使是声誉卓著的《自然》期刊也并非完美，大约 38%的论文在*P*值上存在拼写错误和计算错误。^(3) 其他评论发现了一些错误分类的数据、数据的错误重复、完全包含了错误的数据集以及其他混乱问题，而这些问题都被未能充分描述分析过程的论文所掩盖，使得这些错误难以被发现。^(4)

这类错误是可以预见的。科学家们或许咖啡因摄入量超常，但毕竟他们还是人类，持续的发表压力意味着彻底的文档记录和复制工作往往被忽视。研究人员没有动力让他们的数据和计算结果供检查，也没有动力花时间复制其他研究人员的结果。

随着这些问题变得越来越广为人知，软件工具已经进步到可以使分析步骤更容易记录和共享。然而，科学家们尚未广泛采用这些工具，没有它们，彻底检查工作仍然是一个艰难的过程，这在遗传学中发生的一场著名灾难中得到了体现。

# 不可重复的遗传学

问题始于 2006 年，当时一种新的基因测试承诺能够将化疗治疗精确地针对患者特定的癌症变异。杜克大学的研究人员进行了试验，表明他们的技术可以确定肿瘤对哪些药物最敏感，从而避免患者因无效治疗而产生副作用。肿瘤学家对这一前景感到兴奋，其他研究人员也开始了自己的研究。但在此之前，他们请了两位生物统计学家，Keith Baggerly 和 Kevin Coombes，来检查数据。

这比他们预期的要困难得多。原始论文没有提供足够的细节来复制分析，因此巴格利和库姆布斯与杜克大学的研究人员进行了通信，要求提供原始数据和更多细节。很快，他们发现了问题。一些数据被错误标记——对某种药物有抗药性的细胞组被标记为敏感，反之亦然。一些样本在数据中被重复出现，有时同时标记为敏感和抗药。杜克大学研究人员发布的更正修复了一些问题，但同时引入了更多重复的数据。有些数据不小心被错位了一次，以至于在分析不同的细胞系时，使用了来自另一组细胞的测量结果。基因微阵列（我在关于伪复制的部分中提到过）在不同批次之间差异显著，而且微阵列设备的效应无法与真正的生物学差异区分开来。声称显示一种药物结果的图形实际上包含了另一种药物的结果。

简而言之，研究一团糟。^(5) 尽管许多错误已被提交给杜克大学的研究人员，但几项使用基因结果的临床试验仍然开始，并得到了美国国家癌症研究所的资助。巴格利和库姆布斯尝试在与原始研究相同的学术期刊上发表他们的回应，但在几种情况下，他们的文章被拒绝——开创性的研究比繁琐的统计细节更有趣。尽管如此，国家癌症研究所注意到了这些问题，并要求杜克大学的管理层进行审查。大学回应称，成立了一个外部审查委员会，但该委员会没有访问巴格利和库姆布斯的结果。毫不奇怪，他们没有发现错误，试验继续进行。^(6)

错误直到后来才引起严重关注，在巴格利和库姆布斯发表他们的发现之后，一本行业杂志报道称杜克大学的首席研究员阿尼尔·波蒂（Anil Potti）伪造了他的简历。他的几篇论文被撤回，波蒂最终因欺诈指控辞去了杜克大学的职务。使用这些结果的几项试验被停止，一家为销售该技术而设立的公司也关闭了。^(7)

Potti 案例说明了两个问题：现代科学中许多研究缺乏可重复性，以及在学术期刊上发表负面和矛盾结果的困难。我将把后者的问题留到下一章讨论。可重复性已经成为一个流行的时髦词汇，你可能能理解为什么：Baggerly 和 Coombes 估计他们花了 2000 小时去弄清楚 Potti 做了什么以及哪里出了问题。很少有学者能有这么多空闲时间。如果 Potti 的分析软件和数据公开供检查，怀疑的同事就不必费劲地重构他工作的每一步——他们只需要阅读代码，看看每个图表和图形的来源。

问题不仅仅是 Potti 没有轻易分享他的数据。科学家们通常不会记录和文档化他们从原始数据到结果的转化过程，除非是在科学论文中以一种通常模糊的形式，或者记录在实验室笔记本中。原始数据必须进行编辑、转换成其他格式，并与其他数据集链接；统计分析必须进行，有时使用定制软件；图表和表格必须根据结果创建。这通常是手工完成的，数据片段被复制粘贴到不同的数据文件和电子表格中——一个极易出错的过程。除了负责的研究生的记忆外，通常没有这些步骤的最终记录，尽管我们希望能够在学生毕业多年后仍然能够检查和重现每一步过程。

# 让可重复性变得简单

理想情况下，这些步骤应该是 *可重复的*：完全自动化，计算机源代码可供检查，作为工作的一份最终记录。错误很容易被发现并纠正，任何科学家都可以下载数据集和代码，得出完全相同的结果。更好的是，代码还应结合其目的的描述。

统计软件正在不断进步，以使这一切成为可能。例如，一个名为 Sweave 的工具可以轻松地将使用流行的 R 编程语言进行的统计分析嵌入到用 LATEX 编写的论文中，LATEX 是一种常用于科学和数学出版物的排版系统。结果看起来就像任何科学论文，但另一位阅读论文并对其方法感兴趣的科学家可以下载源代码，准确查看所有数字和图表是如何计算的。但由于学术期刊使用复杂的排版和出版系统，目前还不接受 Sweave 出版物，因此其使用受到限制。

类似的工具也正在其他编程语言中涌现。例如，使用 Python 编程语言的数据分析师可以通过 IPython Notebook 记录他们的进展，该工具将文本描述、Python 代码以及由 Python 代码生成的图表和图形结合在一起。IPython Notebook 可以像分析过程的叙述一样阅读，解释数据是如何被读取、处理、过滤、分析和绘制的，代码和文本相伴而行。任何一步的错误都可以被纠正，代码重新运行以获得新结果。笔记本还可以转化为网页或 LATEX 文档，这样其他研究人员就不需要安装 IPython 来阅读代码。最棒的是，IPython Notebook 系统已经扩展到支持其他语言，例如 R。

在计算密集型领域，如计算生物学和统计学，期刊已经开始采纳代码共享政策，鼓励公开发布分析源代码。这些政策尚未像数据共享政策那样广泛应用，但它们正在变得越来越普遍。^(8) 一项更全面的策略，旨在确保可重复性和便于错误检测，将遵循由一群生物医学研究人员制定的“可重复计算研究的十条简单规则”。^(9) 这些规则包括自动化数据操作和重新格式化、使用软件版本控制系统记录所有分析软件和自定义程序的更改、存储所有原始数据，并将所有脚本和数据公开供公众分析。每个科学家都经历过读论文时感到困惑，心想“他们到底是怎么得到*那个*数字的？”而这些规则将使这个问题更容易回答。

这是一项相当繁重的工作，对于已经知道如何进行分析的科学家来说，缺乏动力。为什么要花那么多时间将代码适应于*其他*人使用，而不是做更多的研究呢？其实这样做有很多好处。自动化数据分析使得在新数据集上尝试软件变得轻松，或者测试每个部分是否正确运行。使用版本控制系统意味着你可以记录每一次的更改，这样你就再也不会陷入困惑，想着“这段代码为什么上周二能运行，但今天却不行？”而且，全面的计算和代码记录意味着你随时都可以重新执行它；我曾经非常尴尬，因为我需要为一篇论文重新格式化图表，结果才意识到自己不记得当时用了什么数据来制作这些图表。我的混乱分析让我花了一整天的时间在恐慌中重做图表。

但即便科学家们*已经*完全自动化了他们的分析，出于可理解的原因，他们仍然不愿分享他们的代码。如果竞争对手使用了它并先于你做出发现怎么办？由于他们不需要披露他们的代码，他们也不需要透露他们使用了你的代码；他们可以仅凭你的工作获得学术荣誉。如果代码是基于不能共享的专有或商业软件呢？还有一些代码质量差到让科学家觉得分享它是尴尬的。

《社区研究与学术编程许可证》（CRAPL），由 Matt Might 起草，用于学术软件的版权协议，在其“定义”部分包括以下内容：

1.  “程序”指的是提供给“你”的源代码、shell 脚本、可执行文件、对象、库和构建文件的集合，或这些文件经你修改后的版本。

    [程序中任何设计的出现纯属巧合，不应以任何方式被误认为是深思熟虑的软件构建证据。]

1.  “你”指的是那些足够勇敢和愚蠢到愿意使用该程序的人。

1.  “文档”指的是程序。

1.  “作者”可能是那个因咖啡因过量而在提交截止日期前才让程序正常工作的研究生。

CRAPL 还规定，用户必须“同意免责作者，免受任何关于程序中的黑客行为、权宜之计或信念跳跃的羞耻、尴尬或嘲笑。”虽然 CRAPL 可能不是最严格的法律许可协议，但它确实反映了学术代码作者面临的问题：为公众使用编写软件比为个人使用编写代码要复杂得多，包括文档编写、测试和清理多次黑客攻击中积累的无用代码。额外的工作对程序员几乎没有好处，因为即便是重要的软件，程序员也得不到任何学术积分，即使这些软件花费了几个月的时间编写。而科学家会利用机会检查代码并找出漏洞吗？没有人通过检查代码中的拼写错误来获得科学荣誉。

# 实验，清洗，重复

另一个解决方案可能是复制。如果科学家们从头开始仔细重现其他科学家的实验，收集全新的数据，并验证他们的结果——这是一个费时费力的过程——那么排除拼写错误导致结果错误的可能性就容易多了。复制实验还能排除偶然的假阳性，前提是复制实验有足够的统计能力来检测相关效应。许多科学家认为，实验复制是科学的核心；没有经过独立测试和全球范围的反复验证，任何新想法都不会被接受。

这并不完全正确。复制性研究很少是为了自身目的而进行的（除非在某些领域——物理学家喜欢对物理常数进行越来越精确的测量）。由于复制复杂结果可能需要几个月的时间，复制通常只有在研究人员需要将先前的结果用于自己的研究时才会发生。否则，复制性研究很少被视为值得发表的研究。少数例外情况包括可复现性项目，这个项目是由于心理学家对许多重要结果可能无法通过复制而产生的担忧而发起的。该项目由大量心理学家的合作进行，正在稳步重新测试来自知名心理学期刊的文章。初步结果令人鼓舞，大多数结果在新的试验中得以重现，但仍有很长的路要走。

另一个例子是，制药公司安捷伦的癌症研究人员重新测试了 53 项癌症研究中的重要临床前研究。（“临床前”是指这些研究未涉及人体患者，因为它们是在测试新的、未经验证的理念。）尽管与原作者合作，安捷伦的研究人员仅能重现其中六项研究。^(10) 拜耳的研究人员在测试已发表论文中提到的潜在新药时也报告了类似的困难。^(11)

这令人担忧。那么这种趋势对于那些较少依赖猜测的医学研究是否也适用呢？显然是的。在医学领域，被引用最多的研究文章中，有四分之一在发表后未经过验证，而三分之一的研究结果在后续的研究中被发现夸大或错误。^(12)这虽然不像安捷伦的结果那样极端，但足以让人怀疑是否仍有重大错误在重要研究中未被发现。复制性研究并不像我们希望的那样普遍，而结果也并不总是理想的。

小贴士

+   使用电子表格、分析脚本或程序来自动化数据分析，并且这些工具可以通过已知输入进行测试。如果有人怀疑存在错误，你应该能够查看你的代码，了解你究竟做了什么。

+   推论：将所有分析程序与已知输入进行测试，并确保结果合理。理想情况下，使用自动化测试来检查代码更改，确保不引入错误。

+   在编写软件时，遵循科学计算的最佳实践：*[`www.plosbiology.org/article/info:doi/10.1371/journal.pbio.1001745`](http://www.plosbiology.org/article/info:doi/10.1371/journal.pbio.1001745)*。

+   在使用程序和脚本分析数据时，遵循“可复现计算研究的十条简单规则”。^(9)

+   使用像 Sweave 这样的可复现研究工具，将分析中的数据自动纳入你的论文中。

+   尽可能使所有数据可用，通过专门的数据库如 GenBank 和 PDB，或通过通用的数据存储库如 Dryad 和 Figshare。

+   发布你的软件源代码、电子表格或分析脚本。许多期刊允许你将这些作为补充材料随论文提交，或者你可以将文件存放在 Dryad 或 Figshare 上。
