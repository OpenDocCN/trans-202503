# 第一章 统计显著性简介

大多数实验科学归结于测量差异。某种药物是否比另一种更有效？某一版本基因的细胞是否比另一版本的细胞合成更多的酶？某种信号处理算法是否比另一种更有效地检测脉冲星？某种催化剂是否比另一种更有效地加速化学反应？

我们用统计学来判断这些差异。由于运气和随机变异，我们总会观察到*一些*差异，因此统计学家提到*统计显著*的差异时，是指这种差异大到不容易由运气产生。所以首先我们必须学会如何做出这个判断。

# p 值的威力

假设你正在测试感冒药物。你新的药物承诺能将感冒症状的持续时间缩短一天。为了证明这一点，你找到 20 名感冒的患者，一半给他们服用新药，另一半给他们服用安慰剂。然后你追踪他们的感冒时长，并计算在有药和没有药的情况下，感冒的平均时长是多少。

但并不是所有感冒都是一样的。也许平均感冒持续一周，但有些可能只持续几天，其他的可能会拖延两周或更长时间。你研究中的 10 名接受了有效药物治疗的患者可能都得了非常短的感冒。你如何证明你的药物有效，而不是仅仅证明有些患者运气好？

统计假设检验提供了答案。如果你知道典型感冒案例的分布——大致有多少患者感冒时间短，多少患者感冒时间长，多少患者感冒时间正常——你就能判断一个随机样本中的患者感冒时间比平均值长或短的概率有多大。通过进行*假设检验*（也叫*显著性检验*），你可以回答这样一个问题：“即使我的药物完全无效，我的实验产生当前观察结果的几率有多大？”

如果你只在一个人身上测试药物，那么她的感冒比平时稍微短一点也不算太令人惊讶。大多数感冒并不是完全平均的。但如果你在 1000 万名患者身上测试该药物，那么所有这些患者恰好都得到了更短的感冒的可能性就非常小了。更可能的情况是，你的药物实际上有效。

科学家通过一个叫做*p 值*的概念来量化这种直觉。*p 值*是指在假设没有真正效果或没有真正差异的前提下，收集到的数据显示出的差异，是否等于或超过你实际观察到的极端差异的概率。

所以，如果你将药物给 100 名患者，并发现他们的感冒平均缩短了一天，那么这个结果的*p*值是指，如果你的药物实际上没有任何效果，感冒的平均时间比对照组短一天的可能性仅仅是运气的结果。正如你可能猜到的那样，*p*值取决于效应的大小——感冒缩短四天的情况比感冒缩短一天的情况要少见——以及你测试药物的患者数量。

记住，*p*值不是衡量你有多正确或差异有多重要的标准。相反，把它看作是惊讶的衡量标准。如果你假设你的药物无效，且除运气外没有任何理由让两个组之间有所不同，那么*p*值越小，结果就越令人惊讶和幸运——或者你的假设是错误的，药物确实有效。

如何将*p*值转化为这个问题的答案：“这些组之间真的存在差异吗？”一个常见的经验法则是，当*p* < 0.05 时，差异是统计显著的。选择 0.05 并不是因为有什么特别的逻辑或统计原因，而是经过几十年的常用，已成为科学界的惯例。

请注意，*p*值的工作方式是假设你的实验组之间没有差异。这是显著性检验的一个反直觉特性：如果你想证明你的药物有效，你需要通过展示数据与药物*无效*不一致来证明。由于这个原因，*p*值可以扩展到任何你能用数学方式表达的假设，你想推翻它。

但是，*p*值有其局限性。记住，*p*值是惊讶的衡量标准，值越小，意味着你应该越感到惊讶。它不是效应大小的衡量标准。你可以通过测量一个巨大的效应——“这种药物让人活得长四倍”——或者通过以极高的准确性测量一个微小的效应，得到一个很小的*p*值。而且，因为任何药物或干预通常都会有*某些*实际效果，你总是可以通过收集大量数据，检测出极其微小但相对不重要的差异，从而获得统计显著的结果。正如布鲁斯·汤普森所写，

> 统计显著性检验可能涉及一个自我循环的逻辑，疲惫的研究人员在收集了数百名受试者的数据后，再进行统计检验，以评估是否有足够多的受试者，而这些研究人员已经知道这一点，因为他们收集了数据并知道自己很疲劳。这种自我循环的逻辑在知识积累方面造成了相当大的损害。^(1)

简而言之，统计显著性并不意味着你的结果具有任何*实际*意义。至于统计*不*显著，它并不能告诉你太多。一个统计上不显著的差异可能只是噪音，或者它可能代表一个真实的效应，只不过需要更多的数据才能确认。

没有任何数学工具能告诉你你的假设是对还是错；你只能看它是否与数据一致。如果数据稀疏或不清晰，你的结论将会不确定。

## 心理统计学

在其局限性背后，*p* 值存在一些更微妙的问题。回想一下，*p* 值是在假设幸运（而不是你的药物或干预）是实验中唯一因素的前提下计算的，而 *p* 的定义是得到一个结果等于*或更极端*的概率。这意味着 *p* 值迫使你去推理那些实际上没有发生的结果——也就是说，比你观察到的结果更极端的结果。得到这些结果的概率取决于你的实验设计，这使得 *p* 值变得“心理学化”：两个设计不同的实验可能产生相同的数据，但 *p* 值不同，因为*未观察到*的数据是不同的。

假设我问你一系列 12 个关于统计推断的真/假问题，你正确回答了其中 9 个。我想检验你是否是通过随机猜测来回答这些问题。为此，我需要计算你通过随机选择真或假来回答每个问题时，至少答对 9 个问题的概率。假设你以相等的概率选择真或假，我计算得出 *p* = 0.073。^([3]) 由于 *p* > 0.05，因此有可能是你在随机猜测。如果是的话，你 7.3%的时间会答对 9 个或更多的问题。^(2)

但或许最初我并没有打算只问你 12 个问题。也许我有一台计算机，它能够生成无限数量的问题，并且不停地问问题，直到你答错了 3 个。现在，我必须计算你在被问了 15、20 或 47 个问题后，答错 3 个问题的概率。甚至，我还得考虑一个极小的可能性，那就是你在答错 3 个问题之前，已经回答了 175,231 个问题。做这个数学计算后，我发现 *p* = 0.033。由于 *p* < 0.05，我得出结论，随机猜测不太可能得到这个结果。

这很麻烦：两个实验可以收集相同的数据，但得出不同的结论。某种程度上，*p* 值似乎能读取你的意图。

## 内曼-皮尔逊检验

为了更好地理解*p*值的问题，你需要了解一点统计学的历史。统计显著性检验中有两种主要的思维方式。第一种是由 R.A.费舍尔在 1920 年代提出并普及的。费舍尔将*p*值视为一种便捷的、非正式的方法，用来查看一组数据的惊讶程度，而不是某种严格的正式假设检验程序的一部分。*p*值结合实验者的先前经验和领域知识时，可能在决定如何解释新数据时非常有用。

在费舍尔的工作引入后，耶日·内曼和埃贡·皮尔森解决了一些未解答的问题。例如，在感冒药物测试中，你可以选择通过均值、中位数或其他你可能编造的公式来比较两组，只要你能够得出一个* p *值来进行比较。但你怎么知道哪个方法最好呢？在假设检验中，“最好”到底意味着什么？

在科学中，限制两种错误非常重要：*假阳性*，即你得出一个效果存在的结论，但实际并没有效果；以及*假阴性*，即你未能注意到一个真实的效果。从某种意义上讲，假阳性和假阴性是同一枚硬币的两面。如果我们太容易对效果做出结论，我们就容易得到假阳性；如果我们过于保守，我们则容易犯假阴性错误。

内曼和皮尔森认为，尽管完全消除假阳性和假阴性是不可能的，但*的确*可以制定一个正式的决策过程，确保假阳性仅在某个预定义的比例下发生。他们将这个比例称为α，他们的想法是让实验者根据经验和预期设定一个α。例如，如果我们愿意接受 10%的假阳性率，我们将设定α = 0.1。但是，如果我们需要在判断上更加保守，我们可能会将α设置为 0.01 或更低。为了确定哪种测试程序最好，我们需要看在给定的α选择下，哪种方法具有最低的假阴性率。

这一过程在实践中如何运作？在内曼–皮尔森系统下，我们定义一个*原假设*——即没有效果的假设——以及一个*备择假设*，例如“效果大于零”。然后我们构建一个比较这两个假设的测试，并确定如果原假设为真，我们期望看到的结果。我们使用*p*值来执行内曼-皮尔森检验程序，当*p* < α时拒绝原假设。与费舍尔的方法不同，这种方法故意不处理任何单一实验中的证据强度；现在我们仅关注是否拒绝原假设的决定。*p*值的大小不用于比较实验或得出任何结论，除了“可以拒绝原假设”。正如内曼和皮尔森所写，

> 我们倾向于认为，就某个特定假设而言，任何基于概率理论的测试都不能单独提供该假设的真假证据。
> 
> 但我们也可以从另一个角度来看待测试的目的。我们无需希望知道每一个假设是否真实，我们可以寻找一些规则来指导我们在这些假设上的行为，遵循这些规则能确保我们在长期的经验中不会经常出错。^(3)

尽管 Neyman 和 Pearson 的方法在概念上与 Fisher 的方法不同，实践中的科学家常常将两者混淆。^(4),^(5),^(6) Neyman-Pearson 方法就是我们得到“统计显著性”的方法，这种方法有一个预先选定的* p *值阈值，保证长期的假阳性率。但是假设你进行了一项实验并获得了* p * = 0.032。如果你的阈值是传统的* p * < 0.05，那么这个结果就是统计显著的。但如果你的阈值是* p * < 0.033，它依然是统计显著的。所以这就很诱人——也是一种常见的误解——说“我的假阳性率是 3.2%。”

但这样说是不合理的。单个实验并没有假阳性率。假阳性率是由你的*程序*决定的，而不是单个实验的结果。你不能声称每个实验的假阳性率就是* p *，无论最终结果是多少，因为你使用的程序是为了获得长期的假阳性率α。

# 对区间保持信心

显著性测试往往会受到很多关注，“统计显著”这一术语现在已经成为流行语。研究结果，尤其是在生物学和社会科学领域，通常会呈现* p *值。但* p *并不是评估证据权重的唯一方式。*置信区间*可以回答与* p *值相同的问题，且具有更多信息并且更易于解释的优点。

置信区间将一个点估计与该估计的不确定性结合在一起。例如，你可能会说你的新实验药物能减少感冒的平均持续时间 36 小时，并给出 95%的置信区间，介于 24 小时到 48 小时之间。（该置信区间是针对*平均*持续时间的；每个病人的感冒持续时间可能会有很大的差异。）如果你进行 100 次相同的实验，约 95 个置信区间会包含你想要测量的真实值。

置信区间量化了你结论中的不确定性，提供的信息比*p*值要丰富得多，因为*p*值根本没有关于效应大小的任何信息。如果你想检验一个效应是否显著不同于零，你可以构建一个 95%的置信区间，并检查该区间是否包含零。在这个过程中，你还可以获得额外的好处，了解你的估计有多精确。如果置信区间太宽，你可能需要收集更多的数据。

例如，如果你进行临床试验，你可能会得出一个置信区间，表明你的药物能将症状减少 15%到 25%之间。这个效应是统计显著的，因为该区间不包括零，现在你可以根据你对相关疾病的临床知识来评估这个差异的重要性。就像你使用*p*值时一样，这一步是很重要的——你不应该在没有在上下文中评估的情况下就宣称这个结果是一个重大发现。如果症状已经相当无害，也许 15%到 25%的改善并不那么重要。另一方面，对于像自发性人类自燃这样的症状，你可能会对*任何*改善感到兴奋。

如果你能将结果写成置信区间，而不是*p*值，那么你应该这么做。^(7) 置信区间避开了与*p*值相关的大部分解释性细节，使得研究结果更加清晰。那么，为什么置信区间如此不受欢迎呢？在实验心理学研究期刊中，97%的研究论文涉及显著性检验，但只有大约 10%的论文报告了置信区间——而且这些论文中的大部分并没有将置信区间作为支持其结论的证据，而是依赖于显著性检验。^(8) 即便是享有盛誉的期刊*Nature*也未能做到：89%的文章报告了*p*值，但没有任何置信区间或效应量，导致其结果在上下文中无法解读。^(9) 一位期刊编辑指出，“*p*值就像蚊子”，因为它们“有一个进化上的生态位，而且[不幸的是]无论怎么抓、拍打或喷洒，都无法把它们赶走。”^(10)

一种可能的解释是，置信区间未被报告，因为它们通常过于宽泛，令人尴尬。^(11) 另一个原因是同行评审的压力过大——最好按照大家都在做的方式进行统计，否则审稿人可能会拒绝你的论文。或者，可能是关于*p*值的广泛混淆掩盖了置信区间的好处。又或者，统计学课程中过度强调假设检验意味着大多数科学家不知道如何计算和使用置信区间。

期刊编辑们有时试图强制要求报告置信区间。1980 年代中期，作为《*美国公共卫生杂志*》的副主编，肯尼斯·罗思曼开始返回投稿并附上措辞严厉的信件：

> 所有涉及统计假设检验和统计显著性的内容都应从论文中删除。我要求你删除*p*值以及关于统计显著性的评论。如果你不同意我的标准（关于显著性检验不恰当的观点），你可以自由地辩论这一点，或者干脆忽略我可能被认为是错误的观点，选择在其他地方发表。^(12)

在罗思曼担任副主编的三年期间，仅报告*p*值的论文比例急剧下降。虽然在他离开后，显著性检验又重新出现，但随后的编辑们成功地鼓励研究人员同时报告置信区间。然而，尽管报告了置信区间，少数研究人员在他们的文章中讨论这些区间或使用它们来得出结论，而更倾向于仅仅将它们视为显著性检验的一部分。^(12)

罗思曼随后创办了期刊《*流行病学*》，该期刊有着严格的统计报告政策。刚开始时，习惯于显著性检验的作者更倾向于在报告置信区间的同时报告*p*值，但经过 10 年后，态度发生了变化，仅报告置信区间成为了常见的做法。^(12)

或许勇敢（且耐心的）期刊编辑可以效仿罗思曼（Rothman）的例子，改变他们领域中的统计实践。

* * *

^([3]) 我使用一种叫做*二项分布*的概率分布来计算这个结果。在下一段中，我将使用另一种分布，称为*负二项分布*，来计算*p*值。本书的重点不在于概率分布的详细解释；我们更关心如何解释*p*值，而不是如何计算它们。
