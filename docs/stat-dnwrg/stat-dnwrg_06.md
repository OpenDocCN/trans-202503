# 第七章 连续性错误

到目前为止，本书的重点是组间比较。是安慰剂还是药物更有效？允许红灯右转的交叉口比不允许的交叉口造成更多的死亡吗？你为每个组生成一个统计数据——例如交通事故的平均数量——然后查看这些统计数据是否在各组之间显著不同。

但如果你无法将测试对象分为明确的组怎么办？一项关于肥胖对健康影响的研究可能会测量每个参与者的体重指数（BMI）、血压、血糖、静息心率等等。但并没有两个明显的患者组；而是一个从体重过轻到肥胖的光谱。假设你想在这个光谱的两端之间观察健康趋势。

解决这种情况的一种统计技术叫做*回归建模*。它估算每个变量的*边际*效应——即每增加一磅体重对健康的影响，而不仅仅是基于任意临界点两侧的组间差异。这比单纯的组间比较能得出更精细的结果。

但是科学家们经常简化他们的数据，以避免进行回归分析。“超重的人患心脏病的概率高出 50%”这句话比“每增加一个单位的都市相对体重，心脏病的对数几率增加 0.009”更具明显的临床意义。即使有可能建立一个捕捉数据每个细节的统计模型，统计学家也可能出于实际原因选择一个简单的分析方法，而非技术上更优秀的分析方法。正如你所见，简单的模型仍然可能被错误地使用，而简化数据的过程则引入了更多的错误空间。让我们从简化过程开始；在下一章，我将讨论使用完整回归模型时常见的错误。

# 不必要的二分法

一种常见的简化技术是通过将连续测量拆分为两个独立的组来*二分化*变量。在肥胖研究的例子中，例如，你可能会将患者分为“健康”组或“超重”组。通过拆分数据，你就不必为选择正确的回归模型而烦恼。你可以仅仅通过*t*检验来比较这两个组。

这就提出了一个问题：你如何决定在哪里划分数据？也许有一个自然的临界点或者一个广泛接受的定义（比如肥胖），但通常没有。一个常见的解决方案是沿着样本的中位数来划分数据，将数据分为两个相等大小的组——这就是所谓的*中位数划分*。这种方法的一个缺点是，不同的研究者在研究相同的现象时可能会得出不同的划分点，这使得他们的结果难以比较或在荟萃分析中合并。

中位数分割的替代方法是选择给出组间最小*p*值的分界点。你可以将其理解为选择分离组别，使它们“最为不同”。正如你可能想象的那样，这种方法更容易产生假阳性。寻找具有最佳*p*值的分界点实际上是在进行许多假设检验，直到得到你想要的结果。结果与之前的多重比较相同：假阳性率最多增加了 10 倍。^(1) 你的效应大小的置信区间也会误导性地变窄。

二分法问题出现在 1990 年代初期多篇关于 S 期分数的乳腺癌研究论文中，S 期分数是指肿瘤中正在复制和合成新 DNA 的细胞比例。肿瘤学家认为这一比例可能预测癌症的最终进程，从而使医生能够更有效地为患者制定治疗方案。研究人员在研究中将患者分为两组：S 期分数较大的患者和 S 期分数较小的患者。

当然，每个研究选择了不同的“大小”分界点，选择了中位数或给出最佳*p*值的分界点。毫不奇怪，那些选择了“最佳”分界点的研究得出了统计显著的结果。但是当这些结果进行了多重比较校正后，没有一个结果是统计显著的。

进一步的研究表明，S 期分数确实与肿瘤预后相关，但多年来证据薄弱。在其缺陷被公开后，这一方法继续在癌症研究中使用了几年，并且 2005 年发布的癌症预后因素研究报告指南指出：“尽管进行了多年的研究，并且关于肿瘤标志物的报告已有数百篇，但作为临床有用标志物出现的标志物数量仍然非常少。”^(2) 除了统计效能差、结果报告不完整以及抽样偏差外，选择“最佳”分界点被认为是这一问题的主要原因。

# 统计布朗现象

对二分法的主要反对意见是它丢失了信息。你并没有为每个病人或观察值使用精确的数字，而是将观察值分为几个组并丢弃了数字。这降低了研究的统计效能——在很多研究本身就缺乏效能的情况下，这是一个主要问题。你会得到更不精确的相关性估计，并且经常低估效应大小。通常，效能和精确度的损失相当于你丢弃了三分之一的数据。^(3)

让我们回到那个研究肥胖对健康影响的例子。假设你基于*体重指数*（BMI）将患者分为“正常”和“超重”组，假设 25 是正常范围的最大值。（这是临床实践中使用的标准分割点。）但这样你就失去了这个分割点以上的所有 BMI 之间的区别。如果心脏病发病率随体重增加而上升，你就很难判断它上升了*多少*，因为你没有记录像轻度超重和重度肥胖患者之间的差异。

换句话说，假设“正常”组的患者 BMI 恰好为 24，而“超重”组的 BMI 为 26，那么两组之间的主要差异可能令人惊讶，因为它们并没有太大区别。另一方面，如果“超重”组的所有患者 BMI 都为 36，那么主要差异就不那么令人惊讶了，并且每个 BMI 单位之间的差异也会显得更小。二分法消除了这一区别，丢失了有用的信息和统计能力。

也许只用两个组是个愚蠢的选择——那么瘦弱的患者呢？——但是增加组的数量意味着每组中的患者数量会减少。更多的组可能会产生更详细的分析，但每组的心脏病发病率估计将基于较少的数据，并且置信区间会更宽。而且将数据分成更多组意味着需要做更多关于*如何*分割数据的决策，这使得不同的研究更加难以比较，也使得研究人员更容易生成假阳性。

# 混杂因素

你可能会问：如果在对数据进行二分法处理后，我有足够的数据来达到统计显著性，二分法还重要吗？只要我能用额外的数据弥补丧失的统计能力，为什么不使用二分法来简化统计分析呢？

这是一个合理的论点。但不进行二分法分析数据并不难。回归分析是一种常见的程序，几乎所有统计软件包都支持，而且很多书籍都有介绍。回归分析不涉及二分法——它使用完整数据，因此没有需要选择的分割点，也没有统计能力的损失。那么，为什么要稀释你的数据呢？但更重要的是，二分法做的不仅仅是削弱统计能力。出人意料的是，它还引入了假阳性。

我们通常有兴趣控制混杂因素。你可能会测量两个或三个变量（或者两个或三个十几个变量）以及结果变量，并尝试确定每个变量对结果的独特影响，在控制了其他变量的影响之后。如果你有两个变量和一个结果，可以通过对这两个变量进行二分法处理，并使用双向方差分析（ANOVA）表来轻松做到这一点，这是一个简单的、常见的程序，几乎所有主要的统计软件包都支持。

不幸的是，最糟糕的情况不是假阴性。通过二分化并丢弃信息，你消除了区分混杂因素的能力。^(4)

设想一个例子。假设你正在衡量多个变量对一个人获得医疗保健质量的影响。医疗保健质量（可能通过调查测量）是结果变量。对于预测变量，你使用两个测量指标：受试者的个人净资产（以美元计）和受试者个人游艇的长度。

你会期望一个好的统计程序能推导出财富会影响医疗保健质量，而游艇大小则不会。尽管游艇大小和财富往往是一起增长的，但不是你的游艇让你获得更好的医疗保健。通过足够的数据，你会发现相同财富的人可以拥有不同大小的游艇，或者根本没有游艇，但仍然能获得相似质量的医疗服务。这表明财富是主要因素，而不是游艇长度。

但通过二分化变量，你实际上是将数据简化为四个点。每个预测变量只能是“高于中位数”或“低于中位数”，没有更多的信息被记录下来。你不再拥有必要的数据来意识到游艇长度与医疗保健无关。因此，ANOVA 程序错误地声称游艇和医疗保健是相关的。更糟糕的是，这种假相关只有 5% 的情况下是统计上显著的——从 ANOVA 的角度来看，它是一个*真实*的相关性，并且它会在统计检验的统计功效允许的范围内被检测到。

当然，即使没有数据，你也能知道游艇大小不会有影响。你本可以将它排除在分析之外，避免很多麻烦。但通常你并不知道哪些变量最为重要——你依赖统计分析来告诉你。

回归程序可以轻松拟合这些数据而无需任何二分化，同时只会产生你预期的假阳性相关性。（当然，随着财富和游艇大小之间的相关性增强，区分它们的影响变得更加困难。）尽管涉及多个变量的回归数学理论可能比许多实际从事科学工作的研究人员更为复杂，涉及大量的线性代数，但其基本概念和结果容易理解和解释。没有充分的理由不使用它。

提示

+   除非有充分的理由，否则不要随意将连续变量分割成离散的组。使用一种能够充分利用连续变量的统计方法。

+   如果你确实需要将连续变量分组，切勿为了最大化统计显著性而随意选择分组。应提前定义分组，使用与之前类似研究中的分组方式，或使用外部标准（例如医学中对肥胖或高血压的定义）。
