# 第十章：未来可持续性

完成现代化项目的最佳方式是确保在几年内你不会再次经历整个过程。未来可持续性不是关于避免错误；而是知道如何逐步维护和发展技术。

两种类型的问题会促使我们重新思考一个正在老化的工作系统。第一种是使用变化。第二种是衰退。*扩展挑战*是使用类型的变化：我们有了更多的流量或不同类型的流量，跟以前不一样了。也许使用系统的人比之前多了，或者我们添加了一些新功能，随着时间推移，人们使用该技术的目的发生了变化。

*使用变化*的速度并不恒定，因此很难预测。一个系统可能永远不会面临扩展挑战。一个系统可能达到某个使用水平后再也不会增长。或者它可以在短短一段时间内扩展一倍或三倍。或者它可以在多年中缓慢扩展。若真的发生扩展挑战，它们会是什么样子，将取决于多种因素。由于系统使用的变化难以预料，它们也很难被标准化。这是一种优势。当我们标准化某些东西时，我们就不再思考它，不再把它考虑进我们的决策中，有时甚至会忘记它的存在。

*衰退*，另一方面，是不可避免的。它们代表了向不可避免的最终状态自然线性发展的过程。其他因素可能会加速或减缓它们的进程，但最终我们知道最终的结果会是什么。例如，1999 年 9 月 9 日的日期不会因为使用变化而从 1999 年的日历中消失。不管那些被编程为在日期缺失时将 9/9/99 作为空值赋给列的机器系统行为如何，9 月 9 日的事件终究会发生。

内存泄漏是这种变化的另一个很好的例子。系统使用可能会影响泄漏何时成为一个重大问题，但低系统使用量并不会改变内存泄漏的存在，内存泄漏最终会成为问题。解决问题的唯一方法是修复它。

硬件生命周期是另一个例子。最终，芯片、硬盘和电路板都会出现故障，并需要更换。

这些衰退类型是危险的，因为人们会忘记它们。它们的影响会长时间不被察觉，直到某一天它们最终完全崩溃。如果组织特别不幸，问题已经深深嵌入系统中，并且一开始并不清楚究竟是什么出故障了。

举个例子，考虑一下 Y2K 问题。一个令人担忧的情况是，很多计算机程序设计时采用了两位数的年份，这在 2000 年时成了问题，因为缺失的前两位数字与程序假设的数字不同。大多数技术人员都知道 Y2K 故事，但你知道吗，Y2K 并不是第一次出现这种目光短浅的编程错误？也不会是最后一次。

## 时间

软件工程师在程序中常常把时间搞砸，这实在是令人难以置信。在 1960 年代，一些程序只有一位数的年份。TOPS-10 操作系统只能表示 1964 年 1 月 1 日到 1975 年 1 月 4 日之间的日期。工程师通过修补这个问题，添加了三个位，使 TOPS-10 能够表示直到 2052 年 2 月 1 日的日期，但他们从现有的数据结构中取走了这些位，以为它们没有被使用。结果发现，TOPS-10 上的一些程序已经重新利用了这些存储区域，这导致了一些奇怪的错误。^(1)

应该为日期分配多少存储空间是一个永恒的问题。为时间分配无限存储空间是不明智且不切实际的，然而任何数量的存储最终都会用完。程序员必须决定，在多长时间后，程序仍在运行的可能性看起来不大。至少在计算机的早期，倾向于低估软件生命周期。一个正常运行的软件很容易保持运作 10 年、20 年、30 年，甚至更长时间。但在计算机发展的早期，两三十年看起来是很长的时间。如果时间只给出足够的空间直到 1975 年，那么修复可能会延续到 1986 年。1989 年，某些操作系统编程限制了系统的成熟时间为 1997 年——如此这般，循环往复。

这些程序仍然在我们身边，我们还没有到达它们的所有“成熟”日期。某个由世界计算机公司（World Computer Corporation）创建的日期格式将在某一年达到存储限制，我们不知道是否有现有系统在使用它。更值得关注的是 2038 年，届时 Unix 的 32 位日期将达到极限。虽然大多数现代 Unix 实现已经转向 64 位日期，但网络时间协议（NTP）的 32 位日期组件将在 2036 年 2 月 7 日发生溢出，这为我们提供了一个潜在的预览。NTP 负责同步互联网上互相通信的计算机时钟。计算机时钟若严重不同步——通常是五分钟或更长时间——会导致无法建立安全连接。这个要求源自 MIT 在 2005 年发布的 Kerberos 版本 5 规范，它利用时间来防止攻击者重置时钟，继续使用过期的票证。

我们不知道 NTP 和 Unix 溢出将引发哪些问题。大多数计算机可能已经升级，因此不会受到影响。如果运气好，2038 年的这一时刻可能会悄然过去，就像 Y2K 问题一样。但是，时间漏洞并不需要触发全球性的灾难来产生戏剧性的和昂贵的影响。过去的时间漏洞曾一度清空养老金基金，干扰短信发送，崩溃视频游戏，甚至让停车表无法正常工作。2010 年，由于时间漏洞，德国有 2000 万张芯片和密码银行卡无法使用^(2)。2013 年，NASA 因为一个类似 2038 问题的时间漏洞失去了对 3.3 亿美元的“深空撞击”（Deep Impact）探测器的控制。

时间漏洞很棘手，因为它们往往在引入后几十年，甚至有时是几个世纪才会引发问题。IBM 在 1970 年代制造的主机将在 2042 年 9 月 18 日达到溢出点。某些得克萨斯仪器（Texas Instruments）计算器无法接受 2049 年 12 月 31 日之后的日期。某些诺基亚手机只接受到 2079 年 12 月 31 日的日期。多个编程语言和框架使用的时间戳对象将在 2262 年 4 月 11 日发生溢出。

并不是程序员不知道这些漏洞的存在，只是很难想象今天的技术会持续到 2262 年。同时，20 世纪 60 年代编程房间大小的主机的工程师们也从未想过他们的代码会延续几十年，但现在我们知道，这些老旧的程序依然在生产环境中运行。到了 2000 年，旧软件（有时还有与之配套的机器）不仅没有被淘汰，反而由与其创作有两三代代际差距的技术人员在维护。

解决时间漏洞通常相对直接——当我们知道它们的存在时。问题在于，我们往往忘记它们即将到来。我们已经看到由于 2038 漏洞而导致的系统故障。金融机构中必须计算 20 到 30 年后利息支付的程序，实际上是这些类型错误的早期预警检测系统。然而，组织必须了解他们的遗留系统状态（换句话说，是否已经打上补丁），并意识到这些事件正在发生。

## 无法逃避的迁移

面向未来的系统建设并不意味着构建一个永远不需要重新设计或迁移的系统。那是不可能的。它意味着构建并且，更重要的是，*保持*系统更新，以避免一个需要重组正常运作的长期现代化项目。面向未来的秘诀在于将迁移和重新设计变成不需要大力操作的日常例行事务。

大多数现代工程组织已经知道如何处理使用变更——他们会监控活动的增加，并根据需要扩展或缩减基础设施。如果给予足够的时间和优先级，他们会重构和重新设计系统组件，以更好地反映最可能的长期使用模式。早期并频繁地对系统进行更新，只是一个纪律性的问题。那些忽视清理技术负债的组织，最终将不得不进行繁琐且风险较大的遗留系统现代化工作。

我最喜欢的关于为频繁更新设定节奏的比喻来自播客*Legacy Code Rocks*（[`www.legacycode.rocks/`](https://www.legacycode.rocks/)）。推出新功能就像举办一个家庭聚会。在你清理房子之前，举办的家庭聚会越多，房子的状况就越差。虽然这里没有一条适用于所有人的硬性规则，但在每次推出*n*个新功能后，自动安排时间重新评估使用变更和技术负债，将使得更新系统的过程规范化，从而确保其长期健康。当人们把重构和必要的迁移与系统构建错误或故障挂钩时，他们会推迟执行这些操作，直到事情开始崩溃。而当这些变更管理过程成为日常工作的一部分时，就像剪头发或换车油一样，系统可以通过逐步现代化来确保未来的可持续性。

对于恶化问题，需要采取不同的策略。有时它们可以被监控。例如，电池随着时间的推移会老化，它们的性能下降是可以被捕捉和追踪的。某些恶化问题则更加突然。时间错误在爆发之前不会发出任何警告。如果组织已经忘记了它，就没有什么可以监控的了。

说你永远不应该在系统中引入逐渐恶化的变化是天真的；这些问题往往是不可避免的。错误在于假设当问题成熟时，系统仍然无法运行。技术有一种方式可以将其寿命延长，远远超过人们的预期。例如，纽约市地铁的部分开关控制面板可以追溯到 1930 年代。索尔兹伯里大教堂的钟表自 1368 年开始运行。在加利福尼亚州利弗莫尔市的消防站 6 号楼上，有一个自 1901 年以来一直亮着的灯泡。全世界各地，我们的日常生活都受到那些远超预期过期日期的机器的控制。

相反，管理逐渐恶化的问题归结为以下两种做法：

+   如果你引入了一些会逐渐恶化的东西，那就让它优雅地失败。

+   缩短升级之间的时间间隔，让人们有足够的时间来实践这些升级。

## 优雅失败

Y2K 和类似的 bug 没有导致人类文明的终结，原因在于它们不会以均匀的强度影响所有受其影响的系统。不同的机器、不同的编程语言和不同的软件处理同一个问题的方式有很大的差异。有些系统会触发恐慌，有些则会继续运行。是否让系统触发恐慌并崩溃，或者忽略问题并继续执行，通常取决于失败是否发生在事务的关键路径上。

优雅地失败并不总意味着系统避免崩溃。如果一个 bug 导致银行账户的日常批处理任务计算累计利息失败，系统通过默认归零并继续执行来从错误中恢复，这并不是优雅地失败。如果允许这种错误默默地失败，会让很多人很快感到不满，而恐慌会立即提醒工程团队问题所在，以便解决并重新运行批处理任务。

**错误离用户界面有多近？** 如果错误是可能由用户输入触发的，那么优雅地失败意味着捕获错误并记录事件，但最终引导用户再次尝试，并提供一个有用的消息来解释问题。

**错误是否会阻塞其他独立进程？** 为什么会阻塞其他进程？阻塞意味着共享资源，这表明进程并不像最初想的那样独立。对于真正独立的进程，记录错误并最终让系统继续运行可能是可以接受的。

**错误是否出现在更大算法的一个步骤中？** 如果是这样，你可能别无选择，只能触发恐慌。如果你可以在一个多步骤的过程中省略一个步骤，并且不影响最终结果，那么你应该重新考虑这些步骤是否必要。

**错误会破坏数据吗？** 在大多数情况下，坏数据比没有数据更糟。如果某个错误可能会破坏数据，你必须在错误发生时触发恐慌，以便解决问题。

在编程时，考虑到不可避免的退化是很重要的。当你没有意识到你别无选择，只能在潜在的 bug 中编程时，这种思维练习就不太有用了。你无法知道自己不知道的事情。

但是，花些时间考虑你的软件如何处理日期偏差 20 年、时间倒退一秒、出现技术上不可能的数字（比如 11:59:60 pm）或者存储驱动器突然消失等问题，还是值得的。

当有疑虑时，默认触发恐慌。更重要的是让错误被发现，以便能够解决它们。

## 升级之间的时间应该更短，而不是更长

几年前，我为我的厨房买了一个那种俗气的字母板——你知道的，就是那种上面写“生命如花，尽情绽放”或“爱使这个家成为家”的板子。只是，我的写着“真相是反直觉的”。我们在面对退化问题时的直觉反应是尽可能推迟它们，如果不能完全消除它们的话。就个人而言，我认为这是一种错误。我知道通过经验，工程师们做事情的频率越高，他们做得越好，越容易记得需要做的事，并做出相应的计划。

例如，2019 年发生了两个重要的时间错误。第一个是 GPS 纪元的回滚；第二个是闰秒。

GPS 回滚是一个与之前描述的时间错误完全相同的问题。GPS 通过一个 10 字节的存储块表示周数。这意味着它可以存储最多 1024 个值，1024 周等于 19.7 年。与 Y2K 问题类似，当 GPS 达到第 1025 周时，它会重置为零，而计算机无法识别这一点，导致它把所有数据回溯 20 年。

这一情况在 1999 年只发生过一次。尽管商用 GPS 从 1980 年代起就已出现，但到 1999 年时，它并没有真正普及。接收器所用的芯片价格过高，直到计算机足够快，能够将这些数据与路线计算或将物理地标与其坐标关联时，GPS 的便利性才得以体现。由于 GPS 的有用功能尚未准备好进入市场，消费者对该技术的隐私问题更为敏感。1997 年，美国联合包裹公司（UPS）因试图在所有卡车上安装 GPS 接收器而爆发了著名的罢工。

所以，第一次 GPS 回滚的影响是微乎其微的，因为 GPS 当时并不普及。然而，到 2019 年，世界已经发生了翻天覆地的变化。二十年在技术领域是很长的时间。几乎所有手机都配备了 GPS 芯片，且各种应用程序都已在 GPS 的基础上开发出来。

结果证明，人们更换配备 GPS 的设备非常频繁。对于许多用户来说，移动应用更新是无缝且自动的。我们已经习惯了每两到三年就换一部新手机，因此 2019 年的回滚事件大部分并没有引起大规模的混乱。使用旧款手机的用户遇到了一些问题，但他们被鼓励从运营商处购买新手机。

2019 年的第二个时间错误，闰秒，情况略有不同。闰秒字面意思就是：在一年中增加一个额外的秒数，以便让计算机时钟与太阳周期保持同步。与闰年不同，闰秒无法预测。日出到日落之间的秒数取决于地球的自转速度，而这个速度是不断变化的。不同的力量促使地球加速自转，另一些力量则让地球减速。

这是一个有趣的事实：改变地球自转速度的众多因素之一就是气候变化。冰层压迫着地球的陆地块，当冰层融化时，这些陆地块开始向极地漂移。这使得地球自转加快，白天的时间也变得更短。

从 1972 年到 2020 年，共发生了 28 次闰秒，但由于一些力量使地球减速，而另一些力量使其加速，因此每年是否有闰秒之间可能会有显著的间隔。在 1999 年的闰秒之后，直到六年后才需要下一次闰秒。2009 到 2012 年之间没有闰秒。2015 年和 2016 年都发生了闰秒，但接下来的三年里没有发生任何闰秒。

闰秒从来都不是一件愉快的事，但如果可以将每次最近的闰秒问题报告作为参考，长时间的间隔比起其他情况，问题会更加严重。即便是三年这样短的间隔，也足以让新技术得到开发，或者比之前更受欢迎。抽象和假设被提出，它们逐渐融入工作系统中，最终被遗忘。

云计算和智能手机相关的行业正是在闰秒的多年间隔即将到来之时开始发展的。等到下一个闰秒事件发生时，庞大的平台已在没有当时技术的基础上运行。这些技术是由那些可能根本不了解闰秒概念的工程师构建的。一些服务提供商未能及时修补更新以管理闰秒。Reddit、Gawker Media、Mozilla 和 Qantas Airways 都遇到了问题。

随后是另一个多年间隔，直到 2015 年的闰秒才给 Twitter、Instagram、Pinterest、Netflix、Amazon 和 Beats 1（现为 Apple Music 1）带来了问题。相比之下，2016 年的闰秒几乎没有引起什么反响。仅仅六个月的间隔，这次闰秒似乎只在 CloudFlare 的 102 个数据中心中的少数机器上引发了问题。

那么 2019 年的闰秒呢？它发生在又一个多年间隔后，导致了超过 400 次航班取消，因为 Collins Aerospace 的自动依赖监视广播（ADS-B）系统未能正确调整。ADS-B 并不新鲜，但 FAA 已经发布了要求到 2020 年飞机必须配备的规定，因此它的采用比上一轮闰秒时要广泛得多。

一般来说，我们处理问题的能力随着我们处理问题的次数增加而变得更强。当退化现象的成熟日期之间的间隙越长，丧失的知识就越多，而在没有考虑不可避免因素的情况下，关键功能的构建也变得更加可能。尽管 GPS 周转是在 20 年的间隙后发生的，但由于最可能受影响的设备的升级周期加速，它受益匪浅。很少有人使用 20 年的老手机或平板电脑。另一方面，闰秒的引入在每次间隙发生时，几乎始终会导致混乱。

一些退化现象在大规模下具有非常短的间隙，组织无需额外干预。例如，普通的存储驱动器寿命为三到五年。如果你拥有一块驱动器——例如你电脑里的驱动器——通过定期备份并在驱动器最终故障时更换电脑，你可以降低这种不可避免的故障带来的风险。

如果你在运营数据中心，你需要一个策略，以防止驱动器故障瘫痪运营。你需要定期备份，并几乎可以瞬间恢复。这看似是一个巨大的工程挑战，但创建这种韧性的架构在规模中本身就已经内建。数据中心不仅仅拥有几块硬盘和需要在三到五年内更换的驱动器间隔。数据中心通常有成千上万到数十万块驱动器在*持续*故障。2008 年，谷歌宣布它用 4,000 台计算机和 48,000 个存储驱动器在六小时内整理了一个 PB 的数据。一次运行总是会导致至少一块 48,000 个驱动器中的硬盘损坏。^(3) 同一时期进行的正式研究表明，年驱动器故障率为 3%。^(4) 以 3%的故障率，一旦进入数十万块驱动器，你开始看到每天都有多块驱动器故障。

虽然没有人会认为驱动器故障是愉快的，但一旦数据中心达到足够的规模以至于处理驱动器故障成为常规操作时，它们并不会引发停机。因此，与其延长不可避免变化之间的间隔，倒不如缩短它，以确保工程团队在构建时考虑到不可避免的因素，并且负责解决问题的团队知道该如何应对。

## 关于自动化的警告

如果无法完全消除恶化的变化，人们通常会选择自动化其解决方案。在某些情况下，这种自动化能够带来相对较小风险下的巨大价值。例如，定期未能更新 TLS/SSL 证书可能导致整个系统突然停摆，且没有任何预警。自动化更新证书的过程意味着这些证书可以有更短的有效期，这增加了它们的安全性。

在考虑将问题自动化解决时，最需要考虑的是：如果自动化失败，是否能清楚地知道出了什么问题？在大多数情况下，过期的 TLS/SSL 证书会触发明显的警报。要么连接被拒绝，这时证书的有效性应当在检查列表中成为可能的嫌疑问题，要么用户会收到警告，提示连接不安全。

当自动化掩盖了或以其他方式让工程师忘记系统实际上在幕后做了什么时，问题会更加严重。一旦这种知识丧失，所有建立在这些自动化活动上的系统将不会包含任何防范自动化失败的保障措施。自动化的任务成为平台的一部分，如果负责平台的工程师清楚它们的存在并对此负责，那这没有问题。

很少有程序员会考虑如果垃圾回收突然无法正确执行，会发生什么。内存管理曾经是编程中的一个关键部分，但现在大部分责任已经自动化。这之所以可行，是因为开发具有自动垃圾回收功能的编程语言的软件工程师始终把这一问题放在心上。

换句话说，自动化在明确谁对其正常工作负责，以及失败状态能够为用户提供足够信息以了解如何进行问题排查时是有益的。鼓励人们忘记自动化的做法会造成责任空白。自动化如果悄无声息地失败或出现不明确的错误信息，至少会浪费大量宝贵的工程时间，最严重的情况是引发不可预知和危险的副作用。

## 正确地构建错误的东西

在本书的这一章，特别是全书的核心信息是：在没有规模之前，不要为规模做准备。构建一些你之后必须重构的东西，即使这意味着一开始构建一个庞大的单体应用。做错了事情并频繁更新它。

构建“错误”但正确的技术的秘诀在于理解，成功的复杂系统是由稳定的简单系统构成的。在使用负载均衡器、网格网络、队列和分布式计算等技术进行系统扩展之前，简单系统必须是稳定的且具备良好的性能。灾难通常源自于试图立刻构建复杂系统，而忽视了决定所有系统行为（包括计划内和意外情况）基础的稳定性。

估算你的系统能承受多少复杂度的一个好方法是问自己：这个团队有多大？每增加一层复杂度，就需要一个监控策略，并最终需要人来解读监控器所提供的信息。每个服务至少需要三个人。为了便于讨论，一个服务是指具有自己代码库的子系统（尽管谷歌著名地将所有源代码存放在一个单一的代码库中），拥有专用资源（无论是虚拟机还是独立容器），并且假设它与系统的其他组件是松耦合的。

最小的值班轮换人数是六人。所以，一个有六人的大型服务可以有独立的值班轮换，或者两个小型服务可以在他们的团队之间共享一个轮换。当然，人员可以同时属于多个团队，或者同一个团队可以运营多个服务，但一个人不可能精通无限数量的主题，所以每增加一个服务，预计专家水平会减半。一般来说，我更倾向于不让工程师负责超过两个服务，但如果服务相关，我会做出例外。

我列出这些限制只是为了给你提供一个框架，帮助你思考你的系统所依赖的人的能力，以确保系统的未来可扩展性。如果你愿意，可以根据你的实际情况调整具体数字。工程师们的倾向是将系统设计得具有无限扩展性。许多团队在没有足够团队支持的情况下，会根据谷歌或亚马逊的白皮书来建模他们的系统，而忽视了他们并没有一个能够维护谷歌或亚马逊规模的团队。团队所能支持的人力资源，就是系统复杂性的上限。不可避免地，团队会增长，系统的使用也会增长，许多这些架构决策将不得不进行修订。这是可以接受的。

这是一个例子：服务 A 需要向服务 B 发送数据。维护整个系统的团队大约有 11 个人。四个人负责运营，维护服务器并构建工具来帮助执行标准。四个人在数据科学团队，设计模型并编写代码实现这些模型，其余的三个人负责构建 Web 服务。这个三人团队维护服务 B，同时还维护系统中的另一个服务。数据科学团队负责维护服务 A，同时也负责另外两个服务。

这两个团队的人员配置稍显过载，但系统的使用率较低，因此压力并不大。

那么，服务 A 应该如何与服务 B 通信呢？

第一个建议是建立一个消息队列，使得 A 与 B 之间的通信解耦并具有弹性。这将是最具可扩展性的解决方案，但也需要有人来设置消息队列和工作节点，监控它们，并在出现问题时进行响应。哪个团队负责这一切呢？持悲观看法的工程师可能会说是运维团队。通常，当团队无法支撑他们所构建的东西时，就会发生这种情况。系统的某些部分被抛弃，唯一关注它们的团队通常是负责基础设施的团队（通常只有在出现重大问题时才会关注）。

虽然消息队列更具可扩展性，但一个更简单的方案，结合更紧密的耦合，可能在开始时会获得更好的结果。服务 A 可以直接向服务 B 发送 HTTP 请求。责任的委派已经内建。如果错误发生在服务 B 端，服务 B 的团队会被通知。如果错误发生在服务 A 端，服务 A 的团队会被通知。

那么，网络问题怎么办呢？的确，网络有时会出现故障，但如果我们假设这两个服务都托管在一个主要的云服务提供商上，那么导致没有其他问题的单次网络故障的可能性很小。网络问题并不微妙，通常是配置错误造成的，而不是外部干扰因素。

HTTP 请求方案是正确的错误，因为将服务 A 和服务 B 之间的 HTTP 请求迁移到消息队列是简单的。虽然我们暂时失去了内建的容错能力并承受了更高的扩展负担，但它创建了一个当前团队更容易维护的系统。

反例是如果我们交换 HTTP 请求的顺序，让服务 B 轮询服务 A 以获取新数据。虽然这比消息队列简单，但它不必要地消耗资源。服务 A 并不是持续产生新数据，通过轮询服务 B，它可能花费数小时甚至数天发送没有意义的请求。从这种方式转变为使用队列将需要对代码进行重大修改。以这种错误的方式构建系统几乎没有什么价值。

## 反馈回路

另一种思考方式是描绘出维护这个系统如何在工程中产生反馈回路。从工作流、延迟、库存和目标的角度思考工作完成的方式，有助于澄清维护某一复杂系统所需的工作量是否可行。

让我们再看看服务 A 和服务 B 的问题。我们知道有七个人在这两个服务上工作，并且每个人都有八小时的工作日。服务 B 的团队在这些服务和另一个服务之间分配，所以我们可以假设他们在每个他们负责的服务上有四小时的预算。三个人就是每天大约 12 小时。服务 A 的团队维护着总共三个服务，所以他们每个人的预算是 2.5 小时，每个服务每天 10 小时。像这样的模型可能具有以下特点：

1.  库存 *库存* 是任何能够随着时间积累或消耗的元素。传统的系统模型示例是一个装满水的浴缸。水就是库存。在这个模型中，技术债务会持续不断地为每个服务积累，无论工作量多少。债务将通过花费工作时间来偿还。我们的工作周中的任务也是一种库存，团队在操作时会将其消耗掉。那八小时的工作日也是一种库存。当系统稳定时，每天的八小时将被完全消耗并完全恢复。

1.  流动 *流动* 是增加或减少库存的变化。在浴缸的例子中，水龙头流出的水速是流动，如果排水口打开，水从浴缸流出的速度就是另一个流动。在我们的模型中，任何时候，人们可以工作超过八小时，但这样做会降低他们的最终生产力，并且要求他们在之后的某个时候每天工作少于八小时。我们可以通过假设我们为第二天的预算借用了额外的时间来表示这一点。任务通过消耗工作时间来完成；我们可能简化模型，假设每个任务都值一个小时，或者我们可以将任务分为小、中、大不同的大小，并为每种选项设置不同的工时成本。花费工作时间会减少技术债务或工作任务的库存，具体取决于这些小时是如何使用的。

1.  延迟 好的系统模型承认并非所有事情都是瞬时发生的。*延迟*表示流动反应中的时间间隙。在我们的模型中，新工作并不会立即取代旧工作；它是按一周一周计划并分配的。我们可以将每个任务分配之间的时间视为一种延迟。

1.  反馈 *反馈回路* 形成于库存变化影响流动性质时，无论是正面还是负面。在我们的模型中，当人们工作超过总共八小时的预算时，他们会失去未来的时间。工作时间越长，他们为了维持连续八小时的工作日就必须借用更多的时间。最终，他们需要休息以恢复正常。或者，他们可以通过将更多预算花费在服务 A 或服务 B 上来借用时间，但这意味着他们负责的其他服务将被忽视，技术债务将不断积累。

从视觉上看，我们可以像图 10-1 中那样表示这个模型。实线表示流动，虚线表示影响流动速率的变量。

工作时间通过我们的日程安排进入模型，但会受到代表职业倦怠的库存的影响。如果职业倦怠很高，工作时间会减少；如果工作时间很长，职业倦怠则会上升。我们在某一天能够用于某项服务任务的工作时间，取决于我们的团队规模以及团队在同一时间内需要维护的服务或项目的数量。我们能投入到工作任务中的时间越多，完成的任务就越多。当工作任务完成后，剩余的额外时间会用于改善我们的技术债务。

![f10001](img/f10001.png)

图 10-1：团队工作负载中的反馈循环

尽管这个视觉模型看起来只是一个插图，但我们实际上可以为其编程，并用它来探索我们的团队在不同条件下如何管理工作。系统思维者常用的两个工具来实现这些模型是 Loopy（[`ncase.me/loopy/`](https://ncase.me/loopy/)）和 InsightMaker（[`insightmaker.com/`](https://insightmaker.com/)）。这两个工具都免费且开源，允许你尝试不同的配置和交互。

现在，让我们思考几种场景。假设我们有一个冲刺任务，其中 Service A 和 Service B 各有 24 小时的工作任务。这应该没问题；Service A 的团队每周有 50 小时的周容量来处理 Service A，Service B 的团队每周有 60 小时的周容量来处理 Service B。对于 24 小时的冲刺任务，每个团队都有足够的额外时间来消化技术债务。

但是，如果一个冲刺有 70 小时的工作量，会发生什么呢？Service A 的团队可以处理这种情况，如果团队中的四个人每个人从下周借五小时出来，但团队将没有时间管理技术债务，并且下周他们只会有 30 小时的时间来处理 Service A。

如果 70 小时的工作量成为冲刺的常态会怎么样？团队将逐渐疲惫不堪，无法重新思考系统设计或管理他们的技术债务。这个模型是不稳定的，但我们可以通过以下方法恢复平衡：

+   团队将他们的一项服务的所有权转交给另一个团队，从而使他们每天可以花更多时间在 Service A 或 Service B 的任务上。

+   团队允许技术债务在他们的一个或所有服务上累积，直到某个服务失败。

+   团队越来越努力工作，直到成员们感到疲惫不堪，届时他们将无法继续工作一段时间。

团队可能会采取的一种方式是，改变设计，使得集成模式能减轻服务 A 的低容量团队的工作量。假设，服务 B 不再通过 HTTP 连接，而是直接连接到服务 A 的数据库来获取所需数据。服务 A 的团队将不再需要构建一个接收来自服务 B 请求的端点，这意味着他们可以更好地平衡工作负载，管理维护责任，但这种模式会以整体架构质量的牺牲为代价达到平衡。

如果你是 Fred Brooks 的*《神话般的工程人月》*的读者，你可能会对这个模型的前提提出异议。它暗示其中一个可能的解决方案是增加团队成员，而我们知道软件并不是通过人小时来成功构建的。更多的人并不会让软件项目进展得更快。

但是这种模型的核心并不是规划路线图或预算人员数量，而是帮助人们将工程团队视为一个相互连接的系统。糟糕的软件就是没有得到维护的软件。面向未来意味着不断地重新思考和迭代现有系统。人们在构建服务时并不会认为他们会忽视它，直到它成为组织的巨大负担。人们未能维护服务，是因为没有给予足够的时间或资源来进行维护。

如果你大致了解一个平均迭代周期中的工作量以及团队人数，你就可以推测该团队是否有可能成功地维护 X 个服务。如果答案是否定的，那么架构设计可能对当前的团队来说过于复杂。

## 不要停下公交车

总结来说，系统的老化有两种方式。其使用模式发生变化，要求进行规模的扩展或收缩，或者支撑它们的资源逐渐退化，最终导致它们失败。遗留系统现代化本身就是反模式。一个健康的组织运行一个健康的系统，应该能够随着时间的推移不断演进，而不需要重新调整资源去进行正式的现代化努力。

然而，要实现这种健康的状态，我们必须能够看到我们所构建的系统系统的层次和层级关系。我们的技术系统由必须保持稳定的小型系统组成。我们的工程团队也表现为另一个系统，建立反馈回路，决定他们可以投入多少时间和精力来进行技术演进所需的升级。工程系统和技术系统不是相互独立的。

曾经有一位高级主管告诉我：“你说得对，马里安，这个安全漏洞很严重，但我们不能停下‘公交车’。”他的意思是，他不想投入资源来修复这个问题，因为他担心这样会拖慢新开发进度。他是对的，但只有在组织忽视这个问题两三年之后，他才是对的。如果他们在问题被发现时就解决了，可能只需要最低的投入。相反，问题随着工程师将不良代码复制到其他系统，并在其上构建更多内容，逐渐扩大了。他们面临着一个选择：减慢“公交车”的速度，还是等待“公交车”的车轮掉下来。

组织选择尽可能快地推动“公交车”前进，因为他们看不见所有的反馈回路。发布新代码引起了关注，而技术债务则默默累积，毫无声息。导致系统失败的不是它的使用年限，而是组织对它的忽视，逐渐积累的压力最终导致了爆炸性后果。
