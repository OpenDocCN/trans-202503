# 第二十六章：索引

请注意，索引链接指向每个术语的大致位置。

**符号与数字**

α (alpha)，Q-learning 混合，629

β1 (beta 1)，Adam 参数，421

β2 (beta 2)，Adam 参数，421

δ (delta)，误差变化比例，357

Δ (Delta)，误差变化比例，357

ε (epsilon)

数值稳定性参数，419

Q-learning 策略，630

η (eta)，学习率

介绍，377

优化过程中实际问题，389

γ (gamma)

Adagrad 缩放参数，419

折扣因子，611

动量缩放因子，410

λ (lambda)，正则化控制，204

σ (sigma)，标准差，24

θ (theta)，硬币偏差，103

P(A)，简单概率，50

P(A,B)，联合概率，53

P(A|B)，条件概率，50

1-hot 编码，226

1D 卷积，446

1×1 卷积，447

20 问题，269

68-95-99.7 规则，25

**A**

准确度，64

激活函数

指数 ReLU，340

函数画廊，344

Heaviside 步阶，334

身份，331

介绍，318

渗漏 ReLU，337

线性，332

逻辑曲线，341

maxout，338

嘈杂的 ReLU，339

参数化 ReLU，337

分段线性，336

ReLU，336

防止网络崩溃的作用，329

移位 ReLU，338

sigmoid，341

符号函数，335

正弦波，343

softmax，345

softplus，340

阶梯步阶，333

步长，333

swish，341

tanh，341

单位阶跃，334

演员，602

非循环图，324

Adaboost，309

Adadelta，418

Adagrad，417

Adam，420

自适应编码，137

自适应梯度学习，417

自适应矩估计，420

*哈克贝利·芬的冒险*，146

对抗扰动，491

对手，491

智能体，602

AI 冬天，317

祖先（在神经网络中），324

锚点，436

*动物饼干*，559

安斯科姆四重奏，40

回答问题，540

阿波罗航天器，297

神经网络中的弧，323

考古学抛硬币游戏（情景），103

数组，329

人工神经元。*参见* 神经元

大气示例，205

空洞卷积，457

攻击（带扰动），493

注意力

引言，574

键，575

Q/KV，579

QKV，575

查询，575

自注意力，576

值，575

注意力层

引言，578

示意图符号，581

自编码器

基本结构，495

卷积，516

潜在层，501

自动贝叶斯，97

自回归，541

平均值

常见含义，16

调和平均数，71

均值，16

中位数，16

众数，16

平均池化，452

**B**

婴儿身长（情景），33

反向传播，351

反向传播，351

时间反向传播，553

反向传播，376

装袋法，299

平衡决策树，270

基础图像，680

批量梯度下降，401

批归一化

讨论，424

示意图符号，424

巴克斯特，威廉，139

贝叶斯规则

讨论，95

证据，96

假设，102

似然，96

线性拟合，212

多个假设，109

观察，102

后验，96

先验，96

精细化估计，101

重复，101

声明，94

贝叶斯定理。*参见* 贝叶斯规则

贝叶斯，托马斯，83

贝叶斯概率

与频率派相比，85

概述，85

光束搜索，595

钟形曲线，22

伯努利分布，26

BERT，590

双向循环神经网络（bi-RNN），559

偏差

人工神经元中的 318

偏置技巧，320

曲线族中的，204

翻转硬币的结果，86

偏置输入，偏置输出，673

偏差-方差权衡，210

双向 RNN 层

介绍，559

结构符号，559

二分类器网络，378

二进制交叉熵，150

二元相关性，161

绑定（到神经元），314

位，136

非均匀性福祉，173

结构福祉，173

数字块，329

BMI（体质指数），243

强驱动方法，400

提升，302

自助法，299

自助聚合，299

自助法。*见* 自助法

自助法

在集成中的袋装（bagging），299

在统计学中，31

线性，39

多重，39

负面，38

部分，39

正面，38

简单，39

强，38

弱，38

瓶颈，501

在最小值附近反弹，394

边界

分类器，156

决策边界，158

方法，156

简单，58

方框滤波器，451

BPTT（时间反向传播），553

分支（在决策树中），270

构建圣代（场景），548

**C**

*C*（支持向量机参数），285

倒影，157

笛卡尔空间，213

类别数据，226

类别分布，27

分类变量决策树，271

分类。*见* 分类器

分类器。*见* 分类器

类别，156

通道（在张量中），430

作弊

寻找坦克时，185

过拟合时，197

子节点

在决策树中，271

在神经网络中，324

选择商店背景音乐（场景），198

类别，156

分类。*见* 分类器

分类器

二元，156

二元相关性，161

多类别，160

非参数化，264

一对多，161

一对多，161

一对一，163

一对剩余，161

概述，6

参数化，264

训练，182

结束（训练任务），591

完形填空任务，591

聚类

K 均值，166

概述，8

CNN（卷积神经网络）, 431

CNN-LSTM, 557

代码

自适应, 137

固定长度, 141

固定长度, 141

霍夫曼, 148

可变比特率, 143

硬币检测（场景）, 86

投掷硬币

基本思想, 86

贝叶斯方法, 87

投掷硬币游戏（场景）, 87

崩溃, 329

Common Crawl, 597

压缩, 496

压缩比, 148

隐蔽数据投毒, 598

条件概率, 50

置信度（在集成投票中）, 299

置信区间, 32

混淆矩阵

使用贝叶斯法则, 97

正确分析, 77

定义, 60

正确使用, 74

错误使用, 76

连接组, 315

数据准备中的一致性, 223

恒定分布, 22

固定长度编码, 141

污染数据, 187

内容融合, 498

内容损失（用于风格迁移）, 683

上下文（在信息中）, 135

上下文向量, 562

上下文化词嵌入, 571

连续概率分布, 21

连续变量决策树, 272

收敛

GAN 的, 670

Q 学习的, 633

卷积神经网络, 431

卷积

介绍, 433

层, 446

层级示意符号, 471

多维, 443

步长, 453

转置卷积, 457

带有下采样层符号的卷积, 521

带有上采样层符号的卷积, 521

卷积自编码器, 516

卷积神经网络, 431

正确性, 56

相关性, 37

相关系数, 38

成本（神经网络错误）, 352

协方差, 35

cpd（连续概率分布）, 21

信用分配问题, 608

交叉熵, 145

交叉验证

基础, 190

信息泄露, 239

k 折交叉验证, 192

维度灾难, 168

突点, 118

循环（在神经网络中）, 323

**D**

DAG（有向无环图），324

投掷飞镖（场景），48

数据

数据增强，255

清理，221

污染，187

卫生，187

漏泄，187

中毒，598

准备，221

数据集扩展，255

DCGAN（深度卷积生成对抗网络），667

死神经元，345

衰减参数（对于学习率），396

衰减计划，398

决策边界，158

决策节点（在决策树中），270

决策区域，158

决策树桩，303

决策树

平衡，270

分支，270

子节点，271

决策节点，270

深度，270

深度限制，280

远程子节点，271

边缘，270

集成，299

直接子节点，271

内部节点，270

leaf, 270

链接，270

节点，270

过拟合，275

概述，269

父节点，271

剪枝，280

根节点，270

同胞节点，271

树桩，303

子树，271

终端节点，270

不平衡，270

解码器

对于自编码器，496

对于 seq2seq，561

解码器块（变压器），587

反卷积（作为转置卷积），457

深度卷积生成对抗网络，667

深度梦想，675

深度学习

网络结构，326

概述，10

深度强化学习，647

延迟步长，550

延迟指数衰减，398

增量值（对于神经元），360

去噪，519

密集层。*参见* 全连接层

密度，169

因变量，29

部署系统，183

深度

在决策树中，270

深度学习网络的，327

张量的，430

深度限制，280

导数

定义，119

要求，123

后代（在神经网络中），324

确定性

环境，611

函数，118

确定性环境，611

DFR（折扣未来奖励）, 609

查尔斯·狄更斯, 545

数字, 4

膨胀卷积, 457

降维, 243

收益递减（在集成中）, 300

有向无环图, 324

定向散度, 151

有向图, 323

折扣因子, 611

折扣未来奖励, 609

离散概率分布, 20

鉴别信息, 151

鉴别器, 650

决策树中的远离节点, 271

蒸馏, 593

区分奶牛和斑马（场景）, 224

dit, 137

域, 158

下采样, 451

裁员, 457

下游

网络, 542

任务, 542

苏斯博士, 137

随机失活

讨论, 422

线路符号, 422

双重表示, 213

虚拟变量, 226

**E**

*E*（神经网络误差）, 359

早期停止, 202

边

在决策树中, 270

在神经网络中, 323

鸡蛋

受精的, 157

放弃者, 160

未受精的, 157

胜者, 160

yolker, 160

特征狗, 256

特征向量, 256

元素

在神经网络中, 323

在张量中, 430

按元素处理, 234

电梯调度（场景）, 9

ELMo, 571

ELU（指数 ReLU 激活函数）, 340

嵌入

上下文化的, 572

句子, 571

空间, 569

标记, 540

单词, 566

嵌入层线路符号, 574

编码器

对于自编码器, 496

对于 seq2seq, 561

编码器块（变压器）, 587

编码器-解码器注意力, 579

集成, 280

熵, 143

环境（在强化学习中）, 602

环境状态, 605

轮次, 605

纪元, 182

纪元梯度下降, 401

epsilon-贪心策略, 630

epsilon-soft 策略, 630

误差（神经网络误差）, 352

错误曲线（神经网络错误），355

基于错误的衰减，399

事件

在信息理论中，136

在概率中，50

证据，96

期望值，28

专家系统，4

可解释性。*见* 透明度

梯度爆炸，553

开发利用，608

探索或开发困境，608

指数衰减（学习率的），396

指数 ReLU 激活函数，340

超级树，302

极端随机树，302

**F**

f1 分数，71

公正的硬币，86

虚假警报，73

假发现率，73

假阴性，60

假阴性率，73

假阳性，60

假阳性率，73

FC。*见* 完全连接层

特征

作为样本的一部分，157

卷积滤波器的目标，438

特征袋法，301

特征检测器，438

特征工程，5

特征过滤，243

特征图，438

特征选择，243

特征处理，233

前馈网络，322

反馈，602

反馈（神经网络中的），324

受精，157

少量训练，594

张量的纤维大小，430

纤维，234

编码的保真度，497

用球体填充盒子（场景），175

滤波器

核，433

值，433

寻找电影中的动物（场景），566

寻找信息，4

微调，542

有限分布，22

拳头（发莫尔斯电码），138

拟合风速数据（场景），205

固定长度编码，141

固定步长衰减，399

平坦分布，22

扁平层

示例，475

插图，476

示意符号，474

翻转游戏（场景），614

折叠（交叉验证中的），192

卷积滤波器的足迹，436

森林，301

伪造货币（场景），650

分数卷积，457

频率学派概率

与贝叶斯方法相比，85

概览，84

冻结权重

深度梦境（deep dreaming），677

在生成对抗网络中（in a GAN），660

完全模态崩溃（full modal collapse），671

全连接层（fully connected layer）

介绍（introduction），328

示意符号（schematic symbol），328

全连接网络（fully connected network），328

函数（function）

参数（argument），118

连续的（continuous），118

图示（graphed），124

数学的（mathematical），118

返回值（return values），118

单值（single-valued），119

平滑（smooth），118

**G**

*g*（梯度），410

博弈论（game theory），656

生成对抗网络（GAN），649

垃圾进，垃圾出，223

门控递归单元（gated recurrent unit），554

高斯分布（Gaussian distribution），22

GD。*参见* 梯度下降（gradient descent）

泛化（generalization）

准确率（accuracy），197

错误（error），197

损失（loss），197

生成图像（用于风格迁移）（generated image for style transfer），680

生成对抗网络（generative adversarial network），649

生成器（generator），650

生成器-判别器（generator-discriminator），664

基尼不纯度（Gini impurity），281

Glorot 正态初始化（Glorot Normal initialization），325

Glorot 均匀初始化（Glorot Uniform initialization），325

GPT-2，593

GPT-3

讨论（discussion），596

性能（performance），597

梯度（gradient）

定义（definition），126

消失（vanishing），129

零（zero），129

梯度上升（gradient ascent），481

梯度下降（gradient descent）

Adadelta，418

Adagrad，417

Adam，420

批次（batch），401

纪元（epoch），401

小批量（mini-batch），405

动量（momentum），409

Nesterov，414

RMSprop，418

随机（stochastic），403

典型意义（typical meaning），407

Gram 矩阵（Gram matrix），681

图（graph）

无环（acyclic），324

有向（directed），323

介绍（introduction），323

加权图（weighted graph），324

图论（graph theory），323

大丹犬（Great Dane），184

贪婪算法（决策树）（greedy algorithm for decision trees），274

*Green Eggs and Ham*，137

网格（grid），329

真实值（ground truth），57

分组。*参见* 聚类（clustering）

GRU（门控递归单元），554

吉他数据（guitar data），228

**H**

He 正态初始化（He Normal initialization），325

He 均匀初始化（He Uniform initialization），325

头（在注意力层中）（head in attention layer），580

Heaviside 阶跃激活函数（Heaviside step activation function），334

隐藏层（hidden layer），327

隐藏状态（hidden state），549

层级（hierarchy）

特征的（of features），440

过滤器，461

尺度问题，471

高维空间，42

高维怪异性

超立方体，175

hyperorange，178

超球体，175

在超立方体中包装超球体，177

超立方体中的超球体体积，175

山丘（3D 表面），129

命中率，66

福尔摩斯，夏洛克（故事）

使用 GPT2 生成，596

使用 RNN 生成，555

霍普尔，爱德华，685

哈夫曼编码，148

哈士奇，185

超立方体，175

hyperorange，178

超参数

聚类数量，166

调整，168

超球体，175

假设（在贝叶斯规则中），102

**I**

i.i.d.（独立同分布），29

冰淇淋店（场景），55

理想化曲线，205

同分布，29

识别狗品种（场景），183

身份激活函数，331

IG（信息增益），281

图像分类器，6

图像矩阵（用于风格迁移），681

ImageNet，478

直接子节点（在决策树中），271

不纯度，280

初始主义，679

独立同分布，29

自变量，29

惯性，408

无穷分布，24

信息

定义，137

密度，143

发散，151

发现，4

增益，281

属性，136

信息交换

全局上下文，135

局部上下文，135

接收者，134

发送者，134

惊讶，134

信息泄漏

基本概念，187

在交叉验证中，239

信息理论，133

初始状态（在强化学习中），605

神经网络初始化，325

输入层，327

输入张量，329

即时奖励，610

内部节点（在决策树中），270

区间衰减，399

逆变换，234

编码反转，497

**J**

联合概率，53

JPG 编码器，495

垃圾场（场景）, 17

**K**

*k*

PCA 后的维度, 249

聚类数目, 166

折数, 192

最近邻数目, 264

k-means 聚类, 166

k 近邻算法, 264

Kandinsky, Wassily, 685

卷积核（卷积滤波器的核）, 433

核技巧, 287

关键字（对于 Transformer）, 575

切线线（切线）, 122

KL 散度, 151

kNN（k 近邻算法）, 264

Kullback-Leibler 散度, 151

**L**

L 学习, 616

L 表, 617

L 值, 617

标签, 156

语言模型, 541

最后时刻停止, 202

潜在层, 501

潜在空间, 509

潜变量, 501

集成构建中的收益递减法则, 300

层归一化, 583

层次示意图符号

注意力, 581

批归一化, 425

双向递归, 559

卷积, 471

卷积与下采样, 521

卷积与上采样, 521

随机失活, 423

嵌入, 574

展平, 474

完全连接, 328

遮蔽注意力, 588

多头注意力, 581

norm-add, 583

池化, 451

位置信息嵌入, 584

递归, 555

递归单元, 551

重新调整形状, 667

自注意力, 581

层

引言, 322

概述, 10

懒算法（kNN）, 265

叶节点（决策树中的叶子节点）, 270

泄漏 ReLU 激活函数, 337

学习率

在神经网络中, 377

在 Q 学习中, 629

学习率调整

常数, 391

延迟指数衰减, 398

基于误差的衰减, 399

指数衰减, 396

固定步长衰减, 399

间隔衰减, 399

慢速学习法（场景）, 354

LeCun 正态初始化, 325

LeCun 均匀初始化, 325

英语中字母频率, 139

寻求生命的探测器（场景）, 97

似然, 96

直线（在神经网络中），323

使用贝叶斯规则拟合直线（场景），212

线性激活函数，332

线性函数，331

线性层。*参见* 全连接层

链接（在决策树中），270

列表，329

局部感受野，436

逻辑流程，540

逻辑曲线激活函数，341

长短期记忆，553

长期依赖问题，563

循环（在神经网络中），324

损失，352

无损编码，496

有损编码，496

糟糕的学习，616

低维犬种描述（场景），255

低通滤波器，451

LSTM（长短期记忆），553

**M**

*m*

神经元值的修改，357

动量（在梯度下降中），410

机器学习，3

梯度的大小，128

*曼彻斯特候选人*，598

映射（变换），236

边际概率，55

马克思，格劳乔，559

掩码注意力层示意图，588

掩码（注意力解码器），588

匹配面罩（场景），462

矩阵，329

最大池化，452

最大上升，126

最大下降，126

函数的最大值

使用导数查找，125

使用梯度查找，128

全局最大值，119

局部最大值，121

maxout 激活函数，338

McGlassface，眼镜，61

均值，16

均值归一化，228

高斯均值，24

均方误差，365

均值减法，228

中位数，16

慢速最大值，632

用镜子发送消息（场景），497

小批量，405

小批量梯度下降，405

函数的最小值

使用导数查找，125

使用梯度查找，128

全局最小值，119

局部最小值，121

混合油漆颜色（场景），574

MLP（多层感知器），328

MNIST

用于自编码器，505

用于卷积，473

用于 GAN，667

模态崩溃，671

模式, 16

动量, 408

动量梯度下降, 409

莫奈，克劳德, 685

*拇指病*（情境），63

莫尔斯电码, 137

莫尔斯，塞缪尔, 139

MP (*拇指病*), 63

MP3 编码器, 495

均方误差 (MSE), 365

多头注意力

介绍, 580

示意图符号, 581

多类分类, 160

多维卷积, 443

多滤波损失, 676

多层损失, 676

多层感知机, 328

多项式分布, 27

多重相关性, 39

多变量变换, 231

蒙克，爱德华, 685

木偶剧

BERT, 590

ELMo, 571

音乐系统

收集数据, 199

良好拟合, 201

过拟合, 199

欠拟合, 200

**N**

n-gram, 595

朴素贝叶斯分类器, 290

纳什均衡, 657

自然语言生成, 541

自然语言处理, 540

自然语言理解, 540

井字棋, 603

负相关, 38

负协方差, 35

负梯度, 128

负预测值, 73

邻居 (kNN), 265

邻域, 120

Nesterov 动量, 414

网络崩溃, 329

神经网络

介绍, 315

简单示例, 321

神经风格迁移, 680

神经元

人工, 317

死亡, 345

概述, 11

真实, 314

神经递质, 314

新视野号太空探测器, 190

下一句预测, 591

自然语言生成 (NLG), 541

NLP (自然语言处理), 540

NLU (自然语言理解), 540

无免费午餐定理, 422

节点

在决策树中, 270

在神经网络中, 323

节点分裂

基尼不纯度, 281

不纯度, 280

信息增益, 281

概述, 272

纯度, 280

噪声曲线, 205

噪声 ReLU 激活函数, 339

名义数据, 226

非确定性（变分自编码器）, 521

非参数分类器, 264

非线性函数, 331

非线性。*参见* 激活函数

norm-add

定义, 583

示意符号, 583

正态偏差, 24

正态分布, 22

归一化, 228

NSP（下一个句子预测）, 591

数值数据, 226

**O**

OAA（对所有进行分类）, 161

可观测性, 608

观察（在贝叶斯法则中）, 102

离线算法, 403

按需算法（kNN）, 265

一对所有, 161

一对一卷积, 447

独热编码, 226

一对多, 161

一对一, 153

一对其余, 161

在线算法, 405

优化器, 387

序数数据, 226

异常值

清理数据时, 223

在划定边界时, 201

输出层, 327

输出张量, 329

OvA（一对多）, 161

过拟合

决策树, 275

定义, 196

狗的品种, 185

过冲（在梯度下降过程中）, 396

OvO（一对一）, 163

OvR（对其他分类进行分类）, 161

**P**

填充, 440

参数空间（用于自编码器）, 508

参数化混合, 498

参数化分类器, 264

参数化 ReLU 激活函数, 337

改写（文本）, 540

父

在决策树中, 271

在神经网络中, 324

偏相关, 39

部分模态坍塌, 671

PCA。*参见* 主成分分析

PDF（概率密度函数）, 21

惩罚（用于神经网络错误）, 352

感知机

介绍, 316

Mark I 感知机, 317

完美精度, 69

完美召回率, 69

性能指标

插图, 75

总结, 73

扰动, 491

毕加索, 巴勃罗, 681

分段线性激活函数, 336

像素, 430

行星开采（场景）, 97

平台（3D 表面）, 129

多数投票, 299

冥王星图像，190

pmf（概率质量函数），20

点对点前馈层，587

策略，606

多义性，559

贵宾犬，183

池化

平均值，452

讨论，449

层，452

最大值，452

示意符号，451

人口（统计学），32

位置信息

编码，584

示意符号，584

位置信息嵌入，585

正相关，38

正协方差，35

正向预测值，64

后验，96

精度

定义，64

完美，69

精度-召回权衡，67

预测值，57

预测温度下的交通（场景），234

预测，6

预训练，542

主成分分析

描述，244

最大方差线，246

投影，244

先验，96

概率

条件，50

依赖性，50

事件，50

独立性，50

介绍，19

联合，53

边际，55

简单，50

概率密度函数，21

概率分布

钟形曲线，22

伯努利，26

分类的，27

连续，20

垃圾场中的例子，19

有限，22

高斯，22

无限，24

介绍，17

多项式，27

正态，22

均匀，21

概率质量函数，20

ProGAN，670

投影，244

提示（文本生成），541

保护一个运输中的橙子（场景），177

剪枝，280

纯度，280

**Q**

Q 学习，626

Q 表，627

Q 值，627

Q/KV（查询/键，v 值），579

QKV（查询，键，v 值），575

二次代价函数，365

娃娃质量控制（场景），61

质量学习，626

定量数据，226

查询（对于转换器），575

退出者，160

**R**

随机森林, 301

随机标记, 302

随机变量

描述, 20

绘图, 20

介绍, 17

随机性, 16

真实世界数据, 183

记忆

定义, 66

完美的, 69

受体位点（神经元）, 314

配方, 6

重构（编码信号的还原）, 496

循环单元

介绍, 550

原理符号, 551

循环层原理符号, 555

循环神经网络

双向, 558

深度, 557

深度双向, 560

介绍, 548

卷积滤波器的参考点, 436

正则化

批量归一化, 424

丢弃法, 422

介绍, 203

层归一化, 583

强化学习, 602

拒绝, 73

相对熵, 151

发布数据, 183

发布的系统, 183

ReLU 激活函数, 336

记住人名（情景）, 196

重新参数化技巧, 527

表示融合, 498

形状变换层

介绍, 667

原理符号, 667

残差连接, 582

结果奖励, 610

奖励

折扣未来, 609

最终, 604

即时, 610

介绍, 602

结果, 610

总和, 609

总未来, 609

最终的, 604

奖励信号。*见* 奖励

被操控的硬币, 86

强化学习, 602

RMSprop, 418

RNN。*见* 循环神经网络

卷起的图示, 550

决策树中的根节点, 270

旋转验证。*见* 交叉验证

规则, 4

**S**

鞍点（3D 曲面）, 130

样本, 156

样本集, 32

样本处理, 232

SARSA, 639

情景

考古学聚类, 8

考古学掷硬币游戏, 103

婴儿长度, 33

制作圣代, 548

选择商店背景音乐, 198

硬币检测, 86

掷硬币游戏, 87

投掷飞镖, 48

区分牛和斑马, 224

电梯调度, 9

将一个球体填入盒子, 175

寻找电影中的动物, 566

拟合风速数据, 205

Flippers 游戏, 614

伪造货币, 650

冰淇淋店, 55

识别犬种, 183

垃圾场, 17

慢速学习法, 354

寻找生命的探测器, 97

使用贝叶斯定理进行线性拟合, 212

低维度犬种描述, 255

匹配面罩, 462

通过镜子传递信息, 497

油漆混色, 574

*morbus pollicus*, 63

行星采矿, 97

通过温度预测交通, 234

保护运输中的橙子, 177

娃娃的质量控制, 61

记住人的名字, 196

分类蛋, 156

疾病检测, 63

短信, 135

传输书籍, 146

科学记数法, 222

种子（用于文本生成）, 541

选择

有放回抽样（SWR）, 30

无放回抽样（SWOR）, 30

有放回, 30

无放回, 30

有放回选择, 30

无放回选择, 30

选择性持久短期记忆, 553

自注意力, 576

自注意力层

引言, 578

原理图符号, 581

语义学, 546

半监督学习, 501

敏感度, 66

句子嵌入, 571

情感分析

引言, 540

使用 BERT, 592

seq2seq, 561

序列, 539

SGD（随机梯度下降）, 401

香农, 克劳德, 133

移位不变性, 453

移位 ReLU 激活函数, 338

SI（斜截式）空间, 213

西伯利亚哈士奇, 185

同胞（决策树中的）, 271

Sigmoid 激活函数, 341

符号激活函数, 334

简单相关性, 39

简单概率, 50

正弦波激活函数, 343

跳跃连接, 582

熊的群体, 568

切片处理

按元素计算，234

按特征计算，233

按样本计算，232

滑动窗口，543

切线（斜率），122

softmax

作为激活函数，345

在 Q 学习中，630

softplus 激活函数，340

排序玩偶（场景），61

排序鸡蛋（场景），156

空间滤波器，436

特异性，73

分裂。*见*节点分裂

阶梯激活函数，332

标准差，24

标准化，229

星际飞船*忒修斯*，97

状态

代理（强化学习），605

环境（强化学习），605

隐藏（RNN），549

一个 RNN，548

启动（RNN），548

变量（强化学习），605

状态变量，605

统计，15

步骤激活函数，332

斯蒂文森，罗伯特·路易斯，137

随机环境，611

随机梯度下降，401

步长，453

强学习者，303

风格图像，680

风格损失，683

风格矩阵，681

风格迁移，680

主观贝叶斯，97

子树（决策树中的），271

总结（文本），540

监督学习

用于分类，156

概述，6

支持层，327

支持向量，284

支持向量机

核技巧，287

概述，282

严格参数 C，285

支持向量，284

惊讶，134

支持向量机。*见*支持向量机

滤波器（扫掠），433

swish 激活函数，340

SWOR（无替换选择），30

SWR（有替换选择），30

**T**

*双城记*

带嵌入，579

预测，545

切线，122

切线，122

目标（一热编码标签），361

温度（选择输出），595

张量，328

终端节点（在决策树中），270

测试数据

定义，186

永远无法从中学习，186

疾病测试（场景），63

短信（场景），135

TFR（总未来奖励）, 609

阈值

对于人工神经元, 316

对于真实神经元, 314

井字棋, 603

时间步长, 549

蟾蜍视觉系统

层次结构, 461

引言, 437

令牌

嵌入, 566

作为一个词, 540

总未来奖励, 609

总奖励, 609

训练

准确度, 196

基本概念, 182

深度网络概述, 12

错误, 196

流程图, 189

损失, 196

训练集, 182

传递函数。*参见* 激活函数

迁移学习, 542

变换

逆, 234

多变量, 231

概述, 223

单变量, 231

转换器

引言, 586

结构, 588

翻译, 540

传送书籍（场景）, 146

透明度

决策树, 274

一对一分类器, 164

转置卷积, 457

*宝岛*, 137

三重模块冗余, 297

真实发现率, 73

真实负例, 60

真实正例, 60

真实正例率, 66

乔治·特纳, 685

马克·吐温, 146

双月数据集, 275

I 型错误, 73

II 型错误, 73

**U**

不平衡决策树, 270

不确定性, 143

欠拟合, 197

不公平的硬币, 86

未受精, 157

均匀分布, 21

单位（人工神经元）, 315

单位步激活函数, 334

单变量变换, 231

通用语言模型微调, 574

普适扰动, 491

展开图, 550

无监督学习, 8

更新规则, 617

上采样, 457

增大尺寸, 457

用户数据, 183

**V**

VAE（变分自编码器）, 521

艾尔弗雷德·维尔, 139

验证数据。*参见* 验证集

验证错误, 197

验证集

定义, 187

从中估算误差, 189

山谷（3D 表面）, 129

值（对于转换器）, 575

文森特·梵高, 685

消失梯度

在表面上, 128

训练一个 RNN, 553

可变比特率编码, 143

方差

相对于偏置, 204

在统计学中, 26

方差归一化, 229

变分自编码器, 521

向量, 329

顶点（神经网络中的）, 323

VGG16

用于创意应用, 676

引言, 478

可视化滤波器, 480

词汇表, 541

体积, 329

投票（集成方法）

信心, 299

概述, 298

多数投票, 299

加权多数投票, 299

**W**

弱学习器, 303

权重

神经元命名约定, 322

对神经元的作用, 316

深度网络中的概述, 11

权重共享, 433

加权硬币, 86

加权图, 324

加权多数投票, 299

奇异性（高维）, 175

（递归单元的）宽度, 551

胜者（蛋）, 160

神经网络中的连接, 323

词嵌入, 566

**X**

Xavier 正常初始化, 325

Xavier 均匀初始化, 325

**Y**

蛋黄, 160

约克夏犬, 184

**Z**

零梯度, 128

零填充, 442

零点, 436

零维数组, 328

零样本训练, 594

本书的构思是在一对三折科学展览板上贴满了几十张黄色便签纸，每完成一个部分就贴上一个闪亮的星星贴纸。手稿是在 MacBook Pro 和 iMac 上使用 vi 文本编辑器编写的，生成 Markdeep 文件。最终编辑是使用 Microsoft Word 完成的，随后使用 Adobe Acrobat 和 Adobe InDesign 进行处理。插图是用彩色笔手绘的，然后在 Adobe Illustrator 和 Photoshop 中重新绘制。计算机生成的图像是通过在 Jupyter notebooks 中编写的 Python 代码生成的。重要的 Python 库包括 scikit-learn、scikit-image、numpy、scipy、pandas、matplotlib、TensorFlow 和 PyTorch。
