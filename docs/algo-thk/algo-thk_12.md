## 第十二章：**A

**算法运行时间**

![Image](img/common01.jpg)

本书中我们解决的每个竞争编程问题都有一个规定的时间限制，限制了我们的程序允许运行的最长时间。如果我们的程序超时，评测系统就会终止程序并显示“超时”错误。时间限制的设计目的是防止算法上过于简单的解决方案通过测试用例。问题的作者已经有了某些模型解法，并通过设置时间限制来判断我们是否展示了这些解法的思想。因此，除了正确性之外，我们还需要让程序运行得足够快。

### **时机的选择……以及其他一些因素**

大多数算法书籍在讨论运行时间时并不使用时间限制。然而，本书中经常出现时间限制和执行时间的概念。其主要原因是，这些时间可以帮助我们直观地理解程序的效率。我们可以运行程序并测量它所花费的时间。如果我们的程序太慢，超过了问题的时间限制，那么我们就知道我们需要优化当前的代码或找到一种全新的方法。我们不知道评测系统使用的是哪种计算机，但在我们自己的计算机上运行程序仍然有参考价值。假设我们在笔记本电脑上运行程序，发现它在某个小的测试用例上花了 30 秒。如果问题的时间限制是 3 秒，那么我们可以确信我们的程序显然还不够快。

然而，仅仅专注于执行时间是有限的。以下是五个原因：

**执行时间取决于计算机。** 正如前面所提到的，给我们的程序计时只告诉我们在一台计算机上运行程序所花费的时间。这是非常具体的信息，并且它几乎无法帮助我们理解在其他计算机上运行时会发生什么。在阅读本书时，你可能还会注意到，即使在同一台计算机上，程序的执行时间也会有所不同。例如，你可能在一个测试用例上运行程序，发现它需要 3 秒钟；然后你可能再运行一次，同样的测试用例，结果却是 2.5 秒或 3.5 秒。造成这种差异的原因是操作系统正在管理你的计算资源，并根据需要将它们分配到不同的任务中。操作系统所做的决策会影响程序的运行时间。

**执行时间取决于测试用例。** 在一个测试用例上对我们的程序进行计时，只能告诉我们程序在该测试用例上运行了多长时间。假设我们的程序在一个小测试用例上运行需要一秒钟，这看起来可能很快，但关于小测试用例的真相是：每个合理的解决方案都能解决这些问题。如果我让你排序几个数字，或优化地安排几个事件，或者做其他什么，你可以用你第一个正确的想法迅速完成。那么，真正有趣的，是大测试用例。它们是算法独创性体现的地方。我们的程序在大测试用例上或巨大的测试用例上需要多久才能完成？我们不知道。我们也需要在这些测试用例上运行程序。即便我们这样做，也可能会有特定类型的测试用例会导致较差的表现。我们可能会被误导，认为我们的程序比实际更快。

**程序需要实现。** 我们无法对一个没有实现的东西进行计时。假设我们在考虑一个问题，并想出了一个解决方案。它快吗？虽然我们可以通过实现它来了解，但提前知道这个想法是否可能导致一个快速的程序会更好。你不会实现一个你一开始就知道会错误的程序。同样，知道一个程序一开始就太慢，也会很好。

**计时无法解释缓慢。** 如果我们发现程序太慢，那么接下来的任务就是设计一个更快的程序。然而，单纯地计时并不能为我们提供程序为何缓慢的洞见。它只是慢而已。此外，如果我们设法想出一个可能改进程序的办法，我们需要实现它才能看到它是否有效。

**执行时间不容易传达。** 基于上述许多原因，使用执行时间与他人讨论算法效率是困难的。“我的程序在去年购买的这台电脑上运行需要两秒钟，测试用例包含八只鸡和四个鸡蛋，使用我用 C 语言编写的程序。你的程序呢？”

不用担心：计算机科学家已经设计出一种符号来解决计时的这些不足之处。它与计算机无关，与测试用例无关，也与特定的实现无关。它揭示了为什么一个程序会变慢。它容易传达。它被称为*大 O 符号*，接下来就会介绍。

### 大 O 符号

大 O 是计算机科学家用来简洁描述算法效率的记法。它将每个算法归入少数几个效率类别中的一个。效率类别告诉你一个算法有多快，或者等价地说，它做了多少工作。算法越快，做的工作越少；算法越慢，做的工作越多。每个算法都属于一个效率类别；效率类别告诉你相对于它必须处理的输入量，算法做了多少工作。要理解大 O，我们需要了解这些效率类别。我在这里将介绍三种：线性时间、常数时间和平方时间。

#### *线性时间*

假设我们提供一个递增顺序的整数数组，我们想返回其中的最大整数。例如，给定以下数组：

```
[1, 3, 8, 10, 21]
```

我们希望返回`21`。

一种方法是追踪到目前为止找到的最大值。每当我们找到比当前最大值更大的值时，就更新最大值。清单 A-1 实现了这个思想。

```
int find_max(int nums[], int n) {
  int i, max;
  max = nums[0];
  for (i = 0; i < n; i++)
    if (nums[i] > max)
      max = nums[i];
  return max;
}
```

*清单 A-1：在递增整数数组中查找最大值*

代码将`max`设置为`nums`数组中索引为`0`的值，然后通过循环遍历数组，寻找更大的值。不要担心循环的第一次迭代将`max`与其自身进行比较：那只是一次不必要的工作。

与其为特定的测试用例计时，不如思考这个算法根据数组大小所做的工作量。假设数组有五个元素。我们的程序做了什么？它在循环前执行了一个变量赋值，然后在循环中迭代了五次，最后返回结果。如果数组有 10 个元素，那么我们的程序做的也类似，只不过这次它在循环中迭代了 10 次而不是 5 次。那么，如果有一百万个元素呢？我们的程序会迭代一百万次。现在我们可以看到，循环前的赋值和循环后的返回相比，循环所做的工作量更为庞大。尤其是当测试用例变得非常大时，关键是循环的迭代次数。

如果我们的数组有*n*个元素，那么循环会迭代*n*次。在大 O 记法中，我们说这个算法是*O*(*n*)。可以这样理解：对于一个包含*n*个元素的数组，算法的工作量与*n*成正比。*O*(*n*)算法被称为*线性时间算法*，因为问题规模与所做工作的数量之间存在线性关系。如果我们将问题规模加倍，那么工作量也会加倍，从而使运行时间加倍。例如，如果在一个包含二百万个元素的数组上运行需要一秒钟，我们可以预期在一个包含四百万个元素的数组上运行需要大约两秒钟。

请注意，我们不需要运行代码就能得出这个结论。我们甚至不需要写出代码。（好吧……是的，我确实写了代码，但那只是为了让算法更清晰。）说一个算法是 *O*(*n*)，向我们提供了问题规模和运行时增长之间的基本关系。无论我们使用什么计算机，或者查看哪个测试案例，这都是成立的。

#### *常数时间复杂度*

我们知道数组中有一些信息我们还没有利用：整数是按递增顺序排列的。因此，最大的整数一定在数组的末尾。我们可以直接返回它，而不是通过对数组进行穷举搜索最终找到它。列表 A-2 展示了这一新思路。

```
int find_max(int nums[], int n) {
  return nums[n - 1];
}
```

*列表 A-2：在递增整数数组中找到最大值*

这个算法根据数组大小做多少工作？有趣的是，数组的大小已经不再重要！无论数组有 5 个元素，10 个元素，还是一百万个元素，算法都只访问并返回 `nums[n - 1]`，即数组的最后一个元素。算法并不关心大小。在大 O 符号中，我们说这个算法是 *O*(1)。它被称为 *常数时间算法*，因为它所做的工作量是恒定的，随着问题规模的增大并不会增加。

这是最好的算法类型。无论数组多大，我们都可以预期大致相同的运行时间。它显然比线性时间算法要好，后者随着问题规模的增大而变得更慢。不过，并不是所有有趣的问题都能通过常数时间算法解决。例如，如果我们给定的是一个无序的数组，而不是递增的数组，那么常数时间算法就不适用了。我们不可能仅查看固定数量的数组元素，就能保证找到最大值。

#### *另一个例子*

请看列表 A-3 中的算法：它是 *O*(*n*) 还是 *O*(1) 或者其他什么？（注意，我没有列出函数和变量的定义，以避免我们想要编译和运行它。）

```
total = 0;
for (i = 0; i < n; i++)
  total = total + nums[i];
for (i = 0; i < n; i++)
  total = total + nums[i];
```

*列表 A-3：这是什么样的算法？*

假设数组 `nums` 有 *n* 个元素。第一个循环执行 *n* 次，第二个循环也执行 *n* 次。总共有 2*n* 次迭代。作为第一次尝试，直观上可以说这个算法是 *O*(2*n*)。虽然这么说技术上是正确的，但计算机科学家通常会忽略 2，直接写作 *O*(*n*)。

这可能看起来有点奇怪，因为这个算法的速度是列表 A-1 中的算法的两倍，但我们却宣称它们都是 *O*(*n*)。原因在于我们的记法在简洁性和表现力之间的平衡。如果我们保留 2，那么或许更准确，但这也会掩盖它是一个线性时间算法这一事实。无论是 2*n* 还是 3*n*，或者任何乘以 *n* 的数字，它的基本线性运行时增长是不会改变的。

#### *二次时间复杂度*

我们现在已经看到了线性时间算法（在实际中非常快速）和常数时间算法（比线性时间算法还要快）。接下来让我们看一下比线性时间还要慢的算法。代码在示例 A-4 中。

```
total = 0;
for (i = 0; i < n; i++)
  for (j = 0; j < n; j++)
    total = total + nums[j];
```

*示例 A-4：一个二次时间算法*

与示例 A-3 相比，请注意，现在的循环是嵌套的，而不是顺序的。外层循环的每次迭代都会导致内层循环的*n*次迭代。外层循环迭代*n*次。因此，内层循环的总迭代次数以及更新`total`的次数是*n*²。（外层循环的第一次迭代需要*n*的工作量，第二次需要*n*的工作量，第三次需要*n*的工作量，以此类推。总数是*n* + *n* + *n* + …… + *n*，其中我们加上*n*的次数是*n*。）

在大 O 符号中，我们说这个算法是*O*(*n*²)。它被称为*二次时间算法*，因为“二次”是指数学中 2 的幂次。

现在我们来探讨为什么二次时间算法比线性时间算法慢。假设我们有一个需要*n*²步的二次时间算法。在问题规模为 5 时，它需要 5² = 25 步；在问题规模为 10 时，它需要 10² = 100 步；在问题规模为 20 时，它需要 20² = 400 步。注意，当我们将问题规模加倍时，所做的工作是*四倍*增加的。这比线性时间算法要差得多，因为在后者中，问题规模加倍时，工作量只会加倍。

不要惊讶，2*n*²步、3*n*²步等算法也被归类为二次时间算法。大 O 符号隐藏了*n*²项前面的部分，就像它隐藏了线性时间算法中*n*项前面的部分一样。

如果我们有一个算法，它需要 2*n*² + 6*n*步呢？这也是一个二次时间算法。我们正在将 2*n*²的二次运行时间与 6*n*的线性运行时间相加。结果仍然是一个二次时间算法：二次部分的四倍增长很快就会主导线性部分的两倍增长。

#### *本书中的大 O 符号*

关于大 O 符号还有很多可以讲的内容。它有一个正式的数学基础，计算机科学家用它来严格分析算法的运行时间。除了我在这里介绍的三种效率类别外，还有其他效率类别（如果需要，我会介绍本书中出现的其他几种）。如果你有兴趣深入了解，肯定还有更多的内容可以学习，但我在这里介绍的已经足够满足我们的需求了。

本书中的大 O 符号通常根据需要出现。我们可能会为一个问题寻求初步的解决方案，却发现收到评测系统的“超时”错误。在这种情况下，我们需要理解自己哪里出了问题，而这种分析的第一步就是理解我们的运行时间如何随着问题规模的变化而增长。大 O 分析不仅能确认代码运行缓慢的事实，而且通常能揭示出代码中的具体瓶颈。然后，我们可以利用这种更深入的理解来设计更高效的解决方案。
