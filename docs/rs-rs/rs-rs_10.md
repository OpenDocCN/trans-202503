# 第十章：并发（和并行）

![](img/chapterart.png)

本章的目的是为你提供所有必要的信息和工具，使你能够在 Rust 程序中有效地利用并发，实现在库中对并发的支持，并正确使用 Rust 的并发原语。我不会直接教你如何实现并发数据结构或编写高性能的并发应用程序。我的目标是让你充分理解底层机制，从而具备使用它们的能力，无论你将来需要它们做什么。

并发有三种形式：单线程并发（就像我们在第八章讨论的`async`/`await`），单核多线程并发，以及多核并发，后者实现了真正的并行。每种形式都允许在程序中以不同的方式交替执行并发任务。如果考虑到操作系统调度和抢占的细节，甚至还有更多的子形式，但我们不会深入探讨这些。

在类型层面上，Rust 只表示并发的一个方面：多线程。类型要么对多个线程是安全的，要么不是。如果程序有多个线程（因此是并发的），但只有一个核心（因此不是并行的），Rust 必须假设，如果有多个线程，就可能存在并行性。我们接下来要讨论的大多数类型和技术，无论两个线程是否实际并行执行，都同样适用，因此为了保持语言简洁，我会在本章中用*并发*这个词来表示“事物大致同时运行”的非正式意义。当这种区别很重要时，我会特别指出。

Rust 对基于类型的安全多线程的特别之处在于，这不是编译器的特性，而是一个库特性，开发人员可以扩展它来开发复杂的并发契约。由于线程安全通过`Send`和`Sync`实现以及约束在类型系统中表示，并且这些约束会一直传递到应用代码中，整个程序的线程安全性仅通过类型检查就能得到验证。

《Rust 编程语言》已经涵盖了并发的基础知识，包括`Send`和`Sync`特性、`Arc`和`Mutex`，以及通道。因此，我不会在这里重复这些内容，除非在某些其他话题的上下文中特别值得提及。相反，我们将探讨使并发变得困难的原因，以及一些常见的并发模式来应对这些困难。我们还将探索并发和异步的相互作用（以及它们不相互作用的方式），然后深入研究如何使用原子操作实现更低级别的并发操作。最后，我将以一些建议结束本章，帮助你在处理并发代码时保持理智。

## 并发的困境

在我们深入探讨并发编程的良好模式以及 Rust 的并发机制细节之前，花些时间理解并发为何具有挑战性是值得的。换句话说，为什么我们需要并发代码的特殊模式和机制？

### 正确性

并发中的主要难点在于协调对共享资源的访问——特别是写访问。如果很多线程只是为了读取共享资源而共享它，那么通常很容易：你将它放入一个`Arc`中，或者放入你能获得`&'static`引用的地方，这样就完成了。但一旦有线程想要写入，问题就会层出不穷，通常表现为*数据竞争*。简而言之，数据竞争发生在一个线程更新共享状态时，而第二个线程也在访问该状态，可能是在读取它或更新它。如果没有额外的安全措施，第二个线程可能读取到部分被覆盖的状态，破坏第一个线程写入的部分内容，或根本无法看到第一个线程的写入！一般来说，所有的数据竞争都被认为是未定义行为。

数据竞争是更广泛问题类别的一部分，这类问题主要（但不限于）发生在并发环境中：*竞态条件*。竞态条件发生在多个结果可能由一系列指令产生时，这取决于系统中其他事件的相对时机。这些事件可以是线程执行特定代码、定时器触发、网络数据包到达，或任何其他与时间相关的事件。与数据竞争不同，竞态条件并不固有地是坏的，也不被认为是未定义行为。然而，当出现特别奇怪的竞态时，它们会成为漏洞的温床，正如本章中将要展示的那样。

### 性能

开发者通常在程序中引入并发，是希望提高性能。更准确地说，他们希望并发能够通过利用更多的硬件资源来提高每秒操作的总数。这可以在单个核心上通过让一个线程在另一个线程等待时运行，或者在多个核心上通过让线程同时工作（每个核心上一个线程）来完成，这样本来在单个核心上串行执行的操作得以并行化。当开发者谈到并发时，通常指的是后者这种性能提升，常常以“可扩展性”的形式来讨论。在这个语境中，所谓的可扩展性是指“该程序的性能随着核心数的增加而提升”，意味着如果你给程序更多的核心，它的性能会有所提高。

虽然实现这样的加速是可能的，但比看起来更困难。可扩展性的最终目标是线性扩展性，即当核心数翻倍时，程序每单位时间完成的工作量也翻倍。线性扩展性也常被称为完美扩展性。然而，现实中，很少有并发程序能实现这种加速。亚线性扩展性更为常见，即当从一个核心扩展到两个核心时，吞吐量几乎呈线性增长，但增加更多的核心会带来收益递减。有些程序甚至会经历负扩展性，即让程序使用更多核心反而*减少*吞吐量，通常是因为许多线程都在争用某些共享资源。

可以把它想象成一群人试图将气泡膜上的所有气泡戳破——最初，增加更多的人是有帮助的，但到了某个阶段，由于人太多，每个人的工作反而变得更难。 如果参与的人特别低效，你的团队可能最终会站在那讨论接下来谁该戳破气泡，结果一个气泡也没戳破！这种本应并行执行的任务之间的干扰被称为*争用*，它是扩展性差的死敌。争用可能有多种表现方式，但主要的罪魁祸首是互斥、共享资源耗尽和虚假共享。

#### 互斥

当任何时刻只有一个并发任务可以执行特定代码段时，我们称该段代码的执行是互斥的——如果一个线程执行它，其他线程不能同时执行它。这个概念的典型例子是互斥锁，或称为*mutex*，它明确规定每次只有一个线程可以进入程序代码中的某个关键区段。然而，互斥也可能隐式发生。例如，如果你启动一个线程来管理共享资源，并通过`mpsc`通道发送任务给它，那么这个线程实际上实现了互斥，因为每次只有一个任务可以执行。

互斥也可能在调用操作系统或库函数时发生，这些函数在内部强制执行对关键区段的单线程访问。例如，多年来，标准的内存分配器在某些分配时需要互斥，这使得内存分配成为一种在其他高度并行的程序中造成显著争用的操作。同样，许多看起来应该是独立的操作系统操作，比如在同一目录下创建两个不同名字的文件，可能最终必须在内核中顺序执行。

互斥是并行加速中最明显的障碍，因为根据定义，它强制执行程序某些部分的串行执行。即使你让程序的其他部分完美地随着核心数扩展，你能达到的总加速仍然受到互斥、串行部分长度的限制。要注意你的互斥部分，并尽量将其限制在严格必要的地方。

#### 共享资源耗尽

不幸的是，即使你在任务内部实现了完美的并发，任务需要交互的环境本身可能并不具备完美的可扩展性。内核每秒钟只能处理一定数量的 TCP 套接字发送，内存总线也只能同时进行有限数量的读取，而你的 GPU 在并发处理方面也有其容量限制。对此没有解决办法。通常，环境就是在实践中完美可扩展性崩溃的地方，针对这种情况的修复通常需要大规模的重新设计（甚至是新的硬件！），因此我们在本章中不会再多谈这个话题。只要记住，可扩展性很少是你能够“实现”的，而更多的是你需要持续追求的目标。

#### 假共享

假共享发生在两个不应该相互竞争的操作，尽管它们不相关，仍然相互竞争，从而阻碍了高效的并行执行。这通常是因为这两个操作恰好在某个共享资源上发生冲突，即便它们使用的是该资源的不同部分。

这个问题最简单的例子是锁的过度共享，其中一个锁保护某个复合状态，而两个原本独立的操作都需要获取锁来更新它们各自的状态部分。这就意味着这些操作必须串行执行，而无法并行执行。在某些情况下，可以将一个锁分成两个，每个操作负责独立的部分，这样操作就可以并行进行。然而，这种分锁并不总是直接可行——状态可能因为某个第三操作需要对所有状态部分加锁，所以使用了单一锁。通常情况下，你仍然可以分锁，但必须小心不同线程获取分锁的顺序，以避免死锁的发生——死锁发生在两个操作尝试按不同的顺序获取锁时（如果你感兴趣，可以查阅“哲学家就餐问题”）。另外，对于某些问题，你也许能够完全避免临界区，通过使用底层算法的无锁版本，尽管这些也很难做到完美。归根结底，假共享是一个难以解决的问题，没有一个通用的解决方案，但识别出问题本身就是一个良好的开始。

伪共享的一个更微妙的例子出现在 CPU 层面，正如我们在第二章中简要讨论的那样。CPU 在内部按缓存行操作内存——即内存中连续字节的较长序列——而不是按单个字节操作，以摊销内存访问的成本。例如，在大多数英特尔处理器上，缓存行的大小是 64 字节。这意味着每个内存操作实际上最终读取或写入的是 64 字节的整数倍。伪共享的发生是在两个核心希望更新两个不同字节的值，而这两个字节恰好位于同一个缓存行中时；即使这些更新在逻辑上是分离的，这些更新也必须顺序执行。

这看起来可能太低级，不值得关注，但实际上，这种伪共享会严重影响应用程序的并行加速。想象一下，你为每个线程分配了一个整数数组来表示它完成了多少个操作，但这些整数都位于同一个缓存行内——现在，所有原本并行的线程将在每次执行操作时争用那一行缓存。如果这些操作比较快速，*大部分*执行时间可能最终都花费在争用这些计数器上！

避免伪缓存行共享的技巧是通过填充你的值，使其大小与缓存行相等。这样，两个相邻的值总是位于不同的缓存行上。当然，这也会增加数据结构的大小，所以只有当基准测试表明存在问题时，才使用这种方法。

## 并发模型

Rust 有三种常见的并发模式，你很可能会遇到：共享内存并发、工作池和演员模型。要详细介绍每种实现并发的方法本身就足以写一本书，因此在这里，我将专注于这三种模式。

### 共享内存

共享内存并发，从概念上讲，非常简单：线程通过在它们之间共享的内存区域上进行操作来协作。这可能表现为由互斥锁保护的状态，或者存储在支持多个线程并发访问的哈希映射中。多个线程可能在不重叠的数据片段上执行相同的任务，例如多个线程对`Vec`的不同子范围执行某些功能，或者它们可能执行需要一些共享状态的不同任务，例如在数据库中，一个线程处理用户对表的查询，而另一个线程在后台优化用于存储该表的数据结构。

当你使用共享内存并发时，选择合适的数据结构非常重要，尤其是当涉及的线程需要紧密配合时。一个常规的互斥锁可能会限制核心数的扩展，而一个读写锁可能以牺牲写操作速度为代价，允许更多的并发读取，而一个分片的读写锁可能允许完美可扩展的读取，但会使写操作变得高度破坏性。类似地，一些并发哈希映射旨在提供良好的全方位性能，而其他则特别针对例如并发读取，在写操作较为稀少的场景下表现更好。通常，在共享内存并发中，你希望使用那些专门为你的目标用例设计的数据结构，这样你就可以利用那些针对你应用程序不关心的性能方面做出的优化，换取那些你关注的性能优化。

共享内存并发非常适合那些线程需要以一种不交换顺序的方式共同更新一些共享状态的用例。也就是说，如果一个线程必须使用某个函数`f`更新状态`s`，而另一个线程必须使用另一个函数`g`更新状态，并且`f(g(s)) != g(f(s))`，那么共享内存并发可能是必需的。如果情况并非如此，其他两种模式可能会更适合，因为它们通常能带来更简单和更高性能的设计。

### 工作池

在工作池模型中，许多相同的线程从共享的任务队列中获取任务，然后完全独立地执行这些任务。例如，Web 服务器通常会有一个工作池来处理传入的连接，而异步代码的多线程运行时通常使用工作池来共同执行应用程序的所有未来任务（或者更准确地说，是其顶层任务）。

共享内存并发和工作池之间的界限通常是模糊的，因为工作池通常使用共享内存并发来协调它们如何从队列中获取任务，以及如何将未完成的任务返回队列。例如，假设你正在使用数据并行库`rayon`来并行处理一个向量的每个元素。在幕后，`rayon`会启动一个工作池，将向量分割成子区间，然后将子区间分配给池中的线程。当池中的某个线程完成一个子区间时，`rayon`会安排它开始处理下一个未处理的子区间。向量在所有工作线程之间共享，线程通过一个支持工作窃取的共享内存队列样式的数据结构进行协调。

工作窃取是大多数工作池的一个关键特性。其基本前提是，如果某个线程提前完成了工作，并且没有更多未分配的工作可用，那么该线程可以窃取已经分配给其他工作线程但尚未开始的工作。并非所有的工作都需要相同的时间来完成，因此即使每个工作线程被分配了相同的*工作数量*，一些线程可能比其他线程更快完成它们的任务。与其坐着等那些执行较长任务的线程完成，不如让那些提前完成的线程去帮助落后的线程，从而使整体操作能够更快完成。

实现一个支持这种工作窃取的数据结构是相当困难的，尤其是在不引入线程间不断相互窃取工作的显著开销的情况下，但这个特性对高性能的工作池至关重要。如果你需要一个工作池，通常最好的选择是使用已经经过大量工作验证的工作池，或者至少重用现有工作池的数据结构，而不是从头开始自己编写。

当每个线程执行的工作相同，但它所处理的数据*不同*时，工作池是一个不错的选择。在`rayon`并行映射操作中，每个线程执行相同的映射计算，只是它们在不同的底层数据子集上执行这个计算。在一个多线程异步运行时，每个线程简单地调用`Future::poll`，它们只是针对不同的 future 进行调用。如果你开始需要区分线程池中的线程，那么可能需要考虑采用不同的设计。

### Actor

Actor 并发模型在许多方面与工作池模型正好相反。工作池有许多相同的线程共享一个工作队列，而 actor 模型则有许多独立的工作队列，每个队列对应一个“话题”。每个工作队列输入到一个特定的 actor，由它处理所有与应用程序状态子集相关的任务。这个状态可能是数据库连接、文件、度量数据结构，或者任何你能想象的，可能需要多个线程访问的结构。不管是什么，单个 actor 拥有该状态，如果某个任务想与这个状态交互，它需要向拥有者 actor 发送一条消息，概述它希望执行的操作。当拥有者 actor 接收到消息后，它执行指定的操作，并在相关的情况下将操作结果反馈给询问任务。由于 actor 对其内部资源拥有独占访问权限，因此除了进行消息传递所需的同步机制外，不需要其他锁或同步机制。

Actor 模式的一个关键点是，所有的 actor 都会相互通信。例如，负责日志记录的 actor 如果需要写入文件和数据库表，它可能会向负责这两项操作的 actor 发送消息，要求它们分别执行相应的动作，然后继续处理下一个日志事件。通过这种方式，actor 模型更像是一个网络，而不是轮子上的辐条——一个用户请求到网络服务器可能开始时只是向负责该连接的 actor 发出的单一请求，但可能会传递出成百上千条消息，最终到达系统深处的多个 actor，才满足用户的请求。

在 actor 模式中，没有任何要求每个 actor 都必须是一个独立的线程。相反，大多数 actor 系统建议应该有大量的 actor，因此每个 actor 应该映射为一个任务，而不是一个线程。毕竟，actor 只在执行时需要独占其封装的资源，并不关心它是否运行在自己的线程上。实际上，actor 模型通常与工作池模型一起使用——例如，使用多线程异步运行时 Tokio 的应用程序可以为每个 actor 启动一个异步任务，Tokio 会将每个 actor 的执行转变为工作池中的一项工作。因此，给定 actor 的执行可能会在工作池中的线程之间移动，因为 actor 会在执行时交出控制权并在稍后恢复执行，但每次执行时，actor 都会保持对其封装资源的独占访问。

Actor 并发模型非常适合于资源可以相对独立操作，且每个资源内部几乎没有或没有并发机会的情况。例如，操作系统可能会为每个硬件设备分配一个 actor，网络服务器可能会为每个后端数据库连接分配一个 actor。如果你只需要少数几个 actor，或者工作负载在各个 actor 之间差异很大，或者某些 actor 变得很大，那么 actor 模型可能就不太适用了——在这些情况下，你的应用程序可能会因为系统中某个单一 actor 的执行速度成为瓶颈。而且，由于每个 actor 都期望独占它所负责的资源，因此你无法轻易地将这个瓶颈 actor 的执行过程并行化。

## 异步与并行

正如我们在第八章中讨论的，Rust 中的异步性实现了没有并行的并发——我们可以使用诸如 select 和 join 这样的结构，让单个线程轮询多个 future，并在其中一个、一些或所有 future 完成时继续执行。由于没有涉及并行，因此并发使用 futures 并不要求这些 futures 必须是 `Send`。即使是将一个 future 作为附加的顶层任务进行启动，也不需要 `Send`，因为单个执行线程可以同时管理多个 future 的轮询。

然而，在*大多数*情况下，应用程序既需要并发也需要并行。例如，如果一个 Web 应用程序为每个传入的连接构建一个`Future`，因此会有多个活跃连接同时存在，它可能希望异步执行器能够利用主机计算机的多个核心。这不会自然而然地发生：你的代码必须明确告诉执行器哪些`Future`可以并行执行，哪些不能。

特别地，必须向执行器提供两条信息，以便让它知道可以将`Future`中的工作分配到线程池中。第一条是这些`Future`是`Send`类型——如果不是，执行器将不允许将这些`Future`发送到其他线程进行处理，也就无法实现并行；只有构建这些`Future`的线程才能调用它们的`poll`方法。

第二个信息是如何将`Future`分割成可以独立操作的任务。这与第八章讨论的任务与`Future`的关系有关：如果一个巨大的`Future`包含多个`Future`实例，而这些`Future`实例本身对应的是可以并行运行的任务，那么执行器仍然必须在顶层`Future`上调用`poll`，并且必须由单个线程来调用，因为`poll`需要`&mut self`。因此，为了实现并行，必须显式地生成你希望能够并行运行的`Future`。此外，由于第一个要求，执行器函数需要保证传入的`Future`是`Send`类型。

## 低级并发

标准库提供了`std::sync::atomic`模块，它提供了对底层 CPU 原语的访问，像通道和互斥锁这样的高级构造正是基于这些原语构建的。这些原语以以`Atomic`开头的原子类型的形式出现——`AtomicUsize`、`AtomicI32`、`AtomicBool`、`AtomicPtr`等等，还有`Ordering`类型，以及两个名为`fence`和`compiler_fence`的函数。我们将在接下来的几个部分详细讲解这些内容。

这些类型是构建任何需要在线程之间通信的代码的基础块。互斥锁、通道、屏障、并发哈希表、无锁栈以及所有其他同步构造最终都依赖于这些原语来完成它们的工作。它们本身也非常有用，特别是在需要线程间轻量级协作的场景中，这时像互斥锁这样的重型同步机制显得过于繁重——例如，用来递增共享计数器或将共享布尔值设置为`true`。

原子类型之所以特别，是因为它们定义了多个线程尝试并发访问时应该发生的语义。这些类型都支持（大致上）相同的 API：`load`、`store`、`fetch_*`和`compare_exchange`。在本节的其余部分，我们将探讨这些函数的作用，如何正确使用它们，以及它们的应用场景。但首先，我们必须讨论低级内存操作和内存排序。

### 内存操作

非正式地，我们通常将访问变量称为“从内存读取”或“写入内存”。实际上，代码使用变量与实际的 CPU 指令访问内存硬件之间有很多中间机制。理解这些机制，至少从高层次理解它们，对于理解并发内存访问的行为至关重要。

编译器决定在程序读取变量的值或给变量赋新值时发出哪些指令。它可以对代码进行各种转换和优化，并可能最终重新排序程序语句、消除其认为多余的操作，或使用 CPU 寄存器而不是实际内存来存储中间计算结果。编译器在这些转换上受到一定的限制，但最终只有部分变量访问会实际转化为内存访问指令。

在 CPU 级别，内存指令有两种主要形式：加载和存储。加载指令将内存位置的字节拉入 CPU 寄存器，存储指令则将 CPU 寄存器中的字节存储到内存位置。加载和存储操作一次处理较小的内存块：在现代 CPU 上通常是 8 个字节或更少。如果一个变量访问跨越的字节数超过了单次加载或存储可以访问的范围，编译器会自动将其转化为多个加载或存储指令，视情况而定。CPU 在执行程序指令时也有一定的灵活性，以更好地利用硬件并提高程序性能。例如，现代 CPU 常常并行执行指令，甚至在指令之间没有依赖关系时会乱序执行。此外，CPU 与计算机的 DRAM 之间还有几层缓存，这意味着对给定内存位置的加载可能不会看到最新的存储操作，按时钟时间来计算。

在大多数代码中，编译器和 CPU 只能以不影响程序语义的方式转换代码，因此这些转换对程序员是不可见的。然而，在并行执行的上下文中，这些转换可能对应用程序的行为产生重大影响。因此，CPU 通常提供多种不同的加载和存储指令变体，每种变体都有不同的保证，说明 CPU 如何重新排序它们以及它们如何与其他 CPU 上的并行操作交错执行。类似地，编译器（或者更准确地说，编译器编译的语言）提供了不同的注解，您可以使用这些注解为其某些内存访问的子集强制执行特定的执行约束。在 Rust 中，这些注解以原子类型及其方法的形式出现，我们将在本节的剩余部分对其进行详细分析。

### 原子类型

Rust 的原子类型之所以被称为原子类型，是因为它们可以原子地访问——也就是说，原子类型变量的值是一次性写入的，绝不会通过多个存储操作进行写入，从而保证了对该变量的加载操作无法观察到只有部分字节发生了变化，而其他字节尚未发生变化（或还没有发生变化）。通过与非原子类型的对比，最容易理解这一点。例如，将新值重新赋给类型为 `(i64, i64)` 的元组通常需要两条 CPU 存储指令，每条指令对应一个 8 字节的值。如果一个线程执行这两条存储指令，另一个线程则可以（如果暂时忽略借用检查器）在第一次存储之后、第二次存储之前读取该元组的值，从而得到该元组值的不一致视图。它最终会读取到第一个元素的新值和第二个元素的旧值，而第二个元素的值根本没有被任何线程存储过。

CPU 只能原子地访问某些大小的值，因此只有少数几种原子类型，所有这些类型都位于 `atomic` 模块中。每种原子类型的大小都是 CPU 支持原子访问的大小，并且针对诸如值是否为有符号类型、区分原子 `usize` 和指针（其大小与 `usize` 相同）等情况提供了多个变体。此外，原子类型还具有显式的方法，用于加载和存储它们所持有的值，并且还有一些更复杂的方法，我们稍后会回到这些方法，这样可以使程序员编写的代码与最终生成的 CPU 指令之间的映射更加清晰。例如，`AtomicI32::load` 执行对一个有符号 32 位值的单次加载操作，而 `AtomicPtr::store` 执行对一个指针大小的值（在 64 位平台上为 64 位）的单次存储操作。

### 内存排序

大多数原子类型的方法都接受一个 `Ordering` 类型的参数，该参数决定了原子操作所受的内存排序限制。在不同线程之间，原子值的加载和存储可能仅由编译器和 CPU 以与每个原子操作请求的内存排序兼容的交错方式来排序。在接下来的几个部分中，我们将看到一些示例，说明为什么对排序的控制对于从编译器和 CPU 获得预期的语义是重要且必要的。

内存排序常常显得直觉上反常，因为我们人类习惯从上到下阅读程序，并想象它们是逐行执行的——但实际上，当代码运行到硬件时，并不是按这个方式执行的。内存访问可能会被重新排序，甚至可能完全省略，一个线程的写操作可能不会立即被其他线程看到，即使程序顺序中稍后的写操作已经被观察到。

可以这样理解：每个内存位置看到的是来自不同线程的一系列修改，并且不同内存位置的修改序列是相互独立的。如果两个线程 T1 和 T2 都写入内存位置 M，那么即使 T1 按照用户用秒表测量的顺序先执行，T2 对 M 的写操作仍然可能看起来先于 T1 执行，前提是两者执行之间没有其他约束。实际上，*计算机在确定给定内存位置的值时并不考虑墙钟时间*——唯一重要的是程序员对什么构成有效执行所施加的执行约束。例如，如果 T1 向 M 写入数据，然后启动线程 T2，T2 然后写入 M，计算机必须识别 T1 的写操作发生在前，因为 T2 的存在依赖于 T1。

如果这很难理解，不用担心——内存顺序是个令人费解的话题，而且语言规范通常使用非常精确但不太直观的措辞来描述它。我们可以通过关注底层硬件架构构建一个更易理解的心理模型，尽管它稍微简化一些。基本来说，你的计算机内存是作为一个树状层级结构来组织的，其中叶子节点是 CPU 寄存器，根节点是存储在物理内存芯片上的数据，通常称为主内存。两者之间有多个层级的缓存，并且不同层级的缓存可能位于不同的硬件上。当一个线程向内存位置写入时，实际上发生的是 CPU 启动一个写请求，该请求会从给定的 CPU 寄存器开始，最终向上进入内存层级直到主内存。当一个线程执行加载操作时，请求会沿层级流动，直到找到有该值的缓存层级并从那里返回。问题在于：写操作并非在所有地方都可见，直到所有缓存都更新了被写入的内存位置，但其他 CPU 可以同时对同一内存位置执行指令，这时就会出现奇怪的情况。因此，内存顺序是一种请求精确语义的方法，描述在多个 CPU 访问特定内存位置时发生了什么操作。

记住这一点后，我们来看看 `Ordering` 类型，它是我们作为程序员用来对并发执行施加额外约束的主要机制。

`Ordering` 被定义为一个 `enum`，其变体如 列表 10-1 所示。

```
enum Ordering {
    Relaxed,
    Release,
    Acquire,
    AcqRel,
    SeqCst
}
```

列表 10-1：`Ordering` 的定义

每个这些地方对源代码到执行语义的映射施加不同的限制，接下来我们将依次探讨每一个。

#### 放松顺序

放松排序本质上并不对并发访问该值提供任何保证，除了访问是原子的这一事实。特别地，放松排序并不保证不同线程间内存访问的相对顺序。这是最弱的内存排序形式。示例 10-2 展示了一个简单的程序，其中两个线程使用 `Ordering::Relaxed` 访问两个原子变量。

```
static X: AtomicBool = AtomicBool::new(false);
static Y: AtomicBool = AtomicBool::new(false);

let t1 = spawn(|| {
  1 let r1 = Y.load(Ordering::Relaxed);
  2 X.store(r1, Ordering::Relaxed);
});
let t2 = spawn(|| {
  3 let r2 = X.load(Ordering::Relaxed);
  4 Y.store(true, Ordering::Relaxed)
});
```

示例 10-2：两个竞态线程与 `Ordering::Relaxed`

查看作为 `t2` 生成的线程，你可能会认为 `r2` 永远不会是 `true`，因为所有的值在同一线程将 `true` 赋值给 `Y` 之后，直到读取 `X` 这一行*才*变成 `true`，之前都是 `false`。然而，在放宽的内存排序下，这个结果完全是可能的。原因在于，CPU 允许重新排序涉及的加载和存储操作。让我们逐步分析一下，是什么让 `r2 = true` 成为可能。

首先，CPU 注意到 4 不一定要在 3 之后发生，因为 4 不依赖于 3 的任何输出或副作用。也就是说，4 不依赖于 3 的执行顺序。因此，CPU 决定为了*某些原因*重新排序它们，以使程序运行得更快。于是，CPU 首先执行 4，设置 `Y = true`，即使 3 还没有执行。然后，`t2` 被操作系统挂起，`t1` 执行了几条指令，或者 `t1` 在另一个核心上执行。在 `t1` 中，编译器确实必须首先执行 1，然后执行 2，因为 2 依赖于 1 中读取的值。因此，`t1` 从 `Y`（由 4 写入）读取 `true` 并将其写回 `X`。最后，`t2` 执行 3，读取 `X` 并获得 `true`，正如 2 所写的那样。

```````````` The relaxed memory ordering allows this execution because it imposes no additional constraints on concurrent execution. That is, under relaxed memory ordering, the compiler must ensure only that execution dependencies on any given thread are respected (just as if atomics weren’t involved); it need not make any promises about the interleaving of concurrent operations. Reordering 3 and 4 is permitted for a single-threaded execution, so it is permitted under relaxed ordering as well.    In some cases, this kind of reordering is fine. For example, if you have a counter that just keeps track of metrics, it doesn’t really matter when exactly it executes relative to other instructions, and `Ordering::Relaxed` is fine. In other cases, this could be disastrous: say, if your program uses `r2` to figure out if security protections have already been set up, and thus ends up erroneously believing that they already have been.    You don’t generally notice this reordering when writing code that doesn’t make fancy use of atomics—the CPU has to promise that there is no observable difference between the code as written and what each thread actually executes, so everything seems like it runs in order just as you wrote it. This is referred to as respecting program order or evaluation order; the terms are synonyms.    #### Acquire/Release Ordering    At the next step up in the memory ordering hierarchy, we have `Ordering::Acquire`, `Ordering::Release`, and `Ordering::AcqRel` (acquire plus release). At a high level, these establish an execution dependency between a store in one thread and a load in another and then restrict how operations can be reordered with respect to that load and store. Crucially, these dependencies not only establish a relationship between a store and a load of a single value, but also put ordering constraints on *other* loads and stores in the threads involved. This is because every execution must respect the program order; if a load in thread B has a dependency on some store in thread A (the store in A must execute before the load in B), then any read or write in B after that load must also happen after that store in A.    Concretely, these memory orderings place the following restrictions on execution:    1.  Loads and stores cannot be moved forward past a store with `Ordering::Release`. 2.  Loads and stores cannot be moved back before a load with `Ordering::Acquire`. 3.  An `Ordering::Acquire` load of a variable must see all stores that happened before an `Ordering::Release` store that stored what the load loaded.    To see how these memory orderings change things, Listing 10-3 shows Listing 10-2 again but with the memory ordering swapped out for `Acquire` and `Release`.    ``` static X: AtomicBool = AtomicBool::new(false); static Y: AtomicBool = AtomicBool::new(false);  let t1 = spawn(|| {     let r1 = Y.load(Ordering::Acquire);     X.store(r1, Ordering::Release); }); let t2 = spawn(|| {   1 let r2 = X.load(Ordering::Acquire);   2 Y.store(true, Ordering::Release) }); ```    Listing 10-3: Listing 10-2 with `Acquire/Release` `memory ordering`   ``````````` These additional restrictions mean that it is no longer possible for `t2` to see `r2 = true`. To see why, consider the primary cause of the weird outcome in Listing 10-2: the reordering of 1 and 2. The very first restriction, on stores with `Ordering::Release`, dictates that we cannot move 1 below 2, so we’re all good!    But these rules are useful beyond this simple example. For example, imagine that you implement a mutual exclusion lock. You want to make sure that any loads and stores a thread runs while it holds the lock are executed only while it’s actually holding the lock, and visible to any thread that takes the lock later. This is exactly what `Release` and `Acquire` enable you to do. By performing a `Release` store to release the lock and an `Acquire` load to acquire the lock, you can guarantee that the loads and stores in the critical section are never moved to before the lock was actually acquired or to after the lock was released!   `````````` #### Sequentially Consistent Ordering    Sequentially consistent ordering (`Ordering::SeqCst`) is the strongest memory ordering we have access to. Its exact guarantees are somewhat hard to nail down, but very broadly, it requires not only that each thread sees results consistent with `Acquire/Release```` , but also that all threads see the *same* ordering as one another. This is best seen by way of contrast with the behavior of `Acquire` and `Release`. Specifically, `Acquire/Release` `` ordering does *not* guarantee that if two threads A and B atomically load values written by two other threads X and Y, A and B will see a consistent pattern of when X wrote relative to Y. That’s fairly abstract, so consider the example in Listing 10-4, which shows a case where `Acquire/Release` `ordering can produce unexpected results. Afterwards, we’ll see how sequentially consistent ordering avoids that particular unexpected outcome.` `` ```   ````````` ```````` ``````` ``` static X: AtomicBool = AtomicBool::new(false); static Y: AtomicBool = AtomicBool::new(false); static Z: AtomicI32 = AtomicI32::new(0);  let t1 = spawn(|| {     X.store(true, Ordering::Release); }); let t2 = spawn(|| {     Y.store(true, Ordering::Release); }); let t3 = spawn(|| {     while (!X.load(Ordering::Acquire)) {}   1 if (Y.load(Ordering::Acquire)) {         Z.fetch_add(1, Ordering::Relaxed); } }); let t4 = spawn(|| {     while (!Y.load(Ordering::Acquire)) {}   2 if (X.load(Ordering::Acquire)) {         Z.fetch_add(1, Ordering::Relaxed); } }); ```    Listing 10-4: Weird results with `Acquire/Release` `ordering`   `````` The two threads `t1` and `t2` set `X` and `Y` to `true`, respectively. Thread `t3` waits for `X` to be `true`; once `X` is `true`, it checks if `Y` is `true` and, if so, adds 1 to `Z`. Thread `t4` instead waits for `Y` to become `true`, and then checks if `X` is `true` and, if so, adds 1 to `Z`. At this point the question is: what are the possible values for `Z` after all the threads terminate? Before I show you the answer, try to work your way through it given the definitions of `Release` and `Acquire` ordering in the previous section.    First, let’s recap the conditions under which `Z` is incremented. Thread `t3` increments `Z` if it sees that `Y` is `true` after it observes that `X` is `true`, which can happen only if `t2` runs before `t3` evaluates the load at 1. Conversely, thread `t4` increments `Z` if it sees that `X` is `true` after it observes that `Y` is `true`, so only if `t1` runs before `t4` evaluates the load at 2. To simplify the explanation, let’s assume for now that each thread runs to completion once it runs.    Logically, then, `Z` can be incremented twice if the threads run in the order 1, 2, 3, 4—both `X` and `Y` are set to `true`, and then `t3` and `t4` run to find that their conditions for incrementing `Z` are met. Similarly, `Z` can trivially be incremented just once if the threads run in the order 1, 3, 2, 4\. This satisfies `t4`’s condition for incrementing `Z`, but not `t3`’s. Getting `Z` to be `0`, however, *seems* impossible: if we want to prevent `t3` from incrementing `Z`, `t2` has to run after `t3`. Since `t3` runs only after `t1`, that implies that `t2` runs after `t1`. However, `t4` won’t run until after `t2` has run, so `t1` must have run and set `X` to `true` by the time `t4` runs, and so `t4` will increment `Z`.    Our inability to get `Z` to be `0` stems mostly from our human inclination for linear explanations; this happened, then this happened, then this happened. Computers aren’t limited in the same way and have no need to box all events into a single global order. There’s nothing in the rules for `Release` and `Acquire` that says that `t3` must observe the same execution order for `t1` and `t2` as `t4` observes. As far as the computer is concerned, it’s fine to let `t3` observe `t1` as having executed first, while having `t4` observe `t2` as having executed first. With that in mind, an execution in which `t3` observes that `Y` is `false` after it observes that `X` is `true` (implying that `t2` runs after `t1`), while in the same execution `t4` observes that `X` is `false` after it observes that `Y` is `true` (implying that `t2` runs before `t1`), is completely reasonable, even if that seems outrageous to us mere humans.    As we discussed earlier, `Acquire/Release` ``requires only that an `Ordering::Acquire` load of a variable must see all stores that happened before an `Ordering::Release` store that stored what the load loaded. In the ordering just discussed, the computer *did* uphold that property: `t3` sees `X == true`, and indeed sees all stores by `t1` prior to it setting `X = true`—there are none. It also sees `Y == false`, which was stored by the main thread at program startup, so there aren’t any relevant stores to be concerned with. Similarly, `t4` sees `Y = true` and also sees all stores by `t2` prior to setting `Y = true`—again, there are none. It also sees `X == false`, which was stored by the main thread and has no preceding store. No rules are broken, yet it just seems wrong somehow.``   ````` Our intuitive expectation was that we could put the threads in some global order to make sense of what every thread saw and did, but that was not the case for `Acquire/Release` `ordering in this example. To achieve something closer to that intuitive expectation, we need sequential consistency. Sequential consistency requires all the threads taking part in an atomic operation to coordinate to ensure that what each thread observes corresponds to (or at least appears to correspond to) *some* single, common execution order. This makes it easier to reason about but also makes it costly.`   ````Atomic loads and stores marked with `Ordering::SeqCst` instruct the compiler to take any extra precautions (such as using special CPU instructions) needed to guarantee sequential consistency for those loads and stores. The exact formalism around this is fairly convoluted, but sequential consistency essentially ensures that if you looked at all the related `SeqCst` operations from across all your threads, you could put the thread executions in *some* order so that the values that were loaded and stored would all match up.    If we replaced all the memory ordering arguments in Listing 10-4 with `SeqCst`, `Z` could not possibly be `0` after all the threads have exited, just as we originally expected. Under sequential consistency, it must be possible to say either that `t1` definitely ran before `t2` or that `t2` definitely ran before `t1`, so the execution where `t3` and `t4` see different orders is not allowed, and thus `Z` cannot be `0`.    ### Compare and Exchange    In addition to `load` and `store`, all of Rust’s atomic types provide a method called `compare_exchange`. This method is used to atomically *and conditionally* replace a value. You provide `compare_exchange` with the last value you observed for an atomic variable and the new value you want to replace the original value with, and it will replace the value only if it is still the same as it was when you last observed it. To see why this is important, take a look at the (broken) implementation of a mutual exclusion lock in Listing 10-5. This implementation keeps track of whether the lock is held in the static atomic variable `LOCK`. We use the Boolean value `true` to represent that the lock is held. To acquire the lock, a thread waits for `LOCK` to be `false`, then sets it to `true` again; it then enters its critical section and sets `LOCK` to `false` to release the lock when its work (`f`) is done.    ``` static LOCK: AtomicBool = AtomicBool::new(false);  fn mutex(f: impl FnOnce()) {     // Wait for the lock to become free (false).     while LOCK.load(Ordering::Acquire)       { /* .. TODO: avoid spinning .. */ }     // Store the fact that we hold the lock.     LOCK.store(true, Ordering::Release);     // Call f while holding the lock.     f();     // Release the lock.     LOCK.store(false, Ordering::Release); } ```    Listing 10-5: An incorrect implementation of a mutual exclusion lock    This mostly works, but it has a terrible flaw—two threads might both see `LOCK == false` at the same time and both leave the `while` loop. Then they both set `LOCK` to `true` and both enter the critical section, which is exactly what the `mutex` function was supposed to prevent!    The issue in Listing 10-5 is that there is a gap between when we load the current value of the atomic variable and when we subsequently update it, during which another thread might get to run and read or touch its value. It is exactly this problem that `compare_exchange` solves—it swaps out the value behind the atomic variable *only* if its value still matches the previous read, and otherwise notifies you that the value has changed. Listing 10-6 shows the corrected implementation using `compare_exchange`.    ``` static LOCK: AtomicBool = AtomicBool::new(false);  fn mutex(f: impl FnOnce()) {     // Wait for the lock to become free (false).     loop {       let take = LOCK.compare_exchange(           false,           true,           Ordering::AcqRel,           Ordering::Relaxed       );       match take {         Ok(false) => break,         Ok(true) | Err(false) => unreachable!(),  Err(true) => { /* .. TODO: avoid spinning .. */ }       }     }     // Call f while holding the lock.     f();     // Release the lock.     LOCK.store(false, Ordering::Release); } ```    Listing 10-6: A corrected implementation of a mutual exclusion lock    This time around, we use `compare_exchange` in the loop, and it takes care of both checking that the lock is currently not held and storing `true` to take the lock as appropriate. This happens through the first and second arguments to `compare_exchange`, respectively: in this case, `false` and then `true`. You can read the invocation as “Store `true` only if the current value is `false`.” The `compare_exchange` method returns a `Result` that indicates either that the value was successfully updated (`Ok`) or that it could not be updated (`Err`). In either case, it also returns the current value. This isn’t too useful with an `AtomicBool` since we know what the value must be if the operation failed, but for something like an `AtomicI32`, the updated current value will let you quickly recompute what to store and then try again without having to do another load.    Unlike simple loads and stores, `compare_exchange` takes *two* `Ordering` arguments. The first is the “success ordering,” and it dictates what memory ordering should be used for the load and store that the `compare_exchange` represents in the case that the value was successfully updated. The second is the “failure ordering,” and it dictates the memory ordering for the load if the loaded value does not match the expected current value. These two orderings are kept separate so that the developer can give the CPU leeway to improve execution performance by reordering loads and stores on failure when appropriate, but still get the correct ordering on success. In this case, it’s okay to reorder loads and stores across failed iterations of the lock acquisition loop, but it’s *not* okay to reorder loads and stores inside the critical section in such a way that they end up outside of it.    Even though its interface is simple, `compare_exchange` is a very powerful synchronization primitive—so much so that it’s been theoretically proven that you can build all other distributed consensus primitives using only `compare_exchange`! For that reason, it is the workhorse of many, if not most, synchronization constructs when you really dig into the implementation details.    Be aware, though, that a `compare_exchange` requires that a single CPU has exclusive access to the underlying value, and it is therefore a form of mutual exclusion at the hardware level. This in turn means that `compare_exchange` can quickly become a scalability bottleneck: only one CPU can make progress at a time, so there’s a portion of your code that will not scale with the number of cores. In fact, it’s probably worse than that—the CPUs have to coordinate to ensure that only one CPU succeeds at a `compare_exchange` for a variable at a time (take a look at the MESI protocol if you’re curious about how that works), and that coordination grows quadratically more costly the more CPUs are involved!    ### The Fetch Methods    Fetch methods (`fetch_add`, `fetch_sub`, `fetch_and`, and the like) are designed to allow more efficient execution of atomic operations that commute—that is, operations that have meaningful semantics regardless of the order they execute in. The motivation for this is that the `compare_exchange` method is powerful, but also costly—if two threads both want to update a single atomic variable, one will succeed, while the other will fail and have to retry. If many threads are involved, they all have to mediate sequential access to the underlying value, and there will be plenty of spinning while threads retry on failure.    For simple operations that commute, rather than fail and retry just because another thread modified the value, we can tell the CPU what operation to perform on the atomic variable. It’ll then perform that operation on whatever the current value happens to be when the CPU eventually gets exclusive access. Think of an `AtomicUsize` that counts the number of operations a pool of threads has completed. If two threads both complete a job at the same time, it doesn’t matter which one updates the counter first as long as both their increments are counted.    The fetch methods implement these kinds of commutative operations. They perform a read *and* a store operation in a single step and guarantee that the store operation was performed on the atomic variable when it held exactly the value returned by the method. As an example, `AtomicUsize::fetch_add(1, Ordering::Relaxed)` never fails—it always adds 1 to the current value of the `AtomicUsize`, no matter what it is, and returns the value of the `AtomicUsize` precisely when this thread’s 1 was added.    The fetch methods tend to be more efficient than `compare_exchange` because they don’t require threads to fail and retry when multiple threads contend for access to a variable. Some hardware architectures even have specialized fetch method implementations that scale much better as the number of involved CPUs grows. Nevertheless, if enough threads try to operate on the same atomic variable, those operations will begin to slow down and exhibit sublinear scaling due to the coordination required. In general, the best way to significantly improve the performance of a concurrent algorithm is to split contended variables into more atomic variables that are each less contended, rather than switching from `compare_exchange` to a fetch method.    ## Sane Concurrency    Writing correct and performant concurrent code is harder than writing sequential code; you have to consider not only possible execution interleavings but also how your code interacts with the compiler, the CPU, and the memory subsystem. With such a wide array of footguns at your disposal, it’s easy to want to throw your hands in the air and just give up on concurrency altogether. In this section we’ll explore some techniques and tools that can help ensure that you write correct concurrent code without (as much) fear.    ### Start Simple    It is a fact of life that simple, straightforward, easy-to-follow code is more likely to be correct. This principle also applies to concurrent code—always start with the simplest concurrent design you can think of, then measure, and only if measurement reveals a performance problem should you optimize your algorithm.    To follow this tip in practice, start out with concurrency patterns that do not require intricate use of atomics or lots of fine-grained locks. Begin with multiple threads that run sequential code and communicate over channels, or that cooperate through locks, and then benchmark the resulting performance with the workload you care about. You’re much less likely to make mistakes this way than by implementing fancy lockless algorithms or by splitting your locks into a thousand pieces to avoid false sharing. For many use cases, these designs are plenty fast enough; it turns out a lot of time and effort has gone into making channels and locks perform well! And if the simple approach is fast enough for your use case, why introduce more complex and error-prone code?    If your benchmarks indicate a performance problem, then figure out exactly which part of your system scales poorly. Focus on fixing that bottleneck in isolation where you can, and try to do so with small adjustments where possible. Maybe it’s enough to split a lock in two rather than move to a concurrent hash table, or to introduce another thread and a channel rather than implement a lock-free work stealing queue. If so, do that.    Even when you do have to work directly with atomics and the like, keep things simple until there’s a proven need to optimize—use `Ordering::SeqCst` and `compare_exchange` at first, and then iterate if you find concrete evidence that those are becoming bottlenecks that must be taken care of.    ### Write Stress Tests    As the author, you have a lot of insight into where bugs in your code may hide, without necessarily knowing what those bugs are (yet, anyway). Writing stress tests is a good way to shake out some of the hidden bugs. Stress tests don’t necessarily perform a complex sequence of steps but instead have lots of threads doing relatively simple operations in parallel.    For example, if you were writing a concurrent hash map, one stress test might be to have *N* threads insert or update keys and *M* threads read keys in such a way that those *M*+*N* threads are likely to often choose the same keys. Such a test doesn’t test for a particular outcome or value but instead tries to trigger many possible interleavings of operations in the hopes that buggy interleavings might reveal themselves.    Stress tests resemble fuzz tests in many ways; whereas fuzzing generates many random inputs to a given function, the stress test instead generates many random thread and memory access schedules. Just like fuzzers, stress tests are therefore only as good as the assertions in your code; they can’t tell you about a bug that doesn’t manifest in some easy-to-spot way like an assertion failure or some other kind of panic. For that reason, it’s a good idea to litter your low-level concurrency code with assertions, or `debug_assert_*` if you’re worried about runtime cost in particularly hot loops.    ### Use Concurrency Testing Tools    The primary challenge in writing concurrent code is to handle all the possible ways the execution of different threads can interleave. As we saw in the `Ordering::SeqCst` example in Listing 10-4, it’s not just the thread scheduling that matters, but also which memory values are possible for a given thread to observe at any given point in time. Writing tests that execute every possible legal execution is not only tedious but also difficult—you need very low-level control over which threads execute when and what values their reads return, which the operating system likely doesn’t provide.    #### Model Checking with Loom    Luckily, a tool already exists that can simplify this execution exploration for you in the form of the `loom` crate. Given the relative release cycles of this book and that of a Rust crate, I won’t give any examples of how to use Loom here, as they’d likely be out of date by the time you read this book, but I will give an overview of what it does.    Loom expects you to write dedicated test cases in the form of closures that you pass into a Loom model. The model keeps track of all cross-thread interactions and tries to intelligently explore all possible iterations of those interactions by executing the test case closure multiple times. To detect and control thread interactions, Loom provides replacement types for all the types in the standard library that allow threads to coordinate with one another; that includes most types under `std::sync` and `std::thread` as well as `UnsafeCell` and a few others. Loom expects your application to use those replacement types whenever you run the Loom tests. The replacement types tie into the Loom executor and perform a dual function: they act as rescheduling points so that Loom can choose which operation to run next after each possible thread interaction point, and they inform Loom of new possible interleavings to consider. Essentially, Loom builds up a tree of all the possible future executions for each point at which multiple execution interleavings are possible and then tries to execute all of them, one after the other.    Loom attempts to fully explore all possible executions of the test cases you provide it with, which means it can find bugs that occur only in extremely rare executions that stress testing would not find in a hundred years. While that’s great for smaller test cases, it’s generally not feasible to apply that kind of rigorous testing to larger test cases that test more involved sequences of operations or require many threads to run at once. Loom would simply take too long to get decent coverage of the code. In practice, you may therefore want to tell Loom to consider only a subset of the possible executions, which Loom’s documentation has more details on.    Like with stress tests, Loom can catch only bugs that manifest as panics, so that’s yet another reason to spend some time placing strategic assertions in your concurrent code! In many cases, it may even be worthwhile to add additional state tracking and bookkeeping instructions to your concurrent code to give you better assertions.    #### Runtime Checking with ThreadSanitizer    For larger test cases, your best bet is to run the test through a couple of iterations under Google’s excellent `ThreadSanitizer`, also known as TSan. TSan automatically augments your code by placing extra bookkeeping instructions prior to every memory access. Then, as your code runs, those bookkeeping instructions update and check a special state machine that flags any concurrent memory operations that indicate a problematic race condition. For example, if thread B writes to some atomic value X, but has not synchronized (lots of hand waving here) with the thread that wrote the previous value of X that indicates a write/write race, which is nearly always a bug.    Since TSan only observes your code running and does not execute it over and over again like Loom, it generally only adds a constant-factor overhead to the runtime of your program. While that factor can be significant (5–15 times at the time of writing), it’s still small enough that you can execute even most complex test cases in a reasonable amount of time.    At the time of writing, to use TSan you need to use a nightly version of the Rust compiler and pass in the `-Zsanitizer=thread` command-line argument (or set it in `RUSTFLAGS`), though hopefully in time this will be a standard supported option. Other sanitizers are also available that check things like out-of-bounds memory accesses, use-after-free, memory leaks, and reads of uninitialized memory, and you may want to run your concurrent test suite through those too!    ## Summary    In this chapter, we first covered common correctness and performance pitfalls in concurrent Rust, and some of the high-level concurrency patterns that successful concurrent applications tend to use to work around them. We also explored how asynchronous Rust enables concurrency without parallelism, and how to explicitly introduce parallelism in asynchronous Rust code. We then dove deeper into Rust’s many different lower-level concurrency primitives, including how they work, how they differ, and what they’re all for. Finally, we explored techniques for writing better concurrent code and looked at tools like Loom and TSan that can help you vet that code. In the next chapter we’ll continue our journey through the lower levels of Rust by digging into foreign function interfaces, which allow Rust code to link directly against code written in other languages.```` ````` `````` ``````` ```````` ````````` `````````` ``````````` ````````````
