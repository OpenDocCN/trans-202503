# 第九章：不安全代码

![](img/chapterart.png)

仅仅提到不安全代码，常常会引发 Rust 社区以及许多旁观者的强烈反应。有些人认为这“没什么大不了”，而另一些人则谴责它为“Rust 所有承诺的谎言”。在本章中，我希望揭开一些谜团，解释什么是 `unsafe`，什么不是，以及如何安全地使用它。在写这本书时，也可能是你阅读时，Rust 对不安全代码的具体要求仍在确定中，即使它们都被敲定下来，完整的描述也超出了本书的范围。相反，我会尽力为你提供构建模块、直觉和工具，以帮助你顺利应对大多数不安全代码。

本章的主要收获应该是：不安全代码是 Rust 提供给开发者的机制，目的是利用那些编译器无法检查的不可变性，无论其原因是什么。我们将讨论 `unsafe` 如何实现这一点，这些不可变性可能是什么，以及我们因此可以做些什么。

关键是，不安全代码不是绕过 Rust 各种规则（如借用检查）的方式，而是通过超越编译器的推理来执行这些规则。当你编写不安全代码时，责任在你，确保生成的代码是安全的。从某种意义上说，`unsafe` 作为关键字在允许通过 `unsafe {}` 执行不安全操作时具有误导性；它并不是说包含的代码 *是* 不安全的，而是说在这个特定的上下文中，代码被允许执行本应不安全的操作，因为这些操作 *是* 安全的。

本章的其余部分分为四个部分。我们将首先简要讨论关键字的使用方式，然后探索 `unsafe` 允许你做的事情。接下来，我们将看看你在编写安全的不安全代码时必须遵循的规则。最后，我会给你一些关于如何安全地编写不安全代码的建议。

## 不安全关键字

在讨论 `unsafe` 授予你的能力之前，我们需要先谈谈它的两种不同含义。`unsafe` 关键字在 Rust 中有双重作用：它将特定函数标记为不安全调用 *并且* 它使你能够在特定代码块中调用不安全的功能。例如，列表 9-1 中的方法被标记为不安全，尽管它不包含任何不安全代码。在这里，`unsafe` 关键字作为警告，提醒调用者，在调用 `decr` 的代码中，必须手动检查额外的保证。

```
impl<T> SomeType<T> {
    pub unsafe fn decr(&self) {
        self.some_usize -= 1;
    }
}
```

列表 9-1：仅包含安全代码的不安全方法

列表 9-2 展示了第二种用法。在这里，方法本身没有标记为不安全，尽管它包含不安全的代码。

```
impl<T> SomeType<T> {
    pub fn as_ref(&self) -> &T {
        unsafe { &*self.ptr }
    }
}
```

列表 9-2：包含不安全代码的安全方法

这两个示例在`unsafe`的使用上有所不同，因为它们代表了不同的契约。`decr`要求调用者在调用该方法时小心，而`as_ref`则假设调用者在调用其他不安全方法（如`decr`）时*已经*小心。为了理解原因，假设`SomeType`实际上是一个引用计数类型，如`Rc`。即使`decr`只是递减一个数字，这个递减可能会通过安全方法`as_ref`触发未定义行为。如果你调用`decr`，然后丢弃某个`T`的倒数第二个`Rc`，引用计数降为零，`T`将被丢弃——但是程序可能仍然会在最后一个`Rc`上调用`as_ref`，最终导致悬挂引用。

相反，只要没有方法通过安全代码破坏`Rc`的引用计数，就可以像`as_ref`中的代码那样安全地解引用`Rc`中的指针——`&self`的存在证明了指针仍然有效。我们可以利用这一点为调用者提供一个安全的 API，进行一个本来不安全的操作，这是如何负责任地使用`unsafe`的核心内容。

出于历史原因，今天每个`unsafe fn`在 Rust 中都包含一个隐式的不安全块。也就是说，如果你声明一个`unsafe fn`，你可以在这个`fn`中调用任何不安全方法或原始操作。然而，这个决定现在被认为是一个错误，并且正在通过已经接受并实施的 RFC 2585 来撤销。该 RFC 警告在没有显式`unsafe`块的情况下执行不安全操作的`unsafe fn`。这个 lint 也很可能在未来的 Rust 版本中成为一个硬错误。其理念是减少“脚枪半径”——如果每个`unsafe fn`都是一个巨大的不安全块，你可能会不小心执行不安全操作而没有意识到！例如，在示例 9-1 中的`decr`，根据当前规则，你也可以在没有任何`unsafe`注解的情况下加入`*std::ptr::null()`。

`unsafe`作为标记与不安全块作为启用不安全操作的机制之间的区别非常重要，因为你必须以不同的方式思考它们。一个`unsafe fn`告诉调用者，在调用该`fn`时必须小心，并且他们必须确保该函数的文档化安全不变量成立。

与此同时，一个`unsafe`块意味着编写该块的开发者已经仔细检查过其中执行的任何不安全操作的安全不变量。如果你想要一个大致的现实世界类比，`unsafe fn`就像是一个未签署的合同，它要求调用代码的作者“郑重承诺 X、Y 和 Z。”与此同时，`unsafe {}`则是调用代码的作者在签署块中所有不安全合同时的同意。记住这一点，在我们继续阅读本章的内容时。

## 强大之力

所以，一旦你签署了`unsafe {}`的“不安全合同”，你可以做什么呢？老实说，并没有太多。或者说，它并没有启用太多新功能。在`unsafe`块内部，你被允许解引用原始指针并调用`unsafe fn`。

就这样。技术上来说，你可以做一些其他事情，比如访问可变的和外部静态变量，访问联合体的字段，但这些并不会对讨论产生太大影响。老实说，这些就够了。总的来说，这些能力使你能够制造各种混乱，比如通过`mem::transmute`将类型互相转换，解引用指向不知道在哪里的原始指针，将`&'a`转换为`&'static`，或者让类型在跨线程边界共享，即使它们本身不是线程安全的。

在这一节中，我们不会过多担心这些能力可能会出错的情况。我们会把这些留到后面那部分沉闷、负责任的大人部分去讨论。相反，我们将看看这些闪亮的新玩具，以及我们能用它们做些什么。

### 处理原始指针

使用`unsafe`的一个最基本的原因是处理 Rust 的原始指针类型：`*const T`和`*mut T`。你可以把它们看作是与`&T`和`&mut T`大致相当，只是它们没有生命周期，且不像`&`引用那样受相同的有效性规则约束，我们将在本章稍后讨论这些规则。这些类型通常被称为*指针*和*原始指针*，主要是因为许多开发者本能地将引用称为指针，而将它们称为原始指针可以让这个区别更加清晰。

由于适用于`*`的规则比`&`少，你可以在`unsafe`块外将引用转换为指针。只有当你想反过来做，从`*`转换为`&`时，才需要使用`unsafe`。通常，你会将指针转回引用，以便对指向的数据执行有用的操作，例如读取或修改其值。因此，指针上常用的操作是`unsafe { &*ptr }`（或`&mut *`）。这里的`*`可能看起来有些奇怪，因为代码只是构造一个引用，而不是解引用指针，但如果你看一下类型就能明白；如果你有一个`*mut T`，并且想要得到一个`&mut T`，那么`&mut ptr`只会给你一个`&mut *mut T`。你需要`*`来表明你想要`ptr`所指向内容的可变引用。

#### 无法表示的生命周期

由于原始指针没有生命周期，它们可以在 Rust 的生命周期系统无法静态表示所指向值的存活性的情况下使用，例如我们在第八章讨论的自引用结构体中的自指针。指向`self`的指针只要`self`还存在（并且没有移动，`Pin`正是为了这个目的），就一直有效，但这不是你通常可以命名的生命周期。虽然整个自引用类型可能是`'static`，但自指针却不是——如果它是静态的，即使你把指针交给别人，他们也能一直使用它，即使`self`已经不存在了！以列表 9-3 中的类型为例；在这里，我们尝试将构成一个值的原始字节与它的存储表示一起存储。

```
struct Person<'a> {
    name: &'a str,
    age: usize,
}
struct Parsed {
    bytes: [u8; 1024],
    parsed: Person<'???>,
}
```

列表 9-3：尝试命名自引用引用的生命周期，但失败了

`Person`中的引用想要引用存储在`Parsed`中的`bytes`数据，但我们无法为这个引用指定生命周期。它既不是`'static`，也不是类似`'self`（不存在）的东西，因为如果`Parsed`被移动，该引用将不再有效。

由于指针没有生命周期，它们避免了这个问题，因为你不需要能够命名生命周期。相反，你只需要确保在使用指针时，它仍然有效，这就是你在编写`unsafe { &*ptr }`时所签署的内容。在列表 9-3 中的示例中，`Person`将存储一个`*const str`，然后在合适的时候不安全地将其转换为`&str`，当它能保证指针仍然有效时。

类似的问题出现在像`Arc`这样的类型中，它包含一个指向某个值的指针，该值在某段时间内是共享的，但这段时间只有在运行时才能知道，当最后一个`Arc`被释放时。这个指针有点像是`'static`，但实际上不是——就像在自引用的情况中，当最后一个`Arc`引用消失时，指针就不再有效，因此生命周期更像是`'self`。在`Arc`的兄弟类型`Weak`中，生命周期也是“当最后一个`Arc`消失时”，但由于`Weak`不是`Arc`，所以生命周期甚至不与`self`相关。因此，`Arc`和`Weak`内部都使用原始指针。

#### 指针运算

使用原始指针，你可以进行任意的指针运算，就像在 C 语言中一样，使用`.offset()`、`.add()`和`.sub()`将指针移动到同一分配中的任何字节。这通常用于高度优化空间的数据结构，比如哈希表，在这些结构中，为每个元素存储一个额外的指针会增加太多开销，而使用切片又不可行。这些是相对小众的用例，我们在本书中不会深入讨论，但如果你想了解更多，可以阅读`hashbrown::RawTable`的代码（[`github.com/rust-lang/hashbrown/`](https://github.com/rust-lang/hashbrown/)）。

即使你不打算将指针转换为引用，指针算术方法也是不安全的。造成这种情况的原因有几个，但最主要的是，指针指向原始分配末尾之外的地方是非法的。这样做会触发未定义行为，编译器可以决定吞掉你的代码，并将其替换为只有编译器能理解的任意胡话。如果你确实使用这些方法，请仔细阅读文档！

#### 到指针再回

当你需要使用指针时，通常是因为你有某个普通的 Rust 类型，比如引用、切片或字符串，然后你需要暂时转到指针的世界，然后再返回到原始的普通类型。因此，一些关键的标准库类型为你提供了一种方法，将它们转换为原始组成部分，例如切片的指针和长度，并通过这些相同的部分将其转换回完整的类型。例如，你可以通过 `as_ptr` 获取切片的数据指针，通过 `[]::len` 获取它的长度。然后，你可以通过将这些相同的值传递给 `std::slice::from_raw_parts` 来重建切片。`Vec`、`Arc` 和 `String` 也有类似的方法，它们返回指向底层分配的原始指针，而 `Box` 则有 `Box::into_raw` 和 `Box::from_raw`，它们执行相同的操作。

#### 对类型的自由操作

有时，你有一个类型 `T`，并希望将其视为另一个类型 `U`。无论是因为你需要进行极速的零拷贝解析，还是因为你需要调整某些生命周期，Rust 提供了一些（非常不安全的）工具来实现这一点。

其中最常用的指针转换是：你可以将 `*const T` 转换为任何其他 `*const U`（`mut` 同理），而且你甚至不需要使用 `unsafe`。不安全性只在你之后尝试将转换后的指针作为引用使用时出现，因为你必须断言该原始指针实际上可以作为它所指向类型的引用来使用。

这种指针类型转换在处理外部函数接口（FFI）时特别有用——你可以将任何 Rust 指针转换为 `*const std::ffi::c_void` 或 `*mut std::ffi::c_void`，然后将其传递给一个期望空指针的 C 函数。类似地，如果你从 C 函数那里获得一个你之前传入的空指针，你可以轻松地将其转换回原来的类型。

指针转换在你希望将一串字节解释为普通数据时也非常有用——例如整数、布尔值、字符、数组，或这些类型的 `#[repr(C)]` 结构体——或者直接将这些类型以字节流的形式写出，而不进行序列化。如果你想尝试这样做，有很多安全不变量需要记住，但我们稍后再讨论这些。

### 调用不安全函数

可以说，`unsafe` 最常用的特性是它允许你调用不安全函数。在栈的更深处，这些函数大多数是不安全的，因为它们在某些基本层面上操作原始指针，但在栈的更高层，你通常通过函数调用与不安全性进行交互。

调用不安全函数的结果没有限制，因为它完全取决于你交互的库。但是，*一般来说*，不安全函数可以分为三类：与非 Rust 接口交互的、不进行安全检查的以及具有自定义不变式的。

#### 外部函数接口

Rust 允许你使用 `extern` 块声明在 Rust 以外的语言中定义的函数和静态变量（我们将在第十一章详细讨论）。当你声明这样的块时，你是在告诉 Rust，这些在其中出现的项将在最终程序二进制文件链接时由某个外部源实现，例如你正在集成的 C 库。由于 `extern` 是 Rust 控制之外的，它们在访问时本质上是不安全的。如果你从 Rust 调用 C 函数，所有的保证都不成立——它可能会覆盖你整个内存内容，并将你精心安排的引用弄乱，变成指向内核某处的随机指针。类似地，`extern` 静态变量可能随时被外部代码修改，甚至可能被填充上与其声明类型完全不符的各种无效字节。不过，在不安全块中，只要你愿意担保外部代码遵守 Rust 的规则，你就可以随心所欲地访问 `extern`。

#### 我放弃安全检查

一些不安全操作可以通过引入额外的运行时检查变得完全安全。例如，访问切片中的项是危险的，因为你可能会尝试访问超出切片长度的项。但是，鉴于这种操作的普遍性，如果切片的索引操作是危险的，那将是非常不幸的。因此，安全的实现包括边界检查（取决于你使用的方法），如果提供的索引超出范围，它会导致程序崩溃或返回一个 `Option`。这样，即使你传入一个超出切片长度的索引，也不会导致未定义行为。另一个例子是在哈希表中，哈希表对你提供的键进行哈希，而不是让你自己提供哈希值；这确保了你永远不会使用错误的哈希值去访问某个键。

然而，在追求极致性能的过程中，一些开发者可能会发现这些安全检查在他们的紧密循环中增加了过多的开销。为了应对对性能要求极高且调用者知道索引在有效范围内的情况，许多数据结构提供了不包含这些安全检查的特定方法版本。这些方法通常在名称中包含 `unchecked` 这个词，以表明它们盲目地信任所提供的参数是安全的，并且不执行那些烦人的、缓慢的安全检查。一些例子包括 `NonNull::new_unchecked`、`slice::get_unchecked`、`NonZero::new_unchecked`、`Arc::get_mut_unchecked` 和 `str::from_utf8_unchecked`。

在实践中，对于不安全方法的安全性和性能权衡，通常是无法值得的。和性能优化一样，先进行测量，再进行优化。

#### 自定义不变量

大多数 `unsafe` 的使用都在一定程度上依赖于自定义不变量。也就是说，它们依赖于 Rust 本身所提供的不变量之外的约定，这些约定是特定于特定应用程序或库的。由于有许多函数属于这一类，因此很难给出一个好的概述。不过，我将举一些可能在实践中遇到并希望使用的带有自定义不变量的 `unsafe` 函数的例子：

**`MaybeUninit::assume_init`**

1.  `MaybeUninit` 类型是 Rust 中少数几种可以存储不合法值的方式之一。你可以将 `MaybeUninit<T>` 看作是一个当前可能不合法使用的 `T`。例如，`MaybeUninit<NonNull>` 允许存储一个空指针，`MaybeUninit<Box>` 允许存储一个悬空堆指针，而 `MaybeUninit<bool>` 允许存储数字 3 的比特模式（通常它只能是 0 或 1）。这在你按位构建一个值或处理最终会变得合法的零值或未初始化内存时非常有用（比如通过调用 `std::io::Read::read` 填充）。`assume_init` 函数断言 `MaybeUninit` 现在包含一个对类型 `T` 有效的值，因此可以作为 `T` 使用。

````` **`ManuallyDrop::drop`**    1.  The `ManuallyDrop` type is a wrapper type around a type `T` that does not drop that `T` when the `ManuallyDrop` is dropped. Or, phrased differently, it decouples the dropping of the outer type (`ManuallyDrop`) from the dropping of the inner type (`T`). It implements safe access to the `T` through `DerefMut<Target = T>` but also provides a `drop` method (separately from the `drop` method of the `Drop` trait) to drop the wrapped `T` *without* dropping the `ManuallyDrop`. That is, the `drop` function takes `&mut self` despite dropping the `T`, and so leaves the `ManuallyDrop` behind. This comes in handy if you have to explicitly drop a value that you cannot move, such as in implementations of the `Drop` trait. Once that value is dropped, it is no longer safe to try to access the `T`, which is why the call to `drop` is unsafe—it asserts that the `T` will never be accessed again.    **`std::ptr::drop_in_place`**    1.  `drop_in_place` lets you call a value’s destructor directly through a pointer to that value. This is unsafe because the pointee will be left behind after the call, so if some code then tries to dereference the pointer, it’ll be in for a bad time! This method is particularly useful when you may want to reuse memory, such as in an arena allocator, and need to drop an old value in place without reclaiming the surrounding memory.    **`Waker::from_raw`**    1.  In Chapter 8 we talked about the `Waker` type and how it is made up of a data pointer and a `RawWaker` that holds a manually implemented vtable. Once a `Waker` has been constructed, the raw function pointers in the vtable, such as `wake` and `drop`, can be called from safe code (through `Waker::wake` and `drop(waker)`, respectively). `Waker::from_raw` is where the asynchronous executor asserts that all the pointers in its vtable are in fact valid function pointers that follow the contract set forth in the documentation of `RawWakerVTable`.    **`std::hint::unreachable_unchecked`**    1.  The `hint` module holds functions that give hints to the compiler about the surrounding code but do not actually produce any machine code. The `unreachable_unchecked` function in particular tells the compiler that it is impossible for the program to reach a section of the code at runtime. This in turn allows the compiler to make optimizations based on that knowledge, such as eliminating conditional branches to that location. Unlike the `unreachable!` macro, which panics if the code does reach the line in question, the effects of an erroneous `unreachable_unchecked` are hard to predict. The compiler optimizations may cause peculiar and hard-to-debug behavior, not to mention that your program will continue running when something it believed to be true was not!    **`std::ptr::{read,write}_{unaligned,volatile}`**    1.  The `ptr` module holds a number of functions that let you work with *odd* pointers—those that do not meet the assumptions that Rust generally makes about pointers. The first of these functions are `read_unaligned` and `write_unaligned`, which let you access pointers that point to a `T` even if that `T` is not stored according to `T`’s alignment (see the section on alignment in Chapter 2). This might happen if the `T` is contained directly in a byte array or is otherwise packed in with other values without proper padding. The second notable pair of functions is `read_volatile` and `write_volatile`, which let you operate on pointers that don’t point to normal memory. Concretely, these functions will always access the given pointer (they won’t be cached in a register, for example, even if you read the same pointer twice in a row), and the compiler won’t reorder the volatile accesses relative to other volatile accesses. Volatile operations come in handy when working with pointers that aren’t backed by normal DRAM memory—we’ll discuss this further in Chapter 11. Ultimately, these methods are unsafe because they dereference the given pointer (and to an owned `T`, at that), so you as the caller need to sign off on all the contracts associated with doing so.    **`std::thread::Builder::spawn_unchecked`**    1.  The normal `thread::spawn` that we know and love requires that the provided closure is `'static`. That bound stems from the fact that the spawned thread might run for an indeterminate amount of time; if we were allowed to use a reference to, say, the caller’s stack, the caller might return well before the spawned thread exits, rendering the reference invalid. Sometimes, however, you know that some non-`'static` value in the caller will outlive the spawned thread. This might happen if you join the thread before dropping the value in question, or if the value is dropped only strictly after you know the spawned thread will no longer use it. That’s where `spawn_unchecked` comes in—it does not have the `'static` bound and thus lets you implement those use cases as long as you’re willing to sign the contract saying that no unsafe accesses will happen as a result. Be careful of panics, though; if the caller panics, it might drop values earlier than you planned and cause undefined behavior in the spawned thread!    Note that all of these methods (and indeed all unsafe methods in the standard library) provide explicit documentation for their safety invariants, as should be the case for any unsafe method.    ### Implementing Unsafe Traits    Unsafe traits aren’t unsafe to *use*, but unsafe to *implement*. This is because unsafe code is allowed to rely on the correctness (defined by the trait’s documentation) of the implementation of unsafe traits. For example, to implement the unsafe trait `Send`, you need to write `unsafe impl Send for ...`. Like unsafe functions, unsafe traits generally have custom invariants that are (or at least should be) specified in the documentation for the trait. Thus, it’s difficult to cover unsafe traits as a group, so here too I’ll give some common examples from the standard library that are worth going over.    #### Send and Sync    The `Send` and `Sync` traits denote that a type is safe to send or share across thread boundaries, respectively. We’ll talk more about these traits in Chapter 10, but for now what you need to know is that they are auto-traits, and so they’ll usually be implemented for most types for you by the compiler. But, as tends to be the case with auto-traits, `Send` and `Sync` will not be implemented if any members of the type in question are not themselves `Send` or `Sync`.    In the context of unsafe code, this problem occurs primarily due to raw pointers, which are neither `Send` nor `Sync`. At first glance, this might seem reasonable: the compiler has no way to know who else may have a raw pointer to the same value or how they may be using it at the moment, so how can the type be safe to send across threads? Now that we’re seasoned unsafe developers though, that argument seems weak—after all, dereferencing a raw pointer is already unsafe, so why should handling the invariants of `Send` and `Sync` be any different?    Strictly speaking, raw pointers could be both `Send` and `Sync`. The problem is that if they were, the types that contain raw pointers would automatically be `Send` and `Sync` themselves, even though their author might not realize that was the case. The developer might then unsafely dereference the raw pointers without ever thinking about what would happen if those types were sent or shared across thread boundaries, and thus inadvertently introduce undefined behavior. Instead, the raw pointer types block these automatic implementations as an additional safeguard to unsafe code to make authors explicitly sign the contract that they have also followed the `Send` and `Sync` invariants.    #### GlobalAlloc    The `GlobalAlloc` trait is how you implement a custom memory allocator in Rust. We won’t talk too much about that topic in this book, but the trait itself is interesting. Listing 9-4 gives the required methods for the `GlobalAlloc` trait.    ``` pub unsafe trait GlobalAlloc {     pub unsafe fn alloc(&self, layout: Layout) -> *mut u8;     pub unsafe fn dealloc(&self, ptr: *mut u8, layout: Layout); } ```    Listing 9-4: The `GlobalAlloc` trait with its required methods    At its core, the trait has one method for allocating a new chunk of memory, `alloc`, and one for deallocating a chunk of memory, `dealloc`. The `Layout` argument describes the type’s size and alignment, as we discussed in Chapter 2. Each of those methods is unsafe and carries a number of safety invariants that its callers must uphold.    `GlobalAlloc` itself is also unsafe because it places restrictions on the implementer of the trait, not the caller of its methods. Only the unsafety of the trait ensures that implementers agree to uphold the invariants that Rust itself assumes of its memory allocator, such as in the standard library’s implementation of `Box`. If the trait was not unsafe, an implementer could safely implement `GlobalAlloc` in a way that produced unaligned pointers or incorrectly sized allocations, which would trigger unsafety in otherwise safe code that assumes that allocations are sane. This would break the rule that safe code should not be able to trigger memory unsafety in other safe code, and thus cause all sorts of mayhem.    #### Surprisingly Not Unpin    The `Unpin` trait is not unsafe, which comes as a surprise to many Rust developers. It may even come as a surprise to you after reading Chapter 8. After all, the trait is supposed to ensure that self-referential types aren’t invalidated if they’re moved after they have established internal pointers (that is, after they’ve been placed in a `Pin`). It seems strange, then, that `Unpin` can be used to safely remove a type from a `Pin`.    There are two main reasons why `Unpin` isn’t an unsafe trait. First, it’s unnecessary. Implementing `Unpin` for a type that you control does not grant you the ability to safely pin or unpin a `!Unpin` type; that still requires unsafety in the form of a call to `Pin::new_unchecked` or `Pin::get_unchecked_mut`. Second, there is already a safe way for you to unpin any type you control: the `Drop` trait! When you implement `Drop` for a type, you’re passed `&mut self`, even if your type was previously stored in a `Pin` and is `!Unpin`, all without any unsafety. That potential for unsafety is covered by the invariants of `Pin::new_unchecked`, which must be upheld to create a `Pin` of such an `!Unpin` type in the first place.    #### When to Make a Trait Unsafe    Few traits in the wild are unsafe, but those that are all follow the same pattern. A trait should be unsafe if safe code that assumes that trait is implemented correctly can exhibit memory unsafety if the trait is *not* implemented correctly.    The `Send` trait is a good example to keep in mind here—safe code can easily spawn a thread and pass a value to that spawned thread, but if `Rc` were ``Send, that sequence of operations could trivially lead to memory unsafety. Consider what would happen if you cloned an `Rc<Box>` and sent it to another thread: the two threads could easily both try to deallocate the `Box` since they do not correctly synchronize access to the `Rc`’s reference count.``   ````The `Unpin` trait is a good counterexample. While it is possible to write unsafe code that triggers memory unsafety if `Unpin` is implemented incorrectly, no entirely safe code can trigger memory unsafety due to an implementation of `Unpin`. It’s not always easy to determine that a trait can be safe (indeed, the `Unpin` trait was unsafe throughout most of the RFC process), but you can always err on the side of making the trait unsafe, and then make it safe later on if you realize that is the case! Just keep in mind that that is a backward incompatible change.    Also keep in mind that just because it feels like an incorrect (or even malicious) implementation of a trait would cause a lot of havoc, that’s not necessarily a good reason to make it unsafe. The `unsafe` marker should first and foremost be used to highlight cases of *memory* unsafety, not just something that can trigger errors in business logic. For example, the `Eq`, `Ord`, `Deref`, and `Hash` traits are all safe, even though there is likely much code out in the world that would go haywire if faced with a malicious implementation of, say, `Hash` that returned a different random hash each time it was called. This extends to unsafe code too—there is almost certainly unsafe code out there that would be memory-unsafe in the presence of such an implementation of `Hash`—but that does not mean `Hash` should be unsafe. The same is true for an implementation of `Deref` that dereferenced to a different (but valid) target each time. Such unsafe code would be relying on a contract of `Hash` or `Deref` that does not actually hold; `Hash` never claimed that it was deterministic, and neither did `Deref`. Or rather, the authors of those implementations never used the `unsafe` keyword to make that claim!    ## Great Responsibility    So far, we’ve looked mainly at the various things that you are allowed to do with unsafe code. But unsafe code is allowed to do those things only if it does so safely. Even though unsafe code can, say, dereference a raw pointer, it must do so only if it knows that pointer is valid as a reference to its pointee at that moment in time, subject to all of Rust’s normal requirements of references. In other words, unsafe code is given access to tools that could be used to do unsafe things, but it must do only safe things using those tools.    That, then, raises the question of what *safe* even means in the first place. When is it safe to dereference a pointer? When is it safe to transmute between two different types? In this section, we’ll explore some of the key invariants to keep in mind when wielding the power of `unsafe`, look at some common gotchas, and get familiar with some of the tools that help you write safer unsafe code.    The exact rules around what it means for Rust code to be safe are still being worked out. At the time of writing, the Unsafe Code Guidelines Working Group is hard at work nailing down all the dos and don’ts, but many questions remain unanswered. Most of the advice in this section is more or less settled, but I’ll make sure to call out any that isn’t. If anything, I’m hoping that this section will teach you to be careful about making assumptions when you write unsafe code, and prompt you to double-check the Rust reference before you declare your code production-ready.    ### What Can Go Wrong?    We can’t really get into the rules unsafe code must abide by without talking about what happens if you violate those rules. Let’s say you do mutably access a value from multiple threads concurrently, construct an unaligned reference, or dereference a dangling pointer—now what?    Unsafe code that is not ultimately safe is referred to as having *undefined behavior*. Undefined behavior generally manifests in one of three ways: not at all, through visible errors, or through invisible corruption. The first is the happy case—you wrote some code that is truly not safe, but the compiler generated sane code that the computer you’re running the code on executes in a sane way. Unfortunately, the happiness here is very brittle. Should a new and slightly smarter version of the compiler come along, or some surrounding code cause the compiler to apply another optimization, the code may no longer do something sane and tip over into one of the worse cases. Even if the same code is compiled by the same compiler, if it runs on a different platform or host, the program might act differently! This is why it is important to avoid undefined behavior even if everything currently seems to work fine. Not to do so is like playing a second round of Russian roulette just because you survived the first.    Visible errors are the easiest undefined behavior to catch. If you dereference a null pointer, for example, your program will (in all likelihood) crash with an error, which you can then debug back to the root cause. That debugging may itself be difficult, but at least you have a notification that something is wrong. Visible errors can also manifest in less severe ways, such as deadlocks, garbled output, or panics that are printed but don’t trigger a program exit, all of which tell you that there is a bug in your code that you have to go fix.    The worst manifestation of undefined behavior is when there is no immediate visible effect, but the program state is invisibly corrupted. Transaction amounts might be slightly off from what they should be, backups might be silently corrupted, or random bits of internal memory could be exposed to external clients. The undefined behavior could cause ongoing corruption, or extremely infrequent outages. Part of the challenge with undefined behavior is that, as the name implies, the behavior of the non-safe unsafe code is not defined—the compiler might eliminate it entirely, dramatically change the semantics of the code, or even miscompile surrounding code. What that does to your program is entirely dependent on what the code in question does. The unpredictable impact of undefined behavior is the reason why *all* undefined behavior should be considered a serious bug, no matter how it *currently* manifests.    ### Validity    Perhaps the most important concept to understand before writing unsafe code is *validity*, which dictates the rules for what values inhabit a given type—or, less formally, the rules for a type’s values. The concept is simpler than it sounds, so let’s dive into some concrete examples.    #### Reference Types    Rust is very strict about what values its reference types can hold. Specifically, references must never dangle, must always be aligned, and must always point to a valid value for their target type. In addition, a shared and an exclusive reference to a given memory location can never exist at the same time, and neither can multiple exclusive references to a location. These rules apply regardless of whether your code uses the references or not—you are not allowed to create a null reference even if you then immediately discard it!    Shared references have the additional constraint that the pointee is not allowed to change during the reference’s lifetime. That is, any value the pointee contains must remain exactly the same over its lifetime. This applies transitively, so if you have an `&` to a type that contains a `*mut T`, you are not allowed to ever mutate the `T` through that `*mut` even though you could write code to do so using `unsafe`. The *only* exception to this rule is a value wrapped by the `UnsafeCell` type. All other types that provide interior mutability, like `Cell`, `RefCell`, and `Mutex`, internally use an `UnsafeCell`.    An interesting result of Rust’s strict rules for references is that for many years, it was impossible to safely take a reference to a field of a packed or partially uninitialized struct that used `repr(Rust)`. Since `repr(Rust)` leaves a type’s layout undefined, the only way to get the address of a field was by writing `&some_struct.field as *const _`. However, if `some_struct` is packed, then `some_struct.field` may not be aligned, and thus creating an `&` to it is illegal! Further, if `some_struct` isn’t fully initialized, then the `some_struct` reference itself cannot exist! In Rust 1.51.0, the `ptr::addr_of!` macro was stabilized, which added a mechanism for directly obtaining a reference to a field without first creating a reference, fixing this particular problem. Internally, it is implemented using something called *raw references* (not to be confused with raw pointers), which directly create pointers to their operands rather than going via a reference. Raw references were introduced in RFC 2582 but haven’t been stabilized themselves yet at the time of writing.    #### Primitive Types    Some of Rust’s primitive types have restrictions on what values they can hold. For example, a `bool` is defined as being 1 byte large but is only allowed to hold the value `0x00` or the value `0x01`, and a `char` is not allowed to hold a surrogate or a value above `char::MAX`. Most of Rust’s primitive types, and indeed most of Rust’s types overall, also cannot be constructed from uninitialized memory. These restrictions may seem arbitrary, but again often stem from the need to enable optimizations that wouldn’t be possible otherwise.    A good illustration of this is the niche optimization, which we discussed briefly when talking about pointer types earlier in this chapter. To recap, the niche optimization tucks away the enum discriminant value in the wrapped type in certain cases. For example, since a reference cannot ever be all zeros, an `Option<&T>` can use all zeros to represent `None`, and thus avoid spending an extra byte (plus padding) to store the discriminator byte. The compiler can optimize Booleans in the same way and potentially take it even further. Consider the type `Option<Option<bool>>>`. Since the compiler knows that the `bool` is either `0x00` or `0x01`, it’s free to use `0x02` to represent `Some(None)` and `0x03` to represent `None`. Very nice and tidy! But if someone were to come along and treat the byte `0x03` as a `bool`, and then place that value in an `Option<Option<bool>>` optimized in this way, bad things would happen.    It bears repeating that it’s not important whether the Rust compiler currently implements this optimization or not. The point is that it is allowed to, and therefore any unsafe code you write must conform to that contract or risk hitting a bug later on should the behavior change.    #### Owned Pointer Types    Types that point to memory they own, like `Box` and `Vec`, are generally subject to the same optimizations as if they held an exclusive reference to the pointed-to memory unless they’re explicitly accessed through a shared reference. Specifically, the compiler assumes that the pointed-to memory is not shared or aliased elsewhere, and makes optimizations based on that assumption. For example, if you extracted the pointer from a `Box` and then constructed two `Box`es from that same pointer and wrapped them in `ManuallyDrop` to prevent a double-free, you’d likely be entering undefined behavior territory. That’s the case even if you only ever access the inner type through shared references. (I say “likely” because this isn’t fully settled in the language reference yet, but a rough consensus has arisen.)    #### Storing Invalid Values    Sometimes you need to store a value that isn’t currently valid for its type. The most common example of this is if you want to allocate a chunk of memory for some type `T` and then read in the bytes from, say, the network. Until all the bytes have been read in, the memory isn’t going to be a valid `T`. Even if you just tried to read the bytes into a slice of `u8`, you would have to zero those `u8`s first, because constructing a `u8` from uninitialized memory is also undefined behavior.    The `MaybeUninit<T>` type is Rust’s mechanism for working with values that aren’t valid. A `MaybeUninit<T>` stores exactly a `T` (it is `#[repr(transparent)]`), but the compiler knows to make no assumptions about the validity of that `T`. It won’t assume that references are non-null, that a `Box<T>` isn’t dangling, or that a `bool` is either 0 or 1\. This means it’s safe to hold a `T` backed by uninitialized memory inside a `MaybeUninit` (as the name implies). `MaybeUninit` is also a very useful tool in other unsafe code where you have to temporarily store a value that may be invalid. Maybe you have to store an aliased `Box<T>` or stash a `char` surrogate for a second—`MaybeUninit` is your friend.    You will generally do only three things with a `MaybeUninit`: create it using the `MaybeUninit::uninit` method, write to its contents using `MaybeUninit::as_mut_ptr`, or take the inner `T` once it is valid again with `MaybeUninit::assume_init`. As its name implies, `uninit` creates a new `MaybeUninit<T>` of the same size as a `T` that initially holds uninitialized memory. The `as_mut_ptr` method gives you a raw pointer to the inner `T` that you can then write to; nothing stops you from reading from it, but reading from any of the uninitialized bits is undefined behavior. And finally, the unsafe `assume_init` method consumes the `MaybeUninit<T>` and returns its contents as a `T` following the assertion that the backing memory now makes up a valid `T`.    Listing 9-5 shows an example of how we might use `MaybeUninit` to safely initialize a byte array without explicitly zeroing it.    ``` fn fill(gen: impl FnMut() -> Option<u8>) {     let mut buf = [MaybeUninit::<u8>::uninit(); 4096];     let mut last = 0;     for (i, g) in std::iter::from_fn(gen).take(4096).enumerate() {         buf[i] = MaybeUninit::new(g);         last = i + 1;     }     // Safety: all the u8s up to last are initialized.     let init: &[u8] = unsafe {        MaybeUninit::slice_assume_init_ref(&buf[..last]) };     // ... do something with init ... } ```    Listing 9-5: Using `MaybeUninit` to safely initialize an array    While we could have declared `buf` as `[0; 4096]` instead, that would require the function to first write out all those zeros to the stack before executing, even if it’s going to overwrite them all again shortly thereafter. Normally that wouldn’t have a noticeable impact on performance, but if this was in a sufficiently hot loop, it might! Here, we instead allow the array to keep whatever values happened to be on the stack when the function was called, and then overwrite only what we end up needing.    ### Panics    An important and often overlooked aspect of ensuring that code using unsafe operations is safe is that the code must also be prepared to handle panics. In particular, as we discussed briefly in Chapter 5, Rust’s default panic handler on most platforms will not crash your program on a panic but will instead *unwind* the current thread. An unwinding panic effectively drops everything in the current scope, returns from the current function, drops everything in the scope that enclosed the function, and so on, all the way down the stack until it hits the first stack frame for the current thread. If you don’t take unwinding into account in your unsafe code, you may be in for trouble. For example, consider the code in Listing 9-6, which tries to efficiently push many values into a `Vec` at once.    ``` impl<T: Default> Vec<T> {     pub fn fill_default(&mut self) {         let fill = self.capacity() - self.len();         if fill == 0 { return; }         let start = self.len();         unsafe {             self.set_len(start + fill);  for i in 0..fill {                 *self.get_unchecked_mut(start + i) = T::default();             }         }     } } ```    Listing 9-6: A seemingly safe method for filling a vector with `Default` values    Consider what happens to this code if a call to `T::default` panics. First, `fill_default` will drop all its local values (which are just integers) and then return. The caller will then do the same. At some point up the stack, we get to the owner of the `Vec`. When the owner drops the vector, we have a problem: the length of the vector now indicates that we own more `T`s than we actually produced due to the call to `set_len`. For example, if the very first call to `T::default` panicked when we aimed to fill eight elements, that means `Vec::drop` will call `drop` on eight `T`s that actually contain uninitialized memory!    The fix in this case is simple: the code must update the length *after* writing all the elements. We wouldn’t have realized there was a problem if we didn’t carefully consider the effect of unwinding panics on the correctness of our unsafe code.    When you’re combing through your code for these kinds of problems, you’ll want to look out for any statements that may panic, and consider whether your code is safe if they do. Alternatively, check whether you can convince yourself that the code in question will never panic. Pay particular attention to anything that calls user-provided code—in those cases, you have no control over the panics and should assume that the user code will panic.    A similar situation arises when you use the `?` operator to return early from a function. If you do this, make sure that your code is still safe if it does not execute the remainder of the code in the function. It’s rarer for `?` to catch you off guard since you opted into it explicitly, but it’s worth keeping an eye out for.    ### Casting    As we discussed in Chapter 2, two different types that are both `#[repr(Rust)]` may be represented differently in memory even if they have fields of the same type and in the same order. This in turn means that it’s not always obvious whether it is safe to cast between two different types. In fact, Rust doesn’t even guarantee that two instances of a single type with generic arguments that are themselves laid out the same way are represented the same way. For example, in Listing 9-7, `A` and `B` are not guaranteed to have the same in-memory representation.    ``` struct Foo<T> {     one: bool,     two: PhantomData<T>, } struct Bar; struct Baz; type A = Foo<Bar>; type B = Foo<Baz>; ```    Listing 9-7: Type layout is not predictable.    The lack of guarantees for `repr(Rust)` is important to keep in mind when you do type casting in unsafe code—just because two types feel like they should be interchangeable, that is not necessarily the case. Casting between two types that have different representations is a quick path to undefined behavior. At the time of writing, the Rust community is actively working out the exact rules for how types are represented, but for now, very few guarantees are given, so that’s what we have to work with.    Even if identical types were guaranteed to have the same in-memory representation, you’d still run into the same problem when types are nested. For example, while `UnsafeCell<T>`, `MaybeUninit<T>`, and `T` all really just hold a `T`, and you can cast between them to your heart’s delight, that goes out the window once you have, for example, an `Option<MaybeUninit<T>>`. Though `Option<T>` may be able to take advantage of the niche optimization (using some invalid value of `T` to represent `None` for the `Option`), `MaybeUninit<T>` can hold any bit pattern, so that optimization does not apply, and an extra byte must be kept for the `Option` discriminator.    It’s not just optimizations that can cause layouts to diverge once wrapper types come into play. As an example, take the code in Listing 9-8; here, the layout of `Wrapper<PhantomData<u8>>` and `Wrapper<PhantomData<i8>>` is completely different even though the provided types are both empty!    ``` struct Wrapper<T: SneakyTrait> {     item: T::Sneaky,     iter: PhantomData<T>, } trait SneakyTrait {     type Sneaky; } impl SneakyTrait for PhantomData<u8> {     type Sneaky = (); } impl SneakyTrait for PhantomData<i8> {     type Sneaky = [u8; 1024]; } ```    Listing 9-8: Wrapper types make casting hard to get right.    All of this isn’t to say that you can never cast types in Rust. Things get a lot easier, for example, when you control all of the types involved and their trait implementations, or if types are `#[repr(C)]`. You just need to be aware that Rust gives very few guarantees about in-memory representations, and write your code accordingly!    ### The Drop Check    The Rust borrow checker is, in essence, a sophisticated tool for ensuring the soundness of code at compile time, which is in turn what gives Rust a way to express code being “safe.” How exactly the borrow checker does its job is beyond the scope of this book, but one check, the *drop check*, is worth going through in some detail since it has some direct implications for unsafe code. To understand drop checking, let’s put ourselves in the Rust compiler’s shoes for a second and look at two code snippets. First, take a look at the little three-liner in Listing 9-9 that takes a mutable reference to a variable and then mutates that same variable right after.    ``` let mut x = true; let foo = Foo(&mut x); x = false; ```    Listing 9-9: The implementation of `Foo` dictates whether this code should compile    Without knowing the definition of `Foo`, can you say whether this code should compile or not? When we set `x = false`, there is still a `foo` hanging around that will be dropped at the end of the scope. We know that `foo` contains a mutable borrow of `x`, which would indicate that the mutable borrow that’s necessary to modify `x` is illegal. But what’s the harm in allowing it? It turns out that allowing the mutation of `x` is problematic only if `Foo` implements `Drop`—if `Foo` doesn’t implement `Drop`, then we know that `Foo` won’t touch the reference to `x` after its last use. Since that last use is before we need the exclusive reference for the assignment, we can allow the code! On the other hand, if `Foo` does implement `Drop`, we can’t allow this code, since the `Drop` implementation may use the reference to `x`.    Now that you’re warmed up, take a look at Listing 9-10. In this not-so-straightforward code snippet, the mutable reference is buried even deeper.    ``` fn barify<’a>(_: &’a mut i32) -> Bar<Foo<’a>> { .. } let mut x = true; let foo = barify(&mut x); x = false; ```    Listing 9-10: The implementations of both `Foo` and `Bar` dictate whether this code should compile    Again, without knowing the definitions of `Foo` and `Bar`, can you say whether this code should compile or not? Let’s consider what happens if `Foo` implements `Drop` but `Bar` does not, since that’s the most interesting case. Usually, when a `Bar` goes out of scope, or otherwise gets dropped, it’ll still have to drop `Foo`, which in turn means that the code should be rejected for the same reason as before: `Foo::drop` might access the reference to `x`. However, `Bar` may not contain a `Foo` directly at all, but instead just a `PhantomData<Foo<'a>>` or a `&'static Foo<'a>`, in which case the code is actually okay—even though the `Bar` is dropped, `Foo::drop` is never invoked, and the reference to `x` is never accessed. This is the kind of code we want the compiler to accept because a human will be able to identify that it’s okay, even if the compiler finds it difficult to detect that this is the case.    The logic we’ve just walked through is the drop check. Normally it doesn’t affect unsafe code too much as its default behavior matches user expectations, with one major exception: dangling generic parameters. Imagine that you’re implementing your own `Box<T>` type, and someone places a `&mut x` into it as we did in Listing 9-9. Your `Box` type needs to implement `Drop` to free memory, but it doesn’t access `T` beyond dropping it. Since dropping a `&mut` does nothing, it should be entirely fine for code to access `&mut x` again after the last time the `Box` is accessed but before it’s dropped! To support types like this, Rust has an unstable feature called `dropck_eyepatch` (because it makes the drop check partially blind). The feature is likely to remain unstable forever and is intended to serve only as a temporary escape hatch until a proper mechanism is devised. The `dropck_eyepatch` feature adds a `#[may_dangle]` attribute, which you can add as a prefix for generic lifetimes and types in a type’s `Drop` implementation to tell the drop check machinery that you won’t use the annotated lifetime or type beyond dropping it. You use it by writing:    ``` unsafe impl<#[may_dangle] T> Drop for .. ```    This escape hatch allows a type to declare that a given generic parameter isn’t used in `Drop`, which enables use cases like `Box<&mut T>`. However, it also introduces a new problem if your `Box<T>` holds a raw heap pointer, `*mut T`, and allows `T` to dangle using `#[may_dangle]`. Specifically, the `*mut T` makes Rust’s drop check think that your `Box<T>` doesn’t own a `T`, and thus that it doesn’t call `T::drop` either. Combined with the `may_dangle` assertion that we don’t access `T` when the `Box<T>` is dropped, the drop check now concludes that it’s fine to have a `Box<T>` where the `T` doesn’t live until the `Box` is dropped (like our shortened `&mut x` in Listing 9-10). But that’s not true, since we *do* call `T::drop`, which may itself access, say, a reference to said `x`.    Luckily, the fix is simple: we add a `PhantomData<T>` to tell the drop check that even though the `Box<T>` doesn’t hold any `T`, and won’t access `T` on drop, it does still own a `T` and will drop one when the `Box` is dropped. Listing 9-11 shows what our hypothetical `Box` type would look like.    ``` struct Box<T> {   t: NonNull<T>, // NonNull not *mut for covariance (Chapter 1)   _owned: PhantomData<T>, // For drop check to realize we drop a T } unsafe impl<#[may_dangle] T>  Drop for Box<T> { /* ... */ } ```    Listing 9-11: A definition for `Box` that is maximally flexible in terms of the drop check    This interaction is subtle and easy to miss, but it arises only when you use the unstable `#[may_dangle]` attribute. Hopefully this subsection will serve as a warning so that when you see `unsafe impl Drop` in the wild in the future, you’ll know to look for a `PhantomData<T>` as well!    ## Coping with Fear    With this chapter mostly behind you, you may now be more afraid of unsafe code than you were before you started. While that is understandable, it’s important to stress that it’s not only *possible* to write safe unsafe code, but most of the time it’s not even that difficult. The key is to make sure that you handle unsafe code with care; that’s half the struggle. And be really sure that there isn’t a safe implementation you can use instead before resorting to `unsafe`.    In the remainder of this chapter, we’ll look at some techniques and tools that can help you be more confident in the correctness of your unsafe code when there’s no way around it.    ### Manage Unsafe Boundaries    It’s tempting to reason about unsafety *locally*; that is, to consider whether the code in the unsafe block you just wrote is safe without thinking too much about its interaction with the rest of the codebase. Unfortunately, that kind of local reasoning often comes back to bite you. A good example of this is the `Unpin` trait—you may write some code for your type that uses `Pin::new_unchecked` to produce a pinned reference to a field of the type, and that code may be entirely safe when you write it. But then at some later point in time, you (or someone else) might add a safe implementation of `Unpin` for said type, and suddenly the unsafe code is no longer safe, even though it’s nowhere near the new `impl`!    Safety is a property that can be checked only at the privacy boundary of all code that relates to the unsafe block. *Privacy boundary* here isn’t so much a formal term as an attempt at describing “any part of your code that can fiddle with the unsafe bits.” For example, if you declare a public type `Foo` in a module `bar` that is marked `pub` or `pub(crate)`, then any other code in the same crate can implement methods on and traits for `Foo`. So, if the safety of your unsafe code depends on `Foo` not implementing particular traits or methods with particular signatures, you need to remember to recheck the safety of that unsafe block any time you add an `impl` for `Foo`. If, on the other hand, `Foo` is not visible to the entire crate, then a much smaller set of scopes is able to add problematic implementations, and thus, the risk of accidentally adding an implementation that breaks the safety invariants goes down accordingly. If `Foo` is private, then only the current module and any submodules can add such implementations.    The same rule applies to access to fields: if the safety of an unsafe block depends on certain invariants over a type’s fields, then any code that can touch those fields (including safe code) falls within the privacy boundary of the unsafe block. Here, too, minimizing the privacy boundary is the best approach—code that cannot get to the fields cannot mess up your invariants!    Because unsafe code often requires this wide-reaching reasoning, it’s best practice to encapsulate the unsafety in your code as best you can. Provide the unsafety in the form of a single module, and strive to give that module an interface that is entirely safe. That way you only need to audit the internals of that module for your invariants. Or better yet, stick the unsafe bits in their own crate so that you can’t leave any holes open by accident!    It’s not always possible to fully encapsulate complex unsafe interactions to a single, safe interface, however. When that’s the case, try to narrow down the parts of the public interface that have to be unsafe so that you have only a very small number of them, give them names that clearly communicate that care is needed, and then document them rigorously.    It is sometimes tempting to remove the `unsafe` marker on internal APIs so that you don’t have to stick `unsafe {}` throughout your code. After all, inside your code you know never to invoke `frobnify` if you’ve previously called `bazzify`, right? Removing the `unsafe` annotation can lead to cleaner code but is usually a bad decision in the long run. A year from now, when your codebase has grown, you’ve paged out some of the safety invariants, and you “just want to hack together this one feature real quick,” chances are that you’ll inadvertently violate one of those invariants. And since you don’t have to type `unsafe`, you won’t even think to check. Plus, even if you never make mistakes, what about other contributors to your code? Ultimately, cleaner code is not a good enough argument to remove the intentionally noisy `unsafe` marker.    ### Read and Write Documentation    It goes without saying that if you write an unsafe function, you must document the conditions under which that function is safe to call. Here, both clarity and completeness are important. Don’t leave any invariants out, even if you’ve already written them somewhere else. If you have a type or module that requires certain global invariants—invariants that must always hold for all uses of the type—then remind the reader that they must also uphold the global invariants in every unsafe function’s documentation too. Developers often read documentation in an ad hoc, on-demand manner, so you can assume they have probably not read your carefully written module-level documentation and need to be given a nudge to do so.    What may be less obvious is that you should also document all unsafe implementations and blocks—think of this as providing proof that you do indeed uphold the contract the operation in question requires. For example, `slice::get_unchecked` requires that the provided index is within the bounds of the slice; when you call that method, put a comment just above it explaining how you know that the index is in fact guaranteed to be in bounds. In some cases, the invariants that the unsafe block requires are extensive, and your comments may get long. That’s a good thing. I have caught mistakes many times by trying to write the safety comment for an unsafe block and realizing halfway through that I actually don’t uphold a key invariant. You’ll also thank yourself a year down the road when you have to modify this code and ensure it’s still safe. And so will the contributor to your project who just stumbled across this unsafe call and wants to understand what’s going on.    Before you get too deep into writing unsafe code, I also highly recommend that you go read the Rustonomicon ([`doc.rust-lang.org/nomicon/`](https://doc.rust-lang.org/nomicon/)) cover to cover. There are so many details that are easy to miss, and will come back to bite you if you’re not aware of them. We’ve covered many of them in this chapter, but it never hurts to be more aware. You should also make liberal use of the Rust reference whenever you’re in doubt. It’s added to regularly, and chances are that if you’re even slightly unsure about whether some assumption you have is right, the reference will call it out. If it doesn’t, consider opening an issue so that it’ll be added!    ### Check Your Work    Okay, so you’ve written some unsafe code, you’ve double- and triple-checked all the invariants, and you think it’s ready to go. Before you put it into production, there are some automated tools that you should run your test suite through (you have a test suite, right?).    The first of these is Miri, the mid-level intermediate representation interpreter. Miri doesn’t compile your code into machine code but instead interprets the Rust code directly. This provides Miri with far more visibility into what your program is doing, which in turn allows it to check that your program doesn’t do anything obviously bad, like read from uninitialized memory. Miri can catch a lot of very subtle and Rust-specific bugs and is a lifesaver for anyone writing unsafe code.    Unfortunately, because Miri has to interpret the code to execute it, code run under Miri often runs orders of magnitude slower than its compiled counterpart. For that reason, Miri should really be used only to execute your test suite. It can also check only the code that actually runs, and thus won’t catch issues in code paths that your test suite doesn’t reach. You should think of Miri as an extension of your test suite, not a replacement for it.    There are also tools known as *sanitizers*, which instrument machine code to detect erroneous behavior at runtime. The overhead and fidelity of these tools vary greatly, but one widely loved tool is Google’s AddressSanitizer. It detects a large number of memory errors, such as use-after-free, buffer overflows, and memory leaks, all of which are common symptoms of incorrect unsafe code. Unlike Miri, these tools operate on machine code and thus tend to be fairly fast—usually within the same order of magnitude. But like Miri, they are constrained to analyzing the code that actually runs, so here too a solid test suite is vital.    The key to using these tools effectively is to automate them through your continuous integration pipeline so they’re run for every change, and to ensure that you add regression tests over time as you discover errors. The tools get better at catching problems as the quality of your test suite improves, so by incorporating new tests as you fix known bugs, you’re earning double points back, so to speak!    Finally, don’t forget to sprinkle assertions generously through unsafe code. A panic is always better than triggering undefined behavior! Check all of your assumptions with assertions if you can—even things like the size of a `usize` if you rely on that for safety. If you’re concerned about runtime cost, make use of the `debug_assert*` macros and the `if cfg!(debug_assertions) || cfg!(test)` construct to execute them only in debug and test contexts.    ## Summary    In this chapter, we’ve walked through the powers that come with the `unsafe` keyword and the responsibilities we accept by leveraging those powers. We also talked about the consequences of writing unsafe unsafe code, and how you really should be thinking about `unsafe` as a way to swear to the compiler that you’ve manually checked that the indicated code is still safe. In the next chapter, we’ll jump into concurrency in Rust and see how you can get all those cores on your shiny new computer to pull in the same direction!```` `````
