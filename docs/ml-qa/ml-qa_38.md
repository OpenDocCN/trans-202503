## **索引**

### **A**

主动学习, 195, 203–204, 222

Adam 优化器, 42, 73, 211, 212–213

适配器方法, 119, 121–123, 125–126, 216

Add & Norm 步骤, 107

对抗样本, 27

对抗验证, 154, 190–191, 218, 221

人工智能（AI）

以数据为中心, 143–146, 217

以模型为中心, 143–144, 145

AlexNet, 6, 7

渐近覆盖保证, 置信区间, 177

注意力机制。*另见* 自注意力机制

巴赫达诺（Bahdanau）, 99–101, 103, 112

转换器, 40, 43–45, 46, 47

增强数据

使用减少过拟合的技术, 24–25, 26, 210

对文本的优化, 93–97, 214–215

自编码器

已定义, 51

潜在空间, 5–6

变分方法, 51–52

自动提示工程方法, 125

自回归解码, 107–110

自回归模型, 54–55, 57, 63–64

辅助任务, 199

### **B**

反向传播, 115–116

回译, 96

词袋模型, 207

连续词袋（CBOW）方法, 90

巴赫达诺注意力机制, 99–101, 103, 112

BART 编码器-解码器架构, 112

小样本学习中的基类, 16

基本线性代数子程序（BLAS）, 148, 152

批量推理, 147–148

批量归一化（BatchNorm）, 73, 213

贝瑞–埃西恩定理，167，171

BERT 模型

用于分类任务，112，215–216

分布假设，91，92

仅编码器架构，107–108

BERTScore，132–133，134，216–217

偏差单元

在卷积层中，70–71

在全连接层中，72，76

二项式比例置信区间，171

BLAS（基本线性代数子程序），148，152

BLEU（双语评估下游任务）分数，128，129–131，133，134

自助法

使用有限数据提高性能，194

包外样本，167–169，170，171

测试集预测，169，170，171，219

### **C**

校准集，176

CBOW（连续词袋模型）方法，90

CE（交叉熵）损失，128，182

中心极限定理，167，171

ChatGPT 模型

自回归模型，54

设计中的随机性，63

强化学习与人类反馈（RLHF），124

无状态与有状态训练，141，217

零样本学习，196

经典的偏差–方差理论，31，35

分类头，215–216

分类任务

采用编码器风格变压器，112，215–216

交叉熵和，128

微调解码器风格变压器，112，216

使用预训练变压器，113–116

Cleanlab 开源库，146

[CLS]标记，108，215–216

CNN。*参见* 卷积神经网络；神经网络

视频数据着色，208

Colossal AI，37，42

COMET 神经网络框架，131，135

计算机视觉

计算参数数量，见 69–73、212–213

分布假设，见 214

全连接层和卷积层，见 75–78、213

用于视觉变压器的大规模训练集，见 79–85、213–214

自注意力机制，见 103、215

概念漂移，见 155–156

置信区间

渐近覆盖保证，见 177

引导式测试集预测，见 169

引导式训练集，见 167–169

与拟合预测的对比，见 173–178

定义，见 164–165

正态近似区间，见 166–167

概述，见 163、173

及预测区间，见 174

的建议，见 170、178

使用不同随机种子重新训练模型，见 169–170

主动学习中的置信度分数，见 204、222

拟合预测

的好处，见 177–178

计算，见 175–176

示例，见 176–177

概述，见 173

及预测区间，见 174

的建议，见 178

连接性，见 80、81

一致性模型，见 56–57、58、212

连续词袋模型（CBOW）方法，见 90

对比学习，见 208

对比自监督学习，见 12–14

卷积层

计算参数数量，见 70–71

作为高通和低通滤波器，见 84

的建议，见 78

用卷积层替换全连接层，见 75–78

卷积神经网络（CNN）。*另见* 神经网络

计算参数数量，见 69–73、212–213

来自 4、6、207 的嵌入

高通和低通滤波器，见 84

归纳偏差，见 80–82

推荐，84

使用视觉变换器，79，82–83，84

卷积操作，61–62

余弦相似度，132，134，216

计数数据，161

协变量偏移，153–154，156，157

CPU 上的数据并行，42，211

交叉熵（CE）损失，128，182

交叉验证

5 折交叉验证，187，188，221

*k*折交叉验证，185–188，221

留一交叉验证（LOOCV），188，221

10 折交叉验证，187，188

CUDA 深度神经网络库（cuDNN），62

### **D**

数据。*另请参见* 有限标签数据

将自监督学习应用于视频，14，208

计数，161

通过减少过拟合，23–27，209–210

自监督学习应用于表格数据，14，208

合成数据生成，96–97

无标签数据，在自监督学习中，10，11

数据增强

减少过拟合，24–25，26，210

对于文本，93–97，214–215

数据中心化 AI，143–146，217

数据分布偏移

概念漂移，155

协变量偏移，153–154

领域偏移，155–156

标签偏移，154–155

概述，153

类型，156–157

数据并行，37，38，39–40，41–42，211

数据集

少样本学习，15

作为随机源的采样和洗牌，60

对于变换器，45

深度玻尔兹曼机（DBMs），50–51，57

死神经元，209

决策树，204

解码器网络（VAE 模型），51–52

解码器

在巴赫达瑙注意力机制中，100–101

在原始 Transformer 架构中，105–106，107

解码器风格的 Transformer。*另见* 编码器风格的 Transformer

当代 Transformer 模型，111–112

分布假设，91

编码器-解码器混合，110

概览，105，108–110

合成数据生成，96–97

与之相关的术语，110

深度玻尔兹曼机（DBMs），50–51，57

深度生成模型。*另见* 生成式 AI 模型

深度学习。*另见* 生成式 AI 模型

嵌入，3–7，207

少量学习，15–18，208–209

彩票假设，19–21，209

多 GPU 训练范式，37–42，211

减少过拟合

与数据相关，23–27，209–210

模型修改，29–36，210

自监督学习，9–14，208

随机性来源，59–65，212

Transformer 的成功，43–47，211

DeepSpeed，37，42

删除，作为数据增强技术的单词，94

确定性算法，62，65

扩散模型，55–56，57，58

维度对比自监督学习，14

直接卷积，61

判别模型，49–50

GAN 中的判别器，52–53

距离，作为编码的嵌入，5

距离函数，179–183

分布假设，89–92，214

域迁移（联合分布迁移），155–156, 157

双重下降，32–33, 36

预训练 Transformer 的下游模型，114

下游任务，11

驱动因子作为随机性源，62

dropout，30, 36, 61, 64–65, 212

### **E**

提前停止，30–31, 35, 210

能源模型（EBMs），50–51

EfficientNetV2 CNN 架构，85

嵌入

分布假设，90–91

在少样本学习中，17

潜在空间，5–6

在原始 Transformer 架构中，106

概述，3–5

表示，6

GPT 模型的涌现性质，110

编码器-解码器模型，110, 111

编码器网络（VAE 模型），51–52

编码器

在 Bahdanau 注意力机制中，100–101

在原始 Transformer 架构中，105–107

编码器风格的 Transformer。*另见*解码器风格的 Transformer

现代 Transformer 模型，111–112

编码器-解码器混合模型，110

概述，105, 107–108

相关术语，110

能源模型（EBMs），50–51

集成方法，33–34, 35, 210, 221, 222

少样本学习中的回合，16

欧几里得距离，181

生成型大语言模型的评估指标

BERTScore, 132–133

BLEU 分数，129–131

概述，127–128

困惑度，128–129

ROUGE 分数，131–132

替代指标，133

外部指标，128

### **F**

基于快速傅里叶变换（FFT）的卷积, 62, 65

FC 层。*见*全连接层

特征选择，作为自注意力的一种形式，46，211

少样本学习。*另见*上下文学习

数据集和术语，15–17

有限标签数据，195–196，203

概述，15

用于减少过拟合，25

基于 FFT（快速傅里叶变换）的卷积，62，65

微调预训练的变换器，113–116，119–124，125–126，216

有限样本下的保型预测保证，177

5 折交叉验证，187，188，221

基于流的模型（标准化流），53–54，57

Fréchet Inception Distance 方法，212

完全连接（FC）层

计算参数数量，70，72

空间不变性或等变性的缺乏，82

推荐，78

用卷积层替代，75–78

用于创建嵌入，6，207

### **G**

泛化准确性，164

泛化性能，32–33

生成对抗网络（GANs），52–53，54，57，58

生成性 AI 模型

自回归模型，54–55

一致性模型，56–57

扩散模型，55–56

基于能量的模型，50–51

基于流的模型，53–54

生成对抗网络，52–53

生成模型与判别模型，49–50

概述，49

随机性与，62–64

推荐，57

变分自编码器，51–52

生成性大型语言模型。*另见*生成性 LLMs 的评估指标；大型语言模型；自然语言处理

GAN 中的生成器，52–53

Gibbs 采样，51

GPT（生成预训练变换器）模型

解码器风格的变换器，91，109–110

分类的微调，112，216

设计上的随机性，63

自我预测，12

GPU。*参见* 多 GPU 训练范式

理解，32–33，36

### **H**

硬注意力，211

硬参数共享，200

硬提示调优，117–118

硬件作为随机性来源，62

卷积神经网络中的层次处理，80

直方图，207

留出验证作为随机性来源，60

同音词，92，214

人类反馈，强化学习中的，124

超参数调优，188

### **I**

图像去噪，56–57

图像生成，51，52，54–57，211–212

图像直方图，207

“一张图像等于 16×16 个词”（Dosovitskiy 等），83，85

ImageNet 数据集，9，14，175

图像处理。*参见* 计算机视觉

重要性加权，154，155，157，218

语境学习，113，116–119，125，216。*另见* 小样本学习

索引，118–119，125

归纳偏差

卷积神经网络中的，80–82

有限的标注数据，202

概述，79

视觉转换器中的，83–84

推理，加速。*参见* 模型推理，加速

图像修复，194–195，208

卷积层中的输入通道，70–71，76–77

输入嵌入，4

输入表示，6，207

InstructGPT 模型，124，126，133，135

操作间并行（模型并行），37，38，39–40，41–42

操作间并行（张量并行），37，38–40，41–42，211

内在指标，128

迭代剪枝，20，31

### **J**

联合分布漂移（领域漂移），155–156，157

### **K**

卷积层中的卷积核大小，70–71，76–77

*k*-折交叉验证

确定 *k* 的适当值，187–188

集成方法，33–34

概述，185–186

作为随机源，60

在选择 *k* 值时的权衡，186–187

知识蒸馏，31–33，35，36，151，199

库尔巴克-莱布勒散度（KL 散度），32，52，211

### **L**

*L*2 距离，181

*L*2 正则化，30，35

有限标签数据。*见* 有限标签数据

标签偏移（先验概率偏移），154–155，156

标签平滑，27

语言变换器。*见* 变换器

大型语言模型（LLMs）。*另见* 自然语言处理；变换器

分布假设，91

评估指标，127–135，216–217

无状态训练与有状态训练，141，217

合成数据生成，96–97

潜空间，3，5–7

层输入归一化技术，34–35

层

卷积层

计算……中的参数数量，70–71

作为高通和低通滤波器，84

针对……的建议，78

用……替换全连接层，75–78

原始变换器架构中的归一化，106–107

微调预训练变换器时的更新，115–116

用于创建嵌入，207

留一法交叉验证（LOOCV），188，221

有限标签数据

主动学习，195

自举数据，194

少样本学习，195–196

归纳偏差，202

标注更多数据，193–194

元学习，196–197

多模态学习，200–202

多任务学习，199–200

概述，193

选择技术的建议，202–203

自监督学习，194–195

自我训练，199

半监督学习，198–199

迁移学习，194

弱监督学习，197–198

线性分类器，114

大型语言模型（LLMs）。*见* 大型语言模型；自然语言处理；变换器

卷积神经网络（CNNs）中的局部连接，80，81

逻辑回归分类器，49–50

LOOCV（留一交叉验证），188，221

循环融合（运算符融合），150–151

循环瓦片化（循环嵌套优化），149–150，151，152，218

LoRA（低秩适配），119，123–124，125，126，216

损失函数，变分自编码器（VAEs），52

彩票假设

概述，19

实践意义和局限性，20–21

训练过程，19–20

低秩适配（LoRA），119，123–124，125，126，216

低秩变换，123

### **M**

MAE（平均绝对误差），183，220–221

多数投票，33

MAPIE 库，178

掩码（缺失）输入自预测方法，12

掩码帧，预测，208

掩码语言建模，91，107–108，194

平均绝对误差（MAE），183，220–221

均方误差（MSE）损失，180–181

自注意力的内存复杂度，103, 215

元数据（元特征）提取，197

元学习，17, 196–197

METEOR 指标, 131, 134

指标，正确的。*见* 正确的指标

缺失（遮蔽）输入自预测方法，12

预测缺失帧，208

Mixup，27

多层感知器（MLPs），50, 82

MNIST 数据集，15, 18, 26, 208, 210

以模型为中心的人工智能，143–144, 145

模型集成，33–34, 35, 210, 221, 222

模型评估。*见* 预测性能和模型评估

模型推理，加速

循环切分，149–150

操作符融合，150–151

概览，147

并行化，147–148

量化，151

向量化，148–149

模型修改，通过减少过拟合，29–36, 210

模型并行（操作符间并行），37, 38, 39–40, 41–42

模型权重初始化作为随机性来源，59–60

MSE（均方误差）损失，180–181

多 GPU 训练范式

数据并行，38

模型并行，38 概览，37

管道并行，40

建议，41–42

序列并行，40–41

加速推理，152, 218

张量并行，38–40

多层感知器（MLPs），50, 82

多模态学习，200–202, 204

多任务学习，199–200, 204

### **N**

朴素贝叶斯分类器，49–50

自然语言处理（NLP）。*另见* 转换器

文本的数据增强，93–97，214–215

分布假设，89–92，214

评估生成型 LLM，127–135，211–212

自注意力，99–103，215

神经网络。*另见* 卷积神经网络；生成型 AI 模型；转换器

注意机制，99–101

计算参数数量，69–73，212–213

嵌入，3–7，207

少样本学习，15–18，208–209

彩票假设，19–21，209

多 GPU 训练范式，37–42，211

减少过拟合

使用数据，23–27，209–210

模型修改，29–36，210

自注意力，99–103

自监督学习，9–14，208

随机性来源，59–65，212

转换器，成功的，43–47，211

下一句/下一个词预测任务，12，108，109，194

NICE（非线性独立成分估计），53–54，58

自然语言处理（NLP）。*见*自然语言处理；转换器

噪声

一致性模型和，56–57

扩散模型和，56

噪声注入，95–96

不合规度量，176–177

非确定性算法，61

非线性独立成分估计（NICE），53–54，58

正态近似区间，166–167，170，171

归一化流（基于流的模型），53–54，57

核采样（top-*p*采样）, 63–64, 212

NVIDIA 显卡, 62, 65

*N*路*K*次（少样本学习）, 15–16

### **O**

ODE（常微分方程）轨迹, 56–57

独热编码, 4, 207

在线资源, xxviii

操作符融合（循环融合）, 150–151

序数回归, 161–162, 218–219

常微分方程（ODE）轨迹, 56–57

异常值检测, 218

袋外自助抽样, 167–169, 170, 171

卷积层中的输出通道, 70–71, 76–77

输出层，更新, 115–116

过拟合

概览, 23

通过数据减少, 23–27, 209–210

通过模型修改减少, 29–36, 210

### **P**

并行化

模型推理，速度提升, 147–148

变压器的, 45–46

参数高效微调, 113, 119–124, 125, 126

参数

在 CNN 中计算数量, 69–73, 212–213

变压器的规模和数量, 45, 47

图像块化归纳偏差, 83, 85, 213–214

困惑度度量, 127–129

管道并行, 37, 40, 41, 42

PixelCNN 模型, 54, 58

像素生成，自回归, 54–55

泊松回归, 161–162, 218–219

多义词, 90

人群参数, 164

正负未标记学习（PU 学习）, 198

训练后量化, 151

预测区间, 173–175, 178

预测区域, 174–175

预测集, 174, 178, 219–220

医疗保健中的预测分析, 146, 217

预测性能与模型评估。*另见* 有限标记数据

置信区间与符合性预测, 173–178, 219–220

构建置信区间, 163–171, 219

*k*折交叉验证, 185–188, 221

泊松回归与序数回归, 161–162, 218–219

合适的评估指标, 179–183, 220–221

训练集与测试集不一致, 189–191, 221

前缀调优, 119, 120–121, 125, 126, 216

前置任务, 10

预训练变换器

适应性, 124–125

分类任务, 113–116

上下文学习、索引与提示调优, 116–119

概述, 113

参数高效的微调, 119–124

强化学习与人类反馈 (RLHF), 124

预训练

仅编码器架构, 107–108

减少过拟合, 25

自监督学习, 10–11

转移学习, 9–10

变换器，通过自监督学习, 45

对于视觉变换器, 83

先验概率偏移 (标签偏移), 154–155, 156

生产与部署

数据分布偏移, 153–157, 218

模型推理，加速, 147–152, 218

无状态与有状态训练, 139–141, 217

提示调优, 117–118

合适的评估指标

标准, 179–180

交叉熵损失, 182

均方误差损失, 180–181

概览, 179

蛋白质建模, 214

近端策略优化, 124, 126

剪枝, 31, 32–33, 35, 36, 151

伪标签生成器, 199 PU 学习（正负样本学习）, 198

PyTorch 框架, 59, 61, 62, 65, 149

### **Q**

量化, 151, 152

量化感知训练, 151

### **R**

随机字符, 95

随机初始化, 209

随机性, 来源

数据集采样和洗牌, 60

不同的运行时算法, 61–62

及生成式 AI, 62–64

硬件与驱动程序, 62

模型权重初始化, 59–60

非确定性算法, 61

概览, 59

随机种子, 169–170

召回导向的摘要评估（ROUGE）得分, 128, 131–132, 133, 134

重构误差, 测量, 218

重构损失, 52

修正线性单元（ReLU）激活函数, 21, 209

循环神经网络（RNNs）, 99–101, 103, 112. *另见* 神经网络

减少过拟合

使用数据, 23–27, 209–210

通过模型修改, 29–36, 210

回归分析、保形预测和置信区间, 178, 220

正则化, 用于减少过拟合, 30–31, 36

强化学习与人类反馈（RLHF）, 124

相对位置嵌入（相对位置编码）, 82, 85

ReLU（修正线性单元）激活函数, 21, 209

重新参数化, 151

表示学习, 11

表示, 3, 6–7

RepVGG CNN 架构, 151, 152

变换器架构中的残差连接, 107

ResNet-34 卷积神经网络, 146, 217

资源, 在线, xxviii

重新训练

使用不同的随机种子, 169–170

无状态, 139–140, 141, 217

RLHF（带有人工反馈的强化学习）, 124

RNN（递归神经网络）, 99–101, 103, 112。*另见* 神经网络

RoBERTa（稳健优化的 BERT 方法）, 108, 112

均方根误差 (RMSE), 183, 220–221

根平方误差, 181

ROUGE（面向召回的概述评估）得分, 128, 131–132, 133, 134

作为随机源的运行时算法, 61–62

### **S**

SAINT 方法, 208

样本对比自监督学习, 14

作为随机源的采样, 60, 65

完整性检查, 189

缩放点积注意力, 40, 42。*另见* 自注意力机制

SCARF 方法, 208

拟合预测的得分方法, 176–177, 178

SE (平方误差) 损失, 181

种子随机生成器, 60, 61

自注意力机制。*另见* 变换器

与 Bahdanau 注意力机制对比, 99–101

概述, 99, 101–102

序列并行性, 40

变换器, 42, 43–45, 46, 47

在视觉变换器中, 83–84

自我预测, 11–12

自监督学习

对比, 12–14

仅编码器架构, 108

利用未标注数据, 11

有限标注数据，194–195，203，204，221–222

概述，9

通过预训练变压器，45

通过减少过拟合，25

自我预测，11–14

与迁移学习的比较，9–11

自我训练，199。*另见* 知识蒸馏

半监督学习，198–199，203

句子打乱，95

[SEP]标记，108

序列并行性，40–41，42

序列到序列（seq2seq）模型，107–110

顺序推理，148

SGD（随机梯度下降）优化器，73，212

跳跃连接，107

孪生网络设置，13

相似性，嵌入作为编码，5

.632 自助法，171

变压器架构中的跳跃连接，107

跳字法（skip-gram）方法，Word2vec，90

更小的模型，通过减少过拟合，31–33

软注意力，211

**软参数共享**，200

软提示，119–121，125

随机性来源

数据集采样和打乱，60

不同的运行时算法，61–62

和生成性 AI，62–64

硬件和驱动程序，62

模型权重初始化，59–60

非确定性算法，61

概述，59

空间注意力，215

空间不变性，80–82

加速推理。*参见* 模型推理，加速

平方误差（SE）损失，181

稳定扩散潜在扩散模型，58

堆叠（堆叠泛化），33

有状态训练，139，140–141，217

无状态训练（无状态再训练），139–140，141，217

统计总体，164

统计两样本检验，218

随机扩散过程，56

随机梯度下降（SGD）优化器，73，212

步长，78，213

结构化剪枝，20

知识蒸馏中的学生，31–32

监督学习，15。*另见* 有限标注数据

少样本学习中的支持集，16

同义词替换（文本增强），93–94

合成数据生成，96–97

### **T**

TabNet，208

表格数据的自监督学习，14，208

知识蒸馏中的教师，31–32

10 折交叉验证，187，188

TensorFlow 框架，59，62，149

张量并行，37，38–40，41–42，211

测试集

自助法，169，170，171

适应性预测，176

与训练集的差异，189–191，221

文本数据增强，93–97，214–215

T5 编码器-解码器架构，112

自注意力的时间复杂度，103，215

top-*k* 采样，63–64，212

训练。*另见* 多 GPU 训练范式；预训练；随机性，来源；重新训练

调整轮次的数量，35，210

后训练量化，151

彩票票假设的过程，19–20

量化感知，151

自训练，199

无状态和有状态，139–141，217

训练集

适应性预测，176，177

与测试集的差异，189–191，221

对于视觉变换器，79–85，213–214

迁移学习

有限标注数据，194，203，204，221–222

使用该方法减少过拟合，25，26，209

与自监督学习相比，9–11

变换器。*参见*自注意力机制

适应预训练语言模型，124–125

注意力机制，40，43–45

分类任务，113–116

当代模型，111–112

解码器，108–110

编码器-解码器混合，110

编码器，107–108

上下文学习、索引和提示调优，116–119

多 GPU 训练范式，40，42

参数数量，45

原始架构，105–110

概述，105，113

并行化，45–46

参数高效微调，119–124

通过自监督学习进行预训练，45

强化学习与人类反馈（RLHF），124

成功，43–47

术语，110

迁移学习，11

翻译

返回，96

不变性和等变性，80–82

令牌，44–46，63–64，102，106–110，117–118

三角不等式，180，181，182

真实泛化准确度，164

二维嵌入，4–5

拼写错误介绍，95

### **U**

自监督学习中的未标记数据，10，11

非结构化剪枝，20

无监督预训练。*参见*自监督学习

### **V**

变分自编码器（VAEs），51–52，53，54，57，58

变分推理，51

向量化，148–149，152，218

VideoBERT 模型，201，204

视频数据，应用自监督学习，14，208

视觉变换器（ViTs）

与卷积神经网络相比，79，82–83，84

归纳偏差，83–84

大规模训练集，79

位置信息，82，85

推荐，84

### **W**

弱监督学习，197–199，203

权重衰减，30，35

加权损失函数，155

权重初始化，59–60

权重归一化，34–35

权重剪枝，19，20

权重

在卷积层中，70–71，76–77

在全连接层中，72，76

权重共享，80，81

胜利彩票（彩票假设），20，21

基于 Winograd 的卷积，62，65

单词删除（文本增强），94

词嵌入。*另见* 嵌入

WordNet 同义词库，94，97

单词位置交换（单词洗牌/排列），94–95

Word2vec 模型，90，92

### **X**

XGBoost 模型，26，209

### **Z**

零-shot 学习，195–196。*另见* 少-shot 学习；上下文学习

*z* 分数（置信区间），166
