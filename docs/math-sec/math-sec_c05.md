# 第七章：使用社交网络分析识别威胁

![](img/chapterart.png)

*社交网络分析 (SNA)* 是图论的一个子集，它用数学方式描述复杂的人类互动；它可以应用于任何涉及人类互动的研究领域。安全研究人员使用 SNA 进行从预测恶意内容传播到识别潜在内部威胁的各种工作。在本章中，我们将进行自己的 SNA：我们将从 Mastodon 的帖子构建一个简单的社交网络图，然后利用它来理解用户之间的影响力和信息交换。具体来说，我们将研究一个真实的社交网络效应，称为*小世界现象*。接下来，我们将看看如何从帖子中构建图形，回答几个研究问题，最后进行一个概念验证项目，在该项目中，你将能够从你自己的 Mastodon 时间线收集数据。

## 小世界现象

斯坦利·米尔格拉姆的*小世界实验*展示了群体从众对人类决策的影响。实验的目的是研究美国社交网络的平均路径长度，其中*路径长度*是指从网络中的一个人到另一个看似不相关的人的信件传递所需的人员数量。米尔格拉姆通常选择美国内布拉斯加州奥马哈和堪萨斯州威奇托的个人作为起点；马萨诸塞州波士顿的人通常作为终点。在收到参与邀请后，被指定为原始信件发送者（起点）的人需要被询问是否个人认识随机选中的最终接收人（终点）。如果认识，起点需要直接将信件转交给终点。在更常见的情况下——起点不认识终点——起点会被要求想到一个更可能认识终点的朋友或亲戚。路径上的人们可以将信件转交给他们认识的任何人，以便将信件送得更接近终点。

从技术上讲，如果社交网络中任意两个人之间通过少数中介熟人很可能被连接，那么该社交网络就展现了小世界现象。Milgram 的研究表明，我们的社会是一个强连接的网络：任何两个网络成员之间，很可能通过三到六个中介熟人相连（这通常被称为 *六度分隔*，即小世界现象的一个特定案例）。在小世界现象中起作用的机制被称为 *优先连接*，即一个人更可能与已经有许多连接的人建立联系。简而言之，统计上你更可能遇到一个去结识许多新人的人，而不是遇到一个几乎没有社交互动的隐士。事实证明，这种类型的网络在自然界中广泛存在，从动物社会结构到人脑中都有观察到。显然，作为安全分析师，理解这一点非常值得我们花时间去研究。

我们分析虚拟帖子和用户数据集的目标是回答以下研究问题：

+   有多少信息会被传播？

+   这个网络中存在哪些社交圈？

+   谁是三位最具影响力的用户？

+   谁是三位最受影响的用户？

+   谁能引入最多的新连接？

在本章的其余部分，我们将依次讨论每个主题，看看如何重新解读之前的理论以深入理解社交网络用户交互，并探索新的图论主题，如残余信息和节点谱系。

## 绘制社交网络数据

为了将我们的社交网络数据转化为图形，首先我们需要将其结构化为可搜索的表格格式。我们将使用 pandas 库，它为我们提供了帮助组织数据并为图形化做准备的函数和数据结构。

首先，让我们看看原始的 JSON 数据。文件 *fake_posts.json*，随书的补充材料提供，包含了 28,034 个类似帖子的对象，格式遵循列表 5-1 中展示的 JSON 架构。

```
{
  ❶ "id": 4912964953915055,
    "created_at": "2019-5-22 23:03:22",
  ❷ "content": "Process within summer especially song when letter nearly.",
    "source": "Data Faker",
  ❸ "in_reply_to_screen_name": "Some User",
    "in_reply_to_id": 1234334523168,
    "in_reply_to_account_id": 346835683,
  ❹ "account": {
        "id": "6336091949992",
        "screen_name": "juliekennedy",
        "location": "846 Adam Spring #616\nE Chicago, IL 21342",
        "description": "Faked profile Data",
        "url": "http://www.smith.com/"
    },
  ❺ "reblog_count": 0,
    "liked_count": 0
}
```

列表 5-1：使用 Mastodon 架构示例的模拟 API 响应

`id` 字段 ❶ 包含一个由 API 在创建帖子时分配给单个记录——即帖子——的数字 ID。`content` 字段 ❷ 包含添加到网络中的数据——也就是构成帖子的文本。一些帖子是原创的，其余的是对帖子、或对回复的回复，依此类推。`reblog_count` 字段 ❺ 统计帖子对象收到多少次回复。表示转发的对象将包含以 `in_reply_to_` 开头的字段名称 ❸。`account` 字段 ❹ 是一个嵌套的 JSON 对象，用于标识帖子的创建者。

### 数据结构化

我们将从帖子对象中保留比 第四章 中的包对象更多的数据，并且帖子对象结构是嵌套的，所以首先我们需要将数据文件加载到 pandas `DataFrame` 对象中。`DataFrame` 对象是 pandas 中用于存储表格数据的行和列数据结构。它在结构上类似于数据库（甚至支持一些相同的操作，如数据过滤和连接）。使用 `DataFrame` 可以为我们提供更方便的语法来排序和选择相关的帖子对象，它也展示了将分析库结合使用的强大功能。通过结合使用工具（在这种情况下是 pandas 和 NetworkX），你可以为特定任务选择合适的工具，而不是强迫某个库做它本来不擅长的事。

列表 5-2 中的代码定义了一个来自示例数据的 `DataFrame` 对象。

```
❶ import pandas as pd
import json

❷ def user_to_series(dict_obj):
    """Convert a nested JSON user into a flat series"""
    renamed = {}
    for k in dict_obj.keys():
        nk = "user_%s" % k
        v = dict_obj[k]
        renamed[nk] = v
    ret = pd.Series(renamed)
    return ret

series_data = [] # 1 JSON object per post object
❸ with open("fake_posts.json") as data:
    text = data.read().strip()
    rows = text.split("\n") # JSON objects stored as list of strings
for row in rows:
  ❹ obj = json.loads(row)   # Converted row string to JSON object
    series_data.append(obj) # Add to JSON list

❺ t_df = pd.DataFrame(series_data) # 1 row per JSON object
❻ post_df = pd.concat([t_df, t_df["account"].apply(user_to_series)], axis=1)
# Data is flat now. Remove the original JSON object feature.
❼ post_df.drop("account", axis=1, inplace=True)
```

列表 5-2：从示例 JSON 数据创建 pandas `DataFrame` 对象

在导入所需的库❶之后，我们定义了一个名为 `user_to_series` 的辅助函数❷，稍后我会详细讲解这个函数；从高层次上看，这个函数将每个 JSON 用户对象转换为适合在 pandas `DataFrame` 中使用的行。我们使用常规方式通过 `with open` ❸ 加载 *fake_posts.json* 文件，使用 `strip` 函数去除任何尾随的空白字符，并通过剩余的 `"\n"` 字符将文件数据分割为行。pandas 库可以从 JSON 对象列表中创建 `DataFrame`，因此我们使用 `json.loads` ❹ 将每一行字符串转换为 JSON 对象，并将这些对象收集到 `series_data` 列表中❺。

不幸的是，当 JSON 包含嵌套对象时，比如 `account` 字段，pandas 并不知道如何解包它们。我们需要使用 pandas 的 `apply` 和 `concat` ❻ 函数将嵌套字段转换为扁平化的 pandas 对象，通过对数据中的每一行应用 `user_to_series` 函数，生成一个扁平的 pandas `Series`。你可以将 *series* 理解为类似于数据库术语中的一行 —— 它将所有与单个条目相关的数据组织在一起。

`pd.concat` pandas 函数将这些新特性附加到当前的 `DataFrame`，并应用于所有行。`axis=1` 参数告诉 pandas 使用序列作为 *特征*（在数据库术语中是列），这使得 `DataFrame` 中每一列与用户字段中的每一项数据相匹配（如用户名和 ID）。每一行代表一个用户，每一列则保存该用户对应字段的值。

最后，我们通过 `DataFrame.drop` ❼ 删除不再需要的原始 `account` 特性。加载完初始数据集并应用了所有列处理后，我们可以通过调用 `post_df.info` 打印出数据的结构。列表 5-3 展示了从 列表 5-2 得到的结构，你可以在补充材料中的 Jupyter notebook *Mastodon_network.ipynb* 中看到它。

```
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 28034 entries, 0 to 28033
Data columns (total 14 columns):
 #   Column                   Non-Null Count  Dtype  
---  ------                   --------------  -----  
 0   id                       28034 non-null  int64  
 1   created_at               28034 non-null  object 
 2   source                   28034 non-null  object 
 3   content                  28034 non-null  object 
 4   in_reply_to_account_id   10302 non-null  object 
 5   in_reply_to_id           10302 non-null  float64
 6   reblogs_count            28034 non-null  int64  
 7   favourites_count         28034 non-null  int64  
 8   user_id                  28034 non-null  object 
 9   user_screen_name         28034 non-null  object 
 10  user_location            28034 non-null  object 
 11  user_description         28034 non-null  object 
 12  user_url                 28034 non-null  object 
 13  in_reply_to_screen_name  10302 non-null  object 
dtypes: float64(1), int64(3), object(10)
memory usage: 3.0+ MB
```

列表 5-3：pandas 中的帖子数据结构

数据结构告诉我们一些重要的信息。首先，`RangeIndex`属性告诉我们当前`DataFrame`对象中有多少行数据。在这个例子中，我们已经加载了 28,034 条帖子记录，索引从`0`到`28033`。接下来，我们可以看到列表中已经没有`account`列，这意味着我们在 Listing 5-2 中的删除操作成功地修改了`DataFrame`。列名右侧的数字表示数据中有多少行在该列中有非空值。我们可以看到大多数列在每一行中都有值，因为非空计数与索引计数相匹配。相比之下，以`in_reply_to_*`开头的列在 28,034 行数据中只有 10,302 行有非空值。这是因为这些值仅出现在作为回复的帖子中。稍后我们将利用原创帖子和回复之间的这一差异。

在值计数的右侧是列中存储的数据类型。如果在导入数据时没有明确地定义数据类型，pandas 会尽力逻辑地解释这些类型。不幸的是，它主要擅长识别整数和浮动类型。对于其他列，你可以看到它为这些列分配了通用类型`object`。这是 pandas 表示它并不清楚如何处理该列数据的一种方式。可能是数据类型无法排序（就像`user_name`列中存储的字符串）或者同一列中可能有两种或更多数据类型（例如`in_reply_to_screen_name`列，其中某些行有整数值，而其他行则有空值）。在开始任何分析之前，理解底层数据的结构非常重要。你会逐渐熟悉可用的不同数据类型以及何时使用它们，但现在我们不需要做任何更改，因此我们将继续查看输出的最后两行。`dtypes`属性只是为了方便总结列中的数据类型。我们可以看到，其中一列被确定为浮动数值，三列被确定为整数，其余列则被 pandas 保留为通用`object`类型。

最后，内存使用行估算了存储整个`DataFrame`对象所需的内存量。你可以使用这个值大致了解应用程序的数据存储需求，但这里也有一些注意事项。根据你的配置，pandas 可能会通过两种方式之一来计算这个数字。默认情况下，pandas 只是将每列数据类型存储一个值所需的字节数与`DataFrame`中的行数相乘。例如，一个`int64`值占用 8 个字节，所以`id`列大约占用 8 × 28,034 = 224,272 字节（约 224KB）。通过对每一列重复这个过程并汇总结果，pandas 快速估算内存使用情况。问题是某些数据类型（比如`object`类型）没有最大大小，因此 pandas 只能猜测这些类型分配的最小空间。这就是为什么内存使用后面会有一个`+`符号的原因。

### 可视化社交网络

在定义了`post_df`对象后，你可以分析数据结构并选择用于图形定义的字段。我们将定义一个节点 ID *u*，作为使用该网络的唯一用户账户。`user_id`和`user_screen_name`具有 1:1 的关系，所以两者都是节点 ID 的良好候选字段。`user_screen_name`字段使得图形更加美观，但`user_id`可能更适合自动化系统——例如，用于通过 ID 查找用户个人信息的网络分析结果。为了让图形更具吸引力和记忆性，我们将使用`user_screen_name`字段；毕竟，看着一堆随机生成的 ID 可没那么有趣。

对于边，我们会查看两位用户是否在一条帖子上进行了互动，具体见 Listing 5-4。

```
❶ G = nx.DiGraph()
❷ for idx in post_df.index:
  ❸ row = post_df.loc[idx]
  ❹ G.add_edge(
      row["in_reply_to_screen_name"], row["user_screen_name"],
    ❺ capacity=len(row["content"])
    )
print(len(G.nodes))
```

Listing 5-4：将帖子数据表示为有向图

`in_reply_to_*`字段使我们能够看到某条帖子是否是对早期帖子的回复（显然是来自其他用户）。当用户 B 回复用户 A 的帖子时，我们将把它视为它们之间的一个边，*e*[*(a→b)*]。我将继续讨论关于边的内容，以及如何解读它们。

首先，我们从之前定义的`DataFrame`对象创建一个有向图❶。

我们遍历`post_df`对象中的每个索引❷，并使用`DataFrame.loc`函数逐行检索❸。每当一个用户转发另一个用户的帖子时，我们会在图中添加一条边❹。创建原始帖子的用户（源节点）保存在`in_reply_to_account_id`字段中，回应的用户（终端节点）保存在`user_screen_name`字段中。接下来，我们将帖子文本的长度作为一个名为`capacity`的特定边权重形式包含在内❺。这是衡量帖子中包含信息的一个非常简单的方式，稍后我们会详细讨论。最后，我们可以打印图节点列表的长度，确认我们已经将 85 个帖子对象添加到图中。

Figure 5-1 展示了从 Listing 5-4 生成的图的 3D 表示。

每个点都是一个节点，代表网络中的不同用户，而每条虚线则是表示两个用户之间的帖子互动的边。没有回复的帖子不会在图中创建边，因此这里没有可视化出来。尽管数据量很大，图看起来一开始有些混乱，但其实还是有一些可以总结的地方。例如，你可以看到这是一个高度连接的网络。观察外围的节点，你会发现大多数用户有很多条边指向不同的其他用户，这意味着他们在某个时刻通过帖子进行了互动。还要注意，有些节点的边远比其他节点多。就像我们在上一章对计算机网络的分析一样，接下来我们将开始理清这片连接的云，看看能否从中得出一些与我们研究相关的有趣观察。

![](img/f05001.png)

图 5-1：社交网络图的 3D 可视化

## 网络分析见解

在构建了我们的图后，我们可以将注意力转向我们的研究问题，首先探讨网络中传播了多少信息。

### 计算信息传播

计算某物所包含的信息量是一个古老的问题，背后有大量深奥的数学研究。大多数真正有用的方法都涉及一个叫做*信息熵*的概念，并深入测量某个值（如一个短语）随机出现的概率。这些度量通常用数学语言来描述是相当复杂的，并且需要展开一整篇关于语言学和马尔可夫链的讨论。相反，我选择了一个粗略的替代方法——文本长度。本质上，每个帖子被当作一个数据单元处理，帖子的总信息在每次互动时交换。当用户回复帖子时，我们就认为信息被传播了。

我们将采用与在 NetworkX 中使用的不同的信息交换率方法（参见《分析机器如何在网络中互动》，《第四章》）。相反，我们将考虑*剩余信息（RI）*得分，即网络中新增的信息量与消耗的信息量之间的差异。例如，您可以将本地图书馆的剩余信息视为它拥有的所有书籍与该地区人们已经阅读的书籍之间的差异。很可能有一些冷门书籍静静地待在书架上，等待某一天有人需要它们。社交网络如 Mastodon 也是如此。当用户创建新帖子时，他们向网络中添加*潜在信息*——也就是说，等待其他用户发现的信息。当用户回复现有帖子时，潜在信息通过信息交换转化为*动能信息*：信息通过阅读和回复原始信息的行为从一个用户流向另一个用户。在这种情况下，信息通过有向边*e*从起始用户流向终端用户，因此边集*p*也可以视为动能交换的集合。

然后，您可以将问题“传播了多少信息？”重新表述为“在多少潜在信息之后，某些信息有可能转化为动能信息？”或者简单地说，“需要多少帖子才能让别人阅读并回复？”我们可以通过计算没有回复的原创帖子(*o*)与有回复的原创帖子(*p*)的比率来回答这个问题；这给出了 RI 得分。暂时假设一整个帖子是一个信息单位，因此对于每条边，信息从*u*交换到*v*。这是一种*平衡交换*，其中帖子中的所有（且仅有的）信息都被传递。如果接收信息的节点每次只能接收一半的信息，那将是*不平衡交换*，因为发送方可以发送更多信息，而接收方无法处理。

使用这些定义，你可以通过比较潜在信息和动能信息的交换比例，观察信息在整个网络中传播的整体趋势。公式*RI* = *|p|* / *|o|* 描述了所有交换发生后，网络中剩余的潜在信息量。结果告诉你大概需要向网络中添加多少信息，才能使其中一部分被另一个用户消费（通过阅读和回复）。如果网络中的每一条帖子都有回应，你将得到 RI 分数为 0 / *n* = 0。当网络中没有剩余信息时，你需要添加一条信息才能被其他用户消费。一个没有回复（全部为原创消息）的网络，其 RI 分数为 *n* / 0 = NaN，这意味着网络中*只有*剩余信息，表示没有已知的潜在信息会变成动能信息。如果原创帖子是回复帖子的两倍，比例为 2:1——每创作两条帖子，就会有一条得到回复。在另一个 RI 分数为 6（6:1 比例）的网络中，只有六分之一的帖子得到回复，意味着信息流的传播更加难以进行。

列表 5-5 使用`DataFrame`对象`post_df`计算示例网络的 RI 分数。

```
❶ o_posts = post_df[post_df["in_reply_to_screen_name"].isna() == True]
r_posts = post_df[post_df["in_reply_to_id"].isna() == False]
if len(r_posts.index.to_list()) != 0:
  ❷ replied_to = r_posts["in_reply_to_id"].values
  ❸ o_no_r = o_posts.loc[o_posts["id"].isin(replied_to) == False]
    p_len = float(len(o_no_r.index.to_list()))
    o_len = float(len(o_posts.index.to_list()) - p_len)
  ❹ info_exchange = float(p_len / o_len)
else:
    info_exchange = -1
print("The RI score is: %.4f " % info_exchange)
```

列表 5-5：将 RI 算法应用于示例数据

为了衡量网络中潜在信息和动能信息的量，我们将原创帖子（没有`in_reply_to_id`值的帖子）收集到`o_posts` ❶ 中，并将转发帖子收集到`r_posts`中。我们将没有收到回复的原创帖子（*p*）分离到`o_no_r` ❸中，并将收到回复的原创帖子（*o*）分离到`o_posts`中，通过从回复列表中收集包含回复的帖子 ID ❷，并创建一个排除`replied_to`帖子的新的列表。`o_no_r`中的帖子代表在所有交换发生后网络中剩余的潜在信息。最后，我们取`o_no_r`和`o_posts`长度的比值来计算 RI 分数 ❹。对于示例数据，结果应约为`2.6358`，表明每收到一条回复，大约有三条原创帖子被创建。

### 识别团体和最有影响力的用户

网络分析的一个关键方面是检测嵌套在大网络中的较小社区或团体。回顾第三章，团体是指一组相互直接连接的节点。在我们的社交网络中，这代表了一群彼此熟悉并且曾经互动过的用户。

让我们开始寻找一些团体，首先通过清理数据集并显示图表来进行。团体只有在节点与其他节点有连接时才有意义，所以首先我们需要清理数据，只包括那些有回复的帖子。我们可以像这样从`DataFrame`中删除没有回复的帖子：

```
r_posts = post_df[post_df["in_reply_to_id"].isna() == False]
```

所有`in_reply_to_id`字段已填充的帖子将被归类到`r_posts`对象中。我们已经在第三章从理论角度讨论了第二个研究问题：“这个网络中存在哪些社交圈？”所以现在让我们应用这些知识，来理解这个网络的潜在结构。社交圈是节点子集*u*，其中所有节点*u*彼此之间都是直接连接的，因此，如果我们假设用户会阅读对其帖子的回复，那么我们可以合理地将有向图转化为无向图，以便识别这些社交圈。清单 5-6 会转换图形，并将社交圈作为列表找到。我们将继续使用有向图进行分析，并结合无向图中的社交圈列表。

```
uG = nx.to_undirected(G)
cliques = list(nx.algorithms.clique.find_cliques(uG))
```

清单 5-6：转换为无向图以寻找社交圈

网络中的社交圈很有趣，因为它们提供了用户互动的图景。较大的社交圈通常代表有某种共同联系的用户；它们可以揭示联盟的形成，甚至预测断裂。社交圈本身也可能很有趣：它们告诉你谁认识谁。例如。然而，真正有价值的分析是当你开始分析不同社交圈的成员时，你才会真正获得洞察。你可能会识别出社交圈的领导者，看看谁对网络中其他成员有影响力或地位。正是我们将要做的事情：我们将运用对网络中潜在社交圈的理解，找到哪些群体包含了我们类似 Mastodon 网络中最有影响力的用户。

在这种情况下，节点的出度表示其他用户回复原始节点所发布帖子的次数。一个具有较高出度的节点可以被视为“更受欢迎”，因为这些帖子通常会引发更多的回应。通过识别与这个受欢迎节点接近的节点，我们可以聚焦于潜在的影响者。清单 5-7 找到有向图中出度最高的节点，然后找到包含该节点的社交圈。

```
❶ deg_ct = G.out_degree()
sorted_deg = sorted(deg_ct, key=lambda kv: kv[1])
top_source = sorted_deg[-1]
❷ source_cliques = [c for c in cliques if top_source[0] in c]
❸ sG = G.subgraph(source_cliques[0])
```

清单 5-7：查找具有最高出度节点的所有最大社交圈

首先，我们获取有向图中所有节点的出度 ❶。在分析关系时，有时你可能想要量化节点之间连接的强度以及其余数据。例如，如果你知道网络中有两个用户结婚了，你可能想要赋予它们之间的边更高的权重，而不是两个同事之间的边。在清单 5-3 中，我们通过捕捉文本的长度作为交换数据量的粗略度量。现在我们可以利用这些信息来评估用户之间交流的质量。为了考虑边的质量以及数量，我们将简单的出度度量替换为加权出度度量，像是 Dijkstra 算法（正如我在第四章中提到的那样，你通过显式传递`weight`参数给最短路径算法来实现）。在按出度数升序排列节点后，我们选择最后一项，即作为帖子来源的目标节点，接着使用列表推导式 ❷ 提取包含目标节点的团体。

图 5-2 显示了通过选择这些团体 ❸ 中的第一个所创建的子图。

![](img/f05002.png)

图 5-2：用户最回应的团体子图

流行用户`dannyhoover`向图中的每个节点发送出边。用户`michaelcruz`和`falvarez`回复了最多其他团体成员的帖子。你可以推测，`dannyhoover`在这些团体成员中比`michaelcruz`或`falvarez`更具影响力。但这并不意味着这两位用户在其他情境下不具影响力。记住，在处理子图时，你获得的信息总是与子图相关，而不是整个图。

对于第三个研究问题，“谁是三个最有影响力的用户？”，我们只需扩展清单 5-7 中的代码，考虑排名前三的源节点。影响力用户是那些能增加更可能引发动态交换的潜在信息的用户。作为练习，尝试确定前三个影响力节点是否在同一个团体中。你能从结果中推断出什么？

### 寻找最受影响的用户

接下来的问题探讨了图中的反向关系；即，“谁是三个最受影响的用户？”与具有最高入度的节点相关。如果你考虑我们对这个网络中影响力的定义，影响者是那些发布原创帖子，且这些帖子可能会得到一个或多个用户回应的人。相反，*受影响*较大的用户是那些回复大量其他用户原创帖子的用户。幸运的是，代码与清单 5-7 非常相似。只需将`G.in_degree`替换为`G.out_degree`，就能生成类似于图 5-3 中的图。

![](img/f05003.png)

图 5-3：寻找最有影响力的用户

用户`juliekennedy`负责网络中大部分的动能信息交换，这意味着他们回复了最多的用户。根据我们的假设，回应帖子的人在某种程度上已经受到了影响（至少足以做出回应），我们可以得出结论，用户`juliekennedy`受到了最多用户的影响。当然，你可以自由地（而且可能应该）对这一假设的有效性进行辩论。我们正在处理一个安全领域，你必须准备好为你在分析中建立的假设进行辩护。在分析像人类互动这样复杂的事物时，请记住，我们所能做出的主张是有限制的，准确性和有效性也有其界限。

### 使用基于主题的信息交换

偏离一下我们的研究问题，我们可以通过*基于主题的信息交换*来回答之前关于影响力的两个问题，具体来说，我们在此过程中考虑某一特定情境或主题下最具影响力和受影响的用户。例如，我们可能会考虑最具影响力的心脏外科医生或最具影响力的黑客。通过用情境实例来分析影响力和受欢迎程度，我们可以更深入地了解我们记录的互动内容。简单来说，我们可以回答“这些用户互动到底是什么？”我们会找出某些特定主题（如环境和政治）下最具影响力和受影响的用户，但你同样可以很容易地扩展这一原则，用于寻找讨论当前事件或其他感兴趣话题的用户。

对于基于主题的信息交换，我们使用*超链接诱导主题搜索*（*HITS*，也称为*中心和权威*）算法，它用于分析有向图中的链接关系^1。最初为互联网搜索引擎设计，用来根据与特定主题的相关性对网页进行评分，HITS 已被应用于许多其他类型的链接分析。在安全性和社交网络分析方面，HITS 可以为通用影响力度量（如信息交换比率，IER）提供有用的背景。例如，安全研究人员曾利用 Twitter 追踪与孟买恐怖袭击相关的信息^2，通过分析与袭击相关的话题并确定哪些用户似乎对事件有最权威的理解。

原始算法背后的直觉非常简单：某些站点，被称为*中心节点*，充当大型网站目录。页面按照与查询主题的相关性排序。一个好的中心节点是指向许多不同主题的其他页面。如果多个中心节点指向同一源页面，那么该页面被视为该主题的权威页面。换句话说，权威节点是指被许多不同中心节点链接的节点。中心节点的得分越高，节点的权威性就越强。一个中心节点连接的权威节点越多，其中心得分越高。现代搜索引擎就是典型的中心节点。这些站点并不是它们所收录的某一特定主题的权威，但它们可以将用户引导到其他*权威*的站点。

在我们的网络中，中心节点将是一个其帖子被大量权威用户转发的用户。信息流的另一端是权威节点，等同于从多个优质信息中心节点转发信息的用户。NetworkX 在底层依赖 SciPy 库将图转换为*稀疏邻接矩阵*（一种列表，记录图中每个可能的连接是否存在）。反过来，SciPy 依赖 NumPy 处理矩阵数学。不幸的是，这一依赖链可能会很脆弱。根据你安装包的方式，运行 *Mastodon_network.ipynb* 文件时，可能会遇到类似 `module 'scipy.sparse' has no attribute 'coo_array'` 的属性错误。我通过使用以下命令安装 NetworkX 版本 2.6.3 暂时解决了这个问题：

```
conda install -y networkx=2.6.3
```

HITS 算法会在一个相关节点子集上迭代执行，通常这个子集是通过某种搜索算法返回的。在每次迭代中，算法会重新计算每个节点的中心得分和权威得分这两个实数值。由于一个好的中心节点的得分应当随着每次迭代而增加，因此它给予每个权威节点的得分也会增加，反之亦然。最终输出的是子集内每个节点的两个得分。

列表 5-8 展示了一种方法，用于查找与包含单词*environment*的帖子相关的中心节点和权威节点，再次使用列表 5-2 中的`DataFrame`对象。

```
❶ post_df["content"] = post_df["content"].str.lower()
❷ env = post_df[post_df["content"].str.contains("environment")]
❸ repl = post_df[post_df["in_reply_to_id"].isin(env["id"].values)]
hG = nx.DiGraph()
for idx in repl.index:
    row = repl.loc[idx]
  ❹ hG.add_edge(row["in_reply_to_screen_name"], row["user_screen_name"])
❺ hub_scores, auth_scores = nx.hits(hG, max_iter=1000, tol=0.01)
```

列表 5-8：构建基于主题的子图并运行 HITS 算法

我们首先将帖子文本转换为小写 ❶（这样我们可以进行不区分大小写的匹配），然后使用内置的 pandas `contains` 函数根据文本内容查找相关行，检索所有包含相关根词的帖子 ❷。这还会匹配如 environment*al*、environment*alist* 等词。我们使用每一行的帖子 ID 提取相关帖子的回复集合 ❸。然后，我们遍历每一条回复，并在结果子图中创建一个有向边，表示相关主题的影响流 ❹。

最后，我们使用得到的子图计算 HITS 中心和权威得分❺。传递给`networkx.hits`函数的`max_iter`参数（NetworkX 核心库的一部分）控制算法在代码未收敛到解时最大迭代次数（有关 HITS 算法如何收敛的描述，请参见 NetworkX 文档）。`tol`参数控制用于检查收敛性的误差容忍度。如果算法未能在容忍度和最大迭代次数内收敛到答案，则会引发`PowerIterationFailedConvergence`异常。

算法从假设所有节点的中心得分和权威得分都为 1 开始。在每个后续步骤中，它计算两个更新规则：

**更新权威得分**

更新每个节点的权威得分，使其等于指向它的每个节点的中心得分之和。也就是说，通过转发被认为是信息中心的用户的消息，某个节点会获得更高的权威得分。其表示为：

![](img/m05001.png)

其中 *n* 是指向 *u* 的传入引用数量，*v* 是 *i*th 边的对端节点。

**更新中心得分**

更新每个节点的中心得分，使其等于指向它的每个节点的权威得分之和。在我们的示例中，通过写作被被认为是该主题权威的节点转发的帖子，某个节点会获得较高的中心得分。其表示为：

![](img/m05002.png)

其中 *n* 是 *u* 的外向引用数量，*v* 是 *i*th 边的对端节点。

现在你可以从给定主题的角度重新审视第二和第三个研究问题。例如，“环境主题的前三个中心节点是谁？”以及“政治主题的前三个权威节点是谁？”图 5-4 中的主题子图展示了我们样本数据的结果。

![](img/f05004.png)

图 5-4：环境和政治主题子图示例

与其标记节点，我使用了`nx.spring_layout`函数来直观地绘制两个主题的影响结构。根据文档说明，

> [Spring 布局]算法模拟了一个基于力的网络表示，将边视为弹簧将节点拉近，同时将节点视为排斥物体，有时称为反重力力。仿真将持续进行，直到位置接近平衡。

这样做的效果是，依据其他节点的相对连接性，高度连接的节点会更倾向于向中心移动。靠近中心的节点对更多用户产生了影响，因此其他节点被推得更远。你可以看到右侧的政治图（图 5-4）中，图的边缘有更多小的聚集影响，只有少数节点显示出比其他节点更大的影响力。而左侧的环境图则显示了一个明显的有影响力的用户位于中心，然后是几组较小的本地影响力集群分布在边缘。在使用`spring_layout`函数时，请记住初始位置是随机的，因此生成的图是随机的（stochastic）。重新运行代码可能会导致不同的可视化布局，但最具影响力的节点总会将其他节点推得比影响力较小的节点更远。

运行 HITS 算法后，你应该会发现环境领域的前三个枢纽（按枢纽得分降序排列）是`williamclarke`、`victoria73`和`nromero`。政治领域的前三个权威（同样按权威得分降序排列）是`wernerbrianna`、`trivera`和`susanjohnson`。请记住，HITS 算法生成的得分仅与主题子集相关。一个在“宠物食品”领域具有高权威得分的节点，在“编程”领域的得分可能会有所不同。

在本章开始时，我提到过社交网络连接和影响力如何被用来预测恶意内容的传播；这就是你第一个真正可以使用的方法。许多恶意软件通过社交网络消息附件传播。一旦你在网络中识别出恶意消息（并提取一些有用的主题信息），你可以利用 HITS 算法预测哪些用户更可能响应该消息。通过这样做，你可以按重要性降序处理风险。一个现实世界的例子发生在我修订本章时。在 COVID-19 大流行的恐慌高峰期，攻击者利用一个被感染的跟踪地图欺骗关注的用户访问恶意网站。一旦这个消息被披露（[`krebsonsecurity.com/2020/03/live-coronavirus-map-used-to-spread-malware`](https://krebsonsecurity.com/2020/03/live-coronavirus-map-used-to-spread-malware)），安全团队使用 HITS 算法追踪哪些用户（如果有的话）可能受到了影响。

### 网络组织分析

我们想要回答的最终问题——“谁能引入最多的新连接？”——虽然有些复杂，但仍然非常重要。研究人员和分析师在分析网络组织时，通常会使用这类信息，网络的范围从街头帮派到军事营地——任何地方，个体可能不会直接互动，但会有一些共同的监管“上层”负责。例如，A 单位的一名士兵可能会把敌军部队的动向信息传递给单位指挥官，后者再将信息转发给基地指挥官。基地指挥官随时与几位不同单位的指挥官保持联系，可能会将信息发送给 B 单位的另一位指挥官，该指挥官随后将采取行动拦截敌军。在美国，这种指挥链体现了一个可以追溯到总统办公室（作为总司令）一直到每个新兵的节点祖先结构。通过检查哪些节点能够促进大量当前未连接的节点之间的连接，你可以开始理解每个人在等级结构中的重要性。

图 5-5 展示了一个你可能更熟悉的树形结构示例——公司组织图。

![](img/f05005.png)

图 5-5：来自组织图的示例树

这棵树的根是位于顶端的 CEO，在他下面是三位直接向他汇报的经理。每位经理下方是构成其团队的下属。理解社会结构中的影响力对规划（或规避）与人类互动相关的安全控制至关重要，例如社交工程；社交工程师直观地利用这一概念来获得其他员工的信任。简单来说，如果你能说服一位有影响力的人介绍你，你就能绕过大多数阻力。当然，如果分公司经理能够做你所需的介绍，你是不会直接联系大型公司的 CEO 的。你和你希望被介绍的对象之间的第一个共同节点就是*最低公共祖先（LCA）*。

为了确定 LCA（最低公共祖先），我们首先需要定义*节点祖先*，它与树的关系（而不是家谱）。在图论中，树是一种特殊类型的图结构，其中任意两个节点之间有且只有一条路径（[`mathworld.wolfram.com/Tree.html`](https://mathworld.wolfram.com/Tree.html)）。树的起始节点是根节点；子节点称为分支节点，除非某个子分支没有自己的分支节点（即死胡同），在这种情况下，它被称为叶节点。

根据定义，简单图没有方向性和环。*多重树*将简单图的概念扩展到包括方向性，从而形成*有向无环图（DAG）*。这个看似简单的变化赋予了图结构许多有趣的属性。例如，DAG 具有*拓扑排序*：节点按顺序排列，使得根节点的值低于叶节点。DAG 是所有图结构中研究最深入的之一，因为它们在自然界中频繁出现。从树木和植物的分支、人体中的血管到河流，再到大多数计算机程序的结构，DAG 可以表示大量的自然和人工系统。在我们的案例中，使用 DAG 来表示节点之间的关系，将使我们能够在社交网络中编码一个成员等级结构。

任意多重树的节点祖先在概念和结构上类似于家谱树。然而，节点的顺序依赖于 DAG 的拓扑排序，而不是严格的时间顺序。最有影响力的用户是那些具有一定出度且没有入度的节点（如`dannyhoover`，比其他用户更有影响力），这些用户形成了不同树的根节点。每个受影响的节点成为树中的一条分支。对于每个分支节点，出度边再次作为分支添加。分支继续，直到所有节点都被放置。这样就形成了具有零出度和一定入度的叶节点（这些用户是最受网络中其他成员影响的）。以这种方式对节点进行排序，可以帮助你了解影响的流动方向。

从形式上讲，节点*u*的祖先是任何其他节点*v*，使得在图中从*v*到*u*存在有向路径，或者用更代数的方式写作：

祖先(u) = ( u ← v ) ∈ E

两个节点（*u*，*v*）的*公共祖先*是那些具有指向*u*和*v*的有向路径的节点*x*，这些节点属于边集的交集：

CommAnc( u ∧ v ) = ( x → u ∈ E ) ∪ ( x → v ∈ E )

两个节点（*u*，*v*）的 LCA（最近公共祖先）是指从两个节点到公共祖先的最短路径距离的节点，这也是从图的根节点到该祖先的最大路径长度。例如，你和你的表兄妹有一些相同的曾祖父母。然而，你们也有一些相同的祖父母。虽然曾祖父母和祖父母都是你的祖先，但由于祖父母距离你这一代更近，所以他们是你的 LCA。图 5-6 展示了同一棵树上祖先的两个例子。

![](img/f05006.png)

图 5-6：一般的祖先示意图

在每个树中，带有虚线轮廓的阴影节点是另外两个阴影节点的 LCA。在左侧，节点*D*和*E*的 LCA 是树的根节点*A*。在右侧，虽然节点*A*仍然是一个共同的祖先，但节点*B*离根节点更远，因此是 LCA。用信息安全的角度来思考，两个节点的 LCA 是它们之间最近的潜在枢纽点。如果节点*G*的用户想要认识节点*E*的用户，他们可以请求节点*B*的用户进行介绍。在清单 5-9 中，我们统计了每个节点作为其他节点对的 LCA 出现的次数。

```
❶ ancestors = list(nx.all_pairs_lowest_common_ancestor(G))
pred_count = {}
for p, lca in ancestors:
  ❷ if p not in G.edges():
        if lca in pred_count.keys():
          ❸ pred_count[lca] += 1
        else:
          ❹ pred_count[lca] = 1
sorted_pred = sorted(pred_count.items(), key=lambda kv: kv[1], reverse=True)

for k in sorted_pred[0:5]:
    print("%s can bridge %d new connection" % (k[0], k[1]))
```

清单 5-9：计算所有节点的 LCA 出现次数

首先，我们使用 NetworkX 函数`nx.all_pairs_lowest_common_ancestor`❶生成祖先列表，该函数返回一个字典，其中键是图中的一对节点，值是该节点对的 LCA 节点。填充好`ancestors`列表后，我们使用`for`循环将节点对赋值给变量`p`，并将结果的祖先赋值给变量`lca`，以便计算`lca`可以连接多少个节点。我们忽略有边连接的节点对，因为其中一个节点是另一个节点的直接祖先❷。例如，图 5-6 中的节点对*B*和*E*可以忽略，尽管 NetworkX 函数生成了该对节点的 LCA。对于每个没有直接边连接的节点对，我们检查其`lca`是否在`pred_count`字典中，该字典统计了节点作为其他两个节点的 LCA 出现的次数。如果 LCA 节点已经在字典中，则将计数加 1❸。否则，创建一个新条目，值为`1`❹。运行此代码后，将打印出前五个用户以及他们可以潜在连接的节点数，如清单 5-10 所示。

```
georgejohnson can bridge 444 new connection
dannyhoover can bridge 444 new connection
vkhan can bridge 372 new connection
judith20 can bridge 336 new connection
david49 can bridge 216 new connection
```

清单 5-10：LCA 分析的结果

根用户`dannyhoover`排名并列第一，且可以在网络中潜在地建立 444 个新连接。由于我们已经认为该用户非常有影响力，这个结果并不意外。由于他们位于树的根部，这也意味着他们是所有节点对的最后一个 LCA（最近公共祖先），如果没有找到其他祖先。因此，这个结果可能不像第二和第三名那么有趣。用户`georgejohnson`和`dannyhoover`获得相同的分数这一点很有意思，可能表明数据中有两个值得研究的结构。

用户`judith20`可以连接 336 个节点。作为练习，检查该用户如何融入树形结构中。谁影响了他们的活动（入度边）？他们又影响了谁（出度边）？他们在哪些中心性指标上得分最高？

## 概念验证：社交网络分析

本章的概念验证代码位于书籍资源中的*social_network/post_graph.py*文件，它允许你将个人时间线的帖子数据捕获为 JSON 数据，您可以使用本书中展示的方法进行分析。

你需要在选择的 Mastodon 实例上注册一个账户（我使用的是 defcon.social）。然后，你需要为自己的 API 凭证注册一个应用程序（[`docs.joinmastodon.org/client/token`](https://docs.joinmastodon.org/client/token)）。注册应用程序后，你将获得一个 API 令牌和 API 令牌密钥，用于标识你的账户下的特定应用程序，并授予访问授权功能（如点赞帖子和关注用户）的权限。根据你选择的 Mastodon 实例，可能需要回答一些问题以符合不同的使用场景；否则，你只需在创建令牌时定义访问令牌的范围。许多 Mastodon 实例对研究人员很友好，只要你计划保护个人数据的隐私。

你将获得一个唯一的 API 密钥，用于标识你的 API 账户与 Mastodon 实例配对，并获得一个 API 密钥秘密，它应像其他加密密钥一样受到保护。

注册后，你可以通过 Python Mastodon 库使用 API 抓取自己的时间线。请参考 Mastodon 库文档（[`mastodonpy.readthedocs.io/en/stable`](https://mastodonpy.readthedocs.io/en/stable)）和 Mastodon API 文档（[`docs.joinmastodon.org/api`](https://docs.joinmastodon.org/api)）了解可以获取的数据以及如何使用此库访问这些数据。

列表 5-11 展示了概念验证代码。

```
❶ from mastodon import Mastodon
import pandas as pd

❷ ACCESS_TOKEN = `"YOUR-TOKEN-HERE"`
BASE_URL = "https://defcon.social"
❸ m = Mastodon(access_token=ACCESS_TOKEN, api_base_url=BASE_URL)

❹ timeline_data = m.timeline(timeline="public")

df = pd.DataFrame(timeline_data)
df["id"] = df["id"].astype(dtype=str)
df["in_reply_to_id"] = df["in_reply_to_id"].astype(dtype=str)
df["in_reply_to_account_id"] = df["in_reply_to_account_id"].astype(dtype=str)

print(df.info())
❺ df.to_csv("mastodon_timeline.csv")
```

列表 5-11：将 Mastodon 公共时间线数据捕获到 CSV 文件

首先，我们导入`mastodon`库❶。获取 API 凭证后，我们通过访问令牌和基本 URL 修改模板文件❷，并从终端运行它。代码使用这些凭证创建一个经过身份验证的 API 对象❸，该对象用于获取时间线数据❹，这些数据方便地以字典格式提供，适合进行 JSON 编码。我们遍历这些结果并将它们写入输出的 CSV 文件❺。

现在，你可以使用类似于列表 5-2 和 5-4 中的代码将数据读取回 pandas `DataFrame`，然后将其塑造成重要特征，最后使用 NetworkX 构建相关的有向（或无向）图。你还可以通过将这段代码与数据处理管道结合起来，在几乎实时的情况下分析状态信息，从而绕过写入中间文件的过程。我们将在第三部分中讨论处理管道。

## 社交网络分析的阴暗面

希望现在你对别人如何快速且轻松地建立社会生活地图有了概念。需要记住的一个重要点是，像地图一样，社交网络图需要解读。当我们解读社交网络信息时，我们不可避免地是通过自己的社会偏见来观察数据。问题的核心是，我们试图将一个高度复杂、多面性的难题——比如人们互动背后的动机——简化为一个严格控制且明确定义的数学模型。为了做到这一点，我们必须基于自己的社会经验应用选择的启发式方法。例如，我之前提到过，你可能会希望将已婚夫妇之间的互动加权比同事之间的互动更高。这展示了我自己的启发式偏见，它源于我的经验、教育和理解，但不一定反映每个人的实际情况。在构建 SNA 模型时，你需要做出许多这样的假设，理解你在何时、何地以及多少程度上允许自己的偏见影响分析至关重要。这也是我推荐与团队一起进行 SNA 的主要原因之一。同行评审，尤其是来自不同背景的同行评审，是解决单一视角解释带来的问题的最佳方法之一。

我推荐保持谨慎的另一个原因是，社会网络分析（SNA）会引发道德和伦理问题。它或许是应用数学在安全领域中的“黑暗艺术”，主要是因为当它被滥用时，可能会产生非常真实且危险的后果。SNA 曾被暴政政府用来攻击异见者、威胁举报人以及操控社会民众。不幸的是，并非所有伦理上有疑问的 SNA 使用方式都容易被发现。有一些工具和网站专门设计用来更容易收集某人的公开（有时是私人）信息。我们生活在一个不断努力平衡隐私与开放的世界中。小世界实验可以用来将电影与凯文·贝肯（Kevin Bacon）联系起来，也可以将我们每个人与任何数量的犯罪人物和组织联系起来。作为分析师，你有责任理解什么是伦理上和道德上合适的。

## 摘要

尽管本章使用 Mastodon 作为示例来展示社交网络分析的概念，但这些概念并不固有地与 Mastodon 平台绑定。美国政府和大学研究人员一直在开发不同的技术，通过分析暗网论坛中讨论的回复网络结构来获取信息，以了解暗网信息在多大程度上可以帮助预测现实世界中的网络攻击。^(3) 在他为美国海军撰写的论文《使用社交网络分析追踪、破坏和干扰暗网络》中，Sean Everton 介绍了 SNA 作为一种追踪和破坏犯罪及恐怖分子网络的策略手段。^(4) 这篇论文既是战术性的，也是战略性的入门，我强烈推荐阅读。

当你将自己的社交网络分析（SNA）扩展到实际应用时，你需要参考你所使用的社交网络的 API 文档。如果没有 API（或者平台开始收取高额费用），你可能需要诉诸传统的网页抓取技术来收集所需的数据。这类任务超出了本书的范围，但有许多优秀的材料可以帮助你实现这一点。

到目前为止，我们使用图分析的所有内容都集中在过去。你可以把它看作是*描述性*安全分析，因为它旨在将事物分类为现在的状态（或者数据捕获时的状态）。然而，*预防性*安全分析则试图分析未来可能发生的事件，从而希望我们能够提前介入，防止安全事件的发生。为了实现这一目标，我们将使用我最喜欢的模拟算法之一——蒙特卡罗模拟，这是下一章的内容。
