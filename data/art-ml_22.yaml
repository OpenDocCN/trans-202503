- en: '**D'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**D'
- en: 'PITFALL: BEWARE OF “P-HACKING”!**'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 陷阱：小心“P-HACKING”！**
- en: In recent years there has been much concern over something that has acquired
    the name *p-hacking*. Though such issues have always been known and discussed,
    things really came to a head with the publication of John Ioannidis’s highly provocatively
    titled paper, “Why Most Published Research Findings Are False” (*PLOS Medicine*,
    August 30, 2005). One aspect of this controversy can be described as follows.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，人们对被称为*p-hacking*的问题表示了极大的关注。尽管这些问题一直为人所知并被讨论，但在约翰·伊奥安尼迪斯（John Ioannidis）那篇标题极具挑衅性的论文《为什么大多数已发布的研究结果是错误的》（*PLOS
    Medicine*，2005年8月30日）发表后，事情真正到了一个高潮。这个争议的一个方面可以这样描述。
- en: Say we have 250 coins, and we suspect that some are unbalanced. (Any coin is
    unbalanced to at least some degree, but let’s put that aside.) We toss each coin
    100 times, and if a coin yields fewer than 40 or more than 60 heads, we will decide
    that it’s unbalanced. For those who know some statistics, this range was chosen
    so that a balanced coin would have only a 5 percent chance of straying more than
    10 heads away from 50 out of 100\. So, while this chance is only 5 percent for
    each particular coin, with 250 coins, the chances are high that at least one of
    them falls outside that [40,60] range, *even if none of the coins is unbalanced*.
    We will falsely declare some coins unbalanced. In reality, it was just a random
    accident that those coins look unbalanced.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有250枚硬币，并且怀疑其中一些是偏的。（任何硬币都至少会有某种程度的不平衡，但我们暂且不提这个。）我们抛每一枚硬币100次，如果一枚硬币出现的正面少于40次或多于60次，我们就认为它是不平衡的。对于懂一些统计学的人来说，这个范围的选择是为了确保一枚平衡硬币有5%的概率出现超过50次正面偏差超过10次。所以，虽然每枚硬币的这个概率只有5%，但有250枚硬币时，至少有一枚硬币很可能会落在[40,60]范围之外，*即使没有一枚硬币是不平衡的*。我们会错误地宣称一些硬币是不平衡的。实际上，这只是巧合，那些硬币看起来不平衡。
- en: 'Or, to give a somewhat frivolous example that still will make the point, say
    we are investigating whether there is any genetic component to sense of humor.
    Is there a humor gene? There are many, many genes to consider— many more than
    250, actually. Testing each one for relation to sense of humor is like checking
    each coin for being unbalanced: even if there is no humor gene, eventually just
    by accident we’ll stumble upon one that seems to be related to humor.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，举个稍微轻松一点的例子，但仍能说明问题，假设我们正在研究幽默感是否与遗传有关。有没有幽默基因？有许多许多基因需要考虑——实际上超过了250个。测试每一个基因与幽默感的关系，就像检查每一枚硬币是否不平衡：即使没有幽默基因，最终我们也可能偶然发现一个看似与幽默相关的基因。
- en: In a complex scientific study, the analyst is testing many genes, or many risk
    factors for cancer, or many exoplanets for the possibility of life, or many economic
    inflation factors, and so on. The term *p-hacking* means that the analyst looks
    at so many different factors that one is likely to emerge as “statistically significant”
    even if no factor has any true impact. A common joke is that the analyst “beats
    the data until they confess,” alluding to a researcher testing so many factors
    that one finally comes out “significant.”
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在一项复杂的科学研究中，分析员测试了许多基因，或者许多与癌症相关的风险因素，或者许多外星行星是否有生命的可能，或者许多经济通胀因素，等等。术语*p-hacking*指的是分析员考虑了如此多的不同因素，以至于其中一个很可能会被认为是“统计学显著”的，即使没有任何因素对结果有真正的影响。一句常见的笑话是，分析员“逼迫数据直到它们承认”，暗指研究人员测试了太多的因素，最终有一个因素会被判定为“显著”。
- en: 'Cassie Kozyrkov, head of decision intelligence at Google, said it quite well:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌决策智能部门负责人Cassie Kozyrkov说得很好：
- en: What the mind does with inkblots, it also does with data. Complex datasets practically
    beg you to find false meaning in them.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 人类对墨迹的处理方式，正如同它处理数据的方式一样。复杂的数据集几乎在要求你在其中找到虚假的意义。
- en: '*This has major implications for ML analysis.* For instance, a popular thing
    in the ML community is to have competitions in which many analysts try their own
    tweaks on ML methods to outdo each other on a certain dataset. Typically these
    are classification problems, and “winning” means getting the lowest rate of misclassification.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*这对机器学习分析有重大影响。* 例如，机器学习社区中有一种流行的做法，就是举办竞赛，许多分析员对机器学习方法进行调整，试图在某一数据集上超越对方。通常这些是分类问题，“获胜”意味着获得最低的错误分类率。'
- en: The trouble is, having 250 ML analysts attacking the same dataset is like having
    250 coins in our example above. Even if the 250 methods they try are all equally
    effective, one of them will emerge by accident as the victor, and it will be annointed
    as a “technological advance.”
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，拥有250个机器学习分析师攻克同一数据集，就像我们上面举例的250个硬币。即使他们尝试的250种方法都同样有效，其中某一个方法也会偶然成为赢家，并被誉为“技术进步”。
- en: Of course, it may well be that one of the 250 methods really is superior. But
    without careful statistical analysis of the 250 data points, it is not clear what’s
    real and what’s just accident. Note, too, that even if one of the 250 methods
    is in fact superior, there is a high probability that it won’t be the winner in
    the competition, again due to random variation.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，可能确实有250种方法中的某一种是优越的。但如果没有对这250个数据点进行仔细的统计分析，就无法确定什么是真实的，什么只是偶然的。还要注意，即使250种方法中确实有一种优越的方法，由于随机变异，它很可能不会在比赛中获胜。
- en: The problem is exacerbated by the fact that a contestant will probably not even
    submit his entry if it appears unlikely to set a new record. This further biases
    the results.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 问题的严重性在于，参赛者很可能不会提交自己的作品，如果看起来不太可能创造新的记录。这进一步加剧了结果的偏差。
- en: As mentioned, this concept is second nature to statisticians, but it is seldom
    mentioned in ML circles. An exception is the blog post “AI Competitions Don’t
    Produce Useful Models” by Lauren Oakden-Rayner, whose excellent graphic is reproduced
    in [Figure D-1](app04.xhtml#appdfig1) with Dr. Oakden-Rayner’s permission.^([1](footnote.xhtml#appdfn1))
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，这个概念对统计学家来说是第二天性，但在机器学习圈子中很少提及。一个例外是Lauren Oakden-Rayner的博客文章《AI竞赛无法产生有用的模型》，她的精彩图表在[图D-1](app04.xhtml#appdfig1)中得到再现，并且得到了Oakden-Rayner博士的许可。^([1](footnote.xhtml#appdfn1))
- en: '![Image](../images/app04fig01.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/app04fig01.jpg)'
- en: '*Figure D-1: AI p-hacking*'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*图D-1：AI p-hacking*'
- en: Rayner uses a simple statistical power analysis to analyze ImageNet, a contest
    in ML image classification. He reckons that at least those “new records” starting
    in 2014 are overfitting, or just noise. With more sophisticated statistical tools,
    a more refined analysis could be done, but the principle is clear.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Rayner使用简单的统计功效分析来分析ImageNet，这是一个机器学习图像分类比赛。他认为，至少从2014年开始的那些“新记录”都是过拟合，或者只是噪声。如果使用更复杂的统计工具，可以做更精细的分析，但原理是明确的。
- en: This also has a big implication for the setting of tuning parameters. Let’s
    say we have four tuning parameters in an ML method, and we try 10 values of each.
    That’s 10⁴ = 10000 possible combinations, a lot more than 250! So again, what
    seems to be the “best” setting for the tuning parameters may be illusory.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这对调优参数的设定也有很大影响。假设我们在一个机器学习方法中有四个调优参数，并且每个参数尝试10个不同的值。那就有10⁴ = 10000种可能的组合，比250多得多！所以，再次强调，看似“最佳”的调优参数设定可能是虚幻的。
- en: The `regtools` function `fineTuning()` takes steps to counter the possibility
    of p-hacking in searches for the best tuning parameter combination.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '`regtools`函数`fineTuning()`采取措施以应对在搜索最佳调优参数组合时可能出现的p-hacking问题。'
