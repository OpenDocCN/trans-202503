- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Information Theory
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 信息理论
- en: '![](Images/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/chapterart.png)'
- en: In this chapter we look at the basics of *information theory*. This is a relatively
    new field of study, introduced to the world in 1948 in a groundbreaking paper,
    which laid the foundation for technologies from modern computers and satellites
    to cell phones and the internet (Shannon 1948). The goal of the original theory
    was to find the most efficient way to communicate a message electronically. But
    the ideas of that theory are deep, broad, and profound. They give us tools for
    measuring how much we know about anything by converting it to a digital form that
    we can study and manipulate.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨*信息理论*的基础知识。这是一个相对较新的研究领域，于1948年通过一篇开创性的论文向世界介绍，为现代计算机、卫星、手机和互联网等技术奠定了基础（Shannon
    1948）。原始理论的目标是找到最有效的方式来电子化传递信息。但该理论的思想深刻、广泛且深远。它为我们提供了测量我们了解某事物的程度的工具，通过将信息转换为我们可以研究和操作的数字形式。
- en: Terms and ideas from information theory form part of the bedrock of deep learning.
    For example, the measurements provided by information theory are useful when we
    evaluate the performance of deep networks. In this chapter, we take a fast tour
    through some of the basics of information theory, while staying free of abstract
    mathematical notation.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 信息理论中的术语和概念构成了深度学习的基石。例如，信息理论提供的度量方法在评估深度网络的性能时非常有用。在本章中，我们将快速浏览信息理论的一些基本概念，同时避免使用抽象的数学符号。
- en: Let’s begin with the word *information*, one of those words that has both an
    everyday meaning and a specialized, scientific meaning. In this case, the meanings
    share a lot conceptual overlap, but while the popular meaning is broad and open
    to personal interpretation, the scientific meaning is precise and defined mathematically.
    Let’s start out by building up to the scientific definition of information, and
    ultimately work our way up to an important measurement that lets us compare two
    probability distributions.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从单词*information*（信息）开始，这是一个既有日常意义，又有专业科学意义的词。在这种情况下，这两种意义有很多概念上的重叠，但日常意义广泛且开放，容易受到个人理解的影响，而科学意义则是精确的，并且通过数学定义的。我们从逐步构建信息的科学定义开始，最终我们将上升到一个重要的度量标准，允许我们比较两个概率分布。
- en: Surprise and Context
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 惊讶与语境
- en: When we receive a communication of any kind, something moved from one place
    to another, whether it was an electrical pulse, some photons of light, or the
    sound of someone’s voice. Speaking broadly, we could say that a *sender* somehow
    transfers some kind of communication to a *receiver*. Let’s introduce some more
    specialized vocabulary.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们收到任何形式的通信时，某些东西从一个地方移动到另一个地方，无论是电脉冲、一些光子，还是某人的声音。广义地说，我们可以说一个*发送者*通过某种方式将某种信息传递给*接收者*。接下来，我们将介绍一些更专业的词汇。
- en: Understanding Surprise
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解惊讶
- en: In this chapter, we sometimes use the term *surprise* to represent how unexpected
    a sender’s communication is to a receiver. Surprise isn’t a formal term. In fact,
    one of our goals in this chapter is to find more formal names for surprise and
    attach specific meanings and measures to them.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们有时使用*惊讶*一词来表示发送者的通信对接收者来说有多么出乎意料。惊讶并不是一个正式术语。事实上，本章的目标之一是为惊讶找到更正式的名称，并附上具体的含义和度量标准。
- en: Let’s suppose that we’re on the receiving end of a message. We want to describe
    how surprised we are by the communication we receive. Being able to do so is useful
    because, as we’ll see, the greater the surprise, the greater the amount of information
    that was delivered.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们是接收者，收到一条消息。我们想描述收到这条信息后我们有多惊讶。能够这样描述很有用，因为正如我们所看到的，惊讶越大，传递的信息量就越大。
- en: Suppose we get an unexpected text message from an unknown number. We open it
    up and the first word is Thanks. How surprised are we? Surely we are at least
    a little surprised, because so far, we don’t know who the message is from or what
    it’s about. But receiving a text thanking us for something does happen, so it’s
    not unheard of.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们收到了一条来自未知号码的意外短信。我们打开短信，第一句话是“Thanks”（谢谢）。我们有多惊讶呢？肯定至少有些惊讶，因为到目前为止，我们不知道这条短信是来自谁，也不知道它的内容是什么。不过，收到感谢我们的短信确实是有可能的，所以这并不是什么闻所未闻的事。
- en: Let’s make up an imaginary and completely subjective surprise scale, where 0
    means something is completely expected, and 100 means it’s a total surprise, as
    in [Figure 6-1](#figure6-1).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来设定一个假想的、完全主观的惊讶量表，其中0表示完全预期的事件，100表示完全出乎意料的事件，就像在[图6-1](#figure6-1)中所示。
- en: '![f06001](Images/f06001.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![f06001](Images/f06001.png)'
- en: 'Figure 6-1: The surprise scale, expressed as a value from 0 to 100'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图6-1：惊讶度量表，表示从0到100的值
- en: On this scale, the word Thanks at the start of an unexpected text message might
    rank a 20\. Now suppose that the first word in our message isn’t Thanks, but instead
    is Hippopotamus. Unless we’re working with those animals or are otherwise involved
    with them, that’s likely to be a rather surprising first word of a message. Let’s
    rank this word at an 80 on the surprise scale, as in [Figure 6-2](#figure6-2).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个量表上，"Thanks"（谢谢）出现在一条意外的文本消息开头时，可能排在20的位置。现在假设我们消息的第一个词不是"Thanks"，而是"Hippopotamus"（河马）。除非我们在处理这些动物或与它们有其他关联，否则这可能是消息中的一个相当令人惊讶的词。我们将这个词的惊讶度定为80，正如图[6-2](#figure6-2)所示。
- en: '![f06002](Images/f06002.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![f06002](Images/f06002.png)'
- en: 'Figure 6-2: Placing messages on our surprise scale'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图6-2：将消息放置在我们的惊讶度量表上
- en: Although hippopotamus might be a big surprise at the start of a message, it
    might not be surprising later on. The difference is context.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管"hippopotamus"（河马）可能在消息开头给人一种很大的惊讶感，但在后续的语境中它可能就不那么令人惊讶了。区别就在于语境。
- en: Unpacking Context
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 拆解语境
- en: For our purposes, we can think of *context* as the environment of the message.
    Since we’re focusing on the meaning of each message, rather than the physical
    way it’s communicated, the context represents the shared knowledge between the
    sender and receiver, which gives the message meaning.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的目的来说，我们可以将*语境*视为信息的环境。由于我们关注的是每条消息的意义，而不是它的传递方式，语境代表了发送者和接收者之间的共享知识，正是这些知识赋予了消息意义。
- en: When the message is a piece of language, this shared knowledge must include
    the words used, since a message of Kxnfq rnggw would carry no meaning. We can
    extend that shared knowledge to include grammar, current interpretations of emoticons
    and abbreviations, shared cultural influences, and so on. This is all called *global
    context*. It’s the general knowledge that we bring to any message, even before
    we’ve read it. In terms of our Bayes’ Rule discussion of Chapter 4, some of this
    global context is captured in our *prior*, since that is how we represent our
    understanding of the environment and what we expect to learn from it.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当信息是一段语言时，这种共享知识必须包括所使用的词汇，因为像Kxnfq rnggw这样的消息是没有意义的。我们可以将这种共享知识扩展到包括语法、表情符号和缩写的当前解释、共享的文化影响等等。这一切都被称为*全球语境*。它是我们在接收任何信息之前就已经带入的普遍知识。就像在第4章讨论的贝叶斯定理一样，部分全球语境被捕捉到我们的*先验*中，因为这就是我们表示对环境的理解以及我们期望从中学到什么的方式。
- en: In contrast to the global context, there is also *local context*. That’s the
    environment composed of the elements of the message itself. In a text message,
    the local context for any given word is the other words in that message.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 与全球语境相对的是*局部语境*。那是由消息本身的元素构成的环境。在一条文本消息中，任何给定单词的局部语境就是该消息中的其他单词。
- en: Let’s imagine that we’re reading a message for the first time, so each word’s
    local context is made up only of the words that preceded it. We can use the context
    to get a handle on surprise. If Hippopotamus is the first word of our message,
    then there is no local context yet, only the global. And if we don’t work with
    hippopotamuses on a regular basis, that word is likely very surprising. But if
    the message begins with, Let’s go down to the river area at the zoo and maybe
    see a big gray, then in that context, the word hippopotamus isn’t very surprising.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们第一次阅读这条消息，因此每个单词的局部语境仅由它前面的单词构成。我们可以利用语境来判断惊讶感。如果"Hippopotamus"是我们消息中的第一个词，那么此时没有局部语境，只有全球语境。如果我们不经常接触河马，这个词可能非常令人惊讶。但是，如果消息的开头是"Let's
    go down to the river area at the zoo and maybe see a big gray"（让我们去动物园的河边，或许能看到一只大灰色的动物），那么在这个语境下，"hippopotamus"（河马）就不再令人惊讶了。
- en: We can describe the amount of surprise carried by a specific word in our global
    context by assigning it a surprise value, as we did in [Figure 6-1](#figure6-1).
    Suppose that we assign a surprise value to every word in the dictionary (a tedious
    job, but certainly possible). If we scale these numbers so that they all add up
    to 1, we’ve created a probability mass function (or pmf), as we discussed in Chapter
    2\. That means we can draw a random variable from that pmf to get a word, with
    the most surprising words coming along more frequently than the less surprising
    words. A more common approach is to set up the pmf to represent how common a word
    is, which is roughly the opposite of surprise. With that setup, we’d expect to
    draw the least surprising, or more common, words more frequently than uncommon
    words.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过为特定单词分配一个惊讶值，来描述在全球语境中该单词所携带的惊讶感，正如我们在[图6-1](#figure6-1)中所做的那样。假设我们为字典中的每个单词分配一个惊讶值（这是一项繁琐的工作，但肯定是可行的）。如果我们将这些数字缩放，使它们的总和为1，就创建了一个概率质量函数（或pmf），正如我们在第二章中讨论的那样。这意味着我们可以从这个pmf中抽取一个随机变量来得到一个单词，其中最令人惊讶的单词比那些不太令人惊讶的单词更常出现。一种更常见的方法是建立一个pmf来表示单词的常见程度，这大致是惊讶感的对立面。在这种设置下，我们预计会更频繁地抽到那些不太令人惊讶或更常见的单词，而不是那些不常见的单词。
- en: We’ll use this idea later in the chapter to devise a scheme for transmitting
    the content of a message in an efficient manner.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章后面使用这个思想，设计一种高效的方案来传输信息的内容。
- en: Measuring Information
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测量信息
- en: In this chapter, we’re going to talk quite a lot about *bits*. In popular language,
    a bit is usually thought of as a little package of data, often labeled either
    0 or 1\. For instance, when we talk about internet speed in “bits per second,”
    we might picture the bits as leaves flowing down a river, and we count them as
    they go by.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将谈论很多关于*比特*的内容。在日常语言中，比特通常被认为是一个小的数据包，通常被标记为0或1。例如，当我们谈论“每秒比特数”的网络速度时，我们可能会把比特想象成河流中漂流的叶子，并在它们经过时进行计数。
- en: This is a convenient idea, but in technical language, a bit is not a thing,
    like a leaf, but a unit, like a gallon or a gram. That is, it isn’t a piece of
    stuff but a way to talk about how much stuff we have. A bit is a container that
    holds just enough storage for what we currently think is the fundamental, indivisible,
    smallest possible chunk of information.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个方便的概念，但从技术角度来说，比特并不是像树叶一样的物体，而是像加仑或克那样的单位。也就是说，它不是一种物质，而是一种衡量我们所拥有的物质多少的方式。比特是一个容器，刚好能够存储我们当前认为的基本、不可分割的最小信息单元。
- en: Speaking of bits as units in this way is technically correct, but it’s inconvenient.
    And most of the time, we can speak casually without any confusion, like when we
    say, “My net connection is 8,000 bits per second,” rather than, “My net connection
    is able to transmit 8,000 bits worth of information per second.” We’ll use the
    more casual language in most of this book, but it’s worthwhile to know the technical
    definition, because it does pop up from time to time in papers and documentation
    where the distinction is important.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 从这种方式谈论比特作为单位在技术上是正确的，但这并不方便。大多数时候，我们可以不加澄清地使用通俗的说法，就像我们说“我的网速是每秒8,000比特”而不是“我的网连接每秒能够传输8,000比特的信息”。我们在本书的大部分内容中将使用更通俗的语言，但了解技术定义是值得的，因为它确实会出现在一些论文和文档中，而在那些地方区分开来是很重要的。
- en: We can measure the amount of information in a text message with a formula that
    tells us how many bits are needed to represent that message. We won’t get into
    the math, but we’ll describe what’s going on. The formula takes two inputs. The
    first is the text of the message. The second is a pmf that describes the surprise
    inherent in each word the message can contain (let’s just call this a *probability
    distribution* for the rest of this chapter). When we take the text of the message
    and the probability distribution together, we can produce a number that tells
    us how many bits of information the message carries.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过一个公式来衡量文本信息的量，这个公式告诉我们需要多少比特来表示该信息。我们不会深入探讨数学内容，但会描述其中的原理。这个公式有两个输入。第一个是信息的文本内容。第二个是描述信息中每个单词所包含的惊讶感的概率质量函数（在本章中，我们将其称为*概率分布*）。当我们将信息的文本内容和概率分布结合起来时，就可以得出一个数字，告诉我们这条信息所携带的比特数。
- en: The formula was designed so that the values it produces for each word (or, more
    generally, each *event*) have four key properties. We’ll illustrate each one using
    a context in which we work in an office, and not on a river.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 该公式的设计是为了使它为每个单词（或者更一般地，每个*事件*）所生成的值具备四个关键属性。我们将在一个我们在办公室工作而不是在河流上的情境中说明每一个属性。
- en: Likely events have low information. Stapler has low information.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可能发生的事件信息量较少。订书机的信息量较少。
- en: Unlikely events have high information. Crocodile has high information.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不太可能发生的事件包含更多信息。鳄鱼包含更多信息。
- en: Likely events have less information than unlikely events. Stapler conveys less
    information than crocodile.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可能发生的事件比不太可能发生的事件包含的信息更少。订书机传递的信息比鳄鱼少。
- en: Finally, the total information due to two *unrelated* events is the sum of their
    individual information values found separately.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，两个*不相关*事件的总信息量是它们各自信息值的和。
- en: The first three properties relate single objects to their information. The oddball
    in the group is property 4, so let’s look at it more carefully.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 前三个属性将单个对象与其信息相关联。这个组中的特例是属性 4，所以我们需要更仔细地看一下它。
- en: In normal conversation, it’s rare for two consecutive words to be completely
    unrelated. But suppose someone asked us for a “kumquat daffodil.” Those words
    are just about completely unrelated, so property 4 says that we could find the
    information in that phrase by adding the information communicated by each word
    independently.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在正常的对话中，两个连续的词完全不相关是很少见的。但假设有人要求我们给出一个“金桔水仙花”。这些词几乎完全不相关，因此属性 4 说明我们可以通过将每个词独立传递的信息相加来找到该短语的信息。
- en: In normal conversation, the words that lead up to any given word often narrow
    the possibilities of what it could be. If someone says, “Today I ate a big,” then
    words like “sandwich” and “pizza” arriving next carry less surprise than “bathtub”
    or “sailboat.” When words are expected, they produce less surprise than when they’re
    not. By contrast, suppose we’re sending a device’s serial number, which is essentially
    an arbitrary sequence of letters and perhaps numbers, like “C02NV91EFY14.” If
    the characters really have no relation to each other, then adding the surprise
    due to each character gives us the overall surprise in the entire message representing
    the serial number.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在正常的对话中，引导到某个给定单词的词汇通常会缩小它可能是什么的范围。如果有人说：“今天我吃了一个大...”，那么接下来的“三明治”和“比萨”这类词比“浴缸”或“帆船”产生的惊讶感要少。当某些词被预期时，它们比没有预期时产生的惊讶感要少。相比之下，假设我们正在传送一个设备的序列号，它本质上是一个任意的字母序列，可能还包含数字，比如“C02NV91EFY14”。如果这些字符之间真的没有关联，那么通过加上每个字符所带来的惊讶感，我们就能得到整个序列号所代表的信息。
- en: 'By combining the surprise of two unrelated words into the sum of their individual
    surprise values, we go from measuring the surprise, or information, in each of
    those words to the surprise in their combination. We can keep combining words
    this way into ever-larger groups until we’ve considered the entire message. Though
    we haven’t gone into the math, we have reached a formal definition of *information*:
    it’s a number produced from a formula that uses one or more events (such as words),
    and a probability distribution to describe how surprising each event would be
    to us. From those two inputs the algorithm provides a number for each event, and
    guarantees that those numbers satisfy the four properties we just listed. We call
    each word’s number its *entropy*, telling us how many bits are needed to communicate
    it.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将两个不相关词汇的惊讶感合并为它们各自惊讶值的和，我们从衡量每个词汇中的惊讶感或信息，转向衡量它们组合中的惊讶感。我们可以这样不断将词汇组合成越来越大的组，直到我们考虑完整个信息。虽然我们没有涉及数学，但我们已经得出了*信息*的正式定义：它是一个通过公式生成的数字，该公式使用一个或多个事件（如单词）和一个概率分布，来描述每个事件对我们来说有多么令人惊讶。通过这两个输入，算法为每个事件提供一个数字，并确保这些数字满足我们刚才列出的四个属性。我们称每个单词的数字为其*熵*，它告诉我们传递该单词需要多少比特。
- en: Adaptive Codes
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自适应编码
- en: The amount of information carried by each event is influenced by the size of
    the probability function we hand to our formula. In other words, the number of
    possible words we might communicate affects the amount of information carried
    by each word we send.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 每个事件所携带的信息量受我们公式中所使用的概率函数大小的影响。换句话说，我们可能传递的词汇数量会影响每个词所携带的信息量。
- en: Suppose we want to transmit the contents of a book from one place to another.
    We might list all the unique words in that book and then assign a number to each
    word, starting perhaps with 0 for the, then 1 for and, and so on. Then, if our
    recipient also has a copy of that word list, we can send the book just by sending
    the number for each word, starting with the first word in the book. The Dr. Seuss
    book *Green Eggs and Ham* contains only 50 different words (Seuss 1960). To represent
    a number between 0 and 49, we need six bits of information per word. By contrast,
    Robert Louis Stevenson’s book *Treasure Island* contains about 10,700 unique words
    (Stevenson 1883). We’d have to use 14 bits per word to uniquely identify each
    word in that book.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们要将一本书的内容从一个地方传送到另一个地方。我们可以列出书中所有独特的单词，然后为每个单词分配一个编号，可能从“the”对应0，接着是“and”对应1，依此类推。然后，如果接收方也有这份单词列表，我们可以仅通过发送每个单词的编号来传送这本书，从书中的第一个单词开始。Dr.
    Seuss的《Green Eggs and Ham》这本书只有50个不同的单词（Seuss 1960）。为了表示0到49之间的一个数字，每个单词我们需要6位信息。相比之下，Robert
    Louis Stevenson的《Treasure Island》这本书包含约10,700个独特的单词（Stevenson 1883）。我们必须为每个单词使用14位来唯一标识书中的每个单词。
- en: Although we could use one giant word list of all English words to send these
    books, it’s more efficient to tailor our list to each book’s individual vocabulary,
    including only the words we actually need. In other words, we can improve our
    efficiency by *adapting* our transmission of information to what’s being communicated.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们可以使用一个包含所有英语单词的大型单词列表来发送这些书籍，但更高效的做法是根据每本书的独特词汇量来定制我们的列表，仅包括我们实际需要的单词。换句话说，我们可以通过*调整*信息传输的方式，使其更符合传递的内容，从而提高效率。
- en: Let’s take that idea and run with it.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们基于这个想法进一步探讨。
- en: Speaking Morse
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 说摩尔斯电码
- en: A great example of adaptation is Morse code. In Morse code, each typographical
    character has an associated pattern of dots and dashes, separated by spaces, as
    shown in [Figure 6-3](#figure6-3).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一个很好的适应性例子是摩尔斯电码。在摩尔斯电码中，每个字母字符都有一个由点和划线组成的模式，点和划线之间由空格分隔，如[图 6-3](#figure6-3)所示。
- en: '![f06003](Images/f06003.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![f06003](Images/f06003.png)'
- en: 'Figure 6-3: Each character in Morse code has an associated pattern of dots,
    dashes, and spaces.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6-3：摩尔斯电码中的每个字符都有一个与之相关的点、划线和空格的模式。
- en: Morse code is traditionally sent by using a telegraph key to enable or disable
    transmission of a clear tone. A dot is a short burst of sound. The length of time
    we hold down the key to send a dot is represented by a unit called the *dit*.
    A dash is held for the duration of three dits. We leave one dit of silence between
    symbols, a silence of three dits between letters, and a silence of seven dits
    between words. These are of course ideal measures. In practice, many people can
    recognize the personal rhythm, called the *fist*, of each of their friends and
    colleagues (Longden 1987).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 摩尔斯电码通常通过使用电报键来发送，电报键用于启用或禁用清晰音调的传输。一个点是一个短促的音符。我们按下电报键发送一个点的时间长度由一个单位来表示，这个单位称为*dit*。一个划线持续三个dit的时间。符号之间留一个dit的静默，字母之间留三个dit的静默，单词之间留七个dit的静默。当然，这些是理想的标准。实际上，许多人可以辨认出每个朋友和同事个人的节奏，这种节奏叫做*fist*（Longden
    1987）。
- en: 'Morse code contains three types of symbols: dots, dashes, and dot-sized spaces.
    Let’s suppose we want to send the message “nice dog” in Morse code. [Figure 6-4](#figure6-4)
    shows the sequence of short tones (dots), long tones (dashes), and dot-sized spaces.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 摩尔斯电码包含三种符号：点、划线和点大小的空格。假设我们要用摩尔斯电码发送“nice dog”这条信息。[图 6-4](#figure6-4)显示了短音（点）、长音（划线）和点大小空格的序列。
- en: '![f06004](Images/f06004.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![f06004](Images/f06004.png)'
- en: 'Figure 6-4: The three symbols of Morse code: dots (solid circles), dashes (solid
    boxes), and silent spaces (empty circles).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6-4：摩尔斯电码的三种符号：点（实心圆）、划线（实心方块）和静默空格（空心圆）。
- en: We typically talk about Morse code strictly in terms of dots and dashes, which
    are called the *symbols*. The assigned set of symbols for any letter is that letter’s
    *pattern*. The length of time it takes to send a message depends on the specific
    patterns assigned to the letters that make up the message’s content. For example,
    even though the letters Q and H both have four symbols, Q requires 13 dits to
    send (3 for each of the 3 dashes, 1 for the dot, and 1 for each of the 3 spaces),
    while we need only 7 dits to send the letter H (4 dots, and 1 for each of the
    3 spaces).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常严格地用点和划来讨论摩尔斯电码，这些被称为*符号*。分配给任何字母的符号集就是该字母的*模式*。发送信息所需的时间取决于组成信息内容的字母所分配的具体模式。例如，尽管字母Q和H都有四个符号，但发送Q需要13个点（每个3个划的3个点，1个点，和每个3个空格的1个点），而发送字母H只需要7个点（4个点，和每个3个空格的1个点）。
- en: Let’s compare the patterns of the different characters. When we look at [Figure
    6-3](#figure6-3), it might not be clear to us that there’s any principle behind
    how the various patterns are assigned. But a beautiful idea is there waiting to
    be uncovered. [Figure 6-5](#figure6-5) shows a list of the 26 Roman letters, sorted
    by their typical frequency in English (Wikipedia 2020). The most frequently used
    letter, E, leads the list.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较不同字符的模式。当我们查看[图6-3](#figure6-3)时，我们可能不清楚这些不同模式分配背后是否有任何原则。但一个美丽的思想正在等待被揭示。[图6-5](#figure6-5)显示了26个罗马字母的列表，按它们在英语中的典型使用频率排序（Wikipedia
    2020）。最常用的字母E排在列表的最前面。
- en: '![f06005](Images/f06005.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![f06005](Images/f06005.png)'
- en: 'Figure 6-5: The Roman letters sorted by their frequency of use in English'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图6-5：按在英语中使用频率排序的罗马字母
- en: 'Now look back at the patterns in [Figure 6-3](#figure6-3). The most frequent
    letter, E, is just a single dot. The next most frequent letter, T, is just a single
    dash. Those are the only two possible patterns with just one symbol, so now we
    move on to two symbols. The letter A is a dot followed by a dash. O is next, and
    it breaks the pattern because it’s too long: three dashes. Let’s come back to
    that later. Returning to our list, the I is two dots, the N is a dash and a dot.
    The last two-letter pattern is M, with two dashes, but that’s pretty far down
    the list from where we’ve gotten so far. Why is O too long and M too short? Morse
    code is almost following our letter-frequency table, but not quite.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在回头看看[图6-3](#figure6-3)中的模式。最常见的字母E只是一个点。下一个最常见的字母T只是一个划。这是唯一两个只有一个符号的模式，因此我们继续往下看两个符号。字母A是一个点后跟一个划。接下来是O，它打破了这个模式，因为它太长：三个划。稍后再回来讨论。返回我们的列表，字母I是两个点，字母N是一个划和一个点。最后的两个字母模式是M，由两个划组成，但它在我们到达的位置上已经很远了。为什么O太长而M太短？摩尔斯电码几乎遵循我们的字母频率表，但还不完全是。
- en: The explanation starts with Samuel Morse, who only defined patterns for the
    numbers 0 through 9 in his original code. Letters and punctuation were added to
    the code by Alfred Vail, who designed those patterns in about 1844 (Bellizzi 11).
    Vail didn’t have an easy way to look up letter frequencies, but he knew he should
    follow them, according to Vail’s assistant, William Baxter. Baxter said,
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 解释从塞缪尔·摩尔斯开始，他在原始代码中仅定义了数字0到9的模式。字母和标点符号是由阿尔弗雷德·维尔添加到代码中的，他大约在1844年设计了这些模式（Bellizzi
    11）。维尔没有一个简单的方法来查找字母的出现频率，但根据维尔的助手威廉·巴克斯特的说法，他知道自己应该遵循这些频率。巴克斯特说，
- en: His general plan was to employ the simplest and shortest combinations to represent
    the most frequently recurring letters of the English alphabet, and the remainder
    for the more infrequent ones. For instance, he found upon investigation that the
    letter e occurs much more frequently than any other letter, and accordingly he
    assigned to it the shortest symbol, a single dot (•). On the other hand, j, which
    occurs infrequently, is expressed by dash-dot-dash-dot (– • – •) (Pope 1887)^([1](#c06-footnote-1))
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 他的总体计划是采用最简单和最短的组合来表示最常出现的英语字母，将其余的字母分配给较不常见的字母。例如，他在调查后发现字母e的出现频率远高于其他字母，因此他为其分配了最短的符号，一个点（•）。另一方面，j的出现频率较低，用划-点-划-点（–
    • – •）表示（Pope 1887）^([1](#c06-footnote-1))
- en: Vail figured that he could estimate the letter frequency table for English text
    by visiting his local newspaper in Morristown, New Jersey, where they were still
    setting stories by hand. In those days, typesetters built up a page one letter
    at a time. For each letter, they would choose an appropriate *slug*, or a metal
    bar with a letter embossed on one end, and place it into a large tray. Vail reasoned
    that the most popular characters would have the greatest number of slugs on hand,
    so he counted up the number of slugs in each letter’s bin. Those popularity counts
    were his proxy for letter frequency in English (McEwen 1997). Given how small
    this sample was, he did a pretty great job, despite imperfections like apparently
    thinking that M was more frequent than O.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Vail认为，他可以通过访问新泽西州莫里斯敦的本地报社来估算英文文本中的字母频率表，当时他们仍在手动排版。那时，排版工人是逐个字母地构建页面。对于每个字母，他们会选择一个合适的*铅字*，即一根金属条的一端上压印有字母，然后将其放入一个大托盘中。Vail推测，最常用的字符会有最多的铅字，因此他数了每个字母托盘中的铅字数量。这些流行度计数成为了他估算英语字母频率的替代指标（McEwen
    1997）。尽管这个样本很小，他还是做得相当好，尽管存在一些不准确的地方，比如他显然认为M比O更常见。
- en: To see how well our frequency chart (and Morse code) lines up with some actual
    text, [Figure 6-6](#figure6-6) shows the frequencies for the letters from *Treasure
    Island* (Stevenson 1883). For this chart, we counted only the letters, which we
    turned into lowercase before counting. We also excluded numbers, spaces, and punctuation.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了查看我们的频率图（和摩尔斯电码）与一些实际文本的对比，[图6-6](#figure6-6)展示了**《宝岛》**中各个字母的频率（史蒂文森 1883）。在这张图中，我们只统计了字母，并在计数前将它们转换为小写字母。我们还排除了数字、空格和标点符号。
- en: '![f06006](Images/f06006.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![f06006](Images/f06006.png)'
- en: 'Figure 6-6: The number of times each letter appears in *Treasure Island* by
    Robert Louis Stevenson. Uppercase letters were counted as lowercase.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图6-6：每个字母在**罗伯特·路易斯·史蒂文森**的《**宝岛**》中出现的次数。大写字母被当作小写字母计算。
- en: The order of the characters in [Figure 6-6](#figure6-6) isn’t a perfect match
    to our letter frequency chart in [Figure 6-5](#figure6-5), but it’s close. [Figure
    6-6](#figure6-6) looks like a probability distribution over the letters A through
    Z. To make it an *actual* probability distribution, we have to scale it so that
    the sum of all the entries is 1\. The result is shown in [Figure 6-7](#figure6-7).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6-6](#figure6-6)中的字符顺序与[图6-5](#figure6-5)中的字母频率图不完全匹配，但差距不大。[图6-6](#figure6-6)看起来像是从A到Z字母的概率分布。为了使其成为一个*实际的*概率分布，我们必须对其进行缩放，使得所有条目的总和为1。结果显示在[图6-7](#figure6-7)中。'
- en: '![f06007](Images/f06007.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![f06007](Images/f06007.png)'
- en: 'Figure 6-7: The probability distribution function (pdf) for characters in *Treasure
    Island*'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图6-7：**《宝岛》**中字符的概率分布函数(pdf)
- en: Now let’s use our probability distribution of letters to improve the efficiency
    of sending *Treasure Island* via Morse code.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们利用字母的概率分布，来提高通过摩尔斯电码发送**《宝岛》**的效率。
- en: Customizing Morse Code
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自定义摩尔斯电码
- en: To motivate our improvements to sending *Treasure Island* via Morse code, let’s
    first take a step backward, and start with an imaginary version of Morse code
    where Mr. Vail didn’t bother to journey down to the newspaper office. Instead,
    let’s say he wanted to assign the same number of dot-and-dash symbols to each
    character. With four symbols he could only label 16 characters, but with five
    symbols he could label 32 characters.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了激励我们改进通过摩尔斯电码发送**《宝岛》**的方式，首先让我们退后一步，开始思考一个假设的摩尔斯电码版本，在这个版本中，Vail先生并没有下到报社去。而是，假设他想为每个字符分配相同数量的点和划符号。使用四个符号，他只能标记16个字符，但使用五个符号，他可以标记32个字符。
- en: '[Figure 6-8](#figure6-8) shows how we might arbitrarily assign such a five-symbol
    pattern to each character. To keep things simple, we made the timing of every
    dot and dash the same by using different tones for the two symbols. So every dot
    (shown here as a black dot) is a high tone lasting one dit, and every dash (shown
    as a red square) is a low tone lasting one dit. The result is that every character
    takes nine dits of time to send (five for the dots and dashes, now high and low
    tones, and four for the silences between them). This is an example of a *constant-length
    code*, also called *a fixed-length code*.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6-8](#figure6-8)展示了我们如何任意地为每个字符分配这样的五符号模式。为了简化起见，我们通过使用不同的音调来表示两个符号，使得每个点和划的时间都相同。所以每个点（这里显示为黑色圆点）是一个持续一拍的高音，而每个划（显示为红色方块）是一个持续一拍的低音。结果是，每个字符需要发送九拍的时间（五拍用于点和划，现在分别是高音和低音，四拍是它们之间的间隔）。这是一个*定长编码*的例子，也叫做*固定长度编码*。'
- en: In [Figure 6-8](#figure6-8) we didn’t create a character for the space, following
    in the footsteps of the original Morse code, where it was assumed that we could
    figure out where the spaces ought to go by looking at the message. Sticking to
    that spirit, we’ll ignore space characters for the rest of this discussion.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 6-8](#figure6-8)中，我们没有为空格创建字符，这与原始的莫尔斯电码保持一致，因为它假定我们可以通过查看信息来判断空格的位置。遵循这一精神，我们将在接下来的讨论中忽略空格字符。
- en: '![f06008](Images/f06008.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![f06008](Images/f06008.png)'
- en: 'Figure 6-8: Assigning five symbols to each character gives us a constant-length
    code. Black circles are high tones; red squares are low tones. They all last one
    dit.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6-8：为每个字符分配五个符号得到固定长度编码。黑色圆圈表示高音调；红色方块表示低音调。它们的持续时间都是一个点。
- en: The first two words in the text of *Treasure Island* are the name “Squire Trelawney.”
    Since every character in our two-tone version of Morse code requires 9 dits, this
    phrase of 15 letters (remember that we’re ignoring the space) requires 9 × 15
    = 135 dits of time to send. Adding in the 14 silences between letters, which take
    3 × 14 = 42 bits, we find the fixed-length message takes 135 + 42 = 177 dits of
    time, as shown in [Figure 6-9](#figure6-9).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 《金银岛》文本中的前两个单词是名称“斯奎尔·特雷劳尼”。由于我们这种双音调版本的莫尔斯电码中的每个字符都需要 9 个点，因此这个包含 15 个字母的短语（记住我们忽略了空格）需要
    9 × 15 = 135 个点的时间来发送。再加上字母之间的 14 个停顿，这些停顿需要 3 × 14 = 42 个比特，我们发现固定长度的信息需要 135
    + 42 = 177 个点的时间，如[图 6-9](#figure6-9)所示。
- en: '![f06009](Images/f06009.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![f06009](Images/f06009.png)'
- en: 'Figure 6-9: The first two words of *Treasure Island*, using our constant-length
    code'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6-9：使用我们固定长度编码的《金银岛》前两个单词
- en: Now compare this to actual Morse code where, for the most part, the most common
    letters have fewer symbols than the uncommon letters. [Figure 6-10](#figure6-10)
    shows this. We’ll continue sending dots and dashes using different tones that
    last one dit each.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在将此与实际的莫尔斯电码进行比较，后者大多数情况下，常见字母所需的符号比不常见字母少。[图 6-10](#figure6-10)展示了这一点。我们将继续使用不同音调的点和划线发送信息，每个音调的持续时间为一个点。
- en: '![f06010](Images/f06010.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![f06010](Images/f06010.png)'
- en: 'Figure 6-10: The first two words of *Treasure Island*, using Morse code'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6-10：使用莫尔斯电码的《金银岛》前两个单词
- en: If we count up the elements (remembering that dots and dashes now take just
    one dit each), we find that the [Figure 6-10](#figure6-10) version requires 101
    dits of time, about half as long as the fixed-length code (101 / 177 ≈ 0.57).
    That savings comes from adapting our code to the content we are sending. We call
    any code that tries to improve efficiency by matching up short patterns with high-probability
    events a *variable-bitrate code*, or more simply, an *adaptive code*. Even in
    this simple example, our adaptive code is almost twice as efficient as the constant-length
    code, cutting our communication time nearly in half.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们计算出这些元素（记住现在点和划每个只需一个点），我们会发现[图 6-10](#figure6-10)版本只需要 101 个点，大约是固定长度编码时间的一半（101
    / 177 ≈ 0.57）。这种节省来自于我们将编码调整为适应所发送的内容。我们将任何通过将短模式与高概率事件匹配来提高效率的编码称为*可变比特率编码*，或更简单地说，*自适应编码*。即便在这个简单的例子中，我们的自适应编码几乎是固定长度编码的两倍高效，几乎将通信时间减半。
- en: Let’s look at the whole text of *Treasure Island*, which contains about 338,000
    characters (excluding spaces, punctuation, etc.). The adaptive code would take
    only about 42 percent of the time required by the fixed-length code. We can send
    the book in less than half the time required by a nonadaptive code.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下整个《金银岛》文本，它包含约 338,000 个字符（不包括空格、标点符号等）。自适应编码只需约固定长度编码所需时间的 42%。我们可以在不到非自适应编码时间的一半内发送这本书。
- en: We can do even better if, instead of using standard Morse code, which is adapted
    to English writing in general, we tune the distribution of symbols to more closely
    match their actual percentages in the text of the specific book we’re sending.
    Of course, we’d have to share our clever encoding with our recipient, but if we’re
    sending a long message, that extra piece of communication is dwarfed by the message
    itself. Let’s take that step, and imagine a custom Treasure Island code that is
    perfectly adapted to the contents of *Treasure Island* specifically. We should
    expect even more savings.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不使用标准的莫尔斯电码（莫尔斯电码通常适用于英语书写），而是将符号的分布调整为更接近我们发送的特定书籍文本中符号的实际百分比，我们的效率会更高。当然，我们必须与接收方共享我们的聪明编码，但如果我们正在发送一条长信息，那额外的沟通成本相比信息本身几乎可以忽略不计。让我们走出这一步，想象一个专门为《金银岛》定制的莫尔斯电码，它完全适应《金银岛》的内容。我们应该预期能够节省更多时间。
- en: Let’s rephrase this using the language of probability. An adaptive code creates
    a pattern for each value in a probability distribution. The value with the highest
    probability receives the shortest possible code. Then we work our way through
    the values, from the highest probability to the lowest, assigning patterns that
    are always as short as possible without repeating. That means each new pattern
    is as long as, or longer than, the pattern assigned to the previous value. That’s
    just what Mr. Vail did in 1844, guided by the number of letters he found in the
    typesetter’s bins of his local newspaper.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用概率的语言重新表述这个问题。自适应编码为概率分布中的每个值创建一个模式。具有最高概率的值将获得尽可能最短的编码。然后，我们按从最高概率到最低概率的顺序逐个处理这些值，为每个值分配始终尽可能短的模式，并且不重复。这意味着每个新模式的长度至少和分配给前一个值的模式相同，或者更长。这正是费尔先生在1844年所做的，他根据当地报纸排字工的字母数量为指导。
- en: Now we can look at any message we want to communicate, identify each character,
    and compare it to the probability distribution that tells us how likely that character
    was in the first place. This tells us how much information, in bits, is carried
    by that character. Thanks to the fourth property in our description of the formula
    for computing information, the total number of bits required to represent the
    message (ignoring context for the moment), is just the sum of the individual numbers
    of bits required by each character.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以查看任何想要传达的信息，识别每个字符，并将其与告诉我们该字符最初出现概率的概率分布进行比较。这告诉我们每个字符承载的信息量，以比特为单位。得益于我们描述计算信息的公式中的第四个属性，表示消息所需的总比特数（暂时忽略上下文），正好是每个字符所需的比特数之和。
- en: We can also perform this process for our message before we send it. That tells
    us just how much information we’re about to communicate to our recipient.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以在发送消息之前进行这个过程。这告诉我们我们即将传达给接收者的具体信息量。
- en: Entropy
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 熵
- en: We’ve discussed *surprise*, which refers to things we didn’t expect. A related
    idea is *uncertainty*, which refers to those times when we know all the things
    that might happen, but we’re not sure which one will actually occur. For instance,
    when we roll a fair six-sided die, we know that each of the six faces has an equal
    probability of coming up, but until we roll it and look, we are uncertain which
    face will be on top. A more formal term for this uncertainty is *entropy*.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论过*惊讶*，它指的是那些我们没有预料到的事情。一个相关的概念是*不确定性*，它指的是那些我们知道所有可能发生的事情，但不确定哪一个会实际发生的时刻。例如，当我们掷一个公正的六面骰子时，我们知道六个面出现的概率是相等的，但直到我们掷出并查看时，我们才不确定哪个面会朝上。这种不确定性的一个更正式的术语是*熵*。
- en: We can assign a number to the uncertainty, or entropy, of an outcome. This number
    often depends on how many outcomes are possible. For example, flipping a coin
    can have only 2 outcomes, but rolling a six-sided die can have 6 outcomes, and
    picking a letter from the alphabet can have 26 outcomes. The uncertainty of these
    three results, or their entropy, is a number that increases in size, from the
    coin to the die to the alphabet, because the number of outcomes is increasing
    in each case. That makes each specific result more uncertain.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以为结果的不确定性或熵赋予一个数值。这个数值通常取决于可能的结果数量。例如，抛硬币只有2种结果，而掷一个六面骰子有6种结果，从字母表中挑选一个字母则有26种结果。这三种结果的不确定性，或它们的熵，是一个数值，它的大小依次增加，从硬币到骰子，再到字母表，因为每种情况中的结果数量在增加。这使得每个具体结果的不可预测性增大。
- en: In those three examples, the probability of each outcome is the same (1/2 for
    each side of the coin, 1/6 for each die face, and 1/26 for each letter). But what
    if the probabilities of the outcomes are different? The formula for computing
    the entropy explicitly takes these different probabilities into account. Essentially,
    it considers all the possible outcomes in a distribution and puts a number to
    the uncertainty describing which outcome is actually produced when we sample the
    distribution.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这三个例子中，每个结果的概率是相同的（硬币每一面是1/2，骰子的每一面是1/6，字母表中的每个字母是1/26）。但如果结果的概率不同呢？计算熵的公式明确考虑了这些不同的概率。本质上，它考虑了分布中的所有可能结果，并为描述在我们从分布中抽样时实际产生哪个结果的不确定性赋予了一个数值。
- en: 'It turns out that the uncertainty of a specific event occurring is the same
    as the number of bits required to send a message with a perfectly adapted code.
    Conceptually, a text message is a set of words drawn from a vocabulary, which
    is no different than the values of a die rolled multiple times. We use the term
    *entropy* for both values: the uncertainty of an event, or the number of bits
    required to communicate that event (and remove the uncertainty).'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，特定事件发生的不确定性与用一个完全适应的编码发送消息所需的比特数相同。从概念上讲，一条文本消息是从词汇表中抽取的一组单词，这与多次掷骰子的值没有什么不同。我们使用术语*熵*来表示这两种情况：事件的不确定性，或者传递该事件所需的比特数（从而消除不确定性）。
- en: Entropy is useful in machine learning because it lets us compare two probability
    distributions. This is a key step in learning. For example, consider a classifier.
    We might have a picture that we have manually decided is 80 percent likely to
    be a dog, but 10 percent likely to be a wolf, 3 percent likely to be a fox, and
    a few other smaller probabilities for other animals. We’d like the system’s predictions
    to match those labels. In other words, we want to compare our manual distribution
    with the system’s predicted distribution and use any differences to improve our
    system. We can invent lots of ways to compare distributions, but the one that
    works best in both theory and practice is based on entropy. Let’s build our way
    up to that comparison, starting with finding the entropy for a single distribution.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 熵在机器学习中非常有用，因为它允许我们比较两个概率分布。这是学习中的关键步骤。例如，考虑一个分类器。我们可能有一张图片，我们手动判定它有80%的可能性是狗，但有10%的可能性是狼，3%的可能性是狐狸，还有其他一些动物的较小概率。我们希望系统的预测与这些标签相匹配。换句话说，我们希望比较我们的手动分布与系统的预测分布，并利用任何差异来改进我们的系统。我们可以发明许多比较分布的方法，但在理论和实践中最有效的方法是基于熵的。让我们从求一个单一分布的熵开始，逐步建立起这种比较方法。
- en: Consider a distribution made up of words. If only one word is in our distribution,
    there is no uncertainty about what word we’ll get when we sample the distribution,
    and therefore the entropy is 0\. If there are lots of words but they all have
    a probability of 0, except for a single word with a probability of 1, there’s
    still no uncertainty, so the entropy is again 0\. When all of the words have the
    same probabilities, we have the maximum uncertainty, since no choice is any more
    probable than any other. In this case, our uncertainty, or entropy, is at a maximum.
    Though it might be convenient to say that maximum entropy should be 1, or 100,
    the actual value is calculated by the formula. What we do know is that no other
    probability distribution will give us a larger entropy.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑由单词组成的分布。如果我们的分布中只有一个单词，那么当我们从分布中抽取单词时就没有不确定性，因此熵为0。如果有很多单词，但它们的概率都是0，除了一个单词的概率是1，那么仍然没有不确定性，所以熵再次为0。当所有单词的概率相同，我们就有了最大的
    uncertainty，因为没有任何选择比其他选择更可能。在这种情况下，我们的不确定性或熵达到了最大值。虽然可以方便地说最大熵应该是1或100，但实际值是通过公式计算的。我们所知道的是，其他任何概率分布都不会给我们更大的熵。
- en: In the next section, we’ll see how to apply entropy to pairs of distributions.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将看到如何将熵应用于分布对的比较。
- en: Cross Entropy
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交叉熵
- en: When we’re training a deep learning system, we’ll often want to have a measure
    that tells us to what degree two probability distributions are the same or different.
    The value we usually use is a quantity called the *cross entropy*, and it too
    is just a number. Recall that the entropy tells us how many bits we need to send
    a message using a code that is perfectly tuned to that message. The cross entropy
    tells us how many bits we need if we use some other, less perfect code. Generally,
    this is larger than the number of bits the perfect code needs (if the alternative
    code happens to be exactly as efficient as the ideal code, the cross entropy has
    its minimum value of 0). The cross entropy is a measurement that lets us compare
    two probability distributions numerically. Identical distributions have a cross
    entropy of 0, while increasingly different pairs of distributions have increasingly
    larger values of cross entropy.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练深度学习系统时，我们通常希望有一个度量标准，告诉我们两个概率分布在多大程度上相同或不同。我们通常使用的值是一种叫做*交叉熵*的量，它也只是一个数字。回想一下，熵告诉我们，使用一个完美匹配消息的编码需要多少位。交叉熵则告诉我们，如果我们使用某种其他的、不太完美的编码，我们需要多少位。通常情况下，交叉熵比完美编码所需的位数要大（如果替代编码恰好与理想编码一样高效，则交叉熵的最小值为
    0）。交叉熵是一个让我们数值化比较两个概率分布的测量工具。相同的分布具有 0 的交叉熵，而越来越不同的分布对的交叉熵值则会越来越大。
- en: To get a feeling for the idea, let’s look at two novels, and build up a word-based
    adaptive code for each. Though our goal is to compare probability distributions,
    and we’re here talking about codes, it’s conceptually easy to go back and forth.
    Recall that by construction, smaller codes correspond to words with higher probabilities,
    while larger codes correspond to words with lower probabilities.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这个概念，让我们看两本小说，并为每本小说建立一个基于单词的自适应编码。虽然我们的目标是比较概率分布，而且我们现在讨论的是编码，但在概念上很容易来回切换。回想一下，通过构造，较小的编码对应于概率较高的单词，而较大的编码对应于概率较低的单词。
- en: Two Adaptive Codes
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 两种自适应编码
- en: The novels *Treasure Island* and *The Adventures of Huckleberry Finn*, by Mark
    Twain, were both written in English at about the same time (Stevenson 1883; Twain
    1885). *Treasure Island* has the larger vocabulary, using about 10,700 unique
    words, compared to about 7,400 unique words in *Huckleberry Finn*. Of course,
    they use very different sets of words, but there’s lots of overlap. Let’s look
    at the 25 most popular words in *Treasure Island*, shown in [Figure 6-11](#figure6-11).
    For the purposes of counting words, we first converted all uppercase letters to
    lowercase. The single-letter pronoun “I” therefore appears in the charts as the
    lower-case “i.”
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 马克·吐温的小说《金银岛》和《哈克贝里·费恩历险记》几乎在同一时期用英语写成（斯蒂文森 1883年；吐温 1885年）。《金银岛》的词汇量更大，使用了大约
    10,700 个独特的单词，而《哈克贝里·费恩历险记》则用了大约 7,400 个独特的单词。当然，这两本书使用的单词集完全不同，但也有很多重叠。让我们来看一下《金银岛》中最流行的
    25 个单词，如[图 6-11](#figure6-11)所示。在计算单词时，我们首先将所有的大写字母转换为小写字母。因此，单字母代词“I”在图表中显示为小写字母“i”。
- en: '![f06011](Images/f06011.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![f06011](Images/f06011.png)'
- en: 'Figure 6-11: The 25 most popular words in *Treasure Island*, sorted by number
    of appearances'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6-11：按出现频率排序的《金银岛》中最流行的 25 个单词
- en: Let’s compare these to the 25 most popular words in *Huckleberry Finn*, shown
    in [Figure 6-12](#figure6-12).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这些与《哈克贝里·费恩历险记》中最流行的 25 个单词进行比较，如[图 6-12](#figure6-12)所示。
- en: Perhaps unsurprisingly, the most popular dozen words in both books are almost
    the same (though in different orders), but then things begin to diverge.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 也许并不令人惊讶，两本书中最流行的十二个单词几乎完全相同（尽管顺序不同），但随后就开始有所不同。
- en: '![f06012](Images/f06012.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![f06012](Images/f06012.png)'
- en: 'Figure 6-12: The 25 most popular words in *Huckleberry Finn*, sorted by number
    of appearances'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6-12：按出现频率排序的《哈克贝里·费恩历险记》中最流行的 25 个单词
- en: Let’s suppose we want to transmit the text of both books, word by word. We could
    go to the English dictionary and assign every word a number starting with 1, then
    2, then 3, and so on. But we know from our earlier Morse code example that we
    can send information more efficiently by using a code that’s adapted to the material
    being sent. Let’s create that kind of code, where the more frequently a word appears,
    the smaller its code number. So super-frequent words like the and and can be sent
    with short codes, while the rare words have longer codes that require us to send
    more bits (in *Treasure Island* about 2,780 words appear only once; in *Huckleberry
    Finn* about 2,280 words appear only once).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要逐字逐句地传输这两本书的文本。我们可以查阅英语词典，为每个单词分配一个编号，顺序为1、2、3，依此类推。但我们从之前的摩尔斯电码示例中知道，通过使用一个适应传输内容的代码，我们可以更高效地传递信息。我们来创建这种类型的代码，单词出现得越频繁，其代码编号就越小。因此，像“the”和“and”这样的高频词可以用短代码传输，而稀有词则有更长的代码，需要传送更多的比特（在*金银岛*中大约有2,780个单词只出现了一次；在*哈克贝利·费恩*中大约有2,280个单词只出现了一次）。
- en: The vocabularies of the two books mostly overlap, but each book has words that
    don’t appear in the other. For instance, the word yonder appears 20 times *Huckleberry
    Finn*, but not even once in *Treasure Island.* And schooner is in *Treasure Island*
    28 times, but it’s nowhere to be found in *Huckleberry Finn*.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 两本书的词汇大部分是重合的，但每本书也有一些在另一册中没有出现的单词。例如，“yonder”这个词在*哈克贝利·费恩*中出现了20次，但在*金银岛*中一次也没有出现。而“schooner”这个词在*金银岛*中出现了28次，但在*哈克贝利·费恩*中完全找不到。
- en: Because we want to be able to send either book with either code, let’s unify
    their vocabularies. For each word in *Huckleberry Finn* that isn’t in *Treasure
    Island*, we add one instance of that word when we make the Treasure Island code.
    We do the same thing for *Huckleberry Finn*. For example, we tack on one instance
    of yonder to the end of the book when we make the Treasure Island code so that
    we can use that code to send *Huckleberry Finn* if we wanted to.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们希望能够使用任意一种代码传输任何一本书，所以我们需要统一它们的词汇表。对于*哈克贝利·费恩*中在*金银岛*中没有出现的每个单词，当我们制作*金银岛*的代码时，我们会加入该单词的一次实例。我们对*哈克贝利·费恩*也做同样的事情。例如，在制作*金银岛*的代码时，我们会将“yonder”这一单词加到书的末尾，这样如果我们想要使用这个代码传输*哈克贝利·费恩*，就可以使用。
- en: Let’s start with the words in *Treasure Island*. We’ll make an adaptive code
    for this text, starting with a tiny code for the and working our way up to huge
    codes for one-time-only words like wretchedness. Now we can send the whole book
    using that code and save time compared to any other code.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从*金银岛*中的单词开始。我们将为这本书制作一个自适应代码，从为“and”分配一个小代码开始，逐渐增加代码长度，直到为像“wretchedness”这样仅出现一次的词分配较大的代码。现在，我们可以使用该代码传输整本书，相较于其他任何代码，这样可以节省时间。
- en: 'Now we’ll do the same thing for *Huckleberry Finn*, and make a code specifically
    for this text, giving the shortest code to and and leaving the big codes for one-time-only
    words like dangerous (shocking, but true: dangerous appears only once in *Huckleberry
    Finn*!). The Huckleberry Finn code now lets us send the contents of this book
    more quickly than any other code.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将对*哈克贝利·费恩*进行相同的操作，并为这段文本创建一个专门的代码，给常见的词分配最短的代码，将像“危险”（令人震惊，但是真的：“危险”这个词在*哈克贝利·费恩*中只出现过一次！）这样的生僻词留给较大的代码。现在，*哈克贝利·费恩*的代码让我们比任何其他代码更快速地传输这本书的内容。
- en: Note that these two codes are different. We’d expect that, because the two books
    have different vocabularies, and cover significantly different subject matter.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这两种代码是不同的。我们可以预期会有这种差异，因为两本书的词汇量不同，且涉及的主题也有显著差异。
- en: Using the Codes
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用这些代码
- en: Now we have two codes, each of which can transmit either book. The Treasure
    Island code is tuned to how many times each word appears in *Treasure Island*,
    and the Huckleberry Finn code is tuned to *Huckleberry Finn*.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了两种代码，每种代码都可以传输任意一本书。*金银岛*的代码针对每个单词在*金银岛*中的出现次数进行了调整，而*哈克贝利·费恩*的代码则针对*哈克贝利·费恩*进行了调整。
- en: The *compression ratio* tells us how much savings we get from using an adaptive
    code versus a fixed-length code. If the ratio is exactly 1, then our adaptive
    code uses exactly as many bits as a nonadaptive code. If the ratio is 0.75, then
    the adaptive code sends only 3/4 the number of bits needed by the nonadaptive
    code. The smaller the compression ratio, the more bits we’re saving (some authors
    define this ratio with the numbers in the other order, so the larger the ratio,
    the better the compression).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*压缩比*告诉我们使用自适应编码与固定长度编码相比，节省了多少数据。如果压缩比正好为1，那么我们的自适应编码使用的位数与非自适应编码完全相同。如果压缩比为0.75，那么自适应编码仅使用非自适应编码所需位数的3/4。压缩比越小，节省的位数就越多（一些作者定义这个比率时，数字的顺序正好相反，所以压缩比越大，压缩效果越好）。'
- en: Let’s try sending our two books word by word. The top bar of [Figure 6-13](#figure6-13)
    shows the compression ratio that we get from sending *Huckleberry Finn* with the
    code we built for it. We used an adaptive code called a *Huffman code*, but the
    results would be similar for most adaptive codes (Huffman 1952; Ferrier 2020).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试逐词发送这两本书。[图6-13](#figure6-13)的顶部条形图显示了我们使用为《**哈克贝里·芬**》构建的代码发送这本书时得到的压缩比。我们使用了一种叫做*霍夫曼编码*的自适应编码，但对于大多数自适应编码，结果都会类似（霍夫曼
    1952；费里尔 2020）。
- en: '![f06013](Images/f06013.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![f06013](Images/f06013.png)'
- en: 'Figure 6-13: Top: The compression ratio from sending *Huckleberry Finn* using
    the code built from that book. Bottom: The compression from using the code built
    from *Treasure Island*.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图6-13：上图：使用为《**哈克贝里·芬**》构建的代码发送《**哈克贝里·芬**》的压缩比。下图：使用为《**宝岛**》构建的代码发送《**哈克贝里·芬**》的压缩比。
- en: This is pretty great. The adaptive code got a compression ratio of a little
    less than 0.5, meaning that to send *Huckleberry Finn* using this code would require
    a little less than half the number of bits required by a fixed-length code. If
    we send *Huckleberry Finn* using the code built from *Treasure Island*, we should
    expect that the compression won’t be as good, because our numbers in that code
    are not matched to the word frequencies we’re encoding. The bottom bar of [Figure
    6-13](#figure6-13) shows this result, with a compression ratio of around 0.54\.
    That’s still pretty great, but not quite as efficient.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常棒。自适应编码的压缩比略低于0.5，这意味着使用这种编码发送《**哈克贝里·芬**》所需的位数将不到固定长度编码的一半。如果我们使用为《**宝岛**》构建的代码来发送《**哈克贝里·芬**》，我们可以预期压缩效果不会那么好，因为该代码中的数字与我们正在编码的词频不匹配。[图6-13](#figure6-13)的下方显示了这一结果，压缩比大约为0.54。这仍然非常不错，但不如前者高效。
- en: Let’s flip the situation around and see how *Treasure Island* does with a code
    built for it, and one built for *Huckleberry Finn*. The results are shown in [Figure
    6-14](#figure6-14).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们换个角度，看看使用为《**宝岛**》构建的代码和为《**哈克贝里·芬**》构建的代码分别有什么效果。结果见[图6-14](#figure6-14)。
- en: '![f06014](Images/f06014.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![f06014](Images/f06014.png)'
- en: 'Figure 6-14: Top: The compression ratio from sending *Treasure Island* using
    the code built from *Huckleberry Finn*. Bottom: The compression ratio for sending
    *Treasure Island,* using the code for *Treasure Island*.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图6-14：上图：使用为《**哈克贝里·芬**》构建的代码发送《**宝岛**》的压缩比。下图：使用为《**宝岛**》构建的代码发送《**宝岛**》的压缩比。
- en: This time we find that *Treasure Island* compressed better than *Huckleberry
    Finn*, which makes sense because we used a code tuned to its word usage. In general,
    the fastest way to send any message is with a code that was built for the contents
    of that message. No other code can do better, and most will do worse.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次我们发现，《**宝岛**》的压缩效果比《**哈克贝里·芬**》更好，这很有道理，因为我们使用了与其词汇使用相匹配的代码。一般来说，发送任何信息的最快方法是使用专门为该信息内容构建的编码。没有其他代码能做得更好，大多数代码会做得更差。
- en: We’ve seen that using the Treasure Island code to send *Huckleberry Finn* gives
    us worse compression. In other words, it requires more bits to send this book
    with a code that is imperfect for this message. This is because each code is based
    on its corresponding probability distribution, and those distributions are different.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，使用《**宝岛**》代码来发送《**哈克贝里·芬**》的压缩效果较差。换句话说，使用不适合该信息的编码来发送这本书需要更多的位数。这是因为每个代码都是基于其对应的概率分布，而这些分布是不同的。
- en: The quantity we use to measure the difference between two probability distributions
    is *cross entropy*.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用来衡量两个概率分布之间差异的量是*交叉熵*。
- en: Note that the situation is not symmetrical. If we want to send words from *Treasure
    Island* using the Huckleberry Finn code, the cross entropy will be different from
    sending *Huckleberry Finn* with the Treasure Island code. We sometimes say that
    the cross entropy function is *asymmetrical* in its arguments, meaning that their
    order matters.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，情况并非对称的。如果我们想用《*金银岛*》中的文字通过《哈克贝里·费恩》代码发送，交叉熵将与用《金银岛》代码发送《哈克贝里·费恩》时不同。我们有时会说交叉熵函数在其参数上是*不对称的*，意味着它们的顺序很重要。
- en: One way to conceptualize this is to picture that our space of probability distributions
    is like the ocean, with currents flowing in different directions in different
    places. The effort required to swim from some point A to another point B, sometimes
    fighting the currents and sometimes getting carried along by them, is generally
    different than the effort required to swim from B to A. In this metaphor, the
    cross entropy is measuring the amount of work, not the actual distance between
    the points. But as A and B get closer together, the work involved in swimming
    between them, in either direction, goes down.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 一种概念化这种情况的方式是将我们的概率分布空间想象成海洋，海流在不同地方朝不同方向流动。从某个A点游到B点所需的努力，时而需要与海流抗争，时而被海流带着走，通常与从B点游到A点的努力不同。在这个比喻中，交叉熵是衡量工作的量，而不是两点之间的实际距离。但随着A和B之间的距离变得更近，游泳所需的工作量在任一方向上都会减少。
- en: Cross Entropy in Practice
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 交叉熵在实践中
- en: Let’s see cross entropy in action. We’ll use it just as we do when we’re training
    a photo classifier and need to compare two probability distributions. The first
    is the label that we manually created to describe what’s in the photo. The second
    is the set of probabilities that the system computes when we show it that photo.
    Our goal is to train the system so that its outputs match our labels. To do that,
    we need to know when the system gets it wrong and put a number to how wrong it
    is. That’s the cross entropy we get by comparing the label and the predictions.
    The larger the cross entropy, the larger the error.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下交叉熵的实际应用。我们将像训练一个照片分类器时一样使用它，比较两个概率分布。第一个是我们手动创建的标签，用来描述照片中的内容。第二个是系统在我们展示给它照片时计算出的概率集。我们的目标是训练系统，使其输出与我们的标签匹配。为了做到这一点，我们需要知道系统出错的地方，并给出一个衡量错误大小的数字。这就是通过比较标签和预测得到的交叉熵。交叉熵越大，误差就越大。
- en: In [Figure 6-15](#figure6-15) we have the output of an imaginary classifier
    that’s predicting the probabilities for a picture of a dog. In most real situations,
    all of the label values would be 0 except for the entry for dog, which would be
    1\. Here we’ve assigned arbitrary probabilities to each of the six labels to better
    show how the system tries to match the label distribution (we can imagine that
    the picture is blurry, so we’re not sure ourselves what animal it shows).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图6-15](#figure6-15)中，我们展示了一个虚拟分类器的输出，预测了一张狗的照片的概率。在大多数实际情况下，所有标签值都会是0，只有狗的条目是1。在这里，我们为六个标签分配了任意的概率，以更好地展示系统如何尝试匹配标签分布（我们可以想象这张照片模糊不清，因此我们自己也不确定它显示的是哪种动物）。
- en: '![f06015](Images/f06015.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![f06015](Images/f06015.png)'
- en: 'Figure 6-15: Classifying a picture of a dog. Left: At the start of training.
    Right: After much training. The cross entropy is lower when the match is better.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图6-15：分类一张狗的照片。左：训练开始时。右：经过大量训练后。当匹配更好时，交叉熵更低。
- en: The figure at the left comes from the start of training. The system’s predictions
    are a pretty poor match to our manual labels. If we run these numbers through
    the cross entropy formula, we get a cross entropy of about 1.9\. On the right,
    we see the results after some training. Now the two distributions are much closer,
    and the cross entropy has dropped to about 1.6\.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧的图像来自训练开始时。系统的预测与我们的手动标签匹配得相当差。如果我们将这些数字带入交叉熵公式，我们得到大约1.9的交叉熵。在右侧，我们看到经过一些训练后的结果。现在这两个分布更为接近，交叉熵降到了大约1.6。
- en: Most deep learning libraries offer built-in routines that compute the cross
    entropy for us in a single step. In [Figure 6-15](#figure6-15) we had six categories.
    When there are only two categories, we can use a routine that’s specialized for
    that case. It’s often called the *binary cross entropy* function.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数深度学习库提供了内置的例程，可以在一步中为我们计算交叉熵。在[图6-15](#figure6-15)中，我们有六个类别。当只有两个类别时，我们可以使用专门为这种情况设计的例程。它通常被称为*二元交叉熵*函数。
- en: Kullback–Leibler Divergence
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 库尔巴克-莱布勒散度
- en: Cross entropy is a great measure for comparing two distributions. By minimizing
    the cross entropy, we minimize the error between the classifier’s outputs and
    our label, improving our system.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵是比较两个分布的一个很好的度量。通过最小化交叉熵，我们可以减少分类器输出与我们的标签之间的误差，从而提高系统性能。
- en: We can make things just a little simpler conceptually with one more step. Let’s
    think of our word distributions as codes again. Recall that the entropy tells
    us how many bits are required to send a message with a perfect, tuned code. And
    the cross entropy tells us how many bits are required to send that message with
    an imperfect code. If we subtract the entropy from the cross entropy, we get the
    number of additional bits required by the imperfect code. The smaller we can get
    this number, the fewer additional bits we need, and the more the corresponding
    probability distributions are the same.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过再增加一步，概念上让事情变得稍微简单一点。让我们再次把词汇分布看作是编码。回想一下，熵告诉我们，在使用完美、调优的编码时，发送一条消息需要多少比特。而交叉熵则告诉我们，使用不完美的编码时，发送该消息需要多少比特。如果我们从交叉熵中减去熵，我们就得到了不完美编码所需的额外比特数。我们能够让这个数字尽可能小，就意味着我们需要的额外比特数越少，相应的概率分布就越相同。
- en: This extra number of bits required by an imperfect code (that is, the increase
    in entropy) goes by a large number of formidable names. The most popular is the
    *Kullback–Leibler divergence* or just *KL divergence*, named for the scientists
    who presented a formula for computing this value. Less frequently, it’s also referred
    to as *discrimination information*, *information divergence*, *directed divergence*,
    *relative entropy*, and *KLIC* (for *Kullback–Leibler information criterion*).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 不完美编码所需的额外比特数（即熵的增加）有许多令人畏惧的名称。最常见的名称是 *Kullback–Leibler 散度*，简称 *KL 散度*，这个名字来源于提出计算该值公式的科学家。较少见的名称还包括
    *判别信息*、*信息散度*、*有向散度*、*相对熵* 和 *KLIC*（即 *Kullback–Leibler 信息准则*）。
- en: 'Like the cross entropy, the KL divergence is asymmetrical: the order of the
    arguments matters. The KL divergence for sending *Treasure Island* with the Huckleberry
    Finn code is written KL(Treasure Island||Huckleberry Finn). The two bars in the
    middle can be thought of as a single separator, like the more frequently seen
    comma. We can think of them as representing the phrase “sent using the code for.”
    If we run through the math, this value is about 0.287\. We can think of this as
    telling us that we’re “paying” around 0.3 extra bits per word because we’re using
    the wrong code (Kurt 2017). The KL divergence for sending *Huckleberry Finn* with
    theTreasure Island code, or KL(Huckleberry Finn||Treasure Island), is much higher,
    at about 0.5\.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 像交叉熵一样，KL 散度是非对称的：参数的顺序很重要。用 Huckleberry Finn 编码发送 *Treasure Island* 的 KL 散度记作
    KL(Treasure Island||Huckleberry Finn)。中间的两个竖线可以看作是一个分隔符，类似于我们更常见的逗号。我们可以将其理解为“使用…编码发送”的意思。如果我们通过数学计算，这个值大约是
    0.287。我们可以把它理解为告诉我们，因为我们使用了错误的编码，所以每个词大约“多付出”了 0.3 个额外比特（Kurt 2017）。用 *Treasure
    Island* 编码发送 *Huckleberry Finn* 的 KL 散度，记作 KL(Huckleberry Finn||Treasure Island)，则要高得多，大约是
    0.5。
- en: The KL divergence tells us the number of additional bits we need in order to
    send our message with an imperfect code. Another way to think about this is that
    the KL divergence describes how much more information we need to turn our imperfectly
    adapted code into a perfect one. We can imagine this as a step of Bayes’ Rule,
    where we go from an approximate prior (the imperfect code) to a better posterior
    (the adapted code). In this case, the KL divergence is telling us just how much
    we learn from that idealized step of Bayes’ Rule (Thomas 2017).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: KL 散度告诉我们，为了使用不完美的编码发送消息，我们需要多少额外的比特。另一种理解方式是，KL 散度描述了我们需要多少更多的信息，将我们的不完美适配的编码转换为完美编码。我们可以将其想象为贝叶斯规则的一步，在这一步中，我们从一个近似的先验（不完美编码）转换为一个更好的后验（适配的编码）。在这种情况下，KL
    散度告诉我们从理想化的贝叶斯规则步骤中，我们学到了多少东西（Thomas 2017）。
- en: We can train our systems either by minimizing the KL divergence, or the cross
    entropy, choosing whichever is more convenient. The KL divergence has nice mathematical
    properties and shows up in many mathematical and algorithmic discussions and even
    deep learning documentation. But in practice, the cross entropy is almost always
    faster to compute. Since minimizing either one has the same effect of improving
    our system, we usually see KL divergence in technical discussions, and cross entropy
    in deep learning programs.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过最小化KL散度或交叉熵来训练系统，选择哪个更方便都可以。KL散度具有良好的数学性质，并且出现在许多数学和算法讨论中，甚至在深度学习文档中也会涉及。但是在实践中，计算交叉熵几乎总是更快的。由于最小化任意一个都能改善系统的效果，因此我们通常在技术讨论中看到KL散度，而在深度学习程序中看到交叉熵。
- en: Summary
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter we looked at some of the basic ideas behind information theory,
    and how we can use them to train a deep learning system. We use these ideas in
    machine learning by translating our codes into probability distributions. That
    just means identifying the code elements with the smallest code numbers as the
    most frequent elements, and as the size of the number goes up, the frequency goes
    down. Interpreted this way, we can calculate the cross entropy of a classifier
    by comparing the list of predicted probabilities it produces in response to an
    input with the list of probabilities we assigned by hand. Our goal in training
    is to make the two distributions as similar as possible, which we can also state
    as trying to minimize the cross entropy.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了一些信息理论的基本思想，以及如何利用这些思想来训练深度学习系统。我们通过将代码转换为概率分布来在机器学习中运用这些思想。这意味着我们将代码中最小的代码元素视为最频繁的元素，而随着数字的增大，频率则会下降。从这个角度解读，我们可以通过比较分类器对输入产生的预测概率列表与我们手动分配的概率列表，来计算交叉熵。我们在训练中的目标是尽可能使这两个分布相似，我们也可以将其表述为尽量最小化交叉熵。
- en: This wraps up the first part of the book. We’ve covered some fundamental ideas
    that have value far beyond deep learning. Statistics, probability, Bayes’ Rule,
    curves, and information theory all can help us make sense of a wide variety of
    problems and even things that come up in everyday life. They can help us improve
    our reasoning about events that happen in the world, and thus help us understand
    the past and prepare for the future.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这标志着本书的第一部分结束。我们已经涵盖了一些具有深远价值的基本概念，这些概念远超深度学习的范畴。统计学、概率论、贝叶斯定理、曲线和信息理论都能帮助我们理解各种各样的问题，甚至是日常生活中遇到的事物。它们能帮助我们改善对世界中发生事件的推理，从而帮助我们理解过去并为未来做准备。
- en: With these fundamentals in our pocket, we’ll now turn to the basic tools of
    machine learning.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有了这些基本原理后，我们将转向机器学习的基本工具。
