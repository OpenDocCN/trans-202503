- en: '**25'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**25**'
- en: CONFIDENCE INTERVALS**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**置信区间**'
- en: '![Image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/common.jpg)'
- en: What are the different ways to construct confidence intervals for machine learning
    classifiers?
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 构建机器学习分类器置信区间的不同方法有哪些？
- en: There are several ways to construct confidence intervals for machine learning
    models, depending on the model type and the nature of your data. For instance,
    some methods are computationally expensive when working with deep neural networks
    and are thus more suitable to less resource-intensive machine learning models.
    Others require larger datasets to be reliable.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种构建机器学习模型置信区间的方法，这取决于模型类型和数据的性质。例如，某些方法在处理深度神经网络时计算成本高昂，因此更适合于资源较少的机器学习模型。其他方法则需要更大的数据集才能可靠。
- en: 'The following are the most common methods for constructing confidence intervals:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是构建置信区间的最常见方法：
- en: Constructing normal approximation intervals based on a test set
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于测试集构建正态近似区间
- en: Bootstrapping training sets
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自举训练集**'
- en: Bootstrapping the test set predictions
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对测试集预测进行自举
- en: Confidence intervals from retraining models with different random seeds
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用不同的随机种子重新训练模型的置信区间
- en: Before reviewing these in greater depth, let’s briefly review the definition
    and interpretation of confidence intervals.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在更深入审查这些内容之前，让我们简要回顾一下置信区间的定义和解释。
- en: '**Defining Confidence Intervals**'
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**定义置信区间**'
- en: A *confidence interval* is a type of method to estimate an unknown population
    parameter. A *population parameter* is a specific measure of a statistical population,
    for example, a mean (average) value or proportion. By “specific” measure, I mean
    there is a single, exact value for that parameter for the entire population. Even
    though this value may not be known and often needs to be estimated from a sample,
    it is a fixed and definite characteristic of the population. A *statistical population*,
    in turn, is the complete set of items or individuals we study.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*置信区间*是一种估计未知总体参数的方法。*总体参数*是统计总体的特定度量，例如平均值或比例。通过“特定”度量，我指的是对于整个总体而言，该参数有一个单一的确切值。尽管这个值可能未知并且通常需要从样本中估计，但它是总体的固定和确定的特征。*统计总体*则是我们研究的所有项目或个体的完整集合。'
- en: In a machine learning context, the population could be considered the entire
    possible set of instances or data points that the model may encounter, and the
    parameter we are often most interested in is the true generalization accuracy
    of our model on this population.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习的背景下，总体可以被认为是模型可能遇到的所有可能实例或数据点的整体集合，我们通常最感兴趣的参数是模型在这个总体上的真实泛化准确性。
- en: The accuracy we measure on the test set estimates the true generalization accuracy.
    However, it’s subject to random error due to the specific sample of test instances
    we happened to use. This is where the concept of a confidence interval comes in.
    A 95 percent confidence interval for the generalization accuracy gives us a range
    in which we can be reasonably sure that the true generalization accuracy lies.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在测试集上测量的准确性估计了真实的泛化准确性。但是，由于我们使用了特定的测试实例样本，这种测量受到随机误差的影响。这就是置信区间的概念发挥作用的地方。泛化准确性的95%置信区间为我们提供了一个范围，在这个范围内我们可以相当确定真实的泛化准确性。
- en: For instance, if we take 100 different data samples and compute a 95 percent
    confidence interval for each sample, approximately 95 of the 100 confidence intervals
    will contain the true population value (such as the generalization accuracy),
    as illustrated in [Figure 25-1](ch25.xhtml#ch25fig1).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们取100个不同的数据样本，并为每个样本计算95%置信区间，大约有100个置信区间中的95个将包含真实的总体值（例如泛化准确性），如[图25-1](ch25.xhtml#ch25fig1)所示。
- en: '![Image](../images/25fig01.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/25fig01.jpg)'
- en: '*Figure 25-1: The concept of 95 percent confidence intervals*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*图25-1：95%置信区间的概念*'
- en: More concretely, if we were to draw 100 different representative test sets from
    the population (for instance, the entire possible set of instances that the model
    may encounter) and compute the 95 percent confidence interval for the generalization
    accuracy from each test set, we would expect about 95 of these intervals to contain
    the true generalization accuracy.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，如果我们从总体中抽取100个不同的代表性测试集，并计算每个测试集的95%置信区间，我们预期大约有95个这些区间将包含真实的泛化准确性。
- en: We can display confidence intervals in several ways. It is common to use a bar
    plot representation where the top of the bar represents the parameter value (for
    example, model accuracy) and the whiskers denote the upper and lower levels of
    the confidence interval (left chart of [Figure 25-2](ch25.xhtml#ch25fig2)). Alternatively,
    the confidence intervals can be shown without bars, as in the right chart of [Figure
    25-2](ch25.xhtml#ch25fig2).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过几种方式来展示置信区间。常见的做法是使用条形图表示，其中条形的顶部代表参数值（例如，模型准确度），而胡须表示置信区间的上下限（[图 25-2](ch25.xhtml#ch25fig2)的左侧图）。另外，也可以像[图
    25-2](ch25.xhtml#ch25fig2)的右侧图那样，直接展示没有条形的置信区间。
- en: '![Image](../images/25fig02.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/25fig02.jpg)'
- en: '*Figure 25-2: Two common plotting variants to illustrate confidence intervals*'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 25-2：两种常见的绘图变体用于说明置信区间*'
- en: 'This visualization is functionally useful in a number of ways. For instance,
    when confidence intervals for two model performances do *not* overlap, it’s a
    strong visual indicator that the performances are significantly different. Take
    the example of statistical significance tests, such as t-tests: if two 95 percent
    confidence intervals do not overlap, it strongly suggests that the difference
    between the two measurements is statistically significant at the 0.05 level.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这种可视化在多方面都非常有用。例如，当两个模型性能的置信区间*不*重叠时，这是一个强烈的视觉指示，表明两者的性能有显著差异。以统计显著性检验为例，如 t
    检验：如果两个 95% 的置信区间不重叠，这强烈表明这两个测量值之间的差异在 0.05 的显著性水平上是统计显著的。
- en: On the other hand, if two 95 percent confidence intervals overlap, we cannot
    automatically conclude that there’s no significant difference between the two
    measurements. Even when confidence intervals overlap, there can still be a statistically
    significant difference.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果两个 95% 的置信区间重叠，我们不能自动得出两个测量值之间没有显著差异的结论。即使置信区间重叠，仍然可能存在统计上显著的差异。
- en: Alternatively, to provide more detailed information about the exact quantities,
    we can use a table view to express the confidence intervals. The two common notations
    are summarized in [Table 25-1](ch25.xhtml#ch25tab1).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，为了提供更详细的关于确切数值的信息，我们可以使用表格视图来表达置信区间。两种常见的表示方法总结在[表 25-1](ch25.xhtml#ch25tab1)中。
- en: '**Table 25-1:** Confidence Intervals'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 25-1：** 置信区间'
- en: '| Model | Dataset A | Dataset B | Dataset C |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 数据集 A | 数据集 B | 数据集 C |'
- en: '| --- | --- | --- | --- |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 1 | 89.1% *±* 1.7% | . . . | . . . |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 89.1% *±* 1.7% | . . . | . . . |'
- en: '| 2 | 79.5% *±* 2.2% | . . . | . . . |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 79.5% *±* 2.2% | . . . | . . . |'
- en: '| 3 | 95.2% *±* 1.6% | . . . | . . . |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 95.2% *±* 1.6% | . . . | . . . |'
- en: '| **Model** | **Dataset A** | **Dataset B** | **Dataset C** |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **数据集 A** | **数据集 B** | **数据集 C** |'
- en: '| 1 | 89.1% (87.4%, 90.8%) | . . . | . . . |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 89.1% (87.4%, 90.8%) | . . . | . . . |'
- en: '| 2 | 79.5% (77.3%, 81.7%) | . . . | . . . |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 79.5% (77.3%, 81.7%) | . . . | . . . |'
- en: '| 3 | 95.2% (93.6%, 96.8%) | . . . | . . . |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 95.2% (93.6%, 96.8%) | . . . | . . . |'
- en: The *±* notation is often preferred if the confidence interval is *symmetric*,
    meaning the upper and lower endpoints are equidistant from the estimated parameter.
    Alternatively, the lower and upper confidence intervals can be written explicitly.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果置信区间是*对称的*，即上下端点距离估计参数的距离相等，那么通常更倾向于使用*±*符号。或者，也可以明确地写出上下置信区间。
- en: '**The Methods**'
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**方法**'
- en: The following sections describe the four most common methods of constructing
    confidence intervals.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的部分将描述构建置信区间的四种最常见的方法。
- en: '***Method 1: Normal Approximation Intervals***'
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***方法 1：正态近似区间***'
- en: The normal approximation interval involves generating the confidence interval
    from a single train-test split. It is often considered the simplest and most traditional
    method for computing confidence intervals. This approach is especially appealing
    in the realm of deep learning, where training models is computationally costly.
    It’s also desirable when we are interested in evaluating a specific model, instead
    of models trained on various data partitions like in *k*-fold cross-validation.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 正态近似区间是通过单次训练-测试分割生成置信区间的。它通常被认为是计算置信区间最简单和最传统的方法。这种方法在深度学习领域特别受欢迎，因为训练模型的计算成本较高。当我们只对评估特定模型感兴趣，而不是像在*k*-折交叉验证中那样使用训练于不同数据划分的模型时，这种方法也更为理想。
- en: How does it work? In short, the formula for calculating the confidence interval
    for a predicted parameter (for example, the sample mean, denoted as ![Image](../images/x-bar.jpg)),
    assuming a normal distribution, is expressed as ![Image](../images/x-bar.jpg)
    *± z × SE*.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 它是如何工作的？简而言之，计算预测参数的置信区间的公式（例如，样本均值，用![Image](../images/x-bar.jpg)表示），假设数据服从正态分布，可以表示为
    ![Image](../images/x-bar.jpg) *± z × SE*。
- en: In this formula, *z* represents the *z*-score, which indicates a particular
    value’s number of standard deviations from the mean in a standard normal distribution.
    *SE* represents the standard error of the predicted parameter (in this case, the
    sample mean).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个公式中，*z*表示*z*-得分，它表示一个特定值距离标准正态分布均值的标准差数。*SE*表示预测参数的标准误差（在这种情况下，是样本均值）。
- en: '**NOTE**'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*Most readers will be familiar with* z*-score tables that are usually found
    in the back of introductory statistics textbooks. However, a more convenient and
    preferred way to obtain* z*-scores is to use functions like SciPy’s* stats.zscore
    *function, which computes the* z*-scores for given confidence levels.*'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*大多数读者应该熟悉* z*-得分表，这些表通常出现在入门统计学教材的后面。然而，更方便且更受欢迎的获取* z*-得分的方式是使用像SciPy的* stats.zscore
    *函数，它可以计算给定置信水平的* z*-得分。*'
- en: For our scenario, the sample mean, denoted as ![Image](../images/x-bar.jpg),
    corresponds to the test set accuracy, ACC[test], a measure of successful predictions
    in the context of a binomial proportion confidence interval.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的场景，样本均值，用![Image](../images/x-bar.jpg)表示，等同于测试集准确度，ACC[test]，这是在二项分布比例置信区间中衡量成功预测的指标。
- en: 'The standard error can be calculated under a normal approximation as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在正态近似下，标准误差可以按以下方式计算：
- en: '![Image](../images/f0166-01.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0166-01.jpg)'
- en: 'In this equation, *n* signifies the size of the test set. Substituting the
    standard error back into the previous formula, we obtain the following:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，*n*表示测试集的大小。将标准误差代入前面的公式中，我们得到以下结果：
- en: '![Image](../images/f0166-02.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0166-02.jpg)'
- en: Additional code examples to implement this method can also be found in the *supplementary/q25_confidence-intervals*
    subfolder in the supplementary code repository at *[https://github.com/rasbt/MachineLearning-QandAI-book](https://github.com/rasbt/MachineLearning-QandAI-book)*.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 实现该方法的附加代码示例也可以在补充代码库的*补充/q25_confidence-intervals*子文件夹中找到，地址为*[https://github.com/rasbt/MachineLearning-QandAI-book](https://github.com/rasbt/MachineLearning-QandAI-book)*。
- en: While the normal approximation interval method is very popular due to its simplicity,
    it has some downsides. First, the normal approximation may not always be accurate,
    especially for small sample sizes or for data that is not normally distributed.
    In such cases, other methods of computing confidence intervals may be more accurate.
    Second, using a single train-test split does not provide information about the
    variability of the model performance across different splits of the data. This
    can be an issue if the performance is highly dependent on the specific split used,
    which may be the case if the dataset is small or if there is a high degree of
    variability in the data.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管由于其简单性，正态近似区间方法非常流行，但它也有一些缺点。首先，正态近似可能并不总是准确，尤其是对于小样本量或数据不服从正态分布的情况。在这种情况下，其他计算置信区间的方法可能更为准确。其次，使用单一的训练-测试划分不能提供关于模型在不同数据划分下表现的变异性的信息。如果模型的表现高度依赖于特定的划分，这可能会成为一个问题，尤其是在数据集较小或数据变异性较大时。
- en: '***Method 2: Bootstrapping Training Sets***'
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***方法2：自助法训练集***'
- en: Confidence intervals serve as a tool for approximating unknown parameters. However,
    when we are restricted to just one estimate, such as the accuracy derived from
    a single test set, we must make certain assumptions to make this work. For example,
    when we used the normal approximation interval described in the previous section,
    we assumed normally distributed data, which may or may not hold.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 置信区间作为一种工具，用于近似未知参数。然而，当我们仅限于一个估计时，比如从单一测试集中得出的准确度，我们必须做出一些假设才能使其有效。例如，当我们使用前面章节中描述的正态近似区间时，我们假设数据服从正态分布，但这可能成立，也可能不成立。
- en: In a perfect scenario, we would have more insight into our test set sample distribution.
    However, this would require access to many independent test datasets, which is
    typically not feasible. A workaround is the bootstrap method, which resamples
    existing data to estimate the sampling distribution.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个完美的场景中，我们可以更深入地了解我们的测试集样本分布。然而，这需要访问许多独立的测试数据集，这通常是不可行的。一个解决方法是自助法，它通过重新抽样现有数据来估计抽样分布。
- en: '**NOTE**'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*In practice, when the test set is large enough, the normal distribution approximation
    will hold, thanks to the central limit theorem. This theorem states that the sum
    (or average) of a large number of independent, identically distributed random
    variables will approach a normal distribution, regardless of the underlying distribution
    of the individual variables. It is difficult to specify what constitutes a large-enough
    test set. However, under stronger assumptions than those of the central limit
    theorem, we can at least estimate the rate of convergence to the normal distribution
    using the Berry–Esseen theorem, which gives a more quantitative estimate of how
    quickly the convergence in the central limit theorem occurs.*'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*实际上，当测试集足够大时，由于中心极限定理，正态分布的近似将成立。该定理表明，大量独立同分布的随机变量的和（或平均值）将趋近于正态分布，无论个别变量的基础分布如何。很难确切指定什么样的测试集算足够大。然而，在比中心极限定理更强的假设下，我们至少可以利用贝里–埃森定理估计收敛到正态分布的速度，这为我们提供了收敛速度的量化估计。*'
- en: In a machine learning context, we can take the original dataset and draw a random
    sample *with replacement*. If the dataset has size *n* and we draw a random sample
    with replacement of size *n*, this implies that some data points will likely be
    duplicated in this new sample, whereas other data points are not sampled at all.
    We can then repeat this procedure for multiple rounds to obtain multiple training
    and test sets. This process is known as *out-of-bag bootstrapping*, illustrated
    in [Figure 25-3](ch25.xhtml#ch25fig3).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，我们可以取原始数据集并进行带有替换的随机抽样。如果数据集大小为 *n*，并且我们进行大小为 *n* 的带替换随机抽样，这意味着某些数据点可能会在这个新样本中重复，而其他数据点根本没有被抽取。然后，我们可以重复这个过程多次，得到多个训练集和测试集。这个过程称为
    *袋外自助法*，如[图25-3](ch25.xhtml#ch25fig3)所示。
- en: '![Image](../images/25fig03.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/25fig03.jpg)'
- en: '*Figure 25-3: Out-of-bag bootstrapping evaluates models on resampled training
    sets.*'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*图25-3：袋外自助法在重新抽样的训练集上评估模型。*'
- en: Suppose we constructed *k* training and test sets. We can now take each of these
    splits to train and evaluate the model to obtain *k* test set accuracy estimates.
    Considering this distribution of test set accuracy estimates, we can take the
    range between the 2.5th and 97.5th percentile to obtain the 95 percent confidence
    interval, as illustrated in [Figure 25-4](ch25.xhtml#ch25fig4).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们构建了 *k* 个训练集和测试集。我们现在可以利用这些划分来训练和评估模型，以获得 *k* 次测试集的准确度估计值。考虑到这些测试集准确度估计值的分布，我们可以取2.5百分位和97.5百分位之间的范围来获得95%的置信区间，如[图25-4](ch25.xhtml#ch25fig4)所示。
- en: '![Image](../images/25fig04.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/25fig04.jpg)'
- en: '*Figure 25-4: The distribution of test accuracies from 1,000 bootstrap samples,
    including a 95 percent confidence interval*'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*图25-4：来自1,000次自助样本的测试准确度分布，包括95%的置信区间*'
- en: Unlike the normal approximation interval method, we can consider this out-of-bag
    bootstrap approach to be more agnostic to the specific distribution. Ideally,
    if the assumptions for the normal approximation are satisfied, both methodologies
    would yield identical outcomes.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 与正态近似区间方法不同，我们可以认为这种袋外自助法更不依赖于具体的分布。理想情况下，如果正态近似的假设得到满足，两种方法将会产生相同的结果。
- en: Since bootstrapping relies on resampling the existing test data, its downside
    is that it doesn’t bring in any new information that could be available in a broader
    population or unseen data. Therefore, it may not always be able to generalize
    the performance of the model to new, unseen data.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 由于自助法依赖于重新抽样现有的测试数据，它的缺点是无法引入来自更广泛人群或未见数据的任何新信息。因此，它可能无法始终将模型的表现推广到新的、未见的数据。
- en: Note that we are using the bootstrap sampling approach in this chapter instead
    of obtaining the train-test splits via *k*-fold cross-validation, because of the
    bootstrap’s theoretical grounding via the central limit theorem discussed earlier.
    There are also more advanced out-of-bag bootstrap methods, such as the .632 and
    .632+ estimates, which are reweighting the accuracy estimates.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，本章使用的是自助抽样方法，而不是通过*k*-折交叉验证获取训练-测试集划分，因为自助抽样方法在中央极限定理的理论基础上具有更强的支持。还有更高级的袋外自助抽样方法，如.632和.632+估计，它们通过重新加权精度估计来进行改进。
- en: '***Method 3: Bootstrapping Test Set Predictions***'
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***方法3：自助抽样测试集预测***'
- en: An alternative approach to bootstrapping training sets is to bootstrap test
    sets. The idea is to train the model on the existing training set as usual and
    then to evaluate the model on bootstrapped test sets, as illustrated in [Figure
    25-5](ch25.xhtml#ch25fig5). After obtaining the test set performance estimates,
    we can then apply the percentile method described in the previous section.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 自助抽样训练集的替代方法是自助抽样测试集。其思想是按常规在现有的训练集上训练模型，然后在自助抽样的测试集上评估模型，如[图25-5](ch25.xhtml#ch25fig5)所示。在获得测试集的性能估计后，我们可以应用前一节中描述的百分位方法。
- en: '![Image](../images/25fig05.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/25fig05.jpg)'
- en: '*Figure 25-5: Bootstrapping the test set*'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*图25-5：自助抽样测试集*'
- en: Contrary to the prior bootstrap technique, this method uses a trained model
    and simply resamples the test set (instead of the training sets). This approach
    is especially appealing for evaluating deep neural networks, as it doesn’t require
    retraining the model on the new data splits. However, a disadvantage of this approach
    is that it doesn’t assess the model’s variability toward small changes in the
    training data.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的自助抽样技术不同，这种方法使用已经训练好的模型，仅对测试集进行重采样（而不是训练集）。这种方法特别适用于评估深度神经网络，因为它不需要在新的数据划分上重新训练模型。然而，这种方法的一个缺点是，它不能评估模型对训练数据小变化的敏感性。
- en: '***Method 4: Retraining Models with Different Random Seeds***'
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***方法4：使用不同随机种子重新训练模型***'
- en: 'In deep learning, models are commonly retrained using various random seeds
    since some random weight initializations may lead to much better models than others.
    How can we build a confidence interval from these experiments? If we assume that
    the sample means follow a normal distribution, we can employ a previously discussed
    method where we calculate the confidence interval around a sample mean, denoted
    as ![Image](../images/x-bar.jpg), as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，模型通常会使用不同的随机种子进行重新训练，因为某些随机权重初始化可能会导致比其他初始化更好的模型。我们如何从这些实验中构建置信区间？如果我们假设样本均值服从正态分布，那么我们可以采用之前讨论过的方法，在样本均值周围计算置信区间，记作
    ![Image](../images/x-bar.jpg)，如下所示：
- en: '![Image](../images/f0169-01.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0169-01.jpg)'
- en: Since in this context we often work with a relatively modest number of samples
    (for instance, models from 5 to 10 random seeds), assuming a *t* distribution
    is deemed more suitable than a normal distribution. Therefore, we substitute the
    *z* value with a *t* value in the preceding formula. (As the sample size increases,
    the *t* distribution tends to look more like the standard normal distribution,
    and the critical values [*z* and *t*] become increasingly similar.)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在这种情况下，我们通常处理的样本数量相对较少（例如，从5到10个随机种子的模型），因此假设*t*分布比正态分布更为适用。因此，我们在之前的公式中将*z*值替换为*t*值。（随着样本量的增加，*t*分布趋向于与标准正态分布相似，而临界值[*z*和*t*]也会越来越接近。）
- en: 'Furthermore, if we are interested in the average accuracy, denoted as ![Image](../images/f0170-01.jpg),
    we consider ACC[test, *j*] corresponding to a unique random seed *j* as a sample.
    The number of random seeds we evaluate would then constitute the sample size *n*.
    As such, we would calculate:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果我们关心平均精度，记作 ![Image](../images/f0170-01.jpg)，我们将ACC[test, *j*]，即对应于唯一随机种子*j*的精度，视为一个样本。我们评估的随机种子数量将构成样本大小*n*。因此，我们将计算：
- en: '![Image](../images/f0170-02.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0170-02.jpg)'
- en: Here, SE is the standard error, calculated as ![Image](../images/f0170-03.jpg),
    while
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，SE是标准误差，计算公式为 ![Image](../images/f0170-03.jpg)，而
- en: '![Image](../images/f0170-04.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0170-04.jpg)'
- en: 'is the average accuracy, which we compute over the *r* random seeds. The standard
    deviation SD is calculated as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 是平均精度，我们在*r*个随机种子上计算得到。标准差SD的计算公式如下：
- en: '![Image](../images/f0170-05.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0170-05.jpg)'
- en: To summarize, calculating the confidence intervals using various random seeds
    is another effective alternative. However, it is primarily beneficial for deep
    learning models. It proves to be costlier than both the normal approximation approach
    (method 1) and bootstrapping the test set (method 3), as it necessitates retraining
    the model. On the bright side, the outcomes derived from disparate random seeds
    provide us with a robust understanding of the model’s stability.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，使用不同随机种子计算置信区间是另一种有效的替代方案。然而，它主要对深度学习模型有益。与正态近似法（方法1）和自助法测试集（方法3）相比，它的成本更高，因为它需要重新训练模型。好的一面是，来自不同随机种子的结果为我们提供了对模型稳定性的深入理解。
- en: '**Recommendations**'
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**建议**'
- en: Each possible method for constructing confidence intervals has its unique advantages
    and disadvantages. The normal approximation interval is cheap to compute but relies
    on the normality assumption about the distribution. The out-of-bag bootstrap is
    agnostic to these assumptions but is substantially more expensive to compute.
    A cheaper alternative is bootstrapping the test only, but this involves bootstrapping
    a smaller dataset and may be misleading for small or nonrepresentative test set
    sizes. Lastly, constructing confidence intervals from different random seeds is
    expensive but can give us additional insights into the model’s stability.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 构建置信区间的每种方法都有其独特的优缺点。正态近似区间计算便宜，但依赖于分布的正态性假设。袋外自助法对这些假设无偏见，但计算开销大。一个更便宜的替代方案是仅对测试集进行自助法，但这涉及对较小的数据集进行自助抽样，可能会对小规模或非代表性的测试集产生误导。最后，从不同的随机种子构建置信区间虽然计算成本高，但能为我们提供关于模型稳定性的额外见解。
- en: '**Exercises**'
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**练习**'
- en: '**25-1.** As mentioned earlier, the most common choice of confidence level
    is 95 percent confidence intervals. However, 90 percent and 99 percent are also
    common. Are 90 percent confidence intervals smaller or wider than 95 percent confidence
    intervals, and why is this the case?'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**25-1.** 如前所述，最常见的置信度选择是95%的置信区间。然而，90%和99%也很常见。90%的置信区间比95%的置信区间更小还是更大，为什么会这样？'
- en: '**25-2.** In “Method 3: Bootstrapping Test Set Predictions” on [page 169](ch25.xhtml#ch00levsec30),
    we created test sets by bootstrapping and then applied the already trained model
    to compute the test set accuracy on each of these datasets. Can you think of a
    method or modification to obtain these test accuracies more efficiently?'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**25-2.** 在《方法3：自助法测试集预测》（[第169页](ch25.xhtml#ch00levsec30)）中，我们通过自助抽样创建了测试集，然后应用已训练的模型计算每个数据集上的测试集准确性。你能想到一种方法或修改来更高效地获得这些测试准确性吗？'
- en: '**References**'
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: 'A detailed discussion of the pitfalls of concluding statistical significance
    from nonoverlapping confidence intervals: Martin Krzywinski and Naomi Altman,
    “Error Bars” (2013), *[https://www.nature.com/articles/nmeth.2659](https://www.nature.com/articles/nmeth.2659)*.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 详细讨论了从不重叠置信区间得出统计显著性结论的陷阱：Martin Krzywinski 和 Naomi Altman，《误差条》（2013），*［https://www.nature.com/articles/nmeth.2659](https://www.nature.com/articles/nmeth.2659)*。
- en: 'A more detailed explanation of the binomial proportion confidence interval:
    *[https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval](https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval)*.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于二项比例置信区间的更详细解释：*［https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval](https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval)*。
- en: 'For a detailed explanation of normal approximation intervals, see Section 1.7
    of my article: “Model Evaluation, Model Selection, and Algorithm Selection in
    Machine Learning” (2018), *[https://arxiv.org/abs/1811.12808](https://arxiv.org/abs/1811.12808)*.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于正态近似区间的详细解释，请参见我的文章第1.7节：“机器学习中的模型评估、模型选择和算法选择”（2018），*［https://arxiv.org/abs/1811.12808](https://arxiv.org/abs/1811.12808)*。
- en: 'Additional information on the central limit theorem for independent and identically
    distributed random variables: *[https://en.wikipedia.org/wiki/Central_limit_theorem](https://en.wikipedia.org/wiki/Central_limit_theorem)*.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于独立同分布随机变量的中心极限定理的更多信息：*［https://en.wikipedia.org/wiki/Central_limit_theorem](https://en.wikipedia.org/wiki/Central_limit_theorem)*。
- en: 'For more on the Berry–Esseen theorem: *[https://en.wikipedia.org/wiki/Berry–Esseen_theorem](https://en.wikipedia.org/wiki/Berry%E2%80%93Esseen_theorem)*.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更多关于 Berry–Esseen 定理的信息：*［https://en.wikipedia.org/wiki/Berry–Esseen_theorem](https://en.wikipedia.org/wiki/Berry%E2%80%93Esseen_theorem)*。
- en: 'The .632 bootstrap addresses a pessimistic bias of the regular outof-bag bootstrapping
    approach: Bradley Efron, “Estimating the Error Rate of a Prediction Rule: Improvement
    on Cross-Validation” (1983), *[https://www.jstor.org/stable/2288636](https://www.jstor.org/stable/2288636)*.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: .632 自助法解决了常规袋外自助法方法的悲观偏差：Bradley Efron，“估计预测规则的错误率：交叉验证的改进”（1983），*[https://www.jstor.org/stable/2288636](https://www.jstor.org/stable/2288636)*。
- en: 'The .632+ bootstrap corrects an optimistic bias introduced in the .632 bootstrap:
    Bradley Efron and Robert Tibshirani, “Improvements on Cross-Validation: The .632+
    Bootstrap Method” (1997), *[https://www.jstor.org/stable/2965703](https://www.jstor.org/stable/2965703)*.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: .632+ 自助法修正了在 .632 自助法中引入的乐观偏差：Bradley Efron 和 Robert Tibshirani，“交叉验证的改进：.632+
    自助法”（1997），*[https://www.jstor.org/stable/2965703](https://www.jstor.org/stable/2965703)*。
- en: 'A deep learning research paper that discusses bootstrapping the test set predictions:
    Benjamin Sanchez-Lengeling et al., “Machine Learning for Scent: Learning Generalizable
    Perceptual Representations of Small Molecules” (2019), *[https://arxiv.org/abs/1910.10685](https://arxiv.org/abs/1910.10685)*.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论引导测试集预测的深度学习研究论文：Benjamin Sanchez-Lengeling 等人，“香气的机器学习：学习小分子的通用感知表示”（2019），*[https://arxiv.org/abs/1910.10685](https://arxiv.org/abs/1910.10685)*。
