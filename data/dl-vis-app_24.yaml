- en: '20'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '20'
- en: Attention and Transformers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力和变换器
- en: '![](Images/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/chapterart.png)'
- en: In Chapter 19 we looked at how to use RNNs to handle sequential data. Though
    powerful, RNNs have a few drawbacks. Because all of the information about an input
    is represented in a single piece of state memory, or context vector, the networks
    inside each recurrent cell need to work hard to compress everything that’s needed
    into the available space. And no matter how large we make the state memory, we
    can always get an input that exceeds what the memory can hold, so something necessarily
    gets lost.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在第19章中，我们探讨了如何使用RNN处理序列数据。尽管RNN非常强大，但也有一些缺点。因为所有关于输入的信息都通过一个单一的状态记忆（或上下文向量）来表示，每个递归单元内部的网络需要努力将所有必要的信息压缩到有限的空间中。而且，无论我们将状态记忆做得多大，总会遇到超出记忆容量的输入，因此总有一些信息会丢失。
- en: Another problem is that an RNN must be trained and used one word at a time.
    This can be a slow way to work, particularly with large databases.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题是，RNN必须一次处理一个词。这可能是一种较慢的工作方式，尤其是在处理大型数据库时。
- en: An alternative approach is based on a small network called an *attention network*,
    which doesn’t have a state memory and can be trained and used in parallel. Attention
    networks can be combined into larger structures called *transformers*, which are
    capable of serving as language models that can perform tasks like translation.
    The building blocks of transformers can be used in other architectures that provide
    even more powerful language models, including generators.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 一种替代方法基于一个叫做*注意力网络*的小型网络，这种网络没有状态记忆，可以并行训练和使用。注意力网络可以组合成更大的结构，叫做*变换器*（transformers），这些变换器能够作为语言模型执行像翻译这样的任务。变换器的构建模块可以用于其他架构，提供更强大的语言模型，包括生成器。
- en: In this chapter, we start with a more powerful way to represent words rather
    than as single numbers, and then build our way up to attention and modern architectures
    that use transformer blocks to perform many NLP tasks.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先介绍了一种更强大的方式来表示词汇，而不是将其作为单一数字，然后逐步构建我们的注意力机制和使用变换器模块执行多种自然语言处理任务的现代架构。
- en: Embedding
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入
- en: In Chapter 19 we promised to improve our word descriptions beyond a single number.
    The value of this change is that it allows us to manipulate the representations
    of words in meaningful ways. For example, we can find a word that is like another
    word, or we can blend two words to find one that’s in between them. This concept
    is key to developing attention, and then transformers.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在第19章中，我们承诺将我们的词汇描述提升到不仅仅是一个数字的层次。这一变化的价值在于它让我们可以以有意义的方式操作词汇的表示。例如，我们可以找到一个与另一个词相似的词，或者我们可以将两个词融合，找到它们之间的一个词。这个概念是发展注意力机制以及后续变换器（transformer）的关键。
- en: The technique is called *word embedding* (or *token embedding* when we use it
    on the more general idea of a token). It’s a bit abstract, so let’s see the ideas
    first with a concrete example.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术称为*词嵌入*（或者当我们应用于更一般的标记概念时，称为*标记嵌入*）。这个概念有点抽象，所以让我们先通过一个具体的例子来看一下这些想法。
- en: Suppose that you work as an animal wrangler on a movie with a tempestuous director.
    Today you’re filming a sequence where the human heroes are chased by some animals.
    The director asks you for a list of animals you can provide in sufficient numbers
    to produce a scary chase. You call your office, they prepare the list, and they
    even arrange those animals into a chart, where the horizontal axis represents
    each adult animal’s average top speed and the vertical axis represents its average
    weight, as in [Figure 20-1](#figure20-1).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你在一部电影中担任动物驯养员，而导演脾气暴躁。今天你正在拍摄一个场景，其中人类英雄被一些动物追赶。导演要求你提供一个可以在数量上满足需求的动物列表，用来拍摄一个令人害怕的追逐场面。你打电话给办公室，他们准备了这个列表，并且将这些动物安排成一张图表，水平轴代表每种成年动物的平均最高速度，垂直轴代表其平均体重，如图[20-1](#figure20-1)所示。
- en: '![F20001](Images/F20001.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![F20001](Images/F20001.png)'
- en: 'Figure 20-1: A collection of animals, organized roughly by land speed horizontally
    and adult weight vertically, though those axis labels aren’t shown (data from
    Reisner 2020)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图20-1：一组动物，按照地面速度大致水平排列，成年体重垂直排列，尽管这些轴标签没有显示（数据来源：Reisner 2020）
- en: But due to a printer error, the chart your office sent you is missing the labels
    on the axes, so you have the chart with the animals laid out in 2D, but you don’t
    know what the axes mean.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，由于打印错误，你的办公室发给你的图表缺少了轴标签，所以你手上有一张二维图表，动物们被排列在其中，但你不知道这些轴代表什么。
- en: The director doesn’t even look at the chart. “Horses,” she says, “I want horses.
    They’re exactly what I want and will be perfect and nothing else will do.” So
    you bring in the horses, and they rehearse the scene.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 导演甚至没有看图表。“马，”她说，“我想要马。它们正是我想要的，完美无缺，别的都不行。”于是你带来了马，它们开始排练场景。
- en: Unfortunately, the director is unhappy. “No, no, no!” she says. “The horses
    are too twitchy and quick. They’re like foxes. Give me horses that are less fox-like.”
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，导演不满意。“不，不，不！”她说。“这些马太过敏捷和迅速，像狐狸一样。给我一些不那么像狐狸的马。”
- en: How on Earth can you satisfy this request? What does it even mean? Happily,
    you can do just as she asks with the chart, just by combining arrows.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 你怎么可能满足这个要求呢？这到底是什么意思？幸运的是，你可以按照她的要求，利用图表通过组合箭头来实现。
- en: 'You only need to do two things with arrows: add them and subtract them. To
    add arrow B to arrow A, place the tail of B onto the head of A. The new arrow
    A + B starts at the tail of A, and ends at the head of B, as in the middle of
    [Figure 20-2](#figure20-2).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你只需要做两件事：加箭头和减箭头。要将箭头B加到箭头A上，把B的尾部放到A的头部。新的箭头A + B从A的尾部开始，到B的头部结束，如[图20-2](#figure20-2)中间所示。
- en: '![F20002](Images/F20002.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![F20002](Images/F20002.png)'
- en: 'Figure 20-2: Arrow arithmetic. Left: Two arrows. Middle: The sum A + B. Right:
    The difference A – B.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图20-2：箭头运算。左：两个箭头。中：和A + B。右：差A – B。
- en: To subtract B from A, just flip B around by 180 degrees to make –B, and add
    together A and –B. The result, A – B, starts at the tail of A and ends at the
    head of –B, as in the right of [Figure 20-2](#figure20-2).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 要从A中减去B，只需将B逆时针旋转180度，变成–B，然后将A和–B加在一起。结果，A – B，从A的尾部开始，到–B的头部结束，如[图20-2](#figure20-2)右侧所示。
- en: Now you can satisfy the director’s desire to remove the fox qualities from the
    horses. Start by drawing an arrow from the bottom left of the chart to the horse,
    and another to the fox, as in the left of [Figure 20-3](#figure20-3).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以满足导演移除马身上狐狸特征的要求。首先，从图表的左下角画一支箭头指向马，再画一支箭头指向狐狸，如[图20-3](#figure20-3)左侧所示。
- en: '![F20003](Images/F20003.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![F20003](Images/F20003.png)'
- en: 'Figure 20-3: Left: Arrows from the bottom left to the horse and fox. Right:
    Subtracting fox from horse gives us a giant sloth.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图20-3：左：从左下角到马和狐狸的箭头。右：从马中减去狐狸得到一只巨大的树懒。
- en: Now subtract foxes from horses, as requested, by subtracting the fox arrow from
    the horse arrow. Following the rules of [Figure 20-2](#figure20-2), that means
    flipping the fox arrow around and placing its tail at the head of the horse arrow.
    We get the right side of [Figure 20-3](#figure20-3).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在按照要求从马中减去狐狸，方法是将狐狸箭头从马箭头中减去。按照[图20-2](#figure20-2)的规则，这意味着将狐狸箭头翻转并将其尾部放在马箭头的头部。我们得到[图20-3](#figure20-3)的右侧。
- en: 'A giant sloth. Well, okay, it’s what the director wanted. We can even write
    this like a little bit of arithmetic: horse – fox = giant sloth (at least, according
    to our diagram).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一只巨大的树懒。好吧，导演想要的就是这个。我们甚至可以像做一些小算术一样写出这个：马 – 狐狸 = 巨大树懒（至少根据我们的图示来说）。
- en: The director throws her latte on the ground. “No no no! Sure, sloths would look
    great, but they hardly move! Make them fast! Give me sloths that are like roadrunners!”
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 导演把她的拿铁扔到地上。“不不不！当然，树懒看起来很好，但它们几乎不动！让它们更快！给我像路跑鸟一样快的树懒！”
- en: 'Now we know just how to satisfy this ridiculous demand: find the arrow from
    the bottom left to the roadrunner, as shown in the left of [Figure 20-4](#figure20-4),
    and add that to the head of the arrow pointing to the sloth, giving us a brown
    bear. That is, horse – fox + roadrunner = brown bear, as in the right of [Figure
    20-4](#figure20-4).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何满足这个荒谬的要求：找到从左下角到路跑鸟的箭头，如[图20-4](#figure20-4)左侧所示，并将其添加到指向树懒的箭头的头部，就得到了一只棕熊。也就是说，马
    – 狐狸 + 路跑鸟 = 棕熊，如[图20-4](#figure20-4)右侧所示。
- en: '![f20004](Images/f20004.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![f20004](Images/f20004.png)'
- en: 'Figure 20-4: Left: We can draw an arrow to the roadrunner. Right: Giant sloth
    + roadrunner = brown bear.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图20-4：左：我们可以画一个指向路跑鸟的箭头。右：巨大的树懒 + 路跑鸟 = 棕熊。
- en: You offer the director a group of brown bears (called a *sleuth* of bears).
    The director rolls her eyes dramatically. “Finally. Something that’s fast like
    horses, but not twitchy like foxes, and quick like roadrunners. It’s only what
    I asked for in the first place.” They shoot the chase scene with bears, and the
    movie later comes out to great acclaim.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你给导演提供了一组棕熊（称为*棕熊群*）。导演翻了个白眼。“终于，像马一样快，但不像狐狸那么敏捷，像路跑鸟一样迅速。这正是我最初要求的。”他们用熊拍摄了追逐场景，电影后来获得了广泛好评。
- en: There are two key elements to this story. The first is that the animals in our
    chart were laid out in a useful way, even though we didn’t know what that way
    was, or what the axes represented about the data.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这个故事有两个关键要素。第一个是，我们图表中的动物虽然我们不知道它们的排列方式或坐标轴代表什么数据，但它们的布局非常有用。
- en: The second key point is that we didn’t need the axis labels after all. We were
    able to navigate the chart just by adding and subtracting arrows pointing to elements
    on the chart itself. That is, we didn’t try to find a “slower horse.” Rather,
    we worked strictly with the animals themselves, and their various attributes came
    along implicitly. Removing the speediness of a fox from a big animal like a horse
    gave us a big, slow animal.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个关键点是，最终我们发现其实不需要坐标轴标签。我们仅通过添加和减去指向图表中元素的箭头，就能在图表中导航。也就是说，我们没有尝试找到一匹“较慢的马”。相反，我们直接与动物本身进行操作，它们的各种属性隐式地随之而来。将狐狸的快速特性移除后，结合像马这样的大动物，我们得到了一只又大又慢的动物。
- en: What does this have to do with processing language?
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这和语言处理有什么关系呢？
- en: Embedding Words
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 嵌入词
- en: To apply what we’ve just seen to words, we’ll replace the animals with words.
    And instead of using only two axes, we’ll place our words in a space of hundreds
    of dimensions.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将我们刚才看到的应用到词汇上，我们将动物替换为词语。并且，不仅仅使用两个坐标轴，我们将在一个拥有数百维的空间中放置这些词语。
- en: We do this with an algorithm that works out what each axis in this space should
    mean as it places every word at the appropriate point. Instead of assigning each
    word a single number, the algorithm assigns the word a whole list of numbers,
    representing its coordinates in a huge space.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过一个算法来实现这一点，算法会自动确定这个空间中每个坐标轴应该代表什么，并将每个词放置在适当的位置。算法并不是为每个词分配一个单一的数字，而是为词分配一个数字列表，表示它在这个庞大空间中的坐标。
- en: This algorithm is called an *embedder*, and we say that this process is one
    of *embedding* the words in the *embedding space*, thereby creating *word embeddings*.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法被称为*嵌入器*，我们说这个过程是*将词嵌入*到*嵌入空间*中，从而创造出*词嵌入*。
- en: The embedder works out for itself how to construct the space and find the coordinates
    of each word so that it’s near similar words. For example, if it sees a lot of
    sentences that begin with I just drank some, then whatever noun comes next is
    interpreted as some kind of drink, and it is placed near other kinds of drinks.
    If it sees I just ate a red, then whatever comes next is interpreted as something
    that’s red and edible, and it is placed near other things that are red and near
    other things that are edible. The same thing is true of dozens or even hundreds
    of other relationships, both obvious and subtle. Because the space has so many
    dimensions and the axes can have arbitrarily complex meanings, words can belong
    simultaneously to many clusters based on seemingly unrelated characteristics.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入器会自动确定如何构造空间，并找出每个词的坐标，使得相似的词靠得更近。例如，如果它看到很多以“I just drank some”开头的句子，那么接下来出现的任何名词都会被理解为某种饮品，并被放置在其他饮品附近。如果它看到“I
    just ate a red”这样的句子，那么接下来出现的任何词都会被理解为红色且可食用的东西，并被放置在其他红色和其他可食用的物体附近。对于其他数十种甚至数百种关系，不管是显而易见的还是微妙的，情况也一样。由于空间有许多维度，并且坐标轴可以有任意复杂的含义，词汇可以同时属于许多不同的群体，基于看似不相关的特征。
- en: 'This idea is both abstract and powerful, so let’s illustrate it with some actual
    examples. We tried a few “word arithmetic” expressions using a pretrained embedding
    of 684,754 words saved in a space of 300 dimensions (spaCy authors 2020). Our
    first test was a famous one: king – man + woman (El Boukkouri 2018). The system
    returned queen as the most likely result, which makes sense: we can imagine that
    the embedder worked out some sense of nobility on one axis and gender on another.
    Other tests were close but not perfect. For example, lemon – yellow + green came
    back with ginger as the best match, but the expected lime wasn’t far back as the
    fifth-closest word. Similarly, trumpet – valves + slide returned saxophone as
    the most likely result, but the expected trombone was the first runner-up.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概念既抽象又强大，所以让我们通过一些实际例子来说明它。我们尝试了一些“词汇算术”表达式，使用的是一个预训练的词嵌入，其中包含684,754个词，保存在一个300维的空间中（spaCy
    authors 2020）。我们的第一个测试是一个著名的例子：king – man + woman（El Boukkouri 2018）。系统返回了queen作为最可能的结果，这很有道理：我们可以想象，嵌入器在一个坐标轴上捕捉到了贵族的概念，在另一个坐标轴上捕捉到了性别的差异。其他测试结果也很接近，但并不完美。例如，lemon
    – yellow + green返回了ginger作为最佳匹配，但预期的lime也排在第五近的词汇。类似地，trumpet – valves + slide返回了saxophone作为最可能的结果，但预期的trombone排在了第一位的候选。
- en: The beauty of training an embedder in a space with hundreds (or even thousands)
    of dimensions is that it can use the space much more efficiently than any person
    probably would, enabling it to simultaneously represent an enormous number of
    relationships.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个嵌入模型在一个拥有数百（甚至数千）维度的空间中的美妙之处在于，它能够比任何人更高效地利用这个空间，从而使其能够同时表示大量的关系。
- en: The word arithmetic we just saw is a fun demonstration of embedding spaces,
    but it also enables us to meaningfully perform operations on words like comparing
    them, scaling them, and adding them, all of which are important to the algorithms
    in this chapter.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚看到的词汇算式是嵌入空间的一个有趣示范，但它也使我们能够有意义地对词汇进行操作，比如比较、缩放和加法，这些操作对于本章的算法都非常重要。
- en: Once we have word embeddings, it’s easy to incorporate them into almost any
    network. Instead of assigning a single integer to each word, we assign the word
    embedding, which is a list of numbers. So instead of processing zero-dimensional
    tensors (single numbers), the system processes one-dimensional tensors (lists
    of numbers).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们拥有了词嵌入，将它们整合到几乎任何网络中都变得容易。我们不是为每个词分配一个单独的整数，而是分配词嵌入，它是一个数字列表。因此，系统处理的不是零维张量（单一数字），而是一维张量（数字列表）。
- en: This neatly addresses the problem we saw in Chapter 19 where predictions that
    were close to the target but not exactly right gave us nonsense. Now we can tolerate
    a bit of imprecision, because similar words are embedded near one another. For
    example, we might give our language model the phrase The dragon approached and
    let out a mighty, expecting the next word to be roar. The algorithm might predict
    a tensor that’s near roar but not exactly on it, giving us bellow or blast instead.
    We probably wouldn’t get back something unrelated, like daffodil.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这巧妙地解决了我们在第 19 章看到的问题，即那些与目标接近但不完全正确的预测往往给我们带来无意义的结果。现在我们能够容忍一些不精确的情况，因为相似的词汇被嵌入在彼此接近的位置。例如，我们可能会给语言模型输入短语“巨龙逼近并发出一声响亮的”，期望下一个词是
    roar。算法可能会预测出一个接近 roar 的张量，但不完全相同，而给出 bellow 或 blast。我们大概率不会得到一个完全无关的词汇，如 daffodil。
- en: '[Figure 20-5](#figure20-5) shows six sets of four related words that we gave
    to a standard word embedder. The more the embeddings of any two words are like
    one another, the higher that pair of words scored, so the darker their intersection
    appears. The graph is symmetric around the diagonal from the upper left to the
    lower right, since the order in which we compare the words doesn’t matter.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 20-5](#figure20-5)展示了我们提供给标准词嵌入模型的六组四个相关词汇。任何两个词嵌入的相似度越高，这对词汇的得分就越高，因此它们的交集看起来越暗。图形在从左上到右下的对角线两侧对称，因为我们比较词汇的顺序并不重要。'
- en: '![F20005](Images/F20005.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![F20005](Images/F20005.png)'
- en: 'Figure 20-5: Comparing pairs of words by comparing the similarity of their
    embeddings'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20-5：通过比较词嵌入的相似性来比较词对。
- en: We can see from [Figure 20-5](#figure20-5) that each word matches itself most
    strongly and also matches related words more strongly than unrelated words. Because
    we placed related words side by side, the graph shows their similarities as small
    blocks. There are a few curiosities, however. For example, why does fish match
    better than average with chocolate and coffee, and why does blue score well with
    caramel? These might be artifacts from the particular training data used for this
    embedder.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从[图 20-5](#figure20-5)看到，每个词与自身的匹配度最高，同时也比与不相关的词汇匹配得更强。由于我们将相关的词汇并排放置，图形显示它们的相似度为小块。然而，也有一些奇特之处。例如，为什么
    fish 与 chocolate 和 coffee 的匹配度比平均值更高？又为什么 blue 与 caramel 的得分较高？这些可能是由于这个嵌入模型使用的特定训练数据所导致的伪影。
- en: The coffee drinks and the flavors score well with one another, perhaps because
    people order coffee drinks with those flavored syrups. There’s also a hint of
    a relationship between the colors and the flavors.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 咖啡饮品和口味之间的得分相对较高，也许是因为人们会用这些口味的糖浆来调味咖啡饮品。颜色和口味之间也隐约存在一些关系。
- en: Many pretrained word embedders are widely available for free, and easily downloaded
    into almost any library. We can simply import them and immediately get the vector
    for any word. The GLoVe (Mikolov et al. 2013a; Mikolov et al. 2013b) and word2vec
    (Pennington, Socher, and Manning 2014) embeddings have been used in many projects.
    The more recent fastText (Facebook Open Source 2020) project offers embeddings
    in 157 languages.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 许多预训练的词嵌入器可以免费获得，并且可以轻松下载到几乎任何库中。我们可以简单地导入它们，立即获得任何单词的向量。GLoVe（Mikolov等，2013a；Mikolov等，2013b）和word2vec（Pennington、Socher和Manning，2014）嵌入在许多项目中得到应用。较新的fastText（Facebook开源，2020）项目提供了157种语言的嵌入向量。
- en: We can also embed entire sentences, so that we can compare them as a whole,
    rather than word by word (Cer et al. 2018). [Figure 20-6](#figure20-6) shows comparisons
    between embeddings for a dozen sentences (TensorFlow 2018). In this book, we focus
    on word embeddings rather than sentences.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以对整个句子进行嵌入，这样我们就可以整体比较句子，而不是逐词比较（Cer等，2018）。[图20-6](#figure20-6)展示了对12个句子（TensorFlow，2018）的嵌入比较。在本书中，我们将重点讨论词嵌入，而不是句子嵌入。
- en: '![F20006](Images/F20006.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![F20006](Images/F20006.png)'
- en: 'Figure 20-6: Comparing sentence embeddings. The larger the score, the more
    the sentences are considered like one another.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图20-6：比较句子嵌入。分数越大，表示句子之间的相似度越高。
- en: ELMo
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ELMo
- en: 'Word embeddings are a huge advance over assigning single integers to words.
    But even though word embeddings are powerful, the approach we described earlier
    to create them has a problem: nuance.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入比为单词分配单个整数要更为先进。然而，即便词嵌入功能强大，我们之前描述的创建词嵌入的方法仍然存在一个问题：细微差别。
- en: As we saw in Chapter 19, many languages have words with different meanings but
    are written and pronounced the same way. If we want to make sense of words, we
    need to distinguish these meanings. One way to do that is to give every meaning
    of a word its own embedding. So cupcake, which has one meaning, has one embedding.
    But train has two embeddings, one for when it’s a noun (as in, “I rode on a train”),
    and one for when it’s a verb (as in, “I like to train dogs”). These two meanings
    of train really are entirely different ideas that just happen to use the same
    sequence of letters.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第19章看到的，许多语言中有些单词的意义不同，但书写和发音是相同的。如果我们想理解这些单词，就需要区分它们的不同含义。做到这一点的一种方法是为每个单词的每个含义分配一个独特的嵌入向量。例如，cupcake
    只有一个意思，因此只有一个嵌入向量。但 train 则有两个嵌入向量，一个表示名词（例如，“我坐了一次火车”），另一个表示动词（例如，“我喜欢训练狗”）。这两个意思实际上是完全不同的概念，只是恰好使用了相同的字母序列。
- en: Such words present two challenges. First, we have to create unique embeddings
    for each meaning. Second, we have to select the correct embedding when such words
    are used as input. Solving these challenges requires that we take into account
    the context of every word. The first algorithm to do this in a big way was called
    *Embedding from Language Models*, but it’s better known by its friendly acronym
    *ELMo* (Peters et al. 2018), which is the name of a Muppet on the children’s television
    show *Sesame Street*. We say that ELMo produces *contextualized word embeddings*.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这类词汇面临两个挑战。首先，我们必须为每个含义创建独特的嵌入向量。其次，当这些词作为输入时，我们必须选择正确的嵌入向量。解决这些问题需要我们考虑每个单词的上下文。第一个能够大规模处理这一问题的算法叫做*Embedding
    from Language Models*，但它更广为人知的是其友好的缩写*ELMo*（Peters等，2018），它的名字来自儿童电视节目《芝麻街》中的一个木偶角色Elmo。我们说ELMo生成了*上下文化的词嵌入*。
- en: ELMo’s architecture is similar to that of a pair of bi-RNNs, which we saw in
    [Figure 19-20](c19.xhtml#figure19-20), but the pieces are organized differently.
    In a standard bi-RNN, we couple two RNNs running in opposite directions.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ELMo的架构类似于我们在[图19-20](c19.xhtml#figure19-20)中看到的一对双向RNN，但它的结构组织方式不同。在标准的双向RNN中，我们将两个朝相反方向运行的RNN相结合。
- en: ELMo changes this around. Although it uses two RNN networks that run forward
    and two that run backward, they are grouped by direction. Each of these groups
    is a two-layer-deep RNN, like the one we saw in [Figure 19-21](c19.xhtml#figure19-21).
    ELMo’s architecture is shown in [Figure 20-7](#figure20-7). It’s traditional to
    draw ELMo diagrams with a red color scheme, since Elmo on *Sesame Street* is a
    bright red character.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ELMo对这一点进行了改变。虽然它使用两个正向RNN网络和两个反向RNN网络，它们按方向分组。每个组都是一个两层深的RNN，就像我们在[图19-21](c19.xhtml#figure19-21)中看到的那样。ELMo的架构如[图20-7](#figure20-7)所示。通常，我们使用红色配色方案绘制ELMo的示意图，因为《芝麻街》中的Elmo是一个明亮的红色角色。
- en: '![F20007](Images/F20007.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![F20007](Images/F20007.png)'
- en: 'Figure 20-7: The structure of ELMo in unrolled form. The input text is at the
    bottom. The embedding of each input element is at the top.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图20-7：ELMo展开后的结构。输入文本位于底部。每个输入元素的嵌入位于顶部。
- en: This architecture means each input word is turned into two new tensors, one
    from the forward networks (labeled F1 and F2), that take into account the preceding
    words, and one from the backward networks (labeled B1 and B2) that consider the
    following words. By concatenating these results together, we get contextualized
    word embeddings informed by all the other words in the sentence.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构意味着每个输入单词都会转化为两个新的张量，一个来自前向网络（标记为F1和F2），考虑到前面的单词，另一个来自反向网络（标记为B1和B2），考虑到后面的单词。通过将这些结果连接在一起，我们得到了一个上下文化的单词嵌入，这些嵌入受到句子中所有其他单词的影响。
- en: Trained versions of ELMo are widely available for free downloads in a variety
    of sizes (Gluon 2020). Once we have a pretrained ELMo, it’s easy to use in any
    language model. We give our entire sentence to ELMo, and we get back a contextualized
    word embedding for each word, given its context.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 训练版的ELMo可以在多个大小的版本中免费提供下载（Gluon 2020）。一旦我们拥有了预训练的ELMo，它可以轻松地用于任何语言模型。我们将整个句子输入ELMo，得到一个上下文化的单词嵌入，基于其上下文。
- en: '[Figure 20-8](#figure20-8) shows four sentences that use the homonym train
    as a verb, and four that use train as a noun. We gave these to a standard ELMo
    model trained on a database of 1 billion words that places each word into a space
    of 1,024 dimensions (TensorFlow 2020a). We extracted ELMo’s embedding of the word
    train in each sentence, and compared its embedding to that of the word train in
    all the other sentences. Although the word is written in the identical way in
    each sentence, ELMo is able to identify the correct embedding based on the word’s
    context.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[图20-8](#figure20-8)展示了四个句子使用“train”作为动词，四个句子使用“train”作为名词的情况。我们将这些句子提供给了一个在10亿单词的数据库上训练的标准ELMo模型，该模型将每个单词放入一个1024维的空间中（TensorFlow
    2020a）。我们提取了ELMo在每个句子中“train”一词的嵌入，并将其与所有其他句子中“train”一词的嵌入进行比较。尽管该词在每个句子中的书写方式相同，ELMo能够根据该词的上下文识别出正确的嵌入。'
- en: '![F20008](Images/F20008.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![F20008](Images/F20008.png)'
- en: 'Figure 20-8: Comparing ELMo’s embeddings of *train* resulting from its use
    in different sentences. Darker colors mean more similar embeddings.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图20-8：比较ELMo在不同句子中使用*train*时的嵌入。较深的颜色表示嵌入更相似。
- en: We usually place embedding algorithms like ELMo on their own layer in a deep
    learning system. This is often the very first layer in a language processing network.
    Our icon for an embedding algorithm, shown in [Figure 20-9](#figure20-9), is meant
    to suggest taking the space of words and placing it inside the larger embedding
    space.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常将像ELMo这样的嵌入算法放置在深度学习系统中的单独层级。这通常是语言处理网络中的第一层。我们为嵌入算法设计的图标，如[图20-9](#figure20-9)所示，旨在表示将单词的空间放入更大的嵌入空间中。
- en: '![F20009](Images/F20009.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![F20009](Images/F20009.png)'
- en: 'Figure 20-9: Our icon for an embedding layer'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图20-9：我们为嵌入层设计的图标
- en: ELMo and other algorithms like it, such as the *Universal Language Model Fine-Tuning*,
    or *ULMFiT* (Howard and Ruder 2018), are typically trained on general-purpose
    databases, such as books and documents from the web. When we need them for some
    specific downstream task, such as medical or legal applications, we usually fine-tune
    them with additional examples from those domains. The result is a set of embeddings
    that include the specialized language of those fields, clustered by their special
    meanings in that jargon.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ELMo和其他类似的算法，如*通用语言模型微调*（Universal Language Model Fine-Tuning，简称*ULMFiT*）（Howard
    和 Ruder 2018），通常在通用数据库上进行训练，例如来自网页的书籍和文档。当我们需要它们来处理某些特定的下游任务时，如医学或法律应用，我们通常会用来自这些领域的额外示例对其进行微调。结果是，一组嵌入包含了这些领域的专业语言，按其在该术语中的特殊含义进行聚类。
- en: We’ll use embeddings in the systems we will build later in this chapter. Those
    networks will rely on the mechanism of attention, so let’s look at that now.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章稍后的系统构建中使用嵌入。这些网络将依赖于注意力机制，因此让我们现在来看一下这个机制。
- en: Attention
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 注意力
- en: In Chapter 19 we saw how to improve translation by taking into account all of
    the words in a sentence. But when we’re translating a particular word, not every
    word in the sentence is equally important, or even relevant.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在第19章中，我们看到如何通过考虑句子中的所有单词来改善翻译。但当我们翻译一个特定的单词时，句子中的每个单词并不是同等重要的，甚至有些单词可能与之无关。
- en: For example, suppose we’re translating the sentence I saw a big dog eat his
    dinner. When we’re translating dog, we probably don’t care about the word saw,
    but to translate the pronoun his correctly may require us to connect that to the
    two words big dog.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们在翻译句子“I saw a big dog eat his dinner”（我看到一只大狗吃晚餐）时。在翻译“dog”时，我们可能不关心单词“saw”，但要正确翻译代词“his”可能需要我们将其与“大狗”这两个词联系起来。
- en: If we can work out, for each word in the input, which other words can influence
    our translation, then we can focus just on those words and ignore the others.
    This would be a big savings in both memory and computation time. And if we can
    work this out in a way that doesn’t depend on processing the words serially, we
    can even do it in parallel.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能够为输入中的每个单词找出哪些其他单词能够影响我们的翻译，那么我们就可以只关注那些单词，忽略其他单词。这将大大节省内存和计算时间。而且，如果我们能够以一种不依赖于串行处理单词的方式来实现这一点，我们甚至可以并行处理。
- en: The algorithm that does this job is called *attention*, or *self-attention*
    (Bahdanau, Cho, and Bengio 2016; Sutskever, Vinyals, and Le 2014; Cho et al. 2014).
    Attention lets us focus our resources on only the parts of the input that matter.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 执行这一任务的算法叫做*注意力*或*自注意力*（Bahdanau, Cho, 和 Bengio 2016；Sutskever, Vinyals, 和 Le
    2014；Cho 等人 2014）。注意力机制使我们能够将资源集中在输入中重要的部分。
- en: Modern versions of attention are often based on a technique called *query, key,
    value*, or simply *QKV*. These terms come from the field of databases and can
    seem somewhat obscure in this context. So we’ll describe the concepts using a
    different set of terms and then connect them back to query, key, and value at
    the end.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现代的注意力机制通常基于一种名为*查询、键、值*（*QKV*）的技术。这些术语来自数据库领域，在这个背景下可能显得有些晦涩。因此，我们将使用一组不同的术语来描述这些概念，并最终将其与查询、键和值连接起来。
- en: A Motivating Analogy
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个激发思考的类比
- en: Let’s begin with an analogy. Suppose that you need to buy some paint, but all
    you’ve been told is that the color should be “light yellow with a bit of dark
    orange.”
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个类比开始。假设你需要购买一些油漆，但你得到的唯一信息是颜色应该是“浅黄色带有一点深橙色”。
- en: At the only paint store in town, the only clerk on duty is new to the paint
    department and isn’t personally familiar with the colors. You both presume you’ll
    need to mix together a few of their standard paints to get the color you want,
    but you don’t know which paints to choose or how much of each to use.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在镇上唯一的油漆店里，唯一在职的店员刚刚加入油漆部门，对颜色不太熟悉。你们都假设你需要将几种标准油漆混合起来以得到你想要的颜色，但你不知道该选择哪些油漆，也不知道每种油漆的用量。
- en: The clerk suggests that you compare your desired color description with the
    color names on each can of paint they carry. Some names will probably match better
    than others. The clerk puts a funnel on top of an empty can and suggests that
    you pour in some of each can of paint on the shelves, guided by how well that
    can’s name matches your description. That is, you’ll compare your desired description
    “light yellow with a bit of dark orange” with what’s printed on the label of each
    can, and the better the match, the more of that paint you’ll pour into the funnel.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 店员建议你将你想要的颜色描述与每个油漆罐上的颜色名称进行比较。有些名称可能比其他名称匹配得更好。店员在一个空油漆罐上放了一个漏斗，并建议你根据每个油漆罐名称与描述的匹配度，将不同的油漆倒入漏斗中。也就是说，你将把你想要的描述“浅黄色带有一点深橙色”与每个油漆罐标签上的内容进行比较，匹配度越好，你就倒入更多这种油漆。
- en: '[Figure 20-10](#figure20-10) shows the idea visually for six cans of paint.
    It shows their names and the quality of each name’s match with your desired color’s
    description. We got good matches on “Sunny Yellow” and “Orange Crush,” though
    a little bit of “Lunch with Teal” snuck in thanks to the match with the word “with.”'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 20-10](#figure20-10)通过六个油漆罐形象地展示了这个概念。它展示了油漆罐的名称以及每个名称与所需颜色描述的匹配度。我们得到了“阳光黄色”和“橙色粉碎”的不错匹配，尽管由于与“with”一词的匹配，稍微混入了一些“午餐配青绿色”。'
- en: '![F20010](Images/F20010.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![F20010](Images/F20010.png)'
- en: 'Figure 20-10: Given a color description (left), we combine some of each can
    based on how well its name matches the description (middle), to get a final result
    (right).'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20-10：根据颜色描述（左），我们根据每个油漆罐名称与描述的匹配程度（中）来混合不同的油漆，最终得到结果（右）。
- en: 'There are three things to focus on in this story. First, there’s your *request*:
    “light yellow with a bit of dark orange.” Second, there’s the *description* on
    each can of paint, like “Sunny Yellow” or “Mellow Blue.” Third, there’s the *content*
    of the paint that’s actually inside each can. In the story, you compared your
    request with each can’s description to find out how well they match. The better
    the match, the more of that can’s content you used in the final mixture.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个故事中，有三件事需要关注。首先是你的*请求*：“浅黄色带有一点深橙色”。第二是每罐油漆上的*描述*，例如“阳光黄”或“温柔蓝”。第三是每罐油漆的*内容*，即罐内实际的油漆。在故事中，你将自己的请求与每罐油漆的描述进行比较，以了解它们的匹配度。匹配度越高，最终混合物中使用的该罐油漆的内容就越多。
- en: That’s attention in a nutshell. Given a request, compare it to the description
    of each possible item and include some of the content of each item based on how
    well its description matches the request.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是注意力机制的核心。给定一个请求，将其与每个可能项目的描述进行比较，并根据描述与请求的匹配程度，选择每个项目的一部分内容。
- en: The authors of the first paper on attention compared this process to a common
    type of transaction used with a database. In database language, we look something
    up by sending a *query* to a database. In such a process, every object in the
    database has a descriptive *key*, which can be different than the actual *value*
    of the object. Note that here the word *value* refers to the contents of the object,
    whether it’s a single number or something more complicated, such as a string or
    tensor.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 第一篇关于注意力机制的论文的作者将这一过程与数据库中常见的一种事务类型进行了比较。在数据库的术语中，我们通过向数据库发送*查询*来查找某个内容。在这样的过程中，数据库中的每个对象都有一个描述性的*键*，它可以与对象的实际*值*不同。请注意，这里所说的*值*指的是对象的内容，无论它是一个单一的数字，还是更复杂的东西，例如字符串或张量。
- en: The database system compares the query (or request) with each key (or description)
    and uses that score to determine how much of the object’s value (or content) to
    include in the final result. So our terms of request, description, and content
    correspond to query, key, and value, or, more commonly, QKV.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库系统将查询（或请求）与每个键（或描述）进行比较，并使用该得分来决定最终结果中包含多少对象的值（或内容）。因此，我们的请求、描述和内容对应于查询、键和值，或更常见的术语，QKV。
- en: Self-Attention
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自注意力
- en: '[Figure 20-11](#figure20-11) shows the fundamental operation of attention in
    abstracted form. Here we have five words of input. Each of the three colored boxes
    represents a small neural network that takes the numerical representation of a
    word and transforms it into something new (often, these networks are each just
    a single fully connected layer). In this example, the word dog is the one we want
    to translate. So a neural network (in red) transforms the tensor for dog and turns
    it into a new tensor representing the query, Q. As the figure shows, two more
    small neural networks translate the tensor for dinner into new tensors, corresponding
    to its key, K (from the blue network) and its value, V (from the green network).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[图20-11](#figure20-11)以抽象形式展示了注意力机制的基本操作。这里有五个输入词。每个彩色框表示一个小型神经网络，该网络接收一个词的数值表示并将其转化为新的东西（通常这些网络每个只是一个单一的全连接层）。在这个例子中，词“dog”是我们想要翻译的词。所以，一个神经网络（红色）将“dog”的张量转换为一个新的张量，代表查询Q。正如图中所示，另外两个小型神经网络将“dinner”的张量翻译成新的张量，分别对应其键K（来自蓝色网络）和其值V（来自绿色网络）。'
- en: '![f20011](Images/f20011.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![f20011](Images/f20011.png)'
- en: 'Figure 20-11: The core step of attention using *dog* for the query to determine
    the relevance of the word *dinner*. Each box represents a small neural network
    that transforms its input into a query, key, or value.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图20-11：使用*dog*作为查询来确定单词*dinner*相关性的注意力核心步骤。每个框代表一个小型神经网络，它将输入转化为查询、键或值。
- en: In practice, we compare the query for dog against the key of every word in the
    sentence, including dog itself. For this illustration, we limit our focus to the
    comparison with the word dinner.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 实际操作中，我们将“dog”的查询与句子中每个单词的键进行比较，包括“dog”本身。在这个例子中，我们只关注与“dinner”一词的比较。
- en: We compare the query and the key to determine how alike they are. We do this
    with a little scoring function that we’re indicating with the letter *S* in a
    circle. Without getting into the math, this function compares two tensors and
    produces a single number. The more that the two tensors are like one another,
    the larger that number. The scoring function is usually designed to produce a
    number between 0 and 1, with larger values indicating a better match.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们比较查询和键，以确定它们的相似度。我们用一个小的评分函数来完成这一操作，函数表示为一个圆圈中的字母*S*。不深入讨论数学部分，这个函数比较两个张量并产生一个单一的数字。两个张量越相似，得出的数字就越大。评分函数通常设计为产生一个介于0和1之间的数字，较大的值表示更好的匹配。
- en: We use the output from the scoring function to scale the tensor representing
    the value for dinner. The more the query and the key match, the larger the output
    of the scaling step, and the more the value of dinner will make it into the output.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用评分函数的输出对表示晚餐的值的张量进行缩放。查询和键的匹配度越高，缩放步骤的输出越大，晚餐的值会更多地影响最终输出。
- en: Let’s see what it looks like when we apply this fundamental step to all the
    words in the input simultaneously. We’ll continue to look at translating the word
    dog. The overall result is the sum of the individual scaled values of all the
    input words. [Figure 20-12](#figure20-12) shows how this looks.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看当我们将这个基本步骤同时应用到输入中的所有单词时会是什么样子。我们将继续观察翻译单词dog的情况。总体结果是所有输入单词的单独缩放值的总和。[图20-12](#figure20-12)展示了这种情况。
- en: '![F20012](Images/F20012.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![F20012](Images/F20012.png)'
- en: 'Figure 20-12: Using attention to simultaneously determine the contribution
    of all five words in the sentence to the word *dog*. The QKV spatial and color
    coding matches [Figure 20-11](#figure20-11). All data flows upward in the figure.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图20-12：使用注意力机制同时确定句子中所有五个单词对单词*dog*的贡献。QKV的空间和颜色编码与[图20-11](#figure20-11)相匹配。图中的所有数据都朝上流动。
- en: There are a few things to note in [Figure 20-12](#figure20-12). First, only
    three neural networks are involved—one each to compute the query, key, and value
    tensors. We use the same “input to query” network (in red in the figure) to turn
    each input into its query, the same “input to key” network (in blue in the figure)
    to turn each input into its key, and the same “input to value” network (in green
    in the figure) to turn each input into its value. We only need to apply these
    transformations once to each word.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图20-12](#figure20-12)中有几点需要注意。首先，只有三个神经网络参与——分别用于计算查询、键和值张量。我们使用相同的“输入到查询”网络（图中的红色部分）将每个输入转化为其查询，使用相同的“输入到键”网络（图中的蓝色部分）将每个输入转化为其键，使用相同的“输入到值”网络（图中的绿色部分）将每个输入转化为其值。我们只需要对每个单词应用一次这些转换。
- en: Second, there’s a dashed line after the scores and before the scaling of the
    values. This represents a softmax step applied to the scores, followed by a division.
    These two operations keep the numbers coming out of the scores from getting too
    big or small. The softmax also exaggerates the influence of close matches.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，得分后和数值缩放前有一条虚线。这代表了对得分应用的softmax步骤，之后是一个除法操作。这两个操作可以防止得分的数值变得过大或过小。softmax还会夸大相似项的影响。
- en: Third, we sum up all the scaled values to get a new tensor for dog, including
    that from the value of dog itself. We often find that each word scores most highly
    with itself. This isn’t a bad thing, as in this case, the most important word
    for translating dog is indeed dog itself. But there are times when other words
    will matter more. Some examples include when word order changes, when a word has
    no direct translation and must rely on other words, or when we’re trying to resolve
    a pronoun.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，我们将所有的缩放值加起来，得到dog的新张量，其中包括dog本身的值。我们常发现每个单词与自己的得分最高。这并不是坏事，因为在这种情况下，翻译dog时最重要的单词确实是dog本身。但有时其他单词会更加重要。一些例子包括单词顺序改变、某个单词没有直接翻译且必须依赖其他单词，或者我们正在试图解析代词时。
- en: The fourth important point is that we apply the processing of [Figure 20-12](#figure20-12)
    to all the words in the input sentence simultaneously. That is, each word is considered
    the query, and the whole process executes independently for that word, as shown
    in [Figure 20-13](#figure20-13).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 第四个重要点是我们将[图20-12](#figure20-12)中的处理同时应用到输入句子中的所有单词。也就是说，每个单词都被视为查询，整个过程独立地为该单词执行，如[图20-13](#figure20-13)所示。
- en: '![F20013](Images/F20013.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![F20013](Images/F20013.png)'
- en: 'Figure 20-13: Applying attention to the other four words in our sentence'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图20-13：将注意力应用于句子中的其他四个单词
- en: 'Our fifth and last point is just an explicit recap of something we’ve been
    noting all along: all of this processing in [Figure 20-12](#figure20-12) and [Figure
    20-13](#figure20-13) together can be done in parallel in just four steps, regardless
    of the length of the sentence. Step 1 transforms the inputs into query, key, and
    value tensors. Step 2 scores all the queries and keys against one another. Step
    3 uses the scores to scale the values, and step 4 adds up the scaled values to
    produce a new output for each input.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第五个也是最后一个要点只是对我们一直在注意的一点进行明确的回顾：在[图 20-12](#figure20-12)和[图 20-13](#figure20-13)中的所有这些处理可以在四个步骤中并行完成，而不依赖于句子的长度。步骤1将输入转化为查询、键和值张量。步骤2对所有查询和键进行互相评分。步骤3使用这些评分来缩放值，步骤4将缩放后的值相加，生成每个输入的新输出。
- en: None of these steps depend on how long the input is, so we can process long
    sentences in the same amount of time required by short ones, as long as we have
    the memory and computing power needed.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤都不依赖于输入的长度，因此我们可以在与短句子相同的时间内处理长句子，只要我们拥有所需的内存和计算能力。
- en: 'We call the process of [Figure 20-12](#figure20-12) and [Figure 20-13](#figure20-13)
    *self-attention*, because the attention mechanism is using the same set of inputs
    for computing everything: the queries, keys, and values. That is, we’re finding
    how much the input should be paying attention to itself.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称[图 20-12](#figure20-12)和[图 20-13](#figure20-13)的过程为*自注意力*，因为注意力机制使用相同的输入集来计算所有内容：查询、键和值。也就是说，我们在找出输入应该如何关注自身。
- en: When we place self-attention in a deep network, we put it onto its own *self-attention
    layer*, often simply called an *attention layer*. The input is a list of words
    in numerical form, and the output is the same.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将自注意力置于深度网络中时，我们将其放入自己的*自注意力层*，通常简洁地称为*注意力层*。输入是一个数字形式的单词列表，输出也是相同的。
- en: The engines that power attention are the scoring function and the neural networks
    that transform the inputs into queries, keys, and values. Let’s consider them
    briefly.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 驱动注意力机制的引擎是评分函数和神经网络，后者将输入转化为查询、键和值。我们简要地考虑一下它们。
- en: The scoring function compares a query to a key, returning a value from 0 to
    1, where the more the two values are similar, the higher their score. So somehow,
    the inputs that we think of as being similar need to have similar values going
    into the scoring function. Now we can see the practical value of embeddings. Recall
    our discussion of *A Tale of Two Cities*, in Chapter 19 where we assigned each
    word a number given by its order in the text. That gave the words keep and flint
    numbers 1,003 and 1,004 respectively. If we just compared these numbers, they
    would get a high similarity score. For most sentences, this is not what we want.
    If we’re using the query value for the verb keep, we usually want it to be similar
    to the keys for synonyms like retain, hold,  and reserve, and not at all like
    the keys for unrelated words like flint, preposterous, or dinosaur. Embeddings
    are the means by which similar words (or words used in similar ways) are given
    similar representations.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 评分函数将查询与键进行比较，返回一个从0到1的值，两个值越相似，得分越高。因此，我们认为相似的输入需要将相似的值传入评分函数。现在我们可以看到嵌入的实际价值。回想我们在第19章讨论的*《双城记》*，我们根据每个单词在文本中的顺序给它们分配了一个数字。这使得单词“keep”和“flint”分别得到了1,003和1,004的编号。如果我们仅仅比较这些数字，它们会得到一个很高的相似度分数。对于大多数句子，这并不是我们想要的。如果我们使用动词“keep”的查询值，我们通常希望它与“retain”、“hold”和“reserve”等同义词的键相似，而与“flint”、“preposterous”或“dinosaur”等无关单词的键完全不同。嵌入是通过它们为相似的单词（或以相似方式使用的单词）提供相似表示的方式。
- en: Doing any necessary fine-tuning to the embeddings is the job of the neural networks,
    which transform the input words into representations where they can be meaningfully
    compared in the context of the sentence they’re used in. The only reason we have
    any chance of that is that the words are already embedded in a space where similar
    words are near one another.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 对嵌入进行必要的微调是神经网络的工作，神经网络将输入的单词转化为可以在它们所使用的句子上下文中有意义地比较的表示。我们之所以有可能做到这一点，是因为单词已经嵌入到一个空间中，其中相似的单词彼此靠近。
- en: Similarly, it’s the job of the network that turns inputs into values to represent
    those values in a way that allow them to be usefully scaled and combined. Mixing
    two embedded words gives us a word that’s somewhere between them.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，将输入转化为值的网络的任务是以一种可以有效缩放和组合这些值的方式表示这些值。将两个嵌入的单词混合会得到一个介于它们之间的单词。
- en: Q/KV Attention
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Q/KV 注意力
- en: In the self-attention network of [Figure 20-12](#figure20-12), the queries,
    keys, and values are all derived from the same inputs, which led to the name self-attention.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 20-12](#figure20-12)的自注意力网络中，查询、键和值都来自相同的输入，这也就是自注意力（self-attention）这个名字的由来。
- en: A popular variation uses one source for the queries and another for the keys
    and values. This more closely matches our paint store analogy, where we came in
    with the query, and the store had the keys and values. We call this variation
    a *Q/KV* network, where the slash indicates that the queries come from one source,
    and the keys and values from another. This version is sometimes used when we add
    attention to a network like seq2seq, where the queries come from the encoder,
    and the keys and values from the decoder, so it’s sometimes also called an *encoder-decoder
    attention* layer. The structure is shown in [Figure 20-14](#figure20-14).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 一个流行的变种使用一个源来获取查询，另一个源来获取键和值。这更接近我们的油漆店类比，我们带着查询进店，而商店有键和值。我们称这种变种为*Q/KV*网络，其中斜杠表示查询来自一个源，键和值来自另一个源。当我们将注意力加入像seq2seq这样的网络时，查询来自编码器，键和值来自解码器，因此有时也称为*编码器-解码器注意力*层。其结构如[图
    20-14](#figure20-14)所示。
- en: '![F20014](Images/F20014.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![F20014](Images/F20014.png)'
- en: 'Figure 20-14: A Q/KV layer is like self-attention as shown in [Figure 20-12](#figure20-12),
    except that the queries don’t come from the inputs.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20-14：Q/KV 层就像自注意力（见[图 20-12](#figure20-12)）一样，区别在于查询（queries）不来自输入。
- en: Multi-Head Attention
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多头注意力
- en: The idea of attention is to identify words that are alike and create a useful
    mix of them. But words can be considered alike based on many different metrics.
    We might consider nouns to be alike, or colors, or spatial ideas like up and down,
    or temporal ideas like yesterday and tomorrow. Which of these is the best choice?
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力的想法是识别相似的词并创建它们的有用组合。但是，词可以根据许多不同的标准被认为是相似的。我们可以考虑名词是相似的，或者颜色，或者空间概念，如上和下，或者时间概念，如昨天和明天。这些中哪一个是最好的选择？
- en: Of course, there is no one best answer. In fact, we often want to compare words
    using multiple criteria at once. For instance, when writing song lyrics, we may
    want to assign high scores to pairs of words that have similar meanings, similar
    sounds in their last syllable, the same number of syllables, and the same stress
    pattern in the syllables. When writing about sports, we might instead want to
    say that players on the same teams and with the same roles are like one another.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，没有一个最佳答案。实际上，我们通常希望同时使用多个标准来比较词语。例如，在写歌词时，我们可能希望对意义相似、最后一个音节发音相似、音节数相同并且重音模式相同的词对赋予高分。而在写关于体育的文章时，我们可能更倾向于说，同一队伍中的球员以及扮演相同角色的球员彼此相似。
- en: We can score words along multiple criteria by simply running multiple independent
    attention networks simultaneously. Each network is called a *head*. By initializing
    each head independently, we hope that during training, each head will learn to
    compare the inputs according to criteria that are simultaneously useful and different
    from those used by the other layers. If we want, we can add additional processing
    to explicitly encourage different heads to attend to different aspects of the
    inputs. The idea is called *multi-head attention*, and we can apply it to both
    self-attention networks like [Figure 20-12](#figure20-12) and Q/KV networks like
    [Figure 20-14](#figure20-14).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过同时运行多个独立的注意力网络，简单地沿多个标准为词汇打分。每个网络称为一个*头*。通过独立初始化每个头，我们希望在训练过程中，每个头将学习根据对比其他层所用的标准，同时有用且不同的标准来比较输入。如果我们愿意，我们可以增加额外的处理，明确鼓励不同的头关注输入的不同方面。这个想法被称为*多头注意力*，我们可以将其应用于像[图
    20-12](#figure20-12)这样的自注意力网络和像[图 20-14](#figure20-14)这样的Q/KV网络。
- en: Each head is a distinct attention network. The more heads we have, the more
    different aspects of the input they can focus on.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 每个头都是一个独立的注意力网络。我们拥有的头越多，它们就能集中注意力的输入方面就越多。
- en: A diagram for a multi-head attention layer is shown in [Figure 20-15](#figure20-15).
    As the figure shows, we usually combine the outputs of the heads into a list and
    run that through a single fully connected layer. This allows the entire multi-head
    network’s output to have the same shape as its input. This approach makes it easy
    to place multiple multi-head networks one after another.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20-15展示了一个多头注意力层的示意图。如图所示，我们通常将各个头的输出组合成一个列表，并将其通过一个单一的全连接层。这使得整个多头网络的输出与其输入具有相同的形状。这种方法使得将多个多头网络串联在一起变得容易。
- en: '![F20015](Images/F20015.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![F20015](Images/F20015.png)'
- en: 'Figure 20-15: A multi-head attention layer. A box with a diamond inside is
    our icon for an attention layer.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20-15：多头注意力层。一个带菱形的框是我们用来表示注意力层的图标。
- en: Attention is a general concept that we can apply in different forms to any kind
    of deep network. For example, in a CNN we can scale a filter’s outputs to emphasize
    the values produced in response to the most relevant locations in the input (Liu
    et al. 2018; H. Zhang et al. 2019).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力是一种通用概念，我们可以将其以不同形式应用于任何类型的深度网络。例如，在CNN中，我们可以缩放滤波器的输出，以强调响应输入中最相关位置的值（Liu等人，2018；H.
    Zhang等人，2019）。
- en: Layer Icons
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 层图标
- en: '[Figure 20-16](#figure20-16) shows our icons for the different types of attention
    layers. Multi-head attention is drawn as a little 3D box, suggesting a stack of
    attention networks. For Q/KV attention, we place a short line inside the diamond
    to identify the Q inputs and bring in the K and V inputs on an adjacent side.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 20-16](#figure20-16)展示了我们为不同类型的注意力层设计的图标。多头注意力被绘制成一个小的3D框，暗示着一堆注意力网络。对于Q/KV注意力，我们在菱形内部放置一条短线，以标识Q输入，并将K和V输入放置在相邻的侧面。'
- en: '![F20016](Images/F20016.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![F20016](Images/F20016.png)'
- en: 'Figure 20-16: Attention layer icons. (a) Self-attention. (b) Multi-head self-attention.
    (c) Q/KV attention. (d) Multi-head Q/KV attention.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20-16：注意力层图标。（a）自注意力。（b）多头自注意力。（c）Q/KV注意力。（d）多头Q/KV注意力。
- en: Transformers
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Transformer
- en: Now that we have embedding and attention, we’re ready to make good on our earlier
    promise to improve on RNNs.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了嵌入和注意力，准备好兑现我们之前承诺的改进RNN的目标。
- en: Our goal is to build a translator based not on RNNs, but on attention networks.
    The key idea is that the attention layers will learn how to transform our inputs
    into their translations, based on the relationships between words.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是构建一个基于注意力网络而非RNN的翻译器。关键思想是，注意力层将学习如何根据词与词之间的关系将我们的输入转换为其翻译。
- en: This approach first appeared in a paper with the great title, “Attention Is
    All You Need” (Vaswani et al. 2017). The authors called their attention-based
    model a *transformer* (an unfortunately ambiguous name, but it’s now firmly stuck
    in the language of the field). The transformer model works so well that we now
    have a new class of language models that not only can be trained in parallel,
    but also can outperform RNNs in a wide variety of tasks.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法首次出现在一篇标题为《Attention Is All You Need》（Vaswani等人，2017）的论文中。作者将他们基于注意力的模型称为*transformer*（这个名字不幸地有些模糊，但现在已经牢牢地成为该领域的术语）。transformer模型效果如此优秀，以至于我们现在拥有了一类新的语言模型，它们不仅可以并行训练，而且在广泛的任务中可以超越RNN。
- en: Transformers use three more ideas we haven’t discussed yet. Let’s cover them
    now, so when we get to the actual transformer architecture, it will be smooth
    sailing.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer使用了我们尚未讨论的三个新概念。让我们现在来介绍它们，这样当我们真正进入transformer架构时，就能顺利进行。
- en: Skip Connections
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 跳跃连接
- en: The first new idea we cover is called a *residual connection* or *skip connection*
    (He et al. 2015). The inspiration is to reduce the amount of work that’s required
    of a deep network layer.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要介绍的第一个新概念被称为*残差连接*或*跳跃连接*（He等人，2015）。其灵感来自于减少深度网络层所需的工作量。
- en: Let’s start with an analogy. Suppose you’re painting a real, physical portrait
    using acrylic paints on canvas. After weeks of sittings, the portrait is done,
    and you send it to your subject for their approval. They say that they like it,
    but they regret having worn a particular ring on one finger, and wish they’d worn
    a different one that they like more. Can you change that?
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个类比开始。假设你正在用丙烯画颜料在画布上画一幅真实的物理肖像画。在经过数周的坐姿之后，肖像完成了，你将它送给你的模特以供审批。他们说他们喜欢这幅画，但他们后悔自己在某个手指上戴了一个特定的戒指，应该戴另一个他们更喜欢的戒指。你能改变它吗？
- en: One way to proceed would be to invite your subject back to the studio and paint
    a whole new portrait from scratch on a blank canvas, only this time with the new
    ring on their finger. That would require a lot of time and effort. If they’d allow
    it, a more expeditious approach would be to take the portrait you have, and unobtrusively
    paint the new ring over the old one.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 一种做法是邀请你的对象回到工作室，从头开始在空白画布上重新绘制一幅全新的肖像，这次是戴上了新戒指。那会需要大量时间和精力。如果他们允许的话，更快速的方法是拿出现有的肖像，悄无声息地在旧戒指上绘制新戒指。
- en: Now consider a layer in a deep network. A tensor comes in, and the layer does
    some processing to change that tensor. If the layer only needs to change the input
    by small amounts, or only in some places, then it would be wasteful to expend
    resources processing the parts of the tensor that don’t need to change. Just as
    with the painting, it would be much more efficient for the layer to compute only
    the changes it wants to make. Then it can combine those changes with the original
    input to produce its output.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在考虑深度网络中的一层。一个张量进入，层对该张量进行一些处理以改变它。如果该层只需要对输入做出小的改变，或者仅仅是在某些地方改变，那么对那些不需要变化的张量部分进行处理将是浪费资源的。就像画画一样，如果该层仅计算它想做的改变，那会更加高效。然后它可以将这些改变与原始输入结合，生成输出。
- en: This idea works beautifully in deep learning networks. It lets us make layers
    that are smaller and faster, and it even improves the flow of gradients in backpropagation,
    which lets us efficiently train networks of dozens or even hundreds of layers.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这个理念在深度学习网络中表现得非常出色。它让我们可以构建更小、更快的层，甚至改善反向传播中的梯度流，从而让我们能够高效地训练由几十层甚至上百层组成的网络。
- en: The mechanism is shown on the left side of [Figure 20-17](#figure20-17). We
    feed an input tensor to some layer as usual, let it compute the changes, and then
    we add the layer’s output to its input tensor.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 该机制在[图 20-17](#figure20-17)的左侧展示。我们像往常一样将输入张量传递到某一层，让它计算变化，然后将该层的输出添加到其输入张量中。
- en: '![F20017](Images/F20017.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![F20017](Images/F20017.png)'
- en: 'Figure 20-17: Left: A skip connection, shown in red. Right: We can place a
    skip connection around multiple layers.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20-17：左侧：跳跃连接，红色标出。右侧：我们可以将跳跃连接放置在多个层之间。
- en: The extra line in the drawing that carries the input to the addition node is
    called a *skip connection*, or a *residual connection* because of its mathematical
    interpretation.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图中额外的线条，传递输入到加法节点，称为*跳跃连接*，或*残差连接*，因为它在数学上的解释。
- en: We can place a skip connection around multiple layers in sequence, if we like,
    as on the right of [Figure 20-17](#figure20-17).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要，我们可以像[图 20-17](#figure20-17)右侧那样，将跳跃连接放置在多个层之间。
- en: The skip connection works because each layer is trying to reduce its own contribution
    to the final error, while participating in the network made up of all the other
    layers. The skip connection is part of the network, so the layer learns it doesn’t
    need to process the parts of the tensor that don’t change. This makes the layer’s
    job simpler, enabling it to be smaller and faster.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 跳跃连接之所以有效，是因为每一层都在尝试减少自己对最终误差的贡献，同时参与由其他所有层组成的网络。跳跃连接是网络的一部分，因此该层学会了不需要处理那些不需要变化的张量部分。这使得该层的工作变得更简单，从而能够变得更小、更快。
- en: We’ll see later that transformers use skip connections not just for efficiency
    and speed, but also because they allow the transformer to cleverly keep track
    of the location of each element in its input.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 稍后我们将看到，变压器不仅仅因为效率和速度使用跳跃连接，更因为它们允许变压器巧妙地跟踪输入中每个元素的位置。
- en: Norm-Add
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Norm-Add
- en: The second idea on our road to transformers is really more of a conceptual and
    notational shorthand. In transformers, we usually apply a regularization step
    called *layer normalization*, or *layer norm*, to the outputs of a layer, as shown
    on the left in [Figure 20-18](#figure20-18) (Vaswani et al. 2017). Layer norm
    belongs to the class of regularization techniques that we saw in Chapter 15, such
    as dropout and batchnorm, which help control overfitting by keeping the values
    flowing through the network from getting too big or too small. The layer norm
    step learns to adjust the values coming out of a layer so that they approximate
    the shape of a Gaussian bump with a mean of 0 and standard deviation of 1.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在通往 Transformer 的道路上，第二个要讲解的概念更像是一种概念性和符号性缩写。在 Transformer 中，我们通常对层的输出应用一个叫做*层归一化*的正则化步骤，或者称为*层规范化*，如[图
    20-18](#figure20-18)左侧所示（Vaswani 等人，2017）。层归一化属于我们在第 15 章看到的正则化技术的范畴，例如 dropout
    和 batchnorm，它们通过控制网络中流动的值不至于过大或过小，从而帮助防止过拟合。层归一化步骤学习调整来自某一层的值，使它们逼近均值为 0，标准差为
    1 的高斯分布。
- en: '![F20018](Images/F20018.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![F20018](Images/F20018.png)'
- en: 'Figure 20-18: Left: A layer normalization followed by the addition step of
    a skip connection. Right: A combined icon for norm-add. This is just a visual
    and conceptual shorthand for the network on the left.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20-18：左侧：层归一化后跟随跳跃连接的加法步骤。右侧：归一化-加法的组合图标。这只是对左侧网络的视觉和概念性缩写。
- en: Performing layer norms is important in getting a transformer to work well, but
    there’s some flexibility about exactly where this step can be located. A popular
    approach places the layer norm just before the addition step of a skip connection,
    as in the left side of [Figure 20-18](#figure20-18). Since these two operations
    always come in pairs, it’s convenient to combine them into a single operation
    that we call *norm-add*. Our icon for norm-add is a combination of the layer norm
    and summation icons, and is shown on the right in [Figure 20-18](#figure20-18).
    This is just a visual shorthand for the two separate steps of layer norm followed
    by skip connection addition.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 执行层归一化在使 Transformer 工作良好时非常重要，但关于这个步骤可以放置的位置有一定的灵活性。一种常见的方法是在跳跃连接的加法步骤之前放置层归一化，如[图
    20-18](#figure20-18)左侧所示。由于这两个操作总是成对出现，因此将它们组合成一个我们称之为*归一化-加法*的单一操作是非常方便的。我们为归一化-加法设计的图标是层归一化和求和图标的组合，如[图
    20-18](#figure20-18)右侧所示。这只是层归一化后跟随跳跃连接加法的两个独立步骤的视觉缩写。
- en: People have experimented with other locations for the layer norm operation,
    such as before the layer (Vaswani et al. 2017), or after the addition node (TensorFlow
    2020b). These approaches differ in their details, but in practice, it seems that
    all of these choices are comparable. We’ll stick with the version in [Figure 20-18](#figure20-18)
    here.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 有人尝试过将层归一化操作放置在其他位置，例如放在层之前（Vaswani 等人，2017）或加法节点之后（TensorFlow 2020b）。这些方法在细节上有所不同，但在实践中，似乎所有这些选择都可以互相比较。我们将在这里继续使用[图
    20-18](#figure20-18)中的版本。
- en: Positional Encoding
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 位置编码
- en: 'The third idea to cover before we get to transformers was designed to solve
    a problem that comes up as soon as we take RNNs out of our system: we lose track
    of where each word is located in the input sentence. This important information
    is inherent in the RNN structure, because the words come in one at a time, allowing
    the hidden state inside a recurrent cell to remember the order in which the words
    arrived.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始讨论 Transformers 之前，需要讲解的第三个概念旨在解决一个问题，即当我们将 RNN 从系统中移除时：我们失去了每个单词在输入句子中的位置。这个重要信息是
    RNN 结构固有的，因为单词是一个一个地输入，这使得循环单元中的隐藏状态能够记住单词到达的顺序。
- en: But as we’ve seen, attention mixes together the representations of multiple
    words. How can later stages know where each word belongs in the sentence?
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，正如我们所看到的，注意力机制将多个单词的表示混合在一起。那么后续阶段如何知道每个单词在句子中的位置呢？
- en: The answer is to insert each word’s position, or index, into the representation
    for the word itself. That way, as the word’s representations get processed, the
    position information naturally comes along for the ride. The generic name for
    this process is *positional encoding*.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案是将每个单词的位置或索引插入到单词本身的表示中。这样，在处理单词的表示时，位置信息自然会跟随其一起传递。这个过程的通用名称是*位置编码*。
- en: A simple approach to positional encoding is to append a few bits to the end
    of each word to hold its location, as shown on the left of [Figure 20-19](#figure20-19).
    But at some point, we might get a sentence that requires more bits than we’ve
    made available, and then we’d be in trouble because we wouldn’t be able to assign
    each word a unique number for its location. And if we make the storage too big,
    it’s just wasted and slows everything down. This approach is also awkward to implement,
    since we then need to introduce some special mechanism for handling those bits
    (Thiruvengadam 2018).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 一种简单的位置编码方法是将几个比特附加到每个单词的末尾，用来表示它的位置，如[图20-19](#figure20-19)左侧所示。但在某些情况下，我们可能会遇到需要更多比特的句子，而这时我们就会遇到问题，因为我们无法为每个单词分配一个唯一的位置编号。如果我们把存储空间做得过大，就会浪费资源并使一切变得更慢。这种方法也很难实现，因为我们需要引入一些特殊的机制来处理这些比特（Thiruvengadam
    2018）。
- en: '![F20019](Images/F20019.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![F20019](Images/F20019.png)'
- en: 'Figure 20-19: Tracking the location of each word in a sentence. Left: Appending
    an index to each word. Middle: Using a function F to turn each index into a vector,
    then adding it to the word’s representation. Right: Our icon for a positional
    embedding layer.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图20-19：跟踪句子中每个单词的位置。左：将索引附加到每个单词。中：使用函数F将每个索引转换为向量，然后将其添加到单词的表示中。右：我们的位置嵌入层的图标。
- en: A better answer is to use a mathematical function that creates a unique vector
    for each position in a sequence. Suppose that our word embeddings are 128 elements
    long. Then we give this function the index of each word (which can be as large
    as it needs to be), and the function gives us back a new 128-element vector that
    somehow describes that location. Basically it turns the index into a unique list
    of values. Our expectation is that the network will learn to associate each of
    these lists with the word’s position in the input.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的答案是使用一个数学函数，为序列中的每个位置创建一个唯一的向量。假设我们的词向量长度为128个元素。然后我们将每个单词的索引（它可以根据需要变得非常大）传递给这个函数，函数会返回一个新的128元素的向量，某种程度上描述了该位置。基本上，它将索引转换为一个唯一的值列表。我们的期望是，网络将学会将这些列表与输入中单词的位置关联起来。
- en: Rather than appending this vector to the word’s representation, we add the two
    vectors together, as in the middle of [Figure 20-19](#figure20-19). Here we literally
    add the number in each element in the encoding to the corresponding number in
    the word’s embedding. The appeal of this approach is that we don’t need any extra
    bits or special processing. This form of positional encoding is called *positional
    embedding*, because of its similarity to the *word embedding* we saw earlier in
    algorithms like ELMo. The right side of the figure shows our icon for this process,
    which is drawn with a little sine wave because a popular choice for the embedding
    function is based on sine waves (derived from Vaswani et al. 2017).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 与其将这个向量附加到单词的表示中，我们将两个向量相加，如[图20-19](#figure20-19)中间所示。在这里，我们将编码中每个元素中的数字加到单词嵌入中相应位置的数字上。这种方法的吸引力在于我们不需要额外的比特或特殊的处理。这种位置编码的形式叫做*位置嵌入*，因为它类似于我们在像ELMo这样的算法中看到的*词嵌入*。图的右侧展示了我们为这个过程设计的图标，它用一个小的正弦波来表示，因为流行的嵌入函数选择基于正弦波（来源于Vaswani
    等，2017）。
- en: It may seem a bit weird to add position information to each word, rather than
    append it, as it changes the word’s representation. It also seems that the position
    information is liable to get lost as the attention network processes the values.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 将位置信息添加到每个单词中，而不是附加到后面，可能看起来有点奇怪，因为这改变了单词的表示。似乎位置信息也容易在注意力网络处理这些值时丢失。
- en: It turns out that the specific function that is frequently used to compute the
    positional embedding vector usually affects only a few bits at one end of the
    word’s vector (Vaswani et al. 2017; Kazemnejad 2019). Furthermore, it appears
    that transformers learn how to distinguish each word’s representation and position
    information during processing so they’re interpreted separately (TensorFlow 2019a).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，用于计算位置嵌入向量的特定函数通常只会影响单词向量一端的几个比特（Vaswani 等，2017；Kazemnejad 2019）。此外，似乎变压器模型在处理过程中学会了区分每个单词的表示和位置信息，因此它们被分别解释（TensorFlow
    2019a）。
- en: But why doesn’t the position embedding get lost altogether during processing?
    After all, attention changes its inputs using neural networks by turning them
    into QKV values and then mixing those values. Surely the positional information
    would be hopelessly scrambled and lost.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 那么为什么位置嵌入在处理过程中不会完全丢失呢？毕竟，注意力通过神经网络将输入转换为QKV值，然后混合这些值。位置相关的信息肯定会被混乱并丢失吧。
- en: The clever solution to this problem is built into the architecture of the transformer
    itself. As we’ll see, the transformer network wraps up each operation (except
    the very last) in a skip connection. The embedding information never gets lost,
    because it gets added back in after every stage of processing. [Figure 20-20](#figure20-20)
    illustrates how positional embedding and norm-add skip connections are structurally
    similar. In short, each layer can change its input vector in any way it wants,
    and then the positional embedding gets added back in so that it’s available to
    the next layer.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题的巧妙解决方案内建于变换器的架构中。正如我们将看到的，变换器网络将每个操作（除了最后一个）包装在跳跃连接中。嵌入信息从未丢失，因为它在每个处理阶段后都会被重新添加。[图
    20-20](#figure20-20)说明了位置嵌入和规范加跳跃连接在结构上的相似性。简而言之，每一层都可以以任何方式改变其输入向量，然后位置嵌入会被重新加回，以便下层使用。
- en: '![F20020](Images/F20020.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![F20020](Images/F20020.png)'
- en: 'Figure 20-20: Left: Creating a position embedding and adding it to a word.
    Right: A norm-add operation implicitly adds a word’s embedding information back
    in after processing.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20-20：左图：创建位置嵌入并将其添加到单词中。右图：一个规范加操作在处理后隐式地将单词的嵌入信息添加回来。
- en: Assembling a Transformer
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 组装一个变换器
- en: We now have all the pieces in place to build a transformer. We’ll continue to
    use word-level translation as our running example.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经具备了构建变换器的所有组成部分。我们将继续使用基于单词的翻译作为我们的运行示例。
- en: It’s important to note that the name *transformer* refers to a wide variety
    of networks inspired by the architecture in the original transformer paper (Vaswani
    et al. 2017). In this discussion, we’ll stick to a generic version.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，*变换器*这一名称指的是受原始变换器论文（Vaswani 等，2017）架构启发的各种网络。在本讨论中，我们将坚持使用一种通用版本。
- en: Our block diagram of a transformer is shown in [Figure 20-21](#figure20-21).
    The blocks marked *E* and *D* are repeated sequences of layers, or *blocks*, built
    around attention layers. We’ll look at both types of block in detail in a moment.
    The big picture is that an encoder stage (built from *encoder blocks*, marked
    with an *E*) accepts a sentence, and a decoder (build from *decoder blocks*, marked
    with a *D*) accepts information from the encoder and produces new output (the
    structure of this diagram is reminiscent in some ways of an unrolled seq2seq diagram,
    but there are no recurrent cells here).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的变换器框图如[图 20-21](#figure20-21)所示。标记为*E*和*D*的模块是由注意力层构建的重复层序列或*模块*。稍后我们会详细介绍这两种模块。大体框架是，编码器阶段（由*编码器模块*，标记为*E*构成）接受一个句子，解码器（由*解码器模块*，标记为*D*构成）接受来自编码器的信息并产生新的输出（该图的结构在某些方面类似于展开的seq2seq图，但这里没有递归单元）。
- en: '![F20021](Images/F20021.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![F20021](Images/F20021.png)'
- en: 'Figure 20-21: A block diagram of a transformer. An input is encoded and then
    decoded. The decoder’s output is fed back to its input autoregressively. The dashed
    lines stand for repeated elements.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20-21：一个变换器的框图。输入被编码然后解码。解码器的输出被递归地反馈到其输入。虚线表示重复的元素。
- en: Both the encoder and decoder begin with word embedding followed by positional
    embedding. The decoder has the usual fully connected layer and softmax at the
    end for predicting the next word. The decoder is autoregressive, so it appends
    each output word to the list of its outputs (shown by the box at the bottom of
    the figure), and that list becomes the decoder’s input for generating the next
    word. The decoder contains multi-head Q/KV attention networks, as in [Figure 20-14](#figure20-14),
    which receive their keys and values from the outputs of the encoder blocks, shown
    in the middle of [Figure 20-21](#figure20-21), where the encoder outputs are delivered
    to the decoder blocks. This illustrates why Q/KV attention is also called encoder-decoder
    attention.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器和解码器都从词嵌入开始，接着是位置嵌入。解码器末尾有常见的全连接层和softmax，用于预测下一个词。解码器是自回归的，因此它将每个输出词附加到其输出列表中（见图底部的框），该列表成为生成下一个词的解码器输入。解码器包含多头Q/KV注意力网络，如[图
    20-14](#figure20-14)所示，这些网络的键和值来自编码器块的输出，见[图 20-21](#figure20-21)中部，编码器的输出被传递到解码器块中。这说明了为什么Q/KV注意力也叫做编码器-解码器注意力。
- en: Let’s look more closely at the blocks in [Figure 20-21](#figure20-21) starting
    with the encoder block, shown in [Figure 20-22](#figure20-22).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看[图 20-21](#figure20-21)中的块，从编码器块开始，见[图 20-22](#figure20-22)。
- en: '![F20022](Images/F20022.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![F20022](Images/F20022.png)'
- en: 'Figure 20-22: The transformer’s encoder block. The first layer is self-attention.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20-22：变压器的编码器块。第一层是自注意力。
- en: The encoder block begins with a layer of multi-head self-attention, shown here
    with eight heads. Because this layer applies self-attention, the queries, keys,
    and values are all derived from the single set of inputs that arrive at the block.
    This multi-head attention is surrounded by a norm-add skip connection to help
    keep the numbers looking like a Gaussian and to retain the positional embeddings.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器块以一层多头自注意力开始，这里展示了八个头。因为这一层应用的是自注意力，查询、键和值都来源于到达该块的单一输入集。这个多头注意力层被一个归一化加跳跃连接所包围，帮助保持数值呈高斯分布，并保留位置信息嵌入。
- en: This is followed by two layers that are usually referred to collectively as
    a *pointwise feed-forward layer* (another unfortunately vague name). Though the
    original transformers paper described these as a pair of modified fully connected
    layers (Vaswani et al. 2017), we can more conveniently think of them as two layers
    of 1×1 convolution (Chromiak 2017; Singhal 2020; A. Zhang et al. 2020). They learn
    how to adjust the output of the multi-head attention layer to remove redundancy
    and focus on just the information that will be of the most value to whatever processing
    comes next. The first convolution uses a ReLU activation function, while the second
    has no activation function. As usual, these two steps are wrapped in a norm-add
    skip connection.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 紧接着是两层，通常统称为*逐点前馈层*（另一个遗憾的模糊名称）。虽然原始的变压器论文将其描述为一对修改过的全连接层（Vaswani等人，2017），我们可以更方便地将其视为两层1×1卷积（Chromiak
    2017；Singhal 2020；A. Zhang等人，2020）。它们学习如何调整多头注意力层的输出，以去除冗余并集中处理接下来最有价值的信息。第一层卷积使用ReLU激活函数，而第二层则不使用激活函数。像往常一样，这两步被包裹在归一化加跳跃连接中。
- en: Now let’s look at the decoder block, shown in [Figure 20-23](#figure20-23).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看解码器块，见[图 20-23](#figure20-23)。
- en: '![F20023](Images/F20023.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![F20023](Images/F20023.png)'
- en: 'Figure 20-23: The transformer’s decoder block. Note that the first attention
    layer is self-attention, whereas the second is Q/KV attention. The triangle on
    the left of the self-attention layer indicates that the layer uses masking.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20-23：变压器的解码器块。请注意，第一个注意力层是自注意力，而第二个是Q/KV注意力。自注意力层左侧的三角形表示该层使用了掩蔽。
- en: At a high level, it looks a lot like the encoder block, with an extra step of
    attention. Let’s walk through the layers.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，它与编码器块非常相似，只是多了一步注意力操作。让我们逐层分析。
- en: We begin with a multi-head self-attention layer, just like the encoder block.
    The input to the layer is the words output so far by the transformer. If we’re
    just beginning, this sentence contains only the `[START]` token. Like any self-attention
    layer, the purpose here is to look at all of the input words and work out which
    ones are most strongly related to which others. As usual, this is wrapped in a
    skip connection with a norm-add node at the end. During training, we add an extra
    detail called *masking* to this self-attention step (indicated with a small triangle
    in [Figure 20-23](#figure20-23)), which we’ll come back to shortly.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个多头自注意力层开始，和编码器模块一样。该层的输入是到目前为止 Transformer 输出的单词。如果我们刚开始，这句话只包含 `[START]`
    标记。像任何自注意力层一样，目的在于查看所有输入单词，并确定哪些单词彼此之间关系最为紧密。像往常一样，这一过程通过一个跳跃连接与一个归一化加节点（norm-add
    node）包裹。训练过程中，我们在这一自注意力步骤中添加一个额外的细节，称为 *遮蔽*（masking），这一点在[图 20-23](#figure20-23)中通过一个小三角形标出，稍后我们会详细解释。
- en: The self-attention layer is followed by a multi-head Q/KV attention layer. The
    query, or Q, vectors come from the output of the previous self-attention layer.
    The keys and values come from the concatenated outputs of all the encoder blocks.
    This layer also is wrapped in a skip connection with a norm-add node at the end.
    This stage uses the outputs of the previous attention network to choose among
    the keys coming from the encoder and then mix the values corresponding to those
    keys. Finally, we have a pair of 1×1 convolutions, following the same pattern
    as in the encoder block.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力层后面是一个多头 Q/KV 注意力层。查询向量（Q）来自前一个自注意力层的输出。键（keys）和值（values）则来自所有编码器模块的连接输出。这个层同样通过一个跳跃连接与一个归一化加节点（norm-add
    node）包裹。该阶段使用前一个注意力网络的输出，在来自编码器的键中进行选择，并混合与这些键对应的值。最后，我们有一对 1×1 卷积，遵循与编码器模块相同的模式。
- en: We can now put the pieces together. [Figure 20-24](#figure20-24) shows the structure
    of a transformer model.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将各个部分组合起来了。[图 20-24](#figure20-24)展示了一个 Transformer 模型的结构。
- en: '![F20024](Images/F20024.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![F20024](Images/F20024.png)'
- en: 'Figure 20-24: A complete transformer. The icons showing two stacked boxes represent
    two consecutive 1×1 convolutions. The dashed lines stand for repeated elements
    that are not drawn.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '图 20-24: 完整的 Transformer。图标显示的两个叠加框代表两个连续的 1×1 卷积。虚线表示没有绘制的重复元素。'
- en: We promised to return to a detail regarding the first attention layer in each
    decoder block. As we mentioned, one of the great values of the attention mechanism
    at the heart of the transformer is that it allows for a lot of parallelism. Whether
    an attention block is given five words or five hundred, it runs in the same amount
    of time.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们曾经承诺回到每个解码器模块中第一个注意力层的一个细节。正如我们提到的，Transformer 中核心的注意力机制的一个重要价值就是它允许大量并行化。无论注意力块接收五个单词还是五百个单词，它都能在相同的时间内运行。
- en: Suppose we’re training the system to predict the next word in a sentence. We
    can provide it with the entire sentence and ask it to predict the first word,
    the second word, the third word, and so on, all in parallel.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们在训练系统预测句子中的下一个单词。我们可以提供整个句子，并让它并行预测第一个单词、第二个单词、第三个单词，以此类推。
- en: But there’s a problem here. Suppose the sentence is My dog loves taking long
    walks. We could give the system My dog loves taking long, and ask it to predict
    the sixth word, walks. But because we’re training in parallel, we want it to use
    this same input to predict each of the previous words, at the same time. That
    is, we also want it to predict the fifth word, long, from the input My dog loves
    taking long.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 但是这里有一个问题。假设句子是 "My dog loves taking long walks"。我们可以给系统输入 "My dog loves taking
    long"，并让它预测第六个单词 "walks"。但因为我们是在并行训练，我们希望它能利用相同的输入同时预测每一个之前的单词。也就是说，我们还希望它能从输入
    "My dog loves taking long" 中预测第五个单词 "long"。
- en: 'That’s too easy: the word long is right there! The system would find that all
    it has to do is return the fifth word, which is definitely not the same as learning
    how to predict it. We want to give the system My dog loves taking long as input,
    but for predicting the fifth word, it should only see My dog loves taking. We
    want to hide, or mask, the word long when we’re trying to predict it. Similarly,
    to predict the fourth word, it should only see My dog loves, to predict the third
    word it should only see My dog, and so on.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这太简单了：单词 long 就在这里！系统会发现它只需要返回第五个单词，而这显然不同于学会如何预测它。我们希望给系统输入 "My dog loves taking
    long"，但在预测第五个单词时，它只应该看到 "My dog loves taking"。当我们尝试预测 long 时，我们希望将其隐藏或屏蔽。同样地，为了预测第四个单词，它应该只看到
    "My dog loves"，为了预测第三个单词，它应该只看到 "My dog"，以此类推。
- en: In short, our transformer will run five parallel computations, each predicting
    a different word, but each computation should only be given the words that came
    before the one it’s supposed to predict.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我们的变换器将执行五个并行计算，每个计算预测一个不同的单词，但每个计算应该只接收它应该预测的单词之前的单词。
- en: The mechanism to pull this off is called *masking*. We add an extra step to
    the first self-attention layer in the decoder block that masks, or hides, the
    words that each prediction step isn’t supposed to see. Thus the computation predicting
    the first word sees no input words, the computation predicting the second word
    only sees My, the one predicting the third word only sees My dog, and so on. Because
    of this extra step, the first attention layer in the decoder block is sometimes
    called a *masked multi-head self-attention* layer, which is a mouthful, so we
    often just refer to it as a *masked attention* layer.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这一点的机制叫做*屏蔽*。我们在解码器模块的第一个自注意力层中增加了一个额外的步骤，屏蔽或隐藏每个预测步骤不应该看到的单词。因此，预测第一个单词的计算不会看到任何输入单词，预测第二个单词的计算只会看到
    "My"，预测第三个单词的计算只会看到 "My dog"，以此类推。由于这个额外的步骤，解码器模块中的第一个注意力层有时被称为*屏蔽多头自注意力*层，这个名字有点难以说清，因此我们通常将其简称为*屏蔽注意力*层。
- en: Transformers in Action
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 变换器的实际应用
- en: Let’s see a transformer in action performing a translation. We trained a transformer
    following roughly the architecture of [Figure 20-24](#figure20-24) to translate
    from Portuguese to English (TensorFlow 2019b). We used a dataset of 50,000 training
    examples, which is small by today’s standards but good enough to demonstrate the
    ideas while also of a practical size to train from on a home computer (Kelly 2020).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下变换器在执行翻译时的表现。我们训练了一个变换器，基本遵循了[图 20-24](#figure20-24)的架构，用于将葡萄牙语翻译成英语（TensorFlow
    2019b）。我们使用了一个包含50,000个训练示例的数据集，按照今天的标准，这个数据集比较小，但足以展示这些概念，同时也足够大，可以在家用计算机上进行训练（Kelly
    2020）。
- en: We gave our trained transformer the Portuguese question, você se sente da mesma
    maneira que eu? which Google Translate renders into English as do you feel the
    same that way I do? Our system produced the translation, do you see , do you get
    the same way i do ? This isn’t perfect, but given the small training database,
    it does a great job of capturing the spirit of the question. As always, more training
    data and training time would surely improve the results.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们给训练好的变换器输入了葡萄牙语问题 "você se sente da mesma maneira que eu?"，而 Google 翻译将其翻译成了英语
    "do you feel the same that way I do?"。我们的系统产生的翻译是 "do you see, do you get the
    same way i do?"。这并不完美，但考虑到小规模的训练数据库，它已经很好地捕捉到了问题的精神。像往常一样，更多的训练数据和训练时间肯定能改进结果。
- en: Heatmaps showing the attention paid to each input word by each output word,
    for each of the eight heads in the final Q/KV attention layer of the decoder,
    are shown in [Figure 20-25](#figure20-25). The brighter the cell, the more attention
    was paid. Note that some input words were broken up into multiple tokens by a
    preprocessor.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 20-25](#figure20-25)显示了热图，展示了每个输出单词在解码器的最终 Q/KV 注意力层中，每个注意力头对每个输入单词的关注程度。单元格越亮，表示越多的注意力被关注。请注意，一些输入单词被预处理器拆分成多个标记。'
- en: Transformers trained on larger datasets than this example, and for longer periods,
    can produce results that are as good or better than RNNs, and they can be trained
    in parallel. They don’t need recurrent cells with finite internal states that
    can run out of memory, nor do they need multiple neural networks to learn how
    to control those states. These are big advantages and explain why transformers
    have replaced RNNs in many applications.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在比这个示例更大的数据集上训练，并且训练时间更长的转换器模型，能够产生与RNN相媲美甚至更好的结果，而且它们可以并行训练。它们不需要具有有限内部状态的递归单元，也不需要多个神经网络来学习如何控制这些状态。这些是巨大的优势，也解释了为什么转换器在许多应用中取代了RNN。
- en: '![F20025](Images/F20025.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![F20025](Images/F20025.png)'
- en: 'Figure 20-25: Heatmaps for each of the eight heads in the final Q/KV attention
    layer of the decoder during a translation of “*Você se sente da mesma maneira
    que eu?*” from Portuguese to English'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图20-25：在将葡萄牙语句子“*Você se sente da mesma maneira que eu?*”翻译成英语时，解码器中最终Q/KV注意力层的八个头部的热力图
- en: One downside of transformers is that the memory required by the attention layers
    grows dramatically with the size of the input. There are ways to adjust the attention
    mechanism, and the transformer in general, to reduce these costs in different
    situations (Tay et al. 2020).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器的一个缺点是，随着输入大小的增加，注意力层所需的内存会急剧增加。为了减少不同情况下的这些开销，有一些方法可以调整注意力机制以及转换器（Tay等，2020）。
- en: BERT and GPT-2
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BERT和GPT-2
- en: The full transformer model of [Figure 20-24](#figure20-24) consists of an encoder,
    which is designed to analyze the input text and create a series of context vectors
    that describe it, and a decoder, which uses that information to autoregressively
    generate a translation of the input.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '[图20-24](#figure20-24)中的完整转换器模型由一个编码器组成，编码器的设计目的是分析输入文本并创建一系列描述它的上下文向量，接着是一个解码器，解码器使用这些信息进行自回归生成输入的翻译。'
- en: The blocks making up the encoder and decoder are not specific to translation.
    Each is just one or more attention layers, followed by a pair of 1×1 convolutions.
    These blocks can be used as general-purpose processors for working out the relationship
    between elements of a sequence, and language in particular. Let’s look at two
    recent architectures that have used transformer blocks in ways that go way beyond
    translation.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 组成编码器和解码器的模块并不特定于翻译。每个模块只是一个或多个注意力层，后面跟着一对1×1卷积层。这些模块可以作为通用处理器，用于处理序列元素之间的关系，尤其是语言方面。让我们看看两个最近的架构，它们以超出翻译范畴的方式使用了转换器模块。
- en: BERT
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: BERT
- en: Let’s use transformer blocks to create a general-purpose language model. It
    can be used for any of the tasks we listed at the start of Chapter 19.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用转换器模块来创建一个通用语言模型。它可以用于我们在第19章开头列出的任何任务。
- en: The system is called *Bidirectional Encoder Representations from Transformers*,
    but it’s more commonly known by its acronym, *BERT* (Devlin et al. 2019) (another
    Muppet from *Sesame Street*, and a nodding reference to the ELMo system we saw
    earlier). The structure of BERT begins with a word embedder and a position embedder,
    followed by multiple transformer encoder blocks. The basic architecture is shown
    in in [Figure 20-26](#figure20-26) (in practice, other details help with training
    and performance, such as dropout layers). In this diagram, we’re showing the many
    inputs and outputs so that it’s clear that BERT is processing an entire sentence.
    For consistency and clarity, we’re only using a single line inside the blocks,
    but the parallel operations are still being carried out. It’s traditional to draw
    BERT diagrams with a yellow color scheme, since Bert on *Sesame Street* is a yellow
    character.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 该系统被称为*双向编码器表示转换器*，但通常以其缩写*BERT*为人们所熟知（Devlin等，2019）（另一个来自*芝麻街*的木偶，也是对我们之前看到的ELMo系统的一个点头式致敬）。BERT的结构从一个词嵌入层和一个位置嵌入层开始，接着是多个转换器编码器模块。BERT的基本架构如[图20-26](#figure20-26)所示（在实际操作中，还有其他有助于训练和性能的细节，如丢弃层）。在这个图中，我们展示了多个输入和输出，以清楚地表明BERT正在处理整个句子。为了保持一致性和清晰性，我们只在模块内部使用单行，但并行操作仍在进行。传统上，BERT的图示通常使用黄色配色方案，因为*芝麻街*中的Bert是一个黄色角色。
- en: '![F20026](Images/F20026.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![F20026](Images/F20026.png)'
- en: 'Figure 20-26: The basic structure of BERT. The dashed lines stand for more
    encoder blocks that are not drawn.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图20-26：BERT的基本结构。虚线表示未绘制的更多编码器模块。
- en: The original “large” version of BERT deserved its name, with 340 million weights,
    or parameters. The system was trained on Wikipedia and over 10,000 books (Zhu
    et al. 2015). Currently, 24 trained versions of the original BERT system are available
    freely online (Devlin et al. 2020), as well as a growing number of variations
    and improvements on the basic approach (Rajasekharan 2019).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的“大型”版本的 BERT 确实名副其实，拥有 3.4 亿个权重或参数。该系统在维基百科和超过 1 万本书籍上进行了训练（Zhu 等人 2015）。目前，原始
    BERT 系统的 24 个训练版本可以免费在线获得（Devlin 等人 2020），同时，基于该方法的变化和改进也在不断增加（Rajasekharan 2019）。
- en: BERT was trained on two tasks. The first is called *next sentence prediction*,
    or *NSP*. In this technique, we give BERT two sentences at once (with a special
    token to separate them), and we ask it to determine if the second sentence reasonably
    follows the first. The second task presents the system with sentences where some
    of the words have been removed, and we ask it to fill in the blanks (language
    educators call this the *cloze task;* Taylor 1953). It’s the linguistic analog
    of the visual process called *closure*, describing the human tendency to fill
    in the blanks in images. Closure is illustrated in [Figure 20-27](#figure20-27).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 经过两项任务的训练。第一项任务叫做*下一个句子预测*，或称*NSP*。在这项技术中，我们一次性给 BERT 两个句子（用一个特殊符号将它们分开），并让它判断第二个句子是否合理地跟随第一个句子。第二项任务给系统提供了一些句子，其中一些词被去掉，我们要求它填补这些空缺（语言学教育者称之为*填空任务*；Taylor
    1953）。这与视觉过程中的*闭合*现象类似，描述了人类倾向于在图像中填补空白。闭合现象在[图 20-27](#figure20-27)中有所展示。
- en: '![F20027](Images/F20027.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![F20027](Images/F20027.png)'
- en: 'Figure 20-27: Demonstrating the principle of closure. Incomplete shapes like
    these are usually filled in by the human visual system to create objects.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20-27：展示闭合原理。像这样的不完整形状通常会被人类视觉系统填补，从而形成物体。
- en: BERT is able to do well on these tasks because, compared to the RNN-based methods
    we saw before, BERT’s attention layers extract much more information from their
    inputs. Our first RNN models were *unidirectional*, reading inputs left to right.
    Then they became *bidirectional*, culminating in ELMo, which can be said to be
    *shallowly bidirectional*, where *shallow* refers to the architecture’s use of
    only two layers in each direction. Thanks to attention, BERT is able to determine
    the influence of every word on every other word, and by repeating the encoder
    block, it can do this many times in a row. BERT is sometimes called *deeply bidirectional*,
    but it might be more useful to think of it as *deeply dense*, since it considers
    every word simultaneously. The notion of direction really doesn’t apply when we’re
    using attention.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 能够在这些任务中表现良好，因为与之前看到的基于 RNN 的方法相比，BERT 的注意力层能从输入中提取更多信息。我们最初的 RNN 模型是*单向的*，从左到右读取输入。然后它们变成了*双向的*，最终发展为
    ELMo，可以说是*浅层双向的*，其中*浅层*指的是每个方向上仅使用两层架构。得益于注意力机制，BERT 能够确定每个词对其他词的影响，并通过重复编码器块，它可以连续执行这一过程多次。BERT
    有时被称为*深度双向的*，但将其视为*深度密集的*可能更为有用，因为它同时考虑每个词。在使用注意力时，方向的概念实际上并不适用。
- en: Let’s take BERT out for a spin. We’ll start with a pretrained model of 12 encoder
    blocks (McCormick and Ryan 2020). We’ll fine-tune it to determine if an input
    sentence is grammatical or not (Warstadt, Singh, and Bowman 2018; Warstadt, Singh,
    and Bowman 2019). This is basically a classification problem, producing a yes/no
    answer. Therefore, our downstream model should be a classifier of some kind. Let’s
    use a simple classifier consisting of a single fully connected layer. Our combined
    pair of models is shown in [Figure 20-28](#figure20-28).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来体验一下 BERT。我们将从一个包含 12 个编码器块的预训练模型开始（McCormick 和 Ryan 2020）。我们将对其进行微调，以确定输入句子是否符合语法（Warstadt,
    Singh, 和 Bowman 2018；Warstadt, Singh, 和 Bowman 2019）。这基本上是一个分类问题，产生是/否的答案。因此，我们的下游模型应该是某种分类器。我们使用一个由单个全连接层组成的简单分类器。我们这对模型的组合展示在[图
    20-28](#figure20-28)中。
- en: '![F20028](Images/F20028.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![F20028](Images/F20028.png)'
- en: 'Figure 20-28: BERT with a small downstream classifier at the end. The dashed
    lines stand for the 10 additional, identical encoder blocks that are present,
    but not drawn.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20-28：BERT 加上一个小型下游分类器。虚线表示存在的 10 个附加的相同编码器块，但没有绘制出来。
- en: After four epochs of training, here are six results from the testing data. The
    first three are grammatical, and the second three are not. BERT produced the correct
    answer on all six.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在四个训练周期之后，以下是测试数据的六个结果。前三个是语法正确的，后三个则不是。BERT 在六个结果中都给出了正确答案。
- en: Chris walks, Pat eats broccoli, and Sandy plays squash.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 克里斯走路，帕特吃西兰花，桑迪打壁球。
- en: There was some particular dog who saved every family.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有一只特别的狗，拯救了每个家庭。
- en: Susan frightens her.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 苏珊吓到了她。
- en: The person confessed responsible.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个人承认负责。
- en: The cat slept soundly and furry.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 猫睡得又香又毛茸茸。
- en: The soundly and furry cat slept.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 那只香甜且毛茸茸的猫睡着了。
- en: On the test set of about 1,000 sentences, this little version of BERT got about
    82 percent of the examples correct. Some BERT variants have achieved more than
    88 percent right on this task (Wang et al. 2019; Wang et al. 2020).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在约1,000个句子的测试集上，这个小版本的BERT正确预测了大约82%的示例。某些BERT变体在这个任务中达到了超过88%的正确率（Wang 等，2019；Wang
    等，2020）。
- en: Let’s try BERT out on another task, called *sentiment analysis*. We’ll classify
    short movie reviews as being either positive or negative in tone. The data comes
    from a database of almost 7,000 movie reviews called *SST2*, where each review
    has been labeled as positive or negative (Socher et al. 2013a; Socher et al. 2013b).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在另一个任务中尝试BERT，称为*情感分析*。我们将把短片影评分类为正面或负面。数据来自一个名为*SST2*的数据库，包含近7,000条电影评论，每条评论都标记为正面或负面（Socher
    等，2013a；Socher 等，2013b）。
- en: For this run, we used a pretrained BERT model called DistillBERT (Sanh et al.
    2020; Alammar 2019) (the term *distilling* is often used when we carefully trim
    a trained neural network to make it smaller and faster without losing much performance).
    We’re again doing a classification task, so we can reuse the model of [Figure
    20-28](#figure20-28).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这次运行，我们使用了一个名为DistillBERT的预训练BERT模型（Sanh 等，2020；Alammar 2019）（术语*蒸馏*通常用于当我们仔细修剪一个已训练的神经网络，使其更小更快，同时不失去太多性能时）。我们再次进行分类任务，因此可以重用[图20-28](#figure20-28)中的模型。
- en: Here are six examples verbatim from the test data (there’s no indication of
    what movies they each refer to). DistillBERT properly classified the first three
    reviews as positive and the second three as negative (the reviews are all lowercase,
    and commas are treated as their own tokens).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是从测试数据中逐字摘录的六个示例（无法得知它们分别指代哪些电影）。DistillBERT正确将前3条评论分类为正面，将后3条评论分类为负面（这些评论都是小写字母，逗号被当作独立的标记）。
- en: a beautiful , entertaining two hours
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一部美丽、娱乐性十足的两小时电影。
- en: this is a shrewd and effective film from a director who understands how to create
    and sustain a mood
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是一部聪明且有效的电影，导演懂得如何创造并维持情绪。
- en: a thoroughly engaging , surprisingly touching british comedy
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一部彻底引人入胜、出人意料地感人的英国喜剧。
- en: the movie slides downhill as soon as macho action conventions assert themselves
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦强势的动作片惯例占据主导，电影便开始下滑。
- en: a zombie movie in every sense of the word mindless , lifeless , meandering ,
    loud , painful , obnoxious
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一部完全意义上的僵尸电影，毫无头脑、毫无生气、漫无目的、喧闹、痛苦、令人讨厌。
- en: it is that rare combination of bad writing , bad direction and bad acting the
    trifecta of badness
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是那种罕见的糟糕写作、糟糕导演和糟糕表演的结合，糟糕的三重奏。
- en: Of the 1,730 reviews in the test set, DistillBERT correctly predicted the sentiment
    of about 82 percent of them.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试集中的1,730条评论中，DistillBERT正确预测了大约82%的情感。
- en: To recap, models based on the BERT architecture are united by their use of a
    sequence of encoder blocks. They create an embedding of a sentence that captures
    enough information that downstream applications can perform a wide range of operations
    upon it. With an appropriate downstream model, BERT can be used to perform many
    of the NLP tasks we mentioned at the start of Chapter 19.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，基于BERT架构的模型的共同点在于它们使用了一系列的编码器块。它们创建了一个句子的嵌入，捕捉了足够的信息，以便下游应用能够对其进行广泛的操作。通过合适的下游模型，BERT可以用来执行我们在第19章开始时提到的许多自然语言处理任务。
- en: If we’re willing to get clever, we can make BERT generate language, but it’s
    not easy (Mishra 2020; Mansimov et al. 2020). A better solution is to use decoder
    blocks, as we’ll see next.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们愿意变得聪明一些，我们可以让BERT生成语言，但这并不容易（Mishra 2020；Mansimov 等，2020）。一个更好的解决方案是使用解码器块，接下来我们将看到这一点。
- en: GPT-2
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GPT-2
- en: We’ve seen how transformers use a series of decoder blocks to generate words
    for a translation. We can also use a sequence of decoder blocks to generate new
    text.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到变换器如何使用一系列解码器块来生成翻译的单词。我们也可以使用一系列解码器块来生成新的文本。
- en: Since we don’t have an encoder stage to receive KV values from, as in the full
    transformer of [Figure 20-24](#figure20-24), let’s remove the Q/KV multi-head
    attention layer from each decoder block, leaving us with just masked self-attention
    and a pair of 1×1 convolutions. The first system to do this in a big way was called
    the *Generative Pre-Training model 2*, or simply *GPT-2* (Radford et al. 2019).
    Its architecture is shown in [Figure 20-29](#figure20-29).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们没有像完整transformer中的[图20-24](#figure20-24)那样接收KV值的编码器阶段，我们将从每个解码器块中移除Q/KV多头注意力层，留下掩蔽自注意力和一对1×1卷积。第一个以这种方式做的系统被称为*生成预训练模型2*，简称*GPT-2*（Radford等人，2019年）。其架构如[图20-29](#figure20-29)所示。
- en: '![F20029](Images/F20029.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![F20029](Images/F20029.png)'
- en: 'Figure 20-29: A block diagram of GPT-2, made out of transformer decoder blocks
    without the Q/KV layer. The dashed lines stand for more repeated, identical decoder
    blocks. Note that because these are versions of decoder blocks, the first multi-head
    attention layer in each block is a masked attention layer.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图20-29：GPT-2的框图，由没有Q/KV层的transformer解码器块构成。虚线表示更多重复的、相同的解码器块。请注意，由于这些是解码器块的版本，每个块中的第一个多头注意力层是一个掩蔽注意力层。
- en: Like BERT, we start with token embedding followed by positional embedding for
    each input word. The self-attention layer in each decoder block uses masking as
    before so that as we compute attention for any given word, we can only use information
    from that word and those that precede it.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 与BERT类似，我们从令牌嵌入开始，然后为每个输入单词添加位置嵌入。每个解码器块中的自注意力层像以前一样使用掩蔽处理，这样在计算任何给定单词的注意力时，我们只能使用该单词以及之前单词的信息。
- en: The original GPT-2 model was released in several different sizes, the largest
    of which processed 512 tokens at a time through 48 decoder blocks with 12 heads
    in each, for a total of 1,542 million parameters. That’s 1.5 *billion* parameters.
    GPT-2 was trained on a dataset called *WebText*, which contained about eight million
    documents for a total of about 40GB of text (Radford et al. 2019).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的GPT-2模型发布了几种不同的大小版本，其中最大的版本一次处理512个token，通过48个解码器块，每个块有12个头，总共有1,542百万个参数。也就是1.5
    *十亿*个参数。GPT-2是在一个名为*WebText*的数据集上进行训练的，该数据集包含大约八百万个文档，总共大约40GB的文本（Radford等人，2019年）。
- en: We typically use GPT-2 by starting with the pretrained model, and then we fine-tune
    it by providing an additional dataset to learn from, adjusting all of the weights
    in the process (Alammar 2018).
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常通过从预训练模型开始使用GPT-2，然后通过提供一个附加数据集来微调它，过程中调整所有的权重（Alammar 2018年）。
- en: We started each of our text generators in Chapter 19 with a seed, but that’s
    only one way to get them started. A simpler approach starts the system with general
    guidance and a prompt. This is called a *zero-shot* scenario, since the system
    has been given zero “shots,” or examples, for it to use as a model for new text.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第19章中启动了每个文本生成器时都使用了一个种子，但那只是让它们开始运行的一种方式。一个更简单的方法是用一般的指导和提示来启动系统。这被称为*零-shot*场景，因为系统在没有任何“示例”的情况下开始工作，也就是说，它没有任何用于生成新文本的示例或模型。
- en: 'For instance, suppose we built a system to advise us on what to wear each day.
    A zero-shot scenario might start with the instruction, Describe today’s outfit,
    followed by the prompt, Today I should wear: The generator takes it from there.
    It has no examples or context to work from, so it might suggest a suit of armor,
    a spacesuit, or a bear skin.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们建立了一个系统来建议每天穿什么。一个零-shot场景可能以指令“描述今天的服装”开始，接着是提示“今天我应该穿：” 然后生成器会继续。它没有任何示例或上下文可以使用，所以它可能会建议穿盔甲、太空服或者熊皮。
- en: 'Alternatively, we can provide one or more examples, or shots. In a one-shot
    scenario, we might give the instruction Describe today’s outfit, followed by the
    example, Yesterday I wore a blue shirt and black pants, and conclude with the
    prompt, Today I should wear: The thinking is that the text that’s provided before
    the prompt can help guide the system into the kind of output we want. In this
    case, the bear skin would be less likely.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，我们可以提供一个或多个示例，或称为shot。在一个one-shot场景中，我们可能给出指令“描述今天的服装”，接着是示例“昨天我穿了一件蓝色衬衫和黑色裤子”，然后以提示“今天我应该穿：”结尾。思路是，在提示之前提供的文本可以帮助引导系统生成我们想要的输出。在这种情况下，熊皮的可能性会小一些。
- en: If we give the system two or three shots, but not many more, we usually call
    it a *few-shot* scenario (these terms don’t have sharp cutoffs). People usually
    prefer generators that require as few shots as possible in order to provide the
    output we want.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们给系统两三次机会，但不再多次尝试，通常称之为*少-shot*情境（这些术语并没有明确的界限）。人们通常倾向于选择那些需要尽可能少的尝试来提供我们所需输出的生成器。
- en: 'Let’s see GPT-2 in action, using a medium-sized, pretrained GPT-2 model (von
    Platen 2020). We won’t do any fine tuning, so the system will generate text based
    only on its core training data. Let’s take a zero-shot approach, and give it no
    information except the starting prompt, I woke up this morning to the roar of
    a hippopotamus. Here’s a typical output, verbatim:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看GPT-2的实际应用，使用一个中等大小、预训练的GPT-2模型（von Platen 2020）。我们不进行微调，因此系统将仅根据其核心训练数据生成文本。我们采取零-shot方法，除了起始提示语“今天早上我被河马的咆哮声吵醒”外，不提供任何信息。以下是一个典型的输出，逐字输出：
- en: I woke up this morning to the roar of a hippopotamus. I was in the middle of
    a long walk, and I saw a huge hippopotamus. I was so excited. I was so excited.
    I was so excited. I was so excited.
  id: totrans-253
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 今天早上我被河马的咆哮声吵醒。我正在进行一次长时间的散步，突然看到一只巨大的河马。我非常激动。我非常激动。我非常激动。我非常激动。
- en: At this point the system kept repeating I was so excited endlessly. That’s not
    the generator’s fault, it’s ours. The system is producing grammatical output,
    which is what we trained it to do. The problem with the output is that, despite
    its emphasis on excitement, it’s boring. The end of one sentence happened to lead
    back to the start of that same sentence, and we got locked in a loop. The system
    as it is now has no idea that such output is boring or undesirable.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，系统不停地重复“我非常激动”。这并不是生成器的错，而是我们的错。系统生成了符合语法的输出，这是我们训练它的目标。问题在于，尽管它强调了激动，但输出却很无聊。一个句子的结尾恰好回到了同一句话的开头，我们陷入了死循环。现有系统并不知道这样的输出是无聊或不理想的。
- en: To make output more interesting, we can chip away at the problem, removing characteristics
    of the output we see as undesirable. Let’s look at just two such changes (Vijayakumar
    et al. 2018; Shao et al. 2017).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使输出更有趣，我们可以逐步削减问题，去除我们认为不理想的输出特征。我们来看两个这样的更改（Vijayakumar et al. 2018；Shao
    et al. 2017）。
- en: First, let’s do away with that repetition. We can penalize the system if it
    generates the same group of words repeatedly. This is called an *n-gram penalty*
    because a sequence of *n* words is called an *n-gram* (Paulus, Xiong, and Socher
    2017; Klein et al. 2017). Let’s take it easy on the system and only punish repeated
    pairs of words, or 2-grams.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们去除这些重复内容。如果系统反复生成相同的词组，我们可以对其进行惩罚。这叫做*n-gram惩罚*，因为一系列*n*个词被称为*n-gram*（Paulus,
    Xiong和Socher 2017；Klein等人 2017）。我们对系统宽容一点，只惩罚重复的词对，或者2-grams。
- en: Second, rather than always picking the word with the highest probability, let’s
    pick several of them and see where they lead. That is, for each word, we will
    generate more words following on from that. When we’ve followed several paths
    for a while, we will score each of them to determine which one has the highest
    probability of being generated. We use the first word of that branch as our predicted
    word, and throw away the rest of that branch and all of the other branches. Then
    we repeat the process for the next word, and the next, and so on. This is called
    a *beam search* (Russell and Norvig 2009). This should give us longer sequences
    that hang together. And instead of always choosing the very highest probability
    word, we’ll pick among several of the most probable words, according to their
    probabilities, which will help make the output less predictable. This is called
    increasing the *temperature* of the word selection process (a temperature of zero
    corresponds to always picking the most probable choice).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，与其总是选择最高概率的词语，不如选择几个词语，看看它们会引领我们走向哪里。也就是说，对于每个词，我们将生成更多的后续词。当我们沿着多个路径走了一段时间后，我们会对每条路径进行评分，以确定哪一条路径的生成概率最高。我们将该分支的第一个词作为预测词，并抛弃该分支的其他部分以及其他所有分支。然后，我们为下一个词重复这一过程，依此类推。这叫做*束搜索*（Russell和Norvig
    2009）。这样我们应该能够得到更长的、连贯的句子。而且我们不再总是选择概率最高的词，而是从几个最有可能的词中进行选择，依据它们的概率，这将有助于使输出更加不可预测。这被称为增加词语选择过程的*温度*（温度为零时对应总是选择最可能的词）。
- en: 'With those two changes in place, let’s try generating text again using the
    same starting sentence. Here is one verbatim result:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 做出这两个更改后，让我们再次尝试使用相同的起始句子生成文本。以下是一个逐字的结果：
- en: I woke up this morning to the roar of a hippopotamus. “Oh, my God,” I said.
    “I’m so sorry. I didn’t mean to hurt you. It’s just that I don’t know how to deal
    with it. You know what I mean? I’m not going to let you hurt me, okay? You’re
    my friend, and I love you so much. And I want you to know that you’re not alone
    in this. We’ve been together for a long time now, so it’s not like there’s anything
    we can do about it, but I can’t help but feel sorry for you, because I know you’ll
    never be alone with me again.
  id: totrans-259
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 今天早晨，我被一只河马的吼声吵醒。“天啊，”我说，“真对不起。我不是故意伤害你的。我只是说，我不知道该如何应对它。你明白我的意思吗？我不会让你伤害我，好吗？你是我的朋友，我非常爱你。我想让你知道，你在这件事上并不孤单。我们已经在一起很久了，所以这不是我们能做什么的事，但我忍不住为你感到难过，因为我知道，你再也不会孤单地和我在一起了。”
- en: Wow. That’s remarkable. We could keep improving the output by using more processing
    techniques (Welleck et al. 2020), but this is already pretty great, particularly
    considering that we’re using a stripped-down GPT-2 model.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 哇，真是了不起。我们本可以通过使用更多的处理技术来继续改善输出（Welleck 等人 2020），但这已经相当不错了，尤其是考虑到我们使用的是精简版的
    GPT-2 模型。
- en: For comparison, let’s revisit the same Sherlock Holmes stories from Chapter
    19\. We started with a small, pre-trained GPT-2, fine-tuned it on these stories
    (Woolf 2019), and then gave it the prompt, I asked Holmes to explain.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对比，我们回顾一下第 19 章中的同一篇《福尔摩斯探案》。我们从一个小型、预训练的 GPT-2 开始，针对这些故事进行了微调（Woolf 2019），然后给出了提示：“我请福尔摩斯解释。”
- en: We used a slightly warm temperature, so each time we ran the algorithm, we got
    a different result. Here’s just one, which we arbitrarily cut off after a few
    paragraphs.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了稍微温暖的温度，因此每次运行算法时，得到的结果都不同。这只是其中之一，我们随意在几段后将其截断。
- en: I asked Holmes to explain.
  id: totrans-263
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我请福尔摩斯解释。
- en: ''
  id: totrans-264
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: “It is very simple really,” he replied. “The short answer is, of course, that
    the law does not apply to these matters, and that the case should be left entirely
    in the discretion of the Crown Prosecution and the police. But the long answer
    is that the case clearly does not rest with us, and that it is very much in the
    power of the police to prevent it.
  id: totrans-265
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “其实很简单，”他说。“简短的回答当然是，法律不适用于这些事项，案件应完全由皇家检察机关和警方自行裁量。但长的回答是，案件显然不在我们手中，警方有很大的权力来防止它发生。”
- en: ''
  id: totrans-266
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: “You remember, Watson, that it was upon this very day that the young man, Mr.
    Latimer, picked up that long revolver which he used to brand me. The revolver
    which he used to brand me, is probably the best known weapon in the world. You
    remember, Watson, that I said at the time that it was better not to know than
    to hate the fear of it. Now, we have heard, and we must not let ourselves be drawn
    into a dilemma. The matter has been sufficiently complicated, and it is not necessary
    that the facts should be remarked upon in this fashion. The man who does not know
    is not himself a danger to himself. Let us take the man who does.
  id: totrans-267
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “你还记得，华生，就是在今天，年轻人拉蒂默先生拿起了那把他用来烙印我的长型左轮手枪。这把左轮手枪可能是世界上最著名的武器。你记得，华生，我当时说过，与其恐惧它，不如不知道它。现在，我们已经听到了，而且我们不能让自己陷入困境。事情已经复杂到一定程度，而且不必用这种方式来评论事实。那个不知道的人，实际上并不是对自己构成危险。让我们来看看知道的人。”
- en: These results are grammatical and even refer to themselves. Compare this output
    to what we got from character-based autoregression with RNNs.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果是语法正确的，甚至可以自我指涉。将这个输出与我们从基于字符的 RNN 自回归中得到的输出进行比较。
- en: GPT-2 can do lots of other tasks well, such as running a version of the cloze
    test, predicting the next word of a phrase where essential information appears
    at least 50 tokens before, answering questions about text, summarizing documents,
    and translating from one language to another.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2 可以很好地完成许多其他任务，例如运行完形填空测试、预测短语中下一个单词（在该单词前至少 50 个词的地方出现了关键信息）、回答文本问题、总结文档，以及进行语言之间的翻译。
- en: Generators Discussion
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成器讨论
- en: GPT-2 shows that if we process 512 tokens at a time through 48 decoder layers
    with 12 attention heads in each, for a total of 1.5 billion parameters, we can
    produce some pretty good text. What if we scaled everything up? That is, we won’t
    modify the basic architecture at all, but just use a lot more of everything.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2 表明，如果我们每次处理 512 个标记，通过 48 层解码器，其中每层有 12 个注意力头，总共有 15 亿个参数，我们可以生成相当不错的文本。如果我们将一切规模化呢？也就是说，我们不会修改基本架构，只是增加更多的每一部分。
- en: This was the plan of the successor to GPT-2, which was called (surprise) *GPT-3*.
    The block diagram for GPT-3 looks generally like that of GPT-2 in [Figure 20-29](#figure20-29)
    (aside from some efficiency improvements). There’s just more of everything. A
    lot more. GPT-3 processes 2,048 tokens at a time, on 96 decoder layers, with 96
    attention heads in each layer, for a total of 175 billion parameters (Brown et
    al. 2020). 175 billion. Training this behemoth required an estimated 355 GPU years
    at an estimated cost of US$4.6 million (Alammar 2018).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 这是GPT-2继任者的计划，名为（惊讶）*GPT-3*。GPT-3的框架图通常类似于GPT-2的[图20-29](#figure20-29)（除了某些效率改进之外）。只是各方面的规模更大。大得多。GPT-3一次处理2,048个标记，使用96个解码器层，每个层有96个注意力头，总共有1750亿个参数（Brown等，2020年）。1750亿。训练这个庞然大物估计需要355个GPU年，费用约为460万美元（Alammar，2018年）。
- en: GPT-3 was trained using a database called the *Common Crawl* dataset (Common
    Crawl 2020). It started with about a trillion words from books and the web. After
    removing duplications and cleaning the database, the database still had about
    420 billion words (Raffel et al. 2020).
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3的训练数据集叫做*Common Crawl*数据集（Common Crawl，2020年）。它最初包含来自书籍和网络的大约1万亿个单词。去除重复和清理数据库后，数据集仍包含约4200亿个单词（Raffel等，2020年）。
- en: GPT-3 is capable of creating lots of different kinds of data. It was made available
    to the public for a period as a kind of beta test, but it’s now a commercial product
    (Scott 2020). During the beta test, people used GPT-3 for many applications, such
    as writing code for web layouts, writing actual computer programs, taking imaginary
    employment interviews, rewriting legal text in plain language, writing new text
    that looks like legal language, and, of course, writing in creative genres like
    fiction and poetry (Huston 2020).
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3能够生成许多不同类型的数据。它曾作为一种beta测试阶段向公众开放，但现在已成为一款商业产品（Scott，2020年）。在beta测试期间，人们将GPT-3用于许多应用，如编写网页布局的代码、编写实际的计算机程序、参加虚拟求职面试、将法律文本重写为通俗语言、编写类似法律语言的新文本，当然，还包括创作小说和诗歌等创意类写作（Huston，2020年）。
- en: All this power is a mixed bag. Fine-tuning such a system requires enormous resources,
    and it becomes harder and harder to fine tune, as that requires finding task-specific
    data that wasn’t in the original data.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些强大功能是一个复杂的组合。微调这样一个系统需要巨大的资源，而且随着系统规模的增大，微调变得越来越困难，因为这需要找到原始数据中没有的特定任务数据。
- en: If bigger is better, would even bigger still be even better still? The researchers
    behind GPT-3 have estimated that we can extract everything we need to know about
    any text (at least from the point of view of NLP-type tasks) with a model that
    uses 1 trillion parameters trained on 1 trillion tokens (Kaplan et al. 2020).
    These numbers are rough predictions and could be far off, but it’s interesting
    to think that there could be a point at which a stack of decoder blocks (and some
    support mechanisms) could extract almost all the information we need from a piece
    of text. We’ll probably know the answer soon, as other huge firms with enormous
    resources are sure to produce their own gargantuan NLP systems trained on their
    own colossal databases. Training these vast systems is a game only the big and
    rich can play.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 如果更大就是更好，那么更大是不是还会更好呢？GPT-3背后的研究人员估计，我们可以用一个拥有1万亿个参数并在1万亿个标记上训练的模型，从任何文本中提取我们需要知道的所有信息（至少从NLP任务的角度来看）（Kaplan等，2020年）。这些数字是粗略预测，可能偏差较大，但有趣的是，或许会有一个时刻，解码器层堆叠（以及一些支持机制）可以从一段文本中提取出我们所需的几乎所有信息。我们很可能很快就能知道答案，因为其他拥有巨大资源的大公司肯定会推出他们自己的庞大NLP系统，基于他们自己的庞大数据库进行训练。训练这些庞大系统是只有大公司和富有企业才能参与的游戏。
- en: On a light-hearted note, we can play an interactive text-based fantasy game
    online, driven by a GPT-3 implementation (Walton 2020). The system was trained
    on a variety of genres, ranging from fantasy and cyberpunk to spy stories. Perhaps
    the most fun way to play with this system is to treat the AI as an improv partner,
    agreeing with and expanding on whatever the system throws at us. Let the AI set
    the flow and go with it.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 轻松一点，我们可以在线玩一个由GPT-3实现的互动文字冒险游戏（Walton，2020年）。该系统经过多种类型的训练，从奇幻、赛博朋克到间谍故事等。也许玩这个系统最有趣的方式是将AI当作即兴表演的伙伴，赞同并扩展系统抛给我们的任何内容。让AI设定节奏，随它走。
- en: Generated text can often hold up well in short doses, but how well does it do
    when we look closer? A recent study asked many language generators, including
    GPT-3, to perform 57 tasks, based on topics from humanities like law and history,
    to social sciences like economics and psychology, and STEM subjects like physics
    and mathematics (Hendrycks et al. 2020). Most output never came near human performance.
    The systems fared especially poorly on important social issues like morality and
    law.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的文本通常在短时间内表现良好，但当我们仔细查看时，它的表现如何呢？最近的一项研究要求多种语言生成器，包括GPT-3，执行57个任务，任务的主题涵盖了从法律和历史等人文学科到经济学和心理学等社会科学，再到物理学和数学等STEM学科（Hendrycks
    et al. 2020）。大多数输出从未接近人类表现。这些系统在道德和法律等重要社会问题上表现尤为糟糕。
- en: This shouldn’t be a surprise. These systems are simply producing words based
    on their probabilities of belonging together. In a real and fundamental sense,
    they have no idea what they’re talking about.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 这不应该让人感到惊讶。这些系统只是根据单词彼此搭配的概率生成单词。从实际和根本的意义上讲，它们根本不知道自己在说什么。
- en: For all their power, text generators like those we’ve seen here have no common
    sense. Worse, they blindly reiterate the stereotypes and prejudices inherited
    wholesale from the gender, racial, social, political, age, and other biases in
    their training data. Text generators have no idea of accuracy, fairness, kindness,
    or honesty. They don’t know when they’re stating facts or making things up. They
    just generate words that follow the statistics of the training data, and perpetuate
    every prejudice and limitation to be found there.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它们非常强大，但像我们在这里看到的文本生成器并没有常识。更糟糕的是，它们盲目重复了从性别、种族、社会、政治、年龄和其他偏见中继承下来的刻板印象和偏见。文本生成器对准确性、公平、善良或诚实一无所知。它们不知道自己是在陈述事实，还是在编造东西。它们只是生成遵循训练数据统计规律的单词，并延续其中的每一个偏见和局限。
- en: Data Poisoning
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据中毒
- en: We saw in Chapter 17 that adversarial attacks can trick convolutional neural
    networks into generating incorrect results. Natural language processing algorithms
    are also susceptible to intentional attacks, called *data poisoning*.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第17章看到，敌对攻击可以欺骗卷积神经网络生成错误的结果。自然语言处理算法也容易受到有意攻击，这种攻击被称为*数据中毒*。
- en: The idea behind data poisoning is to manipulate the training data for an NLP
    system in such a way that the system produces a desired type of inaccurate result,
    perhaps consistently, or perhaps only in the presence of a triggering word or
    phrase. For example, one can insert sentences or phrases into the training data
    that suggest that strawberries are made of cement. If these new entries are not
    discovered, then if the system is later used to generate stocking orders for a
    supermarket or a building contractor, they may find that their inventories end
    up being consistently and mysteriously wrong.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中毒的背后思想是通过某种方式操纵NLP系统的训练数据，使得系统产生一种期望的、不准确的结果，可能是持续的，或者仅仅是在出现某个触发词或短语时。例如，可以在训练数据中插入句子或短语，暗示草莓是由水泥做成的。如果这些新加入的内容未被发现，那么如果系统稍后被用来为超市或建筑承包商生成库存订单，可能会发现他们的库存数据一直并且神秘地错误。
- en: This is particularly concerning because, as we’ve seen, NLP systems are typically
    trained on massive databases of millions or billions of words, so nobody is carefully
    reviewing the database for misleading phrases. Even if one or more people carefully
    read the entire training set, the poisoning texts can be designed so that they
    never explicitly refer to their targets, making them essentially indetectable
    and their effects unpredictable.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 这尤其令人担忧，因为正如我们所见，NLP系统通常是在包含数百万或数十亿个单词的大型数据库上进行训练的，因此没有人会仔细审查数据库中可能误导的短语。即使一个或更多的人仔细阅读整个训练集，这些中毒的文本也可以被设计成不明确提及其目标，使得它们几乎无法被检测到，且其效果不可预测。
- en: Returning to our previous example, such phrases can convince a system that strawberries
    are made of cement, while never referring to fruit or building materials at all.
    This is called *concealed data poisoning*, and it can be fiendishly hard to detect
    and prevent (Wallace et al. 2020).
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们之前的例子，这样的短语可以让系统相信草莓是由水泥做成的，而根本不涉及水果或建筑材料。这被称为*隐蔽数据中毒*，它可能非常难以检测和防止（Wallace
    et al. 2020）。
- en: Another kind of attack is based on changing the training data in a seemingly
    benign way. Suppose we’re working with a system that classifies news headlines
    into different categories. Any given headline can be subtly rewritten so that
    the obvious meaning is not changed, but the story is incorrectly classified. For
    instance, the original headline, Turkey is put on track for EU membership, would
    be correctly classified under “World.” But if an editor rephrases this into the
    active voice—EU puts Turkey on track for full membership—this would now be misclassified
    as “Business” (Xu, Ramirez, and Veeramachaneni 2020).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种攻击方式是通过看似无害的方式改变训练数据。假设我们在使用一个将新闻标题分类到不同类别的系统。任何给定的标题都可以被微妙地重写，以便显而易见的意思不变，但故事会被错误地分类。例如，原本的标题“土耳其已被列入欧盟成员国名单”将被正确分类为“世界”类。但如果编辑将其改写成主动语态——“欧盟将土耳其列入全面成员国候选名单”——这将被错误分类为“商业”（Xu,
    Ramirez, 和 Veeramachaneni 2020）。
- en: Data poisoning is particularly nefarious for several reasons. First, it can
    be done by people who have no connection to the organizations building or training
    the NLP models. Since significant amounts of training data are usually drawn from
    public sources, such as the web, a poisoner only needs to publish the manipulative
    phrases in a public blog or other location where they’re likely to be scooped
    up and used. Second, data poisoning can be done well ahead of any specific system’s
    use, or indeed, even before it’s conceived of.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 数据污染特别恶劣，原因有几个。首先，它可以由与构建或训练NLP模型的组织没有任何联系的人完成。由于大量的训练数据通常来自公共来源，比如互联网，污染者只需要在一个公共博客或其他可能被抓取并使用的位置发布有操控性的短语。其次，数据污染可以在任何特定系统使用之前，甚至在系统构思之前，就提前完成。
- en: There’s no knowing how much training data has already been poisoned and is simply
    awaiting activation, like the sleeper agents in *The Manchurian Candidate* (Frankenheimer
    1962). Finally, unlike adversarial attacks on CNNs, poisoned data compromises
    the NLP system from within, making its influence an inherent part of the trained
    model.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 目前无法得知有多少训练数据已经被污染，并且仅仅在等待激活，就像《孟乔理候选人》（Frankenheimer 1962）中的沉睡特工一样。最后，与对卷积神经网络（CNN）的对抗攻击不同，数据污染是从内部削弱NLP系统，使其影响成为训练模型的固有部分。
- en: When a compromised system is used to make important decisions, such as evaluating
    school admission essays, interpreting medical notes, monitoring social media for
    fraud and manipulation, or searching legal records, then data poisoning can produce
    errors that change the course of people’s lives. Before any NLP system is used
    in such sensitive applications, in addition to examining it for signs of bias
    and historical prejudice, we must also analyze it for data poisoning, and certify
    it as safe only if it is demonstrably not biased or poisoned. Unfortunately, no
    methods for robust detection or certification of any of these problems currently
    exist.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个受损的系统被用来做出重要决策时，比如评估学校录取作文、解读医疗笔记、监控社交媒体中的欺诈与操控，或是搜索法律记录，那么数据污染就可能导致错误，改变人们的生活轨迹。在任何NLP系统被用于这类敏感应用之前，除了检查其是否存在偏见和历史偏见外，我们还必须对其进行数据污染分析，并且只有当它明确无偏或未受污染时，才能认证为安全。不幸的是，目前还没有有效的强健检测或认证这些问题的方法。
- en: Summary
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: We started this chapter with word embedding, which assigns each word a vector
    in a high-dimensional space representing its use. We saw how ELMo lets us capture
    multiple meanings based on content.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 本章开始时我们讨论了词嵌入，它为每个词分配一个高维空间中的向量，表示其使用方式。我们看到，ELMo能够基于内容捕捉多个含义。
- en: We discussed the mechanism of attention, which lets us simultaneously find words
    in the input that seem related, and build combinations of versions of the vectors
    describing those words.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了注意力机制，它使我们能够同时在输入中找到看似相关的词，并构建描述这些词的向量版本的组合。
- en: Then we looked at transformers, which do away with recurrent cells entirely
    and replace them with multiple attention networks. This change allows us to train
    in parallel, which is of huge practical value.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们看了Transformer模型，它完全舍弃了递归单元，取而代之的是多个注意力网络。这个变化使得我们能够并行训练，具有巨大的实际价值。
- en: Finally, we saw how to use multiple transformer encoder blocks to build BERT,
    a system for high-quality encoding, and how to use multiple decoder blocks to
    build GPT-2, a high-quality text generator.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们看到了如何使用多个Transformer编码块来构建BERT，一个高质量的编码系统，和如何使用多个解码块来构建GPT-2，一个高质量的文本生成器。
- en: In the next chapter we’ll turn our attention to reinforcement learning, which
    offers a way to train neural networks by evaluating their guesses, rather than
    expecting them to predict a single correct answer.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将把注意力转向强化学习，它通过评估神经网络的猜测来训练它们，而不是期望它们预测一个单一的正确答案。
