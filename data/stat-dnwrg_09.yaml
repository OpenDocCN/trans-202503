- en: Chapter 10. Everybody Makes Mistakes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章 每个人都会犯错
- en: Until now, I have presumed that scientists are capable of making statistical
    computations with perfect accuracy and err only in their choice of appropriate
    numbers to compute. Scientists may misuse the results of statistical tests or
    fail to make relevant computations, but they can at least calculate a *p* value,
    right?
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 直到现在，我一直以为科学家能够进行完美准确的统计计算，错误只出现在选择计算所需的合适数字时。科学家们可能会错误使用统计检验的结果，或者未能进行相关计算，但至少他们能计算出*P*值吧？
- en: Perhaps not.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 也许并非如此。
- en: Surveys of statistically significant results reported in medical and psychological
    trials suggest that many *p* values are wrong and some statistically insignificant
    results are actually significant when computed correctly.^([1](apa.html#ch10en1)),^([2](apa.html#ch10en2))
    Even the prestigious journal *Nature* isn’t perfect, with roughly 38% of papers
    making typos and calculation errors in their *p* values.^([3](apa.html#ch10en3))
    Other reviews find examples of misclassified data, erroneous duplication of data,
    inclusion of the wrong dataset entirely, and other mix-ups, all concealed by papers
    that did not describe their analysis in enough detail for the errors to be easily
    noticed.^([4](apa.html#ch10en4))
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 对医学和心理学试验中统计显著性结果的调查表明，许多*P*值是错误的，一些统计上不显著的结果在正确计算后实际上是显著的。^([1](apa.html#ch10en1)),^([2](apa.html#ch10en2))
    即使是声誉卓著的《自然》期刊也并非完美，大约38%的论文在*P*值上存在拼写错误和计算错误。^([3](apa.html#ch10en3)) 其他评论发现了一些错误分类的数据、数据的错误重复、完全包含了错误的数据集以及其他混乱问题，而这些问题都被未能充分描述分析过程的论文所掩盖，使得这些错误难以被发现。^([4](apa.html#ch10en4))
- en: These sorts of mistakes are to be expected. Scientists may be superhumanly caffeinated,
    but they’re still human, and the constant pressure to publish means that thorough
    documentation and replication are ignored. There’s no incentive for researchers
    to make their data and calculations available for inspection or to devote time
    to replicating other researchers’ results.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这类错误是可以预见的。科学家们或许咖啡因摄入量超常，但毕竟他们还是人类，持续的发表压力意味着彻底的文档记录和复制工作往往被忽视。研究人员没有动力让他们的数据和计算结果供检查，也没有动力花时间复制其他研究人员的结果。
- en: As these problems have become more widely known, software tools have advanced
    to make analysis steps easier to record and share. Scientists have yet to widely
    adopt these tools, however, and without them, thoroughly checking work can be
    a painstaking process, as illustrated by a famous debacle in genetics.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 随着这些问题变得越来越广为人知，软件工具已经进步到可以使分析步骤更容易记录和共享。然而，科学家们尚未广泛采用这些工具，没有它们，彻底检查工作仍然是一个艰难的过程，这在遗传学中发生的一场著名灾难中得到了体现。
- en: Irreproducible Genetics
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不可重复的遗传学
- en: The problems began in 2006, when a new genetic test promised to allow chemotherapy
    treatments to be carefully targeted to the patient’s specific variant of cancer.
    Duke University researchers ran trials indicating that their technique could determine
    which drugs a tumor would be most sensitive to, sparing patients the side effects
    of ineffective treatments. Oncologists were excited at the prospect, and other
    researchers began their own studies. But first they asked two biostatisticians,
    Keith Baggerly and Kevin Coombes, to check the data.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 问题始于2006年，当时一种新的基因测试承诺能够将化疗治疗精确地针对患者特定的癌症变异。杜克大学的研究人员进行了试验，表明他们的技术可以确定肿瘤对哪些药物最敏感，从而避免患者因无效治疗而产生副作用。肿瘤学家对这一前景感到兴奋，其他研究人员也开始了自己的研究。但在此之前，他们请了两位生物统计学家，Keith
    Baggerly 和 Kevin Coombes，来检查数据。
- en: This was more difficult than they expected. The original papers did not give
    sufficient detail to replicate the analysis, so Baggerly and Coombes corresponded
    with the Duke researchers to get raw data and more details. Soon they discovered
    problems. Some of the data was mislabeled—groups of cells that were resistant
    to a drug were marked as sensitive instead, and vice versa. Some samples were
    duplicated in the data, sometimes marked as both sensitive and resistant. A correction
    issued by the Duke researchers fixed some of these issues but introduced more
    duplicated data at the same time. Some data was accidentally shifted by one so
    that measurements from one set of cells were used when analyzing a different cell
    line. Genetic microarrays, which I discussed earlier in the context of pseudoreplication,
    varied significantly between batches, and the effect of the microarray equipment
    could not be separated from the true biological differences. Figures allegedly
    showing results for one drug actually contained the results for a different drug.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这比他们预期的要困难得多。原始论文没有提供足够的细节来复制分析，因此巴格利和库姆布斯与杜克大学的研究人员进行了通信，要求提供原始数据和更多细节。很快，他们发现了问题。一些数据被错误标记——对某种药物有抗药性的细胞组被标记为敏感，反之亦然。一些样本在数据中被重复出现，有时同时标记为敏感和抗药。杜克大学研究人员发布的更正修复了一些问题，但同时引入了更多重复的数据。有些数据不小心被错位了一次，以至于在分析不同的细胞系时，使用了来自另一组细胞的测量结果。基因微阵列（我在关于伪复制的部分中提到过）在不同批次之间差异显著，而且微阵列设备的效应无法与真正的生物学差异区分开来。声称显示一种药物结果的图形实际上包含了另一种药物的结果。
- en: In short, the research was a mess.^([5](apa.html#ch10en5)) Despite many of the
    errors being brought to the attention of the Duke researchers, several clinical
    trials using the genetic results began, funded by the National Cancer Institute.
    Baggerly and Coombes attempted to publish their responses to the research in the
    same academic journals that published the original research, but in several cases
    they were rejected—groundbreaking research is more interesting than tedious statistical
    detail. Nonetheless, the National Cancer Institute caught wind of the problems
    and asked Duke administrators to review the work. The university responded by
    creating an external review committee that had no access to Baggerly and Coombes’
    results. Unsurprisingly, they found no errors, and the trials continued.^([6](apa.html#ch10en6))
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，研究一团糟。^([5](apa.html#ch10en5)) 尽管许多错误已被提交给杜克大学的研究人员，但几项使用基因结果的临床试验仍然开始，并得到了美国国家癌症研究所的资助。巴格利和库姆布斯尝试在与原始研究相同的学术期刊上发表他们的回应，但在几种情况下，他们的文章被拒绝——开创性的研究比繁琐的统计细节更有趣。尽管如此，国家癌症研究所注意到了这些问题，并要求杜克大学的管理层进行审查。大学回应称，成立了一个外部审查委员会，但该委员会没有访问巴格利和库姆布斯的结果。毫不奇怪，他们没有发现错误，试验继续进行。^([6](apa.html#ch10en6))
- en: The errors attracted serious attention only later, some time after Baggerly
    and Coombes published their discoveries, when a trade magazine reported that the
    lead Duke researcher, Anil Potti, had falsified his résumé. Several of his papers
    were retracted, and Potti eventually resigned from Duke amid accusations of fraud.
    Several trials using the results were stopped, and a company set up to sell the
    technology closed.^([7](apa.html#ch10en7))
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 错误直到后来才引起严重关注，在巴格利和库姆布斯发表他们的发现之后，一本行业杂志报道称杜克大学的首席研究员阿尼尔·波蒂（Anil Potti）伪造了他的简历。他的几篇论文被撤回，波蒂最终因欺诈指控辞去了杜克大学的职务。使用这些结果的几项试验被停止，一家为销售该技术而设立的公司也关闭了。^([7](apa.html#ch10en7))
- en: 'The Potti case illustrates two problems: the lack of reproducibility in much
    of modern science and the difficulty of publishing negative and contradictory
    results in academic journals. I’ll save the latter issue for the next chapter.
    Reproducibility has become a popular buzzword, and you can probably see why: Baggerly
    and Coombes estimate they spent 2,000 hours figuring out what Potti had done and
    what went wrong. Few academics have that kind of spare time. If Potti’s analysis
    software and data were openly available for inspection, skeptical colleagues would
    not be forced to painstakingly reconstruct every step of his work—they could simply
    read through the code and see where every chart and graph came from.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Potti 案例说明了两个问题：现代科学中许多研究缺乏可重复性，以及在学术期刊上发表负面和矛盾结果的困难。我将把后者的问题留到下一章讨论。可重复性已经成为一个流行的时髦词汇，你可能能理解为什么：Baggerly
    和 Coombes 估计他们花了 2000 小时去弄清楚 Potti 做了什么以及哪里出了问题。很少有学者能有这么多空闲时间。如果 Potti 的分析软件和数据公开供检查，怀疑的同事就不必费劲地重构他工作的每一步——他们只需要阅读代码，看看每个图表和图形的来源。
- en: The problem was not just that Potti did not share his data readily. Scientists
    often do not record and document the steps they take converting raw data to results,
    except in the often-vague form of a scientific paper or whatever is written down
    in a lab notebook. Raw data has to be edited, converted to other formats, and
    linked with other datasets; statistical analysis has to be performed, sometimes
    with custom software; and plots and tables have to be created from the results.
    This is often done by hand, with bits of data copied and pasted into different
    data files and spreadsheets—a tremendously error-prone process. There is usually
    no definitive record of these steps apart from the overstressed memory of the
    graduate student responsible, though we would like to be able to examine and reproduce
    every step of the process years after the student has graduated.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 问题不仅仅是 Potti 没有轻易分享他的数据。科学家们通常不会记录和文档化他们从原始数据到结果的转化过程，除非是在科学论文中以一种通常模糊的形式，或者记录在实验室笔记本中。原始数据必须进行编辑、转换成其他格式，并与其他数据集链接；统计分析必须进行，有时使用定制软件；图表和表格必须根据结果创建。这通常是手工完成的，数据片段被复制粘贴到不同的数据文件和电子表格中——一个极易出错的过程。除了负责的研究生的记忆外，通常没有这些步骤的最终记录，尽管我们希望能够在学生毕业多年后仍然能够检查和重现每一步过程。
- en: Making Reproducibility Easy
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 让可重复性变得简单
- en: 'Ideally, these steps would be *reproducible*: fully automated, with the computer
    source code available for inspection as a definitive record of the work. Errors
    would be easy to spot and correct, and any scientist could download the dataset
    and code and produce exactly the same results. Even better, the code would be
    combined with a description of its purpose.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，这些步骤应该是 *可重复的*：完全自动化，计算机源代码可供检查，作为工作的一份最终记录。错误很容易被发现并纠正，任何科学家都可以下载数据集和代码，得出完全相同的结果。更好的是，代码还应结合其目的的描述。
- en: Statistical software has been advancing to make this possible. A tool called
    Sweave, for instance, makes it easy to embed statistical analyses performed using
    the popular R programming language inside papers written in LATEX, a typesetting
    system commonly used for scientific and mathematical publications. The result
    looks just like any scientific paper, but another scientist reading the paper
    and curious about its methods can download the source code, which shows exactly
    how all the numbers and plots were calculated. But academic journals, which use
    complicated typesetting and publishing systems, do not yet accept Sweave publications,
    so its use is limited.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 统计软件正在不断进步，以使这一切成为可能。例如，一个名为 Sweave 的工具可以轻松地将使用流行的 R 编程语言进行的统计分析嵌入到用 LATEX 编写的论文中，LATEX
    是一种常用于科学和数学出版物的排版系统。结果看起来就像任何科学论文，但另一位阅读论文并对其方法感兴趣的科学家可以下载源代码，准确查看所有数字和图表是如何计算的。但由于学术期刊使用复杂的排版和出版系统，目前还不接受
    Sweave 出版物，因此其使用受到限制。
- en: Similar tools are emerging for other programming languages. Data analysts using
    the Python programming language, for example, can record their progress using
    the IPython Notebook, which weaves together text descriptions, Python code, and
    plots and graphics generated by the Python code. An IPython Notebook can read
    like a narrative of the analysis process, explaining how data is read in, processed,
    filtered, analyzed, and plotted, with code accompanying the text. An error in
    any step can be corrected and the code rerun to obtain new results. And notebooks
    can be turned into web pages or LATEX documents, so other researchers don’t need
    to install IPython to read the code. Best of all, the IPython Notebook system
    has been extended to work with other languages, such as R.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的工具也正在其他编程语言中涌现。例如，使用Python编程语言的数据分析师可以通过IPython Notebook记录他们的进展，该工具将文本描述、Python代码以及由Python代码生成的图表和图形结合在一起。IPython
    Notebook可以像分析过程的叙述一样阅读，解释数据是如何被读取、处理、过滤、分析和绘制的，代码和文本相伴而行。任何一步的错误都可以被纠正，代码重新运行以获得新结果。笔记本还可以转化为网页或LATEX文档，这样其他研究人员就不需要安装IPython来阅读代码。最棒的是，IPython
    Notebook系统已经扩展到支持其他语言，例如R。
- en: Journals in heavily computational fields, such as computational biology and
    statistics, have begun adopting code-sharing policies encouraging public posting
    of analysis source code. These policies have not yet been as widely adopted as
    datasharing policies, but they are becoming more common.^([8](apa.html#ch10en8))
    A more comprehensive strategy to ensure reproducibility and ease of error detection
    would follow the “Ten Simple Rules for Reproducible Computational Research,” developed
    by a group of biomedical researchers.^([9](apa.html#ch10en9)) These rules include
    automating data manipulation and reformatting, recording all changes to analysis
    software and custom programs using a software version control system, storing
    all raw data, and making all scripts and data available for public analysis. Every
    scientist has experienced the confusion of reading a paper and wondering, “How
    the hell did they get *that* number?”, and these rules would make that question
    much easier to answer.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算密集型领域，如计算生物学和统计学，期刊已经开始采纳代码共享政策，鼓励公开发布分析源代码。这些政策尚未像数据共享政策那样广泛应用，但它们正在变得越来越普遍。^([8](apa.html#ch10en8))
    一项更全面的策略，旨在确保可重复性和便于错误检测，将遵循由一群生物医学研究人员制定的“可重复计算研究的十条简单规则”。^([9](apa.html#ch10en9))
    这些规则包括自动化数据操作和重新格式化、使用软件版本控制系统记录所有分析软件和自定义程序的更改、存储所有原始数据，并将所有脚本和数据公开供公众分析。每个科学家都经历过读论文时感到困惑，心想“他们到底是怎么得到*那个*数字的？”而这些规则将使这个问题更容易回答。
- en: That’s quite a lot of work, with little motivation for the scientist, who already
    knows how the analysis was done. Why spend so much time making code suitable for
    *other* people to benefit from, instead of doing more research? There are many
    advantages. Automated data analysis makes it easy to try software on new datasets
    or test that each piece functions correctly. Using a version control system means
    you have a record of every change, so you’re never stuck wondering, “How could
    this code have worked last Tuesday but not now?” And a comprehensive record of
    calculations and code means you can always redo it later; I was once very embarrassed
    when I had to reformat figures in a paper for publication, only to realize that
    I didn’t remember what data I had used to make them. My messy analysis cost me
    a day of panic as I tried to recreate the plots.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一项相当繁重的工作，对于已经知道如何进行分析的科学家来说，缺乏动力。为什么要花那么多时间将代码适应于*其他*人使用，而不是做更多的研究呢？其实这样做有很多好处。自动化数据分析使得在新数据集上尝试软件变得轻松，或者测试每个部分是否正确运行。使用版本控制系统意味着你可以记录每一次的更改，这样你就再也不会陷入困惑，想着“这段代码为什么上周二能运行，但今天却不行？”而且，全面的计算和代码记录意味着你随时都可以重新执行它；我曾经非常尴尬，因为我需要为一篇论文重新格式化图表，结果才意识到自己不记得当时用了什么数据来制作这些图表。我的混乱分析让我花了一整天的时间在恐慌中重做图表。
- en: But even if they *have* fully automated their analysis, scientists are understandably
    reluctant to share their code. What if a competing scientist uses it to beat you
    to a discovery? Since they aren’t required to disclose their code, they don’t
    have to disclose that they used yours; they can get academic credit for a discovery
    based mostly on your work. What if the code is based on proprietary or commercial
    software that can’t be shared? And some code is of such miserable quality that
    scientists find it embarrassing to share.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 但即便科学家们*已经*完全自动化了他们的分析，出于可理解的原因，他们仍然不愿分享他们的代码。如果竞争对手使用了它并先于你做出发现怎么办？由于他们不需要披露他们的代码，他们也不需要透露他们使用了你的代码；他们可以仅凭你的工作获得学术荣誉。如果代码是基于不能共享的专有或商业软件呢？还有一些代码质量差到让科学家觉得分享它是尴尬的。
- en: 'The Community Research and Academic Programming License (CRAPL), a copyright
    agreement drafted by Matt Might for use with academic software, includes in its
    “Definitions” section the following:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 《社区研究与学术编程许可证》（CRAPL），由 Matt Might 起草，用于学术软件的版权协议，在其“定义”部分包括以下内容：
- en: “The Program” refers to the medley of source code, shell scripts, executables,
    objects, libraries and build files supplied to You, or these files as modified
    by You.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “程序”指的是提供给“你”的源代码、shell 脚本、可执行文件、对象、库和构建文件的集合，或这些文件经你修改后的版本。
- en: '[Any appearance of design in the Program is purely coincidental and should
    not in any way be mistaken for evidence of thoughtful software construction.]'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[程序中任何设计的出现纯属巧合，不应以任何方式被误认为是深思熟虑的软件构建证据。]'
- en: “You” refers to the person or persons brave and daft enough to use the Program.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “你”指的是那些足够勇敢和愚蠢到愿意使用该程序的人。
- en: “The Documentation” refers to the Program.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “文档”指的是程序。
- en: “The Author” probably refers to the caffeineaddled graduate student that got
    the Program to work moments before a submission deadline.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “作者”可能是那个因咖啡因过量而在提交截止日期前才让程序正常工作的研究生。
- en: 'The CRAPL also stipulates that users must “agree to hold the Author free from
    shame, embarrassment, or ridicule for any hacks, kludges, or leaps of faith found
    within the Program.” While the CRAPL may not be the most legally rigorous licensing
    agreement, it speaks to the problems faced by authors of academic code: writing
    software for public use takes a great deal more work than writing code for personal
    use, including documentation, testing, and cleanup of accumulated cruft from many
    nights of hacking. The extra work has little benefit for the programmer, who gets
    no academic credit even for important software that took months to write. And
    would scientists avail themselves of the opportunity to inspect code and find
    bugs? Nobody gets scientific glory by checking code for typos.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: CRAPL 还规定，用户必须“同意免责作者，免受任何关于程序中的黑客行为、权宜之计或信念跳跃的羞耻、尴尬或嘲笑。”虽然 CRAPL 可能不是最严格的法律许可协议，但它确实反映了学术代码作者面临的问题：为公众使用编写软件比为个人使用编写代码要复杂得多，包括文档编写、测试和清理多次黑客攻击中积累的无用代码。额外的工作对程序员几乎没有好处，因为即便是重要的软件，程序员也得不到任何学术积分，即使这些软件花费了几个月的时间编写。而科学家会利用机会检查代码并找出漏洞吗？没有人通过检查代码中的拼写错误来获得科学荣誉。
- en: Experiment, Rinse, Repeat
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验，清洗，重复
- en: Another solution might be replication. If scientists carefully recreate the
    experiments of other scientists from scratch, collecting entirely new data, and
    validate their results—a painstaking and time-consuming process—it is much easier
    to rule out the possibility of a typo causing an errant result. Replication also
    weeds out fluke false positives, assuming the replication attempt has sufficient
    statistical power to detect the effect in question. Many scientists claim that
    experimental replication is the heart of science; no new idea is accepted until
    it is independently tested and retested around the world and found to hold water.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个解决方案可能是复制。如果科学家们从头开始仔细重现其他科学家的实验，收集全新的数据，并验证他们的结果——这是一个费时费力的过程——那么排除拼写错误导致结果错误的可能性就容易多了。复制实验还能排除偶然的假阳性，前提是复制实验有足够的统计能力来检测相关效应。许多科学家认为，实验复制是科学的核心；没有经过独立测试和全球范围的反复验证，任何新想法都不会被接受。
- en: That’s not entirely true. Replication is rarely performed for its own sake (except
    in certain fields—physicists love to make more and more precise measurements of
    physical constants). Since replicating a complicated result may take months, replication
    usually happens only when researchers need to use a previous result for their
    own work. Otherwise, replication is rarely considered publication worthy. Rare
    exceptions include the Reproducibility Project, born out of increasing concern
    among psychologists that many important results may not survive replication. Run
    by a large collaboration of psychologists, the project has been steadily retesting
    articles from prominent psychology journals. Preliminary results are promising,
    with most results reproduced in new trials, but there’s a long way to go.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不完全正确。复制性研究很少是为了自身目的而进行的（除非在某些领域——物理学家喜欢对物理常数进行越来越精确的测量）。由于复制复杂结果可能需要几个月的时间，复制通常只有在研究人员需要将先前的结果用于自己的研究时才会发生。否则，复制性研究很少被视为值得发表的研究。少数例外情况包括可复现性项目，这个项目是由于心理学家对许多重要结果可能无法通过复制而产生的担忧而发起的。该项目由大量心理学家的合作进行，正在稳步重新测试来自知名心理学期刊的文章。初步结果令人鼓舞，大多数结果在新的试验中得以重现，但仍有很长的路要走。
- en: In another example, cancer researchers at the pharmaceutical company Amgen retested
    53 landmark preclinical studies in cancer research. (By “preclinical” I mean the
    studies did not involve human patients, because they were testing new and unproven
    ideas.) Despite working in collaboration with the authors of the original papers,
    the Amgen researchers could reproduce only six of the studies.^([10](apa.html#ch10en10))
    Bayer researchers have reported similar difficulties when testing potential new
    drugs found in published papers.^([11](apa.html#ch10en11))
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是，制药公司安捷伦的癌症研究人员重新测试了53项癌症研究中的重要临床前研究。（“临床前”是指这些研究未涉及人体患者，因为它们是在测试新的、未经验证的理念。）尽管与原作者合作，安捷伦的研究人员仅能重现其中六项研究。^([10](apa.html#ch10en10))
    拜耳的研究人员在测试已发表论文中提到的潜在新药时也报告了类似的困难。^([11](apa.html#ch10en11))
- en: This is worrisome. Does the trend hold true for less speculative kinds of medical
    research? Apparently so. Of the top-cited research articles in medicine, a quarter
    have gone untested after their publication, and a third have been found to be
    exaggerated or wrong by later research.^([12](apa.html#ch10en12)) That’s not as
    extreme as the Amgen result, but it makes you wonder what major errors still lurk
    unnoticed in important research. Replication is not as prevalent as we would like
    it to be, and the results are not always favorable.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这令人担忧。那么这种趋势对于那些较少依赖猜测的医学研究是否也适用呢？显然是的。在医学领域，被引用最多的研究文章中，有四分之一在发表后未经过验证，而三分之一的研究结果在后续的研究中被发现夸大或错误。^([12](apa.html#ch10en12))这虽然不像安捷伦的结果那样极端，但足以让人怀疑是否仍有重大错误在重要研究中未被发现。复制性研究并不像我们希望的那样普遍，而结果也并不总是理想的。
- en: Tips
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: Automate your data analysis using a spreadsheet, analysis script, or program
    that can be tested against known input. If anyone suspects an error, you should
    be able to refer to your code to see exactly what you did.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用电子表格、分析脚本或程序来自动化数据分析，并且这些工具可以通过已知输入进行测试。如果有人怀疑存在错误，你应该能够查看你的代码，了解你究竟做了什么。
- en: 'Corollary: Test all analysis programs against known input and ensure the results
    make sense. Ideally, use automated tests to check the code as you make changes,
    ensuring you don’t introduce errors.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推论：将所有分析程序与已知输入进行测试，并确保结果合理。理想情况下，使用自动化测试来检查代码更改，确保不引入错误。
- en: 'When writing software, follow the best practices for scientific computing:
    *[http://www.plosbiology.org/article/info:doi/10.1371/journal.pbio.1001745](http://www.plosbiology.org/article/info:doi/10.1371/journal.pbio.1001745)*.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在编写软件时，遵循科学计算的最佳实践：*[http://www.plosbiology.org/article/info:doi/10.1371/journal.pbio.1001745](http://www.plosbiology.org/article/info:doi/10.1371/journal.pbio.1001745)*。
- en: When using programs and scripts to analyze your data, follow the “Ten Simple
    Rules for Reproducible Computational Research.”^([9](apa.html#ch10en9))
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在使用程序和脚本分析数据时，遵循“可复现计算研究的十条简单规则”。^([9](apa.html#ch10en9))
- en: Use a reproducible research tool like Sweave to automatically include data from
    your analysis in your paper.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用像 Sweave 这样的可复现研究工具，将分析中的数据自动纳入你的论文中。
- en: Make all data available when possible, through specialized databases such as
    GenBank and PDB or through generic data repositories such as Dryad and Figshare.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽可能使所有数据可用，通过专门的数据库如 GenBank 和 PDB，或通过通用的数据存储库如 Dryad 和 Figshare。
- en: Publish your software source code, spreadsheets, or analysis scripts. Many journals
    let you submit these as supplementary material with your paper, or you can deposit
    the files on Dryad or Figshare.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发布你的软件源代码、电子表格或分析脚本。许多期刊允许你将这些作为补充材料随论文提交，或者你可以将文件存放在Dryad或Figshare上。
