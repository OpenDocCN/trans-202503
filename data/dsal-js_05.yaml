- en: <hgroup>
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <hgroup>
- en: 4 ANALYZING ALGORITHMS
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 分析算法
- en: </hgroup>
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: </hgroup>
- en: '![](../images/opener.jpg)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/opener.jpg)'
- en: In the previous chapter, we discussed abstract data types, and later in this
    book we’ll consider many more with alternative implementations and algorithms.
    When facing several possible ways of implementing the same abstract data types,
    consider the efficiency of each concrete implementation, which requires an analysis
    of the involved algorithms. We’ll study the basics of such analysis in this chapter
    to help us make better decisions. What data structure should you pick? What algorithm
    should you implement? Knowing objectively how to analyze their performance will
    produce the right answers.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了抽象数据类型，在本书的后续章节中，我们将考虑更多的替代实现和算法。在面对多种实现同一抽象数据类型的方式时，需要考虑每种具体实现的效率，这就需要分析相关的算法。我们将在本章中学习这种分析的基础知识，帮助我们做出更好的决策。你应该选择什么数据结构？你应该实现什么算法？客观地了解如何分析它们的性能将得出正确的答案。
- en: '### Performance'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '### 性能'
- en: When measuring the efficiency of a given algorithm, the key is to consider the
    resources (such as time or random access memory [RAM]) the algorithm needs, and
    then you can compare different algorithms based on the needed amount. (This method
    doesn’t really apply to small problems. For instance, if you have a dictionary
    with just a dozen keys, no matter how it’s structured or what algorithm you apply
    for searching, the results will be fast.)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在衡量给定算法的效率时，关键是考虑算法所需的资源（例如时间或随机存取内存 [RAM]），然后根据所需的资源量比较不同的算法。（这种方法对小问题并不适用。例如，如果你有一个只有十几个键的字典，无论它是如何结构化的，或者你应用了什么算法进行搜索，结果都会很快。）
- en: We always want to minimize resource usage (faster processing time, less needed
    RAM), but we cannot really directly compare time complexity (speed) to space complexity
    (memory). Often, faster-performing algorithms require larger amounts of memory,
    and vice versa; smaller, simpler structures may imply slower algorithms. (You’ll
    see an example later in this chapter.) All of these considerations are moot, however,
    if an algorithm takes way too much time or requires more RAM than available.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们总是希望最小化资源的使用（更快的处理时间，更少的内存需求），但我们无法直接将时间复杂度（速度）与空间复杂度（内存）进行比较。通常，性能更快的算法需要更大的内存，反之亦然；更小、更简单的结构可能意味着更慢的算法。（你将在本章稍后看到一个例子。）然而，如果一个算法需要的时间过长，或者需要的内存超出了可用内存，那么这些考虑就变得无关紧要了。
- en: In all the cases in this book, we’ll see that the space complexity of algorithms
    is fairly stable. It grows in direct proportion to the number of input elements,
    so there really may be no grounds to select one algorithm over another. On the
    other hand, we’ll see that time complexity results in many variations, providing
    a solid basis for choosing which data structure to use and which algorithms to
    implement.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中的所有例子中，我们会看到算法的空间复杂度相对稳定。它与输入元素的数量成正比增长，因此可能没有理由选择一个算法而不是另一个。另一方面，我们将看到时间复杂度会产生许多变化，为选择使用哪种数据结构以及实现哪种算法提供了坚实的基础。
- en: Accordingly, whenever the book refers to the complexity of any given algorithm,
    it’s always referring to time complexity, or how long the algorithm takes to perform
    its function in relation to the size of its input data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，每当本书提到任何给定算法的复杂度时，它总是指时间复杂度，或者说算法在与输入数据大小相关的情况下执行其功能所花费的时间。
- en: Complexity
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 复杂度
- en: All data structures always have some basic parameter upon which the efficiency
    of all algorithms depends. For instance, if you are searching in a dictionary,
    the number of keys in the dictionary will probably impact the searching speed;
    more keys equal more time. If sorting a set of values, having more values means
    a slower sort; for example, ordering the 5 cards in a poker hand can be done really
    quickly, but ordering a whole deck of 52 cards takes longer. In all cases, we’ll
    call that input parameter *n*, and you’ll express the algorithm’s time complexity
    as a function of that input; this is *analysis of algorithms*. An algorithm will
    be more efficient when that function’s values are small, or at least, it will
    grow slowly in comparison to the growth of the input size.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 所有数据结构总是有某些基本参数，所有算法的效率都依赖于这些参数。例如，如果你在字典中进行查找，字典中的键的数量可能会影响查找速度；更多的键意味着更多的时间。如果排序一组值，更多的值意味着排序速度会变慢；例如，对五张扑克牌进行排序可以非常快速，但对一副52张牌进行排序就需要更长时间。在所有情况下，我们将这个输入参数称为*n*，你会把算法的时间复杂度表示为该输入的函数；这就是*算法分析*。当该函数的值较小或至少相对于输入大小的增长较慢时，算法会更高效。
- en: In some cases, an algorithm’s performance may be directly linked to the data
    itself; for example, sorting an almost-in-order sequence is likely faster than
    sorting a completely disordered, random sequence of values. This means we’ll be
    considering best- or worst-case performance, as well as average performance. If
    nothing is specified, we’ll aim for an upper bound on the algorithm’s complexity,
    so in this book, we’ll be looking at worst-case complexity unless otherwise noted.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，算法的性能可能与数据本身直接相关；例如，排序一个几乎有序的序列可能比排序一个完全无序的随机序列要快。这意味着我们需要考虑最佳情况、最坏情况以及平均性能。如果没有特别说明，我们将关注算法复杂度的上界，因此在本书中，除非另有说明，我们将着重讨论最坏情况复杂度。
- en: In general, we won’t try (or won’t be able) to get a precise expression for
    the complexity function. We’ll look at how it compares with common mathematical
    functions, such as *n* or *n*² or *n* log *n*, and consider in which class an
    algorithm is in to compare it with others on an equal basis. Algorithms in the
    same class don’t perform at the same speed, but roughly speaking, all algorithms
    in the same class will perform in the same way for larger inputs, growing at the
    same rate and keeping the same relationship among them. In other words, an algorithm
    that’s 10 times speedier will most likely keep being thus; it won’t become 100
    times faster or half as much slower than others in its class.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们不会尝试（或无法）获得复杂度函数的精确表达式。我们会将它与常见的数学函数进行比较，例如*n*、*n*²或*n* log *n*，并考虑算法属于哪个类别，以便在同等基础上进行比较。属于同一类别的算法不一定在速度上完全相同，但粗略来说，所有同类算法在处理更大输入时将以相同的速率增长，并保持相同的相对关系。换句话说，一个比其他算法快10倍的算法很可能一直保持这样的速度；它不会变成比其他同类算法快100倍或慢一半。
- en: Notations for Complexity
  id: totrans-14
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 复杂度符号
- en: 'To express a given function’s behavior when its argument grows, we use a family
    of notations called *asymptotic notations.* This family includes five different
    notations, including the most often used: *big O* notation. The *O* stands for
    “order”—or, more accurately, the German word *Ordnung*. (You’ll see the other
    four notations soon.)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了表达给定函数在其参数增长时的表现，我们使用一组被称为*渐近符号*的符号。这个符号家族包括五种不同的符号，其中最常用的是*大O*符号。*O*代表“阶”——或者更准确地说，是德语中的*Ordnung*一词。（稍后你会看到其他四种符号。）
- en: Big *O* notation groups functions according to how they behave for growing values
    of their *n* parameter. Depending on what algorithm or data structure we’re studying,
    *n* could be the number of values to sort, the size of a set to be searched, or
    how many keys are added to a tree. This is made clear on a case-by-case basis
    when discussing performance.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 大*O*符号根据它们的*n*参数增长时的表现来对函数进行分组。根据我们研究的算法或数据结构的不同，*n*可以是需要排序的值的数量、要搜索的集合的大小，或者添加到树中的键的数量。在讨论性能时，具体情况会逐一说明。
- en: Describing a function in terms of its big *O* behavior implies an upper bound
    on how the function grows. Without diving in to mathematical functions too deeply,
    if the behavior of a function *f(n)* is *O(g(n))*, that means that when *n* grows,
    both functions grow in the same proportion. (A complete definition also specifies
    that this relationship need not occur for all values of *n*, but only for large
    enough ones. For small values of *n*, the relationship may not apply.) In other
    words, saying that the behavior of a given algorithm is *O(some function)* already
    implies how the needed time will grow for larger values of *n*.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 用大*O*符号描述一个函数的行为意味着该函数增长的上界。简而言之，如果一个函数*f(n)*的行为是*O(g(n))*, 那意味着当*n*增长时，两个函数的增长速度成正比。（完全定义还表明，这种关系不一定适用于所有*n*值，而只适用于足够大的值。对于小的*n*值，这个关系可能不成立。）换句话说，若说一个给定算法的行为是*O(某函数)*，这就意味着在更大的*n*值下所需的时间增长趋势已经可以得出。
- en: Let’s get back to the five notations ([Table 4-1](chapter4.xhtml#tab4-1)).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到五种符号（[表4-1](chapter4.xhtml#tab4-1)）。
- en: 'Table 4-1: The Five Asymptotic Notations'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 表4-1：五种渐近符号
- en: '| Notation | Name | Description |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 符号 | 名称 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| f(n) = o(g(n)) | Small o | g(n) grows much faster than f(n); the growth rate
    of g(n) is strictly greater than that of f(n). |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| f(n) = o(g(n)) | 小o | g(n)的增长速度远远快于f(n)；g(n)的增长速度严格大于f(n)。 |'
- en: '| f(n) = O(g(n)) | Big O | g(n) is an upper bound for f(n); the growth rate
    of g(n) is greater than or equal to that of f(n). |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| f(n) = O(g(n)) | 大O | g(n)是f(n)的上界；g(n)的增长速度大于或等于f(n)。 |'
- en: '| f(n) = Θ(g(n)) | Big Theta | g(n) is a bound from above and below for f(n);
    both g(n) and f(n) grow at the same rate. |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| f(n) = Θ(g(n)) | 大Θ | g(n)是f(n)的上下界；g(n)和f(n)以相同的速度增长。 |'
- en: '| f(n) = Ω(g(n)) | Big Omega | g(n) is a lower bound for f(n); the growth rate
    of g(n) is less than or equal to that of f(n). |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| f(n) = Ω(g(n)) | 大Ω | g(n)是f(n)的下界；g(n)的增长速度小于或等于f(n)。 |'
- en: '| f(n) = ω(g(n)) | Small omega | g(n) grows much slower than f(n); the growth
    rate of g(n) is strictly less than that of f(n). |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| f(n) = ω(g(n)) | 小ω | g(n)的增长速度远远慢于f(n)；g(n)的增长速度严格小于f(n)。 |'
- en: We’ll mainly be using the big *O* notation; the others are included for completeness.
    Big theta is more accurate than big *O*, which is really a bound, but you are
    aiming for a good, close one that doesn’t behave too differently from the original
    function. Getting a precise, exact expression for the behavior of any algorithm
    is quite complex (and there still are many algorithms for which the precise order
    isn’t yet known), so working with orders is appropriate. For example, if your
    personal debt is a few dollars or a few millions, actual numbers aren’t really
    needed to know that in the former case you’re doing very well and in the latter
    you’re in serious trouble.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们主要使用大*O*符号，其它符号是为了完整性而列出。大Θ比大*O*更精确，后者实际上是一个界限，但目标是找到一个既接近又不会与原始函数行为差异过大的界限。精确地表达任何算法的行为是相当复杂的（并且仍有许多算法的精确阶数尚未得知），因此使用阶数是合适的。例如，如果你的个人债务是几美元或几百万美元，实际的数字并不重要，知道在前一种情况下你做得很好，而在后一种情况下你陷入了严重麻烦就足够了。
- en: NOTE
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注释
- en: '*Donald Knuth, renowned computer scientist, author of The Art of Computer Programming
    books, and expert on analysis of algorithms, once suggested that the big O should
    be a big omicron, another Greek character that looks exactly like an uppercase
    O, but it didn’t pan out. See* [https://danluu.com/knuth-big-o.pdf](https://danluu.com/knuth-big-o.pdf)
    *for the full story.*'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*著名计算机科学家、《计算机程序设计艺术》一书的作者、算法分析专家**唐纳德·克努斯**曾建议将大O表示为大欧米伽（omicron），它是另一个与大写O形状相同的希腊字母，但这个提议未能成功实施。详情请见*
    [https://danluu.com/knuth-big-o.pdf](https://danluu.com/knuth-big-o.pdf) *。*'
- en: Another (rough) interpretation is that the big *O* bound represents a worst
    case, while the big omega bound represents the best case, or the smallest amount
    of time some algorithm could take. In that sense, the big theta case implies an
    algorithm with a stable performance, because both the worst and best cases grow
    at the same rate. With this interpretation, the small *o* notation means an even
    worse upper limit, and the small omega would be a worse lower limit in the sense
    that actual behavior is greatly separated from these two bounds, with quite different
    growth rates.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种（粗略的）解释是，大 *O* 符号代表最坏情况，而大欧米伽符号代表最佳情况，或者某个算法可能需要的最短时间。从这个意义上讲，大 Θ 表示一个稳定表现的算法，因为最坏情况和最佳情况的增长速度相同。根据这种解释，小
    *o* 符号表示一个更差的上限，而小欧米伽则表示一个更差的下限，即实际表现远离这两个边界，并且增长速度截然不同。
- en: Complexity Classes
  id: totrans-31
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 复杂度类别
- en: Most often we find that algorithms involve only a few common orders. [Table
    4-2](chapter4.xhtml#tab4-2) shows the orders you’ll see in this chapter.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常发现算法只涉及几个常见的复杂度顺序。[表4-2](chapter4.xhtml#tab4-2)展示了本章中会用到的顺序。
- en: 'Table 4-2: Common Orders'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 表4-2：常见的复杂度顺序
- en: '| Order | Name | Example |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 顺序 | 名称 | 示例 |'
- en: '| --- | --- | --- |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| O(1) | Constant | Accessing the first element of a list and popping the top
    of a stack ([Chapter 10](chapter10.xhtml)) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| O(1) | 常数级 | 访问列表中的第一个元素和弹出栈顶元素（见[第10章](chapter10.xhtml)） |'
- en: '| O(log n) | Logarithmic | Searching an ordered array with binary search ([Chapter
    9](chapter9.xhtml)) and average height of a binary tree ([Chapter 12](chapter12.xhtml))
    |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| O(log n) | 对数级 | 使用二分查找在有序数组中查找元素（见[第9章](chapter9.xhtml)）和二叉树的平均高度（见[第12章](chapter12.xhtml)）
    |'
- en: '| O(n) | Linear | Searching an unordered array ([Chapter 9](chapter9.xhtml))
    and inorder traversal of a tree ([Chapter 12](chapter12.xhtml)) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| O(n) | 线性级 | 在无序数组中查找元素（见[第9章](chapter9.xhtml)）和树的中序遍历（见[第12章](chapter12.xhtml)）
    |'
- en: '| O(n log n) | Log-linear | Sorting an array with heapsort and average behavior
    of quicksort ([Chapter 6](chapter6.xhtml)) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| O(n log n) | 对数线性 | 使用堆排序排序数组和快速排序的平均行为（见[第6章](chapter6.xhtml)） |'
- en: '| O(n2) | Quadratic | Sorting an array with bubble sort and worst case for
    quicksort ([Chapter 6](chapter6.xhtml)) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| O(n²) | 二次方 | 使用冒泡排序排序数组和快速排序的最坏情况（见[第6章](chapter6.xhtml)） |'
- en: '| O(kn) | Exponential | Testing whether a binary formula is a tautology (k
    = 2) and a naive implementation of the Fibonacci series (k = 1.618) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| O(kn) | 指数级 | 测试一个二进制公式是否是重言式（k = 2）和斐波那契数列的朴素实现（k = 1.618） |'
- en: '| O(n!) | Factorial | Finding the optimum traveling salesman solution and sorting
    by random permutations (in [Chapter 6](chapter6.xhtml)) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| O(n!) | 阶乘 | 寻找最优旅行商问题解和通过随机排列排序（见[第6章](chapter6.xhtml)） |'
- en: The last two orders are algorithms that are so slow, you won’t use them in real
    life; their time complexity grows way too fast to be usable.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最后两个顺序的算法非常慢，实际上你在实际中不会使用它们；它们的时间复杂度增长得太快，无法使用。
- en: '[Figure 4-1](chapter4.xhtml#fig4-1) is a simple chart showing how the seven
    functions from [Table 4-2](chapter4.xhtml#tab4-2) behave. Clearly an *O*(log *n*)
    algorithm would be preferred instead of an *O*(*n*²) algorithm.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4-1](chapter4.xhtml#fig4-1)是一个简单的图表，展示了[表4-2](chapter4.xhtml#tab4-2)中七个函数的行为。显然，*O*(log
    *n*) 算法要优于 *O*(*n*²) 算法。'
- en: '![](../images/Figure4-1.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/Figure4-1.jpg)'
- en: 'Figure 4-1: This chart (drawn using Desmos, [https://www.desmos.com/calculator](https://www.desmos.com/calculator))
    shows the seven functions from [Table 4-2](chapter4.xhtml#tab4-2).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图4-1：此图表（使用Desmos绘制，[https://www.desmos.com/calculator](https://www.desmos.com/calculator)）展示了[表4-2](chapter4.xhtml#tab4-2)中的七个函数。
- en: The two first orders at the bottom of the chart (constant and logarithmic) are
    excellently well behaved. When considering linear (the diagonal line from bottom
    left to upper right) and log-linear orders (the closest curve to the diagonal),
    growth starts to be important. The next order, quadratic, goes off the chart for
    *x* = 10 with the value *x*² = 100\. Finally, the exponential and factorial orders
    are even worse behaved; their growth makes them impossible to use.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图表底部的前两个顺序（常数和对数）表现得非常好。当考虑线性（从左下角到右上角的对角线）和对数线性顺序（最接近对角线的曲线）时，增长开始变得重要。下一个顺序，二次方，在
    *x* = 10 时超出图表范围，值为 *x*² = 100。最后，指数和阶乘级别的增长甚至更为严重，它们的增长使得这些算法无法使用。
- en: 'You can look at this behavior another way by answering a simple question: What
    happens with a given algorithm if the input size is 10 times bigger? If the algorithm
    is *O*(1), the amount of time will stay the same, with no growth. With an *O*(log
    *n*) algorithm, the required time would grow, but by a fixed amount. An *O*(*n*)
    algorithm would (nearly) multiply its time by 10, and an *O*(*n*²) algorithm would
    be around 100 times longer. An *O*(*n* log *n*) algorithm would be in between
    those two. The difference is clear, but note for future reference that it’s much
    closer to *O*(*n*).'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过回答一个简单的问题来以另一种方式看待这种行为：如果输入大小是原来的10倍，给定的算法会怎样？如果算法是*O*(1)，那么所需时间将保持不变，不会增长。如果是*O*(log
    *n*)算法，所需时间会增长，但增长量是固定的。一个*O*(*n*)算法的时间将（几乎）乘以10，而一个*O*(*n*²)算法的时间将大约是100倍。一个*O*(*n*
    log *n*)算法则介于两者之间。差异很明显，但请注意，未来参考时，它更接近于*O*(*n*)。
- en: 'The results from the previous paragraph are also the reason why *O*(*n*) is
    used rather than *O*(9*n*) or *O*(22*n*). The ratio between these three algorithms
    is constant, so if *n* grows, they will grow at the same rate. On the other hand,
    an *O*(*n*²) algorithm will grow so much faster, it’s really is a class by itself.
    Constant values are meaningless when comparing classes: *O*(*n*²) will always
    grow faster (and also become larger) than an *O*(*n*) algorithm, even throwing
    in some constant factor, if *n* is large enough.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段的结果也是为什么使用*O*(*n*)而不是*O*(9*n*)或*O*(22*n*)的原因。这三种算法之间的比率是恒定的，因此如果*n*增长，它们会以相同的速度增长。另一方面，*O*(*n*²)算法会增长得更快，它真的属于一个独立的类别。在比较类别时，常数值没有意义：即使加入某个常数因子，如果*n*足够大，*O*(*n*²)也总是比*O*(*n*)算法增长得更快（并且变得更大）。
- en: Performance Measurements
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 性能测量
- en: When measuring an algorithm’s performance, the best-case performance is an algorithm’s
    behavior in ideal conditions; for instance, in the previous section we mentioned
    doing a search and finding the desired element at the first position of an array.
    You can’t ever assume you’ll always get this optimum performance, but it’s a baseline
    to compare other performances.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在衡量一个算法的性能时，最佳情况性能是算法在理想条件下的表现；例如，在前一节中我们提到过进行搜索并在数组的第一个位置找到所需的元素。你不能总是假设你会得到这种最优表现，但它是用来比较其他表现的基准。
- en: The complementary case is *worst-case* performance, which means you try to measure
    how an algorithm will perform in the slowest possible way. For instance, later
    in the book we’ll see algorithms that usually have *O*(*n* log *n*) performance
    that may degenerate to *O*(*n*²) performance for specific input data ordering.
    The worst-case analysis is important, because you should always assume that possibility
    will happen; it’s the safest (but most pessimistic) analysis.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 补充情况是*最坏情况*性能，意味着你尝试衡量一个算法在最慢的可能情况下的表现。例如，在本书的后面，我们将看到一些通常具有*O*(*n* log *n*)性能的算法，对于特定的输入数据顺序，可能会退化为*O*(*n*²*)性能。最坏情况分析很重要，因为你应该总是假设这种可能性会发生；它是最安全（但也是最悲观的）分析。
- en: A third possibility is *average-case* performance, which means determining how
    an algorithm will behave with typical or random input. In [Chapter 6](chapter6.xhtml)
    you’ll see that quicksort’s average performance is *O*(*n* log *n*) despite cases
    when performance is much worse.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 第三种可能性是*平均情况*性能，意味着确定一个算法在典型或随机输入下的表现。在[第6章](chapter6.xhtml)中，你将看到快速排序的平均性能是*O*(*n*
    log *n*)，尽管在某些情况下性能会更差。
- en: The fourth possibility is amortized time. Some algorithms often take a short
    time to perform, but periodically require more time. If you look at one individual
    operation, the result may be poor, but if you consider the average performance
    over a long series of operations, you may find that, overall, the amortized time
    is much better than the worst case, letting you predict the result of sequences
    of operations.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 第四种可能性是摊销时间。一些算法通常需要较短的时间来执行，但周期性地需要更多的时间。如果你只看一个单独的操作，结果可能不好，但如果你考虑在一长系列操作中的平均表现，你可能会发现，总体上，摊销时间比最坏情况要好得多，让你能够预测一系列操作的结果。
- en: 'Let’s consider a simple example: adding elements to a fixed-size array. If
    every time you want to add a new element you need to copy the current array to
    a new (and longer) array, the cost of each addition would be *O*(*n*). However,
    if the array is full, an alternative strategy is to copy it to a new double-sized
    array, leaving empty space to wait for future insertions.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个简单的示例：向固定大小的数组添加元素。如果每次你要添加一个新元素时都需要将当前数组复制到一个新的（且更长的）数组中，那么每次添加的成本将是
    *O*(*n*)。然而，如果数组已满，一种替代策略是将其复制到一个新的双倍大小的数组中，腾出空位等待将来插入。
- en: Let’s look at how this strategy works. Consider a situation with an array that
    is almost fully occupied (cells in gray), with just one empty space (cell in white)
    at the end, as shown in [Figure 4-2](chapter4.xhtml#fig4-2).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个策略是如何运作的。假设有一个几乎被填满的数组（灰色单元），在末尾只有一个空位（白色单元），如[图 4-2](chapter4.xhtml#fig4-2)所示。
- en: '![](../images/Figure4-2.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/Figure4-2.jpg)'
- en: 'Figure 4-2: An array with only one empty space'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4-2：一个只有一个空位的数组
- en: When adding a new element (an *O*(1) operation with constant time), the array
    is full but you don’t need to worry yet ([Figure 4-3](chapter4.xhtml#fig4-3)).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 当添加一个新元素时（这是一个 *O*(1) 操作，具有常数时间复杂度），数组已满，但你还不需要担心（见[图 4-3](chapter4.xhtml#fig4-3)）。
- en: '![](../images/Figure4-3.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/Figure4-3.jpg)'
- en: 'Figure 4-3: Now the array is full.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4-3：现在数组已满。
- en: However, if you need to add another element, there’s no place for it, so you
    copy the array to a new double-sized one and then add the new value, as shown
    in [Figure 4-4](chapter4.xhtml#fig4-4).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你需要添加另一个元素，但没有空位，那么你将数组复制到一个新的双倍大小的数组中，然后添加新值，如[图 4-4](chapter4.xhtml#fig4-4)所示。
- en: '![](../images/Figure4-4.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/Figure4-4.jpg)'
- en: 'Figure 4-4: A new double-sized array provides space for the new value and more.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4-4：一个新的双倍大小的数组为新值和更多空间提供了位置。
- en: 'After this process, which is an *O*(*n*) operation, you now have *n* free cells
    and may rest easily, knowing that upcoming insertions won’t need any copying and
    will be *O*(1). The next time the array becomes full, the process will be repeated:
    a lengthy single duplication followed by many fast additions. Averaging the cost
    of many insertions, the costlier (infrequent) doubling will be compensated for
    by the inexpensive (frequent) simple additions, and the amortized performance
    will be *O*(1).'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，这是一个 *O*(*n*) 操作，你现在有 *n* 个空闲单元，可以放心，因为接下来的插入不需要复制，时间复杂度是 *O*(1)。下次数组满时，过程将会重复：一次长时间的复制，接着是许多快速的添加。通过对多个插入的成本进行平均，代价较高（不频繁）的复制操作将通过代价低廉（频繁）的简单添加操作得到补偿，从而摊销后的性能为
    *O*(1)。
- en: NOTE
  id: totrans-66
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意
- en: '*The following section is much more mathematically minded than the rest of
    the book. If you wish, you can skip the demonstrations and study only the results.
    The rest of the book won’t delve into so much math. This section is simply to
    give you a taste of what complete, formal proofs look like.*'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*接下来的部分数学性较强，如果你愿意，可以跳过演示，只研究结果。书中的其他部分不会涉及这么多数学内容。本节的目的是让你体验完整、正式的证明是怎样的。*'
- en: Analysis of Algorithms in Practice
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实际算法分析
- en: 'Let’s consider examples of actual orders. Suppose you want to search an ordered
    array of length *n* for a given key. The worst case, linear search, is going sequentially
    through the whole array (because you haven’t yet learned the better algorithms
    described later in the book) without finding the key. In this case, the linear
    search performance is *O*(*n*) because you have to go through the whole array:
    *n* steps and *n* (failed) tests. The best-case performance is finding the key
    you wanted on the first attempt: Ω(1).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个实际的排序示例。假设你要在一个长度为 *n* 的有序数组中查找给定的键。最坏情况是线性查找，依次遍历整个数组（因为你还没学到后面书中描述的更好的算法），结果没有找到该键。在这种情况下，线性查找的时间复杂度为
    *O*(*n*)，因为你必须遍历整个数组： *n* 步和 *n* 次（失败的）测试。最好的情况是在第一次尝试时就找到了你想要的键：Ω(1)。
- en: 'The average requires a bit of algebra. You need to consider all cases: you
    could find the given element at the first place, the second place, and so on,
    all the way up the *n*th, which means *n* possibilities in all. On average, you
    have to test (1 + 2 + ... + *n*)/*n* elements. The sum of numbers from 1 to *n*
    equals *n*(*n* + 1)/2, so the average needed (dividing by *n*) ends up (*n* +
    1)/2\. This expression is clearly proportional to *n*, so the average behavior
    of the algorithm is indeed *O*(*n*). If you had to consider using this algorithm,
    you’d think in terms of *O*(*n*), assuming the worst; hoping for the best case
    isn’t realistic.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 计算平均值需要一点代数。你需要考虑所有的情况：你可能会在第一个位置、第二个位置，依此类推，一直到第 *n* 个位置，这意味着一共有 *n* 种可能性。平均而言，你必须测试
    (1 + 2 + ... + *n*)/*n* 个元素。从 1 到 *n* 的数的和等于 *n*(*n* + 1)/2，所以最终的平均值是 (*n* + 1)/2。这个表达式显然与
    *n* 成正比，因此该算法的平均行为确实是 *O*(*n*)。如果你必须考虑使用这个算法，你会以 *O*(*n*) 来思考，假设最坏情况；指望最好的情况是不现实的。
- en: NOTE
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意
- en: '*There’s another way to look at this calculation. The search could succeed
    at the first element or could take up to the nth; on average, (n + 1)/2\. Or,
    it could succeed either at the second element or the (n – 1)th; on average again,
    (n + 1)/2\. The same reasoning applies for the third, fourth, and subsequent elements.
    For each case in which the search finishes in a few steps, a complementary case
    drives the average number of steps up to (n + 1)/2\. Since in every case the average
    is the same, you can conclude that’s the result. You arrive at the same result
    with a bit more “hand waving” but less algebra.*'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*还有一种看待这个计算的方法。搜索可能在第一个元素成功，也可能一直搜索到第 *n* 个元素；平均而言，搜索步骤是 (n + 1)/2。或者，搜索可能在第二个元素或第
    (n – 1) 个元素成功；同样，平均也是 (n + 1)/2。对于第三、第四和后续元素也适用相同的推理。对于每一种搜索在少数步骤内完成的情况，都有一个对应的情况将平均步骤数推高到
    (n + 1)/2。由于每种情况下的平均值相同，你可以得出结论那就是最终结果。这样你就得出了相同的结果，只是少了些代数推导，但却多了一些“手势解释”。*'
- en: Let’s discuss another way of searching an ordered array, a binary search, which
    you’ll see in [Chapter 9](chapter9.xhtml). Instead of starting at the beginning
    of an array and going through all its elements, you start at the *middle* of the
    array. If you find the key you want, you’re done. If not, you can discard half
    the array (if the key you want is less than the middle element, you know it can’t
    be in the higher part of the array) and recursively search in the other part.
    You search in that new part by picking its middle element, comparing, and so on.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论另一种搜索有序数组的方法，二分查找，你将在[第 9 章](chapter9.xhtml)中看到。与其从数组的开始遍历所有元素，不如从数组的*中间*开始。如果你找到了你要的元素，搜索结束。如果没有，你可以丢弃数组的一半（如果你想要的元素小于中间元素，你知道它不可能在数组的较大部分中），然后在另一部分递归地进行搜索。你通过选择新的部分的中间元素进行比较，依此类推。
- en: 'Consider an array with the numbers 4, 9, 12, 22, 34, 56, and 60\. If you wanted
    to check whether 12 was in it, first you’d look at the middle element: 22\. That’s
    not what you want, so you can discard the second half of the array (34, 56, and
    60), because you know that 12, if present, must be in the first half. Now look
    for 12 in the array that is now 4, 9, and 12\. Start by looking at its middle
    element (9) and then discard it and the first half of the array (4). The last
    step of the search looks at an array with a single element (12). Its middle (and
    only) element is what you were looking for, so the search succeeded. If you were
    looking for 13 instead, the search would fail at this point, since no more pieces
    of the array exist.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个包含数字 4、9、12、22、34、56 和 60 的数组。如果你想检查 12 是否在其中，首先你会查看中间的元素：22。那不是你想要的，所以你可以丢弃数组的后半部分（34、56
    和 60），因为你知道，如果 12 存在，它必须在前半部分。现在在剩下的数组 4、9 和 12 中寻找 12。首先查看其中间的元素（9），然后丢弃它和数组的前半部分（4）。搜索的最后一步查看一个只包含一个元素（12）的数组。它的中间（也是唯一）元素就是你要找的，所以搜索成功了。如果你在找
    13，搜索到这里会失败，因为数组中没有剩余的部分。
- en: To see how this algorithm performs, count how many times you need to test an
    element; assume that the array’s length *n* is 2*^(k–)*¹ for some *k* > 0, so
    all halves of the array always have an odd number. (This is just to simplify calculations;
    see question 4.9.) In one case the element is found on the first attempt. In two
    cases the key is found on the second try—namely, the middle elements of the chosen
    halves. In four cases, the third try is successful, and in eight cases, the fourth
    try succeeds. [Figure 4-5](chapter4.xhtml#fig4-5) shows this for an array with
    15 elements.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看这个算法的表现，计算你需要测试一个元素多少次；假设数组的长度 *n* 是 2*^(k–)*¹，其中 *k* > 0，这样数组的每一半总是有一个奇数个元素。（这样做是为了简化计算；参见问题
    4.9。）在一种情况下，元素在第一次尝试中就被找到。在两种情况下，关键元素在第二次尝试中被找到——即，选择的两半中的中间元素。在四种情况下，第三次尝试成功，在八种情况下，第四次尝试成功。[图
    4-5](chapter4.xhtml#fig4-5) 显示了一个包含 15 个元素的数组的情况。
- en: '![](../images/Figure4-5.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/Figure4-5.jpg)'
- en: 'Figure 4-5: Starting in the middle of an array with 15 elements'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4-5：从包含 15 个元素的数组的中间开始
- en: 'For a general array, the total number of comparisons is *S* = (1 × 1 + 2 ×
    2 + 3 × 4 + 4 × 8 + ...+ *k* × 2*^k* ^(− 1)), which you must divide by the number
    of elements in the array to get the average. To calculate *S* do a math trick
    and first write a more general formula. Write *S* = 1 × 2⁰ + 2 × 2¹ + 3 × 2² +
    ... + *k* × 2*^k*^(−1) and then define *f*(*x*) = 1*x*⁰ + 2*x*¹ + 3*x*² + ...
    + *kx**^k*^(−1); note that S = *f*(2). It follows from calculus that *f*(*x*)
    is the derivative of *g*(*x*) = 1 + *x + x*²*+ x*³ *+ ... + x**^k*. Since a well-known
    result says that *g*(*x*) = (*x**^k*^(+1) – 1)/(*x* – 1), by deriving you find
    the following:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个一般的数组，总比较次数是 *S* = (1 × 1 + 2 × 2 + 3 × 4 + 4 × 8 + ...+ *k* × 2*^k* ^(−
    1))，你必须将其除以数组中的元素数量，才能得到平均值。要计算 *S*，可以用一个数学技巧，先写出一个更一般的公式。写 *S* = 1 × 2⁰ + 2 ×
    2¹ + 3 × 2² + ... + *k* × 2*^k*^(−1)，然后定义 *f*(*x*) = 1*x*⁰ + 2*x*¹ + 3*x*² + ...
    + *kx**^k*^(−1)；注意，S = *f*(2)。根据微积分的结果，*f*(*x*) 是 *g*(*x*) = 1 + *x + x*²*+ x*³
    *+ ... + x**^k* 的导数。由于一个著名的结果指出 *g*(*x*) = (*x**^k*^(+1) – 1)/(*x* – 1)，通过求导你可以得到以下结果：
- en: $Equation$![](../images/pg57.jpg)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: $方程$![](../images/pg57.jpg)
- en: Now undo the generalization you just made by setting *x* = 2 and remembering
    that *n* = 2*^k*^(–1). You can write *S* = (*k* + 1)(*n* + 1) – (2*n* + 1). Dividing
    by *n* you find that the average number of comparisons is (*k* – 1) + *k*/*n*.
    You can write *k* = log *n* (taking logarithms in base 2 and rounding upward)
    so the average performance of the algorithm is Θ(log *n*). Whew! The worst case
    (a failed search) requires *k* tests, so again you are justified in saying that
    binary search is an *O*(log *n*) algorithm.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，通过设置 *x* = 2，并记得 *n* = 2*^k*^(–1)，撤销你刚才做的归纳。你可以写出 *S* = (*k* + 1)(*n* + 1)
    – (2*n* + 1)。将其除以 *n*，你会发现平均比较次数是 (*k* – 1) + *k*/*n*。你可以写出 *k* = log *n*（取以 2
    为底的对数并向上取整），因此算法的平均性能是 Θ(log *n*)。呼！最坏情况下（搜索失败）需要 *k* 次测试，因此你仍然可以说二分查找是一个 *O*(log
    *n*) 算法。
- en: Using big *O* notation is “safer” and provides better “cover.” Of course, you
    could also say that the binary search is *o*(*n*) or, even worse, *o*(*n*²) because
    those functions behave in a worse way, growing faster. The small *o* and small
    omega bounds are good for a rough estimate, but you want to be more precise and
    aim for closer bounds whenever possible.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 使用大 *O* 符号是“更安全”的，并且提供了更好的“保护”。当然，你也可以说二分查找是 *o*(*n*) 或者更糟的是 *o*(*n*²)，因为这些函数的增长速度更快，表现更差。小
    *o* 和小欧米伽边界对于粗略估计是好的，但你希望能更精确，并尽可能得到更接近的边界。
- en: Most analysis of algorithms involves recurrences as shown here, and some studies
    are even more complex mathematically than what you just saw. Recurrences usually
    take a few well-known forms, such as *P*(*n*) = *aP*(*n* – 1) + *f*(*n*) or *Q*(*n*)
    = *aQ*(*n*/*b*) + *f*(*n*), among practically infinite possibilities. There are
    several tricks to help find expressions for *M*(*n*) in each case (in particular,
    the “master theorem” quickly provides a solution to the *Q*(*n*) recurrence style,
    but a whole book could be written on that).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数算法分析都涉及递推关系，如这里所示，有些研究在数学上甚至比你刚才看到的更复杂。递推关系通常有几种常见形式，如 *P*(*n*) = *aP*(*n*
    – 1) + *f*(*n*) 或 *Q*(*n*) = *aQ*(*n*/*b*) + *f*(*n*)，在几乎无限的可能性中。对于每种情况，有几种技巧可以帮助找到
    *M*(*n*) 的表达式（特别是，“主定理”可以迅速提供 *Q*(*n*) 递推式的解，但对此可以写一本书）。
- en: Time and Space Complexity Trade-offs
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 时间和空间复杂度的权衡
- en: Earlier in the chapter, we mentioned we’d look at time performance, because
    from the point of view of the storage requirements, algorithms are usually well
    behaved. Let’s explore a simple problem and see how time and space trade-offs
    apply.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章早些时候，我们提到过我们将关注时间性能，因为从存储需求的角度来看，算法通常表现良好。让我们探索一个简单的问题，看看时间和空间的权衡是如何应用的。
- en: 'Say we have a (long) array of numbers and frequently require finding the sum
    of the values in a range of positions, from *i* to *j*, both inclusive, with *i*
    < *j*. (This problem has to do with breaking a long string of text into justified
    lines.) A first solution just needs a couple of auxiliary variables, so extra
    memory requirements are *O*(1), but finding the sums themselves requires *O*(*n*)
    time:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个（很长的）数字数组，并且经常需要查找从位置*i*到位置*j*（都包括在内）范围内的值的和，且*i* < *j*。（这个问题与将一长串文本拆分成对齐的行有关。）第一个解决方案只需要几个辅助变量，因此额外的内存需求是*O*(1)，但计算这些和本身需要*O*(*n*)时间：
- en: '[PRE0]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This function is clear and correct—it consists of just a loop summing all the
    values between from and to inclusive in sum—but its performance will impact the
    process negatively, because you are calling it frequently. For a function that
    will be called many times, a better performance is preferred.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数清晰且正确——它仅包含一个循环，计算从“from”到“to”（包括在内）之间的所有值的和——但它的性能会对过程产生负面影响，因为你会频繁调用它。对于一个将被多次调用的函数，更好的性能是更受欢迎的。
- en: 'You also can apply a concept of dynamic programming (which we’ll study in more
    detail in [Chapter 5](chapter5.xhtml)) and work by tabulation, precomputing the
    sums from position 0 to all other positions:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以应用动态规划的概念（我们将在[第5章](chapter5.xhtml)中更详细地学习），通过表格法来工作，预先计算从位置0到所有其他位置的和：
- en: '[PRE1]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'With this array of partial sums, if you need the sum of elements 0 to *q*,
    you already have those, and for the sum of elements *p* > 0 to *q*, just calculate
    the sum of elements from 0 to *q* minus the sum of elements from 0 to *p* – 1:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个部分和数组，如果你需要从元素0到*q*的和，你已经有了这些值，对于从元素*p* > 0到*q*的和，只需要计算从0到*q*的和减去从0到*p*–1的和：
- en: '[PRE2]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This solution implies further *O*(*n*) processing to compute the partial sums,
    which is only done once, and *O*(*n*) extra memory, but it provides *O*(1) sums
    for ranges, so you can see the trade-off: use more memory to apply faster algorithms
    or save memory by accepting a slower performance.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这个解决方案意味着需要进一步的*O*(*n*)处理来计算部分和，这只需执行一次，并且需要*O*(*n*)额外的内存，但它提供了*O*(1)的区间和，因此你可以看到权衡：使用更多内存来应用更快的算法，或者通过接受较慢的性能来节省内存。
- en: Which version should you choose? That depends on the problem and whether the
    current performance is acceptable, and even possibly whether enough memory is
    available!
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该选择哪个版本？这取决于问题以及当前的性能是否可以接受，甚至可能取决于是否有足够的内存可用！
- en: Summary
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we’ve discussed the definitions that are relevant to studying
    how algorithms perform in terms of either operations or memory requirements. We’ve
    seen several classes of algorithms that help decide how to implement a given solution
    to a problem by comparing efficiency in response to larger inputs. In the next
    chapter, we’ll switch gears and study ways to create algorithms in preparation
    for the rest of the book, where we’ll consider many varied data structures and
    the algorithms that perform them.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了与研究算法在操作或内存需求方面表现相关的定义。我们看到几类算法，通过比较在较大输入情况下的效率，帮助决定如何实现给定问题的解决方案。在下一章，我们将换个角度，研究如何创建算法，为本书的后续内容做准备，在那里我们将考虑许多不同的数据结构以及执行它们的算法。
- en: Questions
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问题
- en: The questions in this chapter are visibly different from all other questions
    in the book because they are more mathematically oriented. Feel free to skip ahead.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的问题明显不同于书中其他所有问题，因为它们更具数学性。你可以随意跳过。
- en: '**4.1  How Fast Did You Say?**'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**4.1  你说多快？**'
- en: An analyst has just completed a study of a new algorithm and concludes that
    its running time, depending on its input size *n*, is exactly 17*n* log *n* –
    2*n*² + 48\. What do you say about that result?
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 一名分析师刚刚完成了一项新算法的研究，并得出结论：根据输入大小*n*，它的运行时间正好是17*n* log *n* – 2*n*² + 48\。你对这个结果怎么看？
- en: '**4.2  Weird Bound?**'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**4.2  奇怪的界限？**'
- en: Is it valid to say that *n* is *O*(*n*²)? What about *o*(*n*²)? Other orders?
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 说*n*是*O*(*n*²)有效吗？那*o*(*n*²)呢？其他的阶次呢？
- en: '**4.3  Of Big** ***O*****s and Omegas**'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**4.3  关于大** ***O*** 和Ω'
- en: What can you deduce if a certain function is both *f*(*n*) = *O*(*g*(*n*)) and
    *f*(*n*) = Ω(*g*(*n*))?
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如果某个函数既是 *f*(*n*) = *O*(*g*(*n*))，又是 *f*(*n*) = Ω(*g*(*n*))，你能推导出什么结论？
- en: '**4.4  Transitivity?**'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**4.4  传递性？**'
- en: 'If *f*(*n*) = *O*(*g*(*n*)) and *g*(*n*) = *O*(*h*(*n*)), how are *f*(*n*)
    and *h*(*n*) related? What if instead of big *O*, you were looking at other orders:
    small *o*, big theta, and so on?'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 *f*(*n*) = *O*(*g*(*n*)) 且 *g*(*n*) = *O*(*h*(*n*))，那么 *f*(*n*) 和 *h*(*n*)
    之间有何关系？如果不是大 *O*，而是考虑其他阶次：小 *o*、大 theta 等等，你会如何看待？
- en: '**4.5  A Bit of Reflection**'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**4.5  一点反思**'
- en: It seems clear that for any function *f*(*n*), you have *f*(*n*) = Θ(*f*(*n*)).
    What would you say if working with other orders instead of big theta?
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来很明显，对于任何函数 *f*(*n*)，都有 *f*(*n*) = Θ(*f*(*n*)）。如果处理其他阶次而不是大 theta，你会怎么说？
- en: '**4.6  Going at It Backward**'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**4.6  倒着来**'
- en: If *f*(*n*) = *O*(*g*(*n*)), what is the order of *g*(*n*) relative to *f*(*n*)?
    What if *f*(*n*) = *o*(*g*(*n*))?
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 *f*(*n*) = *O*(*g*(*n*))，那么 *g*(*n*) 相对于 *f*(*n*) 的阶是什么？如果 *f*(*n*) = *o*(*g*(*n*))
    呢？
- en: '**4.7  One After the Other**'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**4.7  一个接一个**'
- en: 'Suppose you have a process that consists of two steps: the first is an *O*(*n*
    log *n*) algorithm and the second is an *O*(*n*²) algorithm. What’s the order
    of the whole process? Can you give a general rule?'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个包含两个步骤的过程：第一个步骤是一个 *O*(*n* log *n*) 的算法，第二个步骤是一个 *O*(*n*²) 的算法。那么整个过程的阶是什么？你能给出一个通用的规则吗？
- en: '**4.8  Loop the Loop**'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**4.8  循环循环**'
- en: 'A different but related question: suppose your process consists of an *O*(*n*)
    loop that does an *O*(*n*²) process at each step. What’s the order of the whole?
    Again, can you provide a general rule?'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 一个不同但相关的问题：假设你的过程包含一个 *O*(*n*) 循环，在每一步中执行一个 *O*(*n*²) 的过程。那么整个过程的阶是什么？再一次，你能提供一个通用的规则吗？
- en: '**4.9  Almost a Power ...**'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**4.9  几乎是幂次…**'
- en: When analyzing binary search, you learned that if the array’s length is 2*^k*
    ^(–1) for some *k* > 0, the initial array and all subsequent arrays would have
    an odd length. Can you prove this?
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析二分查找时，你学到了如果数组的长度是 2*^k* ^(–1)（其中 *k* > 0），那么初始数组及所有后续数组的长度都会是奇数。你能证明这一点吗？
- en: '**4.10  It Was the Best of Times; It Was the Worst of Times**'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**4.10  最美好的时代；最糟糕的时代**'
- en: What happens if the best-case running time of an algorithm is Ω(*f*(*n*)) and
    the worst case is *O*(*f*(*n*))?
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个算法的最优运行时间是 Ω(*f*(*n*))，而最差运行时间是 *O*(*f*(*n*))，会发生什么？
