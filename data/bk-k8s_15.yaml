- en: '13'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '13'
- en: HEALTH PROBES
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 健康探针
- en: '![image](../images/common01.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/common01.jpg)'
- en: Having a reliable application is about more than just keeping application components
    running. Application components also need to be able to respond to requests in
    a timely way and get data from and make requests of dependencies. This means that
    the definition of a “healthy” application component is different for each individual
    component.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有一个可靠的应用程序不仅仅是让应用组件保持运行。应用组件还需要能够及时响应请求，并从依赖项中获取数据并发出请求。这意味着，“健康”应用组件的定义对于每个组件都是不同的。
- en: At the same time, Kubernetes needs to know when a Pod and its containers are
    healthy so that it can route traffic to only healthy containers and replace failed
    ones. For this reason, Kubernetes allows configuration of custom health checks
    for containers and integrates those health checks into management of workload
    resources such as Deployment.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，Kubernetes需要知道Pod及其容器的健康状态，以便只将流量路由到健康的容器，并替换掉失败的容器。因此，Kubernetes允许为容器配置自定义健康检查，并将这些健康检查集成到工作负载资源的管理中，例如Deployment。
- en: In this chapter, we’ll look at how to define health probes for our applications.
    We’ll look at both network-based health probes and probes that are internal to
    a container. We’ll see how Kubernetes runs these health probes and how it responds
    when a container becomes unhealthy.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何为我们的应用程序定义健康探针。我们将研究基于网络的健康探针和容器内部的探针。我们还将了解Kubernetes如何运行这些健康探针，并且当容器变得不健康时如何响应。
- en: About Probes
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于探针
- en: 'Kubernetes supports three different types of probes:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes支持三种不同类型的探针：
- en: '**Exec** Run a command or script to check on a container.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**Exec** 运行一个命令或脚本来检查容器的状态。'
- en: '**TCP** Determine whether a socket is open.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**TCP** 确定一个套接字是否打开。'
- en: '**HTTP** Verify that an HTTP GET succeeds.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**HTTP** 验证HTTP GET是否成功。'
- en: 'In addition, we can use any of these three types of probes for any of three
    different purposes:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以将这三种探针中的任何一种用于三种不同的用途：
- en: '**Liveness** Detect and restart failed containers.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**Liveness** 检测并重启失败的容器。'
- en: '**Startup** Give extra time before starting liveness probes.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**Startup** 在启动活性探针之前，给容器额外的时间。'
- en: '**Readiness** Avoid sending traffic to containers when they are not prepared
    for it.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**Readiness** 在容器尚未准备好时避免向其发送流量。'
- en: Of these three purposes, the most important is the liveness probe because it
    runs during the primary life cycle of the container and can result in container
    restarts. We’ll look closely at liveness probes and use that knowledge to understand
    how to use startup and readiness probes.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这三种用途中，最重要的是活性探针，因为它在容器的主要生命周期内运行，并可能导致容器重启。我们将详细了解活性探针，并利用这些知识理解如何使用启动探针和就绪探针。
- en: Liveness Probes
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活性探针
- en: A *liveness* probe runs continuously as soon as the container has started running.
    Liveness probes are created as part of the container definition, and a container
    that fails its liveness probe will be restarted automatically.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*活性*探针会在容器启动后持续运行。活性探针作为容器定义的一部分创建，任何未通过活性探针检查的容器将会被自动重启。'
- en: Exec Probes
  id: totrans-18
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Exec 探针
- en: Let’s begin with a simple liveness probe that runs a command inside the container.
    Kubernetes expects the command to finish before a timeout and return zero to indicate
    success, or a non-zero code to indicate a problem.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 首先从一个简单的活性探针开始，该探针会在容器内运行一个命令。Kubernetes期望命令在超时前完成，并返回零表示成功，或者返回非零代码表示存在问题。
- en: '**NOTE**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**NOTE**'
- en: '*The example repository for this book is at* [https://github.com/book-of-kubernetes/examples](https://github.com/book-of-kubernetes/examples).
    *See “Running Examples” on [page xx](ch00.xhtml#ch00lev1sec2) for details on getting
    set up.*'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*本书的示例仓库在* [https://github.com/book-of-kubernetes/examples](https://github.com/book-of-kubernetes/examples)。
    *关于如何设置，请参见[第xx页](ch00.xhtml#ch00lev1sec2)中的“运行示例”。*'
- en: 'Let’s illustrate this with an NGINX web server container. We’ll use this Deployment
    definition:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个NGINX web服务器容器来说明这一点。我们将使用这个Deployment定义：
- en: '*nginx-exec.yaml*'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*nginx-exec.yaml*'
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `exec` section of the `livenessProbe` tells Kubernetes to run a command
    inside the container. In this case, `curl` is used with a `-q` flag so that it
    doesn’t print the page contents but just returns a zero exit code on success.
    Additionally, the `-f` flag causes `curl` to return a non-zero exit code for any
    HTTP error response (that is, any response code of 300 or above).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '`livenessProbe`的`exec`部分告诉Kubernetes在容器内运行一个命令。在这种情况下，使用`curl`并加上`-q`标志，这样它不会打印页面内容，而只是返回一个零退出代码表示成功。另外，`-f`标志使得`curl`对于任何HTTP错误响应（即300以上的响应码）返回非零退出代码。'
- en: The `curl` command runs every 5 seconds based on the `periodSeconds`; it starts
    10 seconds after the container is started, based on `initialDelaySeconds`.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '`curl`命令每5秒运行一次，基于`periodSeconds`；它在容器启动后10秒开始，基于`initialDelaySeconds`。'
- en: 'The automated scripts for this chapter add the *nginx-exec.yaml* file to */opt*.
    Create this Deployment as usual:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的自动化脚本会将*nginx-exec.yaml*文件添加到*/opt*目录。按照平常的方式创建此部署：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The resulting Pod status doesn’t look any different from a Pod without a liveness
    probe:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 结果Pod的状态看起来和没有存活探针的Pod没有什么不同：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'However, in addition to the regular NGINX server process, `curl` is being run
    inside the container every 5 seconds, verifying that it is possible to connect
    to the server. The detailed output from `kubectl describe` shows this configuration:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，除了常规的NGINX服务器进程外，`curl`每5秒在容器内运行一次，验证是否可以连接到服务器。通过`kubectl describe`命令得到的详细输出展示了这一配置：
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Because a liveness probe is defined, the fact that the Pod continues to show
    a `Running` status and no restarts indicates that the check is successful. The
    `#success` field shows that one successful run is sufficient for the container
    to be considered live, whereas the `#failure` value shows that three consecutive
    failures will cause the Pod to be restarted.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 因为定义了存活探针，所以Pod持续显示`Running`状态且没有重启，这表明探针检查成功。`#success`字段显示一个成功的运行就足以认为容器是存活的，而`#failure`值显示连续三次失败会导致Pod被重启。
- en: We used `-q` to discard the logs from `curl`, but even without that flag, any
    logs from a successful liveness probe are discarded. If we want to save the ongoing
    log information from a probe, we need to send it to a file or use a logging library
    to ship it across the network.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`-q`来丢弃`curl`的日志，但即使没有该标志，成功的存活探针的任何日志都会被丢弃。如果我们想保存存活探针的实时日志信息，我们需要将其发送到文件或使用日志库将其发送到网络。
- en: Before moving on to another type of probe, let’s see what happens if a liveness
    probe fails. We’ll patch the `curl` command to try to retrieve a nonexistent path
    on the server, which will cause `curl` to return a non-zero exit code, so our
    probe will fail.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续介绍其他类型的探针之前，让我们先看看如果存活探针失败会发生什么。我们将修补`curl`命令，尝试从服务器检索一个不存在的路径，这会导致`curl`返回非零退出代码，从而使我们的探针失败。
- en: 'We used a patch file in [Chapter 9](ch09.xhtml#ch09) when we edited a Service
    type. Let’s do that again here to make the change:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第9章](ch09.xhtml#ch09)中我们使用了补丁文件来编辑Service类型。这里我们再做一次补丁以应用更改：
- en: '*nginx-404.yaml*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*nginx-404.yaml*'
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Although a patch file allows us to update only the specific fields we care about,
    in this case the patch file has several lines because we need to specify the full
    hierarchy, and we also must specify the name of the container we want to modify
    ➊, so Kubernetes will merge this content into the existing definition for that
    container.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管补丁文件允许我们仅更新我们关心的特定字段，但在这种情况下，补丁文件有几行，因为我们需要指定完整的层次结构，并且还必须指定我们要修改的容器名称➊，以便Kubernetes将这些内容合并到该容器的现有定义中。
- en: 'To patch the Deployment, use the `kubectl patch` command:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 要补丁部署，请使用`kubectl patch`命令：
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Because we changed the Pod specification within the Deployment, Kubernetes
    needs to terminate the old Pod and create a new one:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在部署中修改了Pod的规格，Kubernetes需要终止旧的Pod并创建一个新的Pod：
- en: '[PRE6]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Initially, the new Pod shows a `Running` status. However, if we check back
    again in about 30 seconds, we get an indication that the Pod has an issue:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，新Pod显示`Running`状态。然而，如果我们大约30秒后再次检查，会发现Pod出现了问题：
- en: '[PRE7]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We didn’t change the initial delay or the period for our liveness probe, so
    the first probe started after 10 seconds and the probe runs every 5 seconds. It
    takes three failures to trigger a restart, so it’s not surprising that we see
    one restart after 25 seconds have elapsed.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有改变存活探针的初始延迟和周期，所以第一次探针是在10秒后启动，之后每5秒运行一次。需要三次失败才能触发重启，因此看到在25秒后出现一次重启并不奇怪。
- en: 'The Pod’s event log indicates the reason for the restart:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 的事件日志指出了重启的原因：
- en: '[PRE8]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The event log helpfully provides the output from `curl` telling us the reason
    for the failed liveness probe. Kubernetes will continue to restart the container
    every 25 seconds as each new container starts running and then fails three consecutive
    liveness probes.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 事件日志提供了有用的 `curl` 输出，告诉我们存活探针失败的原因。Kubernetes 将继续每 25 秒重启容器，因为每个新容器启动后都会失败三个连续的存活探针。
- en: HTTP Probes
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: HTTP 探针
- en: The ability to run a command within a container to check health allows us to
    perform custom probes. However, for a web server like this one, we can take advantage
    of the HTTP probe capability within Kubernetes, avoiding the need for `curl` inside
    our container image and also verifying connectivity from outside the Pod.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在容器内运行命令以检查健康状况的能力使我们能够执行自定义探针。然而，对于像这样的 web 服务器，我们可以利用 Kubernetes 中的 HTTP 探针功能，避免在容器镜像内部使用
    `curl`，同时验证从 Pod 外部的连接性。
- en: 'Let’s replace our NGINX Deployment with a new configuration that uses an HTTP
    probe:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个新的配置替换我们的 NGINX Deployment，使用 HTTP 探针：
- en: '*nginx-http.yaml*'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*nginx-http.yaml*'
- en: '[PRE9]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: With this configuration, we tell Kubernetes to connect to port 80 of our Pod
    and do an HTTP GET at the root path of */*. Because our NGINX server is listening
    on port 80 and will serve a welcome file for the root path, we can expect this
    to work.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在此配置中，我们告诉 Kubernetes 连接到 Pod 的 80 端口并在根路径 */* 上执行 HTTP GET。由于我们的 NGINX 服务器监听
    80 端口并将在根路径上提供欢迎文件，我们可以期待它正常工作。
- en: 'We’ve specified the entire Deployment rather than using a patch, so we’ll use
    `kubectl apply` to update the Deployment:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们指定了整个 Deployment，而不是使用补丁，因此我们将使用 `kubectl apply` 来更新 Deployment：
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We could use a patch to make this change as well, but it would be more complex
    this time, because a patch file is merged into the existing configuration. As
    a result, we would require two commands: one to remove the existing liveness probe
    and one to add the new HTTP liveness probe. Better to just replace the resource
    entirely.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用补丁来进行这个更改，但这次会更复杂，因为补丁文件会被合并到现有配置中。因此，我们需要两个命令：一个删除现有的存活探针，另一个添加新的 HTTP
    存活探针。最好是完全替换资源。
- en: '**NOTE**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*The kubectl patch command is a valuable command for debugging, but production
    applications should have YAML resource files under version control to allow for
    change tracking and peer review, and the entire file should always be applied
    every time to ensure that the cluster reflects the current content of the repository.*'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*kubectl patch 命令是一个有价值的调试命令，但生产环境中的应用程序应该将 YAML 资源文件放在版本控制下，以便进行变更跟踪和同行审查，并且每次都应该应用整个文件，以确保集群反映当前仓库的内容。*'
- en: 'Now that we’ve applied the new Deployment configuration, Kubernetes will make
    a new Pod:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经应用了新的 Deployment 配置，Kubernetes 会创建一个新的 Pod：
- en: '[PRE11]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: For an HTTP probe, `kubelet` has the responsibility of running an HTTP GET request
    on the appropriate schedule and confirming the result. By default, any HTTP return
    code in the 200 or 300 series is considered a successful response.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 HTTP 探针，`kubelet` 负责按适当的时间表运行 HTTP GET 请求并确认结果。默认情况下，任何 HTTP 返回码在 200 或 300
    系列内都被视为成功响应。
- en: 'The NGINX server is logging all of its requests, so we can use the container
    logs to see the probes taking place:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: NGINX 服务器会记录所有请求，因此我们可以使用容器日志来查看正在进行的探测：
- en: '[PRE12]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We didn’t specify `periodSeconds` this time, so `kubelet` is probing the server
    at the default rate of once every 10 seconds.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这次我们没有指定 `periodSeconds`，所以 `kubelet` 以默认的每 10 秒一次的频率进行探测。
- en: 'Let’s clean up the NGINX Deployment before moving on:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们先清理一下 NGINX Deployment：
- en: '[PRE13]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We’ve looked at two of the three types of probes; let’s finish by looking at
    TCP.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看过了三种探针中的两种，接下来我们来看看 TCP 探针。
- en: TCP Probes
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: TCP 探针
- en: A database server such as PostgreSQL listens for network connections, but it
    does not use HTTP for communication. We can still create a probe for these kinds
    of containers using a TCP probe. It won’t provide the configuration flexibility
    of an HTTP or exec probe, but it will verify that a container in the Pod is listening
    for connections on the specified port.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 类似 PostgreSQL 这样的数据库服务器监听网络连接，但它不使用 HTTP 进行通信。我们仍然可以使用 TCP 探针为这些类型的容器创建探测。它不能提供
    HTTP 或 exec 探针的配置灵活性，但它可以验证 Pod 中的容器是否在指定端口上监听连接。
- en: 'Here’s a PostgreSQL Deployment with a TCP probe:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个带有 TCP 探针的 PostgreSQL Deployment：
- en: '*postgres-tcp.yaml*'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '*postgres-tcp.yaml*'
- en: '[PRE14]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We saw the requirement for the `POSTGRES_PASSWORD` environment variable in [Chapter
    10](ch10.xhtml#ch10). The only configuration that’s changed for this example is
    the `livenessProbe`. We specify a TCP socket of 5432, as this is the standard
    port for PostgreSQL.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 [第 10 章](ch10.xhtml#ch10) 中看到需要 `POSTGRES_PASSWORD` 环境变量。这个例子中唯一更改的配置是 `livenessProbe`。我们指定了一个
    5432 的 TCP 套接字，因为这是 PostgreSQL 的标准端口。
- en: 'As usual, we can create this Deployment and, after a while, observe that it’s
    running:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，我们可以创建这个部署，并在一段时间后观察它是否正在运行：
- en: '[PRE15]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Again, it is the job of `kubelet` to perform the probe. It does this solely
    by making a TCP connection to the port and then disconnecting. PostgreSQL doesn’t
    emit any logging when this happens, so the only way we know that the probe is
    working is to check that the container continues to run and doesn’t show any restarts:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，执行探针的是 `kubelet`。它仅通过与端口建立 TCP 连接并断开连接来执行此操作。当发生这种情况时，PostgreSQL 不会输出任何日志，因此我们知道探针是否有效的唯一方式是检查容器是否继续运行且没有重启：
- en: '[PRE16]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Before we move on, let’s clean up the Deployment:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们清理一下部署：
- en: '[PRE17]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We’ve now looked at all three types of probes. And although we used these three
    types to create liveness probes, the same three types will work with both startup
    and readiness probes as well. The only difference is the change in the behavior
    of our cluster when a probe fails.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经查看了所有三种类型的探针。虽然我们使用这三种类型来创建存活探针，但这三种类型同样适用于启动探针和就绪探针。唯一的区别是当探针失败时，我们集群的行为会有所不同。
- en: Startup Probes
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 启动探针
- en: Unhealthy containers can create all kinds of difficulties for an application,
    including lack of responsiveness, errors responding to requests, or bad data,
    so we want Kubernetes to respond quickly when a container becomes unhealthy. However,
    when a container is first started, it can take time before it is fully initialized.
    During that time, it might not be able to respond to liveness probes.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 不健康的容器可能会为应用程序带来各种困难，包括响应迟缓、请求响应错误或数据异常，因此我们希望 Kubernetes 在容器变为不健康时能迅速作出反应。然而，当容器首次启动时，可能需要一些时间才能完全初始化。在此期间，它可能无法响应存活探针。
- en: Because of that delay, we’re left with a need to have a long timeout before
    a container fails a probe, so we can give our container enough time for initialization.
    However, at the same time, we need to have a short timeout in order to detect
    a failed container quickly and restart it. The solution is to configure a separate
    *startup probe*. Kubernetes will use the startup probe configuration until the
    probe is successful; then it will switch over to the liveness probe.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这种延迟，我们需要在容器失败探针之前设置一个较长的超时时间，以便容器有足够的时间进行初始化。然而，同时我们又需要一个较短的超时时间，以便快速检测到失败的容器并重启它。解决方法是配置一个单独的
    *启动探针*。Kubernetes 将使用启动探针配置，直到探针成功；然后它会切换到存活探针。
- en: 'For example, we might configure our NGINX server Deployment as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以如下配置我们的 NGINX 服务器部署：
- en: '[PRE18]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Given this configuration, Kubernetes would start checking the container 30 seconds
    after startup. It would continue checking every 10 seconds until the probe is
    successful or until there are 60 failed attempts. The effect is that the container
    has 10 minutes to finish initialization and respond to a probe successfully. If
    the container does not have a successful probe in that time, it will be restarted.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这个配置，Kubernetes 将在启动后 30 秒开始检查容器。它将每 10 秒检查一次，直到探针成功或尝试达到 60 次失败。其效果是容器有 10
    分钟的时间完成初始化并成功响应探针。如果容器在此时间内未能通过探针检查，它将被重启。
- en: As soon as the container has one successful probe, Kubernetes will switch to
    the configuration for `livenessProbe`. Because we didn’t override any timing parameters,
    this will transition to a probe every 10 seconds, with three consecutive failed
    probes leading to a restart. We give the container 10 minutes to be live initially,
    but after that we will allow no more than 30 seconds before restarting it.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦容器成功通过一次探针，Kubernetes 就会切换到 `livenessProbe` 配置。因为我们没有重写任何定时参数，所以这将每 10 秒进行一次探针检查，连续三次探针失败将导致重启。我们最初为容器提供
    10 分钟的存活时间，但之后将在 30 秒内没有响应时重启容器。
- en: The fact that the `startupProbe` is defined completely separately means that
    it is possible to create a different check for startup from the one used for liveness.
    Of course, it’s important to choose wisely so that the container doesn’t pass
    its startup probe before the liveness probe would also pass, because that would
    result in inappropriate restarts.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`startupProbe`被完全独立定义，这意味着可以为启动创建与活跃检查不同的检查。当然，重要的是要明智选择，以确保容器不会在活跃探针通过之前就通过其启动探针，因为那样会导致不适当的重启。'
- en: Readiness Probes
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 就绪探针
- en: The third probe purpose is to check the *readiness* of the Pod. The term *readiness*
    might seem redundant with the startup probe. However, even though completing initialization
    is an important part of readiness for a piece of software, an application component
    might not be ready to do work for many reasons, especially in a highly available
    microservice architecture where components can come and go at any time.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个探针的目的是检查Pod的*就绪性*。*就绪性*这个术语可能与启动探针显得有些重复。然而，尽管完成初始化是软件就绪性的一个重要部分，一个应用组件可能由于多种原因无法准备好执行工作，尤其是在一个高可用的微服务架构中，组件可能随时进出。
- en: Rather than being used for initialization, readiness probes should be used for
    any case in which the container cannot perform any work because of a failure outside
    its control. It may be a temporary situation, as retry logic somewhere else could
    fix the failure. For example, an API that relies on an external database might
    fail its readiness probe if the database is unreachable, but that database might
    return to service at any time.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 就绪探针应当用于任何容器无法执行工作因为超出其控制的故障的情况，而不是用于初始化。这个问题可能是暂时的，因为其他地方的重试逻辑可能会修复这个故障。例如，一个依赖外部数据库的API如果数据库无法访问，可能会失败其就绪探针，但该数据库可能随时恢复服务。
- en: This also creates a valuable contrast with startup and liveness probes. As we
    examined earlier, Kubernetes will restart a container if it fails the configured
    number of startup or liveness probes. But it makes no sense to do that if the
    issue is a failed or missing external dependency, given that restarting the container
    won’t fix whatever is wrong externally.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这也与启动和活跃探针形成了有价值的对比。如前所述，Kubernetes将在容器未通过配置的启动或活跃探针次数时重启容器。但如果问题是外部依赖失败或缺失，重启容器毫无意义，因为重启容器无法解决外部的问题。
- en: At the same time, if a container is missing a required external dependency,
    it can’t do work, so we don’t want to send any work to it. In that situation,
    the best thing to do is to leave the container running and give it an opportunity
    to reestablish the connections it needs, but avoid sending any requests to it.
    In the meantime, we can hope that somewhere in the cluster another Pod for the
    same Deployment is working as expected, making our application as a whole resilient
    to a localized failure.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，如果一个容器缺少所需的外部依赖项，它就无法执行工作，因此我们不希望向它发送任何工作。在这种情况下，最好的做法是保持容器运行，并给予它重新建立所需连接的机会，但避免向它发送任何请求。与此同时，我们可以希望集群中其他地方有一个相同部署的Pod正常工作，使我们的应用整体对局部故障具有弹性。
- en: This is exactly how readiness probes work in Kubernetes. As we saw in [Chapter
    9](ch09.xhtml#ch09), a Kubernetes Service continually watches for Pods that match
    its selector and configures load balancing for its cluster IP that routes traffic
    to those Pods. If a Pod reports itself as not ready, the Service will stop routing
    traffic to it, but `kubelet` will not trigger any other action such as a container
    restart.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是Kubernetes中就绪探针的工作原理。如我们在[第9章](ch09.xhtml#ch09)中所看到的，Kubernetes服务持续监视与其选择器匹配的Pod，并为其集群IP配置负载均衡，将流量路由到这些Pod。如果Pod报告自己未就绪，服务将停止将流量路由到它，但`kubelet`不会触发任何其他操作，如容器重启。
- en: 'Let’s illustrate this situation. We want to have individual control over Pod
    readiness, so we’ll use a somewhat contrived example rather than a real external
    dependency to determine readiness. We’ll deploy a set of NGINX Pods, this time
    with a corresponding Service:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来举一个例子。我们希望对Pod的就绪性进行单独控制，因此我们将使用一个稍微做作的例子，而不是一个真实的外部依赖来决定就绪性。我们将部署一组NGINX
    Pod，并且这次会有一个相应的服务：
- en: '*nginx-ready.yaml*'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*nginx-ready.yaml*'
- en: '[PRE19]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This Deployment keeps its `livenessProbe` as an indicator that NGINX is working
    correctly and adds a `readinessProbe`. The Service definition is identical to
    what we saw in [Chapter 9](ch09.xhtml#ch09) and will route traffic to our NGINX
    Pods.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 此部署将其`livenessProbe`保留为NGINX正常工作的指标，并添加了一个`readinessProbe`。服务定义与[第9章](ch09.xhtml#ch09)中看到的完全相同，并将流量路由到我们的NGINX
    Pod。
- en: 'This file has already been written to */opt*, so we can apply it to the cluster:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 此文件已写入*/opt*，因此我们可以将其应用到集群中：
- en: '[PRE20]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'After these Pods are up and running, they stay running because the liveness
    probe is successful:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这些Pod运行后会保持运行状态，因为存活探针成功：
- en: '[PRE21]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In addition, the Service we created has been allocated a cluster IP:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们创建的服务已分配了一个集群IP：
- en: '[PRE22]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'However, we aren’t able to use that IP address to reach any Pods:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们无法使用该IP地址访问任何Pod：
- en: '[PRE23]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This is because, at the moment, there is nothing for NGINX to serve on the
    */ready* path, so it’s returning `404`, and the readiness probe is failing. A
    detailed inspection of the Pod shows that it is not ready:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为当前NGINX在*/ready*路径上没有内容可提供，因此返回`404`，就绪探针失败。对Pod的详细检查显示它尚未准备就绪：
- en: '[PRE24]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'As a result, the Service does not have any Endpoints to which to route traffic:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，服务没有任何端点可用于路由流量：
- en: '[PRE25]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Because the Service has no Endpoints, it has configured `iptables` to reject
    all traffic:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 因为服务没有端点，已配置`iptables`拒绝所有流量：
- en: '[PRE26]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: To fix this, we’ll need at least one Pod to become ready to ensure that NGINX
    has something to serve on the */ready* path. We’ll use the container’s hostname
    to keep track of which Pod is serving our request.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 要解决此问题，我们需要至少一个Pod准备就绪，以确保NGINX有内容可以提供给*/ready*路径。我们将使用容器的主机名来跟踪哪个Pod正在处理我们的请求。
- en: 'To make one of our Pods ready, let’s first get the list of Pods again, just
    to have the Pod names handy:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 要使其中一个Pod准备就绪，让我们首先再次获取Pod列表，只是为了方便获取Pod名称：
- en: '[PRE27]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now, we’ll choose one and make it report that it is ready:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将选择一个并使其报告为准备就绪：
- en: '[PRE28]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Our Service will start to show a valid Endpoint:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的服务将开始显示一个有效的端点：
- en: '[PRE29]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Even better, we can now reach an NGINX instance via the cluster IP, and the
    content corresponds to the hostname:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的是，我们现在可以通过集群IP访问NGINX实例，内容与主机名对应：
- en: '[PRE30]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Note the `/ready` at the end of the URL so the response is the hostname. If
    we run this command many times, we’ll see that the hostname is the same every
    time. This is because the one Pod that is passing its readiness probe is handling
    all of the Service traffic.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 注意URL末尾的`/ready`，因此响应是主机名。如果多次运行此命令，我们会看到每次主机名都相同。这是因为通过存活探针的唯一Pod正在处理所有服务流量。
- en: 'Let’s make the other two Pods ready as well:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也使其他两个Pod变为准备就绪状态：
- en: '[PRE31]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Our Service now shows all three Endpoints:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的服务现在展示所有三个端点：
- en: '[PRE32]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Running the `curl` command multiple times shows that the traffic is now being
    distributed across multiple Pods:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 多次运行`curl`命令显示流量现在分布在多个Pod之间：
- en: '[PRE33]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The embedded command `$(seq 1 5)` returns the numbers one through five, causing
    the `for` loop to run `curl` five times. If you run this same `for` loop several
    times, you will see a different distribution of hostnames. As described in [Chapter
    9](ch09.xhtml#ch09), load balancing is based on a random uniform distribution
    wherein each endpoint has an equal chance of being selected for each new connection.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入命令`$(seq 1 5)`返回数字一至五，导致`for`循环运行`curl`五次。如果多次运行相同的`for`循环，你将看到主机名的不同分布。如[第9章](ch09.xhtml#ch09)所述，负载均衡基于随机均匀分布，每个端点被选中作为新连接的概率相等。
- en: A good practice is to offer an HTTP readiness endpoint for each application
    that checks the current state of the application and its dependencies and returns
    an HTTP success code (such as `200`) if the component is healthy, and an HTTP
    error code (such as `500`) if not. Some application frameworks such as Spring
    Boot provide application state management that automatically exposes liveness
    and readiness endpoints.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 一个良好的实践是为每个应用程序提供一个HTTP准备就绪端点，检查应用程序及其依赖项的当前状态，并在组件健康时返回HTTP成功代码（如`200`），否则返回HTTP错误代码（如`500`）。某些应用框架（如Spring
    Boot）提供自动公开存活和就绪端点的应用程序状态管理。
- en: Final Thoughts
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结思路
- en: Kubernetes offers the ability to check on our containers and make sure they
    are working as expected, not just that the process is running. These probes can
    include any arbitrary command run inside the container, verifying that a port
    is open for TCP connections, or that the container responds correctly to an HTTP
    request. To build resilient applications, we should define both a liveness probe
    and a readiness probe for each application component. The liveness probe is used
    to restart an unhealthy container; the readiness probe determines whether the
    Pod can handle Service traffic. Additionally, if a component needs extra time
    for initialization, we should also define a startup probe to make sure that give
    it the required initialization time while responding quickly to failure as soon
    as initialization is complete.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 提供了检查我们的容器并确保它们按预期工作的能力，而不仅仅是进程正在运行。这些探针可以包括在容器内运行任意命令，验证端口是否开放以进行
    TCP 连接，或者容器是否正确响应 HTTP 请求。为了构建弹性应用程序，我们应为每个应用程序组件定义一个存活探针和一个就绪探针。存活探针用于重新启动不健康的容器；就绪探针确定
    Pod 是否能处理服务流量。此外，如果组件需要额外的初始化时间，我们还应定义一个启动探针，以确保在初始化完成后能够给予其所需的初始化时间，并在初始化完成后迅速响应失败。
- en: Of course, for our containers to run as expected, other containers in the cluster
    must also be well behaved, not using too many of the cluster’s resources. In the
    next chapter, we’ll look at how we can limit our containers in their use of CPU,
    memory, disk space, and network bandwidth, as well as how we can control the maximum
    amount of total resources available to a user. This ability to specify limits
    and quotas is important to ensure that our cluster can support multiple applications
    with reliable performance.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，为了使我们的容器按预期运行，集群中的其他容器也必须表现良好，不能使用过多的集群资源。在下一章中，我们将看看如何限制我们的容器在使用 CPU、内存、磁盘空间和网络带宽方面，以及如何控制用户可用的总资源量。指定限制和配额的能力对于确保我们的集群能够支持多个应用程序并保持可靠的性能至关重要。
