- en: '**6'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**6'
- en: 'GENERATIVE AI: AI GETS CREATIVE**'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式人工智能：人工智能变得具有创造力**
- en: '![Image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/common.jpg)'
- en: '[*Generative AI*](glossary.xhtml#glo47) is an umbrella term for models that
    create novel output, either independently (randomly) or based on a prompt supplied
    by the user. Generative models do not produce labels but text, images, or even
    video. Under the hood, generative models are neural networks built from the same
    essential components.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[*生成式人工智能*](glossary.xhtml#glo47)是一个涵盖所有可以创建新颖输出的模型的统称，这些模型可以独立（随机地）或根据用户提供的提示进行生成。生成式模型不会产生标签，而是生成文本、图像甚至视频。从底层来看，生成式模型是由相同基本组件构建的神经网络。'
- en: 'We’ll focus on three kinds of generative AI models: generative adversarial
    networks, diffusion models, and large language models. This chapter covers the
    first two. Large language models have recently turned the world of AI on its head.
    They are the subject of [Chapter 7](ch07.xhtml).'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重点介绍三种生成式人工智能模型：生成对抗网络、扩散模型和大型语言模型。本章介绍前两种。大型语言模型最近彻底改变了人工智能的世界。它们是[第7章](ch07.xhtml)的主题。
- en: '****'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '****'
- en: '[*Generative adversarial networks (GANs)*](glossary.xhtml#glo46) consist of
    two separate neural networks trained together. The first network is the [*generator*](glossary.xhtml#glo49).
    Its task is to learn how to create fake inputs for the [*discriminator*](glossary.xhtml#glo32).
    The discriminator’s task is to learn how to differentiate between fake and real
    inputs. The goal of training the two networks together is that the generator becomes
    better at faking out the discriminator while the discriminator tries its best
    to differentiate real from fake.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[*生成对抗网络 (GANs)*](glossary.xhtml#glo46)由两个分开的神经网络组成，它们一起训练。第一个网络是[*生成器*](glossary.xhtml#glo49)，它的任务是学习如何为[*判别器*](glossary.xhtml#glo32)创建假输入。判别器的任务是学习如何区分真假输入。训练这两个网络的目标是使生成器更好地欺骗判别器，而判别器则尽力区分真假。'
- en: At first, the generator is terrible. It outputs noise, and the discriminator
    has no difficulty distinguishing between real and fake. However, the generator
    improves over time, making the discriminator’s job increasingly harder; this in
    turn pushes the discriminator to become a better real versus fake detector. When
    training is declared complete, the discriminator is usually discarded, and the
    now-trained generator is used to produce new output sampled randomly from the
    learned space of the training data.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 一开始，生成器非常糟糕。它输出的是噪声，判别器很容易区分真假。然而，随着时间的推移，生成器逐渐改善，判别器的任务也变得越来越困难；这反过来又促使判别器变得更擅长于区分真假。当训练完成时，判别器通常会被丢弃，经过训练的生成器将用于从训练数据的学习空间中随机采样并生成新的输出。
- en: I haven’t specified *what* the training data is, because all we need to know
    for now is that a GAN is constructed from two competing (adversarial) networks.
    For most applications, it’s the generator we want when all is said and done.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我没有具体说明*训练数据*是什么，因为目前我们只需要知道的是，GAN是由两个竞争（对抗）的网络构成的。在大多数应用中，最终我们所需要的是生成器。
- en: 'Structurally, we can imagine a GAN like the blocks in [Figure 6-1](ch06.xhtml#ch06fig01).
    (I’ll explain the random vector part in time.) Conceptually, we see that the discriminator
    accepts two kinds of inputs: real data and the output of the generator. The discriminator’s
    output is a label: “Real” or “Fake.” Standard neural network training using backpropagation
    and gradient descent trains the generator and discriminator together, but not
    simultaneously.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 从结构上看，我们可以将GAN想象成[图6-1](ch06.xhtml#ch06fig01)中的块。（我会在适当的时候解释随机向量部分。）从概念上讲，我们看到判别器接受两种类型的输入：真实数据和生成器的输出。判别器的输出是一个标签：“真实”或“假”。标准的神经网络训练使用反向传播和梯度下降，同时训练生成器和判别器，但不是同时进行的。
- en: '![Image](../images/ch06fig01.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch06fig01.jpg)'
- en: '*Figure 6-1: Conceptualizing the architecture of a generative adversarial network*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6-1：生成对抗网络架构的概念化*'
- en: 'For example, training with a minibatch of real data—a small subset of the available
    real training data—follows these steps:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，使用一个真实数据的小批量——即可用真实训练数据的一个小子集——进行训练，步骤如下：
- en: Use the generator as it currently is to create a minibatch’s worth of fake data.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用当前的生成器来创建一个小批量的假数据。
- en: Grab a minibatch’s worth of real data from the training set.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从训练集中获取一个小批量的真实数据。
- en: Unfreeze the discriminator’s weights so gradient descent can update them.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解冻判别器的权重，以便梯度下降可以更新它们。
- en: Pass the fake and real samples through the discriminator with labels 0 and 1,
    respectively.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将假样本和真实样本分别通过鉴别器，标记为 0 和 1。
- en: Use backpropagation to take a gradient descent step to update the discriminator’s
    weights.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用反向传播进行梯度下降步骤，以更新鉴别器的权重。
- en: Freeze the discriminator so the generator can be updated without altering the
    discriminator.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 冻结鉴别器，使得生成器可以在不改变鉴别器的情况下进行更新。
- en: Create a minibatch’s worth of generator inputs (the random vector in [Figure
    6-1](ch06.xhtml#ch06fig01)).
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个小批量的生成器输入（即[图 6-1](ch06.xhtml#ch06fig01)中的随机向量）。
- en: Pass the generator inputs through the combined model to update the generator’s
    weights. Mark each of the generator inputs as being real.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将生成器的输入通过合成模型以更新生成器的权重。将每个生成器输入标记为真实。
- en: Repeat from step 1 until the full model is trained.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从步骤 1 开始，直到整个模型训练完成。
- en: The algorithm first updates the discriminator’s weights using the generator
    as it currently is (step 5), then freezes them (step 6) so the generator’s weights
    can be updated without altering the discriminator. This approach is necessary
    because we want the output of the discriminator—the “Real” or “Fake” labels—to
    update the generator portion. Notice that the generator update marks all the fake
    images as real. Doing this scores the generator by how real the fake inputs appear
    to the discriminator.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 算法首先使用当前的生成器更新鉴别器的权重（步骤 5），然后冻结它们（步骤 6），这样生成器的权重可以在不改变鉴别器的情况下更新。这个方法是必要的，因为我们希望鉴别器的输出——“真实”或“假”的标签——更新生成器部分。注意，生成器更新将所有假图像标记为真实。这样做是根据假输入对鉴别器的真实度进行评分，从而评估生成器的表现。
- en: Let’s examine the random vector used as input to the generator. The point of
    a GAN is to learn a representation of the training set that we can think of as
    a data generator, like the data-generating process that produced the real training
    set. However, in this case, the data generator can be viewed as a function that
    takes a random collection of numbers, the random vector, and transforms them into
    an output that might plausibly have come from the training set. In other words,
    the generator acts like a data augmentation device. The random input to the generator
    becomes an example of the training set. In effect, the generator is a proxy for
    the actual data-generating process that created the real training set in the first
    place.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来分析作为生成器输入的随机向量。GAN 的关键是学习一个训练集的表示，我们可以把它当作一个数据生成器，就像生成真实训练集的数据生成过程。然而，在这种情况下，数据生成器可以被看作是一个函数，它接受一组随机数，即随机向量，并将其转化为一个输出，这个输出可能看起来像是来自训练集。换句话说，生成器就像一个数据增强设备。生成器的随机输入成为了训练集的一个示例。实际上，生成器是实际数据生成过程的代理，后者最初创造了真实的训练集。
- en: 'The random vector of numbers is drawn from a probability distribution. Sampling
    from a probability distribution is akin to rolling two dice and asking how likely
    it is that their sum is a seven versus a two. It’s more likely that the sum is
    a seven because there are more ways to add the two numbers and get seven. There’s
    only one way to get two: snake eyes. Sampling from a normal distribution is similar.
    The most common sample returned is the average value of the distribution. Values
    on either side of the average are less likely the further away from the average
    they are, though still possible.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 随机向量是从一个概率分布中抽取的。从概率分布中抽样类似于掷两个骰子，并询问它们的和是七的可能性与是二的可能性哪个更大。和为七的可能性更大，因为有更多种方法可以将两个数字加起来得到七。而得到二的方法只有一种：蛇眼。从正态分布中抽样也类似。返回的最常见样本是分布的平均值。离平均值越远的数值出现的可能性越小，尽管仍然可能出现。
- en: For example, [Figure 6-2](ch06.xhtml#ch06fig02) shows a bar plot of the distribution
    of human heights in inches. The original dataset contained the heights of 25,000
    people, which were then fit into the 30 bins of the figure. The higher the bar,
    the more people fell into that bin.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，[图 6-2](ch06.xhtml#ch06fig02)展示了人体身高分布的条形图。原始数据集包含了 25,000 个人的身高数据，然后将这些数据适配到图中的
    30 个区间。柱子越高，表示该区间内的人数越多。
- en: '![Image](../images/ch06fig02.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/ch06fig02.jpg)'
- en: '*Figure 6-2: The distribution of human height*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6-2：人体身高分布*'
- en: 'Note the shape of the histogram, which looks like a bell—hence its somewhat
    old-fashioned name, the bell curve. Its modern name, the *normal distribution*,
    is due to it showing up so often in nature that it’s the distribution normally
    encountered, especially for data generated by a physical process. From the distribution,
    we see that the height of a randomly selected person will most often be around
    68 inches: more than 10 percent of the sampled population fell into that bin.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 注意直方图的形状，它看起来像一个钟形曲线——因此有了它那有些过时的名字，钟形曲线（bell curve）。它的现代名称是*正态分布*，因为它在自然界中经常出现，成为通常遇到的分布，尤其是那些由物理过程生成的数据。从分布中我们可以看出，随机选择的人的身高通常在68英寸左右：超过10%的样本人口落入了这个区间。
- en: The random vector used by a GAN, also known as the [*noise vector*](glossary.xhtml#glo75),
    works the same way. The average, in this case, is zero, with most samples in the
    range –3 to 3\. Also, each of the *n* elements in the vector follows this range,
    meaning the vector itself is a sample from an *n*-dimensional space, not the one-dimensional
    space of [Figure 6-2](ch06.xhtml#ch06fig02).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: GAN使用的随机向量，也叫做[*噪声向量*](glossary.xhtml#glo75)，工作原理相同。在这种情况下，均值为零，大多数样本的范围在-3到3之间。此外，向量中的每个*n*元素都遵循这个范围，这意味着该向量本身是来自*n*维空间的样本，而不是[图6-2](ch06.xhtml#ch06fig02)中那种一维空间的样本。
- en: The need for labeled datasets is a bane of machine learning. GANs have no such
    restriction. We don’t care what a training sample’s class is, only that it’s an
    instance of real data, regardless of the class label. Of course, we still require
    that the training set reflect the kind of data we want to generate, but the training
    set need not be labeled.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 有标签的数据集是机器学习的一大痛点。生成对抗网络（GAN）没有这样的限制。我们不关心训练样本的类别是什么，只关心它是否是实际数据的实例，不论类别标签是什么。当然，我们仍然要求训练集能反映我们想要生成的数据类型，但训练集不需要带标签。
- en: '****'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '****'
- en: 'Let’s build a generative adversarial network using our old friend, the MNIST
    digits dataset. The generator will learn to transform a random set of 10 numbers
    (meaning *n* is 10) into a digit image. Once trained, we can give the generator
    any collection of 10 values around zero, and the generator will produce a new
    digit image as output, thereby mimicking the process that created the MNIST dataset:
    people writing digits on paper by hand. A trained GAN generator produces an infinite
    supply of the target output.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用我们老朋友——MNIST数字数据集，来构建一个生成对抗网络（GAN）。生成器将学习将一组随机的10个数字（即*n*为10）转换成数字图像。训练完成后，我们可以给生成器任何一组围绕零的10个数值，生成器将输出一个新的数字图像，从而模拟了MNIST数据集的生成过程：人们手写数字在纸上。经过训练的GAN生成器可以源源不断地产生目标输出。
- en: We’ll use a simple GAN based on traditional neural networks to create a generator
    for an infinite supply of MNIST-style digit images. First, we’ll unravel the existing
    MNIST training set so each sample is a 784-dimensional vector, just as we did
    in [Chapter 5](ch05.xhtml). This gives us the real data. To create fake data,
    we need 10-element random vectors that we’ll build by drawing 10 samples from
    a normal distribution with an average value of zero.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个基于传统神经网络的简单GAN，来创建一个用于生成无限量MNIST风格数字图像的生成器。首先，我们将展开现有的MNIST训练集，使每个样本成为一个784维的向量，就像我们在[第5章](ch05.xhtml)中做的那样。这就给了我们真实数据。为了生成假数据，我们需要10元素的随机向量，我们将通过从正态分布中抽取10个样本，生成均值为零的向量。
- en: The generator portion of the model accepts a 10-element noise vector as input
    and produces a 784-element output vector representing the synthesized digit image.
    Recall that the 784 numbers can be rearranged into a 28×28-pixel image. The generator
    model has three hidden layers, with 256, 512, and 1,024 nodes, and an output layer
    of 784 nodes to produce the image. The hidden layer nodes use a modified version
    of the rectified linear unit called a [*leaky ReLU*](glossary.xhtml#glo60). Leaky
    ReLU activations output the input if the input is positive, but if the input is
    negative, the output is a small positive value multiplied by the negative input.
    In other words, they leak a bit. The output layer uses a hyperbolic tangent activation
    function, meaning every one of the 784 output elements will be in the range –1
    to +1\. That’s acceptable. We can scale the values to 0 to 255 when writing an
    image to disk.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的生成器部分接受一个 10 元素的噪声向量作为输入，并生成一个 784 元素的输出向量，代表合成的数字图像。回想一下，这 784 个数字可以重新排列成一个
    28×28 像素的图像。生成器模型有三个隐藏层，分别为 256、512 和 1,024 个节点，并有一个输出层，包含 784 个节点用于生成图像。隐藏层的节点使用一种叫做
    [*leaky ReLU*](glossary.xhtml#glo60) 的修正版本的线性整流单元。Leaky ReLU 激活函数在输入为正时输出输入值，但如果输入为负，输出则为负输入乘以一个小的正值。换句话说，它们会“漏”一点。输出层使用双曲正切激活函数，这意味着每一个
    784 个输出元素的值都会在 -1 到 +1 之间。这是可以接受的。当我们将图像写入磁盘时，可以将这些值缩放到 0 到 255 之间。
- en: 'The generator must map between the random noise vector input and an output
    image. The discriminator must take an image as input, implying a 784-dimensional
    vector. The discriminator has three hidden layers, like the generator, but in
    reverse: 1,024 nodes, then 512 nodes, followed by 256 nodes. The discriminator’s
    output layer has one node with a sigmoid activation function. The sigmoid produces
    values from 0 to 1, which we can interpret as the discriminator’s belief that
    the input is real (output near 1) or fake (output near 0). Notice that the network
    uses nothing more than standard fully connected layers. Advanced GANs use convolutional
    layers, but exploring the details of those networks is outside our scope.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器必须在随机噪声向量输入和输出图像之间进行映射。鉴别器则必须接受一张图像作为输入，这意味着它接受一个 784 维的向量。鉴别器有三个隐藏层，和生成器相似，但顺序相反：先是
    1,024 个节点，再是 512 个节点，最后是 256 个节点。鉴别器的输出层有一个节点，采用 sigmoid 激活函数。Sigmoid 输出值在 0 到
    1 之间，我们可以将其解释为鉴别器对输入是否真实的判断（输出接近 1 表示真实，输出接近 0 表示虚假）。注意，这个网络仅使用了标准的全连接层。高级的 GAN
    使用卷积层，但探讨这些网络的细节超出了我们的讨论范围。
- en: '[Figure 6-3](ch06.xhtml#ch06fig03) shows the generator (top) and discriminator
    (bottom). The symmetry between the two is evident in the numbers of nodes in the
    hidden layers, though notice that the order is reversed in the discriminator.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6-3](ch06.xhtml#ch06fig03) 显示了生成器（上）和鉴别器（下）。两个部分在隐藏层节点数量上的对称性非常明显，尽管请注意，鉴别器中的顺序是反向的。'
- en: '![Image](../images/ch06fig03.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch06fig03.jpg)'
- en: '*Figure 6-3: GAN generator (top) and discriminator (bottom)*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6-3：GAN 生成器（上）与鉴别器（下）*'
- en: The generator accepts a 10-element random vector as input and produces a 784-element
    fake image output vector. The discriminator accepts an image vector, real or fake,
    and outputs a prediction, a number from 0 to 1\. Fake images should produce values
    close to 0 and real images values close to 1\. If the generator is well trained,
    the discriminator will be fooled most of the time, meaning the discriminator’s
    output will be close to 0.5 for all inputs.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器接受一个 10 元素的随机向量作为输入，并生成一个 784 元素的伪图像输出向量。鉴别器接受一张图像向量（无论是真实还是伪造的），并输出一个预测值，这个值在
    0 到 1 之间。伪造的图像应该生成接近 0 的值，而真实图像应该生成接近 1 的值。如果生成器训练得很好，鉴别器大多数时候会被欺骗，这意味着鉴别器的输出对于所有输入都会接近
    0.5。
- en: The entire network is trained for 200 epochs of 468 minibatches each, for a
    total of 93,600 gradient descent steps. We can display samples from the generator
    after each epoch to observe the network as it learns. [Figure 6-4](ch06.xhtml#ch06fig04)
    shows samples after epochs 1, 60, and 200, from left to right.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 整个网络经过 200 次迭代训练，每次迭代包含 468 个小批量，总共 93,600 次梯度下降步骤。我们可以在每次迭代后展示来自生成器的样本，以观察网络在学习过程中的变化。[图
    6-4](ch06.xhtml#ch06fig04) 展示了第 1、60 和 200 次迭代后的样本，从左到右依次排列。
- en: '![Image](../images/ch06fig04.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch06fig04.jpg)'
- en: '*Figure 6-4: Generator output after epochs 1, 60, and 200*'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6-4：生成器在第 1、60 和 200 次迭代后的输出*'
- en: As we’d expect, the generator performs poorly after a single pass through the
    training data, but perhaps not as poorly as we might have thought. Most of the
    generated images look like ones; other digit shapes, like zeros and twos, are
    also present, though noisy.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们预期的那样，生成器在训练数据经过一次训练后表现较差，但可能并不像我们想象的那样糟糕。大多数生成的图像看起来像数字“1”；其他数字形状，如零和二，也有出现，尽管有些噪音。
- en: After 60 epochs, the generator produces a full range of digits. Some are spot
    on, while others are still confused or only partially drawn. After 200 epochs,
    most of the digits are distinct and sharply defined. The generator is trained
    and now available to produce digit images on demand.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 经过60个训练周期，生成器能够生成各种各样的数字。有些非常准确，而有些则仍然模糊不清或只是部分绘制出来。经过200个训练周期后，大多数数字已经清晰明确。生成器已经训练完毕，现在可以按需生成数字图像。
- en: '****'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '****'
- en: Our digit generator will happily create 10,000 new digit images for us, but
    what if we want all those digits to be fours? A random input vector produces a
    random digit, but we don’t get to choose which one. If we select input vectors
    randomly, we can be excused for believing that the mix of output digits will be
    similarly random. I tested that assumption by using the trained generator to create
    1,000 digit images. I then passed those digit images to a convolutional network
    trained on the MNIST dataset. The convolutional network has a test set accuracy
    above 99 percent, giving us confidence in its predictions, assuming the input
    is a digit image. The GAN generator produces realistic digit images, so we’re
    on solid ground.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数字生成器可以很高兴地为我们生成10,000个新的数字图像，但如果我们希望这些图像都是“四”呢？随机输入向量会生成一个随机数字，但我们不能选择具体是哪个数字。如果我们随机选择输入向量，可以理解我们会认为生成的数字输出也是随机的。我通过使用训练好的生成器生成了1,000个数字图像来测试这个假设。然后，我将这些数字图像传递给一个在MNIST数据集上训练的卷积神经网络。该卷积神经网络的测试集准确率超过99%，这让我们对其预测充满信心，前提是输入的是数字图像。GAN生成器可以生成逼真的数字图像，因此我们有充分的理由相信结果是可靠的。
- en: Assuming the generator is acting as we expect, the percentage of each digit
    should, naively, be the same. There are 10 possible digits, so we expect each
    to appear about 10 percent of the time. That’s not what happened. [Table 6-1](ch06.xhtml#ch06tab1)
    shows the actual distribution of occurrences of each digit.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 假设生成器的表现符合我们的预期，那么每个数字的百分比应该大致相同。因为有10个可能的数字，所以我们预期每个数字出现的概率大约是10%。但事实并非如此。[表6-1](ch06.xhtml#ch06tab1)显示了每个数字实际出现的分布情况。
- en: '**Table 6-1:** The Actual Digit Distribution'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**表6-1：** 实际的数字分布'
- en: '| **Digit** | **Percentage** |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| **数字** | **百分比** |'
- en: '| --- | --- |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 0 | 10.3 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 10.3 |'
- en: '| 1 | 21.4 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 21.4 |'
- en: '| 2 | 4.4 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 4.4 |'
- en: '| 3 | 7.6 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 7.6 |'
- en: '| 4 | 9.5 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 9.5 |'
- en: '| 5 | 6.0 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 6.0 |'
- en: '| 6 | 9.1 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 9.1 |'
- en: '| 7 | 14.4 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 14.4 |'
- en: '| 8 | 4.4 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 4.4 |'
- en: '| 9 | 12.9 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 12.9 |'
- en: 'The generator favors ones, followed by sevens, nines, and zeros; eights and
    twos are the least likely outputs. So, not only does the GAN not allow us to select
    the desired digit type, it has definite favorites. Review the leftmost image in
    [Figure 6-4](ch06.xhtml#ch06fig04), showing the epoch 1 samples. Most of those
    digits are ones, so the GAN’s predilection for ones was evident from the beginning
    of training. The GAN learned, but the preponderance of ones is a symptom of a
    problem that sometimes plagues GAN training: namely [*mode collapse*](glossary.xhtml#glo68),
    where the generator learns early on how to create a particularly good example
    or set of examples that fool the discriminator and gets trapped into producing
    only that output and not the desired diversity of images.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器偏向数字“1”，其次是“7”、“9”和“0”；“8”和“2”是最不可能出现的输出。因此，GAN不仅无法让我们选择所需的数字类型，而且它还有明显的偏好。查看[图6-4](ch06.xhtml#ch06fig04)中的最左边图像，展示了第1个周期的样本。大多数数字是“1”，因此从训练开始时，GAN就表现出了偏向“1”的趋势。GAN确实学到了东西，但数字“1”占主导的现象揭示了一个问题，这个问题有时会困扰GAN的训练：即[*模式崩溃*](glossary.xhtml#glo68)，生成器在早期学会了如何生成一个特别好的例子或一组例子，这些例子能够欺骗判别器，从而陷入只生成这一输出的困境，无法产生期望的多样化图像。
- en: We need not throw ourselves on the mercy of a finicky, uncontrollable GAN. Instead,
    we can condition the network during training by passing in an indication of the
    type of digit we want the generator to create. GANs that take this approach are
    known as [*conditional GANs*](glossary.xhtml#glo18). Unlike unconditional GANs,
    they require training sets with labels.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要完全依赖于一个难以控制的GAN。相反，我们可以在训练时对网络进行条件化，传入一个指示符，告诉生成器我们希望生成什么类型的数字。采用这种方法的GAN被称为[*条件GANs*](glossary.xhtml#glo18)。与无条件GAN不同，它们需要带标签的训练集。
- en: In a conditional GAN, the input to the generator is still a random noise vector,
    but attached to it is another vector specifying the desired output class. For
    example, the MNIST dataset has 10 classes, the digits 0 through 9, so the conditional
    vector has 10 elements. If the desired class is the digit 3, the conditional vector
    is all zeros except for element 3, which is set to one. This method of representing
    class information is known as [*one-hot encoding*](glossary.xhtml#glo76) because
    all the elements of the vector are zero except for the element corresponding to
    the desired class label, which is one.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在条件GAN中，生成器的输入仍然是一个随机噪声向量，但它附带了另一个向量，用来指定期望的输出类别。例如，MNIST数据集有10个类别，数字0到9，因此条件向量有10个元素。如果期望的类别是数字3，条件向量除了第3个元素为1，其他元素都是0。这种表示类别信息的方法被称为[*独热编码*](glossary.xhtml#glo76)，因为向量的所有元素都是零，只有与期望类别标签对应的元素为1。
- en: The discriminator also needs the class label. If the input to the discriminator
    is an image, how do we include the class label? One way is to expand the concept
    of one-hot encoding to images. We know that a color image is represented by three
    image matrices, one for the red channel, one for the green channel, and one for
    the blue channel. Grayscale images have only one channel. We can include the class
    label as a set of additional input channels where all the channels are zero except
    for the channel corresponding to the class label, which is one.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器也需要类别标签。如果输入到判别器的是一张图像，我们如何包含类别标签呢？一种方法是将独热编码的概念扩展到图像。我们知道，彩色图像由三个图像矩阵表示，分别对应红色通道、绿色通道和蓝色通道。灰度图像只有一个通道。我们可以将类别标签作为一组额外的输入通道，其中除了与类别标签对应的通道外，其他通道的值都为零，对应的类别标签通道为一。
- en: Including the class label when generating and discriminating between real and
    fake inputs forces each part of the entire network to learn how to produce and
    interpret class-specific output and input. If the class label is 4 and the digit
    produced by the generator looks more like a zero, the discriminator will know
    there’s a class mismatch because it knows about true zeros from the labeled training
    set.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成和区分真实与伪输入时包含类别标签，迫使整个网络的每个部分都学习如何生成和解释特定类别的输出和输入。如果类别标签是4，而生成器生成的数字看起来更像是零，判别器会知道存在类别不匹配，因为它从标记过的训练集中知道真实的零是什么样的。
- en: The benefit of a conditional GAN comes when using the trained generator. The
    user supplies the desired class as a one-hot vector, along with the random noise
    vector used by an unconditional GAN. The generator then outputs a sample based
    on the noise vector, but conditioned on the desired class label. We can think
    of a conditional GAN as a set of unconditional GANs, each trained on a single
    class of images.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 条件GAN的好处在于使用训练好的生成器时。用户提供期望的类别作为一个独热向量，并与无条件GAN使用的随机噪声向量一起输入。然后，生成器基于噪声向量输出一个样本，但该样本是基于期望的类别标签生成的。我们可以将条件GAN看作是多个无条件GAN的集合，每个无条件GAN都训练在单一类别的图像上。
- en: I trained a conditional GAN on the MNIST dataset. For this example, the GAN
    used convolutional layers instead of the fully connected layers used earlier in
    the chapter. I then asked the fully trained generator to produce 10 samples of
    each digit, as shown in [Figure 6-5](ch06.xhtml#ch06fig05).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我在MNIST数据集上训练了一个条件GAN。在这个例子中，GAN使用了卷积层，而不是本章早些时候使用的全连接层。然后，我要求完全训练好的生成器生成每个数字的10个样本，如[图6-5](ch06.xhtml#ch06fig05)所示。
- en: '![Image](../images/ch06fig05.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch06fig05.jpg)'
- en: '*Figure 6-5: The conditional GAN output showing samples for each digit*'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6-5：条件GAN输出，显示每个数字的样本*'
- en: Conditional GANs let us select the desired output class, which unconditional
    GANs cannot do, but what if we want to adjust specific features of the output
    image? For that, we need a controllable GAN.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 条件GAN让我们选择期望的输出类别，而无条件GAN无法做到这一点，但如果我们想调整输出图像的特定特征呢？为此，我们需要一个可控的GAN。
- en: '****'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '****'
- en: Uncontrollable GANs generate images willy-nilly without regard for the class
    label. Conditional GANs introduce class-specific image generation, which is helpful
    if we want to use a GAN to generate synthetic imagery for training other models,
    perhaps to account for a class for which we have relatively few examples. [*Controllable
    GANs*](glossary.xhtml#glo21), on the other hand, allow us to control the appearance
    of specific features in the generated images. When the generator network learns,
    it learns an abstract space that can be mapped to the output images. The random
    noise vector is a point in this space where the number of dimensions is the number
    of elements in the noise vector. Each point becomes an image. Put the same point,
    the same noise vector, into the generator, and the same image will be output.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 不可控的 GAN 会无规则地生成图像，而不考虑类别标签。条件 GAN 引入了类别特定的图像生成，这在我们希望使用 GAN 生成合成图像来训练其他模型时非常有用，特别是当我们想为类别较少的示例生成数据时。另一方面，[*可控
    GAN*](glossary.xhtml#glo21) 允许我们控制生成图像中特定特征的外观。当生成器网络进行学习时，它学习一个可以映射到输出图像的抽象空间。随机噪声向量是这个空间中的一个点，该空间的维度等于噪声向量的元素数量。每个点都会生成一张图像。将相同的点、相同的噪声向量输入生成器时，会输出相同的图像。
- en: Moving through the abstract space represented by the noise vector produces output
    image after output image. Might there be directions in the abstract noise space
    that have meaning for the features in the output image? Here, *feature* means
    something in the image. For example, if the generator produces images of human
    faces, a feature might be whether the face is wearing glasses, has a beard, or
    has red hair.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在噪声向量所表示的抽象空间中移动，会不断生成输出图像。抽象噪声空间中是否存在具有意义的方向，这些方向与输出图像中的特征相关？这里的*特征*指的是图像中的某些元素。例如，如果生成器生成的是人脸图像，那么一个特征可能是脸部是否戴眼镜、是否有胡子或是否有红色头发。
- en: Controllable GANs uncover meaningful directions in the noise space. Moving along
    one of those directions alters the feature related to the direction. Of course,
    the reality is more complex because a single direction might affect multiple features,
    depending on the dimensionality of the noise space and the data learned by the
    generator. In general, smaller noise vectors are more likely to be [*entangled*](glossary.xhtml#glo36),
    meaning single noise vector dimensions affect multiple output features, making
    it difficult to discern interesting directions. Some training techniques and larger
    noise vectors, perhaps with 100 elements instead of the 10 we used earlier, improve
    the model’s chance of assigning interesting feature adjustments to a single direction.
    Ideally, there would be a meaningful feature adjustment for a single noise vector
    element.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 可控 GAN 能够揭示噪声空间中的有意义方向。沿着这些方向移动会改变与该方向相关的特征。当然，现实更为复杂，因为单一方向可能会影响多个特征，这取决于噪声空间的维度和生成器学习到的数据。通常，较小的噪声向量更容易[*纠缠*](glossary.xhtml#glo36)，意味着单一噪声向量的维度会影响多个输出特征，从而使得识别有趣方向变得困难。一些训练技巧和更大的噪声向量（例如，使用
    100 个元素而非我们之前使用的 10 个）可以提高模型将有趣特征调整分配给单一方向的机会。理想情况下，单个噪声向量元素会有一个有意义的特征调整。
- en: Let’s walk through a two-dimensional example to drive the idea home. Learning
    a generator using a two-dimensional noise vector might be difficult, but the concept
    applies to all dimensionalities and is straightforward to illustrate in two dimensions.
    [Figure 6-6](ch06.xhtml#ch06fig06) has what we need.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个二维示例来深入理解这个概念。使用二维噪声向量训练生成器可能会很困难，但这个概念适用于所有维度，并且在二维中容易演示。[图 6-6](ch06.xhtml#ch06fig06)展示了我们所需的内容。
- en: '![Image](../images/ch06fig06.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/ch06fig06.jpg)'
- en: '*Figure 6-6: Moving through a two-dimensional noise space and interpolated
    MNIST digits*'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6-6：在二维噪声空间中移动并插值生成 MNIST 数字*'
- en: The top part of the figure shows a two-dimensional noise space for a generator
    with two inputs, the *x*-coordinate and the *y*-coordinate. Therefore, each point
    in the figure represents an image generated by the GAN. The first image is produced
    from the point at (2, 5) (the circle). A second image comes from the point at
    (6, 1) (the square). The arrow shows a direction through the noise space that
    we somehow learned controls a feature in the output image. If the GAN generates
    faces, it might be that the arrow points in a direction that affects the person’s
    hair color. Moving from the point at (2, 5) to the point at (6, 1) maintains most
    of the output image but changes the hair color from, say, black at (2, 5) to red
    at (6, 1). Points along the arrow represent hair colors intermediate between black
    and red.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图的上部分展示了一个具有两个输入的生成器的二维噪声空间，分别是 *x* 坐标和 *y* 坐标。因此，图中的每个点代表由GAN生成的图像。第一张图像是由点
    (2, 5) 产生的（圆形标记）。第二张图像来自点 (6, 1)（方形标记）。箭头指示通过噪声空间的某个方向，我们以某种方式学习到这个方向控制输出图像中的某个特征。如果GAN生成的是人脸，箭头可能指向一个影响人物发色的方向。从点
    (2, 5) 移动到点 (6, 1) 保持了大部分输出图像，但改变了发色，例如从 (2, 5) 处的黑色变为 (6, 1) 处的红色。箭头上的点代表介于黑色和红色之间的发色。
- en: The bottom of [Figure 6-6](ch06.xhtml#ch06fig06) shows interpolation along the
    third dimension of the GAN we trained to generate digit images. From left to right,
    a three morphs briefly into a nine before becoming a four, as the third element
    of the 10-element noise vector is varied while keeping all the others fixed at
    their initial random values. The noise vector is of relatively low dimensionality,
    implying that it’s unlikely any one dimension is associated with only a single
    digit trait, which is why the whole image changes from an initial three through
    a nine to a four.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6-6](ch06.xhtml#ch06fig06)的底部展示了我们训练的生成对抗网络（GAN）在生成数字图像时沿第三维度的插值。从左到右，数字三短暂变为九，随后变为四，这是因为在保持其他元素初始随机值不变的情况下，调整了10元素噪声向量的第三个元素。噪声向量的维度相对较低，这意味着不太可能有任何一个维度仅仅与单一数字特征相关，这也是为什么整个图像从最初的三经过九变成四的原因。'
- en: Sophisticated GANs can produce realistic yet fake images of human faces. Controllable
    versions learn directions linked to specific facial features. For example, consider
    [Figure 6-7](ch06.xhtml#ch06fig07), which shows two generated fake faces on the
    left and adjusted faces on the right (from Yujun Shen et al., “Interpreting the
    Latent Space of GANs for Semantic Face Editing,” 2019). The adjustments correspond
    to movement through the noise space from the original image position along learned
    directions representing age, glasses, gender, and pose.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 精密的GAN可以生成逼真的假人脸图像。可控版本通过学习与特定面部特征相关的方向来进行调整。例如，考虑[图6-7](ch06.xhtml#ch06fig07)，图中显示了左侧的两个生成假人脸和右侧经过调整的人脸（来源：Yujun
    Shen等人，“解读GAN的潜在空间进行语义面部编辑”，2019年）。这些调整对应于通过噪声空间的移动，从原始图像位置沿着表示年龄、眼镜、性别和姿势的学习方向进行调整。
- en: '![Image](../images/ch06fig07.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/ch06fig07.jpg)'
- en: '*Figure 6-7: Controlling face attributes*'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6-7：控制面部属性*'
- en: The power of controllable GANs is genuinely remarkable, and that the generator
    learns meaningful directions through the noise space is impressive. However, GANs
    are not the only way to create realistic and controllable images. Diffusion models
    likewise generate realistic imagery; moreover, imagery conditioned by user-defined
    text prompts.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 可控GAN的能力确实令人惊叹，生成器能够通过噪声空间学习有意义的方向也令人印象深刻。然而，GAN并不是创建现实且可控图像的唯一方法。扩散模型同样能生成逼真的图像；更重要的是，这些图像可以根据用户定义的文本提示进行条件化。
- en: '****'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '****'
- en: Generative adversarial networks rely on competition between the generator and
    the discriminator to learn to create fake outputs similar to the training data.
    [*Diffusion models*](glossary.xhtml#glo31) represent a competition-free approach
    to the same end.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络依赖于生成器和判别器之间的竞争，来学习生成与训练数据相似的假图像。[*扩散模型*](glossary.xhtml#glo31)则代表了一种无竞争的方式来达到同样的目标。
- en: In a nutshell, training a diffusion model involves teaching it to predict noise
    added to a training image. Inference in a diffusion model involves the opposite,
    turning noise into an image. Great! But what is “noise” when it comes to images?
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，训练扩散模型的过程是教它预测加到训练图像上的噪声。在扩散模型的推理过程中，则是相反的过程，将噪声转化为图像。太棒了！但在图像中，"噪声"到底是什么？
- en: Noise implies randomness, something without structure. You’re in the ballpark
    if you’re thinking of static on a radio or hiss in an audio signal. For a digital
    image, noise means random values added to the pixels. For example, if the pixel
    value should be 127, noise adds or subtracts a small amount so that the value
    becomes, say, 124 or 129\. Random noise added to an image often looks like snow.
    Diffusion models learn how to predict the amount of normally distributed noise
    added to a training image.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 噪声意味着随机性，即没有结构的事物。如果你在想收音机的静电或音频信号中的嘶嘶声，你大致是对的。对于数字图像来说，噪声意味着随机值被添加到像素中。例如，如果像素值应该是127，噪声就会加或减去一个小的量，使得值变成124或129。加入到图像中的随机噪声通常看起来像雪花。扩散模型学习如何预测添加到训练图像中的正态分布噪声量。
- en: We must have several things in place before we train the network. First, we
    need a training dataset. Diffusion models learn from data, like all neural networks.
    As with GANs, labels are not required until we want some say in what the trained
    model will generate.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练网络之前，我们必须准备好几个条件。首先，我们需要一个训练数据集。扩散模型像所有神经网络一样，从数据中学习。与生成对抗网络（GANs）一样，标签在我们希望模型生成特定内容之前是不需要的。
- en: Once we have the training data, we need a neural network architecture. Diffusion
    models are not picky here, but the selected architecture must accept an image
    as input and produce a same-sized image as output. The U-Net architecture mentioned
    briefly in [Chapter 5](ch05.xhtml) is a frequent choice.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了训练数据，就需要一个神经网络架构。扩散模型在这里并不挑剔，但所选架构必须能够接受图像作为输入，并生成同样大小的图像作为输出。在[第5章](ch05.xhtml)中简要提到的U-Net架构是一个常见的选择。
- en: We have data and an architecture; next, we need some way to get the network
    to learn. But learn what? As it happens, forcing the network to learn the noise
    added to an image is all that is required. The math behind this realization isn’t
    trivial. It involves probability theory, but in practice, it boils down to taking
    a training image, adding some known level of normally distributed noise, and comparing
    that known noise to what the model predicts. If the model learns to predict the
    noise successfully, we can later use the model to turn pure noise into an image
    similar to the training data.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有了数据和架构，接下来需要找到一种方法让网络进行学习。但学习什么呢？事实证明，强迫网络学习添加到图像中的噪声就是所需的全部内容。这一认识背后的数学并不简单。它涉及到概率论，但在实际操作中，其核心是取一张训练图像，加入一定程度的正态分布噪声，然后将已知噪声与模型预测的噪声进行比较。如果模型能够成功预测噪声，我们就可以在之后用模型将纯噪声转化为与训练数据相似的图像。
- en: The important part of the previous paragraph is the phrase “known level of normally
    distributed noise.” Normally distributed noise can be characterized by a single
    parameter, a number specifying the level of the noise. Training consists of selecting
    an image from the training set and a level of noise, both at random, and passing
    them as inputs to the network. The output from the network is the model’s estimate
    of the amount of noise. The smaller the difference between the output noise (itself
    an image) and the added noise, the better. Standard backpropagation and gradient
    descent are applied to minimize this difference over minibatches until the model
    is declared trained.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段中重要的部分是“已知程度的正态分布噪声”这一短语。正态分布噪声可以通过一个单一参数来表征，这个数字指定了噪声的程度。训练过程包括从训练集中随机选择一张图像和一个噪声级别，并将它们作为输入传递给网络。网络的输出是模型对噪声量的估计。输出噪声（本身是一个图像）与加入的噪声之间的差异越小越好。通过标准的反向传播和梯度下降方法，逐步减少这个差异，直到模型被认为训练完成。
- en: How noise is added to training images affects how well and how quickly models
    learn. Noise generally follows a fixed [*schedule*](glossary.xhtml#glo88). The
    schedule is such that moving from a current noise level, say noise level 3, to
    the next, level 4, adds a specified amount of noise to the image, where the amount
    of noise depends on a function. If the same amount of noise is added between each
    step, the schedule is linear. However, if the amount of noise added between steps
    depends on the step itself, it is nonlinear and follows some other function.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如何将噪声添加到训练图像中会影响模型学习的效率和速度。噪声通常遵循一个固定的[*时间表*](glossary.xhtml#glo88)。该时间表规定，从当前噪声级别（例如噪声级别3）转到下一个噪声级别4时，会向图像添加一定量的噪声，而噪声量依赖于某个函数。如果每一步之间添加的噪声量相同，则该时间表是线性的。然而，如果每一步之间添加的噪声量取决于该步骤本身，则它是非线性的，并遵循其他某种函数。
- en: Consider [Figure 6-8](ch06.xhtml#ch06fig08), which shows a possible training
    image on the left. Each row shows successive levels of noise added to the training
    image. The top row follows a linear schedule, where moving left to right adds
    the same noise level between each step until the image is almost destroyed. The
    bottom row follows what is known as a cosine schedule, which destroys the image
    less rapidly. This helps diffusion models learn a bit better. For the curious,
    the dapper gentleman in the image is my great-grandfather, Emil Kneusel, circa
    1895.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 参考[图6-8](ch06.xhtml#ch06fig08)，左侧展示了一个可能的训练图像。每一行展示了向训练图像添加的噪声级别。最上面一行遵循线性计划，其中从左到右的每一步都添加相同的噪声级别，直到图像几乎被破坏。最下面一行遵循的是余弦计划，这种方式破坏图像的速度较慢。这有助于扩散模型更好地学习。对于好奇的人来说，图像中的那位穿着考究的绅士是我的曾祖父，埃米尔·克诺伊塞尔，大约1895年拍摄。
- en: '![Image](../images/ch06fig08.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/ch06fig08.jpg)'
- en: '*Figure 6-8: Two ways to turn an image into noise: linear (top) and cosine
    (bottom)*'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6-8：将图像转化为噪声的两种方式：线性（上）和余弦（下）*'
- en: '[Figure 6-8](ch06.xhtml#ch06fig08) presents only nine steps. In practice, diffusion
    models use hundreds of steps, the critical point being that the original image
    is destroyed at the end of the process, leaving only noise. This matters because
    sampling from the diffusion model reverses the process to turn a random noise
    image into a noise-free image. In effect, sampling from the diffusion model moves
    from right to left using the trained network to predict noise that is then subtracted
    to produce the previous image. Repeating this process for all the steps in the
    schedule completes the noise-to-image generation process.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6-8](ch06.xhtml#ch06fig08)仅展示了九个步骤。实际上，扩散模型使用数百个步骤，关键在于，在过程结束时，原始图像被完全破坏，最终只剩下噪声。这一点很重要，因为从扩散模型中采样会反转这一过程，将随机噪声图像转化为无噪声图像。实际上，从扩散模型中采样的过程是从右到左，通过训练好的网络预测噪声，然后将其减去，生成上一步的图像。对计划中的所有步骤重复这一过程，完成从噪声到图像的生成过程。'
- en: '****'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '****'
- en: The description in the previous section can be summarized in two algorithms.
    I encourage you to read through them, but as they are a bit technical, skipping
    ahead to the next section is always an option.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节中的描述可以用两个算法来总结。我鼓励你阅读它们，但由于它们有些技术性，跳到下一节也是一个不错的选择。
- en: 'The forward algorithm trains the diffusion model, and the reverse algorithm
    samples from a trained model during inference to produce output images. Let’s
    begin with the forward algorithm. We repeat the following until we declare the
    model trained:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 前向算法训练扩散模型，反向算法在推理过程中从训练好的模型中采样以生成输出图像。让我们从前向算法开始。我们重复以下过程，直到宣布模型已训练完成：
- en: Pick a training image, *x*[0], at random.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机选择一张训练图像，*x*[0]。
- en: Pick a random time step, *t*, in the range 1 through *T*, the maximum number
    of steps.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在1到*T*的范围内随机选择一个时间步长，*t*，其中*T*是最大步数。
- en: Sample a noise image, *e*, from a standard normal distribution.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从标准正态分布中采样一个噪声图像，*e*。
- en: Define a noisy image, *x*[*t*], using *x*[0], *t*, and *e*.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用*x*[0]、*t*和*e*定义一个噪声图像，*x*[*t*]。
- en: Pass *x*[*t*] through the model and compare the output noise estimate to *e*.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将*x*[*t*]输入模型，并将输出的噪声估计与*e*进行比较。
- en: Apply standard backpropagation and gradient descent to update the model’s weights.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用标准的反向传播和梯度下降方法来更新模型的权重。
- en: The forward algorithm works because there is a straightforward way to get *x*[*t*]
    from *x*[0], the image in the training set, and a randomly selected time step,
    *t*. Here, *T* is the maximum possible time step, at which the training image
    has been turned into pure noise. Typically, *T* is several hundred steps. Recall
    that the diffusion model is trying to learn how to predict the noise in *e*. The
    act of repeatedly forcing the model to get better and better at predicting the
    noise used to corrupt the training image is what lets the reverse step work.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 前向算法之所以有效，是因为有一种直接的方式可以从训练集中的图像*x*[0]和随机选择的时间步长*t*中得到*x*[*t*]。其中，*T*是最大可能的时间步长，此时训练图像已转化为纯噪声。通常，*T*是数百步。回想一下，扩散模型的目标是学习如何预测噪声*e*。反复迫使模型在预测用于破坏训练图像的噪声方面越来越准确，这就是反向步骤能够成功的原因。
- en: 'The reverse algorithm samples from the diffusion model trained by the forward
    algorithm to generate a novel output image, beginning with a pure noise image
    in *x*[*T*] (think the rightmost images in [Figure 6-8](ch06.xhtml#ch06fig08)).
    The diffusion model is used for *T* steps to turn noise into an image by repeating
    the following:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 反向算法从正向算法训练的扩散模型中采样，生成新的输出图像，从纯噪声图像*x*[*T*]开始（可以参考[图6-8](ch06.xhtml#ch06fig08)中的最右边图像）。扩散模型通过重复以下步骤，使用*T*步将噪声转换为图像：
- en: If this isn’t the last step from *x*[1] to *x*[0], sample a noise image, *z*,
    from a standard normal distribution.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果这不是从*x*[*1]到*x*[*0]*的最后一步，从标准正态分布中采样一个噪声图像，*z*。
- en: Create *x*[*t*−1] from *x*[*t*] by subtracting the output of the diffusion model
    from *x*[*t*] and adding *z*.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过从*x*[*t*]中减去扩散模型的输出并加上*z*来创建*x*[*t*−1]。
- en: The reverse algorithm moves from right to left, if thinking in terms of [Figure
    6-8](ch06.xhtml#ch06fig08). Each step to the left is found by subtracting the
    output of the diffusion model using the current image as input, thereby moving
    from time step *t* to the previous time step, *t* – 1\. The standard noise image,
    *z*, ensures that *x*[*t*−1] is a valid sample from the probability distribution
    supplying *x*[*t*−1] from *x*[*t*]. As mentioned, we’re skipping a lot of probability
    theory.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 反向算法从右到左进行，如果按[图6-8](ch06.xhtml#ch06fig08)的方式思考。每一步向左的过程是通过用当前图像作为输入，减去扩散模型的输出，从而从时间步*
    t*移动到前一个时间步*t*–1。标准的噪声图像*z*确保*x*[*t*−1]是一个有效的样本，来自于提供*x*[*t*−1]的概率分布，生成*x*[*t*]。如前所述，我们跳过了大量的概率理论。
- en: The sampling algorithm works because the diffusion model estimates the noise
    in its input. That estimate leads to an estimate of the image that, plausibly,
    created *x*[*t*] from *x*[*t–*1]. Iterating for all *T* steps brings us, ultimately,
    to *x*[0], the output of the network. Notice that unlike our previous networks,
    which had an input and produced an output, diffusion models are run repeatedly,
    each time producing less and less noisy images, until finally they produce an
    image similar to the training data.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 采样算法之所以有效，是因为扩散模型估计了输入中的噪声。这个估计引导我们估算出一个图像，合理地推测这个图像是如何从*x*[*t*]生成*x*[*t–1]的。迭代所有*T*步，最终得到*x*[*0]，即网络的输出。请注意，与我们之前的网络不同，后者有输入并生成输出，扩散模型是反复运行的，每次都生成噪声越来越少的图像，直到最终生成一个与训练数据相似的图像。
- en: '****'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '****'
- en: 'Diffusion models are like standard GANs: unconditional. The image generated
    is not controllable. You might suspect that if a GAN can be conditioned in some
    way to guide the generation process, then a diffusion model might be similarly
    directable. If so, you’re right.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型就像标准的GAN一样：无条件的。生成的图像是不可控的。你可能会怀疑，如果GAN可以通过某种方式进行条件化来引导生成过程，那么扩散模型是否也能以类似的方式进行引导。如果是的话，你是对的。
- en: The GAN we used to generate MNIST-like digit images was conditioned by extending
    the input to the generator with a one-hot vector selecting the desired class label.
    Conditioning a diffusion model isn’t quite that simple, but it is possible to
    supply the network with a signal related to the image during training. Typically,
    that signal is an embedding vector representing a text description of the training
    image’s contents. We briefly encountered embeddings in [Chapter 5](ch05.xhtml)
    and will do so again in [Chapter 7](ch07.xhtml) when discussing large language
    models.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用来生成类似MNIST数字图像的GAN是通过将输入扩展为一个one-hot向量，来选择所需的类别标签，进而条件化生成器。条件化扩散模型则没有那么简单，但确实可以在训练时向网络提供与图像相关的信号。通常，这个信号是一个嵌入向量，代表训练图像内容的文本描述。我们在[第5章](ch05.xhtml)中简要接触过嵌入，之后在[第7章](ch07.xhtml)中讨论大型语言模型时还会再次涉及。
- en: 'All we need to know for now is that a text embedding takes a string like “A
    big red dog” and turns it into a large vector, which we think of as a point in
    a high-dimensional space: a space that has captured meaning and concepts. The
    association of such a text embedding during training while the network is learning
    to predict noise in images conditions the network in much the same way that the
    one-hot class vector conditions a GAN generator.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 目前我们只需要知道的是，文本嵌入将像“一个大红色的狗”这样的字符串转换为一个大的向量，我们可以将其看作是高维空间中的一个点：这个空间捕捉了意义和概念。在训练过程中，文本嵌入的关联作用就像是在网络学习预测图像中的噪声时，条件作用于网络，类似于one-hot类别向量对GAN生成器的条件作用。
- en: After training, the presence of a text embedding when sampling provides a similar
    signal to guide the output image so that it contains elements related to the text.
    At sampling time, the text becomes a prompt, describing the image we want the
    diffusion process to generate.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，当采样时，文本嵌入的存在提供了类似的信号，引导输出图像，使其包含与文本相关的元素。在采样时，文本成为一个提示，描述我们希望扩散过程生成的图像。
- en: Diffusion models typically begin with a random noise image. They need not. If
    we want the output to be similar to an existing image, we can use that image as
    the initial image, with some level of noise added. Samples from that image will
    be, depending on the degree of added noise, more or less similar to it. Now, let’s
    take a tour of conditional diffusion models.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型通常从一张随机噪声图像开始。其实不一定如此。如果我们希望输出图像与现有图像相似，我们可以将该图像作为初始图像，添加一定程度的噪声。根据添加噪声的程度，图像的样本会更多或更少地与其相似。现在，让我们来了解条件扩散模型。
- en: '****'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '****'
- en: Commercial diffusion models, such as DALL-E 2 by OpenAI or Stable Diffusion
    by Stability AI, use the text or image supplied by the user to guide the diffusion
    process toward an output image satisfying the prompt’s requirements. The examples
    shown in this section were generated by Stable Diffusion using the DreamStudio
    online environment. [Figure 6-9](ch06.xhtml#ch06fig09) presents to us Leonardo
    da Vinci’s *Mona Lisa* (upper left) along with five variations of it.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 商业扩散模型，如OpenAI的DALL-E 2或Stability AI的Stable Diffusion，使用用户提供的文本或图像来引导扩散过程，生成满足提示要求的输出图像。本节展示的示例是通过Stable
    Diffusion在DreamStudio在线环境中生成的。[图6-9](ch06.xhtml#ch06fig09)向我们展示了列奥纳多·达·芬奇的*蒙娜丽莎*（左上方）及其五个变体。
- en: '![Image](../images/ch06fig09.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch06fig09.jpg)'
- en: '*Figure 6-9: The* Mona Lisa *as imagined by Stable Diffusion*'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6-9：由Stable Diffusion想象的* 蒙娜丽莎 *'
- en: 'The variations are the products of Stable Diffusion in response to the original
    image and a text prompt:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这些变体是Stable Diffusion根据原始图像和文本提示生成的作品：
- en: '*Portrait of a woman wearing a brown dress in the style of DaVinci, soft, earthen
    colors*'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '*穿棕色连衣裙的女性肖像，达·芬奇风格，柔和的土色调*'
- en: 'The DreamStudio interface lets the user supply an initial image, using a slider
    to set the amount of noise to add, from 0 percent for a pure noise image to 100
    percent for no noise added. (Yes, that seems backward to me, too.) The noisy version
    of the image initializes the diffusion process. The higher the percentage, the
    less noise is added, and the more the initial image influences the final output.
    For the *Mona Lisa*, I used 33 percent. That noise level, along with the prompt
    and a user-selectable style, produced the five variations in [Figure 6-9](ch06.xhtml#ch06fig09).
    The only difference between the variations is the chosen style (top row: anime
    and fantasy art; bottom row: isometric, line art, and photographic).'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: DreamStudio界面允许用户提供初始图像，并使用滑块设置要添加的噪声量，从0%（纯噪声图像）到100%（不添加噪声）。(是的，我也觉得这有点反直觉。)
    图像的噪声版本初始化了扩散过程。百分比越高，添加的噪声越少，初始图像对最终输出的影响越大。对于*蒙娜丽莎*，我使用了33%的噪声。这个噪声级别，再加上提示词和用户可选择的风格，生成了[图6-9](ch06.xhtml#ch06fig09)中的五个变化版本。变化之间唯一的不同是选择的风格（上排：动漫和奇幻艺术；下排：等距画法，线条艺术，和摄影风格）。
- en: The results are impressive. The images were neither painted nor drawn, but diffused
    from a noisy version of the *Mona Lisa* and a text prompt used as a guide to direct
    the diffusion process. It isn’t difficult to appreciate that the ability to generate
    novel images in response to prompts will impact the commercial art world.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 结果令人印象深刻。这些图像既不是画出来的，也不是手工绘制的，而是从*蒙娜丽莎*的噪声版本和作为指导的文本提示中扩散生成的。不难理解，能够根据提示生成新图像的能力将对商业艺术界产生影响。
- en: However, AI image generation isn’t perfect. Errors happen, as demonstrated in
    [Figure 6-10](ch06.xhtml#ch06fig10). I promise I didn’t ask for a five-legged
    border collie, a multi-mouthed *T. rex*, or a picture of a woman like the *Mona
    Lisa* with horribly mutated hands. Diffusion models seem to have particular difficulty
    rendering hands, much like human artists.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，AI图像生成并不完美。错误是难免的，如[图6-10](ch06.xhtml#ch06fig10)所示。我保证我没有要求生成一只五条腿的边境牧羊犬、一只多嘴的*霸王龙*，或者像*蒙娜丽莎*那样，拥有严重畸形手的女性肖像。扩散模型似乎在渲染手部时尤其困难，就像人类艺术家一样。
- en: '![Image](../images/ch06fig10.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch06fig10.jpg)'
- en: '*Figure 6-10: Diffusion model errors*'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6-10：扩散模型错误*'
- en: 'Writing effective prompts has become an art form, one that has already created
    a new kind of job: prompt engineer. The exact form of the text prompt strongly
    influences the image generation process, as does the random noise image initially
    selected. The DreamStudio interface allows users to fix the pseudorandom number
    generator seed, meaning the diffusion process starts with the same noise image
    each time. Fixing the seed while slightly altering the text prompt lets us experiment
    to learn how sensitive the diffusion process can be.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 编写有效的提示语已成为一门艺术，这也催生了一个新兴职业：提示工程师。文本提示的具体形式对图像生成过程有着强烈影响，初始随机噪声图像的选择也同样如此。DreamStudio界面允许用户固定伪随机数生成器种子，这意味着每次扩散过程都从相同的噪声图像开始。通过固定种子并稍微修改文本提示，我们可以进行实验，学习扩散过程的敏感性。
- en: The images in [Figure 6-11](ch06.xhtml#ch06fig11) were generated by permutations
    of the words *ornate*, *green*, and *vase*. (These images are shown in black and
    white in the book, but all are similar shades of green.) The initial noise image
    was the same each time; only the order of the three words varied. Three of the
    vases are similar, but the fourth is quite different. Nonetheless, all four are
    valid exemplars of ornate, green vases.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6-11](ch06.xhtml#ch06fig11)中的图像是通过“华丽的”、“绿色的”和“花瓶”这些词的排列组合生成的。（这些图像在书中以黑白呈现，但所有图像的绿色色调都相似。）每次的初始噪声图像都是相同的，只有这三个词的顺序发生了变化。三个花瓶相似，但第四个则有很大不同。不过，所有四个花瓶都是华丽的绿色花瓶的有效示例。'
- en: '![Image](../images/ch06fig11.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/ch06fig11.jpg)'
- en: '*Figure 6-11: Vases generated by a diffusion model*'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6-11：通过扩散模型生成的花瓶*'
- en: Prompt order and phrasing matter because the embedding vector formed from the
    text prompt differs, even if the prompt words or their meanings are similar. The
    prompts for the first three vases likely landed close to each other in the text
    embedding space, explaining why they look much the same. The last prompt, for
    whatever reason, landed elsewhere, leading to the different qualities of the generated
    image. Interestingly, the prompt for the last image was “ornate, green, vase,”
    the form following grammatical convention.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 提示语的顺序和措辞很重要，因为即使提示词或其含义相似，由文本提示生成的嵌入向量也会有所不同。前三个花瓶的提示语很可能在文本嵌入空间中接近，解释了它们为何如此相似。最后一个提示语由于某种原因落在了其他位置，导致生成图像的特征有所不同。有趣的是，最后一张图像的提示语是“华丽的绿色花瓶”，符合语法规范的形式。
- en: Curious, I altered the prompt “ornate, green, vase,” changing “green” to other
    colors and using the same initial noise image as before. The results are in [Figure
    6-12](ch06.xhtml#ch06fig12). From left to right, the colors specified were red,
    mauve, yellow, and blue. The first three images are similar to the last vase in
    [Figure 6-11](ch06.xhtml#ch06fig11); only the blue vase differs significantly.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 出于好奇，我改变了提示语“华丽的绿色花瓶”，将“绿色”改成了其他颜色，并使用了之前的相同初始噪声图像。结果见[图6-12](ch06.xhtml#ch06fig12)。从左到右，指定的颜色分别是红色、淡紫色、黄色和蓝色。前三张图像与[图6-11](ch06.xhtml#ch06fig11)中的最后一个花瓶相似；只有蓝色花瓶与众不同。
- en: '![Image](../images/ch06fig12.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/ch06fig12.jpg)'
- en: '*Figure 6-12: Generated vases of many colors*'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6-12：生成的各种颜色花瓶*'
- en: I noticed another property of diffusion models during my experiments, namely,
    that the generated images have less noise than the originals. Suppose an input
    image is low resolution and grainy. In that case, the diffusion model’s output
    is higher resolution and clear because the output is not the result of an operation
    applied to the original image but a reimagining of the image using the prompt
    for guidance. Might it be possible to use diffusion models to remove image artifacts
    if absolute fidelity to the original image isn’t strictly required?
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在实验过程中，我注意到扩散模型的另一个特性，即生成的图像比原始图像噪声更少。假设输入图像的分辨率较低且有噪点，那么扩散模型的输出则会具有更高的分辨率且更清晰，因为输出并非对原始图像进行操作的结果，而是根据提示语重新构想的图像。如果对原始图像的绝对保真度要求不严格，是否可以利用扩散模型去除图像伪影？
- en: '[Figure 6-13](ch06.xhtml#ch06fig13) tries to answer this question. The original
    195×256-pixel image upscaled to 586×768 pixels (a factor of 3) is on the left.
    The image was upscaled using a standard image processing program and cubic interpolation.
    The diffusion model output, also 586×768 pixels, is on the right. The diffusion
    model output used the 195×256-pixel original image with 25 percent added noise,
    a photographic style, and the prompt “detailed, original.” The diffusion image
    is better. It’s not identical to the original, but a close copy. I don’t believe
    this approach competes with deep learning–based super-resolution networks, but
    regardless of ultimate utility, it was an interesting application of diffusion
    models.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6-13](ch06.xhtml#ch06fig13)试图回答这个问题。左侧是将原始的195×256像素图像放大到586×768像素（放大因子为3）。这张图像使用标准的图像处理程序和立方插值法进行了放大。右侧是扩散模型输出的图像，大小同样为586×768像素。该扩散模型输出使用了195×256像素的原始图像，添加了25%的噪声，采用了摄影风格，并且提示语为“详细，原始”。扩散图像更好。它并不完全与原始图像相同，但非常接近。我不认为这种方法能够与基于深度学习的超分辨率网络竞争，但无论最终的实用性如何，它都是扩散模型的一个有趣应用。'
- en: '![Image](../images/ch06fig13.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch06fig13.jpg)'
- en: '*Figure 6-13: Diffusion model image enhancement*'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6-13：扩散模型图像增强*'
- en: 'As another example, consider [Figure 6-14](ch06.xhtml#ch06fig14), which shows
    an image of a Western Meadowlark taken at a distance of about 100 meters through
    poor, smoky Colorado air (left). The center image represents a best effort at
    improving the image using a standard image manipulation program (Gimp). The version
    on the right is the output of Stable Diffusion when given the center image with
    a small amount of noise added (about 12 percent) and the following text prompt:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是[图6-14](ch06.xhtml#ch06fig14)，它展示了一张通过约100米远的距离拍摄的西部草地百灵鸟图像，该图像受到科罗拉多州烟雾的影响（左）。中间的图像是使用标准图像处理程序（如Gimp）改善该图像的最佳尝试。右侧的版本是稳定扩散模型的输出，该模型使用了添加了一些噪声（约12%）的中间图像，并且使用了以下文本提示：
- en: '*western meadowlark, highly detailed, high resolution, noise free*'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '*西部草地百灵鸟，高度详细，高分辨率，无噪声*'
- en: '![Image](../images/ch06fig14.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch06fig14.jpg)'
- en: '*Figure 6-14: A diffusion model image enhancement experiment attempting to
    improve a smoke-obscured image of a Western Meadowlark: original (left), best
    effort with a standard image manipulation program (center), enhanced with Stable
    Diffusion (right)*'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6-14：扩散模型图像增强实验，尝试改善一张被烟雾遮挡的西部草地百灵鸟图像：原图（左），使用标准图像处理程序的最佳尝试（中），通过稳定扩散增强后的图像（右）*'
- en: Stable Diffusion didn’t work a miracle, but the output is definitely better
    than the original image.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 稳定扩散模型并没有创造奇迹，但其输出无疑比原始图像要好。
- en: '****'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '****'
- en: 'This chapter explored two kinds of generative networks: generative adversarial
    networks and diffusion models. Both create images from random inputs.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨了两种生成网络：生成对抗网络和扩散模型。两者都通过随机输入生成图像。
- en: GANs jointly train generator and discriminator networks to teach the generator
    to produce output that fools the discriminator. Conditional GANs use class labels
    during training and generation to direct the generator toward outputs that are
    members of a user-specified class. Controllable GANs learn directions through
    the noise vector space related to essential features of the generated output,
    such that movement along those directions predictably alters the output image.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: GAN（生成对抗网络）联合训练生成器和判别器网络，教会生成器生成能够欺骗判别器的输出。条件GAN在训练和生成过程中使用类别标签，引导生成器产生属于用户指定类别的输出。可控GAN通过与生成输出的基本特征相关的噪声向量空间学习方向，使得沿这些方向的移动可以可预测地改变输出图像。
- en: Diffusion models learn to predict the amount of noise in an image. Training
    a diffusion model involves feeding it clean training images that are intentionally
    made noisy by a known amount. The model’s prediction and the known added noise
    are used to update the model’s weights. Conditional diffusion models associate
    an embedding, usually from a text description of the training image content, with
    the noise so that at generation time, the model is directed to images containing
    elements associated with the user’s text prompt. Variations are generated if an
    existing image, with some level of noise added, is used in place of the pure random
    initial image.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型学习预测图像中的噪声量。训练扩散模型时，会给模型提供干净的训练图像，并故意加入已知数量的噪声。模型的预测与已知添加的噪声一起，用于更新模型的权重。条件扩散模型将嵌入信息，通常来自训练图像内容的文本描述，与噪声相关联，以便在生成时，模型能够生成包含与用户文本提示相关的元素的图像。如果使用某个现有图像，并添加一定程度的噪声来替代纯随机初始图像，则会生成变体。
- en: The introduction mentioned three kinds of generative AI models. The last one,
    large language models, is presently threatening to profoundly alter the world
    at a level equal to the industrial revolution, if not the wheel and fire, as some
    AI practitioners claim. Such consequential claims require us to pay attention.
    Therefore, let’s move on to what might very well be true AI at last.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 引言中提到了三种生成式 AI 模型。最后一种，大型语言模型，目前正威胁着以工业革命的规模，甚至是像某些 AI 从业者所说的轮子和火的水平，深刻改变世界。这种重大的声明要求我们给予关注。因此，让我们继续探讨可能最终会成为真正
    AI 的内容。
- en: '**KEY TERMS**'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**关键词**'
- en: conditional GAN, controllable GAN, diffusion model, discriminator, entangled,
    generative adversarial network (GAN), generative AI, generator, leaky ReLU, mode
    collapse, noise vector, one-hot encoding, schedule
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 条件 GAN、可控 GAN、扩散模型、判别器、纠缠、生成对抗网络（GAN）、生成式 AI、生成器、泄漏的 ReLU、模式坍塌、噪声向量、独热编码、调度
