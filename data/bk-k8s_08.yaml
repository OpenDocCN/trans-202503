- en: '6'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '6'
- en: WHY KUBERNETES MATTERS
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么KUBERNETES很重要
- en: '![image](../images/common01.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/common01.jpg)'
- en: Containers enable us to transform the way we package and deploy application
    components, but orchestration of containers in a cluster enables the real advantage
    of a containerized microservice architecture. As described in [Chapter 1](ch01.xhtml#ch01),
    the main benefits of modern application architecture are scalability, reliability,
    and resiliency, and all three of those benefits require a container orchestration
    environment like Kubernetes in order to run many instances of containerized application
    components across many different servers and networks.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 容器使我们能够改变打包和部署应用组件的方式，但在集群中编排容器才能真正发挥容器化微服务架构的优势。正如[第1章](ch01.xhtml#ch01)中所描述，现代应用架构的主要优点是可扩展性、可靠性和弹性，而这三大优点都需要像Kubernetes这样的容器编排环境，才能在多个服务器和网络中运行许多容器化的应用组件实例。
- en: In this chapter, we’ll begin by looking at some cross-cutting concerns that
    exist when running containers across multiple servers in a cluster. We’ll then
    describe the core Kubernetes concepts designed to address those concerns. With
    that introduction complete, we’ll spend the bulk of the chapter actually installing
    a Kubernetes cluster, including important add-on components like networking and
    storage.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将首先讨论在集群中跨多个服务器运行容器时出现的一些跨领域关注点。然后，我们将描述为解决这些关注点而设计的Kubernetes核心概念。在介绍完成后，我们将重点介绍本章的主要内容，即实际安装Kubernetes集群，包括重要的附加组件，如网络和存储。
- en: Running Containers in a Cluster
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在集群中运行容器
- en: The need to distribute our application components across multiple servers is
    not new to modern application architecture. To build a scalable and reliable application,
    we have always needed to take advantage of multiple servers to handle the application’s
    load and preclude a single point of failure. The fact that we are now running
    these components in containers does not change the need for multiple servers;
    we are still ultimately using CPUs and we are still ultimately dependent on hardware.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 将我们的应用组件分布在多个服务器上的需求，对于现代应用架构来说并不新鲜。为了构建可扩展且可靠的应用，我们始终需要利用多个服务器来处理应用负载，并避免单点故障。我们现在将这些组件运行在容器中，并没有改变我们对多台服务器的需求；我们最终仍然使用CPU，并且依赖硬件。
- en: At the same time, a container orchestration environment brings challenges that
    may not have existed with other kinds of application infrastructure. When the
    container is the smallest individual module around which we build our system,
    we end up with application components that are much more self-contained and “opaque”
    from the perspective of our infrastructure. This means that instead of having
    a static application architecture through which we choose in advance what application
    components are assigned to specific servers, with Kubernetes, we try to make it
    possible for any container to run anywhere.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，容器编排环境带来了可能在其他类型的应用基础设施中不存在的挑战。当容器是我们构建系统时最小的单个模块时，我们最终得到的应用组件更加自包含，从我们的基础设施角度来看更具“封闭性”。这意味着，与静态的应用架构不同，在静态架构中我们预先选择将哪些应用组件分配到特定服务器上，而使用Kubernetes时，我们尽量使得任何容器都能在任何地方运行。
- en: Cross-Cutting Concerns
  id: totrans-8
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 跨领域关注点
- en: 'The ability to run any container anywhere maximizes our flexibility, but it
    adds complexity to Kubernetes itself. Kubernetes does not know in advance what
    containers it will be asked to run, and the container workload is continuously
    changing as new applications are deployed or applications experience changes in
    load. To rise to this challenge, Kubernetes needs to account for the following
    design parameters that apply to all container orchestration software, no matter
    what containers are running:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何地方运行任何容器的能力最大化了我们的灵活性，但也增加了Kubernetes本身的复杂性。Kubernetes无法提前知道将要求运行哪些容器，且容器工作负载随着新应用的部署或应用负载变化而不断变化。为了应对这一挑战，Kubernetes需要考虑以下设计参数，这些参数适用于所有容器编排软件，无论运行什么容器：
- en: '**Dynamic scheduling** New containers must be allocated to a server, and allocations
    can change due to configuration changes or failures.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**动态调度** 新的容器必须分配到服务器上，并且由于配置变化或故障，分配可能会发生变化。'
- en: '**Distributed state** The entire cluster must keep information about what containers
    are running and where, even during hardware or network failures.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**分布式状态** 整个集群必须保持关于容器运行状态和位置的信息，即使在硬件或网络故障时也要保持这一信息。'
- en: '**Multitenancy** It should be possible to run multiple applications in a single
    cluster, with isolation for security and reliability.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**多租户** 应该能够在单一集群中运行多个应用程序，同时实现安全性和可靠性的隔离。'
- en: '**Hardware isolation** Clusters must run in cloud environments and on regular
    servers of various types, isolating containers from the differences in these environments.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**硬件隔离** 集群必须在云环境中运行，并且可以运行在各种类型的常规服务器上，实现容器与这些环境之间的隔离。'
- en: The best term to use to refer to these design parameters is *cross-cutting concern*,
    because they apply to any kind of containerized software that we might need to
    deploy, and even to the Kubernetes infrastructure itself. These parameters work
    together with the container orchestration requirements we saw in [Chapter 1](ch01.xhtml#ch01)
    and ultimately drive the Kubernetes architecture and key design decisions.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 用来描述这些设计参数的最佳术语是 *横切关注点*，因为它们适用于我们可能需要部署的任何类型的容器化软件，甚至是 Kubernetes 基础架构本身。这些参数与我们在[第1章](ch01.xhtml#ch01)中看到的容器编排需求一起工作，并最终推动
    Kubernetes 架构和关键设计决策。
- en: Kubernetes Concepts
  id: totrans-15
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Kubernetes 概念
- en: To address these cross-cutting concerns, the Kubernetes architecture allows
    anything to come and go at any time. This includes not only the containerized
    applications deployed to Kubernetes, but also the fundamental software components
    of Kubernetes itself, and even the underlying hardware such as servers, network
    connections, and storage.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些横切关注点，Kubernetes 架构允许任何东西在任何时候进出。这不仅包括部署到 Kubernetes 的容器化应用程序，还包括 Kubernetes
    本身的基本软件组件，甚至包括底层硬件，如服务器、网络连接和存储。
- en: Separate Control Plane
  id: totrans-17
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 分离的控制平面
- en: Obviously, for Kubernetes to be a container orchestration environment, it requires
    the ability to run containers. This ability is provided by a set of worker machines
    called *nodes*. Each node runs a *kubelet* service that interfaces with the underlying
    container runtime to start and monitor containers.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，要让 Kubernetes 成为容器编排环境，它需要能够运行容器。这个能力由一组叫做*节点*的工作机器提供。每个节点运行一个与底层容器运行时接口的
    *kubelet* 服务，用于启动和监控容器。
- en: Kubernetes also has a set of core software components that manage the worker
    nodes and their containers, but these software components are deployed separately
    from the worker nodes. These core Kubernetes software components are together
    referred to as the *control plane*. Because the control plane is separate from
    the worker nodes, the worker nodes can run the control plane, gaining the benefits
    of containerization for the Kubernetes core software components. A separate control
    plane also means that Kubernetes itself has a microservice architecture, which
    allows customization of each Kubernetes cluster. For example, one control plane
    component, the *cloud controller manager*, is used only when deploying Kubernetes
    to a cloud provider, and it’s customized based on the cloud provider used. This
    design provides hardware isolation for application containers and the rest of
    the Kubernetes control plane, while still allowing us to take advantage of the
    specific features of each cloud provider.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 还有一组核心软件组件，负责管理工作节点及其容器，但这些软件组件与工作节点是分开部署的。这些核心 Kubernetes 软件组件统称为*控制平面*。由于控制平面与工作节点分离，工作节点可以运行控制平面，从而让
    Kubernetes 核心软件组件受益于容器化。分离的控制平面也意味着 Kubernetes 本身具有微服务架构，允许对每个 Kubernetes 集群进行定制。例如，一个控制平面组件——*云控制器管理器*，仅在将
    Kubernetes 部署到云提供商时使用，并且它会根据所使用的云提供商进行定制。这样的设计为应用容器和 Kubernetes 控制平面的其他部分提供了硬件隔离，同时仍然可以利用每个云提供商的特定功能。
- en: Declarative API
  id: totrans-20
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 声明式 API
- en: One critical component of the Kubernetes control plane is the *API server*.
    The API server provides an interface for cluster control and monitoring that other
    cluster users and control plane components use. In defining the API, Kubernetes
    could have chosen an *imperative* style, in which each API endpoint is a command
    such as “run a container” or “allocate storage.” Instead, the API is *declarative*,
    providing endpoints such as *create*, *patch*, *get*, and *delete*. The effect
    of these commands is to create, read, update, and delete *resources* from the
    cluster configuration—the specific configuration of each resource tells Kubernetes
    what we want the cluster to do.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 控制平面的一个关键组件是*API 服务器*。API 服务器为集群控制和监视提供接口，其他集群用户和控制平面组件使用它。在定义 API
    时，Kubernetes 可以选择*命令式*风格，其中每个 API 端点都是诸如“运行容器”或“分配存储”的命令。相反，API 是*声明式*的，提供诸如*创建*、*修补*、*获取*和*删除*的端点。这些命令的效果是从集群配置中创建、读取、更新和删除*资源*，每个资源的具体配置告诉
    Kubernetes 我们希望集群执行什么操作。
- en: This declarative API is essential to meet the cross-cutting concerns of dynamic
    scheduling and distributed state. Because a declarative API simply reports or
    updates cluster configuration, reacting to server or network failures that might
    cause a command to be missed is very easy. Consider an example in which the API
    server connection is lost just after an `apply` command is issued to change the
    cluster configuration. When the connection is restored, the client can simply
    query the cluster configuration and determine whether the command was received
    successfully. Or, even easier, the client can just issue the same `apply` command
    again, knowing that as long as the cluster configuration ends up as desired, Kubernetes
    will be trying to do the “right thing” to the actual cluster. This core principle
    is known as *idempotence*, meaning it is safe to issue the same command multiple
    times because it will be applied at most once.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这种声明式 API 对满足动态调度和分布式状态的横切关注点至关重要。因为声明式 API 只报告或更新集群配置，因此很容易对可能导致命令丢失的服务器或网络故障做出反应。考虑一个例子，即使在发出`apply`命令以更改集群配置后，API
    服务器连接丢失。当连接恢复时，客户端只需查询集群配置，并确定命令是否成功接收。或者更简单地，客户端可以再次发出相同的`apply`命令，因为只要集群配置最终符合期望，Kubernetes
    将尝试对实际集群进行“正确的操作”。这个核心原则被称为*幂等性*，意味着可以安全地多次发出相同的命令，因为它最多只会应用一次。
- en: Self-Healing
  id: totrans-23
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 自我修复
- en: Building on the declarative API, Kubernetes is designed to be *self-healing*.
    This means that the control plane components continually monitor both the cluster
    configuration and the actual cluster state and try to bring them into alignment.
    Every resource in the cluster configuration has an associated status and event
    log reflecting how the configuration has actually caused a change in the cluster
    state.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 基于声明式 API，Kubernetes 被设计为*自我修复*。这意味着控制平面组件不断监视集群配置和实际集群状态，并尝试使它们保持一致。集群配置中的每个资源都有一个相关的状态和事件日志，反映了配置如何实际导致集群状态的变化。
- en: The separation of configuration and state makes Kubernetes very resilient. For
    example, a resource representing containers may be in a `Running` state if the
    containers have been scheduled and are actually running. If the Kubernetes control
    plane loses connection to the server on which the containers are running, it can
    immediately set the status to `Unknown` and then work to either reestablish connection
    or treat the node as failed and reschedule the containers.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 配置和状态分离使得 Kubernetes 非常具有弹性。例如，表示容器的资源如果已经被调度并且实际在运行，则可能处于`Running`状态。如果 Kubernetes
    控制平面与运行容器的服务器失去连接，它可以立即将状态设置为`Unknown`，然后努力重新建立连接或将节点视为失败并重新调度容器。
- en: At the same time, using a declarative API and self-healing approach has important
    implications. Because the Kubernetes API is declarative, a “success” response
    to a command means only that the cluster configuration was updated. It does not
    mean that the actual state of the cluster was updated, as it might take time to
    achieve the requested state, or there might be issues that prevent the cluster
    from achieving that state. As a result, we cannot assume that just because we
    created the appropriate resources, the cluster is running the containers we expect.
    Instead, we must watch the status of the resources and explore the event log to
    diagnose any issues that the Kubernetes control plane had in making the actual
    cluster state match the configuration we specified.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，使用声明式 API 和自愈方法具有重要的意义。由于 Kubernetes API 是声明式的，命令的“成功”响应仅意味着集群配置已更新。这并不意味着集群的实际状态已更新，因为可能需要一些时间才能实现请求的状态，或者可能存在一些问题，导致集群无法实现该状态。因此，我们不能仅仅因为创建了适当的资源，就假设集群正在运行我们期望的容器。相反，我们必须监视资源的状态，并查看事件日志，以诊断
    Kubernetes 控制平面在使实际集群状态与我们指定的配置匹配时可能遇到的任何问题。
- en: Cluster Deployment
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集群部署
- en: With some core Kubernetes concepts under our belts, we’ll use the `kubeadm`
    Kubernetes administration tool to deploy a highly available Kubernetes cluster
    across multiple virtual machines.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在掌握了一些 Kubernetes 核心概念后，我们将使用 `kubeadm` Kubernetes 管理工具，在多台虚拟机上部署一个高度可用的 Kubernetes
    集群。
- en: '**CHOOSING A KUBERNETES DISTRIBUTION**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**选择 Kubernetes 发行版**'
- en: Rather than using a particular Kubernetes distribution as we did in [Chapter
    1](ch01.xhtml#ch01), we’ll deploy a “vanilla” Kubernetes cluster using the generic
    upstream repository. This approach gives us the best opportunity to follow along
    with the cluster deployment and will make it easier to explore the cluster in-depth
    in the next several chapters. However, when you’re ready to deploy a Kubernetes
    cluster of your own, especially for production work, consider using a prebuilt
    Kubernetes distribution for ease of management and built-in security. The Cloud
    Native Computing Foundation (CNCF) publishes a set of conformance tests that you
    can use to ensure that the Kubernetes distribution you choose is conformant to
    the Kubernetes specification.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在[第 1 章](ch01.xhtml#ch01)中使用特定的 Kubernetes 发行版不同，我们将使用通用的上游代码库部署一个“原生”的 Kubernetes
    集群。这种方法给我们提供了最好的机会，能够跟随集群的部署过程，并将在接下来的几章中更容易深入探索集群。然而，当你准备好部署自己的 Kubernetes 集群时，特别是在生产环境中，考虑使用一个预构建的
    Kubernetes 发行版，以便于管理并具备内建的安全性。云原生计算基金会（CNCF）发布了一套符合性测试，你可以用来确保你选择的 Kubernetes
    发行版符合 Kubernetes 规范。
- en: Our Kubernetes cluster will be split across four virtual machines, labeled `host01`
    through `host04`. Three of these, `host01` through `host03`, will run control
    plane components, whereas the fourth will act solely as a worker node. We’ll have
    three control plane nodes because that is the smallest number required to run
    a highly available cluster. Kubernetes uses a voting scheme to provide failover,
    and at least three control plane nodes are required; this allows the cluster to
    detect which side should keep running in the event of a network failure. Also,
    to keep the cluster as small as possible for our examples, we’ll configure Kubernetes
    to run regular containers on the control plane nodes even though we would avoid
    doing that for a production cluster.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 Kubernetes 集群将分布在四台虚拟机上，分别标记为 `host01` 到 `host04`。其中三台虚拟机，`host01` 到 `host03`，将运行控制平面组件，而第四台将仅作为工作节点。我们将使用三台控制平面节点，因为这是运行高度可用集群所需的最小数量。Kubernetes
    使用投票机制提供故障转移，至少需要三台控制平面节点；这样，在网络故障发生时，集群能够检测到应该继续运行的节点。此外，为了使集群尽可能小，以便于我们在接下来的示例中使用，我们将配置
    Kubernetes 在控制平面节点上运行常规容器，尽管我们通常会避免在生产集群中这么做。
- en: '**NOTE**'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*The example repository for this book is at* [https://github.com/book-of-kubernetes/examples](https://github.com/book-of-kubernetes/examples).
    *See “Running Examples” on [page xx](ch00.xhtml#ch00lev1sec2) for details on getting
    set up.*'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*本书的示例代码库位于* [https://github.com/book-of-kubernetes/examples](https://github.com/book-of-kubernetes/examples)。*有关设置的详细信息，请参阅
    [第 xx 页](ch00.xhtml#ch00lev1sec2)的“运行示例”部分。*'
- en: Start by following the instructions for this chapter to get all four virtual
    machines up and running, either in Vagrant or AWS. The automated provisioning
    will set up all four machines with `containerd` and `crictl`, so we don’t need
    to do it manually. The automated provisioning script will also set up either `kube-vip`
    or an AWS network load balancer to provide required high-availability functionality,
    as discussed below.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 从本章的说明开始，确保四台虚拟机都启动并运行，无论是在Vagrant中还是在AWS中。自动化配置将为所有四台机器设置`containerd`和`crictl`，因此我们无需手动配置。自动化配置脚本还将设置`kube-vip`或AWS网络负载均衡器，以提供所需的高可用性功能，如下文所述。
- en: '**NOTE**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*You can install Kubernetes automatically using the* extra *provisioning script
    provided with this chapter’s examples. See the README file for this chapter for
    instructions.*'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*你可以使用本章示例提供的额外*provisioning*脚本自动安装Kubernetes。有关说明，请参阅本章的README文件。*'
- en: You’ll need to run commands on each of the four virtual machines, so you might
    want to open terminal tabs for each one. However, the first series of commands
    needs to be run on all of the hosts, so the automation sets up a command called
    `k8s-all` to do that from `host01`. You can explore the content of this script
    in */usr/local/bin/k8s-all* or by looking at the *k8s* Ansible role in the *setup*
    directory of the examples.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要在每个虚拟机上运行命令，因此你可能希望为每个虚拟机打开一个终端标签。但是，第一系列命令需要在所有主机上运行，因此自动化脚本会在`host01`上设置一个名为`k8s-all`的命令来完成这项工作。你可以在*/usr/local/bin/k8s-all*中查看这个脚本的内容，或者查看本示例中*setup*目录下的*k8s*
    Ansible角色。
- en: Prerequisite Packages
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 前提软件包
- en: 'The first step is to make sure the `br_netfilter` kernel module is enabled
    and set to load on boot. Kubernetes uses advanced features of the Linux firewall
    to handle networking across the cluster, so we need this module. Run these two
    commands:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是确保启用`br_netfilter`内核模块并设置为开机加载。Kubernetes使用Linux防火墙的高级功能来处理跨集群的网络通信，因此我们需要这个模块。运行这两个命令：
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The first command ensures that the module is installed for the currently running
    kernel, and the second command adds it to the list of modules to run on boot.
    The slightly odd quoting in the second command ensures that the shell redirection
    happens on the remote hosts.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个命令确保模块已安装在当前运行的内核中，第二个命令将其添加到开机加载模块列表中。第二个命令中的稍微奇怪的引号确保在远程主机上发生shell重定向。
- en: 'Next, in [Listing 6-1](ch06.xhtml#ch06list1), we’ll set some Linux kernel parameters
    to enable advanced network features that are also needed for networking across
    the cluster by using the `sysctl` command:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在[清单 6-1](ch06.xhtml#ch06list1)中，我们将设置一些Linux内核参数，以启用所需的高级网络功能，这些功能也用于通过`sysctl`命令跨集群进行网络通信：
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '*Listing 6-1: Kernel settings*'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 6-1：内核设置*'
- en: 'This command enables the following Linux kernel network features:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令启用以下Linux内核网络功能：
- en: net.ipv4.ip_forward Transfer packets from one network interface to another (for
    example, from an interface inside a container’s network namespace to a host network).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: net.ipv4.ip_forward 将数据包从一个网络接口转发到另一个网络接口（例如，从容器的网络命名空间内的接口到主机网络）。
- en: net.bridge.bridge-nf-call-ip6tables Run IPv6 bridge traffic through the `iptables`
    firewall.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: net.bridge.bridge-nf-call-ip6tables 通过`iptables`防火墙运行IPv6桥接流量。
- en: net.bridge.bridge-nf-call-iptables Run IPv4 bridge traffic through the `iptables`
    firewall.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: net.bridge.bridge-nf-call-iptables 通过`iptables`防火墙运行IPv4桥接流量。
- en: The need for the last two items will become clear in [Chapter 9](ch09.xhtml#ch09)
    when we discuss how Kubernetes provides networking for Services.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最后两项的需求将在[第9章](ch09.xhtml#ch09)中变得更加明确，当我们讨论Kubernetes如何为服务提供网络时。
- en: These `sysctl` changes in [Listing 6-1](ch06.xhtml#ch06list1) do not persist
    after a reboot. The automated scripts do handle making the changes persistent,
    so if you reboot your virtual machines, either run the `extra` provisioning script,
    or run these commands again.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在[清单 6-1](ch06.xhtml#ch06list1)中的这些`sysctl`更改在重启后不会保持。自动化脚本会处理这些更改的持久化，因此如果你重启虚拟机，可以运行`extra`
    provisioning脚本，或者重新运行这些命令。
- en: 'We’ve now finished configuring the Linux kernel to support our Kubernetes deployment
    and are almost ready for the actual install. First we need to install some prerequisite
    packages:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经完成了配置Linux内核以支持Kubernetes部署，准备好进行实际安装。首先，我们需要安装一些前提软件包：
- en: '[PRE2]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `apt-transport-https` package ensures that `apt` can support connecting
    to repositories via secure HTTP. The other two packages are needed for one of
    the cluster add-ons that we’ll install after our cluster is up and running.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`apt-transport-https` 包确保 `apt` 能通过安全 HTTP 协议连接到仓库。其他两个软件包是我们在集群启动并运行后将安装的集群附加组件所需的。'
- en: Kubernetes Packages
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Kubernetes 软件包
- en: 'We can now add the Kubernetes repository to install the `kubeadm` tool that
    will set up our cluster. First, add the GPG key used to check the package signatures:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以添加 Kubernetes 仓库来安装将设置我们集群的 `kubeadm` 工具。首先，添加用于检查软件包签名的 GPG 密钥：
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This command uses `curl` to download the GPG key. It then uses `gpg` to reformat
    it, and then it writes the result to */usr/share/keyrings*. The command line flags
    `fsSL` put `curl` in a mode that behaves better for chained commands, including
    avoiding unnecessary output, following server redirects, and terminating with
    an error if there is a problem.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这条命令使用 `curl` 下载 GPG 密钥。然后它使用 `gpg` 重新格式化密钥，并将结果写入到 */usr/share/keyrings*。命令行标志
    `fsSL` 将 `curl` 设置为一种更适合链式命令的模式，包括避免不必要的输出、跟随服务器重定向，并在出现问题时终止执行。
- en: 'Next, we add the repository configuration:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们添加仓库配置：
- en: '[PRE4]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As before, the quoting is essential to ensure that the command is passed correctly
    via SSH to all the other hosts in the cluster. The command configures `kubernetes-xenial`
    as the distribution; this distribution is used for any version of Ubuntu, starting
    with the older Ubuntu Xenial.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前一样，引号非常重要，确保命令可以通过 SSH 正确传递到集群中的所有其他主机。命令将 `kubernetes-xenial` 配置为发行版；这个发行版用于任何版本的
    Ubuntu，从较旧的 Ubuntu Xenial 开始。
- en: 'After we have created this new repository, we then need to run `apt update`
    on all hosts to download the list of packages:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建完这个新仓库之后，我们需要在所有主机上运行 `apt update` 来下载软件包列表：
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now we can install the packages we need using `apt`:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用 `apt` 安装所需的软件包：
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `source` command loads a file with a variable to install a specific Kubernetes
    version. This file is created by the automated scripts and ensures that we use
    a consistent Kubernetes version for all chapters. You can update the automated
    scripts to choose which Kubernetes version to install.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`source` 命令加载一个带有变量的文件，用于安装特定版本的 Kubernetes。这个文件由自动化脚本创建，确保我们在所有章节中使用一致的 Kubernetes
    版本。你可以更新自动化脚本来选择要安装的 Kubernetes 版本。'
- en: 'The `apt` command installs the following three packages along with some dependencies:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`apt` 命令安装以下三个软件包以及一些依赖项：'
- en: kubelet Service for all worker nodes that interfaces with the container engine
    to run containers as scheduled by the control plane
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: kubelet 服务用于所有工作节点，它与容器引擎接口，按控制平面的调度运行容器。
- en: kubeadm Administration tool that we’ll use to install Kubernetes and maintain
    our cluster
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: kubeadm 是我们用来安装 Kubernetes 并维护集群的管理工具。
- en: kubectl Command line client that we’ll use to inspect our Kubernetes cluster
    and to create and delete resources
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: kubectl 是我们用来检查 Kubernetes 集群并创建、删除资源的命令行客户端。
- en: 'The `kubelet` package starts its service immediately, but because we haven’t
    installed the control plane yet, the service will be in a failed state at first:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubelet` 包会立即启动其服务，但由于我们还没有安装控制平面，服务一开始会处于失败状态：'
- en: '[PRE7]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We need to control the version of the packages we just installed because we
    want to upgrade all of the components of our cluster together. To protect ourselves
    from accidentally updating these packages, we’ll hold them at their current version:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要控制刚刚安装的软件包的版本，因为我们希望将集群的所有组件一起升级。为了防止不小心更新这些软件包，我们将把它们保持在当前版本：
- en: '[PRE8]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This command prevents the standard `apt full-upgrade` command from updating
    these packages. Instead, if we upgrade our cluster, we’ll need to specify the
    exact version that we want by using `apt install`.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这条命令防止标准的 `apt full-upgrade` 命令更新这些软件包。相反，如果我们升级集群，我们需要通过使用 `apt install` 指定我们想要的确切版本。
- en: Cluster Initialization
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 集群初始化
- en: The next command, `kubeadm init`, initializes the control plane and provides
    the `kubelet` worker node service configuration for all the nodes. We’ll run `kubeadm
    init` on one node in our cluster and then use `kubeadm join` on each of the other
    nodes so that they join the existing cluster.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 下一条命令 `kubeadm init` 初始化控制平面，并为所有节点提供 `kubelet` 工作节点服务配置。我们将在集群中的一个节点上运行 `kubeadm
    init`，然后在其他节点上使用 `kubeadm join` 将它们加入现有的集群。
- en: To run `kubeadm init`, we first create a YAML configuration file. This approach
    has a few advantages. It greatly shortens the number of command line flags that
    we need to remember, and it lets us keep the cluster configuration in a repository,
    giving us configuration control over the cluster. We then can update the YAML
    file and rerun `kubeadm` to make cluster configuration changes.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行`kubeadm init`，我们首先创建一个 YAML 配置文件。这种方式有几个优点。它大大减少了我们需要记住的命令行标志数量，而且它让我们可以将集群配置保存在一个版本库中，从而对集群配置进行控制。然后，我们可以更新
    YAML 文件并重新运行`kubeadm`来更改集群配置。
- en: 'The automation scripts for this chapter have populated a YAML configuration
    file in */etc/kubernetes*, so it’s ready to use. The following shows the contents
    of that file:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的自动化脚本已经在*/etc/kubernetes*中填充了一个 YAML 配置文件，所以它已准备好使用。以下是该文件的内容：
- en: '*kubeadm-init.yaml*'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '*kubeadm-init.yaml*'
- en: '[PRE9]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This YAML file has three documents, separated by dashes (`---`). The first
    document is specific to initializing the cluster, the second has more generic
    configuration, and the third is used to provide settings for `kubelet` across
    all the nodes. Let’s look at the purpose of each of these configuration items:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 YAML 文件有三个文档，通过破折号（`---`）分隔。第一个文档是专门用于初始化集群的，第二个文档包含更通用的配置，第三个文档用于提供跨所有节点的`kubelet`设置。我们来看一下每个配置项的用途：
- en: apiVersion / kind Tells Kubernetes about the purpose of each YAML document,
    so it can validate the contents.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: apiVersion / kind 告诉 Kubernetes 每个 YAML 文档的用途，以便它能够验证内容。
- en: bootstrapTokens Configures a secret that other nodes can use to join the cluster.
    The `token` should be kept secret in a production cluster. It is set to expire
    automatically after two hours, so if we want to join more nodes later, we’ll need
    to make another one.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: bootstrapTokens 配置一个密钥，供其他节点用来加入集群。`token`应该在生产集群中保密。它会在两个小时后自动过期，所以如果我们以后想加入更多节点，需要重新生成一个。
- en: nodeRegistration Configuration to pass to the `kubelet` service running on `host01`.
    The `node-ip` field ensures that `kubelet` registers the correct IP address with
    the API server so that the API server can communicate with it. The `taints` field
    ensures that regular containers can be scheduled onto control plane nodes.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: nodeRegistration 配置项，用于传递给在`host01`上运行的`kubelet`服务。`node-ip`字段确保`kubelet`将正确的
    IP 地址注册到 API 服务器，以便 API 服务器能够与之通信。`taints`字段确保常规容器可以被调度到控制平面节点上。
- en: localAPIEndpoint The local IP address that the API server should use. Our virtual
    machine has multiple IP addresses, and we want the API server listening on the
    correct network.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: localAPIEndpoint API 服务器应该使用的本地 IP 地址。我们的虚拟机有多个 IP 地址，我们希望 API 服务器监听正确的网络。
- en: certificateKey Configures a secret that other nodes will use to gain access
    to the certificates for the API server. It’s needed so that all of the API server
    instances in our highly available cluster can use the same certificate. Keep it
    secret in a production cluster.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: certificateKey 配置一个密钥，供其他节点用来获取 API 服务器的证书。这个密钥是必须的，以便在我们高可用集群中的所有 API 服务器实例都可以使用相同的证书。在生产集群中要保密。
- en: networking All containers in the cluster will get an IP address from the `podSubnet`,
    no matter what host they run on. Later, we’ll install a network driver that will
    ensure that every container on all hosts in the cluster can communicate.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: networking 集群中的所有容器都会从`podSubnet`中获取一个 IP 地址，无论它们运行在哪个主机上。稍后，我们将安装一个网络驱动程序，确保集群中所有主机上的容器能够互相通信。
- en: controlPlaneEndpoint The API server’s external address. For a highly available
    cluster, this IP address needs to reach any API server instance, not just the
    first one.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: controlPlaneEndpoint API 服务器的外部地址。对于高可用集群，这个 IP 地址需要能够访问到任何 API 服务器实例，而不仅仅是第一个实例。
- en: serverTLSBootstrap Instructs `kubelet` to use the controller manager’s certificate
    authority to request server certificates.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: serverTLSBootstrap 指示`kubelet`使用控制器管理器的证书授权机构来请求服务器证书。
- en: The `apiVersion` and `kind` fields will appear in every Kubernetes YAML file.
    The `apiVersion` field defines a group of related Kubernetes resources, including
    a version number. The `kind` field then selects the specific resource type within
    that group. This not only allows the Kubernetes project and other vendors to add
    new groups of resources over time, but it also allows updates to existing resource
    specifications while maintaining backward compatibility.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion` 和 `kind` 字段将在每个 Kubernetes YAML 文件中出现。`apiVersion` 字段定义了一组相关的
    Kubernetes 资源，包括版本号。然后，`kind` 字段选择该组中的具体资源类型。这不仅允许 Kubernetes 项目和其他供应商随着时间的推移添加新的资源组，还允许在保持向后兼容的同时更新现有资源的规范。'
- en: '**HIGHLY AVAILABLE CLUSTERS**'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**高可用集群**'
- en: 'The `controlPlaneEndpoint` field is used to configure the most important requirement
    for a highly available cluster: an IP address that reaches all of the API servers.
    We need to establish this IP address immediately when we initialize the cluster
    because it is used to generate certificates with which clients will verify the
    API server’s identity. The best way to provide a cluster-wide IP address depends
    on where the cluster is running; for example, in a cloud environment, using the
    provider’s built-in capability, such as an Elastic Load Balancer (ELB) in Amazon
    Web Services or an Azure Load Balancer, is best.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`controlPlaneEndpoint` 字段用于配置高可用集群的最重要要求：一个可以访问所有 API 服务器的 IP 地址。我们需要在初始化集群时立即设置此
    IP 地址，因为它用于生成客户端验证 API 服务器身份的证书。提供集群范围的 IP 地址的最佳方式取决于集群运行的位置；例如，在云环境中，使用提供商内建的能力（如
    Amazon Web Services 中的弹性负载均衡器（ELB）或 Azure 负载均衡器）是最好的选择。'
- en: Because of the nature of the two different environments, the examples for this
    book use `kube-vip` when running with Vagrant, and ELB when running in Amazon
    Web Services. The top-level *README.md* file in the example documentation has
    more details. The installation and configuration is done automatically so there’s
    nothing more to configure. We can just use `192.168.61.10:6443` and expect traffic
    to get to any of the API server instances running on `host01` through `host03`.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 由于两种不同环境的特性，本书中的示例在使用 Vagrant 运行时使用 `kube-vip`，在使用 Amazon Web Services 运行时使用
    ELB。示例文档中的顶层 *README.md* 文件包含更多细节。安装和配置会自动完成，因此无需进行其他配置。我们可以直接使用 `192.168.61.10:6443`，并期望流量能够到达运行在
    `host01` 至 `host03` 上的任何 API 服务器实例。
- en: 'Because we have the cluster configuration ready to go in a YAML file, the `kubeadm
    init` command to initialize the cluster is simple. We run this command solely
    on `host01`:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们已经准备好集群配置文件（YAML 文件），所以初始化集群的 `kubeadm init` 命令非常简单。我们只需要在 `host01` 上运行此命令：
- en: '[PRE10]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `--config` option points to the YAML configuration file (*kubeadm-init.yaml*)
    that we looked at earlier, and the `--upload-certs` option tells `kubeadm` that
    it should upload the API server’s certificates to the cluster’s distributed storage.
    The other control plane nodes then can download those certificates when they join
    the cluster, allowing all API server instances to use the same certificates so
    that clients will trust them. The certificates are encrypted using the `certificateKey`
    we provided, which means that the other nodes will need this key to decrypt them.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`--config` 选项指向我们之前查看过的 YAML 配置文件（*kubeadm-init.yaml*），而 `--upload-certs` 选项告诉
    `kubeadm` 应该将 API 服务器的证书上传到集群的分布式存储中。其他控制平面节点随后可以在加入集群时下载这些证书，从而使所有 API 服务器实例使用相同的证书，这样客户端就会信任它们。这些证书是使用我们提供的
    `certificateKey` 进行加密的，这意味着其他节点需要此密钥才能解密它们。'
- en: The `kubeadm init` command initializes the control plane’s components on `host01`.
    These components are run in containers and managed by the `kubelet` service, which
    makes them easy to upgrade. Several container images will be downloaded, so the
    command might take a while, depending on the speed of your virtual machines and
    your internet connection.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubeadm init` 命令在 `host01` 上初始化控制平面的组件。这些组件以容器的形式运行，并由 `kubelet` 服务进行管理，这使得它们容易升级。几个容器镜像将被下载，因此根据虚拟机的速度和网络连接的情况，此命令可能需要一段时间。'
- en: Joining Nodes to the Cluster
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将节点加入集群
- en: The `kubeadm init` command prints out a `kubeadm join` command that we can use
    to join other nodes to the cluster. However, the automation scripts have already
    prestaged a configuration file to each of the other nodes to ensure that they
    join as the correct type of node. The servers `host02` and `host03` will join
    as additional control plane nodes, whereas `host04` will join solely as a worker
    node.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubeadm init`命令会输出一个`kubeadm join`命令，我们可以用它将其他节点加入集群。然而，自动化脚本已经将配置文件预先放置到每个其他节点，确保它们以正确类型的节点加入。服务器`host02`和`host03`将作为额外的控制平面节点加入，而`host04`将仅作为工作节点加入。'
- en: 'Here’s the YAML configuration file for `host02` with its specific settings:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这是`host02`的YAML配置文件，带有其特定设置：
- en: '*kubeadm-join.yaml (host02)*'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '*kubeadm-join.yaml（host02）*'
- en: '[PRE11]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This resource has a type of `JoinConfiguration`, but most of the fields are
    the same as the `InitConfiguration` in the *kubeadm-init.yaml* file. Most important,
    the `token` and `certificateKey` match the secret we set up earlier, so this node
    will be able to validate itself with the cluster and decrypt the API server certificates.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 该资源的类型为`JoinConfiguration`，但大部分字段与`kubeadm-init.yaml`文件中的`InitConfiguration`相同。最重要的是，`token`和`certificateKey`与我们之前设置的秘密匹配，因此此节点将能够验证自己并解密API服务器证书。
- en: One difference is the addition of `ignorePreflightErrors`. This section appears
    only when we are installing `kube-vip`, as in that case we need to prestage the
    configuration file for `kube-vip` to the */etc/kubernetes/manifests* directory,
    and we need to tell `kubeadm` that it is okay for that directory to already exist.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 一个不同之处是新增了`ignorePreflightErrors`。这个部分只有在我们安装`kube-vip`时出现，因为在这种情况下，我们需要将`kube-vip`的配置文件预先放置到*/etc/kubernetes/manifests*目录，并且需要告诉`kubeadm`该目录已经存在是可以的。
- en: 'Because we have this YAML configuration file, the `kubeadm join` command is
    simple. Run it on `host02`:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们有这个YAML配置文件，`kubeadm join`命令很简单。在`host02`上运行它：
- en: '[PRE12]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As before, this command runs the control plane components as containers using
    the `kubelet` service on this node, so it will take some time to download the
    container images and start the containers.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前一样，这个命令使用本节点上的`kubelet`服务以容器的方式运行控制平面组件，因此需要一些时间来下载容器镜像并启动容器。
- en: 'When it finishes, run the exact same command on `host03`:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 当它完成时，在`host03`上运行完全相同的命令：
- en: '[PRE13]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The automation script set up the YAML file with the correct IP address for each
    host, so the differences in configuration between each of the hosts is already
    accounted for.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化脚本已经为每个主机设置了正确的IP地址，因此每个主机之间的配置差异已经考虑到。
- en: When this command completes, we’ll have created a highly available Kubernetes
    cluster, with the control plane components running on three separate hosts. However,
    we do not yet have any regular worker nodes. Let’s fix that issue.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 当这个命令完成时，我们将创建一个高可用的Kubernetes集群，控制平面组件在三个独立的主机上运行。然而，我们还没有常规的工作节点。让我们解决这个问题。
- en: 'We’ll begin by joining `host04` as a regular worker node and running exactly
    the same `kubeadm join` command on `host04`, but the YAML configuration file will
    be a little different. Here’s that file:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从将`host04`作为常规工作节点加入开始，并在`host04`上运行完全相同的`kubeadm join`命令，但YAML配置文件会有所不同。以下是该文件：
- en: '*kubeadm-join.yaml (host04)*'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*kubeadm-join.yaml（host04）*'
- en: '[PRE14]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This YAML file is missing the `controlPlane` field, so `kubeadm` configures
    it as a regular worker node rather than a control plane node.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这个YAML文件缺少`controlPlane`字段，因此`kubeadm`将其配置为常规工作节点，而非控制平面节点。
- en: 'Now let’s join `host04` to the cluster:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将`host04`加入集群：
- en: '[PRE15]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This command completes a little faster because it doesn’t need to download
    the control plane container images and run them. We now have four nodes in the
    cluster, which we can verify by running `kubectl` back on `host01`:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令完成得稍微快一些，因为它不需要下载控制平面容器镜像并运行它们。我们现在有四个节点在集群中，可以通过在`host01`上运行`kubectl`来验证：
- en: '[PRE16]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The first command sets an environment variable to tell `kubectl` what configuration
    file to use. The */etc/kubernetes/admin.conf* file was created automatically by
    `kubeadm` when it initialized `host01` as a control plane node. That file tells
    `kubectl` what address to use for the API server, what certificate to use to verify
    the secure connection, and how to authenticate.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个命令设置了一个环境变量，告诉`kubectl`使用哪个配置文件。`/etc/kubernetes/admin.conf`文件是在`kubeadm`初始化`host01`作为控制平面节点时自动创建的。该文件告诉`kubectl`使用哪个地址来访问API服务器，使用哪个证书来验证安全连接，以及如何进行身份验证。
- en: 'The four nodes currently should be reporting a status of `NotReady`. Let’s
    run the `kubectl describe` command to get the node details:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 当前四个节点应该报告状态为`NotReady`。让我们运行`kubectl describe`命令以获取节点详细信息：
- en: '[PRE17]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We haven’t yet installed a network driver for our Kubernetes cluster, and as
    a result, all of the nodes are reporting a status of `NotReady`, which means that
    they won’t accept regular application workloads. Kubernetes communicates this
    by placing a *taint* in the node’s configuration. A taint restricts what can be
    scheduled on a node. We can list the taints on the nodes using `kubectl`:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有为我们的Kubernetes集群安装网络驱动程序，因此所有节点都报告为`NotReady`状态，这意味着它们不会接受常规的应用工作负载。Kubernetes通过在节点配置中放置一个*污点*来传达这一点。污点限制了可以在节点上调度的内容。我们可以使用`kubectl`列出节点上的污点：
- en: '[PRE18]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We select an output format of `json` so that we can use `jq` to print just the
    information we need. Because all the nodes have a status of `NotReady`, they have
    a `not-ready` taint set to `NoSchedule`, which prevents the Kubernetes scheduler
    from scheduling containers onto them.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择`json`格式的输出，以便可以使用`jq`仅打印我们需要的信息。由于所有节点的状态都是`NotReady`，它们都有一个`not-ready`污点，并设置为`NoSchedule`，这会阻止Kubernetes调度器将容器调度到这些节点上。
- en: By specifying `taints` as an empty array in the `kubeadm` configuration, we
    prevented the three control plane nodes from having an additional control plane
    taint. In a production cluster, this taint keeps application containers separate
    from the control plane containers for security reasons, so we would leave it in
    place. For our example cluster, though, it would mean that we need multiple extra
    virtual machines to act as worker nodes, which we don’t want.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在`kubeadm`配置中将`taints`指定为空数组，我们防止了三个控制平面节点有额外的控制平面污点。在生产集群中，这个污点将应用容器与控制平面容器隔离开，以确保安全，因此我们会保留这个污点。不过，在我们的示例集群中，这意味着我们需要多个额外的虚拟机作为工作节点，而我们并不希望这样。
- en: The command `kubectl taint` would allow us to remove the `not-ready` taint manually,
    but the correct approach is to install a network driver as a cluster add-on so
    that the nodes will properly report `Ready`, enabling us to run containers on
    them.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 命令`kubectl taint`允许我们手动移除`not-ready`污点，但正确的方法是安装一个网络驱动程序作为集群附加组件，这样节点将正确报告为`Ready`，从而使我们能够在其上运行容器。
- en: Installing Cluster Add-ons
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装集群附加组件
- en: We’ve installed `kubelet` on four separate nodes and installed the control plane
    on three of those nodes and joined them to our cluster. For the rest, we’ll use
    the control plane to install cluster add-ons. These add-ons are similar to regular
    applications that we would deploy. They consist of Kubernetes resources and run
    in containers, but they provide essential services to the cluster that our applications
    will use.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在四个单独的节点上安装了`kubelet`，并在其中三个节点上安装了控制平面，并将它们加入到我们的集群中。对于剩下的节点，我们将使用控制平面来安装集群附加组件。这些附加组件类似于我们部署的常规应用程序。它们由Kubernetes资源组成，并在容器中运行，但它们为集群提供我们应用程序所需的基本服务。
- en: 'To get a basic cluster up and running, we need to install three types of add-ons:
    a *network driver*, a *storage driver*, and an *ingress controller*. We will also
    install a fourth optional add-on, a *metrics server*.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 要让基础集群启动并运行，我们需要安装三种类型的附加组件：*网络驱动程序*、*存储驱动程序*和*入口控制器*。我们还将安装一个第四个可选附加组件，*度量服务器*。
- en: Network Driver
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 网络驱动程序
- en: Kubernetes networking is based on the Container Network Interface (CNI) standard.
    Anyone can build a new network driver for Kubernetes by implementing this standard,
    and as a result, several choices are available for Kubernetes network drivers.
    We’ll demonstrate different network plug-ins in [Chapter 8](ch08.xhtml#ch08),
    but most of the clusters in this book use the Calico network driver because it
    is the default choice for many Kubernetes platforms.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes网络基于容器网络接口（CNI）标准。任何人都可以通过实现这一标准为Kubernetes构建新的网络驱动程序，因此Kubernetes网络驱动程序有多种选择。我们将在[第8章](ch08.xhtml#ch08)中演示不同的网络插件，但本书中的大多数集群都使用Calico网络驱动程序，因为它是许多Kubernetes平台的默认选择。
- en: 'First, download the primary YAML configuration file for Calico:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，下载Calico的主要YAML配置文件：
- en: '[PRE19]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The `-L` option tells `curl` to follow any HTTP redirects, whereas the `-O`
    option tells `curl` to save the content in a file using the same filename as in
    the URL. The value of the `calico_url` environment variable is set in the `k8s-ver`
    script that also specified the Kubernetes version. This is essential, as Calico
    is sensitive to the specific version of Kubernetes we’re running, so it’s important
    to choose values that are compatible.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '`-L`选项告诉`curl`跟随任何 HTTP 重定向，而`-O`选项则告诉`curl`将内容保存到文件中，文件名与 URL 中的文件名相同。`calico_url`环境变量的值在`k8s-ver`脚本中设置，该脚本还指定了
    Kubernetes 的版本。这是非常重要的，因为 Calico 对我们运行的 Kubernetes 版本非常敏感，因此选择兼容的版本非常关键。'
- en: The primary YAML configuration is written to the local file *tigera-operator.yaml*.
    This refers to the fact that the initial installation is a Kubernetes Operator,
    which then creates all of the other cluster resources to install Calico. We’ll
    explore operators in [Chapter 17](ch17.xhtml#ch17).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 主要的 YAML 配置文件写入本地文件*tigera-operator.yaml*。这指的是初始安装是一个 Kubernetes Operator，之后它会创建所有其他集群资源来安装
    Calico。我们将在[第 17 章](ch17.xhtml#ch17)中探讨 Operator。
- en: 'In addition to this primary YAML configuration, the automated scripts for this
    chapter have added a file called *custom-resources.yaml* that provides necessary
    configuration for our example cluster. We now can tell the Kubernetes API server
    to apply all the resources in these files to the cluster:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这个主要的 YAML 配置文件，本章节的自动化脚本还添加了一个名为*custom-resources.yaml*的文件，为我们的示例集群提供了必要的配置。现在，我们可以告诉
    Kubernetes API 服务器将这些文件中的所有资源应用到集群中：
- en: '[PRE20]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Kubernetes takes a few minutes to download container images and start containers,
    and then Calico will be running in our cluster and our nodes should report a status
    of `Ready`:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 需要几分钟来下载容器镜像并启动容器，之后 Calico 将在我们的集群中运行，节点应该报告为`Ready`状态：
- en: '[PRE21]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Calico works by installing a *DaemonSet*, a Kubernetes resource that tells the
    cluster to run a specific container or set of containers on every node. The Calico
    containers then provide network services for any containers running on that node.
    However, that raises an important question. When we installed Calico in our cluster,
    all of our nodes had a taint that told Kubernetes not to schedule containers on
    them. How was Calico able to run its containers on all the nodes? The answer is
    *tolerations*.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Calico 通过安装一个*DaemonSet*工作，这是一个 Kubernetes 资源，指示集群在每个节点上运行特定的容器或一组容器。Calico
    容器随后为在该节点上运行的任何容器提供网络服务。然而，这引出了一个重要问题。当我们在集群中安装 Calico 时，所有节点都有一个污点，告诉 Kubernetes
    不要在其上调度容器。那么，Calico 是如何在所有节点上运行容器的呢？答案是*容忍*。
- en: 'A toleration is a configuration setting applied to a resource that instructs
    Kubernetes it can be scheduled on a node despite a taint possibly being present.
    Calico specifies a toleration when it adds its DaemonSet to the cluster, as we
    can see with `kubectl`:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 容忍是应用于资源的配置设置，指示 Kubernetes 即使可能存在污点，也可以将该资源调度到节点上。Calico 在将其 DaemonSet 添加到集群时会指定一个容忍设置，正如我们通过`kubectl`所看到的：
- en: '[PRE22]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The `-n` option selects the `calico-system` *Namespace*. Namespaces are a way
    to keep Kubernetes resources separate from one another on a cluster, for security
    reasons as well as to avoid naming collisions. Also, as before, we request JSON
    output and use `jq` to select only the field we’re interested in. If you want
    to see the entire configuration for the resource, use `-o=json` without `jq` or
    use `-o=yaml`.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`-n`选项选择`calico-system`*命名空间*。命名空间是 Kubernetes 用来将集群资源彼此隔离的一种方式，既出于安全原因，也为了避免命名冲突。此外，与之前一样，我们请求
    JSON 输出，并使用`jq`选择我们感兴趣的字段。如果你想查看资源的完整配置，可以使用`-o=json`而不带`jq`，或者使用`-o=yaml`。'
- en: This DaemonSet has three tolerations, and the second one provides the behavior
    we need. It tells the Kubernetes scheduler to go ahead and schedule it even on
    nodes that have a `NoSchedule` taint. Calico then can get itself started before
    the node is ready, and once it’s running, the node changes its status to `Ready`
    so that normal application containers can be scheduled. The control plane components
    needed a similar toleration in order to run on nodes before they show `Ready`.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 DaemonSet 有三个容忍设置，第二个容忍设置提供了我们所需的行为。它告诉 Kubernetes 调度程序即使在节点上存在`NoSchedule`污点，也可以继续调度。这样，Calico
    就可以在节点准备好之前启动，而一旦运行，它会将节点状态更改为`Ready`，从而可以调度正常的应用程序容器。控制平面组件也需要类似的容忍设置，才能在节点显示`Ready`之前运行。
- en: Installing Storage
  id: totrans-146
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 安装存储
- en: The cluster nodes are ready, so if we deployed a regular application, its containers
    would run. However, applications that require persistent storage would fail to
    start because the cluster doesn’t yet have a storage driver. Like network drivers,
    several storage drivers are available for Kubernetes. The Container Storage Interface
    (CSI) provides the standard that storage drivers need to meet to work with Kubernetes.
    We’ll use Longhorn, a storage driver from Rancher; it’s easy to install and doesn’t
    require any underlying hardware like extra block devices or access to cloud-based
    storage.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 集群节点已经准备好，所以如果我们部署一个常规应用，其容器将运行。然而，要求持久存储的应用将无法启动，因为集群还没有存储驱动程序。像网络驱动程序一样，Kubernetes
    有多个存储驱动程序可供选择。容器存储接口（CSI）提供了存储驱动程序与 Kubernetes 配合使用所需满足的标准。我们将使用 Longhorn，这是一个来自
    Rancher 的存储驱动程序；它安装简单，并且不需要额外的硬件支持，如额外的块设备或访问基于云的存储。
- en: 'Longhorn makes use of the iSCSI and NFS software we installed earlier. It expects
    all of our nodes to have the `iscsid` service enabled and running, so let’s make
    sure that’s true on all our nodes:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: Longhorn 利用我们之前安装的 iSCSI 和 NFS 软件。它要求所有节点都启用了并正在运行 `iscsid` 服务，因此我们需要确保所有节点都满足这一要求：
- en: '[PRE23]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We now can install Longhorn on the cluster. The process for installing Longhorn
    looks a lot like Calico. Start by downloading the Longhorn YAML configuration:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以在集群上安装 Longhorn。安装 Longhorn 的过程与 Calico 很相似。首先下载 Longhorn 的 YAML 配置文件：
- en: '[PRE24]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The `longhorn_url` environment variable is also set by the `k8s-ver` script,
    which allows us to ensure compatibility.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '`longhorn_url` 环境变量同样由 `k8s-ver` 脚本设置，这让我们能够确保兼容性。'
- en: 'Install Longhorn using `kubectl`:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `kubectl` 安装 Longhorn：
- en: '[PRE25]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: As before, `kubectl apply` ensures that the resources in the YAML file are applied
    to the cluster, creating or updating them as necessary. The `kubectl apply` command
    supports URLs as the source of the resource it applies to the cluster, but for
    these three installs, we run a separate `curl` command because it’s convenient
    to have a local copy of what was applied to the cluster.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前一样，`kubectl apply` 确保 YAML 文件中的资源被应用到集群中，并根据需要创建或更新它们。`kubectl apply` 命令支持将
    URL 作为资源的来源应用到集群，但对于这三次安装，我们运行一个单独的 `curl` 命令，因为方便拥有一个本地副本来应用到集群中的内容。
- en: Longhorn is now installed on the cluster, which we’ll verify as we explore the
    cluster in the rest of this chapter.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Longhorn 已经安装在集群上，我们将在本章接下来的内容中验证这一点。
- en: Ingress Controller
  id: totrans-157
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 入口控制器
- en: We now have networking and storage, but the networking allows access to containers
    only from within our cluster. We need another service that exposes our containerized
    applications outside the cluster. The easiest way to do that is to use an ingress
    controller. As we’ll describe in [Chapter 9](ch09.xhtml#ch09), an ingress controller
    watches the Kubernetes cluster for *Ingress* resources and routes network traffic.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了网络和存储，但目前的网络仅允许从我们集群内部访问容器。我们还需要一个将容器化应用暴露到集群外部的服务。最简单的方法是使用入口控制器。正如我们在[第9章](ch09.xhtml#ch09)中所描述的，入口控制器监视
    Kubernetes 集群中的 *Ingress* 资源并路由网络流量。
- en: 'We begin by downloading the ingress controller YAML configuration:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先下载入口控制器的 YAML 配置文件：
- en: '[PRE26]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: As in our earlier example, the `ingress_url` environment variable is set by
    the `k8s-ver` script so that we can ensure compatibility. In this case, the URL
    ends in the generic path of *deploy.yaml*, so we use `-o` to provide a filename
    to `curl` to make clear the purpose of the downloaded YAML file.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 和我们之前的例子一样，`ingress_url` 环境变量由 `k8s-ver` 脚本设置，以确保兼容性。在这种情况下，URL 以通用路径 *deploy.yaml*
    结尾，因此我们使用 `-o` 为 `curl` 提供文件名，以明确说明下载的 YAML 文件的用途。
- en: 'Install the ingress controller using `kubectl`:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `kubectl` 安装入口控制器：
- en: '[PRE27]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This creates a lot of resources, but there are two main parts: an NGINX web
    server that actually performs routing of HTTP traffic, and a component that watches
    for changes in Ingress resources in the cluster and configures NGINX accordingly.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这会创建很多资源，但主要有两个部分：一个实际执行 HTTP 流量路由的 NGINX Web 服务器，以及一个监视集群中 Ingress 资源变化并相应配置
    NGINX 的组件。
- en: There’s one more step we need. As installed, the ingress controller tries to
    request an external IP address to allow traffic to reach it from outside the cluster.
    Because we’re running a sample cluster with no access to external IP addresses,
    this won’t work. Instead, we’ll be accessing our ingress controller using port
    forwarding from our cluster hosts. At the moment, our ingress controller is set
    up for this port forwarding, but it’s using a random port. We would like to select
    the port to be sure that we know where to find the ingress controller. At the
    same time, we’ll also add an annotation so that this ingress controller will be
    the default for this cluster.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一步我们需要完成。当前安装的 ingress 控制器尝试请求一个外部 IP 地址，以便允许外部流量访问它。由于我们运行的是一个没有外部 IP 地址访问权限的示例集群，因此此方法不可行。相反，我们将通过集群主机的端口转发来访问
    ingress 控制器。目前，我们的 ingress 控制器已经配置为支持端口转发，但它使用的是一个随机端口。我们希望选择一个端口，以确保知道如何找到 ingress
    控制器。同时，我们还将添加一个注释，以使该 ingress 控制器成为此集群的默认控制器。
- en: 'To apply the port changes, we’re going to provide our Kubernetes cluster an
    with extra YAML configuration with just the changes we need. Here’s that YAML:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应用端口更改，我们将为我们的 Kubernetes 集群提供一个额外的 YAML 配置文件，其中仅包含我们需要的更改。以下是该 YAML 文件：
- en: '*ingress-patch.yaml*'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '*ingress-patch.yaml*'
- en: '[PRE28]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This file specifies the name and Namespace of the Service to ensure that Kubernetes
    knows where to apply these changes. It also specifies the `port` configuration
    we’re updating, along with the `nodePort`, which is the port on our cluster nodes
    that will be used for port forwarding. We’ll look at NodePort service types and
    port forwarding in more detail in [Chapter 9](ch09.xhtml#ch09).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 此文件指定了服务的名称和命名空间，以确保 Kubernetes 知道在哪些位置应用这些更改。它还指定了我们正在更新的 `port` 配置，以及用于端口转发的集群节点端口
    `nodePort`。我们将在 [第 9 章](ch09.xhtml#ch09) 中更详细地讨论 NodePort 服务类型和端口转发。
- en: 'To patch the service, we use the `kubectl patch` command:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 要修补该服务，我们使用 `kubectl patch` 命令：
- en: '[PRE29]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'To apply the annotation, use the `kubectl annotate` command:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 要应用注释，请使用 `kubectl annotate` 命令：
- en: '[PRE30]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Kubernetes reports the change to each resource as we make it, so we know that
    our changes have been applied.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 在我们进行每次更改时都会向每个资源报告更改情况，因此我们可以知道我们的更改已经应用。
- en: Metrics Server
  id: totrans-175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 指标服务器
- en: Our final add-on is a *metrics server* that collects utilization metrics from
    our nodes, enabling the use of autoscaling. To do this, it needs to connect to
    the `kubelet` instances in our cluster. For security, it needs to verify the HTTP/S
    certificate when it connects to a `kubelet`. This is why we configured `kubelet`
    to request a certificate signed by the controller manager rather than allowing
    the `kubelet` to generate self-signed certificates.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最终附加组件是一个 *指标服务器*，它从我们的节点收集利用率指标，从而启用自动扩缩。为此，它需要连接到集群中的 `kubelet` 实例。出于安全考虑，它在连接到
    `kubelet` 时需要验证 HTTP/S 证书。这就是为什么我们将 `kubelet` 配置为请求由控制器管理器签名的证书，而不是允许 `kubelet`
    生成自签名证书的原因。
- en: 'During setup, `kubelet` created a certificate request on each node, but the
    requests were not automatically approved. Let’s find these requests:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置过程中，`kubelet` 在每个节点上创建了一个证书请求，但这些请求并未自动批准。让我们查找这些请求：
- en: '[PRE31]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Each `kubelet` has a client certificate that it uses to authenticate to the
    API server; these were automatically approved during bootstrap. The requests we
    need to approve are for `kubelet-serving` certificates, which are used when clients
    such as our metrics server connect to `kubelet`. As soon as the request is approved,
    the controller manager signs the certificate. The `kubelet` then collects the
    certificate and starts using it.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 `kubelet` 都有一个客户端证书，用于向 API 服务器进行身份验证；这些证书在引导过程中已自动批准。我们需要批准的请求是 `kubelet-serving`
    证书请求，这些证书在客户端（如我们的指标服务器）连接到 `kubelet` 时使用。一旦请求被批准，控制器管理器就会签署证书。然后，`kubelet` 会收集该证书并开始使用它。
- en: 'We can approve all of these requests at once by querying for the name of all
    of the `kubelet-serving` requests and then passing those names to `kubectl certificate
    approve`:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过查询所有 `kubelet-serving` 请求的名称，并将这些名称传递给 `kubectl certificate approve`，一次性批准所有这些请求：
- en: '[PRE32]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We now can install our metrics server by downloading and applying its YAML
    configuration:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以通过下载并应用其 YAML 配置来安装我们的指标服务器：
- en: '[PRE33]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: This component is the last one we need to install, so we can leave this directory.
    With these cluster add-ons, we now have a complete, highly available Kubernetes
    cluster.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这个组件是我们需要安装的最后一个，因此我们可以离开这个目录。通过这些集群附加组件，我们现在拥有一个完整且高可用的 Kubernetes 集群。
- en: Exploring a Cluster
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索集群
- en: Before deploying our first application onto this brand-new Kubernetes cluster,
    let’s explore what’s running on it. The commands we use here will come in handy
    later as we debug our own applications and a cluster that isn’t working correctly.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在将第一个应用程序部署到这个全新的 Kubernetes 集群之前，让我们先探索一下它上面正在运行的内容。我们在这里使用的命令将对以后调试我们自己的应用程序和一个运行不正常的集群时很有帮助。
- en: 'We’ll use `crictl`, the same command we used to explore running containers
    in [Part I](part01.xhtml#part01), to see what containers are running on `host01`:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `crictl`，这是我们在[第一部分](part01.xhtml#part01)中用来探索运行容器的相同命令，来查看在 `host01`
    上运行的容器：
- en: '[PRE34]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The control plane node is very busy, as this list includes Kubernetes control
    plane components, Calico components, and Longhorn components. Running this command
    on all the nodes and sorting out what containers are running where and for what
    purpose would be confusing. Fortunately, `kubectl` provides a clearer picture,
    although knowing that we can get down to these lower-level details and see exactly
    what containers are running on a given node is nice.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 控制平面节点非常忙碌，因为这个列表包括 Kubernetes 控制平面组件、Calico 组件和 Longhorn 组件。如果在所有节点上运行此命令，并且整理出各个容器在哪里运行以及其目的，这将会让人感到困惑。幸运的是，`kubectl`
    提供了更清晰的视图，尽管知道我们可以深入到这些底层细节，准确查看在某个节点上运行的容器是什么，还是很有用的。
- en: To explore the cluster with `kubectl`, we need to know how the cluster resources
    are organized into Namespaces. As mentioned previously, Kubernetes Namespaces
    provide security and avoid name collisions. To ensure idempotence, Kubernetes
    needs each resource to have a unique name. By dividing resources into Namespaces,
    we allow multiple resources to have the same name while still enabling the API
    server to know exactly which resource we mean, which also supports multitenancy,
    one of our cross-cutting concerns.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 `kubectl` 探索集群，我们需要知道集群资源是如何组织到命名空间中的。如前所述，Kubernetes 命名空间提供安全性并避免名称冲突。为了确保幂等性，Kubernetes
    需要每个资源都有一个唯一的名称。通过将资源划分到命名空间中，我们允许多个资源具有相同的名称，同时仍然使 API 服务器能够确切地知道我们指的是什么资源，这也支持多租户，这是我们的一个跨切面问题。
- en: 'Even though we just set up the cluster, it’s already populated with several
    Namespaces:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们刚刚设置了集群，它已经填充了几个命名空间：
- en: '[PRE35]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: As we run `kubectl` commands, they will apply to the `default` Namespace unless
    we use the `-n` option to specify a different Namespace.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行 `kubectl` 命令时，它们将应用于 `default` 命名空间，除非我们使用 `-n` 选项来指定不同的命名空间。
- en: To see what containers are running, we ask `kubectl` to get the list of Pods.
    We look at Kubernetes Pods in much more detail in [Chapter 7](ch07.xhtml#ch07).
    For now, just know that a Pod is a group of one or more containers, much like
    the Pods that we created with `crictl` in [Part I](part01.xhtml#part01).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看哪些容器正在运行，我们可以使用 `kubectl` 获取 Pod 列表。我们将在[第 7 章](ch07.xhtml#ch07)中更详细地查看 Kubernetes
    Pods。现在，只需知道 Pod 是一个或多个容器的集合，类似于我们在[第一部分](part01.xhtml#part01)中使用 `crictl` 创建的
    Pods。
- en: 'If we try to list Pods in the `default` Namespace, we can see that there aren’t
    any yet:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们尝试列出 `default` 命名空间中的 Pods，我们可以看到目前还没有任何 Pods：
- en: '[PRE36]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'So far, as we installed cluster infrastructure components, they’ve been created
    in other Namespaces. That way, when we configure normal user accounts, we can
    prevent those users from viewing or editing the cluster infrastructure. The Kubernetes
    infrastructure components were all installed into the `kube-system` Namespace:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，当我们安装集群基础设施组件时，它们都被创建在其他命名空间中。这样，当我们配置普通用户帐户时，可以防止这些用户查看或编辑集群基础设施。Kubernetes
    基础设施组件都被安装到了 `kube-system` 命名空间：
- en: '[PRE37]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We cover the control plane components in [Chapter 11](ch11.xhtml#ch11). For
    now, let’s explore just one of the control plane Pods, the API server running
    on `host01`. We can get all of the details for this Pod using `kubectl describe`:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第 11 章](ch11.xhtml#ch11)中讨论了控制平面组件。现在，让我们先探索其中一个控制平面 Pod——运行在 `host01` 上的
    API 服务器。我们可以使用 `kubectl describe` 获取此 Pod 的所有详细信息：
- en: '[PRE38]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The Namespace and name together uniquely identify this Pod. We also see the
    node on which the Pod is scheduled, its status, and details about the actual containers,
    including a container ID that we can use with `crictl` to find the container in
    the underlying `containerd` runtime.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 命名空间和名称共同唯一标识这个 Pod。我们还可以看到 Pod 所在的节点、其状态以及关于实际容器的详细信息，包括一个容器 ID，我们可以使用 `crictl`
    来找到底层 `containerd` 运行时中的容器。
- en: 'Let’s also verify that Calico deployed into our cluster as expected:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们还验证一下 Calico 是否按预期部署到集群中：
- en: '[PRE39]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Earlier we saw that Calico installed a DaemonSet resource. Kubernetes has used
    the configuration in this DaemonSet to automatically create a `calico-node` Pod
    for each node. Like Kubernetes itself, Calico also uses a separate control plane
    to handle overall configuration of the network, and the other Pods provide that
    control plane.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 之前我们看到 Calico 安装了一个 DaemonSet 资源。Kubernetes 使用这个 DaemonSet 中的配置，自动为每个节点创建一个
    `calico-node` Pod。像 Kubernetes 本身一样，Calico 也使用一个独立的控制平面来处理网络的整体配置，而其他 Pods 则提供该控制平面。
- en: 'Finally, we’ll see the containers that are running for Longhorn:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将查看为 Longhorn 运行的容器：
- en: '[PRE40]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Like Calico, Longhorn uses DaemonSets so that it can run containers on every
    node. These containers provide storage services to the other containers on the
    node. Longhorn also includes a number of other containers that serve as a control
    plane, including providing the CSI implementation that Kubernetes uses to tell
    Longhorn to create storage when needed.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Calico 类似，Longhorn 使用 DaemonSets，使其能够在每个节点上运行容器。这些容器为节点上的其他容器提供存储服务。Longhorn
    还包括许多其他容器，它们作为控制平面运行，包括提供 Kubernetes 用来指示 Longhorn 在需要时创建存储的 CSI 实现。
- en: 'We put a lot of effort into setting up this cluster, so it would be a shame
    to end this chapter without running at least one application on it. In the next
    chapter, we will look at many different ways to run containers, but let’s quickly
    run a simple NGINX web server in our Kubernetes cluster:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们花了很多精力来设置这个集群，因此，如果在本章结束时没有至少运行一个应用程序，那将是非常可惜的。在下一章中，我们将探讨多种不同的运行容器方式，但我们先在
    Kubernetes 集群中快速运行一个简单的 NGINX 网络服务器：
- en: '[PRE41]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'That may look like an imperative command, but under the hood, `kubectl` is
    creating a Pod resource using the name and container image we specified, and then
    it’s applying that resource on the cluster. Let’s inspect the default Namespace
    again:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来像一个命令式的指令，但实际上，`kubectl` 正在使用我们指定的名称和容器镜像创建一个 Pod 资源，并将该资源应用于集群。让我们再次查看默认的命名空间：
- en: '[PRE42]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: We used `-o wide` to see extra information about the Pod, including its IP address
    and where it was scheduled, which can be different each time the Pod is created.
    In this case, the Pod was scheduled to `host02`, showing that we were successful
    in allowing regular application containers to be deployed to our control plane
    nodes. The IP address comes from the Pod CIDR we configured, and Calico automatically
    assigns it.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了 `-o wide` 来查看关于 Pod 的额外信息，包括其 IP 地址和调度位置，这些每次创建 Pod 时可能会不同。在这个例子中，Pod
    被调度到了 `host02`，这表明我们成功地允许常规应用容器部署到我们的控制平面节点。IP 地址来自我们配置的 Pod CIDR，并由 Calico 自动分配。
- en: 'Calico also handles routing traffic so that we can reach the Pod from any container
    in the cluster as well as from the host network. Let’s verify that, starting with
    a regular `ping`:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: Calico 还处理路由流量，以便我们可以从集群中的任何容器以及从主机网络访问 Pod。让我们验证这一点，从一个常规的 `ping` 开始：
- en: '[PRE43]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Use your Pod’s IP address in the place of the one shown here.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在此处替换为您 Pod 的 IP 地址。
- en: 'We can also use `curl` to verify that the NGINX web server is working:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用 `curl` 来验证 NGINX 网络服务器是否正常工作：
- en: '[PRE44]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The Kubernetes cluster is working and ready for us to deploy applications. Kubernetes
    will take advantage of all of the nodes in the cluster to load balance our applications
    and provide resiliency in the event of any failures.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 集群已经正常工作，准备好供我们部署应用。Kubernetes 将利用集群中的所有节点来负载均衡我们的应用，并在发生故障时提供弹性。
- en: Final Thoughts
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最后的思考
- en: In this chapter, we’ve explored how Kubernetes is architected with the flexibility
    to allow cluster components to come and go at any time. This applies not only
    to containerized applications but also to the cluster components, including control
    plane microservices and the underlying servers and networks the cluster uses.
    We were able to bootstrap a cluster and then dynamically add nodes to it, configure
    those nodes to accept certain types of containers, and then dynamically add networking
    and storage drivers using the Kubernetes cluster itself to run and monitor them.
    Finally, we deployed our first container to a Kubernetes cluster, allowing it
    to automatically schedule the container onto an available node, using our network
    driver to access the container from the host network.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们探讨了 Kubernetes 的架构，具备灵活性，允许集群组件随时加入或退出。这不仅适用于容器化应用程序，也适用于集群组件，包括控制平面微服务以及集群所使用的底层服务器和网络。我们成功地引导启动了一个集群，并动态地向其中添加了节点，配置这些节点接受特定类型的容器，然后使用
    Kubernetes 集群本身动态地添加网络和存储驱动程序来运行和监控它们。最后，我们将第一个容器部署到 Kubernetes 集群，允许它自动将容器调度到可用节点上，并通过我们的网络驱动程序从主机网络访问该容器。
- en: Now that we have a highly available cluster, we can look at how to deploy an
    application to Kubernetes. We’ll explore some key Kubernetes resources that we
    need to create a scalable, reliable application. This process will provide a foundation
    for exploring Kubernetes in detail, including understanding what happens when
    our applications don’t run as expected and how to debug issues with our application
    or the Kubernetes cluster.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个高可用的集群，接下来可以看看如何将应用程序部署到 Kubernetes。我们将探索一些关键的 Kubernetes 资源，这些资源是创建可扩展、可靠的应用程序所必需的。这个过程将为我们深入探索
    Kubernetes 奠定基础，包括了解当我们的应用程序未按预期运行时发生了什么，以及如何调试应用程序或 Kubernetes 集群的问题。
