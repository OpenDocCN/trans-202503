- en: '**INTRODUCTION**'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**简介**'
- en: '![Image](../images/common.jpg)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/common.jpg)'
- en: Thanks to rapid advancements in deep learning, we have seen a significant expansion
    of machine learning and AI in recent years.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 由于深度学习的快速进展，近年来我们见证了机器学习和人工智能的显著扩展。
- en: This progress is exciting if we expect these advancements to create new industries,
    transform existing ones, and improve the quality of life for people around the
    world. On the other hand, the constant emergence of new techniques can make it
    challenging and time-consuming to keep abreast of the latest developments. Nonetheless,
    staying current is essential for professionals and organizations that use these
    technologies.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这项进展令人兴奋，因为如果我们期待这些进步创造新的产业、改造现有产业，并提高全球人民的生活质量，那么它们无疑会带来积极变化。另一方面，技术的不断涌现使得紧跟最新发展变得具有挑战性和耗时。然而，保持与时俱进对使用这些技术的专业人士和组织至关重要。
- en: I wrote this book as a resource for readers and machine learning practitioners
    who want to advance their expertise in the field and learn about techniques that
    I consider useful and significant but that are often overlooked in traditional
    and introductory textbooks and classes. I hope you’ll find this book a valuable
    resource for obtaining new insights and discovering new techniques you can implement
    in your work.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我写这本书是为了为读者和机器学习从业者提供一种资源，帮助他们提升在该领域的专业知识，并学习一些我认为有用且重要的技术，这些技术在传统和入门教材及课程中常常被忽视。希望你能发现本书是获得新见解和发现可以在工作中实现的新技术的宝贵资源。
- en: '**Who Is This Book For?**'
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**这本书适合谁？**'
- en: 'Navigating the world of AI and machine learning literature can often feel like
    walking a tightrope, with most books positioned at either end: broad beginner’s
    introductions or deeply mathematical treatises. This book illustrates and discusses
    important developments in these fields while staying approachable and not requiring
    an advanced math or coding background.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 探索人工智能和机器学习文献的世界常常让人感觉像是在走钢丝，大多数书籍都位于两端：一端是广泛的初学者介绍，另一端是深入的数学论述。本书在保持易于接近的同时，阐明并讨论了这些领域的重要发展，且不需要高级数学或编程背景。
- en: This book is for people with some experience with machine learning who want
    to learn new concepts and techniques. It’s ideal for those who have taken a beginner
    course in machine learning or deep learning or have read an equivalent introductory
    book on the topic. (Throughout this book, I will use *machine learning* as an
    umbrella term for machine learning, deep learning, and AI.)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书适合那些有一定机器学习经验、希望学习新概念和技术的人。它非常适合那些已经参加过机器学习或深度学习初学者课程，或者阅读过相关入门书籍的人。（在本书中，我将*机器学习*作为机器学习、深度学习和人工智能的总称。）
- en: '**What Will You Get Out of This Book?**'
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**你能从这本书中获得什么？**'
- en: This book adopts a unique Q&A style, where each brief chapter is structured
    around a central question related to fundamental concepts in machine learning,
    deep learning, and AI. Every question is followed by an explanation, with several
    illustrations and figures, as well as exercises to test your understanding. Many
    chapters also include references for further reading. These bite-sized nuggets
    of information provide an enjoyable jumping-off point on your journey from machine
    learning beginner to expert.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本书采用独特的问答式风格，每一章简短，围绕与机器学习、深度学习和人工智能基本概念相关的中心问题展开。每个问题后面都有解释，并配有若干插图和图表，同时还有练习题来测试你的理解。许多章节还包括进一步阅读的参考资料。这些简短的信息提供了一个愉快的起点，帮助你从机器学习初学者成长为专家。
- en: The book covers a wide range of topics. It includes new insights about established
    architectures, such as convolutional networks, that allow you to utilize these
    technologies more effectively. It also discusses more advanced techniques, such
    as the inner workings of large language models (LLMs) and vision transformers.
    Even experienced machine learning researchers and practitioners will encounter
    something new to add to their arsenal of techniques.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本书涵盖了广泛的主题。它包括了关于已建立架构（例如卷积网络）的新见解，帮助你更有效地利用这些技术。它还讨论了更先进的技术，如大语言模型（LLMs）和视觉变换器的内部工作原理。即使是经验丰富的机器学习研究人员和从业者也会遇到一些新知识，丰富他们的技术库。
- en: While this book will expose you to new concepts and ideas, it’s not a math or
    coding book. You won’t need to solve any proofs or run any code while reading.
    In other words, this book is a perfect travel companion or something you can read
    on your favorite reading chair with your morning coffee or tea.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本书将向你展示新概念和新思想，但它并不是一本数学或编程书。在阅读时你不需要解决任何证明或运行任何代码。换句话说，本书是你旅行时的完美伴侣，或者你可以在喜欢的阅读椅上，配着早晨的咖啡或茶，轻松阅读。
- en: '**How to Read This Book**'
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**如何阅读本书**'
- en: Each chapter of this book is designed to be self-contained, offering you the
    freedom to jump between topics as you wish. When a concept from one chapter is
    explained in more detail in another, I’ve included chapter references you can
    follow to fill in gaps in your understanding.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的每一章都是自成一体的，允许你根据需要自由跳跃章节。当一个章节中的概念在其他章节中有更详细的解释时，我会提供章节参考，帮助你填补理解的空白。
- en: However, there’s a strategic sequence to the chapters. For example, the early
    chapter on embeddings sets the stage for later discussions on self-supervised
    learning and few-shot learning. For the easiest reading experience and the most
    comprehensive grasp of the content, my recommendation is to approach the book
    from start to finish.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，各章节之间有一个战略性顺序。例如，关于嵌入的早期章节为后续关于自监督学习和少样本学习的讨论奠定了基础。为了获得最流畅的阅读体验和最全面的内容理解，我建议从头到尾依次阅读本书。
- en: Each chapter is accompanied by optional exercises for readers who want to test
    their understanding, with an answer key located at the end of the book. In addition,
    for any papers referenced in a chapter or further reading on that chapter’s topic,
    you can find the complete citation information in that chapter’s “References”
    section.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 每一章后都有可选的练习，供希望检验自己理解的读者使用，答案键位于书末。此外，关于章节中提到的任何论文或进一步阅读的材料，你可以在该章节的“参考文献”部分找到完整的引用信息。
- en: The book is structured into five main parts centered on the most important topics
    in machine learning and AI today.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本书分为五个主要部分，集中讨论当今机器学习和人工智能领域最重要的话题。
- en: '**[Part I: Neural Networks and Deep Learning](part01.xhtml)** covers questions
    about deep neural networks and deep learning that are not specific to a particular
    subdomain. For example, we discuss alternatives to supervised learning and techniques
    for reducing overfitting, which is a common problem when using machine learning
    models for real-world problems where data is limited.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**[第一部分：神经网络与深度学习](part01.xhtml)** 涵盖了关于深度神经网络和深度学习的一些问题，这些问题并不特定于某个子领域。例如，我们讨论了监督学习的替代方法，以及减少过拟合的技术，这是在使用机器学习模型解决数据有限的现实问题时常见的问题。'
- en: '**[Chapter 1: Embeddings, Latent Space, and Representations](ch01.xhtml)**   Delves
    into the distinctions and similarities between embedding vectors, latent vectors,
    and representations. Elucidates how these concepts help encode information in
    the context of machine learning.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**[第1章：嵌入、潜在空间与表示](ch01.xhtml)**   深入探讨了嵌入向量、潜在向量和表示之间的区别与相似性。阐明了这些概念如何在机器学习中帮助编码信息。'
- en: '**[Chapter 2: Self-Supervised Learning](ch02.xhtml)**    Focuses on self-supervised
    learning, a method that allows neural networks to utilize large, unlabeled datasets
    in a supervised manner.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**[第2章：自监督学习](ch02.xhtml)**    重点介绍自监督学习，这是一种使神经网络能够以监督方式利用大量未标注数据集的方法。'
- en: '**[Chapter 3: Few-Shot Learning](ch03.xhtml)**    Introduces few-shot learning,
    a specialized supervised learning technique tailored for small training datasets.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**[第3章：少样本学习](ch03.xhtml)**    介绍了少样本学习，这是一种针对小型训练数据集量身定制的监督学习技术。'
- en: '**[Chapter 4: The Lottery Ticket Hypothesis](ch04.xhtml)**    Explores the
    idea that randomly initialized neural networks contain smaller, efficient subnetworks.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**[第4章：彩票票据假说](ch04.xhtml)**    探讨了一个观点，即随机初始化的神经网络包含更小、更高效的子网络。'
- en: '**[Chapter 5: Reducing Overfitting with Data](ch05.xhtml)**    Addresses the
    challenge of overfitting in machine learning, discussing strategies centered on
    data augmentation and the use of unlabeled data to reduce overfitting.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**[第5章：通过数据减少过拟合](ch05.xhtml)**    解决机器学习中的过拟合问题，讨论了围绕数据增强和利用未标注数据减少过拟合的策略。'
- en: '**[Chapter 6: Reducing Overfitting with Model Modifications](ch06.xhtml)**    Extends
    the conversation on overfitting, focusing on model-related solutions like regularization,
    opting for simpler models, and ensemble techniques.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**[第6章：通过模型修改减少过拟合](ch06.xhtml)**    扩展了关于过拟合的讨论，重点介绍了与模型相关的解决方案，如正则化、选择更简单的模型和集成技术。'
- en: '**[Chapter 7: Multi-GPU Training Paradigms](ch07.xhtml)**    Explains various
    training paradigms for multi-GPU setups to accelerate model training, including
    data and model parallelism.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**[第7章：多GPU训练范式](ch07.xhtml)**    解释了多GPU设置中用于加速模型训练的各种训练范式，包括数据并行和模型并行。'
- en: '**[Chapter 8: The Success of Transformers](ch08.xhtml)**    Explores the popular
    transformer architecture, highlighting features like attention mechanisms, parallelization
    ease, and high parameter counts.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**[第8章：变换器的成功](ch08.xhtml)**    探索了流行的变换器架构，突出其注意力机制、并行化的便捷性以及高参数量等特点。'
- en: '**[Chapter 9: Generative AI Models](ch09.xhtml)**    Provides a comprehensive
    overview of deep generative models, which are used to produce various media forms,
    including images, text, and audio. Discusses the strengths and weaknesses of each
    model type.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**[第9章：生成性AI模型](ch09.xhtml)**    提供了深度生成模型的全面概述，这些模型用于生成各种形式的媒体，包括图像、文本和音频。讨论了每种模型类型的优缺点。'
- en: '**[Chapter 10: Sources of Randomness](ch10.xhtml)**    Addresses the various
    sources of randomness in the training of deep neural networks that may lead to
    inconsistent and non-reproducible results during both training and inference.
    While randomness can be accidental, it can also be intentionally introduced by
    design.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**[第10章：随机性的来源](ch10.xhtml)**    讨论了在训练深度神经网络过程中可能导致训练和推理结果不一致且不可重现的各种随机性来源。虽然随机性可能是偶然的，但也可以通过设计有意引入。'
- en: '**[Part II: Computer Vision](part02.xhtml)** focuses on topics mainly related
    to deep learning but specific to computer vision, many of which cover convolutional
    neural networks and vision transformers.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**[第二部分：计算机视觉](part02.xhtml)** 侧重于主要与深度学习相关但特定于计算机视觉的主题，其中许多涉及卷积神经网络和视觉变换器。'
- en: '**[Chapter 11: Calculating the Number of Parameters](ch11.xhtml)**    Explains
    the procedure for determining the parameters in a convolutional neural network,
    which is useful for gauging a model’s storage and memory requirements.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**[第11章：计算参数数量](ch11.xhtml)**    解释了确定卷积神经网络中参数的过程，这对于评估模型的存储和内存需求非常有用。'
- en: '**[Chapter 12: Fully Connected and Convolutional Layers](ch12.xhtml)**    Illustrates
    the circumstances in which convolutional layers can seamlessly replace fully connected
    layers, which can be useful for hardware optimization or simplifying implementations.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**[第12章：全连接层与卷积层](ch12.xhtml)**    说明了卷积层如何在某些情况下无缝替代全连接层，这对于硬件优化或简化实现非常有用。'
- en: '**[Chapter 13: Large Training Sets for Vision Transformers](ch13.xhtml)**    Probes
    the rationale behind vision transformers requiring more extensive training sets
    compared to conventional convolutional neural networks.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**[第13章：视觉变换器的大型训练集](ch13.xhtml)**    探讨了视觉变换器为何需要比传统卷积神经网络更大规模的训练集的原因。'
- en: '**[Part III: Natural Language Processing](part03.xhtml)** covers topics around
    working with text, many of which are related to transformer architectures and
    self-attention.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**[第三部分：自然语言处理](part03.xhtml)** 涉及与文本处理相关的主题，其中许多与变换器架构和自注意力机制相关。'
- en: '**[Chapter 14: The Distributional Hypothesis](ch14.xhtml)**    Delves into
    the distributional hypothesis, a linguistic theory suggesting that words appearing
    in the same contexts tend to possess similar meanings, which has useful implications
    for training machine learning models.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**[第14章：分布假设](ch14.xhtml)**    深入探讨了分布假设，这是一种语言学理论，认为在相同上下文中出现的词语往往具有相似的意义，这对于训练机器学习模型具有重要意义。'
- en: '**[Chapter 15: Data Augmentation for Text](ch15.xhtml)**    Highlights the
    significance of data augmentation for text, a technique used to artificially increase
    dataset sizes, which can help with improving model performance.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**[第15章：文本数据增强](ch15.xhtml)**    强调了文本数据增强的重要性，这是一种用于人工增加数据集大小的技术，可以帮助提高模型的性能。'
- en: '**[Chapter 16: Self-Attention](ch16.xhtml)**    Introduces self-attention,
    a mechanism allowing each segment of a neural network’s input to refer to other
    parts. Self-attention is a key mechanism in modern large language models.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**[第16章：自注意力机制](ch16.xhtml)**    介绍了自注意力机制，该机制允许神经网络的每个输入片段引用其他部分。自注意力是现代大语言模型中的关键机制。'
- en: '**[Chapter 17: Encoder- and Decoder-Style Transformers](ch17.xhtml)**    Describes
    the nuances of encoder and decoder transformer architectures and explains which
    type of architecture is most useful for each language processing task.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**[第17章：编码器与解码器式转换器](ch17.xhtml)**    讲解了编码器和解码器转换器架构的细微差别，并解释了哪种架构最适合每个语言处理任务。'
- en: '**[Chapter 18: Using and Fine-Tuning Pretrained Transformers](ch18.xhtml)**    Explains
    different methods for fine-tuning pretrained large language models and discusses
    their strengths and weaknesses.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**[第18章：使用和微调预训练的转换器](ch18.xhtml)**    解释了微调预训练的大型语言模型的不同方法，并讨论了它们的优缺点。'
- en: '**[Chapter 19: Evaluating Generative Large Language Models](ch19.xhtml)**    Lists
    prominent evaluation metrics for language models like Perplexity, BLEU, ROUGE,
    and BERTScore.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**[第19章：评估生成型大语言模型](ch19.xhtml)**    列出了用于语言模型的主要评估指标，如困惑度（Perplexity）、BLEU、ROUGE
    和 BERTScore。'
- en: '**[Part IV: Production and Deployment](part04.xhtml)** covers questions pertaining
    to practical scenarios, such as increasing inference speeds and various types
    of distribution shifts.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**[第4部分：生产与部署](part04.xhtml)** 涵盖了与实际场景相关的问题，例如提高推理速度和各种类型的分布变化。'
- en: '**[Chapter 20: Stateless and Stateful Training](ch20.xhtml)**    Distinguishes
    between stateless and stateful training methodologies used in deploying models.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**[第20章：无状态训练与有状态训练](ch20.xhtml)**    区分了在模型部署中使用的无状态训练和有状态训练方法。'
- en: '**[Chapter 21: Data-Centric AI](ch21.xhtml)**    Explores data-centric AI,
    which priori-tizes refining datasets to enhance model performance. This approach
    contrasts with the conventional model-centric approach, which emphasizes improving
    model architectures or methods.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**[第21章：数据驱动的 AI](ch21.xhtml)**    探讨了数据驱动的 AI，这种方法优先考虑通过改进数据集来提升模型性能。与此相对的是传统的以模型为中心的方法，后者强调通过改进模型架构或方法来提升性能。'
- en: '**[Chapter 22: Speeding Up Inference](ch22.xhtml)**    Introduces techniques
    to enhance the speed of model inference without tweaking the model’s architecture
    or compromising accuracy.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**[第22章：加速推理](ch22.xhtml)**    介绍了在不调整模型架构或妥协准确性的情况下，提高模型推理速度的技术。'
- en: '**[Chapter 23: Data Distribution Shifts](ch23.xhtml)**    Post-deployment,
    AI models may face discrepancies between training data and real-world data distributions,
    known as data distribution shifts. These shifts can deteriorate model performance.
    This chapter categorizes and elaborates on common shifts like covariate shift,
    concept drift, label shift, and domain shift.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**[第23章：数据分布变化](ch23.xhtml)**    部署后，AI 模型可能会面临训练数据和现实世界数据分布之间的差异，这被称为数据分布变化。这些变化可能会导致模型性能下降。本章对常见的分布变化进行了分类并详细讲解，如协变量变化、概念漂移、标签变化和领域变化。'
- en: '**[Part V: Predictive Performance and Model Evaluation](part05.xhtml)** dives
    deeper into various aspects of squeezing out predictive performance, such as changing
    the loss function, setting up *k*-fold cross-validation, and dealing with limited
    labeled data.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**[第5部分：预测性能与模型评估](part05.xhtml)** 深入探讨了挖掘预测性能的各个方面，例如改变损失函数、设置 *k* 折交叉验证以及处理有限的标注数据。'
- en: '**[Chapter 24: Poisson and Ordinal Regression](ch24.xhtml)**    Highlights
    the differences between Poisson and ordinal regression. Poisson regression is
    suitable for count data that follows a Poisson distribution, like the number of
    colds contracted on an airplane. In contrast, ordinal regression caters to ordered
    categorical data without assuming equidistant categories, such as disease severity.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**[第24章：泊松回归与有序回归](ch24.xhtml)**    强调了泊松回归和有序回归之间的差异。泊松回归适用于遵循泊松分布的计数数据，例如在飞机上感染感冒的次数。相反，有序回归适用于有序类别数据，而不假设类别之间是等距的，例如疾病严重程度。'
- en: '**[Chapter 25: Confidence Intervals](ch25.xhtml)**    Delves into methods for
    constructing confidence intervals for machine learning classifiers. Reviews the
    purpose of confidence intervals, discusses how they estimate unknown population
    parameters, and introduces techniques such as normal approximation intervals,
    bootstrapping, and retraining with various random seeds.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**[第25章：置信区间](ch25.xhtml)**    深入探讨了为机器学习分类器构建置信区间的方法。回顾了置信区间的目的，讨论了它们如何估计未知的总体参数，并介绍了如正态近似区间、自助法和使用不同随机种子进行再训练等技术。'
- en: '**[Chapter 26: Confidence Intervals vs. Conformal Predictions](ch26.xhtml)**    Discusses
    the distinction between confidence intervals and conformal predictions and describes
    the latter as a tool for creating prediction intervals that cover actual outcomes
    with specific probability.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**[第26章：置信区间与一致性预测](ch26.xhtml)**    讨论了置信区间和一致性预测之间的区别，并将后一者描述为创建预测区间的工具，这些区间以特定的概率覆盖实际结果。'
- en: '**[Chapter 27: Proper Metrics](ch27.xhtml)**    Focuses on the essential properties
    of a proper metric in mathematics and computer science. Examines whether commonly
    used loss functions in machine learning, such as mean squared error and cross-entropy
    loss, satisfy these properties.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**[第27章：合适的度量](ch27.xhtml)**    聚焦于数学和计算机科学中合适度量的基本属性。探讨了机器学习中常用的损失函数，如均方误差和交叉熵损失，是否满足这些属性。'
- en: '**[Chapter 28: The *k* in *k*-Fold Cross-Validation](ch28.xhtml)**    Explores
    the role of the *k* in *k*-fold cross-validation and provides insight into the
    advantages and disadvantages of selecting a large *k*.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**[第28章：*k*-折交叉验证中的*k*](ch28.xhtml)**    探讨了*k*-折交叉验证中*k*的作用，并深入分析了选择较大*k*的优缺点。'
- en: '**[Chapter 29: Training and Test Set Discordance](ch29.xhtml)**    Addresses
    the scenario where a model performs better on a test dataset than the training
    dataset. Offers strategies to discover and address discrepancies between training
    and test datasets, introducing the concept of adversarial validation.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**[第29章：训练集与测试集不一致性](ch29.xhtml)**    讨论了模型在测试数据集上的表现优于训练数据集的情形。提供了发现和解决训练集与测试集之间差异的策略，介绍了对抗验证的概念。'
- en: '**[Chapter 30: Limited Labeled Data](ch30.xhtml)**    Introduces various techniques
    to enhance model performance in situations where data is limited. Covers data
    labeling, bootstrapping, and paradigms such as transfer learning, active learning,
    and multimodal learning.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**[第30章：有限标签数据](ch30.xhtml)**    介绍了在数据有限的情况下增强模型表现的各种技术。涵盖了数据标注、自助法，以及如迁移学习、主动学习和多模态学习等范式。'
- en: '**Online Resources**'
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**在线资源**'
- en: I’ve provided optional supplementary materials on GitHub with code examples
    for certain chapters to enhance your learning experience (see *[https://github.com/rasbt/MachineLearning-QandAI-book](https://github.com/rasbt/MachineLearning-QandAI-book)*).
    These materials are designed as practical extensions and deep dives into topics
    covered in the book. You can use them alongside each chapter or explore them after
    reading to solidify and expand your knowledge.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我在GitHub上提供了可选的补充材料，包含某些章节的代码示例，以增强你的学习体验（见* [https://github.com/rasbt/MachineLearning-QandAI-book](https://github.com/rasbt/MachineLearning-QandAI-book)
    *）。这些材料设计为实际的扩展和深入探讨书中涉及的主题。你可以在阅读每一章时一起使用它们，或者在阅读后探索它们，以巩固和扩展你的知识。
- en: Without further ado, let’s dive in.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 事不宜迟，让我们深入探讨吧。
