- en: '**10'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**10**'
- en: SOURCES OF RANDOMNESS**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机性来源**'
- en: '![Image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/common.jpg)'
- en: What are the common sources of randomness when training deep neural networks
    that can cause non-reproducible behavior during training and inference?
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练深度神经网络时，哪些常见的随机性来源会导致训练和推理过程中行为不可重复？
- en: When training or using machine learning models such as deep neural networks,
    several sources of randomness can lead to different results every time we train
    or run these models, even though we use the same overall settings. Some of these
    effects are accidental and some are intended. The following sections categorize
    and discuss these various sources of randomness.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练或使用机器学习模型，如深度神经网络时，多个随机性来源可能导致每次训练或运行这些模型时结果不同，尽管我们使用的是相同的总体设置。这些效果中有些是偶然的，有些是有意为之。接下来的章节将对这些不同的随机性来源进行分类并讨论。
- en: Optional hands-on examples for most of these categories are provided in the
    *supplementary/q10-random-sources* subfolder at *[https://github.com/rasbt/MachineLearning-QandAI-book](https://github.com/rasbt/MachineLearning-QandAI-book)*.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数这些类别的可选实践示例可以在*补充/q10-random-sources*子文件夹中找到，网址为*[https://github.com/rasbt/MachineLearning-QandAI-book](https://github.com/rasbt/MachineLearning-QandAI-book)*。
- en: '**Model Weight Initialization**'
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**模型权重初始化**'
- en: All common deep neural network frameworks, including TensorFlow and PyTorch,
    randomly initialize the weights and bias units at each layer by default. This
    means that the final model will be different every time we start the training.
    The reason these trained models will differ when we start with different random
    weights is the nonconvex nature of the loss, as illustrated in [Figure 10-1](ch10.xhtml#ch10fig1).
    As the figure shows, the loss will converge to different local minima depending
    on where the initial starting weights are located.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 所有常见的深度神经网络框架，包括TensorFlow和PyTorch，默认会随机初始化每一层的权重和偏置单元。这意味着每次开始训练时，最终的模型都会不同。当我们使用不同的随机权重启动训练时，训练出的模型会有所不同，其原因是损失函数的非凸性质，正如[图10-1](ch10.xhtml#ch10fig1)所示。图中显示，损失函数会根据初始权重的位置收敛到不同的局部最小值。
- en: '![Image](../images/10fig01.jpg)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/10fig01.jpg)'
- en: '*Figure 10-1: Different starting weights can lead to different final weights.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*图10-1：不同的初始权重可能导致不同的最终权重。*'
- en: In practice, it is therefore recommended to run the training (if the computational
    resources permit) at least a handful of times; unlucky initial weights can sometimes
    cause the model not to converge or to converge to a local minimum corresponding
    to poorer predictive accuracy.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 实际操作中，因此建议至少多次运行训练（如果计算资源允许）；不幸的初始权重有时会导致模型无法收敛，或者收敛到一个局部最小值，从而导致较差的预测准确度。
- en: However, we can make the random weight initialization deterministic by seeding
    the random generator. For instance, if we set the seed to a specific value like
    123, the weights will still initialize with small random values. Nonetheless,
    the neural network will consistently initialize with the same small random weights,
    enabling accurate reproduction of results.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，通过给随机生成器设置种子，我们可以使随机权重初始化变得确定性。例如，如果我们将种子设置为特定的值（如123），权重仍然会以小的随机值初始化。尽管如此，神经网络将始终使用相同的小随机权重初始化，从而确保结果的准确重现。
- en: '**Dataset Sampling and Shuffling**'
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**数据集抽样和洗牌**'
- en: When we train and evaluate machine learning models, we usually start by dividing
    a dataset into training and test sets. This requires random sampling since we
    have to decide which examples we put into a training set and which examples we
    put into a test set.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们训练和评估机器学习模型时，通常会首先将数据集划分为训练集和测试集。这需要随机抽样，因为我们必须决定将哪些样本放入训练集，哪些放入测试集。
- en: In practice, we often use model evaluation techniques such as *k*-fold cross-validation
    or holdout validation. In holdout validation, we split the training set into training,
    validation, and test datasets, which are also sampling procedures influenced by
    randomness. Similarly, unless we use a fixed random seed, we get a different model
    each time we partition the dataset or tune or evaluate the model using *k*-fold
    cross-validation since the training partitions will differ.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际操作中，我们通常使用模型评估技术，如*k*-折交叉验证或保留验证。在保留验证中，我们将训练集分成训练、验证和测试数据集，这些也是受随机性影响的抽样过程。类似地，除非使用固定的随机种子，否则每次划分数据集或调整/评估模型时，都会得到不同的模型，因为训练数据划分会有所不同。
- en: '**Nondeterministic Algorithms**'
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**非确定性算法**'
- en: We may include random components and algorithms depending on the architecture
    and hyperparameter choices. A popular example of this is *dropout*.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能根据架构和超参数选择包括随机组件和算法。一个常见的例子就是*dropout*。
- en: Dropout works by randomly setting a fraction of a layer’s units to zero during
    training, which helps the model learn more robust and generalized representations.
    This “dropping out” is typically applied at each training iteration with a probability
    *p*, a hyperparameter that controls the fraction of units dropped out. Typical
    values for *p* are in the range of 0.2 to 0.8.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout通过在训练过程中随机将一部分层的单元设置为零，帮助模型学习更加稳健和泛化的表示。这个“丢弃”通常在每次训练迭代中应用，概率为*p*，它是一个控制丢弃单元比例的超参数。*p*的典型值范围是0.2到0.8。
- en: To illustrate this concept, [Figure 10-2](ch10.xhtml#ch10fig2) shows a small
    neural network where dropout randomly drops a subset of the hidden layer nodes
    in each forward pass during training.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一概念，[图10-2](ch10.xhtml#ch10fig2)展示了一个小型神经网络，在每次训练的前向传播过程中，dropout随机丢弃一部分隐藏层节点。
- en: '![Image](../images/10fig02.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/10fig02.jpg)'
- en: '*Figure 10-2: In dropout, hidden nodes are intermittently and randomly disabled
    during each forward pass in training.*'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*图10-2：在dropout中，隐藏节点在每次训练的前向传播中间歇性地被随机禁用。*'
- en: To create reproducible training runs, we must seed the random generator before
    training with dropout (analogous to seeding the random generator before initializing
    the model weights). During inference, we need to disable dropout to guarantee
    deterministic results. Each deep learning framework has a specific setting for
    that purpose—a PyTorch example is included in the *supplementary/q10-random-sources*
    subfolder at *[https://github.com/rasbt/MachineLearning-QandAI-book](https://github.com/rasbt/MachineLearning-QandAI-book)*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建可重复的训练过程，我们必须在训练前对使用dropout的随机生成器进行初始化（类似于在初始化模型权重之前对随机生成器进行初始化）。在推理过程中，我们需要禁用dropout以保证结果的确定性。每个深度学习框架都有一个特定的设置来实现这一目的——一个PyTorch示例包含在*supplementary/q10-random-sources*子文件夹中，地址是*[https://github.com/rasbt/MachineLearning-QandAI-book](https://github.com/rasbt/MachineLearning-QandAI-book)*。
- en: '**Different Runtime Algorithms**'
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**不同的运行时算法**'
- en: The most intuitive or simplest implementation of an algorithm or method is not
    always the best one to use in practice. For example, when training deep neural
    networks, we often use efficient alternatives and approximations to gain speed
    and resource advantages during training and inference.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 算法或方法的最直观或最简单的实现并不总是实践中最好的选择。例如，在训练深度神经网络时，我们常常使用高效的替代方案和近似值，以在训练和推理过程中获得速度和资源优势。
- en: 'A popular example is the convolution operation used in convolutional neural
    networks. There are several possible ways to implement the convolution operation:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的例子是卷积神经网络中使用的卷积操作。有几种可能的方式来实现卷积操作：
- en: '**The classic direct convolution** The common implementation of discrete convolution
    via an element-wise product between the input and the window, followed by summing
    the result to get a single number. (See [Chapter 12](ch12.xhtml) for a discussion
    of the convolution operation.)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**经典的直接卷积** 是通过输入和窗口之间的元素级乘积实现离散卷积的常见方法，接着将结果求和得到一个单一的数值。（关于卷积操作的讨论，请参见[第12章](ch12.xhtml)）'
- en: '**FFT-based convolution** Uses fast Fourier transform (FFT) to convert the
    convolution into an element-wise multiplication in the frequency domain.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于FFT的卷积** 使用快速傅里叶变换（FFT）将卷积转换为频域中的元素级乘法。'
- en: '**Winograd-based convolution** An efficient algorithm for small filter sizes
    (like 3*×*3) that reduces the number of multiplications required for the convolution.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于Winograd的卷积** 是一个高效的算法，适用于小的滤波器尺寸（例如3×3），可以减少卷积所需的乘法次数。'
- en: Different convolution algorithms have different trade-offs in terms of memory
    usage, computational complexity, and speed. By default, libraries such as the
    CUDA Deep Neural Network library (cuDNN), which are used in PyTorch and TensorFlow,
    can choose different algorithms for performing convolution operations when running
    deep neural networks on GPUs. However, the deterministic algorithm choice has
    to be explicitly enabled. In PyTorch, for example, this can be done by setting
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的卷积算法在内存使用、计算复杂度和速度方面有不同的权衡。默认情况下，像CUDA深度神经网络库（cuDNN）这样的库，在PyTorch和TensorFlow中用于在GPU上运行深度神经网络时，可以选择不同的算法来执行卷积操作。然而，必须显式启用确定性算法选择。例如，在PyTorch中，可以通过设置来实现。
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: While these approximations yield similar results, subtle numerical differences
    can accumulate during training and cause the training to converge to slightly
    different local minima.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些近似结果相似，但在训练过程中，细微的数值差异可能会累积，导致训练收敛到稍微不同的局部最小值。
- en: '**Hardware and Drivers**'
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**硬件与驱动程序**'
- en: Training deep neural networks on different hardware can also produce different
    results due to small numeric differences, even when the same algorithms are used
    and the same operations are executed. These differences may sometimes be due to
    different numeric precision for floating-point operations. However, small numeric
    differences may also arise due to hardware and software optimization, even at
    the same precision.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同硬件上训练深度神经网络可能会产生不同的结果，尽管使用的是相同的算法且执行相同的操作，这些差异通常是由于浮点操作中的数值精度不同。然而，即使在相同精度下，硬件和软件的优化也可能导致小的数值差异。
- en: 'For instance, different hardware platforms may have specialized optimizations
    or libraries that can slightly alter the behavior of deep learning algorithms.
    To give one example of how different GPUs can produce different modeling results,
    the following is a quotation from the official NVIDIA documentation: “Across different
    architectures, no cuDNN routines guarantee bit-wise reproducibility. For example,
    there is no guarantee of bit-wise reproducibility when comparing the same routine
    run on NVIDIA Volta^(TM) and NVIDIA Turing^(TM) [. . .] and NVIDIA Ampere architecture.”'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，不同的硬件平台可能具有专门的优化或库，这些优化或库可能会稍微改变深度学习算法的行为。为了说明不同的GPU可能会产生不同的建模结果，以下是来自NVIDIA官方文档的引用：“在不同的架构之间，没有cuDNN例程能保证逐位重现性。例如，当比较在NVIDIA
    Volta^(TM)和NVIDIA Turing^(TM) [. . .]以及NVIDIA Ampere架构上运行的相同例程时，并不保证逐位重现性。”
- en: '**Randomness and Generative AI**'
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**随机性与生成式AI**'
- en: Besides the various sources of randomness mentioned earlier, certain models
    may also exhibit random behavior during inference that we can think of as “randomness
    by design.” For instance, generative image and language models may create different
    results for identical prompts to produce a diverse sample of results. For image
    models, this is often so that users can select the most accurate and aesthetically
    pleasing image. For language models, this is often to vary the responses, for
    example, in chat agents, to avoid repetition.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 除了之前提到的各种随机性来源之外，某些模型在推理过程中可能会表现出随机行为，我们可以将其视为“设计上的随机性”。例如，生成式图像和语言模型可能会为相同的提示生成不同的结果，从而产生多样的样本结果。对于图像模型，这通常是为了让用户选择最准确和最具美感的图像。对于语言模型，这通常是为了变化回答，例如，在聊天代理中，避免重复。
- en: The intended randomness in generative image models during inference is often
    due to sampling different noise values at each step of the reverse process. In
    diffusion models, a noise schedule defines the noise variance added at each step
    of the diffusion process.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式图像模型在推理过程中预期的随机性通常是由于在反向过程的每一步采样不同的噪声值。在扩散模型中，噪声调度定义了在每一步扩散过程中添加的噪声方差。
- en: Autoregressive LLMs like GPT tend to create different outputs for the same input
    prompt (GPT will be discussed at greater length in [Chapters 14](ch14.xhtml) and
    [17](ch17.xhtml)). The ChatGPT user interface even has a Regenerate Response button
    for that purpose. The ability to generate different results is due to the sampling
    strategies these models employ. Techniques such as top-*k* sampling, nucleus sampling,
    and temperature scaling influence the model’s output by controlling the degree
    of randomness. This is a feature, not a bug, since it allows for diverse responses
    and prevents the model from producing overly deterministic or repetitive outputs.
    (See [Chapter 9](ch09.xhtml) for a more in-depth overview of generative AI and
    deep learning models; see [Chapter 17](ch17.xhtml) for more detail on autoregressive
    LLMs.)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 自回归的语言模型（如GPT）倾向于为相同的输入提示生成不同的输出（GPT将在[第14章](ch14.xhtml)和[第17章](ch17.xhtml)中详细讨论）。ChatGPT用户界面甚至有一个“重新生成回答”按钮来实现这一功能。模型能够生成不同结果的原因在于它们使用的采样策略。像top-*k*采样、核采样和温度缩放等技术通过控制随机性程度来影响模型的输出。这是一个特性，而不是一个缺陷，因为它允许生成多样的回答，并防止模型产生过于确定性或重复的输出。（有关生成式AI和深度学习模型的更深入概述，请参见[第9章](ch09.xhtml)；关于自回归语言模型的更多细节，请参见[第17章](ch17.xhtml)）。
- en: '*Top-*k *sampling*, illustrated in [Figure 10-3](ch10.xhtml#ch10fig3), works
    by sampling tokens from the top *k* most probable candidates at each step of the
    next-word generation process.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图 10-3](ch10.xhtml#ch10fig3)所示，*Top-*k *采样*通过在每一步生成下一个词时，从最有可能的前 *k* 个候选词中进行抽样来工作。
- en: '![Image](../images/10fig03.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/10fig03.jpg)'
- en: '*Figure 10-3: Top-*k *sampling*'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 10-3: Top-*k *采样*'
- en: Given an input prompt, the language model produces a probability distribution
    over the entire vocabulary (the candidate words) for the next token. Each token
    in the vocabulary is assigned a probability based on the model’s understanding
    of the context. The selected top-*k* tokens are then renormalized so that the
    probabilities sum to 1\. Finally, a token is sampled from the renormalized top-*k*
    probability distribution and is appended to the input prompt. This process is
    repeated for the desired length of the generated text or until a stop condition
    is met.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个输入提示，语言模型会对下一个词的整个词汇表（候选词）生成一个概率分布。每个词汇表中的词根据模型对上下文的理解被分配一个概率。然后，选出的前 *k*
    个词会重新归一化，使它们的概率总和为 1。最后，从重新归一化后的 top-*k* 概率分布中抽取一个词，并将其附加到输入提示中。这个过程会重复进行，直到生成文本的长度达到预期，或者满足停止条件。
- en: '*Nucleus sampling* (also known as *top-*p *sampling*), illustrated in [Figure
    10-4](ch10.xhtml#ch10fig4), is an alternative to top-*k* sampling.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*核采样*（也叫做 *top-*p *采样*），如[图 10-4](ch10.xhtml#ch10fig4)所示，是 top-*k* 采样的一个替代方法。'
- en: '![Image](../images/10fig04.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/10fig04.jpg)'
- en: '*Figure 10-4: Nucleus sampling*'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 10-4: 核采样*'
- en: Similar to top-*k* sampling, the goal of nucleus sampling is to balance diversity
    and coherence in the output. However, nucleus and top-*k* sampling differ in how
    to select the candidate tokens for sampling at each step of the generation process.
    Top-*k* sampling selects the *k* most probable tokens from the probability distribution
    produced by the language model, regardless of their probabilities. The value of
    *k* remains fixed throughout the generation process. Nucleus sampling, on the
    other hand, selects tokens based on a probability threshold *p*, as shown in [Figure
    10-4](ch10.xhtml#ch10fig4). It then accumulates the most probable tokens in descending
    order until their cumulative probability meets or exceeds the threshold *p*. In
    contrast to top-*k* sampling, the size of the candidate set (nucleus) can vary
    at each step.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 与 top-*k* 采样类似，核采样的目标是在输出中平衡多样性和一致性。然而，核采样和 top-*k* 采样在每一步生成过程中选择候选词的方式上有所不同。Top-*k*
    采样从语言模型生成的概率分布中选择 *k* 个最有可能的词，而不考虑它们的具体概率值。*k* 的值在整个生成过程中保持不变。另一方面，核采样是根据一个概率阈值
    *p* 来选择词，如[图 10-4](ch10.xhtml#ch10fig4)所示。它会按降序累积最有可能的词，直到它们的累积概率达到或超过阈值 *p*。与
    top-*k* 采样不同，候选集（核）在每一步可能会有所变化。
- en: '**Exercises**'
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**练习**'
- en: '**10-1.** Suppose we train a neural network with top-*k* or nucleus sampling
    where *k* and *p* are hyperparameter choices. Can we make the model behave deterministically
    during inference without changing the code?'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**10-1.** 假设我们用 top-*k* 或核采样训练一个神经网络，其中 *k* 和 *p* 是超参数选择。我们是否可以在推理过程中使模型表现得像确定性一样，而不修改代码？'
- en: '**10-2.** In what scenarios might random dropout behavior during inference
    be desired?'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**10-2.** 在什么情况下，推理过程中的随机 dropout 行为是需要的？'
- en: '**References**'
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: 'For more about different data sampling and model evaluation techniques, see
    my article: “Model Evaluation, Model Selection, and Algorithm Selection in Machine
    Learning” (2018), *[https://arxiv.org/abs/1811.12808](https://arxiv.org/abs/1811.12808)*.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于不同的数据采样和模型评估技术，请参见我的文章：“机器学习中的模型评估、模型选择和算法选择”（2018），*[https://arxiv.org/abs/1811.12808](https://arxiv.org/abs/1811.12808)*。
- en: 'The paper that originally proposed the dropout technique: Nitish Srivastava
    et al., “Dropout: A Simple Way to Prevent Neural Networks from Overfitting” (2014),
    *[https://jmlr.org/papers/v15/srivastava14a.html](https://jmlr.org/papers/v15/srivastava14a.html)*.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '最初提出 dropout 技术的论文：Nitish Srivastava 等人，"Dropout: A Simple Way to Prevent Neural
    Networks from Overfitting"（2014），*[https://jmlr.org/papers/v15/srivastava14a.html](https://jmlr.org/papers/v15/srivastava14a.html)*。'
- en: 'A detailed paper on FFT-based convolution: Lu Chi, Borui Jiang, and Yadong
    Mu, “Fast Fourier Convolution” (2020), *[https://dl.acm.org/doi/abs/10.5555/3495724.3496100](https://dl.acm.org/doi/abs/10.5555/3495724.3496100)*.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于基于 FFT 卷积的详细论文：Lu Chi、Borui Jiang 和 Yadong Mu，“Fast Fourier Convolution”（2020），*[https://dl.acm.org/doi/abs/10.5555/3495724.3496100](https://dl.acm.org/doi/abs/10.5555/3495724.3496100)*。
- en: 'Details on Winograd-based convolution: Syed Asad Alam et al., “Winograd Convolution
    for Deep Neural Networks: Efficient Point Selection” (2022), *[https://arxiv.org/abs/2201.10369](https://arxiv.org/abs/2201.10369)*.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于 Winograd 的卷积详细信息：Syed Asad Alam 等人，《Winograd 卷积在深度神经网络中的应用：高效的点选择》（2022），*[https://arxiv.org/abs/2201.10369](https://arxiv.org/abs/2201.10369)*。
- en: 'More information about the deterministic algorithm settings in Py-Torch: *[https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html](https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html)*.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关 Py-Torch 中确定性算法设置的更多信息：*[https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html](https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html)*。
- en: 'For details on the deterministic behavior of NVIDIA graphics cards, see the
    “Reproducibility” section of the official NVIDIA documentation: *[https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#reproducibility](https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#reproducibility)*.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关 NVIDIA 显卡的确定性行为的详细信息，请参见 NVIDIA 官方文档中的“可复现性”部分：*[https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#reproducibility](https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#reproducibility)*。
