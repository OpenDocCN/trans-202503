- en: '**5'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**5'
- en: 'CONVOLUTIONAL NEURAL NETWORKS: AI LEARNS TO SEE**'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络：人工智能学会看见**
- en: '![Image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/common.jpg)'
- en: Classical machine learning models struggle with appropriate feature selection,
    feature vector dimensionality, and the inability to learn from the structure inherent
    in the input. [*Convolutional neural networks (CNNs)*](glossary.xhtml#glo24) overcome
    these issues by learning to generate new representations of their inputs while
    simultaneously classifying them, a process known as [*end-to-end learning*](glossary.xhtml#glo35).
    CNNs are the representation-learning data processors I referred to in [Chapter
    2](ch02.xhtml).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的机器学习模型在特征选择、特征向量维度以及无法从输入中固有的结构中学习方面存在困难。[ *卷积神经网络（CNNs）* ](glossary.xhtml#glo24)通过学习生成输入的新的表示，并同时进行分类，从而克服了这些问题，这一过程被称为[
    *端到端学习* ](glossary.xhtml#glo35)。CNN是我在[第2章](ch02.xhtml)中提到的表示学习数据处理器。
- en: Elements of what became CNNs appeared at various times throughout the history
    of neural networks, beginning with Rosenblatt’s Perceptron, but the architecture
    that ushered in the deep learning revolution was published in 1998\. Over a decade
    of additional improvements in computing capability were required to unleash the
    full power of CNNs with the appearance of AlexNet in 2012.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 形成CNN的元素在神经网络的发展历史中不同时期都有出现，从Rosenblatt的感知器开始，但引领深度学习革命的架构是在1998年发布的。为了充分释放CNN的力量，还需要十多年的计算能力的改进，直到2012年AlexNet的出现。
- en: Convolutional networks exploit structure in their inputs. We’ll better understand
    what that means as the chapter progresses. In one dimension, the inputs might
    be values that change over time, also known as a time series. In two dimensions,
    we’re talking about images. Three-dimensional CNNs exist to interpret volumes
    of data, like a stack of magnetic resonance images or a volume constructed from
    a LiDAR point cloud. In this chapter, we’ll focus exclusively on two-dimensional
    CNNs.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积网络利用输入中的结构。随着章节的推进，我们将更好地理解这意味着什么。在一维中，输入可能是随时间变化的值，也称为时间序列。在二维中，我们讨论的是图像。三维CNN存在，用于解释数据体积，比如一堆磁共振成像数据或由LiDAR点云构建的体积。在本章中，我们将专注于二维CNN。
- en: The order in which features are presented to a traditional neural network is
    irrelevant. Regardless of whether we present feature vectors to the model as (*x*[0],*x*[1],*x*[2])
    or (*x*[2],*x*[0],*x*[1]), the model will learn just as well because it assumes
    the features are independent and unrelated to each other. Indeed, a strong correlation
    between a pixel value and adjacent pixel values is something traditional machine
    learning models do not want, and their inability to achieve much success with
    such inputs held neural networks back for years.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 向传统神经网络呈现特征的顺序是无关紧要的。无论我们将特征向量以(*x*[0],*x*[1],*x*[2]) 还是 (*x*[2],*x*[0],*x*[1])
    的顺序呈现给模型，模型都会同样学习得很好，因为它假设特征是独立的，相互之间没有关系。事实上，像素值与相邻像素值之间的强相关性是传统机器学习模型不希望出现的，而它们无法在这种输入下取得成功，也使得神经网络在多年里未能取得突破。
- en: Convolutional neural networks, on the other hand, exploit structure in their
    inputs. For a CNN, it matters whether we present the input as (*x*[0],*x*[1],*x*[2])
    or (*x*[2],*x*[0],*x*[1]); the model might learn well with the former and poorly
    with the latter. This isn’t a weakness, but a strength, because we want to apply
    CNNs to situations where there is structure to learn—structure that helps determine
    how best to classify inputs.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）则利用输入中的结构。对于CNN来说，输入是以(*x*[0],*x*[1],*x*[2]) 还是 (*x*[2],*x*[0],*x*[1])
    的顺序呈现是有区别的；模型可能在前者上表现良好，而在后者上表现差。这并不是弱点，而是优势，因为我们希望将CNN应用于那些需要学习结构的情况——这些结构有助于确定如何最佳地分类输入。
- en: Later in the chapter, we’ll compare the performance of a traditional neural
    network to a CNN when classifying small photos of animals and vehicles (the CIFAR-10
    dataset of [Chapter 3](ch03.xhtml)). At that time, we’ll learn the true power
    of exploiting structure. Before that, however, let’s conduct a little experiment.
    We have two datasets. The first is our old friend, the MNIST digits dataset; the
    second is the same collection of digit images, but the order of the pixels in
    the images has been scrambled. The scrambling isn’t random but consistent so that
    the pixel at position (1,12) has been moved to, say, position (26,13), with similarly
    consistent moves for all other pixels. [Figure 5-1](ch05.xhtml#ch05fig01) shows
    some examples of MNIST digits and scrambled versions of the same digits.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章后面，我们将比较传统神经网络与卷积神经网络（CNN）在分类小动物和车辆照片时的表现（见第 3 章的 CIFAR-10 数据集）。到那时，我们将了解利用结构的真正力量。然而，在此之前，让我们做一个小实验。我们有两个数据集，第一个是我们熟悉的
    MNIST 数字数据集；第二个是相同的数字图像集合，但是这些图像中的像素顺序已经被打乱。打乱不是随机的，而是有规律的，使得例如位置（1,12）的像素被移动到了位置（26,13），其他所有像素也做了类似一致的移动。[图
    5-1](ch05.xhtml#ch05fig01) 显示了 MNIST 数字和其打乱版本的一些示例。
- en: '![Image](../images/ch05fig01.jpg)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/ch05fig01.jpg)'
- en: '*Figure 5-1: Example MNIST digits (top) and scrambled versions of the same
    digits (bottom)*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5-1：示例 MNIST 数字（上）和相同数字的打乱版本（下）*'
- en: The scrambled digits are incomprehensible to me. The pixel information between
    the original and scrambled digits is the same—that is, the same collection of
    pixel values is present in both—but the structure is largely gone, and I can no
    longer discern the digits. I claim that a traditional neural network treats its
    inputs holistically and isn’t looking for structure. If that’s the case, a traditional
    neural network shouldn’t care that the digits have been scrambled; it should learn
    just as well when trained using the original or the scrambled dataset. As it turns
    out, that’s precisely what happens. The model learns equally well; scrambling
    changes nothing in terms of performance. Note, though, that the scrambled test
    digits must be used with the scrambled model; we shouldn’t expect the model to
    work when trained on one dataset and tested on the other.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这些打乱的数字我无法理解。原始数字和打乱数字之间的像素信息是相同的——也就是说，两个图像中的像素值集合是一样的——但是结构几乎完全丧失，我再也无法辨认出数字了。我认为传统的神经网络是整体处理输入的，它并不寻找结构。如果是这样，传统神经网络应该不在乎数字是否已经被打乱；它在使用原始数据集或打乱数据集进行训练时应该表现一样好。事实证明，正是如此。模型学习得同样好；打乱数据集对性能没有影响。但请注意，打乱的测试数字必须与打乱的模型一起使用；我们不能期望模型在一个数据集上训练并在另一个数据集上测试时仍能正常工作。
- en: 'We at present know only one fact about CNNs: they pay attention to structure
    in their inputs. Knowing this, should we expect a CNN trained on the scrambled
    dataset to perform as well as one trained on the original dataset? The scrambled
    digits are uninterpretable by us because local structure in the images has been
    destroyed. Therefore, we might expect a model that similarly wants to exploit
    local structure to be unable to interpret the scrambled digits. And that is the
    case: a CNN trained on the scrambled dataset performs poorly compared to one trained
    on the original dataset.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 目前我们只知道一个关于CNN的事实：它们关注输入中的结构。知道这一点后，我们是否应该期待在打乱数据集上训练的CNN表现与在原始数据集上训练的CNN一样好呢？打乱的数字对我们来说是无法解读的，因为图像中的局部结构已经被破坏。因此，我们可能会预期一个同样想要利用局部结构的模型也无法解读这些打乱的数字。事实的确如此：一个在打乱数据集上训练的CNN相比于一个在原始数据集上训练的CNN，表现较差。
- en: 'Why can’t we easily interpret the scrambled digits? We must explore what happens
    in the brain during vision to answer that question. Then we’ll circle back to
    relate that process to what CNNs do. As we’ll learn, CNNs follow the old adage:
    when in Rome, do as the Romans (humans) do.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们无法轻松地解读这些打乱的数字呢？我们必须探讨视觉过程中大脑发生了什么，才能回答这个问题。然后，我们将回到这个过程并将其与卷积神经网络（CNN）所做的工作联系起来。正如我们将要了解的，CNN遵循着古老的谚语：入乡随俗（人类的做法）。
- en: '****'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '****'
- en: Vincent van Gogh is my favorite artist. Something about his style speaks to
    me, something strangely peaceful from a man tormented by mental illness. I believe
    the peace emanating from his work reflects his attempt to calm the turmoil within.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 文森特·梵高是我最喜欢的艺术家。他的风格有某种神奇的吸引力，来自一个被精神疾病折磨的人的奇异平静。我相信，他作品中散发的平静反映了他试图平息内心的动荡。
- en: Consider [Figure 5-2](ch05.xhtml#ch05fig02). It shows Van Gogh’s famous 1889
    painting of his bedroom in Arles. The image is in black and white, an unforgivable
    violence to Vincent’s use of color, but print restrictions require it.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 请看[图5-2](ch05.xhtml#ch05fig02)。它展示了梵高1889年在阿尔勒创作的著名《卧室》画作。图像是黑白的，这对梵高使用色彩来说是一种无法原谅的暴力，但印刷限制要求如此。
- en: '![Image](../images/ch05fig02.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch05fig02.jpg)'
- en: '*Figure 5-2: Van Gogh’s bedroom in Arles, 1889 (public domain)*'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5-2：梵高的阿尔勒卧室，1889年（公共领域）*'
- en: What do you see in the painting? I’m not asking about a higher meaning or impression,
    but objectively, what do you see in the painting? I see a bed, two chairs, a small
    table, a window, and a pitcher on the table, among many other items. I suspect
    you see the same. You saw the bed, two chairs, and table, but how? Photons, particles
    of light, traveled from the image to your eye and were converted into discrete
    objects in your brain. Again, how?
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你在画中看到了什么？我不是在问更深的意义或印象，而是客观地问，你在画中看到了什么？我看到一张床、两把椅子、一张小桌子、一扇窗户和桌上的一个水壶，当然还有许多其他物品。我猜你看到的也差不多。你看到了床、两把椅子和桌子，但你是怎么看到的？光子，光的粒子，从画面传到你的眼睛，并在大脑中转化为离散的物体。再说一次，怎么做到的呢？
- en: I’m asking questions but not yet offering answers. That’s okay for two reasons.
    First, pondering the problem of segmenting an image into a collection of meaningful
    objects is worth some effort on our part. Second, no one yet knows the full answer
    to “how?” Neuroscientists do, however, understand the beginnings of the process.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我在提出问题，但尚未给出答案。这是可以的，原因有两个。首先，思考如何将图像分割成一组有意义的物体是值得我们努力的。其次，目前还没有人完全知道“如何？”的问题。然而，神经科学家确实理解了这个过程的起始部分。
- en: We take for granted the ability to look at a scene and parse it into separate
    and identified objects. For us, the process is effortless, completely automatic.
    We shouldn’t be fooled. We’re the beneficiaries of hundreds of millions of years
    of evolution’s tinkering. For mammals, vision begins in the eye, but parsing and
    understanding begins in the primary visual cortex at the back of our brains.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们理所当然地认为，能够看一个场景并将其解析为单独的已识别物体是理所应当的。对我们来说，这个过程是毫不费力、完全自动化的。但我们不应被愚弄。我们是数亿年进化努力的受益者。对哺乳动物来说，视觉始于眼睛，但解析和理解始于大脑后部的初级视觉皮层。
- en: The primary visual cortex, known as area V1, is sensitive to edges and orientation.
    Immediately, we encounter a clue to how vision works in the brain (as opposed
    to the eye). The brain takes the input sensations, spread over V1 as a warped
    image, and begins by seeking edges and the orientation of the edges. V1 is additionally
    sensitive to color. Mapping the entire visual field over V1, with magnification
    so that most of V1 is occupied by the central 2 percent of our visual field, means
    that edge detection, orientation, and color are local to where they occur.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 初级视觉皮层，通常被称为区域V1，对边缘和方向非常敏感。我们立即就遇到了一个关于视觉如何在大脑中工作的线索（与眼睛相比）。大脑将输入的感觉信息传播到V1上，V1呈现出扭曲的图像，并开始寻找边缘以及边缘的方向。V1还对颜色敏感。将整个视觉场映射到V1上，并进行放大，使得V1的大部分区域由我们视觉场的中央2%占据，这意味着边缘检测、方向和颜色都是局部的，发生在其所在的位置。
- en: V1 sends its detections to area V2, which sends its detections to area V3, and
    so on through V4 to V5, with each area receiving, essentially, a representation
    of larger and more grouped elements of what is in the visual field. The process
    starts with V1 and, eventually, delivers a fully parsed and understood representation
    of what the eyes see. As mentioned, the details much beyond V1 are murky, but
    for our purposes all we need to remember is that V1 is sensitive to edges, the
    orientation of edges, and colors (we might also include textures). Starting simply
    and grouping to separate objects in the scene is the name of the game. CNNs mimic
    this process. It’s fair to say that CNNs literally learn to see the world of their
    inputs.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: V1将其检测结果发送到区域V2，V2再将其检测结果发送到区域V3，以此类推，直到V5，每个区域接收到的基本上是视觉场中更大、更聚合的元素的表示。这个过程从V1开始，最终提供了眼睛所见的完全解析和理解的表示。如前所述，V1以外的细节仍不明确，但就我们来说，我们只需要记住V1对边缘、边缘的方向和颜色（我们也可以包括纹理）很敏感。从简单开始并将场景中的物体分组并区分开来是整个过程的关键。卷积神经网络（CNN）模拟了这一过程。可以公平地说，CNN们实际上学会了如何看待它们的输入世界。
- en: 'CNNs decompose inputs into small parts, then groups of parts and still larger
    groups of groups of parts, until the entire input is transformed from a single
    whole into a new representation: one that is more easily understood by what amounts
    to a traditional neural network sitting at the top of the model. However, mapping
    the input to a new, more easily understood representation does not imply that
    the new representation is more easily understood by *us*.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs 将输入分解为小部分，然后是小部分的组合，再到更大一组部分的组合，直到整个输入从一个整体转变为新的表示形式：这种表示形式更容易被模型顶部的传统神经网络理解。然而，将输入映射到一个新的、更易理解的表示并不意味着这个新表示对*我们*来说更容易理解。
- en: Convolutional neural networks learn during training to partition inputs into
    parts, enabling the top layers of the network to classify successfully. In other
    words, CNNs learn new representations of their inputs and then classify those
    new representations. Indeed, “Learning New Representations from Old” was an early
    title for this chapter.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络在训练过程中学习将输入分割成部分，从而使网络的顶层能够成功分类。换句话说，CNN 学会了输入的新表示形式，然后对这些新表示进行分类。事实上，“从旧的学习新的表示”曾是本章的早期标题。
- en: How do CNNs break their inputs into parts? To answer that question, we must
    first understand the “convolution” part of “convolutional neural network.” Be
    warned, low-level details ahead.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 是如何将输入分解成部分的？为了回答这个问题，我们必须首先理解“卷积神经网络”中的“卷积”部分。请注意，接下来将涉及一些底层细节。
- en: '****'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '****'
- en: '[*Convolution*](glossary.xhtml#glo22) is a mathematical operation with a formal
    definition involving integral calculus. Fortunately for us, convolution is a straightforward
    operation in digital images, using nothing more than multiplication and addition.
    Convolution slides a small square, known as a [*kernel*](glossary.xhtml#glo58),
    over the image from top to bottom and left to right. At each position, convolution
    multiplies the pixel values covered by the square with the corresponding kernel
    values. It then sums all those products to produce a single number that becomes
    the output pixel value for that position. Words only go so far here, so let’s
    try a picture. Consider [Figure 5-3](ch05.xhtml#ch05fig03).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[*卷积*](glossary.xhtml#glo22)是一种数学运算，具有正式的定义，涉及积分计算。幸运的是，在数字图像处理中，卷积是一个简单的操作，仅使用乘法和加法。卷积操作是将一个小方块，称为
    [*卷积核*](glossary.xhtml#glo58)，从上到下、从左到右地滑动图像。在每个位置，卷积将方块覆盖的像素值与相应的卷积核值相乘，然后将所有这些乘积相加，得到一个单一的数值，这个数值成为该位置的输出像素值。仅靠文字难以表达清楚，让我们看看图示。[图
    5-3](ch05.xhtml#ch05fig03)可以帮助理解。'
- en: '![Image](../images/ch05fig03.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch05fig03.jpg)'
- en: '*Figure 5-3: Convolving a kernel over an image*'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5-3：将卷积核卷积到图像上*'
- en: The left side of [Figure 5-3](ch05.xhtml#ch05fig03) shows a grid of numbers.
    These are the pixel values for the center portion of the image in [Figure 5-4](ch05.xhtml#ch05fig04).
    Grayscale pixel values are typically in the range 0 through 255, where lower values
    are darker. The kernel is the 3×3 grid to the right. The convolution operation
    instructs us to multiply each pixel value by the corresponding kernel value. This
    produces the rightmost 3×3 grid of numbers. The final step sums all nine values
    to create a single output, 48, which replaces the center pixel in the output image,
    60 → 48.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-3](ch05.xhtml#ch05fig03)的左侧显示了一组数字。这些是[图 5-4](ch05.xhtml#ch05fig04)中图像中心部分的像素值。灰度像素值通常在
    0 到 255 的范围内，其中较低的值代表较暗的像素。卷积核是右侧的 3×3 网格。卷积操作要求我们将每个像素值与对应的卷积核值相乘，得到右侧的 3×3 数字网格。最后一步是将这九个值相加，得到一个单一的输出值
    48，替代输出图像中的中心像素，60 → 48。'
- en: To complete the convolution, slide the 3×3 solid box one pixel to the right
    and repeat. When the end of a row is reached, move the box down one pixel and
    repeat for the next row, then process row-by-row until the kernel has covered
    the entire image. The convoluted image is the collection of new output pixels.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成卷积，将 3×3 的固体框框向右移动一个像素并重复此过程。当到达一行的末尾时，将框框向下移动一个像素，并对下一行重复，逐行处理，直到卷积核覆盖整个图像。卷积后的图像是所有新输出像素的集合。
- en: At first, convolution might seem like a strange thing to do. However, in digital
    images, convolution is a fundamental operation. An appropriately defined kernel
    lets us filter an image to enhance it in various ways. For example, [Figure 5-4](ch05.xhtml#ch05fig04)
    shows four images. The upper left is the original image, a frequently used test
    image of Gold Hill in Shaftesbury, England. The remaining three images are filtered
    versions of the original. Clockwise from the upper right, we have a blurred version,
    one showing horizontal edges, and one showing vertical edges. Each image is produced
    by convolving a kernel as described previously. The kernel of [Figure 5-3](ch05.xhtml#ch05fig03)
    produces the horizontal-edge image at the lower right. Rotate the kernel by 90
    degrees, and you get the vertical-edge image at the lower left. Finally, make
    all the kernel values 1, and you get the blurred image at the upper right. Note
    that the edge images are inverted to make the detected edges black instead of
    white.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 起初，卷积可能看起来是一个奇怪的操作。然而，在数字图像中，卷积是一个基本操作。适当地定义卷积核可以让我们以多种方式过滤图像并增强其效果。例如，[图 5-4](ch05.xhtml#ch05fig04)展示了四幅图像。左上角是原始图像，这是一张常用的测试图像，来自英国
    Shaftesbury 的 Gold Hill。其余三幅图像是原始图像的过滤版本。从右上角开始，顺时针方向，分别是模糊图像、展示水平边缘的图像和展示垂直边缘的图像。每一幅图像都是通过前面所述的卷积核进行卷积生成的。[图
    5-3](ch05.xhtml#ch05fig03)中的卷积核产生了右下角的水平边缘图像。将卷积核旋转 90 度，就得到了左下角的垂直边缘图像。最后，将所有卷积核的值设为
    1，就得到了右上角的模糊图像。注意，边缘图像是反转的，使得检测到的边缘是黑色而不是白色。
- en: '![Image](../images/ch05fig04.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/ch05fig04.jpg)'
- en: '*Figure 5-4: Convolution kernels in action*'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5-4：卷积核的作用*'
- en: The critical point for us to remember is that convolving an image with different
    kernels highlights different aspects of the image. It isn’t hard to imagine an
    appropriate set of kernels extracting structure relevant to correctly classifying
    the image. This is exactly what CNNs do during end-to-end training and, in a sense,
    what our visual system does in area V1 when it detects edges, orientations, colors,
    and textures.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要记住的关键点是，对图像进行不同卷积核的卷积可以突出图像的不同方面。很容易想象，适当的卷积核集合可以提取出对正确分类图像相关的结构。这正是 CNN
    在端到端训练过程中所做的，某种程度上，也是我们视觉系统在 V1 区域中检测边缘、方向、颜色和纹理时所做的。
- en: We’re making progress. We now have a handle on the core operation of a CNN,
    convolution, so let’s take the next step to learn how convolution is used within
    a model to extract structure and build a new representation of the input.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在取得进展。我们现在已经掌握了 CNN 的核心操作——卷积，所以让我们迈出下一步，学习卷积如何在模型中用于提取结构，并构建输入的新表示。
- en: '****'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '****'
- en: 'The traditional neural networks of [Chapter 4](ch04.xhtml) consist of a single
    kind of layer: a collection of fully connected nodes accepting input from the
    layer below to produce output for the layer above. Convolutional neural networks
    are more flexible and support diverse layer types. Regardless, the data flow is
    the same: from input to layer after layer to the network’s output.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[第 4 章](ch04.xhtml)中的传统神经网络由一种类型的层组成：一组完全连接的节点，从下层接收输入，为上层生成输出。卷积神经网络更加灵活，支持多种类型的层。无论如何，数据流是相同的：从输入到一层又一层，最终到达网络的输出。'
- en: In CNN parlance, the fully connected layers a traditional neural network uses
    are called *dense* layers. CNNs usually use dense layers at the top, near the
    output, because by that time the network has transformed the input into a new
    representation, one that the fully connected layers can classify successfully.
    CNNs make heavy use of convolutional layers and pooling layers.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CNN 的术语中，传统神经网络使用的完全连接层被称为*密集*层。CNN 通常在顶部、靠近输出的地方使用密集层，因为到那个时候，网络已经将输入转换为新的表示，密集层可以成功地进行分类。CNN
    大量使用卷积层和池化层。
- en: '*Convolutional layers* apply a collection of kernels to their input to produce
    multiple outputs, much as [Figure 5-4](ch05.xhtml#ch05fig04) produced three outputs
    from the one input image at the upper left. The kernels are learned during training
    using the same backpropagation and gradient descent approach we encountered in
    [Chapter 4](ch04.xhtml). The values of the learned kernels are the weights of
    the convolutional layer.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*卷积层* 将一组卷积核应用于其输入，以产生多个输出，就像[图 5-4](ch05.xhtml#ch05fig04)从左上角的单一输入图像中产生了三个输出一样。卷积核在训练过程中通过与我们在[第
    4 章](ch04.xhtml)中遇到的相同的反向传播和梯度下降方法进行学习。学习到的卷积核的值即为卷积层的权重。'
- en: '[*Pooling layers*](glossary.xhtml#glo81) have no weights associated with them.
    There’s nothing to learn. Rather, pooling layers perform a fixed operation on
    their inputs: they reduce the spatial extent of their inputs by keeping the largest
    value in a 2×2 square moved without overlap across and then down. The net effect
    is similar to reducing the size of an image by a factor of two. [Figure 5-5](ch05.xhtml#ch05fig05)
    illustrates the process of changing an 8×8 input into a 4×4 output, keeping the
    maximum value in each solid square. Pooling layers are a concession to reduce
    the number of parameters in the network.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[*池化层*](glossary.xhtml#glo81)没有与之相关的权重。没有需要学习的内容。相反，池化层对其输入执行固定操作：通过在2×2的正方形内保留最大值，并将其移动，不重叠地横向和纵向滑动，来减少输入的空间范围。其净效应类似于将图像的大小减少一倍。[图
    5-5](ch05.xhtml#ch05fig05)展示了将一个8×8的输入转换为4×4输出的过程，保留每个实心正方形中的最大值。池化层是为了减少网络中参数的数量而做出的妥协。'
- en: '![Image](../images/ch05fig05.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch05fig05.jpg)'
- en: '*Figure 5-5: Pooling to reduce the spatial extent of the data*'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5-5：池化以减少数据的空间范围*'
- en: 'A typical CNN combines convolutional and pooling layers before topping things
    off with a dense layer or two. ReLU layers are used as well, usually after the
    convolutional and dense layers. For example, a classic CNN architecture known
    as LeNet consists of the following layers:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的卷积神经网络（CNN）将卷积层和池化层结合起来，最后加上一两个全连接层。ReLU层也会被使用，通常在卷积层和全连接层之后。例如，一个经典的CNN架构LeNet由以下层次组成：
- en: '![Image](../images/87fig01.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/87fig01.jpg)'
- en: The model uses three convolutional layers, two pooling layers, and a single
    dense layer with 84 nodes. Each convolutional and dense layer is followed by a
    ReLU layer to map all negative inputs to zero while leaving all positive inputs
    untouched.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型使用三个卷积层、两个池化层和一个包含84个节点的单一全连接层。每个卷积层和全连接层后面都跟着一个ReLU层，用于将所有负输入映射为零，同时保持所有正输入不变。
- en: The number in parentheses for each convolutional layer is the number of filters
    to learn in that layer. A [*filter*](glossary.xhtml#glo44) is a collection of
    convolutional kernels, with one kernel for each input channel. For example, the
    first convolutional layer learns six filters. The input is a grayscale image with
    one channel, so this layer learns six kernels. The second convolutional layer
    learns 16 filters, each with 6 kernels, one for each of the 6 input channels from
    the first convolutional layer. Therefore, the second convolutional layer learns
    a total of 96 kernels. Finally, the last convolutional layer learns 120 filters,
    each with 16 kernels, for another 1,920 kernels. All told, the LeNet model needs
    to learn 2,022 different convolutional kernels.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 每个卷积层的括号内数字是该层需要学习的过滤器数量。[*过滤器*](glossary.xhtml#glo44)是卷积核的集合，每个输入通道有一个卷积核。例如，第一个卷积层学习六个过滤器。输入是一个灰度图像，只有一个通道，因此该层学习六个卷积核。第二个卷积层学习16个过滤器，每个过滤器有6个卷积核，分别对应第一个卷积层的6个输入通道。因此，第二个卷积层一共学习96个卷积核。最后一个卷积层学习120个过滤器，每个过滤器有16个卷积核，总共学习1,920个卷积核。总的来说，LeNet模型需要学习2,022个不同的卷积核。
- en: The hope is that learning so many kernels will produce a sequence of outputs
    that capture essential elements of the structures in the input. If training is
    successful, the output of the final convolutional layer, as a vector input to
    the dense layer, will contain values that clearly differentiate between classes—at
    least, more clearly than can be accomplished by using the image alone.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 希望通过学习这么多的卷积核，能够产生一系列输出，这些输出能够捕捉输入中结构的关键元素。如果训练成功，最后一个卷积层的输出，作为全连接层的向量输入，将包含能够清晰地区分不同类别的值——至少比仅使用图像更清晰。
- en: If it feels like we’re in the weeds, we are, but we will not dig further. We’ve
    reached the lowest level of detail we’ll consider in the book, in fact, but it’s
    a necessary burden, as we cannot understand how CNNs work if we don’t understand
    convolution and convolutional layers.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果感觉我们深入到了细节，那确实是，但我们不会再继续深入。实际上，我们已经到达了本书中考虑的最低细节层次，但这是一个必要的负担，因为如果不理解卷积和卷积层，我们就无法理解CNN是如何工作的。
- en: Perhaps the best way to understand what the layers of a CNN are doing is to
    look at their effect on data flowing through the network. [Figure 5-6](ch05.xhtml#ch05fig06)
    shows how a LeNet model trained on MNIST digits manipulates two input images.
    The output of the first convolutional layer is the six middle images, where gray
    represents zero, darker pixels are increasingly negative, and lighter pixels are
    increasingly positive. The six kernels of the first convolutional layer each produce
    an output image for the single input image. The kernels highlight different portions
    of the inputs as transitions from dark to light.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 也许理解CNN各层作用的最好方法，是观察它们对数据流动的影响。[图 5-6](ch05.xhtml#ch05fig06)展示了一个在MNIST数字上训练的LeNet模型如何处理两个输入图像。第一卷积层的输出是六张中间图像，其中灰色表示零，较暗的像素表示越来越负，较亮的像素表示越来越正。第一卷积层的六个核各自为单个输入图像产生一个输出图像。卷积核突出显示了输入图像的不同部分，呈现从暗到亮的过渡。
- en: '![Image](../images/ch05fig06.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch05fig06.jpg)'
- en: '*Figure 5-6: Input to first convolutional layer to dense layer*'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5-6：从第一卷积层到密集层的输入*'
- en: The rightmost barcode-like pattern is a representation of the dense layer’s
    output. We’re ignoring the output of the second and third convolutional layers
    and jumping directly to the end of the model. The dense layer’s output is a vector
    of 84 numbers. For [Figure 5-6](ch05.xhtml#ch05fig06), I mapped these numbers
    to pixel values, where larger values correspond to darker vertical bars.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 最右侧的条形码样式图案是密集层输出的表示。我们忽略了第二和第三卷积层的输出，直接跳到了模型的最后部分。密集层的输出是一个包含84个数字的向量。对于[图
    5-6](ch05.xhtml#ch05fig06)，我将这些数字映射到像素值，其中较大的值对应于较暗的垂直条形。
- en: Notice that the barcodes for the digits 0 and 8 differ. If the model learned
    well, we might expect the barcodes for the dense layer outputs to share commonalities
    across digits. In other words, the barcodes for zeros should look roughly similar,
    as should the barcodes for eights. Do they? Consider [Figure 5-7](ch05.xhtml#ch05fig07).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，数字 0 和 8 的条形码不同。如果模型学得很好，我们可以预期密集层输出的条形码在不同数字间会有一些共性。换句话说，数字 0 的条形码应该大致相似，数字
    8 的条形码也是如此。它们是否相似呢？请参考[图 5-7](ch05.xhtml#ch05fig07)。
- en: '![Image](../images/ch05fig07.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch05fig07.jpg)'
- en: '*Figure 5-7: Dense layer output for sample inputs*'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5-7：样本输入的密集层输出*'
- en: This figure presents the dense layer outputs for five different zero and eight
    inputs. The barcodes are all different but share similarities according to digit.
    This is especially true for the zeros. The LeNet model has learned how to map
    each 28×28-pixel input image (784 pixels) into a vector of 84 numbers that show
    strong similarities by digit type. Based on our experience with traditional neural
    networks, we can appreciate that this mapping has produced something of lower
    dimensionality that preserves and even emphasizes differences between digits.
    The learned lower-dimensionality vector is akin to a complex concept explained
    with a few well-chosen words. This is exactly what we want a CNN to do. The trained
    model learned to “see” in the world of handwritten digits represented as small
    grayscale images. There’s nothing special about grayscale images, either. CNNs
    are quite happy to work with color images represented by red, green, and blue
    channels, or any number of channels, as when using multiband satellite imagery.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 该图展示了五个不同的零和八的输入的密集层输出。条形码虽然各不相同，但根据数字类型具有相似性。这对零尤其如此。LeNet模型已经学会了如何将每个28×28像素的输入图像（784个像素）映射到一个由84个数字组成的向量，且这些数字在不同数字之间具有强烈的相似性。基于我们在传统神经网络中的经验，我们可以理解，这个映射产生了一个低维度的表示，保留甚至强化了不同数字之间的差异。学习到的低维向量类似于通过几个精心挑选的词语来解释的复杂概念。这正是我们希望CNN能够做的事情。训练后的模型学会了“看”手写数字的世界，这些数字以小的灰度图像表示。灰度图像本身并没有什么特别的。CNN同样可以处理由红、绿、蓝通道表示的彩色图像，或是任何数量的通道，比如使用多波段卫星图像时。
- en: 'We might think of the model this way: the CNN layers before the dense layer
    learned how to act as a function producing an output vector from the input image.
    The true classifier is the dense layer at the top, but it works well because the
    CNN learned the classifier (dense layer) while simultaneously learning the mapping
    function.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以这样理解这个模型：在密集层之前的CNN层学会了如何作为一个函数，根据输入图像产生输出向量。真正的分类器是顶部的密集层，但它之所以有效，是因为CNN在学习映射函数的同时，也学会了分类器（即密集层）。
- en: I stated earlier that higher layers in the CNN pay attention to ever larger
    parts of the input. We can see that this is so by considering the portion of the
    input that influences the output of a kernel at a deeper layer. [Figure 5-8](ch05.xhtml#ch05fig08)
    demonstrates this effect.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前提到过，卷积神经网络（CNN）的高层会关注输入的更大部分。我们可以通过考虑影响深层卷积核输出的输入部分来验证这一点。[图 5-8](ch05.xhtml#ch05fig08)展示了这种效果。
- en: '![Image](../images/ch05fig08.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch05fig08.jpg)'
- en: '*Figure 5-8: The part of the input affecting deeper layers of the model*'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5-8：影响模型深层的输入部分*'
- en: Begin on the right side of the image. The 3×3 grid of squares represents the
    output of a kernel at convolutional layer 1\. We want to know what portion of
    the input influences the value of the shaded pixel. Looking at the previous convolutional
    layer, layer 0, we see that the layer 1 output depends on the nine shaded values
    coming from the layer before.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 从图像的右侧开始。3×3 的网格表示卷积层 1 的输出。我们想知道输入的哪一部分影响了被阴影标记的像素的值。查看前一层卷积层 0，我们看到卷积层 1 的输出依赖于来自前一层的九个阴影标记的值。
- en: The nine shaded values of convolutional layer 0 depend on the 5×5 shaded region
    of the input. It’s 5×5 because each of the nine values is found by sliding a 3×3
    kernel over the shaded 5×5 region of the input. For example, the dotted portion
    of the middle value in layer 0 comes from the similarly shaded 3×3 region of the
    input. In this way, higher CNN layers are affected by larger and larger portions
    of the input. The technical term for this is the [*effective receptive field*](glossary.xhtml#glo33),
    where the effective receptive field of the rightmost shaded value in [Figure 5-8](ch05.xhtml#ch05fig08)
    is the 5×5 shaded region of the input.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层 0 的九个阴影标记的值依赖于输入的 5×5 阴影区域。之所以是 5×5，是因为每个九个值都是通过将 3×3 卷积核滑动到输入的 5×5 阴影区域上得到的。例如，图层
    0 中间值的虚线部分来自输入中相似阴影的 3×3 区域。通过这种方式，CNN 的更高层会受到输入更大区域的影响。这个技术术语叫做 [*有效感受野*](glossary.xhtml#glo33)，其中，[图
    5-8](ch05.xhtml#ch05fig08) 中最右侧阴影值的有效感受野就是输入中 5×5 的阴影区域。
- en: '****'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '****'
- en: It’s time for an experiment. We now have a handle on how CNNs function, so let’s
    put that knowledge to work to compare a traditional neural network with a convolutional
    model. Which will win? I suspect you already know the answer, but let’s prove
    it and gain some experience along the way.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候进行实验了。我们已经了解了 CNN 的功能，接下来让我们将这些知识运用到实践中，比较传统神经网络与卷积模型，看看哪个会胜出？我猜想你已经知道答案了，但让我们验证一下，并在过程中获得一些经验。
- en: We need a dataset. Let’s use a grayscale version of CIFAR-10\. This is a better
    choice than the dinosaur footprint dataset we used in the previous two chapters
    because the footprint images are outlines devoid of texture and background, and
    a CNN will not learn much more from such images than a traditional model. As we
    learned in [Chapter 3](ch03.xhtml), CIFAR-10 contains 32×32-pixel images of animals
    and vehicles, which will likely be more challenging.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个数据集。我们使用 CIFAR-10 的灰度版本。与前两章使用的恐龙足迹数据集相比，这个选择更好，因为足迹图像是没有纹理和背景的轮廓，CNN
    从这种图像中学到的东西与传统模型并无太大区别。正如我们在[第 3 章](ch03.xhtml)中学到的，CIFAR-10 包含 32×32 像素的动物和交通工具图像，这可能会更具挑战性。
- en: 'We’ll train three models: a random forest, a traditional neural network, and
    a convolutional neural network. Is this sufficient? We’ve come to appreciate that
    all three of these models involve randomness, so training once might not give
    us a fair representation of how each model performs. After all, we might get a
    lousy initialization or mix of trees that would throw one of the models off. Therefore,
    let’s train each model 10 times and average the results.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将训练三个模型：随机森林、传统神经网络和卷积神经网络。这样足够了吗？我们已经意识到这三种模型都涉及随机性，所以单次训练可能无法公正地代表每个模型的性能。毕竟，我们可能会遇到糟糕的初始化或树的组合，从而影响某个模型的表现。因此，让我们将每个模型训练
    10 次，并取平均结果。
- en: This experiment will help us understand the differences in performance between
    the models, but we can learn still more about the neural networks by tracking
    their errors as training progresses. The result is a graph, which I’ll present
    and then explain shortly. Before that, however, let me lay out the details of
    the models.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实验将帮助我们了解模型之间性能的差异，但通过跟踪神经网络在训练过程中的错误，我们还能学到更多。结果将是一个图表，我会呈现并简短地解释它。在此之前，先让我阐述一下模型的细节。
- en: The training and test datasets are the same for each model. The traditional
    neural network and the random forest require vector inputs, so each 32×32-pixel
    image is unraveled into a vector of 1,024 numbers. The CNN works with the actual
    two-dimensional images. There are 50,000 images in the training set, 5,000 for
    each of the 10 classes, and 10,000 images in the test set, 1,000 per class.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个模型，训练集和测试集的数据集是相同的。传统的神经网络和随机森林需要向量输入，因此每张32×32像素的图像都会被展开成一个包含1,024个数字的向量。CNN则直接处理实际的二维图像。训练集包含50,000张图像，每个类别有5,000张图像，测试集包含10,000张图像，每个类别有1,000张图像。
- en: 'The random forest uses 300 trees. The traditional neural network has two hidden
    layers of 512 and 100 nodes, respectively. The CNN is more complex, with four
    convolutional layers, two pooling layers, and a single dense layer of 472 nodes.
    Even though the CNN has many more layers, the total number of weights and biases
    to learn is nearly identical to the traditional model: 577,014 versus 577,110.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林使用了300棵树。传统神经网络有两层隐藏层，分别包含512和100个节点。CNN则更复杂，有四个卷积层、两个池化层和一个包含472个节点的全连接层。尽管CNN有更多的层，但需要学习的权重和偏差的总数几乎与传统模型相同：577,014与577,110。
- en: We’ll train the neural networks for 100 epochs, meaning 100 passes through the
    full training set. Fixing the minibatch size at 200 gives us 250 gradient descent
    steps per epoch. Therefore, during training, we’ll update the weights and biases
    of the networks 25,000 times. At the end of each epoch, we’ll capture the error
    made by the model on both the training and test sets. When the dust settles, a
    single graph will reveal everything we want to know.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将训练神经网络100个epoch，即对整个训练集进行100次迭代。将小批量大小固定为200，每个epoch进行250次梯度下降步骤。因此，在训练过程中，我们将更新网络的权重和偏差25,000次。在每个epoch结束时，我们将记录模型在训练集和测试集上的误差。当一切尘埃落定后，一张图表将展示我们想要了解的所有信息。
- en: '[Figure 5-9](ch05.xhtml#ch05fig09) is that graph. It’s the most complex graph
    we’ve seen, so let’s walk through it in detail, beginning with the axes.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-9](ch05.xhtml#ch05fig09)就是那个图表。它是我们见过的最复杂的图表，所以让我们从坐标轴开始，逐步详细分析。'
- en: '![Image](../images/ch05fig09.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/ch05fig09.jpg)'
- en: '*Figure 5-9: CIFAR-10 results for a CNN, MLP, and random forest*'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5-9：CNN、MLP和随机森林在CIFAR-10数据集上的结果*'
- en: The label on the horizontal axis (*x*-axis) is “epoch,” which means a complete
    pass through the training set. Therefore, the graph shows things changing during
    training after every epoch. We also know that each epoch represents 250 gradient
    descent steps. The vertical axis (*y*-axis) is labeled “error” and runs from 0.1
    to 0.8\. This axis represents the fraction of the test or training samples that
    the model gets wrong. The lower the error, the better. A decimal value of 0.1
    means 10 percent, and a value of 0.8 means 80 percent.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 水平轴（*x*-轴）的标签为“epoch”，表示对训练集的完整一次迭代。因此，图表展示了训练过程中每次迭代后的变化。我们还知道每个epoch表示250次梯度下降步骤。垂直轴（*y*-轴）标签为“error”，范围从0.1到0.8。此轴表示模型在测试集或训练集上错误分类的样本比例。误差越低，模型越好。0.1表示10%的错误，0.8表示80%的错误。
- en: The legend in the upper-right corner of the graph tells us that the circles
    and squares relate to the MLP, the traditional neural network, while the triangles
    and pentagons refer to the CNN. Specifically, the circles and triangles track
    the error on the test set for the MLP and CNN, respectively, as the models train.
    Similarly, the squares and pentagons track the error on the training set. Recall
    that the model’s performance on the training set is used to update the weights
    and biases. The test set is used for evaluation and does not contribute to how
    the model is trained.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图表右上角的图例告诉我们，圆圈和方框与MLP（传统神经网络）相关，而三角形和五边形则指代CNN。具体来说，圆圈和三角形分别追踪MLP和CNN在测试集上的误差变化，而方框和五边形则追踪训练集上的误差。请记住，模型在训练集上的表现用于更新权重和偏差，而测试集用于评估，并不会影响模型的训练过程。
- en: The MLP plots show us how well the model learned the training set (squares)
    and the test set (circles) as training continued, epoch after epoch. It’s immediately
    apparent that the model learned the training set better than the test set because
    the training set error decreases continuously. This is what we expect. The gradient
    descent algorithm will update the weights and biases of the MLP, all 577,110 of
    them, to arrive at a lower and lower error on the training set. However, we’re
    not interested in reaching zero error on the training set; instead, we want the
    smallest error possible on the test set because that gives us a reason to believe
    that the MLP has learned to generalize.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: MLP图表展示了模型在训练集（方块）和测试集（圆圈）上的学习效果，随着训练的进行，逐个周期不断变化。可以立即看出，模型在训练集上的学习效果优于测试集，因为训练集的误差不断减少。这是我们所期望的。梯度下降算法会更新MLP的权重和偏差，总共有577,110个参数，以便在训练集上得到更低的误差。然而，我们并不关心训练集上的误差降到零；我们更希望在测试集上得到最小的误差，因为这可以让我们相信，MLP已经学会了如何泛化。
- en: Now consider the circle plot showing us the test set error. It reaches a minimum
    of about 0.56, or 56 percent, at around 40 epochs. After that, the error increases
    slowly but steadily, up to 100 epochs. This effect is classic MLP overfitting.
    The training set error continues to decrease, but the test set error hits a minimum
    and continues to increase after that. [Figure 5-9](ch05.xhtml#ch05fig09) tells
    us that stopping training at 40 epochs would have given us the best-performing
    MLP.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在看看显示测试集误差的圆形图。它在大约40个周期时达到了大约0.56或56%的最小值。之后，误差缓慢但稳定地增加，直到100个周期。这个现象是典型的MLP过拟合。训练集误差继续减少，但测试集误差达到了最小值后开始增加。[图5-9](ch05.xhtml#ch05fig09)告诉我们，在40个周期时停止训练，会得到表现最佳的MLP。
- en: 'We’ll get to the CNN results, but for the moment, consider the dashed line
    at 58 percent error. It’s labeled “RF300” and shows us the test set error from
    a random forest with 300 trees. The random forest doesn’t learn by updating weights
    over epochs, so the 58 percent error is just that: the model’s error. I plotted
    it as a dashed line parallel to the horizontal axis so you can see that, briefly,
    the MLP did slightly better than the random forest, but by 100 epochs, the difference
    between the two models was negligible. In other words, we might take it that classical
    machine learning’s best effort on the grayscale CIFAR-10 dataset is an error of
    about 56 to 58 percent. That’s not a good result. Additional time spent with the
    parameters of the random forest, or the MLP, or starting over with a support vector
    machine might lead to a slight reduction in the error. Still, it’s unlikely to
    overcome the fact that classical machine learning cannot do much with this dataset.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍后会讨论CNN的结果，但此刻，先看看58%误差的虚线。它被标记为“RF300”，展示了一个300棵树的随机森林的测试集误差。随机森林不会通过在多个周期中更新权重来学习，因此58%的误差就是模型的误差。我将其绘制为与横轴平行的虚线，这样你可以看到，短暂的时间内，MLP比随机森林稍微好一些，但到了100个周期后，两个模型之间的差异就几乎可以忽略不计。换句话说，我们可以认为，在灰度CIFAR-10数据集上，经典机器学习的最佳表现是56%到58%的误差。这并不是一个好的结果。通过调整随机森林、MLP的参数，或者从头开始使用支持向量机，可能会稍微减少误差。但仍然不太可能克服经典机器学习在这个数据集上无法做太多的事实。
- en: Finally, consider the CNN’s training (pentagon) and test (triangle) curves.
    By 100 epochs, the CNN is right around 11 percent error on the training set and,
    more importantly, about 23 percent on the test set. In other words, the CNN is
    right 77 percent of the time, or nearly 8 times in 10\. Random guessing will be
    correct about 10 percent of the time on a 10-class dataset, so the CNN has learned
    rather well, and far better than the MLP or random forest.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，看看CNN的训练（五边形）和测试（三角形）曲线。在100个周期时，CNN在训练集上的误差大约为11%，更重要的是，在测试集上的误差约为23%。换句话说，CNN的正确率为77%，或者说是接近8次中的7次。在一个10类数据集上，随机猜测的正确率大约为10%，所以CNN的学习效果非常好，远远优于MLP和随机森林。
- en: 'This is precisely the point of convolutional neural networks: by learning to
    represent the parts of the objects in an image, it becomes possible to learn a
    new representation (formally known as an [*embedding*](glossary.xhtml#glo34))
    that the dense layers of the network can successfully classify.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是卷积神经网络的关键点：通过学习表示图像中物体的各个部分，就可以学习到一种新的表示（正式称为[*嵌入*](glossary.xhtml#glo34)），使得网络的密集层能够成功地进行分类。
- en: The first CNN I trained, in 2015, attempted to detect small airplanes in satellite
    images. My initial, non-CNN approach worked, but it was noisy with many false
    positives (fake detections). The airplanes were there, but so were many other
    things that were not airplanes. I then trained a simple CNN like the one used
    in this experiment. It located the airplanes with ease, and virtually nothing
    but the airplanes. I was dumbfounded and realized then that deep learning was
    a paradigm shift. I’ll argue in [Chapter 7](ch07.xhtml) that as of fall 2022,
    a new, more profound paradigm shift has occurred, but we have some ground yet
    to cover before we’re ready for that discussion.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我在2015年训练的第一个CNN尝试在卫星图像中检测小型飞机。最初的非CNN方法有效，但噪声较大，存在许多假阳性（假检测）。飞机确实存在，但也有许多其他并非飞机的物体。然后，我训练了一个像本实验中使用的简单CNN，它轻松地定位了飞机，几乎只定位了飞机。我感到目瞪口呆，并意识到深度学习是一次范式的转变。我将在[第7章](ch07.xhtml)中讨论，到了2022年秋季，发生了一个新的、更深刻的范式转变，但在我们准备好讨论之前，还有一些内容需要学习。
- en: '****'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '****'
- en: The simple CNNs of this chapter don’t do justice to the zoo of available neural
    network architectures. A decade of fevered development has resulted in a few go-to
    CNN architectures, some with over 100 layers. The architectures have names like
    ResNet, DenseNet, Inception, MobileNet, and U-Net, among many others. The U-Net
    is worthy of a few words.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍的简单CNN并没有充分展示可用神经网络架构的多样性。十年的快速发展促成了一些常用的CNN架构，其中一些甚至有超过100层。这些架构有像ResNet、DenseNet、Inception、MobileNet和U-Net等名称，此外还有很多其他架构。U-Net值得一提。
- en: The CNNs we’ve explored so far accept an input image and return a class label
    like “dog” or “cat.” It doesn’t need to be this way. Some CNN architectures implement
    [*semantic segmentation*](glossary.xhtml#glo89), where the output is another image
    with every pixel labeled by the class to which it belongs. U-Nets do this. If
    every pixel of the dog is marked “dog,” extracting the dog from the image becomes
    trivial. A middle ground between a U-Net and CNNs that assign a single label to
    the entire image is a model that outputs a [*bounding box*](glossary.xhtml#glo14),
    a rectangle surrounding the detected object. The pervasiveness of AI means that
    you’ve likely already seen images with labeled bounding boxes. YOLO (“you only
    look once”) is a popular architecture producing labeled bounding boxes; Faster
    R-CNN is another.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们到目前为止探讨的CNN接受一个输入图像，并返回一个像“狗”或“猫”这样的类别标签。事情不必非得这样。一些CNN架构实现了[*语义分割*](glossary.xhtml#glo89)，其输出是另一个图像，每个像素都标记了它所属的类别。U-Net就是这样做的。如果狗的每个像素都标记为“狗”，那么从图像中提取出狗就变得非常简单。U-Net和将整个图像分配单一标签的CNN之间的一个折中方法是输出一个[*边界框*](glossary.xhtml#glo14)的模型，边界框是包围检测到的物体的矩形。人工智能的普及意味着你很可能已经见过带有标注边界框的图像。YOLO（“you
    only look once”）是一种流行的生成标注边界框的架构；Faster R-CNN则是另一种。
- en: 'We focused on image inputs here, but the input need not be an image. Anything
    representable in an image-like format, where there are two dimensions and structure
    within those dimensions, is a candidate for a 2D CNN. A good example is an audio
    signal, which we usually think of as one-dimensional, a voltage changing over
    time that drives the speaker. However, audio signals contain energy at different
    frequencies. The energy at different frequencies can be displayed in two dimensions:
    the horizontal dimension is time, and the vertical dimension is frequency, usually
    with lower frequencies at the bottom and higher frequencies at the top. The intensity
    of each frequency becomes the intensity of a pixel to transform the audio signal
    from a one-dimensional, time-varying voltage into a two-dimensional spectrogram,
    as shown in [Figure 5-10](ch05.xhtml#ch05fig10).'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里关注的是图像输入，但输入不一定非得是图像。任何可以用图像格式表示的内容，只要具有两个维度，并且在这些维度内有结构，都是二维CNN的候选对象。一个很好的例子是音频信号，我们通常认为它是一维的，即随时间变化的电压信号，驱动扬声器。然而，音频信号在不同的频率上包含能量。不同频率的能量可以在二维上显示：水平维度是时间，垂直维度是频率，通常较低频率位于底部，较高频率位于顶部。每个频率的强度变成一个像素的强度，将音频信号从一维的、随时间变化的电压转换成二维的频谱图，如[图5-10](ch05.xhtml#ch05fig10)所示。
- en: '![Image](../images/ch05fig10.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch05fig10.jpg)'
- en: '*Figure 5-10: Mapping from one-dimensional data to a two-dimensional image*'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5-10：从一维数据到二维图像的映射*'
- en: The spectrogram, here of a crying baby, contains a wealth of information and
    structure that the CNN can learn about to produce a better model than is possible
    with the one-dimensional audio signal alone. The key observation is that any transformation
    of the input data that extracts structure in a form amenable to a CNN is fair
    game.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个哭泣婴儿的频谱图，包含了大量的信息和结构，CNN可以从中学习，从而生成一个比单一的音频信号更好的模型。关键的观察是，任何能提取输入数据结构并使其适应CNN的转换都是可以接受的。
- en: '****'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '****'
- en: 'You have a dataset and need to build a CNN. What architecture should you use?
    What should the minibatch size be? What layers do you need, and in what order?
    Should you use 5×5 or 3×3 convolutional kernels? How many epochs of training is
    enough? Early on, before the development of standard architectures, each of those
    questions had to be answered by the person designing the network. It was a bit
    like medicine of the past: a mix of science, experience, and intuition. The art
    of neural networks meant that practitioners were in high demand, and it was difficult
    for savvy software engineers to add deep learning to their repertoires. Some people
    wondered if software could be used to determine the model’s architecture and training
    parameters automatically (that is, its hyperparameters, introduced in [Chapter
    3](ch03.xhtml)). And so, automatic machine learning, or [*AutoML*](glossary.xhtml#glo9),
    was born.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 你有一个数据集，需要构建一个卷积神经网络（CNN）。你应该使用什么架构？最小批量的大小应该是多少？你需要哪些层，并且顺序如何？你应该使用5×5还是3×3的卷积核？多少轮训练才够？在标准架构发展之前，这些问题都需要由设计网络的人来回答。这有点像过去的医学：科学、经验和直觉的结合。神经网络的艺术意味着从业人员的需求很高，熟练的软件工程师很难将深度学习加入他们的技能库。有些人曾经想知道，是否可以使用软件来自动确定模型的架构和训练参数（也就是它的超参数，见[第3章](ch03.xhtml)）。于是，自动机器学习（[*AutoML*](glossary.xhtml#glo9)）应运而生。
- en: Most cloud-based commercial machine learning platforms, like Microsoft’s Azure
    Machine Learning or Amazon’s SageMaker Autopilot, include an AutoML tool that
    will create the machine learning model for you; you need only supply the dataset.
    AutoML applies to more than just neural networks, and many tools include classical
    machine learning models as well. AutoML’s entire purpose is to locate the best
    model type for the supplied dataset with a minimum of user expertise required.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数基于云的商业机器学习平台，如微软的Azure Machine Learning或亚马逊的SageMaker Autopilot，都包含一个AutoML工具，可以为你创建机器学习模型；你只需要提供数据集。AutoML不仅适用于神经网络，许多工具还包括经典的机器学习模型。AutoML的整个目的就是以最少的用户专业知识，找到适合给定数据集的最佳模型类型。
- en: I want to argue that AutoML only goes so far and that the best deep learning
    practitioners will always outperform it, but that argument rings hollow. It reminds
    me of the assembly language programmers of old pontificating on the impossibility
    of compilers ever producing code that was as good as or better than what they
    could produce. There are few job openings these days for assembly language programmers,
    but tens of thousands for programmers using compiled languages (at least for now;
    see [Chapter 8](ch08.xhtml)). That said, some of us still prefer to roll our own
    models.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我想争辩的是，AutoML也仅仅做到这一点，最优秀的深度学习从业人员始终会超越它，但这个争论听起来空洞。它让我想起了以前的汇编语言程序员，他们曾经断言编译器永远无法生成比他们手写的代码更好或一样好的代码。如今，汇编语言程序员的工作机会很少，但使用编译语言的程序员却有成千上万的职位（至少目前是这样；见[第8章](ch08.xhtml)）。话虽如此，我们中的一些人仍然更喜欢自己构建模型。
- en: '****'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '****'
- en: A consequence of the deep learning revolution was the creation of powerful,
    open source machine learning toolkits with names like TensorFlow and PyTorch.
    Implementing a traditional, fully connected neural network is an exercise for
    machine learning students. It’s not trivial, but it’s something most people can
    accomplish with effort. Properly implementing a CNN, on the other hand, especially
    one supporting a multitude of layer types, is anything but trivial. The AI community
    committed early on to developing open source toolkits supporting deep learning,
    including CNNs. Without these toolkits, progress in AI would be painfully slow.
    Large tech companies like Google, Facebook (Meta), and NVIDIA also signed on,
    and their continued support for toolkit development is critical to AI.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习革命的一个结果是创建了强大且开源的机器学习工具包，如TensorFlow和PyTorch等名字。实现传统的全连接神经网络是机器学习学生的一项练习。虽然不简单，但大多数人通过努力可以完成。另一方面，正确实现一个CNN，尤其是支持多种层类型的CNN，可谓不是件简单事。人工智能社区早期就致力于开发支持深度学习的开源工具包，包括CNN。如果没有这些工具包，AI的发展会非常缓慢。像谷歌、Facebook（Meta）和NVIDIA等大型科技公司也加入了这一行列，它们对工具包开发的持续支持对AI至关重要。
- en: 'What makes the toolkits powerful, besides the mountains of tested, high-performance
    code they contain, is their flexibility. We now appreciate that training a neural
    network, CNN or otherwise, requires two steps: backpropagation and gradient descent.
    Backpropagation works only if the model’s layers support a particular mathematical
    operation known as differentiation. Differentiation is what first semester calculus
    students learn. So long as the toolkits can automatically determine the derivatives
    (what you get when you differentiate), they allow users to implement arbitrary
    layers. The toolkits employ [*automatic differentiation*](glossary.xhtml#glo8)
    by transforming the neural network into a computational graph.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 除了包含大量经过测试的高性能代码外，使得这些工具包强大的另一个原因是它们的灵活性。我们现在认识到，训练一个神经网络，无论是CNN还是其他类型，都需要两个步骤：反向传播和梯度下降。反向传播只有在模型的层支持一种称为微分的特定数学运算时才能发挥作用。微分是高等数学第一学期学生学习的内容。只要这些工具包能够自动计算导数（即微分后的结果），它们就能让用户实现任意层。工具包通过将神经网络转化为计算图，采用了[*自动微分*](glossary.xhtml#glo8)技术。
- en: 'It’s tempting to take a few steps down the path of automatic differentiation
    and computational graphs because the elegance and flexibility therein is a beautiful
    marriage of mathematics and computer science. Unfortunately, you’ll need to take
    my word for it because the level of detail necessary is far beyond what we can
    explore in this book. One key point is that there are two primary approaches to
    automatic differentiation: forward and reverse. Forward automatic differentiation
    is easier to conceptualize and implement in code but is unsuited to neural networks.
    That’s too bad, in a way, because forward automatic differentiation is best implemented
    using dual numbers, an obscure type of number invented (discovered?) by English
    mathematician William Clifford in 1873\. These were a prime example of math for
    math’s sake and largely forgotten until the age of computers, when they were suddenly
    made useful. Reverse automatic differentiation is best for neural networks but
    doesn’t use dual numbers.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管自动微分和计算图的优雅与灵活性在数学和计算机科学的结合中是美妙的，但很难抵挡深入探索的诱惑。不幸的是，你需要相信我，因为实现这些细节的程度远远超出了本书的讨论范围。一个关键点是，自动微分有两种主要方法：前向和反向。前向自动微分更容易理解和用代码实现，但不适用于神经网络。某种程度上来说，这很遗憾，因为前向自动微分最好使用双数来实现，双数是一种由英国数学家威廉·克利福德于1873年发明（发现？）的晦涩数字类型。这些数字最初是数学为数学本身服务的典型例子，直到计算机时代才被重新发现并变得有用。而反向自动微分则更适用于神经网络，但不使用双数。
- en: '****'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '****'
- en: 'This chapter was challenging. We dove more deeply into the details than we
    did in previous chapters or will in the following ones. A summary is definitely
    required. Convolutional neural networks:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这一章很有挑战性。我们深入探讨了比以往章节更为详细的内容，也会在接下来的章节中继续讨论。因此，确实需要一个总结。卷积神经网络：
- en: Thrive on structure in their inputs, which is the complete opposite of classical
    machine learning models
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 依赖输入的结构性，这与经典的机器学习模型完全相反
- en: Learn new representations of their inputs by breaking them into parts and groups
    of parts
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将输入拆分成部分及部分的组合来学习输入的新表示
- en: Use many different kinds of layers combined in various ways
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用多种不同类型的层，并以多种方式组合
- en: Can classify inputs, localize inputs, or assign a class label to every pixel
    in their inputs
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以对输入进行分类、定位输入，或为输入中的每个像素分配类别标签
- en: Are still trained via backpropagation and gradient descent, just like traditional
    neural networks
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仍然通过反向传播和梯度下降进行训练，就像传统神经网络一样
- en: Drove the creation of powerful, open source toolkits that democratized deep
    learning
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推动了强大的开源工具包的创建，使深度学习变得大众化
- en: 'Convolutional neural networks follow in the tradition of classical machine
    learning models: they take an input and assign to it, in some fashion, a class
    label. The network operates as a mathematical function, accepting an input and
    producing an output. The next chapter introduces us to neural networks that generate
    output without input.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络延续了经典机器学习模型的传统：它们接受输入，并以某种方式为其分配一个类别标签。该网络作为一个数学函数运作，接受输入并产生输出。下一章将介绍那些在没有输入的情况下生成输出的神经网络。
- en: 'To paraphrase an old television show: you’re traveling through another dimension,
    a dimension not only of sight and sound but of mind, a journey into a wondrous
    land whose boundaries are that of imagination—next stop, generative AI.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 用一句老电视节目的话来改述：你正在穿越另一个维度，一个不仅仅是视觉和听觉的维度，而是心灵的维度，一次进入神奇土地的旅程，其边界就是想象力——下一站，生成式人工智能。
- en: '**KEY TERMS**'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**关键词**'
- en: automatic differentiation, AutoML, bounding box, computational graph, convolution,
    convolutional layer, convolutional neural network, dense layer, effective receptive
    field, embedding, end-to-end learning, filter, kernel, pooling layer, semantic
    segmentation
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 自动微分、AutoML、边界框、计算图、卷积、卷积层、卷积神经网络、全连接层、有效感受野、嵌入、端到端学习、滤波器、卷积核、池化层、语义分割
