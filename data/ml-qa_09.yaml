- en: '**8'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**8**'
- en: THE SUCCESS OF TRANSFORMERS**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**变换器的成功**'
- en: '![Image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/common.jpg)'
- en: What are the main factors that have contributed to the success of transformers?
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器成功的主要因素有哪些？
- en: In recent years, transformers have emerged as the most successful neural network
    architecture, particularly for various natural language processing tasks. In fact,
    transformers are now on the cusp of becoming state of the art for computer vision
    tasks as well. The success of transformers can be attributed to several key factors,
    including their attention mechanisms, ability to be parallelized easily, unsupervised
    pretraining, and high parameter counts.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，变换器成为了最成功的神经网络架构，特别是在各种自然语言处理任务中。事实上，变换器现在也即将成为计算机视觉任务的最新技术。变换器的成功可以归因于几个关键因素，包括其注意力机制、易于并行化、无监督预训练和高参数量。
- en: '**The Attention Mechanism**'
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**注意力机制**'
- en: The self-attention mechanism found in transformers is one of the key design
    components that make transformer-based LLMs so successful. However, transformers
    are not the first architecture to utilize attention mechanisms.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器中使用的自注意力机制是使基于变换器的大型语言模型（LLM）如此成功的关键设计组件之一。然而，变换器并不是第一个利用注意力机制的架构。
- en: Attention mechanisms were first developed in the context of image recognition
    back in 2010, before being adopted to aid the translation of long sentences in
    recurrent neural networks. ([Chapter 16](ch16.xhtml) compares the attention mechanisms
    found in recurrent neural networks and transformers in greater detail.)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制最早是在2010年图像识别的背景下发展起来的，之后被采用来帮助递归神经网络中的长句子翻译。（[第16章](ch16.xhtml)更详细地比较了递归神经网络和变换器中的注意力机制。）
- en: The aforementioned attention mechanism is inspired by human vision, focusing
    on specific parts of an image (foveal glimpses) at a time to process information
    hierarchically and sequentially. In contrast, the fundamental mechanism underlying
    transformers is a self-attention mechanism used for sequence-to-sequence tasks,
    such as machine translation and text generation. It allows each token in a sequence
    to attend to all other tokens, thus providing context-aware representations of
    each token.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 上述的注意力机制灵感来源于人类视觉，每次专注于图像的特定部分（中央视网膜快照），以分层和顺序的方式处理信息。与此不同，变换器的基本机制是自注意力机制，主要用于序列到序列的任务，例如机器翻译和文本生成。它使得序列中的每个标记都能关注所有其他标记，从而提供每个标记的上下文感知表示。
- en: What makes attention mechanisms so unique and useful? For the following illustration,
    suppose we are using an encoder network on a fixed-length representation of the
    input sequence or image—this can be a fully connected, convolutional, or attention-based
    encoder.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 是什么让注意力机制如此独特且有用？以下示例中，假设我们正在使用编码器网络来处理输入序列或图像的固定长度表示——这可以是全连接的、卷积的或基于注意力的编码器。
- en: In a transformer, the encoder uses self-attention mechanisms to compute the
    importance of each input token relative to other tokens in the sequence, allowing
    the model to focus on relevant parts of the input sequence. Conceptually, attention
    mechanisms allow the transformers to attend to different parts of a sequence or
    image. On the surface, this sounds very similar to a fully connected layer where
    each input element is connected via a weight with the input element in the next
    layer. In attention mechanisms, the computation of the attention weights involves
    comparing each input element to all others. The attention weights obtained by
    this approach are dynamic and input dependent. In contrast, the weights of a convolutional
    or fully connected layer are fixed after training, as illustrated in [Figure 8-1](ch08.xhtml#ch8fig1).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在变换器中，编码器使用自注意力机制来计算每个输入标记相对于序列中其他标记的重要性，从而使模型能够专注于输入序列中相关的部分。从概念上讲，注意力机制使得变换器能够关注序列或图像的不同部分。从表面上看，这听起来很像一个全连接层，其中每个输入元素通过权重与下一层中的输入元素相连接。在注意力机制中，注意力权重的计算涉及将每个输入元素与其他所有元素进行比较。通过这种方法得到的注意力权重是动态的且依赖于输入的。相比之下，卷积层或全连接层的权重在训练后是固定的，如[图8-1](ch08.xhtml#ch8fig1)所示。
- en: '![Image](../images/08fig01.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/08fig01.jpg)'
- en: '*Figure 8-1: The conceptual difference between model weights in fully connected
    layers (top) and attention scores (bottom)*'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*图8-1：全连接层（上）和注意力得分（下）模型权重的概念差异*'
- en: As the top part of [Figure 8-1](ch08.xhtml#ch8fig1) shows, once trained, the
    weights of fully connected layers remain fixed regardless of the input. In contrast,
    as shown at the bottom, self-attention weights change depending on the inputs,
    even after a transformer is trained.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图8-1](ch08.xhtml#ch8fig1)顶部所示，一旦训练完成，完全连接层的权重将保持固定，无论输入如何。相比之下，正如底部所示，自注意力机制的权重会根据输入的不同而变化，即便变换器已经训练完成。
- en: Attention mechanisms allow a neural network to selectively weigh the importance
    of different input features, so the model can focus on the most relevant parts
    of the input for a given task. This provides a contextual understanding of each
    word or image token, allowing for more nuanced interpretations, which is one of
    the aspects that can make transformers work so well.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制使神经网络能够有选择性地权衡不同输入特征的重要性，从而使模型能够专注于给定任务中最相关的输入部分。这提供了对每个单词或图像标记的上下文理解，从而允许更细致的解读，这是变换器如此高效的原因之一。
- en: '**Pretraining via Self-Supervised Learning**'
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**通过自监督学习进行预训练**'
- en: Pretraining transformers via self-supervised learning on large, unlabeled datasets
    is another key factor in the success of transformers. During pre-training, the
    transformer model is trained to predict missing words in a sentence or the next
    sentence in a document, for example. By learning to predict these missing words
    or the next sentence, the model is forced to learn general representations of
    language that can be fine-tuned for a wide range of downstream tasks.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在大规模未标记数据集上使用自监督学习进行预训练是变换器成功的另一个关键因素。例如，在预训练过程中，变换器模型会训练去预测句子中缺失的单词或文档中的下一句话。通过学习预测这些缺失的单词或下一句话，模型被迫学习语言的通用表示，这些表示可以针对各种下游任务进行微调。
- en: While unsupervised pretraining has been highly effective for natural language
    processing tasks, its effectiveness for computer vision tasks is still an active
    area of research. (Refer to [Chapter 2](ch02.xhtml) for a more detailed discussion
    of self-supervised learning.)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然无监督预训练在自然语言处理任务中非常有效，但其在计算机视觉任务中的有效性仍然是一个活跃的研究领域。（有关自监督学习的更详细讨论，请参见[第2章](ch02.xhtml)。）
- en: '**Large Numbers of Parameters**'
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**大量的参数**'
- en: One noteworthy characteristic of transformers is their large model sizes. For
    example, the popular 2020 GPT-3 model consists of 175 billion trainable parameters,
    while other transformers, such as switch transformers, have trillions of parameters.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器的一个显著特点是其大规模的模型。举例来说，2020年流行的GPT-3模型包含了1750亿个可训练参数，而其他变换器，如switch transformers，拥有万亿级的参数。
- en: The scale and number of trainable parameters of transformers are essential factors
    in their modeling performance, particularly for large-scale natural language processing
    tasks. For instance, linear scaling laws suggest that the training loss decreases
    proportionally with an increase in model size, so a doubling of the model size
    can halve the training loss.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器的规模和可训练参数数量是其建模性能的关键因素，尤其是对于大规模自然语言处理任务。例如，线性缩放定律表明，随着模型规模的增加，训练损失按比例减少，因此模型规模加倍可以将训练损失减半。
- en: This, in turn, can lead to better performance on the downstream target task.
    However, it is essential to scale the model size and the number of training tokens
    equally. This means the number of training tokens should be doubled for every
    doubling of model size.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这反过来可以提高下游目标任务的表现。然而，必须同时扩大模型的规模和训练标记的数量。这意味着每当模型规模加倍时，训练标记的数量也应加倍。
- en: Since labeled data is limited, utilizing large amounts of data during un-supervised
    pretraining is vital.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 由于标注数据有限，因此在无监督预训练过程中利用大量数据至关重要。
- en: To summarize, large model sizes and large datasets are critical factors in transformers’
    success. Additionally, using self-supervised learning, the ability to pretrain
    transformers is closely tied to using large model sizes and large datasets. This
    combination has been critical in enabling the success of transformers in a wide
    range of natural language processing tasks.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，大规模模型和大规模数据集是变换器（transformers）成功的关键因素。此外，使用自监督学习，预训练变换器的能力与使用大规模模型和大规模数据集密切相关。这种结合对于变换器在广泛的自然语言处理任务中取得成功至关重要。
- en: '**Easy Parallelization**'
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**易于并行化**'
- en: Training large models on large datasets requires vast computational resources,
    and it’s key that the computations can be parallelized to utilize these resources.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在大型数据集上训练大型模型需要大量的计算资源，而关键在于计算能够并行化，以便充分利用这些资源。
- en: Fortunately, transformers are easy to parallelize since they take a fixed-length
    sequence of word or image tokens as input. For instance, the self-attention mechanism
    used in most transformer architectures involves computing the weighted sum between
    a pair of input elements. Furthermore, these pair-wise token comparisons can be
    computed independently, as illustrated in [Figure 8-2](ch08.xhtml#ch8fig2), making
    the self-attention mechanism relatively easy to parallelize across different GPU
    cores.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，变换器容易并行化，因为它们接受固定长度的单词或图像标记序列作为输入。例如，大多数变换器架构中使用的自注意力机制涉及计算一对输入元素之间的加权和。此外，这些成对标记的比较可以独立计算，如[图
    8-2](ch08.xhtml#ch8fig2)所示，从而使得自注意力机制能够相对容易地在不同的 GPU 核心之间进行并行化。
- en: '![Image](../images/08fig02.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/08fig02.jpg)'
- en: '*Figure 8-2: A simplified self-attention mechanism without weight parameters*'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 8-2：没有权重参数的简化自注意力机制*'
- en: In addition, the individual weight matrices used in the self-attention mechanism
    (not shown in [Figure 8-2](ch08.xhtml#ch8fig2)) can be distributed across different
    machines for distributed and parallel computing.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，自注意力机制中使用的各个权重矩阵（在[图 8-2](ch08.xhtml#ch8fig2)中未显示）可以分布在不同的机器上进行分布式和并行计算。
- en: '**Exercises**'
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**练习**'
- en: '**8-1.** As discussed in this chapter, self-attention is easily parallelizable,
    yet transformers are considered computationally expensive due to self-attention.
    How can we explain this contradiction?'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**8-1.** 如本章所述，自注意力机制易于并行化，但由于自注意力机制的存在，变换器被认为计算开销较大。我们如何解释这一矛盾？'
- en: '**8-2.** Since self-attention scores represent importance weights for the various
    input elements, can we consider self-attention to be a form of feature selection?'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**8-2.** 由于自注意力分数表示各种输入元素的重要性权重，我们能否将自注意力视为一种特征选择的形式？'
- en: '**References**'
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: 'An example of an attention mechanism in the context of image recognition: Hugo
    Larochelle and Geoffrey Hinton, “Learning to Combine Foveal Glimpses with a Third-Order
    Boltzmann Machine” (2010), *[https://dl.acm.org/doi/10.5555/2997189.2997328](https://dl.acm.org/doi/10.5555/2997189.2997328)*.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在图像识别中的一个注意力机制示例：Hugo Larochelle 和 Geoffrey Hinton，《学习将中心视野和三阶玻尔兹曼机结合起来》（2010），*
    [https://dl.acm.org/doi/10.5555/2997189.2997328](https://dl.acm.org/doi/10.5555/2997189.2997328)
    *。
- en: 'The paper introducing the self-attention mechanism with the original transformer
    architecture: Ashish Vaswani et al., “Attention Is All You Need” (2017), *[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)*.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入自注意力机制和原始变换器架构的论文：Ashish Vaswani 等人，《Attention Is All You Need》（2017），* [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)
    *。
- en: 'Transformers can have trillions of parameters: William Fedus, Barret Zoph,
    and Noam Shazeer, “Switch Transformers: Scaling to Trillion Parameter Models with
    Simple and Efficient Sparsity” (2021), *[https://arxiv.org/abs/2101.03961](https://arxiv.org/abs/2101.03961)*.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变换器可以拥有万亿个参数：William Fedus、Barret Zoph 和 Noam Shazeer，《Switch 变换器：通过简单高效的稀疏性扩展到万亿参数模型》（2021），*
    [https://arxiv.org/abs/2101.03961](https://arxiv.org/abs/2101.03961) *。
- en: 'Linear scaling laws suggest that training loss decreases proportionally with
    an increase in model size: Jared Kaplan et al., “Scaling Laws for Neural Language
    Models” (2020), *[https://arxiv.org/abs/2001.08361](https://arxiv.org/abs/2001.08361)*.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性扩展法则表明，随着模型规模的增加，训练损失会按比例减少：Jared Kaplan 等人，《神经语言模型的扩展法则》（2020），* [https://arxiv.org/abs/2001.08361](https://arxiv.org/abs/2001.08361)
    *。
- en: 'Research suggests that in transformer-based language models, the training tokens
    should be doubled for every doubling of model size: Jordan Hoffmann et al., “Training
    Compute-Optimal Large Language Models” (2022), *[https://arxiv.org/abs/2203.15556](https://arxiv.org/abs/2203.15556)*.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 研究表明，在基于变换器的语言模型中，模型大小每增加一倍，训练标记应该增加一倍：Jordan Hoffmann 等人，《训练计算最优的大型语言模型》（2022），*
    [https://arxiv.org/abs/2203.15556](https://arxiv.org/abs/2203.15556) *。
- en: 'For more about the weights used in self-attention and cross-attention mechanisms,
    check out my blog post: “Understanding and Coding the Self-Attention Mechanism
    of Large Language Models from Scratch” at *[https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html)*.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 想了解更多关于自注意力和交叉注意力机制中使用的权重，查看我的博客文章：“从零开始理解和编码大语言模型的自注意力机制”，链接为 *[https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html)*。
