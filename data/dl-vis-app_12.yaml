- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Data Preparation
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备
- en: '![](Images/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/chapterart.png)'
- en: Machine learning algorithms can only work as well as the data they’re trained
    on. In the real world, our data can come from noisy sensors, computer programs
    with bugs, or even incomplete or inaccurate transcriptions of paper records. We
    always need to look at our data and fix any problems before we use it.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法的效果只能与它们所训练的数据一样好。在现实世界中，我们的数据可能来自噪声传感器、带有漏洞的计算机程序，甚至是纸质记录的不完整或不准确的转录。我们始终需要查看我们的数据，并在使用之前修复任何问题。
- en: A rich body of methods has been developed for just this job. They’re referred
    to as techniques for *data preparation*, or *data cleaning*. The idea is to process
    our data beforelearning from it so that our learning systems can use the data
    most efficiently.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成这项任务，已经开发了一系列丰富的方法。它们被称为*数据准备*技术或*数据清洗*技术。其目的是在从数据中学习之前对数据进行处理，以便我们的学习系统能够最有效地使用这些数据。
- en: We also want to make sure that the data itself is well suited to machine learning,
    which may mean adjusting it, for example, by scaling numbers, or combining categories.
    This work is essential because the particular way the data is structured, and
    the numerical ranges it spans, can have a strong effect on the information an
    algorithm can extract from it.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要确保数据本身适合机器学习，这可能意味着需要调整它，例如通过对数字进行缩放，或将类别合并。此工作至关重要，因为数据的具体结构方式以及它所涵盖的数值范围，可能会对算法从中提取的信息产生强烈影响。
- en: Our goal in this chapter is to see how we can adjust the data we’re given, without
    changing its meaning, to get the most efficient and effective learning process.
    We begin with techniques to confirm that our data is clean and ready for training.
    We then consider methods for examining the data itself, and for making sure that
    we’ve got it in the best form for machine learning. This can involve doing simple
    things like replacing strings with numbers or taking more interesting actions
    like scaling the data. Finally, we look at ways to reduce the size of our training
    data. This lets our algorithms run and learn more quickly.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是看看我们如何调整所给的数据，在不改变其含义的情况下，获得最有效和高效的学习过程。我们从确认数据已清洗并准备好进行训练的技术开始。接着，我们将考虑检查数据本身的方法，并确保我们已经将其整理成最适合机器学习的形式。这可能包括做一些简单的事情，比如将字符串替换为数字，或采取更有趣的操作，如对数据进行缩放。最后，我们看看如何减少训练数据的大小。这可以让我们的算法运行和学习得更快。
- en: Basic Data Cleaning
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基本数据清洗
- en: Let’s start by considering some simple ways to ensure that our data is well
    cleaned. The idea is to make sure that we’re starting out with data that has no
    blanks, incorrect entries, or other errors.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先考虑一些简单的方法，确保我们的数据已经清洗干净。其基本思想是确保我们开始使用的数据没有空白、错误条目或其他错误。
- en: If our data is in textual form, then we want to make sure there are no typographical
    errors, misspellings, embedded unprintable characters, or other such corruptions.
    For example, if we have a collection of animal photos along with a text file describing
    them and our system is case-sensitive, then we want to make sure that every giraffe
    is labeled as giraffe and not girafe or Giraffe, and we want to avoid other typos
    or variants like beautiful giraffe or giraffe-extra tall. Every reference to a
    giraffe needs to use the identical string.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的数据是文本形式的，那么我们需要确保没有印刷错误、拼写错误、嵌入的不可打印字符或其他类似的破损。例如，如果我们有一组动物照片，并且有一个文本文件描述这些照片，而我们的系统区分大小写，那么我们需要确保每只长颈鹿都标注为giraffe，而不是girafe或Giraffe，我们还要避免其他拼写错误或变体，如beautiful
    giraffe或giraffe-extra tall。每次提到长颈鹿时，都需要使用相同的字符串。
- en: We should also look for other common-sense things. We want to remove any accidental
    duplicates in our training data, because they will skew our idea of what data
    we’re working with. If we accidentally include a single piece of data multiple
    times, our learner will interpret it as multiple, different samples that just
    happen to have the same value, and thus that sample may have more influence than
    it should.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还应该寻找其他常识性的事项。我们要去除训练数据中的任何重复项，因为它们会扭曲我们对所用数据的理解。如果我们不小心将某一数据项重复多次，我们的学习器会将其视为多个不同的样本，它们恰好具有相同的值，因此该样本可能会产生比应有的更大影响。
- en: We also want to make sure we don’t have any typographical errors, like missing
    a decimal point so we specify a value of 1,000 rather than 1.000, or putting two
    minus signs in front of a number rather than just one. It’s not uncommon to find
    some hand-composed databases with blanks or question marks in them, signifying
    that people didn’t have any data to enter. Some computer-generated databases can
    include a code like `NaN` (not a number), which is a placeholder indicating that
    the computer wanted to print a number but didn’t have a valid number to show.
    More troubling, sometimes when people are missing data for a numerical field,
    they enter something like 0 or –1\. We have to find and fix all such issues before
    we start learning from the data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要确保没有任何拼写错误，比如漏掉小数点导致我们指定了1,000而不是1.000，或者在数字前放了两个减号而不是一个。在一些手工编写的数据库中，发现空白或问号的情况并不罕见，表示人们没有数据可输入。一些计算机生成的数据库可能会包含像`NaN`（不是一个数字）这样的代码，这是一个占位符，表示计算机本来想输出一个数字，但没有有效的数字可显示。更麻烦的是，有时候当人们在填写数值字段时缺少数据时，他们会输入类似0或–1的值。我们必须在开始从数据中学习之前找到并修复所有这些问题。
- en: We also need to make sure that the data is in a format that will be properly
    interpreted by the software we’re giving it to. For example, we can use a format
    known as *scientific notation* to write very large and very small numbers. The
    problem is that such notation has no official format. Different programs use slightly
    different forms for this type of output, and other programs that read that data
    (like the library functions we often use in deep learning) can misinterpret forms
    they’re not expecting. For example, in scientific notation, the value 0.007 is
    commonly printed out as `7e-3` or `7E-3`. When we provide the sequence `7e-3`
    as an input, a program may interpret it as (7 × *e*) – 3, where *e* is Euler’s
    constant, which has a value of about 2.7\. The result is that the computer thinks
    that `7e-3` means that we’re asking it to first multiply the values of 7 and *e*
    together, and then subtract 3, giving us about 16 rather than 0.007\. We need
    to catch these sorts of things so that our programs properly interpret their inputs.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要确保数据的格式能够被我们提供的计算软件正确解读。例如，我们可以使用一种称为*科学计数法*的格式来表示非常大或非常小的数字。问题在于，这种表示法没有一个正式的格式。不同的程序使用略有不同的方式来输出这种格式，而其他读取这些数据的程序（比如我们在深度学习中经常使用的库函数）可能会误解它们无法预料的格式。例如，在科学计数法中，值0.007通常会被表示为`7e-3`或`7E-3`。当我们提供`7e-3`作为输入时，程序可能会将其解释为（7
    × *e*）– 3，其中*e*是欧拉常数，值约为2.7。结果是计算机认为`7e-3`意味着我们要求它先将7和*e*的值相乘，然后减去3，得出大约16，而不是0.007。我们需要捕捉到这些问题，以确保我们的程序能正确解读它们的输入。
- en: We also want to look for missing data. If a sample is missing data for one or
    more features, we might be able to patch the holes manually or algorithmically,
    but it might be better to simply remove the sample altogether. This is a subjective
    call that we usually make on a case-by-case basis.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要查找缺失的数据。如果一个样本缺失了一个或多个特征的数据，我们可能能够手动或通过算法填补这些空缺，但有时候直接删除这个样本可能更好。这是一个主观的决策，通常需要根据具体情况逐个判断。
- en: Lastly, we want to identify any pieces of data that seem dramatically different
    from all the others. Some of these *outliers* might be mere typos, like a forgotten
    decimal point. Others might be the result of human error, like a misdirected copy
    and paste, or when someone forgot to delete an entry from a spreadsheet. When
    we don’t know if an outlier is a real piece of data or an error of some kind,
    we have to use our judgment to decide whether to leave it in or remove it manually.
    This is a subjective decision that depends entirely on what our data represents,
    how well we understand it, and what we want to do with it.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要识别出任何与其他数据明显不同的部分。这些*异常值*可能只是一些拼写错误，比如漏掉了小数点。也有可能是人为错误，比如错误的复制粘贴，或者某人忘记从电子表格中删除一条记录。当我们不确定某个异常值是有效数据还是某种错误时，我们必须运用我们的判断力来决定是否保留它，或者手动将其删除。这是一个主观的决策，完全依赖于我们的数据代表什么，我们对其的理解程度，以及我们想如何处理它。
- en: Though these steps may seem straightforward, in practice, carrying them out
    can be a major effort depending on the size and complexity of our data and how
    messed up it is when we first get it. Many tools are available to help us clean
    data. Some are stand-alone, and others are built into machine-learning libraries.
    Commercial services will also clean data for a fee.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些步骤看起来可能很简单，但在实践中，根据我们的数据的大小和复杂性以及我们首次获取时的混乱程度，执行起来可能是一项重大工作。有许多工具可帮助我们清理数据。有些是独立的，而其他的内置在机器学习库中。商业服务也会收费清理数据。
- en: 'It’s useful to keep in mind this classic computing motto: *garbage in, garbage
    out*. In other words, our results are only as good as our starting data, so it’s
    vital that we start with the best data available, which means working hard to
    make it as clean as we possibly can.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 记住这句经典的计算格言非常有用：*垃圾进，垃圾出*。换句话说，我们的结果只能和我们的起始数据一样好，所以从最好的可用数据开始非常重要，这意味着我们必须努力使其尽可能清洁。
- en: Now that we’ve taken care of the essential small stuff, let’s turn our attention
    to making the data well suited for learning.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经处理了基本的细枝末节，让我们把注意力转向使数据适合学习。
- en: The Importance of Consistency
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一致性的重要性
- en: 'Preparing numbers for learning means applying *transformations* to them, without
    changing the relationships among them that we care about. We cover several such
    transformations later in this chapter, where we might scale all the numbers to
    a given range or eliminate some superfluous data so that the learner has less
    work to do. When we do these things, we must always obey a vital principle: any
    time we modify our training data in some way, *we must also modify all future
    data the same way*.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 准备学习的数字意味着对它们应用*变换*，而不改变我们关心的它们之间的关系。我们在本章后面涵盖了几种这样的变换，例如，我们可能将所有数字缩放到给定范围内或者消除一些多余的数据，这样学习者的工作就少了。当我们做这些事情时，我们必须始终遵守一个重要原则：任何时候我们以某种方式修改我们的训练数据，*我们也必须以同样的方式修改所有未来的数据*。
- en: Let’s look at why this is so important. When we make any changes to our training
    data, we typically modify or combine the values in ways that are designed to improve
    the computer’s learning efficiency or accuracy. [Figure 10-1](#figure10-1) shows
    the idea visually.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看为什么这么重要。当我们对训练数据进行任何更改时，我们通常修改或组合值的方式，旨在提高计算机的学习效率或准确性。[图 10-1](#figure10-1)
    用视觉方式展示了这个想法。
- en: '![F10001](Images/F10001.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![F10001](Images/F10001.png)'
- en: 'Figure 10-1: The flow of preprocessing for training and evaluating'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10-1：用于训练和评估预处理流程的流程图
- en: As the figure shows, we typically determine any needed transformations by looking
    at the entirety of the training set. We transform that data to train our learner,
    and we also use the same transformation for all new data that comes after we’ve
    released our system to the world. The key point is that we must apply the *identical*
    modificationsto all new data before we give it to our algorithm for as long as
    our system is in use. This step must not be skipped.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 正如图所示，我们通常通过查看整个训练集来确定所需的所有变换。我们将数据转换为训练我们的学习者，并且在我们释放系统到世界之后，我们也使用相同的转换来处理所有新数据。关键点在于，我们必须为我们的算法提供的所有新数据应用*相同的*修改，只要我们的系统在使用中。这一步绝不能被忽略。
- en: The fact that we need to reuse the same transformation on all data we evaluate
    pops up again and again in machine learning, often in subtle ways. Let’s first
    look at the problem in a general way with a visual example.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在机器学习中再次强调，每当我们评估数据时，我们都需要重新使用相同的转换方式。让我们首先用一个视觉示例来以一种一般的方式看待这个问题。
- en: Suppose we want to teach a classifier how to distinguish pictures of cows from
    pictures of zebras. We can collect a huge number of photos of both animals to
    use as training data. What most obviously distinguishes pictures of these two
    animals are their different black-and-white markings. To make sure our learner
    pays attention to these elements, we may decide to crop each photo to isolate
    the animal’s hide, and then we train with those isolated patches of texture. These
    cropped photos are all that the learner sees. [Figure 10-2](#figure10-2) shows
    a couple of samples.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想教一个分类器如何区分牛的图片和斑马的图片。我们可以收集大量两种动物的照片作为训练数据。最明显区分这两种动物图片的是它们不同的黑白斑纹。为了确保我们的学习者关注这些元素，我们可能决定裁剪每张照片以隔离动物的皮毛，然后用这些孤立的纹理块来训练。学习者只看到这些裁剪过的照片。[图
    10-2](#figure10-2) 展示了几个样本。
- en: '![F10002](Images/F10002.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![F10002](Images/F10002.png)'
- en: 'Figure 10-2: Left: A patch of texture from a cow. Right: A patch of texture
    from a zebra.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图10-2：左：一块牛的纹理。右：一块斑马的纹理。
- en: Suppose that we’ve trained our system and deployed it, but we forget to tell
    people about this preprocessing step of cropping each image to just the texture.
    Without knowing this vital information, a typical user might give our system complete
    pictures of cows and zebras, like those in [Figure 10-3](#figure10-3), and ask
    the system to identify the animal in each one.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们已经训练并部署了我们的系统，但我们忘记告知用户这一图像预处理步骤——将每张图片裁剪到仅保留纹理的部分。如果没有了解这个关键信息，典型的用户可能会提供完整的牛和斑马照片，像[图10-3](#figure10-3)中那样，并要求系统识别每张图片中的动物。
- en: '![F10003](Images/F10003.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![F10003](Images/F10003.png)'
- en: 'Figure 10-3: Left: A photo of a cow. Right: A photo of a zebra. If we trained
    our system on the isolated patterns of [Figure 10-2](#figure10-2), it could be
    misled by all the extra details in the photos.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图10-3：左：一张牛的照片。右：一张斑马的照片。如果我们在[图10-2](#figure10-2)中训练我们的系统，系统可能会被照片中的额外细节误导。
- en: Humans can pick out the hide patterns from these photos. A computer, on the
    other hand, can be misled by the legs, the heads, the ground, and other details,
    thus reducing its ability to give us good results. The difference between the
    prepared data of [Figure 10-2](#figure10-2) and the unprepared data of [Figure
    10-3](#figure10-3) can result in a system that performs beautifully on our training
    data but gives lousy results in the real world. To avoid this, all new data, like
    that in [Figure 10-3](#figure10-3), must be cropped to produce inputs that are
    just like the training data in [Figure 10-2](#figure10-2).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 人类可以从这些照片中识别出皮肤纹理模式。另一方面，计算机可能会被腿部、头部、地面和其他细节误导，从而降低其提供良好结果的能力。[图10-2](#figure10-2)中的准备数据和[图10-3](#figure10-3)中的未准备数据之间的差异可能会导致系统在训练数据上表现出色，但在实际应用中给出糟糕的结果。为了避免这种情况，所有新数据（如[图10-3](#figure10-3)中的数据）必须裁剪成与[图10-2](#figure10-2)中的训练数据完全相同的输入。
- en: 'Forgetting to transform new data in the same way as we transformed the training
    data is an easy mistake to make but usually causes our algorithms to underperform,
    sometimes to the point of becoming useless. The rule to remember is this: we determine
    how to modify our training data, then we modify it, and then we *remember how
    we modified it*. Any time we deal with more data, we must first *modify that data
    in the identical way* that the training data was modified. We’ll come back to
    this idea later and see how it’s used in practice.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 忘记以与我们转换训练数据相同的方式转换新数据是一个容易犯的错误，但通常会导致我们的算法表现不佳，有时甚至变得毫无用处。记住这个规则：我们首先确定如何修改训练数据，然后修改它，接着我们*记住我们是如何修改它的*。每当我们处理更多数据时，必须首先*以与训练数据相同的方式修改这些数据*。我们稍后会回到这个想法，看看它是如何在实践中使用的。
- en: Types of Data
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据类型
- en: 'Typical databases contain different types of data: floating-point numbers,
    strings, integers that refer to categories, and so on. We’ll treat each of these
    data typesin its own way, so it’s useful to distinguish them and give each unique
    type its own name. The most common naming system is based on whether a kind of
    data can be sorted. Though we rarely use explicit sorting when we do deep learning,
    this naming system is still convenient and widely used.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的数据库包含不同类型的数据：浮动小数、字符串、整数（用于类别标识）等。我们会根据不同的数据类型采用不同的处理方式，因此区分这些数据并为每种类型命名是很有用的。最常见的命名系统是基于数据是否可以排序的标准。尽管我们在深度学习中很少显式进行排序，但这个命名系统仍然很方便并被广泛使用。
- en: 'Recall that each sample is a list of values, each of which is called a *feature*.
    Each feature in a sample can be either of two general varieties: *numerical* or
    *categorical*.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，每个样本是一组值的列表，每个值被称为*特征*。每个样本中的特征可以是两种基本类型之一：*数值型*或*类别型*。
- en: Numerical data is simply a number, either floating-point or integer. We also
    call this *quantitative* data. Numerical, or quantitative, data can be sorted
    just by using its values.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 数值数据就是一个数字，可以是浮动小数或整数。我们也称之为*定量*数据。数值数据或定量数据可以通过其值进行排序。
- en: Categorical data is just about anything else, though often it’s a string that
    describes a label such as cow or zebra. The two types of categorical data correspond
    to data that can be naturally sorted and that which can’t.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 类别数据指的是其他类型的数据，通常它是描述标签的字符串，比如牛或斑马。这两种类别数据分别对应可自然排序的数据和不可排序的数据。
- en: '*Ordinal* data is categorical data that has a known order (hence the name),
    so we can sort it. Strings can be sorted alphabetically, but we can also sort
    them by meaning. For example, we can think of the rainbow colors as ordinal, because
    they have a natural ordering in which they appear in a rainbow, from red to orange
    on to violet. To sort the names of colors by rainbow order, we need to use a program
    that understands the orders of colors in a rainbow. Another example of ordinal
    data are strings that describe a person at different ages, such as infant, teenager,
    and elderly. These strings also have a natural order, so we can sort them as well,
    again by some kind of custom routine.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*有序*数据是具有已知顺序的分类数据（因此得名），因此我们可以对其进行排序。字符串可以按字母顺序排序，也可以按含义排序。例如，我们可以将彩虹的颜色视为有序数据，因为它们在彩虹中有一个自然的排序，从红色到橙色，再到紫色。为了按彩虹顺序排序颜色名称，我们需要使用一个理解彩虹颜色顺序的程序。另一个有序数据的例子是描述一个人不同年龄段的字符串，如婴儿、青少年和老年人。这些字符串也有一个自然的顺序，因此我们也可以对它们进行排序，再次通过某种自定义的程序。'
- en: '*Nominal* data is categorical data without a natural ordering. For example,
    a list of desktop items such as paper clip, stapler, and pencil sharpener has
    no natural or built-in ordering, nor does a collection of pictures of clothing,
    like socks, shirts, gloves, and bowler hats. We can turn nominal data into ordinal
    data just by defining an order and sticking to it. For example, we can assert
    that the order of clothing should be from head to toes, so our previous example
    would have the order bowler hats, shirts, gloves, and socks, thereby turning our
    pictures into ordinal data. The order we create for nominal data doesn’t have
    to make any particular kind of sense, it just has to be defined and then used
    consistently.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*名义*数据是没有自然顺序的分类数据。例如，桌面物品列表，如回形针、订书机和削笔刀，并没有自然的排序，衣物图片的集合，比如袜子、衬衫、手套和圆顶礼帽，也没有自然顺序。我们可以通过定义一个顺序并遵循它，将名义数据转化为有序数据。例如，我们可以声明衣物的顺序应该从头到脚，因此我们之前的例子中的顺序应为圆顶礼帽、衬衫、手套和袜子，从而将这些图片转化为有序数据。我们为名义数据创建的顺序不必有特别的逻辑，只要它被定义并且始终如一地使用即可。'
- en: Machine learning algorithms require numbers as input, so we convert string data
    (and any other nonnumerical data) into numbers before we learn from it. Taking
    strings as an example, we could make a list of all the strings in the training
    data, and assign each one a unique number starting with 0\. Many libraries provide
    built-in routines to create and apply this transformation.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法需要数字作为输入，因此我们在学习之前将字符串数据（和任何其他非数字数据）转化为数字。以字符串为例，我们可以列出所有训练数据中的字符串，并为每个字符串分配一个唯一的数字，从0开始。许多库提供了内置的程序来创建并应用这种转换。
- en: One-Hot Encoding
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一位编码（One-Hot Encoding）
- en: Sometimes it’s useful to turn integers into lists. For instance, we might have
    a classifier with 10 classes where class 3 might be toaster and class 7 might
    be ball-point pen, and so on. When we manually assign a label to a photo of one
    of these objects, we consult this list and give it the correct number. When the
    system makes a prediction, it gives us back a list of 10 numbers. Each number
    represents the system’s confidence that the input belongs to the corresponding
    class.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，将整数转化为列表是有用的。例如，我们可能有一个有10个类别的分类器，其中类别3可能是烤面包机，类别7可能是圆珠笔，依此类推。当我们手动为这些物体之一的照片分配标签时，我们查阅这个列表并给它正确的数字。当系统做出预测时，它会返回一个包含10个数字的列表。每个数字代表系统认为输入属于相应类别的置信度。
- en: This means we are comparing our label (an integer) with the classifier’s output
    (a list). When we build classifiers, it makes sense to compare lists to lists,
    so we need a way to turn our label into a list.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们将标签（一个整数）与分类器的输出（一个列表）进行比较。当我们构建分类器时，将列表与列表进行比较是有意义的，因此我们需要一种方法将标签转化为列表。
- en: 'That’s easily done. Our list form of the label is just the list we want from
    the output. Let’s suppose that we’re labeling a picture of a toaster. We want
    the system’s output to be a list of ten values, with a 1 in the slot for class
    3, corresponding to complete certainty that this is a toaster, and a 0 in every
    other slot, indicating complete certainty that the image is none of those other
    things. So, the list form of our label is the very same thing: ten numbers, all
    0, except for a 1 in slot 3.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这很容易做到。我们标签的列表形式就是我们从输出中想要的列表。假设我们正在标注一个烤面包机的图片。我们希望系统的输出是一个包含十个值的列表，其中第 3 类的槽位为
    1，表示我们完全确定这是一个烤面包机，而其他所有槽位都是 0，表示我们完全确定图像不是其他任何东西。所以，我们的标签的列表形式就是这个：十个数字，全部为 0，除了第
    3 个槽位为 1。
- en: Converting a label like 3 or 7 into this kind of list is called *one-hot encoding*,
    referring to only one entry in the list being “hot,” or marked. The list itself
    is sometimes called a *dummy variable*. When we provide class labels to the system
    during training, we usually provide this one-hot encoded list, or dummy variable,
    rather than a single integer.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 将像 3 或 7 这样的标签转换为这种列表的过程称为*独热编码*，即列表中只有一个条目是“热的”或已标记的。这个列表本身有时被称为*虚拟变量*。当我们在训练过程中向系统提供类别标签时，通常提供的是这个独热编码列表或虚拟变量，而不是单一的整数。
- en: Let’s see this in action. [Figure 10-4](#figure10-4)(a) shows the eight colors
    in the original 1903 box of Crayola Crayons (Crayola 2016). Let’s suppose these
    colors appear as strings in our data. The one-hot labels that we provide to the
    system as our labels are shown in the rightmost column.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看实际应用中的情况。[图 10-4](#figure10-4)(a) 显示了 1903 年原版 Crayola Crayons 盒子中的八种颜色（Crayola
    2016）。假设这些颜色在我们的数据中以字符串形式出现。我们提供给系统的独热标签显示在最右列。
- en: '![F10004](Images/F10004.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![F10004](Images/F10004.png)'
- en: 'Figure 10-4: One-hot encoding for the original eight Crayola colors in 1903\.
    (a) The original eight strings. (b) Each string is assigned a value from 0 to
    7\. (c) Each time the string appears in our data, we replace it with a list of
    eight numbers, all of which are 0 except for a 1 in the position corresponding
    to that string’s value.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10-4：1903年原始八种 Crayola 颜色的独热编码。（a）原始的八个字符串。（b）每个字符串都分配一个从 0 到 7 的值。（c）每次该字符串出现在我们的数据中时，我们将其替换为一个包含八个数字的列表，所有数字为
    0，除了对应该字符串值的位置为 1。
- en: So far, we’ve converted data in one form to another form. Now let’s look at
    some transformations that actually change the values in our data.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经将数据从一种形式转换成了另一种形式。现在，让我们来看一些实际上改变我们数据值的变换。
- en: Normalizing and Standardizing
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标准化和规范化
- en: 'We often work with samples whose features span different numerical ranges.
    For instance, suppose we collected data on a herd of African bush elephants. Our
    data describes each elephant with four values:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常处理特征跨越不同数值范围的样本。例如，假设我们收集了关于一群非洲草原象的数据。我们的数据用四个值描述每只大象：
- en: Age in hours (0, 420,000)
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 年龄（小时）（0, 420,000）
- en: Weight in tons (0, 7)
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重量（吨）（0, 7）
- en: Tail length in centimeters (120, 155)
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尾巴长度（厘米）（120, 155）
- en: Age relative to the historical mean age, in hours (−210,000, 210,000)
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 相对于历史平均年龄的年龄，单位小时（−210,000, 210,000）
- en: These are significantly different ranges of numbers. Generally speaking, because
    of the numerical nature of the algorithms we use, larger numbers may influence
    a learning program more than smaller ones. The values in feature 4 are not only
    large, but they also can be negative.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是显著不同的数字范围。一般来说，由于我们使用的算法是数值型的，较大的数字可能比较小的数字对学习程序的影响更大。特征 4 中的值不仅很大，而且还可能是负数。
- en: For the best learning behavior, we want all of our data to be roughly comparable,
    or to fit in roughly the same range of numbers.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得最佳的学习效果，我们希望所有数据在某种程度上具有可比性，或者大致适合相同的数字范围。
- en: Normalization
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 规范化
- en: A common first step in transforming our data is to *normalize* each feature.
    The word *normal* is used in everyday life to mean “typical,” but it also has
    specialized technical meanings in different fields. In this context, we use the
    word in its statistical sense. We say that when we scale data into some specific
    range, the data has been *normalized.* The most popular choice of ranges for normalization
    are [−1,1] and [0,1], depending on the data and what it means (it doesn’t make
    sense to speak of negative apples or ages, for instance). Every machine learning
    library offers a routine to do this, but we have to remember to call it.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 转换数据的一个常见第一步是对每个特征进行*归一化*。在日常生活中，*normal*这个词通常意味着“典型的”，但在不同领域它也有专门的技术含义。在这个上下文中，我们使用的是统计学上的含义。我们说，当我们将数据缩放到某个特定范围时，数据就被*归一化*了。归一化的最常见范围是[−1,1]和[0,1]，具体取决于数据及其含义（例如，谈论负的苹果或年龄就没有意义）。每个机器学习库都提供了执行此操作的常规方法，但我们必须记得调用它。
- en: '[Figure 10-5](#figure10-5) shows a 2D dataset that we’ll use for demonstration.
    We’ve chosen a guitar because the shape helps us see what happens to the points
    as we move them around. We also added colors strictly as a visual aid, only to
    help us see how the points move. The colors have no other meaning.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-5](#figure10-5)展示了我们将用于演示的二维数据集。我们选择了吉他形状，因为它有助于我们看到数据点在移动时发生的变化。我们还添加了颜色，仅仅作为视觉辅助工具，帮助我们看到数据点的移动。颜色没有其他意义。'
- en: '![F10005](Images/F10005.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![F10005](Images/F10005.png)'
- en: 'Figure 10-5: A guitar shape made of 232 points'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10-5：由232个数据点构成的吉他形状
- en: Typically, these points are the results of measurements, say the age and weights
    of some people, or the tempo and volume of a song. To keep things generic, let’s
    call the two features x and y.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，这些数据点是测量结果，例如某些人的年龄和体重，或者一首歌的节奏和音量。为了保持通用性，我们将这两个特征称为x和y。
- en: '[Figure 10-6](#figure10-6) shows the results of normalizing each feature in
    our guitar-shaped data to the range [−1,1] on each axis.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-6](#figure10-6)显示了将我们吉他形状数据中的每个特征归一化到范围[−1,1]后的结果。'
- en: '![F10006](Images/F10006.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![F10006](Images/F10006.png)'
- en: 'Figure 10-6: The data of [Figure 10-5](#figure10-5) after normalization to
    the range [−1,1] on each axis. The skewing of the shape is due to it being stretched
    more along the Y axis than the X.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10-6：对[图 10-5](#figure10-5)中的数据进行归一化处理后，数据范围被调整到[−1,1]。形状的倾斜是由于在y轴方向上拉伸较多，而x轴方向的拉伸较少。
- en: In [Figure 10-6](#figure10-6), the x values are scaled from −1 to 1, and the
    y values are independently scaled from −1 to 1\. The guitar shape resulting from
    this operation has skewed a little bit because it’s been stretched more vertically
    than horizontally. This happens any time the different dimensions of the starting
    data span different ranges. In our case, the x data originally spanned the range
    of about [–1, 0] and the y data spanned about [–0.5, 0.2]. When we adjusted the
    values, we had to stretch the y values apart more than the x values, causing the
    skewing we see in [Figure 10-6](#figure10-6).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 10-6](#figure10-6)中，x值的范围从−1到1进行缩放，而y值则独立地从−1到1进行缩放。通过这种操作得到的吉他形状稍微发生了倾斜，因为它在垂直方向上被拉伸得比水平方向更长。每当起始数据的不同维度跨越不同范围时，就会发生这种情况。在我们的例子中，x数据最初的范围大约是[–1,
    0]，而y数据的范围大约是[–0.5, 0.2]。当我们调整这些值时，我们必须比x值更大地拉伸y值，导致了在[图 10-6](#figure10-6)中看到的倾斜。
- en: Standardization
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标准化
- en: Another common operation involves *standardizing* each feature. This is a two-step
    process. First, we add (or subtract) a fixed value to all the data for each feature
    so that the mean value of every feature is 0 (this step is also called *mean normalization*
    or *mean subtraction*). In our 2D data, this moves the entire dataset left-right
    and up-down so that the mean value is sitting right on (0,0). Then, instead of
    normalizing or scaling each feature to lie between −1 and 1, we scale it so that
    it has a standard deviation of 1 (this step is also called *variance normalization*).
    Recall from Chapter 2 that this means about 68 percent of the values in that feature
    lie in the range of −1 to 1\.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见操作是对每个特征进行*标准化*。这是一个两步过程。首先，我们为每个特征的所有数据添加（或减去）一个固定值，使得每个特征的均值为0（这个步骤也叫做*均值归一化*或*均值减法*）。在我们的二维数据中，这会将整个数据集左右和上下移动，使得均值恰好位于(0,0)的位置。然后，我们不是将每个特征归一化或缩放到−1到1之间，而是将其缩放到标准差为1（这个步骤也叫做*方差归一化*）。回想第二章，这意味着该特征中大约68%的值位于−1到1的范围内。
- en: In our 2D example, the x values are stretched or compressed horizontally until
    about 68 percent of the data is between −1 and 1 on the X axis, and then the y
    values are stretched or compressed vertically until the same thing is true on
    the Y axis. This necessarily means that points will land outside of the range
    [−1, 1] on each axis, so our results are different than what we get from normalization.
    [Figure 10-7](#figure10-7) shows the application of standardization to our starting
    data in [Figure 10-5](#figure10-5).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的二维示例中，x值会被水平拉伸或压缩，直到大约68%的数据在X轴的−1和1之间，然后y值会被垂直拉伸或压缩，直到Y轴上也满足相同的条件。这必然意味着点会落在每个轴的[−1,1]范围之外，因此我们的结果与归一化所得到的不同。[图10-7](#figure10-7)展示了标准化应用于我们在[图10-5](#figure10-5)中的原始数据。
- en: '![F10007](Images/F10007.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![F10007](Images/F10007.png)'
- en: 'Figure 10-7: The data of [Figure 10-5](#figure10-5) after standardization'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图10-7：标准化后[图10-5](#figure10-5)的数据
- en: Here again we see that when the original shape doesn’t fit a normal distribution,
    a transformation like standardization can skew or otherwise distort the shape
    of the original data. Most libraries offer routines to normalize or standardize
    any or all of our features in one call. This makes it convenient to satisfy some
    algorithms that require their input to be normalized or standardized.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们再次看到，当原始形状不符合正态分布时，像标准化这样的转换可能会扭曲或以其他方式改变原始数据的形状。大多数库提供了一个函数，可以一次性对我们的所有特征进行归一化或标准化处理。这使得满足某些要求输入数据归一化或标准化的算法变得更加方便。
- en: Remembering the Transformation
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 记住转换
- en: Both normalization and standardization routines are controlled by parameters
    that tell them how to do their jobs. Most library routines analyze the data to
    find these parameters and then use them to apply the transformation. Because it’s
    so important to transform future data with the same operations, these library
    calls always give us a way to hang onto these parameters so we can apply the same
    transformations again later.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 归一化和标准化过程都由参数控制，这些参数告诉它们如何执行任务。大多数库函数会分析数据以找到这些参数，然后利用它们来应用转换。由于使用相同操作转换未来数据至关重要，这些库调用总是提供一种方法来保存这些参数，以便我们以后可以再次应用相同的转换。
- en: In other words, when we later receive a new batch of data to evaluate, either
    to evaluate our system’s accuracy or to make real predictions out in the field,
    we do *not* analyze that data to find new normalizing or standardizing transformations.
    Instead, we apply the same normalizing or standardizing steps that we determined
    for the training data.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，当我们稍后收到一批新数据以进行评估时，无论是为了评估系统的准确性，还是为了在实际应用中做出预测，我们*不会*分析这些数据以寻找新的归一化或标准化转换。相反，我们会应用为训练数据确定的相同归一化或标准化步骤。
- en: A consequence of this step is that the newly transformed data is almost never
    itself normalized or standardized. That is, it won’t be in the range [−1,1] on
    both axes, or it won’t have its average at (0,0) and contain 68 percent of its
    data in the range [−1,1] on each axis. That’s fine. What’s important is that we’re
    using the same transformation. If the new data isn’t quite normalized or standardized,
    so be it.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这一过程的结果是，新转换后的数据几乎从未真正归一化或标准化。也就是说，数据的范围不会在两个轴上都在[−1,1]之间，或者它的平均值不会在(0,0)处，并且其68%的数据不会在每个轴上的[−1,1]范围内。这没关系。重要的是我们使用了相同的转换。如果新数据没有完全归一化或标准化，那也无妨。
- en: Types of Transformations
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换类型
- en: Some transformations are *univariate*, which means they work on just one feature
    at a time, each independent of the others (the name comes from combining *uni*,
    for one, with *variate*, which means the same as variable or feature). Others
    are *multivariate*, meaning they work on many features simultaneously.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一些转换是*单变量*的，这意味着它们一次只处理一个特征，每个特征彼此独立（该名称来源于将*uni*（表示“一个”）与*variate*（表示“变量”或“特征”）结合）。其他的则是*多变量*的，意味着它们同时处理多个特征。
- en: Let’s consider normalization. This is usually implemented as a univariate transformer
    that treats each feature as a separate set of data to be manipulated. That is,
    if it’s scaling 2D points to the range [0,1], it would scale all the x values
    to that range, and then independently scale all the y values. The two sets of
    features don’t interact in any way, so how the X axis gets scaled does not depend
    at all on the y values, and vice versa. [Figure 10-8](#figure10-8) shows this
    ideal visually for a normalizer applied to data with three features.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一下标准化。这通常作为一个单变量变换器来实现，它将每个特征当作一个独立的数据集来处理。也就是说，如果它是将二维点缩放到[0,1]范围，它会首先将所有的x值缩放到这个范围，然后独立地将所有的y值缩放到这个范围。两个特征集之间没有任何交互，因此X轴如何被缩放与y值无关，反之亦然。[图10-8](#figure10-8)直观地展示了这一理想情况，展示了对具有三个特征的数据应用的标准化过程。
- en: '![F10008](Images/F10008.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![F10008](Images/F10008.png)'
- en: 'Figure 10-8: When we apply a univariate transformation, each feature is transformed
    independently of the others. Here we are normalizing three features to the range
    [0,1]. (a) The starting ranges of three features. (b) Each of the three ranges
    is independently shifted and stretched to the range [0,1].'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图10-8：当我们应用单变量变换时，每个特征是独立地变换的。在这里，我们将三个特征标准化到[0,1]的范围内。(a) 三个特征的起始范围。(b) 每个特征的范围独立地被移动和拉伸到[0,1]的范围内。
- en: By contrast, a multivariate algorithm looks at multiple features at a time and
    treats them as a group. The most extreme (and most common) version of this process
    is to handle all of the features simultaneously. If we scale our three colored
    bars in a multivariate way, we move and stretch them all as a group until they
    collectivelyfill the range [0,1], as illustrated in [Figure 10-9](#figure10-9).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，多变量算法同时考虑多个特征，并将它们作为一个整体来处理。这个过程的最极端（也是最常见）版本是同时处理所有特征。如果我们以多变量方式缩放这三根彩条，我们将它们作为一个整体进行移动和拉伸，直到它们共同覆盖[0,1]的范围，正如[图10-9](#figure10-9)所示。
- en: We can apply many transformations in either a univariate or multivariate way.
    We choose based on our data and application. For instance, the univariate version
    made sense in [Figure 10-6](#figure10-6) when we scaled our x and y samples because
    they’re essentially independent. But suppose our features are temperature measurements
    made at different times over the course of different days? We probably want to
    scale all the features together so that, as a collection, they span the range
    of temperatures we’re working with.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以采用单变量或多变量方式进行多种变换。我们根据数据和应用场景来选择。例如，当我们缩放x和y样本时，单变量版本在[图10-6](#figure10-6)中是合理的，因为它们本质上是独立的。但假设我们的特征是不同时间、不同日期的温度测量值呢？我们可能希望将所有特征一起缩放，这样作为一个整体，它们就覆盖了我们所使用的温度范围。
- en: '![F10009](Images/F10009.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![F10009](Images/F10009.png)'
- en: 'Figure 10-9: When we apply a multivariate transformation, we treat multiple
    features simultaneously. Here we are again normalizing to the range [0,1]. (a)
    The starting ranges of three features. (b) The bars are shifted and stretched
    as a group so that their collective minimum and maximum values span the range
    [0,1].'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图10-9：当我们应用多变量变换时，我们同时处理多个特征。在这里，我们再次将范围标准化到[0,1]。(a) 三个特征的起始范围。(b) 条形图作为一个整体被移动和拉伸，使得它们的最小值和最大值共同覆盖[0,1]的范围。
- en: Slice Processing
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 切片处理
- en: Given a dataset, we need to think about how we select the data we want to transform.
    There are three approaches, depending on whether we think of *slicing*, or extracting,
    our data by sample, by feature, or by element. These approaches are respectively
    called *samplewise*, *featurewise*, and *elementwise* processing.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个数据集，我们需要思考如何选择我们要转换的数据。根据我们是否按样本、按特征或按元素提取数据，通常有三种方法。这些方法分别称为*样本级*、*特征级*和*元素级*处理。
- en: Let’s look at them in that order. For this discussion, let’s assume that each
    sample in our dataset is a list of numbers. We can arrange the whole dataset in
    a 2D grid, where each row holds a sample and each element in that row is a feature.
    [Figure 10-10](#figure10-10) shows the setup.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们按这个顺序来看看它们。为了讨论的方便，假设我们数据集中的每个样本都是一组数字列表。我们可以将整个数据集安排成一个二维网格，其中每一行包含一个样本，每一行中的每个元素是一个特征。[图10-10](#figure10-10)展示了这种设置。
- en: '![F10010](Images/F10010.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![F10010](Images/F10010.png)'
- en: 'Figure 10-10: Our database for the coming discussion is a 2D grid. Each row
    is a sample that contains multiple features, which make up the columns.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图10-10：我们接下来讨论的数据集是一个二维网格。每一行是一个样本，包含多个特征，这些特征构成了列。
- en: Samplewise Processing
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 样本级处理
- en: The samplewise approach is appropriate when all of our features are aspects
    of the same thing. For example, suppose our input data contains little snippets
    of audio, such as a person speaking into a cell phone. Then the features in each
    sample are the amplitude of the audio at successive moments, as in [Figure 10-11](#figure10-11).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 样本逐一处理方法适用于所有特征都是同一事物的不同方面的情况。例如，假设我们的输入数据包含一些音频片段，例如某人通过手机讲话。那么每个样本中的特征就是音频在连续时刻的振幅，如[图10-11](#figure10-11)所示。
- en: If we want to scale this data to the range [0,1], it makes sense to scale all
    the features in a single sample so the loudest parts are set to 1 and the quietest
    parts to 0\. Thus, we process each sample, one at a time, independent of the other
    samples.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想将这些数据缩放到[0,1]范围内，那么按单个样本缩放所有特征是有意义的，这样最响亮的部分就设为1，最安静的部分设为0。因此，我们一次处理一个样本，独立于其他样本。
- en: '![F10011](Images/F10011.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![F10011](Images/F10011.png)'
- en: 'Figure 10-11: Each sample consists of a series of measurements of a short audio
    waveform. Each feature gives us an instantaneous measurement of the volume of
    the sound at that moment.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图10-11：每个样本由一段短音频波形的测量数据组成。每个特征给出了该时刻声音的音量的瞬时测量值。
- en: Featurewise Processing
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 按特征处理
- en: The featurewise approach is appropriate when our samples represent essentially
    different things.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 按特征处理方法适用于我们的样本代表本质上不同的事物。
- en: Suppose we’ve taken a variety of weather measurements each evening, recording
    the temperature, rainfall, wind speed, and humidity. This gives us four features
    per sample, as in [Figure 10-12](#figure10-12).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们每天晚上都进行各种天气测量，记录温度、降水量、风速和湿度。这为每个样本提供了四个特征，如[图10-12](#figure10-12)所示。
- en: '![F10012](Images/F10012.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![F10012](Images/F10012.png)'
- en: 'Figure 10-12: When we process our data featurewise, we analyze each column
    independently. Top three lines: The original data. Middle line: The range. Bottom
    line: The scaled data.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图10-12：当我们按特征处理数据时，我们独立分析每一列。前三行：原始数据。中间一行：范围。底部一行：缩放后的数据。
- en: It doesn’t make sense to scale this data on a samplewise basis, because the
    units and measurements are incompatible. We can’t compare the wind speed and the
    humidity on an equal footing. But we can analyze all of the humidity values together,
    and the same is true for all the values for temperature, rainfall, and humidity.
    In other words, we modify each feature in turn.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 按样本逐一缩放这些数据没有意义，因为单位和测量方式不兼容。我们不能在同等条件下比较风速和湿度。但我们可以将所有湿度值一起分析，温度、降水量和湿度的所有值也可以以同样的方式处理。换句话说，我们依次修改每个特征。
- en: When we process data featurewise, each column of feature values is sometimes
    called a *fibre*.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们按特征处理数据时，每一列特征值有时被称为*纤维*。
- en: Elementwise Processing
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 元素逐一处理
- en: The elementwise approach treats each element in the grid of [Figure 10-10](#figure10-10)
    as an independent entity and applies the same transformation to every element
    in the grid independently. This is useful, for example, when all of our data represents
    the same kind of thing, but we want to change its units. For instance, suppose
    that each sample corresponds to a family with eight members and contains the heights
    of each of the eight people. Our measurement team reported their heights in inches,
    but we want the heights in millimeters.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 元素逐一处理方法将[图10-10](#figure10-10)网格中的每个元素视为独立实体，并对网格中的每个元素独立应用相同的变换。例如，当我们的所有数据代表相同类型的事物，但我们想改变其单位时，这种方法很有用。例如，假设每个样本对应一个有八个成员的家庭，并包含这八个人的身高。我们的测量团队报告了他们的身高（单位为英寸），但我们想要将其转换为毫米。
- en: We need only multiply every entry in the grid by 25.4 to convert inches to millimeters.
    It doesn’t matter if we think of this as working across rows or along columns,
    since every element is handled the same way.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需要将网格中的每个条目乘以25.4，将英寸转换为毫米。无论我们是沿着行还是列进行操作都没有关系，因为每个元素都以相同的方式处理。
- en: We do this frequently when we work with images. Image data often arrives with
    each pixel in the range [0,255]. We often apply an elementwise scaling operation
    to divide every pixel value in the entire input by 255, giving us data from 0
    to 1.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在处理图像时经常这样做。图像数据通常每个像素的值在[0,255]范围内。我们通常应用元素逐一缩放操作，将整个输入图像的每个像素值除以255，得到的结果是一个0到1之间的数据。
- en: Most libraries allow us to apply transformations using any of these interpretations.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数库允许我们使用这些解释中的任何一种应用变换。
- en: Inverse Transformations
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逆变换
- en: We’ve been looking at different transformations that we can apply to our data.
    However, sometimes we want to undo, or *invert*, those steps so we can more easily
    compare our results to our original data.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们一直在研究可以应用于数据的不同转换。但是，有时我们希望撤销或*反转*这些步骤，这样我们就可以更容易地将结果与原始数据进行比较。
- en: For example, suppose we work for the traffic department of a city that has one
    major highway. Our city is far north, so the temperature often drops below freezing.
    The city managers have noticed that the traffic density seems to vary with temperature,
    with more people staying home on the coldest days. In order to plan for roadwork
    and other construction, the managers want to know how many cars they can predict
    for each morning’s rush-hour commute based on the temperature. Because it takes
    some time to measure and process the data, we decide to measure the temperature
    at midnight each evening and then predict how many cars will be on the road between
    7 and 8 am the next morning. We’re going to start using our system in the middle
    of winter, so we expect temperatures both above and below freezing (0° Celsius).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们为一个有一条主要高速公路的城市交通部门工作。我们的城市位于北方，所以温度常常降到冰点以下。城市管理者注意到，交通密度似乎与温度有关，最冷的日子里更多的人待在家里。为了规划道路施工和其他建设，管理者希望了解他们可以根据温度预测每天早晨高峰时段的车辆数量。由于需要一些时间来测量和处理数据，我们决定每晚午夜测量一次温度，然后预测第二天早晨7点到8点之间路上的车辆数量。我们将在冬季中期开始使用系统，因此我们预期温度会在冰点以上和以下（0°C）波动。
- en: For a few months, we measure the temperature at every midnight, and we count
    the total number of cars passing a particular marker on the road between 7 and
    8 am the next morning. The raw data is shown in [Figure 10-13](#figure10-13).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 几个月来，我们每晚午夜测量温度，并在第二天早晨7点到8点之间统计通过特定路标的总车辆数。原始数据如[图10-13](#figure10-13)所示。
- en: We want to give this data to a machine-learning system that will learn the connection
    between temperature and traffic density. After deployment, we feed in a sample
    consisting of one feature, describing the temperature in degrees, and we get back
    a real number telling us the predicted number of cars on the road.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望将这些数据提供给一个机器学习系统，该系统将学习温度与交通密度之间的关系。部署后，我们输入一个样本，包含一个特征，描述温度（以度数表示），然后返回一个实数，告诉我们预测的路上车辆数量。
- en: '![F10013](Images/F10013.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![F10013](Images/F10013.png)'
- en: 'Figure 10-13: Each midnight we measure the temperature, and then the following
    morning, we measure the number of cars on the road between 7 and 8 am.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图10-13：每个午夜我们测量一次温度，然后在第二天早晨7点到8点之间测量路上的车辆数量。
- en: Let’s suppose that the regression algorithm we’re using works best when its
    input data is scaled to the range [0,1]. We can normalize the data to [0,1] on
    both axes, as in [Figure 10-14](#figure10-14).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们使用的回归算法在输入数据缩放到[0,1]范围内时效果最好。我们可以像[图10-14](#figure10-14)那样，在两个坐标轴上将数据归一化到[0,1]范围。
- en: '![F10014](Images/F10014.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![F10014](Images/F10014.png)'
- en: 'Figure 10-14: Normalizing both ranges to [0,1] makes the data more amenable
    for training.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图10-14：将两个范围都归一化到[0,1]使数据更适合训练。
- en: This looks just like [Figure 10-13](#figure10-13), only now both our scales
    (and data) run from 0 to 1\.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来就像是[图10-13](#figure10-13)，只是现在我们的两个尺度（和数据）都从0到1。
- en: We’ve stressed the importance of remembering this transformation so we can apply
    it to future data. Let’s look at those mechanics in three steps. For convenience,
    let’s use an object-oriented philosophy, where our transformations are carried
    out by objects that remember their own parameters.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们强调记住这种转换的重要性，这样我们才能将其应用于未来的数据。我们将分三步来介绍这些机制。为了方便，我们采用面向对象的哲学，其中我们的转换由对象执行，而对象会记住它们自己的参数。
- en: The first of our three steps is to create a transformer object for each axis.
    This object is capable of performing this transformation (also called a *mapping*).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的三步中的第一步是为每个坐标轴创建一个转换器对象。这个对象能够执行这种转换（也叫做*映射*）。
- en: Second, let’s give that object our input data to analyze. It finds the smallest
    and largest values and uses them to create the transformation that shifts and
    scales our input data to the range [0,1]. We’ll give the temperature data to the
    first transformer and the vehicle count data to the second transformer.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，让我们将输入数据提供给该对象进行分析。它找到最小值和最大值，并利用它们创建转换，将我们的输入数据移位并缩放到[0,1]范围内。我们将温度数据提供给第一个转换器，将车辆计数数据提供给第二个转换器。
- en: So far, we’ve only created the transformers, but we haven’t applied them. Nothing
    has changed in any of our data.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只创建了转换器，但还没有应用它们。我们的数据没有发生任何变化。
- en: '[Figure 10-15](#figure10-15) shows the idea.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-15](#figure10-15)展示了这个过程。'
- en: '![f10015](Images/f10015.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![f10015](Images/f10015.png)'
- en: 'Figure 10-15: Building transformation objects. Left: The temperature data is
    fed to a transformation object, represented by a blue rectangle. Right: We also
    build a yellow transformer for the car counts.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10-15：构建转换对象。左侧：温度数据输入到一个转换对象中，表示为一个蓝色矩形。右侧：我们还为汽车计数构建了一个黄色的转换器。
- en: The third step is to give our data to the transform objects again, but this
    time, we tell them to apply the transformation they have already computed. The
    result is a new set of data that has been transformed to the range [0,1]. [Figure
    10-16](#figure10-16) shows the idea.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 第三步是将我们的数据再次交给转换对象，这次我们告诉它们应用它们已经计算出的转换。结果是一个新的数据集，已经转换到[0,1]的范围内。[图 10-16](#figure10-16)展示了这个过程。
- en: Now we’re ready to learn. We give our transformed data to our learning algorithm
    and let it figure out the relationship between the inputs and the outputs, as
    shown schematically in [Figure 10-17](#figure10-17).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备好进行学习了。我们将转换后的数据提供给学习算法，让它分析输入与输出之间的关系，如[图 10-17](#figure10-17)所示。
- en: Let’s assume that we’ve trained our system, and it’s doing a good job of predicting
    car counts from temperature data.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们已经训练好了系统，并且它能很好地根据温度数据预测汽车数量。
- en: The next day, we deploy our system on a web page for the city managers. On the
    first night, the manager on duty measures a midnight temperature of −10° Celsius.
    She opens up our application, finds the input box for the temperature, types in
    −10, and hits the big “Predict Traffic” button.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 第二天，我们将系统部署到城市管理者的网页上。在第一个晚上，值班经理测得午夜时温度为−10°摄氏度。她打开我们的应用程序，找到温度输入框，输入−10并点击“大型预测交通”按钮。
- en: '![F10016](Images/F10016.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![F10016](Images/F10016.png)'
- en: 'Figure 10-16: Each feature is modified by the transformation we previously
    computed for it. The output of the transformations goes into our learning system.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10-16：每个特征都被我们之前计算出的转换所修改。转换后的输出进入我们的学习系统。
- en: '![F10017](Images/F10017.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![F10017](Images/F10017.png)'
- en: 'Figure 10-17: A schematic view of learning from our transformed features and
    targets'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10-17：从我们转换后的特征和目标中学习的示意图
- en: Uh-oh, something’s gone wrong. We can’t just feed −10 into our trained system,
    because as [Figure 10-17](#figure10-17) shows, it’s expecting a number in the
    range of 0 to 1\. We need to transform the data somehow. The only way that makes
    sense is to apply the same transformation that we applied to the temperatures
    when we trained our system. For example, if in our original dataset −10 became
    the value 0.29, then if the temperature is −10 tonight, we should enter 0.29,
    not –10\.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 哎呀，出现了问题。我们不能直接将−10输入到我们训练过的系统中，因为正如[图 10-17](#figure10-17)所示，它期望输入的是一个范围在0到1之间的数字。我们需要以某种方式对数据进行转换。唯一合理的做法是应用我们在训练系统时对温度所做的相同转换。例如，如果在原始数据集中−10变成了0.29，那么如果今晚的温度是−10，我们应该输入0.29，而不是−10\。
- en: Here’s where we see the value of saving our transformation as an object. We
    can simply tell that object to take the same transformation that it applied to
    our training data and now apply it to this new piece of data. If −10 turned into
    0.29 during training, any new input of −10 turns into 0.29 during deployment as
    well.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这时我们就能看到将转换保存为对象的价值。我们可以简单地告诉这个对象，使用它在训练数据上应用的相同转换，现在将其应用到这组新的数据上。如果−10在训练中变成了0.29，那么任何新的−10输入，在部署时也会变成0.29。
- en: Let’s suppose that we correctly give the temperature 0.29 to the system, and
    it produces a traffic density of 0.32\. This corresponds to the value of some
    number of cars transformed by our car transformation. But that value is between
    0 and 1, because that was the range of the data we trained on representing car
    counts. How do we undo that transformation and turn it into a number of cars?
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正确地将温度0.29提供给系统，它输出的交通密度是0.32\。这对应于通过我们对汽车转换所得到的某个汽车数量的值。但是这个值是在0和1之间，因为这是我们训练时代表汽车数量的数据的范围。我们如何撤销这个转换，把它转换回汽车数量呢？
- en: In any machine learning library, every transformation object comes with a routine
    to *invert*, or undo, its transformation, providing us with an *inverse transformation*.
    In this case, it inverts the normalizing transformation it’s been applying so
    far. If the object transformed 39 cars into the normalized value 0.32, then the
    inverse transformation turns the normalized value 0.32 back into 39 cars. This
    is the value we print out to the city manager. [Figure 10-18](#figure10-18) shows
    these steps.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何机器学习库中，每个转换对象都有一个*逆向*例程，用来撤销其转换，提供一个*逆向转换*。在这种情况下，它会反转之前应用的标准化转换。如果该对象将39辆车转换为标准化值0.32，那么逆向转换会将标准化值0.32转回为39辆车。这就是我们打印给城市经理的值。[图10-18](#figure10-18)展示了这些步骤。
- en: '![F10018](Images/F10018.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![F10018](Images/F10018.png)'
- en: 'Figure 10-18: When we feed a new temperature to our system, we transform it
    using the transformation we computed for our temperature data, turning it into
    a number from 0 to 1\. The value that comes out is then run through the inverse
    of the transformation we computed for the car data, turning it from a scaled number
    into a number of cars.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图10-18：当我们将新的温度输入系统时，我们使用我们为温度数据计算的转换，将其转换为一个0到1之间的数字。出来的值接着会通过我们为汽车数据计算的逆向转换，将其从缩放的数字转换为汽车的数量。
- en: One thing that can seemingly go wrong here is if we get new samples outside
    of the original input range. Suppose we get a surprisingly cold temperature reading
    one night of −50° Celsius, which is far below the minimum value in our original
    data. The result is that the transformed value is a negative number, outside of
    our [0,1] range. The same thing can happen if we get a very hot night, giving
    us a positive temperature that transforms to a value greater than 1, which is
    again outside of [0,1].
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这里可能出现的一个问题是，如果我们获得了超出原始输入范围的新样本。假设我们在某个夜晚测得了一个令人吃惊的低温 −50° 摄氏度，这个温度远低于我们原始数据中的最小值。结果是，经过转换后的值是一个负数，超出了我们的[0,1]范围。如果我们遇到一个非常热的夜晚，测得一个正温度，经过转换后得到的值将大于1，这同样超出了[0,1]范围。
- en: Both situations are fine. Our desire for scaling inputs to [0,1] is to make
    training go as efficiently as possible, and also to keep numerical issues in check.
    Once the system is trained, we can give it any values we want as input, and it
    calculates a corresponding output. Of course, we still have to pay attention to
    our data. If the system predicts a negative number of cars for tomorrow, we don’t
    want to make plans based on that number.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 两种情况都是可以的。我们希望将输入缩放到[0,1]范围内，是为了让训练尽可能高效，并且控制数值问题。一旦系统训练完毕，我们可以给它任何我们想要的输入，它会计算出相应的输出。当然，我们仍然需要注意我们的数据。如果系统预测明天的车数为负数，我们是不能基于这个数字来制定计划的。
- en: Information Leakage in Cross-Validation
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交叉验证中的信息泄漏
- en: We’ve seen how to build the transformation from the training set and then retain
    that transformation and apply it, unchanged, to all additional data. If we don’t
    follow this policy carefully, we can get *information leakage*, where information
    that doesn’t belong in our transformation accidentally sneaks (or leaks) into
    it, affecting the transformation. This means that we don’t transform the data
    the way we intend. Worse, this leakage can lead to the system getting an unfair
    advantage when it evaluates our test data, giving us an overinflated measure of
    accuracy. We may conclude that our system is performing well enough to be deployed,
    only to be disappointed when it has much worse performance when used for real.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到如何从训练集构建转换，然后保持这个转换，并将其不变地应用于所有附加数据。如果我们不仔细遵循这个策略，就可能会发生*信息泄漏*，即本不应包含在转换中的信息意外地渗入（或泄漏）到转换中，从而影响转换。这意味着我们没有按照预期的方式转换数据。更糟糕的是，这种泄漏可能导致系统在评估我们的测试数据时获得不公平的优势，从而给我们一个虚高的准确度衡量值。我们可能会得出系统足够好，可以部署的结论，结果却会在实际使用时表现得更差。
- en: Information leakage is a challenging problem because many of its causes can
    be subtle. As an example, let’s see how information leakage can affect the process
    of cross-validation, which we discussed in Chapter 8\. Modern libraries give us
    convenient routines that provide fast and correct implementations of cross-validation,
    so we don’t have to write our own code to do it. But let’s look under the hood
    anyway. We’ll see how a seemingly reasonable approach leads to information leakage,
    and then how we can fix it. Seeing this in action will help us get better at preventing,
    spotting, and fixing information leakage in our own systems and algorithms.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 信息泄漏是一个具有挑战性的问题，因为它的许多原因可能是微妙的。举个例子，让我们看看信息泄漏是如何影响交叉验证过程的，这一点我们在第8章讨论过。现代库提供了方便的例程，提供快速且正确的交叉验证实现，因此我们不必自己编写代码来实现它。但我们还是来看看底层的实现。我们将看到一个看似合理的方法是如何导致信息泄漏的，然后我们将看看如何修复它。看到实际操作中的这些情况将帮助我们更好地预防、发现和修复我们自己系统和算法中的信息泄漏。
- en: Recall that in cross-validation, we set aside one fold (or section) of the starting
    training set to serve as a temporary validation set. Then we build a new learner
    and train it with the remaining data. When we’re done training, we evaluate the
    learner using the saved fold as the validation set. This means that each time
    through the loop, we have a new training set (the starting data without the samples
    in the selected fold). If we’re going to apply a transformation to our data, we
    need to build it from just the data that’s being used as the training set for
    that learner. We then apply that transformation to the current training set, and
    we apply *the same transformation* to the current validation set. The key thing
    to remember is that because in cross-validation we create a new training set and
    validation set each time through the loop, we need to build a new transformation
    each time through the loop as well.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下在交叉验证中，我们将初始训练集中的一个折叠（或部分）留出作为临时验证集。然后我们建立一个新的学习器，并用其余数据进行训练。当训练完成后，我们使用保存的折叠作为验证集来评估学习器。这意味着每次进入循环时，我们都有一个新的训练集（即去除了选定折叠样本的初始数据）。如果我们要对数据进行转换，我们需要仅基于作为该学习器训练集的数据来构建转换。然后，我们将这个转换应用到当前的训练集，并将*相同的转换*应用到当前的验证集。需要记住的关键点是，因为在交叉验证中我们每次通过循环都会创建一个新的训练集和验证集，所以我们每次通过循环时也需要构建一个新的转换。
- en: Let’s see what happens if we do it incorrectly. [Figure 10-19](#figure10-19)
    shows our starting set of samples at the left. They’re analyzed to produce a transformation
    (shown by the red circle), which is then applied by a transformation routine (marked
    T). To show the change, we colored the transformed samples red, like the transformation.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如果我们做错了会发生什么。[图 10-19](#figure10-19)显示了我们开始的样本集在左侧。它们被分析以生成一个转换（由红圈表示），然后通过转换例程（标记为T）应用该转换。为了展示变化，我们将转换后的样本标为红色，像转换一样。
- en: '![f10019](Images/f10019.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![f10019](Images/f10019.png)'
- en: 'Figure 10-19: A *wrong* way to do cross-validation: building one transform
    based on all the original training data'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10-19：一种*错误*的交叉验证方法：基于所有原始训练数据构建一个转换
- en: Then we enter the cross-validation process. Here the loop is “unrolled,” so
    we’re showing several instances of the training sessions, each associated with
    a different fold. Each time through the loop, we remove a fold, train on the remaining
    samples, and then test with the validation fold and create a score.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们进入交叉验证过程。这里循环是“展开”的，因此我们展示了多个训练会话的实例，每个实例与不同的折叠相关联。每次进入循环时，我们移除一个折叠，使用剩余的样本进行训练，然后用验证折叠进行测试并创建得分。
- en: The problem here is that when we analyzed the input data to build the transformation,
    we included the data in every fold in our analysis. To see why this is a problem,
    let’s look more closely at what’s going on with a simpler and more specific scenario.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的问题是，当我们分析输入数据以构建转换时，我们将每个折叠中的数据都包含在了分析中。为了看看为什么这是一个问题，让我们更仔细地观察一下在一个更简单、更具体的场景中发生了什么。
- en: Suppose that the transformation we want to apply is scaling all the features
    in the training set, as a group, to the range 0 to 1\. That is, we’ll do samplewise
    multivariate normalization. Let’s say that in the very first fold, the smallest
    and largest feature values are 0.01 and 0.99\. In the other folds, the largest
    and smallest values occupy smaller ranges. [Figure 10-20](#figure10-20) shows
    the range of data contained in each of the five folds. We’re going to analyze
    data in all the folds and build our transformation from that.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要应用的转换是将训练集中的所有特征按组缩放到0到1的范围内。也就是说，我们将进行逐样本的多变量标准化。假设在第一个折叠中，最小和最大特征值分别为0.01和0.99。在其他折叠中，最大和最小值位于较小的范围内。[图10-20](#figure10-20)展示了每个折叠中数据的范围。我们将分析所有折叠中的数据，并从中构建我们的转换。
- en: In [Figure 10-20](#figure10-20), our dataset is shown at the left, split into
    five folds. Inside each box, we show the range of values in that fold, with 0
    at the left and 1 at the right. The top fold has features running from 0.01 to
    0.99\. The other folds have values that are well within this range. When we analyze
    all the folds as a group, the range of the first fold dominates, so we only stretch
    the whole dataset by a little bit.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图10-20](#figure10-20)中，左侧显示了我们的数据集被分成五个折叠。在每个框内，我们展示了该折叠的值范围，左侧为0，右侧为1。顶部的折叠包含从0.01到0.99的特征。其他折叠中的值位于此范围内。当我们将所有折叠的数据作为一个整体进行分析时，第一个折叠的范围占主导地位，因此我们只会稍微拉伸整个数据集。
- en: '![F10020](Images/F10020.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![F10020](Images/F10020.png)'
- en: 'Figure 10-20: A *wrong*way to transform our data for cross-validation is to
    transform everything at once before the loop.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图10-20：一种*错误*的数据转换方式是在循环之前一次性转换所有数据。
- en: Now let’s proceed with the cross-validation loop. Our input data is the stack
    of five transformed folds at the far right of [Figure 10-20](#figure10-20). Let’s
    start by extracting the first fold and setting it aside; then we can train with
    the rest of the data, and validate. But we’ve done something bad here, because
    *our training data’s transformation was influenced by the validation data*. This
    is a violation of our basic principle that we create the transform using only
    the values in the training data. But here we used what is now the validation data
    when we computed our transform. We say that information has *leaked* from this
    step’s validation data into the transformation parameters, where it doesn’t belong.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续进行交叉验证循环。我们的输入数据是[图10-20](#figure10-20)右侧的五个转换后的折叠数据堆叠。首先，我们提取第一个折叠并将其放置一旁；然后我们可以使用剩余的数据进行训练和验证。但我们在这里犯了一个错误，因为*我们的训练数据转换受到了验证数据的影响*。这违反了我们的基本原则，即仅使用训练数据中的值来创建转换。然而，我们在计算转换时使用了现在作为验证数据的部分数据。我们称之为信息*泄露*，因为验证数据的内容进入了不该存在的转换参数中。
- en: The right way to build the transformation for the training data is to remove
    the validation data from the samples, thenbuild the transformation from the remaining
    data, and then apply that transformation to both the training and validation data.
    [Figure 10-21](#figure10-21) shows this visually.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 正确的构建训练数据转换的方法是，从样本中移除验证数据，然后从剩余数据中构建转换，再将该转换应用于训练数据和验证数据。[图10-21](#figure10-21)直观地展示了这一过程。
- en: '![f10021](Images/f10021.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![f10021](Images/f10021.png)'
- en: 'Figure 10-21: The proper way to transform our data for cross-validation is
    to first remove the fold samples and then compute the transformation from the
    data that remains.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图10-21：交叉验证时正确的数据转换方式是，首先移除折叠样本，然后从剩余数据中计算转换。
- en: Now we can apply that transformation to both the training set and the validation
    data. Note that here the validation data ends up way outside the range [0,1],
    which is no problem, because that data really is more extreme than the training
    set.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将该转换应用于训练集和验证数据。请注意，这里验证数据最终超出了[0,1]范围，这没有问题，因为这些数据确实比训练集更为极端。
- en: To fix our cross-validation process, we need to use this scheme in the loop
    and compute a new transformation for every training set. [Figure 10-22](#figure10-22)
    shows the right way to proceed.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 为了修正我们的交叉验证过程，我们需要在循环中使用这一方案，并为每个训练集计算新的转换。[图10-22](#figure10-22)展示了正确的处理方法。
- en: '![F10022](Images/F10022.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![F10022](Images/F10022.png)'
- en: 'Figure 10-22: The proper way to do cross-validation'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图10-22：交叉验证的正确方法
- en: For each fold we want to use as a validation set, we analyze the starting samples
    with that fold removed and then apply the resulting transform to both the training
    set and the validation set in the fold. The different colors show that each time
    through the loop, we build and apply a different transformation.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们想要作为验证集的每一个折叠，我们会分析移除该折叠的起始样本，然后将得到的变换应用到训练集和验证集。不同的颜色表明每次循环时，我们构建并应用的是不同的变换。
- en: We’ve discussed information leakage in the context of cross-validation because
    it’s a great example of this tricky topic. Happily, the cross-validation routines
    in modern libraries all do the right thing, so we don’t have to worry about this
    problem ourselves when we use library routines. But this doesn’t take the responsibility
    off of us when we write our own code. Information leakage is often subtle, and
    it can creep into our programs in unexpected ways. It’s important that we always
    think carefully about possible sources of information leakage when we build and
    apply transformations.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在交叉验证的背景下讨论了信息泄漏，因为它是这个棘手话题的一个很好的例子。幸运的是，现代库中的交叉验证程序都能正确处理，所以我们在使用库的例程时不必担心这个问题。但这并不意味着我们在编写自己代码时就可以不管信息泄漏。信息泄漏往往很微妙，它可能以意想不到的方式潜入我们的程序。我们在构建和应用变换时，始终要仔细考虑可能的信息泄漏源，这一点非常重要。
- en: Shrinking the Dataset
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缩小数据集
- en: We’ve been looking at ways to adjust the numbers in our data, and how to select
    the numbers that go into each transformation. Now let’s look at a different kind
    of transformation, designed not just to manipulate the data, but to actually compress
    it. We will literally create a new dataset that’s smaller than the original training
    set, typically by removing or combining features in each sample.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们一直在研究如何调整数据中的数字，以及如何选择进入每个变换的数字。现在，让我们来看一种不同的变换方式，这种方式不仅仅是操作数据，而是实际压缩数据。我们将字面上创建一个比原始训练集更小的新数据集，通常是通过删除或合并每个样本中的特征。
- en: 'This has two advantages: improved speed and accuracy when training. It stands
    to reason that the less data we need to process during training, the faster our
    training goes. By going faster, we mean we can pack in more learning in a given
    amount of time, resulting in a more accurate system.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这有两个好处：提高训练时的速度和准确性。可以推理出，训练时我们处理的数据越少，训练的速度就越快。提到速度，我们是指在给定的时间内，我们能够完成更多的学习，从而使得系统更为准确。
- en: Let’s look at a few ways to shrink our dataset.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看几种缩小数据集的方法。
- en: Feature Selection
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征选择
- en: If we’ve collected features in our data that are redundant, irrelevant, or otherwise
    not helpful, then we should eliminate them so that we don’t waste time on them.
    This process is called *feature selection*, or sometimes, *feature filtering*.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在数据中收集了冗余、不相关或其他无用的特征，那么我们应该将它们剔除，以免浪费时间。这一过程叫做*特征选择*，有时也叫做*特征筛选*。
- en: Let’s consider an example where some data is actually superfluous. Suppose we’re
    hand-labeling images of elephants by entering their size, species, and other characteristics
    into a database. For some reason nobody can quite remember, we also have a field
    for the number of heads. Elephants have only one head, so that field’s going to
    be nothing but 1’s. So that data is not just useless, it also slows us down. We
    ought to remove that field from our data.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个例子，其中一些数据实际上是多余的。假设我们正在手动标注大象的图像，输入它们的大小、物种以及其他特征到数据库中。不知为何，大家记不清楚，我们还设有一个字段用于记录头的数量。大象只有一个头，所以这个字段的值将始终是1。因此，这个数据不仅没用，还会拖慢我们的速度。我们应该从数据中删除这个字段。
- en: We can generalize this idea to removing features that are *almost* useless,
    that contribute very little, or that simply make the least contribution to getting
    the right answer. Let’s continue with our collection of elephant images. We have
    values for each animal’s height, weight, last known latitude and longitude, trunk
    length, ear size, and so on. But for this (imaginary) species, the trunk length
    and ear size may be closely correlated. If so, we can remove (or *filter out*)
    either one and still get the benefit of the information they each represent.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这个想法推广到去除那些*几乎*无用的特征，这些特征贡献很小，或者根本没有对得到正确答案做出太大贡献。继续以我们收集的大象图像为例。我们为每只大象记录了身高、体重、最后已知的纬度和经度、象鼻长度、耳朵大小等等。但对于这个（假设的）物种，象鼻长度和耳朵大小可能是高度相关的。如果是这样，我们可以删除（或*筛选掉*）其中一个特征，依然可以获得它们各自代表的信息的好处。
- en: Many libraries offer tools that can estimate the impact of removing each field
    from a database. We can then use this information to guide us in simplifying our
    database and speeding our learning without sacrificing more accuracy than we’re
    willing to give up. Because removing a feature is a transformation, any features
    we remove from our training set must be removed from all future data as well.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 许多库提供了可以估算从数据库中删除每个字段的影响的工具。我们可以利用这些信息来指导我们简化数据库，加速学习，同时不牺牲我们愿意放弃的准确性。因为删除特征是一个转换过程，因此从训练集中删除的任何特征，也必须从未来的所有数据中删除。
- en: Dimensionality Reduction
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 维度减少
- en: Another approach to reducing the size of our dataset is combining features,
    so one feature can do the work of two or more. This is called *dimensionality
    reduction*, where *dimensionality* refers to the number of features.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 降低数据集大小的另一种方法是组合特征，使一个特征能够完成两个或更多特征的工作。这被称为*维度减少*，其中*维度*指的是特征的数量。
- en: The intuition here is that some of the features in our data might be closely
    related without being entirely redundant. If the relationship is strong, we might
    be able to combine those two features into just one new one. An everyday example
    of this is the *body mass index (BMI)*. This is a single number that combines
    a person’s height and weight. Some measurements of a person’s health can be computed
    with just the BMI. For example, charts that help people decide if they need to
    lose weight can be conveniently indexed by age and BMI (CDC 2017).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的直觉是，我们的数据中某些特征可能密切相关，但并不完全冗余。如果关系很强，我们可能能够将这两个特征合并成一个新的特征。一个日常的例子是*身体质量指数（BMI）*。它是一个结合了身高和体重的单一数值。一些健康测量可以仅通过BMI来计算。例如，帮助人们决定是否需要减肥的图表，可以通过年龄和BMI便捷地索引（CDC
    2017）。
- en: Let’s look at a tool that automatically determines how to select and combine
    features to make the smallest impact on our results.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个工具，它可以自动确定如何选择和组合特征，以对我们的结果影响最小。
- en: Principal Component Analysis
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主成分分析
- en: '*Principal component analysis (PCA)*, is a mathematical technique for reducing
    the dimensionality of data. Let’s get a visual feel for PCA by looking at what
    it does to our guitar data.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '*主成分分析（PCA）*是一种用于减少数据维度的数学技术。让我们通过观察它对吉他数据的处理，直观地了解PCA。'
- en: '[Figure 10-23](#figure10-23) shows our starting guitar data again. As before,
    the colors of the dots are just to make it easier to track them in the following
    figures as the data is manipulated, and don’t have any other meaning.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-23](#figure10-23)再次展示了我们起始的吉他数据。如前所述，点的颜色只是为了在后续图形中跟踪它们，数据操作时不会有其他意义。'
- en: '![F10023](Images/F10023.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![F10023](Images/F10023.png)'
- en: 'Figure 10-23: The starting data for our discussion of PCA'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10-23：我们讨论PCA的起始数据
- en: Our goal is to crunch this two-dimensional data down to one-dimensional data.
    That is, we combine each set of paired x and y values to create a single new number
    based on both of them, just as BMI is a single number that combines a person’s
    height and weight.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是将这二维数据压缩成一维数据。也就是说，我们将每一组成对的x和y值合并成一个新的数值，正如BMI是一个结合了身高和体重的单一数值。
- en: Let’s start by standardizing the data. [Figure 10-24](#figure10-24) shows this
    combination of setting each dimension to have a mean of 0 and a standard deviation
    of 1, as we saw before.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从标准化数据开始。[图 10-24](#figure10-24)展示了这个过程，设定每个维度的均值为0，标准差为1，正如我们之前看到的那样。
- en: We already know that we’re going to try to reduce this 2D data to just 1D. To
    get a feel for the idea before we actually apply it, let’s go through the process
    with one key step missing, and then we’ll put that step back in.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道我们要将这二维数据减少到一维。在实际应用之前，先让我们通过一个关键步骤缺失的过程来感受这个概念，然后再把这个步骤加回去。
- en: To get started, let’s draw a horizontal line on the X axis. We’ll call this
    the *projection line*. Then we’ll *project*, or move, each data point to its closest
    spot on the projection line. Because our line is horizontal, we only need to move
    our points up or down to find their closest point on the projection line.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，我们在X轴上画一条水平线。我们将这条线称为*投影线*。然后我们将*投影*或移动每个数据点到它在投影线上的最近位置。由于我们的线是水平的，我们只需上下移动数据点，找到它们在投影线上的最近点。
- en: '![F10024](Images/F10024.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![F10024](Images/F10024.png)'
- en: 'Figure 10-24: Our input data after standardizing'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10-24：标准化后的输入数据
- en: The results of projecting the data in [Figure 10-24](#figure10-24) onto a horizontal
    projection line are shown in [Figure 10-25](#figure10-25).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 将[图 10-24](#figure10-24)中的数据投影到水平投影线上的结果如[图 10-25](#figure10-25)所示。
- en: '![F10025](Images/F10025.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![F10025](Images/F10025.png)'
- en: 'Figure 10-25: We project each data point of the guitar by moving it to its
    nearest point on the projection line. For clarity, we’re showing the path taken
    by only about 25 percent of the points.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10-25：我们通过将吉他数据集的每个数据点移动到其最近的投影直线点上来进行投影。为了清晰起见，我们只展示了大约 25% 的点的移动路径。
- en: The results after all the points are processed is shown in [Figure 10-26](#figure10-26).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 所有点处理后的结果如[图 10-26](#figure10-26)所示。
- en: '![F10026](Images/F10026.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![F10026](Images/F10026.png)'
- en: 'Figure 10-26: The result of [Figure 10-25](#figure10-25) after all the points
    have been moved to the projection line. Each point now is described only by its
    x coordinate, resulting in a one-dimensional dataset.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10-26：在所有点移动到投影直线后的[图 10-25](#figure10-25)结果。每个点现在仅通过其 x 坐标进行描述，形成了一维数据集。
- en: This is the one-dimensional dataset we were after, because the points only differ
    by their x value (the y value is always 0, so it’s irrelevant). But this would
    be a lousy way to combine our features, because all we did was throw away the
    y values. It’s like computing BMI by simply using the weight and ignoring the
    height.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们所追求的一维数据集，因为这些点仅通过其 x 值不同（y 值始终为 0，因此无关紧要）。但这并不是一个好的特征组合方法，因为我们所做的只是丢弃了
    y 值。这就像通过仅使用体重而忽略身高来计算 BMI。
- en: To improve the situation, let’s include the step we skipped. Instead of using
    a horizontal projection line, we rotate the line around until it’s passing through
    the direction of maximum variance. Think of this as the line that, after projection,
    has the largest range of points. Any library routine that implements PCA finds
    this line for us automatically. [Figure 10-27](#figure10-27) shows this line for
    our guitar data.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 为了改进这种情况，我们可以加入我们之前跳过的步骤。我们不再使用水平投影线，而是旋转该线，直到它通过最大方差的方向。可以将其视为在投影后具有最大点范围的那条线。任何实现
    PCA 的库函数都会自动为我们找到这条线。[图 10-27](#figure10-27)展示了我们的吉他数据的这一线。
- en: '![F10027](Images/F10027.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![F10027](Images/F10027.png)'
- en: 'Figure 10-27: The thick black line is the line of maximum variance through
    our original data. This is our projection line.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10-27：粗黑线是通过原始数据的最大方差线。这就是我们的投影线。
- en: Now we continue just like before. We project each point onto this projection
    line by moving it to its closest point on the line. As before, we do this by moving
    perpendicular to the line until we intersect it. [Figure 10-28](#figure10-28)
    shows this process.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们继续像之前一样操作。我们将每个点投影到这条投影线上，方法是将其移动到与线最近的点。像之前一样，我们通过垂直于这条线的方向移动，直到与它相交。[图
    10-28](#figure10-28)展示了这一过程。
- en: '![F10028](Images/F10028.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![F10028](Images/F10028.png)'
- en: 'Figure 10-28: We project the guitar data onto the projection line by moving
    each point to its closest point on the line. For clarity, we’re showing the path
    taken by only about 25 percent of the points.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10-28：我们通过将吉他数据投影到投影线上的每个点，移动每个点到线上的最近点来进行投影。为了清晰起见，我们只展示了大约 25% 的点的移动路径。
- en: The projected points are shown in [Figure 10-29](#figure10-29). Note that they
    all lie on the line of maximum variance that we found in [Figure 10-27](#figure10-27).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 投影后的点如[图 10-29](#figure10-29)所示。注意，它们都位于我们在[图 10-27](#figure10-27)中找到的最大方差直线上。
- en: '![f10029](Images/f10029.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![f10029](Images/f10029.png)'
- en: 'Figure 10-29: The points of the guitar dataset projected onto the line of maximum
    variance'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10-29：吉他数据集的各点投影到最大方差直线上
- en: For convenience, we can rotate this line of points to lie on the X axis, as
    shown in [Figure 10-30](#figure10-30). Now the y coordinate is irrelevant again,
    and we have 1D data that includes information about each point from both its original
    x and y values.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便，我们可以将这条点线旋转到与 X 轴平行的位置，如[图 10-30](#figure10-30)所示。现在，y 坐标不再重要，我们得到了包含每个点的原始
    x 和 y 值信息的一维数据。
- en: '![F10030](Images/F10030.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![F10030](Images/F10030.png)'
- en: 'Figure 10-30: Rotating the points of [Figure 10-29](#figure10-29) into a horizontal
    position'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10-30：将[图 10-29](#figure10-29)中的点旋转到水平位置
- en: Although the straight line of points in [Figure 10-30](#figure10-30) looks roughly
    like the line of points in [Figure 10-26](#figure10-26), they’re different, because
    the points are distributed differently along the X axis. In other words, they
    have different values, because they were computed by projecting onto a tilted
    line, rather than a horizontal one. [Figure 10-31](#figure10-31) shows the two
    projections together. The PCA result is not just longer, but the points are also
    distributed differently.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管[图10-30](#figure10-30)中的直线与[图10-26](#figure10-26)中的点大致相似，但它们是不同的，因为这些点在X轴上的分布不同。换句话说，它们的数值不同，因为它们是通过投影到一条倾斜的直线，而不是水平直线计算出来的。[图10-31](#figure10-31)展示了这两种投影的对比。PCA的结果不仅更长，而且点的分布也有所不同。
- en: '![F10031](Images/F10031.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![F10031](Images/F10031.png)'
- en: 'Figure 10-31: Comparing the points created by projection to y = 0 in [Figure
    10-26](#figure10-26) (top) and the PCA algorithm in [Figure 10-30](#figure10-30)
    (bottom)'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图10-31：比较通过投影到y = 0的[图10-26](#figure10-26)（顶部）和PCA算法的[图10-30](#figure10-30)（底部）所创建的点
- en: All of the steps we just discussed are carried out automatically by a machine
    learning library when we call its PCA routine.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚才讨论的所有步骤，在调用机器学习库的PCA函数时都会自动执行。
- en: The beauty of the 1D data created by this projection step is that every point’s
    single value (its x value) is a combination of the 2D data it started with. We
    reduced the dimensionality of our dataset by one dimension, but we did so while
    retaining as much information as we could. Our learning algorithms now only have
    to process one feature rather than two, so they’ll run faster. Of course, we’ve
    thrown some information away, so the accuracy may suffer. The trick to using PCA
    effectively is to choose dimensions that can be combined while still staying within
    the performance goals of our project.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个投影步骤创建的一维数据的美妙之处在于，每个点的单一值（其x值）是它最初的二维数据的组合。我们将数据集的维度减少了一个维度，但我们尽可能保留了最多的信息。现在，我们的学习算法只需要处理一个特征而不是两个，因此它们的运行速度会更快。当然，我们丢弃了一些信息，因此准确性可能会受到影响。有效使用PCA的关键是选择可以组合的维度，同时仍保持项目的性能目标。
- en: If we have 3D data, we can imagine placing a plane in the middle of the cloud
    of samples and projecting our data down onto the plane. The library’s job is to
    find the best orientation of that plane. This takes our data from 3D to 2D. If
    we want to go all the way down to 1D, we can imagine projecting the data onto
    a line through the volume of points. In practice, we can use this technique in
    problems with any number of dimensions, where we may reduce the dimensionality
    of the data by tens or more.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有三维数据，可以想象将一个平面放置在样本云的中央，并将数据投影到该平面上。库的任务是找到该平面的最佳朝向。这样，我们的数据就从三维变成了二维。如果我们想进一步降维到一维，可以想象将数据投影到通过点云的直线上。在实际应用中，我们可以在任何维度数的问题中使用这种技术，并将数据的维度减少十个或更多。
- en: 'The critical questions for this kind of algorithm include: How many dimensions
    should we try to compress? Which dimensions should be combined? How they should
    get combined? We usually use the letter *k* to stand for the number of dimensions
    remaining in our data after PCA has done its work. So, in our guitar example,
    *k* was 1\. We can call *k* a parameter of the algorithm, though usually we call
    it a hyperparameterof the entire learning system. As we’ve seen, the letter *k*
    is used for lots of different algorithms in machine learning, which is unfortunate;
    it’s important to pay attention to the context when we see references to *k*.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这种算法的关键问题包括：我们应该压缩多少维度？应该组合哪些维度？它们应如何组合？我们通常使用字母*k*来表示PCA处理后剩余的维度数。因此，在我们的吉他示例中，*k*为1。我们可以把*k*称为算法的一个参数，尽管通常我们将其称为整个学习系统的超参数。正如我们所看到的，字母*k*在机器学习中的许多不同算法中都有使用，这是不幸的；因此，在看到*k*时，我们必须注意上下文。
- en: Compressing too little means our training and evaluation steps are going to
    be inefficient, but compressing too much means we risk eliminating important information
    that we should have kept. To pick the best value for the hyperparameter *k*, we
    usually try out a few different values to see how they do and then pick the one
    that seems to work best. We can automate this search using the techniques of *hyperparameter
    searching* provided by many libraries.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩得太少意味着我们的训练和评估步骤将低效，但压缩得太多则意味着我们有可能丢失应当保留的重要信息。为了选择超参数*k*的最佳值，我们通常会尝试几个不同的值，看它们的效果如何，然后选择效果最好的一项。我们可以利用许多库提供的*超参数搜索*技术来自动化这一过程。
- en: As always, whatever PCA transformations we use to compress our training data
    must also be used in the same way for all future data.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往，无论我们使用什么PCA变换来压缩训练数据，都必须以相同的方式应用于所有未来的数据。
- en: PCA for Simple Images
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 简单图像的PCA
- en: Images are an important and special kind of data. Let’s apply PCA to a simple
    set of images.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图像是重要且特殊的数据类型。让我们对一组简单的图像应用PCA。
- en: '[Figure 10-32](#figure10-32) shows a set of six images, perhaps drawn from
    a huge dataset of tens of thousands of such pictures. If these grayscale images
    are 1,000 pixels on a side, each contains 1,000 × 1,000, or 1 million, pixels.
    Is there any better way to represent them than with a million numbers each?'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-32](#figure10-32)显示了一组六张图像，可能来自一个包含数万个此类图像的大型数据集。如果这些灰度图像的每一张边长为1,000像素，每张图像包含1,000
    × 1,000，或100万个像素。是否有比使用每张图像100万个数字的表示方法更好的方式？'
- en: '![F10032](Images/F10032.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![F10032](Images/F10032.png)'
- en: 'Figure 10-32: Six images we’d like to represent'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10-32：我们希望表示的六张图像
- en: Let’s start by observing that each image in [Figure 10-32](#figure10-32) can
    be re-created from the three images in [Figure 10-33](#figure10-33), each scaled
    by a different amount and then added together.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从观察[图 10-32](#figure10-32)中的每张图像可以通过按不同的比例缩放[图 10-33](#figure10-33)中的三张图像并将它们相加来重新创建开始。
- en: '![F10033](Images/F10033.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![F10033](Images/F10033.png)'
- en: 'Figure 10-33: We can create all six images in [Figure 10-32](#figure10-32)
    by scaling these three by different amounts and adding the results.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10-33：我们可以通过以不同的比例缩放这三张图像并将结果相加，来创建[图 10-32](#figure10-32)中的所有六张图像。
- en: For example, we can reconstruct the first image in [Figure 10-32](#figure10-32)
    by adding 20 percent of the circle, 70 percent of the vertical box, and 40 percent
    of the horizontal box. We often call these scaling factors the *weights*. The
    weights for each of the six starting images are shown in [Figure 10-34](#figure10-34).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以通过将圆形的20%、竖直框的70%和水平框的40%相加来重建[图 10-32](#figure10-32)中的第一张图像。我们通常将这些缩放因子称为*权重*。[图
    10-34](#figure10-34)显示了六张起始图像的权重。
- en: '![F10034](Images/F10034.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![F10034](Images/F10034.png)'
- en: 'Figure 10-34: The weights to use when scaling the three images in [Figure 10-33](#figure10-33)
    to recover the images in [Figure 10-32](#figure10-32)'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10-34：当按比例缩放[图 10-33](#figure10-33)中的三张图像以恢复[图 10-32](#figure10-32)中的图像时，使用的权重。
- en: '[Figure 10-35](#figure10-35) shows the process of scaling and combining the
    components to recover the first original image.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-35](#figure10-35)展示了通过缩放和组合这些成分来恢复第一张原始图像的过程。'
- en: In general, we can represent any image of this type with the three weights of
    the component images that make the best match. To reconstruct any of the input
    images, we need the three simpler pictures (1 million values each) plus the three
    numbers for that specific image. If we had 1,000 images, storing each one would
    take a total of 1,000 megabytes. But using this compressed form, we need a total
    of only 3.001 megabytes.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，我们可以用三种成分图像的权重来表示任何此类图像，这些权重能够产生最佳匹配。为了重建任何输入图像，我们需要这三张简单图像（每张100万个值）以及该特定图像的三个数字。如果我们有1,000张图像，存储每张图像将需要总计1,000兆字节。但使用这种压缩形式，我们只需要总共3.001兆字节。
- en: '![F10035](Images/F10035.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![F10035](Images/F10035.png)'
- en: 'Figure 10-35: Recovering the first image in [Figure 10-32](#figure10-32) by
    scaling the images in [Figure 10-33](#figure10-33)'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10-35：通过缩放[图 10-33](#figure10-33)中的图像来恢复[图 10-32](#figure10-32)中的第一张图像。
- en: For these simple images, it’s easy to find the components as three geometric
    shapes. But when we have more realistic pictures, this won’t generally be possible.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些简单的图像，找到三个几何形状作为成分是很容易的。但当我们处理更现实的图像时，这通常是不可能的。
- en: The good news is that we can use the projection technique we discussed earlier.
    Instead of projecting a collection of points to create a new set of points on
    a line, we can project a collection of images to create a new image. This is a
    more abstract process than the one we followed with the guitar points, but the
    concept is the same. Let’s get a feeling for how PCA handles images by skipping
    the mechanics and focusing on the results.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，我们可以使用前面讨论的投影技术。我们可以通过投影一组图像来生成一张新图像，而不是像之前那样将一组点投影到一条线上，生成一个新的点集。这是一个比我们在吉他点上操作时更抽象的过程，但概念是相同的。让我们通过跳过细节，专注于结果，来感受一下PCA如何处理图像。
- en: Consider again the six starting images of [Figure 10-32](#figure10-32). Remember
    that these are grayscale images, not vector drawings. Let’s ask PCA to find a
    grayscale image that comes closest to representing all the images, in the same
    way that our diagonal line came closest to representing all the points in our
    guitar. Then we can represent each starting image as a sum of this image, scaled
    by an appropriate amount, plus whatever is left over. [Figure 10-36](#figure10-36)
    shows this process.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 再次考虑[图10-32](#figure10-32)中的六个起始图像。请记住，这些是灰度图像，而非矢量图。现在让我们要求PCA找到一张灰度图像，它最接近代表所有图像的方式，就像我们的对角线最接近代表吉他中的所有点一样。然后，我们可以将每个起始图像表示为该图像的一个加权和，适当缩放后再加上剩余部分。[图10-36](#figure10-36)展示了这一过程。
- en: '![F10036](Images/F10036.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![F10036](Images/F10036.png)'
- en: 'Figure 10-36: Running PCA on the images from [Figure 10-32](#figure10-32)'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图10-36：对[图10-32](#figure10-32)中的图像运行PCA
- en: The starting images from 10-32 are shown at the top of [Figure 10-36](#figure10-36).
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 来自10-32的起始图像显示在[图10-36](#figure10-36)顶部。
- en: Now we ask PCA to find an image that corresponds to the line in [Figure 10-28](#figure10-28).
    That is, an image that, in some sense, captures something from all of the inputs.
    Let’s say it found the picture at the left in [Figure 10-36](#figure10-36), showing
    the two bars and the circle all in black and overlaid. We’ll call this the *shared
    image*, which isn’t a formal term but is useful here.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们要求PCA找到与[图10-28](#figure10-28)中的那条线相对应的图像。也就是说，找到一张从某种意义上讲，能够捕捉所有输入内容的图像。假设它找到了[图10-36](#figure10-36)左侧的图片，显示的是两条条形图和一个圆圈，全部是黑色并叠加在一起。我们将其称为*共享图像*，这不是一个正式术语，但在这里很有用。
- en: Let’s now represent each image at the top of [Figure 10-36](#figure10-36) as
    a combination of a scaled version of the shared image, and some other image. To
    do this, we find the lightest pixel in each input image, and scale the shared
    image to that intensity. That scaling factor is shown at the top of the copies
    of the common image in the middle row, where the copies have been scaled in intensity
    by that amount. If we subtract each scaled common image from the source image
    above it in the figure, we get their difference. We could write this as “source
    – common = difference,” or equivalently, “source = common + difference,” which
    is shown in the figure.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将[图10-36](#figure10-36)顶部的每个图像表示为共享图像的缩放版与其他图像的组合。为此，我们找到每个输入图像中最亮的像素，并将共享图像缩放到该亮度。这个缩放因子显示在中间行的公共图像副本顶部，在这些副本中，公共图像的强度被按该因子缩放。如果我们将每个缩放过的公共图像从上方的源图像中减去，我们就得到了它们的差异。我们可以将其写为“源图像
    - 公共图像 = 差异”，或者等价地，“源图像 = 公共图像 + 差异”，如图所示。
- en: We can then run PCA again, this time on the bottom row of [Figure 10-36](#figure10-36).
    Again, it projects these six images to create a new image that is the best match
    for all of them. As before, we can represent each picture as the sum of a scaled
    version of what’s common, plus whatever is left. [Figure 10-37](#figure10-37)
    shows the idea. In this demonstration, we’re supposing that PCA created an image
    of the two overlapping boxes as the best matching image.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以再次运行PCA，这次对[图10-36](#figure10-36)的底部一行进行处理。同样，它将这些六个图像进行投影，生成一个新的图像，该图像最能匹配所有图像。和之前一样，我们可以将每张图片表示为共享部分的一个缩放版加上剩余部分。[图10-37](#figure10-37)展示了这一思想。在这个示例中，我们假设PCA生成了一个包含两个重叠矩形的图像作为最佳匹配图像。
- en: '![F10037](Images/F10037.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![F10037](Images/F10037.png)'
- en: 'Figure 10-37: Running PCA on the bottom row of [Figure 10-36](#figure10-36)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图10-37：对[图10-36](#figure10-36)底部一行运行PCA
- en: Something interesting happened in two images on the bottom row. Let’s look at
    the second column from the left. The image we want to match on the top row is
    a circle and horizontal box, but we’re trying to match it with a scaled pair of
    crossed boxes. To match the top image, we need to add in some of the circle, but
    subtract the vertical box that we just introduced. That just means setting the
    corresponding data in the bottom image to negative values. These are perfectly
    valid numbers to place into our data, though we have to be careful if we try to
    display this image directly. If we reconstruct the top image by adding up the
    two images beneath it, the negative values in the red region in the bottom row
    cancel out the positive values in the vertical box in the middle row, so the sum
    of these images matches the circle and horizontal box at the top. The same reasoning
    applies to the horizontal box in the rightmost column.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在底部行的两张图像中发生了一些有趣的事情。我们来看一下从左数的第二列。我们想要匹配的顶部图像是一个圆形和一个水平框，但我们试图用一对缩放后的交叉框来匹配它。为了匹配顶部图像，我们需要添加一些圆形的部分，但要减去我们刚刚引入的竖直框。这意味着将底部图像中相应的数据设置为负值。这些是完全有效的数据，尽管如果我们直接显示这张图像时需要小心。如果我们通过将下面的两张图像加起来来重建顶部图像，底部行红色区域的负值将抵消中间行竖直框的正值，因此这些图像的和与顶部的圆形和水平框相匹配。右侧列中的水平框也适用相同的推理。
- en: '[Figure 10-38](#figure10-38) summarizes the two steps we’ve seen so far.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-38](#figure10-38)总结了我们到目前为止看到的两步。'
- en: We took only two steps here, but we can repeat this process dozens or hundreds
    of times.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这里只进行了两步，但我们可以重复这个过程几十次甚至上百次。
- en: To represent each starting image, we only need the collection of common images
    and the weight we assigned to each. Since the common images are shared by all
    the images, we can consider them a shared resource. Then each image can be completely
    described by a reference to this shared resource, and the list of weights to be
    applied to the common images.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 为了表示每个起始图像，我们只需要一组公共图像以及我们分配给每个图像的权重。由于这些公共图像是所有图像共享的，我们可以将它们视为共享资源。然后，每个图像都可以通过对这个共享资源的引用以及应用于公共图像的权重列表来完全描述。
- en: '![F10038](Images/F10038.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![F10038](Images/F10038.png)'
- en: 'Figure 10-38: Representing each starting image (top row) as the sum of two
    scaled component images (second and third rows), plus whatever’s left (bottom
    row)'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10-38：将每个起始图像（第一行）表示为两个缩放后的分量图像（第二行和第三行）的和，再加上剩下的部分（底部行）
- en: Each of the common images is called a *component*, and the one we create at
    every step is the *principal* component (since it’s the best of all possible components,
    in the same way that the line in [Figure 10-28](#figure10-28) was the best line).
    We find these principal components by analyzing the input images. Hence, the name
    *principal component analysis*.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 每个公共图像都称为*分量*，我们在每个步骤中创建的那个是*主分量*（因为它是所有可能分量中最优的，就像[图 10-28](#figure10-28)中的线是最优的那样）。我们通过分析输入图像来找到这些主分量。因此，这个方法被称为*主成分分析*。
- en: The more components we include, the more accurately each reconstructed image
    will match its original. We usually aim to produce enough components so that each
    reconstructed image has all the qualities we care about in its original.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们包含的分量越多，每个重建的图像与其原始图像的匹配度就越高。我们通常的目标是产生足够的分量，使得每个重建的图像都能保留其原始图像的所有关键特征。
- en: In this discussion, most of the weights were positive, but we also saw some
    component images that should be subtracted, not added, and thus they produce weights
    with negative values. This is so that the final pixels, when all the components
    are summed together, have the desired values.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在本讨论中，大多数权重是正数，但我们也看到了一些应该被减去而不是加上的分量图像，因此它们产生了负值的权重。这样，最终的像素在所有分量相加后，会得到期望的值。
- en: How well did we do with just two component images? [Figure 10-39](#figure10-39)
    shows the original six images and our reconstructed images using the sum of the
    middle two rows of [Figure 10-38](#figure10-38).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 仅使用两个分量图像我们做得如何？[图 10-39](#figure10-39)显示了原始的六个图像和我们通过将[图 10-38](#figure10-38)中间两行的和来重建的图像。
- en: The matches aren’t perfect, but they’re a good start, particularly for just
    two components. So, we have a common pool of two images (requiring 1 million numbers
    each), and then each image itself can be described with just two numbers. The
    beauty of this scheme is that our algorithms never need to see the common images.
    We just need those for finding the weights that describe each input image (and
    to reconstruct the images, if we want). As far as the learning algorithm is concerned,
    each image is described by only two numbers. This means our algorithms consume
    less memory and run more quickly.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 匹配结果并不完美，但它们是一个不错的起点，特别是在只有两个主成分的情况下。因此，我们有一个包含两张图像的公共池（每张图像需要100万个数字），然后每张图像本身只用两个数字来描述。这个方案的美妙之处在于，我们的算法不需要看到这些公共图像。我们只需要它们来找到描述每个输入图像的权重（如果需要，还可以重建图像）。就学习算法而言，每张图像仅由两个数字描述。这意味着我们的算法消耗更少的内存，运行得更快。
- en: '![F10039](Images/F10039.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![F10039](Images/F10039.png)'
- en: 'Figure 10-39: Our starting six images (top), and the reconstructed images from
    [Figure 10-35](#figure10-35)c (bottom)'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图10-39：我们的六张起始图像（上），以及从[图10-35](#figure10-35)c重建的图像（下）
- en: 'We skipped a step in this example: normally when we use PCA, we standardize
    all the images first. That’s included in our next example.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这个例子中跳过了一个步骤：通常在使用PCA时，我们首先会对所有图像进行标准化。这一点将在我们的下一个例子中介绍。
- en: PCA for Real Images
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 真实图像的PCA
- en: The images in the last section were contrived for simplicity. Now we’ll apply
    PCA to real pictures.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 上一节中的图像是为了简化而设计的。现在我们将对真实图片应用PCA。
- en: Let’s begin with the six pictures of huskies shown in [Figure 10-40](#figure10-40).
    To make our processing easier to see, these images are only 45 × 64 pixels. These
    were aligned by hand so that the eyes and nose are in about the same location
    in each image. This way each pixel in each image has a good chance of representing
    the same part of a dog as the corresponding pixel in the other images. For instance,
    a pixel just below the center is likely to part of a nose, one near the upper
    corners is likely to be an ear, and so on.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从[图10-40](#figure10-40)中展示的六张哈士奇图片开始。为了便于观察处理过程，这些图像的尺寸仅为45 × 64像素。它们是手动对齐的，以确保每张图像中的眼睛和鼻子大致位于相同的位置。这样，每张图像中的每个像素都有很大的机会代表狗的相同部位，就像其他图像中的相应像素一样。例如，位于中心下方的像素很可能属于鼻子，靠近上角的像素很可能属于耳朵，等等。
- en: '![f10040](Images/f10040.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![f10040](Images/f10040.png)'
- en: 'Figure 10-40: Our starting set of huskies'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 图10-40：我们的起始哈士奇图像集
- en: A database of six dogs isn’t a lot of training data, so let’s enlarge our database
    using the idea of *data augmentation,* a common strategy for *amplifying* or *enlarging*
    a dataset. In this case, let’s run through our set of six images in random order
    over and over. Each time through, we’ll pick an image, make a copy, randomly shift
    it horizontally and vertically up to 10 percent on each axis, rotate it up to
    five degrees clockwise or counterclockwise, and maybe flip it left to right. Then
    we append that transformed image to our training set. [Figure 10-41](#figure10-41)
    shows the results of the first two passes through our six dogs. We used this technique
    of creating variations to build a training set of 4,000 dog images.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 六只狗的数据库并不算大量的训练数据，因此让我们通过*数据增强*的方式来扩大我们的数据库，这是*放大*或*扩展*数据集的常见策略。在这种情况下，我们将反复随机遍历我们的六张图像。每次遍历时，我们会选择一张图像，制作一份副本，随机地将其在水平和垂直方向上分别平移最多10%的距离，旋转最多五度，顺时针或逆时针，并可能将其左右翻转。然后，我们将该变换后的图像添加到我们的训练集。[图10-41](#figure10-41)显示了我们第一次和第二次遍历六只狗图像后的结果。我们使用这种创建变体的技术，构建了一个包含4,000张狗图像的训练集。
- en: '![F10041](Images/F10041.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![F10041](Images/F10041.png)'
- en: 'Figure 10-41: Each row shows a set of new images created by shifting, rotating,
    and perhaps horizontally flipping each of our input images.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 图10-41：每一行展示了通过平移、旋转和可能的水平翻转所生成的新图像集。
- en: Since we’d like to run PCA on these images, our first step is to standardize
    them. This means we analyze the same pixel in each of our 4,000 images and adjust
    the collection to have zero mean and unit variance (Turk and Pentland 1991). The
    standardized versions of our first six generated dogs are shown in [Figure 10-42](#figure10-42).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们希望对这些图像进行PCA处理，我们的第一步是对它们进行标准化。这意味着我们会分析在4,000张图像中每个像素的位置，并调整这些图像的集合，使其具有零均值和单位方差（Turk和Pentland
    1991）。我们生成的六只狗的标准化版本见[图10-42](#figure10-42)。
- en: '![F10042](Images/F10042.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![F10042](Images/F10042.png)'
- en: 'Figure 10-42: Our first six huskies after standardization'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 图10-42：标准化后的六只哈士奇
- en: Since 12 images fit nicely into a figure, let’s begin by arbitrarily asking
    PCA to find the 12 images that, when added together with appropriate weights,
    best reconstruct the input images. Each of the projections (or components) found
    by PCA is technically known as an *eigenvector*, from the German *eigen* meaning
    “own” (or roughly “self”), and *vector* from the mathematical name for this kind
    of object. When we create eigenvectors of particular types of things, it’s common
    to create a playful name by combining the prefix eigen with the object we’re processing.
    Hence, [Figure 10-43](#figure10-43) shows our 12 *eigendogs*.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 由于12张图像适合放在一个图中，让我们首先随便要求PCA找出12张图像，这些图像在加上适当的权重后，能够最好地重建输入图像。PCA找到的每个投影（或成分）在技术上被称为*特征向量*，来自德语词“*eigen*”意为“自身”（或大致“自我”），以及“*vector*”是这种数学对象的名称。当我们创建特定类型的事物的特征向量时，通常会通过将前缀“eigen”与我们正在处理的对象结合，来创造一个有趣的名称。因此，[图10-43](#figure10-43)展示了我们的12个*特征狗*。
- en: '![F10043](Images/F10043.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![F10043](Images/F10043.png)'
- en: 'Figure 10-43: The 12 eigendogs produced by PCA'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图10-43：PCA生成的12个特征狗。
- en: Looking at the eigendogs tells us a lot about how PCA is analyzing our images.
    The first eigendog is a big smudge that is darker roughly where most of the dogs
    appear in the image. This is the single image that comes closest to approximating
    every input image. The second eigendog gives us refinements to the first, capturing
    some of the left-right shading differences.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 看这些特征狗告诉我们很多关于PCA如何分析我们的图像的信息。第一个特征狗是一个大模糊区域，颜色较深，恰好是大多数狗出现在图像中的地方。这是唯一一张最接近近似每个输入图像的图像。第二个特征狗在第一个特征狗的基础上进行了改进，捕捉到一些左右阴影的差异。
- en: The next eigendog provides additional detail, and so it goes through all 12
    eigendogs. So, the first eigendog captures the broadest, most common features,
    and each additional eigendog lets us recover a little more detail.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的特征狗提供了更多的细节，依此类推，通过所有12个特征狗来实现。因此，第一个特征狗捕捉到了最广泛、最常见的特征，而每增加一个特征狗，就能让我们恢复更多的细节。
- en: PCA is able not only to create the eigendogs of [Figure 10-43](#figure10-43),
    but also to take as input any picture, and tell us the weight to apply to each
    eigendog image so that, when the weighted images are added together, we get the
    best approximation to the input image.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: PCA不仅能够创建[图10-43](#figure10-43)中的特征狗，还能接收任何图片作为输入，并告诉我们应该对每个特征狗图像应用多少权重，这样当加权图像加在一起时，我们能得到最接近输入图像的最佳近似。
- en: Let’s see how well we can recover our original images by combining these 12
    eigendogs with their corresponding weights. [Figure 10-44](#figure10-44) shows
    the weights that PCA found for each input image. We create the reconstructed dogs
    by scaling each eigendog image from [Figure 10-43](#figure10-43) with its corresponding
    weight, and then adding the results together.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过将这12个特征狗与它们对应的权重结合起来，看看我们能多好地恢复原始图像。[图10-44](#figure10-44)展示了PCA为每个输入图像找到的权重。我们通过将[图10-43](#figure10-43)中的每个特征狗图像按其对应权重缩放，然后将结果加在一起，来创建重建的狗图像。
- en: '![f10044](Images/f10044.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![f10044](Images/f10044.png)'
- en: 'Figure 10-44: Reconstructing our original inputs from a set of 12 eigendogs.
    Top row: The reconstructed dogs. Bottom row: The weights applied to the eigendogs
    of [Figure 10-43](#figure10-43) to build the image directly above. Notice that
    the vertical scales on the bottom row are not all the same.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 图10-44：通过12个特征狗的集合重建我们的原始输入。顶部：重建的狗图像。底部：应用于[图10-43](#figure10-43)中特征狗的权重，以构建上面直接显示的图像。请注意，底部一行的垂直尺度并不完全相同。
- en: The recovered dogs in [Figure 10-44](#figure10-44) are not great. We’ve asked
    PCA to represent all 4,000 images in our training set with just 12 pictures. It
    did its best, but these results are pretty blurry. They do seem to be on the right
    track, though.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10-44](#figure10-44)中的恢复图像不太好。我们要求PCA用仅仅12张图片表示我们训练集中的所有4000张图像。它尽力了，但这些结果还是相当模糊。不过，它们似乎走在了正确的轨道上。'
- en: Let’s try using 100 eigendogs. The first 12 eigendog images look just like those
    in [Figure 10-43](#figure10-43), but then they get more complicated and detailed.
    The results of reconstructing our first set of 6 dogs are shown in [Figure 10-45](#figure10-45).
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用100个特征狗。前12个特征狗图像看起来和[图10-43](#figure10-43)中的一样，但接下来它们变得更复杂和细致。我们用100个特征狗重建我们前6个狗的结果如[图10-45](#figure10-45)所示。
- en: '![F10045](Images/F10045.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![F10045](Images/F10045.png)'
- en: 'Figure 10-45: Reconstructing our original inputs from a set of 100 eigendogs'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 图10-45：通过100个特征狗的集合重建我们的原始输入。
- en: That’s better! They’re starting to look like dogs. But it seems that 100 eigendogs
    is still not enough.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 好一点了！它们开始像狗了。但似乎100个特征狗还是不够。
- en: Let’s crank up our number of eigendogs to 500 and try again. [Figure 10-46](#figure10-46)
    shows the results.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将特征狗的数量增加到500，再试一次。[图 10-46](#figure10-46)展示了结果。
- en: '![F10046](Images/F10046.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![F10046](Images/F10046.png)'
- en: 'Figure 10-46: Reconstructing our original inputs from a set of 500 eigendogs'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10-46：从一组500个特征狗重建我们的原始输入
- en: These are looking pretty great. They are all easily recognized as the 6 standardized
    dogs in [Figure 10-42](#figure10-42). They’re not perfect, but considering that
    we’re adding together different amounts of 500 shared images, we’ve done a fine
    job of matching the original images. There’s nothing special about these first
    6 images. If we look at any of the 4,000 images in our database, they all look
    this good. We could keep increasing the number of eigendogs, and the results would
    continue to improve, with the images getting increasingly sharper and less noisy.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 这些看起来非常不错。它们都能轻松识别为[图 10-42](#figure10-42)中的6种标准化狗。它们并不完美，但考虑到我们正在将500张共享图像的不同数量加在一起，我们在匹配原始图像方面做得很不错。这些前6张图像没有什么特别的地方。如果我们查看数据库中任何一张4,000张图像，它们看起来都一样好。我们可以继续增加特征狗的数量，结果会继续改进，图像会变得更加清晰，噪点也会减少。
- en: In each plot of the weights, the eigendog images that get the most weight are
    the ones at the start, which capture the big structures. As we work our way down
    the list, each new eigendog is generally weighted a little less than the one before,
    so it contributes less to the overall result.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个权重图中，获得最多权重的特征狗图像通常是开始的那些，它们捕捉了大的结构。随着我们向下滚动列表，每个新的特征狗通常被赋予的权重比前一个稍微少一些，因此它对最终结果的贡献较小。
- en: The value of PCA here is not that we can make images that look just like the
    starting set, but rather, that we can use the eigendogs’ representation to reduce
    the amount of data our deep learning system has to process. This is illustrated
    in [Figure 10-47](#figure10-47). Our set of input dogs goes into PCA, which generates
    a set of eigendogs. Then each dog we’d like to classify goes into PCA again, which
    gives us the weights for that image. Those are the values that go into the classifier.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: PCA的价值并不在于我们能生成与初始图像集完全相同的图像，而在于我们可以利用特征狗的表示来减少深度学习系统需要处理的数据量。这一点在[图 10-47](#figure10-47)中得到了说明。我们的输入狗集进入PCA，PCA生成一组特征狗。然后，我们想要分类的每只狗再次进入PCA，PCA给出该图像的权重。这些权重值最终进入分类器。
- en: As we mentioned earlier, rather than training our categorizer on all of the
    pixels from each image, we can train it on just that image’s 100 or 500 weights.
    The categorizer never sees a full image of a million pixels. It never even sees
    the eigendogs. It just gets a list of the weights for each image, and that’s the
    data it uses for analysis and prediction during training. When we want to classify
    a new image, we provide just its weights, and the computer gives us back a class.
    This can save a lot of computation, which translates to a savings in time, and
    perhaps increased quality of final results.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前提到的，我们可以选择仅使用每张图像的100个或500个权重来训练我们的分类器，而不是使用每张图像的所有像素。分类器从未看到过包含百万个像素的完整图像。它甚至从未见过特征狗。它只得到每张图像的权重列表，这些权重数据是它在训练期间用于分析和预测的。当我们想要分类一张新图像时，我们只提供它的权重，计算机会返回一个类别。这可以节省大量的计算，这意味着节省了时间，并可能提高最终结果的质量。
- en: '![F10047](Images/F10047.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![F10047](Images/F10047.png)'
- en: 'Figure 10-47: In the top row we first we use PCA to build a set of eigendogs,
    and then in the bottom row we find the weights for each input to our classifier,
    which only uses those weights to find the input’s class.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10-47：在第一行中，我们首先使用PCA构建一组特征狗，然后在第二行中，我们为分类器的每个输入找到权重，分类器仅使用这些权重来找到输入的类别。
- en: To summarize, the data we hand the classifier is not each input image, but its
    weights. The classifier then proceeds to work out which breed of dog it’s looking
    at based just on those weights. Often we need only a few hundred weights to represent
    input samples with many thousands, or even millions, of features.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们提供给分类器的数据不是每一张输入图像，而是它的权重。然后，分类器根据这些权重来推断它所看到的是什么品种的狗。通常，我们只需要几百个权重，就能表示拥有成千上万、甚至数百万个特征的输入样本。
- en: Summary
  id: totrans-293
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we looked at ways to prepare data. We saw that it’s important
    to inspect our data before we do anything with it and make sure that it’s clean.
    Once our data is clean, we can transform it to better fit our learning algorithms
    in a number of ways. These transformations are built from the training data only.
    It’s important to remember that any transforms we apply to the training data must
    then be applied to every additional sample we give to our algorithm, from validation
    and test data to deployment data provided by real-world users.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了数据准备的方法。我们看到，在对数据进行任何处理之前，检查数据并确保其清洁是非常重要的。一旦数据干净，我们可以通过多种方式对其进行转换，以更好地适应我们的学习算法。这些转换仅基于训练数据构建。需要记住的是，我们对训练数据应用的任何转换，必须同样应用于每个额外的样本，无论是验证数据、测试数据，还是来自真实用户的部署数据。
- en: In the next chapter, we’ll dig into classifiers and survey some of the most
    important algorithms for the job.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将深入探讨分类器，并调查一些最重要的算法。
