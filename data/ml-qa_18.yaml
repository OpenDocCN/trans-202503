- en: '**15'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**15**'
- en: DATA AUGMENTATION FOR TEXT**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**文本数据增强**'
- en: '![Image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/common.jpg)'
- en: How is data augmentation useful, and what are the most common augmentation techniques
    for text data?
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强如何有用？文本数据最常见的增强技术有哪些？
- en: Data augmentation is useful for artificially increasing dataset sizes to improve
    model performance, such as by reducing the degree of overfitting, as discussed
    in [Chapter 5](ch05.xhtml). This includes techniques often used in computer vision
    models, like rotation, scaling, and flipping.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强有助于通过人工增加数据集的大小来提升模型性能，例如减少过拟合的程度，正如[第5章](ch05.xhtml)所讨论的那样。这些技术通常也应用于计算机视觉模型，比如旋转、缩放和翻转。
- en: Similarly, there are several techniques for augmenting text data. The most common
    include synonym replacement, word deletion, word position swapping, sentence shuffling,
    noise injection, back translation, and text generated by LLMs. This chapter discusses
    each of these, with optional code examples in the *supplementary/q15-text-augment*
    subfolder at *[https://github.com/rasbt/MachineLearning-QandAI-book](https://github.com/rasbt/MachineLearning-QandAI-book)*.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，还有几种增强文本数据的技术。最常见的包括同义词替换、词语删除、词语位置交换、句子洗牌、噪声注入、回译和由大语言模型（LLMs）生成的文本。本章将讨论这些技术，每个技术都有可选的代码示例，存放在*补充/q15-text-augment*子文件夹中，地址是
    *[https://github.com/rasbt/MachineLearning-QandAI-book](https://github.com/rasbt/MachineLearning-QandAI-book)*。
- en: '**Synonym Replacement**'
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**同义词替换**'
- en: 'In *synonym replacement*, we randomly choose words in a sentence—often nouns,
    verbs, adjectives, and adverbs—and replace them with synonyms. For example, we
    might begin with the sentence “The cat quickly jumped over the lazy dog,” and
    then augment the sentence as follows: “The cat rapidly jumped over the idle dog.”'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在*同义词替换*中，我们随机选择句子中的词语——通常是名词、动词、形容词和副词——并用同义词替换它们。例如，我们可以从句子“猫迅速跳过懒狗”开始，然后增强句子为：“猫快速跳过懒狗。”
- en: Synonym replacement can help the model learn that different words can have similar
    meanings, thereby improving its ability to understand and generate text. In practice,
    synonym replacement often relies on a thesaurus such as WordNet. However, using
    this technique requires care, as not all synonyms are interchangeable in all contexts.
    Most automatic text replacement tools have settings for adjusting replacement
    frequency and similarity thresholds. However, automatic synonym replacement is
    not perfect, and you might want to apply post-processing checks to filter out
    replacements that might not make sense.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 同义词替换可以帮助模型学习不同的单词可能具有相似的意思，从而提高其理解和生成文本的能力。在实际应用中，同义词替换通常依赖于如 WordNet 这样的词库。然而，使用这种技术时需要小心，因为并不是所有的同义词在所有语境下都是可以互换的。大多数自动文本替换工具都提供调整替换频率和相似度阈值的设置。然而，自动同义词替换并不完美，你可能需要应用后处理检查，以过滤掉可能不合适的替换。
- en: '**Word Deletion**'
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**词语删除**'
- en: '*Word deletion* is another data augmentation technique to help models learn.
    Unlike synonym replacement, which alters the text by substituting words with their
    synonyms, word deletion involves removing certain words from the text to create
    new variants while trying to maintain the overall meaning of the sentence. For
    example, we might begin with the sentence “The cat quickly jumped over the lazy
    dog” and then remove the word *quickly*: “The cat jumped over the lazy dog.”'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*词语删除*是另一种数据增强技术，帮助模型学习。与通过同义词替换单词改变文本的同义词替换不同，词语删除是通过从文本中删除某些词语来创建新的变体，同时尽量保持句子的整体意思。例如，我们可以从句子“猫迅速跳过懒狗”开始，然后删除词语*迅速*，变成“猫跳过懒狗。”'
- en: By randomly deleting words in the training data, we teach the model to make
    accurate predictions even when some information is missing. This can make the
    model more robust when encountering incomplete or noisy data in real-world scenarios.
    Also, by deleting nonessential words, we may teach the model to focus on key aspects
    of the text that are most relevant to the task at hand.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 通过随机删除训练数据中的词语，我们教会模型即使在信息缺失的情况下也能做出准确预测。这可以让模型在遇到现实场景中的不完整或噪声数据时更加鲁棒。而且，通过删除非关键性词语，我们可能教会模型专注于与当前任务最相关的文本关键部分。
- en: 'However, we must be careful not to remove critical words that may significantly
    alter a sentence’s meaning. For example, it would be suboptimal to remove the
    word *cat* in the previous sentence: “The quickly jumped over the lazy dog.” We
    must also choose the deletion rate carefully to ensure that the text still makes
    sense after words have been removed. Typical deletion rates might range from 10
    percent to 20 percent, but this is a general guideline and could vary significantly
    based on the specific use case.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们必须小心，不要删除那些可能显著改变句子意义的关键字。例如，在前面的句子中，如果去掉了*cat*这个词，“The quickly jumped
    over the lazy dog”就会变得不太合适。我们还必须谨慎选择删除率，以确保删除某些词后，文本依然能够保持合理性。典型的删除率可能在10%到20%之间，但这只是一个大致的指导，具体情况可能会根据特定的使用场景有所不同。
- en: '**Word Position Swapping**'
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**词语位置交换**'
- en: 'In *word position swapping*, also known as *word shuffling* or *permutation*,
    the positions of words in a sentence are swapped or rearranged to create new versions
    of the sentence. If we begin with “The cat quickly jumped over the lazy dog,”
    we might swap the positions of some words to get the following: “Quickly the cat
    jumped the over lazy dog.”'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在*词语位置交换*，也被称为*词语洗牌*或*排列组合*中，句子中的词语位置会被交换或重新排列，以创建新的句子版本。例如，如果我们从“猫快速跳过懒狗”开始，我们可能会交换一些词的位置，得到以下句子：“快速猫跳懒狗过。”
- en: While these sentences may sound grammatically incorrect or strange in English,
    they provide valuable training information for data augmentation because the model
    can still recognize the important words and their associations with each other.
    However, this method has its limitations. For example, shuffling words too much
    or in certain ways can drastically change the meaning of a sentence or make it
    completely nonsensical. Moreover, word shuffling may interfere with the model’s
    learning process, as the positional relationships between certain words can be
    vital in these contexts.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些句子在英语中可能听起来语法不正确或奇怪，但它们为数据增强提供了宝贵的训练信息，因为模型仍然能够识别重要的词汇以及它们之间的关联。然而，这种方法也有其局限性。例如，过度洗牌词语，或者以某些方式洗牌，可能会极大地改变句子的意义，甚至使其变得完全无意义。此外，词语洗牌可能会干扰模型的学习过程，因为某些词语之间的位置关系在这些语境中可能至关重要。
- en: '**Sentence Shuffling**'
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**句子洗牌**'
- en: In *sentence shuffling*, entire sentences within a paragraph or a document are
    rearranged to create new versions of the input text. By shuffling sentences within
    a document, we expose the model to different arrangements of the same content,
    helping it learn to recognize thematic elements and key concepts rather than relying
    on specific sentence order. This promotes a more robust understanding of the document’s
    overall topic or category. Consequently, this technique is particularly useful
    for tasks that deal with document-level analysis or paragraph-level understanding,
    such as document classification, topic modeling, or text summarization.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在*句子洗牌*中，段落或文档中的整个句子被重新排列，以创建输入文本的新版本。通过在文档中洗牌句子，我们让模型接触到同一内容的不同排列方式，帮助它学习识别主题元素和关键概念，而不是仅仅依赖于特定的句子顺序。这促进了对文档整体主题或类别的更全面理解。因此，这种技术特别适用于处理文档级分析或段落级理解的任务，如文档分类、主题建模或文本摘要。
- en: In contrast to the aforementioned word-based methods (word position swapping,
    word deletion, and synonym replacement), sentence shuffling maintains the internal
    structure of individual sentences. This avoids the problem of altering word choice
    or order such that sentences become grammatically incorrect or change meaning
    entirely.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 与前述的基于词语的方法（如词语位置交换、词语删除和同义词替换）不同，句子洗牌保持了单个句子的内部结构。这避免了通过改变词语选择或顺序而导致句子语法错误或完全改变意义的问题。
- en: 'Sentence shuffling is useful when the order of sentences is not crucial to
    the overall meaning of the text. Still, it may not work well if the sentences
    are logically or chronologically connected. For example, consider the following
    paragraph: “I went to the supermarket. Then I bought ingredients to make pizza.
    Afterward, I made some delicious pizza.” Reshuffling these sentences as follows
    disrupts the logical and temporal progression of the narrative: “Afterward, I
    made some delicious pizza. Then I bought ingredients to make pizza. I went to
    the supermarket.”'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 句子重排对于那些句子顺序对整体意思不重要的文本来说很有用。不过，如果句子之间有逻辑或时间上的联系，它可能会不太适用。例如，考虑以下段落：“我去了超市。然后我买了做披萨的材料。之后，我做了些美味的披萨。”将这些句子重新排列为：“之后，我做了些美味的披萨。然后我买了做披萨的材料。我去了超市。”这种重排打乱了叙事的逻辑和时间顺序。
- en: '**Noise Injection**'
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**噪声注入**'
- en: '*Noise injection* is an umbrella term for techniques used to alter text in
    various ways and create variation in the texts. It may refer either to the methods
    described in the previous sections or to character-level techniques such as inserting
    random letters, characters, or typos, as shown in the following examples:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*噪声注入*是一个总括性术语，用来描述通过各种方式改变文本并创造文本变异的技术。它既可以指前面提到的方法，也可以指诸如插入随机字母、字符或拼写错误等基于字符层面的技术，以下示例便展示了这一点：'
- en: '**Random character insertion**     “The cat qzuickly jumped over the lazy dog.”
    (Inserted a *z* in the word *quickly*.)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机字符插入**     “猫快速跳过懒狗。”（在单词*quickly*中插入了一个*z*。）'
- en: '**Random character deletion**     “The cat quickl jumped over the lazy dog.”
    (Deleted *y* from the word *quickly*.)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机字符删除**     “猫quickl跳过懒狗。”（从*quickly*中删除了*y*。）'
- en: '**Typo introduction**     “The cat qickuly jumped over the lazy dog.” (Introduced
    a typo in *quickly*, changing it to *qickuly*.)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**拼写错误引入**     “猫qickuly跳过懒狗。”（在*quickly*中引入了拼写错误，将其改为*qickuly*。）'
- en: These modifications are beneficial for tasks that involve spell-checking and
    text correction, but they can also help make the model more robust to imperfect
    inputs.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这些修改对涉及拼写检查和文本修正的任务非常有帮助，但它们也有助于使模型在面对不完美输入时更加稳健。
- en: '**Back Translation**'
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**反向翻译**'
- en: '*Back translation* is one of the most widely used techniques to create variation
    in texts. Here, a sentence is first translated from the original language into
    one or more different languages, and then it is translated back into the original
    language. Translating back and forth often results in sentences that are semantically
    similar to the original sentence but have slight variations in structure, vocabulary,
    or grammar. This generates additional, diverse examples for training without altering
    the overall meaning.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*反向翻译*是创造文本变异最常用的技术之一。在这个方法中，句子首先从原始语言翻译成一种或多种不同的语言，然后再翻译回原始语言。往返翻译通常会产生与原句语义相似但结构、词汇或语法上有轻微差异的句子。这种方式可以生成额外的多样化示例用于训练，而不改变整体意义。'
- en: For example, say we translate “The cat quickly jumped over the lazy dog” into
    German. We might get “Die Katze sprang schnell über den faulen Hund.” We could
    then translate this German sentence back into English to get “The cat jumped quickly
    over the lazy dog.”
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，假设我们将“猫快速跳过懒狗”翻译成德语。我们可能会得到“Die Katze sprang schnell über den faulen Hund。”然后，我们可以将这个德语句子翻译回英文，得到“猫跳过懒狗快速。”
- en: The degree to which a sentence changes through back translation depends on the
    languages used and the specifics of the machine translation model. In this example,
    the sentence remains very similar. However, in other cases or with other languages,
    you might see more significant changes in wording or sentence structure while
    maintaining the same overall meaning.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 通过反向翻译，一个句子变化的程度取决于所使用的语言以及机器翻译模型的具体情况。在这个例子中，句子的变化非常小。然而，在其他情况下或使用其他语言时，你可能会看到词语或句子结构发生更显著的变化，但整体意义保持不变。
- en: This method requires access to reliable machine translation models or services,
    and care must be taken to ensure that the back-translated sentences retain the
    essential meaning of the original sentences.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法需要访问可靠的机器翻译模型或服务，并且必须小心确保反向翻译后的句子保留原句的核心含义。
- en: '**Synthetic Data**'
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**合成数据**'
- en: '*Synthetic data generation* is an umbrella term that describes methods and
    techniques used to create artificial data that mimics or replicates the structure
    of real-world data. All methods discussed in this chapter can be considered synthetic
    data generation techniques since they generate new data by making small changes
    to existing data, thus maintaining the overall meaning while creating something
    new.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*合成数据生成*是一个总括性术语，描述了用于创建模仿或复制真实世界数据结构的人工数据的方法和技术。本章讨论的所有方法都可以视为合成数据生成技术，因为它们通过对现有数据进行小的修改来生成新数据，从而在创建新事物的同时保持整体意义。'
- en: Modern techniques to generate synthetic data now also include using decoder-style
    LLMs such as GPT (decoder-style LLMs are discussed in more detail in [Chapter
    17](ch17.xhtml)). We can use these models to generate new data from scratch by
    using “complete the sentence” or “generate example sentences” prompts, among others.
    We can also use LLMs as alternatives to back translation, prompting them to rewrite
    sentences as shown in [Figure 15-1](ch15.xhtml#ch15fig1).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现代生成合成数据的技术现在还包括使用解码器风格的LLM，例如GPT（解码器风格的LLM将在[第17章](ch17.xhtml)中详细讨论）。我们可以通过使用“完成句子”或“生成示例句子”提示等方式，利用这些模型从头生成新数据。我们还可以将LLM用作反向翻译的替代方法，提示它们重写句子，如[图
    15-1](ch15.xhtml#ch15fig1)所示。
- en: '![Image](../images/15fig01.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/15fig01.jpg)'
- en: '*Figure 15-1: Using an LLM to rewrite a sentence*'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 15-1：使用LLM重写句子*'
- en: Note that an LLM, as shown in [Figure 15-1](ch15.xhtml#ch15fig1), runs in a
    nondeterministic mode by default, which means we can prompt it multiple times
    to obtain a variety of rewritten sentences.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，LLM（如[图 15-1](ch15.xhtml#ch15fig1)所示）默认以非确定性模式运行，这意味着我们可以多次提示它，以获得多种重写的句子。
- en: '**Recommendations**'
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**建议**'
- en: The data augmentation techniques discussed in this chapter are commonly used
    in text classification, sentiment analysis, and other NLP tasks where the amount
    of available labeled data might be limited.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论的数据增强技术通常用于文本分类、情感分析和其他NLP任务，这些任务中可用的标注数据量可能有限。
- en: LLMs are usually pretrained on such a vast and diverse dataset that they may
    not rely on these augmentation techniques as extensively as in other, more specific
    NLP tasks. This is because LLMs aim to capture the statistical properties of the
    language, and the vast amount of data on which they are trained often provides
    a sufficient variety of contexts and expressions. However, in the fine-tuning
    stages of LLMs, where a pretrained model is adapted to a specific task with a
    smaller, task-specific dataset, data augmentation techniques might become more
    relevant again, mainly if the task-specific labeled dataset size is limited.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: LLM通常在如此庞大和多样化的数据集上进行预训练，以至于它们可能不像在其他更具体的NLP任务中那样广泛依赖这些增强技术。这是因为LLM的目标是捕捉语言的统计特性，而它们所训练的数据量通常提供了足够的上下文和表达方式的多样性。然而，在LLM的微调阶段，其中一个预训练模型被调整到一个特定任务，并使用较小的任务特定数据集时，数据增强技术可能会变得更加相关，特别是当任务特定的标注数据集较小时。
- en: '**Exercises**'
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**练习**'
- en: '**15-1.** Can the use of text data augmentation help with privacy concerns?'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**15-1.** 使用文本数据增强能否帮助解决隐私问题？'
- en: '**15-2.** What are some instances where data augmentation may not be beneficial
    for a specific task?'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**15-2.** 在哪些情况下数据增强可能对特定任务没有帮助？'
- en: '**References**'
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: 'The WordNet thesaurus: George A. Miller, “WordNet: A Lexical Database for English”
    (1995), *[https://dl.acm.org/doi/10.1145/219717.219748](https://dl.acm.org/doi/10.1145/219717.219748)*.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'WordNet词库：George A. Miller，“WordNet: A Lexical Database for English”（1995），*
    [https://dl.acm.org/doi/10.1145/219717.219748](https://dl.acm.org/doi/10.1145/219717.219748)
    *。'
