- en: '12'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '12'
- en: Apotheosis
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Apotheosis
- en: '![](image_fi/book_art/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/book_art/chapterart.png)'
- en: 'While we were fiddling around with our Lambda backdoor, someone at Gretsch
    Politico was kind enough to trigger the reverse shell nested in the *ecr-login.sh*
    script. Not once, but multiple times. Most sessions seemed to time out after about
    30 minutes, so we need to be swift and efficient in assessing this new environment
    and finding novel ways of pivoting inside. We open one of the meterpreter sessions
    and spawn a shell on the remote machine:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在摆弄我们的Lambda后门时，Gretsch Politico的某个人好心地触发了嵌套在*ecr-login.sh*脚本中的反向shell。不止一次，而是多次。大多数会话似乎在大约30分钟后超时，因此我们需要迅速且高效地评估这个新环境，并找到在其中横向渗透的新方法。我们打开其中一个meterpreter会话，并在远程机器上生成一个shell：
- en: '[PRE0]'
  id: totrans-4
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We can see that we’re running as root 1 inside a randomly named machine 2. Yes,
    we are probably inside a container. Naturally, then, we run the `env` command
    to reveal any injected secrets, and we run the `mount` command to show folders
    and files shared by the host. We follow these commands with a couple of queries
    to the metadata API, requesting the IAM role attached to the machine (see [Listing
    12-1](#listing12-1)).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，我们以root身份1运行在一个随机命名的机器2上。是的，我们很可能在一个容器内。因此，我们运行了`env`命令来揭示任何注入的机密信息，并运行了`mount`命令来显示主机共享的文件夹和文件。接下来，我们执行了几条查询元数据API的命令，请求该机器上附加的IAM角色（见[列表12-1](#listing12-1)）。
- en: '[PRE1]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Listing 12-1: Output of the `env` and `mount` commands followed by a query
    to the metadata API'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12-1：`env`和`mount`命令的输出，后跟对元数据API的查询
- en: No Kubernetes variables or orchestrator names stand out in the result of the
    `env` command. It seems like we are trapped inside a stand-alone container devoid
    of passwords or secrets in the environment. There’s not even an IAM role attached
    to the underlying machine 2, but just a sneaky little */var/run/docker.sock* 1
    mounted inside the container itself, along with a Docker binary. So thoughtful
    of them!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在`env`命令的结果中，没有Kubernetes变量或协调器名称突出显示。看起来我们被困在一个独立的容器中，环境中没有密码或机密信息。甚至底层机器2上也没有附加IAM角色，只有一个偷偷摸摸的*/var/run/docker.sock*
    1被挂载到容器内部，还有一个Docker二进制文件。真是周到！
- en: We can safely tuck away the ugly JSON one might use to directly query the */var/run/docker.sock*
    via `curl` and promptly execute Docker commands to enumerate the currently running
    containers (see [Listing 12-2](#listing12-2)).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以安全地将可能用于通过`curl`直接查询*/var/run/docker.sock*的丑陋JSON藏起来，并迅速执行Docker命令来枚举当前运行的容器（见[列表12-2](#listing12-2)）。
- en: '[PRE2]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Listing 12-2: A list of containers running on the host'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12-2：主机上运行的容器列表
- en: 'We find that more than 10 containers are running on this machine, all pulled
    from the *983457354409.dkr.ecr.eu-west-1.amazonaws.com* Elastic Container Registry
    (ECR). We know the account ID 983457354409; we saw it authorized in the bucket
    policy of mxrads-dl. Our hunch was right: it was Gretsch Politico after all.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现该机器上运行着超过10个容器，全部从*983457354409.dkr.ecr.eu-west-1.amazonaws.com*弹性容器注册表（ECR）中拉取。我们知道账户ID是983457354409；我们在mxrads-dl的存储桶策略中看到它已被授权。我们的直觉是对的：最终还是Gretsch
    Politico的容器。
- en: 'All the containers found in [Listing 12-2](#listing12-2) were lifted using
    a `master` tag, except for one: the `app-abtest` image 1, which bears the curious
    tag `SUP6541-add-feature-network`.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 所有在[列表12-2](#listing12-2)中找到的容器都使用`master`标签进行启动，除了一个：`app-abtest`镜像1，它带有一个奇怪的标签`SUP6541-add-feature-network`。
- en: 'We might have an idea about what’s going on in this machine, but we still need
    one last piece of information before making a conclusion. Let’s get more information
    using the `docker info` command to display data about the host:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们或许已经对这台机器上发生的事情有了一些了解，但在得出结论之前，我们仍然需要最后一块信息。让我们使用`docker info`命令获取更多主机信息：
- en: '[PRE3]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Hello, Jenkins, our old friend. Now it all makes sense. We can guess that our
    payload is triggered by what we can assume are end-to-end test workloads. The
    job that triggered in this instance probably starts a container that authenticates
    to AWS ECR using the *ecr-login.sh* script and then lifts a subset of production
    containers, indicated by the `master` tag—`datavalley`, `libpredict`, and the
    rest—along with the experimental Docker image of the service to be tested: `ab-test`.
    That explains why it has a different tag than all the other containers.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 嗨，Jenkins，我们的老朋友。现在一切都明了了。我们可以推测，触发我们的负载的，可能是我们可以假设为端到端测试工作负载的某些操作。此实例中触发的任务可能会启动一个容器，使用*ecr-login.sh*脚本进行AWS
    ECR身份验证，然后提升一部分生产容器，这些容器用`master`标签标记——如`datavalley`、`libpredict`等——以及要测试的实验性Docker镜像：`ab-test`。这也解释了为什么它有一个与其他容器不同的标签。
- en: Exposing the Docker socket in this way is a common practice in test environments,
    where Docker is not so much used for its isolation properties, but rather for
    its packaging features. For example, Crane, a popular Docker orchestration tool
    ([https://github.com/michaelsauter/crane/](https://github.com/michaelsauter/crane/)),
    is used to lift containers along with their dependencies. Instead of installing
    Crane on every single machine, a company may package it in a container and pull
    it at runtime whenever needed.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式暴露Docker套接字在测试环境中是常见做法，在这些环境中，Docker并不是主要用于其隔离功能，而是用于其打包功能。例如，Crane，一个流行的Docker编排工具([https://github.com/michaelsauter/crane/](https://github.com/michaelsauter/crane/))，用于提升容器及其依赖项。公司可能不会在每台机器上安装Crane，而是将其打包到一个容器中，并在运行时按需拉取。
- en: From a software vantage point, it’s great. All jobs are using the same version
    of the Crane tool, and the server running the tests becomes irrelevant. From a
    security standpoint, however, this legitimizes the use of Docker-in-Docker tricks
    (Crane runs containers from within its own container), which opens the floodgates
    of hell and beyond.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 从软件的角度来看，这是很棒的。所有任务都使用相同版本的Crane工具，而运行测试的服务器变得无关紧要。然而，从安全的角度来看，这实际上使得使用Docker-in-Docker技巧成为合法（Crane在其自己的容器内运行容器），这为地狱的洪水之门打开了。
- en: Persisting the Access
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 持久化访问
- en: 'Test jobs can only last so long before being discarded. Let’s transform this
    ephemeral access into a permanent one by running a custom meterpreter on a new
    container we’ll label `aws-cli`:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 测试任务只能持续一段时间，然后被丢弃。让我们通过在一个新的容器上运行自定义的meterpreter，并将其标记为`aws-cli`，将这种临时访问转变为永久访问：
- en: '[PRE4]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Our new reverse shell is running in a privileged container that mounts the
    Docker socket along with the entire host filesystem in the */hostOS* 1 directory:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的新反向Shell正在一个特权容器中运行，该容器挂载了Docker套接字，并将整个主机文件系统挂载到*/hostOS* 1目录中：
- en: '[PRE5]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Let the fun begin!
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: As we saw in Chapter 10, Jenkins can quickly aggregate a considerable amount
    of privileges due to its scheduling capabilities. It’s the Lehman Brothers of
    the technological world—a hungry entity in an unregulated realm, encouraged by
    reckless policymakers and one trade away from collapsing the whole economy.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第10章中所看到的，Jenkins由于其调度能力，能够快速聚合大量的权限。它就像是技术世界中的雷曼兄弟——一个在无监管领域中饥渴的存在，受到鲁莽政策制定者的鼓励，且只需一次交易就能让整个经济崩溃。
- en: In this particular occurrence, that metaphorical trade happens to be how Jenkins
    handles environment variables. When a job is scheduled on a worker, it can be
    configured either to pull the two or three secrets it needs to run properly or
    to load every possible secret as environment variables. Let’s find out just how
    lazy Gretsch Politico’s admins really are.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种特殊情况下，这个隐喻中的“交易”恰好是Jenkins如何处理环境变量。当一个任务在一个工作节点上调度时，可以配置为仅拉取它运行所需的两三个密钥，或者加载所有可能的密钥作为环境变量。让我们来看看Gretsch
    Politico的管理员到底有多懒。
- en: 'We single out every process launched by Jenkins jobs on this machine:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们单独列出了在这台机器上由Jenkins任务启动的每一个进程：
- en: '[PRE6]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We copy the PIDs of these processes into a file and iterate over each line
    to fetch their environment variables, conveniently stored at the path */prod/$PID/environ*:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这些进程的PID复制到一个文件中，并逐行遍历以获取它们的环境变量，环境变量便捷地存储在路径*/prod/$PID/environ*下：
- en: '[PRE7]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We upload our harvest to our remote server and apply some minor formatting,
    and then we enjoy the cleartext results (see [Listing 12-3](#listing12-3)).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将收获上传到远程服务器，并进行一些小的格式调整，然后享受明文结果（见[清单12-3](#listing12-3)）。
- en: '[PRE9]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Listing 12-3: The results from collecting environment variables of jobs running
    on the Jenkins machine'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 清单12-3：收集在Jenkins机器上运行的任务环境变量的结果
- en: Marvelous. We scored a GitHub API token to explore GP’s entire codebase, a couple
    of database passwords to harvest some data, and obviously AWS access keys that
    should at least have access to ECR (the AWS container registry) or maybe even
    EC2, if we’re lucky.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了。我们获得了一个GitHub API令牌，得以探索GP的整个代码库，获取了一些数据库密码来收集数据，当然还有AWS访问密钥，至少应该能够访问ECR（AWS容器注册表），如果幸运的话，甚至是EC2。
- en: 'We load them on our server and blindly start exploring AWS services:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们把它们加载到我们的服务器上，然后盲目地开始探索AWS服务：
- en: '[PRE10]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We hit multiple errors as soon as we step outside of ECR. In another time,
    another context, we would fool around with container images, search for hardcoded
    credentials, or tamper with the production tag to achieve code execution on a
    machine—but there is another trail that seems more promising. It was buried inside
    the environment data we dumped in [Listing 12-3](#listing12-3), so let me zoom
    in on it again:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们离开ECR，就会遇到多个错误。在另一个时间、另一个情境下，我们会捣鼓容器镜像，寻找硬编码的凭证，或篡改生产标签以在机器上执行代码——但有一条线索似乎更有希望。它埋藏在我们在[列表12-3](#listing12-3)中转储的环境数据里，让我再聚焦一下它：
- en: '[PRE11]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The `SPARK` here indicates Apache Spark, an open source analytics engine. It
    might seem surprising to let the ECR access keys and database credentials slide
    by just to focus on this lonely IP address, but remember one of our original goals:
    getting user profiles and data segments. This type of data will not be stored
    in your average 100GB database. When fully enriched with all the available information
    about each person, and given the size of MXR Ads’ platform, these data profiles
    could easily reach hundreds if not thousands of terabytes.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的`SPARK`表示Apache Spark，这是一个开源分析引擎。单纯地让ECR访问密钥和数据库凭证绕过，然后专注于这个孤立的IP地址可能令人惊讶，但请记住我们最初的目标之一：获取用户档案和数据段。这种类型的数据不会存储在一般的100GB数据库中。当这些数据完全丰富，并包含关于每个人的所有可用信息时，再加上MXR
    Ads平台的规模，这些数据档案很容易达到数百甚至数千TB。
- en: Two problems commonly arise when companies are dealing with such ridiculous
    volumes. Where do they store the raw data? And how can they process it efficiently?
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 公司在处理如此庞大的数据量时，通常会遇到两个问题。它们将原始数据存储在哪里？如何高效地处理这些数据？
- en: Storing raw data is easy. S3 is cheap and reliable, so that’s a no-brainer.
    Processing gigantic amounts of data, however, is a real challenge. Data scientists
    looking to model and predict behavior at a reasonable cost need a distributed
    system to handle the load—say, 500 machines working in parallel, each training
    multiple models with random hyperparameters until they find the formulas with
    the lowest error rate.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 存储原始数据很容易。S3便宜且可靠，所以这没什么可争议的。然而，处理海量数据却是一个真正的挑战。数据科学家们希望以合理的成本建模并预测行为，需要一个分布式系统来处理负载——比如500台机器并行工作，每台机器训练多个模型，随机调整超参数，直到找到误差率最低的公式。
- en: 'But that raises additional problems. How can they partition the data efficiently
    among the nodes? What if all the machines need the same piece of data? How do
    they aggregate all the results? And most important of all: how do they deal with
    failure? Because there sure is going to be failure. For every 1,000 machines,
    on average 5, if not more, will die for any number of reasons, including disk
    issues, overheating, power outage, and other hazardous events, even in a top-tier
    datacenter. How can they redistribute the failed workload on healthier nodes?'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 但这也带来了额外的问题。如何在节点之间有效地划分数据？如果所有机器都需要相同的数据该怎么办？如何聚合所有结果？最重要的是：他们如何应对故障？因为故障肯定会发生。对于每1000台机器，平均有5台，甚至更多，可能因任何原因发生故障，包括磁盘问题、过热、电力中断以及其他危险事件，即便是在顶级数据中心中也是如此。他们如何在健康节点上重新分配失败的工作负载？
- en: It is exactly these questions that Apache Spark aims to solve with its distributed
    computing framework. If Spark is involved in Gretsch Politico, then it’s most
    likely being used to process massive amounts of data that could very likely be
    the user profiles we are after—hence our interest in the IP address we retrieved
    on the Jenkins machine.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 正是这些问题，Apache Spark旨在通过其分布式计算框架来解决。如果Spark参与了Gretsch Politico，那么它很可能被用来处理大量数据，这些数据很可能就是我们所追求的用户档案——因此我们对在Jenkins机器上获取到的IP地址产生了兴趣。
- en: Breaking into the Spark cluster would automatically empower us to access the
    raw profiling data, learn what kind of processing it goes through, and understand
    how the data is exploited by Gretsch Politico.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 进入Spark集群将自动使我们能够访问原始的性能数据，了解数据经过何种处理，并理解Gretsch Politico是如何利用这些数据的。
- en: 'As of this moment, however, there is not a single hacking post to help us shake
    down a Spark cluster (the same observation can be made about almost every tool
    involved in big data: Yarn, Flink, Hadoop, Hive, and so on). Not even an Nmap
    script to fingerprint the damn thing. We are sailing in uncharted waters, so the
    most natural step is to first understand how to interact with a Spark cluster.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，到目前为止，没有一篇黑客帖子能帮助我们攻破 Spark 集群（几乎所有大数据工具也都是如此：Yarn、Flink、Hadoop、Hive 等等）。甚至没有一个
    Nmap 脚本能指纹化这个该死的东西。我们正在航行在未知的水域，所以最自然的步骤是首先了解如何与 Spark 集群进行交互。
- en: Understanding Spark
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解 Spark
- en: 'A Spark cluster is essentially composed of three major components: a master
    server, worker machines, and a driver. The driver is the client looking to perform
    a calculation; that would be the analyst’s laptop, for instance. The master’s
    sole job is to manage workers and assign them jobs based on memory and CPU requirements.
    Workers execute whatever jobs the master sends their way. They communicate with
    both the master and the driver.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 Spark 集群本质上由三个主要组件组成：主服务器、工作节点和驱动器。驱动器是执行计算的客户端；比如说，分析师的笔记本电脑就是驱动器。主节点的唯一任务是管理工作节点，并根据内存和
    CPU 的需求分配任务。工作节点执行主节点分配的所有任务，并与主节点和驱动器进行通信。
- en: 'Each of these three components is running a Spark process inside a Java virtual
    machine (JVM), even the analyst’s laptop (driver). Here is the kicker, though:
    *security is off by default on Spark.*'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个组件中的每一个都在 Java 虚拟机（JVM）中运行一个 Spark 进程，即使是分析师的笔记本电脑（驱动器）。不过，有个关键点：*Spark 默认禁用安全性*。
- en: We are not only talking about authentication, mind you, which would still be
    bad. No, *security altogether* is disabled, including encryption, access control,
    and, of course, authentication. It’s 2021, folks. Get your shit together.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅仅在谈论认证问题，这已经很糟糕了。不，*安全性整体*被禁用了，包括加密、访问控制，当然还有认证。2021 年了，各位，整理好你们的东西吧。
- en: In order to communicate with a Spark cluster, a couple of network requirements
    are needed according to the official documentation. We first need to be able to
    reach the master on port 7077 to schedule jobs. The worker machines also need
    to be able to initiate connections to the driver (our Jenkins node) to request
    the JAR file to execute, report results, and handle other scheduling steps.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 根据官方文档，为了与 Spark 集群进行通信，需要满足一些网络要求。首先，我们需要能够通过 7077 端口访问主节点，以便调度任务。工作节点还需要能够发起与驱动器（我们的
    Jenkins 节点）之间的连接，请求执行 JAR 文件、报告结果并处理其他调度步骤。
- en: Given the presence of the `SPARK_MASTER` environment variable in [Listing 12-3](#listing12-3),
    we are 90 percent sure that Jenkins runs some Spark jobs, so we can be pretty
    confident that all these network conditions are properly lined up. But just to
    be on the safe side, let’s first confirm that we can at least reach the Spark
    master. The only way to test the second network requirement (that workers can
    connect to the driver) is by submitting a job or inspecting security groups.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 根据[Listing 12-3](#listing12-3)中 `SPARK_MASTER` 环境变量的存在，我们有 90% 的把握认为 Jenkins
    运行了一些 Spark 任务，因此我们可以相当确信所有这些网络条件都已正确配置。但为了确保安全起见，首先确认我们至少能够访问 Spark 主节点。测试第二个网络要求（即工作节点能否连接到驱动器）的唯一方法是提交任务或检查安全组。
- en: 'We add a route to the 10.0.0.0/8 range on Metasploit to reach the Spark master
    IP (10.50.12.67) and channel it through our current meterpreter session:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 Metasploit 上添加一条路由，指向 10.0.0.0/8 范围，以便到达 Spark 主节点 IP（10.50.12.67），并通过当前的
    meterpreter 会话进行通道传输：
- en: '[PRE12]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We then use the built-in Metasploit scanner to probe port 7077:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 接着我们使用内置的 Metasploit 扫描器来探测 7077 端口：
- en: '[PRE13]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: No surprises. We are able to communicate with the master. All right, let’s write
    our first evil Spark application!
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 没有惊讶的事情。我们能够与主节点通信。好吧，让我们写第一个恶意 Spark 应用吧！
- en: Malicious Spark
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 恶意 Spark
- en: Even though Spark is written in Scala, it supports Python programs very well.
    There is a heavy serialization cost to pay for translating Python objects into
    Java objects, but what do we care? We only want a shell on one of the workers.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Spark 是用 Scala 编写的，但它对 Python 程序的支持非常好。将 Python 对象转换为 Java 对象需要支付高昂的序列化成本，但我们又何妨呢？我们只需要一个运行在某个工作节点上的外壳。
- en: 'Python even has a `pip` package that downloads 200MB worth of JAR files to
    quickly set up a working Spark environment:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Python 甚至有一个 `pip` 包，它可以下载 200MB 的 JAR 文件来快速设置一个可用的 Spark 环境：
- en: '[PRE14]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Every Spark application starts with the same boilerplate code that defines the
    `SparkContext`, a client-side connector in charge of communicating with the Spark
    cluster. We start our application with that setup code (see [Listing 12-4](#listing12-4)).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 Spark 应用程序都以相同的模板代码开始，该代码定义了 `SparkContext`，这是一个客户端连接器，负责与 Spark 集群进行通信。我们通过这段设置代码开始我们的应用程序（参见
    [Listing 12-4](#listing12-4)）。
- en: '[PRE15]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Listing 12-4: Malicious Spark application setup code'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Listing 12-4：恶意 Spark 应用程序设置代码
- en: 'This Spark context 1 implements methods that create and manipulate distributed
    data. It allows us to transform a regular Python list from a monolithic object
    into a collection of units that can be distributed over multiple machines. These
    units are called *partitions*. Each partition can hold one, two, or three elements
    of the original list—whatever Spark deems to be optimal. Here we define such a
    collection of partitions composed of 10 elements:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 Spark 上下文 1 实现了创建和操作分布式数据的方法。它允许我们将一个普通的 Python 列表从一个整体对象转换为可以分布在多台机器上的一组单元。这些单元称为
    *分区*。每个分区可以包含原始列表的一个、两个或三个元素——无论 Spark 认为最优的是什么。这里我们定义了一个包含 10 个元素的分区集合：
- en: '[PRE16]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The `partList.getNumPartitions` returns `2` on my computer, indicating that
    it has split the original list into two partitions. Partition 1 likely holds 0,
    1, 2, 3, and 4\. Partition 2 likely holds 5, 6, 7, 8, and 9.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`partList.getNumPartitions` 在我的计算机上返回 `2`，表示它已经将原始列表拆分成了两个分区。分区 1 可能包含 0、1、2、3
    和 4，分区 2 可能包含 5、6、7、8 和 9。'
- en: 'The `partList` is now a collection of partitions. It’s a *resilient distributed
    dataset* *(RDD**)* that supports many iterative methods, known as Spark *transformations*,
    like `map`, `flatMap`, `reduceByKey`, and other methods that will transform the
    data in a distributed manner. Code execution seems like a long shot from MapReduce
    operations, but bear with me: it will all tie up together nicely.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`partList` 现在是一个分区集合。它是一个 *弹性分布式数据集*（*RDD*），支持许多迭代方法，称为 Spark 的 *转换*，例如 `map`、`flatMap`、`reduceByKey`
    等，这些方法以分布式的方式转换数据。代码执行看起来与 MapReduce 操作相差甚远，但请耐心等一下：这一切都会很好地衔接起来。'
- en: Before continuing with our Spark app, I’ll give an example of using the `map`
    API to loop over each element of the partitions, feed them to the function `addTen`,
    and store the result in a new RDD (see [Listing 12-5](#listing12-5)).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续进行我们的 Spark 应用程序之前，我将举一个使用 `map` API 的例子，来遍历每个分区的元素，将它们传递给 `addTen` 函数，并将结果存储在一个新的
    RDD 中（参见 [Listing 12-5](#listing12-5)）。
- en: '[PRE17]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Listing 12-5: Using the `map` API on Spark'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Listing 12-5：在 Spark 上使用 `map` API
- en: 'Now `plusTenList` contains (10, 11, . . .). How is this different from a regular
    Python map or a classic loop? Say, for example, we had two workers and two partitions.
    Spark would send elements 0 through 4 to machine #1 and elements 5 through 9 to
    machine #2\. Each machine would iterate over the list, apply the function `addTen`,
    and return the partial result to the driver (our Jenkins machine), which then
    consolidates it into the final output. Should machine #2 fail during the calculation,
    Spark would automatically reschedule the same workload on machine #1.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '现在，`plusTenList` 包含（10，11，...）。这与常规的 Python map 或经典循环有何不同？举个例子，如果我们有两个工作节点和两个分区，Spark
    会将元素 0 到 4 发送到机器 #1，将元素 5 到 9 发送到机器 #2。每台机器将迭代该列表，应用函数 `addTen`，并将部分结果返回给驱动程序（我们的
    Jenkins 机器），然后驱动程序将其合并为最终输出。如果机器 #2 在计算过程中失败，Spark 会自动重新调度相同的工作负载到机器 #1。'
- en: At this point, I am sure you’re thinking, “Great. Spark is awesome, but why
    the long lecture on maps and RDDs? Can’t we just submit the Python code as is
    and execute code?”
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 到这时，我敢肯定你在想：“太好了，Spark 很强大，但为什么要讲这么多关于 maps 和 RDDs 的内容？我们不能直接提交 Python 代码并执行它吗？”
- en: I wish it were that simple.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望事情能这么简单。
- en: See, if we just append a classic call to `subprocess.Popen` and execute the
    script, we’ll just—well, you can see for yourself in [Listing 12-6](#listing12-6).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 看，假如我们只是附加一个经典的 `subprocess.Popen` 调用并执行脚本，我们就会——嗯，你可以在 [Listing 12-6](#listing12-6)
    中看到结果。
- en: '[PRE18]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Listing 12-6: The Python code executes code locally instead of sending it to
    the Spark cluster.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Listing 12-6：Python 代码在本地执行，而不是将其发送到 Spark 集群。
- en: When we run our test app, we get returned the ID of our own container. The `hostname`
    command in the Python code was executed on our system. It did not even reach the
    Spark master. What happened?
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行测试应用程序时，我们得到了我们自己容器的 ID。Python 代码中的 `hostname` 命令是在我们的系统上执行的，甚至没有到达 Spark
    主节点。发生了什么？
- en: The Spark driver, the process that gets initialized by PySpark when executing
    the code, does not technically send the Python code to the master. First, the
    driver builds a *directed acyclic graph* *(DAG**)*, which is a sort of summary
    of all the operations that are performed on the RDDs, like loading, `map`, `flatMap`,
    storing as a file, and so on (see [Figure 12-1](#figure12-1)).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 驱动程序，即在执行代码时由 PySpark 初始化的进程，技术上并不将 Python 代码发送到主节点。首先，驱动程序构建一个*有向无环图*（*DAG*），这是对在
    RDD 上执行的所有操作的总结，比如加载、`map`、`flatMap`、存储为文件等（见[图 12-1](#figure12-1)）。
- en: '![f12001](image_fi/501263c12/f12001.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![f12001](image_fi/501263c12/f12001.png)'
- en: 'Figure 12-1: Example of a simple DAG composed of two steps: parallelize and
    map'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12-1：由两个步骤组成的简单 DAG 示例：parallelize 和 map
- en: 'The driver then registers the workload on the master by sending a few key properties:
    the workload’s name, the memory requested, the number of initial executors, and
    so forth. The master acknowledges the registration and assigns Spark workers to
    the incoming job. It shares their details (IP and port number) with the driver,
    but no action follows. Up until this point, no real computation is performed.
    The data still sits on the driver’s side.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 驱动程序通过发送一些关键属性来将工作负载注册到主节点：工作负载的名称、请求的内存、初始执行器的数量等等。主节点确认注册并将 Spark 工作节点分配给传入的任务。它将这些工作节点的详细信息（IP
    和端口号）共享给驱动程序，但没有进一步的动作。直到这一点为止，实际上并没有执行任何计算。数据仍然保留在驱动程序一侧。
- en: The driver continues parsing the script and adding steps to the DAG, when needed,
    until it hits what it considers to be an *action*, a Spark API that forces the
    collapse of the DAG. This action could be a call to display an output, save a
    file, count elements, and so on (you can find a list of Spark actions at [http://bit.ly/3aW64Dh](http://bit.ly/3aW64Dh)).
    Then and only then will the DAG be sent to the Spark workers. These workers follow
    the DAG to run the transformations and actions it contains.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 驱动程序继续解析脚本并根据需要将步骤添加到 DAG 中，直到它遇到它认为是*动作*的部分，这是一个强制收缩 DAG 的 Spark API。这个动作可能是显示输出、保存文件、计数元素等调用（你可以在[http://bit.ly/3aW64Dh](http://bit.ly/3aW64Dh)找到
    Spark 动作的列表）。只有到这一点，DAG 才会被发送到 Spark 工作节点。这些工作节点跟随 DAG 执行其中的转换和动作。
- en: Fine. We upgrade our code to add an action (in this case, a `collect` method)
    that will trigger the app’s submission to a worker node (see [Listing 12-7](#listing12-7)).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。我们升级了代码，添加了一个动作（在这种情况下，是 `collect` 方法），它会触发应用程序提交到工作节点（见[清单 12-7](#listing12-7)）。
- en: '[PRE19]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Listing 12-7: Adding an action to the malicious Spark application'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 12-7：向恶意的 Spark 应用程序添加动作
- en: But we’re still missing a crucial piece. Workers only follow the DAG, and the
    DAG only accounts for RDD resources. We need to call Python’s `Popen` in order
    to execute commands on the workers, yet `Popen` is neither a Spark transformation
    like `map` nor an action like `collect`, so it will be omitted from the DAG. We
    need to cheat and include our command execution inside a Spark transformation
    (a map, for instance), as shown in [Listing 12-8](#listing12-8).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我们仍然缺少一个关键部分。工作节点只会遵循 DAG，而 DAG 只涉及 RDD 资源。我们需要调用 Python 的 `Popen` 来在工作节点上执行命令，但
    `Popen` 既不是像 `map` 这样的 Spark 转换，也不是像 `collect` 这样的动作，因此它将被省略在 DAG 之外。我们需要作弊，并将我们的命令执行包含在
    Spark 转换（例如 map）中，如[清单 12-8](#listing12-8)所示。
- en: '[PRE20]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Listing 12-8: Skeleton of the full app executing code on a Spark cluster'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 12-8：在 Spark 集群上执行代码的完整应用框架
- en: 'Instead of defining a new named function and calling it iteratively via `map`
    (like we did in [Listing 12-5](#listing12-5)), we instantiate an anonymous function
    with the prefix `lambda` that accepts one input parameter (each element iterated
    over) 1. When the worker loops over our RDD to apply the `map` transformation,
    it comes across our `lambda` function, which instructs it to run the `hostname`
    command. Let’s try it out:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 与其定义一个新的命名函数并通过 `map` 迭代调用（就像我们在[清单 12-5](#listing12-5)中做的那样），我们实例化一个带有前缀 `lambda`
    的匿名函数，它接受一个输入参数（每个被迭代的元素）1。当工作节点循环遍历我们的 RDD 以应用 `map` 转换时，它会遇到我们的 `lambda` 函数，该函数指示它运行
    `hostname` 命令。我们来试试：
- en: '[PRE21]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: There you go! We made contact with the master. A nice, clean command execution,
    and as promised, at no point in time did Spark bother asking us for credentials.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！我们与主节点建立了联系。一个干净利落的命令执行，正如承诺的那样，在整个过程中，Spark 没有一次要求我们提供凭证。
- en: Should we relaunch the program, our job might get scheduled on another worker
    node altogether. This is expected and is, in fact, at the heart of distributed
    computing. All nodes are identical and have the same configuration (IAM roles,
    network filters, and so on), but they will not necessarily lead the same life.
    One worker may receive a job that spills database credentials to disk, while another
    sorts error messages.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们重新启动程序，我们的任务可能会被调度到另一台工作节点。这是预期的，事实上，它正是分布式计算的核心。所有节点是相同的，具有相同的配置（IAM角色、网络过滤器等），但它们的生命周期不一定完全相同。一台工作节点可能会接收到一个任务，该任务将数据库凭证写入磁盘，而另一台则会对错误消息进行排序。
- en: 'We can force Spark to distribute our workload to *n* machines by building RDDs
    with *n* partitions:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过构建具有*n*分区的RDD，强制Spark将我们的工作负载分配到*n*台机器上：
- en: '[PRE22]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We cannot, however, choose which ones will receive the payload. Time to set
    up a permanent resident on a couple of worker nodes.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们无法选择哪些节点将接收负载。是时候在一些工作节点上设置永久驻留了。
- en: Spark Takeover
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Spark接管
- en: 'To keep our malicious app in play, we want to diligently instruct Linux to
    spawn it in its own process group, in order to ignore interrupt signals sent by
    the JVM when the job is done. We also want the driver to wait a few seconds, until
    our app finishes establishing a stable connection to our attacking infrastructure.
    We need to add these lines to our app:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持我们的恶意应用继续运行，我们希望谨慎地指示Linux在自己的进程组中生成它，以便忽略JVM在任务完成时发送的中断信号。我们还希望驱动程序等待几秒钟，直到我们的应用完成与攻击基础设施的稳定连接。我们需要在应用程序中添加以下几行：
- en: '[PRE23]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'On our attacking infrastructure, we open Metasploit and wait for the app to
    ring back home:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的攻击基础设施上，我们打开Metasploit并等待应用程序回拨到主机：
- en: '[PRE24]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Fantastic! We made it to one of the workers. We’re running as a regular Spark
    user 1, which was trusted enough to be included in the *sudo* group. No complaints
    from this side of the screen. Let’s explore this new entourage by dumping environment
    variables, mounted folders, IAM roles, or anything else that might be useful:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！我们成功地进入了其中一台工作节点。我们以一个普通的Spark用户1身份运行，这个用户足够信任，因此被包括在了*sudo*组中。屏幕这一边没有任何抱怨。让我们通过转储环境变量、挂载的文件夹、IAM角色，或者任何其他可能有用的内容来探索这个新的环境：
- en: '[PRE25]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We learn that Spark workers can impersonate the spark-standalone.ec2 role.
    Like with most IAM roles, it’s hard to know the full extent of its privileges,
    but we can pick up some clues using the `mount` command:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，Spark工作节点可以模拟spark-standalone.ec2角色。像大多数IAM角色一样，很难知道它的完整权限，但我们可以通过使用`mount`命令获得一些线索：
- en: '[PRE26]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'GP seems to use s3fs to locally mount an S3 bucket in */home/spark/notebooks*.
    We dig up the name of the bucket from the list of processes (using the `ps` command
    enriched with the `-edf` argument):'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: GP似乎使用s3fs在*/home/spark/notebooks*本地挂载了一个S3桶。我们通过查看进程列表（使用`ps`命令并加上`-edf`参数）挖掘出了桶的名称：
- en: '[PRE27]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Bingo. The bucket mapped to the *notebooks* folder is named gretsch-notebooks.
    Let’s load the role’s credentials and explore this bucket:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 成功了。映射到*notebooks*文件夹的桶名为gretsch-notebooks。让我们加载角色凭证并探索这个桶：
- en: '[PRE28]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Interesting indeed. The bucket contains files with *.ipynb* extensions, the
    hallmark of Python Jupyter notebooks. A Jupyter notebook is like a web-based Python
    command line interface (CLI) designed for data scientists to easily set up a working
    environment with the ability to graph charts and share their work. These notebooks
    can also be easily hooked to a Spark cluster to execute workloads on multiple
    machines.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 确实很有趣。这个桶包含扩展名为*.ipynb*的文件，这是Python Jupyter笔记本的标志。Jupyter笔记本就像是一个基于Web的Python命令行界面（CLI），旨在帮助数据科学家轻松设置工作环境，具备绘制图表和共享工作的能力。这些笔记本还可以轻松与Spark集群连接，实现在多个机器上执行工作负载。
- en: Data scientists need data to perform their calculations. Most would argue that
    they need production data to make accurate predictions. This data lives in places
    like databases and S3 buckets. It’s only natural, then, that these once-barren
    Jupyter notebooks quickly evolved into a warm pond teeming with hardcoded credentials
    as the scientists had the need for more and more datasets.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家需要数据来进行计算。大多数人会争辩说，他们需要生产数据来做出准确的预测。这些数据通常存储在像数据库和S3桶这样的地方。因此，这些曾经贫瘠的Jupyter笔记本迅速演变成了一个充满硬编码凭证的温暖池塘，因为科学家们需要越来越多的数据集。
- en: 'Let’s sync the whole bucket and begin to look for some AWS credentials. All
    AWS access key IDs start with the magic word `AKIA`, so we `grep` for that term:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们同步整个桶并开始寻找一些AWS凭证。所有的AWS访问密钥ID都以神奇的词`AKIA`开头，所以我们用`grep`来查找这个词：
- en: '[PRE29]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Well, how about that! We collect dozens of personal AWS credentials, probably
    belonging to the whole data department of Gretsch Politico.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 哇，真是了不起！我们收集到了几十个个人AWS凭证，可能属于Gretsch Politico整个数据部门。
- en: 'Let’s also search for occurrences of the common S3 drivers used in Spark, `s3a`
    and `s3n`, and uncover some precious S3 buckets regularly used to load data and
    conduct experiments:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也搜索一下在Spark中常用的S3驱动程序` s3a`和`s3n`的出现情况，揭开一些常用的S3存储桶，定期用于加载数据和进行实验：
- en: '[PRE30]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Look at that first bucket’s name: gretsch-finance 1. That ought to be fun.
    We’ll use one of the AWS keys we retrieved from the same notebook and unload the
    keys under *portfolio/exports/2020*:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 看看第一个存储桶的名称：gretsch-finance 1。这应该会很有趣。我们将使用从同一本笔记本中提取的AWS密钥之一，卸载位于*portfolio/exports/2020*下的密钥：
- en: '[PRE31]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Let’s sample a random file:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们取一个随机文件来查看：
- en: '[PRE32]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: That’s a list of clients, all right! We get not only current customers, but
    prospective ones as well. Details include when they were last approached, where,
    by whom, what the last service they purchased was, and how much they spent on
    the platform.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 没错，这是一份客户列表！我们不仅获得了现有客户的信息，还有潜在客户的详细资料，包括他们最后一次接触的时间、地点、接触人、购买的最后一项服务以及他们在平台上花费了多少。
- en: Using this data, GP could get valuable insights into its customers’ spending
    habits and maybe establish hidden relationships between various properties, such
    as a meeting spot and revenue—who knows, the possibilities are endless. If you
    reach out to a data mining company, you should expect to be part of the experiment
    as well. That’s only fair.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些数据，Gretsch Politico可以深入了解客户的消费习惯，也许还能揭示各种属性之间的潜在关系，例如一个会面地点和收入——谁知道呢，可能性是无穷的。如果你联系一家数据挖掘公司，你应该也做好成为实验一部分的准备。这是公平的。
- en: That’s one goal almost crossed off. We may be able to find more detailed information,
    but for now we have a solid list of potential and verified customers. We can google
    the political parties behind each line and weep for our illusory democracy.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 那几乎是一个目标已经完成。我们可能能找到更详细的信息，但目前我们已经有了一份潜在和经过验证的客户列表。我们可以通过Google搜索每一行背后的政党，并为我们虚幻的民主流泪。
- en: Finding Raw Data
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 寻找原始数据
- en: 'The gretsch-finance bucket proved to be a winner. Let’s check the rest of the
    buckets:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: gretsch-finance存储桶证明是一个成功的目标。让我们检查其余的存储桶：
- en: '[PRE33]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Profiles, social, segments, and so on. The filenames are endearing. This could
    very well be the user data we are after. Notice that the name of the gretsch-hadoop-us1
    bucket suggests a regionalized partitioning. How many regions, and therefore Hadoop
    buckets, are there?
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 配置文件、社交、细分等。文件名很有吸引力。这很可能就是我们要找的用户数据。注意，gretsch-hadoop-us1存储桶的名称暗示了区域化分区。到底有多少个区域，也就有多少个Hadoop存储桶？
- en: '[PRE34]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We find a Hadoop bucket for each of three AWS regions (Northern California,
    Ireland, and Singapore). We download 1,000 files from gretsch-hadoop-usw1 to see
    what kinds of artifacts it contains:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为每个三个AWS区域（北加州、爱尔兰和新加坡）找到了一个Hadoop存储桶。我们从gretsch-hadoop-usw1下载了1,000个文件，以查看它包含哪些类型的文件：
- en: '[PRE35]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We see some files with the extension *.parquet*. *Parquet* is a file format
    known for its high compression ratio, which is achieved by storing data in a columnar
    format. It leverages the accurate observation that, in most databases, a column
    tends to store data of the same type (for example, integers), while a row is more
    likely to store different types of data. Instead of grouping data by row, like
    most DB engines do, Parquet groups them by column, thus achieving over 95 percent
    compression ratios.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到一些扩展名为*.parquet*的文件。*Parquet*是一种以高压缩比著称的文件格式，其压缩效果通过以列式存储数据来实现。它利用了一个准确的观察：在大多数数据库中，一列往往存储相同类型的数据（例如，整数），而一行则更可能存储不同类型的数据。与大多数数据库引擎按行分组数据不同，Parquet按列分组数据，从而实现了超过95%的压缩比。
- en: 'We install the necessary tools to decompress and manipulate *.parquet* files
    and then open a few random files:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们安装了必要的工具来解压和操作*.parquet*文件，然后打开几个随机文件：
- en: '[PRE36]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: We retrieve user IDs, social profiles, interest segments, time spent on ads,
    geolocation, and other alarming information tracking user behavior. Now we have
    something to show for our efforts. The data is erratic, stored in a specialized
    format and hardly decipherable, but we will figure it out eventually.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们检索了用户ID、社交资料、兴趣细分、广告时间、地理位置和其他跟踪用户行为的令人震惊的信息。现在我们有了一些成果。数据是不稳定的，存储在专用格式中，几乎无法解读，但我们最终会搞清楚的。
- en: 'We could provision a few terabytes of storage on our machine and proceed to
    fully pilfer these three buckets. Instead, we just instruct AWS to copy the bucket
    to our own account, but it needs a bit of tweaking to increase the pace first:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在自己的机器上配置几个 TB 的存储空间，接着完全窃取这三个桶。相反，我们只是指示 AWS 将桶复制到我们自己的账户中，但首先需要稍作调整以加快速度：
- en: '[PRE37]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: We have all the data from the three Hadoop buckets. Don’t get too excited, though;
    this data is almost impossible to process without some hardcore exploration, business
    knowledge, and, of course, computing power. Let’s face it, we are way out of our
    league.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们拥有来自三个 Hadoop 桶的所有数据。不过，不要太激动；这些数据几乎不可能在没有大量探索、业务知识和当然的计算能力下处理。老实说，我们完全超出了自己的能力范围。
- en: Gretsch Politico does this kind of processing every day with its little army
    of data experts. Can’t we leverage their work to steal the end result instead
    of reinventing the wheel from scratch?
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Gretsch Politico 每天都由其数据专家小队进行这种处理。我们难道不能利用他们的工作，直接窃取最终结果，而不是从头开始重新发明轮子吗？
- en: Stealing Processed Data
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 偷窃处理过的数据
- en: Data processing and data transformation on Spark are usually only the first
    step of a data’s lifecycle. Once the data is enriched with other inputs, cross-referenced,
    formatted, and scaled out, it is stored on a second medium. There, it can be explored
    by analysts (usually through some SQL-like engine) and eventually fed to training
    algorithms and prediction models (which may or may not run on Spark, of course).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 上进行数据处理和数据转化通常只是数据生命周期的第一步。一旦数据与其他输入丰富、交叉引用、格式化并扩展后，它会被存储在第二介质上。在那里，分析师（通常通过某些类似
    SQL 的引擎）可以进行探索，最终数据会被输入到训练算法和预测模型中（这些算法和模型可能运行在 Spark 上，也可能不运行）。
- en: The question is, where does GP store its enriched and processed data? The quickest
    way to find out is to search the Jupyter notebooks for hints of analytical tool
    mentions, SQL-like queries, graphs and dashboards, and the like (see [Listing
    12-9](#listing12-9)).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 问题是，GP 将其丰富和处理过的数据存储在哪里？最快的方式是搜索 Jupyter 笔记本，查找有关分析工具的提示、SQL 类查询、图表和仪表盘等内容（参见[列表
    12-9](#listing12-9)）。
- en: '[PRE38]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Listing 12-9: SQL queries used in Jupyter notebooks'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12-9：Jupyter 笔记本中使用的 SQL 查询
- en: Maybe we have found something worth investigating. Redshift is a managed PostgreSQL
    database on steroids, so much so that it is no longer appropriate to call it a
    database. It is often referred to as a *data lake*. It’s almost useless for querying
    a small table of 1,000 lines, but give it a few terabytes of data to ingest and
    it will respond with lightning speed! Its capacity can scale up as long as AWS
    has free servers (and the client has cash to spend, of course).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 也许我们发现了一些值得调查的东西。Redshift 是一个经过强化的 PostgreSQL 管理数据库，以至于它已经不再适合称其为数据库。它通常被称为
    *数据湖*。对于查询一个 1,000 行的小表几乎没什么用，但给它几 TB 的数据来摄取，它就能以闪电般的速度响应！它的容量可以随 AWS 的空闲服务器扩展（当然，客户也得有钱花）。
- en: Its notable speed, scalability, parallel upload capabilities, and integration
    with the AWS ecosystem position Redshift as one of the most efficient analytical
    databases in the field—and it’s probably the key to our salvation!
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Redshift 以其显著的速度、可扩展性、并行上传能力以及与 AWS 生态系统的集成，成为该领域最有效的分析数据库之一——它可能是我们救赎的关键！
- en: 'Unfortunately, the credentials we retrieved belong to a sandbox database with
    irrelevant data. Furthermore, none of our AWS access keys can directly query the
    Redshift API:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我们获取的凭证属于一个包含无关数据的沙箱数据库。而且，我们的 AWS 访问密钥都不能直接查询 Redshift API：
- en: '[PRE39]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Time for some privilege escalation, it seems.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 看来是时候进行一些权限提升了。
- en: Privilege Escalation
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 权限提升
- en: 'Going through the dozen IAM access keys we got, we realize that all of them
    belong to the same IAM group and thus share the same basic privileges—that is,
    read/write to a few buckets coupled with some light read-only IAM permissions:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检查我们获得的十二个 IAM 访问密钥，我们意识到它们都属于同一个 IAM 组，因此共享相同的基本权限——也就是，读取/写入一些桶，并附带一些轻量的只读
    IAM 权限：
- en: '[PRE40]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Hold on. Camellia belongs to an additional group called *spark-debug*. Let’s
    take a closer look at the policies attached to this group:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 等一下。Camellia 属于一个名为 *spark-debug* 的附加组。让我们仔细看看这个组所附加的策略：
- en: '[PRE41]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Lovely. Camellia here is probably the person in charge of maintaining and running
    Spark clusters, hence the two policies she’s granted. EC2 full access opens the
    door to more than 450 possible actions on EC2, from starting instances to creating
    new VPCs, subnets, and pretty much anything related to the compute service.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了。Camellia在这里可能是负责维护和运行Spark集群的人，因此她被授予了这两个策略。EC2完全访问权限为她打开了450多种EC2操作的可能性，从启动实例到创建新的VPC、子网，几乎涵盖了与计算服务相关的所有操作。
- en: 'The second policy is custom-made, but we can easily guess what it implies:
    it allows us to assign roles to EC2 instances. We query the latest version of
    the policy document to assert our guess:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个策略是定制的，但我们可以轻松猜测它意味着什么：它允许我们将角色分配给EC2实例。我们查询最新版本的策略文档来确认我们的猜测：
- en: '[PRE42]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: GP may not fully realize it, but with the IAM `PassRole` action, they have implicitly
    given dear Camellia—and, by extension, *us*—total control over their AWS account.
    `PassRole` is a powerful permission that allows us to assign a role to an instance.
    Any role 1. Even an admin one. With `EC2 full access`, Camellia also manages EC2
    instances and can start a machine, stamp it with an admin role, and take over
    the AWS account.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: GP可能没有完全意识到，但通过IAM的`PassRole`操作，他们已经隐性地赋予亲爱的Camellia——以及通过她，*我们*——对他们的AWS账户完全的控制权。`PassRole`是一个强大的权限，允许我们将角色分配给实例。任何角色1，甚至是管理员角色。凭借`EC2完全访问`，Camellia还可以管理EC2实例，启动机器，给它加上管理员角色，然后接管AWS账户。
- en: 'Let’s explore our options in terms of which roles we, as Camellia, can pass
    to an EC2 instance. The only constraint is that the role needs to have *ec2.amazonaws.com*
    in its trust policy:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探讨一下作为Camellia的我们可以传递给EC2实例的角色选项。唯一的限制是该角色需要在其信任策略中包含*ec2.amazonaws.com*：
- en: '[PRE43]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Among the roles we see rundeck, which may just be our promised savior. Rundeck
    is an automation tool for running admin scripts on the infrastructure. GP’s infrastructure
    team did not seem too keen on using Jenkins, so they probably scheduled the bulk
    of their workload on Rundeck. Let’s use Camellia to see what permissions rundeck
    has:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些角色中，我们看到了rundeck，这可能就是我们期待的救世主。Rundeck是一个自动化工具，用于在基础设施上运行管理员脚本。GP的基础设施团队似乎并不热衷于使用Jenkins，因此他们可能将大部分工作负载调度到了Rundeck上。让我们使用Camellia来查看rundeck拥有哪些权限：
- en: '[PRE44]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Yes, that’s the role we need. The rundeck role has close to full admin privileges
    over AWS.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，这就是我们需要的角色。rundeck角色几乎拥有对AWS的完全管理员权限。
- en: 'The plan, therefore, is to spin up an instance in the same subnet as the Spark
    cluster. We carefully reproduce the same attributes to hide in plain sight: security
    groups, tags, everything. We’re finding the attributes so we can later imitate
    them:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，计划是在与Spark集群相同的子网中启动一个实例。我们小心地复制相同的属性，以便在明面上隐藏：安全组、标签，所有内容。我们正在查找这些属性，以便稍后模仿它们：
- en: '[PRE45]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We know for a fact that Spark workers can reach the internet over port 443,
    so we just lazily copy and paste the security groups we just confirmed and launch
    a new instance with the rundeck profile with those attributes:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们确切知道Spark工作节点可以通过443端口访问互联网，因此我们懒得重新验证刚刚确认的安全组，直接复制并粘贴这些安全组，并使用rundeck配置文件启动一个新实例：
- en: '[PRE46]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The script passed as user data (*my_user_data.sh*) will bootstrap our reverse
    shell:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 作为用户数据传递的脚本（*my_user_data.sh*）将启动我们的反向Shell：
- en: '[PRE47]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We run the preceding AWS command and, sure enough, a minute or two later we
    get what we hope will be our last shell, along with admin privileges:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们运行前面的AWS命令，果然，过了一两分钟后，我们得到了我们希望的最后一个Shell，以及管理员权限：
- en: '[PRE48]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Brilliant! We get a bunch of top-security-level keys and tokens belonging to
    the rundeck role. Now that we have these keys, let’s query the classic services
    that may expose, to see what’s active (CloudTrail, GuardDuty, and Access Analyzer):'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！我们得到了属于rundeck角色的一堆顶级安全密钥和令牌。现在我们有了这些密钥，让我们查询可能暴露的经典服务，看看哪些是活跃的（CloudTrail、GuardDuty和Access
    Analyzer）：
- en: '[PRE49]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: All right. CloudTrail is enabled as expected, so logs could be an issue. No
    big surprises there. Insights is disabled 1, though, so we can afford some bulk-write
    API calls if need be. GuardDuty and Access Analyzer return empty lists, so are
    both absent from the mix as well.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，CloudTrail按预期启用，因此日志可能成为一个问题。没有太大意外。尽管如此，Insights被禁用了1，所以如果需要的话，我们可以进行一些批量写入的API调用。GuardDuty和Access
    Analyzer返回空列表，因此它们在这个组合中也缺席。
- en: 'Let’s temporarily blind the log trail and slip an access key into Camellia’s
    user account to improve our persistence. Her privileges are quite enough should
    we want to regain access to GP’s account:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们暂时盲目地隐藏日志轨迹，并向Camellia的用户账户中插入一个访问密钥，以增强我们的持久性。如果我们想重新获得对GP账户的访问，她的权限完全足够：
- en: '[PRE50]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Thirty minutes later, we clean up the EC2 instance and re-enable CloudTrail
    multiregion logging:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 三十分钟后，我们清理了EC2实例并重新启用了CloudTrail多区域日志记录：
- en: '[PRE51]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Finally! We gained stable admin access to GP’s AWS account.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 终于！我们获得了稳定的管理员访问权限，进入了GP的AWS账户。
- en: Infiltrating Redshift
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 渗透Redshift
- en: Now that we have secured access to GP’s AWS account, let’s poke around its Redshift
    clusters (see [Listing 12-10](#listing12-10)). That was our primary incentive
    to take over the account, after all.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经获得了GP的AWS账户访问权限，让我们探索它的Redshift集群（见[Listing 12-10](#listing12-10)）。毕竟，这就是我们接管该账户的主要动机。
- en: '[PRE52]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Listing 12-10: Listing the Redshift clusters'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 'Listing 12-10: 列出Redshift集群'
- en: We get a bunch of clusters running on Redshift, with valuable info. Redshift
    was a good guess. You don’t spawn an ra3.16xlarge cluster 1 that supports 2.5TB
    per node just for the heck of it. That baby must easily cost north of $3,000 a
    day, which makes it all the more tempting to explore. The finance cluster may
    also hold some interesting data.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在Redshift上运行了一些集群，里面有有价值的信息。Redshift是一个不错的选择。你不会仅仅为了随便测试而创建一个支持每个节点2.5TB的ra3.16xlarge集群1。这个集群每天的费用肯定超过$3,000，这也让探索它变得更加诱人。金融集群也可能包含一些有趣的数据。
- en: 'Let’s zoom in on the information of the bi cluster in [Listing 12-10](#listing12-10).
    The initial database created when the cluster came to life is called `datalake`.
    The admin user is the traditional root user. The cluster is reachable at the address
    *bi.cae0svj50m2p.eu-west-1.redshift.amazonaws.com* on port 5439:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们聚焦于[Listing 12-10](#listing12-10)中bi集群的信息。当集群启动时创建的初始数据库叫做`datalake`。管理员用户是传统的root用户。集群可通过地址*bi.cae0svj50m2p.eu-west-1.redshift.amazonaws.com*在5439端口访问：
- en: '[PRE53]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We take a look at the security groups for possible filtering rules preventing
    direct connections to the database:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们查看安全组，以便检查是否有过滤规则阻止直接连接到数据库：
- en: '[PRE54]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'My favorite IP range of all time: 0.0.0.0/0\. This unfiltered IP range was
    probably just used as temporary access granted to test a new SaaS integration
    or to run some queries. . . yet here we are. To be fair, since we already have
    access to GP’s network, this doesn’t matter to us much. The damage is already
    done.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我最喜欢的IP范围：0.0.0.0/0。这种未过滤的IP范围可能仅仅是用于测试新的SaaS集成或运行一些查询时临时赋予的访问权限……但现在我们已经进入了。公平地说，既然我们已经能够访问GP的网络，这对我们来说并不重要。损害已经发生。
- en: 'Redshift is so tightly coupled with the IAM service that we do not need to
    go hunting for credentials for the database. Since we have a beautiful `redshift:*`
    permission attached to our rundeck role, we just create a temporary password for
    any user account on the database (root included):'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: Redshift与IAM服务紧密结合，我们不需要去寻找数据库的凭证。由于我们在rundeck角色上有一个漂亮的`redshift:*`权限，我们只需为任何数据库用户账户（包括root）创建一个临时密码：
- en: '[PRE55]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'With these database credentials, it’s just a matter of downloading the PostgreSQL
    client and pointing it to the Redshift endpoint:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些数据库凭证，我们只需下载PostgreSQL客户端并将其指向Redshift端点：
- en: '[PRE56]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'We export a comprehensive list of tables and columns (stored in the `PG_TABLE_DEF`
    table) and quickly close in on the interesting data:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们导出了包含表和列的全面列表（存储在`PG_TABLE_DEF`表中），并迅速锁定了有趣的数据：
- en: '[PRE57]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Nothing beats a good old-fashioned SQL database where we can query and join
    data to our hearts’ content! This Redshift cluster is the junction of almost every
    data input poured into Gretsch Politico’s infrastructure.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 没有什么比得上一款老式的SQL数据库，能让我们随心所欲地查询和连接数据！这个Redshift集群几乎是Gretsch Politico基础设施中所有数据输入的交汇点。
- en: We find data related to MXR Ads’ performance and the impact it had on people’s
    behavior online. We have their full online activity, including a list of every
    website they visited that had a JavaScript tag related to GP, and even social
    media profiles tied to the people naïve enough to share such data with one of
    GP’s hidden partners. Then, of course, we have the classic data segments bought
    from data providers and what they call “lookalike segments”—that is, interests
    of population A projected over population B because they share some common properties,
    like the device they use, their behavior, and so on.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们找到了与MXR广告的表现以及它对人们在线行为影响相关的数据。我们有他们的完整在线活动，包括他们访问的每个有与GP相关的JavaScript标签的网站的列表，甚至还有那些天真到愿意与GP隐藏合作伙伴共享这些数据的人的社交媒体档案。然后，当然，我们也有从数据提供商那里购买的经典数据分段，以及他们所称的“相似用户群体”——即，A人群的兴趣投射到B人群上，因为他们有一些共同的特征，比如使用的设备、行为等等。
- en: 'We try building a SQL query that compiles most of this data into a single output
    to get a clearer visualization of what is going on:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尝试构建一个 SQL 查询，将大部分数据汇总到一个输出中，以便更清晰地可视化当前的情况：
- en: '[PRE58]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Drum roll, please. Ready? Go! Here’s one customer, Francis Dima:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 请鼓声雷动，准备好了吗？开始！这是一个客户，弗朗西斯·迪马（Francis Dima）：
- en: '[PRE59]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: The things you can learn about people by aggregating a few trackers. Poor Dima
    is tied to more than 160 data segments describing everything from his political
    activities to his cooking habits and medical history. We have the last 500 full
    URLs he visited, his last known location, his Facebook profile full of his likes
    and interests, and, most importantly, a character map enumerating his level of
    influence, impulse, and ad interaction. With this information, just think how
    easy it will be for GP to target this person—any person—to influence their opinion
    about any number of polarizing subjects . . . and, well, to sell democracy to
    the highest bidder.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 通过聚合几个追踪器，你可以了解到关于人们的许多事情。可怜的迪马（Dima）被绑定到超过 160 个数据段，涵盖从他的政治活动到烹饪习惯和医疗历史的所有信息。我们有他访问过的最后
    500 个完整 URL，他最后已知的位置，他的 Facebook 资料，充满了他的兴趣和爱好，最重要的是，一个列出他影响力、冲动和广告互动水平的角色地图。有了这些信息，想想看，GP
    要针对这个人——任何人——以影响他们对任何数量的极化话题的看法，**以及**，嗯，向出价最高者出售民主是多么容易。
- en: 'The finance cluster is another living El Dorado. More than just transactional
    data, it contains every bit of information possible on every customer who has
    expressed the slightest interest in Gretsch Politico’s services, along with the
    creatives they ordered:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 财务集群是另一个活生生的黄金国。不仅仅是交易数据，它包含了所有可能的每个客户的信息，任何曾对 Gretsch Politico 的服务表现出丝毫兴趣的人，以及他们订购的创意：
- en: '[PRE60]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: We export these two clusters in their entirety to an S3 bucket we own and start
    preparing our next move—a press conference, a movie, maybe a book. Who knows?
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这两个集群完整地导出到我们拥有的 S3 存储桶，并开始准备我们的下一步行动——新闻发布会、电影，或许是一本书。谁知道呢？
- en: Resources
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: 'A list of companies relying on Spark: [https://spark.apache.org/powered-by.html](https://spark.apache.org/powered-by.html).'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 依赖于 Spark 的公司列表：[https://spark.apache.org/powered-by.html](https://spark.apache.org/powered-by.html)。
- en: 'A list of Spark actions, from the Apache Spark documentation: [http://bit.ly/3aW64Dh](http://bit.ly/3aW64Dh).'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自 Apache Spark 文档的 Spark 操作列表：[http://bit.ly/3aW64Dh](http://bit.ly/3aW64Dh)。
- en: 'Redshift pricing details: [https://aws.amazon.com/redshift/pricing/](https://aws.amazon.com/redshift/pricing/).'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Redshift 定价详情：[https://aws.amazon.com/redshift/pricing/](https://aws.amazon.com/redshift/pricing/)。
- en: 'More details on `map` and `FlatMap`, with illustrations: [https://data-flair.training/blogs/apache-spark-map-vs-flatmap/](https://data-flair.training/blogs/apache-spark-map-vs-flatmap/).'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于 `map` 和 `FlatMap` 的更多细节，附带插图：[https://data-flair.training/blogs/apache-spark-map-vs-flatmap/](https://data-flair.training/blogs/apache-spark-map-vs-flatmap/)。
