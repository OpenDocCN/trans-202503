- en: '**11**'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**11**'
- en: BUILDING A NEURAL NETWORK MALWARE DETECTOR WITH KERAS
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Keras 构建一个神经网络恶意软件检测器
- en: '![image](../images/common01.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/common01.jpg)'
- en: A decade ago, building a functioning, scalable, and fast neural network was
    time consuming and required quite a lot of code. In the past few years, however,
    this process has become far less painful, as more and more high-level interfaces
    to neural network design have been developed. The Python package `Keras` is one
    of these interfaces.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 十年前，构建一个功能完善、可扩展且快速的神经网络既费时又需要大量代码。然而，在过去的几年里，这一过程变得不那么痛苦了，因为越来越多的高级神经网络设计接口已经开发出来。Python
    包 `Keras` 就是其中之一。
- en: In this chapter, I walk you through how to build a sample neural network using
    the `Keras` package. First, I explain how to define a model’s architecture in
    `Keras`. Second, we train this model to differentiate between benign and malicious
    HTML files, and you learn how to save and load such models. Third, using the Python
    package `sklearn`, you learn how to evaluate the model’s accuracy on validation
    data. Finally, we use what you’ve learned to integrate validation accuracy reporting
    into the model training process.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我将带你通过使用 `Keras` 包构建一个示例神经网络。首先，我会解释如何在 `Keras` 中定义模型架构。其次，我们将训练这个模型来区分良性和恶意的
    HTML 文件，你将学习如何保存和加载这些模型。第三，使用 Python 包 `sklearn`，你将学习如何评估模型在验证数据上的准确性。最后，我们将用你所学的知识将验证准确性报告集成到模型训练过程中。
- en: I encourage you to read this chapter while reading and editing the associated
    code in the data accompanying this book. You can find all the code discussed in
    this chapter there (organized into parameterized functions to make things easier
    to run and adjust), as well as a few extra examples. By the end of this chapter,
    you’ll feel ready to start building some networks of your own!
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我鼓励你在阅读本章的同时，阅读并编辑附带本书的数据中的相关代码。你可以在那里找到本章讨论的所有代码（已经组织成参数化函数，便于运行和调整），以及一些额外的示例。到本章结束时，你将感到准备好开始构建自己的神经网络了！
- en: 'To run code listings in this chapter, you not only need to install the packages
    listed in this chapter’s *ch11/requirements.txt* file (`pip install –r requirements.txt`),
    but also follow the directions to install one of `Keras`’s backend engines on
    your system (TensorFlow, Theano, or CNTK). Install TensorFlow by following the
    directions here: *[https://www.tensorflow.org/install/](https://www.tensorflow.org/install/)*.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行本章中的代码示例，你不仅需要安装本章中 *ch11/requirements.txt* 文件中列出的包（`pip install –r requirements.txt`），还需要按照指示在系统中安装
    `Keras` 的后端引擎（TensorFlow、Theano 或 CNTK）。按照这里的说明安装 TensorFlow： *[https://www.tensorflow.org/install/](https://www.tensorflow.org/install/)*。
- en: '**Defining a Model’s Architecture**'
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**定义模型架构**'
- en: 'To build a neural network, you need to define its architecture: which neurons
    go where, how they connect to subsequent neurons, and how data flows through the
    whole thing. Luckily, `Keras` provides a simple, flexible interface to define
    all this. `Keras` actually supports two similar syntaxes for model definition,
    but we’re going to use the Functional API syntax, as it’s more flexible and powerful
    that the other (“sequential”) syntax.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建一个神经网络，你需要定义它的架构：哪些神经元去哪里，它们如何连接到后续的神经元，以及数据如何在整个网络中流动。幸运的是，`Keras` 提供了一个简单、灵活的接口来定义这一切。`Keras`
    实际上支持两种相似的模型定义语法，但我们将使用功能API语法，因为它比其他（“顺序”）语法更加灵活和强大。
- en: 'When designing a model, you need three things: input, stuff in the middle that
    processes the input, and output. Sometimes your models will have multiple inputs,
    multiple outputs, and very complex stuff in the middle, but the basic idea is
    that when defining a model’s architecture, you’re just defining how the input—your
    data, such as features relating to an HTML file—flows through various neurons
    (stuff in the middle), until finally the last neurons end up yielding some output.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计模型时，你需要三样东西：输入、中间的处理部分以及输出。有时，你的模型会有多个输入、多个输出，并且中间的部分非常复杂，但基本的思路是，当定义一个模型的架构时，你只是定义了输入——你的数据，比如与HTML文件相关的特征——是如何通过各个神经元（即中间部分）流动的，直到最后的神经元输出某些结果。
- en: To define this architecture, `Keras` uses layers. A *layer* is a group of neurons
    that all use the same type of activation function, all receive data from a previous
    layer, and all send their outputs to a subsequent layer of neurons. In a neural
    network, input data is generally fed to an initial layer of neurons, which sends
    its outputs to a subsequent layer, which sends its outputs to another layer, and
    so on and so forth, until the last layer of neurons generates the network’s final
    output.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了定义这个架构，`Keras`使用层。*层*是一组神经元，这些神经元使用相同的激活函数，所有神经元接收来自上一层的数据，并将它们的输出发送到后一层神经元。在神经网络中，输入数据通常会被送入初始层的神经元，这些神经元将其输出传递给后续的层，然后继续传递，直到最后一层神经元生成网络的最终输出。
- en: '[Listing 11-1](ch11.xhtml#ch11list1) is an example of a simple model defined
    using `Keras`’s functional API syntax. I encourage you to open a new Python file
    to write and run the code yourself as we walk through the code, line by line.
    Alternatively, you can try running the associated code in the data accompanying
    this book, either by copying and pasting parts of the *ch11/model_architecture.py*
    file into an ipython session or by running `python` `ch11``/model_architecture.py`
    in a terminal window.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 11-1](ch11.xhtml#ch11list1)是一个使用`Keras`的函数式API语法定义的简单模型示例。我鼓励你打开一个新的Python文件，跟着我们逐行讲解代码，同时自己动手编写和运行代码。或者，你也可以尝试运行本书附带的数据中的相关代码，可以通过将*ch11/model_architecture.py*文件的部分内容复制并粘贴到ipython会话中，或者在终端窗口中运行`python
    ch11/model_architecture.py`来尝试。'
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*Listing 11-1: Defining a simple model using functional API syntax*'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 11-1：使用函数式API语法定义简单模型*'
- en: First, we import the `Keras` package’s `layers` submodule ➊ as well as the `Model`
    class from `Keras`’s `models` submodule ➋.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入`Keras`包的`layers`子模块➊，以及`Keras`的`models`子模块中的`Model`类➋。
- en: Next, we specify what kind of data this model will accept for one observation
    by passing a `shape` value (a tuple of integers) ➌ and a data type (string) ➍
    to the `layers.Input()` function. Here, we declared that the input data to our
    model will be an array of 1,024 floats. If our input was, for example, a matrix
    of integers instead, the first line would look more like `input = Input(shape=(100,
    100,) dtype='int32')`.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过将一个`shape`值（一个整数元组）➌和数据类型（字符串）➍传递给`layers.Input()`函数来指定此模型将接受的单个观测数据的类型。在这里，我们声明输入数据将是一个包含1,024个浮点数的数组。如果我们的输入是整数矩阵，第一行代码将更像是`input
    = Input(shape=(100, 100,) dtype='int32')`。
- en: '**NOTE**'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*If the model takes in variable-sized inputs on one dimension, you can use*
    None *instead of a number—for example,* (100, None,)*.*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果模型在一个维度上接受可变大小的输入，可以使用* None *代替数字——例如，*(100, None,)*。*'
- en: Next, we specify the layer of neurons that this input data will be sent to.
    To do this, we again use the `layers` submodule we imported, specifically the
    `Dense` function ➎, to specify that this layer will be a densely connected (also
    called fully connected) layer, which means that every output from the previous
    layer is sent to every neuron in this layer. `Dense` is the most common type of
    layer you’ll likely use when developing `Keras` models. Others allow you to do
    things like change the shape of the data (`Reshape`) and implement your own custom
    layer (`Lambda`).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们指定该输入数据将被发送到的神经网络层。为此，我们再次使用导入的`layers`子模块，特别是`Dense`函数➎，来指定这一层将是一个密集连接层（也称为全连接层），这意味着前一层的每个输出都会发送到这一层的每个神经元。`Dense`是你在开发`Keras`模型时最常用的层类型。其他层则允许你执行一些操作，例如更改数据的形状（`Reshape`）或实现自定义层（`Lambda`）。
- en: 'We pass the `Dense` function two arguments: `units=512`, to specify that we
    want 512 neurons in this layer, and `activation=''relu''`, to specify that we
    want these neurons to be rectified linear unit (ReLU) neurons. (Recall from [Chapter
    10](ch10.xhtml#ch10) that ReLU neurons use a simple type of activation function
    that outputs whichever is larger: either 0, or the weighted sum of the neuron’s
    inputs.) We use `layers.Dense(units=512, activation=''relu'')` to define the layer,
    and then the last part of the line—`(input)`—declares the input to this layer
    (namely, our `input` object). It’s important to understand that this passing of
    `input` to our layer is how data flow is defined in the model, as opposed to the
    ordering of the lines of the code.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将两个参数传递给 `Dense` 函数：`units=512`，表示我们希望这一层有 512 个神经元，以及 `activation='relu'`，表示我们希望这些神经元使用修正线性单元（ReLU）激活函数。（回想一下
    [第10章](ch10.xhtml#ch10)，ReLU 神经元使用一种简单的激活函数，输出两者中较大的值：要么是 0，要么是神经元输入的加权和。）我们使用
    `layers.Dense(units=512, activation='relu')` 来定义这一层，然后行尾的 `（input）` 表示该层的输入（即我们的
    `input` 对象）。理解这一点很重要，因为传递 `input` 给我们的层就是定义模型中数据流的方式，而不是代码行的顺序。
- en: In the next line, we define our model’s output layer, which again uses the `Dense`
    function. But this time, we designate only a single neuron to the layer and use
    a `'sigmoid'` activation function ➏, which is great for combining a lot of data
    into a single score between 0 and 1\. The output layer takes the `(middle)` object
    as input, declaring that the outputs from our 512 neurons in our `middle` layer
    should all be sent to this neuron.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一行，我们定义了模型的输出层，同样使用了 `Dense` 函数。但这次，我们为该层指定了一个神经元，并使用 `'sigmoid'` 激活函数 ➏，该激活函数非常适合将大量数据合并成一个介于
    0 和 1 之间的单一评分。输出层将 `(middle)` 对象作为输入，声明我们在 `middle` 层的 512 个神经元的输出都将发送到这个神经元。
- en: Now that we’ve defined our layers, we use the `Model` class from the `models`
    submodule to wrap up all these layers together as a model ➐. Note that you *only*
    have to specify your input layer(s) and output layer(s). Because each layer after
    the first is given the preceding layer as input, the final output layer contains
    all the information the model needs about the previous layers. We could have 10
    more `middle` layers declared between our `input` and `output` layers, but the
    line of code at ➐ would remain the same.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了各层，使用 `models` 子模块中的 `Model` 类将所有这些层组合成一个模型 ➐。注意，你*只*需要指定输入层和输出层。因为每一层的输入来自前一层，所以最终的输出层包含了模型所需的所有前一层的信息。我们可以在
    `input` 和 `output` 层之间再声明 10 个 `middle` 层，但在 ➐ 处的代码行将保持不变。
- en: '**Compiling the Model**'
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**编译模型**'
- en: 'Finally, we need to compile our model. We’ve defined the model’s architecture
    and flow of data, but we haven’t yet specified how we want the model to perform
    its training. To do this, we use our `model`’s own `compile` method and pass it
    three parameters:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要编译模型。我们已经定义了模型的架构和数据流，但还没有指定如何执行训练。为此，我们使用模型的 `compile` 方法，并传入三个参数：
- en: The first parameter, `optimizer` ➑, specifies the type of backpropagation algorithm
    to use. You can specify the name of the algorithm you wish to use via a character
    string like we did here, or you can import an algorithm directly from `keras.optimizers`
    to pass in specific parameters to the algorithm or even design your own.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个参数 `optimizer` ➑ 指定了要使用的反向传播算法类型。你可以通过字符字符串指定想要使用的算法名称，如我们这里所做的，或者直接从 `keras.optimizers`
    导入一个算法，传入特定的参数，甚至设计你自己的算法。
- en: The `loss` parameter ➒ specifies the thing that is minimized during the training
    process (backpropagation). Specifically, this specifies the formula you wish to
    use to represent the difference between your true training labels and your model’s
    predicted labels (output). Again, you can specify the name of a loss function,
    or pass in an actual function, like `keras.losses.mean_squared_error`.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` 参数 ➒ 指定了在训练过程中（反向传播）要最小化的目标。具体来说，它指定了你希望用来表示真实训练标签和模型预测标签（输出）之间差异的公式。同样，你可以指定一个损失函数的名称，或者直接传入一个实际的函数，例如
    `keras.losses.mean_squared_error`。'
- en: Lastly, for the `metrics` parameter ➓, you can pass a list of metrics that you
    want `Keras` to report when analyzing model performance during and after training.
    Again, you can pass strings or actual metric functions, like `['categorical_accuracy',
    keras.metrics.top_k_categorical_accuracy]`.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，对于`metrics`参数 ➓，你可以传递一个包含你希望`Keras`在训练期间和训练后分析模型性能时报告的度量列表。例如，你可以传递字符串或实际的度量函数，比如`['categorical_accuracy',
    keras.metrics.top_k_categorical_accuracy]`。
- en: After running the code in [Listing 11-1](ch11.xhtml#ch11list1), run `model.summary()`
    to see the model structure printed to your screen. Your output should look something
    like [Figure 11-1](ch11.xhtml#ch11fig1).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行[列表 11-1](ch11.xhtml#ch11list1)中的代码后，运行`model.summary()`以查看模型结构输出到你的屏幕上。你的输出应当类似于[图
    11-1](ch11.xhtml#ch11fig1)。
- en: '![image](../images/f0202-01.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0202-01.jpg)'
- en: '*Figure 11-1: Output of* model.summary()'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-1：* `model.summary()` *的输出*'
- en: '[Figure 11-1](ch11.xhtml#ch11fig1) shows the output of `model.summary()`. Each
    layer’s description is printed to the screen, along with the number of parameters
    associated with that layer. For example, the `dense_1` layer has 524,800 parameters
    because each of its 512 neurons gets a copy of each of the 1,024 input values
    from the input layer, meaning that there are 1,024 × 512 weights. Add 512 bias
    parameters, and you get 1,024 × 512 + 512 = 524,800.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11-1](ch11.xhtml#ch11fig1)展示了`model.summary()`的输出。每一层的描述都会打印到屏幕上，连同与该层相关的参数数量。例如，`dense_1`层有524,800个参数，因为它的每个512个神经元都从输入层获取每一个1,024个输入值的副本，这意味着有1,024
    × 512个权重。再加上512个偏置参数，得到1,024 × 512 + 512 = 524,800。'
- en: Although we haven’t yet trained our model or tested it on validation data, this
    is a compiled `Keras` model that is ready to train!
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们还没有训练我们的模型或在验证数据上测试它，但这是一个已编译的`Keras`模型，准备好进行训练！
- en: '**NOTE**'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*Check out the sample code in* ch11/model_architecture.py *for an example of
    a slightly more complex model!*'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*查看ch11/model_architecture.py中的示例代码，了解一个稍微复杂一点的模型示例！*'
- en: '**Training the Model**'
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**训练模型**'
- en: To train our model, we need training data. The virtual machine that comes with
    this book includes a set of about half a million benign and malicious HTML files.
    This consists of two folders of benign (*ch11/data/html/benign_files/*) and malicious
    (*ch11/data/html/malicious_files/*) HTML files. (Remember not to open these files
    in a browser!) In this section, we use these to train our neural network to predict
    whether an HTML file is benign (0) or malicious (1).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练我们的模型，我们需要训练数据。本书随附的虚拟机包含约50万份良性和恶意HTML文件的数据。这些数据包括两个文件夹，分别是良性(*ch11/data/html/benign_files/*)和恶意(*ch11/data/html/malicious_files/*)HTML文件。（记住不要在浏览器中打开这些文件！）在本节中，我们使用这些数据训练神经网络，预测HTML文件是良性（0）还是恶意（1）。
- en: '***Extracting Features***'
  id: totrans-36
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***特征提取***'
- en: To do this, we first need to decide how to represent our data. In other words,
    what features do we want to extract from each HTML file to use as input to our
    model? For example, we could simply pass the first 1,000 characters in each HTML
    file to the model, we could pass in the frequency counts of all letters in the
    alphabet, or we could use an HTML parser to develop some more complex features.
    To make things easier, we’ll transform each variable-length, potentially very
    large HTML file into a uniformly sized, compressed representation that allows
    our model to quickly process and learn important patterns.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们首先需要决定如何表示我们的数据。换句话说，我们想从每个HTML文件中提取哪些特征作为模型的输入？例如，我们可以简单地将每个HTML文件的前1,000个字符传递给模型，或者传递字母表中所有字母的频率计数，或者我们可以使用HTML解析器来开发一些更复杂的特征。为了简化操作，我们将每个可变长度、可能非常大的HTML文件转换为一个统一大小的压缩表示，以便我们的模型能够快速处理并学习重要的模式。
- en: In this example, we transform each HTML file into a 1,024-length vector of category
    counts, where each category count represents the number of tokens in the HTML
    file whose hash resolved to the given category. [Listing 11-2](ch11.xhtml#ch11list2)
    shows the feature extraction code.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将每个HTML文件转换为一个长度为1,024的类别计数向量，其中每个类别计数表示HTML文件中哈希值解析为给定类别的标记数量。[列表
    11-2](ch11.xhtml#ch11list2)展示了特征提取代码。
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '*Listing 11-2: Feature extraction code*'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 11-2：特征提取代码*'
- en: You don’t have to understand all the details of this code to understand how
    `Keras` works, but I encourage you to read through the comments in the code to
    better understand what’s going on.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 你不必理解这段代码的所有细节来理解`Keras`的工作原理，但我鼓励你阅读代码中的注释，更好地理解代码的执行过程。
- en: The `extract_features` function starts by reading in an HTML file as a big string
    ➋ and then splits up this string into a set of tokens based on a regular expression
    ➌. Next, the numeric hash of each token is taken, and these hashes are divided
    into categories by taking the modulo of each hash ➍. The final set of features
    is the number of hashes in each category ➎, like a histogram bin count. If you
    want, you can try altering the regular expression `split_regex` ➊ that splits
    up the HTML file into chunks to see how it affects the resulting tokens and features.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`extract_features` 函数首先将一个 HTML 文件作为一个大字符串读取 ➋，然后根据正则表达式 ➌ 将这个字符串拆分成一组标记。接着，取每个标记的数字哈希值，并通过对每个哈希值取模
    ➍ 将这些哈希值分为不同的类别。最终的特征集是每个类别中的哈希数量 ➎，类似于直方图的箱数。如果你愿意，可以尝试更改正则表达式 `split_regex`
    ➊，该表达式将 HTML 文件拆分成多个块，看看它如何影响结果的标记和特征。'
- en: 'If you skipped or didn’t understand all that, that’s okay: just know that our
    `extract_features` function takes the path to an HTML file as input and then transforms
    it into a feature array of length 1,024, or whatever `hash_dim` is.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你跳过了或者没有理解所有内容，也没关系：只需知道我们的 `extract_features` 函数接受一个 HTML 文件的路径作为输入，然后将其转换为一个长度为
    1,024 的特征数组，或者任何 `hash_dim` 的值。
- en: '***Creating a Data Generator***'
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***创建数据生成器***'
- en: Now we need to make our `Keras` model actually train on these features. When
    working with small amounts of data already loaded into memory, you can use a simple
    line of code like [Listing 11-3](ch11.xhtml#ch11list3) to train your model in
    `Keras`.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要让我们的 `Keras` 模型真正基于这些特征进行训练。当数据量较少且已经加载到内存中时，你可以使用类似于 [Listing 11-3](ch11.xhtml#ch11list3)
    的一行代码来在 `Keras` 中训练你的模型。
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '*Listing 11-3: Training your model when data is already loaded into memory*'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 11-3: 当数据已经加载到内存时训练模型*'
- en: However, this isn’t really useful when you start working with large amounts
    of data, because you can’t fit all your training data into your computer’s memory
    at once. To get around this, we use the slightly more complex but more scalable
    `model.fit_generator` function. Instead of passing in all the training data at
    once to this function, you pass a generator that yields training data in batches
    so that your computer’s RAM won’t choke.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当你开始处理大量数据时，这并不太实用，因为你无法一次性将所有训练数据加载到计算机内存中。为了解决这个问题，我们使用了稍微复杂但更具扩展性的 `model.fit_generator`
    函数。你不会一次性将所有训练数据传递给这个函数，而是传递一个生成器，这个生成器会批量生成训练数据，以避免计算机的内存崩溃。
- en: Python generators work just like Python functions, except they have a `yield`
    statement. Instead of returning a single result, generators return an object that
    can be called again and again to yield many, or infinite, sets of results. [Listing
    11-4](ch11.xhtml#ch11list4) shows how we can create our own data generator using
    our feature extraction function.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Python 生成器与 Python 函数非常相似，唯一不同的是它们有一个 `yield` 语句。生成器不会返回单一的结果，而是返回一个可以多次调用的对象，以产生多个或无限数量的结果。
    [Listing 11-4](ch11.xhtml#ch11list4) 展示了我们如何使用特征提取函数创建自己的数据生成器。
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '*Listing 11-4: Writing a data generator*'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 11-4: 编写数据生成器*'
- en: First, the code makes two `assert` statements to check that enough data is there
    ➊. Then inside a `while` ➋ loop (so it’ll just iterate forever), both benign and
    malicious features are grabbed by choosing a random sample ➍ of file keys and
    then extracting features for those files using our `extract_features` function
    ➌. Next, the benign and malicious features and associated labels (0 and 1) are
    concatenated ➎ and shuffled ➏. Finally, these features and labels are returned
    ➐.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，代码做了两个 `assert` 语句来检查数据是否足够 ➊。然后，在一个 `while` ➋ 循环中（这样它就会无限迭代），通过随机选择文件键的样本
    ➍ 来抓取良性和恶性特征，然后使用我们的 `extract_features` 函数提取这些文件的特征 ➌。接下来，将良性和恶性特征及其关联标签（0 和 1）连接起来
    ➎ 并打乱 ➏。最后，返回这些特征和标签 ➐。
- en: Once instantiated, this generator should yield `batch_size` features and labels
    for the model to train on (50 percent malicious, 50 percent benign) each time
    the generator’s `next()` method is called.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦实例化，生成器应该每次调用生成器的 `next()` 方法时，生成 `batch_size` 个特征和标签供模型训练（50% 恶性，50% 良性）。
- en: '[Listing 11-5](ch11.xhtml#ch11list5) shows how to create a training data generator
    using the data that comes with this book, and how to train our model by passing
    the generator to our model’s `fit_generator` method.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[Listing 11-5](ch11.xhtml#ch11list5) 展示了如何使用本书随附的数据创建训练数据生成器，以及如何通过将生成器传递给模型的
    `fit_generator` 方法来训练模型。'
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '*Listing 11-5: Creating the training generator and using it to train the model*'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 11-5: 创建训练生成器并使用它来训练模型*'
- en: Try reading through this code to understand what’s happening. After importing
    a necessary package and creating some parameter variables, we read the filenames
    for our benign ➊ and malicious training data ➋ into memory (but not the files
    themselves). We pass these values to our new `my_generator` function ➌ to get
    our training data generator. Finally, using our `model` from [Listing 11-1](ch11.xhtml#ch11list1),
    we use the `model`’s built-in `fit_generator` method ➍ to start training.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试阅读这段代码以理解发生了什么。在导入必要的包并创建一些参数变量后，我们将无害数据 ➊ 和恶意数据 ➋ 的文件名读入内存（但不读取文件本身）。我们将这些值传递给新的`my_generator`函数
    ➌ 来获取我们的训练数据生成器。最后，使用来自[清单 11-1](ch11.xhtml#ch11list1)的`model`，我们使用`model`内置的`fit_generator`方法
    ➍ 开始训练。
- en: The `fit_generator` method takes three parameters. The `generator` parameter
    ➎ specifies the data generator that produces training data for each *batch*. During
    training, parameters are updated once per batch by averaging all the training
    observations’ signals for that batch. The `steps_per_epoch` parameter ➏ sets the
    number of batches we want the model to process each *epoch*. As a result, the
    total number of observations the model sees per epoch is `batch_size*steps_per_epoch`.
    By convention, the number of observations a model sees per epoch should be equal
    to the dataset size, but in this chapter and in the virtual machine sample code,
    I reduce `steps_per_epoch` to make our code run faster. The `epochs` parameter
    ➐ sets the number of epochs we want to run.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`fit_generator`方法接受三个参数。`generator`参数 ➎ 指定了生成每个*批次*训练数据的数据生成器。在训练过程中，参数会通过对每个批次中的所有训练数据的信号进行平均来更新。`steps_per_epoch`参数
    ➏ 设置我们希望模型每个*周期*处理的批次数量。因此，模型在每个周期看到的观察总数是`batch_size*steps_per_epoch`。按照惯例，模型每个周期看到的观察数量应等于数据集的大小，但在本章及虚拟机示例代码中，我减少了`steps_per_epoch`以加快代码的运行速度。`epochs`参数
    ➐ 设置我们希望运行的周期数。'
- en: Try running this code in the *ch11/* directory that accompanies this book. Depending
    on the power of your computer, each training epoch will take a certain amount
    of time to run. If you’re using an interactive session, feel free to cancel the
    process (CTRL-C) after a few epochs if it’s taking a while. This will stop the
    training without losing progress. After you cancel the process (or the code completes),
    you’ll have a trained model! The readout on your virtual machine screen should
    look something like [Figure 11-2](ch11.xhtml#ch11fig2).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试在本书随附的*ch11/*目录中运行此代码。根据你计算机的性能，每个训练周期需要一定的时间来运行。如果你使用的是交互式会话，运行几轮后如果训练耗时较长，可以随时取消进程（CTRL-C）。这会停止训练而不丢失进度。取消进程后（或代码完成），你将拥有一个训练好的模型！你虚拟机屏幕上的输出应该类似于[图
    11-2](ch11.xhtml#ch11fig2)。
- en: '![image](../images/f0207-01.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0207-01.jpg)'
- en: '*Figure 11-2: Console output from training a* Keras *model*'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-2：训练一个* Keras *模型的控制台输出*'
- en: The top few lines note that TensorFlow, which is the default backend to `Keras`,
    has been loaded. You’ll also see some warnings like in [Figure 11-2](ch11.xhtml#ch11fig2);
    these just mean that the training will be done on CPUs instead of GPUs (GPUs are
    often around 2–20 times faster for training neural networks, but for the purposes
    of this book, CPU-based training is fine). Finally, you’ll see a progress bar
    for each epoch indicating how much longer the given epoch will take, as well as
    the epoch’s loss and accuracy metrics.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 顶部的几行表明，作为`Keras`默认后端的TensorFlow已加载。你还会看到一些警告，像在[图 11-2](ch11.xhtml#ch11fig2)中那样；这些警告只是意味着训练将使用CPU而非GPU（通常GPU在训练神经网络时比CPU快2到20倍，但对于本书的目的，基于CPU的训练是可以的）。最后，你会看到每个训练周期的进度条，显示当前周期剩余的时间，以及该周期的损失和准确率指标。
- en: '***Incorporating Validation Data***'
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***结合验证数据***'
- en: In the previous section, you learned how to train a `Keras` model on HTML files
    using the scalable `fit_generator` method. As you saw, the model prints statements
    during training, indicating each epoch’s current loss and accuracy statistics.
    However, what you really care about is how your trained model does on *validation
    data*, or data that it has never seen before. This better represents the kind
    of data your model will face in a real-life production environment.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，你学会了如何使用可扩展的`fit_generator`方法在HTML文件上训练一个`Keras`模型。正如你所看到的，模型在训练过程中会打印出每个周期当前的损失和准确率统计数据。然而，你真正关心的是训练好的模型在*验证数据*上的表现，或者说它从未见过的数据。这更能代表模型在真实生产环境中将面临的数据。
- en: When trying to design better models and figure out how long to train your model
    for, you should try to maximize *validation accuracy* rather than *training accuracy*,
    the latter of which was shown in [Figure 11-2](ch11.xhtml#ch11fig2). Even better
    would be using validation files originating from dates after the training data
    to better simulate a production environment.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在尝试设计更好的模型并确定训练时长时，应该尽量最大化 *验证准确度*，而不是 *训练准确度*，后者在 [图 11-2](ch11.xhtml#ch11fig2)
    中展示过。更理想的是使用来自训练数据之后日期的验证文件，以更好地模拟生产环境。
- en: '[Listing 11-6](ch11.xhtml#ch11list6) shows how to load our validation features
    into memory using our `my_generator` function from [Listing 11-4](ch11.xhtml#ch11list4).'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 11-6](ch11.xhtml#ch11list6) 展示了如何使用 [列表 11-4](ch11.xhtml#ch11list4) 中的
    `my_generator` 函数将我们的验证特征加载到内存中。'
- en: '[PRE5]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '*Listing 11-6: Reading validation features and labels into memory by using
    the* my_generator *function*'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 11-6：通过使用* my_generator *函数将验证特征和标签读取到内存中*'
- en: This code is very similar to how we created our training data generator, except
    that the file paths have changed and now we want to load all the validation data
    into memory. So instead of just creating the generator, we create a validation
    data generator ➊ with a large `batch_size` ➋ equal to the number of files we want
    to validate on, and we immediately call its `.next()` ➌ method just once.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码与我们创建训练数据生成器的方式非常相似，唯一的区别是文件路径发生了变化，现在我们希望将所有验证数据加载到内存中。所以，我们不仅仅是创建生成器，而是创建了一个验证数据生成器
    ➊，其 `batch_size` ➋ 设置为我们想要验证的文件数量，并且我们立即调用其 `.next()` ➌ 方法，仅调用一次。
- en: Now that we have some validation data loaded into memory, `Keras` allows us
    to simply pass `fit_generator()` our validation data during training, as shown
    in [Listing 11-7](ch11.xhtml#ch11list7).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将一些验证数据加载到内存中，`Keras` 允许我们在训练过程中简单地将验证数据传递给 `fit_generator()`，正如在 [列表
    11-7](ch11.xhtml#ch11list7) 中所示。
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '*Listing 11-7: Using validation data for automatic monitoring during training*'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 11-7：在训练期间使用验证数据进行自动监控*'
- en: '[Listing 11-7](ch11.xhtml#ch11list7) is almost identical to the end of [Listing
    11-5](ch11.xhtml#ch11list5), except that `validation_data` is now passed to `fit_generator`
    ➊. This helps enhance model monitoring by ensuring that validation loss and accuracy
    are calculated alongside training loss and accuracy.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 11-7](ch11.xhtml#ch11list7) 与 [列表 11-5](ch11.xhtml#ch11list5) 的结尾几乎相同，唯一的不同是现在将
    `validation_data` 传递给了 `fit_generator` ➊。这样有助于通过确保在计算训练损失和准确度的同时也计算验证损失和准确度，来增强模型监控。'
- en: Now, training statements should look something like [Figure 11-3](ch11.xhtml#ch11fig3).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，训练语句应该类似于 [图 11-3](ch11.xhtml#ch11fig3) 中的内容。
- en: '![image](../images/f0208-01.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0208-01.jpg)'
- en: '*Figure 11-3: Console output from training a* Keras *model with validation
    data*'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-3：使用验证数据训练* Keras *模型的控制台输出*'
- en: '[Figure 11-3](ch11.xhtml#ch11fig3) is similar to [Figure 11-2](ch11.xhtml#ch11fig2),
    except that instead of just showing training `loss` and `acc` metrics for each
    epoch, now `Keras` also calculates and shows `val_loss` (validation loss) and
    `val_acc` (validation accuracy) for each epoch. In general, if validation accuracy
    is going down instead of up, that’s an indication your model is overfitting to
    your training data, and it would be best to halt training. If validation accuracy
    is going up, as is the case here, it means your model is still getting better
    and you should continue training.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11-3](ch11.xhtml#ch11fig3) 类似于 [图 11-2](ch11.xhtml#ch11fig2)，不同之处在于，现在不仅显示每个
    epoch 的训练 `loss` 和 `acc` 指标，`Keras` 还计算并显示每个 epoch 的 `val_loss`（验证损失）和 `val_acc`（验证准确度）。一般来说，如果验证准确度下降而不是上升，那说明模型可能在过拟合训练数据，这时最好停止训练。如果验证准确度在上升，像这里的情况一样，就意味着模型仍在不断改进，你应该继续训练。'
- en: '***Saving and Loading the Model***'
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***保存和加载模型***'
- en: Now that you know how to build and train a neural network, let’s go over how
    to save it so you can share it with others.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道如何构建和训练神经网络，让我们来看看如何保存它，以便你能与他人共享。
- en: '[Listing 11-8](ch11.xhtml#ch11list8) shows how to save our trained model to
    an *.h5* file ➊ and reload ➋ it (at a potentially later date).'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 11-8](ch11.xhtml#ch11list8) 展示了如何将训练好的模型保存到 *.h5* 文件 ➊ 中并重新加载 ➋（可能在稍后的时间）。'
- en: '[PRE7]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '*Listing 11-8: Saving and loading* Keras *models*'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 11-8：保存和加载* Keras *模型*'
- en: '**Evaluating the Model**'
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**评估模型**'
- en: In the model training section, we observed some default model evaluation metrics
    like training loss and accuracy as well as validation loss and accuracy. Let’s
    now review some more complex metrics to better evaluate our models.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型训练部分，我们观察到一些默认的模型评估指标，如训练损失和准确率，以及验证损失和准确率。接下来，让我们回顾一些更复杂的指标，以便更好地评估我们的模型。
- en: One useful metric for evaluating the accuracy of a binary predictor is called
    *area under the curve (AUC)*. The curve refers to a Receiver Operating Characteristic
    (ROC) curve (see [Chapter 8](ch08.xhtml#ch08)), which plots false-positive rates
    (x-axis) against true-positive rates (y-axis) for all possible score thresholds.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一个用于评估二分类预测器准确度的有用指标叫做*曲线下面积（AUC）*。这条曲线指的是接收者操作特征（ROC）曲线（见[第8章](ch08.xhtml#ch08)），该曲线将假阳性率（x轴）与真阳性率（y轴）对所有可能的分数阈值进行绘制。
- en: For example, our model tries to predict whether a file is malicious by using
    a score between 0 (benign) and 1 (malicious). If we choose a relatively high score
    threshold to classify a file as malicious we’ll get fewer false-positives (good)
    but also fewer true-positives (bad). On the other hand, if we choose a low score
    threshold, we’ll likely have a high false-positive rate (bad) but a very high
    detection rate (good).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们的模型尝试通过使用0（良性）到1（恶性）之间的分数来预测文件是否为恶意。如果我们选择一个相对较高的分数阈值来将文件分类为恶意，那么我们会得到较少的假阳性（好）但也会得到较少的真正阳性（坏）。另一方面，如果我们选择较低的分数阈值，我们可能会有较高的假阳性率（坏）但检测率会非常高（好）。
- en: These two sample possibilities would be represented as two points on our model’s
    ROC curve, where the first would be located toward the left side of the curve
    and the second near the right side. AUC represents all these possibilities by
    simply taking the area under this ROC curve, as shown in [Figure 11-4](ch11.xhtml#ch11fig4).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种样本情况将作为两个点表示在我们模型的ROC曲线上，第一个点位于曲线的左侧，而第二个点则靠近右侧。AUC通过简单地计算ROC曲线下方的面积来表示所有这些可能性，如[图
    11-4](ch11.xhtml#ch11fig4)所示。
- en: In simple terms, an AUC of 0.5 represents the predictive capability of a coin
    flip, while an AUC of 1 is perfect.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，AUC为0.5代表了抛硬币的预测能力，而AUC为1则表示完美。
- en: '![image](../images/f0210-01.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0210-01.jpg)'
- en: '*Figure 11-4: Various sample ROC curves. Each ROC curve (line) corresponds
    to a different AUC value.*'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-4：各种样本的ROC曲线。每条ROC曲线（线）对应一个不同的AUC值。*'
- en: Let’s use our validation data to calculate validation AUC using the code in
    [Listing 11-9](ch11.xhtml#ch11list9).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用我们的验证数据，通过[示例 11-9](ch11.xhtml#ch11list9)中的代码来计算验证集的AUC。
- en: '[PRE8]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '*Listing 11-9: Calculating validation AUC using* sklearn*’s* metric *submodule*'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例 11-9：使用* sklearn*的* metric *子模块计算验证AUC*'
- en: 'Here, we split our `validation_data` tuple into two objects: the validation
    labels represented by `validation_labels` ➊, and flattened validation model predictions
    represented by `validation_scores` ➋. Then, we use the `metrics.roc_curve` function
    from `sklearn` to calculate false-positive rates, true-positive rates, and associated
    threshold values for the model predictions ➌. Using these, we calculate our AUC
    metric, again using an `sklearn` function ➍.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将`validation_data`元组拆分为两个对象：表示验证标签的`validation_labels` ➊，以及表示展平的验证模型预测值的`validation_scores`
    ➋。然后，我们使用`sklearn`中的`metrics.roc_curve`函数来计算假阳性率、真阳性率和与模型预测相关的阈值 ➌。利用这些，我们再次使用`sklearn`函数计算AUC指标
    ➍。
- en: Although I won’t go over the function code here, you can also use the `roc_plot()`
    function included in the *ch11/model_evaluation.py* file in the data accompanying
    this book to plot the actual ROC curve, as shown in [Listing 11-10](ch11.xhtml#ch11list10).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我在这里不会详细介绍函数代码，但你也可以使用本书附带数据中的`ch11/model_evaluation.py`文件中的`roc_plot()`函数来绘制实际的ROC曲线，正如[示例
    11-10](ch11.xhtml#ch11list10)中所示。
- en: '[PRE9]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '*Listing 11-10: Creating a ROC curve plot using the* roc_plot *function from
    this book’s accompanying data, in* ch11/model_evaluation.py'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例 11-10：使用* roc_plot *函数从本书附带的数据中创建ROC曲线图，位于* ch11/model_evaluation.py中。'
- en: Running the code in [Listing 11-10](ch11.xhtml#ch11list10) should generate a
    plot (saved to *roc_curve.png*) that looks like [Figure 11-5](ch11.xhtml#ch11fig5).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 运行[示例 11-10](ch11.xhtml#ch11list10)中的代码应该会生成一张图（保存在*roc_curve.png*中），其样式如[图
    11-5](ch11.xhtml#ch11fig5)所示。
- en: '![image](../images/f0211-01.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0211-01.jpg)'
- en: '*Figure 11-5:* A ROC curve!'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-5：一条ROC曲线！*'
- en: Each point in the ROC curve in [Figure 11-5](ch11.xhtml#ch11fig5) represents
    a specific false-positive rate (x-axis) and true-positive rate (y-axis) associated
    with various model prediction thresholds ranging from 0 to 1\. As false-positive
    rates increase, true-positive rates increase, and vice versa. In production environments,
    you generally have to pick a single threshold (a single point on this curve, assuming
    validation data mimics production data) with which to make your decision, based
    on your willingness to tolerate false positives, versus your willingness to risk
    allowing a malicious file to slip through the cracks.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11-5](ch11.xhtml#ch11fig5)中的ROC曲线上的每个点都代表一个特定的假阳性率（x轴）和真阳性率（y轴），这些是与各种模型预测阈值（从0到1）相关的。当假阳性率增加时，真阳性率也会增加，反之亦然。在生产环境中，您通常需要选择一个特定的阈值（即曲线上的某个点，假设验证数据与生产数据相似）来做出决策，依据您愿意容忍的假阳性率和您愿意冒着漏掉恶意文件的风险之间的平衡。'
- en: '**Enhancing the Model Training Process with Callbacks**'
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**通过回调增强模型训练过程**'
- en: So far, you’ve learned how to design, train, save, load, and evaluate `Keras`
    models. Although this is really all you need to get a fairly good start, I also
    want to introduce `Keras` callbacks, which can make our model training process
    even better.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您已经学习了如何设计、训练、保存、加载和评估`Keras`模型。尽管这已经足够让您有一个不错的起步，但我还想介绍一下`Keras`回调，它们可以让我们的模型训练过程变得更好。
- en: A `Keras` callback represents a set of functions that `Keras` applies during
    certain stages of the training process. For example, you can use a `Keras` callback
    to make sure that an *.h5* file is saved at the end of each epoch, or that validation
    AUC is printed to the screen at the end of each epoch. This can help record and
    inform you more precisely of how your model is doing during the training process.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`Keras`回调代表一组在训练过程中的某些阶段由`Keras`应用的函数。例如，您可以使用`Keras`回调来确保在每个训练周期结束时保存一个*.h5*文件，或者在每个训练周期结束时将验证AUC打印到屏幕上。这有助于记录并更精确地告知您模型在训练过程中的表现。'
- en: We begin by using a built-in callback, and then we try writing our own custom
    callback.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用一个内置回调，然后再尝试编写自定义回调。
- en: '***Using a Built-in Callback***'
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***使用内置回调***'
- en: To use a built-in callback, simply pass your model’s `fit_generator()` method
    a callback instance during training. We’ll use the `callbacks.ModelCheckpoint`
    callback, which evaluates validation loss after each training epoch, and saves
    the current model to a file *if* the validation loss is smaller than any previous
    epoch’s validation losses. To do this, the callback needs access to our validation
    data, so we’ll pass that in to the `fit_generator()` method, as shown in [Listing
    11-11](ch11.xhtml#ch11list11).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用内置回调，只需在训练期间将回调实例传递给模型的`fit_generator()`方法即可。我们将使用`callbacks.ModelCheckpoint`回调，它会在每个训练周期后评估验证损失，并在当前模型的验证损失小于之前任何一个周期的验证损失时保存模型。为了实现这一点，回调需要访问我们的验证数据，因此我们将在`fit_generator()`方法中传入这些数据，如[清单11-11](ch11.xhtml#ch11list11)所示。
- en: '[PRE10]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '*Listing 11-11: Adding a* ModelCheckpoint *callback to the training process*'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单11-11：向训练过程中添加一个ModelCheckpoint回调*'
- en: This code ensures that the model is overwritten ➊ to a single file, `'results/best_model.h5'`
    ➋, whenever `'val_loss'` ➌ (validation loss) reaches a new low. This ensures that
    the current saved model (`'results/best_model.h5'`) always represents the best
    model across all completed epochs with regard to validation loss.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码确保每当`'val_loss'`（验证损失）达到新低时，模型会被覆盖到一个单独的文件`'results/best_model.h5'` ➊。这样可以确保当前保存的模型（`'results/best_model.h5'`）始终代表所有已完成训练周期中验证损失最小的最佳模型。
- en: Alternatively, we can use the code in [Listing 11-12](ch11.xhtml#ch11list12)
    to save the model after every epoch to a *separate* file regardless of validation
    loss.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以使用[清单11-12](ch11.xhtml#ch11list12)中的代码，在每个周期后将模型保存到一个*单独*的文件中，而不考虑验证损失。
- en: '[PRE11]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '*Listing 11-12: Adding a* ModelCheckpoint *callback to the training process
    that saves the model to a different file after each epoch*'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单11-12：向训练过程中添加一个ModelCheckpoint回调，在每个周期后将模型保存到不同的文件中*'
- en: To do this, we use the same code in [Listing 11-11](ch11.xhtml#ch11list11) and
    the same function `ModelCheckpoint`, but with `save_best_only=False` ➍ and a `filepath`
    that asks `Keras` to fill in the epoch number ➎. Instead of only saving the single
    “best” version of our model, [Listing 11-12](ch11.xhtml#ch11list12)’s callback
    saves each epoch’s version of our model, in *results/model_epoch_0.h5*, *results/model_epoch_1.h5*,
    *results/model_epoch_2.h5*, and so on.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们使用[清单 11-11](ch11.xhtml#ch11list11)中的相同代码和相同的 `ModelCheckpoint` 函数，但设置
    `save_best_only=False` ➍，并指定一个 `filepath`，让 `Keras` 填入周期编号 ➎。与只保存“最佳”版本的模型不同，[清单
    11-12](ch11.xhtml#ch11list12)中的回调函数会保存每个周期的模型版本，分别存储为 *results/model_epoch_0.h5*、*results/model_epoch_1.h5*、*results/model_epoch_2.h5*
    等。
- en: '***Using a Custom Callback***'
  id: totrans-115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***使用自定义回调函数***'
- en: Although `Keras` doesn’t support AUC, we can design our own custom callback
    to, for example, allow us to print AUC to the screen after each epoch.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 `Keras` 不直接支持 AUC，但我们可以设计自己的自定义回调函数，例如，让我们在每个周期后将 AUC 打印到屏幕上。
- en: 'To create a custom `Keras` callback, we need to create a class that inherits
    from `keras.callbacks.Callback`, the abstract base class used to build new callbacks.
    We can add one or more of a selection of methods, which will be run automatically
    during training, at times that their names specify: `on_epoch_begin`, `on_epoch_end`,
    `on_batch_begin`, `on_batch_end`, `on_train_begin`, and `on_train_end`.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个自定义的 `Keras` 回调函数，我们需要创建一个继承自 `keras.callbacks.Callback` 的类，这是用于构建新回调函数的抽象基类。我们可以添加一个或多个方法，这些方法会在训练期间自动运行，并且会在它们的名称指定的时间运行：`on_epoch_begin`、`on_epoch_end`、`on_batch_begin`、`on_batch_end`、`on_train_begin`
    和 `on_train_end`。
- en: '[Listing 11-13](ch11.xhtml#ch11list13) shows how to create a callback that
    calculates and prints validation AUC to the screen at the end of each epoch.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 11-13](ch11.xhtml#ch11list13) 展示了如何创建一个回调函数，在每个训练周期结束时计算并打印验证 AUC 到屏幕。'
- en: '[PRE12]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '*Listing 11-13: Creating and using a custom callback to print AUC to the screen
    after each training epoch*'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 11-13：创建并使用自定义回调函数，在每个训练周期后将 AUC 打印到屏幕*'
- en: 'In this example, we first create our `MyCallback` class ➊, which inherits from
    `callbacks.Callbacks`. Keeping things simple, we overwrite a single method, `on_epoch_end`
    ➋, and give it two arguments expected by `Keras`: `epoch` and `logs` (a dictionary
    of log information), both of which `Keras` will supply when it calls the function
    during training.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们首先创建了我们的 `MyCallback` 类 ➊，它继承自 `callbacks.Callbacks`。为了简化，我们只重写了一个方法，`on_epoch_end`
    ➋，并给它提供了 `Keras` 期望的两个参数：`epoch` 和 `logs`（日志信息字典），这两个参数会在训练期间由 `Keras` 调用该函数时传入。
- en: Then, we grab the `validation_data` ➌, which is already stored in the `self`
    object thanks to `callbacks.Callback` inheritance, and we calculate and print
    out AUC ➍ like we did in “[Evaluating the Model](ch11.xhtml#lev191)” on [page
    209](ch11.xhtml#page_209). Note that for this code to work, the validation data
    needs to be passed to `fit_generator()` so that the callback has access to `self.validation_data`
    during training ➎. Finally, we tell the model to train and specify our new callback
    ➏. The result should look something like [Figure 11-6](ch11.xhtml#ch11fig6).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们获取 `validation_data` ➌，它已经通过继承自 `callbacks.Callback` 存储在 `self` 对象中，并像在“[评估模型](ch11.xhtml#lev191)”一节中提到的那样计算并打印出
    AUC ➍，这在[第 209 页](ch11.xhtml#page_209)有讲解。注意，为了使这段代码正常工作，验证数据需要传递给 `fit_generator()`，这样回调函数才能在训练过程中访问到
    `self.validation_data` ➎。最后，我们告诉模型进行训练，并指定我们的新回调函数 ➏。结果应该类似于[图 11-6](ch11.xhtml#ch11fig6)所示。
- en: '![image](../images/f0214-01.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0214-01.jpg)'
- en: '*Figure 11-6: Console output from training a* Keras *model with a custom AUC
    callback*'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-6：使用自定义 AUC 回调训练 Keras 模型时的控制台输出*'
- en: If what you really care about is minimizing validation AUC, this callback makes
    it easy to see how your model is doing during training, thus helping you assess
    whether you should stop the training process (for example, if validation accuracy
    is going consistently down over time).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你真正关心的是最小化验证 AUC，这个回调函数能帮助你轻松查看模型在训练过程中的表现，从而帮助你评估是否应该停止训练过程（例如，如果验证准确率持续下降）。
- en: '**Summary**'
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**总结**'
- en: In this chapter, you learned how to build your own neural network using `Keras`.
    You also learned to train, evaluate, save, and load it. You then learned how to
    enhance the model training process by adding built-in and custom callbacks. I
    encourage you to play around with the code accompanying this book to see what
    changes model architecture and feature extraction can have on model accuracy.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了如何使用 `Keras` 构建自己的神经网络。你还学习了如何训练、评估、保存和加载模型。接着，你学习了如何通过添加内置和自定义回调来提升模型训练过程。我鼓励你尝试修改本书附带的代码，看看改变模型架构和特征提取对模型准确性的影响。
- en: This chapter is meant to get your feet wet, but is not meant as a reference
    guide. Visit *[https://keras.io](https://keras.io)* for the most up-to-date official
    documentation. I strongly encourage you to spend time researching aspects of `Keras`
    that interest you. Hopefully, this chapter has served as a good jumping-off point
    for all your security deep learning adventures!
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 本章旨在让你入门，但并非作为参考指南。请访问 *[https://keras.io](https://keras.io)* 获取最新的官方文档。我强烈建议你花时间研究你感兴趣的
    `Keras` 方面。希望本章能为你所有的安全深度学习冒险提供一个良好的起点！
