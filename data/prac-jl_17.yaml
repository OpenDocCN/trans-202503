- en: '**15'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**15**'
- en: PARALLEL PROCESSING**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**并行处理**'
- en: '*If one ox could not do the job they did not try to grow a bigger ox, but used
    two oxen.*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果一头牛无法完成工作，他们不会尝试养更大的一头牛，而是使用两头牛。*'
- en: —Grace Hopper
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: —格蕾丝·霍普
- en: '![Image](../images/common.jpg)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/common.jpg)'
- en: Parallel processing is a class of strategies for computation where we divide
    a problem into pieces and tackle each piece with a different computer or different
    processing units on a single computer—or a combination of both approaches. This
    chapter treats *true parallel processing*, where different computations occur
    simultaneously, and *concurrent processing*, where we ask the computer to do several
    things at once, but it may have to alternate among them.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 并行处理是一类计算策略，我们将一个问题分解成若干部分，并用不同的计算机或单台计算机上的不同处理单元来解决每个部分——或者两者的组合。本章讨论的是*真正的并行处理*，即不同的计算同时进行，以及*并发处理*，即我们要求计算机同时做几件事，但它可能需要在它们之间交替执行。
- en: While writing effective parallel programs can be tricky, Julia goes a long way
    toward making parallel and concurrent processing as easy as possible. The same
    program may run in parallel or merely concurrently, depending on machine resources,
    but Julia’s abstractions free us to write one version of the program that can
    take advantage of varying runtime environments.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然编写有效的并行程序可能很棘手，但 Julia 在尽可能简化并行和并发处理方面做了大量工作。同一个程序可能在并行或仅仅并发的方式下运行，这取决于机器资源，但
    Julia 的抽象使我们可以编写一个版本的程序，在不同的运行时环境中都能发挥作用。
- en: This chapter will provide an overview of how to implement the major concurrency
    paradigms using facilities built into Julia and several convenient packages.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将概述如何利用 Julia 内建的功能和几个便利的包来实现主要的并发范式。
- en: '**Concurrency Paradigms**'
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**并发范式**'
- en: A natural distinction from the programmer’s point of view is between *multithreading*
    and *multiprocessing*, and that’s the major divide that organizes this chapter.
    This area suffers from some terminological inconsistency. We use multithreading
    to mean programming *aimed* at parallel execution on multiple CPU cores on a single
    machine. A *core* is a processing unit within a CPU chip. Each one is equipped
    with its own resources, such as caches and arithmetic logic units, and can execute
    instructions independently, although it may share some resources with other cores.
    If someone happens to run a multithreaded program on a computer with only one
    core, there won’t be any parallelism happening, but that need not concern us when
    we’re writing the program. The same code will run faster on a multicore machine
    if we’ve written it correctly.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 从程序员的角度来看，一个自然的区分是*多线程*和*多进程*，这是本章的主要划分。这个领域存在一些术语不一致的问题。我们使用多线程来指代旨在实现单台机器上多个
    CPU 核心并行执行的编程。*核心*是 CPU 芯片中的处理单元。每个核心都配备有自己的资源，比如缓存和算术逻辑单元，并可以独立执行指令，尽管它可能与其他核心共享一些资源。如果某人将多线程程序运行在只有一个核心的计算机上，那么不会发生并行处理，但当我们编写程序时，这并不需要担心。如果我们正确编写代码，它将在多核机器上运行得更快。
- en: We use multiprocessing to refer to a style of programming where we launch tasks
    that can be executed by different processes on a single machine or by multiple
    machines (or both).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用多进程编程来指代一种编程风格，在这种风格中，我们启动可以由单台计算机上的不同进程或多台计算机（或两者）执行的任务。
- en: 'The most important distinction between the two styles of programming has to
    do with access to memory: all of the threads in a multithreaded program have access
    to the same pool of memory, while the processes in a multiprocessing program have
    separate memory areas.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种编程风格之间最重要的区别在于内存访问：多线程程序中的所有线程都可以访问相同的内存池，而多进程程序中的进程则有各自独立的内存区域。
- en: '**Multithreading**'
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**多线程**'
- en: This section deals with speeding up the work within a single process by dividing
    it among a number of *tasks*. Since all these tasks exist within the same process,
    they all have access to the same memory space. The task is the basic concept upon
    which Julia’s parallel and concurrent processing is built. It’s a discrete unit
    of work, usually a function call, that’s assigned to a particular thread by the
    *scheduler*. Tasks are inherently asynchronous; once launched, they continue on
    their assigned thread until they’re done or suspend themselves by yielding to
    the scheduler. However, we can synchronize and orchestrate the tasks’ life cycles
    in various ways.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论通过将工作划分为多个*任务*来加速单个进程中的工作。由于所有这些任务都存在于同一进程中，因此它们都可以访问相同的内存空间。任务是Julia并行和并发处理的基本概念。它是一个离散的工作单元，通常是一个函数调用，由*调度器*分配给特定的线程。任务本质上是异步的；一旦启动，它们将在分配的线程上继续执行，直到完成或通过让出给调度器而自行挂起。然而，我们可以通过多种方式同步和协调任务的生命周期。
- en: '**NOTE**'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*You may have done parallel computing with Julia without knowing it. Many linear
    algebra routines, including the matrix multiplication dispatched by *, run multithreaded
    BLAS (Basic Linear Algebra Subprograms) routines that automatically take advantage
    of all CPU cores, transparently to the user. You can verify this by executing
    a matrix multiply in the REPL and keeping an eye on your CPU meters.*'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*你可能已经在不知情的情况下使用Julia进行并行计算。许多线性代数例程，包括由*运算符*调度的矩阵乘法，运行多线程BLAS（基本线性代数子程序）例程，这些例程自动利用所有CPU核心，对用户是透明的。你可以通过在REPL中执行矩阵乘法并关注你的CPU监控工具来验证这一点。*'
- en: When we enter the Julia REPL or use the `julia` command to run a program stored
    on the disk, we have several available command line options. Unless we use the
    `-t` option, Julia uses exactly one thread (and, consequently, one CPU core),
    no matter the hardware configuration on which it’s running.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们进入Julia REPL或使用`julia`命令运行存储在磁盘上的程序时，我们有几个可用的命令行选项。除非使用`-t`选项，否则Julia只使用一个线程（因此，也只使用一个CPU核心），无论其运行的硬件配置如何。
- en: To allow Julia to use all the available threads, use the `-t auto` argument.
    In that case, all of the “available” threads will be all of the *logical* threads
    on the machine. This is often not optimal. A better choice can be `-t` n, where
    n is the number of *physical* cores. For example, the popular Intel Core processors
    provide two logical cores for each physical core using a technique called *hyperthreading*.
    Hyperthreading can yield anything from a modest speedup to an actual slowdown,
    depending on the type of calculation.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 要允许Julia使用所有可用的线程，请使用`-t auto`参数。在这种情况下，所有“可用”的线程将是机器上的所有*逻辑*线程。这通常不是最优选择。更好的选择是使用`-t`
    n，其中n是*物理*核心的数量。例如，流行的Intel Core处理器通过一种称为*超线程*的技术为每个物理核心提供两个逻辑核心。超线程可能会带来从适度加速到实际减速的效果，这取决于计算类型。
- en: On Linux we can use the `lscpu` command at the system shell to get information
    about the CPU. For example, if the output contains the lines
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在Linux上，我们可以在系统命令行使用`lscpu`命令获取关于CPU的信息。例如，如果输出包含以下几行
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: then the machine has a total of two physical compute cores and four logical
    threads provided by hyperthreading. We usually need to experiment to discover
    whether `-t` n (in this case, `-t 2`) or `-t auto` leads to a better outcome.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 那么该机器总共有两个物理计算核心和通过超线程提供的四个逻辑线程。我们通常需要实验才能发现`-t` n（在这种情况下，`-t 2`）或`-t auto`哪个能带来更好的结果。
- en: Within a program, or in the REPL, we can check for the number of available threads
    with
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在程序中或REPL中，我们可以通过以下命令检查可用线程的数量
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: which reports the total number in use and is blind to how many of them represent
    real cores.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 它报告了正在使用的总数，但无法区分其中多少是真正的核心。
- en: With multiple threads, we can speed up our programs by assigning tasks to run
    on more than one CPU core simultaneously, either automatically or by applying
    various levels of control.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多个线程，我们可以通过将任务分配到多个CPU核心上同时运行，从而加速程序，无论是自动的还是通过应用不同级别的控制。
- en: '***Easy Multithreading with Folds***'
  id: totrans-25
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***使用折叠实现简单多线程***'
- en: One automatic way of launching tasks is with the `Folds` package, which provides
    multithreaded versions of `map()`, `sum()`, `maximum()`, `minimum()`, `reduce()`,
    `collect()`, and a handful of other functions over collections. Its use is as
    easy as replacing, for example, `sum()` with `Folds.sum()`. The parallelized function
    takes care of dividing the work among all the available threads.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 启动任务的一种自动方式是使用 `Folds` 包，它提供了 `map()`、`sum()`、`maximum()`、`minimum()`、`reduce()`、`collect()`
    等多个函数的多线程版本。这些函数的使用非常简单，比如将 `sum()` 替换为 `Folds.sum()`。并行化函数会负责在所有可用线程之间分配工作。
- en: As an example, [Listing 15-1](ch15.xhtml#ch15lis1) shows the parallelized map
    of an expensive function over an array.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个例子，[Listing 15-1](ch15.xhtml#ch15lis1) 展示了在一个数组上并行化计算一个开销较大的函数。
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '*Listing 15-1: Easy parallelism with* Folds.jl'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 15-1: 使用* Folds.jl *实现简单的并行化*'
- en: The `@belapsed` macro is part of `BenchmarkTools`. Like the `@btime` macro that
    we’ve used before, it runs the job repeatedly and reports an average of resource
    utilizations. This version is convenient when we just want the CPU time consumed.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '`@belapsed` 宏是 `BenchmarkTools` 的一部分。像我们之前使用过的 `@btime` 宏一样，它会反复运行任务并报告资源利用率的平均值。这个版本在我们仅需获取消耗的
    CPU 时间时特别方便。'
- en: The parallelized version of `map()` gives each thread an approximately equal
    portion of the loop over 5,001 numbers. Ideally, the total compute time should
    be 1/*N*, where *N* is the number of threads. Behind the scenes, it’s creating
    tasks, each with some portion of the loop, and assigning them to available threads;
    it may use two tasks, or more. It also synchronizes the computation, waiting for
    all the tasks to complete before returning.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '`map()` 的并行化版本将每个线程分配到 5,001 个数字的循环中的大致相等部分。理想情况下，总的计算时间应该是 1/*N*，其中 *N* 是线程的数量。在后台，它会创建任务，每个任务处理循环中的一部分，然后将它们分配给可用线程；它可能使用两个任务，或者更多。它还会同步计算，在所有任务完成后才返回结果。'
- en: This REPL session was started using the `-t 2` flag. The results show that the
    parallel version used just slightly more than half the time of the serial computation.
    Since we are running on two (physical) threads, the result indicates an almost
    ideal parallel speedup.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 REPL 会话是通过 `-t 2` 标志启动的。结果显示，使用并行版本的计算时间仅略高于串行计算的一半。由于我们在两个（物理）线程上运行，结果显示了几乎理想的并行加速效果。
- en: 'However, we’re not always so lucky. Whether parallelizing a computation helps,
    hinders, or has no effect is the result of the trade-off between the overhead
    of setting up and managing a set of tasks and the benefits of dividing up the
    work. It’s sensitive to the cost of the calculation per array element, the size
    of the array, and the patterns of memory access. The same calculation on a smaller
    array has a better outcome using the serial `map()`:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们并不总是这么幸运。并行化计算是否有帮助、会造成负面影响或没有效果，取决于设置和管理任务集的开销与分配工作所带来的好处之间的权衡。这种效果受到每个数组元素的计算成本、数组的大小以及内存访问模式的影响。在较小的数组上执行相同的计算时，使用串行
    `map()` 会得到更好的结果：
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here, working on a single processor is actually faster than trying to parallelize
    the short computation. Successful parallel computing requires a good deal of testing.
    We need to ensure that we’re taking good advantage of the hardware and that the
    results running on multiple cores are identical to the results run serially, aside
    from small numerical differences that reordering of floating-point calculations
    can cause in some programs.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，使用单个处理器的计算实际上比试图并行化短计算要快。成功的并行计算需要大量的测试。我们需要确保能够充分利用硬件，并且在多个核心上运行的结果与串行运行的结果是相同的，除了浮点数计算重排序可能在某些程序中导致的微小数值差异。
- en: '***Manual Multithreading with @threads***'
  id: totrans-36
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***使用 @threads 进行手动多线程***'
- en: The `Folds` package is a higher-level interface to the manual multithreading
    that’s the subject of this section. Going manual requires more care, but it can
    provide an extra degree of control that we sometimes need.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`Folds` 包是本节讨论的手动多线程的高级接口。手动操作需要更多的注意，但它可以提供额外的控制，这在某些情况下是我们所需要的。'
- en: '**Threads.@threads**'
  id: totrans-38
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**Threads.@threads**'
- en: The main facility for multithreading in Julia is the `Threads.@threads` macro,
    which is part of `Base`, so it’s always available. To run a loop in parallel,
    we preface it with the macro. As an introduction, [Listing 15-2](ch15.xhtml#ch15lis2)
    tackles the same problem as in the previous section.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Julia 中进行多线程的主要工具是 `Threads.@threads` 宏，它是 `Base` 的一部分，因此始终可用。为了并行运行一个循环，我们只需在循环前加上这个宏。作为介绍，[Listing
    15-2](ch15.xhtml#ch15lis2) 解决了与上一节相同的问题。
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '*Listing 15-2: Timing a threaded loop*'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 15-2：计时线程循环*'
- en: Apparently, the `@threads` version performs similarly to the wrapper from the
    `Folds` package.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，`@threads`版本的表现与`Folds`包中的包装器相似。
- en: The `@threads` macro works by dividing the loop into *N* segments and assigning
    each segment to a separate task. The scheduler apportions these tasks among the
    available threads. Normally *N* is a small multiple of the number of threads,
    so if we have two cores and have used the `-t 2` flag, `@threads` will probably
    divide the loop over 5,001 elements into two or four loops of approximately equal
    length.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '`@threads`宏通过将循环划分为*N*个段落并将每个段落分配给一个独立的任务来工作。调度器将这些任务分配给可用的线程。通常*N*是线程数的小倍数，所以如果我们有两个核心并且使用了`-t
    2`标志，`@threads`可能会将5001个元素的循环分成两个或四个大致相等的循环。'
- en: The `@threads` loop is synchronized in the sense that computation does not continue
    past the end of the loop until all tasks are complete. Different parts of the
    loop, hence different tasks, may take different amounts of time. If this difference
    is large, some threads will be idle waiting for the others to catch up. This is
    why, as mentioned previously, this style of multithreading works best when all
    iterations take roughly the same computing time.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`@threads`循环是同步的，意味着计算不会在循环结束之前继续，直到所有任务完成。循环的不同部分，因此不同的任务，可能需要不同的时间。如果这个时间差异很大，一些线程将会闲置，等待其他线程赶上。这就是为什么，如前所述，当所有迭代的计算时间大致相同时，这种多线程风格的表现最好。'
- en: 'Instead of throwing out the result, let’s try adding together all the `f(x)`s:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 与其丢弃结果，不如尝试将所有的`f(x)`加在一起：
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The parallel results not only differ from the serial result, but it seems that
    we can get different answers for different runs of the parallel program. What
    did we do wrong?
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 并行结果不仅与串行结果不同，而且似乎我们在不同的并行程序运行中会得到不同的答案。我们哪里做错了？
- en: '**Atomic Theory**'
  id: totrans-48
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**原子理论**'
- en: The problem arises when we update `s` within the parallel loop. Multiple independent
    threads trying to access and write to the same scalar variable creates a *race
    condition*, a conflict where the result depends on an order of operations which
    the program does not control. We can get different results from different runs
    because the timings will differ, based on unknown influences such as the other
    tasks that the operating system happens to be performing during the run. There’s
    no problem when updating array locations because in the threaded loop, arrays
    will be divided among the threads and no thread will step on another thread’s
    data.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 问题出现在我们在并行循环中更新`s`时。多个独立的线程尝试访问并写入相同的标量变量时，产生了*竞争条件*，这是一个冲突，其结果取决于操作顺序，而程序无法控制。由于时序的不同，基于操作系统在运行期间执行的其他任务等未知因素的影响，我们可能从不同的运行中得到不同的结果。当更新数组位置时没有问题，因为在多线程循环中，数组将被分配到不同的线程中，没有线程会干扰其他线程的数据。
- en: Julia provides several strategies for protecting a scalar during multithreaded
    execution. One way is to use *atomic variables*, as [Listing 15-3](ch15.xhtml#ch15lis3)
    shows.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Julia 提供了几种在多线程执行期间保护标量的策略。一种方法是使用*原子变量*，如[清单 15-3](ch15.xhtml#ch15lis3)所示。
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '*Listing 15-3: Using an atomic variable*'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 15-3：使用原子变量*'
- en: 'We’ve initialized `s` as an atomic variable using the built-in `Threads.Atomic`
    declaration. It allows only simple types: the various floats, integers, and the
    `Bool` type. We update atomic variables using a small collection of functions
    for the purpose, all namespaced with `Threads`. In addition to `Threads.atomic_add!()`,
    we have `atomic_sub!()` for subtraction, several logical operators, `atomic_xchg!()`
    for setting the variable to a new value, and a few more. We access the value of
    an atomic variable with the odd-looking syntax in the last line of the program.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经使用内建的`Threads.Atomic`声明将`s`初始化为原子变量。它仅允许简单类型：各种浮动数、整数和`Bool`类型。我们通过一小组为此目的设计的函数来更新原子变量，所有函数都在`Threads`命名空间下。除了`Threads.atomic_add!()`，我们还可以使用`atomic_sub!()`进行减法、几个逻辑操作符、`atomic_xchg!()`用于将变量设置为新值，等等。我们通过程序最后一行中看起来有点奇怪的语法来访问原子变量的值。
- en: 'The result is close to the serial result, so the atomic variable fixed the
    problem. The results are close, but not equal: they vary in the last few decimal
    places. The result of a series of floating-point operations can depend on their
    order, and the order varies between serial and parallel runs and among parallel
    runs with different numbers of threads. We’ll also get a minutely different result
    if we run the serial code counting backward in the loop:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 结果接近串行结果，因此原子变量解决了这个问题。结果虽然接近，但不完全相同：它们在最后几位小数上有所不同。一系列浮点操作的结果可能取决于它们的顺序，而顺序在串行和并行运行之间以及在不同线程数的并行运行之间会有所变化。如果我们在循环中反向计数运行串行代码，我们也会得到一个稍微不同的结果：
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: These small variations in the least significant parts of the answers are normal
    and expected, and are something that the numericist must be alert to when comparing
    the results from a parallelized program when run on different computers with possibly
    different numbers of cores.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这些答案中最低有效位的细微变化是正常且可以预期的，数值分析人员在比较并行化程序在不同计算机上运行时的结果时，必须对这些变化保持警觉，因为不同的计算机可能有不同数量的核心。
- en: 'We can also get a correct summation using a different strategy:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用不同的策略得到正确的求和结果：
- en: '[PRE8]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We’ve essentially given each thread its private copy of the summation variable
    and added all the copies together at the end. We use `Threads.nthreads()` to create
    a vector the same length as the number of threads. Within each thread, `Threads.threadid()`
    returns that thread’s unique integer identifier. We use this identifier to index
    into the array of summations ➊, ensuring that each thread updates only the element
    that belongs to it. The sum of sums in the last line should be the same as the
    scalar `s` in the other versions of the program.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，我们为每个线程提供了其私有的求和变量副本，并在最后将所有副本相加。我们使用 `Threads.nthreads()` 创建一个与线程数相同长度的向量。在每个线程中，`Threads.threadid()`
    返回该线程的唯一整数标识符。我们使用这个标识符来索引求和数组 ➊，确保每个线程仅更新属于它的元素。最后一行的求和结果应该与程序的其他版本中的标量 `s` 相同。
- en: The technique of using an array instead of an atomic variable can be faster,
    because before a thread is allowed to read or update an atomic variable, it must
    wait until it’s released by any other thread that’s using it. The use of arrays
    eliminates this *locking* and the consequent waiting time. However, it uses a
    bit more memory for the new arrays.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 使用数组代替原子变量的技术可能更快，因为在一个线程被允许读取或更新原子变量之前，它必须等待直到任何其他线程释放它。使用数组避免了这种*锁定*和随之而来的等待时间。然而，它为新数组使用了稍微更多的内存。
- en: '***Spawning and Synchronizing Tasks***'
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***启动和同步任务***'
- en: The techniques we’ve described in the previous two sections implement parallelism
    by dividing the work among tasks and launching them behind the scenes. Here we’ll
    learn how to take control of spawning and synchronizing tasks.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前两节中描述的技巧通过将工作分配给任务并在后台启动它们来实现并行化。在这里，我们将学习如何控制任务的启动和同步。
- en: '**Launching Tasks with Threads.@spawn**'
  id: totrans-63
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**使用线程启动任务.@spawn**'
- en: We can also launch tasks manually with the `Threads.@spawn` macro, as shown
    in [Listing 15-4](ch15.xhtml#ch15lis4).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以像 [Listing 15-4](ch15.xhtml#ch15lis4) 中展示的那样，使用 `Threads.@spawn` 宏手动启动任务。
- en: '[PRE9]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '*Listing 15-4: Introducing task spawning*'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 15-4: 引入任务启动*'
- en: Since `@belapsed` and the other benchmarking tools in `BenchmarkTools` run code
    multiple times, we place the timed code within a function to force the atomic
    variable to be initialized in each trial run.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 `@belapsed` 和 `BenchmarkTools` 中的其他基准测试工具会多次运行代码，我们将计时代码放入函数中，以确保在每次试验运行时原子变量都会被初始化。
- en: The `@sync` macro ➊ works for any block, not just `for` loops. It synchronizes
    all tasks launched within the lexical scope of the block, which means that the
    statement following its `end` statement will wait until they’re all done. In [Listing
    15-4](ch15.xhtml#ch15lis4), `@sync` ensures that when we access `s[]`, it will
    have its final value, and that the timings include the time for completion of
    all tasks.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`@sync` 宏 ➊ 适用于任何代码块，而不仅仅是 `for` 循环。它同步所有在代码块的词法作用域内启动的任务，这意味着其 `end` 语句后的语句会等待直到所有任务完成。在
    [Listing 15-4](ch15.xhtml#ch15lis4) 中，`@sync` 确保当我们访问 `s[]` 时，它会有最终值，并且计时结果包括所有任务完成的时间。'
- en: The block in [Listing 15-4](ch15.xhtml#ch15lis4) is a version of the function
    in [Listing 15-3](ch15.xhtml#ch15lis3), with manually spawned tasks. In general,
    the loop
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[Listing 15-4](ch15.xhtml#ch15lis4) 中的代码块是 [Listing 15-3](ch15.xhtml#ch15lis3)
    中函数的一个版本，使用了手动启动的任务。通常，循环'
- en: '[PRE10]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: is semantically equivalent to
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 与语义等效
- en: '[PRE11]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: but their implementations are different, in that, as mentioned earlier, `Threads`
    `.@threads` is *coarse-grained*, dividing the loop into a small number of tasks.
    The manually spawned version creates a new task for every loop iteration.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 但是它们的实现是不同的，正如前面提到的，`Threads` `.@threads`是*粗粒度*的，将循环划分为少量的任务。手动启动的版本会为每次循环迭代创建一个新任务。
- en: The fact that the timings in these two examples are almost the same demonstrates
    that spawning a task in Julia has almost no overhead; we can spawn thousands of
    tasks with little performance penalty. If we move a program using tasks to a different
    machine with more cores, it should run faster with no changes required on our
    part.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个例子的时序几乎相同，表明在Julia中启动任务几乎没有开销；我们可以启动成千上万的任务，性能损失几乎可以忽略不计。如果我们将使用任务的程序移到拥有更多核心的机器上，它应该能够更快运行，而且我们不需要做任何修改。
- en: '**NOTE**'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*In this chapter we perform many timings on bare loops at the top level in
    order to compare the effects of different approaches to concurrency and parallelism
    in as few lines of code as possible. In developing a real program, all timing
    studies should be on functions, preferably in modules. Many compiler optimizations
    are available only for code in functions.*'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*在本章中，我们在顶层对裸循环进行了多次计时，以便比较不同并发和并行方法的效果，并尽可能少的代码行。在开发一个真正的程序时，所有的计时研究应该放在函数中，最好是在模块中。许多编译器优化仅对函数中的代码有效。*'
- en: '**Synchronizing**'
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**同步**'
- en: Using `Folds.map()` or `@threads` synchronizes tasks for us. However, if we
    launch tasks with `Threads.@spawn` manually, we can’t know which have completed
    their work at any particular point in the program. That’s why the program in [Listing
    15-4](ch15.xhtml#ch15lis4) needs a `@sync` macro.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`Folds.map()`或`@threads`可以帮助我们同步任务。但是，如果我们手动使用`Threads.@spawn`启动任务，我们无法知道在程序的某个特定时刻哪些任务已经完成。这就是为什么[Listing
    15-4](ch15.xhtml#ch15lis4)中的程序需要`@sync`宏的原因。
- en: 'The following example illustrates what can happen if we neglect synchronization:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例说明了如果我们忽略同步，可能会发生什么情况：
- en: '[PRE12]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'If we run this program, we’ll see the following output:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们运行这个程序，我们会看到如下输出：
- en: '[PRE13]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Each loop iteration launches a task that mutates the global array, writing to
    one of its locations. However, at the end of the loop, the array `W` doesn’t seem
    to have changed.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 每次循环迭代都会启动一个任务，改变全局数组，向其中的某个位置写入数据。然而，在循环结束时，数组`W`似乎没有发生变化。
- en: Each `@spawn` sends off a task to do its work, and the loop continues to the
    next iteration immediately. Although each spawned job has a built-in delay created
    by the `sleep()` call, the loop itself is complete almost instantaneously. We
    then execute the statement following the loop, printing the value of `W`, which
    hasn’t yet been written to.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 每个`@spawn`都会发送一个任务来执行工作，循环会立即继续到下一次迭代。虽然每个被启动的任务都会受到`sleep()`调用引入的内建延迟，但整个循环几乎是瞬间完成的。然后我们执行循环后面的语句，打印出`W`的值，此时它还没有被写入。
- en: 'If we want to wait at the end of the loop for all tasks spawned within it to
    complete, so that `W` is up to date, we use the `@sync` macro:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要等待循环结束时所有在其中启动的任务完成，以便`W`得到更新，我们可以使用`@sync`宏：
- en: '[PRE14]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'When we run this program, we see:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行这个程序时，我们看到：
- en: '[PRE15]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Instead of synchronizing all the tasks within a block, we can wait for some
    of them to complete, letting the others run their courses:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以选择不对所有任务进行同步，而是等待某些任务完成，让其他任务继续执行：
- en: '[PRE16]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We initialize a `jobs` vector to hold the return values of each call to `@spawn`.
    These are `Task`s, a data type that holds information about an asynchronous task.
    The `wait()` function pauses execution until its argument is ready. We change
    the loop a bit to wait on each iteration for `i` seconds, so each task will take
    longer than the preceding one. As soon as the second job is complete, the next
    instruction, printing `W`, is run.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们初始化一个`jobs`向量来存储每次调用`@spawn`返回的值。这些是`Task`，一种数据类型，用来存储关于异步任务的信息。`wait()`函数会暂停执行，直到其参数准备好。我们稍微修改循环，每次迭代等待`i`秒，这样每个任务所需的时间都会比上一个任务更长。第二个任务完成后，下一条指令，打印`W`，会被执行。
- en: 'The program produces this output:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 程序会输出如下内容：
- en: '[PRE17]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We can see that when the `println()` statement is reached, the first two elements
    of `W` are modified, but the remaining tasks are still running (sleeping).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，当`println()`语句被执行时，`W`的前两个元素被修改了，但其余的任务仍然在运行（处于睡眠状态）。
- en: 'Another useful synchronization function is `fetch()`. Like `wait()`, it receives
    a `Task` as an argument and waits for the task to finish:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有用的同步函数是`fetch()`。像`wait()`一样，它接收一个`Task`作为参数，并等待任务完成：
- en: '[PRE18]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'That function prints this output:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数打印出如下输出：
- en: '[PRE19]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Since the result returned by an assignment is the value assigned, the task that
    executes `W[2] = 2` returns 2, and this gets assigned to `job2` by the call to
    `fetch()`. The condition of `W` at this point is its state immediately after the
    second task is complete.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 由于赋值操作返回的是被赋的值，执行 `W[2] = 2` 的任务返回 2，并通过调用 `fetch()` 将其赋值给 `job2`。此时 `W` 的状态是第二个任务完成后的状态。
- en: '**Yielding**'
  id: totrans-100
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**让出**'
- en: After the scheduler places tasks on all available threads, any remaining spawned
    tasks are on the *queue*, waiting for their turn to run. They will have to wait
    until one of the running tasks finishes or *yields* its place. This system is
    called *cooperative multitasking*, and it’s the model Julia usually applies to
    task scheduling. Some operations cause a task to yield automatically. The more
    important ones are waiting for I/O and sleeping. But if a program involves multiple
    tasks that perform long calculations, it’s our job to break up the calculations
    manually and insert `yield()`s in order to provide opportunities for other tasks
    to run, *unless* we don’t mind each thread waiting for each expensive task on
    it to finish (which may indeed be acceptable).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在调度器将任务分配到所有可用线程后，任何剩余的生成任务都会进入*队列*，等待它们运行的时机。它们必须等到其中一个正在运行的任务完成或*让出*其位置。这个系统被称为*协作式多任务*，它是
    Julia 通常应用于任务调度的模型。一些操作会导致任务自动让出。最重要的操作包括等待 I/O 和休眠。但如果程序涉及多个执行长时间计算的任务，那么我们的任务就是手动拆分计算并插入`yield()`，以便为其他任务提供运行的机会，*除非*我们不介意每个线程等待它上面每个昂贵任务的完成（这可能是可以接受的）。
- en: '[Listing 15-5](ch15.xhtml#ch15lis5) contains two functions that each do the
    identical piece of busywork, applying `f(x)`, which was defined in [Listing 15-1](ch15.xhtml#ch15lis1),
    to a range of numbers. The difference between the two functions is that the first
    does the job in one lump, while the second divides the range into two halves,
    calling `yield()` between them. The `yield()` function tells the scheduler that
    it may suspend the task and run the next task from the queue, if there is one
    waiting. After that task is complete, the suspended task will resume.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 15-5](ch15.xhtml#ch15lis5) 包含两个函数，它们各自执行相同的繁琐任务，应用在[清单 15-1](ch15.xhtml#ch15lis1)中定义的
    `f(x)`，对一系列数字进行操作。这两个函数的区别在于，第一个函数一次性完成所有工作，而第二个函数将范围分成两个部分，并在中间调用 `yield()`。`yield()`
    函数告诉调度器可以挂起该任务，并从队列中运行下一个任务（如果有任务在等待）。该任务完成后，挂起的任务将恢复运行。'
- en: '[PRE20]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '*Listing 15-5: Inserting an opportunity to yield*'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 15-5：插入让出机会*'
- en: The functions assume the existence of a global array named `times`. They place
    the result of calling `time()`, within a tuple with the integer `n` identifying
    the task, onto the end of this array as soon as they begin and just before they
    return. The `time()` function returns the system time in seconds to approximately
    microsecond precision. Its value is uninteresting, but we can use the difference
    between two calls to `time()` to find out how much time passed between two code
    locations, which is a pretty accurate measure of how long the intervening calculation
    took.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数假设存在一个名为 `times` 的全局数组。它们将调用 `time()` 的结果与整数 `n`（表示任务标识符）一起放入一个元组，并在任务开始时和返回之前将其放到该数组的末尾。`time()`
    函数返回系统时间（以秒为单位，精度可达到微秒）。其值并不重要，但我们可以通过两次调用 `time()` 之间的差值，来了解两段代码之间经过了多少时间，这是一个非常准确的衡量中间计算所需时间的方法。
- en: '[Listing 15-6](ch15.xhtml#ch15lis6) spawns three tasks using the first function,
    recording the saved times, and then does the same using the modified function
    with the `yield()` call.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 15-6](ch15.xhtml#ch15lis6) 使用第一个函数生成三个任务，并记录保存的时间，然后使用包含 `yield()` 调用的修改函数执行相同的操作。'
- en: '[PRE21]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '*Listing 15-6: Testing the effects of yielding*'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 15-6：测试让出的效果*'
- en: '[Figure 15-1](ch15.xhtml#ch15fig1) plots the task numbers versus the *elapsed*
    times from the start of each thread-spawning loop, for experiments on a single
    thread.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 15-1](ch15.xhtml#ch15fig1) 绘制了任务编号与每个线程生成循环开始后的*经过*时间的关系图，实验是在单个线程上进行的。'
- en: '![Image](../images/ch15fig01.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/ch15fig01.jpg)'
- en: '*Figure 15-1: Timings for cooperative and selfish tasks*'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 15-1：协作式和自私任务的时间*'
- en: We can see from [Figure 15-1](ch15.xhtml#ch15fig1) that each complete loop takes
    about 5.5 seconds. The experiment with yielding (circles) shows that each task
    does its half loop and then allows the next task in the queue to run. It doesn’t
    resume until all subsequent tasks have completed their first halves and yielded.
    In the experiment without yielding (hexagons), each task monopolizes the thread
    until it’s finished.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 从[图 15-1](ch15.xhtml#ch15fig1)中我们可以看到，每个完整的循环大约需要 5.5 秒。实验中使用让步（圆圈）表明每个任务完成一半循环后，允许队列中的下一个任务运行。直到所有后续任务完成它们的第一半并让步后，它才会恢复。没有让步的实验（六边形）中，每个任务都独占线程直到完成。
- en: With only one thread active, the order of task operations is predictable. Also,
    when using one thread it’s impossible for yielding or any rearrangement of tasks
    to decrease the time to completion of all the calculations; we can’t get something
    for nothing. However, in cases where lighter tasks are mixed with more time-consuming
    ones, allowing the latter to yield will get us access to the results of the lighter
    tasks sooner, which can be desirable in some programs. When multiple threads are
    available, yielding gives the scheduler a chance to migrate tasks among threads,
    keeping them all occupied and potentially increasing the total throughput. This
    rearrangement of tasks is called *load balancing*.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在只有一个线程活动时，任务操作的顺序是可以预测的。而且，使用一个线程时，无法通过任务让步或任何任务重排来缩短所有计算完成的时间；我们无法从中获得免费的时间。然而，在较轻任务与耗时任务混合的情况下，允许后者让步将让我们更早获得轻任务的结果，这在某些程序中是非常可取的。当有多个线程可用时，让步可以给调度器一个机会，在各线程之间迁移任务，使它们都保持忙碌，并可能增加总的吞吐量。这个任务重排的过程称为*负载均衡*。
- en: '**Multiprocessing**'
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**多处理**'
- en: If we decide to run with Grace Hopper’s metaphor that starts this chapter, we
    might say that the multithreading explored in the previous section amounts to
    yoking together a team of oxen to pull a big load, while we can compare the multiprocessing
    explored in this section to dividing the load into separate carts and letting
    each ox pull its cart at its own pace.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们决定沿用 Grace Hopper 在本章开头使用的比喻，我们可以说，前一部分讨论的多线程就像将一队牛车连在一起拉动一大车货物，而本部分讨论的多处理则可以比作将负载分成多个独立的车厢，让每头牛以自己的节奏拉动各自的车厢。
- en: Multiprocessing and distributed computing are closely related concepts, and
    the two terms are often used interchangeably. This style of computation divides
    the work into multiple *processes* that have their own memory spaces. The processes
    may share resources on a single computer or on multiple networked computers. Julia’s
    abstractions make it possible to write the multiprocessed program once and run
    it in a variety of environments.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 多处理和分布式计算是紧密相关的概念，这两个术语通常可以互换使用。这种计算方式将工作划分为多个拥有自己内存空间的*进程*。这些进程可以共享单台计算机上的资源，或者在多个联网计算机上共享资源。Julia
    的抽象使得我们可以编写一次多进程程序，并在各种环境中运行它。
- en: Because the various processes don’t have access to the same memory, any data
    that they need must be copied and sent to them, possibly over a network. Because
    of this, distributed computing is most suited to handling time-consuming tasks
    on small data, especially if computing resources are communicating over a slow
    network such as the internet.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 由于各个进程无法访问相同的内存，它们需要的数据必须被复制并发送给它们，可能通过网络传输。因此，分布式计算最适合处理小数据上的耗时任务，特别是在计算资源通过像互联网这样的慢速网络进行通信时。
- en: Running on a cluster uses multiprocessing to distribute the work to an array
    of processors usually communicating over a higher-bandwidth network, combined
    with the multithreading of the previous section to make the best use of each node.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在集群上运行使用多处理将工作分配到多个处理器，通常这些处理器通过更高带宽的网络进行通信，再结合前一部分中的多线程技术，以便充分利用每个节点。
- en: Multiprocessing is based on the same concept of an asynchronous task that forms
    the basis of the multithreading described previously. It adds the concept of the
    process and the possibility of spawning tasks on more than one process. It allows
    us to do this automatically or with control of individual tasks, with program
    interfaces similar to the ones we explored with multithreading.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 多处理基于与前述多线程相同的异步任务概念。它增加了进程的概念以及在多个进程中生成任务的可能性。它允许我们自动或通过控制单个任务来完成这一操作，且程序接口与我们在多线程中探索过的接口类似。
- en: '***Easy Multiprocessing with pmap***'
  id: totrans-120
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***使用 pmap 轻松实现多处理***'
- en: 'To start the Julia REPL or runtime in multiprocessor mode, use the `-p` flag.
    As with the `-t` flag, it usually makes the most sense to ask for a number of
    processes equal to the number of hardware threads available. On a machine with
    two cores, start Julia using `julia -p2`. This creates two *worker processes*
    that can accept tasks. We’ll have (in this case) a total of three processes: the
    two workers and the executive process, in which the REPL runs if we’re working
    interactively. We can assign tasks to workers automatically or by specifying the
    process number.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 要以多进程模式启动Julia REPL或运行时，请使用`-p`标志。与`-t`标志一样，通常最合理的做法是请求与可用硬件线程数相等的进程数。在一台具有两个核心的机器上，使用`julia
    -p2`启动Julia。这将创建两个*工作进程*，它们可以接收任务。在这种情况下，我们将有三个进程：两个工作进程和执行进程，如果我们是交互式工作，REPL将在其中运行。我们可以通过自动分配任务给工作进程或指定进程编号来分配任务。
- en: With the `-p2` flag, each process will be single-threaded, and each will run
    on its own thread on the two-core machine. We can also use the flags `-p2 -t2`,
    which creates two worker processes, each with access to two threads. Then we have
    the option of spawning tasks on either process, and, within each task, running
    multithreaded or multiprocessing loops. At this point it may seem as if we have
    too many options, and that it would be difficult to decide on a strategy. One
    rational approach that takes good advantage of all available computing resources
    is to launch one worker process on each remote machine, using the mechanism described
    in the next section, and use the `-t auto` flag. This strategy allows each networked
    machine to use all its available threads for shared-memory parallel computing
    and helps to avoid unnecessary data movement.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`-p2`标志时，每个进程将是单线程的，每个进程将在双核机器的各自线程上运行。我们还可以使用`-p2 -t2`标志，这将创建两个工作进程，每个进程可以访问两个线程。这样我们就可以选择在任意进程上生成任务，并且在每个任务中运行多线程或多进程的循环。此时，可能会觉得选项太多，很难决定使用哪种策略。一种合理的方法是，按照下一节描述的机制，在每台远程机器上启动一个工作进程，并使用`-t
    auto`标志。这种策略允许每台联网机器使用其所有可用线程进行共享内存并行计算，帮助避免不必要的数据移动。
- en: Starting Julia with the `-p` flag automatically performs the equivalent of `using
    Distributed`, loading the standard library package that provides utilities for
    multiprocessing. We can retrieve the number of available processes with `nworkers()`,
    provided by `Distributed`. One of the useful utilities from `Distributed` is `pmap()`,
    a distributed version of `map()`, as shown in [Listing 15-7](ch15.xhtml#ch15lis7).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`-p`标志启动Julia会自动执行相当于`using Distributed`的操作，加载提供多进程工具的标准库包。我们可以通过`Distributed`提供的`nworkers()`来获取可用进程的数量。`Distributed`中一个有用的工具是`pmap()`，它是`map()`的分布式版本，如[示例15-7](ch15.xhtml#ch15lis7)所示。
- en: '[PRE22]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '*Listing 15-7: The distributed map*'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例15-7：分布式映射*'
- en: Since each process has its own memory, we have to give copies of all function
    definitions to the workers. That’s what the `@everywhere` macro does ➊. We also
    need to decorate module imports, constant definitions, and everything else that
    the workers need to use with `@everywhere`.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个进程都有自己的内存，我们必须将所有函数定义的副本提供给工作进程。这就是`@everywhere`宏的作用➊。我们还需要用`@everywhere`装饰模块导入、常量定义以及工作进程需要使用的其他所有内容。
- en: Once all the workers have copies of the `f()` function, we can repeat our timing
    tests from [Listing 15-1](ch15.xhtml#ch15lis1) using `pmap()`. This works similarly
    to `Folds.map()`, but instead of orchestrating a synchronized computation by spawning
    tasks to multiple threads in the current process, it spawns tasks in multiple
    processes. If we’ve launched Julia with the worker process number equal to the
    number of physical cores, as suggested earlier, normally each of the processes
    launched by `pmap()` will occupy its own hardware thread, and `pmap()` will assign
    tasks to processes, and hence to threads, in a way that attempts to balance the
    load.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有工作进程都有了`f()`函数的副本，我们就可以使用`pmap()`重新进行[示例15-1](ch15.xhtml#ch15lis1)中的计时测试。它的工作方式类似于`Folds.map()`，但是它不是通过在当前进程中生成多个线程来协调同步计算，而是在多个进程中生成任务。如果我们按照前面的建议，以物理核心数为工作进程数启动Julia，通常`pmap()`启动的每个进程将占用其自己的硬件线程，`pmap()`将以一种尽量平衡负载的方式将任务分配给进程，因此也会分配给线程。
- en: '***Networking with Machine Files***'
  id: totrans-128
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***使用机器文件进行网络连接***'
- en: 'Julia makes multiprocessing on a collection of networked computers almost as
    easy as on a single computer. The first step is to create a text file that contains
    the network addresses of the machines that we want to enlist in helping with the
    calculation, along with some other details. The machines in question must have
    Julia installed, and should contain a directory path identical to the path from
    which we’re running the controlling program. We need to have passwordless `ssh`
    access to each machine. Leaving out some optional details, the *machine file*
    contains one line per machine, in the following form:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Julia使得在一组网络连接的计算机上进行多进程操作几乎和在单台计算机上操作一样简单。第一步是创建一个文本文件，文件中包含我们希望参与计算的机器的网络地址及其他一些细节。相关机器必须已安装Julia，并且应包含与我们运行控制程序的路径相同的目录路径。我们需要对每台机器有免密码`ssh`访问权限。省略一些可选的细节后，*机器文件*每行包含一台机器，格式如下：
- en: '[PRE23]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Here n is the number of workers to start on the machine at host, which can be
    an IP address or a hostname that the controlling computer can resolve. The `:`port
    part is optional and only needed for nonstandard `ssh` ports (ports other than
    22).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，n是要在主机上启动的工作进程数量，主机可以是控制计算机可以解析的IP地址或主机名。`:`端口部分是可选的，仅在使用非标准`ssh`端口（22以外的端口）时需要。
- en: 'For this example, I put two computers into a machine file named *machines*.
    Here’s the entire file:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我将两台计算机放入一个名为*machines*的机器文件中。以下是完整的文件内容：
- en: '[PRE24]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Both hostnames are resolved into their IP addresses by entries in my */etc/hosts*
    file. I could have used the IP numbers directly as well. The computer called `tc`
    is in my house, and `pluton`, a server I maintain mostly for serving Pluto notebooks
    that I drafted for this exercise, is about 1,200 miles away. It listens on port
    86 for `ssh` connections, whereas `tc` uses the standard port. The machine file
    specifies that each machine will use two worker processes.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 两个主机名通过我在*/etc/hosts*文件中的条目解析成它们的IP地址。我也可以直接使用IP地址。名为`tc`的计算机在我家，而`pluton`是一台我主要用来提供我为这次练习起草的Pluto笔记本的服务器，它距离约1,200英里。它监听端口86以接收`ssh`连接，而`tc`使用标准端口。机器文件指定每台机器将使用两个工作进程。
- en: To start a REPL that will use these remote resources as well as two worker processes
    on the machine where the REPL is running, we execute
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动一个同时使用这些远程资源以及在运行REPL的机器上使用两个工作进程的REPL，我们执行
- en: '[PRE25]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: omitting other options, such as specifying a project directory.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 省略其他选项，例如指定项目目录。
- en: 'After a modest delay, we get a REPL prompt. At this point the Julia workers
    on both remote computers are running and waiting to receive tasks. Let’s check
    that everyone’s listening:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 经过一段短暂的延迟后，我们得到了REPL提示符。此时，位于两台远程计算机上的Julia工作进程正在运行，并等待接收任务。让我们检查一下每台机器是否都在监听：
- en: '[PRE26]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The host `sp3` is the laptop where the REPL is running. We use `pmap()` to launch
    six processes, asking each one to run the system command `hostname`. There’s no
    guarantee that they’ll be equally divided, as it turns out in this example, or
    that every machine receives a job—but in this case it turns out that six tasks
    was enough. Using `run()` provides a report identifying which worker ID is assigned
    to which machine. If we need merely the output from the shell command, we can
    use `readchomp()` instead of `run()`.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 主机`sp3`是运行REPL的笔记本电脑。我们使用`pmap()`启动六个进程，要求每个进程运行系统命令`hostname`。并不能保证它们会均等分配，如这个例子中所示，或者每台机器都会接收到任务——但在这个例子中，六个任务已经足够。使用`run()`可以提供一份报告，标明哪个工作进程ID被分配到哪台机器。如果我们只需要来自shell命令的输出，可以使用`readchomp()`代替`run()`。
- en: 'The worker numbers range from 2 to 7 because process 1 is the REPL process.
    We can get a list of workers anytime with:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 工作进程的编号范围从2到7，因为进程1是REPL进程。我们可以随时获取工作进程的列表，方法是：
- en: '[PRE27]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Let’s repeat the timing in [Listing 15-7](ch15.xhtml#ch15lis7) on our three-machine
    network, as shown in [Listing 15-8](ch15.xhtml#ch15lis8).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在我们的三台计算机网络上重复[Listing 15-7](ch15.xhtml#ch15lis7)中的计时，如[Listing 15-8](ch15.xhtml#ch15lis8)所示。
- en: '[PRE28]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '*Listing 15-8: A distributed map over a network of computers*'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 15-8: 在一组计算机网络上进行分布式映射*'
- en: The machines `pluton` and `tc` each have two CPU cores, so we have tripled the
    number of cores available for the calculation. We did observe a speedup, but only
    by about 50 percent over performing the calculation confined to the local machine.
    Computing over the internet incurs significant overhead. Monitoring the remote
    machine’s CPU usage shows that both of `tc`’s CPU cores were active during the
    calculation, at about 70 percent utilization, while `pluton`’s two cores were
    nearly quiescent. The ping time to `pluton` during the experiment was about 50
    times longer than to `tc`, as we might expect from their relative distances. Clearly
    Julia’s scheduler sent more units of work to the closer computer while waiting
    to receive responses from the distant machine.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 机器`pluton`和`tc`每台都有两个CPU核心，因此我们已经将可用于计算的核心数增加了三倍。我们确实观察到加速效果，但比在本地机器上执行计算时只快了约50%。通过互联网计算会产生显著的开销。监视远程机器的CPU使用情况时，发现`tc`的两个CPU核心在计算过程中都处于活动状态，利用率约为70%，而`pluton`的两个核心几乎处于静止状态。实验中，`pluton`的ping时间约是`tc`的50倍，正如我们根据它们的相对距离所预期的那样。显然，Julia的调度程序将更多的工作单位分配给了距离较近的计算机，同时等待从远程机器接收响应。
- en: '***Going Manual with @spawnat***'
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***手动使用@spawnat***'
- en: The `@spawnat` macro spawns an asynchronous task, just as `@spawn` does, but
    on a worker process. We can leave the decision about which process is to receive
    the task by using `@spawnat :any`, or pick one with `@spawnat` n. The macro is
    part of `Distributed`, so it is always available if we’ve started Julia with the
    `-p` flag.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '`@spawnat`宏与`@spawn`一样会启动一个异步任务，但它是在工作进程上执行的。我们可以通过使用`@spawnat :any`来决定哪个进程接收任务，或者通过`@spawnat
    n`来指定某个特定进程。该宏是`Distributed`的一部分，因此如果我们使用`-p`标志启动Julia，它将始终可用。'
- en: 'Let’s check that the macro does what we expect by using it to ask each machine
    to report its hostname:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过使用宏来检查它是否按照预期工作，要求每台机器报告其主机名：
- en: '[PRE29]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The `myid()` function returns the process number of the process where it is
    called. The program prints this message when run:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`myid()`函数返回调用它的进程的进程号。运行程序时，输出如下信息：'
- en: '[PRE30]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: We observed a modest speedup when running a `pmap()` over a network of three
    computers in [Listing 15-8](ch15.xhtml#ch15lis8). [Listing 15-9](ch15.xhtml#ch15lis9)
    shows what happens if we try a version of the loop with manual spawning.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例15-8](ch15.xhtml#ch15lis8)中，我们观察到在三台计算机的网络上运行`pmap()`时取得了适度的加速。[示例15-9](ch15.xhtml#ch15lis9)展示了如果我们尝试使用手动生成版本的循环时会发生什么情况。
- en: '[PRE31]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '*Listing 15-9: Spawning too many distributed processes*'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例15-9：启动过多的分布式进程*'
- en: We would observe terrible performance, worse than performing the calculation
    on a single thread. This is because, unlike the coarse-grained concurrency that
    `pmap()` transforms the loop into, the manual multiprocessing in this loop launches
    thousands of tasks on a handful of processes. Each one requires interprocess communication
    to manage, which far outweighs any gains from concurrency. The situation is different
    from the version in [Listing 15-4](ch15.xhtml#ch15lis4), where the fine-grained
    loop performs as well as the coarse-grained loop, because in that case, all computation
    takes place on one process. Creating tasks within a process is very cheap, but
    interprocess communication is not; therefore, `@spawnat` is best used for small
    numbers of expensive tasks that don’t require massive copying of data.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们会观察到非常糟糕的性能，甚至比在单个线程上执行计算还要差。这是因为，与`pmap()`将循环转化为粗粒度并发不同，这个手动多进程的循环会在少数几个进程上启动成千上万的任务。每个任务都需要进行进程间通信来进行管理，这远远超过了并发带来的任何收益。这个情况与[示例15-4](ch15.xhtml#ch15lis4)中的版本不同，因为在那个版本中，精细粒度的循环表现与粗粒度的循环一样好，因为在那种情况下，所有计算都在一个进程内完成。创建进程内的任务非常便宜，但进程间通信则不是；因此，`@spawnat`最好用于少量昂贵的任务，这些任务不需要大量的数据复制。
- en: '***Multiprocessing Threads with @distributed***'
  id: totrans-157
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***使用@distributed进行多进程线程计算***'
- en: The multiprocessed analogy to the `Threads.@threads` macro is the `@distributed`
    macro. While the former divides a loop into a coarse-grained set of tasks on the
    available threads of the local machine or process, the latter divides a loop into
    a coarse-grained set of tasks spawned across processes, which may be across machines
    on a network.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 与`Threads.@threads`宏类似的多进程类比是`@distributed`宏。前者将一个循环划分为在本地机器或进程的可用线程上执行的粗粒度任务集合，而后者则将一个循环划分为跨进程（可能是在网络中的多台机器）执行的粗粒度任务集合。
- en: '[Listing 15-10](ch15.xhtml#ch15lis10) shows the `@distributed` version of the
    threaded loop in [Listing 15-2](ch15.xhtml#ch15lis2).'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例15-10](ch15.xhtml#ch15lis10)展示了[示例15-2](ch15.xhtml#ch15lis2)中使用线程的循环的`@distributed`版本。'
- en: '[PRE32]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '*Listing 15-10: Using* @distributed'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 15-10: 使用* @distributed'
- en: I performed this timing test on my little network of three machines, each with
    two CPU cores. It’s the best time we’ve achieved for this loop so far. We need
    to use the `@sync` macro with this use of `@distributed`, unlike with `Threads.@threads`,
    which always synchronizes. (Even though we’re not using the results of the calculations,
    leaving off the `@sync` renders the timing meaningless, as in that case the loop
    will return immediately after spawning its tasks.)
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我在我的三台机器的小型网络上执行了这个计时测试，每台机器有两个CPU核心。这是我们迄今为止在这个循环中获得的最佳时间。我们需要在使用 `@distributed`
    时使用 `@sync` 宏，而不像 `Threads.@threads`，它始终会同步。（即使我们没有使用计算结果，省略 `@sync` 会使计时变得没有意义，因为在这种情况下，循环会在生成任务后立即返回。）
- en: 'A common pattern is to combine the results from each iteration of a loop, as
    we did in [Listing 15-4](ch15.xhtml#ch15lis4), using an atomic variable. If we
    insert a function between the `@distributed` macro and the `for` keyword, the
    macro will gather the results from each iteration, reduce them using the function,
    and return the result of combining the reductions from each process. Since returning
    this final result implies synchronization, we can leave off the `@sync` when supplying
    a reduction function:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的模式是将循环每次迭代的结果结合起来，就像我们在[Listing 15-4](ch15.xhtml#ch15lis4)中做的那样，使用原子变量。如果我们在
    `@distributed` 宏和 `for` 关键字之间插入一个函数，该宏将收集每次迭代的结果，使用该函数进行归约，并返回将每个进程的归约结果结合起来的最终结果。由于返回最终结果意味着同步，我们可以在提供归约函数时省略
    `@sync`：
- en: '[PRE33]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The loop is equivalent to
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 该循环等同于
- en: '[PRE34]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: which also automatically performs a reduction across multiple processes.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 它还会自动在多个进程之间执行归约操作。
- en: Why is the loop in [Listing 15-10](ch15.xhtml#ch15lis10) faster than the `pmap()`
    version shown in [Listing 15-8](ch15.xhtml#ch15lis8)? Both approaches perform
    the same calculation distributed over the same machines. As always, when setting
    out to increase performance through concurrency, we’re obligated to analyze the
    workloads in our programs. The loop in this case is over 5,001 function evaluations
    that are nontrivial, but also not very expensive (on the local machine `f(105_000)`
    takes 2.77 ms to evaluate). The `pmap()` function, by default, spawns a new task
    for each iteration of the loop. The scheduler will attempt load balancing by apportioning
    these tasks to various processes as they’re spawned. The speedup through concurrency
    is partially offset by the overhead of scheduling and interprocess communication.
    Due to these considerations, `pmap()`, with no additional tuning parameters, works
    best with a small number of expensive tasks, which doesn’t describe the situation
    in this example.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么[Listing 15-10](ch15.xhtml#ch15lis10)中的循环比[Listing 15-8](ch15.xhtml#ch15lis8)中显示的
    `pmap()` 版本更快？两种方法都在相同的机器上执行相同的计算。正如我们所知，当我们通过并发来提升性能时，必须分析程序中的工作负载。此处的循环是对5,001次函数评估的处理，这些评估既不复杂，也不非常昂贵（在本地机器上，`f(105_000)`
    需要2.77毫秒来评估）。默认情况下，`pmap()` 为循环的每次迭代生成一个新任务。调度程序将尝试通过将这些任务分配给各种进程来进行负载平衡。通过并发带来的加速部分被调度和进程间通信的开销所抵消。考虑到这些因素，`pmap()`
    在没有额外调优参数的情况下，最适合少量昂贵任务，而这种情况并不适用于本示例。
- en: In contrast, the coarse-grained concurrency of the `@distributed` loop works
    well in this case, with a large number of relatively light tasks. Far fewer tasks
    are spawned, and more computer time is devoted to calculation, with less interprocess
    communication and scheduling overhead.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，`@distributed` 循环的粗粒度并发在这种情况下效果很好，因为它处理了大量相对轻量的任务。生成的任务大大减少，更多的计算时间被用于计算，进程间通信和调度开销较少。
- en: In the multithreaded examples, there’s little difference in performance between
    the coarse-grained `Threads.@threads` version and the fine-grained `Folds.map()`
    version. This is because there’s no interprocess communication in that case and
    spawning tasks is very fast.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在多线程示例中，粗粒度的 `Threads.@threads` 版本和精细粒度的 `Folds.map()` 版本之间几乎没有性能差异。这是因为在那种情况下没有进程间通信，生成任务非常快。
- en: 'We can tell `pmap()` to break up the loop into larger chunks using the `batch_size`
    keyword argument:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过 `batch_size` 关键字参数告诉 `pmap()` 将循环分解成更大的块：
- en: '[PRE35]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The default for `batch_size` is 1, meaning one task spawned for each iteration.
    A `batch_size` of n divides the loop into segments of length *up to* n, sending
    each loop segment off to a worker process as a separate task. The example shows
    that we can get performance similar to the `@distributed` loop from `pmap()` by
    dividing the work into halves.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '`batch_size` 的默认值为 1，意味着每次迭代会生成一个任务。若 `batch_size` 为 n，则将循环分割成长度 *最多为* n 的片段，并将每个循环片段作为独立任务发送给工作进程。示例表明，我们可以通过将工作分成两半来使
    `pmap()` 达到与 `@distributed` 循环相似的性能。'
- en: '**Summary of Concurrency in Julia**'
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**Julia 中的并发总结**'
- en: It’s likely that any program intended for large-scale, high-performance computing
    will take advantage of a combination of multiprocessing and multithreading. The
    former allows the program to distribute its work over the nodes of a supercomputing
    cluster, while the latter exploits multiple cores on each node. Therefore, Julia
    programs are often run using combinations of startup flags such as `-p`, `-t`,
    and a reference to a `--machine-file`.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 任何针对大规模高性能计算的程序都可能会利用多进程和多线程的组合。前者允许程序将工作分配到超级计算机集群的节点上，而后者则利用每个节点上的多个核心。因此，Julia
    程序通常会使用如 `-p`、`-t` 以及 `--machine-file` 的启动标志组合来运行。
- en: Julia’s abstractions go far in allowing us to write one version of our program
    that will run fast on a single thread, faster on multicore hardware, and even
    faster on a network of computers. Nevertheless, for the best performance, we can’t
    escape the need to carefully consider the patterns of computation in our programs
    and make it possible for Julia’s scheduler and the operating system to take the
    best advantage of the hardware.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Julia 的抽象使我们能够编写一个版本的程序，该程序可以在单线程上运行得很快，在多核硬件上运行得更快，甚至在计算机网络上运行得更快。然而，为了获得最佳性能，我们无法避免需要仔细考虑程序中的计算模式，并使
    Julia 的调度器和操作系统能够最大限度地利用硬件。
- en: '[Table 15-1](ch15.xhtml#ch15tab1) is a highly simplified summary of the main
    utilities for parallel and distributed processing that we’ve explored throughout
    this chapter.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 15-1](ch15.xhtml#ch15tab1) 是我们在本章中探讨的并行和分布式处理的主要工具的高度简化总结。'
- en: '**Table 15-1:** Multithreaded and Distributed Processing'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 15-1：** 多线程和分布式处理'
- en: '| **Model** | **Threaded (shared memory)** | **Distributed (private memory)**
    |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **线程（共享内存）** | **分布式（私有内存）** |'
- en: '| --- | --- | --- |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Startup** | `julia -t n` | `julia -p n` |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| **启动** | `julia -t n` | `julia -p n` |'
- en: '| **Loops** | `Threads.@threads for` | `@distributed for` |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| **循环** | `Threads.@threads for` | `@distributed for` |'
- en: '| **Maps** | `Folds.map()` | `pmap()` |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| **映射** | `Folds.map()` | `pmap()` |'
- en: '| **Launch task** | `Threads.@spawn` | `@spawnat (p` or `:any)` |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| **启动任务** | `Threads.@spawn` | `@spawnat (p` 或 `:any)` |'
- en: Before tuning the parallelization of a program, we should strive to achieve
    the best single-thread performance possible, by applying the optimization principles
    discussed in previous chapters. The most important of these are type stability,
    correct order of memory accesses, and caution around globals. However, even more
    important than these common pitfalls is the choice of an appropriate algorithm,
    a subject largely beyond the scope of this book.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在调整程序的并行化之前，我们应力求实现最佳的单线程性能，方法是应用前几章中讨论的优化原则。其中最重要的包括类型稳定性、正确的内存访问顺序以及对全局变量的谨慎使用。然而，比这些常见陷阱更为重要的是选择合适的算法，这个话题超出了本书的范围。
- en: '**Conclusion**'
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**结论**'
- en: The subject of concurrency in Julia is large and complicated, and could consume
    a book of this size on its own. The next topics of interest after mastering the
    material in this chapter might be using *shared arrays*, which allow multiprocessing-style
    programming using shared memory; *GPU programming*, which uses a graphical processing
    unit as an array processor; and using the *message passing interface (MPI)* library,
    which is popular in Fortran programs for high-performance scientific computing,
    from within Julia. “Further Reading” contains links to starting points for all
    of these topics.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: Julia 中的并发主题庞大且复杂，单独就能写一本同样大小的书。在掌握本章内容之后，接下来的兴趣点可能是使用 *共享数组*，这允许使用共享内存进行多进程编程；*GPU
    编程*，它使用图形处理单元作为数组处理器；以及在 Julia 中使用 *消息传递接口（MPI）* 库，这是 Fortran 程序中用于高性能科学计算的流行工具。“进一步阅读”部分包含了所有这些主题的起点链接。
- en: '**FURTHER READING**'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '**进一步阅读**'
- en: The `Folds` package resides at [*https://github.com/JuliaFolds/Folds.jl*](https://github.com/JuliaFolds/Folds.jl).
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Folds` 包可以在[*https://github.com/JuliaFolds/Folds.jl*](https://github.com/JuliaFolds/Folds.jl)找到。'
- en: '“A quick introduction to data parallelism in Julia” by Takafumi Arakaki, the
    author of `Folds.jl`, is especially welcome, as `Folds` has little documentation:
    [*https://juliafolds.github.io/data-parallelism/tutorials/quick-introduction/*](https://juliafolds.github.io/data-parallelism/tutorials/quick-introduction/).'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 《Julia 中数据并行的简要介绍》由 `Folds.jl` 的作者 Takafumi Arakaki 撰写，尤其值得关注，因为 `Folds` 的文档较少：[*https://juliafolds.github.io/data-parallelism/tutorials/quick-introduction/*](https://juliafolds.github.io/data-parallelism/tutorials/quick-introduction/)。
- en: General Julia performance tips are available at [*https://docs.julialang.org/en/v1/manual/performance-tips/*](https://docs.julialang.org/en/v1/manual/performance-tips/).
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于 Julia 性能优化的一般技巧，请参考[*https://docs.julialang.org/en/v1/manual/performance-tips/*](https://docs.julialang.org/en/v1/manual/performance-tips/)。
- en: For documentation on shared arrays, visit [*https://docs.julialang.org/en/v1/stdlib/SharedArrays/*](https://docs.julialang.org/en/v1/stdlib/SharedArrays/).
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关共享数组的文档，请访问[*https://docs.julialang.org/en/v1/stdlib/SharedArrays/*](https://docs.julialang.org/en/v1/stdlib/SharedArrays/)。
- en: The GitHub organization JuliaGPU ([*https://juliagpu.org*](https://juliagpu.org))
    serves as an umbrella for Julia packages that implement or can exploit graphics
    processing units for parallelization.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JuliaGPU GitHub 组织 ([*https://juliagpu.org*](https://juliagpu.org)) 为实现或能够利用图形处理单元进行并行化的
    Julia 包提供支持。
- en: Examples of GPU programming with Julia are available at [*https://enccs.se/news/2022/07/julia-for-hpc*](https://enccs.se/news/2022/07/julia-for-hpc).
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Julia 进行 GPU 编程的示例，请参考[*https://enccs.se/news/2022/07/julia-for-hpc*](https://enccs.se/news/2022/07/julia-for-hpc)。
- en: The JuliaParallel GitHub organization is home to a number of packages for parallel
    computing in Julia, including the `MPI` package ([*https://github.com/JuliaParallel/MPI.jl*](https://github.com/JuliaParallel/MPI.jl)
    and the `ClusterManagers` package ([*https://github.com/JuliaParallel/ClusterManagers.jl*](https://github.com/JuliaParallel/ClusterManagers.jl))
    for managing job schedulers like Slurm on high-performance computing clusters.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JuliaParallel GitHub 组织托管了多个用于 Julia 并行计算的包，包括 `MPI` 包 ([*https://github.com/JuliaParallel/MPI.jl*](https://github.com/JuliaParallel/MPI.jl))
    和 `ClusterManagers` 包 ([*https://github.com/JuliaParallel/ClusterManagers.jl*](https://github.com/JuliaParallel/ClusterManagers.jl))，用于管理高性能计算集群上的作业调度器，如
    Slurm。
