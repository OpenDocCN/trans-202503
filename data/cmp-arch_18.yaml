- en: '**15**'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**15**'
- en: '**PARALLEL ARCHITECTURES**'
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**并行架构**'
- en: '![Image](../images/f0355-01.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0355-01.jpg)'
- en: 'As we’ve discussed, computing is splitting along two paths: low-power systems,
    forming the Internet of Things; and high-power computing centers, forming the
    cloud. In previous chapters, we’ve looked at the low-power IoT side of the split:
    embedded and smart systems. This chapter will look at the high-power, high-performance
    systems found in the cloud. Specifically, we’ll look at parallelism, the backbone
    of cloud computing.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们讨论过的，计算正沿着两条路径发展：低功耗系统，形成物联网；以及高功耗计算中心，形成云计算。在前面的章节中，我们已经看过了物联网这一低功耗系统的分支：嵌入式和智能系统。本章将重点讨论云中存在的高功耗、高性能系统。具体来说，我们将探讨并行性——云计算的骨干。
- en: The rise of parallelism is related to Moore’s two laws. While Moore’s law for
    density says we can still put more and more transistors on chips, Moore’s law
    for clock speed is now over, meaning we can’t clock single CPUs faster anymore.
    The number of fetch-decode-execute cycles per second is no longer increasing,
    so we need to find new uses for the extra available transistors to try to do more
    work *within* each cycle, rather than making faster cycles.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 并行化的兴起与摩尔定律的两条法则有关。摩尔定律中的密度法则表明我们仍然可以在芯片上放置越来越多的晶体管，而关于时钟速度的摩尔定律已经结束，这意味着我们无法再让单个CPU的时钟速度更快。每秒的取指-解码-执行周期数不再增加，因此我们需要为额外的可用晶体管找到新的用途，尝试在每个周期内做更多的工作，而不是加快周期速度。
- en: 'For a while, we got away with using the extra silicon to boost classical, serial
    architectures: we made more and more complex CISC instructions to get more work
    per instruction; we added more and bigger registers levels of cache onto the CPU
    silicon; we replicated structures such as arithmetic logic units (ALUs) to enable
    simultaneous execution of branches; and we constructed fancier pipelines and out-of-order
    machines. Together, these techniques have recently delivered double digit–percentage
    yearly gains in instructions per cycle (IPC) rather than cycles per second. But
    we may be running out of easy wins in these areas, so we have to think more in
    terms of digital logic being inherently parallel. Luckily for us, it is.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 一段时间里，我们通过使用额外的硅来提升传统的串行架构：我们增加了越来越复杂的CISC指令，以每条指令获得更多的工作；我们在CPU硅片上增加了更多和更大的寄存器级缓存；我们复制了算术逻辑单元（ALU）等结构，以支持分支的同时执行；我们还构建了更复杂的流水线和乱序执行机器。总的来说，这些技术最近实现了指令每周期（IPC）的双位数年增长，而不是每秒周期数的增长。但在这些领域，我们可能已经接近用尽轻松获得进展的空间，因此我们必须更多地思考数字逻辑本身是并行的。幸运的是，它确实是。
- en: We’ve already encountered register- and instruction-level parallelism. Register-level
    parallelism is the simultaneous per-column execution of digital logic acting on
    bits of a register. For example, all the bits in a word can be negated at the
    same time rather than in sequence. Instruction-level parallelism includes pipelining,
    branch prediction, eager execution, and out-of-order execution (OOOE). These concepts
    don’t appear at the instruction set architecture (ISA) level; they’re invisible
    to the assembly programmer. From the programmer’s perspective, they just make
    serial programs execute faster.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经遇到过寄存器级和指令级并行性。寄存器级并行性是指对寄存器位执行数字逻辑的每列同时执行。例如，可以同时对一个字中的所有位进行取反，而不是依次进行。指令级并行性包括流水线、分支预测、急切执行和乱序执行（OOOE）。这些概念在指令集架构（ISA）层面上并不显现；它们对汇编程序员来说是隐形的。从程序员的角度来看，它们只是让串行程序执行得更快。
- en: 'In this chapter, we’ll focus on the higher-level parallelisms that are visible
    in the ISA and therefore may require the attention of the assembly and perhaps
    also the high level–language programmer. We’ll begin by thinking about parallel
    foundations. Then we’ll turn to the two main types of parallelism: *single instruction,
    multiple data (SIMD)*, as found in modern CPUs and GPUs, and *multiple instruction,
    multiple data (MIMD)*, as found in multicores and cloud computing centers. Finally,
    we’ll wrap up by considering more radical, instructionless forms of parallelism
    that might take architecture beyond the concepts of CPUs and programs.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将重点讨论在ISA中可见的较高层次的并行性，这些并行性可能需要汇编程序员甚至高级语言程序员的关注。我们将从并行基础开始思考，然后转向两种主要的并行类型：*单指令、多数据（SIMD）*，这种并行性在现代CPU和GPU中存在，以及*多指令、多数据（MIMD）*，这种并行性出现在多核处理器和云计算中心。最后，我们将通过考虑一些更激进的、无指令的并行形式，来讨论可能将架构推向超越CPU和程序概念的方向。
- en: Serial vs. Parallel Thinking
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 串行与并行思维
- en: Most of the silicon in a serial computer is used to form memory that sits around
    doing nothing until it’s called on to load from or store to the CPU. In this sense,
    serial computing is like having 1,000 people send all their work to a single worker,
    then stand around waiting for the results to come back to them. This effect is
    known as the *serial bottleneck*.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 串行计算中，大部分硅材料用于构建内存，这些内存大部分时间处于空闲状态，直到需要从 CPU 加载或存储数据时才会被调用。从这个角度看，串行计算就像是让 1,000
    个人将所有的工作交给一个工人，然后站在那里等待结果。这种现象被称为*串行瓶颈*。
- en: 'Parallel computing frees these 1,000 people to all work for themselves. Each
    becomes an active unit of computation: they pass data directly to one another
    as needed, and they get massively more work done than if they were standing around
    waiting for that one worker. Similar gains can occur if we use all the digital
    logic in a computer to constantly perform computation rather than wait around
    for the CPU.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 并行计算使得这 1,000 个人可以各自独立工作。每个人都成为一个活跃的计算单元：他们根据需要直接交换数据，从而比站着等待那个唯一工人的时候完成更多的工作。如果我们利用计算机中的所有数字逻辑来不断执行计算，而不是等待
    CPU，也可以获得类似的效率提升。
- en: 'Parallel thus seems to be obviously faster and better than serial computing.
    But at least until the 2010s, computer scientists tended to get stuck in “serial
    thinking.” Most people are at some point taught the concept of programming using
    a recipe, assuming that only you are in the kitchen and that you’re going to perform
    a sequence of tasks, such as:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，显然并行计算比串行计算要快得多，效果更好。但是至少在 2010 年代之前，计算机科学家们大多仍然停留在“串行思维”中。大多数人都曾在某个阶段学到过用食谱进行编程的概念，假设只有你一个人在厨房工作，你将执行一系列任务，例如：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This is fine on a small scale, but if you’re a head chef running a chain of
    popular restaurants, you’d be in charge of a team of workers, and you’d have to
    schedule in optimal ways to produce the food more efficiently. The field of operations
    research is all about optimally scheduling work like this.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这在小规模情况下没问题，但如果你是一个负责管理一连串热门餐厅的主厨，你就得负责一个团队，并且必须以最优方式安排工作，才能更高效地制作食物。运筹学就是专门研究如何像这样优化工作调度的领域。
- en: How do we take a sequence of instructions like the chicken soup recipe and get
    everything done in the shortest period of time? There are well-known algorithms
    to do this. For example, Henry Gantt’s charts, like the one shown in [Figure 15-1](ch15.xhtml#ch15fig1),
    are used to display and reason about sequences of tasks running in parallel over
    time.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何像鸡汤食谱那样将一系列指令处理完，并在最短时间内完成所有工作呢？有一些著名的算法可以实现这一点。例如，亨利·甘特（Henry Gantt）的甘特图，如[图
    15-1](ch15.xhtml#ch15fig1)所示，常用于展示和分析任务在时间上并行运行的顺序。
- en: '![Image](../images/f0357-01.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0357-01.jpg)'
- en: '*Figure 15-1: A parallel Gantt chart for cooking chicken soup*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 15-1：用于烹饪鸡汤的并行甘特图*'
- en: Simple algorithms exist to generate optimal timings for the tasks, given a list
    of dependencies—that is, a list of which tasks depend on the completion of which
    others before they can begin. A critical path can be calculated for the network,
    which is the sequence of jobs that need to be done on time because they’re the
    bottlenecks.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 存在一些简单的算法，可以根据依赖关系列表（即哪些任务必须在其他任务完成之前才能开始）生成任务的最优时间。可以为任务网络计算出一个关键路径，即需要按时完成的工作序列，因为它们是瓶颈任务。
- en: 'Bletchley Park made heavy use of this style of computation. Machines weren’t
    the only types of computers used there: it was still the era of human computation,
    where “computer” was a human job title. Human computers would sit in a computing
    division ([Figure 15-2](ch15.xhtml#ch15fig2)), all doing parts of computations
    under a manager allocating and scheduling the work in parallel. These programmer
    managers thought about how to break down a large mathematical computation into
    components, distribute the tasks, and collect the results together.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 布莱切利公园（Bletchley Park）大量使用这种计算方式。那里不仅使用了机器作为计算工具，仍然是人类计算的时代，“计算机”这个职称指的是人类的工作。人类计算员会坐在一个计算部门（[图
    15-2](ch15.xhtml#ch15fig2)）里，在一位经理的安排下，分工合作并行进行计算。这些程序经理会考虑如何将一个大型的数学计算任务分解成多个组件，分配任务，并将结果汇总。
- en: '![Image](../images/f0358-01.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0358-01.jpg)'
- en: '*Figure 15-2: A human computing division working in parallel, with a manager
    (standing) scheduling the work*'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 15-2：一个人类计算部门在并行工作，经理（站立者）安排工作*'
- en: Given that the management of teams of parallel workers has existed for a long
    time, and is based on how to design a program of work to efficiently accomplish
    a task, why do so many programmers basically ignore it and think instead in terms
    of recipes and serial computing? If the history of computing had been different
    and started from an operations research perspective rather than from serial algorithms,
    we might have had a much better foundation. Programming—and perhaps the foundations
    of computer science—is now having to move toward parallel thinking due to the
    end of Moore’s law for clock speed. For example, today’s school children might
    write their first ever program in Scratch with multiple sprites all running code
    in parallel. And professional programmers are increasingly having to think in
    terms of SIMD and MIMD, which we’ll study next.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于并行工作团队的管理已经存在很长时间，并且其基础是如何设计工作程序以高效完成任务，为什么那么多程序员基本上忽视它，而是转而思考食谱和串行计算呢？如果计算机历史从运筹学的角度开始，而不是从串行算法开始，我们可能会有一个更好的基础。由于摩尔定律对时钟速度的影响已经结束，编程——甚至可能是计算机科学的基础——现在不得不转向并行思维。例如，今天的孩子们可能会在Scratch中编写他们的第一个程序，所有的角色（精灵）都在并行运行代码。而专业程序员则越来越需要以SIMD和MIMD为思维方式进行思考，这就是我们接下来要学习的内容。
- en: Single Instruction, Multiple Data on CPU
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CPU上的单指令多数据
- en: Our first type of parallelism—single instruction, multiple data—means we’re
    going to take a single instruction (for example, “add one”) and execute that instruction
    uniformly on multiple data items at once. We can split SIMD systems into CPU-
    and GPU-based implementations. Here we’ll look at the CPU-based implementation;
    in the next section, we’ll look at the GPU-based one.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一种并行处理类型——单指令多数据——意味着我们将采取单一指令（例如，“加一”）并在多个数据项上同时执行该指令。我们可以将SIMD系统分为基于CPU和基于GPU的实现。这里我们将关注基于CPU的实现；在下一部分，我们将探讨基于GPU的实现。
- en: '*Introduction to SIMD*'
  id: totrans-24
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '*SIMD简介*'
- en: 'SIMD on CPU is a very CISC-style approach: it involves creating additional
    instructions and digital logic to perform parallel operations as single instructions.
    SIMD instructions pack more than one piece of data into a word, then define instructions
    to apply the same instruction to each piece of data in parallel. For example,
    on a 64-bit machine, instead of using a 64-bit register to store one big 64-bit
    integer, we can partition it into four 16-bit chunks that each hold one 16-bit
    integer. We can then use instructions that understand this packing and operate
    on all four chunks at the same time.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: CPU上的SIMD是一种非常CISC风格的方法：它涉及创建额外的指令和数字逻辑，以便将并行操作作为单个指令执行。SIMD指令将多个数据项打包到一个字中，然后定义指令对每个数据项并行应用相同的操作。例如，在一个64位的机器上，我们可以将一个64位寄存器分割成四个16位的块，每个块存储一个16位的整数。然后我们可以使用理解这种打包方式的指令，同时对四个块进行操作。
- en: In a standard CPU, you might have an `ADD` instruction that adds integers from
    registers r1 and r2, storing the result in r3\. In an SIMD machine, however, you’ll
    have an instruction called something like `SIMD-ADD`, still using the same three
    registers, but using a different data representation to perform addition on pairs
    of 16-bit values from the two registers simultaneously; it then stores the output
    in the third register, packed similarly.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在标准的CPU中，你可能有一个`ADD`指令，它将寄存器r1和r2中的整数相加，并将结果存储在r3中。然而，在SIMD机器中，你将有一个类似`SIMD-ADD`的指令，依然使用相同的三个寄存器，但使用不同的数据表示法同时对来自两个寄存器的16位值对进行加法运算；然后将结果以类似的方式存储在第三个寄存器中。
- en: '**NOTE**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*SIMD instructions originated in early supercomputers, such as the famous 1960s
    Cray supercomputers. SIMD was first brought from supercomputers to desktops by
    Intel via their MMX instructions.*'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*SIMD指令起源于早期的超级计算机，如著名的1960年代Cray超级计算机。SIMD最早是通过英特尔的MMX指令从超级计算机引入桌面计算机的。*'
- en: SIMD can split a 64-bit register into two 32-bit chunks, four 16-bit chunks,
    or eight 8-bit chunks. The four-way split is especially useful for 3D games. It’s
    common for 3D programmers to represent 3D coordinates using *four*-dimensional
    vectors, with the fourth dimension serving as a scaling factor to enable *affine
    transformations*. These are transformations like translations and rotations computed
    using simple matrix-vector multiplications. The 16-bit precision of the numbers
    is usually acceptable for games (though maybe not for serious scientific 3D simulations).
    We’re lucky to live in a world whose number of dimensions, when affinated, is
    a power of 2!
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: SIMD可以将一个64位寄存器分割为两个32位块、四个16位块或八个8位块。四分法对于3D游戏特别有用。3D程序员通常使用*四*维向量表示3D坐标，其中第四维作为缩放因子，用于实现*仿射变换*。这些是通过简单的矩阵-向量乘法计算的变换，如平移和旋转。对于游戏来说，通常16位精度的数字是可以接受的（虽然对于严肃的科学3D模拟可能不够）。我们很幸运生活在一个经过仿射变换后，维度数是2的幂的世界！
- en: SIMD is also a good fit for images and video, in which pixel colors are often
    represented by four numbers for RGB and alpha (as discussed in [Chapter 2](ch02.xhtml)).
    More generally, for most types of multimedia, including audio, it’s common to
    need to do many copies of the same operations for signal processing, so SIMD can
    speed this up even when there’s no obvious 4D structure.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: SIMD也非常适合图像和视频处理，其中像素颜色通常由四个数字表示，分别代表RGB和alpha通道（如在[第2章](ch02.xhtml)中讨论的）。更一般来说，对于大多数类型的多媒体，包括音频，通常需要对信号处理执行许多相同操作的拷贝，因此即使没有明显的4D结构，SIMD也能加速这一过程。
- en: SIMD instructions can be created for use with any ordinary registers, but they’ve
    become more interesting as register sizes have increased to 64 bits. Some architectures
    also include extra registers that are longer than their word length, known as
    *vector* registers; these can store 128, 256, or 512 bits, and they’re intended
    primarily for use with SIMD instructions.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: SIMD指令可以为任何普通寄存器创建，但随着寄存器大小增加到64位，它们变得更加有趣。一些架构还包括比其字长更长的额外寄存器，称为*向量*寄存器；这些寄存器可以存储128、256或512位，主要用于SIMD指令。
- en: Now that we understand the theory of CPU SIMD, let’s look at a concrete example
    of how it’s implemented in x86.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们理解了CPU SIMD的理论，接下来我们来看一个x86中如何实现它的具体例子。
- en: '*SIMD on x86*'
  id: totrans-33
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '*x86上的SIMD*'
- en: We saw the names of the various x86 architectures of the 64-bit era in [Chapter
    13](ch13.xhtml). Beyond the basic amd64 instruction set, most of these architectures
    have focused on adding extensions using different forms of parallelism. Most of
    these ideas originated in high-performance computing and high-end servers but
    have also been introduced to desktop architectures.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第13章](ch13.xhtml)中看到了64位时代各种x86架构的名称。除了基本的amd64指令集之外，这些架构大多数集中在使用不同形式的并行性添加扩展。这些想法大多源自高性能计算和高端服务器，但也已引入到桌面架构中。
- en: The classic CISC approach has been to use the extra transistors to add more
    simple machines and instructions to the ISA, each intended to do more work than
    the regular instructions. This has led to thousands of new CISC instructions,
    added for all manner of special cases such as cryptography, multimedia processing,
    and machine learning. There have been disagreements over the standards for these
    extensions. Everyone implements the same base amd64 ISA, but different manufacturers
    extend it in different ways to add extensions. They try to get users hooked on
    their versions and to desert competitors (a well-known strategy called *embrace-extend-extinguish*).
    This creates headaches for compiler writers who have to create multiple back-ends
    to optimize for the different extensions.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的CISC方法是利用额外的晶体管增加更多简单的机器和指令到ISA中，每个指令旨在比常规指令做更多的工作。这导致了成千上万的新CISC指令的出现，针对各种特殊情况，如加密、多媒体处理和机器学习。对于这些扩展的标准存在一些争议。每个人都实现了相同的基础amd64
    ISA，但不同的制造商以不同的方式扩展它，添加各自的扩展。它们试图让用户依赖自己的版本，并抛弃竞争对手（这是一种著名的策略，叫做*拥抱-扩展-消灭*）。这给编译器开发者带来了麻烦，因为他们必须为不同的扩展创建多个后端来进行优化。
- en: Most of the new registers and instructions added to x86 during the 64-bit era
    have been for SIMD. [Figure 15-3](ch15.xhtml#ch15fig3) shows the complete user
    register set of modern amd64.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在64位时代，x86新增的大部分寄存器和指令都与SIMD有关。[图15-3](ch15.xhtml#ch15fig3)展示了现代amd64的完整用户寄存器集。
- en: '![Image](../images/f0360-01.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0360-01.jpg)'
- en: '*Figure 15-3: The full register set for amd64*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*图15-3：amd64的完整寄存器集*'
- en: The SIMD registers are the ones with “MM” in their names. Notice how new SIMD
    registers have appeared over time, usually by extending an existing register to
    have more bits. When extensions are made, x86 backward compatibility requires
    the original shorter form to still be named and usable, as well as the extended
    form. This requires many different versions of instructions to be provided.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: SIMD 寄存器是那些名称中包含“MM”的寄存器。注意到，随着时间的推移，新的 SIMD 寄存器逐渐出现，通常通过扩展现有的寄存器来增加更多的位数。当进行扩展时，x86
    向后兼容性要求原本较短的形式仍然要有名字并且可用，同时也要有扩展后的形式。这就要求提供多种版本的指令。
- en: '**MMX**'
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**MMX**'
- en: '*MMX* was the first x86 SIMD extension. It’s never been officially defined
    what MMX stands for, and indeed this has been a matter of legal trademarking debate
    between Intel and AMD. Suggestions include “matrix math extensions” and “multimedia
    extensions.”'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*MMX* 是第一个 x86 SIMD 扩展。至今没有正式定义 MMX 代表什么，事实上，这一直是英特尔和 AMD 之间关于商标的法律争议问题。有人提出的建议包括“矩阵数学扩展”和“多媒体扩展”。'
- en: MMX extended the previous amd64 floating-point registers to 64 bits, similar
    to how 32-bit registers, such as EAX, were extended to the 64-bit RAX. The new
    registers have names MM0 through MM7 and still exist on modern machines.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: MMX 将之前的 amd64 浮点寄存器扩展到了 64 位，类似于 32 位寄存器（如 EAX）扩展为 64 位的 RAX。新的寄存器命名为 MM0 到
    MM7，并且在现代机器上仍然存在。
- en: Each MMX register can be used for integer-only SIMD as either a single 64-bit
    integer, two 32-bit integers, four 16-bit integers, or eight 8-bit integers. Integer
    SIMD is particularly useful and fast for processing images, including for 2D sprite-based
    games and video codecs.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 MMX 寄存器可以作为一个单独的 64 位整数、两个 32 位整数、四个 16 位整数或八个 8 位整数来进行仅整数的 SIMD 操作。整数 SIMD
    特别适用于处理图像，包括 2D 精灵游戏和视频编解码器。
- en: 'MMX instructions begin with `p` for “packed,” such as `paddd` for “packed add
    doubles.” New move instructions—`movb, movw`, and `movd`—copy arrays of bytes,
    words, or doubles into single MMX registers. For example, the following defines
    two arrays of 32-bit doubles: *a* = [4, 3] and *b* = [1, 5]. It loads *a* as packed
    doubles into MM0 and *b* into MM1\. It then packed adds the doubles, leaving [5,
    8] in MM0:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: MMX 指令以 `p` 表示“打包”，例如 `paddd` 表示“打包加双字”。新的移动指令——`movb`, `movw` 和 `movd`——将字节、字或双字数组复制到单个
    MMX 寄存器中。例如，下面定义了两个 32 位双字数组：*a* = [4, 3] 和 *b* = [1, 5]。它将 *a* 作为打包的双字加载到 MM0
    中，将 *b* 加载到 MM1 中。然后，它对这些双字进行打包加法运算，最终将 [5, 8] 存储在 MM0 中：
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: MMX adds a *lot* of new instructions, because every arithmetic operation has
    to exist in each of the packed forms for bytes, words, and doubles.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: MMX 添加了*大量*的新指令，因为每个算术操作必须在字节、字（word）和双字（double）打包形式中都存在。
- en: '**SSE**'
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**SSE**'
- en: Intel’s version of x86 SIMD has been extended several times since MMX, as SSE,
    SSE2, SSE3, SSE4, and SSE4.2 (where SSE stands for streaming SIMD extensions).
    AMD’s latest incompatible competitor is, confusingly, called SSE4a. Unlike MMX,
    the SSE series provides for floating-point SIMD as well as integers. This makes
    it particularly useful for accelerating 3D math for games and other physics simulations.
    (MMX was unsuccessful by the benchmarks of its time, which focused heavily on
    the 3D game *Quake*.)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 英特尔的 x86 SIMD 版本自 MMX 以来已经多次扩展，分别为 SSE、SSE2、SSE3、SSE4 和 SSE4.2（其中 SSE 代表流式 SIMD
    扩展）。AMD 最新的、不可兼容的竞争版本令人困惑地被称为 SSE4a。与 MMX 不同，SSE 系列不仅支持整数的 SIMD 还支持浮点数。这使得它在加速游戏和其他物理仿真中的
    3D 数学计算时特别有用。（根据当时的基准测试，MMX 在 3D 游戏 *Quake* 中的表现并不成功。）
- en: Unlike MMX’s extension of the old floating-point registers, SSE adds completely
    new, 128-bit vector registers, called XMM0 through XMM31\. The number of these
    has grown with the SSE versions. They can be split into 8-, 16-, 32-, or 64-bit
    chunks, with chunks representing either floating points or integers. Each arithmetic
    operation thus has many instructions depending on these choices.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 与 MMX 扩展旧的浮点寄存器不同，SSE 添加了完全新的 128 位向量寄存器，命名为 XMM0 到 XMM31。这些寄存器的数量随着 SSE 版本的更新而增加。它们可以被拆分为
    8 位、16 位、32 位或 64 位的块，每个块可以表示浮点数或整数。因此，每个算术操作会有多种指令，具体取决于这些选择。
- en: Most SSE instructions have the letter `p` for “packed” added to their mnemonics,
    at either the beginning or the end. For example, the top-left of [Figure 15-4](ch15.xhtml#ch15fig4)
    shows an SSE compare for equality with the `cmpeqps` instruction. The name comes
    from `cmpeq`, the standard x86 instruction, plus `ps` to indicate “packed, single-precision.”
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数 SSE 指令在助记符的开头或结尾加上字母 `p` 来表示“打包（packed）”。例如，[图 15-4](ch15.xhtml#ch15fig4)的左上方展示了用于相等比较的
    SSE 指令 `cmpeqps`。该名称来自于标准的 x86 指令 `cmpeq`，再加上 `ps` 表示“打包，单精度”。
- en: '![Image](../images/f0361-01.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0361-01.jpg)'
- en: '*Figure 15-4: The contents of two SSE registers, XMM0 and XMM1, as SSE instructions
    are carried out, comparing the two sets of data in different ways: equality (top
    left), inequality (top right), less than (bottom left), and not less than (bottom
    right)*'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 15-4：两个 SSE 寄存器 XMM0 和 XMM1 中的内容，执行 SSE 指令时，以不同方式比较这两组数据：相等（左上）、不等（右上）、小于（左下）和不小于（右下）*'
- en: In the top-right of [Figure 15-4](ch15.xhtml#ch15fig4), the `compneqps` instruction
    similarly extends `cmpneq` (compare not equal) to SSE.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 15-4](ch15.xhtml#ch15fig4)的右上角，`compneqps` 指令类似地将 `cmpneq`（不等比较）扩展到 SSE。
- en: 'The following code shows examples of getting arrays of floats in and out of
    SSE’s XMM registers and performing arithmetic on them:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了如何将浮点数数组从 SSE 的 XMM 寄存器中读出并写入，并对其执行算术运算：
- en: '[PRE2]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here, the `addps` instruction adds the four numbers in XMM1 to the four numbers
    in XMM0, and stores the result in XMM0\. For the first float, the result will
    be 1.1 + 5.5 = 6.6\. The `mulps` instruction multiplies the four numbers in XMM1
    with the results from the previous calculation (in XMM0), and stores the result
    in XMM0\. For the first floats, this result will be 5.5 × 6.6 = 36.3\. The `subps`
    instruction subtracts the four numbers from `v2` (in XMM1, still unchanged) from
    the result of the previous calculation (in XMM0). For the first float, its result
    will be 36.3 – 5.5 = 30.8.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`addps` 指令将 XMM1 中的四个数字加到 XMM0 中的四个数字，并将结果存储回 XMM0。对于第一个浮点数，结果将是 1.1 + 5.5
    = 6.6。`mulps` 指令将 XMM1 中的四个数字与先前计算的结果（XMM0 中的结果）相乘，并将结果存储回 XMM0。对于第一个浮点数，结果将是
    5.5 × 6.6 = 36.3。`subps` 指令将来自 `v2`（仍未改变的 XMM1 中）的四个数字从先前计算的结果（XMM0 中）中减去。对于第一个浮点数，结果将是
    36.3 – 5.5 = 30.8。
- en: '**AVX**'
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**AVX**'
- en: Two generations of *Advanced Vector Extensions (AVX)* have added longer vectors
    than SSE, having 256- and 512-bit lengths. The new 256-bit registers are called
    YMM0 through YMM31, and the new 512 registers are called ZMM0 through ZMM31.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 两代 *高级矢量扩展（AVX）* 增加了比 SSE 更长的向量，具有 256 位和 512 位的长度。新的 256 位寄存器称为 YMM0 到 YMM31，新的
    512 位寄存器称为 ZMM0 到 ZMM31。
- en: 'AVX instructions often have the same names as, and behave similarly to, SSE
    instructions, but they start with a `v`. For example, to add eight pairs of 32-bit
    (double) floating-point numbers using AVX-256, we can do this:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: AVX 指令通常与 SSE 指令具有相同的名称，并且行为类似，但它们以 `v` 开头。例如，要使用 AVX-256 对八对 32 位（双精度）浮点数进行加法，我们可以这样做：
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note how the form of AVX arithmetic is different from MMX and SSE, with addition
    now taking three operands rather than two.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 AVX 算术的形式与 MMX 和 SSE 的不同，现在加法需要三个操作数，而不是两个。
- en: '**Domain-Specific Instructions Using SIMD**'
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**使用 SIMD 的领域特定指令**'
- en: 'As mentioned earlier, SIMD is usually considered a very CISC approach, as it
    involves adding lots of new instructions to the ISA. Initially, these arise from
    the many combinations of packing styles, data types, and arithmetic operations.
    Going beyond simple replication of arithmetic across the chunks, CISC SIMD has
    also tended to create further complex instructions. These might include *horizontal
    SIMD*, which means instructions that *combine* information from multiple chunks
    in the same register. For example, there are instructions that find the minimum
    of multiple chunks in a register: `phminposuw` in SSE or `vphminposuw` in AVX.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，SIMD 通常被认为是一种非常 CISC 的方法，因为它涉及向 ISA 添加大量新指令。最初，这些指令来源于许多不同的打包方式、数据类型和算术操作的组合。在简单地跨块复制算术操作的基础上，CISC
    SIMD 还倾向于创建更复杂的指令。这些指令可能包括 *水平 SIMD*，意味着那些 *合并* 来自同一寄存器中多个块的信息的指令。例如，有一些指令可以找到寄存器中多个块的最小值：SSE
    中的 `phminposuw` 或 AVX 中的 `vphminposuw`。
- en: Horizontal SIMD instructions also sometimes sequence simpler SIMD instructions
    together. For example, “dot product of packed double-precision floating-point
    values” (`dppd` on SSE; `vdppd` on AVX) is a single instruction that performs
    a complete vector dot product, often used in games, 3D simulations, and machine
    learning. This consists of first SIMD multiplying pairs of chunks, then summing
    the results horizontally along the register.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 水平 SIMD 指令有时也会将简单的 SIMD 指令串联在一起。例如，“打包双精度浮点值的点积” (`dppd` 在 SSE 上；`vdppd` 在 AVX
    上) 是一条执行完整向量点积的单一指令，常用于游戏、3D 仿真和机器学习中。它首先执行 SIMD 相乘一对对的块，然后在寄存器中水平地对结果进行求和。
- en: 'Cryptography has been a major source of CISC SIMD extensions. For example,
    128-bit AES is the NSA-approved standard for internet encryption. It’s computed
    via four steps: ShiftRows, SubBytes, MixColumns, and AddRoundKey. Intel has added
    CISC instructions for each of these steps, and also a single mega-instruction
    that combines them all to perform an entire round (`aesenc` on SSE; `vaesenc`
    on AVX). If, like most end users, you spend the bulk of your computing time streaming
    videos over HTTPS, then this CISC approach gives a useful targeted speedup for
    your use case. But Intel’s extensions have been controversial, with Linus Torvalds
    stating that the NSA and Intel have likely back-doored them in the digital logic,
    and advising Linux programmers not to use them.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 加密学一直是CISC SIMD扩展的一个重要来源。例如，128位AES是NSA批准的互联网加密标准。它通过四个步骤计算：ShiftRows、SubBytes、MixColumns和AddRoundKey。英特尔为这些步骤中的每一步添加了CISC指令，并且还有一个组合它们的单一大指令来执行完整的一轮（SSE上的`aesenc`；AVX上的`vaesenc`）。如果像大多数终端用户一样，你的大部分计算时间都用于通过HTTPS流式传输视频，那么这种CISC方法为你的使用场景提供了有用的加速。但英特尔的扩展一直颇具争议，Linus
    Torvalds曾表示，NSA和英特尔可能已在数字逻辑中留下后门，并建议Linux程序员不要使用这些扩展。
- en: Machine learning—specifically, neural network—operations have been the latest
    target for SIMD CISC, via Intel’s Vector Neural Network Instructions (AVX512-VNNI)
    and Brain Floating Point (AVX512-BF16) extensions to AVX-512, which arrived in
    Golden Cove and are marketed together as *DL Boost*. For example, “multiply and
    add unsigned and signed bytes with saturation” (`vpdpbusds`) performs a full neuron’s
    sigmoid-like activation from its inputs and weights in a single instruction. Some
    researchers have been able to train neural networks faster than on a GPU using
    these and similar SIMD CISC instructions, so this is now a competition between
    CPU and GPU architectures.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习——特别是神经网络——操作已成为最新的SIMD CISC目标，借助英特尔的向量神经网络指令（AVX512-VNNI）和大脑浮动点（AVX512-BF16）扩展到AVX-512，这些扩展出现在Golden
    Cove架构中，并作为*DL Boost*一起推广。例如，“乘法和加法无符号和有符号字节并进行饱和”（`vpdpbusds`）通过单条指令执行整个神经元的类似Sigmoid激活操作，包括输入和权重。部分研究人员通过使用这些和类似的SIMD
    CISC指令，能够比在GPU上训练神经网络更快，因此现在这已经成为CPU和GPU架构之间的竞争。
- en: '**Compiler Writers and SIMD**'
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**编译器开发者与SIMD**'
- en: The only compiler writers who understand and care about x86 SIMD are the ones
    working for Intel and AMD, so proprietary CISC compilers are likely to go faster
    than open source or third-party compilers (such as gcc) for numerical code on
    CISC architectures.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一理解并关心x86 SIMD的编译器开发者是为英特尔和AMD工作的人员，因此，专有的CISC编译器在CISC架构上的数值代码处理速度可能会比开源或第三方编译器（如gcc）更快。
- en: Intel has released various C libraries, implemented with its own compilers,
    that convert high-level numerical code into SIMD instructions. These include Integrated
    Performance Primitives (IPP), the Math Kernel Library (MKL), and the IPEX PyTorch-to-AVX
    compiler for neural networks.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 英特尔发布了各种C库，使用其自有编译器实现，将高级数值代码转换为SIMD指令。这些库包括集成性能原语（IPP）、数学核心库（MKL）和用于神经网络的IPEX
    PyTorch-to-AVX编译器。
- en: Open source compiler writers find it hard to get excited about particular proprietary
    hardware extensions and CISC, and they generally prefer to spend their valuable,
    scarce time on more general-purpose work to benefit the wider community, such
    as generating beautiful RISC code that will be accelerated through methods like
    pipelining and OOOE.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 开源编译器的开发者通常对特定的专有硬件扩展和CISC不感兴趣，他们一般更愿意将宝贵的时间花费在更具通用性的工作上，旨在造福更广泛的社区，比如生成精美的RISC代码，并通过流水线和超指令执行（OOOE）等方法加速。
- en: '**SIMD ON RISC-V**'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**SIMD与RISC-V**'
- en: SIMD instructions are a fundamentally CISCy idea—they add lots of new instructions
    and digital logic, making the instruction set more complex. However, SIMD extensions
    have also been proposed for RISC-V, such as P for parallel SIMD instructions and
    V for vector instructions.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: SIMD指令本质上是CISC的思想——它们增加了大量新的指令和数字逻辑，使得指令集变得更加复杂。然而，SIMD扩展也已经被提出用于RISC-V，例如用于并行SIMD指令的P扩展和用于向量指令的V扩展。
- en: No real-world architecture is purely RISC or CISC nowadays, and there’s no law
    against a primarily RISC-style architecture such as RISC-V adding some CISCy features,
    especially as RISC-V’s extension system makes them completely optional. There
    have, however, been loud opposing voices in the open source RISC-V community,
    offended by this potential CISC insertion. Even its founders have published an
    “SIMD considered harmful” warning.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在没有任何现实世界中的架构是纯粹的 RISC 或 CISC，而且没有法律禁止像 RISC-V 这样主要采用 RISC 风格的架构加入一些 CISC 特性，特别是因为
    RISC-V 的扩展系统使得这些特性完全是可选的。然而，在开源 RISC-V 社区中确实有一些强烈反对的声音，他们对这种潜在的 CISC 插入感到不满。即使是其创始人也发布了“SIMD
    被认为有害”的警告。
- en: Good RISC style is rather to make use of extra available silicon to optimize
    pipelines and OOOE, for example by replicating ALUs, registers, and other components
    needed to run several branches in parallel. This approach may be made harder by
    the existence of SIMD instructions, especially the most extreme CISCy, multi-step
    ones, such as dot products, which do both multiplication and addition. Multicores
    are generally more acceptable to RISC, and RISC-V has an A extension for atomic
    memory instructions that provides multicore transactions for them.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 好的 RISC 风格实际上是利用额外的可用硅来优化流水线和超出顺序执行（OOOE），例如通过复制 ALU、寄存器和其他组件来支持多个分支的并行执行。SIMD
    指令的存在，特别是最极端的 CISC 风格的多步指令，如点积（既进行乘法又进行加法），可能会使这种方法变得更加困难。多核通常更适合 RISC，RISC-V
    也有一个用于原子内存指令的 A 扩展，提供了多核事务支持。
- en: SIMD on GPU
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GPU 上的 SIMD
- en: SIMD appears on a much larger scale in GPUs. SIMD on CPU gives speedups of 2
    to 64 times, based on the number of chunks packed into a word. By contrast, a
    GPU can scale to thousands of identical instructions running simultaneously across
    the data.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: SIMD 在 GPU 中出现的规模要大得多。CPU 上的 SIMD 提供了 2 到 64 倍的加速，具体取决于每个字中打包的块的数量。相比之下，GPU
    可以扩展到数千条相同的指令同时在数据上运行。
- en: In [Chapter 13](ch13.xhtml), we saw how graphics cards evolved, from providing
    hardware implementations of graphics commands, to providing their own parallel
    machine code for non-graphical computing. Initially, this was very hard, geeky
    work, involving encoding big computational algorithms into shaders as if they
    were graphics computations, exploiting the highly parallel 3D rendering hardware,
    then decoding the resulting images to obtain the computational output.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 13 章](ch13.xhtml) 中，我们看到显卡是如何发展的，从提供图形命令的硬件实现，到为非图形计算提供自己的并行机器码。最初，这是一项非常困难、极客化的工作，涉及将大规模的计算算法编码成着色器，仿佛它们是图形计算，利用高度并行的
    3D 渲染硬件，然后解码生成的图像以获取计算结果。
- en: GPU manufacturers quickly noticed this as a new market and redesigned their
    shader languages into general-purpose GPU instruction sets for general-purpose
    SIMD computing. These can be used to implement graphics shaders as before, but
    now also to implement general, non-graphical SIMD computations. This evolution
    has occurred rapidly to form GPUs that aren’t built for graphics at all, but rather
    for general higher-power scientific and machine learning computation, especially
    neural networks. This is why “graphics processing unit” is now a misnomer; a modern
    GPU is really more of a “general parallel unit.”
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 制造商迅速意识到这是一个新市场，并重新设计了他们的着色器语言，转变为通用 SIMD 计算的通用 GPU 指令集。这些指令集不仅可以用于实现之前的图形着色器，现在还可以用于实现通用的非图形
    SIMD 计算。这一演变迅速发生，形成了不再专为图形设计的 GPU，而是为了通用更高功率的科学计算和机器学习计算，尤其是神经网络。因此，“图形处理单元”现在已经是一个误称；现代
    GPU 更像是一个“通用并行单元”。
- en: '*GPU Architecture*'
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '*GPU 架构*'
- en: It used to be difficult to discuss GPU architectures in general because they
    were each developed by different companies according to different, secret designs.
    However, several of these manufacturers got together to agree on *Khronos* standards,
    which define ways of thinking about GPU hardware architecture at a level of abstraction
    common to most of them. This enables most GPUs, and also some other devices, to
    be viewed as if their hardware was implemented as the standard architecture, so
    the programmer doesn’t have to care about their individual details so much. Programmers
    can also easily swap one GPU for another, including between manufacturers, as
    long as the new manufacturer provides software tools to convert programs from
    Khronos standards to their more specific machine codes.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 过去，讨论GPU架构通常很困难，因为每个GPU都由不同的公司根据不同且保密的设计开发。然而，一些制造商联合起来达成了*Khronos*标准，定义了一种思考GPU硬件架构的方式，这种方式在大多数厂商的抽象层次上是通用的。这使得大多数GPU，以及一些其他设备，能够被视为其硬件实现了标准架构，这样程序员就不需要过多关注它们的个别细节。只要新厂商提供将程序从Khronos标准转换为其更具体机器代码的软件工具，程序员也可以轻松地将一个GPU替换成另一个，甚至是跨厂商的更换。
- en: Khronos defines a hierarchy of named entities. We have a single *host* (the
    computer), which may have multiple *compute devices* inside (the physical GPU
    cards or chips). There are multiple *compute units (CUs)* in these, and they each
    contain *processing elements (PEs)*.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Khronos定义了一种命名实体的层次结构。我们有一个单一的*主机*（计算机），它可能包含多个*计算设备*（物理GPU卡或芯片）。这些设备中有多个*计算单元（CUs）*，每个计算单元包含*处理元素（PEs）*。
- en: The main structure is the CU, whose PEs contain their own registers and ALUs,
    but share a single program counter, instruction register, and control unit. This
    creates the SIMD, as each processing element within the CU executes the same instruction
    from the PC in parallel, but on its own data from its own registers. A CU may
    also contain other structures such as a cache and some shared memory, allowing
    the PEs to communicate with one another. Compute devices typically package several
    independent CUs together. SIMD exists only within single CUs.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 主要结构是CU，其PE包含自己的寄存器和算术逻辑单元（ALU），但共享一个程序计数器、指令寄存器和控制单元。这创建了SIMD，因为CU中的每个处理元素都并行执行来自PC的相同指令，但使用其自身寄存器中的数据。CU还可能包含其他结构，如缓存和共享内存，允许PE彼此通信。计算设备通常将多个独立的CU打包在一起。SIMD只存在于单一的CU内部。
- en: '**NOTE**'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*Khronos standards are designed to be generalizable, not just to many different
    types of GPU but also to any other SIMD-implementing technologies. For example,
    they could also be implemented on an FPGA or on an SIMD CPU in some cases. This
    is why the generic name “compute device” is used in place of “GPU.”*'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*Khronos标准旨在具有普适性，不仅适用于多种不同类型的GPU，还适用于任何实现SIMD的技术。例如，在某些情况下，它们也可以在FPGA或SIMD
    CPU上实现。这就是为什么使用通用名称“计算设备”来替代“GPU”的原因。*'
- en: The die shot in [Figure 15-5](ch15.xhtml#ch15fig5) shows what GPU silicon actually
    looks like. It shows that the die is arranged much more regularly than the layout
    of a CPU, with square CUs split evenly throughout and a general cache in the middle.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[图15-5](ch15.xhtml#ch15fig5)中的硅片照片展示了GPU硅片的实际样子。它显示出硅片的排列比CPU的布局更加规律，方形的CU均匀分布在各处，中央有一个通用缓存。'
- en: '![Image](../images/f0366-01.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0366-01.jpg)'
- en: '*Figure 15-5: A die shot taken from an Nvidia Pascal GPU chip*'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*图15-5：一张来自Nvidia Pascal GPU芯片的硅片照片*'
- en: '*Nvidia GPU Assembly Programming*'
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '*Nvidia GPU汇编编程*'
- en: 'In CPUs, SIMD is expressed by single instructions that perform a fixed number
    (for example, four or eight) of identical operations in parallel. Programs are
    written as a series of such instructions, with one instruction referenced from
    the program counter executing at a time. In GPUs, however, SIMD is usually expressed
    differently: we want to enable large and arbitrary numbers of copies of the instruction
    to run in parallel, rather than a fixed number.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在CPU中，SIMD通过执行单条指令来并行完成固定数量（例如，四个或八个）的相同操作。程序以一系列此类指令编写，每次执行的指令由程序计数器引用。然而，在GPU中，SIMD通常以不同的方式表达：我们希望使大量和任意数量的指令副本并行运行，而不是固定数量。
- en: Khronos defines software-level concepts to express GPU SIMD programs. A *kernel*
    is a usually small function written by the user programmer, with the intent of
    each (assembled) line of the code running as a single instruction on multiple
    data. A *work-item* is one instance of the kernel—that is, the sequence of instructions
    as applied to a single piece of data by running on one processing element. A *work-group*
    is the collection of work-item instances running over the multiple data items.
    Unlike CPU SIMD, kernel code is written by describing its effects on a single
    work-item. When you run a kernel, you choose and specify how many work-items you
    want to launch in parallel.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Khronos定义了软件级的概念，用来表达GPU的SIMD程序。*内核*是用户程序员编写的通常较小的函数，目的是让代码中的每一行（汇编后的）作为单个指令在多个数据上运行。*工作项*是内核的一个实例——也就是说，通过在一个处理元素上运行，指令序列应用到单个数据项上。*工作组*是多个工作项实例的集合，它们在多个数据项上并行运行。与CPU的SIMD不同，内核代码是通过描述其对单个工作项的影响来编写的。当你运行一个内核时，你可以选择并指定要并行启动多少个工作项。
- en: 'Graphics shaders are traditionally small, simple programs that perform a fixed
    sequence of operations on each pixel. They’re thus well suited to SIMD, with work-items
    for each pixel stepping through the same instructions in the same order. However,
    other kinds of compute kernels may require branching. This presents a problem,
    somewhat in the same spirit as pipeline hazards, where different work-items need
    to take different branches. Taking different branches destroys the SIMD because
    the work-items are no longer running the same instructions. There are two methods
    to deal with kernel branching: masking and subgroups.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图形着色器通常是小型、简单的程序，它们对每个像素执行一系列固定的操作。因此，它们非常适合SIMD，每个像素的工作项按照相同的顺序执行相同的指令。然而，其他类型的计算内核可能需要分支。这就会出现一个问题，类似于流水线冒险的情况，不同的工作项需要执行不同的分支。执行不同的分支会破坏SIMD，因为工作项不再执行相同的指令。处理内核分支有两种方法：屏蔽和子组。
- en: Like 1980s CPU designers, modern GPU designers each maintain their own, mutually
    incompatible ISAs that define their platforms. Nvidia is the most popular GPU
    designer at the time of writing, so we’ll learn to program their ISA as an example
    of programming GPUs in general. As with other systems we’ve programmed in this
    book, we’ll simplify the truth a little in order to make learning easier. We’ll
    here assume that all general-purpose Nvidia GPUs implement a single ISA called
    PTX (Parallel Thread Execution), and we’ll learn to program in PTX assembly. You
    can assemble and run PTX programs on any general-purpose Nvidia GPU.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 就像1980年代的CPU设计师一样，现代GPU设计师每个人都维护自己的、不兼容的ISA（指令集架构），这些ISA定义了他们的平台。Nvidia是撰写本文时最流行的GPU设计公司，因此我们将以编程Nvidia的ISA为例，学习如何编程GPU。与我们在本书中编程的其他系统一样，为了简化学习，我们将略微简化事实。我们假设所有通用的Nvidia
    GPU都实现了一个名为PTX（并行线程执行）的单一ISA，我们将学习如何用PTX汇编编程。你可以在任何通用的Nvidia GPU上汇编和运行PTX程序。
- en: '**Data Movement and Arithmetic**'
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**数据移动和算术运算**'
- en: 'The following is a simple PTX kernel program. Like all kernels, many copies
    forming a work-group are intended to run in SIMD parallel, so this code describes
    only the actions of a single work-item:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个简单的PTX内核程序。像所有内核一样，构成工作组的多个副本旨在并行运行，因此这段代码仅描述了单个工作项的操作：
- en: '[PRE4]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'PTX assembly is written with semicolons at line ends, and two slashes for comments.
    Register names are conventionally written starting with a percent symbol. We’ll
    use four groups of registers: `r` denotes 32-bit integer registers; `rd` denotes
    64-bit (double) int registers; `fd` denotes 64-bit (double) floating-point registers;
    and our fourth group, `tid`, denotes internal registers used to store information
    about the parallelism.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: PTX汇编代码的行尾使用分号，并且用双斜杠表示注释。寄存器名称通常以百分号符号（%）开头。我们将使用四组寄存器：`r`表示32位整数寄存器；`rd`表示64位（双精度）整数寄存器；`fd`表示64位（双精度）浮点寄存器；第四组，`tid`，表示用于存储并行性信息的内部寄存器。
- en: As usual, most instructions use three operands, with the first being the destination
    and the others being inputs. As we have several types of register available, most
    instruction names use Amiga-like suffixes, separated by periods, to indicate which
    version is being used. For example, `add.s64` means addition for 64-bit signed
    integers, while `mult.wide.f64` means wide (full) multiplication of 64-bit floats.
    Load `(ld)` and store `(st)` have suffixes to indicate whether to use global or
    local memory. The `cvt` instruction means convert, and with various suffixes it
    converts numbers between signed and unsigned integers and floats of the different
    bit sizes. As usual, `ret` is return.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，大多数指令使用三个操作数，第一个是目标，其他的是输入。由于我们有多种寄存器可用，大多数指令名称使用类似 Amiga 的后缀，通过句点分隔，用来表示使用的是哪一版本。例如，`add.s64`
    表示对 64 位有符号整数的加法运算，而 `mult.wide.f64` 表示对 64 位浮点数进行宽（全）乘法运算。加载 `(ld)` 和存储 `(st)`
    指令有后缀，表示是否使用全局内存或本地内存。`cvt` 指令表示转换，通过各种后缀，它可以在不同位大小的有符号和无符号整数与浮点数之间转换。像往常一样，`ret`
    表示返回。
- en: The above program, as well as the rest of the PTX programs shown here, assumes
    at the start that registers rd1 through rd3 contain the global memory addresses
    of three arrays of doubles, with rd1 and rd2 being inputs, which we’ll nickname
    `x` and `w`, and rd3 being the output, which we’ll nickname `out`. (The reason
    for these conventions will become clear later, when we get to neurons.)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的程序以及这里展示的其他 PTX 程序都假设开始时寄存器 rd1 到 rd3 包含三个双精度数组的全局内存地址，其中 rd1 和 rd2 是输入，我们将其昵称为
    `x` 和 `w`，而 rd3 是输出，我们将其昵称为 `out`。（这些约定的原因稍后会变得清晰，尤其是在我们谈到神经元时。）
- en: The program’s function is very simple. It completely ignores the two inputs.
    It then obtains its threadID, which is a unique integer assigned to each work-item
    in the work-group. For example, if we were to launch a work-group of 5,000 copies
    of the program, each one would be given a unique threadID in the range 0 to 4,999
    in its `tid.x` register during the launch. The work-item then writes a copy of
    its threadID into the corresponding element of the output array. For example,
    the 573rd work-item, with threadID 573, will write the floating-point number 573.0
    into the 573rd element of `out`. If we launch a work-group of 5,000 copies in
    SIMD, they’ll each write a single such number into `out` simultaneously, so that
    the `out` array then contains the list of numbers from 0 to 4,999 when they complete
    together.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 该程序的功能非常简单。它完全忽略了两个输入。然后它获取其 threadID，这是分配给工作组中每个工作项的唯一整数。例如，如果我们启动一个包含 5000
    个程序副本的工作组，每个副本在启动时都会在其 `tid.x` 寄存器中获得一个唯一的 threadID，范围从 0 到 4999。然后，工作项将其 threadID
    的副本写入输出数组的相应元素。例如，第 573 个工作项，threadID 为 573，将浮点数 573.0 写入 `out` 数组的第 573 个元素。如果我们以
    SIMD 启动一个包含 5000 个副本的工作组，它们每个都会同时将这样一个数字写入 `out`，因此当它们一起完成时，`out` 数组将包含从 0 到 4999
    的数字列表。
- en: Although PTX uses 64-bit words (which can be restricted to 32-bit, as seen in
    the example), it still uses byte addressing. This means that adding 1 to an address
    moves forward through memory by 8 bits. To move along by a 64-bit word, we have
    to add 8 to an address. The program thus works by obtaining its threadID, multiplying
    it by 8, adding the result to the address of `out`, and storing a floating-point
    version of the threadID at that address. The final `out` array thus contains [0,
    1, 2, 3, 4, 5, . . .].
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 PTX 使用 64 位字（如在示例中所见，它可以限制为 32 位），但它仍然使用字节寻址。这意味着地址加 1 会使内存向前移动 8 位。要按 64
    位字进行移动，我们必须将地址加 8。因此，程序的工作原理是获取其 threadID，将其乘以 8，将结果加到 `out` 的地址上，并在该地址存储 threadID
    的浮点版本。最终的 `out` 数组因此包含 [0, 1, 2, 3, 4, 5, . . .]。
- en: '**Branching**'
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**分支**'
- en: The definition of SIMD is that parallel copies of the kernel execute identical
    instructions together as the program executes. Branching runs smoothly in SIMD
    in GPUs if and only if all of the copies take the same branches, but it becomes
    complex to handle if they need to take different branches.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: SIMD 的定义是内核的并行副本在程序执行时一起执行相同的指令。如果所有副本都走相同的分支，GPU 中的分支在 SIMD 中顺利运行，但如果它们需要走不同的分支，则变得复杂。
- en: 'For example, the following PTX kernel uses branching:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，下面的 PTX 内核使用了分支：
- en: '[PRE5]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The first two and last four lines are the same as the previous program. But
    after the first two lines, the program tests if the threadID is less than 4\.
    If so, it adds 3.1 to the threadID. If not, it multiplies the threadID by 10.0\.
    Whichever of these results was obtained is then placed in the threadID-th element
    of `out` as before. On completion, `out` thus contains:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 前两行和最后四行与之前的程序相同。但在前两行之后，程序测试线程ID是否小于 4。如果是，它会将 3.1 加到线程ID上。如果不是，它会将线程ID乘以 10.0。无论哪个结果被得到，都会像以前一样将其放入
    `out` 数组中对应的线程ID元素。完成后，`out` 将包含：
- en: '[PRE6]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The complexity here is that we only want a work-item to execute some of the
    lines if a condition is true or false. In PTX, we first test for the condition
    (less than, `lt`) and set a *predicate* register to true or false to store the
    result. Predicate registers are internal registers, usually written as `p1, p2`,
    and so on, which can be set and tested similarly to the status flags we’ve seen
    in the Analytical Engine and other systems. Unlike those flags, there are many
    predicate registers that can each store predicates for long periods without overwriting
    the previous comparison result. Once we’ve set a predicate, we can indicate that
    some lines should be executed only if the predicate is true, or if it is false.
    These indicators are known as *predicate guards*, and in PTX assembly are written
    as, for example, `@%p1` at the start of a line. Different GPUs, including different
    Nvidia models, may handle predicate guards in two ways: masking or subgroups.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的复杂性在于，我们只希望在某个条件为真或假的情况下，某个工作项执行某些行。在 PTX 中，我们首先测试条件（小于，`lt`），并将一个*谓词*寄存器设置为真或假来存储结果。谓词寄存器是内部寄存器，通常写作`p1,
    p2`等，可以像我们在分析引擎和其他系统中看到的状态标志一样进行设置和测试。与那些标志不同的是，谓词寄存器有很多，每个寄存器可以长时间存储谓词，而不会覆盖先前的比较结果。一旦我们设置了一个谓词，就可以指示某些行仅在谓词为真或为假时才应执行。这些指示符被称为*谓词保护*，在
    PTX 汇编中，通常写作`@%p1`，位于某行的开头。不同的 GPU，包括不同的 Nvidia 型号，可能以两种方式处理谓词保护：掩码或子组。
- en: '*Masking* is a simple, pure SIMD method suitable for small branches, such as
    `if...else` statements without jumps. The kernel is executed in SIMD at all times,
    meaning that all copies share the same program counter and execute the same line
    of code at the same time. If a line is guarded, the PE tests the predicate, and
    if the line should not execute, then the PE replaces it with an NOP (no operator),
    as in a CPU pipeline stall. This enables multiple work-items to remain synchronized,
    with those that need to execute the instruction doing so while the others wait
    around for them via these NOPs. This wastes some time, with PEs executing NOPs
    from one branch and real instructions from the other, but it enables all work-items
    to stay synchronized as SIMD at all times.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*掩码*是一种简单的、纯粹的 SIMD 方法，适用于小的分支结构，例如没有跳转的`if...else`语句。内核始终以 SIMD 模式执行，这意味着所有副本共享相同的程序计数器，并同时执行相同的代码行。如果某行被保护，PE
    会测试谓词，如果该行不应执行，那么 PE 会用 NOP（无操作指令）替换它，就像 CPU 管道停顿一样。这使得多个工作项能够保持同步，需要执行指令的工作项会执行，而其他工作项则通过这些
    NOPs等待它们。这样会浪费一些时间，PE 会从一个分支执行 NOP，而从另一个分支执行真正的指令，但它使得所有工作项始终保持同步，保持 SIMD 模式。'
- en: '*Subgroups* (a Khronos term, aka “local groups,” “warps,” “waves,” or “wavefronts”
    by some manufacturers) are a more heavyweight solution that goes beyond pure SIMD
    to accommodate conditional jumps. All the work-items in a work-group start out
    running in pure SIMD until a predicate guard is encountered. When this occurs,
    the work-group is split into two subgroups, with work-items in one subgroup taking
    the branch and those in the other not taking it. The subgroups are then treated
    as two independent SIMD programs and are executed independently, either on two
    different CUs, if available, or in series on a single CU if only one is available.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*子组*（Khronos 术语，也被一些厂商称为“局部组”、“波”或“波前”）是一种更为复杂的解决方案，超越了纯粹的 SIMD 以适应条件跳转。所有工作组中的工作项开始时都是在纯粹的
    SIMD 中运行，直到遇到谓词保护。当这种情况发生时，工作组被分成两个子组，一个子组的工作项执行分支，而另一个子组的工作项不执行分支。然后，这些子组被视为两个独立的
    SIMD 程序，并独立执行，若有可用的两个计算单元（CU），则在两个 CU 上执行；如果只有一个 CU 可用，则在同一个 CU 上按顺序执行。'
- en: Every branch in the program creates an additional subgroup split, so, for example,
    a program with four branches in a series can lead to 2⁴ = 16 subgroups. At this
    point, the number of physically available CUs determines the efficiency of execution,
    rather than the PEs within a CU. This is clearly not sustainable for larger programs
    with many possible branching series. However, subgroups can be merged back together
    (“resynchronized”) if the programmer can find a way to do so. Typically this can
    be done when branching has occurred due to different work-items taking different
    numbers of loop repetitions. In this case, the programmer can ask the work-items
    whose loops have completed first to wait until the others have also completed;
    this is known as a synchronization *barrier* and is represented by a special barrier
    instruction in assembly and machine code.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 程序中的每个分支都会产生一个额外的子组拆分，例如，一个包含四个分支的串行程序可能会导致 2⁴ = 16 个子组。在此时，物理可用的 CUs 数量决定了执行效率，而不是
    CU 中的 PEs。显然，对于具有多个分支序列的大型程序来说，这种方式是不可持续的。然而，如果程序员能找到方法，子组可以重新合并（“重新同步”）。通常，当分支由于不同的工作项执行了不同数量的循环重复次数时，可以实现此操作。在这种情况下，程序员可以要求先完成循环的工作项等待，直到其他工作项也完成；这被称为同步*屏障*，并通过汇编和机器代码中的特殊屏障指令来表示。
- en: The challenges of branching make SIMD quite a restrictive style of programming.
    It’s well suited to graphics shaders, which typically have no or minimal branching,
    but it’s tricky for programs that require parallel threads to take many different
    branches. Neural networks and physical simulations are two major classes of code
    that have similar minimal-branching structure to graphics; thus, they’ve greatly
    benefited from GPU acceleration. If you need different threads to be doing completely
    different things from one another, however, then SIMD isn’t appropriate. You need
    MIMD, as seen later in the chapter.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 分支的挑战使得 SIMD 成为一种相对受限的编程方式。它非常适合图形着色器，因为图形着色器通常没有或只有极少的分支，但对于需要并行线程执行多个不同分支的程序来说，这种方式比较棘手。神经网络和物理模拟是两类代码，它们与图形代码类似，具有最小分支结构，因此它们从
    GPU 加速中受益匪浅。然而，如果你需要不同的线程做完全不同的事情，那么 SIMD 就不合适了。这时你需要 MIMD，如本章后面所见。
- en: '**Large Work-Groups**'
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**大型工作组**'
- en: Sometimes we need to run more copies of a kernel than there are available PEs
    in the CU. For example, a pixel shader needs to run for every one of about eight
    million pixels for a 4K display, while only thousands or tens of thousands of
    PEs may be available.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 有时我们需要运行比 CU 中可用的 PEs 更多的内核副本。例如，一个像素着色器需要为 4K 显示器的每一个大约八百万个像素运行，而可用的 PEs 可能只有几千或几万个。
- en: 'In these cases, a similar subgroup method can be used as in branching: you
    split the work-group into a number of smaller subgroups such that each subgroup
    runs physically together in SIMD on the PEs, and multiple subgroups can either
    run in series on a single CU or simultaneously across several available independent
    CUs. Unlike in branching, these subgroups are chosen to exactly match the total
    number of PEs. These maximal-sized subgroups are known as *blocks* by some manufacturers,
    with the set of subgroups known as a *grid*.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，可以使用类似于分支的子组方法：将工作组拆分为多个较小的子组，使得每个子组在 PEs 上物理地以 SIMD 方式一起运行，多个子组可以在单个
    CU 上串行运行，也可以在多个可用的独立 CU 上同时运行。与分支不同，这些子组的选择是为了精确匹配 PEs 的总数。这些最大大小的子组被一些厂商称为*块*，而子组集合被称为*网格*。
- en: Work-items within every subgroup will be allocated the same set of threadIDs,
    corresponding to the position of the PE in its CU. It’s common to need to convert
    between these and the global “jobID.” For example, if you have eight million pixels
    to compute and 10,000 PEs, the four-millionth job needs to know that it should
    write to the four-millionth pixel rather than to its threadID-th pixel, which
    has a maximum value of 10,0000.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 每个子组中的工作项将分配相同的线程 ID 集合，表示 PE 在其 CU 中的位置。通常需要在这些线程 ID 和全局“作业 ID”之间进行转换。例如，如果你需要计算八百万个像素，而有
    10,000 个 PE，那么第四百万个作业需要知道它应该写入第四百万个像素，而不是它的线程 ID 对应的像素，后者的最大值为 10,000。
- en: 'This is a common need, so PTX provides some extra machinery to assist with
    programming it, as in the following example:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个常见的需求，因此 PTX 提供了一些额外的机制来帮助编程，如以下示例所示：
- en: '[PRE7]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Here, two additional internal registers, `ntid.x` and `ctaid.x`, are loaded
    automatically during kernel launches, with the subgroup size and a new ID saying
    which subgroup is being run. By multiplying and adding these using the dedicated
    `mad` instruction, we recover the global job ID and proceed as usual. (The rest
    of the program is the same as the first one, storing a float version of this jobID
    at the jobID-th location in `out`. The difference is that this now works for much
    larger `out` arrays—with millions of elements.)
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，两个额外的内部寄存器 `ntid.x` 和 `ctaid.x` 在内核启动时自动加载，包含子组大小和一个新 ID，表示当前正在运行的是哪个子组。通过使用专门的
    `mad` 指令将它们相乘和相加，我们可以恢复全局作业 ID，并像往常一样继续执行。（程序的其余部分与第一个程序相同，存储此作业 ID 的浮点版本于 `out`
    中的 jobID-th 位置。不同之处在于，这现在适用于更大的 `out` 数组——包含数百万个元素。）
- en: '**A GPU Neuron**'
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**GPU 神经元**'
- en: 'Now let’s look at a larger example kernel, which computes a neuron for a convolutional
    deep neural network (CNN). This is roughly how GPUs are used in machine learning:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看一个更大的示例内核，它计算卷积深度神经网络（CNN）的一个神经元。这大致就是 GPU 在机器学习中的使用方式：
- en: '[PRE8]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Here, 0d3DA5FD7FE1796495 is floating-point zero. As in all our examples, we
    assume at the start that registers rd1 through rd3 contain the global memory addresses
    of three arrays of doubles, with rd1 and rd2 being inputs which we nickname *x*
    and *w*; rd3 is the output, which we will nickname *out*. The nicknames *x* and
    *w* are chosen because the neuron computes:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，0d3DA5FD7FE1796495 是浮点零。与我们所有的示例一样，我们假设开始时 rd1 到 rd3 寄存器包含三个双精度数组的全局内存地址，其中
    rd1 和 rd2 是输入，我们称之为 *x* 和 *w*；rd3 是输出，我们称之为 *out*。我们选择 *x* 和 *w* 作为别名，因为神经元计算如下：
- en: '![Image](../images/f0372-01.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0372-01.jpg)'
- en: Here, *reLU(a*) = *a* if *a* > 0 and 0 otherwise (*reLU* standing for rectified
    linear unit). *x* is a 1D signal such as a sound wave, and *w* are weights that
    are shared by and convolved across the work-group of neurons.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*reLU(a*) = *a* 如果 *a* > 0，否则为 0（*reLU* 代表修正线性单元）。*x* 是一维信号，如声音波形，而 *w*
    是权重，这些权重在神经元工作组中共享并进行卷积。
- en: The program is based on a loop that iterates over the terms of the sum in the
    above equation. During each iteration *i*, it brings *w*[*i*] and *x*[*i*] into
    registers and multiplies them. Each of these *w*[*i*]*x*[*x*] terms is then added
    into a cumulative sum (`cumsum`). Predicate `p1` is used to determine the end
    of the loop. The *reLU* function is especially easy to implement and fast to run,
    which is why it’s used. We use another predicate, `p0`, to check if `cumsum` >
    0\. If it is, the *reLU* output in fd1 is set to `cumsum`, and otherwise to zero.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 该程序基于一个循环，该循环遍历上述方程式中的和项。在每次迭代中，*i*，它将 *w*[*i*] 和 *x*[*i*] 载入寄存器并进行相乘。每个 *w*[*i*]*x*[*x*]
    项随后被加到累积和 (`cumsum`) 中。谓词 `p1` 用于确定循环的结束。*reLU* 函数特别容易实现且运行速度快，这也是它被使用的原因。我们使用另一个谓词
    `p0` 来检查 `cumsum` 是否大于 0。如果是，fd1 中的 *reLU* 输出被设置为 `cumsum`，否则设置为零。
- en: The recent “deep learning revolution” in machine learning owes much more to
    the ability of GPU SIMD to run models like this at massive scales than it does
    to any new algorithms.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中最近的“深度学习革命”更多归功于 GPU SIMD 在大规模运行类似模型的能力，而非任何新算法。
- en: '*SASS Dialects*'
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '*SASS 方言*'
- en: As a manufacturer releases new models, they may modify their ISA, usually by
    extending it with additional instructions, but often also by breaking backward
    compatibility with older versions (unlike the x86 tradition of retaining backward
    compatibility at any cost). For example, Nvidia’s ISAs are named after famous
    scientists, such as Tesla (2006), Fermi (2010), Kepler (2012), Maxwell (2014),
    Pascal (2016), Volta (2017), Turing (2018), Ampere (2020), Lovelace (2022), and
    Hopper (2022). They share a core set of similar instructions, but with some variations
    between them.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 随着制造商发布新型号，他们可能会修改其 ISA，通常通过扩展额外的指令来实现，但也常常会与旧版本打破向后兼容性（这与 x86 在任何情况下都保持向后兼容的传统不同）。例如，Nvidia
    的 ISA 以著名科学家的名字命名，如 Tesla（2006）、Fermi（2010）、Kepler（2012）、Maxwell（2014）、Pascal（2016）、Volta（2017）、Turing（2018）、Ampere（2020）、Lovelace（2022）和
    Hopper（2022）。它们共享一组核心的类似指令，但在它们之间存在一些差异。
- en: Each of these ISAs has its own assembly language dialect, known as a SASS, whose
    instructions correspond directly to machine code. These assembly languages are
    each compatible only with their particular architecture, so they change every
    couple of years. They aren’t officially documented, and don’t present a stable
    platform for user programmers to learn. Nvidia developed PTX as a single stable
    assembly representation, usable by human programmers, that gets translated during
    assembly to the appropriate SASS dialect.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 每个这些ISA都有自己的汇编语言方言，称为SASS，其指令直接对应于机器代码。这些汇编语言仅与其特定架构兼容，因此每隔几年就会变化。它们没有官方文档，也没有为用户程序员提供稳定的平台进行学习。Nvidia开发了PTX，作为一个稳定的汇编表示形式，供人类程序员使用，并在汇编期间被翻译成适当的SASS方言。
- en: 'The following code shows Turing SASS together with corresponding Turing executable
    machine code, as assembled from the neuron PTX example shown earlier, together
    with wrapper code to interface it to input and output parameters:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了Turing SASS与相应的Turing可执行机器代码，作为从前面展示的神经网络PTX示例汇编而来，并包含与输入和输出参数接口的包装代码：
- en: '[PRE9]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The hex seen here is the actual executable code that’s transferred over the
    bus and run on the GPU for the neural network; it’s a direct translation of the
    SASS assembly. (Compare this with the Baby machine code seen in [Chapter 7](ch07.xhtml)—it’s
    not really so different!)
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这里看到的十六进制是实际的可执行代码，它通过总线传输并在GPU上运行，用于神经网络；它是SASS汇编的直接翻译。（与[第7章](ch07.xhtml)中看到的Baby机器代码进行对比——其实并没有那么不同！）
- en: SASS dialects aren’t officially documented, but we—and the internet—can make
    some guesses as to likely meanings of some instructions based on what we’ve seen
    in the corresponding PTX. `MOV` is a move instruction, with operands being either
    registers or memory locations such as `c[][]` to obtain inputs to the kernel call.
    `LDG` loads from global memory, and `STG` stores to global memory and is used
    to return the output of the kernel call. `TID` is the threadID, which tells us
    which work-item we’re running. `IADD` and `FADD` are integer and floating-point
    addition. `SHL` and `SHR` are shift left and right. `XMAD` is “integer short multiply
    and add.” `BRA` is branch, `NOP` is null operation. `@P0` is a predicate guard,
    where `P0`’s value is set in the previous line by the `ISETP` instruction. The
    usual `JMP`, `CALL`, and `RET` are also provided for control flow.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: SASS方言没有官方文档，但我们——以及互联网——可以根据我们在相应的PTX中看到的内容，猜测一些指令可能的含义。`MOV`是一个移动指令，操作数可以是寄存器或内存位置，例如`c[][]`，用于获取内核调用的输入。`LDG`从全局内存加载数据，`STG`将数据存储到全局内存，并用于返回内核调用的输出。`TID`是线程ID，告诉我们正在运行哪个工作项。`IADD`和`FADD`分别是整数和浮点数加法。`SHL`和`SHR`分别是左移和右移。`XMAD`是“整数短乘加”。`BRA`是分支，`NOP`是空操作。`@P0`是一个谓词保护，其中`P0`的值在前一行由`ISETP`指令设置。常见的`JMP`、`CALL`和`RET`也用于控制流。
- en: SASS dialects also have dedicated instructions for graphics operations. For
    example, there’s `SUST`, surface store, to actually write to the graphics surface,
    as well as instructions to load and query textures and barrier sync (`BAR`).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: SASS方言还具有专门用于图形操作的指令。例如，有`SUST`，即表面存储，用于实际写入图形表面，还有用于加载和查询纹理以及屏障同步（`BAR`）的指令。
- en: To get the executable code onto the GPU, and then to specify when and how many
    copies to launch, the host needs a CPU program. For general computation, you need
    to write this yourself, using tools provided by the GPU manufacturer. For graphics,
    driver software such as Vulkan will do this work if you tell it where your kernel
    (known as a shader in this context) is and what type of shading it does (vertex
    or pixel).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 要将可执行代码传送到GPU，并指定何时以及多少副本进行启动，主机需要一个CPU程序。对于通用计算，您需要自己编写这个程序，使用GPU制造商提供的工具。对于图形，像Vulkan这样的驱动程序软件将完成这项工作，只要您告诉它您的内核（在此上下文中称为着色器）的位置以及它执行的着色类型（顶点或像素）。
- en: '**NOTE**'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*Recent GPUs may have many additional features and optimizations, including
    many CISC-like specialist instructions, and even their own CPU SIMD–style instructions
    to split up registers into parts and operate on them together. Recent approaches
    to branching have begun to abandon SIMD altogether and assign separate program
    counters to PEs, resulting in the machine looking more like the MIMD systems in
    the following sections than conventional SIMD GPUs.*'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '*最近的 GPU 可能具有许多额外的功能和优化，包括许多类似 CISC 的专用指令，甚至还有自己的 CPU SIMD 样式指令，将寄存器拆分成多个部分并一起操作。最近的分支处理方法开始完全放弃
    SIMD，并为 PEs 分配独立的程序计数器，从而使得该机器更像以下章节中描述的 MIMD 系统，而非传统的 SIMD GPU。*'
- en: '*Higher-Level GPU Programming*'
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '*高级 GPU 编程*'
- en: PTX, and occasionally SASS, code is currently written by hand in some cases,
    where human creativity and knowledge of the underlying architecture can allow
    for speed optimizations. However, it’s more common to use higher-level languages
    to compile into GPU assemblers in order to achieve portability between different
    GPUs and to make programming easier.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，PTX 和偶尔的 SASS 代码仍由人工编写，借助人的创造力和对底层架构的了解，能够实现速度优化。然而，更常见的是使用高级语言将代码编译为
    GPU 汇编语言，以实现不同 GPU 之间的可移植性，并简化编程工作。
- en: '*CUDA* is Nvidia’s proprietary C-like language that compiles to PTX and then
    SASS, but not to anything usable by other manufacturers’ GPUs. For example, this
    CUDA program adds two vectors together element-wise:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '*CUDA* 是 Nvidia 专有的类似 C 的语言，编译成 PTX 然后是 SASS，但不能用于其他厂商的 GPU。例如，这个 CUDA 程序逐元素相加两个向量：'
- en: '[PRE10]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'It can be compiled to PTX with Nvidia’s `nvcc` compiler:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以通过 Nvidia 的 `nvcc` 编译器编译成 PTX：
- en: '[PRE11]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '*SPIR-V* (pronounced “spear vee,” as unlike with RISC-V, this *V* is for “Vulkan”)
    is the Khronos standard for representing GPU kernels in an assembly-like language.
    As PTX generalizes over many Nvidia architectures, SPIR-V is intended to generalize
    over *all* manufacturers’ architectures. Like PTX, it’s designed to be converted
    into assembly languages for each specific architecture. As different architectures
    may have different numbers of registers, SPIR-V doesn’t describe registers at
    all. Instead, each instruction’s result is given a unique ID number, which can
    be used similarly to a register ID. When someone writes a converter program for
    a new architecture, they need to think about how to best make use of the available
    registers to realize the computations described in this way. Intel has also worked
    on converting SPIR-V to x86 SIMD, enabling its CPUs to compete against GPUs to
    execute the same code. The following shows SPIR-V code for a roughly equivalent
    kernel to the vector addition seen in the CUDA example:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '*SPIR-V*（发音为“spear vee”，与 RISC-V 不同，这个 *V* 代表“Vulkan”）是 Khronos 标准，用于表示 GPU
    内核的类似汇编语言。由于 PTX 是针对许多 Nvidia 架构的通用化，SPIR-V 旨在对 *所有* 厂商的架构进行通用化。与 PTX 一样，它旨在被转换成每个特定架构的汇编语言。由于不同架构的寄存器数量可能不同，SPIR-V
    根本不描述寄存器。相反，每条指令的结果会被赋予一个唯一的 ID 编号，这个编号可以类似于寄存器 ID 使用。当有人为新架构编写转换程序时，他们需要考虑如何最好地利用可用的寄存器来实现以这种方式描述的计算。英特尔也在致力于将
    SPIR-V 转换为 x86 SIMD，使其 CPU 能够与 GPU 竞争执行相同的代码。以下展示了一个大致等同于 CUDA 示例中向量加法的 SPIR-V
    代码：'
- en: '[PRE12]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Third-party open source efforts are underway at the time of writing to compile
    CUDA to SPIR-V, though they aren’t supported by Nvidia. Nvidia does, however,
    accept SPIR-V as input, providing closed tools to compile it to SASS, via PTX
    and another intermediate language, NVVM.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文写作时，第三方开源项目正在进行中，旨在将 CUDA 编译成 SPIR-V，尽管这些项目并未得到 Nvidia 的支持。然而，Nvidia 确实接受
    SPIR-V 作为输入，提供封闭工具通过 PTX 和另一种中间语言 NVVM 将其编译为 SASS。
- en: '*OpenCL* is another Khronos open standard, defining a language similar to Nvidia’s
    CUDA. Open source compilers are available from OpenCL to SPIR-V. The following
    is an OpenCL kernel roughly equivalent to the CUDA example:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '*OpenCL* 是另一个 Khronos 开放标准，定义了一种类似于 Nvidia CUDA 的语言。OpenCL 到 SPIR-V 的开源编译器可用。以下是一个大致等同于
    CUDA 示例的 OpenCL 内核：'
- en: '[PRE13]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '*GLSL* is the Khronos standard graphical shader language, which also compiles
    to SPIR-V. A sample of GLSL implementing Gouraud shading (from *[https://www.learnopengles.com/tag/gouraud-shading/](https://www.learnopengles.com/tag/gouraud-shading/)*)
    is shown here:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '*GLSL* 是 Khronos 标准的图形着色语言，它也可以编译成 SPIR-V。这里展示了一个实现 Gouraud 着色的 GLSL 示例（来自
    *[https://www.learnopengles.com/tag/gouraud-shading/](https://www.learnopengles.com/tag/gouraud-shading/)*）：'
- en: '[PRE14]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Here, `lightVector` is the vector from a light to a vertex, and `diffuse` is
    the diffuse component given by the dot product of the light vector and vertex
    normal. If the normal and light vector point in the same direction, then it will
    get maximum illumination. The color is multiplied by the diffuse illumination
    level to give the final display color.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`lightVector`是从光源到顶点的向量，而`diffuse`是通过光照向量与顶点法线的点积得到的漫反射成分。如果法线和光照向量朝同一方向指向，则会得到最大照明。颜色会与漫反射照明水平相乘，得出最终的显示颜色。
- en: Multiple Instruction, Multiple Data
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多指令、多数据
- en: SIMD is like a lot of people acting on the same instruction. *Multiple instruction,
    multiple data (MIMD)*, on the other hand, is like a lot of people acting on a
    lot of different instructions. Think of SIMD as a gym class with a trainer shouting
    out instructions, and the class all moving together in response. MIMD, then, is
    more like a gym where everyone has their own personal trainer telling them each
    to do different exercises at the same time. As with SIMD, there are multiple different
    flavors of MIMD, which we’ll explore here.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: SIMD就像是许多人同时执行相同的指令。*多指令多数据（MIMD）*则像是许多人同时执行不同的指令。可以将SIMD类比为一个健身课，教练在喊指令，整个班级一起做动作。而MIMD更像是一个健身房，每个人都有自己的私人教练，指示他们同时做不同的练习。像SIMD一样，MIMD也有多种不同的类型，我们将在这里探讨。
- en: '*MIMD on a Single Processor*'
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '*单处理器上的MIMD*'
- en: The simplest MIMD can occur on a single CPU, in architectures called *very long
    instruction words (VLIW)*. VLIW architectures are related to the vector architectures
    in SIMD. Vector architectures have multiple data items packed into a single large
    register, with a single instruction acting on all of the entities packed into
    that register. In VLIW, each entity in the register has different operations performed
    on it. For example, instead of adding 1 to everything, we could add 1 to the first
    number, divide the second by 7, and multiply the last two together, storing them
    somewhere else.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的MIMD可以在单个CPU上实现，这种架构称为*超长指令字（VLIW）*。VLIW架构与SIMD中的向量架构相关。向量架构将多个数据项打包到一个大寄存器中，并且单一的指令作用于寄存器中所有的实体。在VLIW中，寄存器中的每个实体执行不同的操作。例如，我们不一定将1加到所有数字上，而是可以将1加到第一个数字，将第二个数字除以7，将最后两个数字相乘，并将结果存储到其他位置。
- en: This may seem counterintuitive, but there are certain combinations of instructions
    that tend to reappear. For example, when writing a video codec, there are standard
    complex mathematical operations that you repeat over and over on different data.
    You can design a single long instruction word to perform this exact specialist
    sequence of operations. For example, a single VLIW instruction `ADDABCFPMDEFINCGSFTH`
    might mean “integer add register A to register B, store result in C; floating-point
    multiply registers D and E, store in F; increment register G; and bit-shift register
    H”—all in a single instruction! This might, for example, be a standard but intensive
    part of a video codec computation.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能看起来有些反直觉，但有些指令组合往往会反复出现。例如，在编写视频编解码器时，有一些标准的复杂数学操作会在不同数据上反复执行。你可以设计一个单一的长指令字来执行这一特定的操作序列。例如，一个单独的VLIW指令`ADDABCFPMDEFINCGSFTH`可能意味着“将寄存器A加到寄存器B，结果存储到C；浮点数乘法，将寄存器D和E相乘，结果存储到F；递增寄存器G；并对寄存器H进行位移操作”——这一切都在一个指令中完成！这可能是一个视频编解码计算中标准但密集的部分。
- en: '*Shared-Memory MIMD*'
  id: totrans-157
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '*共享内存MIMD*'
- en: A step above single-CPU MIMD is *shared-memory MIMD*, in which multiple CPUs
    share an addressed memory space. They can communicate with one another by loading
    and storing data within this space. If the CPUs are identical, the style of parallelism
    is known as *symmetric multi-processing (SMP)*. If the CPUs differ, the style
    of parallelism is known as asymmetric multiprocessing (AMP). When the CPUs are
    located on the same CPU piece of silicon, they’re known as *cores*, and the parallelism
    is called *multicore*.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 单CPU MIMD的上一级是*共享内存MIMD*，其中多个CPU共享一个地址空间。它们可以通过在此空间内加载和存储数据来相互通信。如果CPU是相同的，那么这种并行方式称为*对称多处理（SMP）*。如果CPU不同，则这种并行方式称为*非对称多处理（AMP）*。当多个CPU位于同一块硅片上时，它们被称为*核心*，而这种并行方式被称为*多核*。
- en: AMP shared memory goes back to the 1980s, when separate coprocessor chips were
    sometimes plugged in alongside the main CPU for extra operations such as floating-point
    computation. For example, the Sega Megadrive used a Z80 as a second processor
    to look after sound, freeing up its main 68000.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: AMP 共享内存可以追溯到 1980 年代，当时有时会将独立的协处理器芯片与主 CPU 一起插入，用于执行额外的操作，例如浮点计算。例如，世嘉 Megadrive
    使用 Z80 作为第二处理器来负责声音处理，从而释放其主处理器 68000 的负担。
- en: SMP shared-memory computer designs have also existed all through the history
    of x86, beginning with mainboards hosting two or more physical 8086 chips sharing
    the bus and memory.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: SMP 共享内存计算机设计自 x86 历史以来就一直存在，最早的设计是在主板上放置两个或更多物理 8086 芯片，共享总线和内存。
- en: 'With shared-memory MIMD, we have to think about how cache levels should be
    shared. Often the L1 and maybe the L2 cache are stored inside a single CPU and
    are specific to it, while the L3 and maybe the L2 cache are shared between the
    CPUs. This makes managing the caches quite complex. Imagine two CPUs are accessing
    the same address of RAM, caching it independently. The first CPU writes to the
    cache, changing the value for the address. Remember the different cache write
    algorithms we looked at in [Chapter 10](ch10.xhtml): What is the cache going to
    do when the CPU tells it to update the location? Will it just update the local
    cache? Will it send the change straight back to main memory, or wait until the
    cache line is victimized before doing so? If the second CPU tries to read from
    the same address in main memory, how can we ensure it will get the newly updated
    version? We have to be careful when there are multiple CPUs, as they all have
    the ability to write out to shared memory equally, which requires extra communication
    between the CPUs so that the values can be updated and the data can remain in
    sync across all CPUs and their shared cache.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在共享内存 MIMD 中，我们需要考虑缓存层级应如何共享。通常，L1 和可能的 L2 缓存存储在单个 CPU 内，并且是特定于该 CPU 的，而 L3
    和可能的 L2 缓存则在多个 CPU 之间共享。这使得缓存管理变得相当复杂。想象一下，当两个 CPU 正在访问相同的 RAM 地址并独立地缓存它时，第一个
    CPU 写入缓存，改变了该地址的值。记住我们在[第 10 章](ch10.xhtml)中讨论过的不同缓存写入算法：当 CPU 告诉缓存更新该位置时，缓存会怎么做？它会只更新本地缓存吗？它会直接将更改发送回主内存，还是等到缓存行被淘汰时才这么做？如果第二个
    CPU 尝试从主内存中读取相同的地址，我们如何确保它获得最新的更新版本？当有多个 CPU 时，我们必须小心，因为它们都有同等的能力写入共享内存，这需要 CPU
    之间进行额外的通信，以便更新值并保持所有 CPU 及其共享缓存的数据同步。
- en: '**Multicore on x86**'
  id: totrans-162
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**x86上的多核**'
- en: Multicore silicon is now the most common type of shared-memory MIMD, and is
    probably found in your desktop, laptop, and phone. The first dual-core x86 chip
    was the AMD Athlon X2, made from two Hammer K8 cores on the same silicon. This
    was soon followed by Intel’s dual-core Core 2\. Both companies quickly followed
    with 4-, 8-, and 16-core processors—including extra cores from hyperthreading—and
    by 2020 were able to produce 64 cores on high-end processors. [Figure 15-6](ch15.xhtml#ch15fig6)
    shows a die shot of an eight-core Zen2 chiplet.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 多核硅片现在是最常见的共享内存 MIMD 类型，可能出现在你的桌面电脑、笔记本电脑和手机中。第一款双核 x86 处理器是 AMD Athlon X2，它由两个
    Hammer K8 核心组成，位于同一块硅片上。紧接着，英特尔推出了双核 Core 2。两家公司很快推出了 4 核、8 核和 16 核处理器——包括超线程带来的额外核心——到
    2020 年，已经能够在高端处理器中制造出 64 核处理器。[图 15-6](ch15.xhtml#ch15fig6)展示了一个八核 Zen2 芯片的晶片图像。
- en: '*Chiplets* such as this are a recent innovation that split up a large chip
    into several smaller pieces of silicon that are placed together in the same plastic
    package. This is done because chips are now so large and complex that the statistical
    probability of a manufacturing error occurring somewhere has become significant.
    Traditionally, whole chips had to be discarded if any error was present. By using
    chiplets, only the single chiplet with an error needs to be discarded. Multiple
    copies of the chiplet shown here can be combined together—and with additional
    I/O chiplets—to place many more CPUs into a single package than would otherwise
    be reliably possible.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 像这样的*芯片单元*是最近的创新，它将一个大型芯片拆分成几个较小的硅片，并将它们放置在同一个塑料封装中。这是因为如今的芯片非常大且复杂，制造过程中出现错误的统计概率变得相当显著。传统上，如果芯片出现任何错误，整个芯片都会被丢弃。而通过使用芯片单元，只有发生错误的单个芯片单元需要被丢弃。像这里展示的芯片单元可以组合在一起——并与额外的
    I/O 芯片单元一起——将更多的 CPU 放入单一封装中，这在过去是无法可靠实现的。
- en: In [Figure 15-6](ch15.xhtml#ch15fig6), each core has its own L1 and L2 caches,
    with an L3 cache shared between them. Notice how the subcomponents of the cores
    have an almost organic quality in their layouts, like growing mold. This is because—unlike
    the older CPUs we’ve seen in die shots—they were laid out not by human designers
    but by automated routing algorithms, which prioritize efficiency over beauty or
    human comprehension.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图15-6](ch15.xhtml#ch15fig6)中，每个核心都有自己的L1和L2缓存，L3缓存则由它们共享。注意到核心的子组件布局几乎呈现出一种有机的特质，就像生长的霉菌。这是因为——与我们在切片图中看到的老式CPU不同——这些核心的布局不是由人类设计师完成的，而是由自动化布线算法生成的，优先考虑效率而非美观或人类理解。
- en: The cores run independently, with the onus on the software to perform MIMD using
    them. Some new instructions have been added to the x86 ISA to make this programming
    easier, however, such as Intel’s Transactional Synchronization Extensions (TSX).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这些核心独立运行，软件的任务是利用它们进行MIMD（多指令流多数据流）处理。为了简化这种编程，x86指令集架构（ISA）增加了一些新的指令，例如Intel的事务同步扩展（TSX）。
- en: '![Image](../images/f0380-01.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0380-01.jpg)'
- en: '*Figure 15-6: A die shot of an AMD Zen2 chiplet, showing eight cores (four
    rectangles spanning the top, four spanning the bottom) and an L3 cache (eight
    rectangles in the vertical center)*'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '*图15-6：AMD Zen2芯片的切片图，显示了八个核心（四个矩形位于顶部，四个位于底部）和一个L3缓存（八个矩形位于垂直中心）*'
- en: '**LOOP VS. MAP PROGRAMMING**'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '**循环与映射编程**'
- en: 'Sequential and parallel thinking lead to two different ways of programming
    multiple copies of work: loops and maps. For example, in the following code we
    have an array containing four elements that need to be processed. Thinking sequentially,
    you’d create a loop and do something with each element in sequence:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序和并行思维导致了两种不同的编程方式来处理多份工作：循环和映射。例如，在下面的代码中，我们有一个包含四个元素的数组需要处理。按顺序思考时，你会创建一个循环，并依次处理每个元素：
- en: '[PRE15]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The problem with using a loop for this type of work is that it conflates two
    ideas. First, it expresses that we would like each element of data to be processed.
    But second, it also specifies the order in which to process them—in this case,
    starting with the leftmost element and working rightwards one element at a time.
    The first idea is usually what we actually want to express, and if parallelism
    is available we don’t care about the ordering; rather, we want to allow the machine
    to order the work in whatever way gets it done most efficiently.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 使用循环来处理这类工作的问题在于，它混淆了两个概念。首先，它表达了我们希望处理每一个数据元素的需求。但其次，它还指定了处理顺序——在这个例子中，是从最左侧的元素开始，逐个向右处理。第一个概念通常是我们实际想要表达的，如果可以使用并行处理，我们并不关心顺序；相反，我们希望允许机器以最有效的方式安排任务顺序。
- en: 'Some programming languages now provide libraries that parallelize regular code
    across multicores for sections of programs; here’s an example from Python:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 一些编程语言现在提供了库，将常规代码并行化到多核处理器上，用于程序的某些部分；这是一个来自Python的示例：
- en: '[PRE16]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This says that we would like a pool of four computations to take place, in any
    order, such that each computation is performed on one of the elements from the
    data. Assigning data elements to tasks is called *mapping*, hence the `map` function
    here.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们希望有一个包含四个计算的池，这些计算可以按任何顺序进行，以便每个计算都在数据中的一个元素上执行。将数据元素分配给任务的过程称为*映射*，因此这里使用了`map`函数。
- en: If you have four cores in your computer, you can run this Python code, and if
    the system is set up properly, it will know to run on the four cores in parallel
    to complete the task.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的计算机有四个核心，你可以运行这段Python代码，并且如果系统设置正确，它会自动知道并行地在四个核心上运行以完成任务。
- en: '**Non-uniform Memory Access**'
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**非统一内存访问**'
- en: '*Non-uniform memory access (NUMA)* architectures are shared-memory designs
    in which the speed of access to memory differs according to which part of memory
    is accessed and by which CPU.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '*非统一内存访问（NUMA）*架构是共享内存设计，其中访问内存的速度取决于访问的是哪一部分内存，以及由哪个CPU进行访问。'
- en: NUMA requires specialist programmers to understand the architecture and manually
    design programs to take full advantage of it. This includes considering where
    data is located in memory and trying to group data and processors together so
    that loads and stores are done on the fastest available parts of memory.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: NUMA（非统一内存访问）要求专业程序员理解其架构，并手动设计程序以充分利用其优势。这包括考虑数据在内存中的位置，并尝试将数据和处理器组合在一起，以便在内存的最快部分执行加载和存储操作。
- en: As an example of NUMA, say we have four physical enclosures (cases), each containing
    several CPUs and RAM. Initially, this may look like four separate computers, but
    the memory from all four enclosures is connected and mapped together, sharing
    a single address space. These aren’t independent computers; they are, arguably,
    a single multicore computer. Unlike with a regular shared-memory machine, however,
    it takes longer for a CPU to access memory in another enclosure than it would
    to access memory in its own enclosure.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 以NUMA为例，假设我们有四个物理机箱（外壳），每个机箱内有多个CPU和RAM。最初，这看起来像四台独立的计算机，但四个机箱的内存被连接在一起，并映射到一个共享地址空间。这些并不是独立的计算机；可以说，它们是单一的多核计算机。然而，与常规的共享内存机器不同，CPU访问另一个机箱中的内存所需的时间比访问自己机箱中的内存要长。
- en: '**NOTE**'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*You can address 16 exiwords of memory with 64-bit addressing, which is 16
    exibytes if byte addressing is used. This is large enough to cover the entire
    shared memory of current supercomputers. If we want efficient shared-memory computing
    to go above this, however, we may need to move to 128-bit architectures.*'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '*你可以使用64位寻址访问16 exiwords的内存，如果使用字节寻址，则为16 exibytes。这足够大，可以覆盖当前超级计算机的整个共享内存。然而，如果我们希望高效的共享内存计算超过这个范围，我们可能需要转向128位架构。*'
- en: NUMA is used in high-performance computing (HPC), where devices are also known
    as *supercomputers* or “big iron.” These are made from many physical, enclosed
    computers, known as *nodes*, each containing one or more CPUs together with memory,
    all connected by cables. Unlike regular networking, these *interconnect* connections
    and the digital logic controlling them are designed to enable direct access to
    the address spaces of each machine. One possibility for interconnect is to physically
    extend a single main bus along cables connecting all the machines such that every
    CPU, RAM, and I/O module share the same bus. Another option is to map all external
    addresses for a machine to a single I/O module in that machine, which caches all
    loads and stores to these addresses and arranges for them to take place by communicating
    with similar I/O modules on the remote machines. Such communications can also
    include remote DMA (RDMA) to enable large CPU-free bulk transfers between RAM
    and secondary storage across nodes. Most NUMA architectures include an additional
    layer of cache, which locally caches the data from remote machines. This is known
    as *cache-coherent* or cc-NUMA.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: NUMA用于高性能计算（HPC），这些设备也被称为*超级计算机*或“大铁”。它们由许多物理封闭的计算机组成，称为*节点*，每个节点包含一个或多个CPU以及内存，所有节点通过电缆连接。与常规网络不同，这些*互联*连接和控制它们的数字逻辑设计用于直接访问每台计算机的地址空间。一种互联的可能性是通过电缆延伸一个单一的主总线，连接所有机器，使每个CPU、RAM和I/O模块共享同一总线。另一种选择是将所有外部地址映射到该机器中的一个单一I/O模块，该模块缓存所有对这些地址的加载和存储，并通过与远程机器上的类似I/O模块进行通信来安排它们的执行。这种通信还可以包括远程DMA（RDMA），以在节点之间进行大规模、无CPU的批量数据传输。大多数NUMA架构包括一个额外的缓存层，用于本地缓存来自远程机器的数据。这被称为*缓存一致性*或cc-NUMA。
- en: The world’s most powerful publicly known supercomputer in 2022 was *AMD Frontier*,
    at the US Department of Energy’s Oak Ridge National Laboratory, shown in [Figure
    15-7](ch15.xhtml#ch15fig7).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 2022年世界上最强大的公开已知超级计算机是*AMD Frontier*，位于美国能源部的橡树岭国家实验室，如[图15-7](ch15.xhtml#ch15fig7)所示。
- en: '![Image](../images/f0382-01.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0382-01.jpg)'
- en: '*Figure 15-7: The AMD Frontier supercomputer*'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '*图15-7：AMD Frontier超级计算机*'
- en: Frontier consists of 74 liquid-cooled HPE Cray EX cabinets, each containing
    eight chassis of eight blades. Each blade has two AMD CPUs and eight GPUs, giving
    around 9,400 CPUs and 37,000 GPUs in total. It’s capable of performing one quintillion
    floating-point operations per second, known as an *exaflop*. It has 700 PB of
    storage, managed using the Lustre filesystem. The secret sauce is the interconnect
    system, known as HPE Slingshot, which is used with the HyperTransport protocol
    and over 90 miles of cabling—including direct point-to-point connections between
    every pair of nodes—to provide NUMA-style memory, making memory on remote nodes
    appear and act as if it were local. Slingshot uses a similar amount of space,
    electronics, and power as the compute nodes.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: Frontier由74个液冷HPE Cray EX机柜组成，每个机柜内包含八个机箱，每个机箱有八个计算节点。每个节点配备两颗AMD CPU和八个GPU，总共约有9,400个CPU和37,000个GPU。其计算能力可以达到每秒执行一千亿次浮点运算，称为*exaFLOP*。它具有700PB的存储，使用Lustre文件系统进行管理。其核心技术是互连系统，称为HPE
    Slingshot，结合了HyperTransport协议和超过90英里的电缆布线——包括每一对节点之间的直接点对点连接——提供了NUMA式的内存架构，使得远程节点上的内存看起来就像本地内存一样。Slingshot使用的空间、电子设备和电力消耗与计算节点相似。
- en: NUMA supercomputers are used for tasks such as weather and climate prediction,
    physical simulation, and brain modeling, taking advantage of the topographical
    nature of these domains and linking that to the topographical hierarchy of the
    NUMA system. *Topography* means the connectivity over physical space; in the context
    of climate prediction, we’re talking about modeling the 3D space of Earth’s atmosphere.
    Each point interacts heavily with adjacent points, with the amount of interaction
    decreasing with the distance between points. Each point has its own data properties,
    such as wind velocity, temperature, humidity, and wind pressure. To predict what’s
    going to happen over the next few days or months, we discretize the space into
    small chunks and give each chunk to a processor to look after, with neighboring
    chunks of atmosphere given to neighboring processors in the NUMA hierarchy. The
    processor computes details and makes predictions, factoring in data from other
    local chunks.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: NUMA超级计算机用于天气和气候预测、物理模拟和大脑建模等任务，利用这些领域的拓扑特性，并将其与NUMA系统的拓扑层级相连接。*拓扑*指的是物理空间中的连接性；在气候预测的上下文中，我们指的是模拟地球大气的三维空间。每个点与相邻点有着强烈的交互，交互的强度随着点与点之间的距离增大而减少。每个点都有自己的数据属性，如风速、温度、湿度和气压。为了预测未来几天或几个月内的情况，我们将空间离散化为小块，并将每块分配给一个处理器来管理，相邻的大气块分配给NUMA层级中的相邻处理器。每个处理器计算细节并进行预测，考虑来自其他本地块的数据。
- en: NUMA is also sometimes implemented within a single physical computer enclosure,
    in high-end workstations and servers. These systems are more likely to run many
    small, non-interacting programs than a single large scientific program, so specialist
    programming is less likely to be needed.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: NUMA有时也在单一物理计算机外壳内实现，常见于高端工作站和服务器。这些系统更可能运行多个小型、相互独立的程序，而非单一的大型科学程序，因此不太需要专业的编程。
- en: '*MIMD Distributed Computing*'
  id: totrans-190
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '*MIMD分布式计算*'
- en: '*Distributed computing* means that we have multiple CPUs that each have their
    own address space and aren’t directly accessible to each other. Often, these address
    spaces are each contained in separate physical boxes, such as server or ATX cases.
    CPUs in different address spaces can communicate only with one another using I/O.
    Depending on your definition of a computer, these systems can look a lot like
    many separate computers, loosely connected by networking I/O. But in other cases,
    the work they do can be so tightly coupled that it makes more sense to think of
    them as a single, multicore machine that just happens to have multiple address
    spaces linked by slower I/O networks, like an extreme form of NUMA.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '*分布式计算*意味着我们有多个CPU，每个CPU有自己的地址空间，且这些地址空间彼此不可直接访问。通常，这些地址空间被分别放置在不同的物理设备中，如服务器或ATX机箱。不同地址空间中的CPU只能通过I/O进行通信。根据对计算机的定义，这些系统可能看起来像是多个独立的计算机，通过网络I/O松散连接。但在其他情况下，它们的工作紧密结合，以至于把它们看作一台具有多个地址空间并通过较慢的I/O网络连接的单一多核机器（像极端形式的NUMA）更为合理。'
- en: Servers are computers designed to remain powered on at all times that are often
    used for distributed computing as well as for providing online services such as
    websites and databases. Any computer connected to the internet can be used as
    a server, including desktop PCs and Raspberry Pis, but specialized computer designs
    have evolved to better meet servers’ high-reliability requirements. These include
    dual power supplies and auto power-on after an outage to reduce downtime due to
    power grid failures; efficient heat-flow designs and use of ECC-RAM (as in [Chapter
    10](ch10.xhtml)) to reduce internal failures; 19-inch unit form factors to enable
    rack mounting; and various forms of physical security to reduce human interference.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器是始终保持开启的计算机，通常用于分布式计算以及提供在线服务，如网站和数据库。任何连接到互联网的计算机都可以作为服务器使用，包括台式电脑和树莓派，但为更好地满足服务器的高可靠性要求，已经发展出了专用的计算机设计。这些设计包括双电源供应、停电后自动开机以减少因电网故障导致的停机时间；高效的散热设计和使用ECC-RAM（如在[第10章](ch10.xhtml)所述）以减少内部故障；19英寸的机架式单元尺寸；以及各种形式的物理安全措施，以减少人为干扰。
- en: Let’s take a look at a few forms of distributed computing.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看几种分布式计算的形式。
- en: '**Cluster Computing**'
  id: totrans-194
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**集群计算**'
- en: There may be a lot of constant communication between the nodes in a cluster.
    Beowulf is a particular informal standard for building clusters from commodity
    computers (often many old, recycled desktops). Cluster computing, especially Beowulf,
    tends to be quite hacky, amateur, and ad-hoc, but can produce powerful systems
    from low-end machines.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 集群中的节点之间可能会有大量的持续通信。Beowulf是构建集群的一个非正式标准，通常由商品计算机（往往是许多旧的回收台式机）组成。集群计算，特别是Beowulf，往往非常“黑客”式、业余且临时，但可以从低端机器中构建出强大的系统。
- en: '**Grid Computing**'
  id: totrans-196
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**网格计算**'
- en: '*Grid computing*, also known as the *single program, multiple data (SPMD)*
    style of programming, is where we give the same program but separate data to multiple
    identical computers. The computers don’t run the program’s instructions in sync;
    rather, they can all branch differently, depending on the data, running copies
    of the same program all at different places in its execution. Grid computing is
    well suited for applications in data science, speech recognition, data mining,
    bioinformatics, and media processing. Here you have terabytes or more of information
    and want the machines to chug away on separate chunks of it at the same time.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '*网格计算*，也称为*单程序多数据（SPMD）*编程风格，是指将相同的程序但不同的数据提供给多台相同的计算机。计算机并不会同步执行程序指令；相反，它们可以根据数据的不同而产生不同的分支，运行同一程序的多个副本，并且在执行过程中处于不同的地方。网格计算非常适合应用于数据科学、语音识别、数据挖掘、生物信息学和媒体处理等领域。在这种计算方式下，您可能有多个TB的数据，并希望机器能同时处理其中的不同数据块。'
- en: A characteristic of this style is that all the machines are exactly the same,
    and are kept that way by a dedicated technician, who hosts them in a secure environment.
    Lots of identically high-spec servers are stacked together in racks to guarantee
    that each instance of your program will run fast and in exactly the same way as
    the others.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这种风格的一个特点是所有机器完全相同，并且由专门的技术人员保持在相同的状态，这些技术人员将它们托管在一个安全的环境中。大量相同规格的高性能服务器被堆叠在机架中，以确保您的程序实例能够以与其他实例完全相同的方式快速运行。
- en: Grid computers don’t use shared memory; rather, they’re connected by networks
    via I/O. Network capacity is largely used for the compute nodes to access data
    on storage nodes hosting hard drives, rather than to communicate with one another.
    Typically, work is divided into chunks that are sent to compute nodes, which then
    work independently of one another on their given chunks—this is in contrast to
    supercomputers, which are all about dense communication between the processors.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 网格计算机不使用共享内存；相反，它们通过I/O连接网络。网络容量主要用于计算节点访问存储节点上的硬盘数据，而不是节点之间的通信。通常，工作会被划分为数据块并发送到计算节点，然后计算节点独立地处理各自的数据块——这与超级计算机不同，后者强调处理器之间的密集通信。
- en: Because of the relatively weak connections between nodes, grids are sometimes
    built from nodes hosted at geographically separate locations. For example, the
    CERN super-grid links many smaller grids at many universities around the world,
    enabling them to spread load between them for analyzing big data from millions
    of particle physics experiments, as needed to find subtle statistical evidence
    of the Higgs boson.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 由于节点之间的连接相对较弱，网格有时由位于地理位置分散的节点构成。例如，欧洲核子研究组织（CERN）的超大网格将全球多个大学的小型网格连接起来，使它们能够根据需要在分析数百万粒子物理实验的大数据时分摊负载，从而找到希格斯玻色子的微妙统计证据。
- en: '**Decentralized Computing**'
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**去中心化计算**'
- en: As we get even looser in our types of parallelism, we get to *decentralized
    computing*. This is somewhat like multi-site grid computing, but the connection
    between devices is weaker still. A grid is a stack of the same machines kept running,
    healthy, and operating identically by a professional IT technician. By contrast,
    decentralized computing takes a lot of consumer-grade, non-identical computers,
    possibly all owned by different people in different countries, and connects them
    to each other, typically via the public internet. The machines have no shared
    memory, aren’t treated as trusted, and have even less communication across nodes.
    There’s no professional maintaining the machines, nor is there a large setup cost
    for buying identical components.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 随着并行计算类型的逐渐松散，我们进入了*去中心化计算*。这有点像多站点网格计算，但设备之间的连接更加脆弱。网格是由相同的机器组成的堆栈，由专业的IT技术人员维护，保持运行、健康且操作相同。相比之下，去中心化计算利用大量消费者级别的、非相同的计算机，可能这些计算机属于不同国家、不同人的所有，然后通过公共互联网将它们连接在一起。这些计算机没有共享内存，不能被视为可信，也没有节点间的多余通信。没有专业人员维护这些计算机，也没有购买相同组件的高昂成本。
- en: Decentralized computing became popular in the 1990s with the famous Search for
    Extraterrestrial Intelligence (SETI) project. SETI collected big data from large
    radio telescopes pointed at candidate parts of the night sky, then analyzed it
    to look for alien communications signals. You could download the SETI program
    on your desktop, which ran as a screensaver (see [Figure 15-8](ch15.xhtml#ch15fig8))
    when your computer was powered on but not otherwise busy.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 去中心化计算在1990年代因著名的“搜索外星智慧”（SETI）项目而流行起来。SETI从大型射电望远镜收集来自候选夜空区域的大数据，然后分析这些数据，寻找外星通讯信号。你可以在桌面上下载SETI程序，当计算机开机但没有其他任务时，它会作为屏幕保护程序运行（见[图15-8](ch15.xhtml#ch15fig8)）。
- en: '![Image](../images/f0385-01.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0385-01.jpg)'
- en: '*Figure 15-8: The SETI software analyzing radio telescope data for alien communications
    on a home computer*'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '*图15-8：SETI软件在家用计算机上分析射电望远镜数据，寻找外星通讯信号*'
- en: The program would connect to the main SETI server to register, and be sent one
    or more chunks of data to analyze. It would return the results to the server,
    which collected them together with the results from other computers.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 该程序会连接到主SETI服务器进行注册，并接收一个或多个数据块进行分析。分析结果会返回给服务器，服务器将其与其他计算机的结果一起收集。
- en: HTCondor is modern software that enables arbitrary compute jobs to run decentralized
    in the background on regular desktop PCs, for example turning unused desktops
    in an office or classroom into a grid.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: HTCondor是现代软件，能够让任意计算任务在常规桌面PC上去中心化地在后台运行，例如将办公室或教室中未使用的桌面转化为网格。
- en: Unlike grid computing, the worker machines are no longer under the control of
    the central manager, so trust and reliability can’t be assumed. The manager might
    send work to workers that don’t return a reply or that return fraudulent results.
    A standard mitigation is to send the same work to three workers and check that
    they all return the same result—or if two agree, then the third is cheating.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 与网格计算不同，工作机器不再由中央管理员控制，因此不能假设它们是可信的。管理员可能会将任务分配给一些没有返回答复或者返回虚假结果的工作者。一个标准的应对方法是将相同的任务分配给三个工作者，并检查它们是否返回相同的结果——如果有两个一致，那么第三个可能在作弊。
- en: '**Cloud Computing**'
  id: totrans-209
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**云计算**'
- en: The logical evolution of decentralized computing would have been, and might
    still be, for ordinary computer users around the world to routinely connect together
    and trade their unused CPU cycles with each other. This way, when you need to
    run your giant machine learning model, you can run it on one million CPUs all
    around the world that are otherwise sitting idle apart from displaying screensavers,
    instead of having to buy your own personal grid. Then, for the other 99.9 percent
    of the year, you would similarly allow other people to use your own CPU for parts
    of their large computation when it isn’t otherwise being maxed out. Why this still
    hasn’t happened is an interesting social and economic question.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 去中心化计算的逻辑发展应该是，或者说仍然可能是，全球的普通计算机用户定期连接在一起，并相互交换他们未使用的 CPU 计算周期。这样，当你需要运行一个巨大的机器学习模型时，你可以在全球范围内的百万个
    CPU 上运行这些模型，这些 CPU 原本除了显示屏保外几乎处于空闲状态，而不是必须购买自己的个人计算网格。然后，在其他99.9%的时间里，你也可以允许其他人在你的
    CPU 空闲时使用它来进行他们的大规模计算。这种现象为什么还没有发生，是一个有趣的社会和经济问题。
- en: Instead, we’ve seen—as with other aspects of the internet—a few big companies
    move in to dominate the market for distributed computing by maintaining their
    own collections of loosely connected machines, known as *clouds* or *cloud computing*.
    These remove some of the trust, reliability, and payment issues from open distributed
    computing, but at the cost of concerns around privacy and loss of control and
    freedom.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们已经看到——就像互联网的其他方面一样——一些大公司通过维护自己松散连接的机器集群，称为*云*或*云计算*，开始主导分布式计算市场。这些公司消除了开放式分布式计算中的一些信任、可靠性和支付问题，但也带来了隐私问题、控制权丧失和自由的担忧。
- en: '**Compute and Storage**'
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**计算与存储**'
- en: A long-standing debate in distributed computing asks whether it’s better to
    store data on the same machines that are doing the computations, or on separate
    machines.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式计算中，一个长期存在的争论是，是否更好将数据存储在进行计算的机器上，还是存储在单独的机器上。
- en: 'Separating computation from storage means having two different types of machines
    in your distributed network: some specialized for storing data and others specialized
    for computing power. This has the advantage that any available compute node can
    be used to perform computation on any data. Typically, a software filesystem is
    used on the storage nodes, which makes them appear and function as if they were
    a single, very large hard drive. The separation enables the two types of machine
    to be better specialized for their purposes and to be upgraded independently of
    one another; it also makes it easy to balance the ratio of storage to computing
    power. When computation isn’t needed, the compute nodes can be switched off to
    save energy, or made available to other users. When stored data isn’t accessed
    for long periods, it can be relocated down the memory pyramid to tertiary or offline
    memory, then brought back as needed. The disadvantage of this approach is that
    it requires lots of network communication to constantly move data around from
    where it’s stored to where it’s used, which may become a bottleneck.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 将计算与存储分离意味着在分布式网络中拥有两种不同类型的机器：一些专门用于存储数据，另一些专门用于计算。这种做法的优点是任何可用的计算节点都可以用于对任何数据进行计算。通常，存储节点上使用软件文件系统，使其看起来并表现得像是一个单一的、非常大的硬盘。分离使得这两种类型的机器可以更好地专注于各自的任务，并且能够独立升级；它还使得存储与计算能力的比例更容易平衡。当不需要计算时，计算节点可以关闭以节省能源，或者提供给其他用户使用。当存储的数据长时间未被访问时，它可以被转移到内存金字塔中的三级或离线存储中，之后根据需要再调回。该方法的缺点是，它需要大量的网络通信来不断地将数据从存储位置移动到使用位置，这可能会成为瓶颈。
- en: Co-location, on the other hand, means having a single type of machine that stores
    a small part of the data on a local hard drive and also performs computation on
    it. A big dataset can be split across many such machines, each of which performs
    the required computations on the data that it hosts locally, with networking used
    only to transmit the results and to update the data. The advantage here is that
    network communications are minimized, but the disadvantage is that the computation
    for a data chunk can be performed only by the single machine that hosts it, making
    machines easily over- or under-used.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，联合定位意味着使用一种类型的机器，该机器在本地硬盘上存储部分数据，并且对其进行计算。一个大的数据集可以分割到许多这样的机器中，每台机器在本地进行对其托管的数据的所需计算，网络仅用于传输结果和更新数据。其优势在于最小化了网络通信，但劣势在于数据块的计算只能由托管该数据的单一机器完成，导致机器容易被过度使用或不足使用。
- en: Which approach works best depends on the relative speed and costs of networking,
    storage, and computing technologies, which change over time, providing much employment
    for IT consultants swapping between them. Traditional clusters tended to use separation,
    relying on fast networking such as InfiniBand to move the data around quickly.
    However, programmers would sometimes switch to co-location, taking local cache
    copies of data from storage for applications requiring the data to be reread quickly,
    many times. In the 2000s, co-location became more popular, with the *map-reduce*
    algorithm used by search engines finding broader applications through the open
    source software Hadoop and Spark. Map-reduce uses the map replacement for loops
    discussed earlier, but in a recursive manner, with jobs recursively subdividing
    their work to pass to other machines, then collating and merging (that is, reducing)
    their results.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 哪种方法最有效取决于网络、存储和计算技术的相对速度与成本，而这些技术会随时间变化，这也为IT顾问提供了大量的就业机会，他们在这些技术之间来回切换。传统的集群倾向于使用分离式架构，依赖于快速网络（如InfiniBand）迅速移动数据。然而，程序员有时会切换到联合定位，将数据从存储中提取本地缓存副本，用于需要快速多次重新读取数据的应用程序。在2000年代，联合定位变得更加流行，*map-reduce*算法通过开源软件Hadoop和Spark被广泛应用于搜索引擎。Map-reduce使用了之前讨论的替代循环的map方式，但采用递归方式，作业递归地将其工作分解并传递给其他机器，然后将其结果汇总和合并（即减少）。
- en: More recently, the move to cloud computing has seen gains in networking speeds
    in data centers, clearer cost savings from separating storage and computation,
    and the need to dynamically reallocate users and work to different physical machines,
    which has all made separation more attractive again. Some systems try to combine
    elements of both styles, allowing data to be transported over networks to available
    compute nodes, but preferring the data to be computed on its original node if
    possible. Any future move from clouds to decentralized computing, which has slower
    networking than cloud data centers, would likely encourage another swing back
    to co-location.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，云计算的兴起带来了数据中心网络速度的提升，存储和计算分离带来的更明确的成本节省，以及动态重新分配用户和工作到不同物理机器的需求，这一切使得分离式架构再次变得更加吸引人。一些系统尝试结合两种风格的元素，允许数据通过网络传输到可用的计算节点，但如果可能的话，更倾向于在原始节点上计算数据。未来如果云计算转向去中心化计算（其网络速度低于云数据中心），可能会再次促进联合定位的回归。
- en: Instructionless Parallelism
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无指令并行处理
- en: SIMD and MIMD both extend the classical CPU concept of fetching, decoding, and
    executing a sequential program of instructions. But there are many ways to use
    digital logic in parallel that don’t involve creating CPUs and programs of instructions
    at all. Let’s look at some of them here.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: SIMD和MIMD都扩展了经典CPU的概念，即提取、解码并执行一系列顺序指令程序。但实际上，有许多方法可以并行使用数字逻辑，而不涉及创建CPU和指令程序。我们在这里来看一下其中的一些方法。
- en: '*Dataflow Architectures*'
  id: totrans-220
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '*数据流架构*'
- en: Unlike computer scientists, engineers never got hung up on Turing machine serialism
    in the first place. As they deal with the physical world, engineers tend to view
    electronic information processing systems, both analog and digital, as physical
    groups of devices connected together and all operating at all times according
    to the laws of physics, as in a mechanical machine. For them, such systems have
    always been parallel and designed using circuit diagrams, such as [Figure 15-9](ch15.xhtml#ch15fig9),
    rather than as sequential programs of instructions.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 与计算机科学家不同，工程师一开始就没有被图灵机的串行主义所困扰。由于他们处理的是物理世界，工程师倾向于将电子信息处理系统，无论是模拟的还是数字的，视为连接在一起并始终根据物理法则操作的物理设备群体，就像机械机器一样。对于他们来说，这些系统一直都是并行的，并且是通过电路图（如
    [图 15-9](ch15.xhtml#ch15fig9)）进行设计的，而不是作为一系列指令的顺序程序。
- en: '![Image](../images/f0388-01.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0388-01.jpg)'
- en: '*Figure 15-9: A diagram showing parallel information processing in an analog
    guitar distortion pedal*'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 15-9：显示模拟吉他失真踏板中并行信息处理的示意图*'
- en: Rewriting structures like the one in [Figure 15-9](ch15.xhtml#ch15fig9) as sequential
    programs would often appear to engineers to be bizarre, inefficient computer science
    madness. These circuits are composed of hardware components each doing their thing,
    all at the same time, with data flowing continually around connections between
    them. Working with digital logic presents a similar view of the world, until we
    reach the level of [Chapter 7](ch07.xhtml), where we choose to use such logic
    to implement a serial CPU. But digital logic doesn’t have to be used just for
    that purpose. It can also be used in the engineering style of just continuing
    to design higher- and higher-level parallel machines, running together, with connections
    between them. Most of the engineers’ circuit designs, both analog and digital,
    can be translated to LogiSim (or Verilog, VHDL, or Chisel) networks of this form.
    Analog data values can be converted to one of the digital representations we’ve
    seen, and analog operations on them converted to digital arithmetic simple machines.
    As with CPUs, these designs can be burned onto ASIC or FPGA silicon.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 将 [图 15-9](ch15.xhtml#ch15fig9) 中的结构重写为顺序程序通常会让工程师觉得这是一种荒谬、低效的计算机科学疯狂。这些电路由硬件组件组成，每个组件同时执行各自的任务，数据不断在它们之间的连接处流动。处理数字逻辑时也呈现出类似的世界观，直到我们进入
    [第 7 章](ch07.xhtml)，在这里我们选择使用这种逻辑来实现串行 CPU。但数字逻辑不必仅用于此目的。它也可以用于以继续设计更高层次并行机器的工程风格，这些机器协同运行，并且它们之间有连接。大多数工程师的电路设计，无论是模拟的还是数字的，都可以转换为这种形式的
    LogiSim（或 Verilog、VHDL、Chisel）网络。模拟数据值可以转换为我们已经看到的数字表示之一，模拟操作也可以转换为数字算术简单机器。与
    CPU 一样，这些设计可以被烧录到 ASIC 或 FPGA 芯片上。
- en: This approach can be especially efficient for signal processing computations,
    in which a pipeline of processing steps is required. For example, a guitar effects
    unit might require steps of compression, distortion, delay, and reverb. Rather
    than implement these steps in a sequence, they can all exist together in a pipeline,
    as is the case when a guitarist chains together several analog hardware pedals
    implementing one effect each.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法对于信号处理计算特别高效，其中需要一系列的处理步骤。例如，一个吉他效果器可能需要压缩、失真、延迟和混响等步骤。与其将这些步骤按顺序实现，不如让它们在一个管道中一起存在，就像吉他手将多个模拟硬件踏板串联在一起，每个踏板实现一个效果一样。
- en: '*Dataflow Compilers*'
  id: totrans-226
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '*数据流编译器*'
- en: '*Dataflow languages* such as PyTorch, TensorFlow, and Theano, and MATLAB’s
    Simulink, are higher-level languages for specifying parallel information processing.
    These languages enable the programmer to represent the elements of symbolic mathematical
    calculations and their dependencies on one another, and then use specialist compilers
    targeting various types of parallel hardware to order and parallelize them. For
    example, [Figure 15-10](ch15.xhtml#ch15fig10) shows a graphical dataflow description
    of a neural network calculation.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '*数据流语言*，例如 PyTorch、TensorFlow 和 Theano，以及 MATLAB 的 Simulink，是用于指定并行信息处理的高级语言。这些语言使程序员能够表示符号数学计算的元素及其相互依赖关系，然后使用针对各种类型并行硬件的专用编译器对其进行排序和并行化。例如，
    [图 15-10](ch15.xhtml#ch15fig10) 显示了一个神经网络计算的图形数据流描述。'
- en: '![Image](../images/f0389-01.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0389-01.jpg)'
- en: '*Figure 15-10: The dataflow of a PyTorch neural network calculation*'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 15-10：PyTorch 神经网络计算的数据流*'
- en: Like hardware description languages, these aren’t *programming* languages, but
    instead are *declarative* languages, more like writing XML or databases than writing
    traditional imperative programs as sequences of instructions. (SPIR-V can also
    be considered as a mid-level dataflow language due to its abstraction of registers
    to identifiers.) Compilers may also exist from dataflow languages to hardware
    description languages such as Verilog and VHDL, as well as to GPU, CPU SIMD, or
    serial CPU instructions.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 像硬件描述语言一样，这些不是*编程*语言，而是*声明式*语言，更像是编写XML或数据库，而不是编写传统的命令式程序，作为指令序列。(SPIR-V也可以被认为是一种中级数据流语言，因为它将寄存器抽象为标识符。）从数据流语言到硬件描述语言（如Verilog和VHDL），以及到GPU、CPU
    SIMD或串行CPU指令的编译器也可能存在。
- en: An ongoing research area is how to automatically compile a regular, serial,
    C-like language into a dataflow language. OOOE is perhaps just the first tip of
    the iceberg here, optimizing machine code instructions only in small windows of
    time, but we can imagine a day when entire programs are transformed similarly
    and automatically into Verilog or perhaps SPIR-V by using advanced parallel algorithms
    and complexity theory to extract the most parallelized form of the program. Modern
    compilers can do this for “trivial” cases such as converting loops to maps when
    iterations of the loops clearly don’t affect one another. In general, however,
    this is difficult work, not least because so much computer science theory is built
    on serial machines; it may be that big future ideas are needed to rebuild the
    subject with parallelism as a more fundamental starting point. Functional programming
    languages may form part of the solution, as they limit the amount of visible state,
    making it easier to split work into independent and parallelizable pieces.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 一个正在进行的研究领域是如何将常规的串行C语言自动编译为数据流语言。OOOE可能只是冰山一角，它仅在短时间窗口内优化机器代码指令，但我们可以想象，未来某一天，整个程序会通过使用先进的并行算法和复杂性理论，自动地、类似地转化为Verilog或SPIR-V，以提取程序最并行化的形式。现代编译器可以处理一些“简单”的情况，比如将循环转化为映射，当循环的迭代彼此没有明显影响时。然而，一般来说，这项工作是困难的，尤其是因为计算机科学理论大多基于串行机器；可能需要未来的重大创新，才能在并行性作为更基本的起点的框架下重建这个学科。函数式编程语言可能是解决方案的一部分，因为它们限制了可见状态的数量，使得将工作分解为独立且可并行化的部分变得更容易。
- en: '*Hardware Neural Networks*'
  id: totrans-232
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '*硬件神经网络*'
- en: A particular application of dataflow architectures is to enable fast hardware
    implementations of the backpropagation neural network algorithm. We’ve known since
    the 1960s that this algorithm is able to recognize and classify any pattern, given
    enough data and computing time. We’ve also known that it’s highly parallelizable,
    with its neural network being constructed from many “neuron” units that can compute
    independently and pass messages to their neighbors.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 数据流架构的一个特定应用是实现反向传播神经网络算法的快速硬件实现。自20世纪60年代以来，我们就知道这个算法能够识别和分类任何模式，只要提供足够的数据和计算时间。我们还知道它具有高度的并行性，神经网络由许多可以独立计算并向邻居传递信息的“神经元”单元构成。
- en: During the 2010s, GPU architectures first enabled these computations to be implemented
    cheaply in parallel, and were found to enable successful and accurate recognition
    of complex patterns such as faces in images and words in speech. This created
    a huge commercial demand for even faster, specialized architectures to implement
    the backpropagation algorithm even more efficiently than on GPUs.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在2010年代，GPU架构首次使得这些计算能够以低成本并行实现，并且能够成功且准确地识别复杂的模式，如图像中的人脸和语音中的单词。这催生了对更快、更专业化架构的巨大商业需求，以比GPU更高效地实现反向传播算法。
- en: 'There are two main approaches to hardware neural networks in current use: FPGAs
    and NPUs. Let’s consider them now.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 当前使用的硬件神经网络主要有两种方法：FPGA和NPU。我们现在来讨论一下这两者。
- en: '**Backpropagation on FGPAs**'
  id: totrans-236
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**反向传播在FPGA上的应用**'
- en: Researchers have been building backpropagation neural networks on parallel FPGAs
    for many decades. FPGA designs may try to physically lay out circuits in terms
    of component modules for each neuron, or they may just leave the layout to a Chisel
    or Verilog compiler, which tends to produce random-looking circuits that implement
    the same logic, sometimes more efficiently. During the 2010s, these systems were
    built at larger scales for commercial use, especially by “big tech” companies
    for use in *training* neural networks to make predictions about their big data.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员已经在并行FPGA上构建反向传播神经网络几十年了。FPGA设计可能尝试以每个神经元的组件模块物理布局电路，或者可能将布局交给Chisel或Verilog编译器，这样通常会生成随机外观的电路，虽然它们实现相同的逻辑，有时效率更高。在2010年代，这些系统以更大规模进行商业化，特别是由“大型科技”公司用于*训练*神经网络，以便对其大数据进行预测。
- en: '**Backpropagation on Neural Processing Units**'
  id: totrans-238
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**神经处理单元上的反向传播**'
- en: A recent architecture trend has been the production of similar parallel neural
    network hardware on ASICs, which run faster than FPGAs. Such chips are known as
    *neural processor units (NPUs)* or *tensor processor units (TPUs)*.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的架构趋势是生产类似的并行神经网络硬件，这些硬件运行速度比FPGA更快，通常称为*神经处理单元（NPU）*或*张量处理单元（TPU）*。
- en: Some of these are designed as high-power systems for use in *training* neural
    network models, typically deployed in clusters in racks in computing centers.
    Others are designed as low-power embedded systems for use in *running* pretrained
    networks for real-time pattern recognition, and are included in smartphones and
    IoT devices (for example, Intel Neural Compute Sticks and the Arduino-based Genuino).
    These units can power applications such as Snapchat’s real-time face recognition
    and filtering. The difference between Moore’s law for clock speed and for transistor
    size has been a major driver of these systems, with phone designers having lots
    of spare silicon to use up and looking for things to do with it. NPUs were initially
    “pushed” by manufacturers onto phones, looking for applications, rather than “pulled”
    by consumer demand.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 这些系统中的一些被设计为高功率系统，用于*训练*神经网络模型，通常在计算中心的机架中以集群形式部署。另一些则被设计为低功耗嵌入式系统，用于*运行*预训练网络进行实时模式识别，并被包括在智能手机和物联网设备中（例如，英特尔神经计算棒和基于Arduino的Genuino）。这些单元可以为应用提供支持，如Snapchat的实时面部识别和滤镜。摩尔定律在时钟速度和晶体管尺寸上的差异是推动这些系统的主要因素，手机设计师有大量多余的硅材料可供使用，并寻找利用它的方式。NPU最初是由制造商推动到手机上的，寻找应用，而不是由消费者需求推动。
- en: Summary
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: We’ve seen several forms of parallelism in previous chapters, beginning with
    Babbage’s parallel arithmetic instructions and register-level parallelism (ripple-carry
    adders), then instruction-level parallelism such as pipelining and OOOE. At those
    levels, the programmer still writes a serial program and doesn’t need to know
    or care that parallelism is making the program run faster.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们已经看到几种形式的并行性，从巴贝奇的并行算术指令和寄存器级并行性（波纹进位加法器），到指令级并行性，如流水线和超出顺序执行（OOOE）。在这些层面，程序员仍然编写串行程序，不需要知道或关心并行性如何加速程序的运行。
- en: In contrast, the parallelisms seen in this chapter, SIMD and MIMD, *do* affect
    the programmer, who needs to understand their details and write programs to best
    take advantage of them. We looked at architectures in order of the tightness of
    their parallelism, beginning with systems that are clearly single computers and
    gradually making the parallel executions more independent until the systems look
    more like multiple computers connected by networks.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，本章中看到的并行性，SIMD和MIMD，*确实*会影响程序员，程序员需要理解它们的细节并编写程序，以充分利用这些并行性。我们按照并行性的紧密程度顺序查看了架构，首先是那些显然是单一计算机的系统，然后逐渐让并行执行变得更加独立，直到系统看起来更像是通过网络连接的多个计算机。
- en: SIMD is where a single instruction is executed multiple times in parallel, on
    multiple different data items. It can be found in CPUs and GPUs. Typically, user
    assembly programming for CPU requires thinking in terms of parallel SIMD instructions
    with fixed, power-of-two parallel copies, while on GPU the ISAs may be structured
    in terms of instructions for a single thread, allowing more variation in the number
    of threads launched. The CPU style doesn’t easily enable programs with branches,
    while the GPU style does so via masking or serial split subgroup execution.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: SIMD 是指单条指令在多个不同的数据项上并行执行多次。它可以在 CPU 和 GPU 中找到。通常，CPU 的用户汇编编程需要考虑并行 SIMD 指令，使用固定的、二的幂次的并行副本，而在
    GPU 上，指令集架构（ISA）可能以单个线程的指令为结构，允许线程数目更多样化。CPU 风格不容易支持带有分支的程序，而 GPU 风格则通过掩码或串行拆分子组执行来实现这一点。
- en: MIMD is a looser form of parallelism that can enable different programs to run
    on different machines. This includes shared-memory systems, in which all processors
    can load and store in the same address space. These systems can be multicore CPUs
    located in the same physical box as RAM, or large NUMA supercomputers in which
    memory in physically further away boxes takes longer to access than nearby memory.
    Distributed systems are looser still, as each processor or small group of processors
    has its own address space, and communication between nodes occurs only via network
    I/O.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: MIMD 是一种较为松散的并行方式，可以使不同的程序在不同的机器上运行。这包括共享内存系统，其中所有处理器可以在同一地址空间内加载和存储数据。这些系统可以是位于同一物理机箱内的多核
    CPU，也可以是大型 NUMA 超级计算机，其中位于物理位置较远的内存访问时间比附近的内存更长。分布式系统则更加松散，因为每个处理器或小组处理器都有自己的地址空间，节点之间的通信仅通过网络
    I/O 进行。
- en: The boundary between a single versus multiple computers seems blurry. Most people
    would consider that a CPU with SIMD instructions is a single computer. It’s harder
    to classify a NUMA supercomputer or a grid system. Decentralized systems such
    as SETI and Bitcoin combine resources from machines around the world to behave
    in similar ways to grids. Today, almost every computer has been connected to the
    internet at some point, where it has communicated with others, perhaps becoming
    part of a single global computation and computer.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 单台计算机与多台计算机之间的边界似乎有些模糊。大多数人会认为具有 SIMD 指令的 CPU 是一台单独的计算机。对于 NUMA 超级计算机或网格系统来说，划分就更为困难。像
    SETI 和比特币这样的去中心化系统，将来自世界各地的机器资源结合起来，表现得与网格系统相似。今天，几乎每台计算机都曾连接过互联网，在此过程中，它可能与其他计算机进行了通信，甚至成为了全球计算与计算机的一部分。
- en: There are still many programmers untrained in parallel algorithms who see them
    as the exotic stuff of graduate research degrees. The traditional view of parallel
    programming was that “by the time you’ve finished writing your fancy parallel
    thing, Intel will have made a faster processor that makes my serial C code go
    faster than yours.” This doesn’t work anymore. Programming now has to be done
    in parallel because the serial silicon-based architecture has reached its limit.
    This may require some quite foundational change to computer science as a whole.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然有许多程序员未接受过并行算法的训练，他们认为这些算法只是研究生学位的奇特内容。传统的并行编程观念是，“等你完成了你那复杂的并行程序，英特尔就会推出一个更快的处理器，使我的串行
    C 代码比你的快。”这种观念现在已经不再适用。现在编程必须采用并行方式，因为基于硅的串行架构已经达到了极限。这可能需要计算机科学整体的某些基础性变革。
- en: Will you as a programmer have to care about parallel programming? There are
    several possible futures here. In one, you go on writing serial programs as you
    do now, with clever programmers writing compilers to turn those into parallel
    systems. Another scenario, happening now, involves a few programmers creating
    specific libraries to do parallel computing operations, and you calling single
    functions in your serial program to run each one. A third scenario is that you’ll
    need to write more and more SIMD programs by yourself, requiring a significant
    change to your programming style. A fourth is that you’ll need to become an MIMD
    programmer, which is likely a larger style change. Along the way, you might switch
    your loops to maps, and perhaps from imperative to functional programming. Or
    perhaps you’ll stop programming altogether and, like engineers, just design hardware
    circuits to perform computations using a declarative language. This is now a big
    open question, with many programmers placing their career bets by choosing which
    styles to learn.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 作为程序员，你是否需要关心并行编程？这里有几个可能的未来。在其中一个未来中，你继续像现在一样编写串行程序，聪明的程序员们编写编译器，将它们转换为并行系统。另一个现在正在发生的场景是，少数程序员创建特定的库来执行并行计算操作，而你则在串行程序中调用单一函数来运行每一个操作。第三种情况是，你将需要自己编写越来越多的
    SIMD 程序，这将要求你显著改变编程风格。第四种情况是，你需要成为 MIMD 程序员，这可能是一个更大的风格变化。在这个过程中，你可能会将循环转换为映射，或许会从命令式编程转向函数式编程。或者，也许你会完全停止编程，就像工程师一样，仅仅设计硬件电路，通过声明式语言执行计算。这现在是一个很大的悬而未决的问题，许多程序员通过选择学习哪种编程风格来为自己的职业生涯下注。
- en: Exercises
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习
- en: '**x86 SIMD**'
  id: totrans-250
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**x86 SIMD**'
- en: Try running the x86 MMX, SSE, and AVX codes shown in this chapter. You can run
    them on bare metal using *.iso* files as in [Chapter 13](ch13.xhtml). Or, if you
    have some understanding of operating systems, see the [Appendix](bm01.xhtml) for
    how to run them from inside your operating system. This is a faster way to do
    x86 assembly development.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试运行本章中展示的 x86 MMX、SSE 和 AVX 代码。你可以像在[第13章](ch13.xhtml)中那样，使用 *.iso* 文件在裸机上运行它们。或者，如果你对操作系统有一些了解，参见[附录](bm01.xhtml)了解如何在操作系统内部运行它们。这是一种更快速的
    x86 汇编开发方式。
- en: '**Nvidia PTX Programming**'
  id: totrans-252
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**Nvidia PTX 编程**'
- en: 'If you have access to an Nvidia GPU—either on your own PC or via a free cloud
    service with GPU options such as *[https://colab.google](https://colab.google)*—you
    can compile, edit, and run the chapter’s PTX examples. We assumed in the examples
    that someone or something will be calling the kernels and sending inputs to them.
    To create that linkage, create a file *mykernel.ptx* with the following code in
    it:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你有访问 Nvidia GPU 的权限——无论是你自己 PC 上的，还是通过像 *[https://colab.google](https://colab.google)*
    这样的提供 GPU 选项的免费云服务——你可以编译、编辑并运行本章中的 PTX 示例。在示例中，我们假设会有某些人或程序调用内核并将输入传递给它们。为了建立这种关联，创建一个名为
    *mykernel.ptx* 的文件，并在其中写入以下代码：
- en: '[PRE17]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Paste in the code from any of the examples below the indicated line to wrap
    them up. Then assemble to Nvidia executable (cubin) code with:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将下列示例中的代码粘贴到指定行之后，以完成它们。然后使用以下命令将其汇编为 Nvidia 可执行文件（cubin）代码：
- en: '[PRE18]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Here, the `-arch` argument is the code for the Nvidia model you’re using—for
    example, `sm_75` is the real name for Turing.
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里，`-arch` 参数是你使用的 Nvidia 模型的代码——例如，`sm_75` 是 Turing 的真实名称。
- en: 'If you’d like to inspect the executable as human-readable hex and SASS, this
    can be done with:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你想查看可执行文件的人类可读的十六进制和 SASS 格式，可以使用以下命令：
- en: '[PRE19]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: You now need some code to run on the host CPU to manage the process of sending
    this executable to the GPU. You also need to send data inputs for it to run on,
    as well as commands to launch the desired number of kernels and print out their
    results. The following code will do all this, and can be used with any kernel
    that’s been wrapped the way we’ve discussed.
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你现在需要一些代码在主机 CPU 上运行，以管理将此可执行文件发送到 GPU 的过程。你还需要为其运行提供数据输入，并且需要发送命令以启动所需数量的内核并打印出它们的结果。以下代码将完成所有这些操作，并且可以与任何我们讨论过的方式包装的内核一起使用。
- en: '[PRE20]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Compile and run this with Nvidia’s `nvcc` tool:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Nvidia 的 `nvcc` 工具编译并运行：
- en: '[PRE21]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: You should see the result printed on the host terminal.
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该能在主机终端看到打印出的结果。
- en: '**More Challenging**'
  id: totrans-265
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**更具挑战性**'
- en: If you’d like to try programming in SASS, or even Nvidia machine code, third-party
    SASS assemblers are available and documented for many of the Nvidia architectures.
    At *[https://github.com/daadaada/turingas](https://github.com/daadaada/turingas)*
    you can find a SASS assembler for Volta, Turing, and Ampere; this site also has
    SASS assembly code examples and links to similar assemblers for Fermi, Maxwell,
    and Kepler. Nvidia provides a SASS debugger tool at *[https://docs.nvidia.com/gameworks/content/developertools/desktop/ptx_sass_assembly_debugging.htm](https://docs.nvidia.com/gameworks/content/developertools/desktop/ptx_sass_assembly_debugging.htm)*,
    and a GPU emulator at *[https://github.com/gpgpu-sim/gpgpu-sim_distribution](https://github.com/gpgpu-sim/gpgpu-sim_distribution)*.
    Nvidia lists the meaning of SASS mnemonics, but not their arguments and semantics,
    at *[https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html](https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html)*.
    Some of the third-party SASS assemblers include useful example SASS programs.
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你想尝试在SASS中编程，甚至是Nvidia机器代码，许多Nvidia架构都有第三方SASS汇编器，并且有文档说明。在*[https://github.com/daadaada/turingas](https://github.com/daadaada/turingas)*网站上，你可以找到适用于Volta、Turing和Ampere的SASS汇编器；该网站还提供了SASS汇编代码示例，并链接到Fermi、Maxwell和Kepler的类似汇编器。Nvidia提供了一个SASS调试工具，地址是*[https://docs.nvidia.com/gameworks/content/developertools/desktop/ptx_sass_assembly_debugging.htm](https://docs.nvidia.com/gameworks/content/developertools/desktop/ptx_sass_assembly_debugging.htm)*，并且提供了一个GPU模拟器，地址是*[https://github.com/gpgpu-sim/gpgpu-sim_distribution](https://github.com/gpgpu-sim/gpgpu-sim_distribution)*。Nvidia在*[https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html](https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html)*列出了SASS助记符的含义，但没有提供它们的参数和语义。一些第三方SASS汇编器还包括有用的SASS程序示例。
- en: If you have access to a non-Nvidia GPU, find its make and model and see if there’s
    a public ISA and assembler available for it, similar to PTX or SASS. Assemblers
    are sometimes created and documented by third-party reverse engineers, even if
    a GPU manufacturer doesn’t make or document one itself. How does it compare to
    the CPU ISAs you’ve seen?
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你可以访问非Nvidia GPU，找出其品牌和型号，看看是否有类似PTX或SASS的公开ISA和汇编器可用。有时，第三方逆向工程师会为GPU制造商未提供或未文档化的ISA和汇编器创建并记录相关内容。与您曾见过的CPU
    ISA相比，它是如何的？
- en: Simulate a cluster of PCs by running multiple instances of Virtual-Box, as used
    in [Chapter 13](ch13.xhtml). Research how to install and run SGE, MPI, or HTCondor
    across them.
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行多个Virtual-Box实例来模拟一群PC，就像在[第13章](ch13.xhtml)中使用的那样。研究如何在这些实例上安装和运行SGE、MPI或HTCondor。
- en: If you know neural network theory, add backpropagation to the GPU neuron. Add
    code to create and run several layers of several neurons each to learn and run
    some pattern recognition.
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你了解神经网络理论，可以为GPU神经元添加反向传播。添加代码以创建并运行多个神经元层，每层包含多个神经元，用于学习并运行某些模式识别任务。
- en: Further Reading
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'For a reverse engineering of, and third-party open source assembler for, the
    Nvidia Kepler architecture, see X. Zhang et al., “Understanding the GPU Microarchitecture
    to Achieve Bare-Metal Performance Tuning,” in *Proceedings of the 22nd ACM SIGPLAN
    Symposium on Principles and Practice of Parallel Programming* (New York: Association
    for Computing Machinery, 2017).'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 若想了解Nvidia Kepler架构的逆向工程及第三方开源汇编器，请参阅X. Zhang等人的文章，“理解GPU微架构以实现裸机性能调优”，该文发表于*第22届ACM
    SIGPLAN并行编程原理与实践研讨会论文集*（纽约：计算机协会，2017）。
- en: For a fully open source hardware GPU architecture, see MIAOW, *[https://raw.githubusercontent.com/wiki/VerticalResearchGroup/miaow/files/MIAOW_Architecture_Whitepaper.pdf](https://raw.githubusercontent.com/wiki/VerticalResearchGroup/miaow/files/MIAOW_Architecture_Whitepaper.pdf)*.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你对完全开源的硬件GPU架构感兴趣，可以查看MIAOW，* [https://raw.githubusercontent.com/wiki/VerticalResearchGroup/miaow/files/MIAOW_Architecture_Whitepaper.pdf](https://raw.githubusercontent.com/wiki/VerticalResearchGroup/miaow/files/MIAOW_Architecture_Whitepaper.pdf)*。
- en: For more on SPIR-V, see J. Kessenich, “An introduction to SPIR-V,” *[https://registry.khronos.org/SPIR-V/papers/WhitePaper.pdf](https://registry.khronos.org/SPIR-V/papers/WhitePaper.pdf)*.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欲了解更多关于SPIR-V的信息，请参考J. Kessenich的文章，“SPIR-V简介”，* [https://registry.khronos.org/SPIR-V/papers/WhitePaper.pdf](https://registry.khronos.org/SPIR-V/papers/WhitePaper.pdf)*。
- en: 'For an example of compiling Python into CPU-less dataflow digital logic, see
    K. Jurkans and C. Fox, “Python Subset to Digital Logic Dataflow Compiler for Robots
    and IoT,” in *International Symposium on Intelligent and Trustworthy Computing,
    Communications, and Networking (ITCCN-2023)* (Exeter, UK: IEEE, 2023).'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 若想了解如何将Python编译为无CPU的数据流数字逻辑，请参见K. Jurkans和C. Fox的文章，“Python子集到数字逻辑数据流编译器，用于机器人和物联网”，该文发表于*国际智能与可信计算、通信与网络研讨会（ITCCN-2023）*（英国埃克塞特：IEEE，2023）。
