- en: '13'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '13'
- en: Neural Networks
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络
- en: '![](Images/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/chapterart.png)'
- en: Deep learning algorithms are based on building a network of connected computational
    elements. The fundamental unit of such networks is a small bundle of computation
    called an *artificial neuron*, though it’s often referred to simply as a *neuron*.
    The artificial neuron was inspired by human neurons, which are the nerve cells
    that make up our brain and central nervous system and are largely responsible
    for our cognitive abilities.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习算法基于建立一个连接的计算单元网络。这些网络的基本单元是一个小型计算捆绑体，称为*人工神经元*，尽管它通常简称为*神经元*。人工神经元的灵感来自于人类神经元，神经元是构成我们大脑和中枢神经系统的神经细胞，主要负责我们的认知能力。
- en: In this chapter, we see what artificial neurons look like and how to arrange
    them into networks. We then group them into layers, which create deep learning
    networks. We also look at various ways to configure the outputs of these artificial
    neurons so that they produce the most useful results.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将展示人工神经元的样子，并讨论如何将它们安排成网络。接着，我们将它们分组为层，从而形成深度学习网络。我们还会探讨如何配置这些人工神经元的输出，以便它们产生最有用的结果。
- en: Real Neurons
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 真实神经元
- en: In biology, the term *neuron* is applied to a wide variety of complex cells
    distributed throughout every human body. These cells all have similar structure
    and behavior, but they’re specialized for many different tasks. Neurons are sophisticated
    pieces of biology that use a mix of chemistry, physics, electricity, timing, proximity,
    and other means to perform their behaviors and communicate with one another (Julien
    2011; Khanna 2018; Lodish et al. 2000; Purves et al. 2001). A highly simplified
    sketch of a neuron is shown in [Figure 13-1](#figure13-1).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在生物学中，*神经元*一词适用于分布在人类身体各处的多种复杂细胞。这些细胞都具有相似的结构和行为，但它们被专门化以执行许多不同的任务。神经元是复杂的生物体，利用化学、物理、电学、时间、接近性等多种方式来执行其行为并相互通信（Julien
    2011；Khanna 2018；Lodish 等人 2000；Purves 等人 2001）。图 [13-1](#figure13-1) 显示了神经元的一个高度简化的示意图。
- en: '![F13001](Images/F13001.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![F13001](Images/F13001.png)'
- en: 'Figure 13-1: A sketch of a highly simplified biological neuron (in red) with
    a few major structures identified. This neuron’s outputs are communicated to another
    neuron (in blue), only partially shown (adapted from Wikipedia 2020b).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13-1：一个高度简化的生物神经元示意图（红色），标出了几个主要结构。该神经元的输出信号传递给另一个神经元（蓝色），后者仅部分显示（改编自 Wikipedia
    2020b）。
- en: Neurons are information processing machines. One type of information arrives
    in the form of chemicals called *neurotransmitters* that temporarily *bind*, or
    attach, onto *receptor sites* located on the neuron (Goldberg 2015). Let’s sketch
    out what happens next in the broadest possible terms.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元是信息处理机器。信息的一种类型是通过名为*神经递质*的化学物质传递，这些化学物质暂时*结合*或附着在神经元上的*受体位点*（Goldberg 2015）。让我们概述一下接下来会发生什么。
- en: The chemicals that bind to the receptor sites cause electrical signals to travel
    into the body of the neuron. Each of these signals can be either positive or negative.
    All of the electrical signals arriving at the neuron’s body over a short interval
    of time are added together and then compared to a *threshold*. If the total exceeds
    that threshold, a new signal is sent along the axon to another part of the neuron,
    causing specific amounts of neurotransmitters to be released into the environment.
    These molecules then bind with other neurons, and the process repeats.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 结合到受体位点的化学物质会导致电信号进入神经元的主体。每个信号可能是正电或负电。在短时间内到达神经元主体的所有电信号会被加在一起，然后与*阈值*进行比较。如果总和超过该阈值，就会沿着轴突发送一个新信号到神经元的另一部分，导致一定量的神经递质释放到环境中。这些分子随后会与其他神经元结合，过程重复进行。
- en: In this way, information is propagated and modified as it flows through the
    densely connected network of neurons in the brain and central nervous system.
    If two neurons are physically close enough to each other that one can receive
    the neurotransmitters released by the other, we say that the neurons are *connected*,
    even though they may not be actually touching. There is some evidence that the
    particular pattern of connections between neurons is as essential to cognition
    and identity as the neurons themselves (Sporns, Tononi, and Kötter 2005; Seung
    2013). A map of an individual’s neuronal connections is called their *connectome*.
    Connectomes are as unique as fingerprints or iris patterns.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式，信息在大脑和中枢神经系统中通过神经元的密集连接网络传播并被修改。如果两个神经元之间足够接近，以至于一个可以接收到另一个释放的神经递质，我们就说这些神经元是*连接*的，即使它们可能并没有实际接触。有一些证据表明，神经元之间的特定连接模式对认知和身份的形成至关重要，甚至和神经元本身一样重要（Sporns、Tononi
    和 Kötter 2005；Seung 2013）。一个个体神经元连接的地图被称为*连接组*。连接组和指纹或虹膜图案一样独特。
- en: Although real neurons and their surrounding environment are tremendously complex
    and subtle, the basic mechanism described here has an appealing elegance. Responding
    to this, some scientists have attempted to emulate or duplicate the brain by creating
    enormous numbers of simplified neurons and their environment, in hardware or software,
    hoping that interesting behavior will emerge (Furber 2012; Timmer 2014). So far,
    this has not delivered results that most people would call intelligence.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管真实的神经元及其周围环境极其复杂和微妙，但这里描述的基本机制却具有一种迷人的优雅。对此，一些科学家尝试通过创建大量简化的神经元及其环境，使用硬件或软件来模仿或复制大脑，希望能出现有趣的行为（Furber
    2012；Timmer 2014）。到目前为止，这还没有产生大多数人认为是智能的结果。
- en: But we can connect up simplified neurons in specific ways to produce great results
    on a wide range of problems. Those are the types of structures that will be our
    focus in this chapter, and the rest of this book.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，我们可以以特定的方式连接简化的神经元，从而在广泛的问题上产生显著的结果。这些结构将是本章和本书其余部分的重点。
- en: Artificial Neurons
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人工神经元
- en: The “neurons” we use in machine learning are inspired by real neurons in the
    same way that a stick figure drawing is inspired by a human body. There’s a resemblance,
    but only in the most general sense. Almost all of the details are lost along the
    way, and we’re left with something that’s more of a reminder of the original,
    rather than even a simplified copy.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在机器学习中使用的“神经元”受真实神经元的启发，就像木偶画是受人类身体启发一样。它们有相似之处，但仅仅是最一般的层面。几乎所有的细节在这个过程中都丢失了，最终我们得到的更多的是对原始对象的提醒，而不是一个简化的复制品。
- en: This has led to some confusion, particularly in the popular press, where “neural
    network” is sometimes used as a synonym for “electronic brain,” and from there,
    it’s only a short step to general intelligence, consciousness, emotions, and perhaps
    world domination and the elimination of human life. In reality, the neurons we
    use are so abstracted and simplified from real neurons that many people prefer
    instead to call them by the more generic name of *units*. But for better or worse,
    the word *neuron*, the phrase *neural net*, and all the related language are apparently
    here to stay, so we use them in this book as well.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了一些混淆，特别是在大众媒体中，“神经网络”有时被用作“电子大脑”的同义词，从而只需一步之遥就能联想到通用智能、意识、情感，甚至可能是世界统治和消灭人类生命。实际上，我们使用的神经元与真实神经元的抽象和简化程度如此之大，以至于许多人更愿意将它们称为更通用的*单元*。但无论好坏，*神经元*这个词、*神经网络*这个短语以及所有相关的语言显然将会存在，因此我们在本书中也使用它们。
- en: The Perceptron
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 感知机
- en: The history of artificial neurons may be said to begin in 1943, with the publication
    of a paper that presented a massively simplified abstraction of a neuron’s basic
    functions in mathematical form, and described how multiple instances of this object
    could be connected into a *network*, or *net*. The big contribution of this paper
    was that it proved mathematically that such a network could implement any idea
    expressed in the language of mathematical logic (McCulloch and Pitts 1943). Since
    mathematical logic is the basis of machine calculation, that means neurons could
    perform mathematics. This was a big deal, because it provided a bridge between
    the fields of math, logic, computing, and neurobiology.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经元的历史可以追溯到 1943 年，当时发表了一篇论文，呈现了神经元基本功能的一个大幅简化的数学抽象形式，并描述了如何将该对象的多个实例连接成一个*网络*或*网*。这篇论文的重要贡献在于，它从数学上证明了这样的网络可以实现任何在数学逻辑语言中表达的想法（McCulloch
    和 Pitts 1943）。由于数学逻辑是机器计算的基础，这意味着神经元可以执行数学运算。这是一个重大突破，因为它为数学、逻辑、计算机学和神经生物学这几个领域搭建了桥梁。
- en: Building on that insight, in 1957 the *perceptron* was proposed as a simplified
    mathematical model of a neuron (Rosenblatt 1962). [Figure 13-2](#figure13-2) is
    a block diagram of a single perceptron with four inputs.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在此基础上，1957 年提出了感知器作为神经元的简化数学模型（Rosenblatt 1962）。[图 13-2](#figure13-2) 是一个具有四个输入的单个感知器的框图。
- en: '![f13002](Images/f13002.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![f13002](Images/f13002.png)'
- en: 'Figure 13-2: A four-input perceptron'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13-2：一个四输入感知器
- en: Every input to a perceptron is represented by a single floating-point number.
    Each input is multiplied by a corresponding floating-point number called a *weight*.
    The results of these multiplications are all added together. Finally, we compare
    the result to a threshold value. If the result of the summation is greater than
    0, the perceptron produces an output of +1, otherwise it’s −1 (in some versions,
    the outputs are 1 and 0, rather than +1 and −1).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 每个输入到感知器的数据都由一个浮动的浮点数表示。每个输入都会乘以一个对应的浮动浮点数，称为*权重*。这些乘积的结果都会加在一起。最后，我们将结果与一个阈值进行比较。如果求和结果大于
    0，感知器输出 +1，否则输出 −1（在某些版本中，输出为 1 和 0，而不是 +1 和 −1）。
- en: Though the perceptron is a vastly simplified version of a real neuron, it’s
    proven to be a terrific building block for deep learning systems.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管感知器是一个大幅简化的真实神经元版本，但它已经证明是深度学习系统的一个出色构建块。
- en: The history of the perceptron is an interesting part of the culture of machine
    learning, so let’s look at just a couple of its key events; more complete versions
    may be found online (Estebon 1997; Wikipedia 2020a).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器的历史是机器学习文化中的一个有趣部分，我们来看看其中的几个关键事件；更完整的版本可以在线找到（Estebon 1997；Wikipedia 2020a）。
- en: After the principles of the perceptron had been verified in software, a perceptron-based
    computer was built at Cornell University in 1958\. It was a rack of wire-wrapped
    boards the size of a refrigerator, called the Mark I Perceptron (Wikipedia 2020c).
    The device was built to process images, using a grid of 400 photocells that could
    digitize an image at a resolution of 20 by 20 pixels (the word *pixel* hadn’t
    yet been coined). The weight applied to each input of the perceptron was set by
    turning a knob that controlled an electrical component called a potentiometer.
    To automate the learning process, electric motors were attached to the potentiometers
    so the device could literally turn its own knobs to adjust its weights and thereby
    change its calculations, and thus its output. The theory guaranteed that, with
    the right data, the system could learn to separate two different classes of inputs
    that could be split with a straight line.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在感知器原理通过软件验证后，1958 年康奈尔大学建造了一个基于感知器的计算机。它是一个大小如冰箱的线圈板架，名为 Mark I Perceptron（Wikipedia
    2020c）。该设备被设计用来处理图像，使用一个 400 个光电池的网格，能够以 20×20 像素的分辨率对图像进行数字化（当时还没有“*像素*”这个词）。施加在感知器每个输入上的权重是通过转动一个控制电气元件——电位计的旋钮来设置的。为了自动化学习过程，电动机被连接到电位计上，这样设备就可以通过自转旋钮调整权重，从而改变其计算，进而改变其输出。理论上保证，使用正确的数据，系统能够学习将两类不同的输入分开，这些输入能够通过一条直线来划分。
- en: Unfortunately, not many interesting problems involve sets of data that are separated
    by a straight line, and it proved hard to generalize the technique to more complicated
    arrangements of data. After a few years of stalled progress, a book proved that
    the original perceptron technique was fundamentally limited (Minsky and Papert
    1969). It showed that the lack of progress wasn’t due to a lack of imagination,
    but the result of theoretical limits built into the structure of a perceptron.
    Most interesting problems, and even some very simple ones, were provably beyond
    the ability of a perceptron to solve.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，并不是很多有趣的问题涉及被一条直线分隔的数据集，而且将这一技术推广到更复杂的数据排列中也证明很困难。在几年的停滞不前之后，一本书证明了原始感知机技术在理论上存在局限性（Minsky
    和 Papert 1969）。它显示出进展停滞并不是因为缺乏想象力，而是感知机结构中固有的理论限制。大多数有趣的问题，甚至一些非常简单的问题，都证明超出了感知机的解决能力。
- en: This result seemed to signal the end of perceptrons for many people, and a popular
    consensus formed that the perceptron approach was a dead end. Enthusiasm, interest,
    and funding all dried up, and most people directed their research to other problems.
    This period, which lasted roughly between the 1970s and 1990s, was called the
    *AI winter*.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这一结果似乎标志着许多人对感知机的看法的结束，并形成了一个流行的共识：感知机方法是一条死胡同。热情、兴趣和资金都枯竭，大多数人将他们的研究方向转向了其他问题。这个时期大约从1970年代到1990年代，被称为*人工智能寒冬*。
- en: But despite a widespread interpretation that the perceptron book had closed
    the door on perceptrons in general, in fact it had only shown the limitations
    of how they’d been used up to that time. Some people thought that writing off
    the whole idea was an overreaction and that perhaps the perceptron could still
    be a useful tool if applied in a different way. It took roughly a decade and a
    half, but this point of view eventually bore fruit when researchers combined perceptrons
    into larger structures and showed how to train them (Rumelhart, Hinton, and Williams
    1986). These combinations easily surpassed the limitations of any single unit.
    A series of papers then showed that careful arrangements of multiple perceptrons,
    beefed up with a few minor changes, could solve complex and interesting problems.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管普遍的解释认为感知机的书籍已经为感知机的研究关上了大门，但事实上，它只显示了到那时为止感知机使用的局限性。有些人认为放弃整个想法是过度反应，或许通过不同的方式应用感知机仍然可以是一种有用的工具。这一观点最终在十几年后结出了果实，当时研究人员将感知机组合成更大的结构，并展示了如何训练它们（Rumelhart,
    Hinton, 和 Williams 1986）。这些组合轻松超越了任何单一单元的局限性。随后一系列论文显示，通过精心安排多个感知机，并增加一些小的改动，可以解决复杂且有趣的问题。
- en: This discovery rekindled interest in the field, and soon research with perceptrons
    became a hot topic once again, producing a steady stream of interesting results
    that have led to the deep learning systems we use today. Perceptrons remain a
    core component of many modern deep learning systems.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这一发现重新激发了人们对该领域的兴趣，很快感知机的研究再次成为热门话题，产生了一系列有趣的成果，这些成果最终发展成我们今天使用的深度学习系统。感知机仍然是许多现代深度学习系统的核心组成部分。
- en: Modern Artificial Neurons
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 现代人工神经元
- en: 'The neurons we use in modern neural networks are only slightly generalized
    from the original perceptrons. There are two changes: one at the input, and one
    at the output. These modified structures are still sometimes called perceptrons,
    but there’s rarely any confusion because the new versions are used almost exclusively.
    More commonly, they’re just called *neurons*. Let’s look at these two changes.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在现代神经网络中使用的神经元仅在原始感知机的基础上稍作了推广。这里有两个变化：一个发生在输入端，另一个发生在输出端。这些修改后的结构仍然有时被称为感知机，但通常不会产生混淆，因为新的版本几乎专门使用。更常见的是，它们被称为*神经元*。让我们来看看这两个变化。
- en: The first change to the perceptron of [Figure 13-2](#figure13-2) is to provide
    each neuron with one more input, which we call the *bias*. This is a number that
    doesn’t come from the output of a previous neuron. Instead, it’s a number that’s
    directly added into the sum of all the weighted inputs. Every neuron has its own
    bias. [Figure 13-3](#figure13-3) shows our original perceptron, but with the bias
    term included.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对[图13-2](#figure13-2)中感知机的第一个改动是为每个神经元提供一个额外的输入，我们称之为*偏置*。这是一个不来自前一个神经元输出的数值。相反，它是直接加到所有加权输入总和中的数值。每个神经元都有自己的偏置。[图13-3](#figure13-3)展示了我们原始的感知机，但包括了偏置项。
- en: '![F13003](Images/F13003.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![F13003](Images/F13003.png)'
- en: 'Figure 13-3: The perceptron of [Figure 13-2](#figure13-2), but now with a bias
    term'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图13-3：[图13-2](#figure13-2)中的感知器，但现在加入了偏置项。
- en: Our second change to the perceptron of [Figure 13-2](#figure13-2) is at the
    output. The perceptron in that figure tests the sum against a threshold of 0,
    and then produces either a −1 or 1 (or 0 or 1). We generalize this by replacing
    the testing step with a mathematical function that takes the sum (including the
    bias) as input and returns a new floating-point value as output. Because the output
    of a real neuron is called its *activation*, we call this function that calculates
    the artificial neuron’s output the *activation function*. The little test shown
    in [Figure 13-2](#figure13-2) is an activation function, but one that’s rarely
    used anymore. Later in this chapter we’ll survey a variety of activation functions
    that have proved to be popular and useful in practice.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对[图13-2](#figure13-2)中的感知器进行了第二次修改，修改发生在输出部分。该图中的感知器将总和与阈值0进行比较，然后输出−1或1（或0或1）。我们通过用一个数学函数替换测试步骤来进行概括，该函数将总和（包括偏置）作为输入，并返回一个新的浮动点值作为输出。由于真实神经元的输出被称为其*激活*，因此我们将这个计算人工神经元输出的函数称为*激活函数*。在[图13-2](#figure13-2)中显示的小测试是一个激活函数，但现在已经很少使用了。稍后在本章中，我们将回顾一些在实践中证明既受欢迎又实用的激活函数。
- en: Drawing the Neurons
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 绘制神经元
- en: Let’s identify a convention that’s used by most drawings of artificial neurons.
    In [Figure 13-3](#figure13-3) we showed the weights explicitly, and we also included
    the multiplication steps to show how the weights multiply the inputs. This takes
    a lot of room on the page. When we draw diagrams with a lot of neurons, all of
    these details can make for a cluttered and dense figure. So instead, in virtually
    all neural network diagrams, the weights and their multiplications are implied.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个大多数人工神经元图示所使用的约定。在[图13-3](#figure13-3)中，我们显式地显示了权重，并且还包括了乘法步骤，以展示权重是如何乘以输入的。这在页面上占据了大量空间。当我们绘制包含大量神经元的图时，所有这些细节会使得图形显得拥挤和密集。因此，在几乎所有的神经网络图示中，权重及其乘法步骤都是隐含的。
- en: 'This is important, and bears repeating: in neural network diagrams, the weights,
    and the steps where they multiply the inputs, are not drawn. Instead, we’re supposed
    to know that they are there and mentally include them in the diagram. If we show
    the weights at all, we typically label the lines from the inputs with the name
    of the weight. [Figure 13-4](#figure13-4) shows [Figure 13-3](#figure13-3) drawn
    in this style.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这一点非常重要，需要重复强调：在神经网络图示中，权重及其乘法步骤不会被绘制出来。相反，我们应该知道它们存在并在心里将其包含在图示中。如果我们显示权重，通常会标注从输入到权重的线。[图13-4](#figure13-4)展示了[图13-3](#figure13-3)这种风格的绘制方式。
- en: '![F13004](Images/F13004.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![F13004](Images/F13004.png)'
- en: 'Figure 13-4: A neuron is often drawn with the weights on the arrows.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图13-4：神经元通常会在箭头上标出权重。
- en: In [Figure 13-4](#figure13-4), we also changed the threshold test at the end
    to a little picture. This is a drawing of a function called a *step*, and it’s
    meant to give us a visual reminder that any activation function can go into that
    spot. Basically, a number goes into that step, and a new number comes out, determined
    by whichever function we choose for the job.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图13-4](#figure13-4)中，我们还将末尾的阈值测试改为一个小图形。这是一个名为*阶跃*的函数的图示，目的是给我们一个视觉提示，任何激活函数都可以放在这个位置。基本上，一个数字进入这个阶跃，新的数字输出，具体由我们为此任务选择的函数决定。
- en: We usually simplify things again. This time we omit the bias by pretending it’s
    one of the inputs. This not only makes the diagram simpler, but it makes the math
    simpler as well, which, in this case, also leads to more efficient algorithms.
    This simplification is called the *bias trick* (the word *trick* comes from mathematics,
    where it’s a complimentary term sometimes used for a clever simplification of
    a problem). Rather than change the value of the bias, we set the bias to always
    be 1, and change the weight applied to it before it gets summed up with the other
    inputs. [Figure 13-5](#figure13-5) shows this change in labeling. Though the bias
    term is always 1 and only its weight can change, we usually ignore the distinction
    and just talk about the value of the bias.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常会再次简化问题。这一次，我们通过假装偏置是其中一个输入来省略它。这不仅使得图示更简洁，同时也让数学更简单，而在这种情况下，也能导致更高效的算法。这个简化方法被称为*偏置技巧*（"技巧"这个词来源于数学，通常用于形容一些巧妙的简化问题的方法）。我们并不改变偏置的值，而是将偏置的值固定为
    1，并在与其他输入求和之前，改变应用到它的权重。[图 13-5](#figure13-5)展示了这一标签的变化。虽然偏置项的值始终为 1，且只有它的权重会发生变化，但我们通常忽略这一区分，仅讨论偏置的值。
- en: '![F13005](Images/F13005.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![F13005](Images/F13005.png)'
- en: 'Figure 13-5: The bias trick in action. Rather than show the bias term explicitly,
    as in [Figure 13-4](#figure13-4), we pretend it’s another input with its own weight.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13-5：偏置技巧的实际应用。与[图 13-4](#figure13-4)中显式显示偏置项不同，我们假装偏置是另一个输入，并为其赋予自己的权重。
- en: 'We want our artificial neuron diagrams to be as simple as possible because
    when we start building up networks we’ll be showing lots of neurons at once, so
    most of these diagrams take two additional steps of simplification. First, they
    don’t show the bias at all. We’re supposed to remember that the bias is included
    (along with its weight), but it’s not shown. Second, the weights are often omitted
    as well, as in [Figure 13-6](#figure13-6). This is unfortunate, because the weights
    are the most important part of the neuron for us. The reason for this is that
    they are the only things we can change during training. Despite being left out
    of most drawings, they’re so essential that we repeat the key idea yet again:
    *even though we don’t show the weights explicitly, the weights are always there.*'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望我们的人工神经元图示尽可能简洁，因为当我们开始构建网络时，我们会同时展示大量的神经元，因此大多数图示都会进行两个额外的简化步骤。首先，它们完全不显示偏置。我们应该记得偏置是包含在内的（以及它的权重），但它不会显示出来。其次，权重通常也会被省略，就像在[图
    13-6](#figure13-6)中那样。这有点遗憾，因为权重是我们在神经元中最重要的部分。之所以如此，是因为它们是我们在训练过程中唯一能够改变的部分。尽管在大多数图示中没有显示出来，但它们非常关键，我们再次强调这一核心观点：*即使我们没有显式地显示权重，权重始终存在。*
- en: '![F13006](Images/F13006.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![F13006](Images/F13006.png)'
- en: 'Figure 13-6: A typical drawing of an artificial neuron. The bias term and the
    weights are not shown, but they are definitely present.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13-6：典型的人工神经元图示。虽然偏置项和权重未显示，但它们显然是存在的。
- en: Like real neurons, artificial neurons can be wired up in networks, where each
    input comes from the output of another neuron. When we connect neurons together
    into networks, we draw “wires” to connect one neuron’s output to one or more other
    neurons’ inputs. [Figure 13-7](#figure13-7) shows this idea visually.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 像真实神经元一样，人工神经元可以被连接成网络，其中每个输入来自另一个神经元的输出。当我们将神经元连接成网络时，我们画上“连接线”将一个神经元的输出连接到一个或多个其他神经元的输入。[图
    13-7](#figure13-7)直观地展示了这一概念。
- en: '![F13007](Images/F13007.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![F13007](Images/F13007.png)'
- en: 'Figure 13-7: A piece of a larger network of artificial neurons. Each neuron
    receives its inputs from other neurons. The dashed lines show connections to and
    from outside this little cluster.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13-7：一个更大人工神经元网络的一部分。每个神经元的输入来自其他神经元的输出。虚线表示与该小群体之外的连接。
- en: This is a *neural network*. Usually the goal of a network like [Figure 13-7](#figure13-7)
    is to produce one or more values as outputs. We’ll see later how we can interpret
    the numbers at the outputs in meaningful ways.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是*神经网络*。通常，像[图 13-7](#figure13-7)这样的网络的目标是产生一个或多个输出值。稍后我们将看到如何以有意义的方式解读输出的数字。
- en: Even though we’ve said that we usually don’t draw the weights, in discussions,
    sometimes it’s useful to refer to individual weights. Let’s look at a common convention
    for weight names. [Figure 13-8](#figure13-8) shows six neurons. For convenience,
    we’ve labeled each neuron with a letter. Each weight corresponds to how the output
    of one specific neuron is changed on its way to another specific neuron. Each
    of these connections is shown as a line in the figure. To name a weight, we combine
    the name of the output neuron with the input neuron. For example, the weight that
    multiplies the output of A before it’s used by D is called AD.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们已经说过通常不绘制权重，但在讨论中，有时提到单个权重是有用的。让我们来看一下一个常见的权重命名约定。[图13-8](#figure13-8)显示了六个神经元。为了方便，我们用字母标记了每个神经元。每个权重对应的是一个特定神经元的输出在传递到另一个神经元时的变化。图中每一条连接线表示这种联系。为了命名一个权重，我们将输出神经元的名字和输入神经元的名字组合起来。例如，乘以A的输出并由D使用的权重叫做AD。
- en: '![F13008](Images/F13008.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![F13008](Images/F13008.png)'
- en: 'Figure 13-8: The weights are named by combining the names of the output and
    input neurons.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图13-8：权重是通过组合输出神经元和输入神经元的名称来命名的。
- en: From a structural point of view, it makes no difference whether we draw the
    weights inside each neuron, or on the wires that carry values to it. Various authors
    assume one or the other if it makes their discussion easier to follow, but we
    can always take the other viewpoint if we like.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 从结构的角度来看，无论我们将权重绘制在每个神经元内部，还是绘制在传递值到神经元的电线之上，其实并没有区别。如果为了便于讨论，某些作者可能会选择其中一种方式，但如果需要，我们也可以选择另一种视角。
- en: In [Figure 13-8](#figure13-8), we named the weight from neuron A to neuron D
    as AD. Some authors flip this around and write DA, because it’s a more direct
    match to how we often write the equations. It’s always worth a moment to check
    which order is being used in diagrams like this.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图13-8](#figure13-8)中，我们将从神经元A到神经元D的权重命名为AD。有些作者会将其反过来写成DA，因为这种方式更直接地对应我们通常写方程的方式。在查看类似的图示时，花一点时间确认使用的顺序总是值得的。
- en: Feed-Forward Networks
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前馈网络
- en: '[Figure 13-7](#figure13-7) showed a neural network with no apparent structure.
    A key feature of deep learningis that we arrange our neurons into *layers*. Typically,
    the neurons on each layer get their inputs only from the previous layer and send
    their outputs only to the following layer, and neurons do not communicate with
    other neurons on the same layer (there are, as always, exceptions to these rules).'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[图13-7](#figure13-7)展示了一个没有明显结构的神经网络。深度学习的一个关键特点是我们将神经元排列成*层*。通常，每一层的神经元仅从上一层获取输入，并且只将输出传递给下一层，且神经元之间不与同一层的其他神经元进行通信（当然，像往常一样，这些规则也有例外）。'
- en: This organization allows us to process data in stages, with each layer of neurons
    building on the work done by the previous stage. By analogy, consider an office
    tower of many floors. The people on any given floor receive their work only from
    the people on the floor immediately below them, and they pass their work on only
    to the people on the floor immediately above them. In this analogy, each floor
    is a layer, and the people are the neurons on that layer.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这种组织方式允许我们分阶段地处理数据，每一层的神经元都基于前一阶段完成的工作来进行处理。通过类比，可以考虑一栋有多层的办公楼。任何一层的人只会从楼下那一层的人那里获得工作，并且只将工作交给楼上那一层的人。在这个类比中，每一层就是一个层级，楼层上的人就是该层的神经元。
- en: We say that this type of arrangement processes the data *hierarchically*. There
    is some evidence that the human brain is structured to handle some tasks hierarchically,
    including the processing of sensory data like vision and hearing (Meunier et al.
    2009; Serre 2014). But here again, the connection between our computer models
    and real biology is much closer to inspiration than emulation.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们说这种类型的结构处理数据是*分层的*。有一些证据表明，人类大脑在处理某些任务时是分层组织的，包括处理感官数据如视觉和听觉（Meunier 等，2009；Serre，2014）。但在这里，我们的计算机模型和真实生物学的关系更多的是灵感上的借鉴，而不是模仿。
- en: It’s amazing that hooking up neurons in a series of layers produces anything
    useful. As we saw earlier, a single artificial neuron can hardly manage to do
    anything. It takes a bunch of numerical inputs, weights them, adds the results
    together, and then passes that result through a little function. This process
    can identify a straight line that splits a couple of clumps of data, and not much
    else. But if we assemble many thousands of these little units into layers and
    use some clever ideas to train them, then, working together, they’re capable of
    recognizing speech, identifying faces in photographs, and even beating humans
    at games of logic and skill.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 很难相信，将神经元连接成一系列层次能够产生有用的东西。正如我们之前看到的，一个单一的人工神经元几乎做不了什么。它接受一堆数字输入，对其加权，求和，然后通过一个小函数传递结果。这个过程能够识别出一条将几堆数据分开的直线，仅此而已。但如果我们将成千上万的这些小单元组合成层，并使用一些巧妙的想法来训练它们，那么它们在一起工作时就能识别语音、识别人脸，甚至在逻辑和技能游戏中击败人类。
- en: The key to this is organization. Over time people have developed a number of
    ways to organize layers of neurons, resulting in a collection of common layer
    structures. The most common network structure arranges the neurons so that information
    flows in only one direction. We call this a *feed-forward network* because the
    data is flowing forward, with earlier neurons feeding, or delivering values to,
    later neurons. The art of designing a deep learning system lies in choosing the
    right sequence of layers, and the right hyperparameters, to create the basic architecture.
    To build a useful architecture for any given application, we need to understand
    how the neurons relate to one another. Let’s now look at how collections of neurons
    communicate, and how to set up the initial weights before learning begins.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 关键在于组织。随着时间的推移，人们开发了多种方式来组织神经元的层级，形成了常见的层次结构。最常见的网络结构将神经元排列成仅允许信息朝一个方向流动的方式。我们称这种结构为*前馈网络*，因为数据是向前流动的，较早的神经元将值传递给较晚的神经元。设计深度学习系统的艺术在于选择合适的层级顺序和超参数，以构建基本架构。为了为任何给定的应用构建一个有用的架构，我们需要理解神经元之间是如何相互关联的。接下来，让我们看看神经元集合是如何进行通信的，以及如何在学习开始之前设置初始权重。
- en: Neural Network Graphs
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络图
- en: We usually represent neural networks as *graphs*. The study of graphs is so
    large that it is considered a field of mathematics in its own right, called *graph
    theory* (Trudeau 1994). Here, we’re going to stick to the basic ideas of graphs,
    because that’s all we need to organize our neural networks. Though we know we’ll
    usually be working with layers, let’s start out with some general graphs first,
    such as those shown in [Figure 13-9](#figure13-9).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常将神经网络表示为*图*。图的研究非常广泛，以至于它被认为是一个独立的数学领域，叫做*图论*（Trudeau 1994）。在这里，我们将坚持图的基本概念，因为这就是我们组织神经网络所需的全部内容。尽管我们知道通常会处理层级，但让我们先从一些一般性的图开始，例如在[图13-9](#figure13-9)中所示的那些。
- en: A graph is made up of *nodes* (also called *vertices* or *elements*), here shown
    as circles. In this book, nodes are usually neurons, and throughout this book,
    we occasionally refer to one or more neurons in a network like this as nodes.
    The nodes are connected by arrows called *edges* (also called *arcs*, *wires*,
    or simply *lines*). The arrowhead is often left off when the direction of information
    flow is consistent in the drawing, which is almost always left to right or bottom
    to top. Information flows along the edges, carrying the output of one node to
    the inputs of others. Since information flows in only one direction on each edge,
    we sometimes call this kind of graph a *directed graph.*
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一个图由*节点*（也叫*顶点*或*元素*）组成，这里用圆圈表示。在本书中，节点通常是神经元，并且在整本书中，我们偶尔会将像这样的网络中的一个或多个神经元称为节点。节点之间通过箭头连接，箭头被称为*边*（也叫*弧*、*电缆*，或者简单地叫*线*）。当图中的信息流方向一致时，箭头的箭头头通常省略，这通常是从左到右或从下到上的方向。信息沿着边缘流动，将一个节点的输出传递给其他节点的输入。由于每条边上的信息流动仅有一个方向，我们有时将这种图称为*有向图*。
- en: The general idea is that we start things off by putting data into the input
    node or nodes, and then it flows through the edges, visiting nodes where it is
    transformed or changed, until it reaches the output node or nodes. No data ever
    returns to a node once it has left. In other words, information only flows forward,
    and there are no loops, or *cycles*. This kind of graph is like a little factory.
    Raw materials come in one end, and pass through machines that manipulate and combine
    them, ultimately producing one or more finished products at the end.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，我们是通过将数据放入输入节点或节点群开始的，然后它通过边缘流动，访问那些在其中被转换或改变的节点，直到它到达输出节点或节点群。一旦数据离开一个节点，就再也不会返回到那个节点。换句话说，信息只会向前流动，并且没有环路或*循环*。这种图形就像一个小工厂。原材料从一端进入，经过机器的处理和组合，最终在另一端产生一个或多个成品。
- en: '![F13009](Images/F13009.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![F13009](Images/F13009.png)'
- en: 'Figure 13-9: Two neural networks drawn as graphs. Data flows from node to node
    along the edges, following the arrows. When the edges are not labeled with an
    arrow, data usually flows left-to-right or bottom-to-top. (a) Mostly left-to-right
    flow. (b) Mostly bottom-to-top flow.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图13-9：两个作为图形绘制的神经网络。数据沿着边缘从节点流向节点，跟随箭头。当边缘没有标注箭头时，数据通常是从左到右或从下到上流动的。（a）主要是从左到右的流动。（b）主要是从下到上的流动。
- en: We say that a node near the inputs in [Figure 13-9](#figure13-9)(a) is *before*
    a node nearer to the outputs, which comes *after* it. In [Figure 13-9](#figure13-9)(b),
    we’d say a node near the inputs is *below* a node near the outputs, which is *above*
    it. Sometimes this below/above language is used even when the graph is drawn left
    to right, which can be confusing. It can help to think of *below* as “closer to
    the inputs,” and *above* as “closer to the outputs.”
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们说在[图13-9](#figure13-9)(a)中，靠近输入的节点是*在*靠近输出的节点之前的，后者在它之后。在[图13-9](#figure13-9)(b)中，我们会说，靠近输入的节点是*在*靠近输出的节点*下方*，后者*在*它的上方。有时即使图形是从左到右绘制的，这种下/上的说法也会使用，这可能会造成困惑。可以将*下方*理解为“离输入更近”，*上方*理解为“离输出更近”。
- en: We also sometimes say that if data flows from one node to another (let’s say
    it flows from A to B), then node A is an *ancestor* or *parent* of B, and node
    B is a *descendant* or *child* of A.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有时也会说，如果数据从一个节点流向另一个节点（假设它从A流向B），那么节点A是B的*祖先*或*父节点*，而节点B是A的*后代*或*子节点*。
- en: A common rule in neural networks is that there are no loops. This means that
    data coming out of a node can never make its way back into that same node, no
    matter how circuitous a path it follows. The formal name for this kind of graph
    is a *directed acyclic graph* (or *DAG*, pronounced to rhyme with “drag”). The
    word *directed* here means that the edges have arrows (which may only be implied,
    as we mentioned earlier). The word *acyclic* means there are no *cycles*, or loops.
    As always, there are exceptions to the rules, but they’re rare. We’ll see one
    such exception when we discuss recurrent neural networks (RNNs) in Chapter 19.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中的一个常见规则是没有环路。这意味着从一个节点出来的数据永远不能回到该节点，无论它走的路径有多么曲折。此类图形的正式名称是*有向无环图*（或*DAG*，发音与“drag”押韵）。这里的*有向*意味着边缘有箭头（如我们之前提到的，箭头可能只是隐含的）。*无环*意味着没有*循环*或环路。像往常一样，规则也有例外，但它们很少见。当我们在第19章讨论循环神经网络（RNNs）时，我们将看到一个这样的例外。
- en: DAGs are popular in many fields, including machine learning, because they are
    significantly easier to understand, analyze, and design than arbitrary graphs
    that have loops. Including loops can introduce *feedback*, where a node’s output
    is returned to its input. Anyone who’s moved a live microphone too close to a
    speaker is familiar with how quickly feedback can grow out of control. The acyclic
    nature of a DAG naturally avoids the feedback problem, which saves us from dealing
    with this complex issue.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: DAG在许多领域都很受欢迎，包括机器学习，因为它们比包含环路的任意图形更容易理解、分析和设计。包括环路可能会引入*反馈*，即一个节点的输出被返回到它的输入。任何将活麦克风移得太靠近扬声器的人都知道反馈如何迅速失控。DAG的无环特性自然避免了反馈问题，这让我们避免了处理这个复杂问题。
- en: Recall that a graph or network in which data only flows forward from inputs
    to outputs is called *feed-forward*. In Chapter 14, we’ll see that a key step
    in training neural networks involves temporarily flipping the arrows around, sending
    a particular type of information from the output nodes back to the input nodes.
    Although the normal flow of data is still feed-forward, when we push data through
    it backward, generally we call that a *feed-backward*, *backward-flow*, or *reverse-feed*
    algorithm. We reserve the word *feedback* for situations in which a loop in the
    graph can enable a node to receive its own output as input. As we’ve said, we
    generally avoid feedback in neural networks.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，数据仅从输入流向输出的图或网络被称为 *前馈*。在第14章中，我们将看到训练神经网络的关键步骤之一是暂时反转箭头，将一种特定类型的信息从输出节点传回输入节点。尽管数据的正常流动仍然是前馈的，但当我们将数据反向推送时，通常我们称其为
    *反馈*、*反向流动* 或 *反向前馈* 算法。我们将“反馈”这个词保留给图中的环路，环路可以使一个节点将其自身的输出作为输入。正如我们所说的，我们通常避免在神经网络中使用反馈。
- en: Interpreting graphs like those in [Figure 13-9](#figure13-9) usually means picturing
    the information as it flows along the edges, from one node to the next. But this
    picture only makes sense if we make some conventional assumptions. Let’s look
    at those now.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 解读像[图 13-9](#figure13-9)中的图表通常意味着想象信息沿着边缘流动，从一个节点流向下一个节点。但这个图像只有在我们做出一些常规假设的前提下才有意义。现在让我们来看看这些假设。
- en: 'Though we often use the word *flow* in various forms when referring to how
    data moves through the graph, this isn’t like the flow of water through pipes.
    Water flowing through pipes is a *continuous* process: new molecules of water
    flow through the pipes at every moment. The graphs we work with (and the neural
    networks they represent) are *discrete*: information arrives one chunk at a time,
    like text messages.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在描述数据如何在图中流动时经常使用“流动”这个词的各种形式，但这与水流通过管道不同。水流通过管道是一个 *连续* 的过程：水分子在每一时刻都会通过管道流动。我们所使用的图（以及它们代表的神经网络）是
    *离散* 的：信息是逐块到达的，就像文本消息一样。
- en: Recall from [Figure 13-5](#figure13-5) that we can draw a neural network by
    placing a weight on each edge (rather than inside a neuron). We call this style
    of the network a *weighted graph.* As we saw in [Figure 13-6](#figure13-6), we
    rarely draw the weights explicitly, but they are implied. It is always the case
    that in any neural network graph, even if no weights are explicitly shown, we
    are to understand that a unique weight is on each edge and as a value moves from
    one neuron to another along that edge that value is multiplied by the weight.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下[图 13-5](#figure13-5)，我们可以通过在每条边上放置一个权重（而不是在神经元内部）来绘制神经网络。我们将这种风格的网络称为 *加权图*。正如我们在[图
    13-6](#figure13-6)中看到的，我们很少显式地绘制权重，但它们是隐含的。无论如何，在任何神经网络图中，即使没有显式显示权重，我们也应该理解，每条边上都有一个唯一的权重，并且当一个值沿着这条边从一个神经元传到另一个神经元时，该值会被权重乘以。
- en: Initializing the Weights
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 初始化权重
- en: Teaching a neural network involves gradually improving the weights. The process
    begins when we assign initial values to the weights. How should we pick these
    starting values? It turns out that, in practice, how we initialize the weights
    can have a big effect on how quickly our network learns.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 教授神经网络涉及逐步改进权重。这个过程从我们为权重分配初始值开始。我们应该如何选择这些起始值？事实证明，在实践中，初始化权重的方式可能会对我们网络学习的速度产生很大的影响。
- en: Researchers have developed theories for good ways to pick the initial values
    for the weights, and the various algorithms that have proved most useful are each
    named after the lead authors on the publications that describe them. The *LeCun
    Uniform*, *Glorot Uniform* (or *Xavier Uniform*), and *He Uniform* algorithms
    are all based on selecting initial values from a uniform distribution (LeCun et
    al. 1998; Glorot and Bengio 2010; He et al. 2015). It probably won’t be much of
    a surprise that the similarly named *LeCun Normal*, *Glorot Normal* (or *Xavier
    Normal*), and *He Normal* initialization methods draw their values from a normal
    distribution.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员已经开发出了一些理论，提出了选择权重初始值的有效方法，且已被证明最有用的各种算法都以描述这些算法的论文中主要作者的名字命名。*LeCun Uniform*、*Glorot
    Uniform*（或 *Xavier Uniform*）和 *He Uniform* 算法都是基于从均匀分布中选择初始值（LeCun 等，1998；Glorot
    和 Bengio，2010；He 等，2015）。可能并不令人惊讶的是，命名相似的 *LeCun Normal*、*Glorot Normal*（或 *Xavier
    Normal*）和 *He Normal* 初始化方法则是从正态分布中选择它们的值。
- en: We don’t need to get into the math behind these algorithms. Happily, modern
    deep learning libraries offer each of these schemes, plus variations on them.
    Often the technique used by the library by default works great, so we rarely need
    to explicitly choose how to initialize the weights.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要深入了解这些算法背后的数学原理。幸运的是，现代深度学习库提供了这些方案及其变种。通常，库默认使用的技术已经很好地工作，因此我们很少需要显式地选择如何初始化权重。
- en: Deep Networks
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度网络
- en: Of the many possible ways to organize neurons in a network, placing them in
    a series of layers has proven to be both flexible and extremely powerful. Typically,
    neurons within a layer aren’t connected to one another. Their inputs come from
    the previous layer, and their outputs go to the next layer.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在组织神经元的多种方式中，将它们放置在一系列层中已被证明既灵活又极具威力。通常，一个层内的神经元彼此之间不直接连接。它们的输入来自前一层，输出则传递给下一层。
- en: 'In fact, the phrase *deep learning* comes from this structure. If we imagine
    many layers drawn side by side, we might call the network “wide.” If they were
    drawn vertically and we stood at the bottom looking up, we might call it “tall.”
    If we stood at the top and looked down, we might call it “deep.” And that’s all
    that *deep learning* means: a network made of a series of layers that we often
    draw vertically.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，*深度学习*这个术语正来源于这种结构。如果我们将许多层并排绘制，我们可能会称这个网络为“宽”。如果它们是垂直排列的，而我们站在底部向上看，我们可能会称之为“高”。如果我们站在顶部往下看，我们可能会称之为“深”。这就是*深度学习*的全部含义：一个由一系列层构成的网络，我们通常会将其垂直绘制。
- en: A result of organizing neurons in layers is that we can analyze data hierarchically.
    The early layers process the raw input data, and each subsequent layer is able
    to use information from neurons on the previous layer to process larger chunks
    of data. For example, when considering a photograph, the first layer usually looks
    at the individual pixels. The next layer looks at groups of pixels, the one after
    that looks at groups of those groups, and so on. Early layers might notice that
    some pixels are darker than others, whereas later layers might notice that a clump
    of pixels looks like an eye, and a much later layer might notice the collection
    of shapes that reveal that the whole image shows a tiger.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 将神经元按层组织的结果是，我们可以分层次地分析数据。早期的层处理原始输入数据，而每个后续的层能够利用前一层神经元的信息来处理更大块的数据。例如，考虑一张照片，第一层通常查看单个像素。下一层查看像素的组合，再下一层查看那些组合的组合，依此类推。早期的层可能会注意到某些像素比其他像素更暗，而后来的层可能会注意到一簇像素像一个眼睛，甚至更远的层可能会注意到形状的集合，从而揭示整个图像展示了一只老虎。
- en: '[Figure 13-10](#figure13-10) shows an example of a deep learning architecture
    using three layers.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 13-10](#figure13-10)展示了一个使用三层的深度学习架构示例。'
- en: '![F13010](Images/F13010.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![F13010](Images/F13010.png)'
- en: 'Figure 13-10: A deep learning network'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13-10：深度学习网络
- en: When we draw the layers vertically, as in [Figure 13-10](#figure13-10), the
    inputs are almost always drawn at the bottom, and the outputs where we collect
    our results are almost always drawn at the top.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将层垂直绘制时，如同在[图 13-10](#figure13-10)中那样，输入几乎总是绘制在底部，而我们收集结果的输出则几乎总是绘制在顶部。
- en: In [Figure 13-10](#figure13-10), all three layers contain neurons. In practical
    systems, we usually use lots of other kinds of layers, which we might group together
    as *support layers*. We’ll see many such layers in later chapters. When we count
    the number of layers in a network, we usually don’t count these support layers.
    [Figure 13-10](#figure13-10) would be described as a deep network of three layers.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 13-10](#figure13-10)中，所有三个层都包含神经元。在实际系统中，我们通常会使用许多其他类型的层，通常将这些层归类为*支撑层*。在后续章节中，我们将看到许多这样的层。当我们计算一个网络中的层数时，通常不会计算这些支撑层。[图
    13-10](#figure13-10)会被描述为一个由三层构成的深度网络。
- en: The topmost layer that contains neurons (Layer 3 in [Figure 13-10](#figure13-10))
    is called the *output layer*.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 最顶层包含神经元的层（在[图 13-10](#figure13-10)中的第3层）被称为*输出层*。
- en: We would probably expect that Layer 1 in [Figure 13-10](#figure13-10) would
    be called the *input layer*, but it’s not. In a quirk of terminology, the term
    *input layer* is applied to the bottom of the network, labeled “Inputs” in [Figure
    13-10](#figure13-10). There’s no processing in this “layer.” Instead it’s just
    the memory where the input values reside. The input layer is an example of a support
    layer because it has no neurons, and therefore isn’t included when we count the
    layers in a network. The number of layers we count is called the network’s *depth*.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能会认为[图13-10](#figure13-10)中的第1层应该被称为*输入层*，但事实并非如此。在术语上的一个小巧妙用法中，*输入层*是指网络的底部，即[图13-10](#figure13-10)中标为“输入”的部分。在这个“层”中并没有进行任何处理。它只是存放输入值的内存。输入层是一个支持层的例子，因为它没有神经元，因此在计算网络层数时不会被包括在内。我们计算的层数被称为网络的*深度*。
- en: If we imagine standing above the top of [Figure 13-10](#figure13-10) and looking
    down, we only see the output layer. If we imagine we are below the bottom and
    looking up, we only see the input layer. The layers in between aren’t visible
    to us. Each of these layers between the input and output is called a *hidden layer*.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想象站在[图13-10](#figure13-10)的顶部往下看，我们只能看到输出层。如果我们想象站在底部往上看，我们只能看到输入层。介于两者之间的层对我们来说是不可见的。每一层输入和输出之间的层被称为*隐藏层*。
- en: Sometimes the stack is drawn left to right, as in [Figure 13-11](#figure13-11).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 有时堆叠图是从左到右绘制的，如[图13-11](#figure13-11)所示。
- en: '![F13011](Images/F13011.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![F13011](Images/F13011.png)'
- en: 'Figure 13-11: The same deep network of [Figure 13-10](#figure13-10), but drawn
    with data flowing left to right'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图13-11：[图13-10](#figure13-10)的相同深度网络，但数据从左到右流动。
- en: Even when drawn this way, we still use terms that refer to the vertical orientation.
    Authors might say that Layer 2 is “above” Layer 1, and “below” Layer 3\. We can
    always keep things straight regardless of how the diagram is drawn if we think
    of “above” or “higher” as referring to a layer closer to the outputs, and “below”
    or “lower” as meaning closer to the inputs.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 即使以这种方式绘制，我们仍然使用指代垂直方向的术语。作者可能会说第二层“在”第一层“上方”，而且“在”第三层“下方”。无论图示如何绘制，只要我们将“上方”或“更高”理解为接近输出层的层，将“下方”或“更低”理解为接近输入层的层，我们总能保持清晰。
- en: Fully Connected Layers
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 全连接层
- en: A *fully connected layer* (also called an *FC*, *linear*, or *dense* layer)
    is a set of neurons that each receive an input from *every* neuron on the previous
    layer. For example, if there are three neurons in a dense layer, and four neurons
    in the preceding layer, then each neuron in the dense layer has four inputs, one
    from each neuron in the preceding layer, for a total of 3 × 4 = 12 connections,
    each with an associated weight.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*全连接层*（也称为*FC*、*线性层*或*密集层*）是一组神经元，每个神经元都接收来自上一层*每个*神经元的输入。例如，如果在一个密集层中有三个神经元，而前一层有四个神经元，那么密集层中的每个神经元都有四个输入，每个来自前一层的神经元，总共有3
    × 4 = 12个连接，每个连接都有一个相关的权重。'
- en: '[Figure 13-12](#figure13-12)(a) shows a diagram of a fully connected layer
    with three neurons, coming after a layer with four neurons.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[图13-12](#figure13-12)(a)展示了一个有三个神经元的全连接层图示，它位于一个有四个神经元的层之后。'
- en: '![F13012](Images/F13012.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![F13012](Images/F13012.png)'
- en: 'Figure 13-12: A fully connected layer. (a) The colored neurons make up a fully
    connected layer. Each of the neurons in this layer receives an input from every
    neuron in the previous layer. (b) Our schematic symbol for a fully connected layer.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图13-12：全连接层。(a) 彩色神经元组成了全连接层。这一层中的每个神经元都接收来自前一层每个神经元的输入。(b) 我们为全连接层设计的简化符号。
- en: '[Figure 13-12](#figure13-12)(b) shows a schematic shorthand that we’ll use
    for fully connected layers. The idea is that two neurons are at the top and bottom
    of the symbol, and the vertical and diagonal lines are the four connections between
    them. Next to the symbol, we identify how many neurons are in the layer, as we’ve
    done here with the number 3\. When it’s relevant, this is also where we identify
    that layer’s activation function. If a layer is made up of only dense layers,
    it is sometimes called a *fully connected network*, or, in a throwback to earlier
    terminology, a *multilayer perceptron* *(MLP)*.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[图13-12](#figure13-12)(b)展示了我们将用于全连接层的简化符号。这个符号的思想是，两个神经元位于符号的顶部和底部，垂直和对角线表示它们之间的四个连接。在符号旁边，我们标识出该层中有多少个神经元，就像这里的数字3一样。当它与层的激活函数相关时，我们也会在这里标识。如果一个层只由密集层组成，它有时被称为*全连接网络*，或者回溯到早期的术语，称为*多层感知机*（*MLP*）。'
- en: In later chapters, we’ll see many other types of layers that help us organize
    our neurons in useful ways. For example, *convolution layers* and *pooling layers*
    have proven very useful for image processing tasks, and we’ll give them a lot
    of attention.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在后面的章节中，我们将看到许多其他类型的层，它们帮助我们以有用的方式组织神经元。例如，*卷积层*和*池化层*已被证明对于图像处理任务非常有用，我们将给予它们很多关注。
- en: Tensors
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 张量
- en: We’ve seen that a deep learning system is built from a sequence of layers. And
    though the output of any neuron is a single number, we often want to talk about
    the output of an entire layer at once. The key idea that characterizes this collection
    of output numbers is its shape. Let’s see what that means.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，深度学习系统是由一系列层构成的。尽管任何神经元的输出是一个单一的数字，但我们通常想要一次性讨论整个层的输出。表征这一组输出数字的关键概念是其形状。让我们看看这意味着什么。
- en: If the layer contains a single neuron, the layer’s output is just a single number.
    We might describe this as an array, or a list, with one element. Mathematically,
    we can call this a *zero-dimensional array*. The number of dimensions in an array
    tells us how many indices we need to use to identify an element. Since a single
    number needs no indices, that array has zero dimensions.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如果某一层包含一个神经元，那么该层的输出就是一个单一的数字。我们可以将其描述为一个包含一个元素的数组或列表。从数学上讲，我们可以称其为*零维数组*。数组的维度数量告诉我们需要多少个索引来识别一个元素。由于单个数字不需要索引，因此该数组是零维的。
- en: If we have multiple neurons in a layer, then we can describe their collective
    output as a list of all the values. Since we need one index to identify a particular
    output value in this list, this is a one-dimensional (1D) array. [Figure 13-13](#figure13-13)(a)
    shows such an array containing 12 elements.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果该层有多个神经元，那么我们可以将它们的集合输出描述为一个包含所有值的列表。由于我们需要一个索引来识别该列表中特定的输出值，因此这是一个一维（1D）数组。[图13-13](#figure13-13)(a)展示了包含12个元素的这种数组。
- en: '![F13013](Images/F13013.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![F13013](Images/F13013.png)'
- en: 'Figure 13-13: Three tensors, each with 12 elements. (a) A 1D tensor is a list.
    (b) A 2D tensor is a grid. (c) A 3D tensor is a volume. In all cases, and in higher-dimensional
    cases as well, there are no holes and no elements stick out from the block.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图13-13：三个张量，每个包含12个元素。(a) 一维张量是一个列表。(b) 二维张量是一个网格。(c) 三维张量是一个体积。在所有情况下，以及更高维度的情况中，都没有空洞，元素也没有突出于块之外。
- en: We frequently organize our data into other box-like shapes. For instance, if
    the input to our system is a black and white image, it can be represented as a
    2D array, as in [Figure 13-13](#figure13-13)(b), indexed by x and y positions.
    If it’s a color image, then it can be represented as a 3D array, indexed by x
    position, y position, and color channel. A 3D shape is shown in [Figure 13-13](#figure13-13)(c).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常将数据组织成其他类似盒子的形状。例如，如果系统的输入是黑白图像，它可以表示为二维数组，如[图13-13](#figure13-13)(b)所示，按x和y位置进行索引。如果是彩色图像，则可以表示为三维数组，按x位置、y位置和颜色通道进行索引。[图13-13](#figure13-13)(c)展示了一个三维形状。
- en: 'We frequently call a 1D shape an *array*, *list*, or *vector*. To describe
    a 2D shape we often use the terms *grid* or *matrix*, and we can describe a 3D
    shape as a *volume* or *block*. We will often use arrays with even more dimensions.
    Rather than create a mountain of new terms, we use a single term for any collection
    of numbers arranged in a box shape with any number of dimensions: a *tensor* (pronounced
    ten′-sir).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常将一维形状称为*数组*、*列表*或*向量*。要描述二维形状，我们通常使用*网格*或*矩阵*这两个术语，而我们可以将三维形状描述为*体积*或*块*。我们经常使用更高维的数组。为了避免创建大量新的术语，我们使用一个术语来表示任何以盒子形状排列的、具有任意维度的数字集合：*张量*（发音：ten′-sir）。
- en: A tensor is merely a block of numbers with a given number of dimensions and
    a size in each dimension. It has no holes and no bits sticking out. The term *tensor*
    has a more complex meaning in some fields of math and physics, but in machine
    learning, we use this word to mean a collection of numbers organized into a multidimensional
    block. Taken together, the number of dimensions and the size in each dimension
    provide the *shape* of the tensor.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 张量只是一个具有给定维度和每个维度大小的数字块。它没有空洞，也没有部分突出。*张量*这个术语在某些数学和物理学领域有更复杂的含义，但在机器学习中，我们用这个词来表示一个组织成多维块的数字集合。综合来看，维度的数量和每个维度的大小提供了张量的*形状*。
- en: We often refer to a network’s *input tensor* (meaning all the input values),
    and its *output tensor* (meaning all the output values). The outputs of internal
    (or hidden) layers have no special name, so we usually say something like “the
    tensor produced by layer 3” to refer to the multidimensional array of numbers
    coming out of the neurons on layer 3.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常称网络的*输入张量*（指所有输入值）和*输出张量*（指所有输出值）。内部（或隐藏）层的输出没有特定名称，因此我们通常会说类似“第 3 层产生的张量”来指代从第
    3 层神经元输出的多维数组。
- en: Preventing Network Collapse
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 防止网络坍缩
- en: Earlier we promised to return to activation functions. Let’s look at them now.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 之前我们承诺要回到激活函数的问题。现在让我们来看一下它们。
- en: Each activation function, while a small piece of the overall structure, is critical
    to a successful neural network. Without activation functions, the neurons in a
    network combine, or *collapse*, into the equivalent of a single neuron. And, as
    we saw earlier, one neuron has very little computational power.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 每个激活函数，尽管是整体结构中的一个小部分，但对于成功的神经网络至关重要。如果没有激活函数，网络中的神经元会结合或*坍缩*成一个等效的单一神经元。正如我们之前看到的，一个神经元的计算能力非常有限。
- en: Let’s see how a network collapses when it doesn’t have activation functions.
    [Figure 13-14](#figure13-14) shows a little network with two inputs (A and B),
    and five neurons (E through G) on three layers. Every neuron receives an input
    from every neuron on the previous layer, and each connection has a weight, giving
    us a total of ten weights, shown in red.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下没有激活函数的网络是如何坍缩的。[图13-14](#figure13-14)展示了一个包含两个输入（A 和 B）和五个神经元（E 到 G）的简单网络，共三层。每个神经元接收来自前一层每个神经元的输入，每个连接都有一个权重，总共有十个权重，如红色所示。
- en: '![F13014](Images/F13014.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![F13014](Images/F13014.png)'
- en: 'Figure 13-14: A little network of two inputs, five neurons, and ten weights'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图13-14：一个包含两个输入、五个神经元和十个权重的小网络
- en: Let’s suppose for the moment that these neurons don’t have activation functions.
    Then we can write the output of each neuron as a weighted sum of its inputs, as
    in [Figure 13-15](#figure13-15). In this figure, we’re using the mathematical
    convention of leaving out the multiplication sign when possible, so 2A is shorthand
    for 2 × A.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 假设暂时这些神经元没有激活函数。那么我们可以将每个神经元的输出写成其输入的加权和，如[图13-15](#figure13-15)所示。在这个图中，我们采用了数学惯例，在可能的情况下省略乘号，因此
    2A 代表 2 × A。
- en: '![F13015](Images/F13015.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![F13015](Images/F13015.png)'
- en: 'Figure 13-15: Each neuron is labeled with the value of its output.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图13-15：每个神经元都标出了其输出的值。
- en: The outputs of C and D depend only on A and B. Similarly, the outputs of E and
    F only depend on the outputs of C and D, which means that they, too, ultimately
    depend only on A and B. The same argument holds for G. If we start with the expression
    for G, plug in the values for E and F, and then plug in the values for C and D,
    we get a big expression in terms of A and B. If we do that and simplify, we find
    that the output of G is 78A + 86B. We can write this as a single neuron with two
    new weights, as shown in [Figure 13-16](#figure13-16).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: C 和 D 的输出仅依赖于 A 和 B。同样，E 和 F 的输出仅依赖于 C 和 D 的输出，这意味着它们最终也仅依赖于 A 和 B。G 的情况也是如此。如果我们从
    G 的表达式开始，代入 E 和 F 的值，再代入 C 和 D 的值，我们得到一个关于 A 和 B 的大表达式。经过简化后，我们发现 G 的输出是 78A +
    86B。我们可以将其写成一个带有两个新权重的单一神经元，如[图13-16](#figure13-16)所示。
- en: This output of G in [Figure 13-16](#figure13-16) is exactly the same as the
    output of G in [Figure 13-14](#figure13-14). Our whole network has collapsed into
    a single neuron!
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: G 在[图13-16](#figure13-16)中的输出与 G 在[图13-14](#figure13-14)中的输出完全相同。我们的整个网络已经坍缩成了一个单一的神经元！
- en: No matter how big or complicated our neural network is, if it has no activation
    functions, then it will always be equivalent to a single neuron. This is bad news
    if we want our network to be able to do anything more than what one neuron can
    do.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们的神经网络多么庞大或复杂，如果它没有激活函数，那么它始终等同于一个单一的神经元。如果我们希望网络能做的事情超过一个神经元的能力，这可不是好消息。
- en: '![F13016](Images/F13016.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![F13016](Images/F13016.png)'
- en: 'Figure 13-16: This network’s output is exactly the same as the output in [Figure
    13-14](#figure13-14).'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图13-16：该网络的输出与[图13-14](#figure13-14)中的输出完全相同。
- en: In mathematical language, we say that our fully connected network collapsed
    because it only used addition and multiplication, which are in the category of
    *linear functions*. Linear functions can combine as we just saw, but *nonlinear
    functions* are fundamentally different and don’t combine this way. By designing
    activation functions to use *nonlinear* operations, we prevent this kind of collapse.
    We sometimes call an activation function a *nonlinearity*.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学语言中，我们说我们的全连接网络崩溃了，因为它只使用了加法和乘法，这些操作属于*线性函数*。线性函数可以像我们刚才看到的那样组合，但*非线性函数*从根本上不同，并且不会以这种方式组合。通过设计激活函数使用*非线性*操作，我们可以防止这种崩溃。我们有时将激活函数称为*非线性*。
- en: There are many different types of activation functions, each producing different
    results. Generally speaking, the variety is there because in some situations,
    some functions can run into numerical trouble, making training run more slowly
    than it should, or even cease altogether. If that happens, we can substitute an
    alternative activation function that avoids the problem (though of course it has
    its own weak points).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数有很多不同的类型，每种函数都会产生不同的结果。一般来说，这种多样性存在是因为在某些情况下，一些函数可能会遇到数值问题，使得训练速度变慢，甚至完全停止。如果发生这种情况，我们可以替换成一种避免这个问题的激活函数（当然，这种替代函数也有自己的弱点）。
- en: In practice, a handful of activation functions are all we usually use. When
    reading the literature and looking at other people’s networks, we sometimes see
    the rarer activation function. Let’s survey the functions that by most major libraries
    usually provide and then gather together the most common ones.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们通常只使用少数几种激活函数。当阅读文献并查看其他人的网络时，我们有时会看到一些较少见的激活函数。让我们先调查一下大多数主要库通常提供的函数，然后汇集最常用的函数。
- en: Activation Functions
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 激活函数
- en: An *activation function* (sometimes also called a *transfer function*, or a
    *non-linearity*) takes a floating-point number as input and returns a new floating-point
    number as output. We can define these functions by drawing them as little graphs,
    without any equations or code. The horizontal, or X, axis is the input value,
    and the vertical, or Y, axis is the output value. To find the output for any input,
    we locate the input along the X axis, and move directly upward until we hit the
    curve. That’s the output value.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '*激活函数*（有时也叫做*传递函数*，或*非线性*）接受一个浮动点数作为输入，并返回一个新的浮动点数作为输出。我们可以通过画图来定义这些函数，而不需要任何方程或代码。横坐标（X轴）是输入值，纵坐标（Y轴）是输出值。要找到任意输入的输出，我们只需要沿X轴找到输入位置，然后直接向上移动，直到碰到曲线。那就是输出值。'
- en: In theory, we can apply a different activation function to every neuron in our
    network, but in practice, we usually assign the same activation function to all
    the neurons in each layer.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，我们可以为网络中的每个神经元应用不同的激活函数，但实际上，我们通常会为每一层中的所有神经元分配相同的激活函数。
- en: Straight-Line Functions
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 直线函数
- en: Let’s first look at activation functions that are made up of one or more straight
    lines. [Figure 13-17](#figure13-17) shows a few “curves” that are just straight
    lines.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先看看由一条或多条直线组成的激活函数。[图13-17](#figure13-17)展示了一些仅由直线组成的“曲线”。
- en: Let’s look at the leftmost example in [Figure 13-17](#figure13-17). If we pick
    any point on the X axis, and go vertically up until we hit the line, the value
    of that intersection on the Y axis is the same as the value on the X axis. The
    output, or y value, of this curve is always the same as the input, or x value.
    We call this the *identity function*.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来看一下[图13-17](#figure13-17)中的最左边的例子。如果我们在X轴上选择任何一点，并垂直向上直到碰到这条线，那么该交点在Y轴上的值与X轴上的值相同。这个曲线的输出值，或y值，总是与输入值，或x值相同。我们称之为*恒等函数*。
- en: '![F13017](Images/F13017.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![F13017](Images/F13017.png)'
- en: 'Figure 13-17: Straight-line functions. The leftmost function is called the
    identity function.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图13-17：直线函数。最左边的函数叫做恒等函数。
- en: The other curves in [Figure 13-17](#figure13-17) are also straight lines, but
    they’re tilted to different slopes. We call any curve that’s just a single straight
    line a *linear function*, or even (slightly confusingly) a *linear curve*.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[图13-17](#figure13-17)中的其他曲线也是直线，但它们的斜率不同。我们称任何仅由一条直线组成的曲线为*线性函数*，或者更稍微令人困惑的是，称其为*线性曲线*。'
- en: These activation functions do not prevent network collapse. When the activation
    function is a single straight line, then mathematically, it’s only doing multiplication
    and addition, and that means it’s a linear function and the network can collapse.
    These straight-line activation functions usually appear only in two specific situations.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这些激活函数并不会阻止网络崩溃。当激活函数是一条直线时，数学上它只是做了乘法和加法，这意味着它是一个线性函数，网络就会崩溃。这些直线激活函数通常只会出现在两种特定的情况下。
- en: The first application is on a network’s output neurons. There’s no risk of collapse
    since there are no neurons after the output. The top of [Figure 13-18](#figure13-18)
    shows the idea.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个应用是在网络的输出神经元上。由于输出后没有神经元，因此不存在崩溃的风险。[图13-18](#figure13-18)的顶部展示了这个思想。
- en: '![F13018](Images/F13018.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![F13018](Images/F13018.png)'
- en: 'Figure 13-18: Using the identity as an activation function. Top: The identity
    function on an output neuron. Bottom: Using an identity function to insert a step
    of processing between the summation step and a nonlinear activation function for
    any neuron.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图13-18：使用恒等函数作为激活函数。顶部：输出神经元上的恒等函数。底部：使用恒等函数在求和步骤和非线性激活函数之间插入一个处理步骤。
- en: The second situation where we use a straight-line activation function is when
    we want to insert some processing between the summation step in a neuron and its
    activation function. In this case, we apply the identity function to the neuron,
    perform the processing step, and then perform the nonlinear activation function,
    as shown at the bottom of [Figure 13-18](#figure13-18).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 使用直线激活函数的第二种情况是，当我们想在神经元的求和步骤和其激活函数之间插入一些处理时。在这种情况下，我们对神经元应用恒等函数，执行处理步骤，然后再执行非线性激活函数，正如[图13-18](#figure13-18)底部所示。
- en: Since we generally want nonlinear activation functions, we need to get away
    from a single straight line. All of the following activation functions are nonlinear
    and prevent network collapse.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们通常希望使用非线性激活函数，因此我们需要摆脱单一的直线。以下所有的激活函数都是非线性的，并能防止网络崩溃。
- en: Step Functions
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阶梯函数
- en: We don’t want a straight line, but we can’t pick just any curve. Our curve needs
    to be single-valued. As we discussed in Chapter 5, this means that if we look
    upward from any value of x along the X axis, there’s only one value of y above
    us. An easy variation on a linear function is to start with a straight line and
    break it up into several pieces. They don’t even have to join. In the language
    of Chapter 5, this means that they don’t have to be continuous.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不想要一条直线，但也不能随便选任何曲线。我们的曲线需要是单值的。正如我们在第5章讨论的那样，这意味着如果我们从X轴上的任何一个x值向上看，那么上方只有一个y值。线性函数的一个简单变种是从一条直线开始，然后将其分成几段。它们甚至不需要连接。在第5章的语言中，这意味着它们不需要是连续的。
- en: '[Figure 13-19](#figure13-19) shows an example of this approach. We call this
    a *stair-step function*. In this example, it outputs the value 0 if the input
    is from 0 to just less than 0.2, but then the output is 0.2 if the input value
    is from 0.2 to just less than 0.4, and so on. These abrupt jumps don’t violate
    our rule that the curve has only one y output value for each input x value.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '[图13-19](#figure13-19)展示了这种方法的一个例子。我们称之为*阶梯函数*。在这个例子中，如果输入值从0到不到0.2，它的输出值为0；如果输入值从0.2到不到0.4，输出值则为0.2，以此类推。这些突变并不违反我们的规则，即每个输入x值对应一个y输出值。'
- en: '![F13019](Images/F13019.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![F13019](Images/F13019.png)'
- en: 'Figure 13-19: This curve is made up of multiple straight lines. A filled circle
    tells us that the y value there is valid, whereas an open circle tells us that
    there is no curve at that point.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图13-19：这条曲线由多条直线组成。实心圆告诉我们该点的y值是有效的，而空心圆则表示该点没有曲线。
- en: 'The simplest stair-step function has only a single step. This is a frequent
    special case, so it gets its own name: the *step function*. The original perceptron
    of [Figure 13-2](#figure13-2) used a step function as its activation function.
    A step function is usually drawn as in [Figure 13-20](#figure13-20)(a). It has
    one value until some *threshold* and then it has some other value.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的阶梯函数只有一个步骤。这是一个常见的特例，因此它有了自己的名称：*阶跃函数*。[图13-2](#figure13-2)中的原始感知器使用阶跃函数作为其激活函数。阶跃函数通常画成[图13-20](#figure13-20)(a)的样子。它在某个*阈值*之前有一个值，之后则有另一个值。
- en: Different people have different preferences for what happens when the input
    has precisely the value of the threshold. In [Figure 13-20](#figure13-20)(a) we’re
    showing that the value at the threshold is the value of the right side of the
    step, as shown by the solid dot.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的人对输入恰好等于阈值时的情况有不同的偏好。在[图 13-20](#figure13-20)(a)中，我们展示了阈值处的值是阶跃右侧的值，如实心点所示。
- en: '![F13020](Images/F13020.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![F13020](Images/F13020.png)'
- en: 'Figure 13-20: A step function has two fixed values, one each to the left and
    right of a threshold value of x.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13-20：阶跃函数有两个固定值，分别位于阈值 x 的左右两侧。
- en: Often authors are casual about what happens when the input is exactly at the
    transition, and draw the picture as in [Figure 13-20](#figure13-20)(b) in order
    to stress the “step” of the function. This is an ambiguous way to draw the curve
    because we don’t know what value is intended when the input is precisely at the
    threshold, but it’s a common kind of drawing (often we don’t care which value
    is used at the threshold, so we can choose whatever we prefer).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们通常对输入值恰好处于过渡区时的情况比较随意，并且绘制如[图 13-20](#figure13-20)(b)所示的图形，强调函数的“阶跃”性质。这是一种模糊的画法，因为我们不知道当输入恰好处于阈值时所使用的值是什么，但这是一种常见的绘图方式（通常我们不在乎阈值处使用的是哪个值，所以可以选择我们喜欢的任何值）。
- en: A couple of popular versions of the step have their own names. The *unit step*
    is 0 to the left of the threshold, and 1 to the right. [Figure 13-21](#figure13-21)
    shows this function.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 一些流行的步骤函数版本有各自的名称。*单位阶跃*在阈值左侧的值为 0，右侧的值为 1。[图 13-21](#figure13-21)展示了这个函数。
- en: If the threshold value of a unit step is 0, then we give it the more specific
    name of the *Heaviside step*, also shown in [Figure 13-21](#figure13-21).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果单位阶跃的阈值为 0，我们称之为更具体的名称 *Heaviside 步骤函数*，如[图 13-21](#figure13-21)所示。
- en: '![F13021](Images/F13021.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![F13021](Images/F13021.png)'
- en: 'Figure 13-21: Left: The unit step has a value of 0 to the left of the threshold,
    and 1 to the right. Right: The Heaviside step is a unit step where the threshold
    is 0.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13-21：左：单位阶跃函数在阈值左侧的值为 0，右侧的值为 1。右：Heaviside 步骤函数是一种单位阶跃函数，其阈值为 0。
- en: Finally, if we have a Heaviside step (so the threshold is at 0) but the value
    to the left is −1 rather than 0, we call this the *sign function*, shown in [Figure
    13-22](#figure13-22). There’s a popular variation of the sign function where input
    values that are exactly 0 are assigned an output value of 0\. Both variations
    are commonly called “the sign function,” so when the difference matters, it’s
    worth paying attention to figure out which one is being referred to.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果我们有一个 Heaviside 步骤函数（即阈值为 0），但是左侧的值是 −1 而不是 0，我们将其称为*符号函数*，如[图 13-22](#figure13-22)所示。符号函数的一个常见变体是当输入值恰好为
    0 时，输出值也为 0。两种变体通常都被称为“符号函数”，因此，当差异重要时，值得留意以确认指的是哪一种。
- en: '![F13022](Images/F13022.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![F13022](Images/F13022.png)'
- en: 'Figure 13-22: Two versions of the sign function. Left: Values less than 0 are
    assigned an output of −1, all others are 1\. Right: Like the left, except that
    an input of exactly 0 gets the value 0.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13-22：符号函数的两种版本。左：小于 0 的值输出 −1，其他值输出 1。右：与左图相同，唯一不同的是输入值恰好为 0 时，输出值为 0。
- en: Piecewise Linear Functions
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分段线性函数
- en: If a function is made up of several pieces, each of which is a straight line,
    we call it *piecewise linear*. This is still a nonlinear function as long as the
    pieces, taken together, don’t form a single straight line.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个函数由多个部分组成，每部分都是一条直线，那么我们称之为 *分段线性*。只要这些部分加在一起不形成一条直线，它仍然是一个非线性函数。
- en: Perhaps the most popular activation function is a piecewise linear function
    called a *rectifier*, or *rectified linear unit*, which is abbreviated *ReLU*
    (note that the e is lowercase). The name comes from an electronics part called
    a rectifier, which can be used to prevent negative voltages from passing from
    one part of a circuit to another (Kuphaldt 2017). When the voltage goes negative,
    the physical rectifier clamps it to 0, and our rectified linear unit does the
    same thing with the numbers that are fed into it.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 可能最流行的激活函数是一种分段线性函数，称为*整流器*，或*整流线性单元*，缩写为 *ReLU*（请注意，字母 e 是小写的）。这个名字来自于一种叫做整流器的电子元件，它可以防止负电压从电路的一部分传递到另一部分（Kuphaldt
    2017）。当电压变为负值时，物理整流器将其限制为 0，我们的整流线性单元对输入的数字执行相同的操作。
- en: The ReLU’s graph is shown in [Figure 13-23](#figure13-23). It’s made up of two
    straight lines, but thanks to the kink, or bend, this is nota linear function.
    If the input is less than 0, then the output is 0\. Otherwise, the output is the
    same as the input.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 的图形如[图 13-23](#figure13-23)所示。它由两条直线组成，但由于有了折点或弯曲，这并不是一个线性函数。如果输入小于 0，则输出为
    0。否则，输出与输入相同。
- en: '![F13023](Images/F13023.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![F13023](Images/F13023.png)'
- en: 'Figure 13-23: The ReLU, or rectified linear unit. It outputs 0 for all negative
    inputs, otherwise the output is the input.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13-23：ReLU，或称整流线性单元。它对所有负输入输出 0，其他情况输出与输入相同。
- en: The ReLU activation function is popular because it’s a simple and fast way to
    include a nonlinearity at the end of our artificial neurons. But there’s a potential
    problem. As we’ll see in Chapter 14, if changes in the input don’t lead to changes
    in the output, a network can stop learning. And the ReLU has an output of 0 for
    every negative value. If our input changes from, say, –3 to –2, then the output
    of ReLU stays at 0\. Fixing this problem has led to the development of the ReLU
    variations that follow.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 激活函数之所以受欢迎，是因为它是将非线性引入人工神经元末端的一种简单而快速的方式。但也存在潜在的问题。正如我们在第 14 章中将看到的那样，如果输入的变化没有导致输出的变化，网络就会停止学习。而且
    ReLU 对于每个负值的输出都是 0。如果我们的输入从比如 –3 变化到 –2，那么 ReLU 的输出仍然是 0。解决这个问题促使了后续 ReLU 变体的开发。
- en: Despite this issue, ReLU (or leaky ReLU, which we’ll see next) often performs
    well in practice, and people often use it as their default choice when building
    a new network, particularly for fully connected layers. Beyond the fact that these
    activation functions work well in practice, there are good mathematical reasons
    for wanting to use ReLU (Limmer and Stanczak 2017), though we won’t explore them
    here.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这个问题，ReLU（或者接下来我们会看到的泄漏 ReLU）在实践中通常表现良好，而且人们在构建新网络时通常将其作为默认选择，特别是在全连接层中。除了这些激活函数在实践中表现良好的事实外，还有很好的数学理由希望使用
    ReLU（Limmer 和 Stanczak 2017），尽管我们在这里不会深入探讨。
- en: The *leaky ReLU* changes the response for negative values. Rather than output
    a 0 for any negative value, this functions outputs the input, scaled down by a
    factor of 10\. [Figure 13-24](#figure13-24) shows this function.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '*泄漏 ReLU* 改变了对负值的响应。它不会对任何负值输出 0，而是输出输入值，按 10 倍缩小。[图 13-24](#figure13-24)显示了该函数。'
- en: '![F13024](Images/F13024.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![F13024](Images/F13024.png)'
- en: 'Figure 13-24: The leaky ReLU is like the ReLU, but it returns a scaled-down
    value of x when x is negative.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13-24：泄漏 ReLU 类似于 ReLU，但当 x 为负时，它会返回一个缩小的 x 值。
- en: Of course, there’s no need to always scale down the negative values by a factor
    of 10\. A *parametric ReLU* lets us choose by how much negative amounts are scaled,
    as shown in [Figure 13-25](#figure13-25).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，没必要总是将负值缩小 10 倍。*参数化 ReLU* 允许我们选择负值缩放的比例，如[图 13-25](#figure13-25)所示。
- en: When using a parametric ReLU, the essential thing is to never select a factor
    of exactly 1.0, because then we lose the kink, the function becomes a straight
    line, and any neuron we apply this to collapses with those that immediately follow
    it.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 使用参数化 ReLU 时，关键是永远不要选择 1.0 的比例，因为那样我们会失去“折点”，函数会变成直线，应用此函数的神经元会与其后立即跟随的神经元发生崩溃。
- en: '![F13025](Images/F13025.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![F13025](Images/F13025.png)'
- en: 'Figure 13-25: A parametric ReLU is like a leaky ReLU, but the slope for values
    of x that are less than 0 can be specified.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13-25：参数化 ReLU 类似于泄漏 ReLU，但可以指定对于小于 0 的 x 值的斜率。
- en: Another variation on the basic ReLU is the *shifted ReLU*, which just moves
    the bend down and left. [Figure 13-26](#figure13-26) shows an example.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 基本 ReLU 的另一种变体是*偏移 ReLU*，它只是将弯点向下和向左移动。[图 13-26](#figure13-26)展示了一个示例。
- en: '![F13026](Images/F13026.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![F13026](Images/F13026.png)'
- en: 'Figure 13-26: The shifted ReLU moves the bend in the ReLU function down and
    left.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13-26：偏移 ReLU 将 ReLU 函数中的弯点向下和向左移动。
- en: We can generalize the various flavors of ReLU with an activation function called
    *maxout* (Goodfellow et al. 2013). Maxout allows us to define a set of lines.
    The output of the function at each point is the largest valueamong all the lines,
    evaluated at that point. [Figure 13-27](#figure13-27) shows maxout with just two
    lines, forming a ReLU, as well as two other examples that use more lines to create
    more complex shapes.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过一种叫做*maxout*的激活函数（Goodfellow 等人 2013）来概括各种 ReLU 变体。Maxout 允许我们定义一组直线。在每个点，函数的输出是所有直线在该点的最大值。[图
    13-27](#figure13-27)展示了只有两条线的 maxout，形成一个 ReLU，以及另外两个示例，它们使用更多的线条创建更复杂的形状。
- en: '![F13027](Images/F13027.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![F13027](Images/F13027.png)'
- en: 'Figure 13-27: The maxout function lets us build up a function from multiple
    straight lines. The heavy red line is the output of maxout for each set of lines.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图13-27：maxout函数让我们能够通过多条直线构建函数。重的红线是maxout在每一组直线上的输出。
- en: Another variation on the basic ReLU is to add a small random value to the input
    before running it through a standard ReLU. This function is called a *noisy ReLU*.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 基本ReLU的另一种变体是在输入前加上一个小的随机值，然后再通过标准的ReLU进行处理。这个函数叫做*noisy ReLU*。
- en: Smooth Functions
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 平滑函数
- en: As we’ll see in Chapter 14, a key step in teaching neural networks involves
    computing derivatives for the outputs of neurons, which necessarily involve their
    activation functions.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将在第14章看到的，训练神经网络的一个关键步骤是计算神经元输出的导数，而这必然涉及到它们的激活函数。
- en: The activation functions that we saw in the last section (except for the linear
    functions) create their nonlinearities by using multiple straight lines with at
    least one kink in the collection. Mathematically, there is no derivative at the
    kink between a pair of straight lines, and therefore the function is not linear.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一节看到的激活函数（除了线性函数）通过使用多条直线来创造它们的非线性特征，这些直线中至少有一条存在拐角。数学上，在一对直线之间的拐角处没有导数，因此该函数不是线性的。
- en: If these kinks prevent the computation of derivatives, which are necessary for
    teaching a network, why are functions like ReLU useful at all, let alone so popular?
    It turns out that standard mathematical tools can finesse the sharp corners like
    those in ReLU and still produce a derivative (Oppenheim and Nawab 1996). These
    tricks don’t work on all functions, but one of the principles that guided the
    development of the functions we saw earlier is that they allow these methods to
    be used.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这些拐角妨碍了导数的计算，而导数是训练神经网络所必需的，那么像ReLU这样的函数为什么还会有用，甚至如此流行呢？事实证明，标准的数学工具可以巧妙地处理像ReLU中的尖角这样的部分，并且仍然能够计算出导数（Oppenheim
    和 Nawab 1996）。这些技巧并不是对所有函数都适用，但我们之前看到的函数的开发原则之一就是它们允许使用这些方法。
- en: An alternative to using multiple straight lines and then patching up the problems
    is to use smooth functions that inherently have a derivative everywhere. That
    is, they’re smooth everywhere. Let’s look at a few popular and smooth activation
    functions.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多条直线然后修补问题的替代方案是使用天生在每个地方都有导数的平滑函数。也就是说，它们在所有地方都是平滑的。让我们来看几个流行的平滑激活函数。
- en: The *softplus* function simply smooths out the ReLU, as shown in [Figure 13-28](#figure13-28).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '*softplus*函数仅仅是对ReLU进行了平滑处理，如[图13-28](#figure13-28)所示。'
- en: '![F13028](Images/F13028.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![F13028](Images/F13028.png)'
- en: 'Figure 13-28: The softplus function is a smoothed version of the ReLU.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图13-28：softplus函数是ReLU的平滑版本。
- en: We can smooth out the shifted ReLU as well. This is called the *exponential
    ReLU*, or *ELU* (Clevert, Unterthiner, and Hochreiter 2016). It’s shown in [Figure
    13-29](#figure13-29).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以平滑化偏移的ReLU。这被称为*指数型ReLU*，或*ELU*（Clevert, Unterthiner, 和 Hochreiter 2016）。它在[图13-29](#figure13-29)中展示。
- en: '![F13029](Images/F13029.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![F13029](Images/F13029.png)'
- en: 'Figure 13-29: The exponential ReLU, or ELU'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图13-29：指数型ReLU，或ELU
- en: Another way to smooth out the ReLU is called *swish* (Ramachandran, Zoph, and
    Le 2017). [Figure 13-30](#figure13-30) shows what this looks like. In essence
    it’s a ReLU, but with a small, smooth bump just left of 0, which then flattens
    out.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 平滑ReLU的另一种方法叫做*swish*（Ramachandran, Zoph, 和 Le 2017）。[图13-30](#figure13-30)展示了它的样子。实际上，它是一个ReLU，但在0点左侧有一个小的平滑波动，然后逐渐变平。
- en: '![F13030](Images/F13030.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![F13030](Images/F13030.png)'
- en: 'Figure 13-30: The swish activation function'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图13-30：swish激活函数
- en: Another popular smooth activation function is the *sigmoid*, also called the
    *logistic function* or *logistic curve*. This is a smoothed-out version of the
    Heaviside step. The name *sigmoid* comes from the resemblance of the curve to
    an S shape, while the other names refer to its mathematical interpretation. [Figure
    13-31](#figure13-31) shows this function.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种流行的平滑激活函数是*sigmoid*，也称为*logistic函数*或*logistic曲线*。这是Heaviside阶跃函数的平滑版本。*sigmoid*这个名字来源于该曲线与S形状的相似，而其他名称则指代其数学解释。[图13-31](#figure13-31)展示了这个函数。
- en: Closely related to the sigmoid is another mathematical function called the *hyperbolic
    tangent*. It’s much like the sigmoid, only negative values are sent to –1 rather
    than to 0\. The name comes from the curve’s origins in trigonometry. It’s a big
    name, so it’s usually written simply as *tanh*. This is shown in [Figure 13-32](#figure13-32).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 与Sigmoid密切相关的另一个数学函数叫做*双曲正切*。它与Sigmoid非常相似，唯一的区别是负值被映射到−1，而不是0。这个名字来自于它在三角学中的曲线起源。因为它名字较长，所以通常简写为*tanh*。这在[图13-32](#figure13-32)中展示了出来。
- en: We say that the sigmoid and tanh functions both *squash* their entire input
    range from negative to positive infinity into a small range of output values.
    The sigmoid squashes all inputs to the range [0, 1], while tanh squashes them
    to [−1, 1].
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们说Sigmoid和tanh函数都会将它们的整个输入范围从负无穷到正无穷压缩到一个小范围的输出值。Sigmoid将所有输入压缩到[0, 1]范围，而tanh将它们压缩到[−1,
    1]。
- en: '![F13031](Images/F13031.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![F13031](Images/F13031.png)'
- en: 'Figure 13-31: The S-shaped sigmoid function is also called the logistic function
    or logistic curve. It has a value of 0 for very negative inputs, and a value of
    1 for very positive inputs. For inputs in the range of about −6 to 6, it smoothly
    transitions between the two.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图13-31：S型Sigmoid函数也叫做逻辑函数或逻辑曲线。对于非常负的输入，它的值为0，对于非常正的输入，它的值为1。对于大约在−6到6之间的输入，它平滑地在两者之间过渡。
- en: '![F13032](Images/F13032.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![F13032](Images/F13032.png)'
- en: 'Figure 13-32: The hyperbolic tangent function, written tanh, is S-shaped like
    the sigmoid of [Figure 13-31](#figure13-31). The key differences are that it returns
    a value of −1 for very negative inputs, and the transition zone is a bit narrower.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图13-32：双曲正切函数，写作tanh，像[图13-31](#figure13-31)中的Sigmoid一样呈S形。主要的区别在于，它对非常负的输入返回−1，并且过渡区略微狭窄。
- en: The two are shown on top of one another in [Figure 13-33](#figure13-33).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这两者在[图13-33](#figure13-33)中并排显示。
- en: '![F13033](Images/F13033.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![F13033](Images/F13033.png)'
- en: 'Figure 13-33: The sigmoid function (orange) and tanh function (teal), both
    plotted for the range −8 to 8'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图13-33：Sigmoid函数（橙色）和tanh函数（青色），两者在范围−8到8之间的图像。
- en: Another smooth activation function uses a sine wave, as shown in [Figure 13-34](#figure13-34)
    (Sitzmann 2020). This squashes the outputs to the range [–1, 1] like tanh, but
    it doesn’t saturate (or stop changing) for inputs that are far from 0.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个平滑的激活函数使用正弦波，如[图13-34](#figure13-34)所示（Sitzmann 2020）。它像tanh一样将输出压缩到[–1,
    1]范围，但对于远离0的输入，它不会饱和（或停止变化）。
- en: '![F13034n](Images/F13034n.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![F13034n](Images/F13034n.png)'
- en: 'Figure 13-34: A sine wave activation function'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图13-34：正弦波激活函数
- en: Activation Function Gallery
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 激活函数画廊
- en: '[Figure 13-35](#figure13-35) summarizes the activation functions we’ve discussed.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[图13-35](#figure13-35)总结了我们讨论过的激活函数。'
- en: '![F13035n](Images/F13035n.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![F13035n](Images/F13035n.png)'
- en: 'Figure 13-35: A gallery of popular activation functions'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图13-35：流行激活函数的画廊
- en: Comparing Activation Functions
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 比较激活函数
- en: 'ReLU used to be the most popular activation function, but in recent years,
    the leaky ReLU has been gaining in popularity. This is a result of practice: networks
    with leaky ReLU often learn faster.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU曾是最流行的激活函数，但近年来，leaky ReLU的受欢迎程度逐渐上升。这是实践的结果：使用leaky ReLU的网络通常学习得更快。
- en: The reason is that ReLU has a problem, which we mentioned earlier. When a ReLU’s
    input is negative, its output is 0\. If the input is a large negative number,
    then changing it by a small amount still results in a negative input to ReLU and
    an unchanged output of 0\. This means that the derivative is also zero. As we’ll
    see in Chapter 14, when a neuron’s derivative goes to zero, not only does it stop
    learning, but it also makes it more likely that the neurons that precede it in
    the network will stop learning as well. Because a neuron whose output never changes
    no longer participates in learning, we sometimes use rather drastic language and
    say that the neuron has *died*. The leaky ReLU has been gaining in popularity
    over ReLU because, by providing an output that isn’t the same for every negative
    input, its derivative is not 0, and thus it does not die. The sine wave function
    also has a non-zero derivative almost everywhere (except at the very top and bottom
    of each wave).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 原因在于ReLU存在一个问题，我们之前提到过。当ReLU的输入为负时，它的输出为0。如果输入是一个大的负数，那么即使改变它一个很小的量，输入仍然是负数，ReLU的输出仍然是0。这意味着导数也是0。正如我们将在第14章看到的，当一个神经元的导数为零时，不仅它自己停止学习，而且它前面的神经元也更有可能停止学习。因为一个输出永远不变化的神经元不再参与学习，所以我们有时会使用比较极端的说法，称这个神经元*死亡*。leaky
    ReLU因其输出对于每一个负输入都不相同，导数不为0，因此不会“死亡”，而且越来越受到欢迎。正弦波函数几乎在每个地方都有非零的导数（除了每个波的顶部和底部）。
- en: After ReLU and leaky ReLU, sigmoid and tanh are probably the next most popular
    functions. Their appeal is that they’re smooth, and the outputs are bounded to
    [0, 1] or [–1, 1]. Experience has shown that networks learn most efficiently when
    all the values flowing through it are in a limited range.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在ReLU和leaky ReLU之后，sigmoid和tanh可能是最受欢迎的函数。它们的吸引力在于它们平滑，且输出范围被限定在[0, 1]或[–1,
    1]之间。经验表明，当网络中流动的所有值都处于有限范围时，网络的学习效率最高。
- en: There is no firm theory to tell us which activation function works best in a
    specific layer of a specific network. We usually start by making the same choices
    that have worked in other, similar networks that we’ve seen, and then we try alternatives
    if learning goes too slowly.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 没有确凿的理论可以告诉我们在特定网络的特定层中，哪种激活函数最有效。我们通常从那些在其他类似网络中有效的选择开始，然后如果学习过程过于缓慢，我们会尝试替代方案。
- en: A few rules of thumb give us a good starting point in many situations. Generally
    speaking, we often apply ReLU or leaky ReLU to most neurons on hidden layers,
    particularly fully connected layers. For regression networks, we often use no
    activation function on the final layer (or if we must supply one, we use a linear
    activation function, which amounts to the same thing), because we care about the
    specific output value. When we’re classifying with just two classes, we have just
    a single output value. Here we often apply a sigmoid to push the output clearly
    to one class or the other. For classification networks with more than two classes,
    we almost always use a somewhat different kind of activation function, which we’ll
    look at next.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 一些经验法则为我们提供了许多情况下的良好起点。一般来说，我们通常对隐藏层的大多数神经元，特别是全连接层应用ReLU或leaky ReLU。对于回归网络，我们通常对最后一层不使用激活函数（如果必须使用一个，我们使用线性激活函数，这等同于没有激活函数），因为我们关心的是特定的输出值。当我们进行二分类时，只有一个输出值。在这种情况下，我们通常会应用sigmoid函数，将输出清晰地推向某一类别。对于具有多个类别的分类网络，我们几乎总是使用一种略有不同的激活函数，我们接下来将讨论它。
- en: Softmax
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Softmax
- en: There’s an operation that we typically apply only to the output neurons of a
    classifier neural network, and even then, only if there are two or more output
    neurons. It’s not an activation function in the sense that we’ve been using the
    term because it takes as input the outputs of *all* the output neurons simultaneously.
    It processes them together and then produces a new output value for each neuron.
    Though it’s not quite an activation function, it’s close enough in spirit to activation
    functions to merit including it in this discussion.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 有一种操作我们通常只应用于分类神经网络的输出神经元，而且即便如此，只有在输出神经元有两个或更多时才使用。它不是我们通常所说的激活函数，因为它同时接受*所有*输出神经元的输出作为输入。它将这些输出一起处理，然后为每个神经元生成一个新的输出值。尽管它不完全是一个激活函数，但它在概念上与激活函数非常相似，因此值得在此讨论中提及。
- en: The technique is called *softmax*. The purpose of softmax is to turn the raw
    numbers that come out of a classification network into class probabilities.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这个技术称为*softmax*。softmax 的目的是将分类网络输出的原始数字转化为类别概率。
- en: It’s important to note that softmax takes the place of any activation function
    we’d otherwise apply to those output neurons. That is, we give them no activation
    function (or, equivalently, apply the linear function) and then run those outputs
    into softmax.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，softmax 取代了我们通常会应用于这些输出神经元的任何激活函数。也就是说，我们不给它们应用激活函数（或者等效地，应用线性函数），然后将这些输出传递给
    softmax。
- en: 'The mechanics of this process are involved with the mathematics of how the
    network computes its predictions, so we won’t go into those details here. The
    general idea is shown in [Figure 13-36](#figure13-36): *scores* come in, and *probabilities*
    come out.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 这一过程的机制涉及网络如何计算其预测的数学原理，因此我们不会在这里深入探讨这些细节。一般来说，如[图 13-36](#figure13-36)所示：*分数*输入，*概率*输出。
- en: '![F13036n](Images/F13036n.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![F13036n](Images/F13036n.png)'
- en: 'Figure 13-36: The softmax function takes all the network’s outputs and modifies
    them simultaneously. The result is that the scores are turned into probabilities.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13-36：softmax 函数同时修改所有网络输出。结果是将分数转化为概率。
- en: Each output neuron presents a value, or score, that corresponds to how much
    the network thinks the input is of that class. In [Figure 13-36](#figure13-36)
    we’re assuming that we have three classes in our data, named A, B, and C, so each
    of the three output neurons gives us a score for its class. The larger the score,
    the more certain the system is that the input belongs to that class.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 每个输出神经元呈现一个值或分数，表示网络认为输入属于该类别的程度。在[图 13-36](#figure13-36)中，我们假设数据中有三个类别，分别命名为
    A、B 和 C，因此每个输出神经元给出一个该类别的分数。分数越大，系统越确定输入属于该类别。
- en: If one class has a larger score than some other class, it means the network
    thinks that class is more likely. That’s useful. But the scores aren’t designed
    to be compared in any other convenient way. For instance, if the score for A is
    twice that of B, it doesn’t mean that A is twice as likely as B. It just means
    that A is more likely. Because making comparisons like “twice as likely” is so
    useful, we use softmax to turn the output scores into probabilities. Now, if the
    softmax output of A is twice that of B, then indeed A is twice as probable as
    B. That’s such a useful way to look at the network’s output that we almost always
    use softmax at the end of a classification network.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 如果某个类别的分数比其他类别大，意味着网络认为该类别更有可能。这是有用的。但是这些分数并没有设计成以其他方便的方式进行比较。例如，如果 A 的分数是 B
    的两倍，这并不意味着 A 的概率是 B 的两倍，它只是意味着 A 更有可能。因为像“概率是两倍”的比较非常有用，我们使用 softmax 将输出的分数转化为概率。现在，如果
    A 的 softmax 输出是 B 的两倍，那么 A 的概率确实是 B 的两倍。这种看待网络输出的方式非常有用，因此我们几乎总是在分类网络的最后使用 softmax。
- en: 'Any set of numbers that we want to treat as probabilities must satisfy two
    criteria: the values all lie between 0 and 1, and they add up to 1\. If we just
    modify each output of the network independently, we don’t know the other values,
    so we can’t make sure they added up to anything in particular. When we hand all
    the outputs to softmax, it can simultaneously adjust all the values so that they
    sum to 1\.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望作为概率处理的任何一组数字必须满足两个条件：所有值都在 0 和 1 之间，并且它们的总和为 1。如果我们只是独立地修改每个网络输出，我们无法知道其他值，因此不能确保它们的总和符合任何特定要求。当我们将所有输出传递给
    softmax 时，它可以同时调整所有值，使其总和为 1。
- en: Let’s look at softmax in action. Consider the top-left graph of [Figure 13-37](#figure13-37).
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下 softmax 的实际应用。请参见[图 13-37](#figure13-37)的左上角图表。
- en: '![F13037](Images/F13037.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![F13037](Images/F13037.png)'
- en: 'Figure 13-37: The softmax function takes all the network’s outputs and modifies
    them simultaneously. The result is that the scores are turned into probabilities.
    Top row: Scores from a classifier. Bottom row: Results of running the scores in
    the top row through softmax. Note that the graphs in the upper row use different
    vertical scales.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13-37：softmax 函数同时修改所有网络输出。结果是将分数转化为概率。上排：分类器的分数。下排：将上排分数通过 softmax 处理后的结果。请注意，上排图表使用了不同的纵坐标刻度。
- en: The top left of [Figure 13-37](#figure13-37) shows the outputs for a classifier
    with six output neurons, which we’ve labeled A through F. In this example, all
    six of these values are between 0 and 1\. From this graph, we can see that the
    value for class B is 0.1 and the value for class C is 0.8\. As we’ve discussed,
    it is a mistake to conclude from this that the input is 8 times more likely to
    be in class C than class B, because these are scores and not probabilities. We
    can say that class C is more likely than class B, but anything more requires some
    math. To usefully compare these outputs to one another, we can apply softmax to
    carry out that math, and change them into probabilities.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '[图13-37](#figure13-37)的左上方显示了一个具有六个输出神经元的分类器的输出，我们将这些输出标记为A到F。在这个示例中，这六个值都位于0和1之间。从这个图中，我们可以看到B类的值是0.1，而C类的值是0.8。如我们所讨论的那样，得出输入属于C类的可能性是B类的8倍是错误的，因为这些是得分而不是概率。我们可以说C类比B类更可能，但任何更细致的比较需要一些数学计算。为了有意义地比较这些输出，我们可以应用Softmax来进行计算，并将它们转换为概率。'
- en: We show the output of softmax in the graph in the lower left. These are the
    probabilities of the input belonging to each of the six classes. It’s interesting
    to note that the big values, like C and F, get scaled down by a lot, but the small
    values, like B, are hardly scaled at all. This is a natural result of how scores
    between 0 and 1 turn into probabilities. But the ordering of the bars by size
    is still the same as it was for the scores (with C the largest, then F, then D,
    and so on). From the probabilities produced by softmax in the lower figure, we
    can see that class C has a probability of about 0.25, and class B has a probability
    of about 0.15\. We can conclude that the input is a little more than 1.5 times
    more probable to be in class C than class B.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在左下图中展示了Softmax的输出。这些是输入属于六个类中的每一个的概率。值得注意的是，大的值，如C和F，被大幅度缩小，而小的值，如B，几乎没有缩小。这是得分在0到1之间转换为概率时的自然结果。但是，条形图的排序仍然与得分时一样（C最大，其次是F，再是D，依此类推）。从Softmax生成的概率图中，我们可以看到C类的概率大约是0.25，而B类的概率大约是0.15。我们可以得出结论，输入属于C类的可能性是B类的1.5倍多一点。
- en: The middle and right columns of [Figure 13-37](#figure13-37) show the outputs
    for two other hypothetical networks and inputs, before and after softmax. The
    three examples show that the output of softmax depends on whether the inputs are
    all less than 1\. The input ranges in [Figure 13-37](#figure13-37), reading left
    to right, are [0, 0.8], [0, 8], and [0, 3]. Softmax always preserves the ordering
    of its inputs (that is, if we sort the inputs from largest to smallest, they match
    a similar sort on the outputs). But when some input values are greater than 1,
    the largest value tends to stand out more. We say that softmax *exaggerates* the
    influence of the output with the largest value. Sometimes we also say that softmax
    *crushes* the other values, making the largest one dominate the others more obviously.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '[图13-37](#figure13-37)的中间和右侧列显示了另两个假设的网络和输入的输出，分别是在应用Softmax前后的结果。这三个示例显示了Softmax的输出取决于输入是否都小于1。[图13-37](#figure13-37)中的输入范围，从左到右依次是[0,
    0.8]、[0, 8]和[0, 3]。Softmax始终保留输入的排序（也就是说，如果我们按从大到小排序输入，输出的排序也会相似）。但是当一些输入值大于1时，最大的值往往更突出。我们说Softmax
    *放大*了具有最大值的输出的影响。有时我们也说Softmax *压缩*了其他值，使得最大值更加明显地主导了其他值。'
- en: '[Figure 13-37](#figure13-37) shows that the input range makes a big difference
    in the output of softmax. Softmax also has an interesting behavior depending on
    whether the inputs values are all less than 1, all greater than 1, or mixed.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '[图13-37](#figure13-37)显示了输入范围在Softmax输出中产生了很大差异。Softmax还表现出有趣的行为，取决于输入值是否都小于1、都大于1或混合在一起。'
- en: At the far left of [Figure 13-37](#figure13-37) all of the inputs are all less
    than 1, in the range [0, 0.8].
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图13-37](#figure13-37)的最左侧，所有的输入值都小于1，位于范围[0, 0.8]内。
- en: In the middle column, the inputs are all greater than 1, in the range [0, 8].
    Notice that in the output, the value of D (corresponding to the 8) clearly dominates
    all of the other values. Softmax has exaggerated the differences among the outputs,
    making it easier to pick out D as the largest.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在中间的列中，输入值都大于1，位于范围[0, 8]内。注意到在输出中，D的值（对应8）明显主导了其他所有值。Softmax放大了输出之间的差异，使得更容易将D选为最大值。
- en: On the far right of [Figure 13-37](#figure13-37) we have values both less and
    greater than 1, in the range [0, 3]. Here the exaggeration effect is somewhere
    between the left column, where all inputs are less than 1, and the middle column,
    where all inputs are greater than 1.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图13-37](#figure13-37)的最右端，我们有一些小于1和大于1的数值，范围为[0, 3]。这里，夸张效应介于左列（所有输入都小于1）和中间列（所有输入都大于1）之间。
- en: In all cases, though, softmax gives us back probabilities that are each between
    0 and 1, and sum up to 1\. The ordering of the inputs is always preserved, so
    the sequence of largest to smallest input is also the largest to smallest output.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在所有情况下，softmax 都会返回一个概率值，每个值都介于0和1之间，并且总和为1。输入的顺序始终被保留，因此从最大到最小输入的顺序也对应于从最大到最小的输出。
- en: Summary
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'Real, biological neurons are sophisticated nerve cells that process information
    using a fabulously complex range of chemical, electrical, and mechanical processes.
    They serve as the inspiration for a simple bit of computation that we call an
    artificial neuron, despite the enormous gulf between the computer version and
    its biological namesake. An artificial neuron multiplies each input value by a
    corresponding weight, adds the results, then passes that through an activation
    function. We can assemble artificial neurons into networks. Typically, those networks
    are DAGs: they are directed (information flows in only one direction), they are
    acyclic (no neuron ever receives its own output as an input), and they are graphs
    (the neurons are connected to one another). Input data enters at one end, and
    the network’s results appear at the other.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 真实的生物神经元是复杂的神经细胞，通过一种极其复杂的化学、电气和机械过程来处理信息。尽管计算机版本和其生物学原型之间存在巨大的差距，它们依然启发了我们创造出一种简单的计算方式——人工神经元。人工神经元将每个输入值与相应的权重相乘，求和结果，然后通过激活函数处理。我们可以将人工神经元组合成网络。通常，这些网络是有向无环图（DAG）：它们是有向的（信息仅沿一个方向流动），它们是无环的（没有神经元会将自己的输出作为输入），而且它们是图形的（神经元之间是相互连接的）。输入数据从一端进入，网络的结果出现在另一端。
- en: We saw how, if we’re not careful in constructing our network, the entire network
    can collapse into a single neuron. We prevent this by using the activation function,
    a small function that takes each neuron’s output and turns it into a new number.
    These functions are designed to be nonlinear, meaning that they cannot be described
    merely by operations such as addition and multiplication. It is this nonlinearity
    that prevents the network from being equivalent to a single neuron. We concluded
    the chapter by looking at some of the more common activation functions, and how
    softmax can turn the numbers we get from a neural network into class probabilities.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，如果在构建网络时不小心，整个网络可能会崩溃成一个单一的神经元。我们通过使用激活函数来防止这种情况，激活函数是一个小函数，它接收每个神经元的输出并将其转换为一个新的数字。这些函数是非线性的，这意味着它们不能仅通过加法和乘法等操作来描述。正是这种非线性使得网络不等同于单一神经元。我们通过查看一些更常见的激活函数，以及
    softmax 如何将神经网络输出的数字转化为类别概率，来结束本章。
- en: The only difference between untrained deep learning systems and those that have
    been trained and are ready for deployment is in the value of the weights. The
    goal of training, or learning, is to find values for the weights so that the network’s
    output is correct for as many samples as possible. Since the weights start out
    with random numbers, we need some principled way to find these new, useful values.
    In Chapters 14 and 15, we’ll see how neural networks learn by looking at the two
    key algorithms that gradually improve the starting weights, transforming a network’s
    outputs into accurate, useful results.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 未经过训练的深度学习系统与已经训练好并准备部署的系统之间唯一的区别在于权重的数值。训练或学习的目标是找到一组权重值，使得网络的输出尽可能正确地适应尽可能多的样本。由于权重一开始是随机数值，我们需要一些有原则的方法来找到这些新的、有用的数值。在第14章和第15章中，我们将通过查看两个关键算法来了解神经网络是如何学习的，这些算法逐步改进初始权重，将网络的输出转变为准确且有用的结果。
