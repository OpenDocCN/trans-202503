- en: '8'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '8'
- en: OVERLAY NETWORKS
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 覆盖网络
- en: '![image](../images/common01.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/common01.jpg)'
- en: Container networking is complex enough when all of the containers are on a single
    host, as we saw in [Chapter 4](ch04.xhtml#ch04). When we scale up to a cluster
    of nodes, all of which run containers, the complexity increases substantially.
    Not only must we provide each container with its own virtual network devices and
    manage IP addresses, dynamically creating new network namespaces and devices when
    containers are created, but we also need to ensure that containers on one node
    can communicate with containers on all the other nodes.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 当所有容器都在单个主机上时，容器网络已经足够复杂，正如我们在[第4章](ch04.xhtml#ch04)中看到的那样。当我们扩展到一个包含多个节点的集群时，所有节点都运行容器时，复杂性会大幅增加。我们不仅需要为每个容器提供自己的虚拟网络设备，并管理
    IP 地址，动态创建新的网络命名空间和设备，还需要确保一个节点上的容器能够与所有其他节点上的容器进行通信。
- en: In this chapter, we’ll describe how *overlay networks* are used to provide the
    appearance of a single container network across all nodes in a Kubernetes cluster.
    We’ll consider two different approaches for routing container traffic across a
    host network, examining the network configuration and traffic flows for each.
    Finally, we’ll explore how Kubernetes uses the Container Network Interface (CNI)
    standard to configure networking as a separate plug-in, making it easy to shift
    to new technology as it becomes available and allowing for custom solutions where
    needed.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将描述如何使用 *覆盖网络* 来提供跨 Kubernetes 集群所有节点的单一容器网络的表象。我们将考虑两种不同的方法来路由容器流量穿越主机网络，检查每种方法的网络配置和流量流向。最后，我们将探讨
    Kubernetes 如何使用容器网络接口（CNI）标准将网络配置作为一个独立的插件，使其能够轻松切换到新的技术，并在需要时允许自定义解决方案。
- en: Cluster Networking
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集群网络
- en: The fundamental goal of a Kubernetes cluster is to treat a set of hosts (physical
    or virtual machines) as a single computing resource that can be allocated as needed
    to run containers. From a networking standpoint, this means Kubernetes should
    be able to schedule a Pod onto any node without worrying about connectivity to
    Pods on other nodes. It also means that Kubernetes should have a way to dynamically
    allocate IP addresses to Pods in a way that supports that cluster-wide network
    connectivity.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 集群的基本目标是将一组主机（物理机或虚拟机）视为一个单一的计算资源，可以根据需要分配以运行容器。从网络的角度来看，这意味着 Kubernetes
    应该能够将 Pod 调度到任何节点，而不必担心与其他节点上的 Pods 的连接问题。这也意味着 Kubernetes 应该有一种方式，能够动态地为 Pods
    分配 IP 地址，以支持集群范围的网络连接性。
- en: As we’ll see in this chapter, Kubernetes uses a plug-in design to allow any
    compatible network software to allocate IP addresses and provide cross-node network
    connectivity. All plug-ins must follow a couple of important rules. First, Pod
    IP addresses should come from a single pool of IP addresses, although this pool
    can be subdivided by node. This means that we can treat all Pods as part of a
    single flat network, no matter where the Pods run. Second, traffic should be routable
    such that all Pods can see all other Pods and the control plane.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将在本章中看到的，Kubernetes 使用插件设计来允许任何兼容的网络软件分配 IP 地址并提供跨节点的网络连接性。所有插件必须遵循几个重要的规则。首先，Pod
    的 IP 地址应该来自一个单一的 IP 地址池，尽管这个池可以按节点细分。这意味着我们可以将所有 Pods 视为一个单一的平面网络，无论 Pods 运行在哪里。其次，流量应该是可路由的，以便所有
    Pods 都能看到所有其他 Pods 和控制平面。
- en: CNI Plug-ins
  id: totrans-8
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: CNI 插件
- en: Plug-ins communicate with the Kubernetes cluster, specifically with `kubelet`,
    using the CNI standard. CNI specifies how `kubelet` finds and invokes CNI plug-ins.
    When a new Pod is created, `kubelet` first allocates the network namespace. It
    then invokes the CNI plug-in, providing it a reference to the network namespace.
    The CNI plug-in adds network devices to the namespace, assigns an IP address,
    and passes that IP address back to `kubelet`.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 插件通过 CNI 标准与 Kubernetes 集群进行通信，特别是与 `kubelet` 通信。CNI 规范了 `kubelet` 如何查找和调用 CNI
    插件。当创建一个新的 Pod 时，`kubelet` 首先分配网络命名空间。然后它调用 CNI 插件，并为其提供网络命名空间的引用。CNI 插件向命名空间添加网络设备，分配
    IP 地址，并将该 IP 地址返回给 `kubelet`。
- en: 'Let’s see that process in action. To do so, our examples for this chapter include
    two different environments with two different CNI plug-ins: Calico and WeaveNet.
    Both of these plug-ins provide networking for Pods but with different cross-node
    networking. We’ll begin with the Calico environment.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个过程是如何工作的。为了做到这一点，本章的示例包括两种不同的环境和两种不同的 CNI 插件：Calico 和 WeaveNet。这两个插件都为
    Pods 提供网络连接，但在跨节点网络方面有所不同。我们将从 Calico 环境开始。
- en: '**NOTE**'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*The example repository for this book is at* [https://github.com/book-of-kubernetes/examples](https://github.com/book-of-kubernetes/examples).
    *See “Running Examples” on [page xx](ch00.xhtml#ch00lev1sec2) for details on getting
    set up.*'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*本书的示例仓库位于* [https://github.com/book-of-kubernetes/examples](https://github.com/book-of-kubernetes/examples)。
    *有关设置的详细信息，请参阅[第 xx 页](ch00.xhtml#ch00lev1sec2)中的“运行示例”。*'
- en: 'By default, CNI plug-in information is kept in */etc/cni/net.d*. We can see
    the Calico configuration in that directory:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，CNI 插件信息保存在 */etc/cni/net.d* 目录中。我们可以在该目录中查看 Calico 配置：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The file *10-calico.conflist* contains the actual Calico configuration. The
    file *calico-kubeconfig* is used by Calico components to authenticate with the
    control plane; it was created based on a service account created during Calico
    installation. The configuration filename has the *10-* prefix because `kubelet`
    sorts any configuration files it finds and uses the first one.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 文件*10-calico.conflist*包含实际的 Calico 配置。文件*calico-kubeconfig*由 Calico 组件用于与控制平面进行身份验证；它是基于在
    Calico 安装过程中创建的服务账户生成的。配置文件名前缀为*10-*，因为`kubelet`会对它找到的任何配置文件进行排序，并使用第一个文件。
- en: '[Listing 8-1](ch08.xhtml#ch08list1) shows the configuration file, which is
    in JSON format and identifies the network plug-ins to use.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 8-1](ch08.xhtml#ch08list1)显示了配置文件，该文件是 JSON 格式，指定了要使用的网络插件。'
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '*Listing 8-1: Calico configuration*'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 8-1：Calico 配置*'
- en: 'The most important field is `type`; it specifies which plug-in to run. In this
    case, we’re running three plug-ins: `calico`, which handles Pod networking; `bandwidth`,
    which we can use to configure network limits; and `portmap`, which is used to
    expose container ports to the host network. These two plug-ins inform `kubelet`
    of their purposes using the `capabilities` field; as a result, when `kubelet`
    invokes them, it passes in the relevant bandwidth and port mapping configuration
    so that the plug-in can make the necessary network configuration changes.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的字段是`type`；它指定了要运行的插件。在本例中，我们运行了三个插件：`calico`，用于处理 Pod 网络；`bandwidth`，可以用来配置网络限制；以及`portmap`，用于将容器端口暴露到主机网络。这两个插件通过`capabilities`字段告知`kubelet`它们的用途；因此，当`kubelet`调用它们时，它会传递相关的带宽和端口映射配置，以便插件可以进行必要的网络配置更改。
- en: 'To run these plug-ins, `kubelet` needs to know where they are located. The
    default location for the actual plug-in executables is */opt/cni/bin*, and the
    name of the plug-in matches the `type` field:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行这些插件，`kubelet`需要知道它们的位置。实际插件可执行文件的默认位置是 */opt/cni/bin*，插件名称与`type`字段相匹配。
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here, we see a common set of network plug-ins that were installed by `kubeadm`
    along with our Kubernetes cluster. We also see `calico`, which was added to this
    directory by the Calico DaemonSet we installed after cluster initialization.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到一组常见的网络插件，它们是由`kubeadm`与我们的 Kubernetes 集群一起安装的。我们还看到了`calico`，它是由我们在集群初始化后安装的
    Calico DaemonSet 添加到该目录中的。
- en: Pod Networking
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Pod 网络
- en: Let’s look at an example Pod to get a glimpse of how the CNI plug-ins configure
    the Pod’s network namespace. The behavior is very similar to the work we did in
    [Chapter 4](ch04.xhtml#ch04), adding virtual network devices into network namespaces
    to enable communication between containers and with the host network.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看一个示例 Pod，以便了解 CNI 插件如何配置 Pod 的网络命名空间。这个行为与我们在[第 4 章](ch04.xhtml#ch04)中做的非常相似，通过将虚拟网络设备添加到网络命名空间中，来启用容器之间以及与主机网络之间的通信。
- en: 'Let’s create a basic Pod:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个基本的 Pod：
- en: '*pod.yaml*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*pod.yaml*'
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We’ve added the extra field `nodeName` to force this Pod to run on `host01`,
    which will make it easier to find and examine how its networking is configured.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们添加了额外的字段`nodeName`，强制此 Pod 在`host01`上运行，这样更容易找到并检查其网络配置。
- en: 'We start the Pod via the usual command:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过常规命令启动 Pod：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, check to see that it’s running:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，检查它是否正在运行：
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'After it’s running, we can use `crictl` to capture its unique ID:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 它运行后，我们可以使用`crictl`捕获它的唯一 ID：
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: At this point, using the Pod ID, we can find its network namespace. In [Listing
    8-2](ch08.xhtml#ch08list2), we use `jq` to extract only the data we want, just
    as we did in [Chapter 4](ch04.xhtml#ch04). We’ll then assign it to a variable.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，使用 Pod ID，我们可以找到其网络命名空间。在[清单 8-2](ch08.xhtml#ch08list2)中，我们使用`jq`来提取我们想要的数据，就像在[第
    4 章](ch04.xhtml#ch04)中做的那样。然后我们将其赋值给一个变量。
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '*Listing 8-2: Network namespace*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 8-2：网络命名空间*'
- en: 'We now can explore the network namespace to see how Calico set up the IP address
    and network routing for this Pod. First, as expected, this network namespace is
    being used for our Pod:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以探索网络命名空间，查看Calico是如何为这个Pod设置IP地址和网络路由的。首先，正如预期的那样，这个网络命名空间是为我们的Pod使用的：
- en: '[PRE8]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We see the two processes that we should expect. The first is a pause container
    that is always created whenever we create a Pod. This is a permanent container
    to hold the network namespace. The second is our BusyBox container running `sleep`,
    as we configured in the Pod YAML file.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到预期中的两个进程。第一个是一个暂停容器，每当我们创建Pod时，它总是会被创建。这是一个永久容器，用于保持网络命名空间。第二个是我们运行`sleep`的BusyBox容器，正如我们在Pod的YAML文件中配置的那样。
- en: 'Now, let’s see the configured network interfaces:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看配置好的网络接口：
- en: '[PRE9]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Calico has created the network device `eth0@if16` in the network namespace ➊
    and given it an IP address of `172.31.239.205` ➋. Note that the network length
    for that IP address is `/32`, which indicates that any traffic must go through
    a configured router. This is different from how our bridged container networking
    worked in [Chapter 4](ch04.xhtml#ch04). It is necessary so that Calico can provide
    firewall capabilities via network policies.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Calico在网络命名空间➊中创建了网络设备`eth0@if16`，并为其分配了IP地址`172.31.239.205`➋。请注意，该IP地址的网络掩码是`/32`，这表示所有流量必须通过配置好的路由器。这与[第4章](ch04.xhtml#ch04)中桥接容器网络的工作方式不同。这样配置是必要的，以便Calico通过网络策略提供防火墙功能。
- en: 'The choice of IP address for this Pod was ultimately up to Calico. Calico is
    configured with `172.31.0.0/16` for use as the IP address space for Pods. Calico
    decides how to divide this address space up between nodes and then allocates IP
    addresses to each Pod from the range allocated to the node. Calico then passes
    this IP address back to `kubelet` so that it can update the Pod’s status:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 该Pod所选的IP地址最终是由Calico决定的。Calico的IP地址空间配置为`172.31.0.0/16`，用于Pod的IP地址分配。Calico决定如何在节点之间划分该地址空间，并从分配给节点的范围内为每个Pod分配IP地址。然后，Calico将此IP地址返回给`kubelet`，以便更新Pod的状态：
- en: '[PRE10]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: When Calico created the network interface in the Pod, it created it as part
    of a virtual Ethernet (veth) pair. The veth pair acts as a virtual network wire
    that creates a connection to a network interface in the root namespace, allowing
    connections outside the Pod. [Listing 8-3](ch08.xhtml#ch08list3) lets us have
    a look at both halves of the veth pair.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当Calico在Pod中创建网络接口时，它是作为虚拟以太网（veth）对的一部分来创建的。veth对充当一个虚拟网络线缆，创建一个到根命名空间中网络接口的连接，从而允许Pod外部的连接。[清单8-3](ch08.xhtml#ch08list3)让我们看看veth对的两个部分。
- en: '[PRE11]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '*Listing 8-3: Calico veth pair*'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单8-3：Calico veth对*'
- en: The first command prints the network interfaces inside the namespace, whereas
    the second prints the interfaces on the host. Each contains the field `link-netns`
    pointing to the corresponding network namespace of the other interface, showing
    that these two interfaces create a link between our Pod’s namespace and the root
    namespace.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个命令打印命名空间内的网络接口，而第二个命令打印主机上的接口。每个命令都包含字段`link-netns`，指向另一个接口的相应网络命名空间，显示这两个接口创建了Pod命名空间与根命名空间之间的链接。
- en: Cross-Node Networking
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 跨节点网络
- en: So far, the configuration of the virtual network devices in the container looks
    very similar to the container networking in [Chapter 4](ch04.xhtml#ch04), where
    there was no Kubernetes cluster installed. The difference in this case is that
    the network plug-in is configured not just to connect containers on a single node,
    but to connect containers running anywhere in the cluster.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，容器中虚拟网络设备的配置与[第4章](ch04.xhtml#ch04)中的容器网络非常相似，当时并未安装Kubernetes集群。区别在于，网络插件配置不仅仅是为了连接单节点上的容器，而是为了连接在集群中任何地方运行的容器。
- en: '**WHY NOT NAT?**'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么不使用NAT？**'
- en: Regular container networking does, of course, provide connectivity to the host
    network. However, as we’ve discussed, it accomplishes this using Network Address
    Translation (NAT). This is fine for containers running individual client applications,
    as connection tracking enables Linux to route server responses all the way into
    the originating container. It does not work for containers that need to act as
    servers, which is a key use case for a Kubernetes cluster.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 常规的容器网络确实提供与主机网络的连接。然而，正如我们所讨论的，它是通过网络地址转换（NAT）来实现的。这对于运行单个客户端应用程序的容器来说是可以的，因为连接跟踪使得Linux能够将服务器响应路由到原始容器中。但这对于需要充当服务器的容器就不适用了，而这正是Kubernetes集群的一个关键使用场景。
- en: For most private networks that use NAT to connect to a broader network, port
    forwarding is used to expose specific services from within the private network.
    That isn’t a good solution for every container in every Pod, as we would quickly
    run out of ports to allocate. The network plug-ins do end up using NAT, but only
    to connect containers acting as clients to make connections to networks outside
    the cluster. In addition, we will see port forwarding behavior in [Chapter 9](ch09.xhtml#ch09),
    where it will be one possible way to expose Services outside the cluster.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数使用 NAT 连接到更广泛网络的私有网络，端口转发用于从私有网络内部暴露特定服务。对于每个 Pod 中的每个容器来说，这并不是一个好的解决方案，因为我们很快就会用尽可分配的端口。网络插件最终确实使用
    NAT，但仅仅是为了将作为客户端的容器连接到集群外部的网络。此外，我们将在[第 9 章](ch09.xhtml#ch09)中看到端口转发的行为，它将是暴露服务到集群外部的可能方法之一。
- en: The challenge in cross-node networking is that the Pod network has a different
    range of IP addresses from the host network, so the host network does not know
    how to route this traffic. There are a couple of different ways that network plug-ins
    work around this. We’ll begin by continuing with our cluster running Calico. Then,
    we’ll show a different cross-node networking technology using WeaveNet.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 跨节点网络的挑战在于，Pod 网络的 IP 地址范围与主机网络不同，因此主机网络不知道如何路由这些流量。网络插件有几种不同的方法来解决这个问题。我们将继续使用运行
    Calico 的集群开始，然后展示使用 WeaveNet 的不同跨节点网络技术。
- en: Calico Networking
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Calico 网络
- en: Calico performs cross-node networking using Layer 3 routing. This means that
    it routes based on IP addresses, configuring IP routing tables on each host and
    in the Pod to ensure that traffic is sent to the correct host and then to the
    correct Pod. Thus, at the host level, we see the Pod IP addresses as the source
    and destination. Because Calico relies on the built-in routing capabilities of
    Linux, we don’t need to configure our host network switch to route the traffic,
    but we do need to configure any security controls on the host network switch to
    allow Pod IP addresses to travel across the network.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Calico 使用第 3 层路由进行跨节点网络连接。这意味着它基于 IP 地址进行路由，在每个主机和 Pod 中配置 IP 路由表，以确保流量发送到正确的主机，然后到达正确的
    Pod。因此，在主机级别，我们看到 Pod 的 IP 地址作为源地址和目标地址。由于 Calico 依赖于 Linux 的内建路由功能，我们不需要配置主机网络交换机来路由流量，但我们确实需要配置主机网络交换机上的任何安全控制，以允许
    Pod 的 IP 地址跨网络传输。
- en: 'To explore Calico cross-node networking, it helps to have two Pods: one on
    `host01` and the other on `host02`. We’ll use this resource file:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了探索 Calico 跨节点网络连接，最好有两个 Pods：一个在 `host01` 上，另一个在 `host02` 上。我们将使用这个资源文件：
- en: '*two-pods.yaml*'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '*two-pods.yaml*'
- en: '[PRE12]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As always, these files have been loaded into the */opt* directory by the automated
    scripts for this chapter.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，这些文件已经通过自动化脚本加载到本章节的*/opt*目录中。
- en: The `---` separator allows us to put two different Kubernetes resources in the
    same file so that we can manage them together. The only difference in configuration
    with these two Pods is that they each have a `nodeName` field to ensure that they
    are assigned to the correct node.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`---` 分隔符允许我们将两个不同的 Kubernetes 资源放在同一个文件中，以便我们可以一起管理它们。这两个 Pod 的唯一配置差异是它们各自有一个
    `nodeName` 字段，以确保它们被分配到正确的节点。'
- en: 'Let’s delete our existing Pod and replace it with the two that we need:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们删除现有的 Pod，并用我们需要的两个 Pod 替换它：
- en: '[PRE13]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'After these Pods are running, we’ll need to collect their IP addresses:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些 Pods 启动后，我们需要收集它们的 IP 地址：
- en: '[PRE14]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We’re able to extract the Pod IP using a simple `jq` filter because our `kubectl
    get` command is guaranteed to return only one item. If we were running `kubectl
    get` without a filter, or with a filter that might match multiple Pods, the JSON
    output would be a list and we would need to change the `jq` filter accordingly.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能够使用简单的 `jq` 过滤器提取 Pod IP，因为我们的 `kubectl get` 命令保证只返回一个项目。如果我们没有过滤器地运行 `kubectl
    get`，或者使用可能匹配多个 Pods 的过滤器，JSON 输出将是一个列表，我们需要相应地修改 `jq` 过滤器。
- en: 'Let’s quickly verify that we have connectivity between these two Pods:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速验证这两个 Pods 之间的连接性：
- en: '[PRE15]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The `ping` command shows that all three packets arrived successfully, so we
    know the Pods can communicate across nodes.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`ping` 命令显示所有三个数据包成功到达，因此我们知道 Pods 可以跨节点通信。'
- en: 'As in our earlier example, each of these Pods has a network interface with
    a network length of `/32`, meaning that all traffic must go through a router.
    For example, here is the IP configuration and route table for `pod1`:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前的示例所示，每个Pod都有一个网络接口，网络长度为`/32`，意味着所有流量必须经过路由器。例如，以下是`pod1`的IP配置和路由表：
- en: '[PRE16]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Based on this configuration, when we run our `ping` command, the networking
    stack recognizes that the destination IP is not local to any interface. It therefore
    looks up `169.254.1.1` in its Address Resolution Protocol (ARP) table to determine
    where to send the “next hop.” If we try to find an interface either in the container
    or on the host that has the address `169.254.1.1`, we won’t be successful. Rather
    than actually assign that address to an interface, Calico just configures “proxy
    ARP” so that the packet will be sent through the `eth0` end of the veth pair.
    As a result, there is an entry for `169.254.1.1` in the ARP table inside the container:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 根据此配置，当我们运行`ping`命令时，网络栈会识别目标IP不属于任何接口的本地网络。因此，它会在其地址解析协议（ARP）表中查找`169.254.1.1`以确定“下一跳”应该发送到哪里。如果我们尝试在容器或主机上找到一个具有`169.254.1.1`地址的接口，我们是无法成功的。Calico并不会实际将该地址分配给某个接口，而是配置了“代理ARP”，使得数据包通过veth对的`eth0`端发送。因此，容器内的ARP表中会有`169.254.1.1`的条目：
- en: '[PRE17]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: As shown in [Listing 8-3](ch08.xhtml#ch08list3), the hardware address `ee:ee:ee:ee:ee:ee`
    belongs to the host side of the veth pair, so this is sufficient to get the packet
    out of the container and into the root network namespace. From there, IP routing
    takes over.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如[清单 8-3](ch08.xhtml#ch08list3)所示，硬件地址`ee:ee:ee:ee:ee:ee`属于veth对的主机端，因此这足以将数据包从容器中取出并进入根网络命名空间。从那里，IP路由接管。
- en: 'Calico has already configured the routing table to send packets to other cluster
    nodes based on the destination IP address range for that node and to send packets
    to local containers based on their individual IP addresses. We can see the result
    of this in the IP routing table on the host:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Calico已经配置了路由表，根据节点的目标IP地址范围将数据包发送到其他集群节点，并根据每个容器的IP地址将数据包发送到本地容器。我们可以在主机上的IP路由表中看到这个结果：
- en: '[PRE18]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Because the destination address for the ping is within the `172.31.89.192/26`
    network, the packet now is routed to `192.168.61.12`, which is `host02`.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 由于ping的目标地址位于`172.31.89.192/26`网络中，数据包现在被路由到`192.168.61.12`，即`host02`。
- en: 'Let’s look at the routing table on `host02` so that we can follow along with
    the next step:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看`host02`上的路由表，以便跟随接下来的步骤：
- en: '[PRE19]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: If you want to run this command for yourself, make sure you run it from `host02`.
    When our packet arrives at `host02`, it has a route for the specific IP address
    that is the destination of the `ping`. This route sends the packet into the veth
    pair that is attached to the `pod2` network namespace.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想自己运行这个命令，确保从`host02`运行。当我们的数据包到达`host02`时，它已经有了一个特定目标IP地址的路由，这个路由将数据包发送到附加在`pod2`网络命名空间的veth对中。
- en: Now that the ping has arrived, the network stack inside `pod2` sends back a
    reply. The reply goes through the same process to reach the root network namespace
    of `host02`. Based on the `host02` routing table, it is sent to `host01`, where
    a routing table entry for `172.31.239.216` is used to send it to the appropriate
    container.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，ping数据包已经到达，`pod2`内的网络栈会发送回一个回复。这个回复会通过相同的过程，到达`host02`的根网络命名空间。根据`host02`的路由表，它会被发送到`host01`，并使用`172.31.239.216`的路由表条目将数据包发送到适当的容器。
- en: Because Calico is using Layer 3 routing, the host network sees the actual container
    IP addresses. We can confirm that using `tcpdump`. We’ll switch back to `host01`
    for this.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Calico使用的是第3层路由，主机网络可以看到实际的容器IP地址。我们可以使用`tcpdump`来确认这一点。为此，我们将切换回`host01`。
- en: 'First, let’s kick off `tcpdump` in the background:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们在后台启动`tcpdump`：
- en: '[PRE20]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The `-n` flag tells `tcpdump` to avoid trying to lookup hostnames in DNS for
    any IP addresses; this saves time. The `-w pings.pcap` flag tells `tcpdump` to
    write its data to the file *pings.pcap*; the `-i any` flag tells it to listen
    on all network interfaces; the `icmp` filter tells it to listen only to ICMP traffic;
    and finally, `&` at the end puts it in the background.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`-n`标志告诉`tcpdump`避免查找任何IP地址的主机名，这样可以节省时间。`-w pings.pcap`标志告诉`tcpdump`将数据写入文件*pings.pcap*；`-i
    any`标志告诉它监听所有网络接口；`icmp`过滤器告诉它仅监听ICMP流量；最后，`&`放在命令末尾表示将其放入后台。'
- en: The *pcap* filename extension is important because our Ubuntu host system will
    only allow `tcpdump` to read files with that extension.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*pcap* 文件扩展名非常重要，因为我们的 Ubuntu 主机系统只允许 `tcpdump` 读取具有该扩展名的文件。'
- en: 'Now, let’s run `ping` again:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们再次运行 `ping`：
- en: '[PRE21]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The ICMP requests and replies have been collected, but they are being buffered
    in memory.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ICMP 请求和回复已被收集，但它们在内存中被缓冲。
- en: 'To get them dumped to the file, we’ll shut down `tcpdump`:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将它们转储到文件中，我们将关闭 `tcpdump`：
- en: '[PRE22]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'There were three pings, and each ping consists of a request and a reply. Thus,
    we might have expected six packets, but in fact we captured 12\. To see why, let’s
    print the details of the packets that `tcpdump` collected:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 有三个 ping，每个 ping 包括一个请求和一个回复。因此，我们可能期望有六个数据包，但事实上我们捕获了 12 个。为了理解原因，让我们打印出 `tcpdump`
    收集到的数据包的详细信息：
- en: '[PRE23]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The `-e` flag to `tcpdump` prints the hardware addresses; otherwise, we wouldn’t
    be able to tell some of the packets apart. The first hardware address ➊ is the
    hardware address of `eth0` inside the Pod. Next is the same packet again, but
    this time the hardware address is the host interface ➋. We then see the reply,
    first arriving at the host interface and labeled with the hardware address for
    `host02` ➌. Finally, the packet is routed into the Calico network interface corresponding
    to our Pod ➍, and our `ping` has made its round trip.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '`tcpdump` 的 `-e` 标志打印硬件地址；否则，我们无法区分某些数据包。第一个硬件地址 ➊ 是 Pod 内部 `eth0` 的硬件地址。接下来是相同的数据包，但这次硬件地址是主机接口
    ➋。然后我们看到回复，首先到达主机接口，并带有 `host02` 的硬件地址 ➌。最后，数据包被路由到对应我们 Pod 的 Calico 网络接口 ➍，我们的
    `ping` 已经完成了往返。'
- en: 'We’re now done with these two Pods, so let’s delete them:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在完成了这两个 Pod，让我们删除它们：
- en: '[PRE24]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Using Layer 3 routing is an elegant solution to cross-node networking for a
    Kubernetes cluster, as it takes advantage of the routing and traffic forwarding
    capabilities that are native to Linux. However, it does mean that the host network
    sees the Pods’ IP addresses, which may require security rule changes. For example,
    the automated scripts that set up virtual machines in Amazon Web Services (AWS)
    for use with this book not only configure a security group to allow all traffic
    in the Pod IP address space, but they also turn off the “source/destination check”
    for the virtual machine instances. Otherwise, the underlying AWS network infrastructure
    would refuse to pass traffic with unexpected IP addresses to our cluster’s nodes.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Kubernetes 集群来说，使用第三层路由是一个优雅的跨节点网络解决方案，因为它利用了 Linux 原生的路由和流量转发能力。然而，这意味着主机网络能看到
    Pod 的 IP 地址，这可能需要安全规则的更改。例如，为了配合本书，在亚马逊网络服务（AWS）中自动设置虚拟机时，不仅配置了一个安全组以允许 Pod IP
    地址空间内的所有流量，还关闭了虚拟机实例的“源/目标检查”。否则，底层 AWS 网络基础设施将拒绝传递具有意外 IP 地址的流量到我们集群的节点。
- en: WeaveNet
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: WeaveNet
- en: Layer 3 routing is not the only solution for cross-node networking. Another
    option is to “encapsulate” the container packets into a packet that is sent explicitly
    host to host. This is the approach taken by popular network plug-ins such as Flannel
    and WeaveNet. We’ll look at a WeaveNet example, but the traffic using Flannel
    looks very similar.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 第三层路由并不是跨节点网络的唯一解决方案。另一种选择是将容器数据包“封装”到明确从主机到主机发送的数据包中。这是流行的网络插件（如 Flannel 和
    WeaveNet）采取的方法。我们将看一个 WeaveNet 的例子，但使用 Flannel 的流量看起来非常相似。
- en: '**NOTE**'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*Larger clusters based on Calico also use encapsulation for some traffic between
    networks. For example, a cluster that spans multiple regions, or Availability
    Zones, in AWS would likely need to configure Calico to use encapsulation, given
    that it may not be possible or practical to configure all of the routers between
    the regions or Availability Zones with the necessary Pod IP routes for the cluster.*'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 Calico 的较大集群也会使用封装技术来处理某些网络之间的流量。例如，在 AWS 中跨多个区域或可用区的集群可能需要配置 Calico 来使用封装，因为可能无法或不方便为跨区域或可用区的所有路由器配置必要的
    Pod IP 路由。
- en: 'Because everything you might want to do in networking has some defined standard,
    it’s not surprising that there is a standard for encapsulation: Virtual Extensible
    LAN (VXLAN). In VXLAN, each packet is wrapped in a UDP datagram and sent to the
    destination.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 因为在网络中可能会有一些定义的标准，所以有封装的标准也并不奇怪：虚拟可扩展局域网（VXLAN）。在 VXLAN 中，每个数据包都被包装在一个 UDP 数据报中并发送到目的地。
- en: 'We’ll use the same *two-pods.yaml* configuration file to create two Pods in
    our Kubernetes cluster, this time using a cluster built from the *weavenet* directory
    from this chapter’s examples. As before, we end up with one Pod on `host01` and
    the other on `host02`:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用相同的*two-pods.yaml*配置文件，在我们的Kubernetes集群中创建两个Pod，这次使用的是本章示例中*weavenet*目录构建的集群。如同之前一样，我们最终会有一个Pod在`host01`，另一个Pod在`host02`：
- en: '[PRE25]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Let’s check that these Pods are running and allocated correctly to their different
    hosts:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下这些Pod是否正在运行，并且正确分配到它们各自的主机：
- en: '[PRE26]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'After these Pods are running, we can collect their IP addresses using the same
    commands shown earlier:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些Pod运行后，我们可以使用之前显示的相同命令来收集它们的IP地址：
- en: '[PRE27]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Note that the IP addresses assigned look nothing like the Calico example. Further
    exploration shows that the address and routing configuration is also different,
    as demonstrated in [Listing 8-4](ch08.xhtml#ch08list4).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，分配的IP地址看起来与Calico示例完全不同。进一步探索显示地址和路由配置也有所不同，正如[清单 8-4](ch08.xhtml#ch08list4)中所示。
- en: '[PRE28]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '*Listing 8-4: WeaveNet networking*'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 8-4: WeaveNet 网络*'
- en: This time, our Pods are getting IP addresses in a massive `/12` network, corresponding
    to more than one million possible addresses on a single network. In this case,
    our Pod’s networking stack is going to expect to be able to use ARP to directly
    identify the hardware address of any other Pod on the network rather than routing
    traffic to a gateway as we saw with Calico.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，我们的Pod获得了一个大范围的`/12`网络中的IP地址，意味着单个网络中有超过一百万个可能的地址。在这种情况下，我们Pod的网络栈预计能够使用ARP直接识别网络上任何其他Pod的硬件地址，而不是像我们在Calico中看到的那样将流量路由到网关。
- en: 'As before, we do have connectivity between these two Pods:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前一样，我们确实在这两个Pod之间建立了连接：
- en: '[PRE29]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'And now that we’ve run this `ping` command, we should expect that the ARP table
    in the `pod1` networking stack is populated with the hardware address of the `pod2`
    network interface:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经运行了这个`ping`命令，我们应该期待`pod1`网络栈中的ARP表已经填充了`pod2`网络接口的硬件地址：
- en: '[PRE30]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'As expected, `pod1` has an ARP table entry for `pod2`’s IP address, corresponding
    to the virtual network interface inside `pod2`:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，`pod1`有一个针对`pod2` IP地址的ARP表项，对应于`pod2`内部的虚拟网络接口：
- en: '[PRE31]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The hardware address in the `pod1` ARP table matches the hardware address of
    the virtual network device in `pod2` ➊. To make this happen, WeaveNet is routing
    the ARP request over the network so that the network stack in `pod2` can respond.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '`pod1`的ARP表中的硬件地址与`pod2`虚拟网络设备的硬件地址匹配➊。为了实现这一点，WeaveNet正在通过网络路由ARP请求，以便`pod2`的网络栈能够做出响应。'
- en: Let’s look at how the cross-node routing of ARP and ICMP traffic is happening.
    First, although the IP address management may be different, one important similarity
    between Calico and WeaveNet is that both are using veth pairs to connect containers
    to the host. If you want to explore that, use the commands shown in [Listing 8-2](ch08.xhtml#ch08list2)
    and [Listing 8-3](ch08.xhtml#ch08list3) to determine the network namespace for
    `pod1`, and then use `ip addr` on `host01` to verify that there is a `veth` device
    with a `link-netns` field that corresponds to that network namespace.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看跨节点的ARP和ICMP流量是如何传输的。首先，尽管IP地址管理可能不同，Calico和WeaveNet之间的一个重要相似之处是，二者都使用veth对将容器连接到主机。如果你想深入探索这一点，可以使用[清单
    8-2](ch08.xhtml#ch08list2)和[清单 8-3](ch08.xhtml#ch08list3)中的命令来确定`pod1`的网络命名空间，然后在`host01`上使用`ip
    addr`验证是否存在一个具有`link-netns`字段的`veth`设备，该字段对应于该网络命名空间。
- en: For our purposes, because we’ve seen that before, we’ll take it as a given that
    the traffic goes through the virtual network wire created by the veth pair and
    gets to the host. Let’s start there and trace the ICMP traffic between the two
    Pods.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 出于我们的目的，因为我们之前已经看到过这个情况，我们假设流量是通过由veth对创建的虚拟网络线路传输的，并到达主机。从这里开始，我们追踪这两个Pod之间的ICMP流量。
- en: 'If we use the same `tcpdump` capture as we did with Calico, we’ll be able to
    capture the ICMP traffic, but that will get us only so far. Let’s go ahead and
    look at that:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用与Calico相同的`tcpdump`捕获，我们将能够捕获到ICMP流量，但这只能帮助我们到达一定程度。让我们继续查看一下：
- en: '[PRE32]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'As before, we ran `tcpdump` in the background to capture ICMP on all network
    interfaces, ran our `ping`, and then stopped `tcpdump` so that it would write
    out the packets it captured. This time we have 24 packets to look at, but they
    still don’t tell the whole story:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如同之前一样，我们在后台运行了`tcpdump`来捕获所有网络接口上的ICMP流量，运行了我们的`ping`命令，然后停止了`tcpdump`，让它写出捕获的包。这一次我们有24个数据包可以查看，但它们仍然不能讲述整个故事：
- en: '[PRE33]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: These lines show four packets for a single `ping` request and reply, but the
    hardware addresses aren’t changing. What’s happening is that these ICMP packets
    are being handed between network interfaces unmodified. However, we’re still not
    seeing the actual traffic that’s going between `host01` and `host02`, because
    we never see any hardware addresses that correspond to host interfaces.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这些行显示了一个单独的`ping`请求和回复的四个数据包，但硬件地址并没有发生变化。发生的情况是，这些ICMP数据包在网络接口之间被传递，且没有修改。然而，我们仍然没有看到实际在`host01`和`host02`之间传输的流量，因为我们从未看到任何与主机接口对应的硬件地址。
- en: To see the host-level traffic, we need to tell `tcpdump` to capture UDP and
    then treat it as VXLAN, which enables `tcpdump` to identify the fact that an ICMP
    packet is inside.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看主机级流量，我们需要告诉`tcpdump`捕获UDP流量，然后将其视为VXLAN，这样可以使`tcpdump`识别出ICMP数据包的存在。
- en: 'Let’s start the capture again, this time looking for UDP traffic:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新开始捕获，这次查找UDP流量：
- en: '[PRE34]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: This time we saved the packet data in *vxlan.pcap*. In this example, `tcpdump`
    captured 22 packets. Because there is lots of cross-Pod traffic in our cluster,
    not just ICMP traffic, you might see a different number.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这次我们将数据包数据保存在*vxlan.pcap*中。在这个例子中，`tcpdump`捕获了22个数据包。由于我们集群中有大量的跨Pod流量，而不仅仅是ICMP流量，您可能会看到不同的数量。
- en: The packets we captured cover all of the UDP traffic on `host01`, not just our
    ICMP, so in printing out the packets shown in [Listing 8-5](ch08.xhtml#ch08list5),
    we’ll need to be selective.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们捕获的数据包覆盖了`host01`上的所有UDP流量，而不仅仅是我们的ICMP流量，因此在打印出[清单8-5](ch08.xhtml#ch08list5)中显示的数据包时，我们需要进行选择。
- en: '[PRE35]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '*Listing 8-5: VXLAN capture*'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单8-5：VXLAN捕获*'
- en: The `-T vxlan` flag tells `tcpdump` to treat the packet data it sees as VXLAN
    data. This causes `tcpdump` to look inside and pull out data from the encapsulated
    packets, enabling it to identify ICMP packets when those are hidden inside. We
    then use `grep` with a `-B 1` flag to find those ICMP packets and also print the
    line immediately previous so that we can see the VXLAN wrapper.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '`-T vxlan`标志告诉`tcpdump`将其看到的数据包数据视为VXLAN数据。这使得`tcpdump`可以深入查看并提取封装数据包中的数据，从而识别出那些被隐藏在内部的ICMP数据包。接着，我们使用`grep`和`-B
    1`标志来查找这些ICMP数据包，并打印出它们之前的一行，以便查看VXLAN包装器。'
- en: This capture shows the host’s hardware address, which informs us that we’ve
    managed to capture the traffic moving between hosts. Each ICMP packet is wrapped
    in a UDP datagram and sent across the host network. The IP source and destination
    for these datagrams are the host network IP addresses `192.168.61.11` and `192.168.61.12`,
    so the host network never sees the Pod IP addresses. However, that information
    is still there, in the encapsulated ICMP packet, thus when the datagram arrives
    at its destination, WeaveNet can send the ICMP packet to the correct destination.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这个捕获显示了主机的硬件地址，这表明我们已经成功捕获了在主机之间传输的流量。每个ICMP数据包都被封装在一个UDP数据报中，并通过主机网络发送。这些数据报的IP源和目标地址是主机网络的IP地址`192.168.61.11`和`192.168.61.12`，因此主机网络从未看到Pod的IP地址。然而，这些信息仍然存在于封装的ICMP数据包中，因此，当数据报到达目的地时，WeaveNet能够将ICMP数据包发送到正确的目的地。
- en: The advantage of encapsulation is that all of our cross-node traffic looks like
    ordinary UDP datagrams between hosts. Typically, we don’t need to do any additional
    network configuration to allow this traffic. However, we do pay a price. As you
    can see in [Listing 8-5](ch08.xhtml#ch08list5), each ICMP packet is 98 bytes,
    but the encapsulated packet is 150 bytes. The wrapper needed for encapsulation
    creates network overhead that we have to pay with each packet we send.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 封装的优点是，我们所有的跨节点流量看起来就像主机之间的普通UDP数据报。通常，我们无需做任何额外的网络配置来允许这种流量。然而，我们也付出了代价。正如在[清单8-5](ch08.xhtml#ch08list5)中看到的，每个ICMP数据包大小为98字节，但封装后的数据包为150字节。为了进行封装所需的包装器会产生网络开销，我们需要为每个发送的数据包支付这个开销。
- en: Look back at [Listing 8-4](ch08.xhtml#ch08list4) for another consequence. The
    virtual network interface inside the Pod has a maximum transmission unit (MTU)
    of 1,376\. This represents the largest packet that can be sent; anything bigger
    must to be fragmented into multiple packets and reassembled at the destination.
    This MTU of 1,376 is considerably smaller than the standard MTU of 1,500 on our
    host network. The smaller MTU on the Pod interface ensures that the Pod’s network
    stack will do any required fragmenting. This way, we can guarantee that we don’t
    exceed 1,500 at the host layer, even after the wrapper is added. For this reason,
    if you are using a network plug-in that uses encapsulation, it might be worth
    exploring how to configure jumbo frames to enable an MTU larger than 1,500 on
    the host network.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 请回顾一下[清单8-4](ch08.xhtml#ch08list4)中的另一个结果。Pod内部的虚拟网络接口的最大传输单元（MTU）为1,376。这个值代表可以发送的最大数据包；任何更大的数据包必须被分段并在目的地重新组装。这个1,376的MTU远小于主机网络上的标准1,500。Pod接口上较小的MTU确保Pod的网络栈会进行必要的分段处理。这样，我们可以确保即使添加了封装层，主机层也不会超过1,500。因此，如果你使用的是通过封装实现的网络插件，值得探索如何配置巨型帧，以便在主机网络上启用大于1,500的MTU。
- en: Choosing a Network Plug-in
  id: totrans-139
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 选择网络插件
- en: Network plug-ins can use different approaches to cross-node networking. As is
    universal in engineering, though, there are trade-offs with each approach. Layer
    3 routing uses native capabilities of Linux and is efficient in its use of the
    network bandwidth, but it may require customization of the underlying host network.
    Encapsulation with VXLAN works in any network where we can send UDP datagrams
    between hosts, but it adds overhead with each packet.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 网络插件可以采用不同的方式来实现跨节点的网络连接。然而，正如工程学中的普遍规律，每种方法都有其权衡。第3层路由利用了Linux的原生功能，在使用网络带宽方面效率较高，但可能需要定制底层主机网络。通过VXLAN封装的方法适用于任何可以在主机之间发送UDP数据报的网络，但它会增加每个数据包的开销。
- en: Either way, however, our Pods are getting what they need, which is the ability
    to communicate with other Pods, wherever in the cluster they may be. And in practice,
    the configuration effort and performance difference tends to be small. For this
    reason, the best way to choose a network plug-in is to start with the plug-in
    that is recommended for or installed by default with your particular Kubernetes
    distribution. If you find specific use cases for which the performance doesn’t
    meet your requirements, you’ll then be able to test an alternative plug-in based
    on real network traffic rather than guesswork.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，我们的Pods都能满足其需求，即能够与集群中其他位置的Pods进行通信。实际上，配置工作和性能差异通常很小。因此，选择网络插件的最佳方式是从你的Kubernetes发行版推荐或默认安装的插件开始。如果你发现某些特定用例的性能无法满足要求，你可以基于实际网络流量而不是猜测，测试其他插件。
- en: Network Customization
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网络定制
- en: Some scenarios may require cluster networking that is more complex than a single
    Pod network connected across all cluster nodes. For example, some regulated industries
    require certain data, such as security audit logs, to travel across a separated
    network. Other systems may have specialized hardware so that application components
    that interface with that hardware must be placed on a specific network or virtual
    LAN (VLAN).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 某些场景可能需要比单一Pod网络连接跨所有集群节点更为复杂的集群网络。例如，一些受监管的行业要求某些数据（如安全审计日志）通过一个独立的网络传输。其他系统可能有专门的硬件，要求与该硬件交互的应用组件必须放置在特定的网络或虚拟局域网（VLAN）中。
- en: One of the advantages of a plug-in architecture for networking is that a Kubernetes
    cluster can accommodate these specialized networking scenarios. As long as Pods
    have an interface that can reach (and is reachable from) the rest of the cluster,
    Pods can have additional network interfaces that provide specialized connectivity.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 网络插件架构的一个优势是Kubernetes集群能够容纳这些特定的网络场景。只要Pods有一个接口能够连接到集群的其他部分（并且能够从集群其他部分访问），Pods就可以有额外的网络接口来提供专门的连接。
- en: Let’s look at an example. We’ll configure two Pods on the same node so they
    have a local host-only network they can use for intercommunication. Being a host-only
    network, it doesn’t provide connectivity to the rest of the cluster, so we’ll
    also use Calico to provide cluster networking for Pods.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来看一个例子。我们将配置两个在同一节点上的Pods，使它们拥有一个本地的仅主机网络，可以用于相互通信。由于是仅主机网络，它不提供与集群其他部分的连接，因此我们还将使用Calico为Pods提供集群网络。
- en: Because of the need to configure both Calico and our host-only network, we’ll
    be invoking two separate CNI plug-ins that will create virtual network interfaces
    in our Pods’ network namespaces. As we saw in [Listing 8-1](ch08.xhtml#ch08list1),
    it’s possible to configure multiple CNI plug-ins in a single configuration file.
    However, `kubelet` expects only one of these CNI plug-ins to actually assign a
    network interface and IP address. To work around this, we’ll use Multus, a CNI
    plug-in that is designed to invoke multiple plug-ins but will treat one as primary
    for purposes of reporting IP address information back to `kubelet`. Multus also
    allows us to be selective as to what CNI plug-ins are applied to each Pod.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 由于需要配置 Calico 和我们的仅主机网络，我们将调用两个不同的 CNI 插件，它们将在 Pod 的网络命名空间中创建虚拟网络接口。如同我们在[示例
    8-1](ch08.xhtml#ch08list1)中看到的那样，确实可以在一个配置文件中配置多个 CNI 插件。然而，`kubelet` 只期望其中一个
    CNI 插件实际分配网络接口和 IP 地址。为了解决这个问题，我们将使用 Multus，一个设计用来调用多个插件的 CNI 插件，但会将其中一个插件视为主插件，用于向
    `kubelet` 报告 IP 地址信息。Multus 还允许我们根据需要选择应用哪些 CNI 插件到每个 Pod。
- en: 'We’ll begin by installing Multus into the `calico` example cluster for this
    chapter:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先在本章的 `calico` 示例集群中安装 Multus：
- en: '[PRE36]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: As the filename implies, the primary resource in this YAML file is a DaemonSet
    that runs a Multus container on every host. However, this file installs several
    other resources, including a *CustomResourceDefinition*. This CustomResourceDefinition
    will allow us to configure network attachment resources to tell Multus what CNI
    plug-ins to use for a given Pod.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 正如文件名所示，这个 YAML 文件中的主要资源是一个 DaemonSet，它在每个主机上运行一个 Multus 容器。然而，这个文件还安装了其他几个资源，包括一个
    *CustomResourceDefinition*。这个 CustomResourceDefinition 允许我们配置网络附加资源，告诉 Multus
    在特定 Pod 中使用哪些 CNI 插件。
- en: We’ll look at CustomResourceDefinitions in detail in [Chapter 17](ch17.xhtml#ch17).
    For now, in [Listing 8-6](ch08.xhtml#ch08list6) we’ll just see the NetworkAttachmentDefinition
    that we’ll use to configure Multus.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[第 17 章](ch17.xhtml#ch17)中详细查看 CustomResourceDefinitions。现在，在[示例 8-6](ch08.xhtml#ch08list6)中，我们将看到用于配置
    Multus 的 NetworkAttachmentDefinition。
- en: '*netattach.yaml*'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '*netattach.yaml*'
- en: '[PRE37]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '*Listing 8-6: Network attachment*'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例 8-6：网络附加*'
- en: The `config` field in the `spec` looks a lot like a CNI configuration file,
    which isn’t surprising, as Multus needs to use this information to invoke the
    `macvlan` CNI plug-in when we ask for it to be added to a Pod.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '`spec` 中的 `config` 字段看起来像一个 CNI 配置文件，这并不奇怪，因为 Multus 需要使用这些信息在我们要求将其添加到 Pod
    时调用 `macvlan` CNI 插件。'
- en: 'We need to add this NetworkAttachmentDefinition to the cluster:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将这个 NetworkAttachmentDefinition 添加到集群中：
- en: '[PRE38]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: This definition doesn’t immediately affect any of our Pods; it just provides
    a Multus configuration for future use.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这个定义并不会立即影响任何 Pod；它只是为将来使用提供了 Multus 配置。
- en: 'Of course, to use this configuration, Multus must be invoked. How does that
    happen when we’ve already installed Calico into this cluster? The answer is in
    the */etc/cni/net.d* directory, which the Multus DaemonSet modified on all of
    our cluster nodes as part of its initialization:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，要使用这个配置，必须调用 Multus。那么，当我们已经将 Calico 安装到这个集群时，如何实现这一点呢？答案就在*/etc/cni/net.d*
    目录中，这个目录在 Multus DaemonSet 初始化时会修改我们集群中所有节点上的配置：
- en: '[PRE39]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Multus left the existing Calico configuration files in place, but added its
    own *00-multus.conf* configuration file and a *multus.d* directory. Because the
    *00-multus.conf* file is ahead of *10-calico.conflist* in an alphabetic sort,
    `kubelet` will start to use it the next time it creates a new Pod.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Multus 保留了现有的 Calico 配置文件，但添加了它自己的 *00-multus.conf* 配置文件和 *multus.d* 目录。由于 *00-multus.conf*
    文件在字母排序中排在 *10-calico.conflist* 前面，`kubelet` 会在下次创建新 Pod 时开始使用它。
- en: 'Here’s *00-multus.conf*:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 *00-multus.conf*：
- en: '*00-multus.conf*'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '*00-multus.conf*'
- en: '[PRE40]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The `delegates` field is pulled from the Calico configuration that Multus found.
    This field is used to determine the default CNI plug-ins that Multus always uses
    when it is invoked. The top-level `capabilities` field is needed to ensure that
    Multus will get all the correct configuration data from `kubelet` to be able to
    invoke the `portmap` and `bandwidth` plug-ins.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '`delegates` 字段来自 Multus 找到的 Calico 配置。这个字段用于确定 Multus 在每次调用时始终使用的默认 CNI 插件。顶层的
    `capabilities` 字段是必须的，以确保 Multus 从 `kubelet` 获取所有正确的配置数据，以便能够调用 `portmap` 和 `bandwidth`
    插件。'
- en: 'Now that Multus is fully set up, let’s use it to add a host-only network to
    two Pods. The Pods are defined as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在 Multus 已经完全设置好了，让我们用它向两个 Pod 添加一个仅主机网络。这些 Pod 的定义如下：
- en: '*local-pods.yaml*'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '*local-pods.yaml*'
- en: '[PRE41]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: This time we need both Pods to wind up on `host01` so that the host-only networking
    functions. In addition, we add the `k8s.v1.cni.cncf.io/networks` annotation to
    each Pod. Multus uses this annotation to identify what additional CNI plug-ins
    it should run. The name `macvlan-conf` matches the name we provided in the NetworkAttachmentDefinition
    in [Listing 8-6](ch08.xhtml#ch08list6).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，我们需要这两个 Pod 最终都在 `host01` 上运行，以便仅限主机的网络功能得以实现。此外，我们为每个 Pod 添加了 `k8s.v1.cni.cncf.io/networks`
    注解。Multus 使用这个注解来识别应运行的额外 CNI 插件。`macvlan-conf` 这个名字与我们在 [Listing 8-6](ch08.xhtml#ch08list6)
    中的 NetworkAttachmentDefinition 中提供的名称匹配。
- en: 'Let’s create these two Pods:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建这两个 Pod：
- en: '[PRE42]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'After these Pods are running, we can check that they each have an extra network
    interface:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些 Pod 运行之后，我们可以检查它们是否各自有一个额外的网络接口：
- en: '[PRE43]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The `macvlan` CNI plug-in has added the additional `net1` network interface,
    using the IP address management configuration we provided in the NetworkAttachmentDefinition.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '`macvlan` CNI 插件已添加额外的 `net1` 网络接口，并使用我们在 NetworkAttachmentDefinition 中提供的
    IP 地址管理配置。'
- en: 'These two Pods are now able to communicate with each other using these interfaces:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个 Pod 现在可以通过以下接口相互通信：
- en: '[PRE44]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: This communication goes over the bridge created by the `macvlan` CNI plug-in,
    as opposed to travelling via Calico.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这种通信通过 `macvlan` CNI 插件创建的桥接网络进行，而不是通过 Calico 进行。
- en: Keep in mind that our purpose here is solely to demonstrate custom networking
    without requiring any particular VLAN or complex setup outside our cluster hosts.
    For a real cluster, this kind of host-only network is of limited value because
    it constrains where Pods can be deployed. In this kind of situation, it might
    be preferable to place the two containers into the same Pod so that they will
    always be scheduled together and can use `localhost` to communicate.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，我们在这里的目的仅仅是演示自定义网络，而无需集群主机外部的任何特定 VLAN 或复杂设置。对于实际的集群，这种仅限主机的网络价值有限，因为它限制了
    Pod 的部署位置。在这种情况下，将两个容器放入同一个 Pod 可能更为可取，这样它们总是会一起调度，并可以使用 `localhost` 进行通信。
- en: Final Thoughts
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最后的思考
- en: We’ve looked at a lot of network interfaces and traffic flows in this chapter.
    Most of the time, it’s enough to know that every Pod in the cluster is allocated
    an IP address from a Pod network, and also that any Pod in the cluster can reach
    and is reachable from any other Pod. Any of the Kubernetes network plug-ins provide
    this capability, whether they use Layer 3 routing or VXLAN encapsulation, or possibly
    both.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们已经看了很多网络接口和流量流动。大多数情况下，了解集群中的每个 Pod 都会从 Pod 网络中分配一个 IP 地址，并且集群中的任何 Pod
    都可以与任何其他 Pod 通信，且可以被访问，这就足够了。任何 Kubernetes 网络插件都可以提供这种功能，无论它们使用的是第 3 层路由、VXLAN
    封装，还是两者兼而有之。
- en: At the same time, networking issues do occur in a cluster, and it’s essential
    for cluster administrators and cluster users to understand how the traffic is
    flowing between hosts and what that traffic looks like to the host network in
    order to debug issues with switch and host configuration, or simply to build applications
    that make best use of the cluster.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，集群中确实会发生网络问题，因此集群管理员和用户必须理解流量如何在主机之间流动，以及这些流量对主机网络的表现，以便调试交换机和主机配置问题，或者仅仅为了构建能够充分利用集群的应用程序。
- en: We’re not yet done with the networking layers that are needed to have a fully
    functioning Kubernetes cluster. In the next chapter, we’ll look at how Kubernetes
    provides a Service layer on top of Pod networking to provide load balancing and
    automated failover, and then uses the Service networking layer together with Ingress
    networking to make container services accessible outside the cluster.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有完成使 Kubernetes 集群完全功能所需的网络层。在下一章中，我们将探讨 Kubernetes 如何在 Pod 网络之上提供服务层，以提供负载均衡和自动故障切换，并结合
    Ingress 网络层使容器服务在集群外部可访问。
