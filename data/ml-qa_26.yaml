- en: '**22'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**22**'
- en: SPEEDING UP INFERENCE**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**加速推理**'
- en: '![Image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/common.jpg)'
- en: What are techniques to speed up model inference through optimization without
    changing the model architecture or sacrificing accuracy?
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 有哪些优化技术可以加速模型推理，而不改变模型架构或牺牲准确性？
- en: In machine learning and AI, *model inference* refers to making predictions or
    generating outputs using a trained model. The main general techniques for improving
    model performance during inference include parallelization, vectorization, loop
    tiling, operator fusion, and quantization, which are discussed in detail in the
    following sections.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习和人工智能中，*模型推理*指的是使用训练好的模型进行预测或生成输出。提高推理性能的主要技术包括并行化、矢量化、循环平铺、操作符融合和量化，以下章节将详细讨论这些技术。
- en: '**Parallelization**'
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**并行化**'
- en: One common way to achieve better parallelization during inference is to run
    the model on a batch of samples rather than on a single sample at a time. This
    is sometimes also referred to as *batched inference* and assumes that we are receiving
    multiple input samples or user inputs simultaneously or within a short time window,
    as illustrated in [Figure 22-1](ch22.xhtml#ch22fig1).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 提高推理期间并行化的常见方法之一是一次处理一批样本，而不是逐个处理单一样本。这有时也被称为*批量推理*，假设我们同时或在短时间窗口内接收多个输入样本或用户输入，如[图
    22-1](ch22.xhtml#ch22fig1)所示。
- en: '![Image](../images/22fig01.jpg)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/22fig01.jpg)'
- en: '*Figure 22-1: Sequential inference and batched inference*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 22-1: 顺序推理与批量推理*'
- en: '[Figure 22-1](ch22.xhtml#ch22fig1) shows sequential inference processing one
    item at a time, which creates a bottleneck if there are several samples waiting
    to be classified. In batched inference, the model processes all four samples at
    the same time.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 22-1](ch22.xhtml#ch22fig1)显示了顺序推理一次处理一个项目的方式，如果有多个样本等待分类，这会造成瓶颈。在批量推理中，模型同时处理所有四个样本。'
- en: '**Vectorization**'
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**矢量化**'
- en: '*Vectorization* refers to performing operations on entire data structures,
    such as arrays (tensors) or matrices, in a single step rather than using iterative
    constructs like `for` loops. Using vectorization, multiple operations from the
    loop are performed simultaneously using single instruction, multiple data (SIMD)
    processing, which is available on most modern CPUs.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*矢量化*指的是对整个数据结构（如数组（张量）或矩阵）进行操作，而不是使用`for`循环等迭代结构。通过矢量化，循环中的多个操作会通过单指令多数据（SIMD）处理同时执行，这在大多数现代CPU上都可以实现。'
- en: This approach takes advantage of the low-level optimizations in many computing
    systems and often results in significant speedups. For example, it might rely
    on BLAS.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法利用了许多计算系统中的低级优化，通常会显著提高速度。例如，它可能依赖于BLAS。
- en: '*BLAS* (which is short for *Basic Linear Algebra Subprograms*) is a specification
    that prescribes a set of low-level routines for performing common linear algebra
    operations such as vector addition, scalar multiplication, dot products, matrix
    multiplication, and others. Many array and deep learning libraries like NumPy
    and PyTorch use BLAS under the hood.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*BLAS*（即*基本线性代数子程序*的缩写）是一种规范，规定了一组用于执行常见线性代数操作的低级例程，例如向量加法、标量乘法、点积、矩阵乘法等。许多数组和深度学习库，如NumPy和PyTorch，都在幕后使用BLAS。'
- en: To illustrate vectorization with an example, suppose we wanted to compute the
    dot product between two vectors. The non-vectorized way of doing this would be
    to use a `for` loop, iterating over each element of the array one by one. However,
    this can be quite slow, especially for large arrays. With vectorization, you can
    perform the dot product operation on the entire array at once, as shown in [Figure
    22-2](ch22.xhtml#ch22fig2).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了通过一个例子说明矢量化，假设我们要计算两个向量的点积。非矢量化的做法是使用`for`循环，逐个遍历数组中的每个元素。然而，这种方法可能非常慢，特别是对于大型数组。通过矢量化，你可以一次性对整个数组执行点积操作，如[图
    22-2](ch22.xhtml#ch22fig2)所示。
- en: '![Image](../images/22fig02.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/22fig02.jpg)'
- en: '*Figure 22-2: A classic* for *loop versus a vectorized dot product computation
    in Python*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 22-2: Python中经典的* for *循环与矢量化点积计算*'
- en: In the context of linear algebra or deep learning frameworks like TensorFlow
    and PyTorch, vectorization is typically done automatically. This is because these
    frameworks are designed to work with multidimensional arrays (also known as *tensors*),
    and their operations are inherently vectorized. This means that when you perform
    functions using these frameworks, you automatically leverage the power of vectorization,
    resulting in faster and more efficient computations.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性代数或深度学习框架（如 TensorFlow 和 PyTorch）中，矢量化通常是自动进行的。这是因为这些框架设计用于处理多维数组（也称为*张量*），其操作本质上是矢量化的。这意味着，当你使用这些框架执行函数时，自动利用了矢量化的优势，从而实现更快、更高效的计算。
- en: '**Loop Tiling**'
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**循环分块**'
- en: '*Loop tiling* (also often referred to as *loop nest optimization*) is an advanced
    optimization technique to enhance data locality by breaking down a loop’s iteration
    space into smaller chunks or “tiles.” This ensures that once data is loaded into
    cache, all possible computations are performed on it before the cache is cleared.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*循环分块*（也常被称为*循环嵌套优化*）是一种先进的优化技术，通过将循环的迭代空间拆分成更小的块或“瓦片”来增强数据局部性。这确保了数据一旦加载到缓存中，在缓存清空之前，所有可能的计算都会在其上执行。'
- en: '[Figure 22-3](ch22.xhtml#ch22fig3) illustrates the concept of loop tiling for
    accessing elements in a two-dimensional array. In a regular `for` loop, we iterate
    over columns and rows one element at a time, whereas in loop tiling, we subdivide
    the array into smaller tiles.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 22-3](ch22.xhtml#ch22fig3) 说明了二维数组中访问元素的循环分块概念。在常规的`for`循环中，我们一次迭代处理一个元素，遍历列和行，而在循环分块中，我们将数组细分为更小的块。'
- en: '![Image](../images/22fig03.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/22fig03.jpg)'
- en: '*Figure 22-3: Loop tiling in a two-dimensional array*'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 22-3：二维数组中的循环分块*'
- en: Note that in languages such as Python, we don’t usually perform loop tiling,
    because Python and many other high-level languages do not allow control over cache
    memory like lower-level languages such as C and C++ do. These kinds of optimizations
    are often handled by underlying libraries like NumPy and PyTorch when performing
    operations on large arrays.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在像 Python 这样的语言中，我们通常不会进行循环分块，因为 Python 和许多其他高级语言不像 C 和 C++ 这样的低级语言那样可以控制缓存内存。这类优化通常由底层库（如
    NumPy 和 PyTorch）在对大数组进行操作时处理。
- en: '**Operator Fusion**'
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**操作符融合**'
- en: '*Operator fusion*, sometimes called *loop fusion*, is an optimization technique
    that combines multiple loops into a single loop. This is illustrated in [Figure
    22-4](ch22.xhtml#ch22fig4), where two separate loops to calculate the sum and
    the product of an array of numbers are fused into a single loop.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*操作符融合*，有时也叫做*循环融合*，是一种优化技术，它将多个循环合并为一个循环。这在[图 22-4](ch22.xhtml#ch22fig4)中得到了说明，其中两个独立的循环分别计算一个数字数组的和与积，被融合成了一个循环。'
- en: '![Image](../images/22fig04.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/22fig04.jpg)'
- en: '*Figure 22-4: Fusing two* for *loops (left) into one (right)*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 22-4：将两个* for *循环（左）融合为一个（右）*'
- en: Operator fusion can improve the performance of a model by reducing the overhead
    of loop control, decreasing memory access times by improving cache performance,
    and possibly enabling further optimizations through vectorization.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 操作符融合可以通过减少循环控制的开销、通过提高缓存性能减少内存访问时间，并可能通过矢量化启用进一步的优化，从而提高模型的性能。
- en: You might think this behavior of vectorization would be incompatible with loop
    tiling, in which we break a `for` loop into multiple loops. However, these techniques
    are actually complementary, used for different optimizations, and applicable in
    different situations. Operator fusion is about reducing the total number of loop
    iterations and improving data locality when the entire data fits into cache. Loop
    tiling is about improving cache utilization when dealing with larger multidimensional
    arrays that do not fit into cache.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会认为，矢量化行为与循环分块不兼容，因为在循环分块中，我们将一个`for`循环拆分为多个循环。然而，这些技术实际上是互补的，用于不同的优化，并适用于不同的场景。操作符融合旨在减少循环迭代的总次数，并在整个数据适合缓存时提高数据局部性。而循环分块则是在处理无法完全装入缓存的更大多维数组时，改进缓存利用率。
- en: Related to operator fusion is the concept of *reparameterization*, which can
    often also be used to simplify multiple operations into one. Popular examples
    include training a network with multibranch architectures that are reparameterized
    into single-stream architectures during inference. This reparameterization approach
    differs from traditional operator fusion in that it does not merge multiple operations
    into a single operation. Instead, it rearranges the operations in the network
    to create a more efficient architecture for inference. In the so-called RepVGG
    architecture, for example, each branch during training consists of a series of
    convolutions. Once training is complete, the model is reparameterized into a single
    sequence of convolutions.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 与操作符融合相关的是*重参数化*的概念，它通常也可以用来将多个操作简化为一个操作。流行的例子包括训练一个具有多分支架构的网络，在推理时将其重参数化为单流架构。与传统的操作符融合不同，这种重参数化方法并不会将多个操作合并为一个操作，而是重新排列网络中的操作，以创建更高效的推理架构。例如，在所谓的RepVGG架构中，每个分支在训练过程中由一系列卷积组成。一旦训练完成，模型就会重参数化为一个单一的卷积序列。
- en: '**Quantization**'
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**量化**'
- en: '*Quantization* reduces the computational and storage requirements of machine
    learning models, particularly deep neural networks. This technique involves converting
    the floating-point numbers (technically discrete but representing continuous values
    within a specific range) for implementing weights and biases in a trained neural
    network to more discrete, lower-precision representations such as integers. Using
    less precision reduces the model size and makes it quicker to execute, which can
    lead to significant improvements in speed and hardware efficiency during inference.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*量化*减少了机器学习模型的计算和存储需求，特别是深度神经网络。该技术涉及将浮点数（技术上是离散的，但在特定范围内表示连续值）用于实现训练好的神经网络中的权重和偏置转换为更离散、更低精度的表示形式，如整数。使用较低的精度可以减小模型的大小并加快执行速度，从而在推理过程中显著提高速度和硬件效率。'
- en: In the realm of deep learning, it has become increasingly common to quantize
    trained models down to 8-bit and 4-bit integers. These techniques are especially
    prevalent in the deployment of large language models.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习领域，将训练好的模型量化为8位和4位整数已变得越来越常见。这些技术在大型语言模型的部署中尤为常见。
- en: There are two main categories of quantization. In *post-training quantization*,
    the model is first trained normally with full-precision weights, which are then
    quantized after training. *Quantization-aware training*, on the other hand, introduces
    the quantization step during the training process. This allows the model to learn
    to compensate for the effects of quantization, which can help maintain the model’s
    accuracy.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 量化主要有两大类。*训练后量化*是先使用全精度权重进行常规训练，训练完成后再进行量化。*量化感知训练*则是在训练过程中引入量化步骤。这使得模型可以学习补偿量化的影响，从而有助于保持模型的准确性。
- en: However, it’s important to note that quantization can occasionally lead to a
    reduction in model accuracy. Since this chapter focuses on techniques to speed
    up model inference *without* sacrificing accuracy, quantization is not as good
    a fit for this chapter as the previous categories.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，值得注意的是，量化有时可能会导致模型准确度的降低。由于本章重点讨论在*不*牺牲准确度的情况下加速模型推理，因此量化不如前述技术适合本章内容。
- en: '**NOTE**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*Other techniques to improve inference speeds include knowledge distillation
    and pruning, discussed in [Chapter 6](ch06.xhtml). However, these techniques affect
    the model architecture, resulting in smaller models, so they are out of scope
    for this chapter’s question.*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*提高推理速度的其他技术包括知识蒸馏和剪枝，这些在[第6章](ch06.xhtml)中讨论过。然而，这些技术会影响模型架构，导致模型变小，因此不在本章问题的讨论范围内。*'
- en: '**Exercises**'
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**练习**'
- en: '**22-1.** [Chapter 7](ch07.xhtml) covered several multi-GPU training paradigms
    to speed up model training. Using multiple GPUs can, in theory, also speed up
    model inference. However, in reality, this approach is often not the most efficient
    or most practical option. Why is that?'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**22-1.** [第7章](ch07.xhtml)介绍了几种多GPU训练范式，以加速模型训练。理论上，使用多个GPU也可以加速模型推理。然而，在实际操作中，这种方法通常并不是最有效或最实用的选择。为什么会这样呢？'
- en: '**22-2.** Vectorization and loop tiling are two strategies for optimizing operations
    that involve accessing array elements. What would be the ideal situation in which
    to use each?'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**22-2.** 向量化和循环平铺是两种优化涉及访问数组元素操作的策略。在哪些理想情况下使用每种方法最合适？'
- en: '**References**'
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: 'The official BLAS website: *[https://www.netlib.org/blas/](https://www.netlib.org/blas/)*.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 官方BLAS网站：*[https://www.netlib.org/blas/](https://www.netlib.org/blas/)*。
- en: 'The paper that proposed loop tiling: Michael Wolfe, “More Iteration Space Tiling”
    (1989), *[https://dl.acm.org/doi/abs/10.1145/76263.76337](https://dl.acm.org/doi/abs/10.1145/76263.76337)*.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提出循环平铺方法的论文：Michael Wolfe，“更多的迭代空间平铺”（1989），*[https://dl.acm.org/doi/abs/10.1145/76263.76337](https://dl.acm.org/doi/abs/10.1145/76263.76337)*。
- en: 'RepVGG CNN architecture merging operations in inference mode: Xiaohan Ding
    et al., “RepVGG: Making VGG-style ConvNets Great Again” (2021), *[https://arxiv.org/abs/2101.03697](https://arxiv.org/abs/2101.03697)*.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RepVGG CNN架构合并推理模式中的操作：Xiaohan Ding 等人，“RepVGG：让VGG风格的卷积神经网络再度伟大”（2021），*[https://arxiv.org/abs/2101.03697](https://arxiv.org/abs/2101.03697)*。
- en: 'A new method for quantizing the weights in large language models down to 8-bit
    integer representations: Tim Dettmers et al., “LLM.int8(): 8-bit Matrix Multiplication
    for Transformers at Scale” (2022), *[https://arxiv.org/abs/2208.07339](https://arxiv.org/abs/2208.07339)*.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种将大语言模型中的权重量化到8位整数表示的新方法：Tim Dettmers 等人，“LLM.int8()：大规模变换器的8位矩阵乘法”（2022），*[https://arxiv.org/abs/2208.07339](https://arxiv.org/abs/2208.07339)*。
- en: 'A new method for quantizing the weights in LLMs farther down to 4-bit integers:
    Elias Frantar et al., “GPTQ: Accurate Post-Training Quantization for Generative
    Pre-trained Transformers” (2022), *[https://arxiv.org/abs/2210.17323](https://arxiv.org/abs/2210.17323)*.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种将大语言模型中的权重量化到4位整数的新方法：Elias Frantar 等人，“GPTQ：生成预训练变换器的精确后训练量化”（2022），*[https://arxiv.org/abs/2210.17323](https://arxiv.org/abs/2210.17323)*。
