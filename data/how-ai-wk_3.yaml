- en: '**3'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**3'
- en: 'CLASSICAL MODELS: OLD-SCHOOL MACHINE LEARNING**'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 经典模型：老派机器学习**
- en: '![Image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/common.jpg)'
- en: Beginning piano students don’t start with Liszt’s “La Campanella,” but “Mary
    Had a Little Lamb” or “Twinkle, Twinkle, Little Star.” The simpler pieces contain
    the basics of playing the piano, and mastering the basics allows students to progress
    over time. This principle holds in most areas of study, including artificial intelligence.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 初学钢琴的学生不会从李斯特的《钟声》开始，而是从《小星星》或《玛丽有只小羊羔》开始。这些简单的曲子包含了弹钢琴的基础，掌握这些基础可以让学生随着时间的推移不断进步。这个原则适用于大多数学习领域，包括人工智能。
- en: 'To reach our ultimate goal of understanding modern AI, we must begin in the
    “simpler” world of classical machine learning. What holds for the classical models
    is generally true for more advanced neural networks. This chapter explores three
    classical models: nearest neighbors, random forests, and support vector machines.
    Understanding these will prepare us for the neural networks of [Chapter 4](ch04.xhtml).'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 为了达到理解现代人工智能的最终目标，我们必须从“简单”的经典机器学习世界开始。经典模型的规律通常也适用于更先进的神经网络。本章将探讨三种经典模型：最近邻、随机森林和支持向量机。理解这些将为我们理解[第4章](ch04.xhtml)的神经网络做好准备。
- en: '****'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '****'
- en: '[Figure 3-1](ch03.xhtml#ch03fig01) shows the training samples for a made-up
    dataset with two features (*x*[0] and *x*[1]) and three classes (circles, squares,
    and triangles). We saw a similar plot in [Chapter 1](ch01.xhtml); see [Figure
    1-2](ch01.xhtml#ch01fig02). As with the iris dataset, every shape in the figure
    represents a sample from the training set. [Figure 3-1](ch03.xhtml#ch03fig01)
    is the tool we’ll use to understand the nearest neighbors classical model.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3-1](ch03.xhtml#ch03fig01)展示了一个虚构数据集的训练样本，包含两个特征（*x*[0] 和 *x*[1]）和三个类别（圆形、方形和三角形）。我们在[第1章](ch01.xhtml)中看到过类似的图表；见[图1-2](ch01.xhtml#ch01fig02)。与鸢尾花数据集类似，图中的每一个形状都代表训练集中的一个样本。[图3-1](ch03.xhtml#ch03fig01)是我们理解最近邻经典模型的工具。'
- en: '![Image](../images/ch03fig01.jpg)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch03fig01.jpg)'
- en: '*Figure 3-1: A made-up training set with three classes and two features*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*图3-1：一个包含三类和两个特征的虚构训练集*'
- en: As mentioned in the previous chapter, [*nearest neighbor*](glossary.xhtml#glo71)
    classifiers are the simplest of models—so simple that there’s no model to train;
    the training data *is* the model. To assign a class label to a new, unknown input,
    find the training sample closest to the unknown sample and return that sample’s
    label. That’s all there is to it. Despite their simplicity, nearest neighbor classifiers
    are quite effective if the training data represents what the model will encounter
    in the wild.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一章所提到的，[*最近邻*](glossary.xhtml#glo71)分类器是最简单的模型——简单到没有需要训练的模型；训练数据*就是*模型。为了给一个新的、未知的输入分配类标签，找到与未知样本最接近的训练样本并返回该样本的标签。就是这么简单。尽管它们很简单，如果训练数据代表了模型在现实中遇到的情况，最近邻分类器仍然非常有效。
- en: As a natural extension to the nearest neighbor model, locate the *k* training
    samples nearest the unknown sample. *k* is often a number like 3, 5, or 7, though
    it can be any number. This type of model uses a majority voting system, so the
    assigned class label is the one that’s most common among the *k* training samples.
    If there’s a tie, select the label randomly. For example, if the model is contemplating
    the 5-nearest neighbors to an unknown sample, and two are class 0 while another
    two are class 3, then assign the label by choosing randomly between 0 and 3; on
    average, you’ll make the correct choice 50 percent of the time.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最近邻模型的自然扩展，找到与未知样本最接近的*k*个训练样本。*k*通常是3、5或7这样的数字，尽管它可以是任何数字。这种模型使用多数投票系统，因此分配的类标签是*k*个训练样本中最常见的标签。如果出现平局，随机选择标签。例如，如果模型正在考虑一个未知样本的5个最近邻，其中两个是类别0，另两个是类别3，则通过在0和3之间随机选择来分配标签；平均而言，你会在50%的时间里做出正确选择。
- en: 'Let’s use the nearest neighbor concept to classify some unknown inputs. [Figure
    3-2](ch03.xhtml#ch03fig02) shows the training samples again, along with two unknown
    samples: the diamond and the pentagon. We want to assign these samples to one
    of the three classes: circle, square, or triangle. The nearest neighbor approach
    says to locate the training sample closest to each unknown sample. For the diamond,
    that’s the square to its upper left; for the pentagon, it appears to be the triangle
    to the upper right. Therefore, a nearest neighbor classifier assigns class square
    to the diamond and class triangle to the pentagon.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用最近邻概念来对一些未知输入进行分类。[图 3-2](ch03.xhtml#ch03fig02) 再次展示了训练样本，并加上了两个未知样本：菱形和五边形。我们想将这些样本分配到三个类别中的一个：圆形、方形或三角形。最近邻方法的思路是找到与每个未知样本最接近的训练样本。对于菱形，最近的训练样本是其左上方的方形；对于五边形，最近的训练样本是其右上方的三角形。因此，最近邻分类器会将菱形分为方形类别，将五边形分为三角形类别。
- en: '![Image](../images/ch03fig02.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch03fig02.jpg)'
- en: '*Figure 3-2: Classifying unknown samples*'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3-2：分类未知样本*'
- en: I suspect you’ve noticed the lines connecting the unknown samples in [Figure
    3-2](ch03.xhtml#ch03fig02) to the three nearest training samples. These are the
    samples to use if *k* is 3\. In this case, the classifier would again assign class
    square to the diamond, because all three of the nearest training samples are squares.
    For the pentagon, two of the three nearest neighbors are triangles and one is
    a square, so it would also again assign class triangle to the pentagon.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我猜您已经注意到 [图 3-2](ch03.xhtml#ch03fig02) 中连接未知样本与三个最近训练样本的线条。这些样本是当 *k* 为 3 时要使用的样本。在这种情况下，分类器仍然会将菱形分为方形类别，因为三个最近的训练样本都是方形。对于五边形，三个最近邻样本中有两个是三角形，一个是方形，因此它也会将五边形分为三角形类别。
- en: This example uses two-dimensional feature vectors, *x*[0] and *x*[1], so we
    can visualize the process. We’re not restricted to models with only two features;
    we can have dozens or even hundreds. The idea of “nearest” (distance) still has
    mathematical meaning even when there are too many features to graph. Indeed, many
    mathematical concepts qualify as distance measures, and in practice, nearest neighbor
    classifiers may use any of the measures depending on the dataset.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本例使用了二维特征向量，*x*[0] 和 *x*[1]，这样我们可以直观地展示这个过程。我们并不局限于仅有两个特征的模型；我们可以使用几十个甚至上百个特征。即使特征太多无法绘制图形，“最近”
    (距离) 的概念仍然具有数学意义。事实上，许多数学概念都可以作为距离度量，在实际应用中，最近邻分类器可能根据数据集使用任何一种度量方法。
- en: For example, let’s return to [Chapter 1](ch01.xhtml)’s MNIST digits dataset.
    The samples are small, grayscale images of the digits 0 through 9 that we unravel
    into vectors of 784 elements. Therefore, each digit sample in the training set
    is a single point in a 784-dimensional space, just as in the previous example
    each sample was a point in a 2-dimensional space.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们回到 [第 1 章](ch01.xhtml) 的 MNIST 数字数据集。样本是小型的灰度数字图像，包含数字 0 到 9，我们将其展开为 784
    个元素的向量。因此，训练集中的每个数字样本都是 784 维空间中的一个点，就像之前的例子中，每个样本都是二维空间中的一个点一样。
- en: The full MNIST dataset has 60,000 training examples, meaning the training space
    consists of 60,000 points scattered throughout the 784-dimensional space (not
    quite, but more on that soon). It also has 10,000 test samples that we can use
    to evaluate the nearest neighbor model. I trained 1-nearest neighbor models using
    all 60,000 training samples, then 6,000 samples, then 600, before ending with
    a mere 60\. Sixty samples in the training set implies about six examples of each
    digit. I say “about” because I sampled the training set randomly, so there might
    be eight of one digit and only three of another. In every case, I tested the model
    using all 10,000 test samples, thereby mimicking using the model in the real world.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的 MNIST 数据集包含 60,000 个训练样本，这意味着训练空间由 60,000 个点组成，分布在 784 维空间中（虽然不是完全如此，但稍后会详细说明）。它还包含
    10,000 个测试样本，我们可以用来评估最近邻模型。我用所有 60,000 个训练样本训练了一个 1-最近邻模型，然后使用 6,000 个样本，再用 600
    个样本，最后只用了 60 个样本。训练集中的 60 个样本大约包含每个数字的六个例子。我说“约”是因为我随机抽样了训练集，所以某个数字可能有八个样本，而另一个数字可能只有三个样本。在每种情况下，我都使用所有
    10,000 个测试样本来测试模型，从而模拟了在现实世界中使用模型的情况。
- en: '[Table 3-1](ch03.xhtml#ch03tab1) shows the model’s performance as the number
    of training examples changed.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 3-1](ch03.xhtml#ch03tab1) 显示了随着训练样本数量变化，模型的表现。'
- en: '**Table 3-1:** Changing the Training Set Size'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 3-1：** 改变训练集大小'
- en: '| **Training set size** | **Accuracy (%)** |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| **训练集大小** | **准确率 (%)** |'
- en: '| --- | --- |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 60,000 | 97 |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 60,000 | 97 |'
- en: '| 6,000 | 94 |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 6,000 | 94 |'
- en: '| 600 | 86 |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 600 | 86 |'
- en: '| 60 | 66 |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 60 | 66 |'
- en: Recall that accuracy is the percentage of the test samples that the model classified
    correctly by assigning the correct digit label, 0 through 9\. When using the entire
    training set the model is correct 97 times out of 100, on average. Even when the
    training set is made 10 times smaller, the accuracy is still 94 percent. With
    600 training examples—about 60 per digit—the accuracy falls to 86 percent. It’s
    only when the training set shrinks to a mere six examples of each digit, on average,
    that the accuracy falls dramatically to 66 percent.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，准确率是指模型通过正确标记数字标签（0到9）正确分类的测试样本的百分比。当使用完整的训练集时，模型平均每100次预测中有97次是正确的。即便将训练集缩小为原来的十分之一，准确率仍然达到94%。当训练样本数为600个（每个数字大约60个样本）时，准确率降至86%。只有当训练集缩小到每个数字仅平均6个样本时，准确率才会急剧下降至66%。
- en: However, before we’re too harsh on our nearest neighbor model, remember that
    there are 10 digit classes, so random guessing will be correct, on average, about
    1 time in 10, for an accuracy of about 10 percent. In this light, even the 60-sample
    model is six times better than guessing randomly. Let’s explore this phenomenon
    a bit to see if we can gain some insight into why the nearest neighbor model does
    well with so little training data.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我们对最近邻模型过于苛刻之前，记住有10个数字类别，因此随机猜测平均大约有1次正确，准确率约为10%。从这个角度来看，即使是60样本模型，也比随机猜测好六倍。让我们稍微探讨一下这个现象，看看能否深入了解为什么最近邻模型在如此少的训练数据下仍然表现良好。
- en: Imagine you’re alone in a basketball arena, sitting in the middle of the court.
    A speck of dust is suspended in the air somewhere in the arena. For convenience,
    the speck stays fixed in its position. Now imagine 59 more specks of dust inhabiting
    the air. Those 60 specks of dust are the 60 digit samples in our training set,
    and the arena is the three-dimensional world in which the digit image vectors
    live.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你独自一人在一个篮球场上，坐在场地中央。场馆的某个地方空中漂浮着一粒尘土。为了方便，假设这粒尘土保持在固定位置。现在，想象另外59粒尘土也漂浮在空中。这60粒尘土就是我们训练集中的60个数字样本，而篮球场则是数字图像向量所在的三维世界。
- en: Now imagine a new speck of dust has appeared right in front of your nose. It’s
    a new digit vector you want to classify. The nearest neighbor model calculates
    the distance between that speck of dust and the 60 specks whose digit labels you
    know. The closest speck of dust to the new one is below the rim of the basket
    you’re facing, at a distance of 47 feet (14 meters). It’s a three, so the model
    returns a label of 3\. Is it reasonable to think that the closest speck represents
    the proper label for the unknown sample? After all, there are only 60 specks of
    dust in the whole arena.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设一粒新的尘土出现在你鼻子前面。它是你想要分类的新数字向量。最近邻模型计算这粒尘土与已知数字标签的60粒尘土之间的距离。距离新尘土最近的是位于你正面对的篮筐边缘下方、47英尺（14米）远的一粒尘土。它标记为3，所以模型返回标签3。认为这粒最接近的尘土代表未知样本的正确标签合理吗？毕竟，整个场地只有60粒尘土。
- en: We need to consider two competing effects to provide a reasonable answer to
    this question. First, we should answer “no” because it seems silly to believe
    that we can represent the giant volume of the arena with 60 specks of dust. There’s
    too little data in the training set to fill the arena’s space. This observation,
    known as the [*curse of dimensionality*](glossary.xhtml#glo25), refers to the
    fact that as the number of dimensions increases, so too, at a very rapid rate,
    does the number of samples needed to fill the space. In other words, the number
    of points increases rapidly, meaning the number of training samples necessary
    to represent the space increases rapidly—exponentially, to be more precise. The
    curse of dimensionality is one of the banes of classical machine learning.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要考虑两个相互竞争的因素，才能合理地回答这个问题。首先，我们应该回答“否”，因为相信我们能用60粒尘土代表这个庞大的场地似乎很荒谬。训练集中的数据太少，无法填满整个场地的空间。这个现象被称为[*维度灾难*](glossary.xhtml#glo25)，指的是随着维度的增加，需要填充空间的样本数也会以非常快的速度增加。换句话说，点的数量迅速增加，这意味着为了表示这个空间，所需的训练样本数量也会迅速增加——更准确地说是呈指数级增加。维度灾难是经典机器学习的痛点之一。
- en: The curse of dimensionality says we should have no hope of properly classifying
    digits when we have only 60 training samples and 784 dimensions . . . yet our
    nearest neighbor classifier still works. Not very well, but better than random
    guessing. Why? The reason has to do with the digits dataset and how similar examples
    of the different classes are to each other. All examples of fives look like a
    5; if they didn’t, we wouldn’t recognize them as fives. Therefore, while there
    are 784 dimensions to the space of digits, most digits in a class will land relatively
    close to that class’s other digits. In other words, the specks of dust representing
    fives are likely clustered or grouped near each other, probably in a thin, tube-like
    region that snakes its way through the arena. The other digits are likely grouped
    similarly. Because of this, the nearest sample has a better chance of being from
    the same digit class than we initially suspected when considering the curse of
    dimensionality. Based on this observation, we upgrade our “no” answer to a wishy-washy
    “probably.”
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 维度灾难意味着，当我们只有 60 个训练样本和 784 个维度时，我们应该不指望能正确分类数字……然而我们的最近邻分类器仍然有效。虽然效果不佳，但比随机猜测要好。为什么？原因与数字数据集以及不同类别之间的样本相似度有关。所有的数字
    5 看起来都像 5；如果它们不是这样，我们就无法识别它们为 5。因此，尽管数字空间有 784 个维度，但同一类别的大多数数字会相对接近该类别的其他数字。换句话说，表示数字
    5 的“尘埃颗粒”可能会聚集在一起，形成一个细长的、弯曲的区域，穿过整个空间。其他数字也可能以类似方式分组。正因为如此，最近的样本比我们最初根据维度灾难所预期的更有可能来自相同的数字类别。基于这一观察，我们将原来的“没有”答案升级为模棱两可的“可能”。
- en: We talk about this effect mathematically by saying that the digit data lies
    on a [*manifold*](glossary.xhtml#glo65) with an effective dimensionality that
    is well below the 784 dimensions of the vectors representing the digits. That
    data often lies on lower-dimensional manifolds is a boon if we can make use of
    that information. The nearest neighbor model uses the information because the
    training data is the model. Later in the book, when we discuss convolutional neural
    networks, we’ll understand that such models learn new ways to represent their
    inputs, which is akin to learning how to represent the lower-dimensional manifold
    on which the data lives.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过数学方式讨论这一效应，称数字数据位于一个具有有效维度的[*流形*](glossary.xhtml#glo65)上，而这个有效维度远低于表示数字的
    784 维向量。如果我们能利用这一信息，数据常常位于较低维度的流形上，这是一个好消息。最近邻模型利用了这一信息，因为训练数据就是模型。稍后在书中，当我们讨论卷积神经网络时，我们将了解到，这类模型学习新的方式来表示输入数据，这类似于学习如何表示数据所在的低维流形。
- en: Before we get too excited about how well our nearest neighbor classifier performs
    with the digits dataset, though, let’s bring ourselves back to reality by attempting
    to classify real images. The CIFAR-10 dataset consists of 50,000 small 32×32-pixel
    color images from 10 different classes, including a mix of vehicles, like airplanes,
    cars, and trucks, and animals, like dogs, cats, and birds. Unraveling each of
    these images creates a vector of 3,072 elements, so we’re asking our classifier
    to separate images in a 3,072-dimensional space. [Table 3-2](ch03.xhtml#ch03tab2)
    shows how it fares.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们过于兴奋于最近邻分类器在数字数据集上的表现之前，先让我们回到现实，尝试对真实图像进行分类。CIFAR-10 数据集包含 50,000 张 32×32
    像素的小型彩色图像，来自 10 个不同类别，包括车辆（如飞机、汽车和卡车）和动物（如狗、猫和鸟）。每张图像展开后会生成一个 3,072 元素的向量，因此我们要求分类器在一个
    3,072 维的空间中区分这些图像。[表 3-2](ch03.xhtml#ch03tab2) 展示了它的表现。
- en: '**Table 3-2:** Classifying CIFAR-10 with Nearest Neighbor'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 3-2：** 使用最近邻分类器对 CIFAR-10 进行分类'
- en: '| **Training set size** | **Accuracy (%)** |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| **训练集大小** | **准确率（%）** |'
- en: '| --- | --- |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 50,000 | 35.4 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 50,000 | 35.4 |'
- en: '| 5,000 | 27.1 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 5,000 | 27.1 |'
- en: '| 500 | 23.3 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 500 | 23.3 |'
- en: '| 50 | 17.5 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 50 | 17.5 |'
- en: As with MNIST, random guessing leads to an accuracy of 10 percent. While our
    classifier performs better than this with all variations of training set size,
    its best accuracy is little more than 35 percent—nowhere near the 97 percent achieved
    with MNIST. Sobering realizations like this led many in the machine learning community
    to lament that generic image classification might be beyond our grasp. Thankfully,
    it isn’t, but none of the classical machine learning models do it well.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 和 MNIST 一样，随机猜测的准确率为 10%。尽管我们的分类器在所有训练集大小的变种中表现得比这更好，但其最佳准确率仅为 35% 多一点——远远低于
    MNIST 达到的 97%。这样的令人清醒的现实让许多机器学习领域的人感叹，通用图像分类可能超出了我们的掌控范围。幸运的是，这并非如此，但没有任何经典的机器学习模型能做到很好。
- en: If we think in terms of manifolds—the idea that data often lives in a lower-dimensional
    space than the dimensionality of the data itself—then these results aren’t surprising.
    CIFAR-10 contains real-world photographs, often referred to as natural images.
    Natural images are far more complex than simple images like MNIST digits, so we
    should expect them to exist in a higher-dimensional manifold and consequently
    be harder to learn to classify. As it happens, there are numerical approaches
    to estimating the true dimensionality of data. For MNIST, even though the images
    live in a 784-dimensional space, the data is closer to 11-dimensional. For CIFAR-10,
    the intrinsic dimensionality is closer to 21 dimensions, so we expect to need
    far more training data to perform on par with MNIST.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们从流形的角度思考——即数据往往存在于一个比数据本身维度更低的空间中——那么这些结果并不令人惊讶。CIFAR-10包含了现实世界的照片，通常被称为自然图像。自然图像比像MNIST数字那样简单的图像复杂得多，因此我们应该预期它们存在于一个高维流形中，因此更难以学习和分类。实际上，有一些数值方法可以估计数据的真实维度。对于MNIST，尽管图像存在于一个784维的空间中，但数据更接近于11维。对于CIFAR-10，本征维度更接近于21维，因此我们预期需要更多的训练数据才能达到与MNIST相当的表现。
- en: Nearest neighbor models aren’t used often these days. Two issues contribute
    to why. First, while training a nearest neighbor model is effectively instantaneous
    because there’s nothing to train, *using* a nearest neighbor model is slow because
    we have to calculate the distance between the unknown sample and each of the training
    set samples. This calculation time grows as the square of the number of samples
    in the training set. The more training data we have, the better we expect the
    model to perform, but the slower it runs. Double the size of the training set,
    and the search time increases by a factor of four.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在最近邻模型不常被使用。造成这种情况的有两个问题。首先，虽然训练最近邻模型实际上是瞬时完成的，因为没有什么需要训练的，*但使用*最近邻模型却很慢，因为我们必须计算未知样本与每个训练样本之间的距离。这个计算时间随着训练集样本数的平方增长。我们拥有的训练数据越多，模型的表现越好，但运行速度却越慢。训练集的大小翻倍，搜索时间增加四倍。
- en: 'Decades of study of nearest neighbor classifiers have uncovered all manner
    of tricks to mitigate the time it takes to find the nearest neighbor, or nearest
    *k* neighbors, but the effect remains: increasing the number of training samples
    increases the time it takes to use the classifier.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 数十年来，对最近邻分类器的研究揭示了许多技巧，用以减少找到最近邻或最近*k*个邻居所需的时间，但问题依然存在：增加训练样本的数量会增加使用分类器所需的时间。
- en: The second issue is common to all classical machine learning models, as well
    as the traditional neural networks we’ll discuss in [Chapter 4](ch04.xhtml). These
    models are holistic, meaning they interpret their input vectors as a single entity
    without parts. This is *not* the right thing to do in many cases. For example,
    writing a four uses multiple strokes, and there are definite parts that distinguish
    the four from an eight. Classical machine learning models don’t explicitly learn
    about these parts or where they appear, or that they might appear in multiple
    locations. Modern convolutional neural networks, however, do learn these things.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个问题是所有经典机器学习模型以及我们将在[第4章](ch04.xhtml)讨论的传统神经网络共同面临的。这些模型是整体的，意味着它们将输入向量视为一个整体，而没有拆解成各个部分。在许多情况下，这种做法是*不正确*的。例如，写一个四需要多个笔画，并且有明显的部分区分四和八。经典机器学习模型并不会明确学习这些部分或它们出现的位置，或者它们可能出现在多个位置。然而，现代卷积神经网络却能学习这些特征。
- en: In sum, nearest neighbor models are straightforward to understand and trivial
    to train, but slow to use and unable to explicitly understand structure in their
    inputs. Let’s change gears to contemplate the forest and the trees.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，最近邻模型易于理解且训练简单，但使用时速度较慢，且无法明确理解输入数据的结构。接下来让我们转变思路，思考森林与树木的问题。
- en: '****'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '****'
- en: We briefly explored decision trees, comprising a series of yes/no questions
    asked about an unknown sample, in [Chapter 1](ch01.xhtml). You begin at the root
    node and traverse the tree by answering the node’s question. If the answer is
    “yes,” move down one level to the left. If the answer is “no,” move down to the
    right. Continue answering questions until you reach a leaf (a node with no question),
    and assign the unknown sample whatever label is in the leaf node.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第1章](ch01.xhtml)中简要探讨了决策树，它由一系列关于未知样本的“是/否”问题组成。你从根节点开始，通过回答节点的问题来遍历树。如果答案是“是”，就往左走一层；如果答案是“否”，就往右走一层。继续回答问题，直到你到达叶子节点（没有问题的节点），并为未知样本分配叶子节点中的标签。
- en: Decision trees are deterministic; once constructed, they don’t change. Therefore,
    traditional decision tree algorithms return the same decision tree for the same
    training set. More often than not, the tree doesn’t work all that well. If that
    happens, is there anything we can do? Yes! We can grow a forest of trees.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是确定性的；一旦构建完成，它们就不会改变。因此，传统的决策树算法对于相同的训练集会返回相同的决策树。通常情况下，这棵树的效果并不是很好。如果发生这种情况，我们能做些什么呢？当然可以！我们可以构建一片树的森林。
- en: But if decision trees are deterministic, won’t the forest be nothing more than
    the same tree, over and over, like a mass of clones? It will, if we don’t do anything
    clever along the way. Fortunately, humans are clever. Researchers realized around
    the year 2000 that introducing randomness produces a forest of unique trees, each
    with its own strengths and weaknesses, but collectively better than any single
    tree. A [*random forest*](glossary.xhtml#glo83) is a collection of decision trees,
    each randomly different from the others. The forest’s prediction is a combination
    of its trees’ predictions. Random forests are a manifestation of the wisdom of
    crowds.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果决策树是确定性的，那么森林不就变成了相同的树一遍又一遍地重复，就像一堆克隆体一样吗？如果我们不采取任何巧妙的措施，确实会这样。幸运的是，人类是聪明的。大约在2000年，研究人员意识到引入随机性可以产生一片独特的树的森林，每棵树都有其独特的优缺点，但集体表现要优于任何单棵树。[
    *随机森林*](glossary.xhtml#glo83)是由决策树组成的集合，每棵树在随机性上都有所不同。森林的预测是其各棵树预测结果的结合。随机森林体现了群体智慧的力量。
- en: Using randomness to build a classifier seems counterintuitive at first. If on
    Tuesday we present the model with sample X and it tells us that sample X is a
    member of class Y, then we don’t want it to tell us that it’s a member of class
    Z if we happen to present the same sample on Saturday. Fortunately, the randomness
    of a random forest doesn’t work that way. Give a trained forest sample X as input,
    and it always gives us class Y as output, even if it’s February 29.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 使用随机性来构建分类器乍一看似乎违反直觉。如果我们在星期二向模型提供样本X，并且它告诉我们样本X属于类别Y，那么如果我们在星期六再次提供相同的样本，我们不希望它告诉我们它属于类别Z。幸运的是，随机森林的随机性并不是这样工作的。给定一个经过训练的森林样本X作为输入，它总是给我们输出类别Y，即使那天是2月29日。
- en: 'Three steps go into growing a random forest: bagging (also called bootstrapping),
    random feature selection, and ensembling. Bagging and random feature selection
    help combat overfitting, a concept mentioned in [Chapter 1](ch01.xhtml). Single
    decision trees are prone to overfitting.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 构建随机森林包含三个步骤：自助法（也称为自举法）、随机特征选择和集成。自助法和随机特征选择有助于对抗过拟合，这是在[第1章](ch01.xhtml)中提到的一个概念。单一决策树容易发生过拟合。
- en: All three steps work together to grow a forest of decision trees whose combined
    outputs produce a (hopefully) better-performing model. Explainability is the price
    paid for this gain in power. A single decision tree explains itself by the series
    of questions and answers that produce its output. With dozens or hundreds of decision
    trees combining their output, explainability goes out the window, but we can live
    with that in many cases.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个步骤协同作用，成长为一个决策树的森林，它们的联合输出生成一个（希望）性能更好的模型。为了获得这种性能提升，我们需要付出可解释性的代价。单一决策树通过一系列问题和答案来解释自己，以产生其输出。而当数十棵或数百棵决策树结合其输出时，可解释性就不复存在了，但在许多情况下我们可以接受这一点。
- en: As I’ve already mentioned several times, the training set is key to conditioning
    the model. This remains true with random forests. We have as a starting point
    a training set. As we grow the forest, decision tree by decision tree, we use
    the existing training set to create tree-specific training sets unique to the
    current decision tree. This is where bagging comes in.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我之前多次提到的，训练集是训练模型的关键。对于随机森林来说，这一点依然成立。我们的起始点是一个训练集。当我们逐步构建森林时，每一棵决策树都是根据已有的训练集生成特定的树训练集，而这些训练集是每棵决策树独有的。这就是bagging发挥作用的地方。
- en: '[*Bagging*](glossary.xhtml#glo12) refers to constructing a new dataset from
    the current dataset by random sampling with replacement. The phrase “with replacement”
    means we might select a training sample more than once or not at all. This technique
    is used in statistics to understand a measurement’s bounds. We’ll use the following
    example dataset of test scores to figure out what that means:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[*Bagging*](glossary.xhtml#glo12)是通过有放回的随机抽样从当前数据集中构建新数据集的方法。术语“有放回”意味着我们可能会多次选择某个训练样本，或者根本不选择它。该技术在统计学中用于理解测量的边界。我们将使用以下的考试成绩数据集来解释这是什么意思：'
- en: 95, 88, 76, 81, 92, 70, 86, 87, 72
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 95, 88, 76, 81, 92, 70, 86, 87, 72
- en: One way to assess a class’s performance on the test is to calculate the average
    score by taking the sum of all the scores divided by the number of scores. The
    sum is 747, and there are 9 scores, giving us an average of 83.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 评估班级在测试中的表现的一种方法是通过计算所有分数的和除以分数的数量来得出平均分。总和是747，分数有9个，因此平均分为83。
- en: Collectively, the test scores are a sample from a mythical parent process that
    generates test scores for the particular test taken. This isn’t a common way to
    think about test scores, but it’s a machine learning way to think about what a
    dataset represents. The test scores from another group of students represent another
    sample from the parent process for this test. If we have many classes’ worth of
    test scores, we can get an idea about the true average test score, or at least
    the range over which we expect to find that average score, with a high degree
    of confidence.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，考试成绩是来自一个虚构的母体过程的样本，这个母体过程为所做的特定考试生成成绩。这并不是思考考试成绩的常见方式，但它是机器学习中理解数据集代表什么的一种方式。来自另一组学生的考试成绩是这个考试母体过程的另一个样本。如果我们有多个班级的考试成绩，我们就可以非常有把握地了解真实的平均分，或者至少了解我们期望找到该平均分的范围。
- en: 'We could give the test to many different classes to get multiple average scores,
    one per class, but instead we’ll use bagging to create new datasets from the collection
    of test scores we do have and look at their averages. To do that, we pick values
    from the collection of test scores at random, not caring if we’ve already picked
    this particular score or never pick that one. Here are six such bootstrapped datasets:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将测试分发给多个班级，从而得到多个班级的平均分，但我们会使用bagging从已有的考试成绩数据集中创建新数据集，并查看它们的平均值。为此，我们从考试成绩集合中随机选择值，无论之前是否已经选择过该分数，或者根本没有选择过某个分数。以下是六个这样通过自助抽样生成的数据集：
- en: 86, 87, 87, 76, 81, 81, 88, 70, 95
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 86, 87, 87, 76, 81, 81, 88, 70, 95
- en: 87, 92, 76, 87, 87, 76, 87, 92, 92
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 87, 92, 76, 87, 87, 76, 87, 92, 92
- en: 95, 70, 87, 92, 70, 92, 72, 70, 72
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 95, 70, 87, 92, 70, 92, 72, 70, 72
- en: 88, 86, 87, 70, 81, 72, 86, 95, 70
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 88, 86, 87, 70, 81, 72, 86, 95, 70
- en: 86, 86, 92, 86, 87, 86, 70, 81, 87
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 86, 86, 92, 86, 87, 86, 70, 81, 87
- en: 76, 88, 88, 88, 88, 72, 86, 95, 70
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 76, 88, 88, 88, 88, 72, 86, 95, 70
- en: The respective averages of each are 83.4, 86.2, 80.0, 81.7, 84.6, and 83.4 percent.
    The lowest is 80.0 percent, and the highest is 86.2 percent. This gives us some
    reason to believe that a large number of samples will produce an average more
    or less in that range.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 每个数据集的平均值分别为83.4、86.2、80.0、81.7、84.6和83.4百分比。最低值是80.0百分比，最高值是86.2百分比。这让我们有理由相信，样本数量较大时，平均值大致会落在这个范围内。
- en: This is how a statistician might use bagging. For us, the critical part is the
    six new datasets bootstrapped from the original dataset. When growing a random
    forest, every time we need a new decision tree, we’ll first use bagging to produce
    a new dataset, then train the decision tree using that dataset, not the original.
    Notice that many of the six datasets have repeated values. For example, dataset
    1 used both 81 and 87 twice, but never 72\. This randomization of the given dataset
    helps create decision trees that behave differently from one another yet are aligned
    with what the original dataset represents.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是统计学家可能如何使用自助法（bagging）。对我们来说，关键部分是从原始数据集生成的六个新数据集。构建随机森林时，每当我们需要一个新的决策树时，我们首先使用自助法生成一个新数据集，然后使用该数据集训练决策树，而不是使用原始数据集。请注意，这六个数据集中的许多都有重复的值。例如，数据集
    1 中同时使用了 81 和 87 两次，但从未使用过 72。对给定数据集的这种随机化帮助创建了彼此行为不同的决策树，但它们仍与原始数据集所代表的内容保持一致。
- en: The second trick a random forest uses is to train the decision tree on a randomly
    selected set of features. Let’s use the toy dataset in [Table 3-3](ch03.xhtml#ch03tab3)
    to understand what that means. As always, each row is a feature vector, a sample
    for which we know the proper class label. The columns are the values of that feature
    for each sample.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林使用的第二个技巧是基于随机选择的特征集训练决策树。让我们使用 [表 3-3](ch03.xhtml#ch03tab3) 中的玩具数据集来理解这意味着什么。像往常一样，每一行是一个特征向量，是一个我们知道正确类别标签的样本。列是每个样本的特征值。
- en: '**Table 3-3:** A Toy Dataset'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 3-3：** 一个玩具数据集'
- en: '| **#** | ***x***[**0**] | ***x***[**1**] | ***x***[**2**] | ***x***[**3**]
    | ***x***[**4**] | ***x***[**5**] |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| **#** | ***x***[**0**] | ***x***[**1**] | ***x***[**2**] | ***x***[**3**]
    | ***x***[**4**] | ***x***[**5**] |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| **1** | 0.52 | 0.95 | 0.81 | 0.78 | 0.97 | 0.36 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 0.52 | 0.95 | 0.81 | 0.78 | 0.97 | 0.36 |'
- en: '| **2** | 0.89 | 0.37 | 0.66 | 0.55 | 0.75 | 0.45 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 0.89 | 0.37 | 0.66 | 0.55 | 0.75 | 0.45 |'
- en: '| **3** | 0.49 | 0.98 | 0.49 | 0.39 | 0.42 | 0.24 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| **3** | 0.49 | 0.98 | 0.49 | 0.39 | 0.42 | 0.24 |'
- en: '| **4** | 0.43 | 0.51 | 0.90 | 0.78 | 0.19 | 0.22 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| **4** | 0.43 | 0.51 | 0.90 | 0.78 | 0.19 | 0.22 |'
- en: '| **5** | 0.51 | 0.16 | 0.11 | 0.48 | 0.34 | 0.54 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| **5** | 0.51 | 0.16 | 0.11 | 0.48 | 0.34 | 0.54 |'
- en: '| **6** | 0.48 | 0.99 | 0.62 | 0.58 | 0.72 | 0.42 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| **6** | 0.48 | 0.99 | 0.62 | 0.58 | 0.72 | 0.42 |'
- en: '| **7** | 0.80 | 0.84 | 0.72 | 0.26 | 0.93 | 0.23 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| **7** | 0.80 | 0.84 | 0.72 | 0.26 | 0.93 | 0.23 |'
- en: '| **8** | 0.50 | 0.70 | 0.13 | 0.35 | 0.96 | 0.82 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| **8** | 0.50 | 0.70 | 0.13 | 0.35 | 0.96 | 0.82 |'
- en: '| **9** | 0.70 | 0.54 | 0.62 | 0.72 | 0.14 | 0.53 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| **9** | 0.70 | 0.54 | 0.62 | 0.72 | 0.14 | 0.53 |'
- en: What does this dataset represent? I have no idea; it’s made up. My cheeky answer
    is a good reminder that machine learning models don’t understand what their datasets
    represent. They process numbers without context. Is it a pixel value? The number
    of square feet in a house? The crime rate of a county per 100,000 people? It doesn’t
    matter to the machine learning model—it’s all just numbers.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集代表了什么？我不知道，它是虚构的。我的调皮回答是一个很好的提醒，即机器学习模型并不理解它们的数据集代表什么。它们处理的是没有上下文的数字。它是一个像素值吗？房屋的平方英尺数？每十万人中的犯罪率？对于机器学习模型来说，这些都不重要——它们不过是数字而已。
- en: This toy dataset consists of nine feature vectors, each with six features, *x*[0]
    through *x*[5]. The forest’s decision trees use a randomly selected subset of
    the six features. For example, say we randomly keep features *x*[0], *x*[4], and
    *x*[5]. [Table 3-4](ch03.xhtml#ch03tab4) shows the dataset now used to train the
    decision tree.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这个玩具数据集包含九个特征向量，每个特征向量有六个特征，*x*[0] 到 *x*[5]。森林的决策树使用从六个特征中随机选择的子集。例如，假设我们随机选择保留特征
    *x*[0]、*x*[4] 和 *x*[5]。 [表 3-4](ch03.xhtml#ch03tab4) 展示了现在用于训练决策树的数据集。
- en: '**Table 3-4:** A Random Collection of Features'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 3-4：** 一个随机选择的特征集'
- en: '| **#** | ***x***[**0**] | ***x***[**4**] | ***x***[**5**] |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| **#** | ***x***[**0**] | ***x***[**4**] | ***x***[**5**] |'
- en: '| --- | --- | --- | --- |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **1** | 0.52 | 0.97 | 0.36 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 0.52 | 0.97 | 0.36 |'
- en: '| **2** | 0.89 | 0.75 | 0.45 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 0.89 | 0.75 | 0.45 |'
- en: '| **3** | 0.49 | 0.42 | 0.24 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| **3** | 0.49 | 0.42 | 0.24 |'
- en: '| **4** | 0.43 | 0.19 | 0.22 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| **4** | 0.43 | 0.19 | 0.22 |'
- en: '| **5** | 0.51 | 0.34 | 0.54 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| **5** | 0.51 | 0.34 | 0.54 |'
- en: '| **6** | 0.48 | 0.72 | 0.42 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| **6** | 0.48 | 0.72 | 0.42 |'
- en: '| **7** | 0.80 | 0.93 | 0.23 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| **7** | 0.80 | 0.93 | 0.23 |'
- en: '| **8** | 0.50 | 0.96 | 0.82 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| **8** | 0.50 | 0.96 | 0.82 |'
- en: '| **9** | 0.70 | 0.14 | 0.53 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| **9** | 0.70 | 0.14 | 0.53 |'
- en: Each decision tree in the forest has been trained on a bootstrapped version
    of the dataset using only a subset of the available features. We’ve used randomness
    twice to grow a forest of trees that are all subtly different from each other,
    in both what data they’re trained on and which features they pay attention to.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 森林中的每棵决策树都是在使用数据集的自助采样版本上训练的，并且只使用了部分可用特征。我们已经通过两次引入随机性，生长出了一片树木，彼此之间在训练数据和关注的特征上都有细微的不同。
- en: 'Now that we have a forest, how do we use it? Enter the last of the three pieces:
    ensembling. Musically, an ensemble is a collection of musicians playing diverse
    instruments. The random forest is also an ensemble, with each decision tree a
    different musician playing a different instrument.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了一片森林，接下来该如何使用它呢？这就是集成方法的最后一环：集成。音乐中的合奏是由演奏不同乐器的音乐家组成。同样，随机森林也是一个合奏，每棵决策树就像一个不同的音乐家，演奏着不同的乐器。
- en: A musical ensemble produces a single output, the music, by combining the notes
    played by each instrument. Likewise, a random forest produces a single output,
    a class label, by combining the labels produced by each decision tree, typically
    by voting like a *k*-nearest neighbors classifier. We assign the winning label
    to the input.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 一个音乐合奏通过结合每个乐器演奏的音符来产生单一的输出——音乐。同样，随机森林通过结合每棵决策树生成的标签来产生单一的输出——类别标签，通常通过类似 *k*
    最近邻分类器的投票方式。我们将获得最多票数的标签分配给输入。
- en: For example, if we want to use the random forest to classify sample X, and there
    are 100 trees in the random forest (already trained), we give each tree sample
    X. The trees know which subsets of sample X’s features to use to arrive at a leaf
    with a label. We now have 100 possible class labels, the output from the forest’s
    100 decision trees. If 78 of the trees assign sample X to class Y, the random
    forest proclaims sample X to be an instance of class Y.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们想用随机森林来分类样本 X，且随机森林中有 100 棵树（已经训练过），我们将样本 X 交给每棵树。每棵树知道应该使用样本 X 特征的哪些子集来得出一个带标签的叶子节点。现在，我们有
    100 个可能的类别标签，这些标签来自森林中的 100 棵决策树。如果有 78 棵树将样本 X 分配给类别 Y，随机森林就会宣布样本 X 为类别 Y 的一个实例。
- en: The random assignment of features to trees, combined with bootstrapped datasets
    and ensemble voting, gives a random forest its power. Ensembling is an intuitively
    attractive idea that isn’t restricted to random forests. Nothing stops us from
    training multiple model types on the same dataset and then combining their predictions
    in some way to arrive at a joint conclusion about an input sample. Each of the
    models will have its own strengths and weaknesses. When combined, the strengths
    tend to enhance the output quality, making the sum greater than the parts.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 特征随机分配给树，结合自助采样数据集和集成投票，赋予了随机森林其强大的能力。集成方法是一个直观吸引人的想法，不仅限于随机森林。我们完全可以训练多个模型类型在同一数据集上，然后以某种方式结合它们的预测，得出对输入样本的联合结论。每个模型都有其自己的优缺点，组合后，优点通常会增强输出质量，使总和大于部分之和。
- en: We have one more classical machine learning model to investigate, the support
    vector machine (SVM). After that, we’ll pit the models against each other to gain
    intuition about how they behave and provide a baseline against which we can compare
    the performance of neural networks.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还有一个经典的机器学习模型需要研究，即支持向量机（SVM）。之后，我们将把这些模型相互对比，以便直观地了解它们的表现，并提供一个基准，便于我们将神经网络的表现与之对比。
- en: '****'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '****'
- en: 'To understand support vector machines is to understand four concepts: margins,
    support vectors, optimization, and kernels. The math is a bit hairy, even for
    math people, but we’ll set that aside and focus instead on gaining a conceptual
    understanding.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解支持向量机，首先需要理解四个概念：边界、支持向量、优化和核函数。数学稍显复杂，即便是数学专业的人也会觉得有点难，但我们先不深入其中，而是专注于获得一个概念上的理解。
- en: Support vector machines are best understood visually, so we’ll begin with the
    example toy dataset in [Figure 3-3](ch03.xhtml#ch03fig03). This is a two-class
    dataset (circles and squares) with two-dimensional feature vectors, features *x*[0]
    and *x*[1].
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机最好通过可视化来理解，因此我们将从 [图 3-3](ch03.xhtml#ch03fig03) 中的玩具数据集开始。这个数据集是一个二分类数据集（圆形和方形），包含二维特征向量，特征为
    *x*[0] 和 *x*[1]。
- en: '![Image](../images/ch03fig03.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch03fig03.jpg)'
- en: '*Figure 3-3: A two-class toy dataset with two features, x[0] and x[1]*'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3-3：一个具有两个特征的二类玩具数据集，特征为 x[0] 和 x[1]*'
- en: A classifier for this dataset is straightforward to construct because a line
    easily separates the dataset by class, with all the squares above it and to the
    right and all the circles below and to the left. But where should it go? There
    are an infinite number of lines that we might use. For example, we might pass
    the line just below all the squares. That line separates the classes, but if we
    encounter a sample from class square that lands just below the line when we use
    the classifier, we’ll make a mistake and assign the sample to class circle because
    it’s below the line we declared separates the classes. Similarly, if we place
    the line just above all the circles, we might call a new sample that’s actually
    a circle a square because it landed slightly above that line.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个数据集，构建一个分类器是相对简单的，因为一条线就能轻松将数据集按类别分开，所有的正方形都在它的上方和右侧，所有的圆形都在它的下方和左侧。但是这条线应该放在哪里呢？我们可能用无数条线来实现这一目标。例如，我们可以将线放置在所有正方形的下方。那条线确实能够分隔这些类别，但如果我们遇到一个来自正方形类别的样本，它正好落在这条线的下方，那么当我们使用分类器时就会出错，将该样本归类为圆形，因为它在我们设定的分隔线下方。类似地，如果我们将线放在所有圆形的上方，那么我们可能会把一个新样本误判为正方形，即便它实际上是圆形，因为它稍微位于那条线的上方。
- en: Given what we know based on the training data, we should place the separating
    line as far from each group as possible. Here’s where the concept of a margin
    comes into play. SVMs seek to maximize the margin between the two groups, meaning
    finding the place with the widest separation between classes. When they have the
    maximum margin, they place the boundary, here a line, in the middle of the margin
    because that’s the most sensible thing to do based on the information contained
    in the training data.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们从训练数据中了解到的情况，我们应该尽可能将分隔线放置在每个组的最远位置。这时，边界的概念就变得重要。SVM旨在最大化两个组之间的边界，即找到类别之间最宽的分隔位置。当它们有最大边界时，它们将边界放在边界的中间，因为这是基于训练数据中信息最合适的做法。
- en: '[Figure 3-4](ch03.xhtml#ch03fig04) shows the training data with three additional
    lines. The dashed lines define the margin, and the heavy continuous line marks
    the boundary placed by the SVM to maximize the distance between classes. This
    is the best position for the line to minimize labeling errors between the two
    classes. This, in a nutshell, is all an SVM does.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3-4](ch03.xhtml#ch03fig04)展示了训练数据和三条额外的线。虚线定义了边界，粗实线标记了SVM设置的边界，以最大化两个类别之间的距离。这是最适合的线位置，能够最小化两个类别之间的标签错误。从本质上来说，这就是SVM所做的一切。'
- en: '![Image](../images/ch03fig04.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch03fig04.jpg)'
- en: '*Figure 3-4: The maximal margin separating line (heavy) and maximum margins
    (dashed)*'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '*图3-4：最大边界分隔线（粗线）和最大边界（虚线）*'
- en: The three other parts of an SVM—support vectors, optimization, and kernels—are
    used to find the margins and the separating line. In [Figure 3-4](ch03.xhtml#ch03fig04),
    notice that the dashed lines pass through some of the data points. These points
    are the support vectors that the algorithm finds to define the margin. Where do
    those support vectors come from? Recall that the figure’s points represent specific
    feature vectors in the training set. Support vectors are members of the training
    set found via an optimization algorithm. Optimization involves finding the best
    of something according to some criteria. The optimization algorithm used by an
    SVM locates the support vectors that define the maximum margin and, ultimately,
    the separating line. In [Chapter 1](ch01.xhtml), we used an optimization algorithm
    when we discussed fitting data to a curve, and we’ll use one again when training
    neural networks.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: SVM的另外三个部分——支持向量、优化和核函数——用于寻找边界和分隔线。在[图3-4](ch03.xhtml#ch03fig04)中，可以看到虚线穿过一些数据点。这些点是支持向量，算法通过它们来定义边界。这些支持向量来自哪里？回想一下，图中的点代表训练集中的特征向量。支持向量是通过优化算法在训练集中找到的成员。优化是根据某些标准寻找最佳解的过程。SVM使用的优化算法定位定义最大边界的支持向量，最终确定分隔线。在[第1章](ch01.xhtml)中，我们在讨论将数据拟合到曲线时使用了优化算法，当我们训练神经网络时也会再次使用优化算法。
- en: 'We’re almost there; we have only one SVM concept remaining: kernels. As opposed
    to the popcorn variety or the kernel at the heart of your computer’s operating
    system, mathematical kernels relate two things—here, two feature vectors. The
    example in [Figure 3-4](ch03.xhtml#ch03fig04) uses a linear kernel, meaning it
    uses the training data feature vectors as they are. Support vector machines admit
    many kinds of kernels to relate two feature vectors, but the linear kernel is
    the most common. Another kind, called a Gaussian kernel (or, even more verbose
    and impressive, a radial basis function kernel), often helps in situations where
    the linear kernel fails because the feature vectors are in a different kind of
    relationship to each other.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们快到了；我们只剩下一个SVM概念：核函数。与爆米花的品种或计算机操作系统核心中的内核不同，数学核函数是用来关联两件事物——这里是两个特征向量。[图
    3-4](ch03.xhtml#ch03fig04)中的示例使用了线性核函数，这意味着它使用训练数据的特征向量原样。支持向量机接受多种类型的核函数来关联两个特征向量，但线性核函数是最常见的。另一种类型，称为高斯核函数（或者更为冗长且令人印象深刻的径向基函数核），通常有助于在线性核函数失败的情况下，因为特征向量之间存在不同的关系。
- en: The kernel transforms the feature vectors into a different representation, an
    idea central to what convolutional neural networks do. One of the issues that
    made classical machine learning stumble for so long is that the data supplied
    to the models was too complex in its raw form for the model to make meaningful
    distinctions between classes. This is related to the idea of manifolds and intrinsic
    dimensionality introduced in our discussion of nearest neighbors.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 核函数将特征向量转换为不同的表示，这是卷积神经网络所做的核心思想之一。传统机器学习之所以长时间陷入困境的一个问题是，提供给模型的数据在原始形式下过于复杂，无法使模型在不同类别之间做出有意义的区分。这与我们在讨论最近邻时介绍的流形和内在维度的概念相关。
- en: Classical machine learning practitioners spent considerable effort trying to
    minimize the number of features needed by a model, paring the features down to
    the minimal set necessary for the model to distinguish between classes. This approach
    was termed *feature selection* or *dimensionality reduction*, depending on the
    algorithm used. Similarly, especially with SVMs, they used kernels to map the
    given feature vectors to a new representation, making separating classes easier.
    These approaches were human-led endeavors; we selected the features or the kernels
    in the hopes that they’d make the problem more manageable. But, as we’ll learn,
    modern deep learning lets the data speak for itself when learning new representations
    of the information the data contains.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的机器学习从业者花费了大量精力试图最小化模型所需的特征数量，将特征减少到模型区分不同类别所必需的最小集合。这种方法被称为*特征选择*或*维度减少*，具体取决于所使用的算法。类似地，尤其是在支持向量机（SVM）中，使用核函数将给定的特征向量映射到一个新的表示，使得类别分离变得更加容易。这些方法是人为主导的努力；我们选择特征或核函数，希望它们能够使问题更容易处理。但正如我们将要学习的那样，现代深度学习让数据自己发声，在学习数据包含的信息的新表示时，数据本身就能起到决定性作用。
- en: In practice, training a support vector machine means locating good values for
    the parameters related to the kernel used. If the kernel is linear, as in the
    previous example, there’s only one value to find, universally called *C*. It’s
    a number, like 1 or 10, affecting how well the support vector machine performs.
    If using the Gaussian kernel, we have *C* and another parameter, known by the
    Greek letter *γ* (gamma). The art of training an SVM involves finding the magic
    values that work best for the dataset at hand.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，训练支持向量机意味着为与所用核函数相关的参数找到合适的值。如果核函数是线性的，如前面的示例中所示，则只需找到一个值，通常称为*C*。这是一个数字，例如1或10，它影响支持向量机的表现。如果使用高斯核函数，我们有*C*和另一个参数，用希腊字母*γ*（gamma）表示。训练SVM的艺术就在于找到最适合当前数据集的最佳值。
- en: The magic values used by a model are its [*hyperparameters*](glossary.xhtml#glo55).
    Neural networks have many hyperparameters; even more than SVMs. However, my experience
    has taught me that it’s often easier to tune a neural network—especially a modern
    deep neural network—than a support vector machine. I freely confess my bias here;
    others might disagree.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 模型使用的魔法值是其[*超参数*](glossary.xhtml#glo55)。神经网络有许多超参数；比SVM还要多。然而，我的经验告诉我，调整神经网络—特别是现代深度神经网络—通常比调整支持向量机更容易。我在这里坦率地承认我的偏见；其他人可能会不同意。
- en: Support vector machines are mathematically elegant, and practitioners use that
    elegance to tweak the hyperparameters and the kernel used, along with a suite
    of old-school data preparation approaches, to construct a well-performing model
    that works well on data in the wild. Every step of this process relies on the
    intuition and experience of the human building the model. If they’re knowledgeable
    and experienced, they’ll likely succeed if the dataset is amenable to such a model,
    but success isn’t assured. On the other hand, deep neural networks are big, kind
    of clunky, and live or die by the raw data they’re fed. That said, by coming to
    the problem with a minimal set of assumptions, neural networks can generalize
    over elements of the dataset that humans cannot fathom, which I think is often
    why modern neural networks can do what was previously believed to be next to impossible.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机在数学上非常优雅，实践者利用这种优雅来调整超参数和所用的核函数，并结合一套传统的数据预处理方法，构建出一个在实际数据中表现良好的模型。这个过程的每一步都依赖于构建模型的人类的直觉和经验。如果他们具有足够的知识和经验，并且数据集适合使用这种模型，他们通常会成功，但成功并不保证。另一方面，深度神经网络虽然庞大、略显笨重，并且完全依赖于输入的原始数据，但通过最小化假设，它们能够对数据集中的一些元素进行泛化，这些元素是人类无法理解的，我认为这也是现代神经网络能够完成以前认为几乎不可能完成的任务的原因。
- en: 'SVMs are binary classifiers: they distinguish between two classes, as in the
    dataset in [Figure 3-3](ch03.xhtml#ch03fig03). But sometimes we need to distinguish
    between more than two classes. How can we do that with an SVM?'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机（SVM）是二分类器：它们区分两个类别，如[图 3-3](ch03.xhtml#ch03fig03)中的数据集所示。但有时我们需要区分超过两个类别。我们如何用支持向量机做到这一点呢？
- en: We have two options for generalizing SVMs to multiclass problems. Assume we
    have 10 classes in the dataset. The first generalization approach trains 10 SVMs,
    the first of which attempts to separate class 0 from the other nine classes. The
    second likewise attempts to separate class 1 from the remaining nine, and so on,
    giving us a collection of models, each trying to separate one class from all the
    others. To classify an unknown sample, we give the sample to each SVM and return
    the class label of the model that produced the largest decision function value—the
    [*metric*](glossary.xhtml#glo66), or measurement, the SVM uses to decide its confidence
    in its output. This option is known as [*one-versus-rest*](glossary.xhtml#glo78)
    or *one-versus-all*. It trains as many SVMs as there are classes.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两种方法可以将支持向量机推广到多类别问题。假设我们有10个类别的数据集。第一种推广方法训练10个支持向量机，第一个尝试将类别0与其他9个类别分开。第二个同样尝试将类别1与其余9个类别分开，依此类推，得到一组模型，每个模型都试图将一个类别与其他所有类别分开。要对一个未知样本进行分类，我们将样本传递给每个支持向量机，并返回具有最大决策函数值的模型的类别标签——该值是SVM用来决定其输出置信度的[*度量*](glossary.xhtml#glo66)，或称为测量。这个方法被称为[*一对多*](glossary.xhtml#glo78)或*一对其余*。它训练的支持向量机数量等于类别数量。
- en: The other option is [*one-versus-one*](glossary.xhtml#glo77), which trains a
    separate SVM for each possible pair of classes. The unknown sample is given to
    each model, and the class label that shows up most often is assigned to it. One-versus-one
    isn’t practical if the number of classes becomes too large. For example, for the
    10 classes in CIFAR-10, we’d need 45 different SVM machines. And if we tried this
    approach with the 1,000 classes in the ImageNet dataset, we’d be waiting a long
    time for the 499,500 different SVMs to train.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是[*一对一*](glossary.xhtml#glo77)，它为每一对类别训练一个单独的支持向量机。将未知样本传递给每个模型，最终分配给出现次数最多的类别标签。如果类别数量过多，一对一方法就不太实际。例如，对于CIFAR-10中的10个类别，我们需要45个不同的支持向量机。而如果我们尝试在ImageNet数据集（有1000个类别）上使用这种方法，那么我们将需要等待499,500个不同的支持向量机训练完成。
- en: Support vector machines were well suited to the computing power commonly available
    in the 1990s and early 2000s, which is why they held neural networks at bay for
    so long. However, with the advent of deep learning, there’s little reason to resort
    to an SVM (in my opinion).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机非常适合90年代和2000年代初期常见的计算能力，这也是它们能长时间压制神经网络的原因。然而，随着深度学习的出现，使用支持向量机的理由已经不多了（在我看来）。
- en: '****'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '****'
- en: Let’s test the three classical models explored in this chapter using an open
    source dataset consisting of dinosaur footprint outlines that comes from the 2022
    paper “A Machine Learning Approach for the Discrimination of Theropod and Ornithischian
    Dinosaur Tracks” by Jens N. Lallensack, Anthony Romilio, and Peter L. Falkingham.
    The footprint images were released under the Creative Commons CC BY 4.0 license,
    which allows reuse with attribution.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用本章探讨的三个经典模型，测试一个开源数据集，该数据集包含来自 2022 年 Jens N. Lallensack、Anthony Romilio
    和 Peter L. Falkingham 论文《A Machine Learning Approach for the Discrimination of
    Theropod and Ornithischian Dinosaur Tracks》中恐龙足迹轮廓的样本。足迹图像已根据创作共用协议 CC BY 4.0
    许可证发布，允许带有署名的重用。
- en: '[Figure 3-5](ch03.xhtml#ch03fig05) contains samples from the dataset. Theropod
    footprints (think *T. rex*) are in the top row, and ornithischian footprints (think
    duckbilled dinos like hadrosaurs) are at the bottom. The images used by the models
    were inverted to be white on a black background, rescaled to 40×40 pixels, and
    unraveled to become 1,600-dimensional vectors. The dataset is small by modern
    standards, with 1,336 training samples and 335 test samples.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-5](ch03.xhtml#ch03fig05) 包含了数据集中的样本。兽脚类恐龙足迹（比如*霸王龙*）位于上排，鸟臀目恐龙足迹（比如鸭嘴龙等）位于下排。模型使用的图像被反转，背景为黑色，图像为白色，重新缩放为
    40×40 像素，并展开成 1,600 维的向量。按照现代标准，这个数据集相对较小，包含 1,336 个训练样本和 335 个测试样本。'
- en: '![Image](../images/ch03fig05.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch03fig05.jpg)'
- en: '*Figure 3-5: Theropod (top) and ornithischian (bottom) footprints*'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3-5：兽脚类（上）和鸟臀类（下）恐龙足迹*'
- en: 'I trained the following models:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我训练了以下模型：
- en: Nearest neighbor (*k* = 1, 3, 7)
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最近邻（*k* = 1, 3, 7）
- en: A random forest with 300 trees
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个包含 300 棵树的随机森林
- en: A linear support vector machine
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性支持向量机
- en: A radial basis function support vector machine
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 径向基函数支持向量机
- en: After training, I tested the models with the held-out test set. I also timed
    how long it took to train each model and to test each model after training. Using
    a model after training is [*inference*](glossary.xhtml#glo57), meaning I tracked
    the inference time on the test set.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，我用保留的测试集对模型进行了测试。我还记录了每个模型训练所需的时间，以及训练后测试每个模型所需的时间。训练后使用模型进行推理是[**推理**](glossary.xhtml#glo57)，这意味着我跟踪了在测试集上的推理时间。
- en: '**NOTE**'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '**注**'
- en: '*This isn’t a programming book, but if you’re familiar with programming, especially
    Python, feel free to contact me at* [rkneuselbooks@gmail.com](mailto:rkneuselbooks@gmail.com)
    *and I’ll send you the dataset and code.*'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '*这不是一本编程书籍，但如果你熟悉编程，尤其是 Python，可以随时联系我，邮箱是* [rkneuselbooks@gmail.com](mailto:rkneuselbooks@gmail.com)
    *，我会把数据集和代码发给你。*'
- en: '[Table 3-5](ch03.xhtml#ch03tab5) shows the results. Evaluating how well a model
    works is, as you might expect, a critical component of the machine learning process.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 3-5](ch03.xhtml#ch03tab5) 显示了结果。评估模型的表现，如你所料，是机器学习过程中至关重要的一部分。'
- en: '**Table 3-5:** Classifying Dinosaur Footprints'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 3-5：** 恐龙足迹分类'
- en: '| **Model** | **ACC** | **MCC** | **Train** | **Test** |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **ACC** | **MCC** | **训练** | **测试** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| RF300 | 83.3 | 0.65 | 1.5823 | 0.0399 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| RF300 | 83.3 | 0.65 | 1.5823 | 0.0399 |'
- en: '| RBF SVM | 82.4 | 0.64 | 0.9296 | 0.2579 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| RBF SVM | 82.4 | 0.64 | 0.9296 | 0.2579 |'
- en: '| 7-NN | 80.0 | 0.58 | 0.0004 | 0.0412 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 7-NN | 80.0 | 0.58 | 0.0004 | 0.0412 |'
- en: '| 3-NN | 77.6 | 0.54 | 0.0005 | 0.0437 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 3-NN | 77.6 | 0.54 | 0.0005 | 0.0437 |'
- en: '| 1-NN | 76.1 | 0.50 | 0.0004 | 0.0395 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 1-NN | 76.1 | 0.50 | 0.0004 | 0.0395 |'
- en: '| Linear SVM | 70.7 | 0.41 | 2.8165 | 0.0007 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 线性支持向量机 | 70.7 | 0.41 | 2.8165 | 0.0007 |'
- en: 'The first column on the left identifies the model: from top to bottom, random
    forest, radial basis function support vector machine, nearest neighbors (with
    7, 3, and 1 neighbor), and linear support vector machine.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧第一列标识了模型：从上到下分别是随机森林、径向基函数支持向量机、最近邻（7、3 和 1 个邻居）以及线性支持向量机。
- en: The ACC and MCC columns are metrics calculated from the confusion matrix, the
    single most crucial part of the machine learning practitioner’s toolbox when evaluating
    a model (see [Chapter 1](ch01.xhtml)). For binary classifiers like the ones we
    have here, the confusion matrix counts the number of times a theropod test sample
    was correctly identified, the same for ornithischian test samples, and the number
    of times one was confused for the other.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ACC 和 MCC 列是从混淆矩阵计算得出的指标，混淆矩阵是机器学习从业者评估模型时最为关键的工具之一（见[第 1 章](ch01.xhtml)）。对于像我们这里的二分类器，混淆矩阵统计了兽脚类测试样本被正确识别的次数、鸟臀类测试样本的识别次数，以及它们相互混淆的次数。
- en: 'Visually, the confusion matrix for a binary model looks like this:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 从视觉上看，二分类模型的混淆矩阵如下所示：
- en: '|  | **Ornithischian** | **Theropod** |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '|  | **鸟脚类** | **兽脚类** |'
- en: '| --- | --- | --- |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Ornithischian** | TN | FP |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| **鸟脚类** | TN | FP |'
- en: '| **Theropod** | FN | TP |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| **兽脚类** | FN | TP |'
- en: 'The rows are the actual class label from the held-out test set. The columns
    are the labels assigned by the models. The cells are the counts of the number
    of times each combination of actual label and model-assigned label happened. The
    letters are the standard way to refer to what the numbers in the cells mean: TN
    is [*true negative*](glossary.xhtml#glo98), TP is [*true positive*](glossary.xhtml#glo99),
    FP is *false positive*, and FN is *false negative*. For the dinosaur footprint
    models, theropod is class 1, the “positive” class, making ornithischian class
    0, or the “negative” class.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这些行是来自验证测试集的实际类别标签。列是模型分配的标签。单元格是每种实际标签和模型分配标签组合发生的次数。字母是标准方式，用于表示单元格中数字的含义：TN
    是 [*真负*](glossary.xhtml#glo98)，TP 是 [*真阳性*](glossary.xhtml#glo99)，FP 是 *假阳性*，FN
    是 *假阴性*。对于恐龙足迹模型，兽脚类是类别 1，“正类”，使鸟脚类成为类别 0，或“负类”。
- en: The number of times the model called an ornithischian footprint “ornithischian”
    is the TN count. Similarly, the TP count represents the number of times the model
    was right about a theropod footprint. The goal is to get TN and TP as high as
    possible while making FP and FN, the mistakes, as low as possible.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 模型将鸟脚类足迹标记为“鸟脚类”的次数就是 TN 计数。同样，TP 计数表示模型正确识别兽脚类足迹的次数。目标是尽可能提高 TN 和 TP，同时尽可能减少
    FP 和 FN，即错误。
- en: 'In [Table 3-5](ch03.xhtml#ch03tab5), ACC refers to the accuracy: how many times
    was the classifier’s assigned label correct? While accuracy is the most natural
    metric to consider, it isn’t always the best, especially if the number of examples
    per class isn’t nearly equal. The random forest performed the best in terms of
    accuracy, correctly labeling more than 83 out of every 100 test images. The linear
    SVM was the worst; it was right only about 71 times out of 100\. Random guessing
    would be correct about 50 percent of the time because we have two classes, though,
    so even the linear SVM was learning from the footprint images. We define the accuracy
    in terms of the cells of the confusion matrix by adding TP and TN and dividing
    that sum by the sum of all four cells.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [表 3-5](ch03.xhtml#ch03tab5) 中，ACC 指的是准确率：分类器分配标签正确的次数。虽然准确率是最直观的度量标准，但它并不总是最好的，特别是当每个类别的样本数不均等时。随机森林在准确率方面表现最好，每
    100 张测试图像中正确标记了超过 83 张。线性支持向量机表现最差，每 100 次中只有约 71 次正确。由于我们有两个类别，随机猜测的正确率大约是 50%，因此即便是线性支持向量机也能从足迹图像中学习。我们通过将
    TP 和 TN 相加，并将该和除以所有四个单元格的总和，来定义准确率。
- en: 'The MCC column, which stands for *Matthews correlation coefficient*, introduces
    a new metric. It’s a different combination of the four numbers in the confusion
    matrix. MCC is my favorite metric for classifiers, and it is increasingly understood
    to be the best single-number measure of how well a model performs. (These metrics
    apply to more advanced deep learning models as well.) [Table 3-5](ch03.xhtml#ch03tab5)
    is sorted by MCC, which, for this example, also happens to sort by ACC. For a
    binary model, the lowest possible MCC is –1, and the highest is 1\. Random guessing
    gives an MCC of 0\. An MCC of 1 means the model makes no mistakes. An MCC of –1,
    which never actually happens in practice, means that the model is perfectly wrong:
    in our case, it would label all theropod tracks ornithischian and all ornithischian
    tracks theropod. If you have a perfectly wrong classifier, swap the output labels
    to make it perfectly right.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: MCC 列代表 *马修斯相关系数*，它引入了一种新的度量标准。它是混淆矩阵中四个数字的不同组合。MCC 是我最喜欢的分类器度量标准，它越来越被认为是衡量模型表现的最佳单一指标。（这些度量标准同样适用于更先进的深度学习模型。）[表
    3-5](ch03.xhtml#ch03tab5) 按 MCC 排序，在这个例子中，排序结果也恰好按 ACC 排列。对于二分类模型，最低的 MCC 为 -1，最高为
    1。随机猜测的 MCC 为 0。如果模型没有任何错误，MCC 为 1。如果 MCC 为 -1（实际上从未在实践中发生），意味着模型完全错误：在我们的例子中，它会将所有兽脚类足迹标记为鸟脚类，将所有鸟脚类足迹标记为兽脚类。如果你有一个完全错误的分类器，交换输出标签即可使其完全正确。
- en: The Train and Test columns list times in seconds. The Train column tells us
    how long it took to train the model before using it. The nearest neighbor models
    take virtually no time, a mere fraction of a millisecond, because there’s nothing
    to train. Recall that a nearest neighbor model is the training set itself; there
    is no model to condition to approximate the data in some way.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: “训练”和“测试”列显示的是秒数。“训练”列告诉我们在使用模型之前训练它所花费的时间。最近邻模型几乎不需要时间，只有毫秒的一小部分，因为它不需要训练。回想一下，最近邻模型就是训练集本身；它没有需要调整的模型来近似数据。
- en: The slowest model was the linear SVM. Curiously, the more complex radial basis
    function model trained in roughly one-third the time (a difference that can be
    attributed to how such models are implemented in code). The next slowest model
    to train was the random forest. This makes sense because there were 300 decision
    trees in the forest, and each of them had to be trained independently.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 最慢的模型是线性SVM。有趣的是，复杂的径向基函数模型的训练时间约为线性SVM的三分之一（这种差异可以归因于这些模型在代码中的实现方式）。下一个训练速度较慢的模型是随机森林。这个情况是合理的，因为森林中有300棵决策树，每一棵都必须独立训练。
- en: The inference time, in the Test column, was roughly the same between the nearest
    neighbor and random forest models. The SVM models were respectively slow (RBF)
    and very fast (linear), again reflecting differences in the implementation. Notice
    that the nearest neighbor models take longer to use than to train. This is the
    reverse of the usual scenario, especially for neural networks, as we’ll see later
    in the book. Typically, training is slow but needs to be done only once, while
    inference is fast. For nearest neighbor models, the larger the training set, the
    slower the inference time—a significant strike against them.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在“测试”列中，最近邻和随机森林模型的推理时间大致相同。SVM模型分别较慢（RBF）和非常快（线性），这再次反映了实现上的差异。注意，最近邻模型的使用时间比训练时间要长。这与通常的情况相反，尤其是对于神经网络，正如我们在本书后面将看到的那样。通常，训练时间较长，但只需进行一次，而推理时间较短。对于最近邻模型，训练集越大，推理时间越慢——这是它们的一个显著缺点。
- en: 'There are two main things to take away from this exercise: a general understanding
    of the performance of the classical models, which we’ll use as a baseline against
    which to compare a neural network in [Chapter 4](ch04.xhtml), and that even classical
    models can do well on this particular dataset. Their performance was on par with
    that of human experts (meaning paleontologists), who also labeled the dinosaur
    footprint outlines. According to the original paper by Lallensack et al. from
    which the dinosaur dataset was taken, the human experts were correct only 57 percent
    of the time. They were also allowed to label tracks as “ambiguous,” a luxury the
    models don’t have; the models always make a class assignment, with no “I don’t
    know” option. We can coerce some model types into making such statements, but
    the classical models of this chapter are not well suited to that.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这次练习的两个主要收获是：对经典模型性能的整体理解，我们将以此作为基准，用于与神经网络进行比较，详见[第4章](ch04.xhtml)，以及即便是经典模型，在这个特定数据集上也能表现得很好。它们的表现与人类专家（即古生物学家）相当，这些专家也标注了恐龙足迹的轮廓。根据Lallensack等人原始论文中的数据（该数据集即来源于该论文），人类专家的正确率只有57%。他们还被允许将足迹标注为“模糊”，这是模型无法做到的奢侈选择；模型总是做出类别分配，没有“我不知道”的选项。我们可以让某些模型类型做出类似声明，但本章中的经典模型并不适合这种做法。
- en: '****'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '****'
- en: Are the classical models symbolic AI or connectionism? Are they AI at all? Do
    they learn, or are they merely mathematical tricks? My answers to these questions
    follow.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 经典模型是符号主义人工智能还是联结主义？它们究竟是人工智能吗？它们会学习吗，还是仅仅是数学技巧？我对这些问题的回答如下。
- en: In [Chapter 1](ch01.xhtml), I characterized the relationship between AI, machine
    learning, and deep learning as a series of nested concepts, with deep learning
    a form of machine learning and machine learning a form of AI (see [Figure 1-1](ch01.xhtml#ch01fig01)).
    This is the proper way to describe the relationship for most people, and it fits
    with [Chapter 2](ch02.xhtml)’s history. From this perspective, the classical models
    of this chapter are a form of AI.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](ch01.xhtml)中，我将人工智能、机器学习和深度学习的关系描述为一系列嵌套的概念，其中深度学习是机器学习的一种形式，而机器学习是人工智能的一种形式（参见[图1-1](ch01.xhtml#ch01fig01)）。这是大多数人理解这种关系的正确方式，并且与[第2章](ch02.xhtml)的历史内容相符。从这个角度来看，本章的经典模型可以被视为人工智能的一种形式。
- en: But are the classical models symbolic AI or connectionist AI? I say neither.
    They are not symbolic AI because they don’t manipulate logical rules or statements,
    and they’re not connectionist because they don’t employ a network of simple units
    that learn their proper association as they work with the data. Instead, I consider
    these models to be a fancy form of curve fitting—the output of an algorithm employing
    an optimization process to produce a function that best characterizes the training
    data, and, hopefully, the data encountered by the model in the wild.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，经典模型是符号AI还是连接主义AI呢？我说都不是。它们不是符号AI，因为它们不操作逻辑规则或陈述；它们也不是连接主义AI，因为它们没有采用一个简单单元的网络，单元在与数据互动时学习它们的正确关联。相反，我认为这些模型是一种复杂的曲线拟合形式——它们是一个算法的输出，该算法使用优化过程生成一个函数，该函数最能描述训练数据，并且希望能够适应模型在实际环境中遇到的数据。
- en: For a support vector machine, the function is the structure of the model in
    terms of the support vectors it locates during its optimization process. A decision
    tree’s function is generated by a specific algorithm designed to repeatedly split
    the training data into smaller and smaller groups until a leaf is created that
    (usually) contains only examples from a single class. Random forests are merely
    collections of such functions working in parallel.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 对于支持向量机来说，函数是模型的结构，它通过在优化过程中定位支持向量来形成。决策树的函数是通过特定算法生成的，该算法设计用于反复将训练数据划分成越来越小的组，直到生成一个叶子节点，该节点（通常）只包含来自单一类别的示例。随机森林仅仅是并行工作的这种函数的集合。
- en: Tree classifiers are almost a form of genetic programming. [*Genetic programming*](glossary.xhtml#glo50)
    creates computer code by simulating evolution via natural selection, where improved
    fitness corresponds to “is a better solution to the problem.” Indeed, genetic
    programming is a kind of [*evolutionary algorithm*](glossary.xhtml#glo38), and
    evolutionary algorithms, along with [*swarm intelligence*](glossary.xhtml#glo93)
    algorithms, implement robust, generic optimization. Some people consider evolutionary
    algorithms and swarm intelligence to be AI. I don’t, though I frequently use them
    in my work. Swarms don’t learn; they search a space representing possible solutions
    to a problem.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树分类器几乎是一种遗传编程形式。[*遗传编程*](glossary.xhtml#glo50)通过模拟自然选择中的进化过程来创建计算机代码，其中改进的适应度对应于“是问题的更好解决方案”。实际上，遗传编程是一种[*进化算法*](glossary.xhtml#glo38)，进化算法以及[*群体智能*](glossary.xhtml#glo93)算法实现了强大而通用的优化。一些人认为进化算法和群体智能是人工智能，但我并不这么认为，尽管我在工作中经常使用它们。群体智能并不学习；它们只是搜索一个表示问题可能解决方案的空间。
- en: Nearest neighbor models are even simpler; there is no function to create. If
    we have *all* the possible data generated by some parent process—that is, the
    thing creating the feature vectors that we’re trying to model—then we don’t need
    a model. To assign a class label to a feature vector, we simply look it up in
    the feature vector “phone book” and return the label we find there. Since we have
    all possible feature vectors with labels, there’s nothing to approximate, and
    any feature vector encountered in the wild will necessarily be in the book.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 最近邻模型甚至更简单；不需要创建任何函数。如果我们拥有由某个父进程生成的*所有*可能的数据——也就是说，生成我们尝试建模的特征向量的那个过程——那么我们就不需要模型。为了给特征向量分配一个类别标签，我们只需在特征向量的“电话簿”中查找，并返回找到的标签。由于我们拥有所有可能的带标签的特征向量，因此没有需要近似的东西，任何在实际环境中遇到的特征向量必定会在电话簿中。
- en: Barring access to all possible feature vectors for the problem at hand, a nearest
    neighbor model uses the closest feature vector in the incomplete phone book represented
    by the training data.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 除非能够访问所有可能的特征向量，否则最近邻模型使用训练数据所代表的、不完整的电话簿中最接近的特征向量。
- en: As an example, suppose we live in a town of 3,000 people, and all of them are
    in the phone book. (Are there still such things as phone books? If not, pretend.)
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们住在一个有3000人的小镇，所有人都在电话簿里。（现在还有电话簿吗？如果没有，请假装有。）
- en: If we want to find Nosmo King’s phone number, we look in the book under “King”
    and scan until we hit “Nosmo,” and we have it. Suppose, however, that we don’t
    have a complete listing of all 3,000 people, but 300 selected at random. We still
    want to know Nosmo King’s phone number (class label), but it’s not in the phone
    book. However, there is a Burg R. King. There’s a good chance Burg is related
    to Nosmo because of the shared last name, so we return Burg’s phone number as
    Nosmo’s. Clearly, the more complete the phone book, the better the chance we’ll
    find our desired name or someone in that person’s household. That’s essentially
    all that a nearest neighbor model does.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想找诺斯莫·金的电话号码，我们会在电话簿中找到“King”并扫描直到找到“Nosmo”，然后就能得到它。然而，假设我们没有包含所有3,000人的完整名单，而是随机挑选了300个。我们依然想知道诺斯莫·金的电话号码（类别标签），但是在电话簿中找不到。然而，那里有一个Burg
    R. King。由于他们共享同样的姓氏，Burg 很可能与 Nosmo 有关，因此我们把 Burg 的电话号码当作 Nosmo 的号码返回。显然，电话簿越完整，我们找到目标名字或该人家庭成员的机会就越大。这基本上就是最近邻模型所做的事情。
- en: '****'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '****'
- en: To recap, support vector machines, decision trees, and random forests use data
    to generate functions according to a carefully crafted algorithm designed by a
    human. That is neither symbolic AI nor connectionism to me, but curve fitting
    or, perhaps more accurately, optimization. Nearest neighbor models are even worse;
    in their case, there’s no function at all.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，支持向量机、决策树和随机森林利用数据根据人类精心设计的算法生成函数。对我来说，这既不是符号人工智能，也不是联结主义，而是曲线拟合，或者更准确地说，是优化。最近邻模型更糟糕；在这种情况下，根本没有函数。
- en: This doesn’t mean that AI is bogus, but it does mean that what practitioners
    have in mind when they talk about AI is likely different from what the general
    public considers “artificial intelligence.”
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不意味着人工智能是虚假的，但它确实意味着当从业者谈论人工智能时，他们所想到的内容很可能与公众所认为的“人工智能”不同。
- en: 'However, all is not lost. There is a machine learning model worthy of the connectionist
    label: the neural network. It’s at the heart of the AI revolution, and it’s capable
    of actually learning from data. So, let’s put classical models and symbolic AI
    aside and devote our attention to neural networks.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一切并非失去。确实有一个值得冠以联结主义标签的机器学习模型：神经网络。它是人工智能革命的核心，能够真正从数据中学习。所以，让我们将经典模型和符号人工智能放在一边，把注意力集中在神经网络上。
- en: '**KEY TERMS**'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '**关键术语**'
- en: bagging, curse of dimensionality, evolutionary algorithm, false negative, false
    positive, genetic programming, hyperparameters, inference, manifold, metric, nearest
    neighbor, one-versus-one, one-versus-rest, random forest, support vector machine,
    swarm intelligence, true negative, true positive
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 包装法、维度灾难、进化算法、假阴性、假阳性、遗传编程、超参数、推理、流形、度量、最近邻、一对一、一对多、随机森林、支持向量机、群体智能、真阴性、真阳性
