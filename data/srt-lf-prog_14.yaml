- en: '**14'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**14'
- en: MACHINE INTELLIGENCE**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 机器智能**
- en: '![Image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/common.jpg)'
- en: How many times a day do you submit something like “cats and meetloafs” to a
    search engine only to have it come back with, “Did you mean *cats and meatloaves*?”
    You probably take this for granted, without pausing to consider how the search
    engine knew not only that there was an error in your input, but also how to fix
    it. It’s not very likely that someone wrote a program to match all possible errors
    with corrections. Instead, some sort of *machine intelligence* must be at work.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 你一天会多少次向搜索引擎提交类似“cats and meetloafs”的内容，结果却返回“Did you mean *cats and meatloaves*?”你可能对此视为理所当然，却从未停下来思考搜索引擎是如何知道你的输入不仅有错误，而且知道如何修正它的。很难想象有人写了一个程序来匹配所有可能的错误并给出修正。而是某种*机器智能*在发挥作用。
- en: Machine intelligence is an advanced topic that includes the related fields of
    *machine learning*, *artificial intelligence*, and *big data*. These are all concepts
    you’ll likely encounter as a programmer, so this chapter gives you a high-level
    overview.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 机器智能是一个高级话题，包含了*机器学习*、*人工智能*和*大数据*等相关领域。这些都是你作为程序员可能会遇到的概念，因此本章为你提供了一个高层次的概述。
- en: '*Artificial intelligence* was first out of the gate when the term was coined
    at a Dartmouth College workshop in 1956\. *Machine learning* was a close second
    with the *perceptron* in 1957, which we’ll discuss shortly. Today, machine learning
    has a huge lead, thanks in part to two trends. First, technological progress has
    dramatically increased storage size while reducing the price of it, and it has
    also led to faster processors and networking. Second, the internet has facilitated
    the collection of large amounts of data, and people can’t resist poking at all
    of it. For example, data from one large company’s book-scanning and translation
    project was used to dramatically improve a separate language translation project.
    Another example is a mapping project that fed into the development of self-driving
    cars. The results from these and other projects were so compelling that machine
    learning now is being applied to a large number of applications. More recently,
    people have also realized that these same two trends—cheaper storage and more
    computing power, and the collection of large amounts of data—support revitalizing
    artificial intelligence, resulting in a lot of new work in that area as well.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*人工智能*在1956年达特茅斯学院的一个研讨会上首次被提出。当时，*机器学习*紧随其后，1957年提出的*感知机*便是其中之一，我们将很快讨论这一点。如今，机器学习遥遥领先，这在一定程度上归功于两个趋势。首先，技术进步大幅增加了存储空间，同时降低了存储成本，还使得处理器和网络速度得到了提升。其次，互联网促进了大量数据的收集，人们无法抗拒对这些数据的探索。例如，一家大型公司的图书扫描和翻译项目中的数据被用来显著改善另一个语言翻译项目。另一个例子是一个地图项目，它为自动驾驶汽车的开发提供了支持。这些项目及其他项目的成果如此令人信服，以至于机器学习现在被应用于大量的应用场景。最近，人们还意识到，这两大趋势——更便宜的存储、更强大的计算能力，以及大量数据的收集——也有助于复兴人工智能，促使该领域出现了许多新的工作。'
- en: 'The rush to employ machine intelligence, however, mimics the “What can possibly
    go wrong?” philosophy that already pervades the computer security world. We don’t
    yet know enough to avoid producing psychotic systems such as HAL from the 1968
    movie *2001: A Space Odyssey*.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，急于采用机器智能的做法模仿了计算机安全领域中“有什么可能出错？”的哲学思想。我们尚未足够了解如何避免产生像1968年电影《2001太空漫游》中的HAL那样的精神错乱系统。
- en: '**Overview**'
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**概述**'
- en: You should know by now that programming is the grunt work needed to implement
    solutions to problems. Defining the problems and their solutions is the interesting
    and harder part. Many would rather devote their entire careers trying to get computers
    to do this work instead of doing it themselves (another example of the “peculiar
    engineering laziness” mentioned in [Chapter 5](ch05.xhtml#ch05)).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在应该知道，编程是实现问题解决方案所需的繁重工作。定义问题及其解决方案才是有趣且更具挑战性的部分。许多人宁愿将整个职业生涯都投入到让计算机完成这些工作的工作中，而不是自己亲自去做（这也是[第5章](ch05.xhtml#ch05)中提到的“独特的工程懒惰”现象的另一个例子）。
- en: As noted earlier, it all started with artificial intelligence; machine learning
    and big data came later. Though the term wasn’t coined until the 1956 Dartmouth
    workshop, the notion of artificial intelligence goes all the way back to Greek
    myths. Many philosophers and mathematicians since have worked to develop formal
    systems that codify human thought. While this hasn’t yet led to true artificial
    intelligence, it has laid a lot of groundwork. For example, George Boole, who
    gave us our algebra in [Chapter 1](ch01.xhtml#ch01), published *An Investigation
    of the Laws of Thought on Which Are Founded the Mathematical Theories of Logic
    and Probabilities* in 1854.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，一切始于人工智能；机器学习和大数据则是后来出现的。虽然这个术语直到1956年的达特茅斯研讨会才被创造出来，但人工智能的概念早在古希腊神话中就有了。自那以后，许多哲学家和数学家致力于开发能够编码人类思维的正式系统。尽管这一努力尚未导致真正的人工智能，但它为此奠定了大量基础。例如，乔治·布尔（George
    Boole）在[第一章](ch01.xhtml#ch01)中为我们提供了代数，他于1854年出版了*《思维法则的研究：逻辑学和概率数学理论的基础》*。
- en: We’ve accumulated a lot of information about human decision making but still
    don’t really know how humans think. For example, you and your friends can probably
    distinguish a cat from a meatloaf. But you might be taking different paths to
    reach that distinction. Just because the same data yields the same result doesn’t
    mean that we all processed it the same way. We know the inputs and outputs, we
    understand parts of the “hardware,” but we don’t know much about the “programming”
    that transforms one into the other. It follows that the path that a computer uses
    to recognize cats and meatloaves would also be different.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经积累了大量关于人类决策过程的信息，但仍然不真正了解人类是如何思考的。例如，你和你的朋友们可能能够区分猫和肉饼，但你们可能采取了不同的路径来得出这一区分。仅仅因为相同的数据得出相同的结果，并不意味着我们处理这些数据的方式相同。我们知道输入和输出，我们理解部分“硬件”，但对将二者转化的“编程”知之甚少。因此，计算机识别猫和肉饼的路径也会有所不同。
- en: One thing that we do think we know about human thought processing is that we’re
    really good at statistics unconsciously—not the same thing as consciously suffering
    through “sadistics” class. For example, linguists have studied how humans acquire
    language. Infants perform a massive amount of statistical analysis as part of
    learning to tease important sounds out of the environment, separate them into
    phonemes, and then group them into words and sentences. It’s an ongoing debate
    as to whether humans have specialized machinery for this sort of thing or whether
    we’re just using general-purpose processing for it.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为关于人类思维处理的一个已知事实是，我们在无意识中非常擅长统计分析——这与在“统计学”课堂上有意识地受苦是两回事。例如，语言学家研究了人类是如何习得语言的。婴儿在学习从环境中提取重要声音、将其分解成音素，并将这些音素组合成单词和句子的过程中，进行大量的统计分析。至于人类是否有专门的设备来处理这种情况，还是我们仅仅使用通用处理机制来完成，这仍然是一个持续的争论话题。
- en: Infant learning via statistical analysis is made possible, at least in part,
    by the existence of a large amount of data to analyze. Barring exceptional circumstances,
    infants are constantly exposed to sound. Likewise, there is a constant barrage
    of visual information and other sensory input. Infants learn by processing a big
    amount of data, or just *big data*.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 婴儿通过统计分析来学习，至少部分原因是因为有大量数据可以分析。除非有特殊情况，婴儿不断接触到声音。同样，婴儿也会不断接收到视觉信息和其他感官输入。婴儿通过处理大量数据来学习，或者说是*大数据*。
- en: The massive growth in compute power, storage capacity, and various types of
    network-connected sensors (including cell phones) has led to the collection of
    huge amounts of data, and not just by the bad guys from the last chapter. Some
    of this data is organized and some isn’t. Organized data might be sets of wave
    height measurements from offshore buoys. Unorganized data might be ambient sound.
    What do we do with all of it?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 计算能力、存储容量和各种类型的网络连接传感器（包括手机）的巨大增长，导致了大量数据的收集，而这些数据并不仅仅是上章所提到的坏人收集的。这些数据中有些是有组织的，有些则没有。组织好的数据可能是来自海上浮标的波浪高度测量数据。无组织的数据可能是环境声音。我们该如何处理这些数据呢？
- en: Well, statistics is one of those well-defined branches of mathematics, which
    means that we can write programs that perform statistical analysis. We can also
    write programs that can be trained using data. For example, one way to implement
    spam filters (which we’ll look at in more detail shortly) is to feed lots of collected
    spam and nonspam into a statistical analyzer while telling the analyzer what is
    spam and what isn’t. In other words, we’re feeding organized *training data* into
    a program and telling the program what that data means (that is, what’s spam and
    what’s not). We call this *machine learning (ML)*. Kind of like Huckleberry Finn,
    we’re “gonna learn that machine” instead of “teaching” it.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 统计学是数学中一个定义明确的分支，这意味着我们可以编写执行统计分析的程序。我们也可以编写可以使用数据进行训练的程序。例如，实现垃圾邮件过滤器的一种方法（我们稍后会更详细地讨论）是将大量收集到的垃圾邮件和非垃圾邮件输入统计分析器，同时告诉分析器哪些是垃圾邮件，哪些不是。换句话说，我们将有组织的*训练数据*输入程序，并告诉程序这些数据意味着什么（即，什么是垃圾邮件，什么不是）。我们称之为*机器学习（ML）*。有点像哈克贝里·芬，我们是“要让机器学会”，而不是“教它”。
- en: In many respects, machine learning systems are analogous to human autonomic
    nervous system functions. In humans, the brain isn’t actively involved in a lot
    of low-level processes, such as breathing; it’s reserved for higher-level functions,
    such as figuring out what’s for dinner. The low-level functions are handled by
    the autonomic nervous system, which only bothers the brain when something needs
    attention. Machine learning is currently good for recognizing, but that’s not
    the same thing as taking action.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多方面，机器学习系统类似于人类的自主神经系统功能。对于人类来说，大脑不会积极参与许多低级过程，例如呼吸；它主要负责更高级的功能，比如决定晚餐吃什么。低级功能由自主神经系统处理，只有当某些事情需要注意时，才会打扰到大脑。目前，机器学习在识别方面表现良好，但这并不等同于采取行动。
- en: Unorganized data is a different beast. We’re talking big data, meaning that
    it’s not something that humans can comprehend without assistance. In this case,
    various statistical techniques are used to find patterns and relationships. For
    example, this approach—sometimes known as *data mining*—could be used to extract
    music from ambient sound (after all, as French composer Edgard Varèse said, music
    is organized sound).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 非组织化数据是一个完全不同的难题。我们所说的是大数据，这意味着没有辅助工具，人类无法理解这些数据。在这种情况下，会使用各种统计技术来寻找模式和关系。例如，这种方法——有时被称为*数据挖掘*——可以用来从环境声音中提取音乐（毕竟，正如法国作曲家埃德加·瓦雷兹所说，音乐就是有组织的声音）。
- en: All of this is essentially finding ways to transform complex data into something
    simpler. For example, one could train a machine learning system to recognize cats
    and meatloaves. It would transform very complex image data into simple cat and
    meatloaf bits. It’s a *classification* process.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切本质上是在寻找将复杂数据转化为更简单内容的方法。例如，可以训练一个机器学习系统来识别猫和肉饼。它会将非常复杂的图像数据转化为简单的猫和肉饼的片段。这是一个*分类*过程。
- en: Back in [Chapter 5](ch05.xhtml#ch05), we discussed the separation of instructions
    and data. In [Chapter 13](ch13.xhtml#ch13), I cautioned against allowing data
    to be treated as instructions for reasons of security. But there are times when
    it makes sense, because data-driven classification can only get us so far.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](ch05.xhtml#ch05)中，我们讨论了指令与数据的分离。在[第13章](ch13.xhtml#ch13)中，我提醒过不要让数据被当作指令处理，出于安全考虑。但有时这样做是有意义的，因为数据驱动的分类只能带我们走到一定程度。
- en: One can’t, for example, create a self-driving car based on classification alone.
    A set of complicated programs that acts on classifier outputs must be written
    to implement behavior such as “Don’t hit cats, but it’s okay and possibly beneficial
    to society to drive over a meatloaf.” There are many ways in which this behavior
    could be implemented, including combinations of swerving and changing speed. There
    are large numbers of variables to consider, such as other traffic, the relative
    position of obstacles, and so on.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，仅凭分类是无法创建自动驾驶汽车的。必须编写一套复杂的程序来处理分类器输出，实现“不要撞到猫，但可以且可能对社会有益地压过肉饼”等行为。这种行为可以通过多种方式来实现，包括转向和改变速度的组合。需要考虑大量的变量，例如其他交通情况、障碍物的相对位置等等。
- en: People don’t learn to drive from complex and detailed instructions such as “Turn
    the wheel one degree to the left and put one gram of pressure on the brake pedal
    for three seconds to avoid the cat,” or “Turn three degrees to the right and floor
    it to hit the meatloaf dead on.” Instead, they work from goals such as “Don’t
    hit cats.” People “program” themselves, and, as mentioned earlier, we have no
    way yet to examine that programming to determine how they choose to accomplish
    goals. If you observe traffic, you’ll see a lot of variation in how people accomplish
    the same basic tasks.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 人们并不是通过复杂且详细的指令来学习驾驶，比如“将方向盘向左转动一度，并在刹车踏板上施加一克压力，保持三秒钟，以避免撞到猫”或者“向右转三度并踩到底，以确保准确撞到肉饼”。相反，他们的目标是“不要撞到猫”。人们会“编程”自己，正如前面提到的，我们目前还无法检查这些编程，以确定他们选择如何完成目标。如果你观察交通情况，你会发现人们完成相同基本任务的方式有很多变化。
- en: People are not just refining their classifiers in cases like this; they’re writing
    new programs. When computers do this, we call it *artificial intelligence (AI)*.
    AI systems write their own programs to accomplish goals. One way to achieve this
    without damaging any actual cats is to provide an AI system with simulated input.
    Of course, philosophical thermostellar devices such as Bomb 20 from the 1974 movie
    *Dark Star* show that this doesn’t always work out well.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 人们在这种情况下不仅仅是在完善他们的分类器；他们还在编写新的程序。当计算机执行这些任务时，我们称之为*人工智能（AI）*。人工智能系统会编写自己的程序来完成目标。实现这一点的一种方法是给人工智能系统提供模拟输入，当然，也有像1974年电影《黑暗之星》中的Bomb
    20这样的哲学性恒星热力学装置，显示出这并不总是能取得良好的效果。
- en: A big difference between machine learning and artificial intelligence is the
    ability to examine a system and “understand” the “thought processes.” This is
    currently impossible in machine learning systems but is possible with AI. It’s
    not clear if that will continue when AI systems get huge, especially as it’s unlikely
    that the processes will resemble human thought.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习与人工智能的一个重大区别在于是否能够检查一个系统并“理解”其“思维过程”。目前，在机器学习系统中这是不可能的，但在人工智能中是可行的。当人工智能系统变得庞大时，这是否能继续下去仍不确定，尤其是在这些系统的过程可能与人类思维完全不同的情况下。
- en: '**Machine Learning**'
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**机器学习**'
- en: Let’s see if we can come up with a way to distinguish a photograph of a cat
    from a photograph of a meatloaf. This is a lot less information than a human has
    available with real cats and meatloaves. For example, humans have discovered that
    cats typically run away when chased, which is not common meatloaf behavior. We’ll
    try to create a process that, when presented with a photograph, will tell us whether
    “it sees” a cat, a meatloaf, or neither. [Figure 14-1](ch14.xhtml#ch14fig01) shows
    the original images of a meatloaf-looking Tony Cat on the left and an actual meatloaf
    on the right.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看能否想出一种方法，区分猫的照片和肉饼的照片。这比人类面对真实猫和肉饼时能获取的信息要少得多。例如，人类发现猫通常会在被追赶时逃跑，而肉饼并没有这种行为。我们将尝试创建一个流程，当给定一张照片时，它能告诉我们它是否“看到”了一只猫、一块肉饼，或者都没有。[图
    14-1](ch14.xhtml#ch14fig01)显示了左侧看起来像肉饼的托尼猫和右侧的真正肉饼的原始图像。
- en: '![Image](../images/14fig01-2.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/14fig01-2.jpg)'
- en: '*Figure 14-1: Original Tony Cat and meatloaf*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 14-1：原始托尼猫和肉饼*'
- en: There’s a high probability that you’ll encounter statistics at all levels of
    machine intelligence, so we’ll start by reviewing some of the basics.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 你很可能会在机器智能的各个层面遇到统计学问题，所以我们将从回顾一些基本概念开始。
- en: '***Bayes***'
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***贝叶斯***'
- en: English minister Thomas Bayes (1701–1761) must have been concerned about the
    chances of his flock getting into heaven because he thought a lot about probability.
    In particular, Bayes was interested in how the probabilities of different events
    combined. For example, if you’re a backgammon player, you’re probably well aware
    of the probability distribution of numbers that result from rolling a pair of
    six-sided dice. He’s responsible for the eponymous *Bayes’* theorem.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 英国牧师托马斯·贝叶斯（1701–1761）一定非常关心他的信徒能否进入天堂，因为他思考了很多关于概率的内容。特别是，贝叶斯对不同事件概率的结合方式很感兴趣。例如，如果你是一个掷双骰子的西洋跳棋玩家，你可能很清楚掷出一对六面骰子的概率分布。他是著名的*贝叶斯*定理的提出者。
- en: The part of his work that’s relevant here is what’s called a *naive Bayes classifier*.
    Leaving our meatloaf with the cat for a while, let’s try to separate out messages
    that are spam from messages that aren’t. Messages are collections of words. Certain
    words are more likely to occur in spam, from which we can infer that messages
    without those words might not be spam.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 他工作中与此相关的部分是所谓的*朴素贝叶斯分类器*。将我们的肉饼暂时留给猫一会儿，让我们尝试将垃圾邮件与非垃圾邮件区分开来。消息是由单词组成的。某些单词在垃圾邮件中更常见，基于这一点我们可以推断出，未包含这些单词的消息可能不是垃圾邮件。
- en: We’ll start by collecting some simple statistics. Let’s assume that we have
    a representative sample of messages, 100 that are spam and another 100 that aren’t.
    We’ll break the messages up into words and count the number of messages in which
    each word occurs. Since we’re using 100 messages, that magically gives us the
    percentage. Partial results are shown in [Table 14-1](ch14.xhtml#ch14tab01).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从收集一些简单的统计数据开始。假设我们有一个代表性的消息样本，其中100条是垃圾邮件，另外100条不是垃圾邮件。我们将这些消息分解成单词，并计算每个单词出现的消息数量。由于我们使用了100条消息，这就自动给出了百分比。部分结果显示在[表14-1](ch14.xhtml#ch14tab01)中。
- en: '**Table 14-1:** Words in Messages Statistics'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**表14-1：消息中的单词统计**'
- en: '| **Word** | **Spam percentage** | **Nonspam percentage** |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| **单词** | **垃圾邮件百分比** | **非垃圾邮件百分比** |'
- en: '| --- | --- | --- |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| meatloaf | 80 | 0 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| meatloaf | 80 | 0 |'
- en: '| hamburger | 75 | 5 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| hamburger | 75 | 5 |'
- en: '| catnip | 0 | 70 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| catnip | 0 | 70 |'
- en: '| onion | 68 | 0 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| onion | 68 | 0 |'
- en: '| mousies | 1 | 67 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| mousies | 1 | 67 |'
- en: '| the | 99 | 98 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 这 | 99 | 98 |'
- en: '| and | 97 | 99 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 和 | 97 | 99 |'
- en: You can see that some words are common to both spam and nonspam. Let’s apply
    this table to an unknown message that contains “hamburger and onion:” respectively,
    the spam percentages are 75, 97, and 68, and the nonspam percentages are 5, 97,
    and 0\. What’s the probability that this message is or isn’t spam?
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，一些词语在垃圾邮件和非垃圾邮件中都是常见的。让我们将这个表格应用到一个包含“hamburger 和 onion”的未知消息中：垃圾邮件的概率分别是75、97和68，非垃圾邮件的概率是5、97和0。这个消息是垃圾邮件的概率是多少，或者说它不是垃圾邮件的概率是多少？
- en: 'Bayes’ theorem tells us how to combine probabilities (*p*) where *p*[0] is
    the probability that a message containing the word *meatloaf* is spam, *p*[1]
    is the probability that a message containing the word *hamburger* is spam, and
    so on:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理告诉我们如何结合概率（*p*），其中*p*[0]是包含词语*meatloaf*的消息是垃圾邮件的概率，*p*[1]是包含词语*hamburger*的消息是垃圾邮件的概率，依此类推：
- en: '![Image](../images/eq389-01.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/eq389-01.jpg)'
- en: This can be visualized as shown in [Figure 14-2](ch14.xhtml#ch14fig02). Events
    and probabilities such as those from [Table 14-1](ch14.xhtml#ch14tab01) are fed
    into the classifier, which yields the probability that the events describe what
    we want.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图14-2](ch14.xhtml#ch14fig02)所示，可以将其可视化。像[表14-1](ch14.xhtml#ch14tab01)中的事件和概率这样的数据被输入到分类器中，分类器给出这些事件描述我们想要的概率。
- en: '![Image](../images/14fig02.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/14fig02.jpg)'
- en: '*Figure 14-2: Naive Bayes classifier*'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*图14-2：朴素贝叶斯分类器*'
- en: We can build a pair of classifiers, one for spam and one for nonspam. Plugging
    in the numbers from the above example, this gives us a 99.64 percent chance that
    the message is spam and a 0 percent chance that it’s not.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以构建一对分类器，一个用于垃圾邮件，另一个用于非垃圾邮件。将上述例子中的数字代入，得出的结果是该消息是垃圾邮件的概率为99.64%，而不是垃圾邮件的概率为0%。
- en: You can see that this technique works pretty well. Statistics rules! There are,
    of course, a lot of other tricks needed to make a decent spam filter. For example,
    understanding what’s meant by “naive.” It doesn’t mean that Bayes didn’t know
    what he was doing. It means that just like rolling dice, all of the events are
    unrelated. We could improve our spam filtering by looking at the relationship
    between words, such as the fact that “and and” appears only in messages about
    Boolean algebra. Many spammers try to evade filters by including a large amount
    of “word salad” in their messages, which is rarely grammatically correct.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，这个技巧相当有效。统计学规则！当然，制作一个好的垃圾邮件过滤器需要很多其他技巧。例如，理解“naive”（朴素）的含义。这并不意味着贝叶斯不知道他在做什么，而是说，就像掷骰子一样，所有的事件是相互独立的。我们可以通过观察单词之间的关系来改进垃圾邮件过滤，例如，“and
    and”这个词组仅出现在布尔代数相关的消息中。许多垃圾邮件发送者试图通过在消息中加入大量“乱七八糟”的词汇来绕过过滤器，而这些内容通常语法上不正确。
- en: '***Gauss***'
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***高斯***'
- en: German mathematician Johann Carl Friedrich Gauss (1777–1855) is another statistically
    important person. You can blame him for the *bell curve*, also called a *normal
    distribution* or *Gaussian distribution*. It looks like [Figure 14-3](ch14.xhtml#ch14fig03).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 德国数学家约翰·卡尔·弗里德里希·高斯（1777–1855）是另一位在统计学上重要的人物。你可以把*钟形曲线*、也叫做*正态分布*或*高斯分布*归咎于他。它的形状如[图
    14-3](ch14.xhtml#ch14fig03)所示。
- en: '![Image](../images/14fig03.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/14fig03.jpg)'
- en: '*Figure 14-3: Bell curve*'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 14-3：钟形曲线*'
- en: The bell curve is interesting because samples of observed phenomena fit the
    curve. For example, if we measure the height of basketball players at a park and
    determine the mean height μ, some players will be taller and some will be shorter.
    (By the way, μ is pronounced “mew,” making it the preferred Greek letter for cats.)
    Of the players, 68 percent will be within one *standard deviation* or σ, 95 percent
    within two standard deviations, and so on. It’s more accurate to say that the
    height distribution converges on the bell curve as the number of samples increases,
    because the height of a single player isn’t going to tell us much. Carefully sampled
    data from a well-defined population can be used to make assumptions about larger
    populations.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 钟形曲线之所以有趣，是因为观测现象的样本符合这条曲线。例如，如果我们在公园里测量篮球运动员的身高并确定平均身高μ，一些球员会更高，而一些则会更矮。（顺便说一下，μ发音为“mew”，这也使得它成为猫的首选希腊字母。）在这些球员中，68%的球员的身高会在一个*标准差*（σ）之内，95%的球员会在两个标准差之内，依此类推。更准确的说法是，随着样本数量的增加，身高分布会收敛到钟形曲线，因为单个球员的身高并不能告诉我们太多信息。经过精心抽样的来自明确人口的数据可以用于对更大人群做出假设。
- en: While that’s all very interesting, there are plenty of other applications of
    the bell curve, some of which we can apply to our cat and meatloaf problem. American
    cartoonist Bernard Kliban (1935–1990) teaches us that a cat is essentially a meatloaf
    with ears and a tail. It follows that if we could extract features such as ears,
    tails, and meatloaves from photographs, we could feed them into a classifier that
    could identify the subject matter for us.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些都很有趣，但钟形曲线还有很多其他应用，有些我们可以应用到我们的猫和肉饼问题中。美国漫画家伯纳德·克里班（1935–1990）教导我们，猫本质上是一个带耳朵和尾巴的肉饼。因此，如果我们能从照片中提取出耳朵、尾巴和肉饼等特征，就可以将它们输入到分类器中，由它来识别主题。
- en: We can make object features more recognizable by tracing their outlines. Of
    course, we can’t do this unless we can find their edges. This is difficult because
    both cats and a fair number of meatloaves are fuzzy. Cats in particular have a
    lot of distinct hairs that are edges unto themselves, but not the ones we want.
    While it might seem counterintuitive, our first step is to blur the images slightly
    to eliminate some of these unwanted aspects. Blurring an image means applying
    a low-pass filter, like we saw for audio in [Chapter 6](ch06.xhtml#ch06). Fine
    details in an image are “high frequencies.” It’s intuitive if you think about
    the fine details changing faster as you scan across the image.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过追踪物体的轮廓使其特征更加可识别。当然，除非我们能找到它们的边缘，否则无法做到这一点。这是困难的，因为猫和许多肉饼都是模糊的。尤其是猫，它们有许多独特的毛发，这些毛发本身就是边缘，但不是我们想要的边缘。虽然这看起来有些违反直觉，但我们的第一步是轻微模糊图像，以消除一些这些不需要的特征。模糊图像意味着应用一个低通滤波器，就像我们在[第六章](ch06.xhtml#ch06)中看到的音频处理一样。图像中的细节属于“高频”部分。如果你想到细节变化越快，图像的扫描也会越快速，就会觉得这一点很直观。
- en: Let’s see what Gauss can do for us. Let’s take the curve from [Figure 14-3](ch14.xhtml#ch14fig03)
    and spin it around μ to make a three-dimensional version, as shown in [Figure
    14-4](ch14.xhtml#ch14fig04).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看高斯能为我们做些什么。让我们取[图 14-3](ch14.xhtml#ch14fig03)中的曲线，将其围绕μ旋转，形成一个三维版本，如[图
    14-4](ch14.xhtml#ch14fig04)所示。
- en: '![Image](../images/14fig04.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/14fig04.jpg)'
- en: '*Figure 14-4: Three-dimensional Gaussian distribution*'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 14-4：三维高斯分布*'
- en: We’ll drag this across the image, centering μ over each pixel in turn. You can
    imagine that parts of the curve cover other pixels surrounding the center pixel.
    We’re going to generate a new value for each pixel by multiplying the values of
    the pixels under the curve by the value of the curve and then adding the results
    together. This is called a *Gaussian blur*. You can see how it works in [Figure
    14-5](ch14.xhtml#ch14fig05). The image in the middle is a magnified copy of what’s
    in the square in the image on the left. In the right-hand image, you can see how
    the Gaussian blur weights a set of pixels from the center image.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将沿着图像拖动这个操作，将μ依次定位在每个像素上。你可以想象，曲线的部分会覆盖到中心像素周围的其他像素。我们将通过将曲线下方像素的值与曲线的值相乘，然后将结果加起来，来为每个像素生成一个新值。这就是所谓的*高斯模糊*。你可以在[图
    14-5](ch14.xhtml#ch14fig05)中看到它的工作原理。中间的图像是左侧图像中方框内内容的放大复制。在右侧的图像中，你可以看到高斯模糊如何对中心图像的像素进行加权。
- en: '![Image](../images/14fig05.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/14fig05.jpg)'
- en: '*Figure 14-5: Gaussian blur*'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 14-5：高斯模糊*'
- en: The process of combining the value of a pixel with the values of its neighbors
    might seem convoluted to you, and in fact it’s mathematically known as *convolution*.
    The array of weights is called a *kernel* or *convolution kernel*. Let’s look
    at some examples.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 将一个像素的值与其邻居的值结合的过程可能对你来说有点复杂，实际上它在数学上被称为*卷积*。这些权重数组被称为*核*或*卷积核*。让我们看一些例子。
- en: '[Figure 14-6](ch14.xhtml#ch14fig06) shows a 3×3 and a 5×5 kernel. Note that
    the weights all add up to 1 to preserve brightness.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 14-6](ch14.xhtml#ch14fig06)展示了3×3和5×5的卷积核。请注意，所有的权重加起来总和为1，以保持亮度。'
- en: '![Image](../images/14fig06.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/14fig06.jpg)'
- en: '*Figure 14-6: Gaussian convolution kernels*'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 14-6：高斯卷积核*'
- en: '[Figure 14-7](ch14.xhtml#ch14fig07) shows an original image on the left. Your
    eye can trace the outline of the tree trunks even though there are many gaps.
    The center image shows the results of a 3×3 kernel. While it’s fuzzier, the edges
    are easier to discern. The right image shows the results of a 5×5 kernel.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 14-7](ch14.xhtml#ch14fig07)展示了左侧的原始图像。即使有许多空隙，你的眼睛仍然可以追踪到树干的轮廓。中间的图像展示了3×3卷积核的结果。虽然它更模糊，但边缘更容易辨认。右侧的图像展示了5×5卷积核的结果。'
- en: You can think of the image as if it’s a mathematical function of the form *brightness*
    = *f*(*x*, *y*). The value of this function is the pixel brightness at each coordinate
    location. Note that this is a *discrete* function; the values for *x* and *y*
    must be integers. And, of course, they have to be inside the image boundaries.
    In a similar fashion, you can think of a convolution kernel as a small image whose
    value is *weight* = *g*(*x*, *y*). Thus, the process of performing the convolution
    involves iterating through the neighboring pixels covered by the kernel, multiplying
    the pixel values by the weights, and adding them together.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将图像看作一个数学函数，形式为*亮度* = *f*(*x*, *y*)。这个函数的值是每个坐标位置的像素亮度。请注意，这是一个*离散*函数；*x*和*y*的值必须是整数。当然，它们必须在图像的边界内。类似地，你可以将卷积核看作一个小图像，其值为*权重*
    = *g*(*x*, *y*)。因此，执行卷积的过程包括遍历被卷积核覆盖的相邻像素，将像素值与权重相乘，然后将它们加起来。
- en: '![Image](../images/14fig07.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/14fig07.jpg)'
- en: '*Figure 14-7: Gaussian blur examples*'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 14-7：高斯模糊示例*'
- en: '[Figure 14-8](ch14.xhtml#ch14fig08) shows our original images blurred using
    a 5×5 Gaussian kernel.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 14-8](ch14.xhtml#ch14fig08)展示了使用5×5高斯核模糊处理的原始图像。'
- en: '![Image](../images/14fig08.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/14fig08.jpg)'
- en: '*Figure 14-8: Blurred cat and meatloaf*'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 14-8：模糊的猫和肉饼*'
- en: Note that because the convolution kernels are bigger than 1 pixel, they hang
    off the edges of the image. There are many approaches to dealing with this, such
    as not going too close to the edge (making the result smaller) and making the
    image larger by drawing a border around it.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于卷积核大于1个像素，它们会超出图像的边缘。处理这种情况有很多方法，例如避免过于接近边缘（使结果变小）以及通过在图像周围绘制边框来增大图像大小。
- en: '***Sobel***'
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***Sobel***'
- en: 'There’s a lot of information in [Figure 14-1](ch14.xhtml#ch14fig01) that isn’t
    really necessary for us to identify the subject matter, such as color. For example,
    in his book *Understanding Comics: The Invisible Art* (Tundra), Scott McCloud
    shows that we can recognize a face from just a circle, two dots, and a line; the
    rest of the details are unnecessary and can be ignored. Accordingly, we’re going
    to simplify our images.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图14-1](ch14.xhtml#ch14fig01)中有很多信息，对于我们识别主题并不是特别必要，比如颜色。例如，在他的书《理解漫画：隐形的艺术》(Tundra)中，Scott
    McCloud展示了我们可以仅通过一个圆形、两个点和一条线来识别一张脸；其余的细节都是不必要的，可以忽略。因此，我们将简化我们的图像。
- en: Let’s try to find the edges now that we’ve made them easier to see by blurring.
    There are many definitions of *edge*. Our eyes are most sensitive to changes in
    brightness, so we’ll use that. The change in brightness is just the difference
    in brightness between a pixel and its neighbor.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试寻找边缘，既然我们已经通过模糊使它们更容易看到。对于*边缘*有很多定义。我们的眼睛对亮度变化最为敏感，因此我们将使用这个定义。亮度的变化就是一个像素与其邻近像素之间的亮度差异。
- en: About half of calculus is about change, so we can apply that here. A *derivative*
    of a function is just the slope of the curve generated by the function. If we
    want the change in brightness from one pixel to the next, then the formula is
    just *brightness* = *f*(*x* + 1, *y*) – *f*(*x*, *y*).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 微积分的一半内容就是关于变化的，所以我们可以在这里应用它。一个函数的*导数*就是由该函数生成的曲线的斜率。如果我们想要得到一个像素到下一个像素的亮度变化，那么公式就是*brightness*
    = *f*(*x* + 1, *y*) – *f*(*x*, *y*)。
- en: Take a look at the horizontal row of pixels in [Figure 14-9](ch14.xhtml#ch14fig09).
    The brightness level is plotted underneath, and beneath that is plotted the change
    in brightness. You can see that it looks spiky. That’s because it only has a nonzero
    value during a change.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 看看[图14-9](ch14.xhtml#ch14fig09)中的水平像素行。亮度水平绘制在下面，下面是亮度变化的图示。你可以看到它看起来是尖锐的。这是因为它只有在变化时才有非零值。
- en: '![Image](../images/14fig09.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/14fig09.jpg)'
- en: '*Figure 14-9: Edges are changes in brightness*'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*图14-9：边缘是亮度的变化*'
- en: There’s a problem with measuring brightness changes this way—the changes happen
    in the cracks between the pixels. We want the changes to be in the middle of the
    pixels. Let’s see if Gauss can help us out here. When we were blurring, we centered
    μ on a pixel. We’ll take the same approach, but instead of using the bell curve,
    we’ll use its first derivative, shown in [Figure 14-10](ch14.xhtml#ch14fig10),
    which plots the slope of the curve from [Figure 14-3](ch14.xhtml#ch14fig03).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式测量亮度变化有一个问题——变化发生在像素之间的空隙中。我们希望变化发生在像素的中间。让我们看看高斯是否能帮助我们解决这个问题。当我们进行模糊处理时，我们将μ集中在一个像素上。我们将采用相同的方法，但不是使用钟形曲线，而是使用它的第一次导数，如[图14-10](ch14.xhtml#ch14fig10)所示，该图绘制了[图14-3](ch14.xhtml#ch14fig03)中曲线的斜率。
- en: '![Image](../images/14fig10.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/14fig10.jpg)'
- en: '*Figure 14-10: Slope of the Gaussian curve from [Figure 14-3](ch14.xhtml#ch14fig03)*'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*图14-10：来自[图14-3](ch14.xhtml#ch14fig03)的高斯曲线斜率*'
- en: If we call the positive and negative peaks of the curve +1 and –1 and center
    those over the neighboring pixels, the change of brightness for a pixel is Δbrightness[*n*]
    = 1 × pixel[*n* – 1] – 1 × pixel[*n* + 1]. You can see how this plays out in [Figure
    14-11](ch14.xhtml#ch14fig11).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将曲线的正峰和负峰分别标记为+1和–1，并将它们集中在相邻的像素上，那么一个像素的亮度变化可以表示为Δbrightness[*n*] = 1 ×
    pixel[*n* – 1] – 1 × pixel[*n* + 1]。你可以在[图14-11](ch14.xhtml#ch14fig11)中看到这一变化。
- en: '![Image](../images/14fig11.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/14fig11.jpg)'
- en: '*Figure 14-11: Brightness change centered on pixels*'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*图14-11：以像素为中心的亮度变化*'
- en: Of course, this has the same image edge problem that we saw in the last section,
    so we don’t have values for the end pixels. At the moment, we don’t care about
    the direction of change, just the amount, so we calculate the *magnitude* by taking
    the absolute value.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这和上一节中看到的图像边缘问题一样，所以我们没有端点像素的值。目前，我们不关心变化的方向，只关心变化的量，因此我们通过取绝对值来计算*大小*。
- en: Detecting edges this way works pretty well, but many people have tried to improve
    on it. One of the winning approaches, the *Sobel operator*, was announced by American
    scientist Irwin Sobel along with Gary Feldman in a 1968 paper.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式检测边缘效果很好，但很多人尝试对其进行改进。一个成功的方法是*Sobel算子*，它由美国科学家Irwin Sobel与Gary Feldman在1968年的一篇论文中提出。
- en: Similar to what we did with our Gaussian blur kernel, we generate a Sobel edge
    detection kernel using values from the slope of the Gaussian curve. We saw the
    two-dimensional version in [Figure 14-10](ch14.xhtml#ch14fig10), and [Figure 14-12](ch14.xhtml#ch14fig12)
    shows the three-dimensional version.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于我们使用高斯模糊卷积核时所做的，我们利用高斯曲线的斜率值生成了一个 Sobel 边缘检测卷积核。我们在[图 14-10](ch14.xhtml#ch14fig10)中看到了二维版本，而[图
    14-12](ch14.xhtml#ch14fig12)展示了三维版本。
- en: '![Image](../images/14fig12.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/14fig12.jpg)'
- en: '*Figure 14-12: Three-dimensional slope of Gaussian curve from [Figure 14-10](ch14.xhtml#ch14fig10)*'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 14-12：来自[图 14-10](ch14.xhtml#ch14fig10)的高斯曲线的三维斜率*'
- en: Since this isn’t symmetrical around both axes, Sobel used two versions—one for
    the horizontal direction and another for the vertical. [Figure 14-13](ch14.xhtml#ch14fig13)
    shows both kernels.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这在两个轴上并不对称，Sobel 使用了两个版本——一个用于水平方向，另一个用于垂直方向。[图 14-13](ch14.xhtml#ch14fig13)展示了这两个卷积核。
- en: '![Image](../images/14fig13.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/14fig13.jpg)'
- en: '*Figure 14-13: Sobel kernels*'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 14-13：Sobel 卷积核*'
- en: Applying these kernels produces a pair of *gradients*, G*[x]* and G*[y]*, for
    each pixel; you can think of a gradient as a slope. Since we have a gradient in
    each Cartesian direction, we can use trigonometry to convert them into polar coordinates,
    yielding a *magnitude G* and a *direction* θ, as shown in [Figure 14-14](ch14.xhtml#ch14fig14).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 应用这些卷积核会为每个像素产生一对*梯度*，G*[x]* 和 G*[y]*；你可以将梯度视为斜率。由于我们在每个笛卡尔方向上都有一个梯度，我们可以利用三角学将其转换为极坐标，从而得到*大小
    G*和*方向*θ，如[图 14-14](ch14.xhtml#ch14fig14)所示。
- en: '![Image](../images/14fig14.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/14fig14.jpg)'
- en: '*Figure 14-14: Gradient magnitude and direction*'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 14-14：梯度的大小和方向*'
- en: The gradient magnitude tells us how “strong” an edge we have, and the direction
    gives us its orientation. Keep in mind that direction is perpendicular to the
    object; a horizontal edge has a vertical gradient.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度的大小告诉我们边缘的“强度”，而方向则告诉我们其方向。请记住，方向是垂直于物体的；水平边缘的梯度是垂直方向的。
- en: You might have noticed that the magnitude and direction calculation is really
    just the transformation from Cartesian to polar coordinates that we saw in [Chapter
    11](ch11.xhtml#ch11). Changing coordinate systems is a handy trick. In this case,
    once we’re in polar coordinates, we don’t have to worry about division by zero
    or huge numbers when denominators get small. Most math libraries have a function
    of the form `atan2(y, x)` that calculates the arctangent without division.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，大小和方向的计算实际上只是我们在[第 11 章](ch11.xhtml#ch11)中看到的从笛卡尔坐标到极坐标的转换。坐标系的变化是一种便捷的技巧。在这种情况下，一旦我们进入极坐标，我们就不必担心分母为零或分母很小时出现的巨大数值问题。大多数数学库都有一种形式为`atan2(y,
    x)`的函数，它可以计算反正切而无需进行除法运算。
- en: '[Figure 14-15](ch14.xhtml#ch14fig15) shows the gradient magnitudes for both
    images.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 14-15](ch14.xhtml#ch14fig15)展示了两幅图像的梯度大小。'
- en: '![Image](../images/14fig15.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/14fig15.jpg)'
- en: '*Figure 14-15: Sobel magnitudes for blurred cat and meatloaf*'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 14-15：模糊的猫和肉饼的 Sobel 梯度大小*'
- en: There’s an additional issue with the direction, which is that it has more information
    than we can use. Take a look at [Figure 14-16](ch14.xhtml#ch14fig16).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 方向上还有一个额外的问题，那就是它包含了比我们能使用的更多的信息。请看[图 14-16](ch14.xhtml#ch14fig16)。
- en: '![Image](../images/14fig16.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/14fig16.jpg)'
- en: '*Figure 14-16: Pixel neighbors*'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 14-16：像素邻居*'
- en: As you can see, because a pixel has only eight neighbors, there are really only
    four directions we care about. [Figure 14-17](ch14.xhtml#ch14fig17) shows how
    the direction is quantized into the four “bins.”
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，由于一个像素只有八个邻居，实际上只有四个方向是我们关心的。[图 14-17](ch14.xhtml#ch14fig17)展示了方向是如何被量化为四个“箱子”的。
- en: '![Image](../images/14fig17.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/14fig17.jpg)'
- en: '*Figure 14-17: Gradient direction bins*'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 14-17：梯度方向箱子*'
- en: '[Figure 14-18](ch14.xhtml#ch14fig18) shows the binned Sobel directions for
    the blurred images. You can see the correspondence between the directions and
    the magnitudes. The top row is the horizontal bin followed by the diagonally up
    bin, the vertical bin, and the diagonally down bin.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 14-18](ch14.xhtml#ch14fig18)展示了模糊图像的 Sobel 方向。你可以看到方向和大小之间的对应关系。第一行是水平方向的箱子，接下来是上对角线方向箱子、垂直方向箱子，以及下对角线方向箱子。'
- en: '![Image](../images/14fig18.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/14fig18.jpg)'
- en: '*Figure 14-18: Sobel directions for blurred cat and meatloaf*'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 14-18：模糊的猫和肉饼的 Sobel 方向*'
- en: As you can see, the Sobel operator is finding edges, but they’re not great edges.
    They’re fat, which makes it possible to mistake them for object features. Skinny
    edges would eliminate this problem, and we can use the Sobel directions to help
    find them.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，Sobel算子能找到边缘，但这些边缘并不完美。它们很粗，这可能会让人误将它们当作物体的特征。细边缘可以消除这个问题，而且我们可以利用Sobel方向来帮助找到它们。
- en: '***Canny***'
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***凯尼***'
- en: Australian computer scientist John Canny improved on edge detection in 1986
    by adding some additional steps to the Sobel result. The first is *nonmaximum
    suppression*. Looking back at [Figure 14-15](ch14.xhtml#ch14fig15), you can see
    that some of the edges are fat and fuzzy; it’ll be easier later to figure out
    the features in our image if the edges are skinny. Nonmaximum suppression is a
    technique for *edge thinning*.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 澳大利亚计算机科学家约翰·凯尼（John Canny）在1986年通过在Sobel结果中增加一些额外的步骤，改进了边缘检测。第一个步骤是*非最大值抑制*。回顾[图
    14-15](ch14.xhtml#ch14fig15)，你可以看到一些边缘很粗且模糊；如果边缘更细，稍后我们将更容易提取图像中的特征。非最大值抑制是一种*边缘细化*技术。
- en: Here’s the plan. We compare the gradient magnitude of each pixel with that of
    its neighbors in the direction of the gradient. If its magnitude is greater than
    that of the neighbors, the value is preserved; otherwise, it’s suppressed by being
    set to 0 (see [Figure 14-19](ch14.xhtml#ch14fig19)).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这是计划。我们将每个像素的梯度大小与其邻居在梯度方向上的梯度大小进行比较。如果它的梯度大小大于邻居的，就保留该值；否则，将其设为0，从而抑制该值（见[图
    14-19](ch14.xhtml#ch14fig19)）。
- en: '![Image](../images/14fig19.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/14fig19.jpg)'
- en: '*Figure 14-19: Nonmaximum suppression*'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 14-19：非最大值抑制*'
- en: You can see in [Figure 14-19](ch14.xhtml#ch14fig19) that the center pixel on
    the left is kept, as it has a greater magnitude (that is, it’s lighter) than its
    neighbors, while the center pixel on the right is suppressed because its neighbors
    have greater magnitude.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[图 14-19](ch14.xhtml#ch14fig19)中看到，左侧的中心像素被保留，因为它的梯度大小大于邻居（即，它比较亮），而右侧的中心像素则被抑制，因为它的邻居具有更大的梯度大小。
- en: '[Figure 14-20](ch14.xhtml#ch14fig20) shows how nonmaximum suppression thins
    the edges produced by the Sobel operator.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 14-20](ch14.xhtml#ch14fig20)展示了非最大值抑制如何细化由Sobel算子产生的边缘。'
- en: '![Image](../images/14fig20.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/14fig20.jpg)'
- en: '*Figure 14-20: Nonmaximum suppression cat and meatloaf results*'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 14-20：非最大值抑制猫和肉饼结果*'
- en: This is looking pretty good, although it makes a good case for meatloaf avoidance.
    Nonmaximum suppression found a lot of edges in the images. If you look back at
    [Figure 14-15](ch14.xhtml#ch14fig15), you’ll see that many of these edges have
    low gradient magnitudes. The final step in Canny processing is *edge tracking
    with hysteresis*, which removes “weak edges,” leaving only “strong edges.”
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来相当不错，尽管这也为避免肉饼提供了有力的理由。非最大值抑制发现了图像中的很多边缘。如果你回头看看[图 14-15](ch14.xhtml#ch14fig15)，你会看到这些边缘中的许多具有较低的梯度大小。凯尼处理的最后一步是*使用滞后效应的边缘追踪*，它去除了“弱边缘”，只留下了“强边缘”。
- en: Back in [Chapter 2](ch02.xhtml#ch02), you learned that hysteresis involves comparison
    against a pair of thresholds. We’re going to scan the nonmaximum suppression results
    looking for edge pixels (white in [Figure 14-20](ch14.xhtml#ch14fig20)) whose
    gradient magnitude is greater than a high threshold. When we find one, we’ll make
    it a final edge pixel. We’ll then look at its neighbors. Any neighbor whose gradient
    magnitude is greater than a low threshold is also marked as a final edge pixel.
    We follow each of these paths using recursion until we hit a gradient magnitude
    that’s less than the low threshold. You can think of this as starting on a clear
    edge and tracing its connections until they peter out. You can see the results
    in [Figure 14-21](ch14.xhtml#ch14fig21). The strong edges are white; the rejected
    weak edges are gray.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](ch02.xhtml#ch02)中，你学过滞后效应涉及与一对阈值的比较。我们将扫描非最大值抑制结果，寻找梯度大小大于高阈值的边缘像素（见[图
    14-20](ch14.xhtml#ch14fig20)中的白色像素）。当我们找到一个时，将其标记为最终的边缘像素。然后，我们会检查它的邻居。任何梯度大小大于低阈值的邻居，也将被标记为最终的边缘像素。我们会使用递归沿着这些路径进行追踪，直到遇到梯度大小小于低阈值的地方。你可以把它想象成从一个明显的边缘开始，并追踪它的连接，直到它们逐渐消失。你可以在[图
    14-21](ch14.xhtml#ch14fig21)中看到结果。强边缘是白色的，被拒绝的弱边缘是灰色的。
- en: '![Image](../images/14fig21.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/14fig21.jpg)'
- en: '*Figure 14-21: Edge tracking with hysteresis*'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 14-21：使用滞后效应的边缘追踪*'
- en: You can see that a lot of the edges from nonmaximum suppression are gone and
    the object edges are fairly visible.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，许多非最大值抑制后的边缘消失了，物体边缘变得更明显。
- en: There is a great open source library for computer vision called *OpenCV* that
    you can use to play with all sorts of image processing, including what we’ve covered
    in this chapter.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个非常棒的计算机视觉开源库，叫做*OpenCV*，你可以使用它来进行各种图像处理，包括我们在本章中讨论的内容。
- en: '***Feature Extraction***'
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***特征提取***'
- en: The next step is easy for people but difficult for computers. We want to extract
    features from the images in [Figure 14-21](ch14.xhtml#ch14fig21). I’m not going
    to cover feature extraction in detail because it involves a lot of math that you
    probably haven’t yet encountered, but we’ll touch on the basics.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步对人类来说很容易，但对计算机却很困难。我们希望从[图 14-21](ch14.xhtml#ch14fig21)中的图像提取特征。我不会详细讲解特征提取，因为它涉及很多你可能还没接触过的数学，但我们会触及一些基础内容。
- en: There are a large number of feature extraction algorithms. Some, like the *Hough
    transform*, are good for extracting geometric shapes such as lines and circles.
    That’s not very useful for our problem because we’re not looking for geometric
    shapes. Let’s do something simple. We’ll scan our image for edges and follow them
    to extract objects. We’ll take the shortest path if we find edges that cross.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 有大量的特征提取算法。有些像*霍夫变换*，适合提取几何形状，如直线和圆形。对于我们的问题，这不是很有用，因为我们并不寻找几何形状。我们来做一些简单的事。我们将扫描图像中的边缘，并沿着它们提取物体。如果我们发现交叉的边缘，将选择最短路径。
- en: This gives us blobs, ears, cat toys, and squigglies, as shown in [Figure 14-22](ch14.xhtml#ch14fig22).
    Only a representative sample is shown.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这给了我们斑点、耳朵、猫玩具和曲线，正如[图 14-22](ch14.xhtml#ch14fig22)所示。这里只展示了一个代表性的样本。
- en: '![Image](../images/14fig22.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/14fig22.jpg)'
- en: '*Figure 14-22: Extracted features*'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 14-22：提取的特征*'
- en: 'Now that we have these features, we can do just what we did with our earlier
    spam detection example: feed them into classifiers (as shown in [Figure 14-23](ch14.xhtml#ch14fig23)).
    The classifier inputs marked + indicate that there’s some chance that the feature
    is indicative of our desired result, while – means that it’s counterindicative
    and 0 means that it has no contribution.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们拥有了这些特征，我们可以做和之前的垃圾邮件检测示例相同的事情：将它们输入到分类器中（如[图 14-23](ch14.xhtml#ch14fig23)所示）。分类器输入中标记为+表示该特征有可能指示我们期望的结果，而-表示它是相反的，0则表示它没有任何贡献。
- en: '![Image](../images/14fig23.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/14fig23.jpg)'
- en: '*Figure 14-23: Feature classification*'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 14-23：特征分类*'
- en: Notice that there’s information in our images that can be used to improve on
    naive classification, such as the cat toys. They’re commonly found near cats but
    rarely associated with meatloaves.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们的图像中有些信息可以用来改进简单的分类，比如猫玩具。它们通常出现在猫附近，但很少与肉饼有关。
- en: This example isn’t good for much other than showing the steps of feature classification,
    which are summarized in [Figure 14-24](ch14.xhtml#ch14fig24).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例除了展示特征分类的步骤外，几乎没有其他用途，这些步骤总结在[图 14-24](ch14.xhtml#ch14fig24)中。
- en: '![Image](../images/14fig24.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/14fig24.jpg)'
- en: '*Figure 14-24: Image recognition pipeline*'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 14-24：图像识别管道*'
- en: While meatloaves are mostly sedentary, cats move around a lot and have a plethora
    of cute poses. Our example will work only for the objects in our sample images;
    it’s not going to recognize the image in [Figure 11-44](ch11.xhtml#ch11fig44)
    on [page 323](ch11.xhtml#page_323) as a cat. And because of context issues, it
    has little chance of recognizing that the cat in [Figure 14-25](ch14.xhtml#ch14fig25)
    *is* Meat Loaf.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管肉饼大多数时间都静止不动，猫却四处奔走，并且有各种各样可爱的姿势。我们的示例只适用于样本图像中的物体；它不会将[图 11-44](ch11.xhtml#ch11fig44)中的图像（见[第323页](ch11.xhtml#page_323)）识别为猫。而且由于上下文问题，它几乎不可能识别[图
    14-25](ch14.xhtml#ch14fig25)中的猫*就是*肉饼。
- en: '![Image](../images/14fig25.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/14fig25.jpg)'
- en: '*Figure 14-25: Cat or Meat Loaf?*'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 14-25：猫还是肉饼？*'
- en: '***Neural Networks***'
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***神经网络***'
- en: At a certain level, it doesn’t really matter what data we use to represent objects.
    We need to be able to deal with the huge amount of variation in the world. Just
    like people, computers can’t change the inputs. We need better classifiers to
    deal with the variety.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在某个层面上，使用什么数据来表示物体并不重要。我们需要能够处理世界上大量的变化。就像人类一样，计算机不能改变输入。我们需要更好的分类器来处理这些变化。
- en: One of the approaches used in artificial intelligence is to mimic human behavior.
    We’re pretty sure that *neurons* play a big part. Humans have about 86 billion
    neurons, although they’re not all in the “brain”—nerve cells are also neurons,
    which is possibly why some people think with their gut.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能中的一种方法是模仿人类行为。我们相当确定*神经元*在其中起着重要作用。人类大约有860亿个神经元，尽管它们并不全在“大脑”中——神经细胞也是神经元，这可能就是为什么有些人会用直觉来思考。
- en: You can think of a neuron as a cross between logic gates from [Chapter 2](ch02.xhtml#ch02)
    and analog comparators from [Chapter 6](ch06.xhtml#ch06). A simplified diagram
    of a neuron is shown in [Figure 14-26](ch14.xhtml#ch14fig26).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以把神经元想象成[第二章](ch02.xhtml#ch02)中的逻辑门和[第六章](ch06.xhtml#ch06)中的模拟比较器的结合体。神经元的简化图示见[图14-26](ch14.xhtml#ch14fig26)。
- en: '![Image](../images/14fig26.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/14fig26.jpg)'
- en: '*Figure 14-26: Neuron*'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '*图14-26：神经元*'
- en: The *dendrites* are the inputs, and the *axon* is the output. The *axon terminals*
    are just connections from the axon to other neurons; neurons only have a single
    output. Neurons differ from something like an AND gate in that not all inputs
    are treated equally. Take a look at [Figure 14-27](ch14.xhtml#ch14fig27).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '*树突*是输入，而*轴突*是输出。*轴突末端*只是轴突与其他神经元的连接；神经元只有一个输出。神经元不同于像与门（AND gate）这样的逻辑门，因为并非所有输入都会被同等对待。请查看[图14-27](ch14.xhtml#ch14fig27)。'
- en: '![Image](../images/14fig27.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/14fig27.jpg)'
- en: '*Figure 14-27: Simplified gate model of a neuron*'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '*图14-27：神经元的简化门模型*'
- en: 'The value of each dendrite input is multiplied by some *weight*, and then all
    of the weighted values are added together. This is similar to a Bayes classifier.
    If these values are less than the *action potential*, the comparator output is
    `false`; otherwise, it’s `true`, causing the neuron to *fire* by setting the flip-flop
    output to `true`. The axon output is a *pulse*; as soon as it goes to `true`,
    the flip-flop is reset and goes back to `false`. Or, if you learned neuroscience
    from Mr. Miyagi: ax-on, ax-off. Neuroscientists might quibble with the depiction
    of the comparator as having hysteresis; real neurons do, but it’s time-dependent,
    which this model isn’t.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 每个树突输入的值都会乘以某个*权重*，然后所有加权后的值会被加在一起。这类似于贝叶斯分类器。如果这些值小于*动作电位*，比较器输出为`false`；否则，输出为`true`，使得神经元通过将触发器输出设置为`true`来*激活*。轴突输出是一个*脉冲*；一旦它变为`true`，触发器会被重置并回到`false`。或者，如果你是从宫崎老师那里学的神经科学：轴开，轴关。神经科学家可能会对比较器具有滞后的表现提出异议；真实的神经元确实有，但它是时间依赖的，而这个模型并没有考虑这一点。
- en: Neurons are like gates in that they’re “simple” but can be connected together
    to make complicated “circuits,” or *neural networks*. The key takeaway from neurons
    is that they fire based on their weighted inputs. Multiple combinations of inputs
    can cause a neuron to fire.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元像逻辑门一样，它们是“简单”的，但可以连接在一起形成复杂的“电路”，即*神经网络*。从神经元中可以得出的关键结论是，它们基于加权输入触发。多个输入组合可以导致神经元激活。
- en: The first attempt at an artificial neuron was the *perceptron* invented by American
    psychologist Frank Rosenblatt (1928–1971). A diagram is shown in [Figure 14-28](ch14.xhtml#ch14fig28).
    An important aspect of perceptrons is that the inputs and outputs are binary;
    they can only have values of `0` or `1`. The weights and threshold are real numbers.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个人工神经元的尝试是由美国心理学家弗兰克·罗森布拉特（1928–1971）发明的*感知机*。在[图14-28](ch14.xhtml#ch14fig28)中展示了一个示意图。感知机的一个重要方面是其输入和输出是二进制的；它们只能取`0`或`1`的值。权重和阈值是实数。
- en: '![Image](../images/14fig28.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/14fig28.jpg)'
- en: '*Figure 14-28: Perceptron*'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '*图14-28：感知机*'
- en: Perceptrons created a lot of excitement in the AI world. But then it was discovered
    that they didn’t work for certain classes of problems. This, among other factors,
    led to what was called the “AI winter” during which funding dried up.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机在人工智能领域引起了极大的关注。但随后人们发现它们无法解决某些问题类别。这一发现，加上其他因素，导致了所谓的“人工智能冬天”，期间资金支持枯竭。
- en: It turns out that the problem was in the way that the perceptrons were being
    used. They were organized as a single “layer,” as shown in [Figure 14-29](ch14.xhtml#ch14fig29),
    where each circle is a perceptron.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，问题出在感知机的使用方式上。它们被组织成一个单一的“层”，如[图14-29](ch14.xhtml#ch14fig29)所示，其中每个圆圈都是一个感知机。
- en: '![Image](../images/14fig29.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/14fig29.jpg)'
- en: '*Figure 14-29: Single-layer neural network*'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '*图14-29：单层神经网络*'
- en: Inputs can go to multiple perceptrons, each of which makes one decision and
    produces an output. Many of the issues with perceptrons were solved by the invention
    of the multilayer neural network, as shown in [Figure 14-30](ch14.xhtml#ch14fig30).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 输入可以传递给多个感知器，每个感知器做出一个决策并产生一个输出。感知器的许多问题通过多层神经网络的发明得到了解决，如[图14-30](ch14.xhtml#ch14fig30)所示。
- en: '![Image](../images/14fig30.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/14fig30.jpg)'
- en: '*Figure 14-30: Multilayer neural network*'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '*图14-30：多层神经网络*'
- en: This is also known as a *feedforward network* since the outputs produced from
    each layer are fed forward into the next layer. There can be an arbitrary number
    of *hidden layers*, so named because they’re not connected to either the inputs
    or outputs. Although [Figure 14-30](ch14.xhtml#ch14fig30) shows the same number
    of neurons in each layer, that’s not a requirement. Determining the number of
    layers and number of neurons per layer for a particular problem is a black art
    outside the scope of this book. Neural networks like this are much more capable
    than simple classifiers.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这也被称为*前馈网络*，因为每一层的输出会传递到下一层。可以有任意数量的*隐藏层*，之所以这样命名是因为它们既不与输入层相连，也不与输出层相连。虽然[图14-30](ch14.xhtml#ch14fig30)显示了每层相同数量的神经元，但这并不是必要条件。确定特定问题所需的层数和每层神经元数量是一门黑科技，超出了本书的讨论范围。像这样的神经网络比简单的分类器要强大得多。
- en: Neuroscientists don’t yet know how dendrite weights are determined. Computer
    scientists had to come up with something because otherwise artificial neurons
    would be useless. The digital nature of perceptrons makes this difficult because
    small changes in weights don’t result in proportional changes in the output; it’s
    an all-or-nothing thing. A different neuron design, the *sigmoid neuron*, addresses
    this problem by replacing the perceptron comparator with a *sigmoid function*,
    which is just a fancy name for a function with an S-shaped curve. [Figure 14-31](ch14.xhtml#ch14fig31)
    shows both the perceptron transfer function and the sigmoid function. Sure looks
    a lot like our discussion of analog and digital back in [Chapter 2](ch02.xhtml#ch02),
    doesn’t it?
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 神经科学家目前还不知道树突的权重是如何确定的。计算机科学家必须想出一种方法，否则人工神经元将毫无用处。感知器的数字化特性使得这一点变得困难，因为权重的微小变化不会导致输出的比例性变化；它是一个全或无的问题。另一种神经元设计，*sigmoid
    神经元*，通过用一个*sigmoid 函数*替换感知器比较器来解决这个问题，sigmoid 函数其实只是一个S形曲线的函数。 [图14-31](ch14.xhtml#ch14fig31)展示了感知器的传输函数和sigmoid函数。看起来很像我们在[第2章](ch02.xhtml#ch02)中讨论的模拟与数字的区别，对吧？
- en: '![Image](../images/14fig31.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/14fig31.jpg)'
- en: '*Figure 14-31: Artificial neuron transfer functions*'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '*图14-31：人工神经元传输函数*'
- en: The guts of a sigmoid neuron are shown in [Figure 14-32](ch14.xhtml#ch14fig32).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '[图14-32](ch14.xhtml#ch14fig32)展示了一个 sigmoid 神经元的内部结构。'
- en: '![Image](../images/14fig32.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/14fig32.jpg)'
- en: '*Figure 14-32: Sigmoid neuron*'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '*图14-32：Sigmoid 神经元*'
- en: One thing that’s not obvious here is that the sigmoid neuron inputs and outputs
    are “analog” floating-point numbers. Just for accuracy, there’s also a bias used
    in a sigmoid neuron, but it’s not essential for our understanding here.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这里一个不太显眼的地方是，sigmoid 神经元的输入和输出是“模拟”的浮动点数。为了准确起见，sigmoid 神经元中还有一个偏置，但它对我们这里的理解并不是必需的。
- en: The weights for neural networks built from sigmoid neurons can be determined
    using a technique called *backpropagation* that’s regularly lost and rediscovered,
    the latter most recently in a 1986 paper by David Rumelhart (1942–2011), Geoffrey
    Hinton, and Roland Williams. Backpropagation uses wads of linear algebra, so we’re
    going to gloss over the details. You’ve probably learned how to solve simultaneous
    equations in algebra; linear algebra comes into play when there are large numbers
    of equations with large numbers of variables.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 sigmoid 神经元构建的神经网络的权重可以通过一种叫做*反向传播*的技术来确定，这项技术曾多次被遗忘并重新发现，最近一次是在1986年由David
    Rumelhart（1942–2011）、Geoffrey Hinton和Roland Williams的论文中提出。反向传播涉及大量的线性代数，因此我们将略过具体细节。你可能已经学会了如何解代数中的联立方程；当方程和变量的数量都很大时，线性代数就会派上用场。
- en: The general idea behind backpropagation is that we provide inputs for something
    known, such as cat features. The output is examined, and if we know that the inputs
    represent a cat, we’d expect the output to be `1` or pretty close to it. We can
    calculate an *error function*, which is the actual output subtracted from the
    desired output. The weights are then adjusted to make the error function value
    as close to `0` as possible.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播的基本思想是，我们提供一些已知的输入，例如猫的特征。然后检查输出，如果我们知道这些输入代表猫，我们希望输出是`1`或者非常接近它。接下来，我们可以计算一个*误差函数*，即实际输出减去期望输出。然后调整权重，以使误差函数值尽可能接近`0`。
- en: This is commonly done using an algorithm called *gradient descent*, which is
    a lot like Dante’s descent into hell if you don’t like math. Let’s get a handle
    on it using a simple example. Remember that “gradient” is just another word for
    “slope.” We’ll try different values for the weights and plot the value of the
    error function. It might look something like [Figure 14-33](ch14.xhtml#ch14fig33),
    which resembles one of those relief maps that shows mountains, valleys, and so
    on.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常通过一种叫做*梯度下降法*的算法来完成，如果你不喜欢数学的话，它就像但丁的地狱之旅。让我们通过一个简单的例子来理解它。记住，“梯度”只是“斜率”的另一种说法。我们将尝试不同的权重值，并绘制误差函数的值。它可能看起来像[图
    14-33](ch14.xhtml#ch14fig33)，类似于那种展示山脉、山谷等的地形图。
- en: All that’s involved in gradient descent is to roll a ball around on the map
    until it lands in the deepest valley. That’s where the value of the error function
    is at its minimum. We set the weights to the values that represent the ball’s
    position. The reason that this algorithm gets a fancy name is that we’re doing
    this for very large numbers of weights, not just the two in our example. And it’s
    even more complicated when there are weights in multiple layers, such as we saw
    in [Figure 14-30](ch14.xhtml#ch14fig30).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降法所涉及的只是将一个球在地图上滚动，直到它落入最深的山谷。那就是误差函数值最小的地方。我们将权重设置为代表球的位置的值。这个算法之所以有个 fancy
    名字，是因为我们在处理非常多的权重，而不仅仅是我们示例中的两个。当权重存在于多个层时，情况变得更加复杂，就像我们在[图 14-30](ch14.xhtml#ch14fig30)中看到的那样。
- en: '![Image](../images/14fig33.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/14fig33.jpg)'
- en: '*Figure 14-33: Gradient topology*'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 14-33: 梯度拓扑*'
- en: You might have noticed the mysterious disappearance of the output pulsing mechanism
    that we saw in [Figure 14-27](ch14.xhtml#ch14fig27). The neural networks that
    we’ve seen so far are essentially combinatorial logic, not sequential logic, and
    are effectively DAGs. There is a sequential logic variation called a *recurrent
    neural network*. It’s not a DAG, which means that outputs from neurons in a layer
    can connect back to the inputs of neurons in an earlier layer. The storing of
    outputs and clocking of the whole mess is what keeps it from exploding. These
    types of networks perform well for processing sequences of inputs, such as those
    found in handwriting and speech recognition.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能注意到了在[图 14-27](ch14.xhtml#ch14fig27)中看到的输出脉冲机制神秘消失的现象。到目前为止，我们看到的神经网络本质上是组合逻辑，而非顺序逻辑，并且实际上是有向无环图（DAG）。有一种顺序逻辑的变种叫做*递归神经网络*。它不是一个
    DAG，这意味着层中的神经元的输出可以连接回早期层中神经元的输入。存储输出并对整个过程进行时钟控制是防止它崩溃的关键。这种类型的网络在处理输入序列时表现良好，比如手写和语音识别中所遇到的输入。
- en: 'There’s yet another neural network variation that’s especially good for image
    processing: the *convolutional neural network*. You can visualize it as having
    inputs that are an array of pixel values similar to the convolution kernels that
    we saw earlier.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一种特别适合图像处理的神经网络变种：*卷积神经网络*。你可以将它想象成输入是一个像我们之前看到的卷积核一样的像素值数组。
- en: One big problem with neural networks is that they can be “poisoned” by bad training
    data. We can’t tell what sort of unusual behavior might occur in adults who watched
    too much television as kids, and the same is true with machine learning systems.
    There might be a need for machine psychotherapists in the future, although it’s
    hard to imagine sitting down next to a machine and saying, “So tell me how you
    really feel about pictures of cats.”
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的一个大问题是，它们可能会受到糟糕训练数据的“污染”。我们无法知道看了太多电视的孩子在成年后会表现出什么不寻常的行为，机器学习系统也是如此。未来可能需要机器心理治疗师，尽管很难想象坐在机器旁边，跟它说，“那么，告诉我你对猫咪的图片究竟是怎么想的。”
- en: The bottom line on neural networks is that they’re very capable classifiers.
    They can be trained to convert a large amount of input data into a smaller amount
    of outputs that describe the inputs in a way that we desire. Sophisticates might
    call this *reducing dimensionality*. Now we have to figure out what to do with
    that information.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的关键在于，它们是非常有能力的分类器。它们可以被训练将大量输入数据转换为更少的输出，以我们期望的方式描述这些输入。复杂一些的说法是，这叫做*降维*。现在我们需要搞清楚如何利用这些信息。
- en: '***Using Machine Learning Data***'
  id: totrans-184
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***使用机器学习数据***'
- en: How would we build something like a self-driving ketchup bottle using classifier
    outputs? We’ll use the test scenario shown in [Figure 14-34](ch14.xhtml#ch14fig34).
    We’ll move the ketchup bottle one square at a time as if it’s a king on a chessboard
    with the goal of hitting the meatloaf while avoiding the cat.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何利用分类器的输出构建一个像自动驾驶的番茄酱瓶这样的物体？我们将使用[图14-34](ch14.xhtml#ch14fig34)中展示的测试场景。我们将把番茄酱瓶按棋盘上的国王方式，一次移动一个方格，目标是击中肉饼，同时避开猫。
- en: '![Image](../images/14fig34.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/14fig34.jpg)'
- en: '*Figure 14-34: Test scenario*'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '*图14-34：测试场景*'
- en: In this “textbook example,” the classifiers give us the positions of the cat
    and meatloaf. Since the shortest distance between two points is a straight line,
    and since we’re on an integer grid, the most efficient way to reach the meatloaf
    is to use the Bresenham line-drawing algorithm from “[Straight Lines](ch11.xhtml#ch11lev2sec4)”
    on [page 292](ch11.xhtml#page_292). Of course, it’ll have to be modified as shown
    in [Figure 14-35](ch14.xhtml#ch14fig35), because cats and condiments don’t go
    well together.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个“教科书示例”中，分类器给出了猫和肉饼的位置。由于两点之间的最短距离是一条直线，而且我们是在整数网格上，最有效的到达肉饼的方式是使用[“直线”](ch11.xhtml#ch11lev2sec4)中的布雷森汉姆直线算法，[第292页](ch11.xhtml#page_292)。当然，它必须按照[图14-35](ch14.xhtml#ch14fig35)所示进行修改，因为猫和调味品不太搭配。
- en: '![Image](../images/14fig35.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/14fig35.jpg)'
- en: '*Figure 14-35: Autonomous ketchup bottle algorithm*'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '*图14-35：自动化番茄酱瓶算法*'
- en: As you can see, it’s pretty simple. We make a beeline for the meatloaf, and
    if the cat’s in the way, we jog and make another beeline for the meatloaf. Of
    course, this gets much more complicated in the real world, where the cat can move
    and there may be other obstacles.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这很简单。我们直奔肉饼，如果猫挡道，我们就绕开它，继续朝肉饼前进。当然，在现实世界中，这会变得更加复杂，因为猫可以移动，而且可能还会有其他障碍物。
- en: Now let’s look at one of the other machine intelligence subfields to see a different
    way to approach this problem.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下另一个机器智能的子领域，看看如何以不同的方式解决这个问题。
- en: '**Artificial Intelligence**'
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**人工智能**'
- en: Early artificial intelligence results, such as learning to play checkers and
    solve various logic problems, were exciting and produced a lot of funding. Unfortunately,
    these early successes didn’t scale to harder problems, and funding dried up.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的人工智能成果，如学习下跳棋和解决各种逻辑问题，令人兴奋并且获得了大量资金支持。不幸的是，这些早期的成功并未扩展到更困难的问题，资金也因此枯竭。
- en: One of the attendees at the 1956 Dartmouth workshop where the term was coined
    was American scientist John McCarthy (1927–2011), who designed the LISP programming
    language while at the Massachusetts Institute of Technology. This language was
    used for much of the early AI work, and LISP officially stands for *List Processor*—but
    anybody familiar with the language syntax knows it as *Lots of Insipid Parentheses*.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 1956年达特茅斯研讨会的与会者之一是美国科学家约翰·麦卡锡（John McCarthy，1927–2011），他在麻省理工学院设计了LISP编程语言。这种语言广泛用于早期的人工智能工作，LISP正式代表*列表处理器*（List
    Processor），但任何熟悉该语言语法的人都会知道它也被称为*大量无聊的括号*。
- en: LISP introduced several new concepts to high-level programming languages. Of
    course, that wasn’t hard back in 1958, since only one other high-level language
    existed at the time (FORTRAN). In particular, LISP included singly linked lists
    (see [Chapter 7](ch07.xhtml#ch07)) as a data type with programs as lists of instructions.
    This meant that programs could modify themselves, which is an important distinction
    with machine learning systems. A neural network can adjust weights but can’t change
    its algorithm. Because LISP can generate code, it can modify or create new algorithms.
    While it’s not quite as clean, JavaScript also supports *self-modifying code*,
    although it’s dangerous to do in the minimally constrained environment of the
    web.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: LISP为高级编程语言引入了几个新概念。当然，在1958年那时这并不困难，因为当时只有另一种高级语言（FORTRAN）。特别地，LISP包括了单链表（见[第7章](ch07.xhtml#ch07)）作为一种数据类型，并且程序是由指令的列表构成。这意味着程序可以修改自身，这是与机器学习系统的一个重要区别。神经网络可以调整权重，但不能改变其算法。由于LISP可以生成代码，它可以修改或创建新的算法。虽然不如干净，但JavaScript也支持*自修改代码*，尽管在网页这一约束最小的环境中这样做是很危险的。
- en: Early AI systems quickly became constrained by the available hardware technology.
    One of the most common machines used for research at the time was the DEC PDP-10,
    whose address space was initially limited to 256K 36-bit words, and eventually
    expanded to 4M. This isn’t enough to run the machine learning examples in this
    chapter. American programmer Richard Greenblatt and computer engineer Tom Knight
    began work in the early 1970s at MIT to develop *Lisp machines*, which were computers
    optimized to run LISP. However, even in their heyday, only a few thousand of these
    machines were ever built, possibly because general-purpose computers were advancing
    at a faster pace.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的人工智能系统很快就受限于当时可用的硬件技术。那时最常用于研究的机器之一是DEC PDP-10，其地址空间最初仅限于256K 36位字，后来扩展到4M。但这不足以运行本章中的机器学习示例。美国程序员理查德·格林布拉特和计算机工程师汤姆·奈特在1970年代初期开始在麻省理工学院开发*Lisp机器*，这是一种优化运行LISP的计算机。然而，即使在它们的鼎盛时期，也只制造了几千台这样的机器，可能是因为通用计算机的发展速度更快。
- en: Artificial intelligence started making a comeback in the 1980s with the introduction
    of *expert systems*. These systems assist users, such as medical professionals,
    by asking questions and guiding them through a knowledge database. This should
    sound familiar to you; it’s a serious application of our “Guess the Animal” game
    from [Chapter 10](ch10.xhtml#ch10). Unfortunately, expert systems seem to have
    matured into annoying phone menus.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能在1980年代重新崛起，随着*专家系统*的出现。这些系统通过提问和引导用户（如医疗专业人员）浏览知识数据库来提供帮助。这对你来说应该很熟悉；它是我们在[第10章](ch10.xhtml#ch10)中“猜动物”游戏的一个实际应用。不幸的是，专家系统似乎已经发展成了令人讨厌的电话菜单。
- en: We’re going to attack our self-driving ketchup bottle problem with a *genetic
    algorithm*, which is a technique that mimics evolution (see [Figure 14-36](ch14.xhtml#ch14fig36)).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用*遗传算法*来解决自动驾驶番茄酱瓶的问题，这是一种模仿进化过程的技术（见[图14-36](ch14.xhtml#ch14fig36)）。
- en: '![Image](../images/14fig36.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/14fig36.jpg)'
- en: '*Figure 14-36: Genetic algorithm*'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '*图14-36：遗传算法*'
- en: We randomly create a set of car *cells* that each have a position and a direction
    of movement. Each cell is moved one step, and then a “goodness” score is calculated,
    in this case using the distance formula. We breed the two best-performing cells
    to create a new one, and we kill off the worst-performing cell. Because it’s evolution,
    we also randomly mutate one of the cells. We keep stepping until one of the cells
    reaches the goal. The steps that the cell took to reach the goal are the generated
    program.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随机创建一组具有位置和运动方向的汽车*单元*。每个单元移动一步，然后计算一个“好坏”评分，在这种情况下使用距离公式。我们繁殖两个表现最好的单元来创建一个新单元，然后淘汰表现最差的单元。由于这是进化过程，我们还随机突变其中一个单元。我们不断地执行这些步骤，直到某个单元到达目标。单元到达目标所走的步骤就是生成的程序。
- en: Let’s look at the results from running the algorithm with 20 cells. We got lucky
    and found the solution shown in [Figure 14-37](ch14.xhtml#ch14fig37) in only 36
    iterations.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看用20个单元运行算法的结果。我们运气不错，在仅仅36次迭代中就找到了[图14-37](ch14.xhtml#ch14fig37)中显示的解决方案。
- en: '![Image](../images/14fig37.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/14fig37.jpg)'
- en: '*Figure 14-37: Good genetic algorithm results*'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '*图14-37：良好的遗传算法结果*'
- en: Our algorithm created the simple program in [Listing 14-1](ch14.xhtml#ch14list01)
    to accomplish the goal. The important point is that a programmer didn’t write
    this program; our AI did it for us. The variables `x` and `y` are the position
    of the ketchup bottle.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的算法在[清单 14-1](ch14.xhtml#ch14list01)中创建了这个简单的程序来完成目标。重要的点是，这个程序不是由程序员编写的；是我们的AI为我们做的。变量`x`和`y`是番茄酱瓶的位置。
- en: '[PRE0]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*Listing 14-1: Generated code*'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 14-1: 生成的代码*'
- en: Of course, being genetics, it’s random and doesn’t always work out so cleanly.
    Another run of the program took 82 iterations to find the solution in [Figure
    14-38](ch14.xhtml#ch14fig38). Might make the case for “intelligent design.”
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，作为基因学，它是随机的，并不总是那么干净利落。程序的另一次运行花费了82次迭代才找到[图 14-38](ch14.xhtml#ch14fig38)中的解决方案。这或许可以为“智能设计”辩护。
- en: '![Image](../images/14fig38.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/14fig38.jpg)'
- en: '*Figure 14-38: Strange genetic algorithm results*'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 14-38: 奇怪的遗传算法结果*'
- en: 'You can see that AI programs can generate surprising results. But they’re not
    all that different from what children come up with when exploring the world; it’s
    just that people are now paying more attention. In fact, many AI results that
    surprise some people were predicted long ago. For example, there was a lot of
    press when a company’s AI systems created their own private language to communicate
    with each other. That’s not a new idea to anybody who has seen the 1970 science
    fiction film *Colossus: The Forbin Project*.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，AI程序可以生成令人惊讶的结果。但它们与孩子们在探索世界时得出的结果并没有太大不同；只不过现在人们关注得更多了。事实上，许多让一些人感到惊讶的AI结果早在很久之前就已经被预测过了。例如，当一家公司的AI系统创造了自己的私人语言相互交流时，媒体报道了大量内容。对于任何看过1970年科幻电影*《巨像：福尔宾计划》*的人来说，这个想法并不新鲜。
- en: '**Big Data**'
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**大数据**'
- en: If it’s not obvious from the examples so far, we’re processing a lot of data.
    An HD (1920×1080) video camera produces around a third of a gigabyte of data every
    second. The Large Hadron Collider generates about 25 GiB/second. It’s estimated
    that network-connected devices generate about 50 GiB/second, up from about 1 MiB/second
    a quarter-century ago. Most of this information is garbage; the challenge is to
    mine the useful parts.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如果从目前的例子中还不明显的话，我们正在处理大量的数据。一台高清（1920×1080）摄像机每秒钟产生大约三分之一GB的数据。大型强子对撞机每秒钟产生大约25
    GiB的数据。估计网络连接的设备每秒钟产生大约50 GiB的数据，而25年前大约只有1 MiB/秒。大部分信息是垃圾；挑战在于如何挖掘出有用的部分。
- en: Big data is a moving target; it refers to data that is too big and complex to
    process using brute-force techniques in the technology of the day. The amount
    of data created 25 years ago is fairly trivial to process using current technology,
    but it wasn’t back then. Data collection is likely to always exceed data analysis
    capabilities, so cleverness is required.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据是一个动态的目标；它指的是那些过于庞大和复杂，无法使用当时技术的蛮力方式处理的数据。25年前产生的数据使用当前技术处理是相当简单的，但当时并非如此。数据收集往往总是超过数据分析的能力，因此需要聪明的办法。
- en: The term *big data* refers not only to analysis but also to the collection,
    storage, and management of the data. For our purposes, we’re concerned with the
    analysis portion.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '*大数据*一词不仅指分析，还包括数据的收集、存储和管理。就我们而言，我们关心的是分析部分。'
- en: A lot of the data that’s collected is personal in nature. It’s not a great idea
    to share your bank account information or medical history with strangers. And
    data that is collected for one purpose is often used for another. The Nazis used
    census data to identify and locate Jews for persecution, for example, and American
    census data was used to locate and round up Japanese Americans for internment,
    despite a provision in the law keeping personally identifiable portions of that
    data confidential for 75 years.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 很多收集到的数据都是个人性质的。与陌生人分享你的银行账户信息或病史并不是一个好主意。而且，为一个目的收集的数据往往被用于另一个目的。例如，纳粹曾利用人口普查数据识别并定位犹太人进行迫害，美国的人口普查数据也曾被用于定位并关押日裔美国人，尽管法律中有一项条款规定，这些数据中的个人身份信息应保密75年。
- en: A lot of data is released for research purposes in “anonymized” form, which
    means that any personally identifying information has been removed. But it’s not
    that simple. Big data techniques can often reidentify individuals from anonymized
    data. And many policies designed to make reidentification difficult have actually
    made it easier.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 许多数据以“匿名化”形式发布用于研究，这意味着任何个人身份信息都已被移除。但事情并不像看起来那么简单。大数据技术通常能够从匿名化数据中重新识别出个人。而许多旨在使重新识别变得困难的政策，实际上反而使它变得更容易。
- en: In America, the Social Security number (SSN) is regularly misused as a personal
    identifier. It was never designed for this use. In fact, a Social Security card
    contains the phrase “not for identification purposes,” which was part of the original
    law—one that’s rarely enforced and now has a huge number of exceptions.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在美国，社会安全号码（SSN）常常被误用为个人身份标识符。它从未设计用于这个用途。事实上，社会安全卡上写有“非身份识别用途”的字样，这是原始法律的一部分——这部分法律很少被执行，并且现在有许多例外。
- en: 'An SSN has three fields: a three-digit area number (AN), a two-digit group
    number (GN), and a four-digit serial number (SN). The area number is assigned
    based on the postal code of the mailing address on the application form. Group
    numbers are assigned in a defined but nonconsecutive order. Serial numbers are
    assigned consecutively.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 社会安全号码（SSN）有三个字段：一个三位数的区域编号（AN），一个两位数的组编号（GN），以及一个四位数的序列号（SN）。区域编号是根据申请表中邮寄地址的邮政编码分配的。组编号按照一个定义的、但不是连续的顺序分配。序列号是按顺序分配的。
- en: A group of Carnegie Mellon researchers published a paper in 2009 that demonstrated
    a method for successfully guessing SSNs. Two things made it easy. First is the
    existence of the Death Master File. (That’s “Death” “Master File,” not “Death
    Master” “File.”) It’s a list of deceased people made available by the Social Security
    Administration ostensibly for fraud prevention. It conveniently includes names,
    birth dates, death dates, SSNs, and postal codes. How does a list of dead people
    help us guess the SSNs of the living?
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 一组卡内基梅隆大学的研究人员在2009年发表了一篇论文，展示了一种成功猜测SSN的方法。有两个因素使得这变得容易。首先是死亡主文件的存在。（那是“死亡”
    “主文件”，而不是“死亡主” “文件”）它是一个由社会保障局提供的已故人员名单，名义上是为了防止欺诈。它方便地包含了姓名、出生日期、死亡日期、社会安全号码（SSN）和邮政编码。死者名单怎么帮助我们猜测活人的SSN呢？
- en: Well, it’s not the only data out there. Voter registration lists include birth
    data, as do many online profiles.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 其实，这并不是唯一的数据来源。选民登记名单也包括出生数据，许多在线个人资料也是如此。
- en: Statistical analysis of the ANs and postal codes in the Death Master File can
    be used to link ANs to geographic areas. The rules for assigning GNs and SNs are
    straightforward. As a result, the Death Master File information can be used to
    map ANs to postal codes. Separately obtained birth data can also be linked to
    postal codes. These two sources of information can be interleaved, sorting by
    birth date. Any gap in the Death Master SSN sequence is a living person whose
    SSN is between the preceding and following Death Master entries. An example is
    shown in [Table 14-2](ch14.xhtml#ch14tab02).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 对死亡主文件中区域编号（AN）和邮政编码进行统计分析，可以将区域编号与地理区域关联。组编号和序列号的分配规则很简单。因此，死亡主文件中的信息可以用来将区域编号与邮政编码关联。单独获得的出生数据也可以与邮政编码关联。这两个信息来源可以交替使用，按出生日期排序。死亡主文件中的任何SSN序列的空白都代表一个活着的人，他们的SSN介于前后死亡主文件条目之间。一个例子见于[表14-2](ch14.xhtml#ch14tab02)。
- en: '**Table 14-2:** Combining Data from Postal Code 89044'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '**表14-2：** 合并来自邮政编码89044的数据'
- en: '| **Death Master File** | **Guessed SSN** | **Birth records** |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| **死亡主文件** | **猜测的社会安全号码（SSN）** | **出生记录** |'
- en: '| --- | --- | --- |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Name** | **DOB** | **SSN** |  | **DOB** | **Name** |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| **姓名** | **出生日期（DOB）** | **社会安全号码（SSN）** |  | **出生日期（DOB）** | **姓名** |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| John Many Jars | 1984-01-10 | 051-51-1234 |  |  |  |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| John Many Jars | 1984-01-10 | 051-51-1234 |  |  |  |'
- en: '| John Fish | 1984-02-01 | 051-51-1235 |  |  |  |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| John Fish | 1984-02-01 | 051-51-1235 |  |  |  |'
- en: '| John Two Horns | 1984-02-12 | 051-51-1236 |  |  |  |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| John Two Horns | 1984-02-12 | 051-51-1236 |  |  |  |'
- en: '|  |  |  | 051-51-1237 | 1984-02-14 | Jon Steinhart |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 051-51-1237 | 1984-02-14 | Jon Steinhart |'
- en: '| John Worfin | 1984-02-20 | 051-51-1238 |  |  |  |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| John Worfin | 1984-02-20 | 051-51-1238 |  |  |  |'
- en: '| John Bigboote | 1984-03-15 | 051-51-1239 |  |  |  |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| John Bigboote | 1984-03-15 | 051-51-1239 |  |  |  |'
- en: '| John Ya Ya | 1984-04-19 | 051-51-1240 |  |  |  |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| John Ya Ya | 1984-04-19 | 051-51-1240 |  |  |  |'
- en: '|  |  |  | 051-51-1241 | 1984-04-20 | John Gilmore |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 051-51-1241 | 1984-04-20 | John Gilmore |'
- en: '| John Fledgling | 1984-05-21 | 051-51-1242 |  |  |  |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| John Fledgling | 1984-05-21 | 051-51-1242 |  |  |  |'
- en: '|  |  |  | 051-51-1243 | 1984-05-22 | John Perry Barlow |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 051-51-1243 | 1984-05-22 | John Perry Barlow |'
- en: '| John Grim | 1984-06-02 | 051-51-1244 |  |  |  |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| John Grim | 1984-06-02 | 051-51-1244 |  |  |  |'
- en: '| John Littlejohn | 1984-06-03 | 051-51-1245 |  |  |  |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| John Littlejohn | 1984-06-03 | 051-51-1245 |  |  |  |'
- en: '| John Chief Crier | 1984-06-12 | 051-51-1246 |  |  |  |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| John Chief Crier | 1984-06-12 | 051-51-1246 |  |  |  |'
- en: '|  |  |  | 051-51-1247 | 1984-07-05 | John Jacob Jingleheimer Schmidt |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 051-51-1247 | 1984-07-05 | John Jacob Jingleheimer Schmidt |'
- en: '| John Small Berries | 1984-08-03 | 051-51-1250 |  |  |  |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| John Small Berries | 1984-08-03 | 051-51-1250 |  |  |  |'
- en: Of course, it’s not quite as straightforward as this example, but it’s not much
    more difficult either. For example, we only know the range for SSN SNs if there’s
    a gap in the Death Master data, such as the one shown between John Chief Crier
    and John Small Berries. Many organizations often ask you for the last four digits
    of your SSN for identification. As you can see from the example, those are the
    hardest ones to guess, so don’t make it easy by giving these out when asked. Push
    for some other means of identification.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这个例子并不像看起来这么简单，但也没有比这更难。例如，只有当死亡主数据中出现间隙时，我们才能知道SSN SN的范围，像约翰·首席报信人（John
    Chief Crier）和约翰·小浆果（John Small Berries）之间的间隙一样。许多组织通常会要求你提供SSN的最后四位数字作为身份验证。如你从示例中看到的，这些是最难猜到的数字，所以不要轻易提供这些信息。要求使用其他身份验证方式。
- en: Here’s another example. The Massachusetts Group Insurance Commission (GIC) released
    anonymized hospital data for the purpose of improving health care and controlling
    costs. Massachusetts governor William Weld assured the public that patient privacy
    was protected. You can probably see where this is going, with the moral being
    that he should have kept his mouth shut.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有另一个例子。麻省集团保险委员会（GIC）发布了匿名化的医院数据，目的是改善医疗保健和控制成本。麻省州长威廉·威尔德向公众保证，患者的隐私得到了保护。你可能已经能猜到接下来会发生什么了，教训就是他应该保持沉默。
- en: Governor Weld collapsed during a ceremony on May 18, 1996, and was admitted
    to the hospital. MIT graduate student Latanya Sweeney knew that the governor resided
    in Cambridge, and she spent $20 to purchase the complete voter rolls for that
    city. She combined the GIC data with the voter data, much as we did in [Table
    14-2](ch14.xhtml#ch14tab02), and easily deanonymized the governor’s data. She
    sent his health records, including prescriptions and diagnoses, to his office.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 威尔德州长在1996年5月18日的一个仪式上倒下，并被送往医院。麻省理工学院的研究生拉塔尼亚·斯威尼（Latanya Sweeney）知道州长住在剑桥市，于是她花了20美元购买了该市的完整选民名单。她将GIC数据与选民数据结合，就像我们在[表14-2](ch14.xhtml#ch14tab02)中做的一样，并轻松地将州长的数据去匿名化。她将他的健康记录，包括处方和诊断，发送到了他的办公室。
- en: While this was a fairly easy case since the governor was a public figure, your
    phone probably has more compute power than was available to Sweeney in 1996\.
    Computing resources today make it possible to tackle much harder cases.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这是一个相对简单的案例，因为州长是公众人物，但你的手机可能拥有比1996年斯威尼使用的计算能力更多的计算资源。如今的计算资源使得处理更复杂的案例成为可能。
- en: '**Summary**'
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**总结**'
- en: We’ve covered a lot of really complicated material in this chapter. You learned
    that machine learning, big data, and artificial intelligence are interrelated.
    You’ve also learned that many more math classes are in store if you want to go
    into this field. No cats were harmed in the creation of this chapter.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中讨论了很多非常复杂的内容。你了解到机器学习、大数据和人工智能是相互关联的。你还了解到，如果你想从事这个领域，你将面临更多的数学课程。制作本章时，没有任何猫受到伤害。
