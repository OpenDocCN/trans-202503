- en: '**10 Neural Networks**'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**10 神经网络**'
- en: '*The human brain has 100 billion neurons, each neuron connected to 10 thousand
    other neurons. Sitting on your shoulders is the most complicated object in the
    known universe.*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*人类大脑有1000亿个神经元，每个神经元与其他1万个神经元相连。你肩膀上的东西是已知宇宙中最复杂的物体。*'
- en: —Michio Kaku
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: —加贺美智夫
- en: '![Image](../images/pg537_Image_824.jpg)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/pg537_Image_824.jpg)'
- en: '***Khipu* on display at the Machu Picchu Museum, Cusco, Peru (photo by Pi3.124)**'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '***马丘比丘博物馆展示的*基普*，位于秘鲁库斯科（照片由Pi3.124提供）***'
- en: The *khipu* (or *quipu*) is an ancient Incan device used for recordkeeping and
    communication. It comprised a complex system of knotted cords to encode and transmit
    information. Each colored string and knot type and pattern represented specific
    data, such as census records or calendrical information. Interpreters, known as
    *quipucamayocs*, acted as a kind of accountant and decoded the stringed narrative
    into understandable information.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*基普*（或*基普*）是古代印加文明用来记录和传递信息的工具。它由一套复杂的结绳系统构成，用来编码和传递信息。每条颜色不同的绳子、结的类型和样式都代表了特定的数据，如人口普查记录或日历信息。被称为*基普卡马约克*的解读者充当了一种会计人员的角色，将这些串联的故事解码成可理解的信息。'
- en: I began with inanimate objects living in a world of forces, and I gave them
    desires, autonomy, and the ability to take action according to a system of rules.
    Next, I allowed those objects, now called *creatures*, to live in a population
    and evolve over time. Now I’d like to ask, What is each creature’s decision-making
    process? How can it adjust its choices by learning over time? Can a computational
    entity process its environment and generate a decision?
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我从生活在力量世界中的无生命物体开始，赋予它们欲望、自治权以及根据规则系统采取行动的能力。接着，我让这些物体（现在称为*生物*）生活在一个群体中并随时间进化。现在我想问，是什么决定了每个生物的决策过程？它如何通过学习随时间调整其选择？一个计算实体能否处理它的环境并生成决策？
- en: To answer these questions, I’ll once again look to nature for inspiration—specifically,
    the human brain. A brain can be described as a biological **neural network**,
    an interconnected web of neurons transmitting elaborate patterns of electrical
    signals. Within each neuron, dendrites receive input signals, and based on those
    inputs, the neuron fires an output signal via an axon (see [Figure 10.1](ch10.xhtml#ch10fig1)).
    Or something like that. How the human brain actually works is an elaborate and
    complex mystery, one that I’m certainly not going to attempt to unravel in rigorous
    detail in this chapter.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这些问题，我再次从自然界寻找灵感——具体来说，是人类的大脑。大脑可以被描述为一种生物学**神经网络**，这是一个相互连接的神经元网络，传递复杂的电信号模式。在每个神经元内部，树突接收输入信号，基于这些输入，神经元通过轴突发出输出信号（见[图
    10.1](ch10.xhtml#ch10fig1)）。或者类似的东西。人类大脑究竟是如何工作的，这仍然是一个复杂且精细的谜团，肯定不是我在本章中打算严格详细解开的问题。
- en: '![Image](../images/pg538_Image_825.jpg)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/pg538_Image_825.jpg)'
- en: 'Figure 10.1: A neuron with dendrites and an axon connected to another neuron'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.1：一个神经元，带有树突和与另一个神经元相连的轴突
- en: Fortunately, as you’ve seen throughout this book, developing engaging animated
    systems with code doesn’t require scientific rigor or accuracy. Designing a smart
    rocket isn’t rocket science, and neither is designing an artificial neural network
    brain science. It’s enough to simply be inspired by the *idea* of brain function.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，正如你在本书中所见，开发引人入胜的动画系统并不需要严格的科学性或准确性。设计一枚智能火箭并不需要火箭科学，设计一个人工神经网络也不需要大脑科学。仅仅受到*大脑功能*这一*概念*的启发就足够了。
- en: In this chapter, I’ll begin with a conceptual overview of the properties and
    features of neural networks and build the simplest possible example of one, a
    network that consists of a single neuron. I’ll then introduce you to more complex
    neural networks by using the ml5.js library. This will serve as a foundation for
    [Chapter 11](ch11.xhtml#ch11), the grand finale of this book, where I’ll combine
    GAs with neural networks for physics simulation.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我将首先概述神经网络的特性和功能，并构建一个最简单的神经网络示例，即由一个神经元组成的网络。然后，我将通过使用ml5.js库向您介绍更复杂的神经网络。这将为[第11章](ch11.xhtml#ch11)奠定基础，本书的高潮部分，我将在那里结合遗传算法与神经网络进行物理模拟。
- en: '**Introducing Artificial Neural Networks**'
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**介绍人工神经网络**'
- en: Computer scientists have long been inspired by the human brain. In 1943, Warren
    S. McCulloch, a neuroscientist, and Walter Pitts, a logician, developed the first
    conceptual model of an artificial neural network. In their paper “A Logical Calculus
    of the Ideas Immanent in Nervous Activity,” they describe a **neuron** as a single
    computational cell living in a network of cells that receives inputs, processes
    those inputs, and generates an output.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学家们长期受到人脑的启发。1943年，神经科学家沃伦·S·麦卡洛克和逻辑学家沃尔特·皮茨开发了第一个人工神经网络的概念模型。在他们的论文《神经活动中固有思想的逻辑演算》中，他们将**神经元**描述为一个计算单元，生活在由多个细胞组成的网络中，接收输入、处理输入并生成输出。
- en: Their work, and the work of many scientists and researchers who followed, wasn’t
    meant to accurately describe how the biological brain works. Rather, an *artificial*
    neural network (hereafter referred to as just a *neural network*) was intended
    as a computational model based on the brain, designed to solve certain kinds of
    problems that were traditionally difficult for computers.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的工作，以及许多后来的科学家和研究人员的工作，并不是为了准确描述生物大脑的工作原理。相反，*人工*神经网络（以下简称*神经网络*）旨在作为一个基于大脑的计算模型，设计来解决传统上对计算机来说很困难的某些问题。
- en: Some problems are incredibly simple for a computer to solve but difficult for
    humans like you and me. Finding the square root of 964,324 is an example. A quick
    line of code produces the value 982, a number my computer can compute in less
    than a millisecond, but if you asked me to calculate that number myself, you’d
    be in for quite a wait. On the other hand, certain problems are incredibly simple
    for you or me to solve, but not so easy for a computer. Show any toddler a picture
    of a kitten or puppy, and they’ll quickly be able to tell you which one is which.
    Listen to a conversation in a noisy café and focus on just one person’s voice,
    and you can effortlessly comprehend their words. But need a machine to perform
    one of these tasks? Scientists have spent entire careers researching and implementing
    complex solutions, and neural networks are one of them.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一些问题对计算机来说非常简单，但对你我这样的普通人来说却很困难。例如，求964,324的平方根。只需一行简单的代码就能得出值982，这个数字我的计算机不到一毫秒就能计算出来，但如果你让我自己计算，我得让你等上很久。另一方面，某些问题对你我来说非常简单，但对计算机来说却不那么容易。给任何一个幼儿看一张小猫或小狗的照片，他们能很快告诉你哪个是哪个。坐在嘈杂的咖啡馆里，专心听某个人的声音，你能轻松理解他们的讲话。但是，如果让机器执行这些任务呢？科学家们为此花费了大半生的时间，研究并实施复杂的解决方案，而神经网络就是其中之一。
- en: 'Here are some of the easy-for-a-human, difficult-for-a-machine applications
    of neural networks in software today:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些今天神经网络在软件中的“人类容易、机器难”的应用：
- en: '**Pattern recognition:** Neural networks are well suited to problems when the
    aim is to detect, interpret, and classify features or patterns within a dataset.
    This includes everything from identifying objects (like faces) in images, to optical
    character recognition, to more complex tasks like gesture recognition.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模式识别：** 神经网络非常适合那些旨在检测、解释和分类数据集中各类特征或模式的问题。这包括从识别图像中的物体（如面部）到光学字符识别，再到更复杂的任务，如手势识别。'
- en: '**Time-series prediction and anomaly detection:** Neural networks are utilized
    both in forecasting, such as predicting stock market trends or weather patterns,
    and in recognizing anomalies, which can be applied to areas like cyberattack detection
    and fraud prevention.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间序列预测与异常检测：** 神经网络既用于预测，如预测股市趋势或天气模式，也用于识别异常，这些可以应用于网络攻击检测和防止欺诈等领域。'
- en: '**Control and adaptive decision-making systems:** These applications range
    from autonomous vehicles like self-driving cars and drones to adaptive decision-making
    used in game playing, pricing models, and recommendation systems on media platforms.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制与自适应决策系统：** 这些应用从自动驾驶汽车和无人机等自主驾驶工具，到游戏玩法、定价模型以及媒体平台上的推荐系统等自适应决策系统不等。'
- en: '**Signal processing and soft sensors:** Neural networks play a crucial role
    in devices like cochlear implants and hearing aids by filtering noise and amplifying
    essential sounds. They’re also involved in *soft sensors*, software systems that
    process data from multiple sources to give a comprehensive analysis of the environment.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**信号处理与软传感器：** 神经网络在耳蜗植入物和助听器等设备中起着至关重要的作用，通过过滤噪音并放大重要声音。它们还参与了*软传感器*的工作，即处理来自多个来源的数据，提供关于环境的全面分析的软件系统。'
- en: '**Natural language processing (NLP):** One of the biggest developments in recent
    years has been the use of neural networks for processing and understanding human
    language. They’re used in various tasks including machine translation, sentiment
    analysis, and text summarization, and are the underlying technology behind many
    digital assistants and chatbots.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自然语言处理（NLP）：** 近年来最大的进展之一就是神经网络在处理和理解人类语言方面的应用。它们被用于多种任务，包括机器翻译、情感分析和文本摘要，是许多数字助手和聊天机器人的核心技术。'
- en: '**Generative models:** The rise of novel neural network architectures has made
    it possible to generate new content. These systems can synthesize images, enhance
    image resolution, transfer style between images, and even generate music and video.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成模型：** 新型神经网络架构的兴起使得生成新内容成为可能。这些系统能够合成图像、提高图像分辨率、在图像之间转移风格，甚至生成音乐和视频。'
- en: Covering the full gamut of applications for neural networks would merit an entire
    book (or series of books), and by the time that book was printed, it would probably
    be out of date. Hopefully, this list gives you an overall sense of the features
    and possibilities.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 覆盖神经网络应用的所有领域可能需要一本完整的书（或一系列书籍），而且等到书出版时，它可能就已经过时了。希望这个列表能给你一个整体的概念，帮助你了解这些功能和可能性。
- en: '**How Neural Networks Work**'
  id: totrans-24
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**神经网络的工作原理**'
- en: 'In some ways, neural networks are quite different from other computer programs.
    The computational systems I’ve been writing so far in this book are **procedural**:
    a program starts at the first line of code, executes it, and goes on to the next,
    following instructions in a linear fashion. By contrast, a true neural network
    doesn’t follow a linear path. Instead, information is processed collectively,
    in parallel, throughout a network of nodes, with each node representing a neuron.
    In this sense, a neural network is considered a **connectionist** system.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 从某些方面来说，神经网络与其他计算机程序有很大的不同。我在本书中所写的计算系统都是**过程化**的：程序从第一行代码开始，执行完后继续执行下一行，按照线性顺序执行指令。与之相反，一个真正的神经网络并不会遵循线性路径。相反，信息是通过整个节点网络集体并行处理的，每个节点代表一个神经元。从这个意义上讲，神经网络被视为一种**连接主义**系统。
- en: In other ways, neural networks aren’t so different from some of the programs
    you’ve seen. A neural network exhibits all the hallmarks of a complex system,
    much like a cellular automaton or a flock of boids. Remember how each individual
    boid was simple to understand, yet by following only three rules—separation, alignment,
    cohesion—it contributed to complex behaviors? Each individual element in a neural
    network is equally simple to understand. It reads an input (a number), processes
    it, and generates an output (another number). That’s all there is to it, and yet
    a network of many neurons can exhibit incredibly rich and intelligent behaviors,
    echoing the complex dynamics seen in a flock of boids.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 从某种意义上说，神经网络与你见过的一些程序并没有太大区别。神经网络展现了复杂系统的所有特征，类似于细胞自动机或鸟群。还记得每个个体鸟群（boid）是如此简单易懂，但通过遵循三条规则——分离、对齐、聚合——它却能产生复杂的行为吗？神经网络中的每个个体元素同样简单易懂。它读取输入（一个数字），处理它，并生成输出（另一个数字）。就这么简单，然而，由许多神经元组成的网络却能展现出极为丰富和智能的行为，回响着鸟群中所见的复杂动态。
- en: '![Image](../images/pg540_Image_826.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/pg540_Image_826.jpg)'
- en: 'Figure 10.2: A neural network is a system of neurons and connections.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2：神经网络是由神经元和连接组成的系统。
- en: In fact, a neural network isn’t just a complex system, but a complex *adaptive*
    system, meaning it can change its internal structure based on the information
    flowing through it. In other words, it has the ability to learn. Typically, this
    is achieved by adjusting **weights**. In [Figure 10.2](ch10.xhtml#ch10fig2), each
    arrow represents a connection between two neurons and indicates the pathway for
    the flow of information. Each connection has a weight, a number that controls
    the signal between the two neurons. If the network generates a *good* output (which
    I’ll define later), there’s no need to adjust the weights. However, if the network
    generates a *poor* output—an error, so to speak—then the system adapts, altering
    the weights with the hope of improving subsequent results.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，神经网络不仅仅是一个复杂的系统，它还是一个复杂的*自适应*系统，这意味着它可以根据流经其中的信息改变其内部结构。换句话说，它具有学习的能力。通常，这是通过调整**权重**来实现的。在[图10.2](ch10.xhtml#ch10fig2)中，每个箭头表示两个神经元之间的连接，并指示信息流动的路径。每个连接都有一个权重，这是控制两个神经元之间信号的数字。如果网络产生了一个*好的*输出（我稍后会定义），则不需要调整权重。然而，如果网络产生了一个*差的*输出——可以说是一个错误——那么系统就会进行自我调整，改变权重，以期改进后续的结果。
- en: 'Neural networks may use a variety of strategies for learning, and I’ll focus
    on one of them in this chapter:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络可能会使用多种学习策略，本章将重点介绍其中的一种：
- en: '**Supervised learning:** Essentially, this strategy involves a teacher that’s
    smarter than the network itself. Take the case of facial recognition. The teacher
    shows the network a bunch of faces, and the teacher already knows the name associated
    with each face. The network makes its guesses; then the teacher provides the network
    with the actual names. The network can compare its answers to the known correct
    ones and make adjustments according to its errors. The neural networks in this
    chapter follow this model.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监督学习：** 本质上，这种策略涉及一个比网络本身更聪明的教师。以人脸识别为例，教师向网络展示一组人脸，并且教师已经知道每张人脸对应的名字。网络进行猜测，然后教师提供正确的名字。网络可以将其答案与已知的正确答案进行比较，并根据错误进行调整。本章中的神经网络遵循这种模型。'
- en: '**Unsupervised learning:** This technique is required when you don’t have an
    example dataset with known answers. Instead, the network works on its own to uncover
    hidden patterns in the data. An application of this is clustering: a set of elements
    is divided into groups according to an unknown pattern. I won’t be showing any
    instances of unsupervised learning, as the strategy is less relevant to the book’s
    examples.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无监督学习：** 当你没有带有已知答案的示例数据集时，就需要使用这种技术。相反，网络会自主工作，发掘数据中的隐藏模式。其应用之一是聚类：一组元素根据一个未知模式被划分为不同的组。我不会展示无监督学习的实例，因为这种策略与本书的示例关系较小。'
- en: '**Reinforcement learning:** This strategy is built on observation: a learning
    agent makes decisions and looks to its environment for the results. It’s rewarded
    for good decisions and penalized for bad decisions, such that it learns to make
    better decisions over time. I’ll discuss this strategy in more detail in [Chapter
    11](ch11.xhtml#ch11).'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**强化学习：** 这一策略建立在观察基础上：学习代理做出决策，并根据其环境来查看结果。它因做出正确决策而获得奖励，因做出错误决策而受到惩罚，从而随着时间的推移学会做出更好的决策。我将在[第11章](ch11.xhtml#ch11)中更详细地讨论这一策略。'
- en: The ability of a neural network to learn, to make adjustments to its structure
    over time, is what makes it so useful in the field of **machine learning**. This
    term can be traced back to the 1959 paper “Some Studies in Machine Learning Using
    the Game of Checkers,” in which computer scientist Arthur Lee Samuel outlines
    a “self-learning” program for playing checkers. The concept of an algorithm enabling
    a computer to learn without explicit programming is the foundation of machine
    learning.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的学习能力，即随着时间推移对其结构进行调整的能力，是它在**机器学习**领域如此有用的原因。这个术语可以追溯到1959年发表的论文《使用跳棋进行机器学习的研究》，在这篇论文中，计算机科学家亚瑟·李·塞缪尔提出了一个用于下跳棋的“自学习”程序。使计算机在没有显式编程的情况下学习的算法概念是机器学习的基础。
- en: 'Think about what you’ve been doing throughout this book: coding! In traditional
    programming, a computer program takes inputs and, based on the rules you’ve provided,
    produces outputs. Machine learning, however, turns this approach upside down.
    Instead of you writing the rules, the system is given example inputs and outputs,
    and generates the rules itself! Many algorithms can be used to implement machine
    learning, and a neural network is just one of them.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 想一想你在这本书中做了什么：编码！在传统的编程中，计算机程序接受输入，并根据你提供的规则生成输出。然而，机器学习却颠倒了这种方式。系统不是由你编写规则，而是给定示例输入和输出，并自行生成规则！可以用许多算法来实现机器学习，神经网络只是其中之一。
- en: Machine learning is part of the broad, sweeping field of **artificial intelligence
    (AI)**, although the terms are sometimes used interchangeably. In their thoughtful
    and friendly primer *A People’s Guide to AI*, Mimi Onuoha and Diana Nucera (aka
    Mother Cyborg) define AI as “the theory and development of computer systems able
    to perform tasks that normally require human intelligence.” Machine learning algorithms
    are one approach to these tasks, but not all AI systems feature a self-learning
    component.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是**人工智能（AI）**这个广泛领域的一部分，尽管这两个术语有时可以互换使用。在 Mimi Onuoha 和 Diana Nucera（又名
    Mother Cyborg）所著的友好入门书籍 *A People’s Guide to AI* 中，他们将 AI 定义为“能够执行通常需要人类智慧的任务的计算机系统的理论与发展”。机器学习算法是实现这些任务的一种方法，但并非所有的
    AI 系统都具备自我学习的组件。
- en: '**Machine Learning Libraries**'
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**机器学习库**'
- en: Today, leveraging machine learning in creative coding and interactive media
    isn’t only feasible but increasingly common, thanks to third-party libraries that
    handle a lot of the neural network implementation details under the hood. While
    the vast majority of machine learning development and research is done in Python,
    the world of web development has seen the emergence of powerful JavaScript-based
    tools. Two libraries of note are TensorFlow.js and ml5.js.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，在创意编码和互动媒体中利用机器学习不仅是可行的，而且越来越普遍，这得益于处理大量神经网络实现细节的第三方库。尽管绝大多数机器学习开发和研究都是用
    Python 完成的，但在 Web 开发领域，基于 JavaScript 的强大工具也逐渐涌现。值得注意的两个库是 TensorFlow.js 和 ml5.js。
- en: TensorFlow.js is an open source library that lets you define, train, and run
    neural networks directly in the browser using JavaScript, without the need to
    install or configure complex environments. It’s part of the TensorFlow ecosystem,
    which is maintained and developed by Google. TensorFlow.js is a powerful tool,
    but its low-level operations and highly technical API can be intimidating to beginners.
    Enter ml5.js, a library built on top of TensorFlow.js and designed specifically
    for use with p5.js. Its goal is to be beginner friendly and make machine learning
    approachable for a broad audience of artists, creative coders, and students. I’ll
    demonstrate how to use ml5.js in “Machine Learning with ml5.js” on [page 521](ch10.xhtml#ch00lev1sec90).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow.js 是一个开源库，允许你使用 JavaScript 直接在浏览器中定义、训练和运行神经网络，而无需安装或配置复杂的环境。它是 TensorFlow
    生态系统的一部分，由 Google 维护和开发。TensorFlow.js 是一个强大的工具，但其底层操作和高度技术化的 API 对初学者来说可能有些令人畏惧。这时，ml5.js
    应运而生，它是建立在 TensorFlow.js 之上的库，专为与 p5.js 一起使用而设计。它的目标是让初学者更易上手，使机器学习变得更加亲民，面向广泛的艺术家、创意编码者和学生。我将在《使用
    ml5.js进行机器学习》一章中展示如何使用 ml5.js，具体内容见 [第521页](ch10.xhtml#ch00lev1sec90)。
- en: A benefit of libraries like TensorFlow.js and ml5.js is that you can use them
    to run pretrained models. A machine learning **model** is a specific setup of
    neurons and connections, and a **pretrained** model is one that has already been
    prepared for a particular task. For example, popular pretrained models are used
    for classifying images, identifying body poses, recognizing facial landmarks or
    hand positions, and even analyzing the sentiment expressed in a text. You can
    use such a model as is or treat it as a starting point for additional learning
    (commonly referred to as **transfer learning**).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 像 TensorFlow.js 和 ml5.js 这样的库的一个好处是，你可以用它们来运行预训练模型。一个机器学习**模型**是神经元和连接的特定配置，而**预训练**模型是已经为特定任务准备好的模型。例如，常见的预训练模型用于图像分类、识别身体姿势、识别面部标志或手部位置，甚至分析文本中的情感。你可以直接使用这样的模型，也可以将其作为进一步学习的起点（通常称为**迁移学习**）。
- en: Before I get to exploring the ml5.js library, however, I’d like to try my hand
    at building the simplest of all neural networks from scratch, using only p5.js,
    to illustrate how the concepts of neural networks and machine learning are implemented
    in code.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在我开始探索 ml5.js 库之前，我想先从零开始构建最简单的神经网络，只使用 p5.js，以此来说明神经网络和机器学习的概念是如何在代码中实现的。
- en: '**The Perceptron**'
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**感知机**'
- en: 'A **perceptron** is the simplest neural network possible: a computational model
    of a single neuron. Invented in 1957 by Frank Rosenblatt at the Cornell Aeronautical
    Laboratory, a perceptron consists of one or more inputs, a processor, and a single
    output, as shown in [Figure 10.3](ch10.xhtml#ch10fig3).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**感知机**是最简单的神经网络：一个单一神经元的计算模型。感知机是由 Frank Rosenblatt 于 1957 年在康奈尔航空实验室发明的，它由一个或多个输入、一个处理器和一个输出组成，如[图
    10.3](ch10.xhtml#ch10fig3)所示。'
- en: '![Image](../images/pg543_Image_827.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/pg543_Image_827.jpg)'
- en: 'Figure 10.3: A simple perceptron with two inputs and one output'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.3：一个简单的感知机，具有两个输入和一个输出
- en: 'A perceptron follows the **feed-forward** model: data passes (feeds) through
    the network in one direction. The inputs are sent into the neuron, are processed,
    and result in an output. This means the one-neuron network diagrammed in [Figure
    10.3](ch10.xhtml#ch10fig3) reads from left to right (forward): inputs come in,
    and output goes out.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机遵循**前馈**模型：数据在网络中单向流动。输入被送入神经元，经过处理后产生输出。这意味着在[图 10.3](ch10.xhtml#ch10fig3)中描绘的单神经元网络是从左到右（前向）读取的：输入进来，输出出去。
- en: 'Say I have a perceptron with two inputs, the values 12 and 4\. In machine learning,
    it’s customary to denote each input with an *x*, so I’ll call these inputs *x*[0]
    and *x*[1]:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我有一个感知机，两个输入值分别是 12 和 4。在机器学习中，通常用 *x* 来表示每个输入，所以我将这些输入命名为 *x*[0] 和 *x*[1]：
- en: '| **Input** | **Value** |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| **输入** | **值** |'
- en: '| --- | --- |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| *x*[0] | 12 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| *x*[0] | 12 |'
- en: '| *x*[1] | 4 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| *x*[1] | 4 |'
- en: '**Perceptron Steps**'
  id: totrans-52
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**感知机步骤**'
- en: To get from these inputs to an output, the perceptron follows a series of steps.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从这些输入得到一个输出，感知机遵循一系列步骤。
- en: '**Step 1: Weight the Inputs**'
  id: totrans-54
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**步骤 1：加权输入**'
- en: 'Each input sent into the neuron must first be weighted, meaning it’s multiplied
    by a value, often a number from –1 to +1\. When creating a perceptron, the inputs
    are typically assigned random weights. I’ll call my weights *w*[0] and *w*[1]:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 每个输入进入神经元之前，必须先加权，也就是说它会与一个值相乘，通常这个值在–1到+1之间。在创建感知机时，输入通常会被赋予随机权重。我将我的权重命名为
    *w*[0] 和 *w*[1]：
- en: '| **Weight** | **Value** |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| **权重** | **值** |'
- en: '| --- | --- |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| *w*[0] | 0.5 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| *w*[0] | 0.5 |'
- en: '| *w*[1] | –1 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| *w*[1] | –1 |'
- en: 'Each input needs to be multiplied by its corresponding weight:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 每个输入需要与其对应的权重相乘：
- en: '| **Input** | **Weight** | **Input × Weight** |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| **输入** | **权重** | **输入 × 权重** |'
- en: '| --- | --- | --- |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 12 | 0.5 | 6 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 0.5 | 6 |'
- en: '| 4 | –1 | –4 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 4 | –1 | –4 |'
- en: '**Step 2: Sum the Inputs**'
  id: totrans-65
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**步骤 2：求和输入**'
- en: 'The weighted inputs are then added together:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将加权输入相加：
- en: 6 + −4 = 2
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 6 + −4 = 2
- en: '**Step 3: Generate the Output**'
  id: totrans-68
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**步骤 3：生成输出**'
- en: The output of a perceptron is produced by passing the sum through an **activation
    function** that reduces the output to one of two possible values. Think of this
    binary output as an LED that’s only *off* or *on*, or as a neuron in an actual
    brain that either fires or doesn’t fire. The activation function determines whether
    the perceptron should “fire.”
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机的输出是通过将总和传递通过一个**激活函数**生成的，这个激活函数将输出缩减为两个可能值中的一个。可以将这个二进制输出想象成一个LED灯，它只有*关闭*或*开启*两种状态，或者像大脑中的神经元，要么发射信号，要么不发射信号。激活函数决定了感知机是否应该“发射”信号。
- en: 'Activation functions can get a little bit hairy. If you start reading about
    them in an AI textbook, you may soon find yourself reaching in turn for a calculus
    textbook. However, your new friend the simple perceptron provides an easier option
    that still demonstrates the concept. I’ll make the activation function the sign
    of the sum. If the sum is a positive number, the output is 1; if it’s negative,
    the output is –1:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数可能有些复杂。如果你开始阅读人工智能教材，你可能很快就会想拿起微积分教材。然而，你的新朋友——简单的感知机提供了一个更容易理解的选项，同时仍能演示这个概念。我将激活函数定义为总和的符号。如果总和是正数，输出为1；如果是负数，输出为–1：
- en: sign(2) = +1
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: sign(2) = +1
- en: '**Putting It All Together**'
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**将所有部分结合起来**'
- en: 'Putting the preceding three parts together, here are the steps of the **perceptron
    algorithm**:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 将前面提到的三部分结合起来，以下是**感知机算法**的步骤：
- en: For every input, multiply that input by its weight.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个输入，将该输入与它的权重相乘。
- en: Sum all the weighted inputs.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对所有加权输入求和。
- en: Compute the output of the perceptron by passing that sum through an activation
    function (the sign of the sum).
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将该和值传递给激活函数（和值的符号），计算感知器的输出。
- en: 'I can start writing this algorithm in code by using two arrays of values, one
    for the inputs and one for the weights:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以通过使用两个值数组来开始编写这个算法，一个用于输入，一个用于权重：
- en: '[PRE0]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The “for every input” in step 1 implies a loop that multiplies each input by
    its corresponding weight. To obtain the sum, the results can be added up in that
    same loop:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤1中的“对于每个输入”意味着一个循环，将每个输入与其对应的权重相乘。为了获得和，可以在同一个循环中将结果相加：
- en: '![Image](../images/pg545_Image_828.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/pg545_Image_828.jpg)'
- en: 'With the sum, I can then compute the output:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 有了和，我就可以计算输出：
- en: '![Image](../images/pg545_Image_829.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/pg545_Image_829.jpg)'
- en: You might be wondering how I’m handling the value of 0 in the activation function.
    Is 0 positive or negative? The deep philosophical implications of this question
    aside, I’m choosing here to arbitrarily return a –1 for 0, but I could easily
    change the `>` to `>=` to go the other way. Depending on the application, this
    decision could be significant, but for demonstration purposes here, I can just
    pick one.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，我是如何处理激活函数中的0值的。0是正数还是负数？撇开这个问题的深刻哲学意义，我在这里选择任意将0返回为-1，但我也可以很容易地将`>`改为`>=`来改变方向。根据应用的不同，这个决定可能会很重要，但在这里为了演示的目的，我可以随便选一个。
- en: Now that I’ve explained the computational process of a perceptron, let’s look
    at an example of one in action.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我已经解释了感知器的计算过程，接下来让我们看一个实际应用的例子。
- en: '**Simple Pattern Recognition Using a Perceptron**'
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**使用感知器进行简单模式识别**'
- en: I’ve mentioned that neural networks are commonly used for pattern recognition.
    The scenarios outlined earlier require more complex networks, but even a simple
    perceptron can demonstrate a fundamental type of pattern recognition in which
    data points are classified as belonging to one of two groups. For instance, imagine
    you have a dataset of plants and want to identify them as either *xerophytes*
    (plants that have evolved to survive in an environment with little water and lots
    of sunlight, like the desert) or *hydrophytes* (plants that have adapted to living
    submerged in water, with reduced light). That’s how I’ll use my perceptron in
    this section.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我提到过神经网络通常用于模式识别。前面描述的场景需要更复杂的网络，但即使是一个简单的感知器，也能演示一种基本的模式识别方法，其中数据点被分类为属于两个组中的一个。例如，假设你有一个植物数据集，并且想要将它们识别为*耐旱植物*（能够在水少、阳光多的环境中生存的植物，如沙漠）或*水生植物*（适应于生活在水中并且光线较弱的植物）。这就是我将在本节中使用感知器的方式。
- en: One way to approach classifying the plants is to plot their data on a 2D graph
    and treat the problem as a spatial one. On the x-axis, plot the amount of daily
    sunlight received by the plant, and on the y-axis, plot the amount of water. Once
    all the data has been plotted, it’s easy to draw a line across the graph, with
    all the xerophytes on one side and all the hydrophytes on the other, as in [Figure
    10.4](ch10.xhtml#ch10fig4). (I’m simplifying a little here. Real-world data would
    probably be messier, making the line harder to draw.) That’s how each plant can
    be classified. Is it below the line? Then it’s a xerophyte. Is it above the line?
    Then it’s a hydrophyte.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 对植物进行分类的一种方法是将它们的数据绘制在二维图表上，并将问题视为空间问题。在x轴上绘制植物每天接收到的阳光量，在y轴上绘制水分量。一旦所有数据都被绘制出来，就很容易在图表上画一条线，将所有耐旱植物放在一边，所有水生植物放在另一边，如[图10.4](ch10.xhtml#ch10fig4)所示。（这里我稍作简化，现实世界的数据可能更为复杂，画线会更难。）这样，每个植物就能被分类了。它在直线下方吗？那么它就是耐旱植物。它在直线上方吗？那么它就是水生植物。
- en: '![Image](../images/pg546_Image_830.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/pg546_Image_830.jpg)'
- en: 'Figure 10.4: A collection of points in 2D space divided by a line, representing
    plant categories according to their water and sunlight intake'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.4：二维空间中的点集合，通过一条线将植物分类，表示根据它们的水分和阳光摄取量
- en: In truth, I don’t need a neural network—not even a simple perceptron—to tell
    me whether a point is above or below a line. I can see the answer for myself with
    my own eyes, or have my computer figure it out with simple algebra. But just like
    solving a problem with a known answer—“to be or not to be”—was a convenient first
    test for the GA in [Chapter 9](ch09.xhtml#ch09), training a perceptron to categorize
    points as being on one side of a line versus the other will be a valuable way
    to demonstrate the algorithm of the perceptron and verify that it’s working properly.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，我并不需要一个神经网络——甚至不需要一个简单的感知机——来告诉我一个点是在直线的上方还是下方。我可以用自己的眼睛看到答案，或者让计算机通过简单的代数来算出。但就像在[第9章](ch09.xhtml#ch09)中通过已知答案“生存还是毁灭”来测试遗传算法一样，训练感知机来分类点是否位于直线的两侧，将是展示感知机算法并验证其正常工作的有价值的方式。
- en: 'To solve this problem, I’ll give my perceptron two inputs: *x*[0] is the x-coordinate
    of a point, representing a plant’s amount of sunlight, and *x*[1] is the y-coordinate
    of that point, representing the plant’s amount of water. The perceptron then guesses
    the plant’s classification according to the sign of the weighted sum of these
    inputs. If the sum is positive, the perceptron outputs a +1, signifying a hydrophyte
    (above the line). If the sum is negative, it outputs a –1, signifying a xerophyte
    (below the line). [Figure 10.5](ch10.xhtml#ch10fig5) shows this perceptron (note
    the shorthand of *w*[0] and *w*[1] for the weights).'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我将给感知机两个输入：*x*[0]是一个点的x坐标，代表植物的阳光量，*x*[1]是该点的y坐标，代表植物的水量。感知机根据这些输入的加权和的符号来推测植物的分类。如果加权和为正，感知机输出+1，表示水生植物（在直线之上）。如果加权和为负，它输出-1，表示旱生植物（在直线之下）。[图10.5](ch10.xhtml#ch10fig5)展示了这个感知机（注意
    *w*[0] 和 *w*[1] 是权重的简写）。
- en: '![Image](../images/pg547_Image_831.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/pg547_Image_831.jpg)'
- en: 'Figure 10.5: A perceptron with two inputs (*x*[0] and *x*[1]), a weight for
    each input (*w*[0] and *w*[1]), and a processing neuron that generates the output'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.5：一个有两个输入（*x*[0] 和 *x*[1]）的感知机，每个输入都有一个权重（*w*[0] 和 *w*[1]），以及一个生成输出的处理神经元
- en: This scheme has a pretty significant problem, however. What if my data point
    is (0, 0), and I send this point into the perceptron as inputs *x*[0] = 0 and
    *x*[1] = 0? No matter what the weights are, multiplication by 0 is 0\. The weighted
    inputs are therefore still 0, and their sum will be 0 too. And the sign of 0 is
    . . . hmmm, there’s that deep philosophical quandary again. Regardless of how
    I feel about it, the point (0, 0) could certainly be above or below various lines
    in a 2D world. How is the perceptron supposed to interpret it accurately?
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这个方案有一个相当显著的问题。假设我的数据点是(0, 0)，我将这个点作为输入 *x*[0] = 0 和 *x*[1] = 0 送入感知机。无论权重如何，0乘以任何数都是0。因此，加权输入仍然是0，它们的和也将是0。而0的符号是……嗯，又回到了那个深刻的哲学困境。无论我对它的感觉如何，点(0,
    0)在二维世界中肯定可以位于不同的直线之上或之下。感知机应该如何准确地解释它呢？
- en: To avoid this dilemma, the perceptron requires a third input, typically referred
    to as a **bias** input. This extra input always has the value of 1 and is also
    weighted. [Figure 10.6](ch10.xhtml#ch10fig6) shows the perceptron with the addition
    of the bias.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这种困境，感知机需要一个第三个输入，通常称为**偏置**输入。这个额外的输入始终为1，并且也有权重。[图10.6](ch10.xhtml#ch10fig6)展示了加入偏置后的感知机。
- en: '![Image](../images/pg547_Image_832.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/pg547_Image_832.jpg)'
- en: 'Figure 10.6: Adding a bias input, along with its weight, to the perceptron'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.6：向感知机添加一个偏置输入及其权重
- en: How does this affect point (0, 0)?
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这如何影响点(0, 0)？
- en: '| **Input** | **Weight** | **Result** |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| **输入** | **权重** | **结果** |'
- en: '| --- | --- | --- |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 0 | *w[0]* | 0 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 0 | *w[0]* | 0 |'
- en: '| 0 | *w*[1] | 0 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 0 | *w*[1] | 0 |'
- en: '| 1 | *w*[bias] | *w*[bias] |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 1 | *w*[bias] | *w*[bias] |'
- en: 'The output is then the sum of the weighted results: 0 + 0 + *w*[bias]. Therefore,
    the bias by itself answers the question of where (0, 0) is in relation to the
    line. If the bias’s weight is positive, (0, 0) is above the line; if negative,
    it’s below. The extra input and its weight *bias* the perceptron’s understanding
    of the line’s position relative to (0, 0)!'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是加权结果的总和：0 + 0 + *w*[bias]。因此，偏置本身就回答了点(0, 0)相对于直线的位置。如果偏置的权重是正的，(0, 0)在直线之上；如果是负的，它就在直线之下。额外的输入及其权重对感知机理解直线相对于(0,
    0)的位置产生了*偏置*！
- en: '**The Perceptron Code**'
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**感知机代码**'
- en: 'I’m now ready to assemble the code for a `Perceptron` class. The perceptron
    needs to track only the input weights, which I can store using an array:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我现在准备组装`Perceptron`类的代码了。感知器只需要跟踪输入权重，我可以使用数组来存储它们：
- en: '![Image](../images/pg548_Image_832a.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/pg548_Image_832a.jpg)'
- en: 'The constructor can receive an argument indicating the number of inputs (in
    this case, three: *x*[0], *x*[1], and a bias) and size the `weights` array accordingly,
    filling it with random values to start:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 构造函数可以接收一个参数，指示输入的数量（在此案例中为三个：*x*[0]，*x*[1]，以及一个偏置），并相应地调整`weights`数组的大小，初始时填充随机值：
- en: '![Image](../images/pg548_Image_833.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/pg548_Image_833.jpg)'
- en: 'A perceptron’s job is to receive inputs and produce an output. These requirements
    can be packaged together in a `feedForward()` method. In this example, the perceptron’s
    inputs are an array (which should be the same length as the array of weights),
    and the output is a number, +1 or –1, as returned by the activation function based
    on the sign of the sum:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器的工作是接收输入并生成输出。这些需求可以打包在一个`feedForward()`方法中。在这个例子中，感知器的输入是一个数组（其长度应与权重数组相同），而输出是一个数字，+1
    或 –1，作为激活函数根据和的符号返回的结果：
- en: '![Image](../images/pg548_Image_834.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/pg548_Image_834.jpg)'
- en: Presumably, I could now create a `Perceptron` object and ask it to make a guess
    for any given point, as in [Figure 10.7](ch10.xhtml#ch10fig7).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我现在可以创建一个`Perceptron`对象，并要求它对任何给定点进行预测，如[图10.7](ch10.xhtml#ch10fig7)所示。
- en: '![Image](../images/pg549_Image_835.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/pg549_Image_835.jpg)'
- en: 'Figure 10.7: An (*x*, *y*) coordinate from the 2D space is the input to the
    perceptron.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.7：二维空间中的一个 (*x*, *y*) 坐标是感知器的输入。
- en: 'Here’s the code to generate a guess:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这是生成预测的代码：
- en: '![Image](../images/pg549_Image_836.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/pg549_Image_836.jpg)'
- en: Did the perceptron get it right? Maybe yes, maybe no. At this point, the perceptron
    has no better than a 50/50 chance of arriving at the correct answer, since each
    weight starts out as a random value. A neural network isn’t a magic tool that
    can automatically guess correctly on its own. I need to teach it how to do so!
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器猜对了吗？可能是，也可能不是。此时，感知器猜对的概率也就是50/50，因为每个权重一开始都是随机值。神经网络并不是一个能自动正确猜测的魔法工具。我需要教它如何做到这一点！
- en: 'To train a neural network to answer correctly, I’ll use the supervised learning
    method I described earlier in the chapter. Remember, this technique involves giving
    the network inputs with known answers. This enables the network to check whether
    it has made a correct guess. If not, the network can learn from its mistake and
    adjust its weights. The process is as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练神经网络正确回答，我将使用本章前面描述的监督学习方法。记住，这种方法涉及给网络提供带有已知答案的输入，使得网络能够检查自己是否做出了正确的预测。如果没有，网络可以从错误中学习并调整权重。这个过程如下：
- en: Provide the perceptron with inputs for which there is a known answer.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提供感知器已知答案的输入。
- en: Ask the perceptron to guess an answer.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让感知器进行一次预测。
- en: Compute the error. (Did it get the answer right or wrong?)
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算误差。（它猜对了吗？）
- en: Adjust all the weights according to the error.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据误差调整所有权重。
- en: Return to step 1 and repeat!
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回到第1步并重复！
- en: This process can be packaged into a method on the `Perceptron` class, but before
    I can write it, I need to examine steps 3 and 4 in more detail. How do I define
    the perceptron’s error? And how should I adjust the weights according to this
    error?
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程可以打包成`Perceptron`类中的一个方法，但在编写之前，我需要更详细地检查步骤3和4。我该如何定义感知器的误差？以及我应该如何根据这个误差调整权重？
- en: 'The perceptron’s error can be defined as the difference between the desired
    answer and its guess:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器的误差可以定义为期望答案与预测答案之间的差异：
- en: error = desired output − guess output
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: error = 期望输出 − 预测输出
- en: 'Does this formula look familiar? Think back to the formula for a vehicle’s
    steering force that I worked out in [Chapter 5](ch05.xhtml#ch05):'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式看起来很熟悉吗？回想一下我在[第5章](ch05.xhtml#ch05)中计算的车辆转向力公式：
- en: steering = desired velocity − current velocity
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: steering = 期望速度 − 当前速度
- en: This is also a calculation of an error! The current velocity serves as a guess,
    and the error (the steering force) indicates how to adjust the velocity in the
    correct direction. Adjusting a vehicle’s velocity to follow a target is similar
    to adjusting the weights of a neural network toward the correct answer.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是一种误差计算！当前速度作为预测值，而误差（转向力）则指示如何将速度调整到正确的方向。调整车辆的速度以跟随目标类似于调整神经网络的权重以接近正确答案。
- en: 'For the perceptron, the output has only two possible values: +1 or –1\. Therefore,
    only three errors are possible. If the perceptron guesses the correct answer,
    the guess equals the desired output and the error is 0\. If the correct answer
    is –1 and the perceptron guessed +1, then the error is –2\. If the correct answer
    is +1 and the perceptron guessed –1, then the error is +2\. Here’s that process
    summarized in a table:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 对于感知机来说，输出只有两个可能的值：+1 或 –1。因此，只有三种误差是可能的。如果感知机猜测正确，猜测值等于期望输出，误差为0。如果正确答案是–1，而感知机猜测为+1，则误差为–2。如果正确答案是+1，而感知机猜测为–1，则误差为+2。下面是该过程的总结表格：
- en: '| **Desired** | **Guess** | **Error** |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| **期望** | **猜测** | **误差** |'
- en: '| --- | --- | --- |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| –1 | –1 | 0 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| –1 | –1 | 0 |'
- en: '| –1 | +1 | –2 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| –1 | +1 | –2 |'
- en: '| +1 | –1 | +2 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| +1 | –1 | +2 |'
- en: '| +1 | +1 | 0 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| +1 | +1 | 0 |'
- en: 'The error is the determining factor in how the perceptron’s weights should
    be adjusted. For any given weight, what I’m looking to calculate is the change
    in weight, often called Δweight (or *delta weight*, Δ being the Greek letter delta):'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 误差是决定感知机权重如何调整的关键因素。对于任何给定的权重，我需要计算的是权重的变化，通常称为Δweight（或*delta weight*，Δ为希腊字母delta）：
- en: new weight = weight + Δweight
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 新权重 = 权重 + Δweight
- en: 'To calculate Δweight, I need to multiply the error by the input:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算Δweight，我需要将误差与输入相乘：
- en: Δweight = error × input
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Δweight = 误差 × 输入
- en: 'Therefore, the new weight is calculated as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，新的权重计算如下：
- en: new weight = weight + error × input
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 新权重 = 权重 + 误差 × 输入
- en: To understand why this works, think again about steering. A steering force is
    essentially an error in velocity. By applying a steering force as an acceleration
    (or Δvelocity), the velocity is adjusted to move in the correct direction. This
    is what I want to do with the neural network’s weights. I want to adjust them
    in the right direction, as defined by the error.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解为什么这样有效，再次考虑一下引导。引导力本质上是速度的误差。通过将引导力作为加速度（或Δ速度）施加，速度就能调整到正确的方向。这正是我想要在神经网络的权重中做的。我想根据误差，将它们调整到正确的方向。
- en: 'With steering, however, I had an additional variable that controlled the vehicle’s
    ability to steer: the maximum force. A high maximum force allowed the vehicle
    to accelerate and turn quickly, while a lower force resulted in a slower velocity
    adjustment. The neural network will use a similar strategy with a variable called
    the **learning constant**:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在引导中，我还有一个额外的变量控制着车辆的转向能力：最大力。较高的最大力允许车辆快速加速和转弯，而较低的力则导致较慢的速度调整。神经网络将使用类似的策略，通过一个叫做**学习常数**的变量：
- en: new weight = weight + (error × input) × learning constant
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 新权重 = 权重 + (误差 × 输入) × 学习常数
- en: A high learning constant causes the weight to change more drastically. This
    may help the perceptron arrive at a solution more quickly, but it also increases
    the risk of overshooting the optimal weights. A small learning constant will adjust
    the weights more slowly and require more training time, but will allow the network
    to make small adjustments that could improve overall accuracy.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 较高的学习常数会使权重变化更剧烈。这可能帮助感知机更快地找到解决方案，但也增加了超越最优权重的风险。较小的学习常数会使权重调整得更慢，需要更多的训练时间，但能够让网络进行小幅度调整，从而提高整体准确性。
- en: 'Assuming the addition of a `learningConstant` property to the `Perceptron`
    class, I can now write a training method for the perceptron following the steps
    I outlined earlier:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 假设向`Perceptron`类添加了一个`learningConstant`属性，我现在可以按照之前列出的步骤，编写一个感知机的训练方法：
- en: '![Image](../images/pg551_Image_837.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/pg551_Image_837.jpg)'
- en: 'Here’s the `Perceptron` class as a whole:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是整个`Perceptron`类：
- en: '![Image](../images/pg552_Image_839.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/pg552_Image_839.jpg)'
- en: To train the perceptron, I need a set of inputs with known answers. However,
    I don’t happen to have a real-world dataset (or time to research and collect one)
    for the xerophytes and hydrophytes scenario. In truth, though, the purpose of
    this demonstration isn’t to show you how to classify plants. It’s about how a
    perceptron can learn whether points are above or below a line on a graph, and
    so any set of points will do. In other words, I can just make up the data.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练感知机，我需要一组已知答案的输入。然而，我恰好没有现实世界的数据集（也没有时间去研究和收集一个）来用于干旱植物和水生植物的情境。事实上，这个演示的目的并不是告诉你如何分类植物。而是要展示感知机如何学习判断点在图表中是位于线的上方还是下方，因此任何一组点都可以用来做演示。换句话说，我可以随便编造数据。
- en: What I’m describing is an example of **synthetic data**, artificially generated
    data that’s often used in machine learning to create controlled scenarios for
    training and testing. In this case, my synthetic data will consist of a set of
    random input points, each with a known answer indicating whether the point is
    above or below a line. To define the line and generate the data, I’ll use simple
    algebra. This approach allows me to clearly demonstrate the training process and
    show how the perceptron learns.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我所描述的是一个**合成数据**的例子，合成数据是人工生成的数据，通常用于机器学习中，创建用于训练和测试的受控场景。在这种情况下，我的合成数据将由一组随机输入点组成，每个点都有一个已知的答案，指示该点是在线上方还是下方。为了定义这条直线并生成数据，我将使用简单的代数。这种方法使我能够清楚地演示训练过程，并展示感知机是如何学习的。
- en: 'The question therefore becomes, how do I pick a point and know whether it’s
    above or below a line (without a neural network, that is)? A line can be described
    as a collection of points, where each point’s y-coordinate is a function of its
    x-coordinate:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 所以问题变成了，如何选择一个点并知道它是在直线的上方还是下方（也就是说，不使用神经网络）？一条直线可以描述为一组点，其中每个点的 y 坐标是其 x 坐标的一个函数：
- en: '*y* = *f*(*x*)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* = *f*(*x*)'
- en: 'For a straight line (specifically, a linear function), the relationship can
    be written like this:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一条直线（特别是一个线性函数），它们之间的关系可以写成这样：
- en: '*y* = *mx* + *b*'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* = *mx* + *b*'
- en: Here *m* is the slope of the line, and *b* is the value of *y* when *x* is 0
    (the y-intercept). Here’s a specific example, with the corresponding graph in
    [Figure 10.8](ch10.xhtml#ch10fig8).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 *m* 是直线的斜率，*b* 是当 *x* 为 0 时的 *y* 值（即 y 截距）。以下是一个具体的例子，以及[图 10.8](ch10.xhtml#ch10fig8)中的对应图形。
- en: '![Image](../images/pg553_Image_840.jpg)![Image](../images/pg553_Image_841.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/pg553_Image_840.jpg)![Image](../images/pg553_Image_841.jpg)'
- en: 'Figure 10.8: A graph of ![Image](../images/pg553_Image_842.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.8：一个图表 ![Image](../images/pg553_Image_842.jpg)
- en: 'I’ll arbitrarily choose that as the equation for my line, and write a function
    accordingly:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我将任意选择这个作为我的直线方程，并相应地编写一个函数：
- en: '![Image](../images/pg553_Image_843.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/pg553_Image_843.jpg)'
- en: Now there’s the matter of the p5.js canvas defaulting to (0, 0) in the top-left
    corner with the y-axis pointing down. For this discussion, I’ll assume I’ve built
    the following into the code to reorient the canvas to match a more traditional
    Cartesian space.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，p5.js 画布默认将 (0, 0) 放在左上角，且 y 轴指向下方。为了本次讨论，我假设我已经在代码中做了以下处理，以将画布重新定向以匹配更传统的笛卡尔坐标系。
- en: '![Image](../images/pg554_Image_844.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/pg554_Image_844.jpg)'
- en: 'I can now pick a random point in the 2D space:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我可以在二维空间中选择一个随机点：
- en: '[PRE1]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'How do I know if this point is above or below the line? The line function *f*(*x*)
    returns the *y* value on the line for that x-position. I’ll call that *y*[line]:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我如何知道这个点是在直线的上方还是下方呢？直线函数 *f*(*x*) 返回该 x 位置上的 *y* 值。我称之为 *y*[line]：
- en: '![Image](../images/pg554_Image_845.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/pg554_Image_845.jpg)'
- en: If the *y* value I’m examining is above the line, it will be greater than *y*[line],
    as in [Figure 10.9](ch10.xhtml#ch10fig9).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我检查的 *y* 值在直线之上，那么它会大于 *y*[line]，如[图 10.9](ch10.xhtml#ch10fig9)所示。
- en: '![Image](../images/pg554_Image_846.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/pg554_Image_846.jpg)'
- en: 'Figure 10.9: If *y*[line] is less than *y*, the point is above the line.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.9：如果 *y*[line] 小于 *y*，则该点在直线之上。
- en: 'Here’s the code for that logic:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是实现该逻辑的代码：
- en: '![Image](../images/pg554_Image_847.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/pg554_Image_847.jpg)'
- en: 'I can then make an input array to go with the `desired` output:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我可以创建一个输入数组，并与 `desired` 输出一起使用：
- en: '![Image](../images/pg555_Image_848.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/pg555_Image_848.jpg)'
- en: 'Assuming that I have a `perceptron` variable, I can train it by providing the
    inputs along with the desired answer:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我有一个 `perceptron` 变量，我可以通过提供输入和期望的答案来训练它：
- en: '[PRE2]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If I train the perceptron on a new random point (and its answer) for each cycle
    through `draw()`, it will gradually get better at classifying the points as above
    or below the line.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我在每次通过 `draw()` 循环时都在一个新的随机点（及其答案）上训练感知机，它将逐渐提高对这些点是在线上方还是下方的分类能力。
- en: '![Image](../images/pg555_Image_849.jpg)![Image](../images/pg556_Image_850.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/pg555_Image_849.jpg)![Image](../images/pg556_Image_850.jpg)'
- en: In [Example 10.1](ch10.xhtml#ch10ex1), the training data is visualized alongside
    the target solution line. Each point represents a piece of training data, and
    its color is determined by the perceptron’s current classification—gray for +1
    or white for –1\. I use a small learning constant (0.0001) to slow down how the
    system refines its classifications over time.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [示例 10.1](ch10.xhtml#ch10ex1) 中，训练数据与目标解线一起进行可视化展示。每个点代表一条训练数据，其颜色由感知器当前的分类决定——灰色代表
    +1，白色代表 –1。我使用了一个小的学习常数（0.0001），以减缓系统在时间推移过程中对分类结果的调整。
- en: An intriguing aspect of this example lies in the relationship between the perceptron’s
    weights and the characteristics of the line dividing the points—specifically,
    the line’s slope and y-intercept (the *m* and *b* in *y* = *mx* + *b*). The weights
    in this context aren’t just arbitrary or “magic” values; they bear a direct relationship
    to the geometry of the dataset. In this case, I’m using just 2D data, but for
    many machine learning applications, the data exists in much higher-dimensional
    spaces. The weights of a neural network help navigate these spaces, defining *hyperplanes*
    or decision boundaries that segment and classify the data.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例的一个有趣之处在于感知器的权重与分割点的线特征之间的关系——特别是线的斜率和 y 截距（*m* 和 *b* 在 *y* = *mx* + *b*
    中）。在这种情况下，权重并非只是任意的或“神奇”的数值；它们与数据集的几何形状有直接关系。在这里，我只使用了二维数据，但对于许多机器学习应用来说，数据通常存在于更高维的空间中。神经网络的权重有助于在这些空间中导航，定义
    *超平面* 或决策边界，进而对数据进行分割和分类。
- en: '![Image](../images/pencil.jpg) **Exercise 10.1**'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '![Image](../images/pencil.jpg) **练习 10.1**'
- en: 'Modify the code from [Example 10.1](ch10.xhtml#ch10ex1) to also draw the perceptron’s
    current decision boundary during the training process—its best guess for where
    the line should be. Hint: Use the perceptron’s current weights to calculate the
    line’s equation.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 修改 [示例 10.1](ch10.xhtml#ch10ex1) 中的代码，在训练过程中绘制感知器当前的决策边界——它对分界线应该在哪的最佳猜测。提示：使用感知器当前的权重来计算这条线的方程。
- en: While this perceptron example offers a conceptual foundation, real-world datasets
    often feature more diverse and dynamic ranges of input values. For the simplified
    scenario here, the range of values for *x* is larger than that for *y* because
    of the canvas size of 640×240\. Despite this, the example still works—after all,
    the sign activation function doesn’t rely on specific input ranges, and it’s such
    a straightforward binary classification task.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个感知器示例提供了一个概念基础，但现实世界的数据集通常具有更多样化和动态的输入值范围。在这里的简化场景中，*x* 的值范围大于 *y*，因为画布的大小是
    640×240。尽管如此，示例仍然有效——毕竟，符号激活函数并不依赖于特定的输入范围，而且这是一个简单的二元分类任务。
- en: However, real-world data often has much greater complexity in terms of input
    ranges. To this end, **data normalization** is a critical step in machine learning.
    Normalizing data involves mapping the training data to ensure that all inputs
    (and outputs) conform to a uniform range—typically 0 to 1, or perhaps –1 to 1\.
    This process can improve training efficiency and prevent individual inputs from
    dominating the learning process. In the next section, using the ml5.js library,
    I’ll build data normalization into the process.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，现实世界中的数据通常具有更复杂的输入范围。为此，**数据标准化**是机器学习中的一个关键步骤。数据标准化涉及将训练数据映射到一个统一的范围——通常是
    0 到 1，或可能是 –1 到 1。这一过程可以提高训练效率，并防止个别输入主导学习过程。在接下来的章节中，使用 ml5.js 库，我将把数据标准化纳入其中。
- en: '![Image](../images/pencil.jpg) **Exercise 10.2**'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '![Image](../images/pencil.jpg) **练习 10.2**'
- en: Instead of using supervised learning, can you train the neural network to find
    the right weights by using a GA?
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用监督学习，你能否通过使用遗传算法（GA）训练神经网络来找到合适的权重？
- en: '![Image](../images/pencil.jpg) **Exercise 10.3**'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '![Image](../images/pencil.jpg) **练习 10.3**'
- en: Incorporate data normalization into the example. Does this improve the learning
    efficiency?
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据标准化纳入示例中。这是否能提高学习效率？
- en: '**Putting the “Network” in Neural Network**'
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**将“网络”放入神经网络中**'
- en: A perceptron can have multiple inputs, but it’s still just a single, lonely
    neuron. Unfortunately, that limits the range of problems it can solve. The true
    power of neural networks comes from the *network* part. Link multiple neurons
    together and you’re able to solve problems of much greater complexity.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 一个感知器可以有多个输入，但它仍然只是一个孤独的神经元。不幸的是，这限制了它能够解决的问题范围。神经网络的真正力量来自于 *网络* 部分。将多个神经元连接在一起，你就能够解决更为复杂的问题。
- en: If you read an AI textbook, it will say that a perceptron can solve only **linearly
    separable** problems. If a dataset is linearly separable, you can graph it and
    classify it into two groups simply by drawing a straight line (see [Figure 10.10](ch10.xhtml#ch10fig10),
    left). Classifying plants as xerophytes or hydrophytes is a linearly separable
    problem.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你阅读一本人工智能教材，会发现它说感知机只能解决**线性可分**的问题。如果数据集是线性可分的，你可以通过画一条直线将其在图上分类为两组（见[图 10.10](ch10.xhtml#ch10fig10)，左）。将植物分为耐旱植物或耐水植物就是一个线性可分的问题。
- en: '![Image](../images/pg558_Image_851.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/pg558_Image_851.jpg)'
- en: 'Figure 10.10: Data points that are linearly separable (left) and data points
    that are nonlinearly separable, as a curve is required to separate the points
    (right)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.10：线性可分的数据点（左）和需要曲线来分离的数据点（右）
- en: 'Now imagine you’re classifying plants according to soil acidity (x-axis) and
    temperature (y-axis). Some plants might thrive in acidic soils but only within
    a narrow temperature range, while other plants prefer less acidic soils but tolerate
    a broader range of temperatures. A more complex relationship exists between the
    two variables, so a straight line can’t be drawn to separate the two categories
    of plants, *acidophilic* and *alkaliphilic* (see [Figure 10.10](ch10.xhtml#ch10fig10),
    right). A lone perceptron can’t handle this type of **nonlinearly separable**
    problem. (Caveat here: I’m making up these scenarios. If you happen to be a botanist,
    please let me know if I’m anywhere close to reality.)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 现在想象一下，你正在根据土壤酸度（x 轴）和温度（y 轴）对植物进行分类。一些植物可能在酸性土壤中茁壮成长，但仅在一个狭窄的温度范围内；而其他植物则更喜欢不太酸性的土壤，但能够适应更广泛的温度范围。两者之间存在更复杂的关系，因此无法通过一条直线将这两类植物——*酸性植物*
    和 *碱性植物*（见[图 10.10](ch10.xhtml#ch10fig10)，右）分开。一个单一的感知机无法处理这种**非线性可分**的问题。（这里有个警告：我是在虚构这些情境。如果你恰好是植物学家，请告诉我我是否接近现实。）
- en: One of the simplest examples of a nonlinearly separable problem is XOR (exclusive
    or). This is a logical operator, similar to the more familiar AND and OR. For
    *A* AND *B* to be true, both *A* and *B* must be true. With OR, either *A* or
    *B* (or both) can be true. These are both linearly separable problems. The truth
    tables in [Figure 10.11](ch10.xhtml#ch10fig11) show their solution space. Each
    true or false value in the table shows the output for a particular combination
    of true or false inputs. See how you can draw a straight line to separate the
    true outputs from the false ones?
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 非线性可分问题的一个最简单的例子就是 XOR（异或）。这是一种逻辑操作符，类似于更常见的 AND 和 OR。为了使 *A* AND *B* 为真，*A*
    和 *B* 必须同时为真。而 OR 操作中，*A* 或 *B*（或两者）可以为真。这两个问题都是线性可分的。[图 10.11](ch10.xhtml#ch10fig11)中的真值表展示了它们的解空间。表中的每个真值或假值展示了一个特定真值或假值输入组合的输出。看看你是否可以画一条直线将真值输出和假值输出分开？
- en: '![Image](../images/pg558_Image_852.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/pg558_Image_852.jpg)'
- en: 'Figure 10.11: Truth tables for the AND and OR logical operators. The true and
    false outputs can be separated by a line.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.11：AND 和 OR 逻辑运算符的真值表。真值和假值输出可以用一条线来分隔。
- en: The XOR operator is the equivalent of (OR) AND (NOT AND). In other words, *A*
    XOR *B* evaluates to true only if one of the inputs is true. If both inputs are
    false or both are true, the output is false. To illustrate, let’s say you’re having
    pizza for dinner. You love pineapple on pizza, and you love mushrooms on pizza,
    but put them together—yech! And plain pizza, that’s no good either!
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: XOR 操作符等同于 (OR) 和 (NOT AND)。换句话说，*A* XOR *B* 仅在其中一个输入为真时结果为真。如果两个输入都为假或都为真，输出为假。举个例子，假设你晚餐吃披萨。你喜欢披萨上放菠萝，也喜欢放蘑菇，但把它们放在一起——呕！而普通的披萨也不好吃！
- en: The XOR truth table in [Figure 10.12](ch10.xhtml#ch10fig12) isn’t linearly separable.
    Try to draw a straight line to separate the true outputs from the false ones—you
    can’t!
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10.12](ch10.xhtml#ch10fig12)中的 XOR 真值表不是线性可分的。试着画一条直线来将真值输出和假值输出分开——你做不到！'
- en: '![Image](../images/pg559_Image_853.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/pg559_Image_853.jpg)'
- en: 'Figure 10.12: The truth tables for whether you want to eat the pizza (left)
    and XOR (right). Note how the true and false outputs can’t be separated by a single
    line.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.12：你是否想吃披萨（左）和 XOR（右）的真值表。注意如何真值和假值输出无法通过一条直线分隔。
- en: The fact that a perceptron can’t even solve something as simple as XOR may seem
    extremely limiting. But what if I made a network out of two perceptrons? If one
    perceptron can solve the linearly separable OR and one perceptron can solve the
    linearly separate NOT AND, then two perceptrons combined can solve the nonlinearly
    separable XOR.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 一个感知器甚至不能解决像 XOR 这样简单的问题，这可能看起来非常有限。但如果我用两个感知器构建一个网络呢？如果一个感知器能够解决线性可分的 OR 问题，另一个感知器能够解决线性可分的
    NOT AND 问题，那么两个感知器结合起来可以解决非线性可分的 XOR 问题。
- en: When you combine multiple perceptrons, you get a **multilayered perceptron**,
    a network of many neurons (see [Figure 10.13](ch10.xhtml#ch10fig13)). Some are
    input neurons and receive the initial inputs, some are part of what’s called a
    **hidden layer** (as they’re connected to neither the inputs nor the outputs of
    the network directly), and then there are the output neurons, from which the results
    are read.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将多个感知器结合在一起时，你就得到了**多层感知器**，这是一种由许多神经元组成的网络（参见[图 10.13](ch10.xhtml#ch10fig13)）。其中一些是输入神经元，接收初始输入，一些是**隐藏层**的一部分（因为它们既不直接与网络的输入相连，也不与输出相连），然后是输出神经元，从中读取结果。
- en: Up until now, I’ve been visualizing a singular perceptron with one circle representing
    a neuron processing its input signals. Now, as I move on to larger networks, it’s
    more typical to represent all the elements (inputs, neurons, outputs) as circles,
    with arrows that indicate the flow of data. In [Figure 10.13](ch10.xhtml#ch10fig13),
    you can see the inputs and bias flowing into the hidden layer, which then flows
    to the output.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 直到现在，我一直在可视化一个单一的感知器，使用一个圆圈表示一个神经元处理它的输入信号。现在，当我转向更大的网络时，通常会将所有元素（输入、神经元、输出）表示为圆圈，使用箭头来指示数据流动的方向。在[图
    10.13](ch10.xhtml#ch10fig13)中，你可以看到输入和偏差流入隐藏层，然后再流向输出。
- en: '![Image](../images/pg560_Image_854.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/pg560_Image_854.jpg)'
- en: 'Figure 10.13: A multilayered perceptron has the same inputs and output as the
    simple perceptron, but now it includes a hidden layer of neurons.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.13：多层感知器与简单感知器具有相同的输入和输出，但现在它包括了一个隐藏层的神经元。
- en: 'Training a simple perceptron is pretty straightforward: you feed the data through
    and evaluate how to change the input weights according to the error. With a multilayered
    perceptron, however, the training process becomes more complex. The overall output
    of the network is still generated in essentially the same manner as before: the
    inputs multiplied by the weights are summed and fed forward through the various
    layers of the network. And you still use the network’s guess to calculate the
    error (desired result – guess). But now so many connections exist between layers
    of the network, each with its own weight. How do you know how much each neuron
    or connection contributed to the overall error of the network, and how it should
    be adjusted?'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个简单的感知器是相当直接的：你将数据传入并根据误差评估如何调整输入权重。然而，对于多层感知器来说，训练过程变得更加复杂。网络的整体输出仍然是以与之前基本相同的方式生成的：输入乘以权重后求和，并通过网络的各个层向前传播。你仍然使用网络的猜测值来计算误差（期望结果
    - 猜测值）。但现在网络层之间有了如此多的连接，每个连接都有自己的权重。你如何知道每个神经元或连接在网络整体误差中的贡献，以及该如何调整它们？
- en: The solution to optimizing the weights of a multilayered network is **backpropagation**.
    This process takes the error and feeds it backward through the network so it can
    adjust the weights of all the connections in proportion to how much they’ve contributed
    to the total error. The details of backpropagation are beyond the scope of this
    book. The algorithm uses a variety of activation functions (one classic example
    is the sigmoid function) as well as some calculus. If you’re interested in continuing
    down this road and learning more about how backpropagation works, you can find
    my “Toy Neural Network” project at the Coding Train website with accompanying
    video tutorials (*[https://thecodingtrain.com/neural-network](https://thecodingtrain.com/neural-network)*).
    They go through all the steps of solving XOR using a multilayered feed-forward
    network with backpropagation. For this chapter, however, I’d instead like to get
    some help and phone a friend.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 优化多层网络权重的解决方案是**反向传播**。这个过程将误差传递回网络，通过这种方式，它可以根据每个连接对总误差的贡献程度调整所有连接的权重。反向传播的详细过程超出了本书的范围。该算法使用多种激活函数（其中一个经典例子是
    sigmoid 函数）以及一些微积分。如果你有兴趣继续探索这个领域，了解更多关于反向传播是如何工作的内容，你可以在 Coding Train 网站找到我的“玩具神经网络”项目，并配有视频教程
    (*[https://thecodingtrain.com/neural-network](https://thecodingtrain.com/neural-network)*)。这些教程会详细介绍如何使用多层前馈网络和反向传播解决
    XOR 问题。然而，在本章中，我更希望得到一些帮助，打个电话给朋友。
- en: '**Machine Learning with ml5.js**'
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**使用 ml5.js 进行机器学习**'
- en: That friend is ml5.js. This machine learning library can manage the details
    of complex processes like backpropagation so you and I don’t have to worry about
    them. As I mentioned earlier in the chapter, ml5.js aims to provide a friendly
    entry point for those who are new to machine learning and neural networks, while
    still harnessing the power of Google’s TensorFlow.js behind the scenes.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 那个朋友就是 ml5.js。这个机器学习库能够处理复杂过程的细节，比如反向传播，这样你和我就不必担心这些问题。如本章早些时候所提到的，ml5.js 旨在为机器学习和神经网络的新手提供一个友好的入门点，同时仍然借助
    Google 的 TensorFlow.js 在后台提供强大的功能。
- en: 'To use ml5.js in a sketch, you must import it via a `<script>` element in your
    *index.html* file, much as you did with Matter.js and Toxiclibs.js in [Chapter
    6](ch06.xhtml#ch06):'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 要在草图中使用 ml5.js，你必须通过 `<script>` 元素将其导入到 *index.html* 文件中，正如你在[第6章](ch06.xhtml#ch06)中做过的那样，导入
    Matter.js 和 Toxiclibs.js：
- en: '[PRE3]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: My goal for the rest of this chapter is to introduce ml5.js by developing a
    system that can recognize mouse gestures. This will prepare you for [Chapter 11](ch11.xhtml#ch11),
    where I’ll add a neural network “brain” to an autonomous steering agent and tie
    machine learning back into the story of the book. First, however, I’d like to
    talk more generally through the steps of training a multilayered neural network
    model using supervised learning. Outlining these steps will highlight important
    decisions you’ll have to make before developing a learning model, introduce the
    syntax of the ml5.js library, and provide you with the context you’ll need before
    training your own machine learning models.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 本章剩下的目标是通过开发一个能够识别鼠标手势的系统来介绍 ml5.js。这将为你准备好[第11章](ch11.xhtml#ch11)，在该章节中，我将为一个自主导航代理添加一个神经网络“脑”，并将机器学习重新融入到本书的故事中。然而，首先我想更一般性地讲解使用监督学习训练多层神经网络模型的步骤。概述这些步骤将突出在开发学习模型之前你需要做出的一些重要决策，介绍
    ml5.js 库的语法，并为你训练自己的机器学习模型提供必要的背景知识。
- en: '**The Machine Learning Life Cycle**'
  id: totrans-214
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**机器学习生命周期**'
- en: 'The life cycle of a machine learning model is typically broken into seven steps:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型的生命周期通常被分为七个步骤：
- en: '**Collect the data.** Data forms the foundation of any machine learning task.
    This stage might involve running experiments, manually inputting values, sourcing
    public data, or a myriad of other methods (like generating synthetic data).'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**收集数据。** 数据是任何机器学习任务的基础。这个阶段可能包括运行实验、手动输入值、获取公共数据或采用各种其他方法（如生成合成数据）。'
- en: '**Prepare the data.** Raw data often isn’t in a format suitable for machine
    learning algorithms. It might also have duplicate or missing values, or contain
    outliers that skew the data. Such inconsistencies may need to be manually adjusted.
    Additionally, as I mentioned earlier, neural networks work best with normalized
    data, which has values scaled to fit within a standard range. Another key part
    of preparing data is separating it into distinct sets: training, validation, and
    testing. The training data is used to teach the model (step 4), while the validation
    and testing data (the distinction is subtle—more on this later) are set aside
    and reserved for evaluating the model’s performance (step 5).'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**准备数据。** 原始数据通常不适合机器学习算法使用。它可能包含重复值或缺失值，或者包含偏离数据的异常值。这些不一致的地方可能需要手动调整。此外，正如我之前提到的，神经网络在标准化数据上表现最佳，即数据值经过缩放以适应标准范围。准备数据的另一个关键部分是将数据分成不同的集合：训练集、验证集和测试集。训练数据用于训练模型（步骤
    4），而验证和测试数据（这两者的区别很微妙——稍后会详细讲解）则被保留并用于评估模型的表现（步骤 5）。'
- en: '**Choose a model.** Design the architecture of the neural network. Different
    models are more suitable for certain types of data and outputs.'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择模型。** 设计神经网络的架构。不同的模型更适合某些类型的数据和输出。'
- en: '**Train the model.** Feed the training portion of the data through the model
    and allow the model to adjust the weights of the neural network based on its errors.
    This process is known as **optimization**: the model tunes the weights so they
    result in the fewest number of errors.'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**训练模型。** 将数据的训练部分输入模型，并根据模型的错误调整神经网络的权重。这个过程被称为**优化**：模型调整权重，以使错误数量最少。'
- en: '**Evaluate the model.** Remember the testing data that was set aside in step
    2? Since that data wasn’t used in training, it provides a means to evaluate how
    well the model performs on new, unseen data.'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**评估模型。** 记得在步骤 2 中留出的测试数据吗？因为这些数据没有用于训练，它为评估模型在新数据上的表现提供了依据。'
- en: '**Tune the parameters.** The training process is influenced by a set of parameters
    (often called **hyperparameters**) such as the learning rate, which dictates how
    much the model should adjust its weights based on errors in prediction. I called
    this the `learningConstant` in the perceptron example. By fine-tuning these parameters
    and revisiting steps 4 (training), 3 (model selection), and even 2 (data preparation),
    you can often improve the model’s performance.'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**调整参数。** 训练过程受一组参数（通常称为**超参数**）的影响，比如学习率，它决定了模型应该根据预测误差调整权重的程度。我在感知机示例中称其为`learningConstant`。通过微调这些参数，并重新审视步骤
    4（训练）、步骤 3（模型选择）甚至步骤 2（数据准备），你通常可以改善模型的表现。'
- en: '**Deploy the model.** Once the model is trained and its performance is evaluated
    satisfactorily, it’s time to use the model out in the real world with new data!'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**部署模型。** 一旦模型经过训练并且其性能得到了满意的评估，就可以将模型用于真实世界中的新数据！'
- en: These steps are the cornerstone of supervised machine learning. However, even
    though 7 is a truly excellent number, I think I missed one more critical step.
    I’ll call it step 0.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤是监督式机器学习的基石。然而，尽管 7 是一个非常完美的数字，我觉得我漏掉了一个更关键的步骤。我将其称为步骤 0。
- en: '**Identify the problem.** This initial step defines the problem that needs
    solving. What is the objective? What are you trying to accomplish or predict with
    your machine learning model?'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**确定问题。** 这个初步步骤定义了需要解决的问题。目标是什么？你希望通过你的机器学习模型实现什么，或者预测什么？'
- en: 'This zeroth step informs all the other steps in the process. After all, how
    are you supposed to collect your data and choose a model without knowing what
    you’re even trying to do? Are you predicting a number? A category? A sequence?
    Is it a binary choice, or are there many options? These sorts of questions often
    boil down to choosing between two types of tasks that the majority of machine
    learning applications fall into: classification and regression.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步零定义了整个过程的其他步骤。毕竟，如果你不知道自己到底在做什么，怎么收集数据和选择模型呢？你是在预测一个数字？一个类别？一个序列？是二元选择，还是有很多选项？这些问题通常归结为在大多数机器学习应用中选择两种任务类型之一：分类和回归。
- en: '**Classification and Regression**'
  id: totrans-226
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**分类与回归**'
- en: '**Classification** is a type of machine learning problem that involves predicting
    a **label** (also called a **category** or **class**) for a piece of data. If
    this sounds familiar, that’s because it is: the simple perceptron in [Example
    10.1](ch10.xhtml#ch10ex1) was trained to classify points as above or below a line.
    To give another example, an image classifier might try to guess if a photo is
    of a cat or a dog and assign the corresponding label (see [Figure 10.14](ch10.xhtml#ch10fig14)).'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '**分类**是一个机器学习问题类型，涉及预测数据的**标签**（也叫做**类别**或**类**）。如果这听起来很熟悉，那是因为它确实如此：[示例 10.1](ch10.xhtml#ch10ex1)中的简单感知机被训练用于将点分类为位于直线之上或之下。举个例子，一个图像分类器可能会尝试猜测一张照片是猫还是狗，并为其分配相应的标签（见[图
    10.14](ch10.xhtml#ch10fig14)）。'
- en: '![Image](../images/pg563_Image_855.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/pg563_Image_855.jpg)'
- en: 'Figure 10.14: Labeling images as cats or dogs'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.14：将图像标记为猫或狗
- en: Classification doesn’t happen by magic. The model must first be shown many examples
    of dogs and cats with the correct labels in order to properly configure the weights
    of all the connections. This is the training part of supervised learning.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 分类并非凭空发生。模型必须首先展示许多带有正确标签的狗和猫的样本，以便正确配置所有连接的权重。这是有监督学习中的训练部分。
- en: The classic “Hello, world!” demonstration of machine learning and supervised
    learning is a classification problem of the MNIST dataset. Short for *Modified
    National Institute of Standards and Technology*, **MNIST** is a dataset that was
    collected and processed by Yann LeCun (Courant Institute, NYU), Corinna Cortes
    (Google Labs), and Christopher J.C. Burges (Microsoft Research). Widely used for
    training and testing in the field of machine learning, this dataset consists of
    70,000 handwritten digits from 0 to 9; each is a 28×28-pixel grayscale image (see
    [Figure 10.15](ch10.xhtml#ch10fig15) for examples). Each image is labeled with
    its corresponding digit.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的“Hello, world!”机器学习和有监督学习示例是一个MNIST数据集的分类问题。MNIST是*修改版国家标准与技术研究院*的缩写，**MNIST**是由Yann
    LeCun（纽约大学Courant研究所）、Corinna Cortes（谷歌实验室）和Christopher J.C. Burges（微软研究院）收集和处理的数据集。该数据集在机器学习领域中广泛用于训练和测试，包含了70,000个手写数字，从0到9；每个数字是一个28×28像素的灰度图像（参见[图
    10.15](ch10.xhtml#ch10fig15)中的示例）。每个图像都标注了对应的数字。
- en: '![Image](../images/pg563_Image_856.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/pg563_Image_856.jpg)'
- en: 'Figure 10.15: A selection of handwritten digits 0–9 from the MNIST dataset
    (courtesy of Suvanjanprasai)'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.15：MNIST数据集中手写数字0–9的选取样本（由Suvanjanprasai提供）
- en: 'MNIST is a canonical example of a training dataset for image classification:
    the model has a discrete number of categories to choose from (10 to be exact—no
    more, no less). After the model is trained on the 70,000 labeled images, the goal
    is for it to classify new images and assign the appropriate label, a digit from
    0 to 9.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST是一个经典的图像分类训练数据集示例：该模型有一个离散的类别可供选择（准确来说是10个——不多也不少）。在模型经过70,000个标记图像的训练后，目标是让其对新图像进行分类，并分配适当的标签，即0到9之间的数字。
- en: '**Regression**, on the other hand, is a machine learning task for which the
    prediction is a continuous value, typically a floating-point number. A regression
    problem can involve multiple outputs, but thinking about just one is often simpler
    to start. For example, consider a machine learning model that predicts the daily
    electricity usage of a house based on input factors like the number of occupants,
    the size of the house, and the temperature outside (see [Figure 10.16](ch10.xhtml#ch10fig16)).'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '**回归**，另一方面，是一种机器学习任务，其预测结果是一个连续值，通常是一个浮动的数字。回归问题可以涉及多个输出，但从一个输出开始通常更为简单。例如，考虑一个机器学习模型，它根据房屋的入住人数、房屋大小和外部温度等输入因素来预测房屋的日常电力使用量（见[图
    10.16](ch10.xhtml#ch10fig16)）。'
- en: '![Image](../images/pg564_Image_857.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/pg564_Image_857.jpg)'
- en: 'Figure 10.16: Factors like weather and the size and occupancy of a home can
    influence its daily electricity usage.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.16：天气、房屋大小和居住人数等因素可能会影响房屋的日常电力使用量。
- en: Rather than picking from a discrete set of output options, the goal of the neural
    network is now to guess a number—any number. Will the house use 30.5 kilowatt-hours
    of electricity that day? Or 48.7 kWh? Or 100.2 kWh? The output prediction could
    be any value from a continuous range.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 与从离散的输出选项中选择不同，神经网络的目标是猜测一个数字——任何数字。那天这座房子会使用30.5千瓦时的电力吗？还是48.7千瓦时？或者100.2千瓦时？输出预测可能是一个连续范围内的任何值。
- en: '**Network Design**'
  id: totrans-239
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**网络设计**'
- en: 'Knowing what problem you’re trying to solve (step 0) also has a significant
    bearing on the design of the neural network—in particular, on its input and output
    layers. I’ll demonstrate with another classic “Hello, world!” classification example
    from the field of data science and machine learning: the iris dataset. This dataset,
    which can be found in the Machine Learning Repository at the University of California,
    Irvine, originated from the work of American botanist Edgar Anderson.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 知道你要解决的问题（第0步）对神经网络的设计有重要影响，特别是对其输入层和输出层的设计。我将用数据科学和机器学习领域的另一个经典“Hello, world!”分类示例来演示：鸢尾花数据集。这个数据集可以在加利福尼亚大学尔湾分校的机器学习库中找到，来源于美国植物学家埃德加·安德森（Edgar
    Anderson）的研究。
- en: 'Anderson collected flower data over many years across multiple regions of the
    United States and Canada. For more on the origins of this famous dataset, see
    “The Iris Data Set: In Search of the Source of *Virginica*” by Antony Unwin and
    Kim Kleinman (*[https://academic.oup.com/jrssig/article/18/6/26/7038520](https://academic.oup.com/jrssig/article/18/6/26/7038520)*).
    After carefully analyzing the data, Anderson built a table to classify iris flowers
    into three distinct species: *Iris setosa*, *Iris versicolor*, and *Iris virginica*
    (see [Figure 10.17](ch10.xhtml#ch10fig17)).'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 安德森在美国和加拿大的多个地区收集了多年的花卉数据。关于这个著名数据集的起源，详见安东尼·安温（Antony Unwin）和金·克莱因曼（Kim Kleinman）的《鸢尾花数据集：寻找*维吉尼卡*的来源》一文（*
    [https://academic.oup.com/jrssig/article/18/6/26/7038520](https://academic.oup.com/jrssig/article/18/6/26/7038520)
    *）。在仔细分析数据后，安德森构建了一个表格，将鸢尾花分为三种不同的物种：*鸢尾花雪絮种*、*鸢尾花变色种*和*鸢尾花维吉尼卡种*（见[图 10.17](ch10.xhtml#ch10fig17)）。
- en: '![Image](../images/pg565_Image_858.jpg)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/pg565_Image_858.jpg)'
- en: 'Figure 10.17: Three distinct species of iris flowers'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.17：三种不同的鸢尾花物种
- en: 'Anderson included four numeric attributes for each flower: sepal length, sepal
    width, petal length, and petal width, all measured in centimeters. (He also recorded
    color information, but that data appears to have been lost.) Each record is then
    paired with the appropriate iris categorization:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 安德森为每朵花包括了四个数值属性：花萼长度、花萼宽度、花瓣长度和花瓣宽度，所有数据均以厘米为单位。（他还记录了颜色信息，但该数据似乎已丢失。）每条记录都会与适当的鸢尾花分类配对：
- en: '| **Sepal Length** | **Sepal Width** | **Petal Length** | **Petal Width** |
    **Classification** |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| **花萼长度** | **花萼宽度** | **花瓣长度** | **花瓣宽度** | **分类** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 5.1 | 3.5 | 1.4 | 0.2 | *Iris setosa* |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 5.1 | 3.5 | 1.4 | 0.2 | *鸢尾花雪絮种* |'
- en: '| 4.9 | 3.0 | 1.4 | 0.2 | *Iris setosa* |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 4.9 | 3.0 | 1.4 | 0.2 | *鸢尾花雪絮种* |'
- en: '| 7.0 | 3.2 | 4.7 | 1.4 | *Iris versicolor* |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 7.0 | 3.2 | 4.7 | 1.4 | *鸢尾花变色种* |'
- en: '| 6.4 | 3.2 | 4.5 | 1.5 | *Iris versicolor* |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 6.4 | 3.2 | 4.5 | 1.5 | *鸢尾花变色种* |'
- en: '| 6.3 | 3.3 | 6.0 | 2.5 | *Iris virginica* |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 6.3 | 3.3 | 6.0 | 2.5 | *鸢尾花维吉尼卡种* |'
- en: '| 5.8 | 2.7 | 5.1 | 1.9 | *Iris virginica* |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 5.8 | 2.7 | 5.1 | 1.9 | *鸢尾花维吉尼卡种* |'
- en: In this dataset, the first four columns (sepal length, sepal width, petal length,
    petal width) serve as inputs to the neural network. The output is the classification
    provided in the fifth column. [Figure 10.18](ch10.xhtml#ch10fig18) depicts a possible
    architecture for a neural network that can be trained on this data.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个数据集中，前四列（花萼长度、花萼宽度、花瓣长度、花瓣宽度）作为神经网络的输入。输出是第五列中提供的分类。[图 10.18](ch10.xhtml#ch10fig18)展示了一个可以在此数据上训练的神经网络可能的架构。
- en: '![Image](../images/pg566_Image_859.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/pg566_Image_859.jpg)'
- en: 'Figure 10.18: A possible network architecture for iris classification'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.18：鸢尾花分类的可能网络架构
- en: On the left are the four inputs to the network, corresponding to the first four
    columns of the data table. On the right are three possible outputs, each representing
    one of the iris species labels. In between is the hidden layer, which, as mentioned
    earlier, adds complexity to the network’s architecture, necessary for handling
    nonlinearly separable data. Each node in the hidden layer is connected to every
    node that comes before and after it. This is commonly called a **fully connected**
    or **dense** layer.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧是网络的四个输入，分别对应数据表的前四列。右侧是三个可能的输出，每个输出代表一种鸢尾花物种标签。中间是隐藏层，如前所述，它为网络架构增加了复杂性，这是处理非线性可分数据所必需的。隐藏层中的每个节点都与前后节点相连接。这通常被称为**全连接**层或**密集**层。
- en: You might also notice the absence of explicit bias nodes in this diagram. While
    biases play an important role in the output of each neuron, they’re often left
    out of visual representations to keep the diagrams clean and focused on the primary
    data flow. (The ml5.js library will ultimately manage the biases for me internally.)
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 你也许会注意到，在这个图表中没有明确的偏置节点。虽然偏置在每个神经元的输出中起着重要作用，但它们通常在视觉表示中被省略，以保持图表的简洁并专注于主要的数据流。（ml5.js
    库最终会在内部为我管理偏置。）
- en: The neural network’s goal is to “activate” the correct output for the input
    data, just as the perceptron would output a +1 or –1 for its single binary classification.
    In this case, the output values are like signals that help the network decide
    which iris species label to assign. The highest computed value activates to signify
    the network’s best guess about the classification.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的目标是“激活”正确的输出，以适应输入数据，就像感知机会对其单一的二分类输出 +1 或 -1 一样。在这种情况下，输出值就像是信号，帮助网络决定要分配哪个鸢尾花物种标签。最高计算值的激活代表了网络对分类的最佳猜测。
- en: The key takeaway here is that a classification network should have as many inputs
    as there are values for each item in the dataset, and as many outputs as there
    are categories. As for the hidden layer, the design is much less set in stone.
    The hidden layer in [Figure 10.18](ch10.xhtml#ch10fig18) has five nodes, but this
    number is entirely arbitrary. Neural network architectures can vary greatly, and
    the number of hidden nodes is often determined through trial and error or other
    educated guessing methods (called *heuristics*). In the context of this book,
    I’ll be relying on ml5.js to automatically configure the architecture based on
    the input and output data.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的关键点是，分类网络应该有与数据集中每个项目的值相等的输入数目，并且输出的数目应该等于类别的数量。至于隐藏层，它的设计并不是固定的。[图 10.18](ch10.xhtml#ch10fig18)中的隐藏层有五个节点，但这个数字完全是任意的。神经网络架构可以有很大的差异，隐藏节点的数量通常通过反复试验或其他有根据的猜测方法（称为*启发式方法*）来确定。在本书的上下文中，我将依赖于ml5.js来根据输入和输出数据自动配置架构。
- en: 'What about the inputs and outputs in a regression scenario, like the household
    electricity consumption example I mentioned earlier? I’ll go ahead and make up
    a dataset for this scenario, with values representing the occupants and size of
    the house, the day’s temperature, and the corresponding electricity usage. This
    is much like a synthetic dataset, given that it’s not data collected for a real-world
    scenario—but whereas synthetic data is generated automatically, here I’m manually
    inputting numbers from my own imagination:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 那么在回归场景中，如我之前提到的家庭电力消耗的例子，输入和输出该如何处理呢？我将为这个场景编造一个数据集，包含居住人数、房屋面积、当天的温度，以及相应的电力使用量。这就像是一个合成数据集，考虑到它并非为真实世界情境收集的数据——但与自动生成的合成数据不同，在这里我正在手动输入我自己想象中的数字：
- en: '| **Occupants** | **Size (m²)** | **Temperature Outside (°C)** | **Electricity
    Usage (kWh)** |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| **居住人数** | **面积 (m²)** | **外部温度 (°C)** | **电力使用 (kWh)** |'
- en: '| 4 | 150 | 24 | 25.3 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 150 | 24 | 25.3 |'
- en: '| 2 | 100 | 25.5 | 16.2 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 100 | 25.5 | 16.2 |'
- en: '| 1 | 70 | 26.5 | 12.1 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 70 | 26.5 | 12.1 |'
- en: '| 4 | 120 | 23 | 22.1 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 120 | 23 | 22.1 |'
- en: '| 2 | 90 | 21.5 | 15.2 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 90 | 21.5 | 15.2 |'
- en: '| 5 | 180 | 20 | 24.4 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 180 | 20 | 24.4 |'
- en: '| 1 | 60 | 18.5 | 11.7 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 60 | 18.5 | 11.7 |'
- en: The neural network for this problem should have three input nodes corresponding
    to the first three columns (occupants, size, temperature). Meanwhile, it should
    have one output node representing the fourth column, the network’s guess about
    the electricity usage. And I’ll arbitrarily say the network’s hidden layer should
    have four nodes rather than five. [Figure 10.19](ch10.xhtml#ch10fig19) shows this
    network architecture.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个问题的神经网络，应该有三个输入节点，分别对应前面三列（居住人数、面积、温度）。与此同时，它应该有一个输出节点，表示第四列，即网络对电力使用量的预测。我随便说一下，网络的隐藏层应该有四个节点，而不是五个。[图
    10.19](ch10.xhtml#ch10fig19)展示了这种网络架构。
- en: '![Image](../images/pg567_Image_860.jpg)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/pg567_Image_860.jpg)'
- en: 'Figure 10.19: A possible network architecture for three inputs and one regression
    output'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.19：一个可能的网络架构，具有三个输入和一个回归输出
- en: Unlike the iris classification network, which is choosing from three labels
    and therefore has three outputs, this network is trying to predict just one number,
    so it has only one output. I’ll note, however, that a single output isn’t a requirement
    of regression. A machine learning model can also perform a regression that predicts
    multiple continuous values, in which case the model would have multiple outputs.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 与鸢尾花分类网络不同，后者是从三个标签中选择，因此有三个输出，而这个网络试图预测一个数字，因此只有一个输出。然而，我需要指出的是，单一输出并不是回归的要求。机器学习模型也可以执行预测多个连续值的回归，在这种情况下，模型将有多个输出。
- en: '**ml5.js Syntax**'
  id: totrans-273
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**ml5.js 语法**'
- en: The ml5.js library is a collection of machine learning models that can be accessed
    using the syntax `ml5.`functionName`()`. For example, to use a pretrained model
    that detects hand positions, you can use `ml5.handPose()`. For classifying images,
    you can use `ml5.imageClassifier()`. While I encourage you to explore all that
    ml5.js has to offer (I’ll reference some of these pretrained models in upcoming
    exercise ideas), for this chapter I’ll focus on only one function in ml5.js, `ml5.neuralNetwork()`,
    which creates an empty neural network for you to train.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: ml5.js 库是一个集合，包含可以通过语法 `ml5.`functionName`() ` 访问的机器学习模型。例如，要使用一个预训练的模型来检测手部位置，可以使用
    `ml5.handPose()`。要分类图像，可以使用 `ml5.imageClassifier()`。虽然我鼓励你探索 ml5.js 提供的所有功能（接下来的练习中我会提到一些这些预训练模型），但本章我将重点介绍
    ml5.js 中的一个函数，`ml5.neuralNetwork()`，它为你创建一个空的神经网络供你训练。
- en: 'To use this function, you must first create a JavaScript object that will configure
    the model being created. Here’s where some of the big-picture factors I just discussed—is
    this a classification or a regression task? How many inputs and outputs?—come
    into play. I’ll begin by specifying the task I want the model to perform ( `"regression"`
    or `"classification"` ):'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用这个函数，首先必须创建一个 JavaScript 对象来配置正在创建的模型。此时，我刚才讨论的一些大局因素——这是一个分类任务还是回归任务？有多少个输入和输出？——开始发挥作用。我将首先指定模型要执行的任务（`"regression"`
    或 `"classification"`）：
- en: '[PRE4]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This, however, gives ml5.js little to go on in terms of designing the network
    architecture. Adding the inputs and outputs will complete the rest of the puzzle.
    The iris flower classification has four inputs and three possible output labels.
    This can be configured as part of the `options` object with a single integer for
    the number of inputs and an array of strings listing the output labels:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这给 ml5.js 在设计网络架构时提供的信息很少。添加输入和输出将完成其余的工作。鸢尾花分类有四个输入和三个可能的输出标签。可以将其配置为 `options`
    对象的一部分，其中包含一个整数表示输入数量，以及一个包含输出标签的字符串数组：
- en: '[PRE5]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The electricity regression scenario had three input values (occupants, size,
    temperature) and one output value (usage in kWh). With regression, there are no
    string output labels, so only an integer indicating the number of outputs is required:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 电力回归场景有三个输入值（居住者、大小、温度）和一个输出值（kWh 的使用量）。使用回归时，输出没有字符串标签，因此只需要一个整数来表示输出的数量：
- en: '[PRE6]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: You can set many other properties of the model through the `options` object.
    For example, you could specify the number of hidden layers between the inputs
    and outputs (there are typically several), the number of neurons in each layer,
    which activation functions to use, and more. In most cases, however, you can leave
    out these extra settings and let ml5.js make its best guess on how to design the
    model based on the task and data at hand.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过 `options` 对象设置模型的许多其他属性。例如，你可以指定输入和输出之间的隐藏层数量（通常有几个），每个层中的神经元数量，要使用的激活函数等等。然而，在大多数情况下，你可以省略这些额外的设置，让
    ml5.js 根据任务和手头的数据猜测如何设计模型。
- en: '**Building a Gesture Classifier**'
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**构建手势分类器**'
- en: I’ll now walk through the steps of the machine learning life cycle with an example
    problem well suited for p5.js, building all the code for each step along the way
    using ml5.js. I’ll begin at step 0 by articulating the problem. Imagine for a
    moment that you’re working on an interactive application that responds to gestures.
    Maybe the gestures are ultimately meant to be recorded via body tracking, but
    you want to start with something much simpler—a single stroke of the mouse (see
    [Figure 10.20](ch10.xhtml#ch10fig20)).
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我将通过一个非常适合 p5.js 的示例问题，带你走过机器学习生命周期的每个步骤，在此过程中使用 ml5.js 编写每一步的代码。我将从第 0 步开始，阐明问题。假设你正在开发一个响应手势的互动应用程序。也许这些手势最终是通过身体追踪记录的，但你想从一个更简单的开始——鼠标的单次点击（见[图
    10.20](ch10.xhtml#ch10fig20)）。
- en: '![Image](../images/pg569_Image_861.jpg)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/pg569_Image_861.jpg)'
- en: 'Figure 10.20: A single mouse gesture as a vector between a start and end point'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.20：单一鼠标手势作为起点和终点之间的向量
- en: 'Each gesture could be recorded as a vector extending from the start to the
    end point of a mouse movement. The x- and y-components of the vector will be the
    model’s inputs. The model’s task could be to predict one of four possible labels
    for the gesture: *up*, *down*, *left*, or *right*. With a discrete set of possible
    outputs, this sounds like a classification problem. The four labels will be the
    model’s outputs.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 每个手势都可以被记录为一个从起点到终点的鼠标移动向量。向量的 x 和 y 分量将作为模型的输入。模型的任务可能是预测该手势的四个可能标签之一：*上*、*下*、*左*
    或 *右*。由于输出是有限的离散集，这听起来像是一个分类问题。这四个标签将是模型的输出。
- en: Much like some of the GA demonstrations in [Chapter 9](ch09.xhtml#ch09)—and
    like the simple perceptron example earlier in this chapter—the problem I’m selecting
    here has a known solution and could be solved more easily and efficiently without
    a neural network. The direction of a vector can be classified with the `heading()`
    function and a series of `if` statements! However, by using this seemingly trivial
    scenario, I hope to explain the process of training a machine learning model in
    an understandable and friendly way. Additionally, this example will make it easy
    to check that the code is working as expected. When I’m done, I’ll provide some
    ideas about how to expand the classifier to a scenario that couldn’t use simple
    `if` statements.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在 [第9章](ch09.xhtml#ch09)中的一些遗传算法示例——以及本章早些时候的简单感知器示例——我在这里选择的问题是一个已知的有解问题，并且可以在没有神经网络的情况下更轻松、高效地解决。通过
    `heading()` 函数和一系列 `if` 语句，就可以对向量的方向进行分类！然而，通过使用这个看似微不足道的场景，我希望以一种易于理解且友好的方式来解释训练机器学习模型的过程。此外，这个示例将使得检查代码是否按预期工作变得简单。当我完成时，我将提供一些如何将分类器扩展到无法使用简单
    `if` 语句的场景的想法。
- en: '**Collecting and Preparing the Data**'
  id: totrans-288
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**收集和准备数据**'
- en: 'With the problem established, I can turn to steps 1 and 2: collecting and preparing
    the data. In the real world, these steps can be tedious, especially when the raw
    data you collect is messy and needs a lot of initial processing. You can think
    of this like having to organize, wash, and chop all your ingredients before you
    can start cooking a meal from scratch.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 确定问题后，我可以进入步骤 1 和 2：收集和准备数据。在现实世界中，这些步骤可能会很繁琐，特别是当你收集到的原始数据很杂乱，需要大量初步处理时。你可以把这看作是需要在做饭之前先整理、清洗和切割所有食材的过程。
- en: For simplicity, I’d instead like to take the approach of ordering a machine
    learning “meal kit,” with the ingredients (data) already portioned and prepared.
    This way, I’ll get straight to the cooking itself, the process of training the
    model. After all, this is really just an appetizer for what will be the ultimate
    meal in [Chapter 11](ch11.xhtml#ch11), when I apply neural networks to steering
    agents.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，我更倾向于采取一种订购机器学习“餐包”的方法，所有食材（数据）都已分好份并准备好。这样，我就可以直接开始“烹饪”过程，即训练模型。毕竟，这其实只是对
    [第11章](ch11.xhtml#ch11)的开胃菜，届时我将应用神经网络来引导代理。
- en: 'With that in mind, I’ll handcode some example data and manually keep it normalized
    within a range of –1 and +1\. I’ll organize the data into an array of objects,
    pairing the x- and y-components of a vector with a string label. I’m picking values
    that I feel clearly point in a specific direction and assigning the appropriate
    label—two examples per label:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 有鉴于此，我将手动编写一些示例数据，并将其规范化到 –1 到 +1 之间。我会将数据组织成一个对象数组，配对向量的 x 和 y 分量与字符串标签。我选择的值明确指向一个特定的方向，并分配相应的标签——每个标签有两个示例：
- en: '[PRE7]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[Figure 10.21](ch10.xhtml#ch10fig21) shows the same data expressed as arrows.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10.21](ch10.xhtml#ch10fig21) 显示了以箭头形式表示的相同数据。'
- en: '![Image](../images/pg571_Image_862.jpg)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/pg571_Image_862.jpg)'
- en: 'Figure 10.21: The input data visualized as vectors (arrows)'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.21：将输入数据可视化为向量（箭头）
- en: In a more realistic scenario, I’d probably have a much larger dataset that would
    be loaded in from a separate file, instead of written directly into the code.
    For example, JavaScript Object Notation (JSON) and comma-separated values (CSV)
    are two popular formats for storing and loading data. JSON stores data in key-value
    pairs and follows the same exact format as JavaScript object literals. CSV is
    a file format that stores tabular data (like a spreadsheet). You could use numerous
    other data formats, depending on your needs and the programming environment you’re
    working with.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个更现实的场景中，我可能会有一个更大的数据集，这些数据将从一个单独的文件加载，而不是直接写入代码中。例如，JavaScript 对象表示法（JSON）和逗号分隔值（CSV）是两种常见的数据存储和加载格式。JSON
    以键值对的形式存储数据，遵循与 JavaScript 对象字面量完全相同的格式。CSV 是一种存储表格数据（如电子表格）的文件格式。根据你的需求和所使用的编程环境，你还可以使用许多其他数据格式。
- en: In the real world, the values in that larger dataset would actually come from
    somewhere. Maybe I would collect the data by asking users to perform specific
    gestures and recording their inputs, or by writing an algorithm to automatically
    generate larger amounts of synthetic data that represent the idealized versions
    of the gestures I want the model to recognize. In either case, the key would be
    to collect a diverse set of examples that adequately represent the variations
    in how the gestures might be performed. For now, however, let’s see how it goes
    with just a few servings of data.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中，那个更大的数据集中的值实际上会来自某个地方。也许我会通过要求用户执行特定手势并记录他们的输入来收集数据，或者通过编写算法自动生成大量合成数据，这些数据代表我希望模型识别的理想化手势版本。无论哪种方式，关键是收集一个多样化的示例集，充分代表手势执行方式的不同变化。不过，现在让我们看看仅凭少量数据会怎样。
- en: '![Image](../images/pencil.jpg) **Exercise 10.4**'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '![Image](../images/pencil.jpg) **练习 10.4**'
- en: Create a p5.js sketch that collects gesture data from users and saves it to
    a JSON file. You can use `mousePressed()` and `mouseReleased()` to mark the start
    and end of each gesture, and `saveJSON()` to download the data into a file.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个 p5.js 草图，收集用户的手势数据并将其保存到 JSON 文件中。你可以使用 `mousePressed()` 和 `mouseReleased()`
    来标记每个手势的开始和结束，使用 `saveJSON()` 将数据下载到文件中。
- en: '**Choosing a Model**'
  id: totrans-300
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**选择模型**'
- en: 'I’ve now come to step 3 of the machine learning life cycle, selecting a model.
    This is where I’m going to start letting ml5.js do the heavy lifting for me. To
    create the model with ml5.js, all I need to do is specify the task, the inputs,
    and the outputs:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我已经进入了机器学习生命周期的第 3 步，选择一个模型。在这一阶段，我将开始让 ml5.js 为我做繁重的工作。为了用 ml5.js 创建模型，我所需要做的就是指定任务、输入和输出：
- en: '[PRE8]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: That’s it! I’m done! Thanks to ml5.js, I can bypass a host of complexities such
    as the number of layers and neurons per layer to have, the kinds of activation
    functions to use, and how to set up the algorithms for training the network. The
    library will make these decisions for me.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！我完成了！多亏了 ml5.js，我可以绕过一堆复杂的工作，比如每层的神经元数、激活函数的种类，以及如何设置训练网络的算法。这个库会为我做出这些决定。
- en: Of course, the default ml5.js model architecture may not be perfect for all
    cases. I encourage you to read the ml5.js documentation for additional details
    on how to customize the model. I’ll also point out that ml5.js is able to infer
    the inputs and outputs from the data, so those properties aren’t entirely necessary
    to include here in the `options` object. However, for the sake of clarity (and
    since I’ll need to specify them for later examples), I’m including them here.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，默认的 ml5.js 模型架构可能并不适用于所有情况。我鼓励你阅读 ml5.js 的文档，了解如何自定义模型的更多细节。我还要指出的是，ml5.js
    能够从数据中推断输入和输出，因此这些属性并不完全需要包含在 `options` 对象中。不过，为了清晰起见（并且因为后续示例中我需要指定它们），我在这里包含了这些属性。
- en: The `debug` property, when set to `true`, turns on a visual interface for the
    training process. It’s a helpful tool for spotting potential issues during training
    and for getting a better understanding of what’s happening behind the scenes.
    You’ll see what this interface looks like later in the chapter.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 当 `debug` 属性设置为 `true` 时，它会启动训练过程的可视化界面。这是一个有助于在训练过程中发现潜在问题，并更好地理解后台发生的事情的工具。你将在本章后面看到这个界面的样子。
- en: '**Training the Model**'
  id: totrans-306
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**训练模型**'
- en: Now that I have the data in a `data` variable and a neural network initialized
    in the `classifier` variable, I’m ready to train the model. That process starts
    with adding the data to the model. And for that, it turns out I’m not quite done
    with preparing the data.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我已经将数据存储在 `data` 变量中，并且在 `classifier` 变量中初始化了神经网络，我已经准备好训练模型。这个过程从将数据添加到模型开始。为了实现这一点，事实证明我还没有完全准备好数据。
- en: Right now, my data is neatly organized in an array of objects, each containing
    the x- and y-components of a vector and a corresponding string label. This is
    a typical format for training data, but it isn’t directly consumable by ml5.js.
    (Sure, I could have initially organized the data into a format that ml5.js recognizes,
    but I’m including this extra step because it will likely be necessary when you’re
    using a dataset that has been collected or sourced elsewhere.) To add the data
    to the model, I need to separate the inputs from the outputs so that the model
    understands which are which.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我的数据整齐地组织在一个对象数组中，每个对象包含一个向量的 x 和 y 组件以及一个相应的字符串标签。这是训练数据的典型格式，但 ml5.js 并不能直接使用它。（当然，我本可以一开始就将数据组织成
    ml5.js 能识别的格式，但我之所以包括这个额外的步骤，是因为当你使用从其他地方收集或获取的数据集时，这个步骤可能是必需的。）为了将数据添加到模型中，我需要将输入数据与输出数据分开，以便模型理解哪些是输入，哪些是输出。
- en: 'The ml5.js library offers a fair amount of flexibility in the kinds of formats
    it will accept, but I’ll choose to use arrays—one for the `inputs` and one for
    the `outputs`. I can use a loop to reorganize each data item and add it to the
    model:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: ml5.js 库在接受的格式方面提供了相当大的灵活性，但我选择使用数组——一个用于 `inputs`，一个用于 `outputs`。我可以使用循环来重新组织每个数据项并将其添加到模型中：
- en: '![Image](../images/pg572_Image_863.jpg)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/pg572_Image_863.jpg)'
- en: What I’ve done here is set the **shape** of the data. In machine learning, this
    term describes the data’s dimensions and structure. It indicates how the data
    is organized in terms of rows, columns, and potentially even deeper, into additional
    dimensions. Understanding the shape of your data is crucial because it determines
    the way the model should be structured.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里所做的是设置数据的**形状**。在机器学习中，这个术语描述了数据的维度和结构。它表示数据如何在行、列及可能更深的额外维度中组织。理解数据的形状至关重要，因为它决定了模型的结构方式。
- en: Here, the input data’s shape is a 1D array containing two numbers (representing
    *x* and *y*). The output data, similarly, is a 1D array containing just a single
    string label. Every piece of data going in and out of the network will follow
    this pattern. While this is a small and simple example, it nicely mirrors many
    real-world scenarios in which the inputs are numerically represented in an array,
    and the outputs are string labels.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，输入数据的形状是一个包含两个数字的 1D 数组（代表 *x* 和 *y*）。输出数据同样是一个包含单个字符串标签的 1D 数组。所有进出网络的数据都会遵循这个模式。虽然这是一个小而简单的示例，但它很好地反映了许多现实世界场景，其中输入数据以数组的形式表示数字，输出则是字符串标签。
- en: 'After passing the data into the `classifier`, ml5.js provides a helper function
    to normalize it. As I’ve mentioned, normalizing data (adjusting the scale to a
    standard range) is a critical step in the machine learning process:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在将数据传递给 `classifier` 之后，ml5.js 提供了一个辅助函数来归一化数据。正如我之前提到的，归一化数据（调整数据的尺度至标准范围）是机器学习过程中至关重要的一步：
- en: '![Image](../images/pg573_Image_864.jpg)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/pg573_Image_864.jpg)'
- en: In this case, the handcoded data was limited to a range of –1 to +1 from the
    get-go, so calling `normalizeData()` here is likely redundant. Still, this function
    call is important to demonstrate. Normalizing your data ahead of time as part
    of the preprocessing step will absolutely work, but the auto-normalization feature
    of ml5.js is a big help!
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，手动编码的数据从一开始就被限制在 -1 到 +1 的范围内，因此在这里调用 `normalizeData()` 可能是多余的。然而，调用这个函数非常重要，因为它有助于演示。提前对数据进行归一化作为预处理步骤肯定是有效的，但
    ml5.js 的自动归一化功能也非常有帮助！
- en: 'Now for the heart of the machine learning process: actually training the model.
    Here’s the code:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是机器学习过程的核心：实际训练模型。这里是代码：
- en: '![Image](../images/pg573_Image_865.jpg)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/pg573_Image_865.jpg)'
- en: Yes, that’s it! After all, the hard work has already been completed. The data
    was collected, prepared, and fed into the model. All that remains is to call the
    `train()` method, sit back, and let ml5.js do its thing.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，就这样！毕竟，繁重的工作已经完成。数据已经收集、准备好，并输入到模型中。剩下的就是调用 `train()` 方法，坐下来，让 ml5.js 自动执行剩余的工作。
- en: 'In truth, it isn’t *quite* that simple. If I were to run the code as written
    and then test the model, the results would probably be inadequate. Here’s where
    another key term in machine learning comes into play: **epochs**. The `train()`
    method tells the neural network to start the learning process. But how long should
    it train for? You can think of an epoch as one round of practice, one cycle of
    using the entire training dataset to update the weights of the neural network.
    Generally speaking, the more epochs you go through, the better the network will
    perform, but at a certain point you’ll have diminishing returns. The number of
    epochs can be set by passing in an `options` object into `train()`.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，这并不是*那么*简单。如果我按原样运行代码然后测试模型，结果可能会不尽如人意。这时，机器学习中的另一个关键术语就派上用场了：**epochs（训练轮次）**。`train()`方法告诉神经网络开始学习过程。但是，它应该训练多久呢？你可以把一个epoch想象成一次练习，使用整个训练数据集来更新神经网络的权重。一般来说，经过的epoch越多，网络的表现会越好，但到了一定阶段，你会遇到收益递减的情况。epoch的数量可以通过将`options`对象传递给`train()`来设置。
- en: '![Image](../images/pg574_Image_866.jpg)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/pg574_Image_866.jpg)'
- en: The number of epochs is an example of a hyperparameter, a global setting for
    the training process. You can set others through the `options` object (the learning
    rate, for example), but I’m going to stick with the defaults. You can read more
    about customization options in the ml5.js documentation.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 训练的epoch数量是超参数的一个例子，超参数是训练过程的全局设置。你可以通过`options`对象设置其他超参数（例如学习率），但我将使用默认设置。你可以在ml5.js文档中阅读更多关于自定义选项的信息。
- en: The second argument to `train()` is optional, but it’s good to include one.
    It specifies a callback function that runs when the training process is complete—in
    this case, `finshedTraining()`. (See the “Callbacks” box for more on callback
    functions.) This is useful for knowing when you can proceed to the next steps
    in your code. Another optional callback, which I usually name `whileTraining()`,
    is triggered after each epoch. However, for my purposes, knowing when the training
    is done is plenty!
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '`train()`的第二个参数是可选的，但最好包括一个。它指定了一个回调函数，该函数在训练过程完成时运行——在这种情况下是`finshedTraining()`。（有关回调函数的更多信息，请参见“回调函数”框。）这对于知道何时可以继续执行代码中的下一步非常有用。另一个可选的回调函数，我通常将其命名为`whileTraining()`，会在每个epoch之后触发。然而，就我的目的而言，知道训练何时完成就足够了！'
- en: '![Image](../images/zoom.jpg) **Callbacks**'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '![Image](../images/zoom.jpg) **回调函数**'
- en: A **callback function** in JavaScript is a function you don’t actually call
    yourself. Instead, you provide it as an argument to another function, intending
    for it to be *called back* automatically at a later time (typically associated
    with an event, like a mouse click). You’ve seen this before when working with
    Matter.js in [Chapter 6](ch06.xhtml#ch06), where you specified a function to call
    whenever a collision was detected.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: JavaScript中的**回调函数**是一种你并不会直接调用的函数。相反，你将它作为参数传递给另一个函数，目的是让它在稍后的某个时刻*自动调用*（通常与某个事件相关，比如鼠标点击）。你以前在[第6章](ch06.xhtml#ch06)使用Matter.js时见过这种情况，你指定了一个函数，当检测到碰撞时会被调用。
- en: Callbacks are needed for **asynchronous** operations, when you want your code
    to continue along with animating or doing other things while waiting for another
    task (like training a machine learning model) to finish. A classic example of
    this in p5.js is loading data into a sketch with `loadJSON()`.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 回调函数在**异步**操作中是必需的，当你希望代码在等待另一个任务（比如训练机器学习模型）完成时，继续进行动画或其他操作。p5.js中的经典例子是使用`loadJSON()`加载数据到草图中。
- en: JavaScript also provides a more recent approach for handling asynchronous operations
    known as **promises**. With promises, you can use keywords like `async` and `await`
    to make your asynchronous code look more like traditional synchronous code. While
    ml5.js also supports this style, I’ll stick to using callbacks to stay aligned
    with p5.js style.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: JavaScript还提供了一种更现代的方法来处理异步操作，这就是**promise（承诺）**。通过promise，你可以使用`async`和`await`等关键字，使你的异步代码看起来更像传统的同步代码。虽然ml5.js也支持这种风格，但为了与p5.js的风格保持一致，我还是会坚持使用回调函数。
- en: '**Evaluating the Model**'
  id: totrans-327
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**评估模型**'
- en: If `debug` is set to `true` in the initial call to `ml5.neuralNetwork()`, a
    visual interface should appear after `train()` is called, covering most of the
    p5.js page and canvas (see [Figure 10.22](ch10.xhtml#ch10fig22)). This interface,
    called the *Visor*, represents the evaluation step.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在初始调用`ml5.neuralNetwork()`时将`debug`设置为`true`，则在调用`train()`之后应该会出现一个视觉界面，覆盖大部分p5.js页面和画布（参见[图10.22](ch10.xhtml#ch10fig22)）。这个界面被称为*Visor*，代表了评估步骤。
- en: '![Image](../images/pg575_Image_867.jpg)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/pg575_Image_867.jpg)'
- en: 'Figure 10.22: The Visor, with a graph of the loss function and model details'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.22：Visor，展示了损失函数图和模型细节
- en: The Visor comes from TensorFlow.js (which underlies ml5.js) and includes a graph
    that provides real-time feedback on the progress of the training. This graph plots
    the loss of the model on the y-axis against the number of epochs along the x-axis.
    **Loss** is a measure of how far off the model’s predictions are from the correct
    outputs provided by the training data. It quantifies the model’s total error.
    When training begins, it’s common for the loss to be high because the model has
    yet to learn anything. Ideally, as the model trains through more epochs, it should
    get better at its predictions, and the loss should decrease. If the graph goes
    down as the epochs increase, this is a good sign!
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: Visor 来自 TensorFlow.js（它是 ml5.js 的基础），包括一个图表，实时反馈训练进度。这个图表将模型的损失值绘制在 y 轴上，将训练周期数绘制在
    x 轴上。**损失**是衡量模型预测与训练数据提供的正确输出之间差距的指标。它量化了模型的总误差。当训练开始时，损失通常较高，因为模型尚未学到任何东西。理想情况下，随着模型训练的进行，它的预测应该会变得更好，损失应该会减少。如果图表随着训练周期的增加而下降，那是一个好兆头！
- en: Running the training for the 200 epochs depicted in [Figure 10.21](ch10.xhtml#ch10fig21)
    might strike you as a bit excessive. In a real-world scenario with more extensive
    data, I would probably use fewer epochs, like the 25 I specified in the original
    code snippet. However, because the dataset here is so tiny, the higher number
    of epochs helps the model get enough practice with the data. Remember, this is
    a toy example, aiming to make the concepts clear rather than to produce a sophisticated
    machine learning model.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图 10.21 中描述的 200 次训练周期，你可能会觉得有些过多。在一个现实世界的场景中，数据量更大的话，我可能会使用更少的周期，比如我在原始代码片段中指定的
    25 次。然而，由于这里的数据集非常小，较多的周期有助于模型更充分地与数据进行训练。记住，这是一个示例，目的是让概念变得清晰，而不是为了生产一个复杂的机器学习模型。
- en: Below the graph, the Visor shows a Model Summary table with details on the lower-level
    TensorFlow.js model architecture created behind the scenes. The summary includes
    layer names, neuron counts per layer (in the Output Shape column), and a parameters
    count, which is the total number of weights, one for each connection between two
    neurons. In this case, dense_Dense1 is the hidden layer with 16 neurons (a number
    chosen by ml5.js), and dense_Dense2 is the output layer with 4 neurons, one for
    each classification category. (TensorFlow.js doesn’t think of the inputs as a
    distinct layer; rather, they’re merely the starting point of the data flow.) The
    *batch* in the Output Shape column doesn’t refer to a specific number but indicates
    that the model can process a variable amount of training data (a batch) for any
    single cycle of model training.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在图表下方，Visor 显示了一个模型摘要表，包含了幕后创建的低级 TensorFlow.js 模型架构的详细信息。摘要包括每层的名称、每层的神经元数量（在输出形状列中）以及参数计数，参数计数是每个神经元连接之间的权重总数。在这种情况下，dense_Dense1
    是具有 16 个神经元的隐藏层（这是 ml5.js 选择的数字），而 dense_Dense2 是具有 4 个神经元的输出层，每个神经元对应一个分类类别。（TensorFlow.js
    并不把输入视为一个独立的层；它们只是数据流的起点。）输出形状列中的 *batch* 并不指代特定的数字，而是表示模型可以处理任意数量的训练数据（一个批次），用于单次模型训练周期。
- en: 'Before moving on from the evaluation stage, I have a loose end to tie up. When
    I first outlined the steps of the machine learning life cycle, I mentioned that
    preparing the data typically involves splitting the dataset into three parts to
    help with the evaluation process:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入评估阶段之前，我有一个细节需要补充。当我首次概述机器学习生命周期的步骤时，我提到准备数据通常涉及将数据集分成三部分，以帮助评估过程：
- en: '**Training:** The primary dataset used to train the model'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练集：** 用于训练模型的主要数据集'
- en: '**Validation:** A subset of the data used to check the model during training,
    typically at the end of each epoch'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**验证集：** 在训练过程中用于检查模型的数据子集，通常在每个训练周期结束时进行'
- en: '**Testing:** Additional untouched data never considered during the training
    process, for determining the model’s final performance after the training is completed'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试集：** 在训练过程中从未使用过的额外数据，用于在训练完成后确定模型的最终表现'
- en: You may have noticed that I never did this. For simplicity, I’ve instead used
    the entire dataset for training. After all, my dataset has only eight records;
    it’s much too small to divide three sets! With a large dataset, this three-way
    split would be more appropriate.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，我并没有这样做。为了简化，我直接使用了整个数据集进行训练。毕竟，我的数据集只有八条记录；把它分成三组太小了！如果数据集更大，三分法则会更为合适。
- en: 'Using such a small dataset risks the model **overfitting** the data, however:
    the model becomes so tuned to the specific peculiarities of the training data
    that it’s much less effective when working with new, unseen data. The main reason
    to use a validation set is to monitor the model during the training process. As
    training progresses, if the model’s accuracy improves on the training data but
    deteriorates on the validation data, it’s a strong indicator that overfitting
    might be occurring. (The testing set is reserved strictly for the final evaluation,
    one more chance after training is complete to gauge the model’s performance.)'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用如此小的数据集会有导致模型**过拟合**数据的风险：模型会过度调整到训练数据的特定特性，以至于在处理新的、未见过的数据时效果大大下降。使用验证集的主要原因是监控模型在训练过程中的表现。随着训练的进行，如果模型在训练数据上的准确率提高，但在验证数据上的准确率下降，这通常是过拟合发生的强烈信号。（测试集严格保留用于最终评估，是训练完成后评估模型性能的最后一次机会。）
- en: For more realistic scenarios, ml5.js provides a way to split up the data, as
    well as automatic features for employing validation data. If you’re inclined to
    go further, you can explore the full set of neural network examples on the ml5.js
    website (*[https://ml5js.org](https://ml5js.org)*).
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更现实的场景，ml5.js 提供了一种方法来拆分数据，并且自动化处理验证数据的功能。如果你有兴趣深入了解，可以在 ml5.js 网站上探索完整的神经网络示例
    (*[https://ml5js.org](https://ml5js.org)*)。
- en: '**Tuning the Parameters**'
  id: totrans-341
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**调优参数**'
- en: After the evaluation step, there’s typically an iterative process of adjusting
    hyperparameters and going through training again to achieve the best performance
    from the model. While ml5.js offers capabilities for parameter tuning (which you
    can learn about in the library’s reference), it isn’t really geared toward making
    low-level, fine-grained adjustments to a model. Using TensorFlow.js directly might
    be your best bet if you want to explore this step in more detail, since it offers
    a broader suite of tools and allows for lower-level control over the training
    process.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估步骤之后，通常会有一个迭代过程，即调整超参数并重新进行训练，以便从模型中获得最佳性能。虽然 ml5.js 提供了参数调优的功能（你可以在库的参考文档中了解更多），但它并不专门用于对模型进行低级、精细的调整。如果你希望更详细地探索这一步骤，直接使用
    TensorFlow.js 可能是最好的选择，因为它提供了更广泛的工具集，并允许对训练过程进行更低级的控制。
- en: In this case, tuning the parameters isn’t strictly necessary. The graph in the
    Visor shows a loss all the way down at 0.1, which is plenty accurate for my purposes.
    I’m happy to move on.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，调整参数并非绝对必要。Visor 中的图表显示损失值已经降到 0.1，这对于我的目的来说已经足够准确了。我准备继续进行下一步。
- en: '**Deploying the Model**'
  id: totrans-344
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**部署模型**'
- en: It’s finally time to deploy the model and see the payoff of all that hard work.
    This typically involves integrating the model into a separate application to make
    predictions or decisions based on new, previously unseen data. For this, ml5.js
    offers the convenience of a `save()` function to download the trained model to
    a file from one sketch and a `load()` function to load it for use in a completely
    different sketch. This saves you from having to retrain the model from scratch
    every single time you need it.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 现在终于到了部署模型的时刻，看看所有辛勤工作的成果。这通常涉及将模型集成到一个独立的应用程序中，根据新的、以前未见过的数据做出预测或决策。为此，ml5.js
    提供了一个方便的 `save()` 函数，可以将训练好的模型从一个草图下载到文件中，还提供了 `load()` 函数，可以将其加载到另一个完全不同的草图中使用。这样你就不必每次都从头开始重新训练模型。
- en: 'While a model would typically be deployed to a different sketch from the one
    where it was trained, I’m going to deploy the model in the same sketch for the
    sake of simplicity. In fact, once the training process is complete, the resulting
    model is, in essence, already deployed in the current sketch. It’s saved in the
    `classifier` variable and can be used to make predictions by passing the model
    new data through the `classify()` method. The shape of the data sent to `classify()`
    should match that of the input data used in training—in this case, two floating-point
    numbers, representing the x- and y-components of a direction vector:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管模型通常会部署到与训练时不同的草图中，但为了简化，我将把模型部署在同一个草图中。事实上，一旦训练过程完成，得到的模型本质上已经在当前草图中部署。它被保存在`classifier`变量中，并可以通过`classify()`方法向模型传入新数据来进行预测。传递给`classify()`的数据的形状应该与训练时使用的输入数据形状相匹配——在这个例子中，是两个浮点数，分别表示方向向量的x分量和y分量：
- en: '![Image](../images/pg577_Image_868.jpg)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/pg577_Image_868.jpg)'
- en: 'The second argument to `classify()` is another callback function for accessing
    the results:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '`classify()`的第二个参数是另一个回调函数，用于访问结果：'
- en: '[PRE9]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The model’s prediction arrives in the argument to the callback, which I’m calling
    `results` in the code. Inside, you’ll find an array of the possible labels, sorted
    by **confidence**, a probability value that the model assigns to each label. These
    probabilities represent how sure the model is of that particular prediction. They
    range from 0 to 1, with values closer to 1 indicating higher confidence and values
    near 0 suggesting lower confidence:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的预测结果会作为回调函数的参数返回，我在代码中称之为`results`。在其中，你会找到一个按**信心**排序的标签数组，信心值是模型为每个标签分配的概率值。这些概率值表示模型对特定预测的确定性。它们的范围从0到1，值越接近1表示信心越高，而接近0则表示信心较低：
- en: '[PRE10]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In this example output, the model is highly confident (approximately 96.7 percent)
    that the correct label is `"right"`, while it has minimal confidence (0.03 percent)
    in the `"left"` label. The confidence values are normalized and add up to 100
    percent.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子的输出中，模型对正确标签为“`right`”的信心非常高（约为96.7%），而对“`left`”标签的信心极低（0.03%）。信心值经过归一化，所有值加起来总和为100%。
- en: All that remains now is to fill out the sketch with code so the model can receive
    live input from the mouse. The first step is to signal the completion of the training
    process so the user knows the model is ready. I’ll include a global `status` variable
    to track the training process and ultimately display the predicted label on the
    canvas. The variable is initialized to `"training"` but updated to `"ready"` through
    the `finishedTraining()` callback.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来需要做的就是用代码填充草图，使模型能够接收来自鼠标的实时输入。第一步是向用户发出训练过程完成的信号，以便他们知道模型已经准备好了。我将包含一个全局`status`变量来跟踪训练过程，并最终在画布上显示预测标签。该变量初始化为“training”，但通过`finishedTraining()`回调函数更新为“ready”。
- en: '![Image](../images/pg579_Image_869.jpg)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/pg579_Image_869.jpg)'
- en: Finally, I’ll use p5.js’s mouse functions to build a vector while the mouse
    is being dragged and call `classifier.classify()` on that vector when the mouse
    is clicked.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我将使用p5.js的鼠标功能，在拖动鼠标时构建一个向量，并在点击鼠标时调用`classifier.classify()`对该向量进行分类。
- en: '![Image](../images/pg579_Image_870.jpg)'
  id: totrans-356
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/pg579_Image_870.jpg)'
- en: Since the `results` array is sorted by confidence, if I just want to use a single
    label as the prediction, I can access the first element of the array with `results[0].label`,
    as in the `gotResults()` function in [Example 10.2](ch10.xhtml#ch10ex2). This
    label is passed to the `status` variable to be displayed on the canvas.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`results`数组是按信心排序的，如果我只想使用单一标签作为预测结果，我可以通过`results[0].label`访问数组的第一个元素，就像在[示例
    10.2](ch10.xhtml#ch10ex2)中的`gotResults()`函数一样。这个标签会传递给`status`变量，最终显示在画布上。
- en: '![Image](../images/pencil.jpg) **Exercise 10.5**'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '![Image](../images/pencil.jpg) **练习 10.5**'
- en: 'Divide [Example 10.2](ch10.xhtml#ch10ex2) into three sketches: one for collecting
    data, one for training, and one for deployment. Use the `ml5.neuralNetwork` functions
    `save()` and `load()` for saving and loading the model to and from a file, respectively.'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 将[示例 10.2](ch10.xhtml#ch10ex2)分为三个草图：一个用于收集数据，一个用于训练，另一个用于部署。使用`ml5.neuralNetwork`函数`save()`和`load()`分别保存和加载模型到文件中。
- en: '![Image](../images/pencil.jpg) **Exercise 10.6**'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '![Image](../images/pencil.jpg) **练习 10.6**'
- en: Expand the gesture-recognition model to classify a sequence of vectors, capturing
    more accurately the path of a longer mouse movement. Remember, your input data
    must have a consistent shape, so you’ll have to decide how many vectors to use
    to represent a gesture and store no more and no less for each data point. While
    this approach can work, other machine learning models (such as recurrent neural
    networks) are specifically designed to handle sequential data and might offer
    more flexibility and potential accuracy.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展手势识别模型以分类一系列向量，更准确地捕捉较长鼠标移动的路径。请记住，输入数据必须具有一致的形状，因此你需要决定使用多少个向量来表示一个手势，并为每个数据点存储相同数量的向量，既不多也不少。虽然这种方法可行，但其他机器学习模型（如递归神经网络）专门设计用于处理序列数据，可能提供更多的灵活性和潜在的准确性。
- en: '![Image](../images/pencil.jpg) **Exercise 10.7**'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '![Image](../images/pencil.jpg) **练习 10.7**'
- en: One of the pretrained models in ml5.js is called *Handpose*. The input of the
    model is an image, and the prediction is a list of 21 key points—x- and y-positions,
    also known as *landmarks*—that describe a hand.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: ml5.js中的一个预训练模型叫做*Handpose*。该模型的输入是一张图片，预测结果是一个包含21个关键点——x和y位置，也称为*地标*——的列表，描述了手部。
- en: '![Image](../images/pg581_Image_872.jpg)'
  id: totrans-364
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/pg581_Image_872.jpg)'
- en: Can you use the outputs of the `ml5.handpose()` model as the inputs to an `ml5.neuralNetwork()`
    and classify various hand gestures (like a thumbs-up or thumbs-down)? For hints,
    you can watch my video tutorial that walks you through this process for body poses
    in the machine learning track on the Coding Train website (*[https://thecodingtrain.com/pose-classifier](https://thecodingtrain.com/pose-classifier)*).
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 你能将`ml5.handpose()`模型的输出作为`ml5.neuralNetwork()`的输入，并分类各种手势（如竖大拇指或竖小拇指）吗？如果需要提示，你可以观看我在Coding
    Train网站上的视频教程，教程会引导你完成机器学习路径中的身体姿势分类过程 (*[https://thecodingtrain.com/pose-classifier](https://thecodingtrain.com/pose-classifier)*）。
- en: '![Image](../images/bird.jpg) **The Ecosystem Project**'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '![Image](../images/bird.jpg) **生态系统项目**'
- en: Incorporate machine learning into your ecosystem to enhance the behavior of
    creatures. How could classification or regression be applied?
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 将机器学习融入你的生态系统，以增强生物的行为。如何应用分类或回归方法？
- en: Can you classify the creatures of your ecosystem into multiple categories? What
    if you use an initial population as a training dataset, and as new creatures are
    born, the system classifies them according to their features? What are the inputs
    and outputs for your system?
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你能将生态系统中的生物分类为多个类别吗？如果你使用初始种群作为训练数据集，并且随着新生物的诞生，系统根据它们的特征进行分类呢？你的系统的输入和输出是什么？
- en: Can you use a regression to predict the life span of a creature based on its
    properties? Think about how size and speed affected the life span of the bloops
    from [Chapter 9](ch09.xhtml#ch09). Could you analyze how well the regression model’s
    predictions align with the actual outcomes?
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你能使用回归预测生物的寿命吗？考虑一下大小和速度如何影响第[9章](ch09.xhtml#ch09)中的“bloop”生物的寿命。你能分析回归模型的预测结果与实际结果的对比吗？
- en: '![Image](../images/pg582_Image_873.jpg)'
  id: totrans-370
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/pg582_Image_873.jpg)'
