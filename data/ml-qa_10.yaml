- en: '**9'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**9'
- en: GENERATIVE AI MODELS**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式AI模型**
- en: '![Image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/common.jpg)'
- en: What are the popular categories of deep generative models in deep learning (also
    called *generative AI*), and what are their respective downsides?
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习中（也叫*生成AI*）流行的生成模型分类有哪些，它们各自的缺点是什么？
- en: 'Many different types of deep generative models have been applied to generating
    different types of media: images, videos, text, and audio. Beyond these types
    of media, models can also be repurposed to generate domain-specific data, such
    as organic molecules and protein structures. This chapter will first define generative
    modeling and then outline each type of generative model and discuss its strengths
    and weaknesses.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 许多不同类型的深度生成模型已经应用于生成不同类型的媒体：图像、视频、文本和音频。除了这些媒体类型，模型还可以重新用于生成特定领域的数据，例如有机分子和蛋白质结构。本章将首先定义生成建模，然后概述每种类型的生成模型，并讨论其优缺点。
- en: '**Generative vs. Discriminative Modeling**'
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**生成式与判别式建模**'
- en: 'In traditional machine learning, there are two primary approaches to modeling
    the relationship between input data (*x*) and output labels (*y*): generative
    models and discriminative models. *Generative models* aim to capture the underlying
    probability distribution of the input data *p*(*x*) or the joint distribution
    *p*(*x*, *y*) between inputs and labels. In contrast, *discriminative models*
    focus on modeling the conditional distribution *p*(*y*|*x*) of the labels given
    the inputs.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的机器学习中，有两种主要的建模方法来描述输入数据（*x*）与输出标签（*y*）之间的关系：生成模型和判别模型。*生成模型*旨在捕捉输入数据的潜在概率分布*p*（*x*）或输入与标签之间的联合分布*p*（*x*,
    *y*）。相比之下，*判别模型*则侧重于建模给定输入的标签的条件分布*p*（*y*|*x*）。
- en: A classic example that highlights the differences between these approaches is
    to compare the naive Bayes classifier and the logistic regression classifier.
    Both classifiers estimate the class label probabilities *p*(*y*|*x*) and can be
    used for classification tasks. However, logistic regression is considered a discriminative
    model because it directly models the conditional probability distribution *p*(*y*|*x*)
    of the class labels given the input features without making assumptions about
    the underlying joint distribution of inputs and labels. Naive Bayes, on the other
    hand, is considered a generative model because it models the joint probability
    distribution *p*(*x*, *y*) of the input features *x* and the output labels *y*.
    By learning the joint distribution, a generative model like naive Bayes captures
    the underlying data generation process, which enables it to generate new samples
    from the distribution if needed.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 一个经典的例子，突出展示这些方法之间差异的是比较朴素贝叶斯分类器和逻辑回归分类器。这两种分类器都估计类标签的概率*p*（*y*|*x*），并可以用于分类任务。然而，逻辑回归被视为判别模型，因为它直接建模类标签在给定输入特征下的条件概率分布*p*（*y*|*x*），而不对输入和标签的潜在联合分布做任何假设。另一方面，朴素贝叶斯被视为生成模型，因为它建模输入特征*x*和输出标签*y*的联合概率分布*p*（*x*,
    *y*）。通过学习联合分布，像朴素贝叶斯这样的生成模型捕捉到潜在的数据生成过程，这使得它能够在需要时从该分布中生成新的样本。
- en: '**Types of Deep Generative Models**'
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**深度生成模型的类型**'
- en: When we speak of *deep* generative models or deep generative AI, we often loosen
    this definition to include all types of models capable of producing realistic-looking
    data (typically text, images, videos, and sound). The remainder of this chapter
    briefly discusses the different types of deep generative models used to generate
    such data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论*深度*生成模型或深度生成AI时，我们通常会放宽这个定义，将所有能够生成逼真数据（通常是文本、图像、视频和声音）的模型都包括在内。本章的其余部分将简要讨论用于生成此类数据的不同类型的深度生成模型。
- en: '***Energy-Based Models***'
  id: totrans-10
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***基于能量的模型***'
- en: '*Energy-based models (EBMs)* are a class of generative models that learn an
    energy function, which assigns a scalar value (energy) to each data point. Lower
    energy values correspond to more likely data points. The model is trained to minimize
    the energy of real data points while increasing the energy of generated data points.
    Examples of EBMs include *deep Boltzmann machines (DBMs)*. One of the early breakthroughs
    in deep learning, DBMs provide a means to learn complex representations of data.
    You can think of them as a form of unsupervised pretraining, resulting in models
    that can then be fine-tuned for various tasks.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*基于能量的模型（EBM）*是一类生成模型，它学习一个能量函数，为每个数据点分配一个标量值（能量）。较低的能量值对应更可能的数据点。模型的训练目标是最小化真实数据点的能量，同时增加生成数据点的能量。EBM的例子包括*深度玻尔兹曼机（DBM）*。作为深度学习的早期突破之一，DBM提供了一种学习数据复杂表示的方式。你可以将它们看作是一种无监督的预训练形式，从而得到可以针对各种任务进行微调的模型。'
- en: Somewhat similar to naive Bayes and logistic regression, DBMs and multilayer
    perceptrons (MLPs) can be thought of as generative and discriminative counterparts,
    with DBMs focusing on capturing the data generation process and MLPs focusing
    on modeling the decision boundary between classes or mapping inputs to outputs.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 与朴素贝叶斯和逻辑回归有些相似，深度玻尔兹曼机（DBM）和多层感知器（MLP）可以被视为生成模型和判别模型的对应物，DBM侧重于捕捉数据生成过程，而MLP侧重于建模类之间的决策边界或将输入映射到输出。
- en: A DBM consists of multiple layers of hidden nodes, as shown in [Figure 9-1](ch09.xhtml#ch9fig1).
    As the figure illustrates, along with the hidden layers, there’s usually a visible
    layer that corresponds to the observable data. This visible layer serves as the
    input layer where the actual data or features are fed into the network. In addition
    to using a different learning algorithm than MLPs (contrastive divergence instead
    of backpropagation), DBMs consist of binary nodes (neurons) instead of continuous
    ones.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: DBM由多个隐藏节点层组成，如[图9-1](ch09.xhtml#ch9fig1)所示。正如图所示，除了隐藏层外，通常还有一个可见层对应于可观察的数据。这个可见层作为输入层，将实际数据或特征输入网络。除了使用与MLP不同的学习算法（对比散度代替反向传播），DBM还由二值节点（神经元）组成，而不是连续节点。
- en: '![Image](../images/09fig01.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/09fig01.jpg)'
- en: '*Figure 9-1: A four-layer deep Boltzmann machine with three stacks of hidden
    nodes*'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9-1：一个四层的深度玻尔兹曼机，包含三堆隐藏节点*'
- en: Suppose we are interested in generating images. A DBM can learn the joint probability
    distribution over the pixel values in a simple image dataset like MNIST. To generate
    new images, the DBM then samples from this distribution by performing a process
    called *Gibbs sampling*. Here, the visible layer of the DBM represents the input
    image. To generate a new image, the DBM starts by initializing the visible layer
    with random values or, alternatively, uses an existing image as a seed. Then,
    after completing several Gibbs sampling iterations, the final state of the visible
    layer represents the generated image.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有兴趣生成图像。DBM可以学习一个简单图像数据集（如MNIST）中像素值的联合概率分布。为了生成新图像，DBM会通过执行一种叫做*吉布斯采样*的过程从这个分布中采样。在这里，DBM的可见层表示输入图像。为了生成新图像，DBM首先通过随机值初始化可见层，或者使用现有的图像作为种子。然后，在完成几次吉布斯采样迭代后，最终的可见层状态即为生成的图像。
- en: DBMs played an important historical role as one of the first deep generative
    models, but they are no longer very popular for generating data. They are expensive
    and more complicated to train, and they have lower expressivity compared to the
    newer models described in the following sections, which generally results in lower-quality
    generated samples.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: DBM作为最早的深度生成模型之一，曾在历史上发挥了重要作用，但现在它们在生成数据方面已不再那么流行。它们训练成本高且更加复杂，并且与接下来几节描述的新模型相比，它们的表达能力较低，这通常导致生成的样本质量较差。
- en: '***Variational Autoencoders***'
  id: totrans-18
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***变分自编码器***'
- en: '*Variational autoencoders (VAEs)* are built upon the principles of variational
    inference and autoencoder architectures. *Variational inference* is a method for
    approximating complex probability distributions by optimizing a simpler, tractable
    distribution to be as close as possible to the true distribution. *Autoencoders*
    are unsupervised neural networks that learn to compress input data into a low-dimensional
    representation (encoding) and subsequently reconstruct the original data from
    the compressed representation (decoding) by minimizing the reconstruction error.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*变分自编码器（VAE）*是建立在变分推理和自编码器架构原理基础上的。*变分推理*是一种通过优化一个更简单、可处理的分布，使其尽可能接近真实分布，从而逼近复杂概率分布的方法。*自编码器*是无监督神经网络，学习将输入数据压缩成低维表示（编码），并随后通过最小化重建误差从压缩的表示中重建原始数据（解码）。'
- en: 'The VAE model consists of two main submodules: an encoder network and a decoder
    network. The encoder network takes, for example, an input image and maps it to
    a latent space by learning a probability distribution over the latent variables.
    This distribution is typically modeled as a Gaussian with parameters (mean and
    variance) that are functions of the input image. The decoder network then takes
    a sample from the learned latent distribution and reconstructs the input image
    from this sample. The goal of the VAE is to learn a compact and expressive latent
    representation that captures the essential structure of the input data while being
    able to generate new images by sampling from the latent space. (See [Chapter 1](ch01.xhtml)
    for more details on latent representations.)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: VAE模型由两个主要子模块组成：编码器网络和解码器网络。编码器网络例如接收输入图像，并通过学习潜在变量的概率分布将其映射到潜在空间。这个分布通常被建模为一个高斯分布，参数（均值和方差）是输入图像的函数。解码器网络则从学习到的潜在分布中采样，并根据该样本重建输入图像。VAE的目标是学习一个紧凑且富有表现力的潜在表示，既能捕捉输入数据的基本结构，又能通过从潜在空间采样生成新图像。（有关潜在表示的更多细节，请参见[第1章](ch01.xhtml)）
- en: '[Figure 9-2](ch09.xhtml#ch9fig2) illustrates the encoder and decoder submodules
    of an auto-encoder, where *x′* represents the reconstructed input *x*. In a standard
    variational autoencoder, the latent vector is sampled from a distribution that
    approximates a standard Gaussian distribution.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9-2](ch09.xhtml#ch9fig2)展示了自编码器的编码器和解码器子模块，其中*x′*表示重建的输入*x*。在标准变分自编码器中，潜在向量是从近似标准高斯分布的分布中采样的。'
- en: '![Image](../images/09fig02.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/09fig02.jpg)'
- en: '*Figure 9-2: An autoencoder*'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9-2：自编码器*'
- en: 'Training a VAE involves optimizing the model’s parameters to minimize a loss
    function composed of two terms: a reconstruction loss and a Kullback–Leibler-divergence
    (KL-divergence) regularization term. The reconstruction loss ensures that the
    decoded samples closely resemble the input images, while the KL-divergence term
    acts as a surrogate loss that encourages the learned latent distribution to be
    close to a predefined prior distribution (usually a standard Gaussian). To generate
    new images, we then sample points from the latent space’s prior (standard Gaussian)
    distribution and pass them through the decoder network, which generates new, diverse
    images that look similar to the training data.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 训练VAE涉及优化模型的参数，以最小化由两个项组成的损失函数：重建损失和Kullback–Leibler散度（KL散度）正则化项。重建损失确保解码后的样本与输入图像高度相似，而KL散度项作为一个代理损失，鼓励学习到的潜在分布接近预定义的先验分布（通常是标准高斯分布）。为了生成新图像，我们从潜在空间的先验（标准高斯）分布中采样点，并将其通过解码器网络，生成新的、具有多样性的图像，这些图像看起来类似于训练数据。
- en: Disadvantages of VAEs include their complicated loss function consisting of
    separate terms, as well as their often low expressiveness. The latter can result
    in blurrier images compared to other models, such as generative adversarial networks.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: VAE的缺点包括其复杂的损失函数，由不同的项组成，以及其通常较低的表达能力。后者可能导致生成的图像比其他模型（如生成对抗网络）模糊。
- en: '***Generative Adversarial Networks***'
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***生成对抗网络***'
- en: '*Generative adversarial networks (GANs)* are models consisting of interacting
    subnetworks designed to generate new data samples that are similar to a given
    set of input data. While both GANs and VAEs are latent variable models that generate
    data by sampling from a learned latent space, their architectures and learning
    mechanisms are fundamentally different.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*生成对抗网络（GANs）*是由互相作用的子网络组成的模型，旨在生成与给定输入数据集相似的新数据样本。虽然GAN和VAE都是通过从学习到的潜在空间进行采样来生成数据的潜变量模型，但它们的架构和学习机制是根本不同的。'
- en: GANs consist of two neural networks, a generator and a discriminator, that are
    trained simultaneously in an adversarial manner. The generator takes a random
    noise vector from the latent space as input and generates a synthetic data sample
    (such as an image). The discriminator’s task is to distinguish between real samples
    from the training data and fake samples generated by the generator, as illustrated
    in [Figure 9-3](ch09.xhtml#ch9fig3).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: GAN（生成对抗网络）由两个神经网络组成，一个生成器和一个判别器，它们以对抗的方式同时进行训练。生成器将来自潜在空间的随机噪声向量作为输入，生成合成数据样本（例如图像）。判别器的任务是区分来自训练数据的真实样本和生成器生成的假样本，如[图
    9-3](ch09.xhtml#ch9fig3)所示。
- en: '![Image](../images/09fig03.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/09fig03.jpg)'
- en: '*Figure 9-3: A generative adversarial network*'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9-3：生成对抗网络*'
- en: The generator in a GAN somewhat resembles the decoder of a VAE in terms of its
    functionality. During inference, both GAN generators and VAE decoders take random
    noise vectors sampled from a known distribution (for example, a standard Gaussian)
    and transform them into synthetic data samples, such as images.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: GAN中的生成器在功能上有点类似于VAE（变分自编码器）的解码器。在推理过程中，GAN生成器和VAE解码器都会从已知分布（例如标准高斯分布）中抽取随机噪声向量，并将其转化为合成数据样本，如图像。
- en: One significant disadvantage of GANs is their unstable training due to the adversarial
    nature of the loss function and learning process. Balancing the learning rates
    of the generator and discriminator can be difficult and can often result in oscillations,
    mode collapse, or non-convergence. The second main disadvantage of GANs is the
    low diversity of their generated outputs, often due to mode collapse. Here, the
    generator is able to fool the discriminator successfully with a small set of samples,
    which are representative of only a small subset of the original training data.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: GAN的一个显著缺点是由于损失函数和学习过程的对抗性，训练过程不稳定。平衡生成器和判别器的学习率可能会非常困难，往往导致振荡、模式崩塌或不收敛。GAN的第二个主要缺点是生成的输出缺乏多样性，这通常是由于模式崩塌。此时，生成器能够通过一小部分样本成功欺骗判别器，而这些样本仅代表原始训练数据中的一小部分。
- en: '***Flow-Based Models***'
  id: totrans-33
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***基于流的模型***'
- en: The core concept of *flow-based models*, also known as *normalizing flows*,
    is inspired by long-standing methods in statistics. The primary goal is to transform
    a simple probability distribution (like a Gaussian) into a more complex one using
    invertible transformations.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*基于流的模型*，也称为*归一化流*，其核心概念受长期以来统计学方法的启发。其主要目标是通过可逆变换将简单的概率分布（如高斯分布）转化为更复杂的分布。'
- en: Although the concept of normalizing flows has been a part of the statistics
    field for a long time, the implementation of early flow-based deep learning models,
    particularly for image generation, is a relatively recent development. One of
    the pioneering models in this area was the *non-linear independent components
    estimation (NICE)* approach. NICE begins with a simple probability distribution,
    often something straightforward like a normal distribution. You can think of this
    as a kind of “random noise,” or data with no particular shape or structure. NICE
    then applies a series of transformations to this simple distribution. Each transformation
    is designed to make the data look more like the final target (for instance, the
    distribution of real-world images). These transformations are “invertible,” meaning
    we can always reverse them back to the original simple distribution. After several
    successive transformations, the simple distribution has morphed into a complex
    distribution that closely matches the distribution of the target data (such as
    images). We can now generate new data that looks like the target data by picking
    random points from this complex distribution.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管归一化流的概念在统计学领域已经存在很长时间，但早期基于流的深度学习模型，特别是用于图像生成的模型，还是相对较新的发展。其中一个开创性的模型是*非线性独立成分估计（NICE）*方法。NICE
    从一个简单的概率分布开始，通常是一些简单的分布，如正态分布。你可以将其视为一种“随机噪声”或没有特定形状或结构的数据。然后，NICE 对这个简单的分布应用一系列变换。每个变换的目的是让数据看起来更像最终的目标（例如，真实世界图像的分布）。这些变换是“可逆的”，意味着我们可以始终将其反转回原始的简单分布。经过几次连续的变换，简单的分布已经转变为一个复杂的分布，这个复杂分布与目标数据（如图像）的分布非常接近。我们现在可以通过从这个复杂的分布中随机选择点来生成看起来像目标数据的新数据。
- en: '[Figure 9-4](ch09.xhtml#ch9fig4) illustrates the concept of a flow-based model,
    which maps the complex input distribution to a simpler distribution and back.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-4](ch09.xhtml#ch9fig4)展示了基于流的模型的概念，该模型将复杂的输入分布映射到更简单的分布，然后再映射回来。'
- en: '![Image](../images/09fig04.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/09fig04.jpg)'
- en: '*Figure 9-4: A flow-based model*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9-4：基于流的模型*'
- en: At first glance, the illustration is very similar to the VAE illustration in
    [Figure 9-2](ch09.xhtml#ch9fig2). However, while VAEs use neural network encoders
    like convolutional neural networks, the flow-based model uses simpler decoupling
    layers, such as simple linear transformations. Additionally, while the decoder
    in a VAE is independent of the encoder, the data-transforming functions in the
    flow-based model are mathematically inverted to obtain the outputs.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 初看起来，这个图示与[图 9-2](ch09.xhtml#ch9fig2)中的 VAE 图示非常相似。然而，虽然 VAE 使用像卷积神经网络这样的神经网络编码器，基于流的模型使用的是更简单的解耦层，如简单的线性变换。此外，虽然
    VAE 中的解码器与编码器是独立的，但基于流的模型中的数据变换函数是数学上可逆的，用来获得输出。
- en: Unlike VAEs and GANs, flow-based models provide exact likelihoods, which gives
    us insights into how well the generated samples fit the training data distribution.
    This can be useful in anomaly detection or density estimation, for example. However,
    the quality of flow-based models for generating image data is usually lower than
    GANs. Flow-based models also often require more memory and computational resources
    than GANs or VAEs since they must store and compute inverses of transformations.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 与 VAE 和 GAN 不同，基于流的模型提供精确的似然值，这使我们能够了解生成样本与训练数据分布的拟合程度。例如，这在异常检测或密度估计中可能非常有用。然而，基于流的模型生成图像数据的质量通常低于
    GAN。基于流的模型还通常需要比 GAN 或 VAE 更多的内存和计算资源，因为它们必须存储和计算变换的逆。
- en: '***Autoregressive Models***'
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***自回归模型***'
- en: '*Autoregressive models* are designed to predict the next value based on current
    (and past) values. LLMs for text generation, like ChatGPT (discussed further in
    [Chapter 17](ch17.xhtml)), are one popular example of this type of model.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*自回归模型*旨在根据当前（和过去）的值预测下一个值。用于文本生成的 LLM（如 ChatGPT，详见[第 17 章](ch17.xhtml)）是这种模型的一个流行例子。'
- en: Similar to generating one word at a time, in the context of image generation,
    autoregressive models like PixelCNN try to predict one pixel at a time, given
    the pixels they have seen so far. Such a model might predict pixels from top left
    to bottom right, in a raster scan order, or in any other defined order.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于一次生成一个单词，在图像生成的背景下，自回归模型如 PixelCNN 尝试根据已看到的像素预测一个像素，直到完成所有像素的生成。这样的模型可能会按从左上到右下的顺序（光栅扫描顺序）或任何其他定义的顺序预测像素。
- en: 'To illustrate how autoregressive models generate an image one pixel at a time,
    suppose we have an image of size *H × W* (where *H* is the height and *W* is the
    width), ignoring the color channel for simplicity’s sake. This image consists
    of *N* pixels, where *i* = 1, . . . , *N*. The probability of observing a particular
    image in the dataset is then *P*(*Image*) = *P*(*i*[1], *i*[2], . . . , *i[N]*).
    Based on the chain rule of probability in statistics, we can decompose this joint
    probability into conditional probabilities:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明自回归模型如何逐个像素生成图像，假设我们有一个大小为 *H × W* 的图像（其中 *H* 是高度，*W* 是宽度），为了简化起见，忽略颜色通道。该图像由
    *N* 个像素组成，其中 *i* = 1, . . . , *N*。在数据集中观察到特定图像的概率为 *P*(*Image*) = *P*(*i*[1],
    *i*[2], . . . , *i[N]*)。根据统计学中的链式法则，我们可以将这个联合概率分解为条件概率：
- en: '![Image](../images/f0055-01.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0055-01.jpg)'
- en: Here, *P*(*i*[1]) is the probability of the first pixel, *P*(*i*[2]|*i*[1])
    is the probability of the second pixel given the first pixel, *P*(*i*[3]|*i*[1],
    *i*[2]) is the probability of the third pixel given the first and second pixels,
    and so on.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*P*(*i*[1]) 是第一个像素的概率，*P*(*i*[2]|*i*[1]) 是给定第一个像素的情况下第二个像素的概率，*P*(*i*[3]|*i*[1],
    *i*[2]) 是给定第一个和第二个像素的情况下第三个像素的概率，以此类推。
- en: In the context of image generation, an autoregressive model essentially tries
    to predict one pixel at a time, as described earlier, given the pixels it has
    seen so far. [Figure 9-5](ch09.xhtml#ch9fig5) illustrates this process, where
    pixels *i*[1], . . . , *i*[53] represent the context and pixel *i*[54] is the
    next pixel to be generated.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像生成的背景下，自回归模型本质上是尝试逐个像素地预测，如前所述，给定它迄今为止所看到的像素。[图 9-5](ch09.xhtml#ch9fig5)
    说明了这一过程，其中像素 *i*[1], . . . , *i*[53] 代表上下文，像素 *i*[54] 是下一个要生成的像素。
- en: '![Image](../images/09fig05.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/09fig05.jpg)'
- en: '*Figure 9-5: Autoregressive pixel generation*'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9-5：自回归像素生成*'
- en: The advantage of autoregressive models is that the next-pixel (or word) prediction
    is relatively straightforward and interpretable. In addition, autoregressive models
    can compute the likelihood of data exactly, similar to flow-based models, which
    can be useful for tasks like anomaly detection. Furthermore, autoregressive models
    are easier to train than GANs as they don’t suffer from issues like mode collapse
    and other training instabilities.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 自回归模型的优势在于，下一像素（或单词）的预测相对直接且易于解释。此外，自回归模型可以精确计算数据的似然度，类似于基于流的模型，这对于异常检测等任务非常有用。此外，自回归模型比
    GANs 更容易训练，因为它们不会遇到模式崩塌和其他训练不稳定性等问题。
- en: However, autoregressive models can be slow at generating new samples. This is
    because they have to generate data one step at a time (for example, pixel by pixel
    for images), which can be computationally expensive. Autoregressive models may
    also struggle to capture long-range dependencies because each output is conditioned
    only on previously generated outputs.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，自回归模型在生成新样本时可能较慢。这是因为它们必须逐步生成数据（例如，逐个像素生成图像），这在计算上可能非常昂贵。自回归模型可能还难以捕捉长程依赖性，因为每个输出仅依赖于之前生成的输出。
- en: In terms of overall image quality, autoregressive models are therefore usually
    worse than GANs but are easier to train.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 就整体图像质量而言，自回归模型通常不如 GANs，但它们更容易训练。
- en: '***Diffusion Models***'
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***扩散模型***'
- en: As discussed in the previous section, flow-based models transform a simple distribution
    (such as a standard normal distribution) into a complex one (the target distribution)
    by applying a sequence of invertible and differentiable transformations (flows).
    Like flow-based models, *diffusion models* also apply a series of transformations.
    However, the underlying concept is fundamentally different.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一节所述，基于流的模型通过应用一系列可逆且可微分的变换（流），将简单分布（如标准正态分布）转化为复杂分布（目标分布）。与基于流的模型类似，*扩散模型*也应用一系列变换。然而，基础概念是根本不同的。
- en: Diffusion models transform the input data distribution into a simple noise distribution
    over a series of steps using stochastic differential equations. Diffusion is a
    stochastic process in which noise is progressively added to the data until it
    resembles a simpler distribution, like Gaussian noise. To generate new samples,
    the process is then reversed, starting from noise and progressively removing it.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型通过使用随机微分方程将输入数据分布转化为简单的噪声分布，这一过程是逐步进行的。扩散是一个随机过程，在这个过程中，噪声逐渐添加到数据中，直到它变得像更简单的分布，比如高斯噪声。为了生成新样本，这个过程被逆转，从噪声开始，并逐渐去除噪声。
- en: '[Figure 9-6](ch09.xhtml#ch9fig6) outlines the process of adding and removing
    Gaussian noise from an input image *x*. During inference, the reverse diffusion
    process is used to generate a new image *x*, starting with the noise tensor *z[n]*
    sampled from a Gaussian distribution.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9-6](ch09.xhtml#ch9fig6)概述了向输入图像*x*中添加和去除高斯噪声的过程。在推理过程中，反向扩散过程被用来生成新的图像*x*，从一个从高斯分布中采样的噪声张量*z[n]*开始。'
- en: '![Image](../images/09fig06.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/09fig06.jpg)'
- en: '*Figure 9-6: The diffusion process*'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9-6：扩散过程*'
- en: While both diffusion models and flow-based models are generative models aiming
    to learn complex data distributions, they approach the problem from different
    angles. Flow-based models use deterministic invertible transformations, while
    diffusion models use the aforementioned stochastic diffusion process.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管扩散模型和基于流的模型都是旨在学习复杂数据分布的生成模型，但它们从不同的角度解决这个问题。基于流的模型使用确定性的可逆变换，而扩散模型则使用上述的随机扩散过程。
- en: Recent projects have established state-of-the-art performance in generating
    high-quality images with realistic details and textures. Diffusion models are
    also easier to train than GANs. The downside of diffusion models, however, is
    that they are slower to sample from since they require running a series of sequential
    steps, similar to flow-based models and autoregressive models.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的项目已经在生成高质量的图像方面取得了最先进的表现，图像具有逼真的细节和纹理。与生成对抗网络（GANs）相比，扩散模型的训练也更为简单。然而，扩散模型的缺点是，它们从模型中采样时较慢，因为它们需要执行一系列顺序步骤，这与基于流的模型和自回归模型类似。
- en: '***Consistency Models***'
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***一致性模型***'
- en: '*Consistency models* train a neural network to map a noisy image to a clean
    one. The network is trained on a dataset of pairs of noisy and clean images and
    learns to identify patterns in the clean images that are modified by noise. Once
    the network is trained, it can be used to generate reconstructed images from noisy
    images in one step.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*一致性模型*训练一个神经网络，将有噪声的图像映射为干净图像。该网络在一个有噪声和干净图像的配对数据集上进行训练，并学习识别噪声改变后的干净图像中的模式。一旦网络训练完成，它就可以用于从有噪声的图像中在一步内生成重建图像。'
- en: Consistency model training employs an *ordinary differential equation (ODE)*
    trajectory, a path that a noisy image follows as it is gradually denoised. The
    ODE trajectory is defined by a set of differential equations that describe how
    the noise in the image changes over time, as illustrated in [Figure 9-7](ch09.xhtml#ch9fig7).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一致性模型训练采用*常微分方程（ODE）*轨迹，即一个有噪声的图像在逐步去噪过程中所遵循的路径。ODE轨迹由一组微分方程定义，这些方程描述了图像中的噪声如何随时间变化，正如在[图9-7](ch09.xhtml#ch9fig7)中所示。
- en: '![Image](../images/09fig07.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/09fig07.jpg)'
- en: '*Figure 9-7: Trajectories of a consistency model for image denoising*'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9-7：一致性模型在图像去噪中的轨迹*'
- en: As [Figure 9-7](ch09.xhtml#ch9fig7) demonstrates, we can think of consistency
    models as models that learn to map any point from a probability flow ODE, which
    smoothly converts data to noise, to the input.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 正如[图9-7](ch09.xhtml#ch9fig7)所示，我们可以将一致性模型看作是学习从概率流ODE中映射任意点的模型，该ODE将数据平滑地转化为噪声，再转化回输入。
- en: At the time of writing, consistency models are the most recent type of generative
    AI model. Based on the original paper proposing this method, consistency models
    rival diffusion models in terms of image quality. Consistency models are also
    faster than diffusion models because they do not require an iterative process
    to generate images; instead, they generate images in a single step.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 截至本文写作时，一致性模型是最新的生成性人工智能模型。根据提出此方法的原始论文，一致性模型在图像质量上可与扩散模型相媲美。一致性模型也比扩散模型更快，因为它们不需要迭代过程来生成图像；相反，它们通过单步生成图像。
- en: However, while consistency models allow for faster inference, they are still
    expensive to train because they require a large dataset of pairs of noisy and
    clean images.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管一致性模型允许更快的推理过程，但它们的训练仍然非常昂贵，因为它们需要大量的有噪声和干净图像的配对数据集。
- en: '**Recommendations**'
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**建议**'
- en: Deep Boltzmann machines are interesting from a historical perspective since
    they were one of the pioneering models to effectively demonstrate the concept
    of unsupervised learning. Flow-based and autoregressive models may be useful when
    you need to estimate exact likelihoods. However, other models are usually the
    first choice when it comes to generating high-quality images.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 深度玻尔兹曼机从历史角度来看是很有趣的，因为它们是最早有效展示无监督学习概念的先驱性模型之一。基于流的模型和自回归模型可能在需要精确估计似然时非常有用。然而，在生成高质量图像时，其他模型通常是首选。
- en: In particular, VAEs and GANs have competed for years to generate the best high-fidelity
    images. However, in 2022, diffusion models began to take over image generation
    almost entirely. Consistency models are a promising alternative to diffusion models,
    but it remains to be seen whether they become more widely adopted to generate
    state-of-the-art results. The trade-off here is that sampling from diffusion models
    is generally slower since it involves a sequence of noise-removal steps that must
    be run in order, similar to autoregressive models. This can make diffusion models
    less practical for some applications requiring fast sampling.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，VAEs和GANs多年来一直在竞争生成最佳高保真图像。然而，2022年，扩散模型几乎完全接管了图像生成。尽管一致性模型是扩散模型的有前景的替代方案，但它是否会被更广泛采用以生成最先进的结果仍有待观察。这里的权衡是，扩散模型的采样通常较慢，因为它涉及一系列必须按顺序运行的去噪步骤，类似于自回归模型。这使得扩散模型在某些需要快速采样的应用中可能不那么实用。
- en: '**Exercises**'
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**练习**'
- en: '**9-1.** How would we evaluate the quality of the images generated by a generative
    AI model?'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**9-1.** 我们如何评估生成型AI模型所生成图像的质量？'
- en: '**9-2.** Given this chapter’s description of consistency models, how would
    we use them to generate new images?'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**9-2.** 根据本章对一致性模型的描述，我们如何使用它们生成新图像？'
- en: '**References**'
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: 'The original paper proposing variational autoencoders: Diederik P. Kingma and
    Max Welling, “Auto-Encoding Variational Bayes” (2013), *[https://arxiv.org/abs/1312.6114](https://arxiv.org/abs/1312.6114)*.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提出了变分自编码器的原始论文：Diederik P. Kingma和Max Welling，"自编码变分贝叶斯"（2013），* [https://arxiv.org/abs/1312.6114](https://arxiv.org/abs/1312.6114)*。
- en: 'The paper introducing generative adversarial networks: Ian J. Good-fellow et
    al., “Generative Adversarial Networks” (2014), *[https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661)*.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入生成对抗网络的论文：Ian J. Goodfellow等人，"生成对抗网络"（2014），* [https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661)*。
- en: 'The paper introducing NICE: Laurent Dinh, David Krueger, and Yoshua Bengio,
    “NICE: Non-linear Independent Components Estimation” (2014), *[https://arxiv.org/abs/1410.8516](https://arxiv.org/abs/1410.8516)*.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入NICE的论文：Laurent Dinh、David Krueger和Yoshua Bengio，"NICE：非线性独立成分估计"（2014），*
    [https://arxiv.org/abs/1410.8516](https://arxiv.org/abs/1410.8516)*。
- en: 'The paper proposing the autoregressive PixelCNN model: Aaron van den Oord et
    al., “Conditional Image Generation with PixelCNN Decoders” (2016), *[https://arxiv.org/abs/1606.05328](https://arxiv.org/abs/1606.05328)*.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提出了自回归PixelCNN模型的论文：Aaron van den Oord等人，"使用PixelCNN解码器的条件图像生成"（2016），* [https://arxiv.org/abs/1606.05328](https://arxiv.org/abs/1606.05328)*。
- en: 'The paper introducing the popular Stable Diffusion latent diffusion model:
    Robin Rombach et al., “High-Resolution Image Synthesis with Latent Diffusion Models”
    (2021), *[https://arxiv.org/abs/2112.10752](https://arxiv.org/abs/2112.10752)*.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入流行的Stable Diffusion潜在扩散模型的论文：Robin Rombach等人，"使用潜在扩散模型进行高分辨率图像合成"（2021），*
    [https://arxiv.org/abs/2112.10752](https://arxiv.org/abs/2112.10752)*。
- en: 'The Stable Diffusion code implementation: *[https://github.com/CompVis/stable-diffusion](https://github.com/CompVis/stable-diffusion)*.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stable Diffusion的代码实现：* [https://github.com/CompVis/stable-diffusion](https://github.com/CompVis/stable-diffusion)*。
- en: 'The paper originally proposing consistency models: Yang Song et al., “Consistency
    Models” (2023), *[https://arxiv.org/abs/2303.01469](https://arxiv.org/abs/2303.01469)*.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最初提出一致性模型的论文：Yang Song等人，"一致性模型"（2023），* [https://arxiv.org/abs/2303.01469](https://arxiv.org/abs/2303.01469)*。
