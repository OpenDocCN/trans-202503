- en: '**28'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**28**'
- en: THE K IN K-FOLD CROSS-VALIDATION**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**K折交叉验证中的K值**'
- en: '![Image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/common.jpg)'
- en: '*k*-fold cross-validation is a common choice for evaluating machine learning
    classifiers because it lets us use all training data to simulate how well a machine
    learning algorithm might perform on new data. What are the advantages and disadvantages
    of choosing a large *k*?'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '*k*-折交叉验证是评估机器学习分类器的常见选择，因为它使我们能够使用所有训练数据来模拟机器学习算法在新数据上的表现。选择较大*k*的优缺点是什么？'
- en: We can think of *k*-fold cross-validation as a workaround for model evaluation
    when we have limited data. In machine learning model evaluation, we care about
    the generalization performance of our model, that is, how well it performs on
    new data. In *k*-fold cross-validation, we use the training data for model selection
    and evaluation by partitioning it into *k* validation rounds and folds. If we
    have *k* folds, we have *k* iterations, leading to *k* different models, as illustrated
    in [Figure 28-1](ch28.xhtml#ch28fig1).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将*k*-折交叉验证视为当数据有限时进行模型评估的一种解决方案。在机器学习模型评估中，我们关注模型的泛化性能，即它在新数据上的表现如何。在*k*-折交叉验证中，我们通过将训练数据划分为*k*个验证轮次和折叠，利用训练数据进行模型选择和评估。如果我们有*k*个折叠，我们就有*k*次迭代，产生*k*个不同的模型，如[图28-1](ch28.xhtml#ch28fig1)所示。
- en: '![Image](../images/28fig01.jpg)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/28fig01.jpg)'
- en: '*Figure 28-1: An example of* k *-fold cross-validation for model evaluation
    where* k *= 5*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*图28-1：一个*k*-折交叉验证用于模型评估的示例，其中*k*=5*'
- en: Using *k*-fold cross-validation, we usually evaluate the performance of a particular
    hyperparameter configuration by computing the average performance over the *k*
    models. This performance reflects or approximates the performance of a model trained
    on the complete training dataset after evaluation.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 使用*k*-折交叉验证时，我们通常通过计算*k*个模型的平均表现来评估特定超参数配置的表现。这个表现反映或近似于在评估后通过完整训练数据集训练得到的模型的表现。
- en: The following sections cover the trade-offs of selecting values for *k* in *k*-fold
    cross-validation and address the challenges of large *k* values and their computational
    demands, especially in deep learning contexts. We then discuss the core purposes
    of *k* and how to choose an appropriate value based on specific modeling needs.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分将讨论在*k*-折交叉验证中选择*k*值的权衡，并探讨较大*k*值及其计算需求，尤其是在深度学习背景下的挑战。接着我们将讨论*k*的核心用途，以及如何根据具体的建模需求选择适当的值。
- en: '**Trade-offs in Selecting Values for k**'
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**选择k值时的权衡**'
- en: 'If *k* is too large, the training sets are too similar between the different
    rounds of cross-validation. The *k* models are thus very similar to the model
    we obtain by training on the whole training set. In this case, we can still leverage
    the advantage of *k*-fold cross-validation: evaluating the performance for the
    entire training set via the held-out validation fold in each round. (Here, we
    obtain the training set by concatenating all *k* – 1 training folds in a given
    iteration.) However, a disadvantage of a large *k* is that it is more challenging
    to analyze how the machine learning algorithm with the particular choice of hyperparameter
    setting behaves on different training datasets.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*k*过大，不同交叉验证轮次之间的训练集会过于相似。因此，*k*个模型与通过在整个训练集上训练得到的模型非常相似。在这种情况下，我们仍然可以利用*k*-折交叉验证的优势：通过每轮中保留的验证折叠来评估整个训练集的表现。（这里，我们通过将每次迭代中所有*k*
    - 1个训练折叠连接在一起，得到训练集。）然而，大*k*的一个缺点是，分析具有特定超参数设置的机器学习算法在不同训练数据集上的表现变得更加困难。
- en: Besides the issue of too-similar datasets, running *k*-fold cross-validation
    with a large value of *k* is also computationally more demanding. A larger *k*
    is more expensive since it increases both the number of iterations and the training
    set size at each iteration. This is especially problematic if we work with relatively
    large models that are expensive to train, such as contemporary deep neural networks.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 除了数据集过于相似的问题，使用较大值的*k*进行k折交叉验证在计算上也更为复杂。较大的*k*会增加迭代次数和每次迭代中的训练集大小，因此计算成本更高。如果我们使用的是相对较大的模型，训练代价较高，如当代深度神经网络，这个问题尤为严重。
- en: A common choice for *k* is typically 5 or 10, for practical and historical reasons.
    A study by Ron Kohavi (see “[References](ch28.xhtml#ch00lev143)” at the end of
    this chapter) found that *k* = 10 offers a good bias and variance trade-off for
    classical machine learning algorithms, such as decision trees and naive Bayes
    classifiers, on a handful of small datasets.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*k*的常见选择通常是5或10，出于实际和历史原因。Ron Kohavi的一项研究（请参见本章末的“[参考文献](ch28.xhtml#ch00lev143)”）发现，*k*
    = 10对于经典机器学习算法（如决策树和朴素贝叶斯分类器）在一些小型数据集上的偏差和方差平衡效果良好。'
- en: For example, in 10-fold cross-validation, we use 9/10 (90 percent) of the data
    for training in each round, whereas in 5-fold cross-validation, we use only 4/5
    (80 percent) of the data, as shown in [Figure 28-2](ch28.xhtml#ch28fig2).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在10折交叉验证中，我们每轮使用9/10（90%）的数据进行训练，而在5折交叉验证中，我们每轮仅使用4/5（80%）的数据，如[图28-2](ch28.xhtml#ch28fig2)所示。
- en: '![Image](../images/28fig02.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/28fig02.jpg)'
- en: '*Figure 28-2: A comparison of 5-fold and 10-fold cross-validation*'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*图28-2：5折交叉验证与10折交叉验证的比较*'
- en: However, this does not mean large training sets are bad, since they can reduce
    the pessimistic bias of the performance estimate (mostly a good thing) if we assume
    that the model training can benefit from more training data. (See [Figure 5-1](ch05.xhtml#ch5fig1)
    on [page 24](ch05.xhtml#ch5fig1) for an example of a learning curve.)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这并不意味着大的训练集不好，因为如果我们假设模型训练可以从更多的训练数据中获益，较大的训练集可以减少性能估计的悲观偏差（这通常是件好事）。（有关学习曲线的示例，请参见[图5-1](ch05.xhtml#ch5fig1)在[第24页](ch05.xhtml#ch5fig1)的内容。）
- en: In practice, both a very small and a very large *k* may increase variance. For
    instance, a larger *k* makes the training folds more similar to each other since
    a smaller proportion is left for the held-out validation sets. Since the training
    folds are more similar, the models in each round will be more similar. In practice,
    we may observe that the variance of the held-out validation fold scores is more
    similar for larger values of *k*. On the other hand, when *k* is large, the validation
    sets are small, so they may contain more random noise or be more susceptible to
    quirks of the data, leading to more variation in the validation scores across
    the different folds. Even though the models themselves are more similar (since
    the training sets are more similar), the validation scores may be more sensitive
    to the particularities of the small validation sets, leading to higher variance
    in the overall cross-validation score.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际操作中，过小或过大的*k*可能会增加方差。例如，较大的*k*使得训练折叠之间的相似性更强，因为较小的比例数据被留作验证集。由于训练折叠之间更为相似，每轮中的模型也会更为相似。在实践中，我们可能会发现，较大的*k*值下，验证折叠得分的方差较小。另一方面，当*k*值较大时，验证集较小，因此可能包含更多的随机噪声，或更容易受到数据特征的影响，从而导致不同折叠间的验证得分差异更大。尽管模型本身更为相似（因为训练集更相似），但验证得分可能会更容易受到小型验证集的特殊性影响，从而导致整体交叉验证得分的方差更大。
- en: '**Determining Appropriate Values for k**'
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**确定适当的k值**'
- en: When deciding upon an appropriate value of *k*, we are often guided by computational
    performance and conventions. However, it’s worthwhile to define the purpose and
    context of using *k*-fold cross-validation. For example, if we care primarily
    about approximating the predictive performance of the final model, using a large
    *k* makes sense. This way, the training folds are very similar to the combined
    training dataset, yet we still get to evaluate the model on all data points via
    the validation folds.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在决定适当的*k*值时，我们通常会受到计算性能和常规的指导。然而，定义使用*k*-折交叉验证的目的和背景是值得的。例如，如果我们主要关心近似最终模型的预测性能，使用较大的*k*值是有意义的。这样，训练折叠与合并的训练数据集非常相似，同时我们仍然可以通过验证折叠在所有数据点上评估模型。
- en: On the other hand, if we care to evaluate how sensitive a given hyperparameter
    configuration and training pipeline is to different training datasets, then choosing
    a smaller number for *k* makes more sense.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果我们关心评估给定的超参数配置和训练管道对不同训练数据集的敏感性，那么选择一个较小的*k*值更为合理。
- en: Since most practical scenarios consist of two steps—tuning hyperparameters and
    evaluating the performance of a model—we can also consider a two-step procedure.
    For instance, we can use a smaller *k* during hyperparameter tuning. This will
    help speed up the hyperparameter search and probe the hyperparameter configurations
    for robustness (in addition to the average performance, we can also consider the
    variance as a selection criterion). Then, after hyperparameter tuning and selection,
    we can increase the value of *k* to evaluate the model.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 由于大多数实际场景包括两个步骤——调整超参数和评估模型性能——我们也可以考虑采用两步程序。例如，在调整超参数时，可以使用较小的*k*。这将有助于加速超参数搜索并探测超参数配置的鲁棒性（除了平均性能外，我们还可以将方差作为选择标准）。然后，在超参数调整和选择之后，我们可以增加*k*的值来评估模型。
- en: However, reusing the same dataset for model selection and evaluation introduces
    biases, and it is usually better to use a separate test set for model evaluation.
    Also, nested cross-validation may be preferred as an alternative to *k*-fold cross-validation.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，重复使用相同的数据集进行模型选择和评估会引入偏差，通常最好使用单独的测试集进行模型评估。此外，嵌套交叉验证可能是*k*-折交叉验证的替代方法。
- en: '**Exercises**'
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**练习**'
- en: '**28-1.** Suppose we want to provide a model with as much training data as
    possible. We consider using *leave-one-out cross-validation (LOOCV)*, a special
    case of *k*-fold cross-validation where *k* is equal to the number of training
    examples, such that the validation folds contain only a single data point. A colleague
    mentions that LOOCV is defective for discontinuous loss functions and performance
    measures such as classification accuracy. For instance, for a validation fold
    consisting of only one example, the accuracy is always either 0 (0 percent) or
    1 (99 percent). Is this really a problem?'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**28-1.** 假设我们希望为模型提供尽可能多的训练数据。我们考虑使用*留一交叉验证（LOOCV）*，这是*k*-折交叉验证的特例，其中*k*等于训练样本的数量，这样验证集仅包含一个数据点。一位同事提到，对于不连续的损失函数和性能度量（如分类准确度），LOOCV存在缺陷。例如，对于仅包含一个样本的验证集，准确率始终为0（0％）或1（99％）。这真的是一个问题吗？'
- en: '**28-2.** This chapter discussed model selection and model evaluation as two
    use cases of *k*-fold cross-validation. Can you think of other use cases?'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**28-2.** 本章讨论了模型选择和模型评估作为*k*-折交叉验证的两个应用场景。你能想到其他应用场景吗？'
- en: '**References**'
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: 'For a longer and more detailed explanation of why and how to use *k*-fold cross-validation,
    see my article: “Model Evaluation, Model Selection, and Algorithm Selection in
    Machine Learning” (2018), *[https://arxiv.org/abs/1811.12808](https://arxiv.org/abs/1811.12808)*.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如需更详细的解释，了解为何以及如何使用*k*-折交叉验证，请参阅我的文章：《机器学习中的模型评估、模型选择与算法选择》（2018），* [https://arxiv.org/abs/1811.12808](https://arxiv.org/abs/1811.12808)
    *。
- en: 'The paper that popularized the recommendation of choosing *k* = 5 and *k* =
    10: Ron Kohavi, “A Study of Cross-Validation and Bootstrap for Accuracy Estimation
    and Model Selection” (1995), *[https://dl.acm.org/doi/10.5555/1643031.1643047](https://dl.acm.org/doi/10.5555/1643031.1643047)*.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推荐选择*k* = 5和*k* = 10的论文：Ron Kohavi，“《交叉验证与自助法在准确性估计和模型选择中的应用研究》”（1995），* [https://dl.acm.org/doi/10.5555/1643031.1643047](https://dl.acm.org/doi/10.5555/1643031.1643047)
    *。
