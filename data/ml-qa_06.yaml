- en: '**5'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**5'
- en: REDUCING OVERFITTING WITH DATA**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 使用数据减少过拟合**
- en: '![Image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/common.jpg)'
- en: Suppose we train a neural network classifier in a supervised fashion and notice
    that it suffers from overfitting. What are some of the common ways to reduce overfitting
    in neural networks through the use of altered or additional data?
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们以监督方式训练神经网络分类器，并注意到它存在过拟合问题。通过改变或增加数据，有哪些常见的方式可以通过使用额外的数据来减少神经网络中的过拟合？
- en: '*Overfitting*, a common problem in machine learning, occurs when a model fits
    the training data too closely, learning its noise and outliers rather than the
    underlying pattern. As a result, the model performs well on the training data
    but poorly on unseen or test data. While it is ideal to prevent overfitting, it’s
    often not possible to completely eliminate it. Instead, we aim to reduce or minimize
    overfitting as much as possible.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*过拟合*，是机器学习中常见的问题，当模型过于紧密地拟合训练数据时，学习了其中的噪声和异常值，而不是其潜在模式。因此，模型在训练数据上表现良好，但在未见或测试数据上表现不佳。虽然理想情况下应该预防过拟合，但通常不可能完全消除它。相反，我们的目标是尽可能地减少或最小化过拟合。'
- en: The most successful techniques for reducing overfitting revolve around collecting
    more high-quality labeled data. However, if collecting more labeled data is not
    feasible, we can augment the existing data or leverage unlabeled data for pretraining.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 减少过拟合的最成功技术围绕着收集更多高质量标记数据展开。然而，如果收集更多标记数据不可行，我们可以增强现有数据或利用未标记数据进行预训练。
- en: '**Common Methods**'
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**常见方法**'
- en: 'This chapter summarizes the most prominent examples of dataset-related techniques
    that have stood the test of time, grouping them into the following categories:
    collecting more data, data augmentation, and pretraining.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章总结了经受住时间考验的与数据集相关的技术的最显著示例，将它们分为以下几类：收集更多数据、数据增强和预训练。
- en: '***Collecting More Data***'
  id: totrans-8
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***收集更多数据***'
- en: One of the best ways to reduce overfitting is to collect more (good-quality)
    data. We can plot learning curves to find out whether a given model would benefit
    from more data. To construct a learning curve, we train the model to different
    training set sizes (10 percent, 20 percent, and so on) and evaluate the trained
    model on the same fixed-size validation or test set. As shown in [Figure 5-1](ch05.xhtml#ch5fig1),
    the validation accuracy increases as the training set sizes increase. This indicates
    that we can improve the model’s performance by collecting more data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 减少过拟合的最佳方法之一是收集更多（高质量的）数据。我们可以绘制学习曲线来确定一个给定模型是否会从更多数据中受益。为构建学习曲线，我们对不同大小的训练集（例如10%，20%等）进行训练，并在同一固定大小的验证或测试集上评估训练好的模型。如图[5-1](ch05.xhtml#ch5fig1)所示，随着训练集大小的增加，验证准确率也会提高。这表明我们可以通过收集更多数据来改善模型的性能。
- en: '![Image](../images/05fig01.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/05fig01.jpg)'
- en: '*Figure 5-1: The learning curve plot of a model fit to different training set
    sizes*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5-1：适应不同训练集大小的模型的学习曲线图*'
- en: The gap between training and validation performance indicates the degree of
    overfitting—the more extensive the gap, the more overfitting occurs. Conversely,
    the slope indicating an improvement in the validation performance suggests the
    model is underfitting and can benefit from more data. Typically, additional data
    can decrease both underfitting and overfitting.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和验证性能之间的差距指示了过拟合的程度——差距越大，过拟合越严重。相反，验证性能提升的斜率表明模型欠拟合，并且可以从更多数据中受益。通常情况下，额外的数据可以减少欠拟合和过拟合。
- en: '***Data Augmentation***'
  id: totrans-13
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***数据增强***'
- en: Data augmentation refers to generating new data records or features based on
    existing data. It allows for the expansion of a dataset without additional data
    collection.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强指的是基于现有数据生成新的数据记录或特征。它允许扩展数据集而无需额外收集数据。
- en: Data augmentation allows us to create different versions of the original input
    data, which can improve the model’s generalization performance. Why? Augmented
    data can help the model improve its ability to generalize, since it makes it harder
    to memorize spurious information via training examples or features—or, in the
    case of image data, exact pixel values for specific pixel locations. [Figure 5-2](ch05.xhtml#ch5fig2)
    highlights common image data augmentation techniques, including increasing brightness,
    flipping, and cropping.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强使我们能够创建原始输入数据的不同版本，从而提高模型的泛化性能。为什么？增强数据有助于模型提高其泛化能力，因为它使得通过训练样本或特征记住虚假信息变得更加困难——或者，在图像数据的情况下，针对特定像素位置的精确像素值。[图5-2](ch05.xhtml#ch5fig2)突出展示了常见的图像数据增强技术，包括增加亮度、翻转和裁剪。
- en: '![Image](../images/05fig02.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/05fig02.jpg)'
- en: '*Figure 5-2: A selection of different image data augmentation techniques*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5-2：不同图像数据增强技术的选择*'
- en: Data augmentation is usually standard for image data (see [Figure 5-2](ch05.xhtml#ch5fig2))
    and text data (discussed further in [Chapter 15](ch15.xhtml)), but data augmentation
    methods for tabular data also exist.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强通常是图像数据（参见[图5-2](ch05.xhtml#ch5fig2)）和文本数据（在[第15章](ch15.xhtml)中进一步讨论）中的标准做法，但表格数据的增强方法也存在。
- en: Instead of collecting more data or augmenting existing data, it is also possible
    to generate new, synthetic data. While more common for image data and text, generating
    synthetic data is also possible for tabular datasets.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 除了收集更多数据或增强现有数据，还可以生成新的合成数据。虽然图像数据和文本数据中更常见生成合成数据，但对于表格数据也可以生成合成数据。
- en: '***Pretraining***'
  id: totrans-20
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***预训练***'
- en: As discussed in [Chapter 2](ch02.xhtml), self-supervised learning lets us leverage
    large, unlabeled datasets to pretrain neural networks. This can also help reduce
    over-fitting on the smaller target datasets.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第2章](ch02.xhtml)所述，自监督学习让我们能够利用大型未标注数据集对神经网络进行预训练。这也有助于减少在较小目标数据集上的过拟合。
- en: As an alternative to self-supervised learning, traditional transfer learning
    on large labeled datasets is also an option. Transfer learning is most effective
    if the labeled dataset is closely related to the target domain. For instance,
    if we train a model to classify bird species, we can pretrain a network on a large,
    general animal classification dataset. However, if such a large animal classification
    dataset is unavailable, we can also pretrain the model on the relatively broad
    ImageNet dataset.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 作为自监督学习的替代方案，传统的迁移学习在大型标注数据集上也是一个选择。迁移学习在标注数据集与目标领域密切相关时最为有效。例如，如果我们训练一个模型来分类鸟类物种，我们可以在一个大型的通用动物分类数据集上进行预训练。然而，如果这样的动物分类数据集不可用，我们也可以在相对广泛的ImageNet数据集上预训练模型。
- en: A dataset may be extremely small and unsuitable for supervised learning—for
    example, if it contains only a handful of labeled examples per class. If our classifier
    needs to operate in a context where the collection of additional labeled data
    is not feasible, we may also consider few-shot learning.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 某个数据集可能非常小，不适合监督学习——例如，如果它每个类别只有少量标注样本。如果我们的分类器需要在一个无法收集更多标注数据的情境下工作，我们也可以考虑少样本学习。
- en: '**Other Methods**'
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**其他方法**'
- en: 'The previous sections covered the main approaches to using and modifying datasets
    to reduce overfitting. However, this is not an exhaustive list. Other common techniques
    include:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的部分介绍了使用和修改数据集以减少过拟合的主要方法。然而，这并不是一个详尽无遗的列表。其他常见的技术包括：
- en: Feature engineering and normalization
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征工程和归一化
- en: The inclusion of adversarial examples and label or feature noise
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对抗样本、标签或特征噪声的包含
- en: Label smoothing
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签平滑
- en: Smaller batch sizes
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更小的批量大小
- en: Data augmentation techniques such as Mixup, Cutout, and CutMix
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据增强技术，如Mixup、Cutout和CutMix
- en: The next chapter covers additional techniques to reduce overfitting from a model
    perspective, and it concludes by discussing which regularization techniques we
    should consider in practice.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将介绍从模型角度减少过拟合的额外技术，并以讨论我们在实践中应考虑的正则化技术作为结尾。
- en: '**Exercises**'
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**练习**'
- en: '**5-1.** Suppose we train an XGBoost model to classify images based on manually
    extracted features obtained from collaborators. The dataset of labeled training
    examples is relatively small, but fortunately, our collaborators also have a labeled
    training set from an older project on a related domain. We’re considering implementing
    a transfer learning approach to train the XGBoost model. Is this a feasible option?
    If so, how could we do it? (Assume we are allowed to use only XGBoost and not
    another classification algorithm or model.)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**5-1.** 假设我们训练一个 XGBoost 模型，基于从合作者那里手动提取的特征对图像进行分类。标注训练示例的数据集相对较小，但幸运的是，我们的合作者还拥有来自相关领域旧项目的标注训练集。我们正在考虑实施迁移学习方法来训练
    XGBoost 模型。这是一个可行的选择吗？如果是的话，我们该如何操作？（假设我们只能使用 XGBoost，而不能使用其他分类算法或模型。）'
- en: '**5-2.** Suppose we’re working on the image classification problem of implementing
    MNIST-based handwritten digit recognition. We’ve added a decent amount of data
    augmentation to try to reduce overfitting. Unfortunately, we find that the classification
    accuracy is much worse than it was before the augmentation. What are some potential
    reasons for this?'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**5-2.** 假设我们正在处理基于 MNIST 的手写数字识别图像分类问题。我们添加了相当多的数据增强以减少过拟合。不幸的是，我们发现分类精度比增强前差得多。这可能有哪些原因？'
- en: '**References**'
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: 'A paper on data augmentation for tabular data: Derek Snow, “DeltaPy: A Framework
    for Tabular Data Augmentation in Python” (2020), *[https://github.com/firmai/deltapy](https://github.com/firmai/deltapy)*.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '关于表格数据增强的论文：Derek Snow，"DeltaPy: A Framework for Tabular Data Augmentation
    in Python"（2020），* [https://github.com/firmai/deltapy](https://github.com/firmai/deltapy)*。'
- en: 'The paper proposing the GReaT method for generating synthetic tabular data
    using an auto-regressive generative large language model: Vadim Borisov et al.,
    “Language Models Are Realistic Tabular Data Generators” (2022), *[https://arxiv.org/abs/2210.06280](https://arxiv.org/abs/2210.06280)*.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提出 GReaT 方法的论文，该方法使用自回归生成的大型语言模型生成合成表格数据：Vadim Borisov 等人，"Language Models Are
    Realistic Tabular Data Generators"（2022），* [https://arxiv.org/abs/2210.06280](https://arxiv.org/abs/2210.06280)*。
- en: 'The paper proposing the TabDDPM method for generating synthetic tabular data
    using a diffusion model: Akim Kotelnikov et al., “TabDDPM: Modelling Tabular Data
    with Diffusion Models” (2022), *[https://arxiv.org/abs/2209.15421](https://arxiv.org/abs/2209.15421)*.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '提出 TabDDPM 方法的论文，该方法使用扩散模型生成合成表格数据：Akim Kotelnikov 等人，"TabDDPM: Modelling Tabular
    Data with Diffusion Models"（2022），* [https://arxiv.org/abs/2209.15421](https://arxiv.org/abs/2209.15421)*。'
- en: 'Scikit-learn’s user guide offers a section on preprocessing data, featuring
    techniques like feature scaling and normalization that can enhance your model’s
    performance: *[https://scikit-learn.org/stable/modules/preprocessing.html](https://scikit-learn.org/stable/modules/preprocessing.html)*.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scikit-learn 的用户指南提供了一个关于数据预处理的章节，包含了像特征缩放和归一化这样的技术，可以增强模型的表现：* [https://scikit-learn.org/stable/modules/preprocessing.html](https://scikit-learn.org/stable/modules/preprocessing.html)*。
- en: 'A survey on methods for robustly training deep models with noisy labels that
    explores techniques to mitigate the impact of incorrect or misleading target values:
    Bo Han et al., “A Survey of Label-noise Representation Learning: Past, Present
    and Future” (2020), *[https://arxiv.org/abs/2011.04406](https://arxiv.org/abs/2011.04406)*.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '关于如何使用带噪标签稳健训练深度模型的调查，探讨了缓解错误或误导性目标值影响的技术：Bo Han 等人，"A Survey of Label-noise
    Representation Learning: Past, Present and Future"（2020），* [https://arxiv.org/abs/2011.04406](https://arxiv.org/abs/2011.04406)*。'
- en: 'Theoretical and empirical evidence to support the idea that controlling the
    ratio of batch size to learning rate in stochastic gradient descent is crucial
    for good modeling performance in deep neural networks: Fengxiang He, Tongliang
    Liu, and Dacheng Tao, “Control Batch Size and Learning Rate to Generalize Well:
    Theoretical and Empirical Evidence” (2019), *[https://dl.acm.org/doi/abs/10.5555/3454287.3454390](https://dl.acm.org/doi/abs/10.5555/3454287.3454390)*.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '理论和实证证据支持控制批量大小与学习率的比率在随机梯度下降中对深度神经网络建模性能的重要性：Fengxiang He、Tongliang Liu 和
    Dacheng Tao，"Control Batch Size and Learning Rate to Generalize Well: Theoretical
    and Empirical Evidence"（2019），* [https://dl.acm.org/doi/abs/10.5555/3454287.3454390](https://dl.acm.org/doi/abs/10.5555/3454287.3454390)*。'
- en: 'Inclusion of adversarial examples, which are input samples designed to mislead
    the model, can improve prediction performance by making the model more robust:
    Cihang Xie et al., “Adversarial Examples Improve Image Recognition” (2019), *[https://arxiv.org/abs/1911.09665](https://arxiv.org/abs/1911.09665)*.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含对抗样本，这些输入样本旨在误导模型，通过使模型更加稳健来提高预测性能：Cihang Xie 等人，《对抗样本提高图像识别》（2019），* [https://arxiv.org/abs/1911.09665](https://arxiv.org/abs/1911.09665)*。
- en: 'Label smoothing is a regularization technique that mitigates the impact of
    potentially incorrect labels in the dataset by replacing hard 0 and 1 classification
    targets with softened values: Rafael Müller, Simon Kornblith, and Geoffrey Hinton,
    “When Does Label Smoothing Help?” (2019), *[https://arxiv.org/abs/1906.02629](https://arxiv.org/abs/1906.02629)*.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签平滑是一种正则化技术，通过用软化的值替代硬性的 0 和 1 分类目标，从而减轻数据集中可能存在的错误标签的影响：Rafael Müller、Simon
    Kornblith 和 Geoffrey Hinton，《标签平滑何时有帮助？》（2019），* [https://arxiv.org/abs/1906.02629](https://arxiv.org/abs/1906.02629)*。
- en: 'Mixup, a popular method that trains neural networks on blended data pairs to
    improve generalization and robustness: Hongyi Zhang et al., “Mixup: Beyond Empirical
    Risk Minimization” (2018), *[https://arxiv.org/abs/1710.09412](https://arxiv.org/abs/1710.09412)*.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mixup 是一种流行的方法，通过对混合数据对进行训练，提升神经网络的泛化能力和稳健性：Hongyi Zhang 等人，《Mixup: 超越经验风险最小化》（2018），*
    [https://arxiv.org/abs/1710.09412](https://arxiv.org/abs/1710.09412)*。'
