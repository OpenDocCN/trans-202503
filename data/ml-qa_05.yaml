- en: '**4'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**4'
- en: THE LOTTERY TICKET HYPOTHESIS**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 彩票假设**
- en: '![Image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/common.jpg)'
- en: What is the lottery ticket hypothesis, and, if it holds true, how is it useful
    in practice?
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 彩票假设是什么？如果它成立，在实践中有什么用处？
- en: The lottery ticket hypothesis is a concept in neural network training that posits
    that within a randomly initialized neural network, there exists a subnetwork (or
    “winning ticket”) that can, when trained separately, achieve the same accuracy
    on a test set as the full network after being trained for the same number of steps.
    This idea was first proposed by Jonathan Frankle and Michael Carbin in 2018.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 彩票假设是神经网络训练中的一个概念，它认为在一个随机初始化的神经网络中，存在一个子网络（或称“中奖票”），当单独训练时，可以在测试集上达到与完整网络相同的准确度，而训练的步数与完整网络相同。这个概念最初由Jonathan
    Frankle和Michael Carbin在2018年提出。
- en: This chapter illustrates the lottery hypothesis step by step, then goes over
    *weight pruning*, one of the key techniques to create smaller subnetworks as part
    of the lottery hypothesis methodology. Lastly, it discusses the practical implications
    and limitations of the hypothesis.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章逐步阐述了彩票假设，然后介绍了*权重修剪*，这是根据彩票假设方法论创建更小子网络的关键技术之一。最后，讨论了该假设的实际意义和局限性。
- en: '**The Lottery Ticket Training Procedure**'
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**彩票假设训练过程**'
- en: '[Figure 4-1](ch04.xhtml#ch4fig1) illustrates the training procedure for the
    lottery ticket hypothesis in four steps, which we’ll discuss one by one to help
    clarify the concept.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-1](ch04.xhtml#ch4fig1)展示了彩票假设的训练过程，共有四个步骤，我们将逐一讨论这些步骤，以帮助澄清这一概念。'
- en: '![Image](../images/04fig01.jpg)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/04fig01.jpg)'
- en: '*Figure 4-1: The lottery hypothesis training procedure*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-1：彩票假设训练过程*'
- en: In [Figure 4-1](ch04.xhtml#ch4fig1), we start with a large neural network ➊
    that we train until convergence ➋, meaning we put in our best efforts to make
    it perform as well as possible on a target dataset (for example, minimizing training
    loss and maximizing classification accuracy). This large neural network is initialized
    as usual using small random weights.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 4-1](ch04.xhtml#ch4fig1)中，我们从一个大型神经网络 ➊ 开始，训练直到收敛 ➋，这意味着我们尽最大努力让它在目标数据集上表现得尽可能好（例如，最小化训练损失和最大化分类准确性）。这个大型神经网络像往常一样使用小的随机权重进行初始化。
- en: Next, as shown in [Figure 4-1](ch04.xhtml#ch4fig1), we prune the neural network’s
    weight parameters ➌, removing them from the network. We can do this by setting
    the weights to zero to create sparse weight matrices. Here, we can either prune
    individual weights, known as *unstructured* pruning, or prune larger “chunks”
    from the network, such as entire convolutional filter channels. This is known
    as *structured* pruning.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，如[图 4-1](ch04.xhtml#ch4fig1)所示，我们修剪神经网络的权重参数➌，将它们从网络中移除。我们可以通过将权重设置为零来创建稀疏的权重矩阵。这里，我们可以选择修剪单个权重，这称为*非结构化*修剪，或者修剪更大“块”的部分，例如整个卷积滤波器通道。这称为*结构化*修剪。
- en: The original lottery hypothesis approach follows a concept known as *iterative
    magnitude pruning*, where the weights with the lowest magnitudes are removed in
    an iterative fashion. (We will revisit this concept in [Chapter 6](ch06.xhtml)
    when discussing techniques to reduce overfitting.)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的彩票假设方法遵循一个称为*迭代幅度修剪*的概念，其中幅度最小的权重会以迭代的方式被移除。（我们将在[第六章](ch06.xhtml)中讨论减少过拟合的技术时重新回顾这个概念。）
- en: After the pruning step, we reset the weights to the original small random values
    used in step 1 in [Figure 4-1](ch04.xhtml#ch4fig1) and train the pruned network
    ➍. It’s worth emphasizing that we do not reinitialize the pruned network with
    any small random weights (as is typical for iterative magnitude pruning), and
    instead we reuse the weights from step 1.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在修剪步骤之后，我们将权重重置为[图 4-1](ch04.xhtml#ch4fig1)中第1步使用的原始小随机值，并训练修剪后的网络 ➍。值得强调的是，我们不会使用任何小的随机权重重新初始化修剪后的网络（这是迭代幅度修剪中常见的做法），而是重新使用第1步中的权重。
- en: We then repeat the pruning steps 2 through 4 until we reach the desired network
    size. For example, in the original lottery ticket hypothesis paper, the authors
    successfully reduced the network to 10 percent of its original size without sacrificing
    classification accuracy. As a nice bonus, the pruned (sparse) network, referred
    to as the *winning ticket*, even demonstrated improved generalization performance
    compared to the original (large and dense) network.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们重复剪枝步骤2至4，直到达到所需的网络大小。例如，在原始彩票票假设论文中，作者成功地将网络大小减少到原始大小的10%，而不牺牲分类精度。作为一个额外的好处，修剪后的（稀疏的）网络，即所谓的*中奖票*，甚至展示了比原始（大而密集）网络更好的泛化性能。
- en: '**Practical Implications and Limitations**'
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**实践意义与局限性**'
- en: If it’s possible to identify smaller subnetworks that have the same predictive
    performance as their up-to-10-times-larger counterparts, this can have significant
    implications for both neural training and inference. Given the ever-growing size
    of modern neural network architectures, this can help cut training costs and infrastructure.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果能够识别出那些预测性能与其最大可达十倍大网络相当的小型子网络，这将对神经网络训练和推理产生重大影响。鉴于现代神经网络架构的不断增大，这有助于降低训练成本和基础设施。
- en: Sound too good to be true? Maybe. If winning tickets can be identified efficiently,
    this would be very useful in practice. However, at the time of writing, there
    is no way to find the winning tickets without training the original network. Including
    the pruning steps would make this even more expensive than a regular training
    procedure. Moreover, after the publication of the original paper, researchers
    found that the original weight initialization may not work to find winning tickets
    for larger-scale networks, and additional experimentation with the initial weights
    of the pruned networks is required.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 听起来像是太好了不真实？也许吧。如果能高效识别中奖票，那在实践中将非常有用。然而，在写作时，尚无方法能在不训练原始网络的情况下找到中奖票。包括剪枝步骤会使得这个过程比常规训练程序更昂贵。此外，在原始论文发布后，研究人员发现原始权重初始化可能无法在更大规模的网络中找到中奖票，需要进一步对修剪后的网络初始权重进行实验。
- en: The good news is that winning tickets do exist. Even if it’s currently not possible
    to identify them without training their larger neural network counterparts, they
    can be used for more efficient inference after training.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，中奖票确实存在。即使目前无法在不训练它们更大神经网络对等体的情况下识别它们，但在训练后，它们可以用于更高效的推理。
- en: '**Exercises**'
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**练习**'
- en: '**4-1.** Suppose we’re trying out the lottery ticket hypothesis approach and
    find that the performance of the subnetwork is not very good (compared to the
    original network). What next steps might we try?'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**4-1.** 假设我们在尝试彩票票假设方法时发现子网络的性能不太好（与原始网络相比）。我们可以尝试哪些下一步措施？'
- en: '**4-2.** The simplicity and efficiency of the rectified linear unit (ReLU)
    activation function have made it one of the most popular activation functions
    in neural network training, particularly in deep learning, where it helps to mitigate
    problems like the vanishing gradient. The ReLU activation function is defined
    by the mathematical expression max(0, *x*). This means that if the input *x* is
    positive, the function returns *x*, but if the input is negative or 0, the function
    returns 0\. How is the lottery ticket hypothesis related to training a neural
    network with ReLU activation functions?'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**4-2.** 修正线性单元（ReLU）激活函数的简洁性和高效性使其成为神经网络训练中最流行的激活函数之一，特别是在深度学习中，它有助于缓解如梯度消失等问题。ReLU激活函数通过数学表达式max(0,
    *x*)定义。这意味着，如果输入的*x*为正，函数返回*x*；但如果输入为负数或0，函数返回0。彩票票假设如何与使用ReLU激活函数训练神经网络相关联？'
- en: '**References**'
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: 'The paper proposing the lottery ticket hypothesis: Jonathan Fran-kle and Michael
    Carbin, “The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks”
    (2018), *[https://arxiv.org/abs/1803.03635](https://arxiv.org/abs/1803.03635)*.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '提出彩票票假设的论文：Jonathan Fran-kle 和 Michael Carbin，"The Lottery Ticket Hypothesis:
    Finding Sparse, Trainable Neural Networks"（2018），* [https://arxiv.org/abs/1803.03635](https://arxiv.org/abs/1803.03635)*。'
- en: 'The paper proposing structured pruning for removing larger parts, such as entire
    convolutional filters, from a network: Hao Li et al., “Pruning Filters for Efficient
    ConvNets” (2016), *[https://arxiv.org/abs/1608.08710](https://arxiv.org/abs/1608.08710)*.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提出结构化剪枝以移除网络中较大部分（例如整个卷积滤波器）的论文：Hao Li 等，"Pruning Filters for Efficient ConvNets"（2016），*
    [https://arxiv.org/abs/1608.08710](https://arxiv.org/abs/1608.08710)*。
- en: 'Follow-up work on the lottery hypothesis, showing that the original weight
    initialization may not work to find winning tickets for larger-scale networks,
    and additional experimentation with the initial weights of the pruned networks
    is required: Jonathan Frankle et al., “Linear Mode Connectivity and the Lottery
    Ticket Hypothesis” (2019), *[https://arxiv.org/abs/1912.05671](https://arxiv.org/abs/1912.05671)*.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于彩票假设的后续工作，表明原始的权重初始化方法可能无法找到更大规模网络的“中奖票据”，需要对修剪后网络的初始权重进行额外实验：Jonathan Frankle
    等人，"线性模式连接性与彩票票据假设"（2019），*[https://arxiv.org/abs/1912.05671](https://arxiv.org/abs/1912.05671)*。
- en: 'An improved lottery ticket hypothesis algorithm that finds smaller networks
    that match the performance of a larger network exactly: Vivek Ramanujan et al.,
    “What’s Hidden in a Randomly Weighted Neural Network?” (2020), *[https://arxiv.org/abs/1911.13299](https://arxiv.org/abs/1911.13299)*.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种改进的“彩票票据假设”算法，通过找到较小的网络，精确匹配更大网络的表现：Vivek Ramanujan 等人，"随机加权神经网络中隐藏的是什么？"（2020），*[https://arxiv.org/abs/1911.13299](https://arxiv.org/abs/1911.13299)*。
