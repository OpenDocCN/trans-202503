- en: '**1'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**1'
- en: EMBEDDINGS, LATENT SPACE, AND REPRESENTATIONS**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**嵌入、潜在空间和表示**'
- en: '![Image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/common.jpg)'
- en: In deep learning, we often use the terms *embedding vectors*, *representations*,
    and *latent space*. What do these concepts have in common, and how do they differ?
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，我们常用术语*嵌入向量*、*表示*和*潜在空间*。这些概念有什么共同之处，它们有何不同？
- en: 'While these three terms are often used interchangeably, we can make subtle
    distinctions between them:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这三个术语常常互换使用，但我们可以对它们进行细微的区分：
- en: Embedding vectors are representations of input data where similar items are
    close to each other.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入向量是输入数据的表示，其中相似的项目彼此靠近。
- en: Latent vectors are intermediate representations of input data.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 潜在向量是输入数据的中间表示。
- en: Representations are encoded versions of the original input.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表示是原始输入的编码版本。
- en: The following sections explore the relationship between embeddings, latent vectors,
    and representations and how each functions to encode information in machine learning
    contexts.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分探讨嵌入、潜在向量和表示之间的关系，以及它们在机器学习中的功能和作用。
- en: '**Embeddings**'
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**嵌入**'
- en: Embedding vectors, or *embeddings* for short, encode relatively high-dimensional
    data into relatively low-dimensional vectors.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入向量，简称*嵌入*，将相对高维的数据编码为相对低维的向量。
- en: We can apply embedding methods to create a continuous dense (non-sparse) vector
    from a (sparse) one-hot encoding. *One-hot encoding* is a method used to represent
    categorical data as binary vectors, where each category is mapped to a vector
    containing 1 in the position corresponding to the category’s index, and 0 in all
    other positions. This ensures that the categorical values are represented in a
    way that certain machine learning algorithms can process. For example, if we have
    a categorical variable Color with three categories, Red, Green, and Blue, the
    one-hot encoding would represent Red as [1, 0, 0], Green as [0, 1, 0], and Blue
    as [0, 0, 1]. These one-hot encoded categorical variables can then be mapped into
    continuous embedding vectors by utilizing the learned weight matrix of an embedding
    layer or module.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以应用嵌入方法，从（稀疏的）独热编码创建一个连续的稠密（非稀疏）向量。*独热编码*是一种将分类数据表示为二进制向量的方法，每个类别被映射为一个向量，其中对应类别索引的位置为1，其他位置为0。这确保了分类值以某些机器学习算法可以处理的方式表示。例如，如果我们有一个名为Color的分类变量，包含三种类别：红色、绿色和蓝色，独热编码将红色表示为[1,
    0, 0]，绿色表示为[0, 1, 0]，蓝色表示为[0, 0, 1]。这些独热编码的分类变量随后可以通过利用嵌入层或模块的学习权重矩阵映射到连续的嵌入向量。
- en: We can also use embedding methods for dense data such as images. For example,
    the last layers of a convolutional neural network may yield embedding vectors,
    as illustrated in [Figure 1-1](ch01.xhtml#ch1fig1).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用嵌入方法处理稠密数据，如图像。例如，卷积神经网络的最后几层可能会产生嵌入向量，如[图 1-1](ch01.xhtml#ch1fig1)所示。
- en: '![Image](../images/01fig01.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/01fig01.jpg)'
- en: '*Figure 1-1: An input embedding (left) and an embedding from a neural network
    (right)*'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 1-1：一个输入嵌入（左）和来自神经网络的嵌入（右）*'
- en: To be technically correct, all intermediate layer outputs of a neural network
    could yield embedding vectors. Depending on the training objective, the output
    layer may also produce useful embedding vectors. For the sake of simplicity, the
    convolutional neural network in [Figure 1-1](ch01.xhtml#ch1fig1) associates the
    second-to-last layer with embeddings.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，神经网络的所有中间层输出都可能产生嵌入向量。根据训练目标，输出层也可能产生有用的嵌入向量。为了简化，[图 1-1](ch01.xhtml#ch1fig1)中的卷积神经网络将倒数第二层与嵌入关联。
- en: Embeddings can have higher or lower numbers of dimensions than the original
    input. For instance, using embeddings methods for extreme expression, we can encode
    data into two-dimensional dense and continuous representations for visualization
    purposes and clustering analysis, as illustrated in [Figure 1-2](ch01.xhtml#ch1fig2).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入可以比原始输入具有更高或更低的维度。例如，使用极端表达的嵌入方法，我们可以将数据编码为二维稠密且连续的表示形式，用于可视化和聚类分析，如[图 1-2](ch01.xhtml#ch1fig2)所示。
- en: '![Image](../images/01fig02.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/01fig02.jpg)'
- en: '*Figure 1-2: Mapping words (left) and images (right) to a two-dimensional feature
    space*'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 1-2：将单词（左）和图像（右）映射到二维特征空间*'
- en: A fundamental property of embeddings is that they encode *distance* or *similarity*.
    This means that embeddings capture the semantics of the data such that similar
    inputs are close in the embeddings space.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入的一个基本属性是它们编码*距离*或*相似性*。这意味着嵌入捕捉了数据的语义，使得相似的输入在嵌入空间中接近。
- en: For readers interested in a more formal explanation using mathematical terminology,
    an embedding is an injective and structure-preserving map between an input space
    *X* and the embedding space *Y*. This implies that similar inputs will be located
    at points in close proximity within the embedding space, which can be seen as
    the “structure-preserving” characteristic of the embedding.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些对使用数学术语进行更正式解释的读者，嵌入是输入空间*X*和嵌入空间*Y*之间的单射且结构保留的映射。这意味着相似的输入将在嵌入空间中靠近的位置，这可以看作是嵌入的“结构保留”特征。
- en: '**Latent Space**'
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**潜在空间**'
- en: '*Latent space* is typically used synonymously with *embedding space*, the space
    into which embedding vectors are mapped.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*潜在空间*通常与*嵌入空间*同义，即嵌入向量被映射到的空间。'
- en: Similar items can appear close in the latent space; however, this is not a strict
    requirement. More loosely, we can think of the latent space as any feature space
    that contains features, often compressed versions of the original input features.
    These latent space features can be learned by a neural network, such as an autoencoder
    that reconstructs input images, as shown in [Figure 1-3](ch01.xhtml#ch1fig3).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 相似的项目可以在潜在空间中靠近出现；然而，这不是严格要求的。更宽泛地说，我们可以将潜在空间看作是包含特征的任何特征空间，通常是原始输入特征的压缩版本。这些潜在空间特征可以通过神经网络学习，例如重建输入图像的自编码器，如[图1-3](ch01.xhtml#ch1fig3)所示。
- en: '![Image](../images/01fig03.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/01fig03.jpg)'
- en: '*Figure 1-3: An autoencoder reconstructing the input image*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*图1-3：自编码器重建输入图像*'
- en: The bottleneck in [Figure 1-3](ch01.xhtml#ch1fig3) represents a small, intermediate
    neural network layer that encodes or maps the input image into a lower-dimensional
    representation. We can think of the target space of this mapping as a latent space.
    The training objective of the autoencoder is to reconstruct the input image, that
    is, to minimize the distance between the input and output images. In order to
    optimize the training objective, the autoencoder may learn to place the encoded
    features of similar inputs (for example, pictures of cats) close to each other
    in the latent space, thus creating useful embedding vectors where similar inputs
    are close in the embedding (latent) space.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[图1-3](ch01.xhtml#ch1fig3)中的瓶颈表示一个小型的中间神经网络层，该层将输入图像编码或映射为低维表示。我们可以将这种映射的目标空间看作是潜在空间。自编码器的训练目标是重建输入图像，也就是说，最小化输入图像和输出图像之间的距离。为了优化训练目标，自编码器可能会学习将相似输入（例如，猫的图片）在潜在空间中放置得很近，从而创建有用的嵌入向量，使得相似的输入在嵌入（潜在）空间中接近。'
- en: '**Representation**'
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**表示**'
- en: A *representation* is an encoded, typically intermediate form of an input. For
    instance, an embedding vector or vector in the latent space is a representation
    of the input, as previously discussed. However, representations can also be produced
    by simpler procedures. For example, one-hot encoded vectors are considered representations
    of an input.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*表示*是输入的编码，通常是中间形式。例如，嵌入向量或潜在空间中的向量就是输入的表示，如前所述。然而，表示也可以通过更简单的过程生成。例如，独热编码向量被视为输入的表示。'
- en: The key idea is that the representation captures some essential features or
    characteristics of the original data to make it useful for further analysis or
    processing.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 关键思想是表示捕捉了原始数据的一些基本特征或特性，使其对进一步分析或处理有用。
- en: '**Exercises**'
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**练习**'
- en: '**1-1.** Suppose we’re training a convolutional network with five convolutional
    layers followed by three fully connected (FC) layers, similar to AlexNet (*[https://en.wikipedia.org/wiki/AlexNet](https://en.wikipedia.org/wiki/AlexNet)*),
    as illustrated in [Figure 1-4](ch01.xhtml#ch1fig4).'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**1-1.** 假设我们正在训练一个卷积网络，具有五个卷积层，后接三个全连接（FC）层，类似于AlexNet（*[https://en.wikipedia.org/wiki/AlexNet](https://en.wikipedia.org/wiki/AlexNet)*），如[图1-4](ch01.xhtml#ch1fig4)所示。'
- en: '![Image](../images/01fig04.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/01fig04.jpg)'
- en: '*Figure 1-4: An illustration of AlexNet*'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*图1-4：AlexNet的示意图*'
- en: We can think of these fully connected layers as two hidden layers and an output
    layer in a multilayer perceptron. Which of the neural network layers can be utilized
    to produce useful embeddings? Interested readers can find more details about the
    AlexNet architecture and implementation in the original publication by Alex Krizhevsky,
    Ilya Sutskever, and Geoffrey Hinton.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这些全连接层看作是一个多层感知机中的两个隐藏层和一个输出层。神经网络的哪些层可以用来生成有用的嵌入表示？感兴趣的读者可以在Alex Krizhevsky、Ilya
    Sutskever和Geoffrey Hinton的原始出版物中找到有关AlexNet架构和实现的更多细节。
- en: '**1-2.** Name some types of input representations that are not embeddings.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**1-2.** 列举一些不是嵌入表示的输入表示类型。'
- en: '**References**'
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: 'The original paper describing the AlexNet architecture and implementation:
    Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, “ImageNet Classification
    with Deep Convolutional Neural Networks” (2012), *[https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks)*.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述AlexNet架构和实现的原始论文：Alex Krizhevsky、Ilya Sutskever和Geoffrey Hinton，"使用深度卷积神经网络进行ImageNet分类"（2012），
    *[https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks)*。
