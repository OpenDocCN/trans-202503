- en: '**2'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**2'
- en: CLASSIFICATION MODELS**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类模型**
- en: '![Image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/common.jpg)'
- en: 'The last chapter briefly introduced *classification applications*, in which
    we predict dummy or categorical variables. These differ from the *numeric applications*
    we’ve analyzed, such as predicting the number of bike riders, which is a numeric
    entity. For instance, in a marketing application, we might wish to predict whether
    a customer will purchase a certain product. In that case, we’d represent the “Y”
    outcome with a dummy variable, using 1 for buying the item and 0 for not buying
    it. There are two *classes* here: Buy and Not Buy.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 上一章简要介绍了 *分类应用*，即我们预测虚拟或分类变量。这些与我们分析过的 *数值应用* 不同，例如预测骑车人数，它是一个数值实体。例如，在市场营销应用中，我们可能希望预测一个顾客是否会购买某个产品。在这种情况下，我们会用一个虚拟变量表示“Y”结果，购买该产品用1表示，不购买用0表示。这里有两个
    *类别*：购买和不购买。
- en: 'We discussed an example of categorical *Y* in the “Some Terminology” box in
    [Section 1.1](ch01.xhtml#ch01lev1). In that example, a physician has divided patients
    into three classes—that is, three categories—depending on their spinal condition:
    normal (NO), disk hernia (DH), and spondylolisthesis (SL). Here *Y* is the class,
    or category, for the given patient. Thus *Y* is a categorical variable with three
    categories. If *Y* is coded as an R *factor*, which is usually the case, then
    the factor will have three levels. On the other hand, we might code *Y* as a vector
    of dummy variables, with one for each class.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第1.1节](ch01.xhtml#ch01lev1)的“某些术语”框中讨论了一个分类 *Y* 的例子。在那个例子中，一位医生根据患者的脊柱状况将其分为三类——即三种类别：正常（NO）、椎间盘突出（DH）和脊椎滑脱（SL）。在这里，*Y*
    是给定患者的类别。因此，*Y* 是一个具有三类的分类变量。如果 *Y* 被编码为R中的 *factor*（通常是这种情况），那么该因子将有三个级别。另一方面，我们也可以将
    *Y* 编码为一组虚拟变量，每个类别对应一个虚拟变量。
- en: We will analyze this vertebrae data in detail in [Section 2.3.1](ch02.xhtml#ch02lev3sec1),
    stating where to obtain it and so on. But let’s do a sneak preview to illustrate
    the previously described notions of class, or category.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[第2.3.1节](ch02.xhtml#ch02lev3sec1)中详细分析这些椎骨数据，说明如何获取它们等。但让我们先做一个简短的预览，来说明前面提到的类别或分类的概念。
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The vertebral condition is in the last column. We see that in this dataset,
    there were 60 patients of class DH and so on.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 椎骨状况位于最后一列。我们可以看到，在这个数据集中，DH类的患者有60位，依此类推。
- en: This chapter goes into more detail regarding classification applications, beginning
    with a brief discussion of a conceptual issue, the notion of a *regression function*,
    and then getting right to the data analysis. We’ll bring in some new datasets,
    again analyzing them using `qeKNN()` but showing some special issues that arise
    in classification contexts.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将更详细地讨论分类应用，从对一个概念问题的简要讨论——回归函数的概念开始，然后直接进入数据分析。我们将引入一些新的数据集，再次使用 `qeKNN()`
    进行分析，并展示在分类情境中出现的一些特殊问题。
- en: 2.1 Classification Is a Special Case of Regression
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 分类是回归的一个特例
- en: Classification applications are quite common in ML. In fact, they probably form
    the majority of ML applications. How does the regression function *r*(*t*) (see
    [Section 1.6](ch01.xhtml#ch01lev6)) play out in such contexts?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 分类应用在机器学习中相当常见。事实上，它们可能构成了大多数机器学习应用。那么，回归函数 *r*(*t*)（见[第1.6节](ch01.xhtml#ch01lev6)）在这样的情境中是如何表现的呢？
- en: Recall that the regression function relates mean *Y* to *X*. If we are predicting
    weight from height and age, then *r*(71, 25) means the mean weight of all people
    of height 71 inches and age 25\. But how does this work if *Y* is a dummy variable?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 回忆一下，回归函数将均值 *Y* 与 *X* 关联。如果我们从身高和年龄来预测体重，那么 *r*(71, 25) 表示身高为71英寸且年龄为25岁的所有人的平均体重。但如果
    *Y* 是一个虚拟变量，这又是如何工作的呢？
- en: In classification settings, the regression function, a conditional mean, becomes
    a conditional probability. To see why, consider a classification application in
    which the outcome *Y* is represented by a dummy variable coded 1 or 0, such as
    the marketing example at the start of this chapter. After collecting our *k*-nearest
    neighbors, we average their 1s and 0s. Say, for example, that *k* = 8, and the
    outcomes for those 8 neighbors are 0,1,1,0,0,0,1,0\. The average is then (0 +
    1 + 1 + 0 + 0 + 0 + 1 + 0) / 8 = 3/8 = 0.375\. Since this means that 3/8 of the
    outcomes were 1s, you can think of the average of 0-or-1 outcomes as the *probability*
    of a 1.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类设置中，回归函数，即条件均值，变成了条件概率。为了理解这一点，考虑一个分类应用，其中结果*Y*由一个虚拟变量表示，编码为1或0，类似于本章开头的营销示例。在收集我们的*k*最近邻之后，我们对其1和0进行平均。例如，假设*k*
    = 8，且这8个邻居的结果分别是0、1、1、0、0、0、1、0。然后，平均值为(0 + 1 + 1 + 0 + 0 + 0 + 1 + 0) / 8 = 3/8
    = 0.375。由于这意味着3/8的结果是1，因此可以将0或1的平均值视为1的*概率*。
- en: In the marketing example, the regression function is the probability that a
    customer will buy a certain product, conditioned on the customer’s feature values,
    such as age, gender, income, and so on. We then guess either Buy or Not Buy according
    to whichever class has the larger probability. Since there are just two classes
    here, that’s equivalent to saying we guess Buy if and only if its class probability
    is greater than 0.5\. (This strategy minimizes the overall probability of misclassification.
    However, other criteria are possible, which is a point we will return to later.)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在营销示例中，回归函数是根据客户的特征值（如年龄、性别、收入等）预测客户购买某个产品的概率。然后，我们根据哪个类别的概率更大来猜测购买或不购买。由于这里只有两个类别，这等同于说，如果购买的类别概率大于0.5，我们就猜测购买。(这个策略最小化了整体的错误分类概率。不过，其他标准也是可能的，这是我们稍后会回到的一个点。)
- en: The same is true in multiclass settings. Consider the medical application above.
    For a new patient whose vertebral status is to be predicted, ML would give the
    physician three probabilities—one for each class. As above, these probabilities
    come from averaging 1s and 0s in dummy variables, with one dummy for each class.
    The preliminary diagnosis would be the spinal class with the highest estimated
    probability.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在多分类环境中也是如此。考虑上面的医学应用。对于一个新患者，其脊椎状况需要预测，机器学习将给医生三个概率值——每个类别一个。如上所述，这些概率是通过对虚拟变量中的1和0进行平均得到的，每个类别有一个虚拟变量。初步诊断将是具有最高估计概率的脊椎类别。
- en: 'In summary:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 总结：
- en: The function *r*() defined for use in predicting numeric quantities applies
    to classification settings as well. In those settings, mean values reduce to probabilities,
    and we use those to predict class.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 用于预测数值的函数*r*()同样适用于分类设置。在这些设置中，均值变成了概率，我们利用这些概率来预测类别。
- en: This view is nice as a unifying concept.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这个观点作为一个统一的概念是很不错的。
- en: '**NOTE**'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*The reader may wonder why we use three dummy variables to classify patients’
    spinal conditions. As noted in [Section 1.4](ch01.xhtml#ch01lev4), just two should
    suffice here; however, certain other methods covered later on require more than
    two. For consistency, we’ll always take the approach of using as many dummies
    as classes. Note that this convention is for* Y*; categorical features in* X *will
    still usually have one fewer dummy than values that the feature can take on.*'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*读者可能会想，为什么我们使用三个虚拟变量来分类患者的脊椎状况。如[第1.4节](ch01.xhtml#ch01lev4)所述，实际上只需要两个虚拟变量即可；然而，稍后介绍的某些其他方法需要更多的虚拟变量。为了保持一致，我们将始终使用与类别数相等的虚拟变量数量。请注意，这个约定适用于*Y*；*X*中的类别特征通常会比该特征可以取的值少一个虚拟变量。*'
- en: Therefore, classification problems are really special cases of regression, which
    is a point we’ll often return to in this book. However, the field uses some confusing
    terminology, which one must be aware of.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，分类问题实际上是回归的特殊情况，这是我们在本书中经常会提到的一点。然而，该领域使用了一些令人困惑的术语，需要我们特别注意。
- en: Previously, we distinguished between, on the one hand, *numerical* applications,
    say, predicting the *number* of bike riders, and, on the other hand, *classification*
    applications, such as the earlier marketing and medical examples where we are
    predicting a *class*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们区分了两种应用，一种是*数值*应用，例如预测*骑行人数*，另一种是*分类*应用，例如前面提到的营销和医学示例，我们是在预测一个*类别*。
- en: However, it is customary in the ML field to call numeric applications *regression
    problems*. This, of course, is a source of confusion, since both numeric and classification
    applications involve the regression function! Sigh . . . As long as you are aware
    of this, it’s not a big issue, but in this book we will use the term *numeric-Y
    applications* for clarity.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在机器学习领域，通常将数值应用称为*回归问题*。当然，这也是一个混淆源，因为数值和分类应用都涉及回归函数！唉……只要你意识到这一点，它其实不是大问题，但在本书中我们将使用*数值-Y应用*这个术语，以便更清晰。
- en: '2.2 Example: The Telco Churn Dataset'
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 示例：电信流失数据集
- en: 'For our first example of a classification model, we’ll take the Telco Customer
    Churn dataset. In marketing circles, the term *churn* refers to customers moving
    from one purveyor of a service to another. A service will then hope to identify
    customers who are likely “flight risks,” or those with a substantial probability
    of leaving. So, we have two classes: Churn or No Churn (that is, Leave or Stay).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们第一个分类模型的示例，我们将使用电信客户流失数据集。在营销领域，*流失*一词指的是客户从一个服务提供商转移到另一个服务提供商。服务提供商希望能够识别出可能“流失”的客户，或者那些有较大可能离开的客户。因此，我们有两个类别：流失（Churn）或不流失（No
    Churn）（即，离开或留下）。
- en: 'You can download and learn more about the dataset at [*https://www.kaggle.com/blastchar/telco-customer-churn*](https://www.kaggle.com/blastchar/telco-customer-churn).
    Let’s load it and take a look:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[*https://www.kaggle.com/blastchar/telco-customer-churn*](https://www.kaggle.com/blastchar/telco-customer-churn)
    下载并了解更多关于数据集的信息。让我们加载它并看一看：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: That last column is the response; `Yes` in the `Churn` column means yes, the
    customer bolted.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一列是响应；`Churn`列中的`Yes`表示客户流失了。
- en: Let’s move on to data preparation, such as checking for NA values. This is a
    rather complex dataset, necessitating extra prep—a bonus for learners of data
    science like readers of this book!
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将进行数据准备，比如检查NA值。这是一个相当复杂的数据集，需要额外的准备工作——对于像本书读者这样的数据科学学习者来说，这是一个额外的奖励！
- en: '***2.2.1 Pitfall: Factor Data Read as Non-factor***'
  id: totrans-29
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***2.2.1 陷阱：将因素数据读取为非因素数据***'
- en: 'Many features in the telco dataset are R factors (that is, nonnumeric quantities
    such as `gender` and `InternetService`). Most R ML packages, including the `qe*`-series
    functions, allow factors. But wait . . . they’re not factors after all. By default,
    `read.csv()` treats nonnumeric items as character strings:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 电信数据集中的许多特征是R中的因素（即非数值量，如`gender`和`InternetService`）。大多数R机器学习包，包括`qe*`系列函数，允许使用因素类型。但等一下...
    它们实际上并不是因素。默认情况下，`read.csv()` 将非数值项目视为字符字符串：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Since our software expects R factors, we need to tell R to treat nonnumeric
    items as factors. (Actually, depending on your version of R, and your default
    settings, this may actually be your default value. If you are not sure, go ahead
    and set this in your call.)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的软件预期使用R因素，我们需要告诉R将非数值项目视为因素。（实际上，取决于你使用的R版本和默认设置，这可能已经是你的默认值。如果你不确定，建议在调用时进行设置。）
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Failure to do this will result in character or numeric values, causing problems
    when we run the ML functions.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不这样做，将会导致字符或数值问题，运行机器学习函数时会出现问题。
- en: '***2.2.2 Pitfall: Retaining Useless Features***'
  id: totrans-35
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***2.2.2 陷阱：保留无用特征***'
- en: 'In some datasets, some columns have no predictive value and should be removed.
    The `customerID` feature is of no predictive value (though it might be if we had
    multiple data points for each customer), so we’ll delete the first column:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在一些数据集中，某些列没有预测价值，应该被移除。`customerID` 特征没有预测价值（尽管如果我们为每个客户有多个数据点，它可能会有预测价值），所以我们将删除第一列：
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Retaining useless features can lead to overfitting.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 保留无用特征可能会导致过拟合。
- en: '***2.2.3 Dealing with NA Values***'
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***2.2.3 处理NA值***'
- en: 'As noted in [Section 1.16](ch01.xhtml#ch01lev16), many datasets contain NA
    values for missing (not available) data. Let’s see if this is the case here:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在[第1.16节](ch01.xhtml#ch01lev16)中提到的，许多数据集包含缺失（不可用）数据的NA值。我们来看看这里是否也存在这种情况：
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: There are indeed 11 NA values present.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 确实存在11个NA值。
- en: 'We’ll use listwise deletion here as a first-level analysis (see [Section 1.16](ch01.xhtml#ch01lev16)),
    but if we were to pursue the matter further, we may look more closely at the NA
    pattern. R actually has a `complete.cases()` function, which returns `TRUE` for
    the rows that are intact. Let’s delete the other cases:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里使用逐行删除作为第一步分析（参见[第1.16节](ch01.xhtml#ch01lev16)），但如果我们进一步深入研究，我们可能会更仔细地查看NA值的模式。R实际上有一个`complete.cases()`函数，它会返回`TRUE`，表示该行数据完整。让我们删除其他不完整的行：
- en: '[PRE6]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'If we don’t need to know which particular cases are excluded, we could simply
    run:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不需要知道哪些具体案例被排除，我们可以简单地运行：
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: How many cases are left?
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 还剩下多少案例？
- en: '[PRE8]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: It’s generally a good idea to check this.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，检查这个是一个好主意。
- en: Among other things, the number of remaining rows affects the size of the value
    we choose for the number of near neighbors *k* (see [Section 1.12.4](ch01.xhtml#ch01lev12sec4)).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，剩余行数影响我们为近邻数*k*选择的值的大小（参见[第1.12.4节](ch01.xhtml#ch01lev12sec4)）。
- en: '***2.2.4 Applying the k-Nearest Neighbors Method***'
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***2.2.4 应用 k-最近邻方法***'
- en: 'Let’s see how to call the `qeKNN()` function in classification problems, say,
    by setting *k* to 75:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在分类问题中调用`qeKNN()`函数，假设将*k*设置为75：
- en: '[PRE9]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As an example of prediction, say we have a new case to predict, such as a hypothetical
    customer like the one in row 8 of the data but who is male and a senior citizen.
    To set this up, we’ll copy row 8 of `tc` and make the stated changes in the `gender`
    and `SeniorCitizen` columns:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 作为预测的一个例子，假设我们有一个新案例需要预测，像数据中第8行的假设客户，但此人是男性且是老年人。为此，我们将复制`tc`的第8行，并在`gender`和`SeniorCitizen`列中进行所述的更改：
- en: '[PRE10]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note, too, that since this revised row will be our “X,” we need to remove the
    “Y” portion—that is, remove the `Churn` column. We saw earlier that `Churn` is
    column 21, but remember, we removed column 1, so it’s now in column 20.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，由于这行数据将作为我们的“X”，我们需要删除“Y”部分——也就是删除`Churn`列。我们之前看到，`Churn`是第21列，但记得我们删除了第1列，所以它现在在第20列。
- en: '[PRE11]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Alternatively, we could have used the `subset()` function in base R,
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们也可以使用R基本包中的`subset()`函数，
- en: '[PRE12]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: the `data.table` package, or the tidyverse. As noted on [page 11](ch01.xhtml#ch01lev5sec1),
    it is up to readers to choose whichever R constructs they feel most comfortable
    with; our focus in this book is on ML, with R playing only a supporting role.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`data.table`包或tidyverse。正如在[第11页](ch01.xhtml#ch01lev5sec1)所述，读者可以选择任何他们最为熟悉的R构造；本书的重点是机器学习，R只扮演一个辅助角色。'
- en: 'Now we make the prediction for the new case:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们为新案例做出预测：
- en: '[PRE13]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The class of interest to us is Churn, so we check the probability of Churn for
    this case, which is 0.32\. Since it’s under 0.5, we guess No Churn.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感兴趣的类别是Churn，因此我们检查这个案例的Churn概率，结果是0.32。由于小于0.5，我们猜测是没有Churn。
- en: Calls to `qeKNN()` for classification problems are essentially the same as for
    numeric-outcome applications. The form of the output is slightly different.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类问题的`qeKNN()`调用与数值输出应用几乎相同，输出形式略有不同。
- en: But . . . how does `qeKNN()` know that this is a classification application
    rather than a numeric- *Y* problem? Our specified *Y* variable, `Churn`, is an
    R factor, signaling to `qeKNN()` that we are running a classification application.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 但是……`qeKNN()`是如何知道这是一个分类应用，而不是一个数值型的*Y*问题呢？我们指定的*Y*变量`Churn`是一个R因子，这告诉`qeKNN()`我们正在运行的是一个分类应用。
- en: Let’s check the classification accuracy.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下分类准确率。
- en: '[PRE14]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We have a misclassification rate of about 22 percent, which is not too bad.
    Once again, however, I must emphasize that those numbers are subject to sampling
    variation, which we will discuss further in [Chapter 3](ch03.xhtml).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的误分类率大约是22%，这还算不错。然而，我必须再次强调，这些数字是受样本变异影响的，我们将在[第3章](ch03.xhtml)进一步讨论。
- en: '***2.2.5 Pitfall: Overfitting Due to Features with Many Categories***'
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***2.2.5 陷阱：由于具有多个类别的特征而导致的过拟合***'
- en: Suppose we had not removed the `customerID` column in our original data, `telco`.
    How many distinct IDs are there?
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们在原始数据`telco`中没有删除`customerID`列。那么有多少个不同的ID？
- en: '[PRE15]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The number of IDs is also the number of rows, as there’s one record per customer.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ID的数量也是行数，因为每个客户对应一条记录。
- en: Recall that `qeKNN()`, like functions in many R packages, internally converts
    factors to dummy variables. If we had not removed this column, there would have
    been 7,042 columns in the internal version of `tc` just stemming from this ID
    column! Not only would the result be unwieldy, but the presence of all these columns
    would dilute the power of k-NN.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，`qeKNN()`就像许多R包中的函数一样，会内部将因子转换为虚拟变量。如果我们没有删除这一列，`tc`的内部版本将会有7,042列，这些列仅来自这个ID列！这样不仅结果会变得笨重，而且所有这些列的存在还会稀释k-NN的效果。
- en: This latter phenomenon is known as *overfitting*. Using too many features will
    actually reduce accuracy in predicting future cases. [Chapter 3](ch03.xhtml) covers
    this in greater detail, but for now, one might loosely think of the data as being
    “shared” by too many features, with not much data available to each one. Note
    that overfitting is a concern both in classification and numeric-*Y* applications.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这个现象被称为*过拟合*。使用过多的特征实际上会减少预测未来案例的准确性。[第3章](ch03.xhtml)对此有更详细的讨论，但现在可以粗略地认为，数据被过多特征“共享”，而每个特征能用到的数据很少。请注意，过拟合在分类和数值-*Y*应用中都是一个问题。
- en: There also may be computational issues if we were to include the ID. Having
    7,000 customer IDs would mean 7,000 dummy variables. That means the internal data
    matrix has over 7000 × 7000 entries—about 50 million. And at 8 bytes each, that
    means something like 0.4GB of RAM. We’ve got to remove this column.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们包含客户ID，可能会遇到计算上的问题。拥有7000个客户ID意味着7000个虚拟变量。这意味着内部数据矩阵有超过7000 × 7000个条目——大约5000万个条目。每个条目占用8字节，那么就需要大约0.4GB的内存。我们必须去掉这一列。
- en: Even with ML packages that directly accept factor data, one must keep an eye
    on what the package is doing—and what we are feeding into it. It’s good practice
    to watch for R factors with a large number of levels. They may appear useful,
    and may in fact be so. But they can also lead to overfitting and computational
    or memory problems.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是直接接受因子数据的机器学习包，也必须关注包在做什么——以及我们输入的数据。良好的实践是关注R因子中有大量级别的情况。它们可能看起来很有用，实际上可能确实如此，但它们也可能导致过拟合以及计算或内存问题。
- en: '2.3 Example: Vertebrae Data'
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 示例：脊椎数据
- en: Consider another UCI dataset, Vertebral Column Dataset,^([1](footnote.xhtml#ch2fn1))
    which is on diseases of the vertebrae. It is described by the curator as a dataset
    that contains “values for six biomechanical features used to classify orthopaedic
    patients into 3 classes (normal, disk hernia, or spondilolysthesis [ *sic* ]).”
    They abbreviate the three classes as NO, DH, and SL.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑另一个UCI数据集——脊椎柱数据集，^([1](footnote.xhtml#ch2fn1))，该数据集与脊椎疾病相关。数据集的管理者描述它为“用于将骨科患者分类为三类（正常、椎间盘疝或脊椎滑脱[*sic*]）的六个生物力学特征的值。”他们将这三类分别缩写为NO、DH和SL。
- en: This example is similar to the last one, but with three classes instead of two.
    Recall that in a two-class problem, we predict on the basis of whether the probability
    of the class of interest is greater than 0.5\. In the telco example, that class
    was Churn, so we predict either Churn or No Churn, depending on whether the Churn
    probability is greater than 0.5\. That probability turned out to be 0.32, so we
    predicted No Churn. But with three or more classes, none of the probabilities
    might be above 0.5; we simply choose the class with the largest probability.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子与上一个类似，但有三个类别而不是两个。回想一下，在一个二分类问题中，我们是根据感兴趣类别的概率是否大于0.5来进行预测的。在电信例子中，那个类别是流失，因此我们预测流失或不流失，取决于流失的概率是否大于0.5。该概率为0.32，所以我们预测为不流失。但是在有三个或更多类别的情况下，可能没有任何概率大于0.5；我们只需选择概率最大的类别。
- en: '***2.3.1 Analysis***'
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***2.3.1 分析***'
- en: Let’s read in the data and, as usual, take a look around.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们读取数据，并像往常一样，先浏览一下。
- en: '[PRE16]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The patient status is column `V7`. We see, by the way, that the curator of the
    dataset decided to group the rows by patient class. That’s why the `qe*`-series
    functions randomly choose the holdout sets.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 患者状态位于列`V7`。顺便提一下，我们看到数据集的管理者决定按患者类别对行进行分组。这就是为什么`qe*`系列函数会随机选择留出集的原因。
- en: Let’s fit the model. The question of how to choose *k* is still open, but we
    need to take into account the size of our dataset. We only have 310 cases here,
    in contrast to the *n* = 7032 we had in the customer churn example. Recall from
    [Section 1.12.4](ch01.xhtml#ch01lev12sec4) that the larger our number of data
    points *n* is, the larger we can set the number of nearest neighbors *k*, so with
    this small dataset, let’s try a small value here, say, *k* = 5.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来拟合模型。如何选择*k*的问题仍然悬而未决，但我们需要考虑数据集的大小。这里只有310个案例，而在客户流失的例子中我们有*n* = 7032个。回想一下[第1.12.4节](ch01.xhtml#ch01lev12sec4)，随着数据点数量*n*的增大，我们可以设置更大的最近邻数*k*，所以对于这个小数据集，我们尝试一个较小的值，比如*k*
    = 5。
- en: '[PRE17]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: As an example of prediction, consider a patient similar to the first one in
    our data but with `V2` being 25 rather than 22.55\. What would our predicted class
    be?
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 作为预测的一个例子，考虑一个与我们数据中第一个患者相似的患者，但`V2`的值为25，而不是22.55。我们的预测类别会是什么？
- en: '[PRE18]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We’d predict the DH class, with an estimated probability of 0.6\. Now let’s
    find the overall accuracy of our predictions using this model.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们会预测DH类别，估算概率为0.6。现在让我们使用这个模型找到我们预测的整体准确度。
- en: '[PRE19]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We would have an error rate of about 19 percent. Note, though, that due to the
    small sample size (310), our predictions would, in this case, be quite susceptible
    to sampling variation.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的错误率大约为19%。不过需要注意的是，由于样本量较小（310），在这种情况下，我们的预测很容易受到抽样变异的影响。
- en: '2.4 Pitfall: Error Rate Improves Only Slightly Using the Features'
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 陷阱：使用特征时错误率仅略有改善
- en: When working with any ML method, on any dataset, it’s important to check whether
    your predictions have a better chance of success than those based on random chance,
    without using the features. Recall our analysis along those lines in [Section
    1.12.2](ch01.xhtml#ch01lev12sec2). Let’s see an example in the classification
    realm.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用任何机器学习方法时，无论在什么数据集上，重要的是检查你的预测是否比随机猜测（不使用特征）更有可能成功。回顾我们在[第1.12.2节](ch01.xhtml#ch01lev12sec2)中的分析。让我们看一个分类领域的例子。
- en: Consider the telco dataset in [Section 2.2](ch02.xhtml#ch02lev2). We found that
    in using 19 features to predict whether a customer will be loyal or not, we would
    err about 22 percent of the time. Is 22 percent good?
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑[第2.2节](ch02.xhtml#ch02lev2)中的电信数据集。我们发现，在使用19个特征预测客户是否会忠诚时，错误率大约为22%。22%的错误率算好吗？
- en: To answer that, consider what happens if we don’t have any feature to predict
    from. Then we would be forced to predict on the basis of what most customers do.
    What percentage of them bolt?
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这个问题，考虑一下如果我们没有任何特征来预测会发生什么。那么我们就只能根据大多数客户的行为来进行预测。那他们中的多少人会离开呢？
- en: '[PRE20]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This says about 27 percent of the customers leave. We will often do computations
    of this form, so let’s review how it works. The expression
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着大约27%的客户会离开。我们经常进行这种形式的计算，所以让我们回顾一下它是如何工作的。这个表达式
- en: '[PRE21]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: evaluates to a bunch of TRUEs and FALSEs. But in R, as in most computer languages,
    TRUE and FALSE are taken to be 1 and 0, respectively. So, we are taking the mean
    of a bunch of 1s and 0s, giving us the proportion of 1s. That’s the proportion
    of `Yes` entries.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这个表达式的结果是许多TRUE和FALSE。但是在R语言中，像大多数编程语言一样，TRUE和FALSE分别表示1和0。因此，我们计算的是一堆1和0的平均值，这给出了1的比例。那就是`Yes`的比例。
- en: So, without customer information, we would simply predict everyone to stay—and
    we would be wrong 27 percent of the time. In other words, using customer information
    reduces our error rate from 27 to 22 percent—helpful, yes, but not dramatically
    so. Of course, we might do better with other values of *k*, and the reader is
    urged to try some, but it does put our analysis in perspective.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，如果没有客户信息，我们将简单地预测每个人都会留下——而我们会错误地预测27%的时间。换句话说，使用客户信息将我们的错误率从27%降低到22%——有帮助，是的，但并不是特别显著。当然，我们可能会通过其他的*k*值获得更好的结果，建议读者试试看，但这也让我们的分析变得有了可比性。
- en: As you can see, a seemingly “good” error rate may be little or no better than
    random chance. Always remember to check the unconditional class probabilities—that
    is, the ones computed without *X*.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，看似“好的”错误率可能与随机猜测几乎没有什么不同。记住，总是要检查无条件的类别概率——也就是说，计算时不使用*X*。
- en: Let’s consider the example in [Section 2.3](ch02.xhtml#ch02lev3) in this regard.
    We achieved an error rate of about 26 percent. If we didn’t use the features,
    guessing instead the most prevalent class overall, would our error rate increase?
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从[第2.3节](ch02.xhtml#ch02lev3)的例子来考虑这个问题。我们达到了大约26%的错误率。如果我们不使用特征，而是猜测最常见的类别，我们的错误率会增加吗？
- en: To that end, let’s see what proportion each class has, ignoring our six features.
    We can answer this question easily.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，让我们看看忽略我们六个特征时，每个类别的比例。我们可以轻松地回答这个问题。
- en: '[PRE22]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The call to `table()` gives us the category counts, so dividing by the total
    number of counts gives us the proportions.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`table()`函数给出了各类别的计数，因此通过将计数总数除以总计数，我们可以得到各类别的比例。
- en: If we did not use the features, we’d always guess the SL class, as it is the
    most common. We would then be wrong a proportion of 1 − 0.4838710 = 0.516129 of
    the time, which is much worse than the 26 percent error rate we attained using
    the features. Using the features greatly enhances our predictive ability in this
    case.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不使用特征，我们总是会猜测SL类别，因为它是最常见的。那样的话，我们会错误地预测1 − 0.4838710 = 0.516129的比例，这比我们使用特征时得到的26%的错误率要糟糕得多。在这种情况下，使用特征大大提高了我们的预测能力。
- en: 'Actually, the `qe*`-series functions in `regtools` compute the featureless
    error rate for us. In the vertebrae example above:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，`regtools` 中的 `qe*` 系列函数会为我们计算没有特征的错误率。在上述椎骨示例中：
- en: '[PRE23]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Note that the result is a little different from the earlier figure of 0.516129\.
    This is because the latter was computed on the full dataset, while this one was
    computed with holdout: the mean came from the training set while the *Y* values
    were from the holdout set.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，结果与之前的 0.516129 略有不同。这是因为后者是在完整数据集上计算的，而此结果是在使用留出法的情况下计算的：均值来自训练集，而*Y*值则来自留出集。
- en: As seen here, in classification settings, `baseAcc` will show the overall misclassification
    rate if one does not use the features. Of course, the same comparison—error rates
    using the features and not—is of interest in numeric *Y* settings. Therefore,
    if we were to ignore the features, our guess for a new *Y* would be the overall
    mean value of *Y* in the training set, which is analogous to our using the conditional
    mean if we do use the features. Without the features, the analog of MAPE is then
    the mean absolute difference between the overall mean *Y* and the actual *Y* in
    the test set. This is reported in `baseAcc`.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如这里所示，在分类问题中，如果不使用特征，`baseAcc` 将显示总体错误分类率。当然，类似的比较——使用特征与不使用特征的误差率——在数值 *Y*
    设置中同样是我们关注的重点。因此，如果我们忽略特征，预测新的 *Y* 将是训练集中的 *Y* 的总体均值，这与我们在使用特征时利用条件均值的情况类似。没有特征时，MAPE
    的类比是总体均值 *Y* 与测试集中的实际 *Y* 之间的平均绝对差。这一结果被报告在 `baseAcc` 中。
- en: As an example, consider again the bike ridership data from [Chapter 1.](ch01.xhtml)
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，再次考虑[第 1 章](ch01.xhtml)中的自行车骑行数据。
- en: '[PRE24]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Here, using our chosen set of 5 predictors, MAPE was about 1,204, versus 1,785
    using no predictors.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，使用我们选择的 5 个预测变量时，MAPE 大约是 1,204，而使用没有预测变量时，MAPE 为 1,785。
- en: 2.5 The Confusion Matrix
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5 混淆矩阵
- en: In multiclass problems, the overall error rate is only the start of the story.
    We might also calculate the (unfortunately named) *confusion matrix*, which computes
    per-class error rates. Let’s see how this plays out in the vertebrae data.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在多分类问题中，总体错误率仅仅是故事的开始。我们还可能计算（不幸命名的）*混淆矩阵*，它计算每个类别的错误率。让我们看看它在椎骨数据中的表现。
- en: The `qe*`-series functions include the confusion matrix in the return value.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`qe*` 系列函数在返回值中包括了混淆矩阵。'
- en: '[PRE25]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Of the 6 + 2 + 0 = 8 data points with actual class DH, 6 were correctly classified
    as DH, but 2 were misclassified as NO, though none were wrongly predicted as SL.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在 6 + 2 + 0 = 8 个实际类别为 DH 的数据点中，6 个被正确分类为 DH，2 个被误分类为 NO，但没有一个被错误预测为 SL。
- en: This type of analysis enables a more finely detailed assessment of our predictive
    power. It’s quite frequently used by MLers to identify potential areas of weakness
    of one’s model.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分析方法使我们能够更细致地评估预测能力。机器学习领域的人经常使用它来识别模型潜在的弱点。
- en: '2.6 Clearing the Confusion: Unbalanced Data'
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.6 消除混淆：不平衡数据
- en: Here, we will discuss issues regarding *unbalanced data*, a common situation
    in classification problems that is much discussed in ML circles.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将讨论有关 *不平衡数据* 的问题，这是分类问题中常见的情况，也是机器学习领域中经常讨论的话题。
- en: Recall that in our customer churn example earlier in this chapter, about 73
    percent of the customers were “loyal,” while 27 percent moved to another telco.
    With the 7,032 cases in our data, those figures translate to 5,141 loyal cases
    and 1,901 cases of churn. The loyal cases outnumber the churn ones by a ratio
    of more than 2.5 to 1\. Often, this ratio can be 100 to 1 or even more. Such a
    situation is termed *unbalanced*. (In this section, our discussion will mainly
    cover the two-class case, but multiclass cases are similar.)
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下本章早些时候的客户流失示例，大约 73% 的客户是“忠诚”的，而 27% 的客户转移到了其他电信公司。在我们数据中的 7,032 个案例中，这些数字转化为
    5,141 个忠诚客户和 1,901 个流失客户。忠诚客户的数量是流失客户的 2.5 倍以上。通常，这一比例可以达到 100:1 甚至更多。这种情况被称为
    *不平衡*。 (本节的讨论主要集中在二分类问题，但多分类问题也是类似的。)
- en: Many analysts recommend that if one’s dataset has unbalanced class sizes, one
    should modify the data to create equal class counts. Their reasoning is that application
    of ML methods to unbalanced data will result in almost all predictions being that
    we guess the class of a new data point to be the dominant class—for instance,
    we always guess No Churn rather than Churn in the telco data. That is not very
    informative!
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 许多分析师建议，如果数据集存在不平衡的类别大小，应通过修改数据来创建相等的类别计数。他们的理由是，应用机器学习方法于不平衡数据时，几乎所有的预测都会是我们将新数据点的类别猜为占主导地位的类别——例如，在电信数据中，我们总是猜测“未流失”（No
    Churn）而不是“流失”（Churn）。这并不提供太多有用的信息！
- en: Illustrations of the problem and offered remedies appear in numerous parts of
    the ML literature, ranging from web tutorials^([2](footnote.xhtml#ch2fn2)) to
    major CRAN packages, such as `caret`, `parsnp`, and `mlr3`. In spite of warnings
    by statisticians,^([3](footnote.xhtml#ch2fn3)) all of these sources recommend
    that you artificially equalize the class counts in your data, say, by discarding
    “excess” data from the dominant class.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习文献的许多地方，都有关于该问题的例子和解决办法，从网络教程^([2](footnote.xhtml#ch2fn2))到主要的CRAN包，如`caret`、`parsnp`和`mlr3`。尽管统计学家已提出警告^([3](footnote.xhtml#ch2fn3))，所有这些资源仍然建议通过人工平衡数据中的类别计数，比如通过丢弃占主导地位类别的“多余”数据来实现。
- en: While it is true that unbalanced data will result in always, or almost always,
    predicting the dominant class, remedying by artificially equalizing the class
    sizes is unwarranted and, in many cases, harmful. Clearly, discarding data is
    generally not a good idea; it will always weaken one’s analysis. Moreover, depending
    on the goals of the given application, it may actually be desirable to always
    guess the dominant class.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管不平衡的数据确实会导致总是或几乎总是预测占主导地位的类别，但通过人工平衡类别大小来解决这个问题是没有必要的，而且在许多情况下是有害的。显然，丢弃数据通常不是一个好主意；它总是会削弱分析的效果。此外，根据给定应用的目标，实际上可能希望总是预测占主导地位的类别。
- en: '***2.6.1 Example: The Kaggle Appointments Dataset***'
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***2.6.1 示例：Kaggle预约数据集***'
- en: To illustrate how best to deal with unbalanced data, let’s look at a dataset
    from Kaggle,^([4](footnote.xhtml#ch2fn4)) a firm with the curious business model
    of operating data science competitions. The goal is to use this dataset to predict
    whether a patient will fail to keep a doctor’s appointment; if the medical office
    can flag the patients at risk of not showing up, staff can make extra efforts
    to avoid the economic losses resulting from no-shows.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明如何最好地处理不平衡数据，我们来看一个来自Kaggle的数据集，^([4](footnote.xhtml#ch2fn4)) Kaggle是一个通过举办数据科学竞赛来运作的公司。目标是使用这个数据集来预测一个患者是否会错过医生的预约；如果医疗机构能够标记出有可能未按时到达的患者，工作人员可以采取额外的措施，避免因缺席而带来的经济损失。
- en: 'Read in the data:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 读取数据：
- en: '[PRE26]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Should we remove the patient ID, as we did in the telco data, to avoid overfitting?
    We see that on average, each patient appears less than twice in the data, meaning
    there is not much data per patient:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是否应该像在电信数据中那样去除患者ID，以避免过拟合？我们发现平均而言，每个患者在数据中出现的次数少于两次，这意味着每个患者的数据量不多：
- en: '[PRE27]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'So, yes, we probably should not include this feature. Using the same reasoning,
    it makes sense to remove the appointment ID, neighborhood, appointment day, and
    scheduled day variables:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，是的，我们可能不应该包括这个特征。以相同的逻辑来看，去除预约ID、社区、预约日期和预定日期变量也有意义：
- en: '[PRE28]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'About 20 percent of cases are no-shows (counterintuitively, `Yes` here means
    “Yes, the patient didn’t show up”):'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 大约20%的情况是未到诊（反直觉的是，`Yes`在这里表示“是的，患者没有按时到达”）：
- en: '[PRE29]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Yes, it is indeed unbalanced data.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，这确实是一个不平衡的数据集。
- en: 'Recall that the concern is that our predicted *Y*s will also be unbalanced—that
    is, most or all will predict the patient to show up. Let’s check this by inspecting
    the output of running `qeKNN()`. Here is the call:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，问题在于我们的预测*Y*也可能是不平衡的——也就是说，大多数或所有的预测都会认为患者会按时到达。我们可以通过检查运行`qeKNN()`的输出结果来验证这一点。以下是调用方式：
- en: '[PRE30]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'There is a lot of information in most `qe*`-series function return values.
    Here are the components of the `qeKNN()` function output:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数`qe*`系列函数的返回值中包含了大量信息。以下是`qeKNN()`函数输出的各个组件：
- en: '[PRE31]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'That `holdoutPreds` component is actually the return value of `regtools::kNN()`,
    discussed in [Section 1.17](ch01.xhtml#ch01lev17). (The R notation `p::e` means
    the entity `e` in the package `p`.) Let’s see what is in there:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 那个`holdoutPreds`组件实际上是`regtools::kNN()`的返回值，详细讨论见[第1.17节](ch01.xhtml#ch01lev17)。
    （R语言中的`p::e`表示包`p`中的实体`e`。）我们来看一下里面的内容：
- en: '[PRE32]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Checking the documentation, we find that `predClasses` is the vector of predicted
    *Y*s in the holdout set, which is just what we need. Let’s tabulate them:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 检查文档后，我们发现`predClasses`是保留集中的预测*Y*值向量，这正是我们需要的。我们来列出这些值：
- en: '[PRE33]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: First, how do we deal with *two* dollar signs (`$`) in this expression?
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们如何处理这个表达式中的*两个*美元符号（`$`）？
- en: '[PRE34]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Remember, the notation `u$v` means component `v` within the object `u`. So,
    `u$v$w` means the component *w* within the object `u$v` ! So yes, we have an R
    list within an R list here, which is common in the R world.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，符号`u$v`表示对象`u`中的组件`v`。所以，`u$v$w`表示对象`u$v`中的组件*w*! 所以，是的，我们这里有一个R列表嵌套在另一个R列表中，这在R语言中是常见的。
- en: At any rate, we see that with our model here, we predict in the vast majority
    of cases that the patient will show up (again, Yes means a no-show), confirming
    the concern many people have about unbalanced data.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，我们看到在我们当前的模型中，我们在绝大多数情况下预测患者会到场（再次说明，Yes表示未到场），这验证了许多人对不平衡数据的担忧。
- en: 'In fact, the predictions are even more unbalanced than the data itself; we
    saw about 20 percent of the *Y*s in the overall dataset were no-shows, while here
    that is true for only 1.3 percent of the predicted holdout *Y*s. Actually, this
    is to be expected: remember, if the model finds the probability of breaking the
    appointment to be greater than 0.5, the prediction will be that they don’t show
    up, even if they actually did.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，预测结果比数据本身更加不平衡；我们看到整体数据集中大约20%的*Y*值是未到场的，而在这里，预测的保留集*Y*中只有1.3%的未到场。这其实是可以预料的：记住，如果模型认为错过预约的概率大于0.5，那么预测结果将是未到场，即使实际上他们到场了。
- en: 'This is exactly the problem cited by the sources mentioned earlier who recommend
    artificially balancing the data. They recommend altering the data in a manner
    that makes all classes represented equally in the data. They suggest one of the
    following remedies (or variants) to equalize the class sizes:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是之前提到的来源所指出的问题，他们推荐人工平衡数据。他们建议通过某种方式改变数据，使所有类别在数据中得到平等表示。他们建议以下一种（或变体）方法来平衡类别大小：
- en: '**Downsample**'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '**下采样**'
- en: Replace the No cases in our data with 22,319 randomly chosen elements from the
    original 88,208\. We then will have 22,319 Yes records and 22,139 No records,
    thus achieving balance.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 用从原始88,208个记录中随机选取的22,319个元素替换No记录。这样，我们将拥有22,319个Yes记录和22,139个No记录，从而实现平衡。
- en: '**Upsample**'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '**上采样**'
- en: Replace the Yes cases with 88,208 randomly selected elements from the original
    22,319 (with replacement). We then will have 88,208 Yes records and 88,208 No
    records, thus achieving balance.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 用从原始22,319个记录中随机选取的88,208个元素（带放回）替换Yes记录。这样，我们将拥有88,208个Yes记录和88,208个No记录，从而实现平衡。
- en: One would apply one’s chosen ML method, say, k-NN, to the modified data, and
    then predict new cases to be no-shows according to whether the estimated conditional
    probability is larger than 0.5 or not.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将应用所选的机器学习方法，比如k-NN，来处理修改后的数据，然后根据估计的条件概率是否大于0.5来预测新案例是否未到场。
- en: '***2.6.2 A Better Approach to Unbalanced Data***'
  id: totrans-155
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***2.6.2 更好的不平衡数据处理方法***'
- en: Again, downsampling is undesirable; data is precious and shouldn’t be discarded.
    The other approach to balancing, upsampling, doesn’t make sense either—why would
    adding fully duplicate data help?
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，下采样是不可取的；数据非常宝贵，不应丢弃。另一种平衡方法——上采样——也没有意义，为什么增加完全重复的数据会有所帮助呢？
- en: 'In addition, balancing assumes equal adverse impact from false negatives and
    false positives, which is unlikely in applications like the appointments data.
    One could set up formal utility values here for the relative costs of false negatives
    and false positives. But in many applications, we need more flexibility than what
    a fully mechanical algorithm gives us. For instance, consider credit card fraud.
    As noted in global accounting network PricewaterhouseCoopers’s publication *Fraud:
    A Guide to Its Prevention,* *Detection and Investigation*, “Every fraud incident
    is different, and reactive responses will vary depending on the facts that are
    unique to each case.”^([5](footnote.xhtml#ch2fn5))'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，平衡假设了假阴性和假阳性带来的负面影响相同，但在像预约数据这样的应用中，这种假设不太可能成立。我们可以在这里设置正式的效用值，来表示假阴性和假阳性的相对成本。但在许多应用中，我们需要比完全机械化算法更灵活的方法。例如，考虑信用卡欺诈问题。正如全球会计网络普华永道（PricewaterhouseCoopers）在其出版物《欺诈：防范、检测与调查指南》中指出的那样，“每个欺诈事件都是不同的，反应性的应对措施会根据每个案件独特的事实有所不同。”^([5](footnote.xhtml#ch2fn5))
- en: The easier and better solution is to simply have our ML algorithm flag the cases
    in which there is a substantial probability of fraud, say, above some specified
    threshold, then take it “by hand” from there. Once the algorithm has selected
    a set of possible instances of fraud, the (human) auditor will take into account
    that estimated probability—now worrying not only that it is larger than the threshold
    but also *how much larger*—as well as such factors as the amount of the charge,
    special characteristics not measured in the available features, and so on.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 更简单和更好的解决方案是让我们的机器学习算法标记出存在较高欺诈概率的案例，比如超过某个指定阈值，然后由人工进一步处理。一旦算法选择出可能的欺诈实例，(人工)审计员将会考虑到该估计概率——现在不仅要担心它是否超过阈值，还要考虑*超过了多少*——以及诸如费用金额、未在可用特征中衡量的特殊特征等因素。
- en: The auditor may not give priority, for instance, to a case in which the probability
    is above the threshold but in which the monetary value of the transaction is small.
    On the other hand, the auditor may give this transaction a closer look, even if
    the monetary value is small, if the probability is much higher than the threshold
    value.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 审计员可能不会优先考虑，例如，概率高于阈值但交易金额较小的案例。另一方面，如果概率远高于阈值，审计员即使交易金额较小，可能也会对这个交易进行更仔细的审查。
- en: Thus, the practical solution to the unbalanced-data “problem” is not to artificially
    resample the data but instead to identify individual cases of interest in the
    context of the application. This then means cases of sufficiently high probability
    to be of concern, again in the context of the given application.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，解决不平衡数据“问题”的实际方案不是人工重采样数据，而是识别在应用背景下的个别关注案例。这意味着要关注那些有足够高概率值得关注的案例，同样是在给定应用的背景下。
- en: Recall that the `probs` component of a `qe*`-series call in a classification
    application gives the estimated probabilities of the various classes—exactly what
    we need. For instance, in the missed-appointments dataset, we can check `probs`
    to find the probability a patient will be a no-show.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，在分类应用中的`qe*`系列调用的`probs`组件给出了各种类别的估计概率——这正是我们所需要的。例如，在缺席约会的数据集中，我们可以检查`probs`，以找出一个患者未到场的概率。
- en: Note that `probs` has one row for each case to be predicted. Since the missed-appointments
    data has two classes, there will be two columns. We saw previously that the Yes
    class (that is, missed appointments) is listed second. Let’s take a look.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`probs`每一行对应一个要预测的案例。由于缺席约会数据有两个类别，因此将有两列。我们之前看到“是”类别（即缺席约会）排在第二位。我们来看看。
- en: '[PRE35]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'There are quite a few patients who, despite being more likely than not to keep
    the appointment, still have a substantial risk of no-show. For instance, 18,427
    people have a 0.2 estimated probability of failing to keep their appointment.
    An additional 28,435 patients have more than a 25 percent chance of not showing
    up:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 有相当多的患者，尽管保持约会的可能性较大，但仍然有较大的缺席风险。例如，18,427人预计有0.2的概率未能保持约会。另有28,435名患者缺席的概率超过25%：
- en: '[PRE36]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The reasonable approach would be to decide on a threshold for no-show probability,
    then determine which patients fail to meet that threshold. With a threshold of
    0.75, for instance, any patient whose probability of keeping the appointment is
    greater than that level might be given special attention. We would make extra
    phone calls to them, explain penalties for missed appointments, and so on.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 合理的做法是决定一个缺席概率的阈值，然后确定哪些患者未能达到该阈值。例如，设定一个0.75的阈值，任何约会保持概率大于该水平的患者可能会受到特别关注。我们会额外拨打电话给他们，解释缺席约会的惩罚等。
- en: If those calls prove too burdensome, we can increase the threshold. Or, if the
    calls turn out to be highly effective, we can reduce it. Either way, the point
    is that the power is now in the hands of the end user of the data, where it ought
    to be. Artificially balancing the data denies the user that power.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这些调用过于繁重，我们可以提高阈值。或者，如果这些调用效果非常显著，我们可以降低阈值。无论哪种方式，关键是现在的控制权掌握在数据的最终用户手中，而这正是它应有的地方。人为地平衡数据剥夺了用户的这一权力。
- en: 2.7 Receiver Operating Characteristic and Area Under Curve
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.7 接收者操作特征与曲线下面积
- en: We have seen MAPE used as a measure of predictive power in numeric-*Y* problems,
    while overall misclassification error (OME) is used in classification applications.
    Both are very popular, but in the classification case, there are other common
    measures, two of which we will discuss now.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，MAPE 被用作数值型 *Y* 问题中的预测能力衡量标准，而整体误分类错误（OME）则用于分类应用中。这两者都非常流行，但在分类问题中，还有其他常见的衡量标准，下面我们将讨论其中两个。
- en: '***2.7.1 Details of ROC and AUC***'
  id: totrans-170
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***2.7.1 ROC 和 AUC 的详细信息***'
- en: Many analysts use the *Area Under Curve (AUC)* value as an overall measure of
    predictive power in classification problems. The curve under consideration is
    the *Receiver Operating Characteristic (ROC)* curve.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 许多分析师使用*曲线下面积（AUC）*值作为分类问题中预测能力的总体衡量标准。所讨论的曲线是*接收者操作特征（ROC）*曲线。
- en: To understand ROC, recall [Section 2.6.2](ch02.xhtml#ch02lev6sec2), where we
    spoke of a threshold for class prediction. If the estimated class probability
    is on one side of the threshold, we predict Class 1, and on the other side, we
    predict Class 0\. Threshold values can be anywhere from 0 to 1; we choose our
    value based on our goals in the particular application.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解 ROC，请回顾 [第 2.6.2 节](ch02.xhtml#ch02lev6sec2)，我们在其中讨论了分类预测的阈值。如果估计的类别概率在阈值的一侧，我们预测为
    Class 1，另一侧则预测为 Class 0。阈值可以是从 0 到 1 之间的任何值；我们根据特定应用中的目标选择阈值。
- en: 'The ROC curve then explores various scenarios. How well would we predict if,
    say, we take the threshold to be 0.4? What about 0.7? And so on? The question
    “How well would we predict?” is addressed by two numbers: the *true positive rate
    (TPR)* and the *false positive rate (FPR)*.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ROC 曲线会探索各种情境。比如，如果我们将阈值设为 0.4，我们的预测效果如何？如果是 0.7 呢？以此类推。“我们能预测得有多好？”这个问题由两个数字来回答：*真正阳性率（TPR）*和*假阳性率（FPR）*。
- en: TPR, also known as the *sensitivity*, is the probability that we guess Class
    1, given that the class actually is Class 1\. FPR, the *specificity*, is the probability
    that we guess Class 1, given that the true class is Class 0\. Note that the threshold
    value determines FPR and TPR. The ROC curve then plots TPR against FPR as the
    threshold varies.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: TPR，也称为*灵敏度*，是指在实际类别为 Class 1 时，我们预测为 Class 1 的概率。FPR，*特异性*，是指在实际类别为 Class 0
    时，我们预测为 Class 1 的概率。请注意，阈值决定了 FPR 和 TPR。ROC 曲线将 TPR 和 FPR 随阈值变化绘制出来。
- en: The AUC is then the total area under the ROC curve. It takes on values between
    0 and 1\. The higher the curve, the better, as it implies that for any fixed FPR,
    TPR is high. Thus, the closer AUC is to 1.0, the better the predictive ability.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: AUC 是 ROC 曲线下的总面积。它的取值范围在 0 到 1 之间。曲线越高越好，因为这意味着对于任何固定的假阳性率（FPR），真正阳性率（TPR）较高。因此，AUC
    越接近 1.0，预测能力越好。
- en: '***2.7.2 The qeROC() Function***'
  id: totrans-176
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***2.7.2 qeROC() 函数***'
- en: 'The `qeROC()` function wraps `roc()` in the `pROC` package. It performs ROC
    analysis on the output of a `qe*-` function, say, `qeKNN()`, using the call form:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '`qeROC()` 函数是 `pROC` 包中 `roc()` 函数的封装。它对 `qe*-` 函数（例如 `qeKNN()`）的输出进行 ROC 分析，调用形式如下：'
- en: '[PRE37]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Here, `dataIn` is the dataset on which the `qe*-` function had been called,
    `qeOut` is the output of that function, `yName` is the name of *Y* in `dataIn`,
    and `yLevelName` is the *Y* level (in the R factor sense) of interest. Note that
    the latter allows for the multiclass case.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`dataIn` 是调用了 `qe*-` 函数的数据集，`qeOut` 是该函数的输出，`yName` 是 `dataIn` 中 *Y* 的名称，`yLevelName`
    是感兴趣的 *Y* 水平（在 R 因子意义下）。请注意，后者允许处理多类别情况。
- en: As noted, `qeROC()` calls `pPROC::roc()`. The return value of the former consists
    of the return value of the latter. It may be useful to assign the return value
    to a variable for further use (see below), but if this is not done, the ROC curve
    is plotted and the AUC value is printed out.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，`qeROC()` 调用了 `pROC::roc()`。前者的返回值包括后者的返回值。将返回值赋给一个变量以供进一步使用可能很有用（见下文），但如果没有这么做，ROC
    曲线将被绘制，并打印出 AUC 值。
- en: Note that `qeROC()` operates on the holdout set. This is important for the same
    reasons that the `testAcc` output of the `qe*`-series functions use the holdout
    set (see [Section 1.12](ch01.xhtml#ch01lev12)).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`qeROC()` 是在持出集（holdout set）上操作的。这一点很重要，原因与 `qe*` 系列函数中的 `testAcc` 输出使用持出集相同（见
    [第 1.12 节](ch01.xhtml#ch01lev12)）。
- en: '***2.7.3 Example: Telco Churn Data***'
  id: totrans-182
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***2.7.3 示例：电信流失数据***'
- en: Let’s see what the ROC curve has to say about the Telco Churn data, continuing
    our earlier k-NN analysis.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 ROC 曲线对电信流失数据的解释，继续我们之前的 k-NN 分析。
- en: '[PRE38]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The plot is shown in [Figure 2-1](ch02.xhtml#ch02fig01). Let’s see how to interpret
    it.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图示见 [图 2-1](ch02.xhtml#ch02fig01)。让我们来看看如何解读它。
- en: '![Image](../images/ch02fig01.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/ch02fig01.jpg)'
- en: '*Figure 2-1: ROC, Telco Churn data*'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2-1：ROC，电信流失数据*'
- en: The 45-degree line is drawn to represent pure guessing, with our rate of prediction
    of a positive class being the same, regardless of whether we have a true positive
    or not. Again, the further the ROC curve is above this line, the better.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 45 度线被画出以表示纯粹的猜测，正类预测的比率是一样的，无论我们是否有真正的正例。再说一次，ROC 曲线越是高于这条线，结果就越好。
- en: '***2.7.4 Example: Vertebrae Data***'
  id: totrans-189
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***2.7.4 示例：脊椎数据***'
- en: 'The `qeROC()` function can also be used in multiclass settings:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '`qeROC()` 函数也可以在多类设置中使用：'
- en: '[PRE39]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Since this is a small dataset, we opted for larger holdout (under the 10 percent
    default, the holdout would only have 31 cases). But even then, one must keep in
    mind that those AUC values are subject to considerable sample variation. Thus
    we must be cautious in concluding that we are less accurate in predicting the
    `'NO'` cases.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个小数据集，我们选择了较大的保留集（在 10% 的默认设置下，保留集只有 31 个案例）。但即便如此，我们必须记住，这些 AUC 值会受到相当大的样本变异性的影响。因此，我们在得出结论认为我们在预测
    `'NO'` 类别时准确度较低时，必须谨慎。
- en: Anyway, what do these numbers mean in this multiclass context? Actually, they
    are the same ones one would get by running three individual ROC analyses (on the
    same holdout set); `qeROC()` is just a convenience function in this case, alleviating
    the user of the need to run `roc()` three times. And since no new computation
    is done (the class probabilities are scaled to sum to 1.0), this also saves the
    user computation time if the dataset is large.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 不管怎样，这些数字在多类情况下意味着什么呢？实际上，它们与运行三次独立的 ROC 分析（在相同的保留集上）得到的结果是一样的；`qeROC()` 在这种情况下只是一个便利函数，免去了用户需要运行
    `roc()` 三次的麻烦。而且，由于没有进行新的计算（类概率已缩放至总和为 1.0），如果数据集很大，这也节省了用户的计算时间。
- en: '***2.7.5 Pitfall: Overreliance on AUC***'
  id: totrans-194
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***2.7.5 陷阱：过度依赖 AUC***'
- en: AUC can be a useful number to complement OME, and many analysts regard it as
    an integral part of their ML toolkits. However, one should use it cautiously.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: AUC 可以是补充 OME 的一个有用指标，许多分析师将其视为他们机器学习工具包的一个重要组成部分。然而，使用时应谨慎。
- en: Mathematically, AUC is the average value of ROC over all possible threshold
    values. But recall, each threshold value corresponds implicitly to a relative
    utility. In a credit card fraud dataset, for instance, we may believe it is far
    worse to decide a transaction is legitimate when it is actually fraudulent, compared
    to vice versa. The problem, then, is that there are some threshold values that
    we would never even consider using for a particular application. Yet they are
    averaged into the AUC value, thus rendering the latter less meaningful.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学角度来看，AUC 是所有可能阈值上 ROC 的平均值。但请记住，每个阈值隐含地对应着一个相对的效用。例如，在信用卡欺诈数据集中，我们可能认为，判断一笔交易是合法的，而实际上它是欺诈的，比反过来判断要更严重。问题在于，有一些阈值我们甚至不会考虑在特定应用中使用。然而，它们被平均到
    AUC 值中，从而使得后者的意义减少。
- en: 2.8 Conclusions
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.8 结论
- en: 'We now have a solid foundation in the two basic types of ML problems: numeric-*Y*
    and classification. Along the way, we’ve picked up some miscellaneous skills,
    such as removing useless features and dealing with NA values.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经建立了机器学习问题的两种基本类型的坚实基础：数值型 *Y* 和分类问题。在此过程中，我们还掌握了一些杂项技能，如去除无用特征和处理 NA 值。
- en: It’s now time to take a serious look at how to choose the value of *k* and,
    more generally, hyperparameters in all ML methods, which is a topic that we have
    been glossing over so far. We will look at this in detail in the next two chapters.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候认真考虑如何选择 *k* 的值，以及更一般的，在所有机器学习方法中选择超参数的问题，这是我们迄今为止一直忽略的话题。我们将在接下来的两章中详细讨论这一点。
