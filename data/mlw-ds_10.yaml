- en: '**10**'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**10**'
- en: '**DEEP LEARNING BASICS**'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度学习基础**'
- en: '![image](../images/common01.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/common01.jpg)'
- en: Deep learning is a type of machine learning that has advanced rapidly in the
    past few years, due to improvements in processing power and deep learning techniques.
    Usually, *deep learning* refers to deep, or many-layered, neural networks, which
    excel at performing very complex, often historically human-centric tasks, like
    image recognition and language translation.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是机器学习的一种类型，近年来由于处理能力和深度学习技术的进步而迅速发展。通常，*深度学习*指的是深度或多层神经网络，这些网络在执行非常复杂、通常以人为主的任务（如图像识别和语言翻译）方面表现出色。
- en: For example, detecting whether a file contains an exact copy of some malicious
    code you’ve seen before is simple for a computer program and doesn’t require advanced
    machine learning. But detecting whether a file contains malicious code that is
    somewhat similar to malicious code you’ve seen before is a far more complex task.
    Traditional signature-based detection schemes are rigid and perform poorly on
    never-before-seen or obfuscated malware, whereas deep learning models can see
    through superficial changes and identify core features that make a sample malicious.
    The same goes for network activity, behavioral analysis, and other related fields.
    This ability to pick out useful characteristics within a mass of noise makes deep
    learning an extremely powerful tool for cybersecurity applications.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，检测一个文件是否包含你以前见过的恶意代码的完全复制对计算机程序来说很简单，不需要高级的机器学习技术。但检测一个文件是否包含与以前见过的恶意代码相似的恶意代码则是一个复杂得多的任务。传统的基于签名的检测方案是死板的，对于以前未见过或经过混淆的恶意软件表现不佳，而深度学习模型能够看穿表面变化，识别出使样本具有恶意特征的核心要素。网络活动、行为分析和其他相关领域也同样适用。深度学习通过从一堆噪声中提取有用特征的能力，使其成为网络安全应用中极为强大的工具。
- en: 'Deep learning is just a type of machine learning (we covered machine learning
    in general in [Chapters 6](ch06.xhtml#ch06) and [7](ch07.xhtml#ch07)). But it
    often leads to models that achieve better accuracy than approaches we discussed
    in these preceding chapters, which is why the entire field of machine learning
    has emphasized deep learning in the last five years or so. If you’re interested
    in working at the cutting edge of security data science, it’s essential to learn
    how to use deep learning. A note of caution, however: deep learning is harder
    to understand than the machine learning approaches we discussed early in this
    book, and it requires some commitment, and high-school level calculus, to fully
    understand. You’ll find that the time you invest in understanding it will pay
    dividends in your security data science work in terms of your ability to build
    more accurate machine learning systems. So we urge you to read this chapter carefully
    and work at understanding it until you get it! Let’s get started.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习只是机器学习的一种类型（我们在[第6章](ch06.xhtml#ch06)和[第7章](ch07.xhtml#ch07)中讨论了机器学习的总体内容）。但它通常会产生比我们在前几章中讨论的方法更高的准确性，这也是为什么在过去五年左右，整个机器学习领域都强调深度学习的原因。如果你有兴趣在安全数据科学的前沿工作，那么学习如何使用深度学习是至关重要的。然而，需要注意的是：深度学习比我们在本书前面讨论的机器学习方法更难理解，完全掌握它需要一定的时间投入，以及高中水平的微积分知识。你会发现，投入的时间将会为你在安全数据科学工作中带来回报，尤其是在构建更精确的机器学习系统方面。因此，我们敦促你仔细阅读这一章，并努力理解，直到完全掌握！让我们开始吧。
- en: '**What Is Deep Learning?**'
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**什么是深度学习？**'
- en: Deep learning models learn to view their training data as a nested hierarchy
    of concepts, which allows them to represent incredibly complex patterns. In other
    words, these models not only take into consideration the original features you
    give them, but automatically combine these features to form new, optimized meta-features,
    which they then combine to form even more features, and so on.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型学会将它们的训练数据视为一个嵌套的概念层次结构，这使它们能够表示非常复杂的模式。换句话说，这些模型不仅考虑你给它们的原始特征，还会自动将这些特征组合成新的、优化的元特征，接着再将这些元特征组合成更多的特征，依此类推。
- en: “Deep” also refers to the architecture used to accomplish this, which usually
    consists of multiple layers of processing units, each using the previous layer’s
    outputs as its inputs. Each of these processing units is called a *neuron*, and
    the model architecture as a whole is called a *neural network*, or a *deep neural
    network* when there are many layers.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: “深度”也指的是用于实现这一目标的架构，通常由多个处理单元层组成，每一层使用上一层的输出作为其输入。每个处理单元被称为*神经元*，整体架构被称为*神经网络*，如果有很多层，则称为*深度神经网络*。
- en: To see how this architecture can be helpful, let’s think about a program that
    attempts to classify images either as a bicycle or a unicycle. For a human, this
    is an easy task, but programming a computer to look at a grid of pixels and tell
    which object it represents is quite difficult. Certain pixels that indicate that
    a unicycle exists in one image will mean something else entirely in the next if
    the unicycle has moved slightly, been placed at a different angle, or has a different
    color.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解这种架构如何有所帮助，我们可以考虑一个程序，该程序尝试将图像分类为自行车或独轮车。对于人类来说，这是一项简单的任务，但编程让计算机查看像素网格并判断图像代表的是什么物体却相当困难。如果独轮车稍微移动、放置在不同的角度，或者颜色发生变化，那么在一张图片中表示独轮车的某些像素，在下一张图片中可能完全代表其他意思。
- en: Deep learning models get past this by breaking the problem down into more manageable
    pieces. For example, a deep neural network’s first layer of neurons might first
    break down the image into parts and just identify low-level visual features, like
    edges and borders of shapes in the image. These created features are fed into
    the next layer of the network to find patterns among the features. These patterns
    are then fed into subsequent layers, until the network is identifying general
    shapes and, eventually, complete objects. In our unicycle example, the first layer
    might find lines, the second might see lines forming circles, and the third might
    identify that certain circles are actually wheels. In this way, instead of looking
    at a mass of pixels, the model can see that each image has a certain number of
    “wheel” meta-features. It can then, for example, learn that two wheels likely
    indicate a bicycle, whereas one wheel means a unicycle.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型通过将问题分解成更易管理的部分来解决这个问题。例如，深度神经网络的第一层神经元可能首先将图像分解成部分，只识别图像中的低级视觉特征，如边缘和形状的边界。这些创建的特征被送入网络的下一层，以便在这些特征中找到模式。然后，这些模式被送入后续层，直到网络识别出一般形状，最终识别出完整的物体。在我们的独轮车示例中，第一层可能会找到线条，第二层可能会看到线条形成圆形，第三层可能会识别出某些圆形实际上是车轮。通过这种方式，模型不再只是查看一堆像素，而是能够看到每张图像中有一定数量的“车轮”元特征。它可以学到，例如，两个车轮可能表示一辆自行车，而一个车轮则意味着一辆独轮车。
- en: In this chapter, we focus on how neural networks actually work, both mathematically
    and structurally. First, I use a very basic neural network as an example to explain
    exactly what a neuron is and how it connects to other neurons to create a neural
    network. Second, I describe the mathematical processes used to train these networks.
    Finally, I describe some popular types of neural networks, how they’re special,
    and what they’re good at. This will set you up nicely for [Chapter 11](ch11.xhtml#ch11),
    where you’ll actually create deep learning models in Python.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将重点讲解神经网络的工作原理，包括其数学原理和结构。首先，我将使用一个非常基础的神经网络作为示例，解释什么是神经元以及它如何连接到其他神经元，从而构建出一个神经网络。其次，我将描述用于训练这些网络的数学过程。最后，我将介绍一些流行的神经网络类型，它们的特殊之处以及它们擅长的领域。这将为你后续在[第11章](ch11.xhtml#ch11)中实际使用Python创建深度学习模型打下基础。
- en: '**How Neural Networks Work**'
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**神经网络的工作原理**'
- en: Machine learning models are simply big mathematical functions. For example,
    we take input data (such as an HTML file represented as a series of numbers),
    apply a machine learning function (such as a neural network), and we get an output
    that tells us how malicious the HTML file looks. Every machine learning model
    is just a function containing adjustable parameters that get optimized during
    the training process.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型本质上只是大型的数学函数。例如，我们输入数据（比如表示为一系列数字的HTML文件），应用一个机器学习函数（比如神经网络），然后得到一个输出，告诉我们HTML文件看起来有多恶意。每个机器学习模型实际上就是一个包含可调参数的函数，这些参数在训练过程中会不断优化。
- en: But how does a deep learning function actually work and what does it look like?
    Neural networks are, as the name implies, just networks of many neurons. So, before
    we can understand how neural networks work, we first need to know what a neuron
    is.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 但深度学习函数究竟是如何工作的，长什么样呢？神经网络，顾名思义，就是由许多神经元组成的网络。所以，在我们理解神经网络如何工作之前，首先需要了解什么是神经元。
- en: '***Anatomy of a Neuron***'
  id: totrans-15
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***神经元的结构***'
- en: Neurons themselves are just a type of small, simple function. [Figure 10-1](ch10.xhtml#ch10fig1)
    shows what a single neuron looks like.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元本身只是一个小而简单的函数。[图10-1](ch10.xhtml#ch10fig1)展示了一个单独神经元的样子。
- en: '![image](../images/f0177-01.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0177-01.jpg)'
- en: '*Figure 10-1: Visualization of a single neuron*'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*图10-1：单个神经元的可视化*'
- en: You can see that input data comes in from the left, and a single output number
    comes out on the right (though some types of neurons generate multiple outputs).
    The value of the output is a function of the neuron’s input data and some parameters
    (which are optimized during training). Two steps occur inside every neuron to
    transform the input data into the output.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到输入数据从左侧进入，单一的输出值从右侧输出（尽管某些类型的神经元会产生多个输出）。输出的值是神经元输入数据和一些参数的函数（这些参数在训练过程中得到优化）。每个神经元内部进行两步操作，将输入数据转换为输出。
- en: First, a weighted sum of the neuron’s inputs is calculated. In Figuree 10-1,
    each input number, *x*[i], travelling into the neuron gets multiplied by an associated
    *weight* value, *w*[i]. The resulting values are added together (yielding a weighted
    sum) to which a *bias* term is added. The bias and weights are the parameters
    of the neuron that are modified during training to optimize the model.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，计算神经元输入的加权和。在图10-1中，每个输入值*x*[i]，进入神经元后都会与相应的*权重*值*w*[i]相乘。将得到的结果相加（得到加权和），然后加上一个*偏置*项。偏置和权重是神经元的参数，这些参数在训练过程中会被调整，以优化模型。
- en: Second, an *activation function* is applied to the weighted sum plus bias value.
    The purpose of an activation function is to apply a nonlinear transformation to
    the weighted sum, which is a *linear* transformation of the neuron’s input data.
    There are many common types of activation functions, and they tend to be quite
    simple. The only requirement of an activation function is that it’s differentiable,
    which enables us to use backpropagation to optimize parameters (we discuss this
    process shortly in “[Training Neural Networks](ch10.xhtml#lev172)” on [page 189](ch10.xhtml#page_189)).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步，应用一个*激活函数*于加权和加偏置的值。激活函数的目的是对加权和进行非线性变换，而加权和本身是神经元输入数据的*线性*变换。常见的激活函数类型有很多，并且它们通常都很简单。激活函数唯一的要求是可微分，这使我们可以利用反向传播来优化参数（我们将在
    “[训练神经网络](ch10.xhtml#lev172)” 中进一步讨论此过程，参见[第189页](ch10.xhtml#page_189)）。
- en: '[Table 10-1](ch10.xhtml#ch10tab1) shows a variety of other common activation
    functions and explains which ones tend to be good for which purposes.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[表10-1](ch10.xhtml#ch10tab1)展示了各种常见的激活函数，并解释了哪些激活函数适合用于哪些目的。'
- en: '**Table 10-1:** Common Activation Functions'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**表10-1：** 常见激活函数'
- en: '| **Name** | **Plot** | **Equation** | **Description** |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| **名称** | **图示** | **方程** | **描述** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Identity | ![image](../images/f0178-01.jpg) | *f*(*x*) = *x* | Basically:
    no activation function! |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| Identity | ![image](../images/f0178-01.jpg) | *f*(*x*) = *x* | 基本上：没有激活函数！
    |'
- en: '| ReLU | ![image](../images/f0178-02.jpg) | ![image](../images/f0178-03.jpg)
    | Just max(0, *x*).ReLUs enable fast learning and are more resilient to the vanishing
    gradient problem (explained later in this chapter) compared to other functions,
    like the sigmoid. |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| ReLU | ![image](../images/f0178-02.jpg) | ![image](../images/f0178-03.jpg)
    | 仅为max(0, *x*)。与其他激活函数（如sigmoid）相比，ReLU能够实现快速学习，并且在应对梯度消失问题（将在本章后面解释）时更加稳定。 |'
- en: '| Leaky ReLU | ![image](../images/f0179-01.jpg) | ![image](../images/f0179-02.jpg)
    | Like normal ReLU, but instead of 0, a small constant fraction of *x* is returned.
    Generally you choose *α* to be very small, like 0.01\. Also, *α* stays fixed during
    training. |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| Leaky ReLU | ![image](../images/f0179-01.jpg) | ![image](../images/f0179-02.jpg)
    | 类似于普通的ReLU，但返回的是一个小常数*α*与*x*的乘积，而不是0。通常你会选择*α*非常小，比如0.01。而且，*α*在训练过程中保持固定。 |'
- en: '| PReLU | ![image](../images/f0179-03.jpg) | ![image](../images/f0179-04.jpg)
    | This is just like leaky ReLU, but in PReLU, *α* is a parameter whose value is
    optimized during the training process, along with the standard weight and bias
    parameters. |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| PReLU | ![image](../images/f0179-03.jpg) | ![image](../images/f0179-04.jpg)
    | 这与Leaky ReLU类似，但在PReLU中，*α*是一个参数，其值会在训练过程中与标准的权重和偏置参数一起优化。 |'
- en: '| ELU | ![image](../images/f0179-05.jpg) | ![image](../images/f0179-06.jpg)
    | Like PReLU in that *α* is a parameter, but instead of going down infinitely
    with a slope of *α* when *x* < 0, the curve is bounded by *α*, because *e*^(*x*)
    will always be between 0 and 1 when *x* < 0. |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| ELU | ![image](../images/f0179-05.jpg) | ![image](../images/f0179-06.jpg)
    | 类似于PReLU，其中*α*是一个参数，但当* x * < 0时，曲线不是无限下降而是被* α *所限制，因为* e *^(*x*) 在 *x* < 0
    时始终介于0和1之间。 |'
- en: '| Step | ![image](../images/f0179-07.jpg) | ![image](../images/f0179-08.jpg)
    | Just a step function: the function returns 0 unless *x* ≤ 0, in which case the
    function returns 1. |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 步骤 | ![image](../images/f0179-07.jpg) | ![image](../images/f0179-08.jpg)
    | 仅仅是一个阶跃函数：该函数除非* x * ≤ 0，否则返回0；当* x * ≤ 0时，函数返回1。 |'
- en: '| Gaussian | ![image](../images/f0179-09.jpg) | *f*(*x*) = *e*^(*-x*²) | A
    bell-shaped curve whose maximum value tops out at 1 when *x* = 0. |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 高斯 | ![image](../images/f0179-09.jpg) | *f*(*x*) = *e*^(*-x*²) | 一条钟形曲线，当
    *x* = 0 时，最大值为1。 |'
- en: '| Sigmoid | ![image](../images/f0180-01.jpg) | ![image](../images/f0180-02.jpg)
    | Because of the vanishing gradient problem (explained later in this chapter),
    sigmoid activation functions are often only used in the final layer of a neural
    network. Because the output is continuous and bounded between 0 and 1, sigmoid
    neurons are a good proxy for output probabilities. |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| Sigmoid | ![image](../images/f0180-01.jpg) | ![image](../images/f0180-02.jpg)
    | 由于消失梯度问题（本章后面会解释），Sigmoid 激活函数通常只用于神经网络的最后一层。由于输出是连续的并且被限制在0和1之间，Sigmoid 神经元非常适合用作输出概率的代理。
    |'
- en: '| Softmax | (multi-output) | ![image](../images/f0180-03.jpg) | Outputs multiple
    values that sum to 1\. Softmax activation functions are often used in the final
    layer of a network to represent classification probabilities, because Softmax
    forces all outputs from a neuron to sum to 1. |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| Softmax | （多输出） | ![image](../images/f0180-03.jpg) | 输出多个加和为1的值。Softmax 激活函数通常用于网络的最后一层来表示分类概率，因为Softmax强制神经元的所有输出加和为1。
    |'
- en: '*Rectified linear unit (ReLU)* is by far the most common activation function
    used today, and it’s simply max(0, *s*). For example, let’s say your weighted
    sum plus bias value is called *s*. If *s* is above zero, then your neuron’s output
    is *s*, and if *s* is equal to or below zero, then your neuron’s output is 0\.
    You can express the entire function of a ReLU neuron as simply max(0, *weighted-sum-of-inputs*
    + *bias*), or more concretely, as the following for *n* inputs:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*修正线性单元（ReLU）* 迄今为止是最常用的激活函数，它就是max(0, *s*)。举个例子，假设你的加权和加偏置值叫做 *s*。如果 *s* 大于零，那么神经元的输出就是
    *s*；如果 *s* 小于或等于零，则神经元的输出为0。你可以简单地将ReLU神经元的整个函数表示为max(0, *加权和输入* + *偏置*)，或者更具体地说，对于
    *n* 个输入如下所示：'
- en: '![image](../images/f0180-04.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0180-04.jpg)'
- en: Nonlinear activation functions are actually a key reason why networks of such
    neurons are able to approximate any continuous function, which is a big reason
    why they’re so powerful. In the following sections, you learn how neurons are
    connected together to form a network, and later you’ll gain an understanding of
    why nonlinear activation functions are so important.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 非线性激活函数实际上是使得这种神经元网络能够逼近任何连续函数的关键原因，这也是它们如此强大的一个重要原因。在接下来的部分中，你将学习神经元如何连接在一起形成一个网络，随后你将理解为什么非线性激活函数如此重要。
- en: '***A Network of Neurons***'
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***神经元网络***'
- en: To create a neural network, you arrange neurons in a *directed graph* (a network)
    with a number of layers, connecting to form a much larger function. [Figure 10-2](ch10.xhtml#ch10fig2)
    shows an example of a small neural network.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个神经网络，你将神经元安排在一个*有向图*（一个网络）中，形成多个层级，连接起来构成一个更大的函数。[图 10-2](ch10.xhtml#ch10fig2)展示了一个小型神经网络的示例。
- en: '![image](../images/f0181-01.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0181-01.jpg)'
- en: '*Figure 10-2: Example of a very small, four-neuron neural network, where data
    is passed from neuron to neuron via the connections.*'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 10-2：一个非常小的四神经元神经网络示例，数据通过连接从神经元传递到神经元。*'
- en: 'In [Figure 10-2](ch10.xhtml#ch10fig2), we have our original inputs: *x*[1],
    *x*[2], and *x*[3] on the left side. Copies of these *x*[*i*] values are sent
    along the connections to each neuron in the *hidden layer* (a layer of neurons
    whose output is not the final output of the model), resulting in three output
    values, one from each neuron. Finally, each output of these three neurons is sent
    to a final neuron, which outputs the neural network’s final result.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 10-2](ch10.xhtml#ch10fig2)中，我们有原始输入：*x*[1]、*x*[2]和*x*[3]在左侧。这些*x*[*i*]值的副本沿着连接发送到每个神经元的*隐藏层*（一个神经元的层，其输出不是模型的最终输出），从而产生三个输出值，每个神经元一个。最后，这三个神经元的每个输出都被发送到最终神经元，该神经元输出神经网络的最终结果。
- en: Every connection in a neural network is associated with a *weight* parameter,
    *w*, and every neuron also contains a *bias* parameter, *b* (added to the weighted
    sum), so the total number of optimizable parameters in a basic neural network
    is the number of edges connecting an input to a neuron, plus the number of neurons.
    For example, in the network shown in [Figure 10-2](ch10.xhtml#ch10fig2), there
    are 4 total neurons, plus 9 + 3 edges, yielding a total of 16 optimizable parameters.
    Because this is just an example, we’re using a very small neural network—real
    neural networks often have thousands of neurons and millions of connections.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中的每个连接都与一个*权重*参数，*w*，相关联，每个神经元还包含一个*偏置*参数，*b*（加到加权和中），因此一个基本神经网络中可以优化的参数总数是连接输入到神经元的边的数量，再加上神经元的数量。例如，在[图
    10-2](ch10.xhtml#ch10fig2)中，总共有4个神经元，加上9 + 3条边，总共16个可优化参数。由于这是一个示例，我们使用了一个非常小的神经网络——实际的神经网络通常有成千上万个神经元和数百万条连接。
- en: '***Universal Approximation Theorem***'
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***通用逼近定理***'
- en: 'A striking aspect of neural networks is that they are *universal approximators*:
    given enough neurons, and the right weight and bias values, a neural network can
    emulate basically any type of behavior. The neural network shown in [Figure 10-2](ch10.xhtml#ch10fig2)
    is *feed-forward*, which means the data is always flowing forward (from left to
    right in the image).'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的一个显著特点是它们是*通用逼近器*：只要有足够的神经元，以及正确的权重和偏置值，神经网络几乎可以模拟任何类型的行为。在[图 10-2](ch10.xhtml#ch10fig2)中显示的神经网络是*前馈型*的，这意味着数据始终是向前流动的（在图像中从左到右）。
- en: The *universal approximation theorem* describes the concept of universality
    more formally. It states that a feed-forward network with a single hidden layer
    of neurons with nonlinear activation functions can approximate (with an arbitrarily
    small error) any continuous function on a compact subset of **R**^(**n**).[¹](footnote.xhtml#ch10fn1)
    That’s a bit of a mouthful, but it just means that with enough neurons, a neural
    network can *very* closely approximate any continuous, bounded function with a
    finite number of inputs and outputs.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*通用逼近定理*更正式地描述了普遍性概念。它声明，一个具有单个隐藏层的前馈网络（该层的神经元具有非线性激活函数）可以逼近（具有任意小的误差）**R**^(**n**)的任何连续函数。[¹](footnote.xhtml#ch10fn1)
    这有点复杂，但它的意思是，通过足够的神经元，神经网络可以*非常*精确地逼近任何具有有限输入和输出的连续有界函数。'
- en: In other words, the theorem states that regardless of the function we want to
    approximate, there’s theoretically some neural network with the right parameters
    that can do the job. For example, if you draw a squiggly, continuous function,
    *f*(*x*), like in [Figure 10-3](ch10.xhtml#ch10fig3), there exists some neural
    network such that for every possible input of *x*, *f*(*x*) ≈ network(*x*), no
    matter how complicated the function *f*(*x*). This is one reason neural networks
    can be so powerful.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，定理指出，不管我们想要逼近什么函数，理论上总有一个神经网络，具备正确的参数，可以完成这个任务。例如，如果你画一个波动的连续函数，*f*(*x*)，就像在[图
    10-3](ch10.xhtml#ch10fig3)中那样，存在某个神经网络，对于每一个可能的*x*输入，*f*(*x*) ≈ 网络(*x*)，无论*f*(*x*)多么复杂。这也是神经网络如此强大的原因之一。
- en: '![image](../images/f0182-01.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0182-01.jpg)'
- en: '*Figure 10-3: Example of how a small neural net could approximate a funky function.
    As the number of neurons grows, the difference between* y *and ŷ will approach
    0.*'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 10-3：小型神经网络如何逼近复杂函数的示例。随着神经元数量的增加，*y*与ŷ之间的差异将接近0。*'
- en: In the next sections, we build a simple neural network by hand to help you understand
    how and why we can model such different types of behavior, given the right parameters.
    Although we do this on a very small scale using just a single input and output,
    the same principle holds true when you’re dealing with multiple inputs and outputs,
    and incredibly complex behaviors.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们手动构建一个简单的神经网络，以帮助你理解在给定正确参数的情况下，如何以及为什么我们可以建模如此不同的行为。虽然我们仅使用单一的输入和输出进行非常小规模的操作，但同样的原理适用于处理多个输入和输出，以及极其复杂的行为。
- en: '***Building Your Own Neural Network***'
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***构建你自己的神经网络***'
- en: To see this universality in action, let’s try building our own neural network.
    We start with two ReLU neurons, using a single input *x*, as shown in [Figure
    10-4](ch10.xhtml#ch10fig4). Then, we see how different weight and bias values
    (parameters) can be used to model different functions and outcomes.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看到这种普遍性，让我们尝试构建我们自己的神经网络。我们从两个 ReLU 神经元开始，使用单一的输入 *x*，如[图 10-4](ch10.xhtml#ch10fig4)所示。然后，我们看看不同的权重和偏置值（参数）如何用于建模不同的函数和结果。
- en: '![image](../images/f0182-02.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0182-02.jpg)'
- en: '*Figure 10-4: Visualization of two neurons being fed input data* x'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 10-4：两个神经元输入数据 x 的可视化*'
- en: Here, both neurons have a weight of 1, and both use a ReLU activation function.
    The only difference between the two is that neuron[1] applies a bias value of
    –1, while neuron[2] applies a bias value of –2\. Let’s see what happens when we
    feed neuron[1] a few different values of *x*. [Table 10-2](ch10.xhtml#ch10tab2)
    summarizes the results.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，两个神经元的权重都是 1，并且都使用 ReLU 激活函数。它们之间的唯一区别是神经元[1]应用了一个偏置值 –1，而神经元[2]应用了偏置值 –2。让我们看看当我们向神经元[1]输入不同的
    *x* 值时会发生什么。[表格 10-2](ch10.xhtml#ch10tab2) 总结了结果。
- en: '**Table 10-2:** Neuron[1]'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**表格 10-2：** 神经元[1]'
- en: '| **Input** | **Weighted sum** | **Weighted sum + bias** | **Output** |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| **输入** | **加权和** | **加权和 + 偏置** | **输出** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| *x* | *x** *w*[*x*→1] | *x** *w*[*x*→1] + bias[1] | max(0, *x** *w*[*x*→1]
    + bias[1]) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| *x* | *x** *w*[*x*→1] | *x** *w*[*x*→1] + bias[1] | max(0, *x** *w*[*x*→1]
    + bias[1]) |'
- en: '| --- | --- | --- | --- |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 0 | 0 * 1 = 0 | 0 + –1 = –1 | max(0, –1) = 0 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 * 1 = 0 | 0 + –1 = –1 | max(0, –1) = 0 |'
- en: '| 1 | 1 * 1 = 1 | 1 + –1 = 0 | max(0, 0) = 0 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 * 1 = 1 | 1 + –1 = 0 | max(0, 0) = 0 |'
- en: '| 2 | 2 * 1 = 2 | 2 + –1 = 1 | max(0, 1) = 1 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 2 * 1 = 2 | 2 + –1 = 1 | max(0, 1) = 1 |'
- en: '| 3 | 3 * 1 = 3 | 3 + –1 = 2 | max(0, 2) = 2 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 3 * 1 = 3 | 3 + –1 = 2 | max(0, 2) = 2 |'
- en: '| 4 | 4 * 1 = 4 | 4 + –1 = 3 | max(0, 3) = 3 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 4 * 1 = 4 | 4 + –1 = 3 | max(0, 3) = 3 |'
- en: '| 5 | 5 * 1 = 5 | 5 + –1 = 4 | max(0, 4) = 4 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 5 * 1 = 5 | 5 + –1 = 4 | max(0, 4) = 4 |'
- en: The first column shows some sample inputs for *x*, and the second shows the
    resulting weighted sum. The third column adds the bias parameter, and the fourth
    column applies the ReLU activation function to yield the neuron’s output for a
    given input of *x*. [Figure 10-5](ch10.xhtml#ch10fig5) shows the graph of the
    neuron[1] function.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 第一列展示了一些 *x* 的输入示例，第二列展示了相应的加权和。第三列加上了偏置参数，第四列应用 ReLU 激活函数，得到给定 *x* 输入时神经元的输出。[图
    10-5](ch10.xhtml#ch10fig5) 展示了神经元[1]的函数图。
- en: '![image](../images/f0183-01.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0183-01.jpg)'
- en: '*Figure 10-5: Visualization of neuron[1] as a function. The x-axis represents
    the neuron’s single input value, and the y-axis represents the neuron’s output.*'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 10-5：神经元[1]作为函数的可视化。x轴表示神经元的单一输入值，y轴表示神经元的输出。*'
- en: 'Because neuron[1] has a bias of –1, the output of neuron[1] stays at 0 until
    the weighted sum goes above 1, and then it goes up with a certain slope, as you
    can see in [Figure 10-5](ch10.xhtml#ch10fig5). That slope of 1 is associated with
    the *w*[*x*→1] weight value of 1\. Think about what would happen with a weight
    of 2: because the weighted sum value would double, the angle in [Figure 10-5](ch10.xhtml#ch10fig5)
    would occur at *x* = 0.5 instead of *x* = 1, and the line would go up with a slope
    of 2 instead of 1.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 由于神经元[1]的偏置是 –1，神经元[1]的输出始终为 0，直到加权和超过 1，然后以一定的斜率上升，正如你在[图 10-5](ch10.xhtml#ch10fig5)中看到的那样。斜率为
    1，这与 *w*[*x*→1] 的权重值 1 相关。想象一下如果权重是 2 会发生什么：因为加权和值会翻倍，[图 10-5](ch10.xhtml#ch10fig5)
    中的角度会出现在 *x* = 0.5，而不是 *x* = 1，直线的斜率将变为 2，而不是 1。
- en: Now let’s look at neuron[2], which has a bias value of –2 (see [Table 10-3](ch10.xhtml#ch10tab3)).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看神经元[2]，它的偏置值为 –2（见[表格 10-3](ch10.xhtml#ch10tab3)）。
- en: '**Table 10-3:** Neuron[2]'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**表格 10-3：** 神经元[2]'
- en: '| **Input** | **Weighted sum** | **Weighted sum + bias** | **Output** |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| **输入** | **加权和** | **加权和 + 偏置** | **输出** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| *x* | *x** *w*[*x*→2] | *x** *w*[*x*→2] + bias[2] | max(0, *x** *w*[*x*→2])
    + bias[2]) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| *x* | *x** *w*[*x*→2] | *x** *w*[*x*→2] + bias[2] | max(0, *x** *w*[*x*→2])
    + bias[2]) |'
- en: '| --- | --- | --- | --- |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 0 | 0 * 1 = 0 | 0 + –2 = –2 | max(0, –2) = 0 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 * 1 = 0 | 0 + –2 = –2 | max(0, –2) = 0 |'
- en: '| 1 | 1 * 1 = 1 | 1 + –2 = –1 | max(0, –1) = 0 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 * 1 = 1 | 1 + –2 = –1 | max(0, –1) = 0 |'
- en: '| 2 | 2 * 1 = 2 | 2 + –2 = 0 | max(0, 0) = 0 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 2 * 1 = 2 | 2 + –2 = 0 | max(0, 0) = 0 |'
- en: '| 3 | 3 * 1 = 3 | 3 + –2 = 1 | max(0, 1) = 1 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 3 * 1 = 3 | 3 + –2 = 1 | max(0, 1) = 1 |'
- en: '| 4 | 4 * 1 = 4 | 4 + –2 = 2 | max(0, 2) = 2 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 4 * 1 = 4 | 4 + –2 = 2 | max(0, 2) = 2 |'
- en: '| 5 | 5 * 1 = 5 | 5 + –2 = 3 | max(0, 3) = 3 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 5 * 1 = 5 | 5 + –2 = 3 | max(0, 3) = 3 |'
- en: Because neuron[2]’s bias is –2, the angle in [Figure 10-6](ch10.xhtml#ch10fig6)
    occurs at *x* = 2 instead of *x* = 1.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 因为neuron[2]的偏置是–2，所以[图10-6](ch10.xhtml#ch10fig6)中的角度出现在*x* = 2而不是*x* = 1。
- en: '![image](../images/f0184-01.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0184-01.jpg)'
- en: '*Figure 10-6: Visualization of neuron[2] as a function*'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '*图10-6：neuron[2]作为函数的可视化*'
- en: So now we’ve built two very simple functions (neurons), both doing nothing over
    a set period, then going up infinitely with a slope of 1\. Because we’re using
    ReLU neurons, the slope of each neuron’s function is affected by its weights,
    while its bias and weight terms both affect where the slope begins. When you use
    other activation functions, similar rules apply. By adjusting parameters, we could
    change the angle and slope of each neuron’s function however we wanted.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 所以现在我们已经构建了两个非常简单的函数（神经元），它们在一段时间内什么都不做，然后以斜率1无限增长。因为我们使用的是ReLU神经元，每个神经元的函数斜率会受到其权重的影响，而其偏置和权重项则决定了斜率的起点。当使用其他激活函数时，也会遵循类似的规则。通过调整参数，我们可以随意改变每个神经元函数的角度和斜率。
- en: In order to achieve universality, however, we need to combine neurons together,
    which will allow us to approximate more complex functions. Let’s connect our two
    neurons up to a third neuron, as shown in [Figure 10-7](ch10.xhtml#ch10fig7).
    This will create a small three-neuron network with a single hidden layer, composed
    of neuron[1] and neuron[2].
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为了实现普适性，我们需要将神经元组合在一起，这将使我们能够逼近更复杂的函数。让我们将两个神经元连接到第三个神经元，如[图10-7](ch10.xhtml#ch10fig7)所示。这将创建一个由neuron[1]和neuron[2]组成的小型三神经元网络，并包含一个隐藏层。
- en: In [Figure 10-7](ch10.xhtml#ch10fig7), input data *x* is sent to both neuron[1]
    and neuron[2]. Then, neuron[1] and neuron[2]’s outputs are sent as inputs to neuron[3],
    which yields the network’s final output.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图10-7](ch10.xhtml#ch10fig7)中，输入数据*x*被发送到neuron[1]和neuron[2]。然后，neuron[1]和neuron[2]的输出作为输入传送到neuron[3]，最终得出网络的输出结果。
- en: '![image](../images/f0185-01.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0185-01.jpg)'
- en: '*Figure 10-7: Visualization of a small three-neuron network*'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*图10-7：小型三神经元网络的可视化*'
- en: If you inspect the weights in [Figure 10-7](ch10.xhtml#ch10fig7), you’ll notice
    that the weight *w*[1→3] is 2, doubling neuron[1]’s contribution to neuron[3].
    Meanwhile, *w*[2→3] is –1, inverting neuron[2]’s contribution. In essence, neuron[3]
    is simply applying its activation function to neuron[1] * 2 – neuron[2]. [Table
    10-4](ch10.xhtml#ch10tab4) summarizes the inputs and corresponding outputs for
    the resulting network.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你检查[图10-7](ch10.xhtml#ch10fig7)中的权重，你会发现权重 *w*[1→3] 是2，意味着neuron[1]对neuron[3]的贡献被放大了两倍。同时，*w*[2→3]
    是–1，表示neuron[2]的贡献被反转。本质上，neuron[3]只是在将其激活函数应用于 neuron[1] * 2 – neuron[2]。 [表10-4](ch10.xhtml#ch10tab4)总结了该网络的输入和相应的输出。
- en: '**Table 10-4:** A Three-Neuron Network'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**表10-4：三神经元网络**'
- en: '| **Original network input** | **Inputs to neuron[3]** | **Weighted sum** |
    **Weighted sum + bias** | **Final network output** |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| **原始网络输入** | **输入到neuron[3]的值** | **加权和** | **加权和 + 偏置** | **最终网络输出** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| *x* | neuron[1] | neuron[2] | (neuron[1] * *w*[1→3]) + (neuron[2] * *w*[2→3])
    | (neuron[1] * *w*[1→3]) + (neuron[2] * *w*[2→3]) + bias[3] | max(0, (neuron[1]
    * *w*[1→3]) + (neuron[2] * *w*[2→3]) + bias[3]) |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| *x* | neuron[1] | neuron[2] | (neuron[1] * *w*[1→3]) + (neuron[2] * *w*[2→3])
    | (neuron[1] * *w*[1→3]) + (neuron[2] * *w*[2→3]) + bias[3] | max(0, (neuron[1]
    * *w*[1→3]) + (neuron[2] * *w*[2→3]) + bias[3]) |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 0 | 0 | 0 | (0 * 2) + (0 * –1) = 0 | 0 + 0 + 0 = 0 | max(0, 0) = 0 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 0 | (0 * 2) + (0 * –1) = 0 | 0 + 0 + 0 = 0 | max(0, 0) = 0 |'
- en: '| 1 | 0 | 0 | (0 * 2) + (0 * –1) = 0 | 0 + 0 + 0 = 0 | max(0, 0) = 0 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 0 | (0 * 2) + (0 * –1) = 0 | 0 + 0 + 0 = 0 | max(0, 0) = 0 |'
- en: '| 2 | 1 | 0 | (1 * 2) + (0 * –1) = 2 | 2 + 0 + 0 = 2 | max(0, 2) = 2 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1 | 0 | (1 * 2) + (0 * –1) = 2 | 2 + 0 + 0 = 2 | max(0, 2) = 2 |'
- en: '| 3 | 2 | 1 | (2 * 2) + (1 * –1) = 3 | 4 + –1 + 0 = 3 | max(0, 3) = 3 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 2 | 1 | (2 * 2) + (1 * –1) = 3 | 4 + –1 + 0 = 3 | max(0, 3) = 3 |'
- en: '| 4 | 3 | 2 | (3 * 2) + (2 * –1) = 4 | 6 + –2 + 0 = 4 | max(0, 4) = 4 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 3 | 2 | (3 * 2) + (2 * –1) = 4 | 6 + –2 + 0 = 4 | max(0, 4) = 4 |'
- en: '| 5 | 4 | 3 | (4 * 2) + (3 * –1) = 5 | 8 + –3 + 0 = 5 | max(0, 5) = 5 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 4 | 3 | (4 * 2) + (3 * –1) = 5 | 8 + –3 + 0 = 5 | max(0, 5) = 5 |'
- en: 'The first column shows original network input, *x*, followed by the resulting
    outputs of neuron[1] and neuron[2]. The rest of the columns show how neuron[3]
    processes the outputs: the weighted sum is calculated, bias is added, and finally
    in the last column the ReLU activation function is applied to achieve the neuron
    and network outputs for each original input value for *x*. [Figure 10-8](ch10.xhtml#ch10fig8)
    shows the network’s function graph.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 第一列显示原始网络输入 *x*，随后是神经元[1]和神经元[2]的输出。其余列显示神经元[3]如何处理输出：计算加权和，加入偏置，最后在最后一列应用ReLU激活函数，从而得出每个原始输入值
    *x* 的神经元和网络输出。[图 10-8](ch10.xhtml#ch10fig8) 显示了网络的功能图。
- en: '![image](../images/f0186-01.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0186-01.jpg)'
- en: '*Figure 10-8: Visualization of our network’s inputs and associated outputs*'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 10-8：我们的网络输入和相应输出的可视化*'
- en: We can see that through the combination of these simple functions, we can create
    a graph that goes up for any period or slope desired over different points, as
    we did in [Figure 10-8](ch10.xhtml#ch10fig8). In other words, we’re much closer
    to being able to represent any finite function for our input *x*!
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，通过这些简单函数的组合，我们可以创建一个图形，使其在不同点上具有任意的上升斜率或周期，正如我们在[图 10-8](ch10.xhtml#ch10fig8)中所做的那样。换句话说，我们离能够为我们的输入
    *x* 表示任何有限函数的目标更近了！
- en: '***Adding Another Neuron to the Network***'
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***向网络中添加另一个神经元***'
- en: We’ve seen how to make our network’s function’s graph go up (with any slope)
    by adding neurons, but how would we make the graph go down? Let’s add another
    neuron (neuron[4]) to the mix, as shown in [Figure 10-9](ch10.xhtml#ch10fig9).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到如何通过添加神经元使网络的功能图上升（具有任意斜率），但我们如何让图形下降呢？让我们向网络中添加另一个神经元（神经元[4]），如[图 10-9](ch10.xhtml#ch10fig9)所示。
- en: '![image](../images/f0186-02.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0186-02.jpg)'
- en: '*Figure 10-9: Visualization of a small four-neuron network with a single hidden
    layer*'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 10-9：带有单个隐藏层的小型四神经元网络的可视化*'
- en: In [Figure 10-9](ch10.xhtml#ch10fig9), input data *x* is sent to neuron[1],
    neuron[2], and neuron[4]. Their outputs are then fed as inputs to neuron[3], which
    yields the network’s final output. Neuron[4] is the same as neuron[1] and neuron[2],
    but with its bias set to –4\. [Table 10-5](ch10.xhtml#ch10tab5) summarizes the
    output of neuron[4].
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 10-9](ch10.xhtml#ch10fig9)中，输入数据 *x* 被传送到神经元[1]、神经元[2] 和神经元[4]。它们的输出随后作为输入传递给神经元[3]，最终产生网络的最终输出。神经元[4]与神经元[1]和神经元[2]相同，但其偏置被设置为–4。[表
    10-5](ch10.xhtml#ch10tab5) 总结了神经元[4]的输出。
- en: '**Table 10-5:** Neuron[4]'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 10-5：神经元[4]**'
- en: '| **Input** | **Weighted sum** | **Weighted sum + bias** | **Output** |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| **输入** | **加权和** | **加权和 + 偏置** | **输出** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| *x* | *x* * *w*[*x*→4] | (*x* * *w*[*x*→4]) + bias[4] | max(0, (*x* * *w*[*x*→4])
    + bias[4]) |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| *x* | *x* * *w*[*x*→4] | (*x* * *w*[*x*→4]) + 偏置[4] | max(0, (*x* * *w*[*x*→4])
    + 偏置[4]) |'
- en: '| --- | --- | --- | --- |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 0 | 0 * 1 = 0 | 0 + –4 = –4 | max(0, –4) = 0 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 * 1 = 0 | 0 + –4 = –4 | max(0, –4) = 0 |'
- en: '| 1 | 1 * 1 = 1 | 1 + –4 = –3 | max(0, –3) = 0 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 * 1 = 1 | 1 + –4 = –3 | max(0, –3) = 0 |'
- en: '| 2 | 2 * 1 = 2 | 2 + –4 = –2 | max(0, –2) = 0 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 2 * 1 = 2 | 2 + –4 = –2 | max(0, –2) = 0 |'
- en: '| 3 | 3 * 1 = 3 | 3 + –4 = –1 | max(0, –1) = 0 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 3 * 1 = 3 | 3 + –4 = –1 | max(0, –1) = 0 |'
- en: '| 4 | 4 * 1 = 4 | 4 + –4 = 0 | max(0, 0) = 0 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 4 * 1 = 4 | 4 + –4 = 0 | max(0, 0) = 0 |'
- en: '| 5 | 5 * 1 = 5 | 5 + –4 = 1 | max(0, 1) = 1 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 5 * 1 = 5 | 5 + –4 = 1 | max(0, 1) = 1 |'
- en: To make our network graph descend, we subtract neuron[4]’s function from that
    of neuron[1] and neuron[2] in neuron[3]’s weighted sum by setting the weight connecting
    neuron[4] to neuron[3] to –2\. [Table 10-6](ch10.xhtml#ch10tab6) shows the new
    output of the entire network.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让我们的网络图形下降，我们通过将连接神经元[4]到神经元[3]的权重设置为–2，从神经元[1]和神经元[2]的函数中减去神经元[4]的函数，来调整神经元[3]的加权和。[表
    10-6](ch10.xhtml#ch10tab6) 显示了整个网络的新输出。
- en: '**Table 10-6:** A Four-Neuron Network'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 10-6：四神经元网络**'
- en: '| **Original network input** | **Inputs to neuron[3]** | **Weighted sum** |
    **Weighted sum + bias** | **Final network output** |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| **原始网络输入** | **神经元[3]的输入** | **加权和** | **加权和 + 偏置** | **最终网络输出** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| *x* | neuron[1] | neuron[2] | neuron[4] | (neuron[1] * *w*[1→3]) + (neuron[2]
    * *w*[2→3]) + (neuron[4] * *w*[4→3]) | (neuron[1] * *w*[1→3]) + (neuron[2] * *w*[2→3])
    + (neuron[4] * *w*[4→3]) + bias[3] | max(0, (neuron[1] * *w*[1→3]) + (neuron[2]
    * *w*[2→3]) + (neuron[4] * *w*[4→3]) + bias[3]) |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| *x* | 神经元[1] | 神经元[2] | 神经元[4] | (神经元[1] * *w*[1→3]) + (神经元[2] * *w*[2→3])
    + (神经元[4] * *w*[4→3]) | (神经元[1] * *w*[1→3]) + (神经元[2] * *w*[2→3]) + (神经元[4] *
    *w*[4→3]) +偏置[3] | max(0, (神经元[1] * *w*[1→3]) + (神经元[2] * *w*[2→3]) + (神经元[4]
    * *w*[4→3]) + 偏置[3]) |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 0 | 0 | 0 | 0 | (0 * 2) + (0 * –1) + (0 * –2) = 0 | 0 + 0 + 0 + 0 = 0 | max(0,
    0) = 0 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 0 | 0 | (0 * 2) + (0 * –1) + (0 * –2) = 0 | 0 + 0 + 0 + 0 = 0 | max(0,
    0) = 0 |'
- en: '| 1 | 0 | 0 | 0 | (0 * 2) + (0 * –1) + (0 * –2) = 0 | 0 + 0 + 0 + 0 = 0 | max
    (0, 0) = 1 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 0 | 0 | (0 * 2) + (0 * –1) + (0 * –2) = 0 | 0 + 0 + 0 + 0 = 0 | max
    (0, 0) = 1 |'
- en: '| 2 | 1 | 0 | 0 | (1 * 2) + (0 * –1) + (0 * –2) = 2 | 2 + 0 + 0 + 0 = 2 | max
    (0, 2) = 2 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1 | 0 | 0 | (1 * 2) + (0 * –1) + (0 * –2) = 2 | 2 + 0 + 0 + 0 = 2 | max
    (0, 2) = 2 |'
- en: '| 3 | 2 | 1 | 0 | (2 * 2) + (1 * –1) + (0 * –2) = 3 | 4 + –1 + 0 + 0 = 3 |
    max (0, 3) = 3 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 2 | 1 | 0 | (2 * 2) + (1 * –1) + (0 * –2) = 3 | 4 + –1 + 0 + 0 = 3 |
    max (0, 3) = 3 |'
- en: '| 4 | 3 | 2 | 0 | (3 * 2) + (2 * –1) + (0 * –2) = 4 | 6 + –2 + 0 + 0 = 4 |
    max (0, 4) = 4 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 3 | 2 | 0 | (3 * 2) + (2 * –1) + (0 * –2) = 4 | 6 + –2 + 0 + 0 = 4 |
    max (0, 4) = 4 |'
- en: '| 5 | 4 | 3 | 1 | (4 * 2) + (3 * –1) + (1 * –2) = 5 | 8 + –3 + –2 + 0 = 3 |
    max (0, 3) = 3 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 4 | 3 | 1 | (4 * 2) + (3 * –1) + (1 * –2) = 5 | 8 + –3 + –2 + 0 = 3 |
    max (0, 3) = 3 |'
- en: '[Figure 10-10](ch10.xhtml#ch10fig10) shows what this looks like.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-10](ch10.xhtml#ch10fig10) 显示了这一过程的具体样子。'
- en: '![image](../images/f0188-01.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0188-01.jpg)'
- en: '*Figure 10-10: Visualization of our four-neuron network*'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 10-10：我们的四神经元网络的可视化*'
- en: Hopefully, now you can see how the neural network architecture allows us to
    move up and down at any rate over any points on the graph, just by combining a
    number of simple neurons (universality!). We could continue adding more neurons
    to create far more sophisticated functions.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 希望现在你能看到神经网络架构是如何通过结合多个简单的神经元（普适性！）来使我们在图表上的任何点上以任意速率上下移动的。我们可以继续添加更多神经元，以创建更加复杂的功能。
- en: '***Automatic Feature Generation***'
  id: totrans-139
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***自动特征生成***'
- en: You’ve learned that a neural network with a single hidden layer can approximate
    any finite function with enough neurons. That’s a pretty powerful idea. But what
    happens when we have multiple hidden layers of neurons? In short, automatic feature
    generation happens, which is perhaps an even more powerful aspect of neural networks.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经学到，具有单个隐藏层的神经网络可以通过足够的神经元来逼近任何有限的函数。这是一个相当强大的想法。但如果我们有多个隐藏层的神经元会怎样呢？简而言之，自动特征生成发生了，这可能是神经网络的一个更强大的方面。
- en: Historically, a big part of the process of building machine learning models
    was feature extraction. For an HTML file, a lot of time would be spent deciding
    what numeric aspects of an HTML file (number of section headers, number of unique
    words, and so on) might aid the model.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 历史上，构建机器学习模型的一个重要部分是特征提取。对于 HTML 文件来说，通常会花费大量时间来决定 HTML 文件的哪些数字特征（例如章节标题的数量、唯一单词的数量等等）可能有助于模型。
- en: Neural networks with multiple layers and automatic feature generation allow
    us to offload a lot of that work. In general, if you give fairly raw features
    (such as characters or words in an HTML file) to a neural network, each layer
    of neurons can learn to represent those raw features in ways that work well as
    inputs to later layers. In other words, a neural network will learn to count the
    number of times the letter *a* shows up in an HTML document, if that’s particularly
    relevant to detecting malware, with no real input from a human saying that it
    is or isn’t.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有多个层和自动特征生成的神经网络使我们能够卸载大量的工作。一般来说，如果你将相对原始的特征（例如 HTML 文件中的字符或单词）提供给神经网络，每一层神经元都可以学习以适当的方式表示这些原始特征，这些表示将作为后续层的输入。换句话说，神经网络将学会统计字母*a*在
    HTML 文档中出现的次数，如果这对检测恶意软件特别相关，即使没有人类明确指出它是否相关。
- en: In our image-processing bicycle example, nobody specifically told the network
    that edges or wheel meta-features were useful. The model learned that those features
    were useful as inputs to the next neuron layer during the training process. What’s
    especially useful is that these lower-level learned features can be used in different
    ways by later layers, which means that deep neural networks can estimate many
    incredibly complex patterns using far fewer neurons and parameters than a single-layered
    network could.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的图像处理自行车示例中，没人特别告诉网络边缘或车轮的元特征是有用的。模型在训练过程中学会了这些特征作为输入传递到下一个神经元层时的有用性。特别有用的是，这些低级的学习到的特征可以被后续层以不同的方式使用，这意味着深度神经网络可以使用比单层网络更少的神经元和参数来估计许多极其复杂的模式。
- en: Not only do neural networks perform a lot of the feature extraction work that
    previously took a lot of time and effort, they do it in an optimized and space-efficient
    way, guided by the training process.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络不仅完成了以前需要大量时间和精力的特征提取工作，而且它们以优化和节省空间的方式完成这些工作，且由训练过程引导。
- en: '**Training Neural Networks**'
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**训练神经网络**'
- en: So far, we’ve explored how, given a large number of neurons and the right weights
    and bias terms, a neural network can approximate complex functions. In all our
    examples so far, we set those weight and bias parameters manually. However, because
    real neural networks normally contain thousands of neurons and millions of parameters,
    we need an efficient way to optimize these values.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经探讨了如何在给定大量神经元以及合适的权重和偏置项的情况下，神经网络可以逼近复杂的函数。在我们迄今为止的所有示例中，我们手动设置了这些权重和偏置参数。然而，由于真实的神经网络通常包含成千上万的神经元和数百万个参数，我们需要一种高效的方式来优化这些值。
- en: Normally, when training a model, we start with a training dataset and a network
    with a bunch of non-optimized (randomly initialized) parameters. Training requires
    optimizing parameters to minimize an objective function. In supervised learning,
    where we’re trying to train our model to be able to predict a label, like 0 for
    “benign” and 1 for “malware,” that *objective function* is going to be related
    to the network’s prediction error during training. For some given input *x* (for
    example, a specific HTML file), this is the difference between the label *y* we
    know is correct (for example, 1.0 for “is malware”) and the output *ŷ* we get
    from the current network (for example, 0.7). You can think of the error as the
    difference between the predicted label *ŷ* and the known, true label *y*, where
    network (*x*) = *ŷ*, and the network is trying to approximate some unknown function
    *f*, such that *f*(*x*) = *y*. In other words, network = ![images](../images/fcap.jpg).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在训练一个模型时，我们从一个训练数据集和一个拥有一堆未优化（随机初始化）参数的网络开始。训练需要优化参数，以最小化目标函数。在监督学习中，我们试图训练模型以预测标签，比如
    0 代表“良性”而 1 代表“恶意软件”，这个*目标函数*将与网络在训练过程中的预测误差相关。对于某个给定的输入 *x*（例如，特定的 HTML 文件），这是我们知道的正确标签
    *y*（例如，1.0 代表“是恶意软件”）和我们从当前网络得到的输出 *ŷ*（例如，0.7）之间的差异。你可以把误差看作是预测标签 *ŷ* 和已知的真实标签
    *y* 之间的差异，其中网络 (*x*) = *ŷ*，而网络试图逼近某个未知的函数 *f*，使得 *f*(*x*) = *y*。换句话说，网络 = ![images](../images/fcap.jpg)。
- en: The basic idea behind training networks is to feed a network an observation,
    *x*, from your training dataset, receive some output, *ŷ* , and then figure out
    how changing your parameters will shift *ŷ* closer to your goal, *y*. Imagine
    you’re in a spaceship with various knobs. You don’t know what each knob does,
    but you know the direction you want to go in (*y*). To solve the problem, you
    step on the gas and note the direction you went (*ŷ* ). Then, you turn a knob
    just a *tiny* bit and step on the gas again. The difference between your first
    and second directions tells you how much that knob affects your direction. In
    this way, you can eventually figure out how to fly the spaceship quite well.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 训练网络的基本思想是将一个来自训练数据集的观测值 *x* 输入网络，接收一个输出 *ŷ*，然后找出如何改变参数能使 *ŷ* 更接近你的目标 *y*。想象你在一个宇宙飞船中，飞船上有各种旋钮。你不知道每个旋钮的作用，但你知道你想去的方向是
    (*y*)。为了解决这个问题，你踩下油门并记录你行进的方向 (*ŷ*)。然后，你稍微调节一个旋钮，再次踩下油门。你第一次和第二次的方向差异告诉你那个旋钮对方向的影响有多大。通过这种方式，你最终可以学会如何非常好地驾驶飞船。
- en: Training a neural network is similar. First, you feed a network an observation,
    *x*, from your training dataset, and you receive some output, *ŷ* . This step
    is called *forward propagation* because you feed your input *x* forward through
    the network to get your final output *ŷ* . Next, you determine how each parameter
    affects your output *ŷ* . For example, if your network’s output is 0.7, but you
    know the correct output should be closer to 1, you can try increasing a parameter,
    *w*, just a little bit, seeing whether *ŷ* gets closer to or further away from
    *y*, and by how much.[²](footnote.xhtml#ch10fn2) This is called the partial derivative
    of *ŷ* with respect to *w*, or *∂ŷ/∂w*.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络类似。首先，你将一个来自训练数据集的观测值，*x*，输入网络，并接收到某些输出，*ŷ*。这一步骤被称为*前向传播*，因为你将输入 *x* 向前传递通过网络，得到最终的输出
    *ŷ*。接下来，你需要确定每个参数如何影响你的输出 *ŷ*。例如，如果你的网络输出是 0.7，但你知道正确的输出应该更接近 1，你可以尝试稍微增加一个参数，*w*，看看
    *ŷ* 是接近还是远离 *y*，并且相差多少。[²](footnote.xhtml#ch10fn2) 这就叫做 *ŷ* 关于 *w* 的偏导数，或者 *∂ŷ/∂w*。
- en: Parameters all throughout the network are then nudged just a *tiny* bit in a
    direction that causes *ŷ* to shift a little closer to *y* (and therefore network
    closer to *f* ). If *∂ŷ/∂w* is positive, then you know you should increase *w*
    by a small amount (specifically, proportional to *∂*(*y* – *ŷ*)/*∂w*), so that
    your new *ŷ* will move slightly away from 0.7 and toward 1 (*y*). In other words,
    you teach your network to approximate the *unknown* function *f* by correcting
    its mistakes on training data with *known* labels.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 网络中的所有参数随后都会在一个方向上微调一个 *极小* 的量，使得 *ŷ* 稍微接近 *y*（从而网络更接近 *f*）。如果 *∂ŷ/∂w* 为正，则说明你应该增加
    *w* 一个小量（具体来说，比例为 *∂*(*y* – *ŷ*)/*∂w*），以使新的 *ŷ* 从 0.7 稍微远离，接近 1（*y*）。换句话说，你通过纠正训练数据中带标签的错误，教会你的网络近似
    *未知* 函数 *f*。
- en: The process of iteratively calculating these partial derivatives, updating parameters,
    and then repeating is called *gradient descent.* However, with a network of thousands
    of neurons, millions of parameters, and often millions of training observations,
    all of that calculus requires a lot of computation. To get around this, we use
    a neat algorithm called *backpropagation* that makes these calculations computationally
    feasible. At its core, backpropagation allows us to efficiently calculate partial
    derivatives along computational graphs like a neural network!
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 反复计算这些偏导数、更新参数，然后重复这一过程的过程称为 *梯度下降*。然而，对于包含成千上万神经元、数百万参数和通常数百万训练样本的网络来说，所有这些微积分运算需要大量计算。为了绕开这个问题，我们使用一个巧妙的算法，称为
    *反向传播*，它使得这些计算在计算上可行。反向传播的核心是，它允许我们高效地沿着计算图（如神经网络）计算偏导数！
- en: '***Using Backpropagation to Optimize a Neural Network***'
  id: totrans-152
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***使用反向传播优化神经网络***'
- en: In this section, we construct a simple neural network to showcase how backpropagation
    works. Let’s assume that we have a training example whose value is *x* = 2 and
    an associated true label of *y* = 10\. Usually, *x* would be an array of many
    values, but let’s stick to a single value to keep things simple. Plugging in these
    values, we can see in [Figure 10-11](ch10.xhtml#ch10fig11) that our network outputs
    a *ŷ* value of 5 with an input *x* value of 2.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们构建了一个简单的神经网络，展示反向传播是如何工作的。假设我们有一个训练样本，其值为 *x* = 2，并且其真实标签为 *y* = 10。通常，*x*
    会是一个包含多个值的数组，但为了简单起见，我们就用一个单一的值。将这些值代入后，我们可以看到在 [图 10-11](ch10.xhtml#ch10fig11)
    中，当输入 *x* 为 2 时，网络输出的 *ŷ* 值为 5。
- en: '![image](../images/f0190-01.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0190-01.jpg)'
- en: '*Figure 10-11: Visualization of our three-neuron network, with an input of*
    x *= 2*'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 10-11：我们三层神经网络的可视化，输入为* x *= 2*'
- en: To nudge our parameters so that our network’s output *ŷ* , given *x* = 2, moves
    closer to our known *y* value of 10, we need to calculate how *w*[1→3] affects
    our final output *ŷ* . Let’s see what happens when we increase *w*[1→3] by just
    a bit (say, 0.01). The weighted sum in neuron[3] becomes 1.01 * 2 + (1 * 3), making
    the final output *ŷ* change from 5 to 5.02, resulting in an increase of 0.02\.
    In other words, the partial derivative of *ŷ* with respect to *w*[1→3] is 2, because
    changing *w*[1→3] yields twice that change in *ŷ* .
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 为了调整我们的参数，使得网络的输出 *ŷ*（在 *x* = 2 下）更接近我们已知的 *y* 值 10，我们需要计算 *w*[1→3] 如何影响最终的输出
    *ŷ*。让我们来看一下，当我们将 *w*[1→3] 增加一点（比如 0.01）时会发生什么。神经元[3]中的加权和变为 1.01 * 2 + (1 * 3)，使得最终的输出
    *ŷ* 从 5 改变为 5.02，结果增加了 0.02。换句话说，*ŷ* 关于 *w*[1→3] 的偏导数为 2，因为改变 *w*[1→3] 会使 *ŷ*
    改变两倍。
- en: Because *y* is 10 and our current output *ŷ* (given our current parameter values
    and *x* = 2) is 5, we now know that we should increase *w*[1→3] by a small amount
    to move *y* closer to 10.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 *y* 为 10，而当前的输出 *ŷ*（在当前参数值和 *x* = 2 下）为 5，所以我们现在知道应该稍微增加 *w*[1→3]，以将 *y*
    移动得更接近 10。
- en: 'That’s fairly simple. But we need to be able to know which direction to push
    *all* parameters in our network, not just ones in a neuron in the final layer.
    For example, what about *w*[*x*→1]? Calculating *∂ŷ*/*∂w*[*x*→1] is more complicated
    because it only *indirectly* affects *ŷ* . First, we ask neuron[3]’s function
    how *ŷ* is affected by neuron[1]’s output. If we change the output of neuron[1]
    from 2 to 2.01, the final output of the neuron[3] changes from 5 to 5.01, so *∂ŷ*/*∂*neuron[1]
    = 1\. To know how much *w*[*x*→1] affects *ŷ* , we just have to multiply *∂ŷ*/*∂*neuron[1]
    by how much *w*[*x*→1] affects the output of neuron[1]. If we change *w*[*x*→1]
    from 1 to 1.01, the output of neuron[1] changes from 2 to 2.02, so *∂*neuron[1]/*∂w*[*x*→1]
    is 2\. Therefore:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这相当简单。但是我们需要能够知道在网络中如何调整*所有*参数的方向，而不仅仅是最终层中神经元的参数。例如，*w*[*x*→1]呢？计算 *∂ŷ*/*∂w*[*x*→1]
    更复杂，因为它只是*间接地*影响*ŷ*。首先，我们询问神经元[3]的函数，看看神经元[1]的输出如何影响*ŷ*。如果我们将神经元[1]的输出从 2 改为 2.01，神经元[3]的最终输出将从
    5 改为 5.01，所以 *∂ŷ*/*∂*neuron[1] = 1。为了知道 *w*[*x*→1] 如何影响 *ŷ*，我们只需要将 *∂ŷ*/*∂*neuron[1]
    乘以 *w*[*x*→1] 如何影响神经元[1]的输出。如果我们将 *w*[*x*→1] 从 1 改为 1.01，神经元[1]的输出将从 2 改为 2.02，所以
    *∂*neuron[1]/*∂w*[*x*→1] 是 2。因此：
- en: '![image](../images/f0191-01.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0191-01.jpg)'
- en: 'Or:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 或者：
- en: '![image](../images/f0191-02.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0191-02.jpg)'
- en: You may have noticed that we just used the chain rule.[³](footnote.xhtml#ch10fn3)
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，我们刚才使用了链式法则。[³](footnote.xhtml#ch10fn3)
- en: In other words, to figure out how a parameter like *w*[*x*→1] deep inside a
    network affects our final output *ŷ* , we multiply the partial derivatives at
    each point along the path between our parameter *w*[*x*→1] and *ŷ* . This means
    that if *w*[*x*→1] is fed into a neuron whose outputs are fed into ten other neurons,
    calculating *w*[*x*→1]’s effect on *ŷ* would involve summing over all the paths
    that led from *w*[*x*→1] to *ŷ* , instead of just one. [Figure 10-12](ch10.xhtml#ch10fig12)
    visualizes the paths affected by the sample weight parameter *w*[*x*→2].
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，要弄清楚像 *w*[*x*→1] 这样深处于网络中的参数如何影响我们的最终输出 *ŷ*，我们需要将每一条路径上偏导数的值相乘，直到路径结束。也就是说，如果
    *w*[*x*→1] 输入到一个神经元，这个神经元的输出输入到十个其他神经元，那么计算 *w*[*x*→1] 对 *ŷ* 的影响将涉及对所有从 *w*[*x*→1]
    到 *ŷ* 的路径进行求和，而不仅仅是计算一条路径的影响。[图 10-12](ch10.xhtml#ch10fig12) 可视化了由示例权重参数 *w*[*x*→2]
    影响的路径。
- en: '![image](../images/f0191-03.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0191-03.jpg)'
- en: '*Figure 10-12: Visualization of the paths affected by* w[*x*→2] *(shown in
    dark gray): the weight associated with the connection between input data* x *and
    the middle neuron in the first (leftmost) layer*'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 10-12：受* w[*x*→2] *影响的路径的可视化（显示为深灰色）：输入数据* x *与第一层（最左边）中间神经元之间连接的权重*'
- en: Note that the hidden layers in this network are not fully connected layers,
    which helps explain why the second hidden layer’s bottom neuron isn’t highlighted.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个网络中的隐藏层并不是完全连接的层，这有助于解释为什么第二个隐藏层的底部神经元没有被高亮显示。
- en: '***Path Explosion***'
  id: totrans-167
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***路径爆炸***'
- en: But what happens when our network gets even larger? The number of paths we need
    to add to calculate the partial derivative of a low-level parameter increases
    exponentially. Consider a neuron whose output is fed into a layer of 1,000 neurons,
    whose outputs are fed into 1,000 more neurons, whose outputs are then fed into
    a final output neuron.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 但是当我们的网络变得更大时会发生什么呢？我们需要添加的路径数量以指数级增加，以计算低层参数的偏导数。考虑一个神经元，它的输出输入到一层 1000 个神经元，这些神经元的输出再输入到另外
    1000 个神经元，最后这些输出再输入到一个最终的输出神经元。
- en: That results in one million paths! Luckily, going over every single path and
    then summing them to get the *∂ŷ*/(*∂*parameter) is not necessary. This is where
    backpropagation comes in handy. Instead of walking along every single path that
    leads to our final output(s), *ŷ* , partial derivatives are calculated layer by
    layer, starting from the top down, or backward.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了上百万条路径！幸运的是，逐条遍历每一条路径并将其加总以获得 *∂ŷ*/(*∂*parameter) 是不必要的。这时反向传播就显得非常有用了。我们不需要沿着每一条路径走到最终输出
    *ŷ*，而是逐层计算偏导数，从上到下，或者反向进行。
- en: 'Using the chain rule logic from the last section, we can calculate any partial
    derivative *∂ŷ*/*∂w*, where *w* is a parameter connecting an output from layer[*i*–1]
    to a neuron[*i*] in layer[*i*], by summing over the following for all neuron[*i*][+1],
    where each neuron[*i*][+1] is a neuron in layer[*i*][+1] to which neuron[*i*]
    (*w*’s neuron) is connected:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上一节中的链式法则逻辑，我们可以计算任何偏导数 *∂ŷ*/*∂w*，其中 *w* 是连接层[*i*–1]的输出到层[*i*]中神经元[*i*]的一个参数，通过对所有神经元[*i*][+1]进行求和，其中每个神经元[*i*][+1]
    是层[*i*][+1] 中与神经元[*i*]（即 *w* 所连接的神经元）相连的神经元：
- en: '![image](../images/f0192-01.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0192-01.jpg)'
- en: By doing this layer by layer from the top down, we limit path explosion by consolidating
    derivatives at each layer. In other words, derivatives calculated in a top-level
    layer[*i*+1] (like *∂ŷ*/*∂*neuron[*i*+1]) are recorded to help calculate derivatives
    in layer[*i*]. Then to calculate derivatives in layer[*i*][–1], we use the saved
    derivatives from layer[*i*] (like *∂ŷ*/*∂*neuron[*i*]). Then, layer[*i*][–2] uses
    derivatives from layer[*i*–1], and so on and so forth. This trick greatly reduces
    the amount of calculations we have to repeat and helps us to train neural networks
    quickly.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 通过从上到下逐层进行，我们通过在每一层合并导数来限制路径爆炸。换句话说，在顶层[*i*+1]中计算的导数（如*∂ŷ*/*∂*neuron[*i*+1]）会被记录下来，帮助计算第[*i*]层的导数。然后，为了计算第[*i*]层的导数[–1]，我们使用来自第[*i*]层的已保存导数（如*∂ŷ*/*∂*neuron[*i*]）。接着，第[*i*]层[–2]使用来自第[*i*–1]层的导数，以此类推。这个技巧大大减少了我们需要重复的计算量，并帮助我们更快速地训练神经网络。
- en: '***Vanishing Gradient***'
  id: totrans-173
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***梯度消失***'
- en: One issue that very deep neural networks face is the *vanishing gradient* problem.
    Consider a weight parameter in the first layer of a neural network that has ten
    layers. The signal it gets from backpropagation is the summation of all paths’
    signals from this weight’s neuron to the final output.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 很深的神经网络面临的一个问题是*梯度消失*问题。考虑一个神经网络中第一层的权重参数，这个网络有十层。它从反向传播中得到的信号是该权重的神经元到最终输出的所有路径信号的总和。
- en: The problem is that each path’s signal is likely to be incredibly tiny, because
    we calculate that signal by multiplying partial derivatives at each point along
    the ten-neuron-deep path, all of which tend to be numbers smaller than 1\. This
    means that a low-level neuron’s parameters are updated based on the summation
    of a massive number of very tiny numbers, many of which end up canceling one another
    out. As a result, it can be difficult for a network to coordinate sending a strong
    signal down to parameters in lower layers. This problem gets exponentially worse
    as you add more layers. As you learn in the following section, certain network
    designs try to get around this pervasive problem.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，每条路径的信号可能会变得非常微小，因为我们通过在沿着十层神经元深的路径的每一点上乘以偏导数来计算该信号，这些偏导数的值往往小于1。这意味着低层神经元的参数是基于大量非常小的数值的总和来更新的，其中许多数值会相互抵消。因此，网络很难协调向较低层的参数发送强信号。随着更多层的添加，这个问题会呈指数级恶化。正如你在下一部分中将学到的，某些网络设计试图绕过这个普遍存在的问题。
- en: '**Types of Neural Networks**'
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**神经网络的类型**'
- en: For simplicity’s sake, every example I’ve shown you so far uses a type of network
    called a feed-forward neural network. In reality, there are many other useful
    network structures you can use for different classes of problems. Let’s discuss
    some of the most common classes of neural networks and how they could be applied
    in a cybersecurity context.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简便起见，到目前为止我展示的每个例子都使用了一种叫做前馈神经网络的网络类型。实际上，还有许多其他有用的网络结构可以用于不同类别的问题。让我们讨论一些最常见的神经网络类别，以及它们在网络安全背景下的应用。
- en: '***Feed-Forward Neural Network***'
  id: totrans-178
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***前馈神经网络***'
- en: 'The simplest (and first) kind of neural network, a feed-forward neural network,
    is kind of like a Barbie doll with no accessories: other types of neural networks
    are usually just variations on this “default” structure. The feed-forward architecture
    should sound familiar: it consists of stacks of layers of neurons. Each layer
    of neurons is connected to some or all neurons in the next layer, but connections
    never go backward or form cycles, hence the name “feed forward.”'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单（也是最初）类型的神经网络——前馈神经网络——就像是没有配件的芭比娃娃：其他类型的神经网络通常只是这种“默认”结构的变种。前馈架构应该听起来很熟悉：它由一层层神经元堆叠组成。每一层神经元都与下一层中的某些或所有神经元相连，但连接从不反向或形成循环，因此得名“前馈”。
- en: In feed-forward neural networks, every connection that exists is connecting
    a neuron (or original input) in layer *i* to a neuron in layer *j* > *i*. Each
    neuron in layer *i* doesn’t necessarily have to connect to every neuron in layer
    *i* + 1, but all connections must be feeding forward, connecting previous layers
    to later layers.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在前馈神经网络中，存在的每个连接都是将第*i*层的神经元（或原始输入）连接到第*j*层的神经元，其中*j* > *i*。第*i*层中的每个神经元不一定要与第*i*+1层中的每个神经元连接，但所有连接必须是前馈的，将前面的层与后面的层连接起来。
- en: Feed-forward networks are generally the kind of network you throw at a problem
    first, unless you already know of another architecture that works particularly
    well on the problem at hand (such as convolutional neural networks for image recognition).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈网络通常是你在面对问题时首先采用的网络，除非你已经知道另一种在当前问题上表现特别好的架构（比如用于图像识别的卷积神经网络）。
- en: '***Convolutional Neural Network***'
  id: totrans-182
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***卷积神经网络***'
- en: A *convolutional neural network (CNN)* contains convolutional layers, where
    the input that feeds into each neuron is defined by a window that slides over
    the input space. Imagine a small square window sliding over a larger picture where
    only the pixels visible through the window will be connected to a specific neuron
    in the next layer. Then, the window slides, and the new set of pixels are connected
    to a new neuron. [Figure 10-13](ch10.xhtml#ch10fig13) illustrates this.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 一个*卷积神经网络（CNN）*包含卷积层，其中每个神经元输入的内容是通过一个滑动窗口定义的，该窗口在输入空间上滑动。想象一个小方形窗口在一张较大的图片上滑动，只有通过窗口可见的像素才会连接到下一层的特定神经元。然后，窗口继续滑动，新的像素集合连接到新的神经元。[图
    10-13](ch10.xhtml#ch10fig13)展示了这一过程。
- en: The structure of these networks encourages localized feature learning. For example,
    it’s more useful for a network’s lower layers to focus on the relationship between
    nearby pixels in an image (which form edges, shapes, and so on) than to focus
    on the relationship between pixels randomly scattered across an image (which are
    unlikely to mean much). The sliding windows explicitly force this focus, which
    improves and speeds up learning in areas where local feature extraction is especially
    important.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这些网络的结构鼓励局部特征学习。例如，网络的低层更专注于图像中相邻像素之间的关系（这些关系形成边缘、形状等），而不是专注于图像中随机散布的像素之间的关系（这些关系通常意义不大）。滑动窗口明确地强制了这种聚焦，这有助于在局部特征提取尤为重要的领域加快学习速度。
- en: '![image](../images/f0194-01.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0194-01.jpg)'
- en: '*Figure 10-13: Visualization of a 2 × 2 convolutional window sliding over a
    3 × 3 input space with a stride (step size) of 1, to yield a 2 × 2 output*'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 10-13：一个 2 × 2 的卷积窗口在 3 × 3 的输入空间上滑动，步幅（步长）为 1，得到 2 × 2 的输出*'
- en: Because of their ability to focus on localized sections of the input data, convolutional
    neural networks are extremely effective at image recognition and classification.
    They’ve also been shown to be effective for certain types of natural language
    processing, which has implications for cybersecurity.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 由于能够专注于输入数据的局部部分，卷积神经网络在图像识别和分类中非常有效。它们也已被证明在某些类型的自然语言处理任务中有效，这对网络安全有重要意义。
- en: After each convolutional window’s values are fed to specific neurons in a convolutional
    layer, a sliding window is again slid over *these* neurons’ outputs, but instead
    of them being fed to standard neurons (for example, ReLUs) with weights associated
    with each input, they’re fed to neurons that have no weights (that is, fixed at
    1) and a max (or similar) activation function. In other words, a small window
    is slid over the convolutional layer’s outputs, and the maximum value of each
    window is taken and passed to the next layer. This is called a *pooling layer*.
    The purpose of pooling layers is to “zoom out” on the data (usually, an image),
    thereby reducing the size of the features for faster computation, while retaining
    the most important information.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个卷积窗口的值被输入到卷积层的特定神经元后，窗口再次滑动，覆盖*这些*神经元的输出，但不同于将其输入到标准神经元（例如 ReLU）并为每个输入分配权重的方式，它们被输入到没有权重的神经元（即固定为
    1）和最大值（或类似）激活函数中。换句话说，一个小窗口被滑过卷积层的输出，每个窗口的最大值被取出并传递到下一层。这称为*池化层*。池化层的目的是对数据（通常是图像）进行“缩小”，从而减少特征的大小以加速计算，同时保留最重要的信息。
- en: Convolutional neural networks can have one or multiple sets of convolutional
    and pooling layers. A standard architecture might include a convolutional layer,
    a pooling layer, followed by another set of convolutional and pooling layers,
    and finally a few fully connected layers, like in feed-forward networks. The goal
    of this architecture is that these final fully connected layers receive fairly
    high-level features as inputs (think wheels on a unicycle), and as a result are
    able to accurately classify complex data (such as images).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络可以包含一个或多个卷积层和池化层的组合。标准架构可能包括一个卷积层，一个池化层，接着是另一组卷积层和池化层，最后是几层全连接层，就像在前馈神经网络中一样。这个架构的目标是使这些最终的全连接层接收相对较高层次的特征作为输入（例如单轮车的轮子），从而能够准确地分类复杂的数据（如图像）。
- en: '***Autoencoder Neural Network***'
  id: totrans-190
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***自编码神经网络***'
- en: An *autoencoder* is a type of neural network that tries to compress and then
    decompress an input with minimal difference between the original training input
    and the decompressed output. The goal of an autoencoder is to learn an efficient
    representation for a set of data. In other words, autoencoders act like optimized
    lossy compression programs, where they compress input data into a smaller representation,
    then decompress it back to its original input size.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '*自编码器* 是一种神经网络，它试图在原始训练输入和解压后的输出之间保持最小差异，来压缩并解压输入。自编码器的目标是学习一组数据的高效表示。换句话说，自编码器充当像优化过的有损压缩程序一样，它们将输入数据压缩成更小的表示，再解压回原始输入大小。'
- en: Instead of the neural network optimizing parameters by minimizing the difference
    between known labels (*y*) and predicted labels (*ŷ* ) for a given input *x*,
    the network tries to minimize the difference between the original input *x* and
    the reconstructed output ![images](../images/xcap.jpg).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器网络不是通过最小化已知标签（*y*）与预测标签（*ŷ*）之间的差异来优化参数，而是通过最小化原始输入 *x* 与重建输出之间的差异来优化！[images](../images/xcap.jpg)。
- en: Structurally, autoencoders are usually very similar to standard feed-forward
    neural networks, except that middle layers contain fewer neurons than early and
    later stage layers, as shown in [Figure 10-14](ch10.xhtml#ch10fig14).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在结构上，自编码器通常与标准的前馈神经网络非常相似，不同之处在于中间层的神经元数量比早期和后期的层要少，正如[图 10-14](ch10.xhtml#ch10fig14)所示。
- en: '![image](../images/f0195-01.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0195-01.jpg)'
- en: '*Figure 10-14: Visualization of an autoencoder network*'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 10-14：自编码器网络的可视化*'
- en: As you can see, the middle layer is much smaller than the leftmost (input) and
    rightmost (output) layers, which each have the same size. The last layer should
    always contain the same number of outputs as the original inputs, so each training
    input *x*[*i*] can be compared to its compressed and reconstructed cousin ![images](../images/xcap1.jpg).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，中间层比最左侧（输入）和最右侧（输出）层要小得多，而这两层的大小相同。最后一层应始终包含与原始输入相同数量的输出，这样每个训练输入 *x*[*i*]
    就可以与其压缩和重建后的副本进行比较！[images](../images/xcap1.jpg)。
- en: After an autoencoder network has been trained, it can be used for different
    purposes. Autoencoder networks can simply be used as efficient compress/decompress
    programs. For example, autoencoders trained to compress image files can create
    images that look far clearer than the same image compressed via JPEG to the same
    size.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器网络训练完成后，可以用于不同的目的。自编码器网络可以简单地作为高效的压缩/解压程序。例如，训练用于压缩图像文件的自编码器，可以创建比使用JPEG压缩相同图像到相同大小时更加清晰的图像。
- en: '***Generative Adversarial Network***'
  id: totrans-198
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***生成对抗网络***'
- en: A *generative adversarial network (GAN)* is a system of *two* neural networks
    competing with each other to improve themselves at their respective tasks. Typically,
    the *generative* network tries to create fake samples (for example, some sort
    of image) from random noise. Then a second *discriminator* network attempts to
    tell the difference between real samples and the fake, generated samples (for
    example, distinguishing between real images of a bedroom and generated images).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '*生成对抗网络（GAN）* 是一组 *两个* 神经网络互相竞争、提升各自任务的系统。通常，*生成* 网络尝试从随机噪声中生成假样本（例如某种图像）。然后，第二个
    *判别器* 网络试图区分真实样本和假生成样本之间的差异（例如，区分真实卧室图像与生成的图像）。'
- en: Both neural networks in a GAN are optimized with backpropagation. The generator
    network optimizes its parameters based on how well it fooled the discriminator
    network in a given round, while the discriminator network optimizes its parameters
    based on how accurately it could discriminate between generated and real samples.
    In other words, their loss functions are direct opposites of one another.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: GAN 中的两个神经网络都通过反向传播进行优化。生成器网络根据它在某一回合中如何欺骗判别器网络来优化其参数，而判别器网络则根据它能够多准确地区分生成样本和真实样本来优化其参数。换句话说，它们的损失函数是彼此的直接对立面。
- en: GANs can be been used to generate real-looking data or enhance low-quality or
    corrupted data.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: GANs 可用于生成逼真的数据或增强低质量或损坏的数据。
- en: '***Recurrent Neural Network***'
  id: totrans-202
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***递归神经网络***'
- en: '*Recurrent networks (RRNs)* are a relatively broad class of neural networks
    in which connections between neurons form directed cycles whose activation functions
    are dependent on time-steps. This allows the network to develop a memory, which
    helps it learn patterns in sequences of data. In RNNs, the inputs, the outputs,
    or both the inputs and outputs are some sort of time series.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '*递归神经网络（RNN）* 是一类相对广泛的神经网络，其中神经元之间的连接形成有向循环，其激活函数依赖于时间步。这样，网络就能发展出记忆，帮助它学习数据序列中的模式。在
    RNN 中，输入、输出或输入和输出都可以是某种时间序列。'
- en: RNNs are great for tasks where data order matters, like connected handwriting
    recognition, speech recognition, language translation, and time series analysis.
    In the context of cybersecurity, they’re relevant to problems like network traffic
    analysis, behavioral detection, and static file analysis. Because program code
    is similar to natural language in that order matters, it can be treated as a time
    series.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: RNNs 非常适用于数据顺序重要的任务，如连写识别、语音识别、语言翻译和时间序列分析。在网络安全领域，它们与诸如网络流量分析、行为检测和静态文件分析等问题相关。因为程序代码类似于自然语言，顺序同样重要，因此可以将其视为时间序列。
- en: One issue with RNNs is that due to the vanishing gradient problem, each time-step
    introduced in an RNN is similar to an entire extra layer in a feed-forward neural
    network. During backpropagation, the vanishing gradient problem causes signals
    in lower-level layers (or in this case, earlier time-steps) to become incredibly
    faint.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: RNNs 的一个问题是，由于梯度消失问题，RNN 中每个时间步引入的内容类似于前馈神经网络中的整个额外层。在反向传播过程中，梯度消失问题使得低层（或在这种情况下，较早的时间步）中的信号变得非常微弱。
- en: A *long short-term memory (LSTM) network* is a special type of RNN designed
    to address this problem. LSTMs contain *memory cells* and special neurons that
    try to decide what information to remember and what information to forget. Tossing
    out most information greatly limits the vanishing gradient problem because it
    reduces path explosion.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '*长短期记忆（LSTM）网络*是一种专门设计用来解决这一问题的 RNN。LSTM 包含 *记忆单元* 和特殊的神经元，旨在决定记住哪些信息以及忘记哪些信息。丢弃大部分信息大大限制了梯度消失问题，因为它减少了路径爆炸。'
- en: '***ResNet***'
  id: totrans-207
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***ResNet***'
- en: A *ResNet* (short for *residual network*) is a type of neural network that creates
    *skip connections* between neurons in early/shallow layers of the network to deeper
    layers by skipping one or more intermediate layers. Here the word *residual* refers
    to the fact that these networks learn to pass numerical information directly between
    layers, without that numerical information having to pass through the kinds of
    activation functions we illustrated in [Table 10-1](ch10.xhtml#ch10tab1).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '*ResNet*（*残差网络*的简称）是一种神经网络，通过在网络的早期/浅层与更深层之间创建 *跳跃连接*，即跳过一个或多个中间层来实现。这里的 *残差*
    一词指的是这些网络学习直接在层之间传递数据信息，而无需通过我们在 [表 10-1](ch10.xhtml#ch10tab1) 中说明的激活函数。'
- en: This structure helps greatly reduce the vanishing gradient problem, which enables
    ResNets to be incredibly deep—sometimes more than 100 layers.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这种结构大大减少了梯度消失问题，使得 ResNet 能够变得非常深——有时可以达到超过 100 层。
- en: Very deep neural networks excel at modeling extremely complex, odd relationships
    in input data. Because ResNets are able to have so many layers, they are especially
    suited to complex problems. Like feed-forward neural networks, ResNets are important
    more because of their general effectiveness at solving complex problems rather
    than their expertise in very specific problem areas.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 非常深的神经网络擅长建模输入数据中极其复杂和异常的关系。由于 ResNet 可以拥有如此多的层，它们特别适合复杂问题。像前馈神经网络一样，ResNet
    更重要的是因其在解决复杂问题方面的通用有效性，而不是在非常具体的领域中的专长。
- en: '**Summary**'
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**总结**'
- en: In this chapter, you learned about the structure of neurons and how they are
    connected together to form neural networks. You also explored how these networks
    are trained via backpropagation, and you discovered some benefits and issues that
    neural networks have, such as universality, automatic feature generation, and
    the vanishing gradient problem. Finally, you learned the structures and benefits
    of a few common types of neural networks.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了神经元的结构以及它们如何连接在一起形成神经网络。你还探讨了这些网络是如何通过反向传播进行训练的，并且发现了一些神经网络的优缺点，例如普适性、自动特征生成以及梯度消失问题。最后，你了解了几种常见神经网络的结构和优点。
- en: In the next chapter, you’ll actually build neural networks to detect malware,
    using Python’s `Keras` package.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，你将使用 Python 的`Keras`包实际构建神经网络来检测恶意软件。
