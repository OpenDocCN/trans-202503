- en: '**11'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**11'
- en: 'LINEAR MODELS ON STEROIDS: NEURAL NETWORKS**'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类似类固醇的线性模型：神经网络**
- en: '![Image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/common.jpg)'
- en: The method of neural networks (NNs) is probably the best-known ML technology
    among the general public. The science fiction−sounding name is catchy—even more
    so with the advent of the term *deep learning*—and NNs have become the favorite
    approach to image classification in applications that also intrigue the general
    public, such as facial recognition.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络（NNs）可能是公众最熟悉的机器学习技术。这个听起来像科幻小说中的名字非常吸引人——尤其是随着*深度学习*这一术语的出现——神经网络已成为图像分类的首选方法，应用于公众也感兴趣的领域，如面部识别。
- en: 'Yet NNs are probably the most challenging ML technology to use well, with problems
    such as:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，神经网络可能是最具挑战性的机器学习技术之一，使用时会遇到如下问题：
- en: “Black box” operation, where it’s not clear what’s going on inside
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “黑箱”操作，内部发生了什么并不明确
- en: Numerous hyperparameters to tune
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要调优的超参数众多
- en: Tendency toward overfitting
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过拟合的倾向
- en: Possibly lengthy computation time, with some large-data cases running for hours
    or even days when large amounts of RAM may be needed
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能需要较长的计算时间，在某些大数据案例中，当需要大量内存时，计算可能需要几个小时甚至几天。
- en: Convergence issues
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收敛问题
- en: Let’s see what all the fuss is about.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这到底有什么大惊小怪。
- en: 11.1 Overview
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1 概述
- en: The term *neural network* alludes to an ML method that is inspired by the biology
    of human thought. In a two-class classification problem, for instance, the predictor
    variables serve as inputs to a *neuron*, outputting 1 or 0, with 1 meaning that
    the neuron *fires*—and we decide class 1\. NNs consist of several *hidden layers*
    in which the outputs of one layer of neurons are fed into the next layer and so
    on, until the process reaches the final output layer. This, too, has been given
    biological interpretation. The terms *node* and *units* are synonymous with *neurons*.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*神经网络*这个术语指的是一种受人类思维生物学启发的机器学习方法。例如，在一个二分类问题中，预测变量作为*神经元*的输入，输出1或0，1表示神经元*激活*——我们便决定为类别1。神经网络由若干个*隐藏层*组成，其中一个神经层的输出作为输入传递给下一层，以此类推，直到处理过程到达最终的输出层。这个过程也被赋予了生物学解释。术语*节点*和*单元*与*神经元*是同义的。'
- en: The method was later generalized, using *activation functions* with outputs
    other than just 1 and 0 and allowing backward feedback from later layers to earlier
    ones. This led development of the field somewhat away from the biological motivation,
    and some questioned the biological interpretation anyway, but NNs have a strong
    appeal for many in the machine learning community. Indeed, well-publicized large
    projects using *deep learning* have revitalized interest in NNs.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法后来被推广，使用了*激活函数*，其输出不仅限于1和0，并允许后续层对前面层进行反向反馈。这使得该领域的发展在某种程度上偏离了生物学的动机，尽管有些人对生物学解释提出质疑，但神经网络（NNs）在机器学习社区中仍具有强大的吸引力。事实上，使用*深度学习*的广为宣传的大型项目重新激发了人们对神经网络的兴趣。
- en: '[Figure 11-1](ch11.xhtml#ch11fig01), generated by the `neuralnet` package on
    our vertebrae data, illustrates how the method works. (We will not be using that
    package, but it does produce nice displays.)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11-1](ch11.xhtml#ch11fig01)，由`neuralnet`包在我们的脊椎数据上生成，展示了该方法的工作原理。（我们不会使用该包，但它确实能生成漂亮的展示图。）'
- en: '![Image](../images/ch11fig01.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch11fig01.jpg)'
- en: '*Figure 11-1: Vertebrae NN with one hidden layer*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-1：具有一个隐藏层的脊椎神经网络*'
- en: 'Here’s the overview:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是概述：
- en: A neural network consists of a number of *layers* (three in this case).
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络由若干个*层*组成（本例中为三个）。
- en: In pictures describing a particular network, there is an input layer on the
    far left (the vertebrae measurements here) and an output layer on the far right,
    which, in this case, is giving the class predictions.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在描述特定网络的图像中，最左侧是输入层（这里是脊椎测量数据），最右侧是输出层，本例中输出的是类别预测。
- en: There are one or more *hidden* layers in between, with one in this case.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中间有一个或多个*隐藏层*，本例中为一个。
- en: The outputs of one layer are fed as inputs into the next layer.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一层的输出作为输入传递给下一层。
- en: The output is typically a single number, for regression problems, and *c* numbers,
    for *c*-class classification problems.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出通常是一个单一的数字，回归问题为一个数字，*c*分类问题则为*c*个数字。
- en: The inputs into a layer are fed through what amounts to a linear model. The
    outputs of a layer are fed through an *activation function*, which is analogous
    to kernel functions in SVM, to accommodate nonlinear relations. In [Figure 11-1](ch11.xhtml#ch11fig01),
    the activation function used was our old friend the logit, *a*(*t*) = 1/[1 + exp
    (−*t*)] (though in an entirely different context; we are *not* performing logistic
    regression).
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层的输入通过相当于线性模型的方式传递。层的输出通过一个*激活函数*传递，这类似于SVM中的核函数，用于适应非线性关系。在[图11-1](ch11.xhtml#ch11fig01)中，使用的激活函数是我们熟悉的logit函数，*a*(*t*)
    = 1/[1 + exp(−*t*)]（尽管是在完全不同的背景下；我们*不是*在执行逻辑回归）。
- en: How does all this play out in [Figure 11-1](ch11.xhtml#ch11fig01)? Let’s look
    at some of the numbers in the diagram. For example, the input to the first circle,
    center column is 1.0841 · 1 + 0.84311 V1 + 0.49439 V2 + . . . , which is a linear
    combination of the features. A different linear combination is fed into the second
    circle. The output of each circle is fed into the next layer.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 那么这一切如何在[图11-1](ch11.xhtml#ch11fig01)中展现出来呢？让我们看看图中一些数字。例如，第一圆圈的输入（中间列）是1.0841
    · 1 + 0.84311 V1 + 0.49439 V2 + ……，这是特征的线性组合。不同的线性组合被输入到第二个圆圈。每个圆圈的输出被传递到下一个层。
- en: How are the coefficients (*weights*) in these linear combinations computed?
    We forgo a detailed mathematical answer here, but in essence, we minimize the
    sum of squared prediction errors in the regression case. In the classification
    case, we choose the weights to minimize the overall misclassification rate or
    a variant thereof.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这些线性组合中的系数（*权重*）是如何计算的呢？我们在这里省略详细的数学解答，但本质上，我们在回归问题中最小化平方预测误差之和。在分类问题中，我们选择权重来最小化总体的错误分类率，或其变体。
- en: 11.2 Working on Top of a Complex Infrastructure
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2 在复杂基础设施之上工作
- en: Before we begin, a few words are in order regarding `qeNeural()`, our `qe*`-series
    function for building neural networks.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，关于`qeNeural()`，即我们用于构建神经网络的`qe*`系列函数，先说几句。
- en: As we’ve noted throughout the book, the `qe*`-series functions are mainly wrappers—that
    is, convenient wrappers to other functions. This is done so that the series can
    provide a uniform, quick-and-easy user interface to a variety of ML algorithms.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本书中反复提到的，`qe*`系列函数主要是封装器——即，便捷的封装器，封装其他函数。这么做是为了让这个系列能够提供统一、快速且简便的用户界面，支持多种机器学习算法。
- en: 'Our function `qeSVM()`, for instance, wraps the `svm()` function in the package
    `e1071`. What about `qeNeural()`? There is quite a tale here! What happens (approximately)
    is:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们的`qeSVM()`函数封装了`e1071`包中的`svm()`函数。那么`qeNeural()`呢？这里有一段有趣的故事！大致发生的事情是：
- en: The function `qeNeural()` wraps the `regtools()` function `krsFit()`.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 函数`qeNeural()`封装了`regtools()`包中的`krsFit()`函数。
- en: The function `krsFit()` wraps a number of functions in the R `keras` package
    for NNs.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`krsFit()`函数封装了R的`keras`包中多个用于神经网络的函数。'
- en: The R `keras` package wraps the R `tensorflow` package.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R的`keras`包封装了R的`tensorflow`包。
- en: The R `tensorflow` package wraps the Python package of the same name.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R的`tensorflow`包封装了同名的Python包。
- en: And much of `tensorflow` is actually written in the C language.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 而且，`tensorflow`的大部分代码实际上是用C语言编写的。
- en: And much of this, in turn, depends on the function `reticulate()` from the package
    of the same name. Its role is to translate between R and Python.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 而且这一切大多依赖于同名包中的`reticulate()`函数。它的作用是实现R和Python之间的转换。
- en: Setting this up, then, can be a bit delicate. See the RStudio site for help
    with your particular platform (for instance, [*https://tensorflow.rstudio.com/tutorials/quickstart/beginner.xhtml*](https://tensorflow.rstudio.com/tutorials/quickstart/beginner.xhtml)).
    The R interfaces in the above list, as well as `reticulate`, were developed by
    RStudio.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，设置这个过程可能有些微妙。有关特定平台的帮助，请参阅RStudio网站（例如，[*https://tensorflow.rstudio.com/tutorials/quickstart/beginner.xhtml*](https://tensorflow.rstudio.com/tutorials/quickstart/beginner.xhtml)）。上述列表中的R接口以及`reticulate`是由RStudio开发的。
- en: It’s important to keep these points in mind about the “bilingual” nature of
    the software. For example, one implication is that even if you call `set.seed()`
    before your NNs run, you still will notice some variation from one run to the
    next. This will mystify you if you don’t know that Python has its own random number
    generator!
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 牢记这些关于软件“二语”特性的要点是很重要的。例如，一个含义是，即使在运行神经网络前调用了`set.seed()`，你仍然会注意到不同运行之间会有一些变化。如果你不知道Python有自己独立的随机数生成器的话，这一点会让你感到困惑！
- en: '11.3 Example: Vertebrae Data'
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.3 示例：脊椎数据
- en: 'Say we wish to fit a model and then do prediction. As before, we’ll specify
    no holdout set so that as much data as possible is used in the prediction:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们希望拟合一个模型并进行预测。如前所述，我们将不指定保留集，以便尽可能多的数据用于预测：
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The fitting process is iterative, and a report is given on each iteration or
    *epoch*. The number of epochs is a hyperparameter. This and the other hyperparameters
    will be discussed in the next section.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 拟合过程是迭代的，并且每次迭代或*周期*都会给出报告。周期数是一个超参数。这个以及其他超参数将在下一节中讨论。
- en: As an example of prediction, consider a patient similar to the first one in
    our data, but with V2 being 18 rather than 22.55\. What would be our predicted
    class?
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 作为预测的一个例子，考虑一个与数据中第一个病人相似的病人，但其 V2 为 18 而不是 22.55。我们预测的类别会是什么？
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We predict class DH.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们预测的是 DH 类。
- en: 11.4 Neural Network Hyperparameters
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.4 神经网络超参数
- en: 'NN libraries are notorious for having tons of hyperparameters. Our `qeNeural()`
    function has been designed to avoid this, having only a few hyperparameters. The
    call form is:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络库以拥有大量超参数而著名。我们的 `qeNeural()` 函数经过设计，避免了这一点，只有少量的超参数。调用形式是：
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here is what the NN-specific arguments represent:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是与神经网络特定参数相关的说明：
- en: hidden   Specifies the number of hidden layers and number of units per layer
    (this need not be constant across layers). The default means two hidden layers
    with 100 units each. If a number in this vector is fractional, it indicates *dropout*,
    which will be discussed below.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: hidden   指定隐藏层的数量以及每层的单元数量（每层的数量不必相同）。默认设置意味着两层隐藏层，每层有 100 个单元。如果这个向量中的某个数值是小数，则表示*丢弃*，下面将进行讨论。
- en: nEpoch   Specifies the number of epochs.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: nEpoch   指定周期数。
- en: acts   Specifies the activation functions, with one for each hidden layer.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: acts   指定激活函数，每个隐藏层有一个激活函数。
- en: learnRate   Very similar to what we saw in gradient boosting (see [Section 6.3.8](ch06.xhtml#ch06lev3sec8)).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: learnRate   与我们在梯度提升中看到的非常相似（见[第 6.3.8 节](ch06.xhtml#ch06lev3sec8)）。
- en: conv, xShape   Arguments used in image classification settings, which will be
    discussed in [Chapter 12](ch12.xhtml).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: conv，xShape   在图像分类设置中使用的参数，将在[第 12 章](ch12.xhtml)中讨论。
- en: 'The analyst can use the `keras` package directly for more detailed control.
    These arguments address one or both of these aims:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 分析师可以直接使用 `keras` 包来进行更详细的控制。这些参数旨在实现以下一个或两个目标：
- en: 'Controlling the Bias-Variance Trade-off: `hidden`, `nEpoch`'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制偏差-方差权衡：`hidden`，`nEpoch`
- en: 'Dealing with convergence issues: `nEpoch`, `acts`, `learnRate`'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理收敛问题：`nEpoch`，`acts`，`learnRate`
- en: The reader may be surprised to see the number of epochs—that is, the number
    of iterations—in the Bias-Variance Trade-off list above. In most iterative algorithms,
    the more iterations the better. But empirically, analysts have found that having
    too many iterations in NNs may result in overfitting.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 读者可能会对上面偏差-方差权衡列表中的周期数（即迭代次数）感到惊讶。在大多数迭代算法中，迭代次数越多越好。但从经验上看，分析师发现，神经网络中迭代次数过多可能会导致过拟合。
- en: 11.5 Activation Functions
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5 激活函数
- en: If we were to simply input and output linear functions at each layer, we would
    have linear functions of linear functions of linear functions . . . , which would
    still be a linear function after all that combining. To be able to model nonlinear
    relations, we instead place *activation functions*, *a*(*t*), at the output of
    each layer.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在每一层中简单地输入和输出线性函数，我们将得到线性函数的线性函数的线性函数……，经过多次组合后仍然是线性函数。为了能够建模非线性关系，我们在每一层的输出处放置*激活函数*，*a*(*t*)。
- en: Over the years, there has been some debate as to good choices for the activation
    function. In principle, any nonlinear function should work, but issues do arise,
    especially concerning the all-important convergence problem.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，关于激活函数的最佳选择一直存在一些争论。从原则上讲，任何非线性函数都应该有效，但问题确实会出现，尤其是与极其重要的收敛问题相关。
- en: Consider once again [Figure 6-2](ch06.xhtml#ch06fig02). The minimum around 2.2
    comes at a rather sharp dip (in calculus terms, a large second derivative). But
    what if the curve were to look like that in [Figure 11-2](ch11.xhtml#ch11fig02)?
    There is a rather shallow trough near the minimum, extending, say, between −4
    and 4\. Here even a larger learning rate might have us spending many iterations
    with almost no progress. This is the *vanishing gradient problem*. And if the
    curve is very sharp near the minimum, we may have an *exploding gradient problem*,
    which can wreak havoc with even very small learning rates.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 再次考虑[图 6-2](ch06.xhtml#ch06fig02)。在2.2附近的最小值出现在一个相当陡峭的下凹处（用微积分术语来说，是一个大的二阶导数）。但如果曲线像[图
    11-2](ch11.xhtml#ch11fig02)中的那样呢？在最小值附近有一个相当浅的槽，假设其范围是−4到4。即使使用更大的学习率，我们也可能会在几次迭代中几乎没有进展。这就是*梯度消失问题*。如果曲线在最小值附近非常陡峭，我们可能会遇到*梯度爆炸问题*，即使在非常小的学习率下，也会对网络造成严重影响。
- en: '![Image](../images/ch11fig02.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch11fig02.jpg)'
- en: '*Figure 11-2: Shallow minimum region*'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-2：浅最小值区域*'
- en: The choice of activation function plays a big role in these things. There is
    a multiplicative effect across layers. (Again, for those who know calculus, this
    is the Chain Rule in action.) And quantities in the interval (−1,1) become smaller
    and smaller when multiplied together, so that multiplicative effect results in
    smaller and smaller numbers, hence a vanishing gradient. If the gradient is large
    at each layer, we may develop an exploding gradient.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数的选择在这些事情中起着重要作用。各层之间存在乘法效应。（再说一次，对于懂得微积分的人来说，这是链式法则在起作用。）区间（−1,1）中的量在相乘时会变得越来越小，因此乘法效应导致数值变得越来越小，从而产生梯度消失。如果每一层的梯度很大，我们可能会遇到梯度爆炸问题。
- en: 'After years of trial and error, the popular choice among NN users today is
    the *Rectified Linear Unit (ReLU)*: *f*(*x*) is 0 for *x* < 0 but is equal to
    *x* for *x* ≥ 0.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 经历了多年的反复试验，今天神经网络用户中流行的选择是*修正线性单元（ReLU）*：*f*(*x*) 在 *x* < 0 时为 0，而在 *x* ≥ 0
    时等于 *x*。
- en: 11.6 Regularization
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.6 正则化
- en: As noted, NNs have a tendency to overfit, with many having thousands of weights
    and some even millions. Remember, the weights are essentially linear regression
    coefficients, so the total number of weights is effectively the new value of *p*
    (that is, our number of features). We must find some way to reduce that value.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，神经网络有过拟合的趋势，许多网络拥有成千上万的权重，有些甚至有百万级的权重。请记住，权重本质上是线性回归系数，因此权重的总数实际上是*p*的新值（即我们的特征数量）。我们必须找到某种方法来减少这个值。
- en: '***11.6.1 L1 and L2 Regularization***'
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***11.6.1 L1和L2正则化***'
- en: Since NNs (typically) minimize a sum of squares, we can apply a penalty term
    to reduce the size of the solution, just as in the cases of ridge regression and
    the LASSO. Recall also that in the LASSO, with the *ℓ*[1] penalty, this tends
    to produce a sparse solution, with most coefficients being 0s.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 由于神经网络（通常）最小化平方和，我们可以应用惩罚项来减少解的大小，就像岭回归和LASSO中的情况一样。还要回想一下，在LASSO中，使用*ℓ*[1]惩罚时，这往往会产生一个稀疏解，大多数系数为0。
- en: Well, that is exactly what we want here. We fear we have too many weights, and
    we hope that applying an *ℓ*[1] penalty will render most of them nil.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，这正是我们在这里想要的。我们担心我们有太多的权重，并且希望应用*ℓ*[1]惩罚能使大部分权重归零。
- en: However, that may not happen with NNs due to the use of nonlinear activation
    functions. The problem is that the contours in [Figure 9-3](ch09.xhtml#ch09fig03)
    are no longer ellipses, and thus the “first contact point” will not likely be
    at a corner of the diamond.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于使用了非线性激活函数，这种情况在神经网络中可能不会发生。问题在于，[图 9-3](ch09.xhtml#ch09fig03)中的等高线不再是椭圆形的，因此“第一次接触点”不太可能位于菱形的一个角落。
- en: Nevertheless, *ℓ*[1] will still shrink the weights, as will *ℓ*[2], so we should
    achieve dimension reduction in some sense.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，*ℓ*[1]仍然会缩小权重，*ℓ*[2]也是如此，因此我们应该在某种意义上实现维度减少。
- en: '***11.6.2 Regularization by Dropout***'
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***11.6.2 通过Dropout进行正则化***'
- en: If a weight is 0, then in a picture of the network, such as [Figure 11-1](ch11.xhtml#ch11fig01),
    the corresponding link is removed. So, if the goal is to remove some links, why
    not simply remove some links directly? Or better yet, remove entire nodes. That
    is exactly what *dropout* does.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果某个权重为0，那么在网络图中，例如[图 11-1](ch11.xhtml#ch11fig01)，对应的连接将被移除。所以，如果目标是移除一些连接，为什么不直接移除一些连接呢？或者更好的是，移除整个节点。这正是*dropout*的作用。
- en: For instance, if our dropout rate is 0.2, we randomly (and temporarily) choose
    20 percent of the links from the given layer and remove them. There are further
    details that we will not list here, but this is the essence of the method.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们的丢弃率是0.2，我们会随机（且暂时地）选择给定层的20%的连接并将其移除。这里还有更多的细节，我们不在此列举，但这就是该方法的核心。
- en: '11.7 Example: Fall Detection Data'
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.7 示例：跌倒检测数据
- en: Let’s revisit the dataset analyzed in [Section 8.9.4](ch08.xhtml#ch08lev9sec4).
    We’ll do a grid search for a good hyperparameter combination.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新审视在[第8.9.4节](ch08.xhtml#ch08lev9sec4)中分析的数据集。我们将进行网格搜索，寻找一个合适的超参数组合。
- en: Recall that the `qeFT()` argument `pars` defines the grid in that it specifies
    the range of values we wish to explore.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 请回忆一下，`qeFT()`函数中的参数`pars`定义了网格，因为它指定了我们希望探索的值范围。
- en: '[PRE3]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: So, we are varying the number of neurons per layer (5, 100, 250) and the dropout
    rate (none, 0.2, 0.5). We could also have varied `nEpoch` and even the activation
    functions. Note, too, that we could have tried having different numbers of neurons
    in different layers.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们在每层神经元的数量（5, 100, 250）和丢弃率（无、0.2、0.5）之间进行变化。我们也可以改变`nEpoch`，甚至是激活函数。还要注意，我们也可以尝试在不同的层中使用不同数量的神经元。
- en: 'Here are the results:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是结果：
- en: '[PRE4]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The first thing to notice is how much smaller the smallest value is than the
    largest. In fact, the latter is actually about the same as the base accuracy:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 首先需要注意的是，最小值与最大值之间的差距有多小。事实上，后者实际上与基础精度差不多：
- en: '[PRE5]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Without the features, we would have an error rate of 72 percent.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有特征，我们的错误率将是72%。
- en: So, exploring the use of different values of the hyperparameter really paid
    off here.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，在这里，探索不同超参数值的使用确实带来了显著的收获。
- en: But still, interesting patterns emerge here, notably the effect of the learning
    rate. The smaller values tended to do poorly. Remember, if our learning rate is
    too small, not only might it slow down convergence, but it also may leave us stuck
    at a local minimum.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 但即便如此，还是能发现一些有趣的模式，特别是学习率的影响。较小的值往往效果较差。记住，如果我们的学习率太小，不仅可能会减慢收敛速度，还可能会让我们卡在局部最小值。
- en: Finally, note that in this case, a smaller value for the dropout rate seemed
    to produce better results.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请注意，在这种情况下，较小的丢弃率似乎能产生更好的结果。
- en: '11.8 Pitfall: Convergence Problems'
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.8 陷阱：收敛问题
- en: As noted, it is often a challenge to configure NN analysis so that proper convergence
    to a good solution is attained. In some cases, one might even encounter the *broken
    clock problem*—that is, the network predicts the same value no matter what the
    inputs are.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，配置神经网络分析以确保正确地收敛到一个好的解通常是一项挑战。在某些情况下，可能会遇到*坏时钟问题*——即网络无论输入是什么，都预测相同的值。
- en: 'Or, one might encounter output like this:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，可能会遇到如下输出：
- en: '[PRE6]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Here `nan` stands for “not a number.” That ominous-sounding message may mean
    the code attempted to divide by 0, which may be due to the vanishing gradient
    problem.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这里`nan`代表“不是一个数字”。这个听起来不祥的消息可能意味着代码试图除以0，这可能是由于梯度消失问题导致的。
- en: The following describes a few tricks we can try, typically specified via one
    or more hyperparameters.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 以下描述了一些可以尝试的技巧，通常是通过一个或多个超参数指定的。
- en: In some cases, convergence problems may be solved by scaling the data, either
    using the R `scale()` function or by mapping to [0,1]. It is recommended that
    one routinely scale one’s data; in `qeNeural()`, scaling is actually hardwired
    into the software.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，通过对数据进行缩放可以解决收敛问题，可以使用R的`scale()`函数，或者将数据映射到[0,1]之间。建议常规地对数据进行缩放；在`qeNeural()`中，缩放实际上是硬编码到软件中的。
- en: 'Here are some values to tweak:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些可调节的值：
- en: '**Learning rate**   Discussed in [Section 6.3.8](ch06.xhtml#ch06lev3sec8).'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**学习率**  在[第6.3.8节](ch06.xhtml#ch06lev3sec8)中讨论。'
- en: '**Activation function**   Try changing to one with a steeper/shallower slope.
    For example, the function *a*(*t*) = 1/(1 + exp (−2*t*)) is steeper around *t*
    = 0 than the ordinary logistic function.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**激活函数**  尝试更改为具有更陡峭/更平缓斜率的函数。例如，函数*a*(*t*) = 1/(1 + exp(−2*t*))在*t* = 0附近比普通的逻辑函数更陡峭。'
- en: '**Early stopping**   In most algorithms, the more iterations the better, but
    in NNs, many issues depart from conventional wisdom. Running the algorithm for
    too long may result in convergence to a poor solution. This leads to the notion
    of *early stopping*, of which there are many variants.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**早停法**  在大多数算法中，迭代次数越多越好，但在神经网络中，许多问题偏离了传统的智慧。运行算法过长可能导致收敛到一个较差的解。这就引出了*早停法*的概念，且有许多变体。'
- en: '**Momentum**   The rough idea here is that “We’re on a roll,” with the last
    few epochs producing winning moves in the right direction, reducing validation
    error each time. So, instead of calculating the next step size individually, why
    not combine the last few step sizes? The next step size will be set to a weighted
    average of the last few, with heavier weight on the more recent ones. (This hyperparameter
    is not available in `qeNeural()` but may be accessed through the `keras` package
    directly.)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**动量** 这里的大致想法是，“我们在顺利推进”，过去几个周期产生了朝正确方向前进的有效步骤，每次都减少验证误差。所以，为什么不结合过去几个步骤的步长呢？下一步的步长将设置为过去几个步长的加权平均值，并且对最近的步长赋予更大的权重。（这个超参数在`qeNeural()`中不可用，但可以通过`keras`包直接访问。）'
- en: Note that regression applications, as opposed to classification, may be especially
    prone to convergence problems, since *Y* is unbounded.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，与分类不同，回归应用可能特别容易出现收敛问题，因为*Y*是无界的。
- en: 11.9 Close Relation to Polynomial Regression
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.9 与多项式回归的密切关系
- en: In [Section 8.11](ch08.xhtml#ch08lev11), we introduced polynomial regression,
    a linear model in which the features are in polynomial form. So, for instance,
    instead of just having people’s heights and ages as features, in a quadratic model
    we now would also have the squares of heights and ages, as well as a cross-product
    term, height × age.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第8.11节](ch08.xhtml#ch08lev11)中，我们介绍了多项式回归，这是一种线性模型，其中特征是多项式形式的。所以，例如，在一个二次模型中，我们不仅有人的身高和年龄作为特征，还会有身高和年龄的平方，以及一个交叉乘积项，身高
    × 年龄。
- en: Polynomials popped up again in SVM, with polynomial kernels. We might have,
    for instance, not just height and age but also the squares of heights and ages,
    as well the height × age term. And we noted that even the use of the radial basis
    function, a nonpolynomial kernel, is approximately a polynomial due to Taylor
    series expansion.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式再次出现在支持向量机（SVM）中，使用了多项式核。例如，我们可能不仅仅考虑身高和年龄，还包括身高和年龄的平方，以及身高 × 年龄项。我们还注意到，即使是使用径向基函数（radial
    basis function），这是一种非多项式核，由于泰勒级数展开，它大致上也可以视为多项式。
- en: It turns out that NNs essentially do polynomial regression as well. To see this,
    let’s look again at [Figure 11-1](ch11.xhtml#ch11fig01). Suppose we take as our
    activation function the squaring function *t*² . That is not a common choice at
    all, but we’ll start with that and then extend the argument.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，神经网络本质上也在进行多项式回归。为了验证这一点，让我们再次看看[图11-1](ch11.xhtml#ch11fig01)。假设我们将激活函数选择为平方函数*t*²。这个选择并不常见，但我们从这个例子开始，然后进一步扩展这个论点。
- en: So, in the hidden layer in [Figure 11-1](ch11.xhtml#ch11fig01), a circle forms
    a linear combination of the inputs and then outputs the square of the linear combination.
    That means the outputs of the hidden layer are second-degree polynomials in the
    inputs. If we were to have a second hidden layer, its outputs would be fourth-degree
    polynomials.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在[图11-1](ch11.xhtml#ch11fig01)的隐藏层中，一个圆形会形成输入的线性组合，然后输出线性组合的平方。这意味着隐藏层的输出是输入的二次多项式。如果我们有第二个隐藏层，那么它的输出将是四次多项式。
- en: What if our activation function itself were to be a polynomial? Then again,
    each successive layer would give us higher and higher degree polynomials in the
    inputs. Since NNs minimize the sum of squared prediction errors, just as in the
    linear model, you can see that the minimizing solution will be that of polynomial
    regression.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的激活函数本身是一个多项式呢？那么，每一层将会为我们提供更高次的输入多项式。由于神经网络（NN）最小化的是平方预测误差的和，正如线性模型一样，你可以看出，最小化的解将是多项式回归的解。
- en: And what about the popular activation functions? One is the *hyperbolic tangent*,
    *tanh*(*t*), whose graph looks similar to the logistic function. But it too has
    a Taylor series expansion, so what we are doing is approximately polynomial regression.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，流行的激活函数呢？其中之一是*双曲正切*，*tanh*（*t*），其图形看起来与逻辑函数相似。但它也有泰勒级数展开，因此我们所做的实际上是近似的多项式回归。
- en: ReLU does not have a Taylor series expansion, but we can form a polynomial approximation
    there too.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU没有泰勒级数展开，但我们也可以形成一个多项式近似。
- en: In that case, why not just use polynomial regression in the first place? Why
    NNs? One answer is that it just would be computationally infeasible for large-
    *p* data, where we could have a very large number of polynomial terms in calling
    `lm()` or `glm()`. This would cause memory issues. (It’s less of a problem for
    NNs because they find the least squares solutions iteratively. This may cause
    convergence problems but at least uses less memory.) The kernel trick is very
    helpful here, and there is even the *kernel ridge regression* method that applies
    this to linear ridge models, but it turns out that this too is infeasible for
    large-*n* cases.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么一开始不直接使用多项式回归呢？为什么使用神经网络（NN）？一个答案是，对于大规模的*p*数据，直接使用`lm()`或`glm()`可能会涉及非常多的多项式项，这样计算上是不可行的，这会导致内存问题。（对于神经网络而言，这不是问题，因为它们通过迭代找到最小二乘解。虽然这可能会导致收敛问题，但至少使用的内存较少。）核技巧在这里非常有用，甚至有一种*核岭回归*方法，将其应用于线性岭回归模型，但事实证明，这对于大规模*n*数据也是不可行的。
- en: NNs have their own computational issues, as noted, but through trying many combinations
    of hyperparameters, we may still have a good outcome. Also, if we manage to find
    a good NN fit on some classes of problems, sometimes we can tweak it to find a
    good NN fit on some related class (*transfer learning*).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面所提到的，神经网络有自己的计算问题，但通过尝试多种超参数组合，我们仍然可能得到良好的结果。此外，如果我们能够在某些类别的问题上找到一个好的神经网络拟合，有时我们可以调整它，以便在一些相关类别的问题上找到一个好的神经网络拟合（*迁移学习*）。
- en: 11.10 Bias vs. Variance in Neural Networks
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.10 神经网络中的偏差与方差
- en: We often refer to the number of hidden layers as the *depth* of a network and
    the number of units per layer as the *width*. The larger the product of these
    two (actually depth times the square of the width), the more weights or parameters
    the network has. As discussed in [Section 8.10.1](ch08.xhtml#ch08lev10sec1), the
    more parameters a model has, the more variance increases, even though bias is
    reduced.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常将隐藏层的数量称为网络的*深度*，每层的单元数量称为*宽度*。这两者的乘积（实际上是深度乘以宽度的平方）越大，网络的权重或参数就越多。如[第8.10.1节](ch08.xhtml#ch08lev10sec1)中讨论的，模型的参数越多，方差就越大，即使偏差有所减少。
- en: This can be seen as well in light of the polynomial regression connection to
    NNs described in the previous section. Roughly speaking, the larger the number
    of hidden layers in an NN, the higher the degree of a polynomial regression approximation.
    And the higher the degree of a polynomial regression model, the smaller the bias
    but the larger the variance.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这一点也可以从前一节中提到的多项式回归与神经网络的联系来说明。粗略地说，神经网络中隐藏层的数量越大，多项式回归的近似度就越高。而且，多项式回归模型的阶数越高，偏差越小，但方差越大。
- en: So, NNs are not immune to the Bias-Variance Trade-off. This must be kept in
    mind when designing an NN’s architecture.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，神经网络并不是偏差-方差权衡的免疫者。在设计神经网络架构时，必须牢记这一点。
- en: 11.11 Discussion
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.11 讨论
- en: NNs have played a major role in the “ML revolution” of recent years, with notable
    success in certain types of applications. But they can incur huge computational
    costs, in some cases having run times measured in hours or even days, and can
    have vexing convergence problems.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络在近年来的“机器学习革命”中发挥了重要作用，在某些类型的应用中取得了显著的成功。但它们也可能带来巨大的计算成本，在某些情况下，运行时间可达数小时甚至数天，并且可能存在令人头痛的收敛问题。
- en: In addition, folklore in the ML community suggests that NNs are not especially
    effective with *tabular data*, meaning the type stored in data frames—that is,
    every dataset we have seen so far in this book. The reader may wish to reserve
    NNs for usage in applications such as image recognition and natural language processing,
    which will be covered in the next two chapters.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，机器学习社区的传言表明，神经网络在处理*表格数据*时并不特别有效，这里指的是存储在数据框中的数据——也就是本书中迄今为止所讨论的所有数据集。读者可能希望将神经网络保留用于图像识别和自然语言处理等应用，这些内容将在接下来的两章中讨论。
