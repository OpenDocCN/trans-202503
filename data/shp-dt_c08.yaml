- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Homotopy Algorithms
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 同伦算法
- en: '![](image_fi/book_art/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/book_art/chapterart.png)'
- en: 'In this chapter, we’ll explore algorithms related to homotopy, a way to classify
    topological objects based on path types around the object, including homotopy-based
    calculations of regression parameters. Local minima and maxima often plague datasets:
    they provide suboptimal stopping points for algorithms that explore solution spaces
    locally. In the next few pages, we’ll see how homotopy solves this problem.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将探讨与同伦相关的算法，一种基于物体周围路径类型对拓扑物体进行分类的方法，包括基于同伦的回归参数计算。局部极小值和极大值经常困扰数据集：它们为局部探索解空间的算法提供了次优的停止点。在接下来的几页中，我们将看到同伦是如何解决这个问题的。
- en: Introducing Homotopy
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引入同伦
- en: Two paths or functions are *homotopic* to each other if they can be continuously
    deformed into each other within the space of interest. Imagine a golf course and
    a pair of golfers, one who is a better putter than the other. The ball can travel
    to the hole along many different paths. Imagine tracing out the path of each shot
    on the green with rope. One path might be rather direct. The other might meander
    quite a bit before finding the hole, particularly if the green is hilly. A bad
    golfer may have to make many shots, resulting in a long, jagged path. But no matter
    how many hills exist or how many strokes it takes for the golfer’s ball to make
    it into the hole, we could shorten each of these paths by deforming the rope,
    as depicted in [Figure 8-1](#figure8-1).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两条路径或函数可以在感兴趣的空间内连续变形为彼此，则它们是*同伦的*。想象一下一个高尔夫球场和一对高尔夫球手，其中一个比另一个更擅长推杆。球可以沿着许多不同的路径到达洞口。想象一下，用绳子标出每次击球的路径。一条路径可能相当直接，而另一条路径可能会曲折很远，特别是当果岭崎岖不平时。一个差劲的高尔夫球手可能需要打很多次，这样就会形成一条又长又曲折的路径。但无论果岭上有多少丘陵，或者球手的球需要多少次击打才能进洞，我们都可以通过变形绳子来缩短这些路径，正如在[图
    8-1](#figure8-1)中所示。
- en: '![](image_fi/503083c08/f08001_m.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c08/f08001_m.png)'
- en: 'Figure 8-1: A long path to the hole of a golf course (left) deformed to a shorter
    path to the hole (right)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-1：高尔夫球场到洞口的长路径（左）变形为到洞口的短路径（右）
- en: Let’s stretch the analogy somewhat and imagine a sinkhole has appeared in the
    golf course. Topological objects and spaces with holes can complicate this deformation
    process and lead to many different possible paths from one point to another. Paths
    can connect two points on an object. Depending on the object’s properties, these
    paths can sometimes “wiggle” enough to overlap with another path without having
    to cut the path into pieces to get around an obstacle (usually a hole). Winding
    paths around holes presents a problem to continuous deformation of one path into
    another. It’s not possible for the path to wind or wiggle around a hole, such
    that a path between points will necessarily overlap with another path. Different
    types of paths begin to emerge as holes and paths around holes are added. One
    path might make only one loop around a hole before connecting two points. Another
    might make several loops around a hole before connecting two points. Imagine golfing
    again. Let’s say that the green has an obstacle (such as a rock or a water hazard)
    in the middle of it, creating a torus with tricky hills around it that can force
    a bad shot to require circumnavigating the rock to get back to the hole, as you
    can see in [Figure 8-2](#figure8-2).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍微扩展一下这个类比，假设高尔夫球场上出现了一个天坑。具有孔洞的拓扑物体和空间可能会使变形过程变得复杂，并导致从一个点到另一个点的多条可能路径。路径可以连接物体上的两个点。根据物体的性质，这些路径有时可以“扭曲”到足以与另一条路径重叠，而不必将路径切割成片段来绕过障碍物（通常是孔洞）。绕过孔洞的曲折路径为路径的连续变形带来了问题。路径不可能绕着一个孔洞扭曲或弯曲，直到两点之间的路径必然与另一条路径重叠。随着孔洞和孔洞周围的路径的增加，开始出现不同类型的路径。一条路径可能仅绕一个孔做一次循环，然后连接两个点。另一条路径可能需要绕一个孔做几次循环，才能连接两个点。再想象一下打高尔夫。假设球场的果岭中央有一个障碍物（例如一块岩石或一个水障碍），形成一个环面（圆环形状），周围有复杂的丘陵，迫使一个糟糕的击球手绕过岩石才能回到洞口，正如你在[图
    8-2](#figure8-2)中所看到的那样。
- en: '![](image_fi/503083c08/f08002_m.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c08/f08002_m.png)'
- en: 'Figure 8-2: Two paths with the same start and end points on a torus course
    (donut) that cannot be morphed into each other without being cut or having the
    inner hole removed'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-2：在环面（甜甜圈形状）球场上，两个起点和终点相同的路径，无法通过变形而不被切割或去除内部孔洞来变换成彼此
- en: In this scenario, we can no longer deform paths into each other without cutting
    the line or removing the hole. As more holes or holes of larger dimension are
    added, more classes of equivalent paths begin to emerge, with equivalent paths
    having the same number of loops around one or more holes. A two-dimensional course
    will have fewer possible paths from the tee to the hole than a three-dimensional
    course, as fewer possible types of obstacles and holes in the course exist. A
    space with many holes or obstacles in many different dimensions presents a lot
    of obstacles that paths can wind around. This means many unique paths exist for
    that space.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们不再能够将路径变形为彼此相同的形状，而不切断路径或移除孔洞。随着更多孔洞或更大维度的孔洞的添加，会出现更多等价路径的类别，这些等价路径在一个或多个孔洞周围有相同数量的回路。二维的赛道从起点到终点的可能路径少于三维赛道，因为赛道中的障碍物和孔洞的种类较少。一个包含多个维度的多个孔洞或障碍物的空间，会给路径带来更多可以绕过的障碍物。这意味着对于这个空间存在许多独特的路径。
- en: Given that datasets often contain holes of varying dimension, many different
    classes of paths may exist in the data. Random walks on the data, common in Bayesian
    analyses and robotic navigation path-finding tasks, may not be equivalent. This
    can be an advantage in navigation problems, allowing the system to choose from
    a set of different paths with different cost weights related to length, resource
    allocation, and ease of movement. For instance, in the path-finding problem in
    [Figure 8-3](#figure8-3), perhaps obstacle 2 has sharp ends that could harm the
    system should it get too close, making the leftmost path the ideal one for the
    system to take.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到数据集通常包含不同维度的孔洞，数据中可能存在许多不同类别的路径。常见于贝叶斯分析和机器人导航路径寻找任务中的随机游走，可能并不等价。这在导航问题中可以是一种优势，允许系统从一组不同的路径中选择，且这些路径有着与长度、资源分配和移动难易度相关的不同成本权重。例如，在[图
    8-3](#figure8-3)中的路径寻找问题中，可能障碍物 2 有尖锐的边缘，如果系统太接近，可能会造成伤害，从而使得最左侧的路径成为系统的理想选择。
- en: '![](image_fi/503083c08/f08003.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c08/f08003.png)'
- en: 'Figure 8-3: An example obstacle course with navigation from a start point to
    a finish point with several possible solutions'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-3：一个示例障碍赛道，包含从起点到终点的导航，且有多个可能的解决方案
- en: '[Figure 8-3](#figure8-3) shows three paths, and none of them can be deformed
    into another of the paths without moving an obstacle or cutting the path. These
    are unique paths in the space. By counting the total number of unique paths, we
    can classify the space topologically.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 8-3](#figure8-3)展示了三条路径，其中没有一条可以变形为另一条路径，除非移动障碍物或切断路径。这些是在空间中的独特路径。通过计算独特路径的总数，我们可以从拓扑上对空间进行分类。'
- en: Introducing Homotopy-Based Regression
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引入基于同伦的回归
- en: As mentioned, datasets often contain obstacles in the form of local optima,
    that is, local maximums and minimums. Gradient descent algorithms and other stepwise
    optimization algorithms can get stuck there. You can think of this as the higher-dimensional
    version of hills and valleys (saddle points, which are higher-dimensional inflection
    points, can also pose optimization issues). Getting stuck in a local optimum provides
    less-than-ideal solutions to an optimization problem.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，数据集常常包含局部最优解形式的障碍物，即局部最大值和最小值。梯度下降算法和其他逐步优化算法可能会陷入这些局部最优解。你可以把它看作是高维版的丘陵和山谷（鞍点，即更高维的拐点，也可能带来优化问题）。陷入局部最优解通常会给优化问题提供不理想的解。
- en: Homotopy-based algorithms can help with the estimation of parameters in high-dimensional
    data containing many local optima, under which conditions many common algorithms
    such as gradient descent can struggle. Finding a solution in a space with fewer
    local optima and then continuously deforming that solution to the original space
    can lead to better accuracy of estimates and variables selected in a model.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 基于同伦的算法可以帮助估计包含多个局部最优解的高维数据中的参数，在这些条件下，许多常见的算法如梯度下降可能会遇到困难。通过在一个局部最优解较少的空间中找到一个解，并将其持续变形到原始空间，可以提高模型中估计的准确性和所选变量的效果。
- en: To provide more insight, consider a blindfolded person trying to navigate through
    an industrial complex ([Figure 8-4](#figure8-4)). Without a tether, they are sure
    to bump into obstacles and potentially think they have hit their target when they
    are stopped by one of the larger obstacles.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供更多的见解，假设一个蒙眼的人试图穿越一个工业区（[图 8-4](#figure8-4)）。没有任何连接，他们肯定会碰到障碍物，并可能误以为自己已经达到了目标，直到被其中一个更大的障碍物停住。
- en: '![](image_fi/503083c08/f08004.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c08/f08004.png)'
- en: 'Figure 8-4: A blindfolded person navigating an obstacle course'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-4：一个蒙眼的人在障碍课程中导航
- en: However, if they are given a rope connecting their starting point to their ending
    point, they can navigate between the points a bit better and know that any obstacle
    they encounter is likely not the true ending point. There are many possible ways
    to connect the start and finish points. [Figure 8-5](#figure8-5) shows one possible
    rope configuration.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果他们有一根绳子将起点和终点连接起来，他们就能更好地在这些点之间导航，并且知道他们遇到的任何障碍物可能都不是最终的终点。连接起点和终点的方式有很多种。[图
    8-5](#figure8-5)展示了其中一种可能的绳子配置。
- en: '![](image_fi/503083c08/f08005.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c08/f08005.png)'
- en: 'Figure 8-5: A blindfolded person navigating an obstacle course with a guide
    rope'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-5：一个蒙眼的人在带有引导绳的障碍课程中导航
- en: A blindfolded person struggling to avoid physical obstacles is analogous to
    a machine learning algorithm avoiding local optima. For example, let’s consider
    a function of two variables with a global maximum and minimum but other local
    optima, as derived in [Listing 8-1](#listing8-1).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一个蒙眼的人努力避免物理障碍物，可以类比为一个机器学习算法避免局部最优解。例如，我们可以考虑一个具有全局最大值和最小值，但存在其他局部最优解的二元函数，如[示例
    8-1](#listing8-1)中所推导的。
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Listing 8-1: A script that creates a function of two variables with a global
    minimum and maximum but many other local optima'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 8-1：一个创建二元函数的脚本，该函数具有全局最小值和最大值，但有许多其他局部最优解
- en: The code in [Listing 8-1](#listing8-1) produces the plot in [Figure 8-6](#figure8-6),
    from which we can see many minima and maxima. The other optima are local optima,
    some of which are very close to the global minimum or maximum.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 8-1](#listing8-1)中的代码生成了[图 8-6](#figure8-6)中的图表，从中我们可以看到许多极小值和极大值。其他的最优解是局部最优解，其中一些非常接近全局最小值或最大值。'
- en: '![](image_fi/503083c08/f08006.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c08/f08006.png)'
- en: 'Figure 8-6: A scatterplot of three-dimensional data, namely, a function with
    many local optima'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-6：三维数据的散点图，即一个具有多个局部最优解的函数
- en: An algorithm trying to optimize this function will likely get stuck in one of
    the local optima, as the values near the local optima are increasing or decreasing
    from that optimum’s value. Some algorithms that have been known to struggle with
    this type of optimization include gradient descent and the expectation-maximization
    (EM) algorithm, among others. Optimization strategies such as evolutionary algorithms
    will also likely take a long time to find global solutions, making them less ideal
    for this type of data.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一个试图优化这个函数的算法很可能会陷入其中一个局部最优解，因为局部最优解附近的值会从该最优解的值上升或下降。一些已知在这种优化类型中表现不佳的算法包括梯度下降法和期望最大化（EM）算法等。像进化算法这样的优化策略也很可能需要很长时间才能找到全局解，这使得它们对于这种数据来说不太理想。
- en: Homotopy-based calculations provide an effective solution to this problem of
    local optima traps; algorithms employing homotopy-based calculations can wiggle
    around or out of local optima. In essence, these algorithms start with an easy
    optimization problem, in which no local optima are present, and deform the solution
    slowly according to the dataset and its geometry, avoiding local optima as the
    deformation proceeds.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 基于同伦的计算为这个局部最优解陷阱问题提供了有效的解决方案；使用基于同伦的计算的算法可以在局部最优解之间摆动，或者脱离局部最优解。实质上，这些算法从一个简单的优化问题开始，在这个问题中没有局部最优解，并根据数据集及其几何形状缓慢变形解，随着变形的进行，避免局部最优解。
- en: 'Homotopy-based optimization methods commonly used these days in machine learning
    include support vector machines, Lasso, and even neural networks. The lasso2 package
    of R is one package that implements homotopy-based models; in this case, lasso2
    implements a homotopy-based model for the Lasso algorithm. Let’s first explore
    model fit and solutions for the data generated in [Listing 8-1](#listing8-1),
    in which the outcome has many local optima and the predictors are collinear, a
    problem for many machine learning algorithms. Add the following to the code in
    [Listing 8-1](#listing8-1):'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 目前在机器学习中常用的基于同伦的优化方法包括支持向量机、Lasso，甚至神经网络。R的lasso2包是一个实现基于同伦模型的包；在这个案例中，lasso2实现了一个基于同伦的Lasso算法模型。让我们首先探讨在[示例
    8-1](#listing8-1)中生成的数据的模型拟合和解，其中结果有多个局部最优解，并且预测变量是共线性的，这对于许多机器学习算法来说是一个问题。将以下代码添加到[示例
    8-1](#listing8-1)中的代码：
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, the model is ready to be built and tested. The outcome of interest (our
    variable `z`) is not normally distributed, but a Gaussian distribution is the
    closest available distribution for use in the model. In the following addition
    to the script in [Listing 8-1](#listing8-1), the `etastart` parameter needs to
    be set to null before starting the model iterations, and a bound needs to be in
    place to guide the homotopy-based parameter search. Generally, a lower setting
    is best:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，模型已经准备好构建和测试。我们关心的结果（我们的变量`z`）并不是正态分布的，但高斯分布是模型中最接近的可用分布。在以下脚本的补充部分 [Listing
    8-1](#listing8-1) 中，`etastart` 参数需要在开始模型迭代之前设置为 null，并且需要设置一个边界来引导基于同伦法的参数搜索。通常情况下，较低的设置效果最好：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This script now fits a homotopy-based Lasso model to the training data and
    then predicts test data outcomes based on this model, allowing us to assess the
    model fit. The mean square error for this sample, calculated in the final line,
    should be near 2.30\. (Again, results may vary with R versions, as the seeding
    and sampling algorithms changed.) The results of the model suggest that one term
    dominates the behavior of the function:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本现在将基于同伦法的 Lasso 模型拟合到训练数据上，然后基于该模型预测测试数据的结果，允许我们评估模型拟合度。该样本的均方误差，计算在最后一行，应该接近
    2.30\。（再说一遍，结果可能因 R 版本不同而有所变化，因为种子和采样算法发生了变化。）模型的结果表明，有一个项主导了函数的行为：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: These results, which may vary for readers with different versions of R, show
    that only one variable is selected as important to the model. `x` contributes
    more to the prediction of `z` than `y` contributes, according to our model. Linear
    regression isn’t a great tool to use on this problem, given the nonlinear relationships
    between `x` or `y` and `z`, but it does find some consistency in the relationship.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果可能会因不同 R 版本的读者而有所不同，显示出只有一个变量被选为模型的重要变量。根据我们的模型，`x` 对 `z` 的预测贡献大于 `y` 的贡献。考虑到
    `x` 或 `y` 与 `z` 之间的非线性关系，线性回归并不是解决这个问题的好工具，但它确实发现了这种关系中的一些一致性。
- en: 'To compare with another method, let’s create a linear regression model and
    add it to [Listing 8-1](#listing8-1):'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了与另一种方法进行比较，让我们创建一个线性回归模型，并将其添加到 [Listing 8-1](#listing8-1)：
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This code trains a linear model on the training data and predicts test set
    outcomes, similar to how the homotopy-based model was fit with the previous code.
    You may get a warning with your regression model, as there is covarying behavior
    of `x` and `y` (which presents issues to linear regression models per the assumption
    of noncollinearity). Let’s take a look at this model’s results:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码在训练数据上训练一个线性模型，并预测测试集的结果，类似于之前的代码中如何拟合基于同伦法的模型。你可能会收到回归模型的警告，因为`x`和`y`存在共变行为（根据非共线性假设，这对线性回归模型是有问题的）。让我们来看一下这个模型的结果：
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The mean square error (MSE) for this sample should be near 2.30, which is the
    same as the homotopy-based model. MSE accounts for both variance and bias in the
    estimator, giving a balanced view of how well the algorithm is performing on a
    regression task. However, the collinearity is problematic for the linear regression
    model. Penalized models avoid this issue, including homotopy-based Lasso models.
    Of note, the coefficients found by the linear regression and the homotopy-based
    Lasso model are identical. Typically, models with different optimization strategies
    will vary a bit on their estimates. In this case, the sample size is probably
    large and the number of predictors few enough for both algorithms to converge
    to a global optimum.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 该样本的均方误差（MSE）应该接近 2.30，与基于同伦法的模型相同。MSE 既考虑了方差，也考虑了估计器的偏差，提供了算法在回归任务中表现的平衡视角。然而，线性回归模型存在共线性问题。惩罚性模型可以避免这个问题，包括基于同伦法的
    Lasso 模型。值得注意的是，线性回归和基于同伦法的 Lasso 模型找到的系数是相同的。通常，采用不同优化策略的模型在估计上会有些许不同。在这种情况下，样本量可能较大，预测变量较少，导致两种算法都收敛到全局最优解。
- en: Comparing Results on a Sample Dataset
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在样本数据集上比较结果
- en: Let’s return to our self-reported educational dataset and explore the relationships
    between school experiences, IQ, and self-reported depression. Because we don’t
    know what the function between these predictors and depression should be, we don’t
    know what sort of local optima might exist. However, we do know that a training
    dataset with 7 predictors and 16 individuals (70 percent of the data) will be
    sparse, and it’s possible that local optima are a problem in the dataset. There
    is evidence that geometry-based linear regression models work better on sparse
    datasets than other algorithms, and it’s possible that our homotopy-based Lasso
    model will work better, as well.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到自我报告的教育数据集，探索学校经历、IQ和自我报告的抑郁症之间的关系。由于我们不知道这些预测变量与抑郁症之间的函数关系应该是什么，我们不确定是否存在局部最优解。然而，我们确实知道，包含7个预测变量和16个个体（占数据的70%）的训练数据集会比较稀疏，并且局部最优解可能会是数据集中的一个问题。有证据表明，基于几何的线性回归模型在稀疏数据集上比其他算法表现得更好，因此我们的基于同伦的Lasso模型也可能会表现得更好。
- en: Let’s create [Listing 8-2](#listing8-2) and partition the data into training
    and test sets.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来创建[清单 8-2](#listing8-2)，并将数据划分为训练集和测试集。
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Listing 8-2: A script that loads and then analyzes the Quora IQ sample'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 8-2：加载并分析Quora IQ样本的脚本
- en: 'Now, let’s run a homotopy-based Lasso model and a logistic regression model
    to compare results on this small, real-world dataset:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们运行基于同伦的Lasso模型和逻辑回归模型，并对这个小型真实世界数据集的结果进行比较：
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: From running the models in the previous script addition, we should see that
    the homotopy-based Lasso model has a higher accuracy (~85 percent) than the logistic
    regression (~70 percent); additionally, the logistic regression model spits out
    a warning message about fitted probabilities of 0 or 1 occurring. This means the
    data is quite separated into groups, which can happen when small data with strong
    relationships to an outcome is split. Depending on your version of R or GUI, you
    may end up with a different sample and, thus, somewhat different fit statistics
    and results. Because this is a relatively small sample to begin with, it’s possible
    that you’ll have slightly different results than the ones presented here. Some
    samples may not have any instances of a given predictor within the dataset. Larger
    samples would create more stable models across samples.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面脚本中运行模型的结果来看，我们应该会发现基于同伦的Lasso模型的准确率（大约85%）高于逻辑回归模型（大约70%）；此外，逻辑回归模型会输出一个关于拟合概率为0或1的警告信息。这意味着数据被明显分成了不同的组别，这种情况通常发生在小样本数据中，且数据与结果之间有较强的关系。根据你使用的R版本或图形用户界面（GUI），你可能会得到不同的样本，因此拟合统计和结果可能会略有不同。由于这个样本本身就相对较小，结果可能会与这里呈现的结果略有不同。某些样本中可能根本没有某些预测变量的实例。更大的样本会产生跨样本更稳定的模型。
- en: 'Let’s look more closely at the homotopy-based Lasso model and its coefficients:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地查看基于同伦的Lasso模型及其系数：
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: From the previous output, we can see that, for this sample, higher IQ, endorsement
    of boredom, and being put in a remedial class increase the likelihood of self-reported
    depression. However, outside learning has a strong protective effect. In fact,
    outside learning can completely counterbalance the risk from boredom and being
    placed in a remedial course. This suggests that parents of profoundly gifted children
    who are experiencing school issues may be able to mitigate some of the potential
    adverse outcomes, such as depression, by providing outside learning opportunities,
    such as college courses in the evening, tutoring outside of school, or other opportunities
    for the child to learn. The role of outside learning opportunities has been explored
    to some extent in the giftedness literature with similar results, but more research
    is needed on this topic.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的输出结果中，我们可以看到，对于这个样本，较高的IQ、无聊的认同以及被安排进入辅导班会增加自我报告的抑郁症的可能性。然而，外部学习具有强大的保护作用。事实上，外部学习能够完全抵消由于无聊和被安排进辅导班带来的风险。这表明，对于那些在学校遇到问题的天才儿童，家长可以通过提供外部学习机会，例如晚间大学课程、课外辅导或其他学习机会，来减轻一些潜在的不良后果，例如抑郁症。外部学习机会在天才儿童文献中已经在一定程度上得到了探讨，并得出了类似的结果，但这一话题仍需要进一步的研究。
- en: 'Now, let’s compare these results with the results of the logistic regression
    model:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将这些结果与逻辑回归模型的结果进行比较：
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Examining the previous output, it seems that the logistic regression model could
    not handle the dataset, giving errors and spitting out very large coefficients.
    This is likely related to the smallness of the data, where the linear system is
    underdetermined; however, this is not a situation where the number of predictors
    outnumber the number of individuals in the sample, so it is likely a function
    of the data itself rather than purely sample size.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检查之前的输出，似乎逻辑回归模型无法处理该数据集，出现了错误并且输出了非常大的系数。这可能与数据的较小规模有关，其中线性系统是欠定的；然而，这并不是一个预测因子的数量超过样本中个体数量的情况，因此更可能是数据本身的特点，而不仅仅是样本量问题。
- en: Note the model fails to find any significant predictors of self-reported depression.
    Linear regression can’t handle this dataset very well, and the results are not
    reliable. For some samples, certain variables may not be computable in the linear
    regression model at all. Homotopy-based models (and other types of penalized models)
    often work better on small datasets, and there is some evidence that they perform
    better for datasets with many local optima. While this dataset is a bit small
    for fitting a model, it does demonstrate the power of homotopy-based optimization
    (and penalized regression, in general) on very small datasets, and its results
    make a lot more sense than the linear regression model’s results.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，模型未能找到任何显著的自我报告抑郁症的预测因子。线性回归对这个数据集处理得不好，结果不可靠。对于某些样本，某些变量可能在线性回归模型中根本无法计算。同伦模型（以及其他类型的惩罚模型）通常在小型数据集上表现更好，并且有证据表明它们在具有多个局部极值的数据集上表现得更好。尽管这个数据集对于拟合模型来说有些小，但它确实展示了基于同伦的优化（以及一般的惩罚回归）在非常小的数据集上的强大能力，而且其结果比线性回归模型的结果更有意义。
- en: Summary
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we gave you an overview of homotopy and its applications in
    machine learning, including through an example of homotopy as an extension of
    regression-based algorithms on a simulated problem and a real dataset. Homotopy
    can help regression algorithms avoid local optima that often trap local optimizers.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们概述了同伦及其在机器学习中的应用，包括通过一个同伦作为回归算法扩展的示例，应用于一个模拟问题和一个真实数据集。同伦可以帮助回归算法避免局部极值，这些局部极值往往困住局部优化器。
- en: Other uses of homotopy algorithms in the field of artificial intelligence include
    navigational problems. For instance, an autonomous cart may need to navigate the
    halls and obstacles of a hospital by weighting different possible paths from its
    current location to its destination. Homotopy algorithms are often used to generate
    the possible paths, which are then weighted by time cost or hazard cost of the
    route. Bounds can also be placed to avoid generating paths that obviously aren’t
    viable (such as going through areas where the cart can’t physically go or wouldn’t
    be wanted—such as an operating room). It’s likely that this branch of topological
    data analysis will grow in the coming years, and we encourage you to explore other
    uses of homotopy in machine learning, robotics, differential equations, and engineering.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 同伦算法在人工智能领域的其他应用包括导航问题。例如，一个自主小车可能需要通过加权从当前位置到目的地的不同可能路径，来穿越医院的走廊和障碍物。同伦算法常被用来生成可能的路径，接着再根据时间成本或路线的危险成本对这些路径进行加权。还可以设置限制条件，以避免生成显然不可行的路径（例如通过小车无法物理通行的区域，或不希望进入的区域——如手术室）。这一拓扑数据分析的分支在未来几年可能会增长，我们鼓励你探索同伦在机器学习、机器人学、微分方程和工程学中的其他应用。
