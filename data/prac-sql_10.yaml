- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Inspecting and Modifying Data
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 检查与修改数据
- en: '![](Images/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/chapterart.png)'
- en: If I were to propose a toast to a newly minted class of data analysts, I’d raise
    my glass and say, “May your data arrive perfectly structured and free of errors!”
    In reality, you’ll sometimes receive data in such a sorry state that it’s hard
    to analyze without modifying it. This is called *dirty data*, a general label
    for data with errors, missing values, or poor organization that makes standard
    queries ineffective. In this chapter, you’ll use SQL to clean a set of dirty data
    and perform other useful maintenance tasks to make data workable.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我要向刚刚获得认证的数据分析师们敬酒，我会举杯说道：“愿你们的数据完美结构化，且无任何错误！”但现实中，你有时会收到状态糟糕的数据，难以分析，除非先进行修改。这就是所谓的*脏数据*，是一个总称，用于描述包含错误、缺失值或组织不良的数据，这些数据会使标准查询变得无效。在这一章中，你将使用SQL清理一组脏数据，并执行其他有用的维护任务，使数据变得可用。
- en: Dirty data can have multiple origins. Converting data from one file type to
    another or giving a column the wrong data type can cause information to be lost.
    People also can be careless when inputting or editing data, leaving behind typos
    and spelling inconsistencies. Whatever the cause may be, dirty data is the bane
    of the data analyst.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 脏数据可能有多种来源。从一种文件类型转换为另一种文件类型，或将列赋予错误的数据类型，可能导致信息丢失。人在输入或编辑数据时也可能不小心，留下拼写错误和不一致的地方。不论原因是什么，脏数据都是数据分析师的噩梦。
- en: You’ll learn how to examine data to assess its quality and how to modify data
    and tables to make analysis easier. But the techniques you’ll learn will be useful
    for more than just cleaning data. The ability to make changes to data and tables
    gives you options for updating or adding new information to your database as it
    becomes available, elevating your database from a static collection to a living
    record.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 你将学习如何检查数据质量，如何修改数据和表格以便更容易进行分析。但你将学到的技巧不仅仅适用于清理数据。能够对数据和表格进行修改，让你有机会在新信息可用时更新或添加它们，从而使你的数据库从静态集合变成一个动态记录。
- en: Let’s begin by importing our data.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从导入数据开始吧。
- en: Importing Data on Meat, Poultry, and Egg Producers
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 导入肉类、家禽和蛋类生产商的数据
- en: For this example, we’ll use a directory of US meat, poultry, and egg producers.
    The Food Safety and Inspection Service (FSIS), an agency within the US Department
    of Agriculture, compiles and updates this database regularly. The FSIS is responsible
    for inspecting animals and food at more than 6,000 meat processing plants, slaughterhouses,
    farms, and the like. If inspectors find a problem, such as bacterial contamination
    or mislabeled food, the agency can issue a recall. Anyone interested in agriculture
    business, food supply chain, or outbreaks of foodborne illnesses will find the
    directory useful. Read more about the agency on its site at [https://www.fsis.usda.gov/](https://www.fsis.usda.gov/).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用美国肉类、家禽和蛋类生产商的目录。食品安全和检验局（FSIS），是美国农业部下属的一个机构，负责定期汇编并更新该数据库。FSIS负责检查超过6,000家肉类加工厂、屠宰场、农场等地的动物和食品。如果检查员发现问题，例如细菌污染或标签错误，机构可以发出召回通知。任何对农业业务、食品供应链或食源性疾病爆发感兴趣的人都将发现该目录很有用。你可以在该机构的网站[https://www.fsis.usda.gov/](https://www.fsis.usda.gov/)上了解更多信息。
- en: The data we’ll use comes from [https://www.data.gov/](https://www.data.gov/),
    a website run by the US federal government that catalogs thousands of datasets
    from various federal agencies ([https://catalog.data.gov/dataset/fsis-meat-poultry-and-egg-inspection-directory-by-establishment-name/](https://catalog.data.gov/dataset/fsis-meat-poultry-and-egg-inspection-directory-by-establishment-name/)).
    I’ve converted the Excel file posted on the site to CSV format, and you’ll find
    a link to the file *MPI_Directory_by_Establishment_Name.csv* along with other
    resources for this book at [https://nostarch.com/practical-sql-2nd-edition/](https://nostarch.com/practical-sql-2nd-edition/).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的数据来自[https://www.data.gov/](https://www.data.gov/)，这是美国联邦政府运营的网站，汇集了来自各种联邦机构的数千个数据集（[https://catalog.data.gov/dataset/fsis-meat-poultry-and-egg-inspection-directory-by-establishment-name/](https://catalog.data.gov/dataset/fsis-meat-poultry-and-egg-inspection-directory-by-establishment-name/)）。我已将该网站上发布的Excel文件转换为CSV格式，你可以在[https://nostarch.com/practical-sql-2nd-edition/](https://nostarch.com/practical-sql-2nd-edition/)找到*MPI_Directory_by_Establishment_Name.csv*文件和本书的其他资源链接。
- en: To import the file into PostgreSQL, use the code in [Listing 10-1](#listing10-1)
    to create a table called `meat_poultry_egg_establishments` and use `COPY` to add
    the CSV file to the table. As in previous examples, use pgAdmin to connect to
    your `analysis` database, and then open the Query Tool to run the code. Remember
    to change the path in the `COPY` statement to reflect the location of your CSV
    file.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 要将文件导入到 PostgreSQL 中，使用 [清单 10-1](#listing10-1) 中的代码创建一个名为 `meat_poultry_egg_establishments`
    的表，并使用 `COPY` 将 CSV 文件添加到表中。像之前的示例一样，使用 pgAdmin 连接到你的 `analysis` 数据库，然后打开查询工具运行代码。记得更改
    `COPY` 语句中的路径，以反映你的 CSV 文件的位置。
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Listing 10-1: Importing the FSIS Meat, Poultry, and Egg Inspection Directory'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 10-1：导入 FSIS 肉类、家禽和蛋类检查目录
- en: The table has 10 columns. We add a natural primary key constraint to the `establishment_number`
    column 1, which will hold unique values that identify each establishment. Most
    of the remaining columns relate to the company’s name and location. You’ll use
    the `activities` column 2, which describes activities at the company, in the “Try
    It Yourself” exercise at the end of this chapter. We set most columns to `text`.
    In PostgreSQL, `text` is a varying length data type that affords us up to 1GB
    of data (see Chapter 4). The column `dbas` contains strings of more than 1,000
    characters in its rows, so we’re prepared to handle that. We import the CSV file
    3 and then create an index on the `company` column 4 to speed up searches for
    particular companies.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 该表有 10 列。我们为 `establishment_number` 列 1 添加了一个自然主键约束，该列将保存唯一的值以标识每个机构。其余大部分列与公司的名称和位置相关。你将在本章末尾的“动手实践”练习中使用
    `activities` 列 2，它描述了公司的活动。我们将大部分列设置为 `text` 类型。在 PostgreSQL 中，`text` 是一个可变长度的数据类型，允许我们存储最多
    1GB 的数据（参见第 4 章）。`dbas` 列包含超过 1,000 个字符的字符串，因此我们已经准备好处理这些数据。我们导入 CSV 文件 3，然后在
    `company` 列 4 上创建索引，以加速对特定公司的查询。
- en: 'For practice, let’s use the `count()` aggregate function introduced in Chapter
    9 to check how many rows are in the `meat_poultry_egg_establishments` table:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，让我们使用第 9 章中介绍的 `count()` 聚合函数来检查 `meat_poultry_egg_establishments` 表中有多少行：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The result should show 6,287 rows. Now let’s find out what the data contains
    and determine whether we can glean useful information from it as is, or if we
    need to modify it in some way.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 结果应该显示 6,287 行。现在，让我们了解数据包含了什么，并确定是否可以直接从中提取有用信息，或者是否需要以某种方式进行修改。
- en: Interviewing the Dataset
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集采访
- en: Interviewing data is my favorite part of analysis. We interview a dataset to
    discover its details—what it holds, what questions it can answer, and how suitable
    it is for our purposes—the same way a job interview reveals whether a candidate
    has the skills required.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 采访数据是我分析中最喜欢的部分。我们采访数据集以发现其细节——它包含了什么，它能回答哪些问题，以及它对我们的目的是否合适——就像工作面试揭示候选人是否具备所需技能一样。
- en: The aggregate queries from Chapter 9 are a useful interviewing tool because
    they often expose the limitations of a dataset or raise questions you may want
    to ask before drawing conclusions and assuming the validity of your findings.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 第 9 章中的聚合查询是一个有用的采访工具，因为它们常常揭示数据集的局限性，或者提出在得出结论并假设结果有效之前你可能想要问的问题。
- en: For example, the `meat_poultry_egg_establishments` table’s rows describe food
    producers. At first glance, we might assume that each company in each row operates
    at a distinct address. But it’s never safe to assume in data analysis, so let’s
    check using the code in [Listing 10-2](#listing10-2).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，`meat_poultry_egg_establishments` 表的行描述了食品生产商。乍一看，我们可能会假设每一行中的每个公司都在一个不同的地址上运营。但在数据分析中，假设永远是不安全的，所以让我们使用
    [清单 10-2](#listing10-2) 中的代码来验证。
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Listing 10-2: Finding multiple companies at the same address'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 10-2：查找位于相同地址的多个公司
- en: Here, we group companies by unique combinations of the `company`, `street`,
    `city`, and `st` columns. Then we use `count(*)`, which returns the number of
    rows for each combination of those columns and gives it the alias `address_count`.
    Using the `HAVING` clause introduced in Chapter 9, we filter the results to show
    only cases where more than one row has the same combination of values. This should
    return all duplicate addresses for a company.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们按 `company`、`street`、`city` 和 `st` 列的唯一组合对公司进行分组。然后，我们使用 `count(*)`，它返回每个组合的行数，并给它一个别名
    `address_count`。使用第 9 章介绍的 `HAVING` 子句，我们筛选结果，只显示那些具有相同值组合的多行数据。这应该返回公司所有重复的地址。
- en: 'The query returns 23 rows, which means there are close to two dozen cases where
    the same company is listed multiple times at the same address:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 查询返回了23行，这意味着大约有两打的记录，其中同一公司在同一地址出现多次：
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This is not necessarily a problem. There may be valid reasons for a company
    to appear multiple times at the same address. For example, two types of processing
    plants could exist with the same name. On the other hand, we may have found data
    entry errors. Either way, it’s a wise practice to eliminate concerns about the
    validity of a dataset before relying on it, and this result should prompt us to
    investigate individual cases before we draw conclusions. However, this dataset
    has other issues that we need to look at before we can get meaningful information
    from it. Let’s work through a few examples.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这不一定是个问题。公司在同一地址出现多次可能有合理的原因。例如，可能存在两个同名的加工厂。另一方面，我们可能也发现了数据录入错误。不论如何，在依赖数据集之前，消除对其有效性的疑虑是明智的做法，这个结果应该促使我们在得出结论之前，先调查各个具体案例。然而，在我们能够从数据中提取有意义信息之前，这个数据集还有其他问题需要我们注意。我们来通过几个例子探讨一下。
- en: Checking for Missing Values
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查缺失值
- en: 'Next, we’ll check whether we have values from all states and whether any rows
    are missing a state code by asking a basic question: How many meat, poultry, and
    egg processing companies are there in each state? We’ll use the aggregate function
    `count()` along with `GROUP BY` to determine this, as shown in [Listing 10-3](#listing10-3).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将检查是否包含了所有州的值，并且通过一个简单的问题来检查是否有行缺少州代码：每个州的肉类、家禽和蛋类加工公司有多少家？我们将使用聚合函数`count()`并结合`GROUP
    BY`来确定这个数字，如[清单10-3](#listing10-3)所示。
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Listing 10-3: Grouping and counting states'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 清单10-3：按州分组并计数
- en: 'The query is a simple count that tallies the number of times each state postal
    code (`st`) appears in the table. Your result should include 57 rows, grouped
    by the state postal code in the column `st`. Why more than the 50 US states? Because
    the data includes Puerto Rico and other unincorporated US territories, such as
    Guam and American Samoa. Alaska (`AK`) is at the top of the results with a count
    of `17` establishments:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 该查询是一个简单的计数，统计每个州邮政编码（`st`）在表中出现的次数。你的结果应包括57行，按州邮政编码（在`st`列中）分组。为什么超过50个美国州？因为数据中还包括波多黎各和其他未合并的美国领土，例如关岛和美属萨摩亚。阿拉斯加（`AK`）位于结果顶部，共有17家企业：
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: However, the row at the bottom of the list has a `NULL` value in the `st` column
    and a `3` in `st_count`. That means three rows have a `NULL` in `st`. To see the
    details of those facilities, let’s query those rows.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，列表底部的这一行在`st`列中有`NULL`值，并且`st_count`中有`3`。这意味着有三行`st`的值是`NULL`。为了查看这些设施的详细信息，我们来查询这些行。
- en: In [Listing 10-4](#listing10-4), we add a `WHERE` clause with the `st` column
    and the `IS NULL` keywords to find which rows are missing a state code.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在[清单10-4](#listing10-4)中，我们添加了一个`WHERE`子句，结合`st`列和`IS NULL`关键字来查找哪些行缺少州代码。
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Listing 10-4: Using `IS NULL` to find missing values in the `st` column'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 清单10-4：使用`IS NULL`查找`st`列中的缺失值
- en: 'This query returns three rows that don’t have a value in the `st` column:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这个查询返回了三行`st`列没有值的记录：
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: That’s a problem, because any counts that include the `st` column will be incorrect,
    such as the number of establishments per state. When you spot an error such as
    this, it’s worth making a quick visual check of the original file you downloaded.
    Unless you’re working with files in the gigabyte range, you can usually open a
    CSV file in one of the text editors I noted in Chapter 1 and search for the row.
    If you’re working with larger files, you might be able to examine the source data
    using utilities such as `grep` (on Linux and macOS) or `findstr` (on Windows).
    In this case, a visual check of the file from [https://www.data.gov/](https://www.data.gov/)
    confirms that, indeed, there was no state listed in those rows in the file, so
    the error is organic to the data, not one introduced during import.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个问题，因为任何包含`st`列的计数都会不准确，例如按州统计的企业数量。当你发现类似的错误时，值得快速检查一下你下载的原始文件。除非你处理的是数吉字节大小的文件，否则通常可以在我在第一章中提到的文本编辑器中打开CSV文件并搜索该行。如果你处理的是较大的文件，可以使用`grep`（在Linux和macOS上）或`findstr`（在Windows上）等工具查看源数据。在这个例子中，来自[https://www.data.gov/](https://www.data.gov/)的文件经过可视化检查后，确认确实在文件中的这些行没有列出州名，因此这个错误是数据本身的问题，而不是导入过程中引入的错误。
- en: In our interview of the data so far, we’ve discovered that we’ll need to add
    missing values to the `st` column to clean up this table. Let’s look at what other
    issues exist in our dataset and make a list of cleanup tasks.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在对数据的初步审查中，我们发现需要向`st`列添加缺失的值以清理这个表格。让我们看看数据集中还有哪些问题，并列出清理任务。
- en: Checking for Inconsistent Data Values
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查不一致的数据值
- en: Inconsistent data is another factor that can hamper our analysis. We can check
    for inconsistently entered data within a column by using `GROUP BY` with `count()`.
    When you scan the unduplicated values in the results, you might be able to spot
    variations in the spelling of names or other attributes.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 不一致的数据是另一个可能妨碍我们分析的因素。我们可以通过使用`GROUP BY`和`count()`来检查列中不一致输入的数据。当你扫描结果中的去重值时，可能会发现名字或其他属性的拼写差异。
- en: For example, many of the 6,200 companies in our table are multiple locations
    owned by just a few multinational food corporations, such as Cargill or Tyson
    Foods. To find out how many locations each company owns, we count the values in
    the `company` column. Let’s see what happens when we do, using the query in [Listing
    10-5](#listing10-5).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们表格中的6,200家公司中，许多是由少数跨国食品公司拥有的多个地点，如嘉吉公司（Cargill）或泰森食品（Tyson Foods）。为了找出每家公司拥有多少个地点，我们可以统计`company`列中的值。让我们看看使用[列表
    10-5](#listing10-5)中的查询时会发生什么。
- en: '[PRE8]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Listing 10-5: Using `GROUP BY` and `count()` to find inconsistent company names'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10-5：使用`GROUP BY`和`count()`来查找不一致的公司名称
- en: 'Scrolling through the results reveals a number of cases in which a company’s
    name is spelled in several different ways. For example, notice the entries for
    the Armour-Eckrich brand:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动查看结果可以发现一些公司的名称以多种不同的方式拼写。例如，注意到Armour-Eckrich品牌的条目：
- en: '[PRE9]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: At least four different spellings are shown for seven establishments that are
    likely owned by the same company. If we later perform any aggregation by company,
    it would help to standardize the names so all the items counted or summed are
    grouped properly. Let’s add that to our list of items to fix.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 对于七个可能由同一家公司拥有的机构，至少有四种不同的拼写方式。如果我们稍后按公司进行聚合，标准化名称会有所帮助，这样所有计数或求和的项目可以正确分组。让我们将这一点加入需要修复的事项列表。
- en: Checking for Malformed Values Using length()
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用length()检查格式错误的值
- en: It’s a good idea to check for unexpected values in a column that should be consistently
    formatted. For example, each entry in the `zip` column in the `meat_poultry_egg_establishments
    table should be formatted in the style of US ZIP codes with five digits. However,
    that’s not what is in our dataset.`
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 检查列中应该保持一致格式的意外值是个好主意。例如，`meat_poultry_egg_establishments`表中的`zip`列每个条目应该按照美国ZIP代码格式（五位数字）进行格式化。然而，这并不是我们数据集中所包含的内容。
- en: '[PRE10]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
