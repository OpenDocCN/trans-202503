- en: '15'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '15'
- en: '**LOOKING AHEAD: OTHER PARALLEL SYSTEMS AND PARALLEL PROGRAMMING MODELS**'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**展望未来：其他并行系统和并行编程模型**'
- en: '![image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/common.jpg)'
- en: In the previous chapter, we discussed shared memory parallelism and multithreaded
    programming. In this chapter, we introduce other parallel programming models and
    languages for different classes of architecture. Namely, we introduce parallelism
    for hardware accelerators focusing on graphics processing units (GPUs) and general-purpose
    computing on GPUs (GPGPU computing), using CUDA as an example; distributed memory
    systems and message passing, using MPI as an example; and cloud computing, using
    MapReduce and Apache Spark as examples.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了共享内存并行性和多线程编程。在本章中，我们介绍了适用于不同架构类别的其他并行编程模型和语言。具体而言，我们介绍了面向硬件加速器的并行性，重点讨论图形处理单元（GPU）和基于GPU的通用计算（GPGPU计算），以CUDA为例；分布式内存系统和消息传递，以MPI为例；以及云计算，举例说明MapReduce和Apache
    Spark。
- en: 'A Whole New World: Flynn’s Taxonomy of Architecture'
  id: totrans-4
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 全新世界：弗林架构分类法
- en: '*Flynn’s taxonomy* is commonly used to describe the ecosystem of modern computing
    architecture ([Figure 15-1](ch15.xhtml#ch15fig1)).'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*弗林分类法*通常用于描述现代计算架构的生态系统（[图15-1](ch15.xhtml#ch15fig1)）。'
- en: '![image](../images/15fig01.jpg)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/15fig01.jpg)'
- en: '*Figure 15-1: Flynn’s taxonomy classifies the ways in which a processor applies
    instructions.*'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*图15-1：弗林分类法对处理器如何应用指令进行了分类。*'
- en: The horizontal axis refers to the data stream, whereas the vertical axis refers
    to the instruction stream. A *stream* in this context is a flow of data or instructions.
    A *single stream* issues one element per time unit, similar to a queue. In contrast,
    *multiple streams* typically issue many elements per time unit (think of multiple
    queues). Thus, a single instruction stream (SI) issues a single instruction per
    time unit, whereas a multiple instruction stream (MI) issues many instructions
    per time unit. Likewise, a single data stream (SD) issues one data element per
    time unit, whereas a multiple data stream (MD) issues many data elements per time
    unit.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 水平轴表示数据流，而垂直轴表示指令流。在这个上下文中，*流*指的是数据或指令的流动。*单流*每个时间单位发出一个元素，类似于队列。相反，*多流*通常每个时间单位发出多个元素（可以想象成多个队列）。因此，单一指令流（SI）每个时间单位发出一个指令，而多重指令流（MI）每个时间单位发出多个指令。同样，单一数据流（SD）每个时间单位发出一个数据元素，而多重数据流（MD）每个时间单位发出多个数据元素。
- en: A processor can be classified into one of four categories based on the types
    of streams it employs.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 处理器可以根据它使用的流的类型被分类为四种类型之一。
- en: '**SISD**   Single instruction/single data systems have a single control unit
    processing a single stream of instructions, allowing it to execute only one instruction
    at a time. Likewise, the processor can process only a single stream of data or
    process one data unit at a time. Most commercially available processors prior
    to the mid-2000s were SISD machines.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**SISD** 单指令/单数据系统具有一个控制单元，处理一个指令流，只能一次执行一个指令。同样，处理器只能处理一个数据流或每次处理一个数据单元。2000年代中期之前，大多数商业化的处理器都是SISD机器。'
- en: '**MISD**   Multiple instruction/single data systems have multiple instruction
    units performing on a single data stream. MISD systems were typically designed
    for incorporating fault tolerance in mission-critical systems, such as the flight
    control programs for NASA shuttles. That said, MISD machines are rarely used in
    practice anymore.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**MISD** 多指令/单数据系统具有多个指令单元，在一个数据流上执行。MISD系统通常是为在关键任务系统中集成容错功能而设计的，例如NASA航天飞机的飞行控制程序。尽管如此，MISD机器如今在实践中已经很少使用。'
- en: '**SIMD**   Single instruction/multiple data systems execute the *same* instruction
    on multiple data simultaneously and in lockstep fashion. During “lockstep” execution,
    all instructions are placed into a queue, while data is distributed among different
    compute units. During execution, each compute unit executes the first instruction
    in the queue simultaneously, before simultaneously executing the next instruction
    in the queue, and then the next, and so forth. The most well-known example of
    the SIMD architecture is the graphics processing unit. Early supercomputers also
    followed the SIMD architecture. We discuss GPUs more in the next section.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**SIMD** 单指令/多数据系统同时并行地对多个数据执行*相同*的指令，并且以同步方式执行。在“同步”执行过程中，所有指令都会被放入队列中，而数据则在不同的计算单元之间分配。在执行过程中，每个计算单元首先同时执行队列中的第一条指令，然后同时执行队列中的下一条指令，再接着是下一条，以此类推。SIMD架构最著名的例子是图形处理单元（GPU）。早期的超级计算机也采用了SIMD架构。我们将在下一节中进一步讨论GPU。'
- en: '**MIMD**   Multiple instruction/multiple data systems represent the most widely
    used architecture class. They are extremely flexible and have the ability to work
    on multiple instructions or multiple data streams. Since nearly all modern computers
    use multicore CPUs, most are classified as MIMD machines. We discuss another class
    of MIMD systems, distributed memory systems, in “Distributed Memory Systems, Message
    Passing, and MPI” on [page 746](ch15.xhtml#lev1_115).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**MIMD** 多指令/多数据系统代表了最广泛使用的架构类别。它们非常灵活，能够同时处理多条指令或多个数据流。由于几乎所有现代计算机都使用多核CPU，因此大多数计算机都被归类为MIMD机器。在“分布式内存系统、消息传递和MPI”一节中，我们将讨论另一类MIMD系统——分布式内存系统，具体内容见[第746页](ch15.xhtml#lev1_115)。'
- en: '15.1 Heterogeneous Computing: Hardware Accelerators, GPGPU Computing, and CUDA'
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.1 异构计算：硬件加速器、GPGPU计算和CUDA
- en: '*Heterogeneous computing* is computing using multiple, different processing
    units found in a computer. These processing units often have different ISAs, some
    managed by the OS, and others not. Typically, heterogeneous computing means support
    for parallel computing using the computer’s CPU cores and one or more of its accelerator
    units such as *graphics processing units* (GPUs) or *field programmable gate arrays*
    (FPGAs).^([1](ch15.xhtml#fn15_1))'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*异构计算* 是使用计算机中的多个不同处理单元进行计算。这些处理单元通常有不同的指令集架构（ISA），有些由操作系统管理，而有些则不由操作系统管理。通常，异构计算意味着支持使用计算机的CPU核心和一个或多个加速单元（如*图形处理单元*（GPU）或*现场可编程门阵列*（FPGA））进行并行计算。[1](ch15.xhtml#fn15_1)'
- en: It is increasingly common for developers to implement heterogeneous computing
    solutions to large, data-intensive and computation-intensive problems. These types
    of problems are pervasive in scientific computing as well as in a more diverse
    range of applications to Big Data processing, analysis, and information extraction.
    By making use of the processing capabilities of both the CPU and the accelerator
    units that are available on a computer, a programmer can increase the degree of
    parallel execution in their application, resulting in improved performance and
    scalability.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 开发者越来越多地实现异构计算解决方案，以应对大型数据密集型和计算密集型问题。这类问题在科学计算中非常普遍，同时也广泛应用于大数据处理、分析和信息提取等领域。通过利用计算机上CPU和加速单元的处理能力，程序员可以提高应用程序的并行执行程度，从而提高性能和可扩展性。
- en: In this section, we introduce heterogeneous computing using hardware accelerators
    to support general-purpose parallel computing. We focus on GPUs and the CUDA programming
    language.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了使用硬件加速器的异构计算，以支持通用并行计算。我们将重点讨论GPU和CUDA编程语言。
- en: 15.1.1 Hardware Accelerators
  id: totrans-18
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 15.1.1 硬件加速器
- en: In addition to the CPU, computers have other processing units that are designed
    to perform specific tasks. These units are not general-purpose processing units
    like the CPU, but are special-purpose hardware that is optimized to implement
    functionality that is specific to certain devices or that is used to perform specialized
    types of processing in the system. FPGAs, Cell processors, and GPUs are three
    examples of these types of processing units.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 除了CPU，计算机还配备了其他处理单元，旨在执行特定任务。这些单元并不是像CPU那样的通用处理单元，而是为实现特定设备功能或执行系统中的特定类型处理而优化的专用硬件。FPGA、Cell处理器和GPU就是这类处理单元的三个例子。
- en: FPGAs
  id: totrans-20
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: FPGA
- en: An FPGA is an integrated circuit that consists of gates, memory, and interconnection
    components. They are reprogrammable, meaning that they can be reconfigured to
    implement specific functionality in hardware, and they are often used to prototype
    application-specific integrated circuits (ASICs). FPGAs typically require less
    power to run than a full CPU, resulting in energy-efficient operation. Some example
    ways in which FPGAs are integrated into a computer system include as device controllers,
    for sensor data processing, for cryptography, and for testing new hardware designs
    (because they are reprogrammable, designs can be implemented, debugged, and tested
    on an FPGA). FPGAs can be designed as a circuit with a high number of simple processing
    units. FPGAs are also low-latency devices that can be directly connected to system
    buses. As a result, they have been used to implement very fast parallel computation
    that consists of regular patterns of independent parallel processing on several
    data input channels. However, reprogramming FPGAs takes a long time, and their
    use is limited to supporting fast execution of specific parts of parallel workloads
    or for running a fixed program workload.^([2](ch15.xhtml#fn15_2))
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: FPGA 是一种集成电路，由门、电池和互联组件组成。它们是可重新编程的，意味着可以重新配置以实现硬件中的特定功能，通常用于原型设计应用特定集成电路（ASIC）。与完整的
    CPU 相比，FPGA 通常消耗更少的功耗，从而实现更高效的能源使用。FPGA 集成到计算机系统中的一些常见方式包括作为设备控制器、传感器数据处理、加密和测试新硬件设计（由于其可重新编程的特性，可以在
    FPGA 上实现、调试和测试设计）。FPGA 可以设计为包含大量简单处理单元的电路。FPGA 也是低延迟设备，可以直接连接到系统总线。因此，它们被用于实现非常快速的并行计算，执行由若干数据输入通道上的独立并行处理所构成的规则模式。然而，重新编程
    FPGA 需要较长时间，它们的使用被局限于支持并行工作负载的特定部分的快速执行或运行固定程序工作负载。^([2](ch15.xhtml#fn15_2))
- en: GPUs and Cell Processors
  id: totrans-22
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: GPU 与 Cell 处理器
- en: A Cell processor is a multicore processor that consists of one general-purpose
    processor and multiple coprocessors that are specialized to accelerate a specific
    type of computation, such as multimedia processing. The Sony PlayStation 3 gaming
    system was the first Cell architecture, using the Cell coprocessors for fast graphics.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Cell 处理器是一种多核处理器，包含一个通用处理器和多个专门加速特定类型计算（如多媒体处理）的协处理器。索尼 PlayStation 3 游戏系统是第一个采用
    Cell 架构的设备，使用 Cell 协处理器来加速图形处理。
- en: GPUs perform computer graphics computations—they operate on image data to enable
    high-speed graphics rendering and image processing. A GPU writes its results to
    a frame buffer, which delivers the data to the computer’s display. Driven by computer
    gaming applications, today sophisticated GPUs come standard in desktop and laptop
    systems.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 执行计算机图形计算——它们操作图像数据以实现高速图形渲染和图像处理。GPU 将结果写入帧缓冲区，将数据传输到计算机显示器。受计算机游戏应用的推动，如今先进的
    GPU 已成为台式机和笔记本系统的标准配置。
- en: In the mid 2000s, parallel computing researchers recognized the potential of
    using accelerators in combination with a computer’s CPU cores to support general-purpose
    parallel computing.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在 2000 年代中期，平行计算研究人员认识到，将加速器与计算机的 CPU 核心结合使用以支持通用并行计算的潜力。
- en: 15.1.2 GPU Architecture Overview
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 15.1.2 GPU 架构概述
- en: GPU hardware is designed for computer graphics and image processing. Historically,
    GPU development has been driven by the video game industry. To support more detailed
    graphics and faster frame rendering, a GPU device consists of thousands of special-purpose
    processors, specifically designed to efficiently manipulate image data, such as
    the individual pixel values of a two-dimensional image, in parallel.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 硬件是为计算机图形和图像处理设计的。GPU 的发展历程主要受到视频游戏行业的推动。为了支持更细致的图形和更快的帧渲染，GPU 设备由成千上万的专用处理器组成，专门设计用来高效地并行处理图像数据，比如二维图像中的单个像素值。
- en: The hardware execution model implemented by GPUs is *single instruction*/*multiple
    thread* (SIMT), a variation of SIMD. SIMT is like multithreaded SIMD, where a
    single instruction is executed in lockstep by multiple threads running on the
    processing units. In SIMT, the total number of threads can be larger than the
    total number of processing units, requiring the scheduling of multiple groups
    of threads on the processors to execute the same sequence of instructions.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: GPU实现的硬件执行模型是*单指令*/*多线程*（SIMT），是SIMD的变种。SIMT类似于多线程的SIMD，其中单个指令由在处理单元上运行的多个线程同步执行。在SIMT中，线程的总数可以大于处理单元的总数，因此需要在处理器上调度多个线程组，以执行相同的指令序列。
- en: As an example, NVIDIA GPUs consist of several streaming multiprocessors (SMs),
    each of which has its own execution control units and memory space (registers,
    L1 cache, and shared memory). Each SM consists of several scalar processor (SP)
    cores. The SM includes a warp scheduler that schedules *warps*, or sets of application
    threads, to execute in lockstep on its SP cores. In lockstep execution, each thread
    in a warp executes the same instruction each cycle but on different data. For
    example, if an application is changing a color image to grayscale, then each thread
    in a warp executes the same sequence of instructions at the same time to set a
    pixel’s RGB value to its grayscale equivalent. Each thread in the warp executes
    these instructions on a different pixel data value, resulting in multiple pixels
    of the image being updated in parallel. Because the threads are executed in lockstep,
    the processor design can be simplified so that multiple cores share the same instruction
    control units. Each unit contains cache memory and multiple registers that it
    uses to hold data as it’s manipulated in lockstep by the parallel processing cores.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 以NVIDIA的GPU为例，它们由多个流式多处理器（SM）组成，每个SM都有自己的执行控制单元和内存空间（寄存器、L1缓存和共享内存）。每个SM由多个标量处理器（SP）核心组成。SM包括一个warp调度器，调度*warp*，即一组应用程序线程，在其SP核心上同步执行。在同步执行中，warp中的每个线程每个周期执行相同的指令，但操作不同的数据。例如，如果一个应用程序正在将彩色图像转换为灰度图像，那么warp中的每个线程将同时执行相同的指令序列，将像素的RGB值设置为其灰度值。warp中的每个线程在不同的像素数据值上执行这些指令，从而使图像的多个像素并行更新。由于线程是同步执行的，处理器设计可以简化，使得多个核心共享相同的指令控制单元。每个单元包含缓存内存和多个寄存器，用于在并行处理核心的同步操作中存储数据。
- en: '[Figure 15-2](ch15.xhtml#ch15fig2) shows a simplified GPU architecture that
    includes a detailed view of one of its SM units. Each SM consists of multiple
    SP cores, a warp scheduler, an execution control unit, an L1 cache, and shared
    memory space.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[图15-2](ch15.xhtml#ch15fig2)展示了一个简化的GPU架构，包含其某个SM单元的详细视图。每个SM由多个SP核心、一个warp调度器、一个执行控制单元、一个L1缓存和共享内存空间组成。'
- en: '![image](../images/15fig02.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/15fig02.jpg)'
- en: '*Figure 15-2: An example of a simplified GPU architecture with 2,048 cores.
    This shows the GPU divided into 64 SM units, and the details of one SM consisting
    of 32 SP cores. The SM’s warp scheduler schedules thread warps on its SPs. A warp
    of threads executes in lockstep on the SP cores.*'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*图15-2：一个简化的GPU架构示例，包含2,048个核心。该图展示了GPU被划分为64个SM单元，以及其中一个SM的详细信息，包含32个SP核心。SM的warp调度器在其SP核心上调度线程warp。线程warp在SP核心上同步执行。*'
- en: 15.1.3 GPGPU Computing
  id: totrans-33
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 15.1.3 GPGPU计算
- en: '*General Purpose GPU* (GPGPU) computing applies special-purpose GPU processors
    to general-purpose parallel computing tasks. GPGPU computing combines computation
    on the host CPU cores with SIMT computation on the GPU processors. GPGPU computing
    performs best on parallel applications (or parts of applications) that can be
    constructed as a stream processing computation on a grid of multidimensional data.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*通用GPU*（GPGPU）计算将专用GPU处理器应用于通用并行计算任务。GPGPU计算结合了主机CPU核心上的计算和GPU处理器上的SIMT计算。GPGPU计算在能够作为网格多维数据流处理计算构建的并行应用程序（或应用程序的一部分）上表现最佳。'
- en: The host operating system does not manage the GPU’s processors or memory. As
    a result, space for program data needs to be allocated on the GPU and the data
    copied between the host memory and the GPU memory by the programmer. GPGPU programming
    languages and libraries typically provide programming interfaces to GPU memory
    that hide some or all of the difficulty of explicitly managing GPU memory from
    the programmer. For example, in CUDA a programmer can include calls to CUDA library
    functions to explicitly allocate CUDA memory on the GPU and to copy data between
    CUDA memory on the GPU and host memory. A CUDA programmer can also use CUDA unified
    memory, which is CUDA’s abstraction of a single memory space on top of host and
    GPU memory. CUDA unified memory hides the separate GPU and host memory, and the
    memory copies between the two, from the CUDA programmer.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 主机操作系统不管理GPU的处理器或内存。因此，程序数据需要在GPU上分配空间，并由程序员在主机内存和GPU内存之间复制数据。GPGPU编程语言和库通常提供GPU内存的编程接口，隐藏了程序员在显式管理GPU内存时的一些或全部困难。例如，在CUDA中，程序员可以调用CUDA库函数，显式地在GPU上分配CUDA内存，并在GPU上的CUDA内存和主机内存之间复制数据。CUDA程序员还可以使用CUDA统一内存，这是CUDA对主机和GPU内存之上的单一内存空间的抽象。CUDA统一内存隐藏了独立的GPU和主机内存，以及它们之间的内存复制，免去了CUDA程序员的处理。
- en: GPUs also provide limited support for thread synchronization, which means that
    GPGPU parallel computing performs particularly well for parallel applications
    that are either embarrassingly parallel or have large extents of independent parallel
    stream-based computation with very few synchronization points. GPUs are massively
    parallel processors, and any program that performs long sequences of independent
    identical (or mostly identical) computation steps on data may perform well as
    a GPGPU parallel application. GPGPU computing also performs well when there are
    few memory copies between host and device memory. If GPU–CPU data transfer dominates
    execution time, or if an application requires fine-grained synchronization, GPGPU
    computing may not perform well or provide much, if any, gain over a multithreaded
    CPU version of the program.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: GPU还提供了有限的线程同步支持，这意味着GPGPU并行计算对于那些典型的并行应用表现尤为出色，尤其是那些轻松并行或具有大范围独立并行流计算且几乎没有同步点的应用。GPU是大规模并行处理器，任何在数据上执行长时间序列独立相同（或大部分相同）计算步骤的程序，都可能作为GPGPU并行应用表现良好。GPGPU计算在主机与设备内存之间进行较少内存复制时也表现良好。如果GPU-CPU数据传输占据了执行时间，或应用程序需要精细的同步，GPGPU计算可能无法提供良好的性能，甚至无法超越多线程的CPU版本。
- en: 15.1.4 CUDA
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 15.1.4 CUDA
- en: CUDA (Compute Unified Device Architecture)^([3](ch15.xhtml#fn15_3)) is NVIDIA’s
    programming interface for GPGPU computing on its graphics devices. CUDA is designed
    for heterogeneous computing in which some program functions run on the host CPU,
    and others run on the GPU device. Programmers typically write CUDA programs in
    C or C++ with annotations that specify CUDA kernel functions, and they make calls
    to CUDA library functions to manage GPU device memory. A CUDA *kernel function*
    is a function that is executed on the GPU, and a CUDA *thread* is the basic unit
    of execution in a CUDA program. CUDA threads are scheduled in warps that execute
    in lockstep on the GPU’s SMs, executing CUDA kernel code on their part of data
    stored in GPU memory. Kernel functions are annotated with `__global__` to distinguish
    them from host functions. CUDA `__device__` functions are helper functions that
    can be called from a CUDA kernel function.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA（计算统一设备架构）^([3](ch15.xhtml#fn15_3))是NVIDIA为其图形设备上的GPGPU计算提供的编程接口。CUDA旨在支持异构计算，其中一些程序功能在主机CPU上运行，其他则在GPU设备上运行。程序员通常用C或C++编写CUDA程序，并添加注释来指定CUDA内核函数，然后调用CUDA库函数来管理GPU设备内存。CUDA
    *内核函数*是执行在GPU上的函数，CUDA *线程*是CUDA程序中基本的执行单元。CUDA线程在GPU的SM单元中以warp的形式调度执行，并在其数据部分（存储在GPU内存中）上执行CUDA内核代码。内核函数使用`__global__`进行注释，以将其与主机函数区分开。CUDA的`__device__`函数是可以从CUDA内核函数中调用的辅助函数。
- en: 'The memory space of a CUDA program is separated into host and GPU memory. The
    program must explicitly allocate and free GPU memory space to store program data
    manipulated by CUDA kernels. The CUDA programmer must either explicitly copy data
    to and from the host and GPU memory, or use CUDA unified memory that presents
    a view of memory space that is directly shared by the GPU and host. Here is an
    example of CUDA’s basic memory allocation, memory deallocation, and explicit memory
    copy functions:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA程序的内存空间分为主机内存和GPU内存。程序必须显式地分配和释放GPU内存空间，以存储CUDA内核操作的数据。CUDA程序员必须显式地将数据从主机和GPU内存之间复制，或者使用CUDA统一内存，该内存呈现一个直接由GPU和主机共享的内存空间视图。以下是CUDA基本内存分配、内存释放和显式内存复制函数的示例：
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'CUDA threads are organized into *blocks*, and the blocks are organized into
    a *grid*. Grids can be organized into one-, two-, or three-dimensional groupings
    of blocks. Blocks, likewise, can be organized into one-, two-, or three-dimensional
    groupings of threads. Each thread is uniquely identified by its thread (*x*,*y*,*z*)
    position in its containing block’s (*x*,*y*,*z*) position in the grid. For example,
    a programmer could define two-dimensional block and grid dimensions as the following:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA线程被组织成*块*，而块又被组织成*网格*。网格可以组织成一维、二维或三维的块分组。同样，块可以组织成一维、二维或三维的线程分组。每个线程通过其在包含块中的(*x*,*y*,*z*)位置以及其在网格中的(*x*,*y*,*z*)位置被唯一标识。例如，程序员可以将二维块和网格维度定义如下：
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'When a kernel is invoked, its blocks/grid and thread/block layout is specified
    in the call. For example, here is a call to a kernel function named `do_something`
    specifying the grid and block layout using `gridDim` and `blockDim` defined above
    (and passing parameters `dev_array` and 100):'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 当调用一个内核时，它的块/网格和线程/块布局在调用中被指定。例如，这里是调用一个名为`do_something`的内核函数，使用上述定义的`gridDim`和`blockDim`来指定网格和块的布局（并传递参数`dev_array`和100）：
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[Figure 15-3](ch15.xhtml#ch15fig3) shows an example of a two-dimensional arrangement
    of thread blocks. In this example, the grid is a 3 × 2 array of blocks, and each
    block is a 4× 3 array of threads.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 15-3](ch15.xhtml#ch15fig3)展示了一个线程块二维排列的示例。在此示例中，网格是一个3 × 2的块数组，每个块是一个4 ×
    3的线程数组。'
- en: '![image](../images/15fig03.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/15fig03.jpg)'
- en: '*Figure 15-3: The CUDA thread model. A grid of blocks of threads. Blocks and
    threads can be organized into one-, two-, or three-dimensional layouts. This example
    shows a grid of two-dimensional blocks, 3 × 2 blocks per grid, and each block
    has a two-dimensional set of threads, 4 × 3 threads per block).*'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 15-3：CUDA线程模型。一个由线程块组成的网格。块和线程可以组织成一维、二维或三维布局。此示例显示了一个二维块的网格，每个网格有3 × 2个块，每个块有4
    × 3个线程。*'
- en: 'A thread’s position in this layout is given by the (*x*,*y*) coordinate in
    its containing block (`threadId.x`, `threadId.y`) and by the (*x*,*y*) coordinate
    of its block in the grid (`blockIdx.x`, `blockIdx.y`). Note that block and thread
    coordinates are (*x*,*y*) based, with the *x*-axis being horizontal, and the *y*-axis
    vertical. The (0,0) element is in the upper left. The CUDA kernel also has variables
    that are defined to the block dimensions (`blockDim.x` and `blockDim.y`). Thus,
    for any thread executing the kernel, its (row, col) position in the two-dimensional
    array of threads in the two-dimensional array of blocks can be logically identified
    as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 线程在此布局中的位置由其所在块的(*x*,*y*)坐标（`threadId.x`，`threadId.y`）和其在网格中的块的(*x*,*y*)坐标（`blockIdx.x`，`blockIdx.y`）给出。请注意，块和线程的坐标是基于(*x*,*y*)的，其中*x*轴是水平的，*y*轴是垂直的。元素(0,0)位于左上角。CUDA内核还有一些变量，用于定义块的维度（`blockDim.x`和`blockDim.y`）。因此，对于执行内核的任何线程，其在二维线程数组中的（行，列）位置可以逻辑上表示为如下：
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Although not strictly necessary, CUDA programmers often organize blocks and
    threads to match the logical organization of program data. For example, if a program
    is manipulating a two-dimensional matrix, it often makes sense to organize threads
    and blocks into a two-dimensional arrangement. This way, a thread’s block (*x*,*y*)
    and its thread (*x*,*y*) within a block can be used to associate a thread’s position
    in the two-dimensional blocks of threads with one or more data values in the two-dimensional
    array.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管并非严格必要，CUDA程序员通常会将块和线程组织成与程序数据的逻辑结构相匹配。例如，如果程序正在处理二维矩阵，通常会将线程和块组织成二维的排列方式。这样，线程的块(*x*,*y*)及其块内的线程(*x*,*y*)可以用来将线程在二维线程块中的位置与二维数组中的一个或多个数据值关联起来。
- en: 'Example CUDA Program: Scalar Multiply'
  id: totrans-51
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例CUDA程序：标量乘法
- en: 'As an example, consider a CUDA program that performs scalar multiplication
    of a vector:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 作为示例，考虑一个执行向量标量乘法的CUDA程序：
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Because the program data comprises one-dimensional arrays, using a one-dimensional
    layout of blocks/grid and threads/block works well. This is not necessary, but
    it makes the mapping of threads to data easier.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 由于程序数据包含一维数组，使用一维的块/网格布局和线程/块的方式效果良好。这不是必须的，但它使线程与数据的映射更加容易。
- en: 'When run, the main function of this program will do the following:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 程序运行时，主函数将执行以下操作：
- en: 1\. Allocate host-side memory for the vector `x` and initialize it.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 为向量`x`分配主机端内存并初始化。
- en: 2\. Allocate device-side memory for the vector `x` and copy it from host memory
    to GPU memory.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 为向量`x`分配设备端内存，并将其从主机内存复制到GPU内存。
- en: 3\. Invoke a CUDA kernel function to perform vector scalar multiply in parallel,
    passing as arguments the device address of the vector `x` and the scalar value
    `a`.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 调用一个CUDA内核函数，执行向量标量乘法并行计算，传入向量`x`的设备地址和标量值`a`作为参数。
- en: 4\. Copy the result from GPU memory to host memory vector `x`.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 将结果从GPU内存复制到主机内存中的向量`x`。
- en: In the example that follows, we show a CUDA program that performs these steps
    to implement scalar vector multiplication. We have removed some error handling
    and details from the code listing, but the full solution is available online.^([4](ch15.xhtml#fn15_4))
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，我们展示了一个执行这些步骤以实现标量向量乘法的CUDA程序。我们已从代码清单中删除了一些错误处理和细节，但完整的解决方案可以在网上找到。^([4](ch15.xhtml#fn15_4))
- en: 'The main function of the CUDA program performs the aforementioned steps:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA程序的主函数执行上述步骤：
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Each CUDA thread executes the CUDA kernel function `scalar_multiply`. A CUDA
    kernel function is written from an individual thread’s point of view. It typically
    consists of two main steps: (1) the calling thread determines which portion of
    the data it is responsible for based on its thread’s position in its enclosing
    block and its block’s position in the grid; (2) the calling thread performs application-specific
    computation on its portion of the data. In this example, each thread is responsible
    for computing scalar multiplication on exactly one element in the array. The kernel
    function code first calculates a unique index value based on the calling thread’s
    block and thread identifier. It then uses this value as an index into the array
    of data to perform scalar multiplication on its array element (`array[index] =`
    `array[index] * scalar`). CUDA threads running on the GPU’s SM units each compute
    a different index value to update array elements in parallel.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 每个CUDA线程执行CUDA内核函数`scalar_multiply`。CUDA内核函数是从单个线程的角度编写的。它通常包括两个主要步骤：（1）调用线程根据其在线程块中的位置和块在网格中的位置，确定它负责处理的数据部分；（2）调用线程对其负责的数据部分执行应用程序特定的计算。在这个示例中，每个线程负责计算数组中的一个元素的标量乘法。内核函数的代码首先根据调用线程的块和线程标识符计算出一个唯一的索引值。然后，它使用该值作为数组数据的索引，执行标量乘法操作（`array[index]
    =` `array[index] * scalar`）。运行在GPU的SM单元上的CUDA线程每个计算一个不同的索引值，以并行更新数组元素。
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: CUDA Thread Scheduling and Synchronization
  id: totrans-65
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: CUDA线程调度与同步
- en: Each CUDA thread block is run by a GPU SM unit. An SM schedules a warp of threads
    from the same thread block to run its processor cores. All threads in a warp execute
    the same set of instructions in lockstep, typically on different data. Threads
    share the instruction pipeline but get their own registers and stack space for
    local variables and parameters.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 CUDA 线程块由 GPU SM 单元运行。一个 SM 调度来自同一个线程块的线程波在其处理器核心上运行。一个波中的所有线程在锁步执行相同的指令，通常处理不同的数据。线程共享指令管道，但为本地变量和参数提供各自的寄存器和堆栈空间。
- en: Because blocks of threads are scheduled on individual SMs, increasing the threads
    per block increases the degree of parallel execution. Because the SM schedules
    thread warps to run on its processing units, if the number of threads per block
    is a multiple of the warp size, then no SM processor cores are wasted in the computation.
    In practice, using a number of threads per block that is a small multiple of the
    number of processing cores of an SM works well.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 由于线程块在单独的 SM 上调度，增加每个线程块的线程数会提高并行执行的程度。因为 SM 在其处理单元上调度线程波（warp）执行，如果每个线程块的线程数是波大小的倍数，那么在计算中就不会浪费
    SM 处理器核心。实际上，使用每个线程块的线程数为 SM 处理核心数的小倍数通常效果良好。
- en: CUDA guarantees that all threads from a single kernel call complete before any
    threads from a subsequent kernel call are scheduled. Thus, there is an implicit
    synchronization point between separate kernel calls. Within a single kernel call,
    however, thread blocks are scheduled to run the kernel code in any order on the
    GPU SMs. As a result, a programmer should not assume any ordering of execution
    between threads in different thread blocks. CUDA provides some support for synchronizing
    threads, but only for threads that are in the same thread block.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 保证所有来自单个内核调用的线程在任何后续内核调用的线程调度之前完成。因此，在不同的内核调用之间存在一个隐式的同步点。然而，在单个内核调用内，线程块会以任意顺序在
    GPU SM 上调度运行内核代码。因此，程序员不应假设不同线程块之间的线程执行顺序。CUDA 为同步线程提供了一些支持，但仅限于同一个线程块中的线程。
- en: 15.1.5 Other Languages for GPGPU Programming
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 15.1.5 用于 GPGPU 编程的其他语言
- en: There are other programming languages for GPGPU computing. OpenCL, OpenACC,
    and OpenHMPP are three examples of languages that can be used to program any graphics
    device (they are not specific to NVIDIA devices). OpenCL (Open Computing Language)
    has a similar programming model to CUDA’s; both implement a lower-level programming
    model (or implement a thinner programming abstraction) on top of the target architectures.
    OpenCL targets a wide range of heterogeneous computing platforms that include
    a host CPU combined with other compute units, which could include CPUs or accelerators
    such as GPUs and FPGAs. OpenACC (Open Accelerator) is a higher-level abstraction
    programming model than CUDA or OpenCL. It is designed for portability and programmer
    ease. A programmer annotates portions of their code for parallel execution, and
    the compiler generates parallel code that can run on GPUs. OpenHMPP (Open Hybrid
    Multicore Programming) is another language that provides a higher-level programming
    abstraction for heterogeneous programming.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他用于 GPGPU 计算的编程语言。OpenCL、OpenACC 和 OpenHMPP 是三个可以用来编程任何图形设备的语言示例（它们不是专门为
    NVIDIA 设备设计的）。OpenCL（开放计算语言）的编程模型与 CUDA 相似；两者都在目标架构上实现了低级编程模型（或者实现了更薄的编程抽象）。OpenCL
    旨在支持广泛的异构计算平台，包括主机 CPU 结合其他计算单元（可能包括 CPU 或加速器，如 GPU 和 FPGA）。OpenACC（开放加速器）是比 CUDA
    或 OpenCL 更高层次的抽象编程模型，旨在提高可移植性和程序员的易用性。程序员为代码中的并行执行部分添加注解，编译器会生成可以在 GPU 上运行的并行代码。OpenHMPP（开放混合多核编程）是另一种提供更高级编程抽象的语言，适用于异构编程。
- en: 15.2 Distributed Memory Systems, Message Passing, and MPI
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.2 分布式内存系统、消息传递与 MPI
- en: '[Chapter 14](ch14.xhtml#ch14) describes mechanisms like Pthreads (see “Hello
    Threading! Writing Your First Multithreaded Program” on [page 677](ch14.xhtml#lev1_106))
    and OpenMP (see “Implicit Threading with OpenMP” on [page 729](ch14.xhtml#lev1_111))
    that programs use to take advantage of multiple CPU cores on a *shared memory
    system*. In such systems, each core shares the same physical memory hardware,
    allowing them to communicate data and synchronize their behavior by reading from
    and writing to shared memory addresses. Although shared memory systems make communication
    relatively easy, their scalability is limited by the number of CPU cores in the
    system.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[第14章](ch14.xhtml#ch14)描述了像Pthreads（见“你好线程！编写你的第一个多线程程序”[第677页](ch14.xhtml#lev1_106)）和OpenMP（见“使用OpenMP的隐式线程”[第729页](ch14.xhtml#lev1_111)）等机制，程序使用这些机制来利用*共享内存系统*中的多个CPU核心。在这种系统中，每个核心共享相同的物理内存硬件，允许它们通过读取和写入共享内存地址来交换数据和同步行为。尽管共享内存系统使得通信相对容易，但其可扩展性受到系统中CPU核心数量的限制。'
- en: As of 2019, high-end commercial server CPUs generally provide a maximum of 64
    cores. For some tasks, though, even a few hundred CPU cores isn’t close enough.
    For example, imagine trying to simulate the fluid dynamics of the Earth’s oceans
    or index the entire contents of the World Wide Web to build a search application.
    Such massive tasks require more physical memory and processors than any single
    computer can provide. Thus, applications that require a large number of CPU cores
    run on systems that forego shared memory. Instead, they execute on systems built
    from multiple computers, each with their own CPU(s) and memory, that communicate
    over a network to coordinate their behavior.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 截至2019年，高端商业服务器CPU通常提供最多64个核心。然而，对于某些任务，即使有几百个CPU核心也远远不够。例如，想象一下尝试模拟地球海洋的流体动力学，或索引整个万维网的内容以构建搜索应用程序。这些庞大的任务需要比任何单一计算机能提供的更多的物理内存和处理器。因此，要求大量CPU核心的应用程序运行在摒弃共享内存的系统上。相反，它们在由多台计算机构成的系统上运行，每台计算机都有自己的CPU和内存，通过网络通信来协调它们的行为。
- en: A collection of computers working together is known as a *distributed memory
    system* (or often just *distributed system*).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 一组计算机共同工作被称为*分布式内存系统*（或通常称为*分布式系统*）。
- en: '**Warning A NOTE ON CHRONOLOGY**'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**警告 关于时间顺序的说明**'
- en: Despite the order in which they’re presented in this book, systems designers
    built distributed systems long before mechanisms like threads or OpenMP existed.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本书中按顺序介绍了它们，但系统设计师在线程或OpenMP等机制出现之前就已经构建了分布式系统。
- en: Some distributed memory systems integrate hardware more closely than others.
    For example, a *supercomputer* is a high-performance system in which many *compute
    nodes* are tightly coupled (closely integrated) to a fast interconnection network.
    Each compute node contains its own CPU(s), GPU(s), and memory, but multiple nodes
    might share auxiliary resources like secondary storage and power supplies. The
    exact level of hardware sharing varies from one supercomputer to another.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一些分布式内存系统比其他系统更紧密地集成硬件。例如，*超级计算机*是一种高性能系统，其中许多*计算节点*被紧密耦合（紧密集成）到一个快速互连网络中。每个计算节点包含自己的CPU、GPU和内存，但多个节点可能共享辅助资源，如二级存储和电源供应。硬件共享的具体程度在不同的超级计算机之间有所不同。
- en: On the other end of the spectrum, a distributed application might run on a loosely
    coupled (less integrated) collection of fully autonomous computers (*nodes*) connected
    by a traditional local area network (LAN) technology like Ethernet. Such a collection
    of nodes is known as a *commodity off-the-shelf* (COTS) cluster. COTS clusters
    typically employ a *shared-nothing architecture* in which each node contains its
    own set of computation hardware (i.e., CPU(s), GPU(s), memory, and storage). [Figure
    15-4](ch15.xhtml#ch15fig4) illustrates a shared-nothing distributed system consisting
    of two shared-memory computers.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，分布式应用程序可能运行在一组松散耦合（集成度较低）的完全自主计算机（*节点*）上，这些计算机通过像以太网这样的传统局域网（LAN）技术连接。这样的节点集合被称为*商用现成*（COTS）集群。COTS集群通常采用*无共享架构*，其中每个节点包含自己的计算硬件（即CPU、GPU、内存和存储）。[图15-4](ch15.xhtml#ch15fig4)展示了一个由两个共享内存计算机组成的无共享分布式系统。
- en: '![image](../images/15fig04.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/15fig04.jpg)'
- en: '*Figure 15-4: The major components of a shared-nothing distributed memory architecture
    built from two compute nodes*'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*图15-4：由两个计算节点构建的无共享分布式内存架构的主要组件*'
- en: 15.2.1 Parallel and Distributed Processing Models
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 15.2.1 并行与分布式处理模型
- en: Application designers often organize distributed applications using tried-and-true
    designs. Adopting application models like these helps developers reason about
    an application because its behavior will conform to well-understood norms. Each
    model has its unique benefits and drawbacks—there’s no one-size-fits-all solution.
    We briefly characterize a few of the more common models in the subsections that
    follow, but note that we’re not presenting an exhaustive list.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序设计师通常使用经过验证的设计来组织分布式应用程序。采用这样的应用模型有助于开发人员推理应用程序，因为其行为将符合公认的规范。每种模型都有其独特的优缺点——没有一种通用的解决方案。我们将在以下小节中简要介绍一些常见的模型，但请注意，这并不是一个详尽的列表。
- en: Client/Server
  id: totrans-83
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 客户端/服务器
- en: 'The *client/server model* is an extremely common application model that divides
    an application’s responsibilities among two actors: client processes and server
    processes. A server process provides a service to clients that ask for something
    to be done. Server processes typically wait at well-known addresses to receive
    incoming connections from clients. Upon making a connection, a client sends requests
    to the server process, which either satisfies those requests (e.g., by fetching
    a requested file) or reports an error (e.g., the file doesn’t exist or the client
    can’t be properly authenticated).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*客户端/服务器模型*是一个非常常见的应用模型，它将应用程序的职责分配给两个角色：客户端进程和服务器进程。服务器进程向请求某些操作的客户端提供服务。服务器进程通常在已知的地址等待，接收来自客户端的连接请求。一旦建立连接，客户端向服务器进程发送请求，服务器进程要么满足请求（例如，通过提取请求的文件），要么报告错误（例如，文件不存在或客户端无法正确认证）。'
- en: Although you may not have considered it, you access web pages via the client/server
    model! Your web browser (client) connects to a website (server) at a public address
    (e.g., `[diveintosystems.org](http://diveintosystems.org)`) to retrieve the page’s
    contents.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然你可能没有意识到，访问网页实际上就是通过客户端/服务器模型进行的！你的网页浏览器（客户端）连接到网站（服务器），该网站位于一个公共地址（例如，[diveintosystems.org](http://diveintosystems.org)），以检索网页内容。
- en: Pipeline
  id: totrans-86
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 管道
- en: The *pipeline model* divides an application into a distinct sequence of steps,
    each of which can process data independently. This model works well for applications
    whose workflow involves linear, repetitive tasks over large data inputs. For example,
    consider the production of computer-animated films. Each frame of the film must
    be processed through a sequence of steps that transform the frame (e.g., adding
    textures or applying lighting). Because each step happens independently in a sequence,
    animators can speed up rendering by processing frames in parallel across a large
    cluster of computers.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*管道模型*将应用程序划分为一系列独立的步骤，每个步骤都可以独立地处理数据。这个模型适用于工作流涉及大数据输入的线性、重复任务的应用程序。例如，考虑计算机动画电影的制作。电影的每一帧都必须通过一系列步骤进行处理，这些步骤会改变帧的内容（例如，添加纹理或应用光照）。由于每个步骤在序列中是独立进行的，动画师可以通过在大规模计算机集群中并行处理帧来加速渲染过程。'
- en: Boss/Worker
  id: totrans-88
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 主管/工人
- en: In the *boss/worker model*, one process acts as a central coordinator and distributes
    work among the processes at other nodes. This model works well for problems that
    require processing a large, divisible input. The boss divides the input into smaller
    pieces and assigns one or more pieces to each worker. In some applications, the
    boss might statically assign each worker exactly one piece of the input. In other
    cases, the workers might repeatedly finish a piece of the input and then return
    to the boss to dynamically retrieve the next input chunk. Later in this section,
    we’ll present an example program in which a boss divides an array among many workers
    to perform scalar multiplication on an array.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在*主管/工人模型*中，一个进程充当中央协调器，并将工作分配给其他节点上的进程。这个模型适用于需要处理大型可分输入的问题。主管将输入分成较小的块，并将一个或多个块分配给每个工人。在某些应用中，主管可能静态地将每个工人分配一个输入块。在其他情况下，工人可能会反复完成一个输入块，然后返回主管处动态获取下一个输入块。稍后在本节中，我们将展示一个示例程序，其中主管将一个数组分配给多个工人，以执行数组的标量乘法。
- en: Note that this model is sometimes called other names, like “master/ worker”
    or other variants, but the main idea is the same.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个模型有时也被称为其他名称，如“主/工人”或其他变体，但其核心思想是相同的。
- en: Peer-to-Peer
  id: totrans-91
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 对等网络
- en: Unlike the boss/worker model, a *peer-to-peer* application avoids relying on
    a centralized control process. Instead, peer processes self-organize the application
    into a structure in which they each take on roughly the same responsibilities.
    For example, in the BitTorrent file sharing protocol, each peer repeatedly exchanges
    parts of a file with others until they’ve all received the entire file.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 与老板/工人模型不同，*对等应用程序*避免依赖集中控制进程。相反，对等进程自我组织应用程序，将其构造成一个结构，在这个结构中，它们各自承担大致相同的职责。例如，在BitTorrent文件共享协议中，每个对等体不断与其他对等体交换文件的部分，直到它们都接收到完整的文件。
- en: Lacking a centralized component, peer-to-peer applications are generally robust
    to node failures. On the other hand, peer-to-peer applications typically require
    complex coordination algorithms, making them difficult to build and rigorously
    test.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 由于缺乏集中组件，对等应用程序通常对节点故障具有较强的鲁棒性。另一方面，对等应用程序通常需要复杂的协调算法，这使得它们难以构建和严格测试。
- en: 15.2.2 Communication Protocols
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 15.2.2 通信协议
- en: Whether they are part of a supercomputer or a COTS cluster, processes in a distributed
    memory system communicate via *message passing*, whereby one process explicitly
    sends a message to processes on one or more other nodes, which receive it. It’s
    up to the applications running on the system to determine how to utilize the network—some
    applications require frequent communication to tightly coordinate the behavior
    of processes across many nodes, whereas other applications communicate to divide
    up a large input among processes and then mostly work independently.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 无论它们是超计算机的一部分还是COTS集群，分布式内存系统中的进程通过*消息传递*进行通信，其中一个进程显式地将消息发送给一个或多个其他节点上的进程，而这些进程接收它。系统上运行的应用程序决定如何利用网络——一些应用程序需要频繁通信，以紧密协调跨多个节点的进程行为，而其他应用程序则通过通信将大型输入分配给进程，然后大多独立工作。
- en: 'A distributed application formalizes its communication expectations by defining
    a communication *protocol*, which describes a set of rules that govern its use
    of the network, including:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 一个分布式应用程序通过定义通信*协议*来形式化其通信期望，协议描述了一组规则，规范了它如何使用网络，包括：
- en: When a process should send a message
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进程何时应该发送消息
- en: To which process(es) it should send the message
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应该将消息发送给哪个进程
- en: How to format the message
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何格式化消息
- en: Without a protocol, an application might fail to interpret messages properly
    or even deadlock (see “Deadlock” on [page 700](ch14.xhtml#lev3_120)). For example,
    if an application consists of two processes, and each process waits for the other
    to send it a message, neither process will ever make progress. Protocols add structure
    to communication to reduce the likelihood of such failures.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有协议，应用程序可能无法正确解释消息，甚至可能发生死锁（请参见[第700页](ch14.xhtml#lev3_120)的“死锁”）。例如，如果一个应用程序由两个进程组成，每个进程都等待对方发送消息，那么这两个进程都永远不会取得进展。协议为通信添加了结构，从而减少了这种故障的可能性。
- en: To implement a communication protocol, applications require basic functionality
    for tasks like sending and receiving messages, naming processes (addressing),
    and synchronizing process execution. Many applications look to the Message Passing
    Interface for such functionality.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现通信协议，应用程序需要执行一些基本功能，如发送和接收消息、命名进程（寻址）和同步进程执行。许多应用程序依赖消息传递接口来实现这些功能。
- en: 15.2.3 Message Passing Interface
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 15.2.3 消息传递接口
- en: The *Message Passing Interface* (MPI) defines (but does not itself implement)
    a standardized interface that applications can use to communicate in a distributed
    memory system. By adopting the MPI communication standard, applications become
    *portable*, meaning that they can be compiled and executed on many different systems.
    In other words, as long as an MPI implementation is installed, a portable application
    can move from one system to another and expect to execute properly, even if the
    systems have different underlying characteristics.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '*消息传递接口*（MPI）定义了一个标准化的接口（但并不自己实现），应用程序可以通过它在分布式内存系统中进行通信。通过采用MPI通信标准，应用程序变得*可移植*，意味着它们可以在许多不同的系统上编译和执行。换句话说，只要安装了MPI实现，一个可移植的应用程序就可以从一个系统移动到另一个系统，并期望能够正确执行，即使这些系统具有不同的底层特性。'
- en: MPI allows a programmer to divide an application into multiple processes. It
    assigns each of an application’s processes a unique identifier, known as a *rank*,
    which ranges from 0 to *N –* 1 for an application with *N* processes. A process
    can learn its rank by calling the `MPI_Comm_rank` function, and it can learn how
    many processes are executing in the application by calling `MPI_Comm` `_size`.
    To send a message, a process calls `MPI_Send` and specifies the rank of the intended
    recipient. Similarly, a process calls `MPI_Recv` to receive a message, and it
    specifies whether to wait for a message from a specific node or to receive a message
    from any sender (using the constant `MPI_ANY_SOURCE` as the rank).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: MPI 允许程序员将应用程序划分为多个进程。它为每个应用程序的进程分配一个唯一标识符，称为 *rank*，该标识符的范围从 0 到 *N –* 1，适用于包含
    *N* 个进程的应用程序。一个进程可以通过调用 `MPI_Comm_rank` 函数来获取它的 rank，并且可以通过调用 `MPI_Comm_size`
    来得知应用程序中有多少个进程在执行。要发送消息，进程调用 `MPI_Send` 并指定目标接收者的 rank。类似地，进程通过调用 `MPI_Recv` 来接收消息，并指定是等待来自特定节点的消息，还是接收来自任意发送者的消息（使用常量
    `MPI_ANY_SOURCE` 作为 rank）。
- en: In addition to the basic send and receive functions, MPI also defines a variety
    of functions that make it easier for one process to communicate data to multiple
    recipients. For example, `MPI_Bcast` allows one process to send a message to every
    other process in the application with just one function call. It also defines
    a pair of functions, `MPI_Scatter` and `MPI_Gather`, that allow one process to
    divide up an array and distribute the pieces among processes (scatter), operate
    on the data, and then later retrieve all the data to coalesce the results (gather).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 除了基本的发送和接收函数外，MPI 还定义了多种函数，使一个进程可以更方便地将数据传送给多个接收者。例如，`MPI_Bcast` 允许一个进程通过一次函数调用将消息发送给应用程序中的每个进程。它还定义了一对函数，`MPI_Scatter`
    和 `MPI_Gather`，使得一个进程能够将一个数组拆分并将其片段分发到各个进程（scatter），对数据进行操作，然后再通过 `MPI_Gather`
    函数收集所有数据，以合并结果（gather）。
- en: Because MPI *specifies* only a set of functions and how they should behave,
    each system designer can implement MPI’s functionality in a way that matches the
    capabilities of their particular system. For example, a system with an interconnect
    network that supports broadcasting (sending one copy of a message to multiple
    recipients at the same time) might be able to implement MPI’s `MPI_Bcast` function
    more efficiently than a system without such support.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 MPI *规定* 只提供了一组函数及其行为方式，所以每个系统设计者可以根据其系统的特性来实现 MPI 的功能。例如，具有支持广播的互连网络的系统（即同时将一条消息发送给多个接收者）可能比没有此类支持的系统更高效地实现
    MPI 的 `MPI_Bcast` 函数。
- en: 15.2.4 MPI Hello World
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 15.2.4 MPI Hello World
- en: 'As an introduction to MPI programming, consider the “Hello World” program^([5](ch15.xhtml#fn15_5))
    presented here:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 MPI 编程的入门，考虑这里提供的“Hello World”程序^([5](ch15.xhtml#fn15_5))：
- en: '[PRE7]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: When starting this program, MPI simultaneously executes multiple copies of it
    as independent processes across one or more computers. Each process makes calls
    to MPI to determine how many total processes are executing (with `MPI_Comm_size`)
    and which process it is among those processes (the process’s rank, with `MPI_Comm_rank`).
    After looking up this information, each process prints a short message containing
    the rank and name of the computer (`hostname`) it’s running on before terminating.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 启动此程序时，MPI 会同时在一台或多台计算机上执行多个独立的进程副本。每个进程都会调用 MPI 来确定正在执行的总进程数（通过 `MPI_Comm_size`）以及它在这些进程中的
    rank（通过 `MPI_Comm_rank`）。获取这些信息后，每个进程都会打印一条简短的消息，包含它的 rank 和运行所在计算机的名称（`hostname`），然后终止。
- en: '**Note RUNNING MPI CODE**'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意 运行 MPI 代码**'
- en: To run these MPI examples, you’ll need an MPI implementation like OpenMPI^([6](ch15.xhtml#fn15_6))
    or MPICH^([7](ch15.xhtml#fn15_7)) installed on your system.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行这些 MPI 示例，您需要在系统上安装支持 MPI 的实现，例如 OpenMPI^([6](ch15.xhtml#fn15_6)) 或 MPICH^([7](ch15.xhtml#fn15_7))。
- en: 'To compile this example, invoke the `mpicc` compiler program, which executes
    an MPI-aware version of GCC to build the program and link it against MPI libraries:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 要编译这个示例，请调用 `mpicc` 编译器程序，该程序执行一个支持 MPI 的 GCC 版本，用于构建程序并将其与 MPI 库链接：
- en: '[PRE8]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'To execute the program, use the `mpirun` utility to start up several parallel
    processes with MPI. The `mpirun` command needs to be told which computers to run
    processes on (`--hostfile`) and how many processes to run at each machine (`-np`).
    Here, we provide it with a file named `hosts.txt` that tells `mpirun` to create
    four processes across two computers, one named `lemon`, and another named `orange`:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行该程序，使用`mpirun`工具启动多个并行进程。`mpirun`命令需要指定在哪些计算机上运行进程（`--hostfile`）以及每台机器上运行多少个进程（`-np`）。在这里，我们提供了一个名为`hosts.txt`的文件，告诉`mpirun`在两台计算机上创建四个进程，一台名为`lemon`，另一台名为`orange`：
- en: '[PRE9]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**Warning MPI EXECUTION ORDER**'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**警告：MPI执行顺序**'
- en: You should *never* make any assumptions about the order in which MPI pro- cesses
    will execute. The processes start up on multiple machines, each of which has its
    own OS and process scheduler. If the correctness of your program requires that
    processes run in a particular order, you must ensure that the proper order occurs—for
    example, by forcing certain processes to pause until they receive a message.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 你*永远不要*假设MPI进程的执行顺序。进程在多个机器上启动，每台机器都有自己的操作系统和进程调度器。如果程序的正确性要求进程按照特定顺序执行，你必须确保发生正确的顺序——例如，通过强制某些进程暂停，直到接收到消息。
- en: 15.2.5 MPI Scalar Multiplication
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 15.2.5 MPI标量乘法
- en: For a more substantive MPI example, consider performing scalar multiplication
    on an array. This example adopts the boss/worker model—one process divides the
    array into smaller pieces and distributes them among worker processes. Note that
    in this implementation of scalar multiplication, the boss process also behaves
    as a worker and multiplies part of the array after distributing sections to the
    other workers.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个更具实质性的 MPI 示例，考虑对数组进行标量乘法。这个示例采用了主管/工人模型——一个进程将数组分成更小的块并将它们分发给工人进程。请注意，在这个标量乘法的实现中，主管进程也充当了一个工人的角色，在将数组片段分配给其他工人后，它还会乘以数组的一部分。
- en: To benefit from working in parallel, each process multiplies just its local
    piece of the array by the scalar value, and then the workers all send the results
    back to the boss process to form the final result. At several points in the program,
    the code checks to see whether the rank of the process is zero.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 为了利用并行工作的优势，每个进程仅将其本地数组部分的值乘以标量值，然后所有工人将结果发送回主管进程，以形成最终结果。在程序的多个位置，代码会检查进程的rank是否为0。
- en: '[PRE10]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This check ensures that only one process (the one with rank 0) plays the role
    of the boss. By convention, MPI applications often choose rank 0 to perform one-time
    tasks because no matter how many processes there are, one will always be given
    rank 0 (even if just a single process is executing).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这个检查确保只有一个进程（即rank为0的进程）充当主管角色。根据惯例，MPI应用通常选择rank为0的进程来执行一次性任务，因为无论进程数多少，总会有一个进程被分配rank为0（即使只有一个进程在执行）。
- en: MPI Communication
  id: totrans-124
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: MPI通信
- en: The boss process begins by determining the scalar value and initial input array.
    In a real scientific computing application, the boss would likely read such values
    from an input file. To simplify this example, the boss uses a constant scalar
    value (10) and generates a simple 40-element array (containing the sequence 0
    to 39) for illustrative purposes.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 主管进程首先确定标量值和初始输入数组。在实际的科学计算应用中，主管可能会从输入文件中读取这些值。为了简化这个示例，主管使用一个常数标量值（10），并生成一个简单的40元素数组（包含0到39的序列）以供说明。
- en: 'This program requires communication between MPI processes for three important
    tasks:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 该程序需要在MPI进程之间进行通信，以完成三个重要任务：
- en: 1\. The boss sends the scalar value and the size of the array to *all* of the
    workers.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 主管将标量值和数组大小发送给*所有*工人。
- en: 2\. The boss divides the initial array into pieces and sends a piece to each
    worker.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 主管将初始数组分成若干块，并将每一块分配给一个工人。
- en: 3\. Each worker multiplies the values in its piece of the array by the scalar
    and then sends the updated values back to the boss.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 每个工人将自己负责的数组部分的值乘以标量，然后将更新后的值发送回主管。
- en: Broadcasting Important Values
  id: totrans-130
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 广播重要值
- en: 'To send the scalar value to the workers, the example program uses the `MPI_Bcast`
    function, which allows one MPI process to send the same value to all the other
    MPI processes with one function call:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将标量值发送给工人，示例程序使用`MPI_Bcast`函数，该函数允许一个MPI进程通过一次函数调用将相同的值发送给所有其他MPI进程：
- en: '[PRE11]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This call sends one integer (`MPI_INT`) starting from the address of the `scalar`
    variable from the process with rank 0 to every other process (`MPI_COMM` `_WORLD`).
    All the worker processes (those with nonzero rank) receive the broadcast into
    their local copy of the `scalar` variable, so when this call completes, every
    process knows the scalar value to use.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 此调用将从排名为0的进程开始，向每个其他进程发送一个整数（`MPI_INT`），其数据来源于`scalar`变量的地址（`MPI_COMM_WORLD`）。所有工作进程（即排名非零的进程）都将接收到广播并将数据存入各自的`scalar`变量的本地副本中，因此，当此调用完成时，每个进程都知道要使用的标量值。
- en: '**Note MPI_BCAST BEHAVIOR**'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意MPI_BCAST行为**'
- en: Every process executes `MPI_Bcast`, but it behaves differently depending on
    the rank of the calling process. If the rank matches that of the fourth argument,
    then the caller assumes the role of the sender. All other processes that call
    `MPI_Bcast` act as receivers.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 每个进程都会执行`MPI_Bcast`，但根据调用进程的排名，行为会有所不同。如果排名与第四个参数匹配，那么调用者将充当发送者的角色。所有其他调用`MPI_Bcast`的进程则作为接收者。
- en: 'Similarly, the boss broadcasts the total size of the array to every other process.
    After learning the total array size, each process sets a `local_size` variable
    by dividing the total array size by the number of MPI processes. The `local_size`
    variable represents how many elements each worker’s piece of the array will contain.
    For example, if the input array contains 40 elements and the application consists
    of eight processes, each process is responsible for a five-element piece of the
    array (40 / 8 = 5). To keep the example simple, it assumes that the number of
    processes evenly divides the size of the array:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，主节点将数组的总大小广播到每个其他进程。在获知数组的总大小后，每个进程通过将总数组大小除以MPI进程的数量来设置`local_size`变量。`local_size`变量表示每个工作进程负责的数组元素数量。例如，如果输入数组包含40个元素，而应用程序由八个进程组成，那么每个进程将负责一个五个元素的数组片段（40
    / 8 = 5）。为了简化示例，假设进程的数量能够均匀地整除数组的大小：
- en: '[PRE12]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Distributing the Array
  id: totrans-138
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数组分配
- en: Now that each process knows the scalar value and how many values it’s responsible
    for multiplying, the boss must divide the array into pieces and distribute them
    among the workers. Note that in this implementation, the boss (rank 0) also participates
    as a worker. For example, with a 40-element array and eight processes (ranks 0–7),
    the boss should keep array elements 0–4 for itself (rank 0), send elements 5–9
    to rank 1, elements 10–14 to rank 2, and so on. [Figure 15-5](ch15.xhtml#ch15fig5)
    shows how the boss assigns pieces of the array to each MPI process.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在每个进程都知道了标量值以及它需要负责的元素数量，主节点必须将数组划分成多个部分并将它们分发给工作进程。注意，在这个实现中，主节点（排名0）也作为一个工作进程参与。例如，对于一个包含40个元素的数组和八个进程（排名0–7），主节点应该保留数组元素0–4（排名0），将元素5–9发送给排名1，将元素10–14发送给排名2，依此类推。[图15-5](ch15.xhtml#ch15fig5)展示了主节点如何将数组片段分配给每个MPI进程。
- en: '![image](../images/15fig05.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/15fig05.jpg)'
- en: '*Figure 15-5: The distribution of a 40-element array among eight MPI processes
    (ranks 0–7)*'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '*图15-5：一个40元素数组在八个MPI进程（排名0–7）之间的分配情况*'
- en: 'One option for distributing pieces of the array to each worker combines `{MPI_Send}`
    calls at the boss with an `{MPI_Recv}` call at each worker:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 一种分发数组片段给每个工作进程的选项是将主节点的`{MPI_Send}`调用与每个工作进程的`{MPI_Recv}`调用结合使用：
- en: '[PRE13]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In this code, the boss executes a loop that executes once for each worker process,
    in which it sends the worker a piece of the array. It starts sending data from
    the address of `array` at an offset of `(i * local_size)` to ensure that each
    worker gets a unique piece of the array. That is, the worker with rank 1 gets
    a piece of the array starting at index 5, rank 2 gets a piece of the array starting
    at index 10, etc., as shown in [Figure 15-5](ch15.xhtml#ch15fig5).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，主节点执行一个循环，该循环为每个工作进程执行一次，在其中它将数组的一部分发送给工作进程。它从`array`的地址开始，偏移量为`(i *
    local_size)`，以确保每个工作进程接收到数组的独特片段。也就是说，排名为1的工作进程将获得从索引5开始的数组片段，排名为2的工作进程将获得从索引10开始的数组片段，依此类推，如[图15-5](ch15.xhtml#ch15fig5)所示。
- en: Each call to `MPI_Send` sends `local_size` (5) integers worth of data (20 bytes)
    to the process with rank i. The `0` argument toward the end represents a message
    tag, which is an advanced feature that this program doesn’t need—setting it to
    `0` treats all messages equally.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 每次调用`MPI_Send`都会向排名为i的进程发送`local_size`（5）个整数的数据（20字节）。最后的`0`参数表示消息标签，这是一个高级功能，程序不需要使用该功能——将其设置为`0`表示所有消息是平等的。
- en: The workers all call `MPI_Recv` to retrieve their piece of the array, which
    they store in memory at the address to which `local_array` refers. They receive
    `local_size` (5) integers worth of data (20 bytes) from the node with rank 0\.
    Note that `MPI_Recv` is a *blocking* call, which means that a process that calls
    it will pause until it receives data. Because the `MPI_Recv` call blocks, no worker
    will proceed until the boss sends its piece of the array.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 所有工作线程都调用`MPI_Recv`来获取它们的数组片段，并将其存储在`local_array`所指向的内存地址中。它们从进程号为0的节点接收`local_size`（5个）整数的数据（20字节）。请注意，`MPI_Recv`是一个*阻塞*调用，这意味着调用它的进程会暂停，直到接收到数据。由于`MPI_Recv`调用是阻塞的，因此没有任何工作线程会继续执行，直到老板发送它的数组片段。
- en: Parallel Execution
  id: totrans-147
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 并行执行
- en: After a worker has received its piece of the array, it can begin multiplying
    each array value by the scalar. Because each worker gets a unique subset of the
    array, they can execute independently, in parallel, without the need to communicate.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个工作线程获得它的数组片段后，它可以开始将每个数组值与标量相乘。由于每个工作线程获得的是数组的唯一子集，因此它们可以独立执行，进行并行计算，无需进行通信。
- en: Aggregating Results
  id: totrans-149
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 聚合结果
- en: 'Finally, after workers complete their multiplication, they send the updated
    array values back to the boss, which aggregates the results. Using `MPI_Send`
    and `MPI_Recv`, this process looks similar to the array distribution code we looked
    at earlier, except the roles of sender and receiver are reversed:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在工作线程完成乘法运算后，它们将更新后的数组值发送回老板，老板负责聚合结果。使用`MPI_Send`和`MPI_Recv`，这个过程与我们之前看到的数组分发代码类似，不同的是发送者和接收者的角色交换了：
- en: '[PRE14]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Recall that `MPI_Recv` *blocks* or pauses execution, so each call in the `for`
    loop causes the boss to wait until it receives a piece of the array from worker
    *i*.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，`MPI_Recv`会*阻塞*或暂停执行，因此`for`循环中的每次调用会导致老板等待，直到它从工作线程*i*那里接收到数组的一个片段。
- en: Scatter/Gather
  id: totrans-153
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 分发/聚集
- en: 'Although the `for` loops in the previous example correctly distribute data
    with `MPI_Send` and `MPI_Recv`, they don’t succinctly capture the *intent* behind
    them. That is, they appear to MPI as a series of send and receive calls without
    the obvious goal of distributing an array across MPI processes. Because parallel
    applications frequently need to distribute and collect data like this example
    array, MPI provides functions for exactly this purpose: `MPI_Scatter` and `MPI_Gather`.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前面示例中的`for`循环通过`MPI_Send`和`MPI_Recv`正确地分发了数据，但它们没有简洁地表达出其背后的*意图*。也就是说，它们在MPI看来只是一个发送和接收调用的系列，而没有明确表达出跨MPI进程分发数组的目标。由于并行应用程序经常需要像这个示例数组一样分发和收集数据，MPI提供了专门为此目的设计的函数：`MPI_Scatter`和`MPI_Gather`。
- en: 'These functions provide two major benefits: they allow the entire code blocks
    in the previous example to each be expressed as a single MPI function call, which
    simplifies the code, and they express the *intent* of the operation to the underlying
    MPI implementation, which may be able to better optimize their performance.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数提供了两个主要好处：它们允许将前面示例中的整个代码块用单个MPI函数调用来表达，从而简化代码；并且它们向底层MPI实现表达了操作的*意图*，MPI可能会更好地优化它们的性能。
- en: 'To replace the first loop in the previous example, each process could call
    `MPI_Scatter`:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 为了替换前面示例中的第一个循环，每个进程可以调用`MPI_Scatter`：
- en: '[PRE15]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This function automatically distributes the contents of memory starting at `array`
    in pieces containing `local_size` integers to the `local_array` destination variable.
    The `0` argument specifies that the process with rank 0 (the boss) is the sender,
    so it reads and distributes the `array` source to other processes (including sending
    one piece to itself). Every other process acts as a receiver and receives data
    into its `local_array` destination.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数会自动将从`array`开始的内存内容分成包含`local_size`个整数的块，分发到`local_array`目标变量中。`0`参数指定了进程号为0的进程（即老板）是发送者，因此它读取并分发`array`源数据给其他进程（包括将一块数据发送给它自己）。其他每个进程作为接收者，接收数据到它们的`local_array`目标变量中。
- en: 'After this single call, the workers can each multiply the array in parallel.
    When they finish, each process calls `MPI_Gather` to aggregate the results back
    in the boss’s `array` variable:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一单一调用之后，工作线程可以并行地进行数组的乘法运算。当它们完成后，每个进程都会调用`MPI_Gather`来将结果聚集回老板的`array`变量中：
- en: '[PRE16]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This call behaves like the opposite of `MPI_Scatter`: this time, the `0` argument
    specifies that the process with rank 0 (the boss) is the receiver, so it updates
    the `array` variable, and workers each send `local_size` integers from their `local_array`
    variables.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这个调用的行为类似于`MPI_Scatter`的反向操作：这次，`0`参数指定进程 0（即老板）为接收者，因此它更新`array`变量，而工作进程则分别从它们的`local_array`变量中发送`local_size`个整数。
- en: Full Code for MPI Scalar Multiply
  id: totrans-162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: MPI 标量乘法的完整代码
- en: Here’s a full MPI scalar multiply code listing that uses `MPI_Scatter` and `MPI_Gather`:^([8](ch15.xhtml#fn15_8))
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是使用`MPI_Scatter`和`MPI_Gather`的完整MPI标量乘法代码示例：^([8](ch15.xhtml#fn15_8))
- en: '[PRE17]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In the `main` function, the boss sets up the problem and creates an array. If
    this were solving a real problem (e.g., a scientific computing application), the
    boss would likely read its initial data from an input file. After initializing
    the array, the boss needs to send information about the size of the array and
    the scalar to use for multiplication to all the other worker processes, so it
    broadcasts those variables to every process.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在`main`函数中，老板设置了问题并创建了一个数组。如果这是在解决一个实际问题（例如科学计算应用程序），老板可能会从输入文件中读取初始数据。在初始化数组后，老板需要将数组的大小和乘法使用的标量信息发送给所有其他工作进程，因此它将这些变量广播到每个进程。
- en: Now that each process knows the size of the array and how many processes there
    are, they can each divide to determine how many elements of the array they’re
    responsible for multiplying. For simplicity, this code assumes that the array
    is evenly divisible by the number of processes.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，每个进程都知道数组的大小和进程的数量，它们可以各自进行划分，以确定它们负责乘法运算的数组元素数量。为了简单起见，代码假设数组能够被进程数量均匀地划分。
- en: The boss then uses the `MPI_Scatter` function to send an equal portion of the
    array to each worker process (including itself). Now the workers have all the
    information they need, so they each perform multiplication over their portion
    of the array in parallel. Finally, as the workers complete their multiplication,
    the boss collects each worker’s piece of the array using `MPI_Gather` to report
    the final results.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，老板使用`MPI_Scatter`函数将数组的相等部分发送给每个工作进程（包括它自己）。现在，工作进程已经拥有了所需的全部信息，因此它们各自并行地对自己负责的数组部分进行乘法计算。最后，当工作进程完成乘法操作后，老板使用`MPI_Gather`函数收集每个工作进程的数组部分，以报告最终结果。
- en: 'Compiling and executing this program looks like this:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 编译和执行这个程序的过程如下：
- en: '[PRE18]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 15.2.6 Distributed Systems Challenges
  id: totrans-170
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 15.2.6 分布式系统的挑战
- en: In general, coordinating the behavior of multiple processes in distributed systems
    is notoriously difficult. If a hardware component (e.g., CPU or power supply)
    fails in a shared memory system, the entire system becomes inoperable. In a distributed
    system though, autonomous nodes can fail independently. For example, an application
    must decide how to proceed if one node disappears and the others are still running.
    Similarly, the interconnection network could fail, making it appear to each process
    as if all the others failed.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在分布式系统中协调多个进程的行为非常困难。如果硬件组件（例如 CPU 或电源）在共享内存系统中发生故障，整个系统将无法操作。然而，在分布式系统中，自治节点可以独立地发生故障。例如，如果一个节点消失而其他节点仍在运行，应用程序必须决定如何继续操作。类似地，互联网络可能发生故障，导致每个进程都认为其他所有进程都已失败。
- en: Distributed systems also face challenges due to a lack of shared hardware, namely
    clocks. Due to unpredictable delays in network transmission, autonomous nodes
    cannot easily determine the order in which messages are sent. Solving these challenges
    (and many others) is beyond the scope of this book. Fortunately, distributed software
    designers have constructed several frameworks that ease the development of distributed
    applications. We characterize some of these frameworks in the next section.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式系统由于缺乏共享硬件，尤其是时钟，也面临着挑战。由于网络传输中的延迟不可预测，自治节点无法轻易确定消息发送的顺序。解决这些挑战（以及其他许多问题）超出了本书的范围。幸运的是，分布式软件设计师们构建了多个框架，简化了分布式应用程序的开发。我们将在下一节中介绍这些框架中的一些。
- en: MPI Resources
  id: totrans-173
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: MPI 资源
- en: 'MPI is large and complex, and this section hardly scratches the surface. For
    more information about MPI, we suggest:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: MPI非常庞大且复杂，本节内容几乎只是略微涉及。有关MPI的更多信息，我们建议：
- en: The Lawrence Livermore National Lab’s MPI tutorial, by Blaise Barney.^([9](ch15.xhtml#fn15_9))
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 洛伦斯·利弗莫尔国家实验室的MPI教程，作者：Blaise Barney。^([9](ch15.xhtml#fn15_9))
- en: CSinParallel’s MPI Patterns.^([10](ch15.xhtml#fn15_10))
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CSinParallel的MPI模式。^([10](ch15.xhtml#fn15_10))
- en: '15.3 To Exascale and Beyond: Cloud Computing, Big Data, and the Future of Computing'
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.3 向Exascale迈进：云计算、大数据与计算的未来
- en: Advances in technology have made it possible for humanity to produce data at
    a rate never seen before. Scientific instruments such as telescopes, biological
    sequencers, and sensors produce high-fidelity scientific data at low cost. As
    scientists struggle to analyze this “data deluge,” they increasingly rely on sophisticated
    multinode supercomputers, which form the foundation of *high-performance computing*
    (HPC).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 技术的进步使人类能够以前所未有的速度生成数据。科学仪器，如望远镜、生物测序仪和传感器，以低成本产生高保真度的科学数据。随着科学家们努力分析这一“数据洪流”，他们越来越依赖复杂的多节点超级计算机，而这些计算机构成了*高性能计算*（HPC）的基础。
- en: HPC applications are typically written in languages like C, C++, or Fortran,
    with multithreading and message passing enabled with libraries such as POSIX threads,
    OpenMP, and MPI. Thus far, the vast majority of this book has described architectural
    features, languages, and libraries commonly leveraged on HPC systems. Companies,
    national laboratories, and other organizations interested in advancing science
    typically use HPC systems and form the core of the computational science ecosystem.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: HPC应用程序通常使用C、C++或Fortran等语言编写，并通过POSIX线程、OpenMP和MPI等库启用多线程和消息传递。到目前为止，本书的大部分内容都描述了在HPC系统上常用的架构特征、语言和库。那些有兴趣推动科学发展的公司、国家实验室和其他组织通常使用HPC系统，并构成了计算科学生态系统的核心。
- en: Meanwhile, the proliferation of internet-enabled devices and the ubiquity of
    social media have caused humanity to effortlessly produce large volumes of online
    multimedia, in the form of web pages, pictures, videos, tweets, and social media
    posts. It is estimated that 90% of all online data was produced in the past two
    years, and that society produces 30 terabytes of user data per second (or 2.5
    exabytes per day). The deluge of *user data* offers companies and organizations
    a wealth of information about the habits, interests, and behavior of its users,
    and it facilitates the construction of data-rich customer profiles to better tailor
    commercial products and services. To analyze user data, companies typically rely
    on multinode data centers that share many of the hardware architecture components
    of typical supercomputers. However, these data centers rely on a different software
    stack designed specifically for internet-based data. The computer systems used
    for the storage and analysis of large-scale internet-based data are sometimes
    referred to as *high-end data analysis* (HDA) systems. Companies like Amazon,
    Google, Microsoft, and Facebook have a vested interest in the analysis of internet
    data, and form the core of the data analytics ecosystem. The HDA and data analytics
    revolution started around 2010, and now is a dominant area of cloud computing
    research.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，互联网设备的普及和社交媒体的无处不在使人类轻松地生成大量的在线多媒体内容，如网页、图片、视频、推文和社交媒体帖子。据估计，90%的所有在线数据是在过去两年内产生的，社会每秒生成30TB的用户数据（即每天2.5
    EB）。这股*用户数据*的洪流为公司和组织提供了大量关于用户习惯、兴趣和行为的信息，并促进了数据丰富的客户画像的构建，从而更好地定制商业产品和服务。为了分析用户数据，公司通常依赖多节点数据中心，这些数据中心共享许多典型超级计算机的硬件架构组件。然而，这些数据中心依赖于专为互联网数据设计的不同软件栈。用于存储和分析大规模互联网数据的计算机系统有时被称为*高端数据分析*（HDA）系统。像亚马逊、谷歌、微软和Facebook这样的公司在互联网数据分析中有着重要利益，它们构成了数据分析生态系统的核心。HDA和数据分析革命大约始于2010年，现在已成为云计算研究的主流领域。
- en: '[Figure 15-6](ch15.xhtml#ch15fig6) highlights the key differences in software
    utilized by the HDA and HPC communities. Note that both communities use similar
    cluster hardware that follows a distributed memory model, where each compute node
    typically has one or more multicore processors and frequently a GPU. The cluster
    hardware typically includes a *distributed filesystem* that allows users and applications
    common access to files that reside locally on multiple nodes in the cluster.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '[图15-6](ch15.xhtml#ch15fig6)突出显示了HDA和HPC社区在软件使用方面的关键差异。请注意，这两个社区都使用类似的集群硬件，遵循分布式内存模型，其中每个计算节点通常有一个或多个多核处理器，并且通常配备GPU。集群硬件通常包括*分布式文件系统*，允许用户和应用程序共同访问存储在集群多个节点本地的文件。'
- en: '![image](../images/15fig06.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/15fig06.jpg)'
- en: '*Figure 15-6: Comparison of HDA vs. HPC frameworks. Based on a figure by Jack
    Dongarra and Daniel Reed.^([11](ch15.xhtml#fn15_11))*'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '*图15-6：HDA与HPC框架的比较。基于Jack Dongarra和Daniel Reed的图表。^([11](ch15.xhtml#fn15_11))*'
- en: Unlike supercomputers, which are typically built and optimized for HPC use,
    the HDA community relies on *data centers*, which consist of a large collection
    of general-purpose compute nodes typically networked together via Ethernet. At
    a software level, data centers typically employ virtual machines, large distributed
    databases, and frameworks that enable high-throughput analysis of internet data.
    The term *cloud* refers to the data storage and computing power components of
    HDA data centers.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 与通常为HPC使用而建造和优化的超级计算机不同，HDA社区依赖于*数据中心*，这些数据中心由大量的通用计算节点组成，通常通过以太网相互连接。在软件层面，数据中心通常采用虚拟机、大型分布式数据库以及支持对互联网数据进行高通量分析的框架。*云*一词指的是HDA数据中心中的数据存储和计算能力组件。
- en: In this section, we take a brief look at cloud computing, some of the software
    commonly used to enable cloud computing (specifically MapReduce), and some challenges
    for the future. Please note that this section is not meant to be an in-depth look
    at these concepts; we encourage interested readers to explore the referenced sources
    for greater detail.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将简要介绍云计算、一些常用的云计算软件（特别是MapReduce）以及未来面临的一些挑战。请注意，本节并不打算深入探讨这些概念；我们鼓励感兴趣的读者深入阅读参考资料，以获取更多细节。
- en: 15.3.1 Cloud Computing
  id: totrans-186
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 15.3.1 云计算
- en: '*Cloud computing* is the use or lease of the cloud for a variety of services.
    Cloud computing enables computing infrastructure to act as a “utility”: a few
    central providers give users and organizations access to (a seemingly infinite
    amount of) compute power through the internet, with users and organizations choosing
    to use as much as they want and paying according to their level of use. Cloud
    computing has three main pillars: software as a service (SaaS), infrastructure
    as a service (IaaS), and platform as a service (PaaS).^([12](ch15.xhtml#fn15_12))'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '*云计算*是使用或租赁云端进行各种服务的方式。云计算使计算基础设施可以作为一种“公用设施”运行：少数几个中央提供商通过互联网为用户和组织提供（看似无限的）计算能力，用户和组织可以选择使用所需的计算资源，并根据使用量付费。云计算有三个主要支柱：软件即服务（SaaS）、基础设施即服务（IaaS）和平台即服务（PaaS）。^([12](ch15.xhtml#fn15_12))'
- en: Software as a Service
  id: totrans-188
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 软件即服务
- en: '*Software as a service* (SaaS) refers to software provided directly to users
    through the cloud. Most people utilize this pillar of cloud computing without
    even realizing it. Applications that many people use daily (e.g., web mail, social
    media, and video streaming) depend upon cloud infrastructure. Consider the classic
    application of web mail. Users are able to log on and access their web mail from
    any device, send and receive mail, and seemingly never run out of storage space.
    Interested organizations can in turn “rent” cloud email services to provide email
    to their own clients and employees, without incurring the hardware and maintenance
    cost of running the service themselves. Services in the SaaS pillar are managed
    completely by cloud providers; organizations and users do not (beyond configuring
    a few settings, perhaps) manage any part of the application, data, software, or
    hardware infrastructure, all which would be necessary if they were trying to set
    up the service on their own hardware. Prior to the advent of cloud computing,
    organizations interested in providing web mail for their users would need their
    own infrastructure and dedicated IT support staff to maintain it. Popular examples
    of SaaS providers include Google’s G Suite and Microsoft’s Office 365.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '*软件即服务*（SaaS）是指通过云端直接提供给用户的软件。大多数人甚至在未意识到的情况下就使用了云计算的这一支柱。许多人每天使用的应用程序（例如，网页邮件、社交媒体和视频流）依赖于云基础设施。以经典的网页邮件应用为例，用户能够在任何设备上登录并访问网页邮件，收发邮件，并且似乎永远不会耗尽存储空间。感兴趣的组织也可以“租用”云端邮件服务，为自己的客户和员工提供邮件服务，而无需承担自行运行服务所需的硬件和维护成本。SaaS支柱中的服务完全由云提供商管理；组织和用户不需要管理任何应用程序、数据、软件或硬件基础设施（除非配置一些设置）。如果他们试图在自己的硬件上设置该服务，就必须管理所有这些。云计算出现之前，想要为用户提供网页邮件服务的组织，需要自己拥有基础设施和专门的IT支持人员来维护这些服务。SaaS提供商的流行例子包括谷歌的G
    Suite和微软的Office 365。'
- en: Infrastructure as a Service
  id: totrans-190
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基础设施即服务
- en: '*Infrastructure as a service* (IaaS) allows people and organizations to “rent
    out” computational resources to meet their needs, usually in the form of accessing
    virtual machines that are either general purpose or preconfigured for a particular
    application. One classic example is Amazon’s Elastic Compute Cloud (EC2) service
    from Amazon Web Services (AWS). EC2 enables users to create fully customizable
    virtual machines. The term *elastic* in EC2 refers to a user’s ability to grow
    or shrink their compute resource requests as needed, paying as they go. For example,
    an organization may use an IaaS provider to host its website or deploy its own
    series of custom-built applications to users. Some research labs and classrooms
    use IaaS services in lieu of lab machines, running experiments in the cloud or
    offering a virtual platform for their students to learn. In all cases, the goal
    is to eliminate the maintenance and capital needed to maintain a personal cluster
    or server for similar purposes. Unlike use cases in the SaaS pillar, use cases
    in the IaaS pillar require clients to configure applications, data, and in some
    cases the virtual machine’s OS itself. However, the host OS and hardware infrastructure
    is set up and managed by the cloud provider. Popular IaaS providers include Amazon
    AWS, Google Cloud Services, and Microsoft Azure.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '*基础设施即服务*（IaaS）允许个人和组织“租用”计算资源以满足其需求，通常以访问虚拟机的形式，这些虚拟机可以是通用的，也可以是为特定应用预先配置的。一个经典的例子是亚马逊的弹性计算云（EC2）服务，来自亚马逊网络服务（AWS）。EC2使用户能够创建完全可定制的虚拟机。EC2中的*弹性*一词指的是用户根据需要增长或缩减计算资源请求的能力，按需付费。例如，一个组织可能会使用IaaS提供商来托管其网站，或将自己定制的应用程序系列部署给用户。一些研究实验室和教室使用IaaS服务替代实验室机器，在云端运行实验或为学生提供虚拟平台进行学习。在所有这些情况下，目标是消除维护和资本支出，这些支出通常用于维持个人集群或服务器以实现类似的目的。与SaaS领域的使用案例不同，IaaS领域的使用案例要求客户端配置应用程序、数据，并且在某些情况下需要配置虚拟机的操作系统。然而，主机操作系统和硬件基础设施由云提供商设置和管理。流行的IaaS提供商包括亚马逊AWS、谷歌云服务和微软Azure。'
- en: Platform as a Service
  id: totrans-192
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 平台即服务
- en: '*Platform as a service* (PaaS) allows individuals and organizations to develop
    and deploy their own web applications for the cloud, eliminating the need for
    local configuration or maintenance. Most PaaS providers enable developers to write
    their applications in a variety of languages and offer a choice of APIs to use.
    For example, Microsoft Azure’s service allows users to code web applications in
    the Visual Studio IDE and deploy their applications to Azure for testing. Google
    App Engine enables developers to build and test custom mobile applications in
    the cloud in a variety of languages. Heroku and CloudBees are other prominent
    examples. Note that developers have control over their applications and data only;
    the cloud provider controls the rest of the software infrastructure and all of
    the underlying hardware infrastructure.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '*平台即服务*（PaaS）允许个人和组织开发并部署他们自己的云端网页应用程序，消除了本地配置或维护的需求。大多数PaaS提供商使开发者能够使用多种语言编写应用程序，并提供多种API供使用。例如，微软Azure的服务允许用户在Visual
    Studio IDE中编写网页应用程序，并将应用程序部署到Azure进行测试。谷歌App Engine使开发者能够在云端使用多种语言构建和测试自定义的移动应用程序。Heroku和CloudBees是另外两个知名的例子。请注意，开发者仅对他们的应用程序和数据拥有控制权；云提供商控制着其余的软件基础设施以及所有底层硬件基础设施。'
- en: 15.3.2 MapReduce
  id: totrans-194
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 15.3.2 MapReduce
- en: Perhaps the most famous programming paradigm used on cloud systems is MapReduce.^([13](ch15.xhtml#fn15_13))
    Although MapReduce’s origins lay in functional programming’s Map and Reduce operations,
    Google was the first to apply the concept to analyzing large quantities of web
    data. MapReduce enabled Google to perform web queries faster than its competitors,
    and enabled Google’s meteoric rise as the go-to web service provider and internet
    giant it is today.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 也许最著名的编程范式是MapReduce。^([13](ch15.xhtml#fn15_13)) 虽然MapReduce的起源在于函数式编程中的Map和Reduce操作，但谷歌是第一个将该概念应用于分析大量网页数据的公司。MapReduce使谷歌能够比其竞争对手更快地执行网页查询，并使谷歌成为今天的网络服务巨头和互联网巨头。
- en: Understanding Map and Reduce Operations
  id: totrans-196
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 理解Map和Reduce操作
- en: The `map` and `reduce` functions in the MapReduce paradigm are based on the
    mathematical operations of Map and Reduce from functional programming. In this
    section, we briefly discuss how these mathematical operations work by revisiting
    some examples presented earlier in the book.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 模型中的 `map` 和 `reduce` 函数是基于函数式编程中的 Map 和 Reduce 数学运算的。在本节中，我们通过回顾书中早些时候介绍的一些示例，简要讨论这些数学运算是如何工作的。
- en: 'The Map operation typically applies the same function to all the elements in
    a collection. Readers familiar with Python may recognize this functionality most
    readily in the list comprehension feature in Python. For example, the following
    two code snippets perform scalar multiplication in Python:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: Map 操作通常将相同的函数应用于集合中的所有元素。熟悉 Python 的读者可能会通过 Python 的列表推导功能最直观地识别这一功能。例如，以下两个代码片段在
    Python 中执行标量乘法：
- en: Regular scalar multiply
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 常规标量乘法
- en: '[PRE19]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Scalar multiply with list comprehension
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 使用列表推导进行标量乘法
- en: '[PRE20]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The list comprehension applies the same function (in this case, multiplying
    an array element with scalar value `s`) to every element `x` in `array`.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 列表推导将相同的函数（在这种情况下，将数组元素与标量值 `s` 相乘）应用于 `array` 中的每个元素 `x`。
- en: A single Reduce operation takes a collection of elements and combines them together
    into a single value using some common function. For example, the Python function
    `sum` acts similarly to a Reduce operation, as it takes a collection (typically
    a Python list) and combines all the elements together using addition. So, for
    example, applying addition to all the elements in the `result` array returned
    from the `scalarMultiply` function yields a combined sum of 50.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 单个 Reduce 操作会将一组元素合并为一个单一的值，使用某种公共函数。例如，Python 函数`sum`类似于 Reduce 操作，因为它接受一个集合（通常是
    Python 列表），并通过加法将所有元素合并在一起。因此，例如，将加法应用于 `scalarMultiply` 函数返回的 `result` 数组中的所有元素，会得到一个合并的和
    50。
- en: The MapReduce Programming Model
  id: totrans-205
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: MapReduce 编程模型
- en: A key feature of MapReduce is its simplified programming model. Developers need
    to implement only two types of functions, `map` and `reduce`; the underlying MapReduce
    framework automates the rest of the work.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 的一个关键特点是其简化的编程模型。开发者只需要实现两种类型的函数：`map` 和 `reduce`；底层的 MapReduce 框架会自动完成其余的工作。
- en: The programmer-written `map` function takes an input (*key*,*value*) pair and
    outputs a series of intermediate (*key*,*value*) pairs that are written to a distributed
    filesystem shared among all the nodes. A combiner that is typically defined by
    the MapReduce framework then aggregates (*key*,*value*) pairs by key, to produce
    (*key*,list(*value*)) pairs that are passed to the programmer-defined `reduce`
    function. The `reduce` function then takes as input a (*key*,list(*value*)) pair
    and combines all the values together through some programmer-defined operation
    to form a final (*key*,*value*), where the *value* in this output corresponds
    to the result of the reduction operation. The output from the `reduce` function
    is written to the distributed filesystem and usually output to the user.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 程序员编写的 `map` 函数接受一个输入（*key*,*value*）对，并输出一系列中间的（*key*,*value*）对，这些对被写入共享的分布式文件系统，供所有节点使用。通常由
    MapReduce 框架定义的合并器随后根据键合并（*key*,*value*）对，以生成（*key*,list(*value*)）对，这些对被传递给程序员定义的
    `reduce` 函数。`reduce` 函数接收（*key*,list(*value*)）对作为输入，并通过某个程序员定义的操作将所有值合并在一起，形成最终的（*key*,*value*），其中输出中的
    *value* 对应于归约操作的结果。`reduce` 函数的输出被写入分布式文件系统，并通常输出给用户。
- en: To illustrate how to use the MapReduce model to parallelize a program, we discuss
    the Word Frequency program. The goal of Word Frequency is to determine the frequency
    of each word in a large text corpus.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明如何使用 MapReduce 模型来并行化一个程序，我们讨论了单词频率程序。单词频率的目标是确定大型文本语料库中每个单词的频率。
- en: A C programmer may implement the following `map` function for the Word Frequency
    program:^(13)
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 一位 C 程序员可能会为单词频率程序实现以下 `map` 函数：^(13)
- en: '[PRE21]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This `map` function receives as input a string (`key`) that corresponds to the
    name of the file, and a separate string (`value`) that contains a component of
    file data. The function then parses words from the input `value` and emits each
    word (`words[i]`) separately with the string value `"1"`. The `emit` function
    is provided by the MapReduce framework and writes the intermediate (*key*,*value*)
    pairs to the distributed filesystem.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`map`函数接收一个字符串（`key`）作为输入，该字符串对应文件的名称，以及一个单独的字符串（`value`），该字符串包含文件数据的一个部分。然后，函数从输入的`value`中解析单词，并分别发出每个单词（`words[i]`）与字符串值`"1"`。`emit`函数由MapReduce框架提供，用于将中间的（*key*,*value*）对写入分布式文件系统。
- en: 'To complete the Word Frequency program, a programmer may implement the following
    `reduce` function:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成单词频率程序，程序员可以实现以下`reduce`函数：
- en: '[PRE22]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This `reduce` function receives as input a string (`key`) that corresponds to
    a particular word, and an `Iterator` struct (again, provided by the MapReduce
    framework) that consists of an aggregated array of items associated with the key
    (`items`), and the length of that array (`length`). In the Word Frequency application,
    `items` corresponds to a list of counts. The function then extracts the number
    of words from the `length` field of the `Iterator` struct, and the array of counts
    from the `items` field. It then loops over all the counts, aggregating the values
    into the variable `total`. Since the `emit` function requires `char *` parameters,
    the function converts `total` to a string prior to calling `emit`.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`reduce`函数接收一个字符串（`key`）作为输入，该字符串对应一个特定的单词，以及一个`Iterator`结构体（同样由MapReduce框架提供），该结构体包含一个与该关键字（`key`）相关联的项数组（`items`）以及该数组的长度（`length`）。在单词频率应用中，`items`对应的是一个计数列表。该函数随后从`Iterator`结构体的`length`字段中提取单词数，并从`items`字段中提取计数数组。接着，它循环遍历所有计数，将值聚合到变量`total`中。由于`emit`函数需要`char
    *`类型的参数，函数会在调用`emit`之前将`total`转换为字符串。
- en: After implementing `map` and `reduce`, the programmer’s responsibility ends.
    The MapReduce framework automates the rest of the work, including partitioning
    the input, generating and managing the processes that run the `map` function (map
    tasks), aggregating and sorting intermediate (*key*,*value*) pairs, generating
    and managing the separate processes that run the `reduce` function (reduce tasks),
    and generating a final output file.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现`map`和`reduce`之后，程序员的工作就完成了。MapReduce框架自动化了其余的工作，包括分割输入、生成并管理运行`map`函数的进程（map任务）、聚合和排序中间的（*key*,*value*）对、生成并管理运行`reduce`函数的独立进程（reduce任务），以及生成最终的输出文件。
- en: 'For simplicity, in [Figure 15-7](ch15.xhtml#ch15fig7) we illustrate how MapReduce
    parallelizes the opening lines of the popular Jonathan Coulton song “Code Monkey”:
    *code monkey get up get coffee, code monkey go to job*.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，[图15-7](ch15.xhtml#ch15fig7)中我们展示了MapReduce如何并行化流行歌曲Jonathan Coulton的《Code
    Monkey》开头几行的处理：*code monkey get up get coffee, code monkey go to job*。
- en: '![image](../images/15fig07.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/15fig07.jpg)'
- en: '*Figure 15-7: Parallelization of the opening lines of the song “Code Monkey”
    using the MapReduce framework*'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '*图15-7：使用MapReduce框架并行化《Code Monkey》歌曲开头的几行*'
- en: '[Figure 15-7](ch15.xhtml#ch15fig7) gives an overview of this process. Prior
    to execution, the boss node first partitions the input into *M* parts, where *M*
    corresponds to the number of map tasks. In [Figure 15-7](ch15.xhtml#ch15fig7),
    *M* = 3, and the input file (`coulton.txt`) is split into three parts. During
    the map phase, the boss node distributes the map tasks among one or more worker
    nodes, with each map task executing independently and in parallel. For example,
    the first map task parses the snippet *code monkey get up* into separate words
    and emits the following four (*key*,*value*) pairs: (`code`,`1`), (`monkey`,`1`),
    (`get`,`1`), (`up`,`1`). Each map task then emits its intermediate values to a
    distributed filesystem that takes up a certain amount of storage on each node.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '[图15-7](ch15.xhtml#ch15fig7)概述了这一过程。在执行之前，主节点首先将输入分割成*M*个部分，其中*M*对应于map任务的数量。在[图15-7](ch15.xhtml#ch15fig7)中，*M*
    = 3，输入文件（`coulton.txt`）被分割成三部分。在map阶段，主节点将map任务分配给一个或多个工作节点，每个map任务独立并行地执行。例如，第一个map任务将片段*code
    monkey get up*解析为单独的单词，并发出以下四个（*key*,*value*）对：(`code`,`1`)，(`monkey`,`1`)，(`get`,`1`)，(`up`,`1`)。每个map任务然后将其中间值发出到分布式文件系统，这些数据会占用每个节点的一定存储空间。'
- en: Prior to the start of the reduce phase, the framework aggregates and combines
    the intermediate (*key*,*value*) pairs into (*key*,list(*value*)) pairs. In [Figure
    15-7](ch15.xhtml#ch15fig7), for example, the (*key*,*value*) pair (`get`,`1`)
    is emitted by two separate map tasks. The MapReduce framework aggregates these
    separate (*key*,*value*) pairs into the single (*key*,list(*value*)) pair (`get`,`[1,1]`).
    The aggregated intermediate pairs are written to the distributed filesystem on
    disk.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在 reduce 阶段开始之前，框架将中间的 (*key*,*value*) 对聚合并合并为 (*key*,list(*value*)) 对。例如，在[图
    15-7](ch15.xhtml#ch15fig7)中，(*key*,*value*) 对 (`get`,`1`) 由两个不同的 map 任务生成。MapReduce
    框架将这两个独立的 (*key*,*value*) 对聚合为单一的 (*key*,list(*value*)) 对 (`get`,`[1,1]`)。这些聚合的中间对被写入到分布式文件系统的磁盘上。
- en: Next, the MapReduce framework directs the boss node to generate *R* reduce tasks.
    In [Figure 15-7](ch15.xhtml#ch15fig7), *R* = 8\. The framework then distributes
    the tasks among its worker nodes. Once again, each reduce task executes independently
    and in parallel. In the reduce phase of this example, the (*key*,list(*value*))
    pair (`get`,`[1,1]`) is reduced to the (*key*,*value*) pair (`get`,`2`). Each
    worker node appends the output of its set of reduce tasks to a final file, which
    is available to the user upon completion.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，MapReduce 框架指示主节点生成 *R* 个 reduce 任务。在[图 15-7](ch15.xhtml#ch15fig7)中，*R*
    = 8。框架随后将任务分配给工作节点。每个 reduce 任务再次独立且并行执行。在本示例的 reduce 阶段，(*key*,list(*value*))
    对 (`get`,`[1,1]`) 被简化为 (*key*,*value*) 对 (`get`,`2`)。每个工作节点将其一组 reduce 任务的输出附加到最终文件中，任务完成后，用户可以访问该文件。
- en: Fault Tolerance
  id: totrans-222
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 容错性
- en: Data centers typically contain thousands of nodes. Consequently, the rate of
    failure is high; consider that if an individual node in a data center has a 2%
    chance of hardware failure, there is a greater than 99.99% chance that some node
    in a 1,000-node data center will fail. Software written for data centers must
    therefore be *fault tolerant*, meaning that it must be able to continue operation
    in the face of hardware failures (or else fail gracefully).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中心通常包含成千上万个节点。因此，故障率较高；例如，如果一个数据中心中的单个节点发生硬件故障的概率是 2%，那么在一个 1000 节点的数据中心中，至少有
    99.99% 的概率会有某个节点发生故障。因此，专为数据中心编写的软件必须具备 *容错性*，即它必须能够在硬件故障的情况下继续运行（否则就会优雅地失败）。
- en: MapReduce was designed with fault tolerance in mind. For any MapReduce run,
    there is one boss node and potentially thousands of worker nodes. The chance that
    a worker node will fail is therefore high. To remedy this, the boss node pings
    individual worker nodes periodically. If the boss node does not receive a response
    from a worker node, the boss redistributes the worker’s assigned workload to a
    different node and re-executes the task.^(13) If the boss node fails (a low probability
    given that it is only one node), the MapReduce job aborts and must be rerun on
    a separate node. Note that sometimes a worker node may fail to respond to the
    boss node’s pings because the worker is bogged down by tasks. MapReduce therefore
    uses the same pinging and work redistribution strategy to limit the effect of
    slow (or straggler) worker nodes.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 在设计时就考虑了容错性。对于每次 MapReduce 执行，系统会有一个主节点和可能成千上万的工作节点。因此，工作节点发生故障的几率较高。为了解决这个问题，主节点会定期
    ping 各个工作节点。如果主节点没有收到某个工作节点的响应，它会将该工作节点分配的工作负载重新分配给其他节点并重新执行任务。^(13)如果主节点发生故障（由于它只有一个节点，因此发生故障的几率较低），MapReduce
    作业将会中止，并且必须在另一个节点上重新运行。请注意，有时工作节点可能因为任务负载过重而无法响应主节点的 ping，这时 MapReduce 会使用相同的
    ping 和工作负载重新分配策略，以限制慢节点（或滞后节点）对系统的影响。
- en: Hadoop and Apache Spark
  id: totrans-225
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Hadoop 和 Apache Spark
- en: The development of MapReduce took the computing world by storm. However, Google’s
    implementation of MapReduce is closed source. As a result, engineers at Yahoo!
    developed Hadoop,^([14](ch15.xhtml#fn15_14)) an open source implementation of
    MapReduce, which was later adopted by the Apache Foundation. The Hadoop project
    consists of an ecosystem of tools for Apache Hadoop, including the Hadoop Distributed
    File System or HDFS (an open source alternative to Google File System), and HBase
    (modeled after Google’s BigTable).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 的发展引起了计算界的轰动。然而，Google 对 MapReduce 的实现是闭源的。因此，Yahoo! 的工程师们开发了 Hadoop,^([14](ch15.xhtml#fn15_14))
    这是一个开源的 MapReduce 实现，后来被 Apache 基金会采纳。Hadoop 项目由一组 Apache Hadoop 工具组成，包括 Hadoop
    分布式文件系统（HDFS，一个开源的 Google 文件系统替代品）和 HBase（模仿 Google 的 BigTable）。
- en: Hadoop has a few key limitations. First, it is difficult to chain multiple MapReduce
    jobs together into a larger workflow. Second, the writing of intermediates to
    the HDFS proves to be a bottleneck, especially for small jobs (smaller than one
    gigabyte). Apache Spark^([15](ch15.xhtml#fn15_15)) was designed to address these
    issues, among others. Due to its optimizations and ability to largely process
    intermediate data in memory, Apache Spark is up to 100 times faster than Hadoop
    on some applications.^([16](ch15.xhtml#fn15_16))
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 有几个关键的限制。首先，将多个 MapReduce 任务链接成一个更大的工作流是困难的。其次，向 HDFS 写入中间数据会成为瓶颈，特别是对于小型任务（小于一吉字节）。Apache
    Spark^([15](ch15.xhtml#fn15_15)) 的设计旨在解决这些问题，等等。由于其优化和能够在内存中大幅处理中间数据的能力，Apache
    Spark 在一些应用上比 Hadoop 快多达 100 倍^([16](ch15.xhtml#fn15_16))。
- en: '15.3.3 Looking Toward the Future: Opportunities and Challenges'
  id: totrans-228
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 15.3.3 展望未来：机会与挑战
- en: Despite the innovations in the internet data analytics community, the amount
    of data produced by humanity continues to grow. Most new data is produced in so-called
    *edge environments*, or near sensors and other data-generating instruments that
    are by definition on the other end of the network from commercial cloud providers
    and HPC systems. Traditionally, scientists and practitioners gather data and analyze
    it using a local cluster, or they move it to a supercomputer or data center for
    analysis. This “centralized” view of computing is no longer a viable strategy
    as improvements in sensor technology have exacerbated the data deluge.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管互联网数据分析领域有所创新，人类产生的数据量仍在持续增长。大多数新数据是在所谓的*边缘环境*中产生的，或者说是靠近传感器和其他数据生成仪器，而这些设备的定义是位于与商业云服务提供商和高性能计算（HPC）系统相对端的网络另一端。传统上，科学家和从业者会收集数据并使用本地集群进行分析，或者将其移到超级计算机或数据中心进行分析。随着传感器技术的进步，数据洪流日益加剧，这种“集中式”计算模式不再是一种可行的策略。
- en: One reason for this explosive growth is the proliferation of small internet-enabled
    devices that contain a variety of sensors. These *Internet of Things* (IoT) devices
    have led to the generation of large and diverse datasets in edge environments.
    Transferring large datasets from the edge to the cloud is difficult, as larger
    datasets take more time and energy to move. To mitigate the logistic issues of
    so-called “Big Data,” the research community has begun to create techniques that
    aggressively summarize data at each transfer point between the edge and the cloud.^([17](ch15.xhtml#fn15_17))
    There is intense interest in the computing research community in creating infrastructure
    that is capable of processing, storing, and summarizing data in edge environments
    in a unified platform; this area is known as *edge* (or *fog*) computing. Edge
    computing flips the traditional analysis model of Big Data; instead of analysis
    occurring at the supercomputer or data center (“last mile”), analysis instead
    occurs at the source of data production (“first mile”).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这种爆炸性增长的一个原因是互联网连接的小型设备的激增，这些设备包含各种传感器。这些*物联网*（IoT）设备导致了在边缘环境中生成大量多样的数据集。将大数据集从边缘传输到云端是困难的，因为更大的数据集需要更多的时间和能量进行传输。为了减轻所谓“大数据”的物流问题，研究界已开始创建技术，在每个边缘与云之间的传输点积极地对数据进行汇总^([17](ch15.xhtml#fn15_17))。计算研究界对于创建能够在边缘环境中处理、存储和汇总数据的统一平台基础设施非常感兴趣；这一领域被称为*边缘*（或*雾计算*）计算。边缘计算颠覆了大数据的传统分析模型；分析不再发生在超级计算机或数据中心（“最后一公里”），而是在数据生产源头（“第一公里”）进行。
- en: In addition to data movement logistics, the other cross-cutting concern for
    the analysis of Big Data is power management. Large, centralized resources such
    as supercomputers and data centers require a lot of energy; modern supercomputers
    require several megawatts (million watts) to power and cool. An old adage in the
    supercomputing community is that “a megawatt costs a megabuck”; in other words,
    it costs roughly $1 million annually to maintain the power requirement of one
    megawatt.^([18](ch15.xhtml#fn15_18)) Local data processing in edge environments
    helps mitigate the logistical issue of moving large datasets, but the computing
    infrastructure in such environments must likewise use the minimal energy possible.
    At the same time, increasing the energy efficiency of large supercomputers and
    data centers is paramount.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 除了数据传输物流，分析大数据的另一个横向问题是电力管理。像超级计算机和数据中心这样的大型集中式资源需要大量的能源；现代超级计算机需要几兆瓦（百万瓦特）来供电和冷却。在超级计算机界有一句老话：“一兆瓦等于一百万美元”；换句话说，维持一兆瓦的电力需求每年大约需要花费100万美元。^([18](ch15.xhtml#fn15_18))
    在边缘环境中进行本地数据处理有助于缓解大数据集传输的物流问题，但此类环境中的计算基础设施同样必须尽可能地节省能源。同时，提高大型超级计算机和数据中心的能效至关重要。
- en: There is also interest in figuring out ways to converge the HPC and cloud computing
    ecosystems to create a common set of frameworks, infrastructure and tools for
    large-scale data analysis. In recent years, many scientists have used techniques
    and tools developed by researchers in the cloud computing community to analyze
    traditional HPC datasets, and vice versa. Converging these two software ecosystems
    will allow for the cross-pollination of research and lead to the development of
    a unified system that allows both communities to tackle the coming onslaught of
    data and potentially share resources. The Big Data Exascale Computing (BDEC) working
    group^([19](ch15.xhtml#fn15_19)) argues that instead of seeing HPC and cloud computing
    as two fundamentally different paradigms, it is perhaps more useful to view cloud
    computing as a “digitally empowered” phase of scientific computing, in which data
    sources are increasingly generated over the internet.^(17) In addition, a convergence
    of culture, training, and tools is necessary to fully integrate the HPC and cloud
    computing software and research communities. BDEC also suggests a model in which
    supercomputers and data centers are “nodes” in a very large network of computing
    resources, all working in concert to deal with data flooding from multiple sources.
    Each node aggressively summarizes the data flowing to it, releasing it to a larger
    computational resource node only when necessary.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，还有兴趣研究如何融合高性能计算（HPC）和云计算生态系统，以创建一套通用的框架、基础设施和工具，用于大规模数据分析。近年来，许多科学家已经使用云计算领域研究人员开发的技术和工具来分析传统的HPC数据集，反之亦然。融合这两个软件生态系统将促进跨领域的研究交流，并促使开发一个统一的系统，让这两个社区能够应对即将到来的数据洪流，并可能共享资源。大数据千万亿次计算（BDEC）工作组^([19](ch15.xhtml#fn15_19))认为，与其将HPC和云计算视为两个截然不同的范式，不如将云计算视为科学计算的“数字赋能”阶段，其中数据源越来越多地通过互联网生成。^(17)
    此外，要完全整合HPC和云计算的软件及研究社区，文化、培训和工具的融合是必要的。BDEC还建议了一种模型，其中超级计算机和数据中心是一个庞大的计算资源网络中的“节点”，所有节点协同工作，共同应对来自多个来源的数据洪流。每个节点都积极地总结流向它的数据，只有在必要时，才将数据释放到更大的计算资源节点。
- en: As the cloud computing and HPC ecosystems look for unification and gird themselves
    against an increasing onslaught of data, the future of computer systems brims
    with exciting possibilities. New fields like artificial intelligence and quantum
    computing are leading to the creation of new *domain-specific architectures* (DSAs)
    and *application-specific integrated circuits* (ASICS) that will be able to handle
    custom workflows more energy efficiently than before (see the TPU^([20](ch15.xhtml#fn15_20))
    for one example). In addition, the security of such architectures, long overlooked
    by the community, will become critical as the data they analyze increases in importance.
    New architectures will also lead to new languages needed to program them, and
    perhaps even new operating systems to manage their various interfaces. To learn
    more about what the future of computer architecture may look like, we encourage
    readers to peruse an article by the 2017 ACM Turing Award winners and computer
    architecture giants, John Hennessy and David Patterson.^([21](ch15.xhtml#fn15_21))
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 随着云计算和高性能计算（HPC）生态系统寻求统一，并为应对日益增长的数据挑战做好准备，计算机系统的未来充满了激动人心的可能性。像人工智能和量子计算这样的新兴领域正在催生出新的*特定领域架构*（DSA）和*应用专用集成电路*（ASIC），这些新架构将比以往更加高效地处理定制工作流（例如，参见
    TPU^([20](ch15.xhtml#fn15_20))）。此外，长期以来被社区忽视的这些架构的安全性，在它们分析的数据变得越来越重要的情况下，也将变得至关重要。新架构还将催生出新的编程语言，甚至可能需要新的操作系统来管理其各种接口。为了更好地了解计算机架构的未来，我们鼓励读者阅读
    2017 年 ACM 图灵奖得主和计算机架构巨头 John Hennessy 和 David Patterson 撰写的文章。^([21](ch15.xhtml#fn15_21))
- en: Notes
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注释
- en: '[1.](ch15.xhtml#rfn15_1) Sparsh Mittal, “A Survey Of Techniques for Architecting
    and Managing Asymmetric Multicore Processors,” *ACM Computing Surveys* 48(3),
    February 2016.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '[1.](ch15.xhtml#rfn15_1) Sparsh Mittal, “面向架构和管理非对称多核处理器的技术调查”，*ACM 计算机调查*
    48(3)，2016 年 2 月。'
- en: '[2.](ch15.xhtml#rfn15_2) “FPGAs and the Road to Reprogrammable HPC,” inside
    HPC, July 2019, *[https://insidehpc.com/2019/07/fpgas-and-the-road-to-reprogrammable-hpc/](https://insidehpc.com/2019/07/fpgas-and-the-road-to-reprogrammable-hpc/)*'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '[2.](ch15.xhtml#rfn15_2) “FPGA 和可重编程 HPC 的道路”，发表于《HPC》杂志，2019 年 7 月，*[https://insidehpc.com/2019/07/fpgas-and-the-road-to-reprogrammable-hpc/](https://insidehpc.com/2019/07/fpgas-and-the-road-to-reprogrammable-hpc/)*'
- en: '[3.](ch15.xhtml#rfn15_3) “GPU Programming,” from CSinParallel: *[https://csinparallel.org/csinparallel/modules/gpu_programming.html](https://csinparallel.org/csinparallel/modules/gpu_programming.html)*;
    CSinParallel has other GPU programming modules: *[https://csinparallel.org](https://csinparallel.org)*'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '[3.](ch15.xhtml#rfn15_3) “GPU 编程”，来自 CSinParallel: *[https://csinparallel.org/csinparallel/modules/gpu_programming.html](https://csinparallel.org/csinparallel/modules/gpu_programming.html)*；CSinParallel
    还有其他 GPU 编程模块： *[https://csinparallel.org](https://csinparallel.org)*'
- en: '[4.](ch15.xhtml#rfn15_4) *[https://diveintosystems.org/book/C15-Parallel/_attachments/scalar_multiply_cuda.cu](https://diveintosystems.org/book/C15-Parallel/_attachments/scalar_multiply_cuda.cu)*'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '[4.](ch15.xhtml#rfn15_4) *[https://diveintosystems.org/book/C15-Parallel/_attachments/scalar_multiply_cuda.cu](https://diveintosystems.org/book/C15-Parallel/_attachments/scalar_multiply_cuda.cu)*'
- en: '[5.](ch15.xhtml#rfn15_5) *[https://diveintosystems.org/book/C15-Parallel/_attachments/hello_world_mpi.c](https://diveintosystems.org/book/C15-Parallel/_attachments/hello_world_mpi.c)*'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '[5.](ch15.xhtml#rfn15_5) *[https://diveintosystems.org/book/C15-Parallel/_attachments/hello_world_mpi.c](https://diveintosystems.org/book/C15-Parallel/_attachments/hello_world_mpi.c)*'
- en: '[6.](ch15.xhtml#rfn15_6) *[https://www.open-mpi.org/](https://www.open-mpi.org/)*'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '[6.](ch15.xhtml#rfn15_6) *[https://www.open-mpi.org/](https://www.open-mpi.org/)*'
- en: '[7.](ch15.xhtml#rfn15_7) *[https://www.mpich.org/](https://www.mpich.org/)*'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '[7.](ch15.xhtml#rfn15_7) *[https://www.mpich.org/](https://www.mpich.org/)*'
- en: '[8.](ch15.xhtml#rfn15_8) Available at *[https://diveintosystems.org/book/C15-Parallel/_attachments/scalar_multiply_mpi.c](https://diveintosystems.org/book/C15-Parallel/_attachments/scalar_multiply_mpi.c)*'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '[8.](ch15.xhtml#rfn15_8) 可通过 *[https://diveintosystems.org/book/C15-Parallel/_attachments/scalar_multiply_mpi.c](https://diveintosystems.org/book/C15-Parallel/_attachments/scalar_multiply_mpi.c)*
    获得'
- en: '[9.](ch15.xhtml#rfn15_9) *[https://hpc-tutorials.llnl.gov/mpi/](https://hpc-tutorials.llnl.gov/mpi/)*'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '[9.](ch15.xhtml#rfn15_9) *[https://hpc-tutorials.llnl.gov/mpi/](https://hpc-tutorials.llnl.gov/mpi/)*'
- en: '[10.](ch15.xhtml#rfn15_10) *[http://selkie.macalester.edu/csinparallel/modules/Patternlets/build/html/MessagePassing/MPI_Patternlets.html](http://selkie.macalester.edu/csinparallel/modules/Patternlets/build/html/MessagePassing/MPI_Patternlets.html)*'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '[10.](ch15.xhtml#rfn15_10) *[http://selkie.macalester.edu/csinparallel/modules/Patternlets/build/html/MessagePassing/MPI_Patternlets.html](http://selkie.macalester.edu/csinparallel/modules/Patternlets/build/html/MessagePassing/MPI_Patternlets.html)*'
- en: '[11.](ch15.xhtml#rfn15_11) D. A. Reed and J. Dongarra, “Exascale Computing
    and Big Data,” *Communications of the ACM* 58(7), 56–68, 2015.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '[11.](ch15.xhtml#rfn15_11) D. A. Reed 和 J. Dongarra，“超大规模计算与大数据，”*《ACM通讯》*
    58(7), 56–68, 2015年。'
- en: '[12.](ch15.xhtml#rfn15_12) M. Armbrust et al., “A View of Cloud Computing,”
    *Communications of the ACM* 53(4), 50–58, 2010.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '[12.](ch15.xhtml#rfn15_12) M. Armbrust 等人，“云计算展望，”*《ACM通讯》* 53(4), 50–58, 2010年。'
- en: '[13.](ch15.xhtml#rfn15_13) Jeffrey Dean and Sanjay Ghemawat, “MapReduce: Simplified
    Data Processing on Large Clusters,” *Proceedings of the Sixth Conference on Operating
    Systems Design and Implementation*, Vol. 6, USENIX, 2004.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '[13.](ch15.xhtml#rfn15_13) Jeffrey Dean 和 Sanjay Ghemawat，“MapReduce：大规模集群上的简化数据处理，”*《第六届操作系统设计与实现会议论文集》*，第6卷，USENIX，2004年。'
- en: '[14.](ch15.xhtml#rfn15_14) *[https://hadoop.apache.org/](https://hadoop.apache.org/)*'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '[14.](ch15.xhtml#rfn15_14) *[https://hadoop.apache.org/](https://hadoop.apache.org/)*'
- en: '[15.](ch15.xhtml#rfn15_15) *[https://spark.apache.org/](https://spark.apache.org/)*'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '[15.](ch15.xhtml#rfn15_15) *[https://spark.apache.org/](https://spark.apache.org/)*'
- en: '[16.](ch15.xhtml#rfn15_16) DataBricks, “Apache Spark,” *[https://databricks.com/spark/about](https://databricks.com/spark/about)*'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '[16.](ch15.xhtml#rfn15_16) DataBricks，“Apache Spark，”*[https://databricks.com/spark/about](https://databricks.com/spark/about)*'
- en: '[17.](ch15.xhtml#rfn15_17) M. Asch et al., “Big Data and Extreme-Scale Computing:
    Pathways to Convergence – Toward a shaping strategy for a future software and
    data ecosystem for scientific inquiry,” *The International Journal of High Performance
    Computing Applications* 32(4), 435–479, 2018.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '[17.](ch15.xhtml#rfn15_17) M. Asch 等人，“大数据与极大规模计算：融合路径——面向科学研究未来软件与数据生态系统的构建战略，”*《国际高性能计算应用期刊》*
    32(4), 435–479, 2018年。'
- en: '[18.](ch15.xhtml#rfn15_18) M. Halper, “Supercomputing’s Super Energy Needs,
    and What to Do About Them,” CACM News, *[https://cacm.acm.org/news/192296-supercomputings-super-energy-needs-and-what-to-do-about-them/fulltext](https://cacm.acm.org/news/192296-supercomputings-super-energy-needs-and-what-to-do-about-them/fulltext)*'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '[18.](ch15.xhtml#rfn15_18) M. Halper，“超级计算的超级能源需求及其应对措施，”CACM新闻，*[https://cacm.acm.org/news/192296-supercomputings-super-energy-needs-and-what-to-do-about-them/fulltext](https://cacm.acm.org/news/192296-supercomputings-super-energy-needs-and-what-to-do-about-them/fulltext)*'
- en: '[19.](ch15.xhtml#rfn15_19) *[https://www.exascale.org/bdec/](https://www.exascale.org/bdec/)*'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '[19.](ch15.xhtml#rfn15_19) *[https://www.exascale.org/bdec/](https://www.exascale.org/bdec/)*'
- en: '[20.](ch15.xhtml#rfn15_20) N. P. Jouppi et al., “In-Datacenter Performance
    Analysis of a Tensor Processing Unit,” *Proceedings of the 44th Annual International
    Symposium on Computer Architecture*, ACM, 2017.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '[20.](ch15.xhtml#rfn15_20) N. P. Jouppi 等人，“数据中心内张量处理单元的性能分析，”*《第44届国际计算机架构年会论文集》*，ACM，2017年。'
- en: '[21.](ch15.xhtml#rfn15_21) J. Hennessy and D. Patterson, “A New Golden Age
    for Computer Architecture,” *Communications of the ACM* 62(2), 48–60, 2019.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '[21.](ch15.xhtml#rfn15_21) J. Hennessy 和 D. Patterson，“计算机架构的新黄金时代，”*《ACM通讯》*
    62(2), 48–60, 2019年。'
