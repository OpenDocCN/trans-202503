- en: '**16'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**16'
- en: SELF-ATTENTION**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力**
- en: '![Image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/common.jpg)'
- en: Where does self-attention get its name, and how is it different from previously
    developed attention mechanisms?
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力的名称来源于哪里，它与以前开发的注意力机制有何不同？
- en: '*Self-attention* enables a neural network to refer to other portions of the
    input while focusing on a particular segment, essentially allowing each part the
    ability to “attend” to the whole input. The original attention mechanism developed
    for recurrent neural networks (RNNs) is applied between two different sequences:
    the encoder and the decoder embeddings. Since the attention mechanisms used in
    transformer-based large language models is designed to work on all elements of
    the same set, it is known as *self*-attention.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*自注意力*使神经网络能够在关注某个特定部分时，参考输入的其他部分，本质上允许每个部分能够“关注”整个输入。为循环神经网络（RNN）开发的原始注意力机制应用于两个不同的序列之间：编码器和解码器嵌入。由于在基于变换器的大型语言模型中使用的注意力机制旨在作用于同一集合的所有元素，因此它被称为*自*注意力。'
- en: This chapter first discusses an earlier attention mechanism developed for RNNs,
    the Bahdanau mechanism, in order to illustrate the motivation behind developing
    attention mechanism. We then compare the Bahdanau mechanism to the self-attention
    mechanism prevalent in transformer architectures today.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章首先讨论为RNN开发的早期注意力机制——Bahdanau机制，目的是阐明开发注意力机制的动机。然后，我们将Bahdanau机制与今天在变换器架构中流行的自注意力机制进行比较。
- en: '**Attention in RNNs**'
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**RNN中的注意力**'
- en: One example of an attention mechanism used in RNNs to handle long sequences
    is *Bahdanau attention*. Bahdanau attention was developed to make machine learning
    models, particularly those used in translating languages, better at understanding
    long sentences. Before this type of attention, the whole input (such as a sentence
    in English) was squashed into a single chunk of information, and important details
    could get lost, especially if the sentence was long.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 一个用于处理长序列的RNN注意力机制的例子是*Bahdanau注意力*。Bahdanau注意力是为了使机器学习模型，特别是用于翻译语言的模型，更好地理解长句子而开发的。在这种注意力机制之前，整个输入（例如英语句子）会被压缩成一块信息，重要细节可能会丢失，尤其是当句子很长时。
- en: To understand the difference between regular attention and self-attention, let’s
    begin with the illustration of the Bahdanau attention mechanism in [Figure 16-1](ch16.xhtml#ch16fig1).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解常规注意力与自注意力的区别，我们先通过[图16-1](ch16.xhtml#ch16fig1)来说明Bahdanau注意力机制。
- en: '![Image](../images/16fig01.jpg)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/16fig01.jpg)'
- en: '*Figure 16-1: The Bahdanau mechanism uses a separate RNN to compute attention
    weights.*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*图16-1：Bahdanau机制使用单独的RNN来计算注意力权重。*'
- en: In [Figure 16-1](ch16.xhtml#ch16fig1), the *α* values represent the attention
    weights for the second sequence element and each other element in the sequence
    from 1 to *T*. Furthermore, this original attention mechanism involves two RNNs.
    The RNN at the bottom, computing the attention weights, represents the encoder,
    while the RNN at the top, producing the output sequence, is a decoder.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图16-1](ch16.xhtml#ch16fig1)中，*α*值表示第二个序列元素与序列中从1到*T*的每个其他元素的注意力权重。此外，这种原始的注意力机制涉及两个RNN。底部的RNN计算注意力权重，代表编码器，而顶部的RNN生成输出序列，是解码器。
- en: 'In short, the original attention mechanism developed for RNNs is applied between
    two different sequences: the encoder and decoder embeddings. For each generated
    output sequence element, the decoder RNN at the top is based on a hidden state
    plus a context vector generated by the encoder. This context vector involves *all*
    elements of the input sequence and is a weighted sum of all input elements where
    the attention scores (*α*’s) represent the weighting coefficients. This allows
    the decoder to access all input sequence elements (the context) at each step.
    The key idea is that the attention weights (and context) may differ and change
    dynamically at each step.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，原始的RNN注意力机制应用于两个不同的序列之间：编码器和解码器嵌入。对于每个生成的输出序列元素，顶部的解码器RNN基于隐藏状态和编码器生成的上下文向量。这个上下文向量涉及输入序列的*所有*元素，并且是所有输入元素的加权和，其中注意力得分（*α*）代表加权系数。这使得解码器在每一步都能访问到所有输入序列元素（上下文）。关键思想是，注意力权重（和上下文）可能会在每一步动态变化。
- en: The motivation behind this complicated encoder-decoder design is that we cannot
    translate sentences word by word. This would result in grammatically incorrect
    outputs, as illustrated by the RNN architecture (a) in [Figure 16-2](ch16.xhtml#ch16fig2).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这种复杂的编码器-解码器设计背后的动机是我们不能逐词翻译句子。这会导致语法错误的输出，正如[图 16-2](ch16.xhtml#ch16fig2)中的
    RNN 架构（a）所示。
- en: '![Image](../images/16fig02.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/16fig02.jpg)'
- en: '*Figure 16-2: Two RNN architecture designs for translating text*'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 16-2：两种 RNN 架构设计用于文本翻译*'
- en: '[Figure 16-2](ch16.xhtml#ch16fig2) shows two different sequence-to-sequence
    RNN designs for sentence translation. [Figure 16-2](ch16.xhtml#ch16fig2)(a) represents
    a regular sequence-to-sequence RNN that may be used to translate a sentence from
    German to English word by word. [Figure 16-2](ch16.xhtml#ch16fig2)(b) depicts
    an encoder-decoder RNN that first reads the whole sentence before translating
    it.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 16-2](ch16.xhtml#ch16fig2)展示了两种不同的序列到序列 RNN 设计用于句子翻译。[图 16-2](ch16.xhtml#ch16fig2)(a)表示一个常规的序列到序列
    RNN，可以逐词地将德语句子翻译成英语。[图 16-2](ch16.xhtml#ch16fig2)(b)描绘了一个编码器-解码器 RNN，它首先读取整个句子，然后再进行翻译。'
- en: RNN architecture (a) is best suited for time series tasks in which we want to
    make one prediction at a time, such as predicting a given stock price day by day.
    For tasks like language translation, we typically opt for an encoder-decoder RNN,
    as in architecture (b) in [Figure 16-2](ch16.xhtml#ch16fig2). Here, the RNN encodes
    the input sentence, stores it in an intermediate hidden representation, and generates
    the output sentence. However, this creates a bottleneck where the RNN has to memorize
    the whole input sentence via a single hidden state, which does not work well for
    longer sequences.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 结构（a）最适合用于时间序列任务，我们需要一次做出一个预测，例如逐日预测某个股票价格。对于像语言翻译这样的任务，我们通常选择编码器-解码器 RNN，如[图
    16-2](ch16.xhtml#ch16fig2)中的结构（b）。在这里，RNN 编码输入句子，将其存储在中间的隐藏表示中，并生成输出句子。然而，这会产生一个瓶颈，RNN
    必须通过一个单一的隐藏状态记住整个输入句子，这对于较长的序列并不有效。
- en: The bottleneck depicted in architecture (b) prompted the Bahdanau attention
    mechanism’s original design, allowing the decoder to access all elements in the
    input sentence at each time step. The attention scores also give different weights
    to the different input elements depending on the current word that the decoder
    generates. For example, when generating the word *help* in the output sequence,
    the word *helfen* in the German input sentence may get a large attention weight,
    as it’s highly relevant in this context.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 架构（b）中展示的瓶颈促使了 Bahdanau 注意力机制的最初设计，使解码器可以在每个时间步骤访问输入句子中的所有元素。注意力分数还会根据解码器生成的当前词，给不同的输入元素赋予不同的权重。例如，在生成输出序列中的词*help*时，德语输入句子中的词*helfen*可能会得到较大的注意力权重，因为在这个上下文中它是高度相关的。
- en: '**The Self-Attention Mechanism**'
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**自注意力机制**'
- en: The Bahdanau attention mechanism relies on a somewhat complicated encoder-decoder
    design to model long-term dependencies in sequence-to-sequence language modeling
    tasks. Approximately three years after the Bahdanau mechanism, researchers worked
    on simplifying sequence-to-sequence modeling architectures by asking whether the
    RNN backbone was even needed to achieve good language translation performance.
    This led to the design of the original transformer architecture and self-attention
    mechanism.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Bahdanau 注意力机制依赖于一种稍显复杂的编码器-解码器设计，用于建模序列到序列语言建模任务中的长期依赖关系。大约在 Bahdanau 机制推出三年后，研究人员开始简化序列到序列建模架构，提出了是否还需要
    RNN 主干才能实现良好的语言翻译性能的问题。这导致了最初的 Transformer 架构和自注意力机制的设计。
- en: In self-attention, the attention mechanism is applied between all elements in
    the same sequence (as opposed to involving two sequences), as depicted in the
    simplified attention mechanism in [Figure 16-3](ch16.xhtml#ch16fig3). Similar
    to the attention mechanism for RNNs, the context vector is an attention-weighted
    sum over the input sequence elements.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在自注意力中，注意力机制应用于同一序列中的所有元素之间（而不是涉及两个序列），正如[图 16-3](ch16.xhtml#ch16fig3)中简化的注意力机制所示。类似于
    RNN 的注意力机制，上下文向量是输入序列元素的注意力加权和。
- en: '![Image](../images/16fig03.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/16fig03.jpg)'
- en: '*Figure 16-3: A simple self-attention mechanism without weight matrices*'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 16-3：一个简单的自注意力机制，没有权重矩阵*'
- en: While [Figure 16-3](ch16.xhtml#ch16fig3) doesn’t include weight matrices, the
    self-attention mechanism used in transformers typically involves multiple weight
    matrices to compute the attention weights.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 [图 16-3](ch16.xhtml#ch16fig3) 中没有包含权重矩阵，但变换器中使用的自注意力机制通常涉及多个权重矩阵来计算注意力权重。
- en: This chapter laid the groundwork for understanding the inner workings of transformer
    models and the attention mechanism. The next chapter covers the different types
    of transformer architectures in more detail.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本章为理解变换器模型及其注意力机制的内部工作原理奠定了基础。下一章将更详细地介绍不同类型的变换器架构。
- en: '**Exercises**'
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**练习**'
- en: '**16-1.** Considering that self-attention compares each sequence element with
    itself, what is the time and memory complexity of self-attention?'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**16-1.** 考虑到自注意力机制将每个序列元素与自身进行比较，那么自注意力的时间和内存复杂度是什么？'
- en: '**16-2.** We discussed self-attention in the context of natural language processing.
    Could this mechanism be useful for computer vision applications as well?'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**16-2.** 我们在自然语言处理的背景下讨论了自注意力机制。这个机制是否也能在计算机视觉应用中发挥作用？'
- en: '**References**'
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: 'The paper introducing the original self-attention mechanism, also known as
    *scaled dot-product* attention: Ashish Vaswani et al., “Attention Is All You Need”
    (2017), *[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)*.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍原始自注意力机制的论文，也被称为 *缩放点积* 注意力：Ashish Vaswani 等人，“Attention Is All You Need”
    (2017)， *[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)*。
- en: 'The Bahdanau attention mechanism for RNNs: Dzmitry Bahdanau, Kyunghyun Cho,
    and Yoshua Bengio, “Neural Machine Translation by Jointly Learning to Align and
    Translate” (2014), *[https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473)*.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bahdanau 注意力机制用于 RNN：Dzmitry Bahdanau、Kyunghyun Cho 和 Yoshua Bengio， “Neural
    Machine Translation by Jointly Learning to Align and Translate” (2014)， *[https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473)*。
- en: 'For more about the parameterized self-attention mechanism, check out my blog
    post: “Understanding and Coding the Self-Attention Mechanism of Large Language
    Models from Scratch” at *[https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html)*.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 想了解更多关于参数化自注意力机制的内容，请查看我的博客文章：“从零开始理解和编写大型语言模型的自注意力机制”，* [https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html)*。
