- en: '**7'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**7'
- en: FINDING A GOOD SET OF HYPERPARAMETERS**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找一个好的超参数组合**
- en: '![Image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/common.jpg)'
- en: As discussed in earlier chapters, especially [Section 3.2.1](ch03.xhtml#ch03lev2sec1),
    most analysts’ approach to the problem of determining good values of hyperparameters
    is to use cross-validation. In this chapter, we’ll learn to use a `qeML` function,
    `qeFT()`, that greatly facilitates the process.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 如前几章所讨论的，特别是[第3.2.1节](ch03.xhtml#ch03lev2sec1)，大多数分析师处理确定超参数的好值的方法是使用交叉验证。本章中，我们将学习使用`qeML`函数`qeFT()`，它极大地简化了这一过程。
- en: 7.1 Combinations of Hyperparameters
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 超参数组合
- en: 'Note that typically we are talking about *sets* of hyperparameters. Suppose,
    for instance, that we wish to use PCA in a k-NN setting. Then we have two hyperparameters:
    the number of neighbors *k* and the number of principal components *m*. Thus we
    are interested in finding a good *combination* of a *k* value and an *m* value.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，通常我们讨论的是超参数的*集合*。举例来说，假设我们希望在k-NN设置中使用PCA。那么我们有两个超参数：邻居数量*k*和主成分数量*m*。因此，我们关注的是找到一个好的*k*值和*m*值的*组合*。
- en: In many cases, the combinations are larger than just pairs. With `qeDT()`, for
    instance, there are hyperparameters `alpha`, `minsplit`, `minbucket`, `maxdepth`,
    and `mtry`. We thus wish to find a good set of five hyperparameters.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，超参数的组合不仅仅是成对的。例如，在`qeDT()`中，有超参数`alpha`、`minsplit`、`minbucket`、`maxdepth`和`mtry`。因此，我们希望找到一个由五个超参数组成的良好组合。
- en: Many ML methods have even more hyperparameters. The more hyperparameters an
    ML method has, the more challenging it is to find a good combination of values.
    The `qeML` function `qeFT()` is aimed at facilitating this search.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习方法有更多的超参数。机器学习方法的超参数越多，找到一个良好的组合值就越具挑战性。`qeML`函数`qeFT()`旨在帮助进行这一搜索。
- en: '**NOTE**'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*Before continuing, note that though ML discussions—as well as some software
    documentation—will often refer to finding the **best** hyperparameter combination,
    this is typically an illusion. Due to p-hacking (see [Section 1.13](ch01.xhtml#ch01lev13)),
    the best combination for a given training set may not be the best for predicting
    new data, which is what counts. Nevertheless, by the end of this chapter, you’ll
    have the tools to dependably determine **good** combinations.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*在继续之前，请注意，尽管机器学习讨论——以及一些软件文档——通常会提到寻找**最佳**超参数组合，但这通常是一个幻觉。由于p-hacking（请参见[第1.13节](ch01.xhtml#ch01lev13)），给定训练集的最佳组合可能并不是预测新数据的最佳组合，而后者才是关键。尽管如此，到本章结束时，您将掌握可靠地确定**良好**组合的工具。*'
- en: 7.2 Grid Searching with qeFT()
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 使用qeFT()进行网格搜索
- en: Many ML packages include functions to do a *grid search*, which means evaluating
    all possible hyperparameter combinations. However, the number of combinations
    is typically so large that a full grid search would take a prohibitive amount
    of time to run.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习包包含用于进行*网格搜索*的函数，这意味着评估所有可能的超参数组合。然而，组合的数量通常非常庞大，进行完整的网格搜索需要耗费巨大的时间。
- en: Some grid search software libraries attempt to solve this problem by evaluating
    only combinations that seem promising, via an iterative search moving through
    narrow parts of the grid. At each iteration, the algorithm updates its guess as
    to what to try next. This saves time but can move in the wrong direction and,
    again, is vulnerable to p-hacking problems.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一些网格搜索软件库试图通过仅评估看似有前景的组合来解决这个问题，通过一个迭代搜索在网格的狭窄部分进行移动。在每次迭代中，算法会更新对下一步应该尝试什么的猜测。这节省了时间，但也可能走错方向，并且同样容易受到p-hacking问题的影响。
- en: The `qeML` function `qeFT()` takes a more cautious approach. It generates a
    large number of random hyperparameter combinations, with the number being specified
    by the user, and evaluates them according to the relevant loss criterion (MAPE
    for numeric- *Y* settings or OME for classification settings). It tabulates and
    displays the results and includes a graphical display option. And, most importantly
    and uniquely, it guards against p-hacking, as will be explained shortly.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '`qeML`函数`qeFT()`采取了更加谨慎的方法。它生成大量的随机超参数组合，数量由用户指定，并根据相关的损失准则（数值型*Y*设置使用MAPE，分类设置使用OME）评估这些组合。它会列出并显示结果，并包括图形显示选项。最重要且独特的是，它防止p-hacking，稍后将对此进行解释。'
- en: The `qeFT()` function is a `qe`-series wrapper for a `regtools` function, `fineTuning()`.
    Recall that another term for hyperparameters is *tuning parameters*. The function
    name is a pun on the old radio days, when tuning to the precise frequency of your
    favorite station was known as “fine-tuning.”
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '`qeFT()`函数是一个`qe`系列的包装器，封装了`regtools`函数`fineTuning()`。回想一下，超参数的另一个术语是*调优参数*。这个函数名是对老式收音机时代的双关语，当时调整到你喜欢的电台的精确频率被称为“微调”。'
- en: '***7.2.1 How to Call qeFT()***'
  id: totrans-15
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***7.2.1 如何调用qeFT()***'
- en: 'Here is the basic `qeFT()` call form:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这是基本的`qeFT()`调用格式：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let’s look at the roles of the arguments:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看这些参数的作用：
- en: data   As in all of the `qe*`-series, our input data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: data   如同所有`qe*`系列中的情况，这是我们的输入数据。
- en: yName   As in all of the `qe*`-series, the name of our *Y* column.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: yName   如同所有`qe*`系列中的情况，这是我们*Y*列的名称。
- en: qeftn   ML function name, such as `qeKNN`.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: qeftn   ML函数名称，例如`qeKNN`。
- en: pars   R list specifying which `qeftn` hyperparameter values we wish to consider,
    such as *k* in k-NN.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: pars   R列表，指定我们希望考虑的`qeftn`超参数值，例如k-NN中的*k*。
- en: nCombs   Number of random combinations of the hyperparameters to evaluate. If
    `NULL`, then all possible combinations will be run.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: nCombs   要评估的超参数随机组合的数量。如果为`NULL`，则会运行所有可能的组合。
- en: nTst   Size of the holdout sets.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: nTst   验证集的大小。
- en: nXval   Number of holdout sets to run for each hyperparameter combination.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: nXval   每个超参数组合运行的验证集数量。
- en: showProgress   For the impatient; print results as they become available.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: showProgress   对于急躁的人；随着结果的生成，打印出来。
- en: In short, we run the specified ML function `qeftn` on `nCombs` combinations
    of hyperparameters using ranges shown in `pars`. For each combination, we generate
    `nXval` training/test partitions of the data, with the test portion being of size
    `nTst`. We then tabulate the resulting MAPE or OME values across all combinations
    of hyperparameters.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我们对`nCombs`个超参数组合运行指定的ML函数`qeftn`，使用`pars`中显示的范围。对于每个组合，我们生成`nXval`个训练/测试数据划分，测试部分的大小为`nTst`。然后，我们统计所有超参数组合中结果的MAPE或OME值。
- en: Note the difference between `qeFT()` and the `replicMeans()` function introduced
    in [Section 3.2.2](ch03.xhtml#ch03lev2sec2). The latter deals with the problem
    that the analyst may feel that a single holdout set is not enough to accurately
    assess performance. The `qeFT()` function does this too, via the argument `nXval`,
    but it does much more, automating the search process.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 注意`qeFT()`和[第3.2.2节](ch03.xhtml#ch03lev2sec2)中介绍的`replicMeans()`函数之间的区别。后者处理的是分析人员可能认为单一的验证集不足以准确评估性能的问题。`qeFT()`函数也做了这件事，通过参数`nXval`，但它做得更多，自动化了搜索过程。
- en: '7.3 Example: Programmer and Engineer Data'
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 示例：程序员与工程师数据
- en: Returning to the US census data on programmers’ and engineers’ salaries in the
    year 2000 (see [Section 3.2.3](ch03.xhtml#ch03lev2sec3)), let’s find good hyperparameters
    to predict wage income.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 返回到2000年美国普查数据中关于程序员和工程师薪资的信息（见[第3.2.3节](ch03.xhtml#ch03lev2sec3)），我们来找出合适的超参数以预测工资收入。
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The only hyperparameter argument here is `k`. We’ve specified its range as `5:25`—that
    is, we try *k* = 5, *k* = 6, and so on, up through *k* = 25\. Since we’ve left
    out the `nCombs` argument, we investigated all 21 of these by default.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这里唯一的超参数是`k`。我们已将其范围指定为`5:25`——也就是说，我们依次尝试*k* = 5、*k* = 6，依此类推，一直到*k* = 25。由于我们没有提供`nCombs`参数，默认情况下会检查这21种组合。
- en: The `meanAcc` is the primary result, giving us the mean `testAcc` over all cross-validation
    runs. We will explain the `CI` and `bonfCI` columns in the next section.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '`meanAcc`是主要结果，它给我们提供了所有交叉验证运行的`testAcc`均值。我们将在下一节中解释`CI`和`bonfCI`列。'
- en: '***7.3.1 Confidence Intervals***'
  id: totrans-34
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***7.3.1 置信区间***'
- en: At first it would seem that *k* = 5 neighbors is best. Indeed, that is our guess
    as to the optimal *k* for our setting here (meaning this *n*, this feature set,
    this sampled population, and so on). But we should be careful. Here is why.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 起初看起来*k* = 5个邻居是最佳选择。确实，这是我们对当前设置下最佳*k*的猜测（也就是说，这个*n*、这个特征集、这个采样的群体等等）。但我们应该小心。以下是原因。
- en: Any `testAcc` value output from a `qe*`-series function is random, due to the
    randomness of the holdout sets. With `qeFT()`, we look at many holdout sets and
    average the result to obtain `meanAcc`. Since all the holdout sets are random,
    then so is `meanAcc`. Of course, the larger the `nXval`, the better the accuracy.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 从`qe*`系列函数输出的任何`testAcc`值都是随机的，因为验证集是随机的。使用`qeFT()`时，我们会查看多个验证集，并通过平均结果来获得`meanAcc`。由于所有的验证集都是随机的，因此`meanAcc`也是随机的。当然，`nXval`越大，准确度越好。
- en: Thus the `meanAcc` column is only approximate. The idea of the `CI` column is
    to get an idea as to the accuracy of that approximation. Specifically, the values
    in the `CI` column are the right endpoints of approximate 95 percent confidence
    intervals (CIs) for the true mean accuracy of any given combination. (For those
    who know statistics, these are *one-sided* CIs, of the form (− ∞, *a*).)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，`meanAcc` 列只是一个近似值。`CI` 列的作用是让我们大致了解这个近似值的准确性。具体来说，`CI` 列中的值是针对任何给定组合的真实平均准确度的
    95% 置信区间的右端点。（对于那些懂统计学的人来说，这些是*单侧*置信区间，形式为 (− ∞, *a*)。）
- en: In our case here, the `meanAcc` value for 7 neighbors is well within that CI
    for 5 neighbors. It’s really a toss-up between using 5 or 7 neighbors, and their
    `meanAcc` numbers are not too far apart anyway. Thus we should not take the apparent
    superiority of *k* = 5 literally.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的这个例子中，7 个邻居的 `meanAcc` 值完全落在 5 个邻居的置信区间内。实际上，在使用 5 个或 7 个邻居之间几乎是一个抛硬币的选择，而且它们的
    `meanAcc` 数字本来就没有太大差距。因此，我们不应该把 *k* = 5 的明显优越性当作字面意义来解读。
- en: In other words, the `CI` column “keeps us honest,” serving to remind us that
    `meanAcc` is only approximate and giving us some idea whether the apparent top
    few performers are distinguishable from each other.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，`CI` 列“让我们保持诚实”，提醒我们 `meanAcc` 只是一个近似值，并且为我们提供了一个是否能够区分出那些看似表现最好的组合的提示。
- en: But there’s more. When we construct a large number of CIs, their overall validity
    declines due to p-hacking (see [Section 1.13](ch01.xhtml#ch01lev13)). CIs that
    are set individually at a nominal 95 percent level have a much lower overall confidence
    level. To see this, imagine tossing 10 coins. The individual probability of heads
    is 0.5 for each coin, but the probability that *all* of them come up heads is
    much lower. Similarly, if we have ten 95 percent CIs, the probability that they
    are *all* correct is much less than 95 percent.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 但问题不仅仅是这样。当我们构建大量的置信区间时，由于 p-hacking，其总体有效性会下降（参见[第 1.13 节](ch01.xhtml#ch01lev13)）。在名义上的
    95% 水平下单独设定的置信区间会有一个更低的总体置信水平。为了解释这一点，想象一下投掷 10 枚硬币。每枚硬币的正面概率是 0.5，但它们*全都*朝正面朝上的概率要小得多。同样，如果我们有十个
    95% 的置信区间，它们*都*正确的概率远小于 95%。
- en: The `bonfCI` column adjusts for that, using something called *Bonferroni−Dunn*
    CIs. In other words, that column gives us CIs that take into account that we are
    looking at many random CIs. We thus really should look more at that column than
    the `CI` one.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`bonfCI` 列对这一点进行了调整，使用了一个叫做*Bonferroni−Dunn*的置信区间（CIs）。换句话说，该列为我们提供了考虑到我们在查看多个随机置信区间的情况下的置信区间。因此，我们实际上应该更关注该列，而不是`CI`列。'
- en: In our case here, the adjusted CI bounds are only a little larger than the original
    ones. This means we are not in much danger of p-hacking in this simple example.
    But as discussed in [Section 1.13](ch01.xhtml#ch01lev13), it could be an issue
    with an ML algorithm having many hyperparameters. In such a setting, it is quite
    possible that we will pounce on a seemingly “best” combination that actually is
    quite unrepresentative and thus much inferior to some other alternatives.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的这个例子中，调整后的置信区间的界限仅比原始的略大。这意味着在这个简单的例子中，我们不太可能遇到 p-hacking 的问题。但正如在[第 1.13
    节](ch01.xhtml#ch01lev13)中讨论的那样，对于有许多超参数的机器学习算法，这可能会成为一个问题。在这种情况下，我们很可能会抓住一个看似“最佳”的组合，实际上它并不具代表性，因此远不如其他一些选择。
- en: We have no way of knowing that is the case, of course, but a good rule of thumb
    is to consider taking the more moderate combination among several with similar
    `meanAcc` values rather than extremely large or small values of the hyperparameters.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们当然无法知道情况是否如此，但一个好的经验法则是，在几个具有相似 `meanAcc` 值的组合之间，考虑选择更为中等的组合，而不是极大或极小的超参数值。
- en: 'For instance, consider neural networks (we will look at these further in [Chapter
    11](ch11.xhtml)), which typically have a number of hyperparameters, including:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑神经网络（我们将在[第 11 章](ch11.xhtml)中进一步讨论这些），它们通常有许多超参数，包括：
- en: number of layers
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层数
- en: number of neurons per layer
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每层的神经元数量
- en: dropout rate
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 丢弃率
- en: learning rate
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率
- en: momentum
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动量
- en: initial weights
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始权重
- en: In order to investigate a broad variety of hyperparameter combinations, we would
    need to set the `nCombs` argument in `qeFT()` to a very large number, putting
    us at significant risk of finding a combination that is not actually very effective
    but that accidentally looks great. The `bonfCI` column warns us of this; the higher
    the discrepancy between it and the `CI` column, the greater the risk.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了调查各种各样的超参数组合，我们需要将`qeFT()`中的`nCombs`参数设置为一个非常大的数字，这样我们就有很大风险找到一个实际上并不有效，但偶然看起来很好的组合。`bonfCI`列警告我们这一点；它与`CI`列之间的差异越大，风险越大。
- en: On the other hand, we are merely seeking a *good* combination of hyperparameters,
    not the absolute best. For any particular combination, the `bonfCI` figure is
    giving us a reasonable indication as to whether this combination will work well
    in predicting future cases. As with many things in ML, there is no magic formula
    for how to deal with the CIs, but they can act as informal aids to our thinking.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，我们仅仅是在寻找一个*好的*超参数组合，而不是绝对最佳的组合。对于任何特定的组合，`bonfCI`数值为我们提供了一个合理的指示，告诉我们这个组合是否能很好地预测未来的案例。与机器学习中的许多事情一样，如何处理置信区间（CIs）没有固定的魔法公式，但它们可以作为我们思考的非正式辅助手段。
- en: '**NOTE**'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*Here’s a bit of history on Bonferroni−Dunn intervals: Traditionally, only
    the name Bonferroni is used, in honor of the Italian mathematician who developed
    the probability inequality central to the CIs. However, as a former student of
    Professor Olive Jean Dunn, I have been pleased to find that her name is now often
    included, as she was the one who proposed using the inequality for constructing
    CIs.*'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*这是关于Bonferroni−Dunn区间的一些历史：传统上，只有Bonferroni这个名字被使用，以纪念开发了该概率不等式的意大利数学家，这个不等式对于置信区间至关重要。然而，作为Olive
    Jean Dunn教授的前学生，我很高兴发现现在她的名字也常常被包括在内，因为正是她提出了使用这个不等式来构建置信区间。*'
- en: '***7.3.2 The Takeaway on Grid Searching***'
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***7.3.2 网格搜索的要点***'
- en: The takeaway here is that we cannot take the ordering of results in a grid search
    literally. The first few “best” results may actually be similar. Moreover, the
    apparent “best” may actually be unrepresentative. Settle on a “good” combination
    that is hopefully not too extreme rather than trying to optimize.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的要点是，我们不能字面理解网格搜索结果的顺序。最初的几个“最佳”结果可能实际上是相似的。而且，表面上看似“最佳”的结果可能并不具有代表性。与其试图优化，不如选择一个“好”的组合，最好不要过于极端。
- en: '7.4 Example: Programmer and Engineer Data'
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4 示例：程序员和工程师数据
- en: Let’s try predicting occupation instead of wage income.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试预测职业而不是工资收入。
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The CIs, especially the Bonferroni−Dunn ones—which, as noted, are more reliable—suggest
    that any of the first `k` values have about the same predictive ability. The `bonfCI`
    value for 4 neighbors extends to include the `meanAcc` value for 5 neighbors.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 置信区间，特别是Bonferroni−Dunn置信区间——如前所述，更加可靠——表明，任何前`k`个值的预测能力大致相同。对于4个邻居的`bonfCI`值延伸至包括5个邻居的`meanAcc`值。
- en: Note the role of `nXval` here. We simply used too few cross-validations. We
    should try more, but if not, the values of `k`, 1, 2, 3, 4, and 7, look about
    the same. Conservatively, we might choose to use 3 or 4 neighbors.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意这里`nXval`的作用。我们使用的交叉验证次数太少了。我们应该尝试更多的交叉验证次数，但如果不能，我们选择的`k`值（1、2、3、4和7）看起来差不多。保守地说，我们可能会选择使用3或4个邻居。
- en: '7.5 Example: Phoneme Data'
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5 示例：音素数据
- en: 'This dataset, which is included in the `regtools` package, seeks to predict
    one of two phoneme types from five sound measurements. Let’s take a look:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集包含在`regtools`包中，旨在根据五个声音测量值预测两种音素类型中的一种。我们来看看：
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The *Y* column here is `lbl`. As noted, it has two levels, so this is a two-class
    classification problem.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的*Y*列是`lbl`。如前所述，它有两个级别，所以这是一个二类分类问题。
- en: Let’s try `qeDT()` on this data. As noted, the various hyperparameters interact
    with each other, so at first, we might not try using all of them. We might just
    use, say, `alpha`, `minbucket`, and `maxdepth`.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在这组数据上尝试`qeDT()`。如前所述，各种超参数之间是相互影响的，所以一开始我们可能不尝试使用所有超参数。我们可能只使用，比如，`alpha`、`minbucket`和`maxdepth`。
- en: 'We need to specify ranges that we want to investigate for each of these parameters.
    Once again, there is no formula for deciding this, and one must gain insight from
    experience. But as an example, let’s try 0.01, 0.05, 0.10, 0.25, 0.50, and 1 for
    `alpha`, and 1, 5, and 10 for `minbucket`, and so on, as seen in the call:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要为这些参数指定我们希望调查的范围。同样，这没有一个固定的公式来决定，必须通过经验积累来获得洞察。但作为示例，我们可以尝试`alpha`的值为0.01、0.05、0.10、0.25、0.50和1，`minbucket`的值为1、5和10，等等，如调用中所示：
- en: '[PRE4]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Recall the role of `nCombs`. If we set it to `NULL`, that means we want `qeFT()`
    to try all possible combinations of our specified hyperparameter ranges. It turns
    out that there are 216 combinations (not shown). But we had set `nCombs` to 50,
    so `qeFT()` ran 50 randomly chosen combinations among the 216, and thus we see
    only 50 rows in the output here.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下`nCombs`的作用。如果将其设置为`NULL`，则意味着我们希望`qeFT()`尝试所有可能的超参数组合范围。结果显示，共有216种组合（未展示）。但我们将`nCombs`设置为50，因此`qeFT()`在216种组合中随机选择了50种进行测试，因此我们在此只看到50行输出。
- en: The more hyperparameters an ML algorithm has, and the more values we try for
    each one, the more possible combinations we have. In some cases, there are just
    too many to try them all, hence the non- `NULL` use of `nCombs`.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 一个机器学习算法的超参数越多，我们尝试的每个超参数值越多，我们就有更多的可能组合。在某些情况下，组合数实在太多，无法尝试所有可能的组合，因此需要使用非`NULL`的`nCombs`。
- en: Note, too, that the more hyperparameter combinations we run, the greater the
    risk of p-hacking. It is here that the `bonfCI` column is most useful. The fact
    that, in the output above, the `bonfCI` column is very close to the `CI` column
    in most cases tells us that p-hacking is probably not an issue for this data.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意的是，我们运行的超参数组合越多，p-hacking的风险就越大。此时，`bonfCI`列最为有用。事实上，在上面的输出中，`bonfCI`列在大多数情况下与`CI`列非常接近，这告诉我们p-hacking在这组数据中可能不是一个问题。
- en: Now, what might we glean from this output?
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们能从这些输出中得出什么结论呢？
- en: Hyperparameter tuning matters. The lowest OME values were about half of the
    largest ones.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 超参数调优很重要。最低的OME值大约是最大OME值的一半。
- en: Since the first three `CI` values are very close and within each other’s CIs,
    any of the first three hyperparameter combinations should be good.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于前面三个`CI`值非常接近，并且都在彼此的置信区间内，因此前三个超参数组合中的任何一个都应该是好的选择。
- en: The first 20 hyperparameter combinations all had a value of 8 for `maxdepth`.
    This suggests that we might do even better with a value larger than 8.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前20个超参数组合的`maxdepth`值都为8。这表明，值大于8的情况下可能会表现得更好。
- en: Larger values of `alpha` seemed to do better. This suggests that we try some
    additional large values. For instance, we didn’t try any values between 0.50 and
    1, so 0.75 might be worth a try.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 较大的`alpha`值似乎表现得更好。这表明我们可以尝试一些额外的大值。例如，我们没有尝试0.50到1之间的任何值，因此0.75可能值得一试。
- en: The top three combinations all had `mtry = 0`, while the bottom ones had a value
    of 3 for that hyperparameter. We probably should do more detailed investigation
    here.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 排名前三的组合的`mtry`值都为0，而排名靠后的组合则在该超参数上取值为3。我们可能应该在这里做更详细的调查。
- en: The hyperparameters do interact. Look at line 6, for instance. The value of
    `alpha` was smaller than in most top lines, putting a damper on the node-splitting
    process, but this was compensated for in part by small values of `minsplit` and
    `minbucket`, which encourage lots of node splitting. Such negative “correlations”
    are clear in the graphical display capability of `qeFT()` (not shown).
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 超参数确实存在相互作用。例如，看第6行。`alpha`的值比大多数最优行的值要小，这在一定程度上抑制了节点分裂过程，但通过设置较小的`minsplit`和`minbucket`，这种抑制得到了部分补偿，这两者有助于大量节点分裂。这种负面“相关性”在`qeFT()`的图形显示功能中非常明显（未展示）。
- en: 7.6 Conclusions
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.6 结论
- en: No doubt about it, finding a good set of hyperparameters is one of the major
    challenges in ML. But in this chapter we’ve seen tools that can be used for this
    purpose, and we can be reasonably confident that we’ve made a good choice.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 毫无疑问，找到一组好的超参数是机器学习中的一个主要挑战。但在本章中，我们已经看到了可以用于这个目的的工具，我们可以合理地相信我们已经做出了一个好的选择。
