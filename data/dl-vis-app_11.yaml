- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Overfitting and Underfitting
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合与欠拟合
- en: '![](Images/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/chapterart.png)'
- en: Whether we’re a person or a computer, learning general rules about a subject
    from a finite set of examples is a tough challenge. If we don’t pay enough attention
    to the details of the examples, our rules will be too general to be of much use
    when we’re considering new data. On the other hand, if we pay too much attention
    to the details in the examples, our rules will be too specific, and again we’ll
    do a bad job at evaluating new data.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是人还是计算机，从有限的例子中学习一个主题的通用规则都是一个艰难的挑战。如果我们不够重视例子中的细节，我们的规则将过于笼统，以至于在处理新数据时用处不大。另一方面，如果我们过于关注例子中的细节，我们的规则就会过于具体，同样在评估新数据时效果不好。
- en: These phenomena are respectively called *underfitting* and *overfitting*. The
    more common and troublesome problem of the two is overfitting, and if unchecked,
    it can leave us with a system that’s all but useless. We control overfitting and
    rein it in with techniques known collectively as *regularization*.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这些现象分别被称为*欠拟合*和*过拟合*。其中更为常见且令人头疼的问题是过拟合，如果不加以控制，它可能会导致我们得到一个几乎无用的系统。我们通过被统称为*正则化*的技术来控制过拟合并加以遏制。
- en: In this chapter we look at the causes of overfitting and underfitting, and how
    to address them. Finally, we wrap up the chapter by seeing how to use Bayesian
    methods to fit a straight line to a bunch of data points.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将探讨过拟合和欠拟合的原因，以及如何应对这些问题。最后，我们将通过使用贝叶斯方法将一条直线拟合到一组数据点上来总结本章内容。
- en: Finding a Good Fit
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 寻找合适的匹配
- en: When our system learns from the training data so well that it does poorly when
    presented with new data, we say that it’s *overfitting*. When it doesn’t learn
    from the training data well enough and does poorly when presented with new data,
    we say that it’s *underfitting*. Since overfitting is usually a harder problem,
    we’ll look at it first.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的系统从训练数据中学习得非常好，但在面对新数据时表现不佳时，我们称之为*过拟合*。当它没有从训练数据中学得足够好，在面对新数据时表现糟糕时，我们称之为*欠拟合*。由于过拟合通常是一个更难解决的问题，我们将首先讨论它。
- en: Overfitting
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 过拟合
- en: Let’s approach our discussion of overfitting with a metaphor. Suppose we’ve
    been invited to a big open-air wedding where we know almost nobody. Over the course
    of the afternoon, we drift through the gathering guests, exchanging introductions
    and small talk. We’ve decided to make an effort to remember people’s names, so
    each time we meet someone, we make up some kind of mental association between
    their appearance and their name (Foer 2012; Proctor 1978). One of the people we
    meet is a fellow named Walter who has a big walrus mustache. We make a mental
    picture of Walter as a walrus and try to make that picture stick in our minds.
    Later, we meet someone named Erin, and we notice she’s wearing beautiful turquoise
    earrings. We make a mental picture of her earrings that have been shortened in
    one direction, so *earring* becomes *Erin*. We make a similar mental image for
    everyone we meet, and as we mingle and bump into some of the same people again,
    we remember their names with ease. The system is working great.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个比喻来讨论过拟合。假设我们被邀请参加一个大型的户外婚礼，而我们几乎不认识任何人。在整个下午的时间里，我们在聚会的客人中游走，进行介绍和寒暄。我们决定努力记住每个人的名字，因此每次遇到某人时，我们都会在他们的外貌和名字之间建立某种心理联想（Foer
    2012；Proctor 1978）。我们遇到的其中一位叫沃尔特，他有着一副大海象胡须。我们将沃尔特想象成一只海象，并试图让这个形象在脑海中留下深刻印象。之后，我们遇到一位叫艾琳的人，我们注意到她戴着美丽的绿松石耳环。我们将她的耳环想象成一个在某个方向上被缩短的形状，所以*耳环*变成了*艾琳*。我们对每一个遇到的人都做类似的心理联想，当我们再次与一些同样的人相遇时，我们轻松地记住了他们的名字。系统运作得非常好。
- en: That evening at the reception we encounter lots of new people. At one point
    we bump into someone with a big walrus mustache. We smile and say, “Hi again,
    Walter!” only to get a confused expression. This is Bob, someone we haven’t met
    before. The same thing can happen repeatedly. We might be introduced to someone
    with beautiful earrings, but this is Susan, not Erin. The problem is that our
    mental pictures have misled us. It’s not that we didn’t learn people’s names properly,
    because we did. We just learned them in a way that worked only for the people
    in the original group and didn’t generalize when we met more people.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 那天晚上在接待会上，我们遇到了很多新的人。某个时刻，我们碰到了一位留着大海象胡子的男士。我们笑着说：“又见面了，沃尔特！”结果他露出了困惑的表情。原来这是鲍勃，我们以前从未见过他。类似的事情可能反复发生。我们可能被介绍给一个佩戴美丽耳环的人，但她是苏珊，而不是艾琳。问题在于我们的心理图像误导了我们。并不是我们没有正确地记住这些人的名字，因为我们记住了。只是我们以一种仅适用于原始小组中的人的方式来记住名字，当我们遇到更多的人时，无法做到泛化。
- en: To associate someone’s appearance with their name, we need some kind of connection
    between the two ideas. The more robust that connection, the better we can be at
    recognizing that person in a new context, even if they’re wearing a hat or glasses
    or something else that alters their appearance. In the case of our wedding, we
    learned people’s names by connecting them to a single idiosyncratic feature. The
    problem is that when we met someone else with that same feature at the reception,
    we had no way to determine that this was someone new.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 要将某人的外貌与他们的名字联系起来，我们需要在这两个概念之间建立某种联系。这个联系越牢固，我们就越能在新的环境中识别这个人，即使他们戴了帽子、眼镜或其他改变外貌的东西。以我们的婚礼为例，我们通过将人们的名字与某个独特的特征相联系来记住他们的名字。问题是，当我们在接待会上遇到另一个拥有相同特征的人时，我们无法判断他是新认识的人。
- en: At the pre-wedding party, we thought we were doing well because when we evaluated
    our performance using the training data (the names of people at the wedding),
    we got most of the results right. If we focused on the number of successes, we
    say that we achieved a high *training accuracy*. If we focus instead on the number
    of failures, we’d say we had a low *training error* (or *training loss*). But
    when we then went to the reception and needed to evaluate new data (the names
    of the additional people we met), our *generalization accuracy* was low, or equivalently,
    our *generalization error* (or *generalization loss*) was high.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在婚前派对上，我们认为自己表现得很好，因为当我们用训练数据（婚礼上人的名字）评估自己的表现时，大部分结果都正确。如果我们关注成功的次数，我们会说自己达到了很高的*训练准确度*。但如果我们转而关注失败的次数，我们会说自己有较低的*训练误差*（或*训练损失*）。但是当我们去了接待会并需要评估新数据（我们遇到的其他人的名字）时，我们的*泛化准确度*很低，或者换句话说，我们的*泛化误差*（或*泛化损失*）很高。
- en: We saw an example of this same problem in Chapter 8, where we erroneously identified
    a husky on a couch as being a Yorkshire terrier, because we used the couch as
    our only cue for identifying the breed of the dog on it.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第8章中看到了同样问题的一个例子，当时我们错误地将沙发上的一只哈士奇识别为约克夏犬，因为我们仅使用沙发作为识别它品种的唯一线索。
- en: Our errors with both people and dogs were due to overfitting. In other words,
    we learned how to classify the data in front of us, but we used specific details
    in that data, rather than learning general rules that would apply to new data
    as well.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在人和狗身上的错误都源于过拟合。换句话说，我们学会了如何分类眼前的数据，但我们只是使用了数据中的具体细节，而不是学习能够适用于新数据的一般规则。
- en: Machine learning systems are really good at overfitting. Sometimes we say that
    they’re good at *cheating*. If there’s some quirk in the input data that helps
    the system get the correct result, it finds and exploits that quirk, like our
    story in Chapter 8 of how a system was supposed to be solving the hard problem
    of finding camouflaged tanks in photos of trees but probably was taking the easy
    way out and simply noting whether the sky was sunny or cloudy.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统非常擅长过拟合。有时我们会说它们擅长*作弊*。如果输入数据中有某种特征可以帮助系统得到正确的结果，它就会找到并利用这个特征，就像我们在第8章中的故事，系统本应解决在树木照片中找到伪装坦克的难题，但很可能选择了简单的方式，通过注意天空是晴天还是阴天来解决问题。
- en: We can take two actions to control overfitting. First, we can catch when the
    rules get too specific and stop the learning process at that moment. Second, using
    *regularization* methods, we can delay the onset of overfitting by encouraging
    the system to keep learning general rules as long as possible. We’ll look at each
    approach in a moment.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以采取两种措施来控制过拟合。首先，我们可以在规则过于具体时停止学习过程。其次，通过使用*正则化*方法，我们可以通过鼓励系统尽可能长时间地学习一般规则来推迟过拟合的发生。稍后我们将分别讨论这两种方法。
- en: Underfitting
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 欠拟合
- en: The opposite of overfitting is underfitting. In contrast to overfitting, which
    results from using rules that are too precise, underfitting describes the situation
    when our rules are too vague or generic. At the wedding party, we might underfit
    by creating a rule that says, “people wearing pants are named Walter.” Although
    this is accurate for one particular piece of data, this rule is not going to generalize
    well!
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合的对立面是欠拟合。与过拟合不同，过拟合是由于使用了过于精确的规则，欠拟合描述的是当我们的规则过于模糊或通用时的情况。在婚礼聚会上，我们可能会通过创建一个规则来欠拟合：“穿裤子的人叫沃尔特”。虽然这对于某一特定数据是准确的，但这个规则并不能很好地推广！
- en: Underfitting is usually much less of a problem in practice than overfitting.
    We can often cure underfitting just by using more training data. With more examples,
    the system can work out better rules for understanding each piece of data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 与过拟合相比，欠拟合在实际中通常不是一个大问题。我们通常可以通过使用更多的训练数据来解决欠拟合问题。通过更多的示例，系统可以为每一条数据找到更好的理解规则。
- en: Detecting and Addressing Overfitting
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检测和解决过拟合
- en: How do we know when we’ve started to overfit our data? Suppose that we’re using
    a validation set to estimate a system’s generalization error after each epoch
    (when we’re done training, as usual, we use the one-time test set to get a more
    reliable generalization error). The error made by the system in response to the
    validation data is called the *validation error*. It’s an estimate of the errors
    the system will make when it’s deployed, which is called the *generalization error.*
    When the validation error flattens out, or starts to become worse, while the training
    error is improving, we’re overfitting. That’s our cue to stop learning. [Figure
    9-1](#figure9-1) shows the idea visually.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们怎么知道何时开始过拟合数据呢？假设我们使用验证集在每个训练周期后估计系统的泛化误差（像往常一样，当我们完成训练时，我们使用一次性测试集来获得更可靠的泛化误差）。系统对验证数据产生的误差称为*验证误差*。它是系统部署时将产生的误差的估计，这称为*泛化误差*。当验证误差趋于平稳或开始变得更糟，而训练误差在改善时，我们就出现了过拟合。这是我们停止学习的信号。[图
    9-1](#figure9-1)形象地展示了这一点。
- en: '![F09001](Images/F09001.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![F09001](Images/F09001.png)'
- en: 'Figure 9-1: The training and validation errors both go down steadily near the
    start of training, but after a certain point, the validation error starts to increase
    while the training error continues to decrease, signaling overfitting.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9-1：训练误差和验证误差在训练初期都稳步下降，但在某个节点之后，验证误差开始上升，而训练误差继续下降，标志着过拟合的发生。
- en: In [Figure 9-1](#figure9-1), note that the training error continues to decreaseas
    we move into the zone of overfitting, where the validation error is going up.
    That’s because we’re still learning from the training data, but now we’re learning
    information specific to that data, rather than general rules. It’s the performance
    on the validation set that lets us see that this is happening, because our validation
    error (estimating our generalization error) is getting worse. The longer we train
    like this, the worse our system will perform when we deploy it.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 9-1](#figure9-1)中，请注意，随着我们进入过拟合区域，训练误差继续下降，而验证误差开始上升。这是因为我们仍然在从训练数据中学习，但现在我们学到的是特定于该数据的信息，而不是一般规则。正是验证集上的表现让我们看到了这一点，因为我们的验证误差（估算我们的泛化误差）在变得更糟。我们这样训练的时间越长，系统在部署时的表现就会越差。
- en: Let’s see this in action. Suppose that a store’s owner subscribes to a service
    that provides her with background music. The company provides a variety of streams
    with music at different tempos, and they’ve given her a control that lets her
    choose the tempo of the music at any time. Rather than setting the tempo once
    at the start of the day and forgetting about it, she’s been finding herself adjusting
    it frequently through the day, and it’s become a distracting chore. She’s hired
    us to build a system that automatically adjusts the music throughout the day,
    the way she wants it.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个实际应用。假设某家商店的店主订阅了一项背景音乐服务。该公司提供了多种不同节奏的音乐流，并且提供了一个控制器，允许店主随时选择音乐的节奏。她并非每天早上设定好节奏后就不再理会，而是发现自己在一天中频繁调整节奏，这变成了一项令人分心的琐事。于是，她聘请了我们为她建立一个系统，自动根据她的需求调整一天中的音乐。
- en: The first step is to gather data. So, the next morning, we sit across from the
    controls and watch. Each time she adjusts the tempo, we note the time and new
    setting. Our collected data are shown in [Figure 9-2](#figure9-2).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是收集数据。第二天早上，我们坐在控制器前观察。每次她调整节奏时，我们记录下时间和新的设置。我们收集的数据如[图9-2](#figure9-2)所示。
- en: '![F09002](Images/F09002.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![F09002](Images/F09002.png)'
- en: 'Figure 9-2: Our recorded data shows the tempo chosen by our store owner each
    time she adjusted it during the day.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图9-2：我们记录的数据展示了店主在一天中每次调整节奏时所选择的节奏。
- en: Back in our labs that evening, we fit a curve to the data, as in [Figure 9-3](#figure9-3).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 那天晚上，我们回到实验室，将曲线拟合到数据中，如[图9-3](#figure9-3)所示。
- en: '![F09003](Images/F09003.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![F09003](Images/F09003.png)'
- en: 'Figure 9-3: A curve to fit to the data of [Figure 9-2](#figure9-2)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图9-3：一条适合[图9-2](#figure9-2)数据的曲线
- en: This curve is very wiggly, but we might reason that it’s a good solution, because
    it does a good job of matching her recorded choices. The next morning, we program
    the system to follow this pattern. By the middle of the afternoon the owner is
    complaining because the tempo of the music is changing too often and too dramatically.
    It’s distracting her customers.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这条曲线非常弯曲，但我们可能会推测它是一个好的解决方案，因为它很好地匹配了她记录的选择。第二天早上，我们编程让系统按照这个模式进行。到下午中时，店主抱怨音乐的节奏变化得太频繁且太剧烈，分散了顾客的注意力。
- en: This curve is overfitting the data as a result of matching the observed values
    too precisely. Her choices on the day we measured the data were based on the particular
    songs that were playing on that day. Because the service doesn’t play the same
    songs at the same time every day, we don’t want to reproduce the data from that
    one day’s observations so closely. By accommodating every bump and wiggle, we’re
    paying too much attention to the idiosyncrasies in the training data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这条曲线过度拟合了数据，因为它过于精确地匹配了观察到的数值。我们测量数据的那天，她的选择是基于当天播放的特定歌曲。由于服务并不是每天在同一时间播放相同的歌曲，我们不希望如此紧密地重现那一天的观测数据。通过适应每一个波动和曲线，我们过分关注了训练数据中的个性化特征。
- en: It would be great if we could watch her choices for several more days and use
    all of that data to come up with a more general plan, but she doesn’t want us
    taking up room in her store again. The data we have is all we’re going to get.
    We want a schedule with less variation, so the next night we reduce the accuracy
    of our match to the data. We aim for something that doesn’t jump around as much
    as before and get the gentle curve of [Figure 9-4](#figure9-4).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能再观察她的选择几天，并利用所有这些数据制定一个更通用的计划，那就太好了，但她不希望我们再次占用她店里的空间。我们拥有的数据就是我们能得到的全部。我们希望制定一个变化较小的计划，所以第二天晚上我们减少了与数据匹配的精确度。我们的目标是做出一个不像之前那样波动太大的计划，得到如[图9-4](#figure9-4)所示的平滑曲线。
- en: '![F09004](Images/F09004.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![F09004](Images/F09004.png)'
- en: 'Figure 9-4: A gentle curve for matching our tempo data'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图9-4：用于匹配我们节奏数据的平滑曲线
- en: We find the next day that our client still isn’t satisfied because this curve
    is much too coarse and ignores important features like her desire to use slower
    tempos in the morning and more upbeat songs in the afternoon. This curve is underfitting
    the data.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们第二天发现，客户仍然不满意，因为这条曲线太粗糙，忽略了重要的特征，比如她早上希望使用较慢的节奏，下午则希望使用更有活力的歌曲。这条曲线未能很好地拟合数据。
- en: What we want is a solution that’s not trying to match all of the data exactly
    but is getting a good feeling for the general trends. We want something that’s
    not too precise a match, or too loose, but “just right.” The next day, we set
    up the system according to the curve in [Figure 9-5](#figure9-5).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要的是一个不试图精确匹配所有数据，而是能够很好地把握总体趋势的解决方案。我们希望这个解决方案既不会过于精确，也不会过于松散，而是“恰到好处”。第二天，我们根据[图9-5](#figure9-5)中的曲线设置了系统。
- en: '![F09005](Images/F09005.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![F09005](Images/F09005.png)'
- en: 'Figure 9-5: A curve that matches our tempo data well enough, but not too well'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图9-5：一条足够匹配我们的节奏数据的曲线，但又不至于过度匹配
- en: Our client is happy with this curve and the tempos of the songs it chooses over
    the day. We’ve found a good compromise between underfitting and overfitting. In
    this example, finding the best curve was a matter of personal taste, but later
    on we’ll see algorithmic ways to find this sweet spot between underfitting and
    overfitting.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的客户对这条曲线和它选择的歌曲节奏感到满意。我们在欠拟合和过拟合之间找到了一个很好的折衷。在这个例子中，找到最佳曲线是个人品味的问题，但稍后我们将看到一些算法方法，帮助我们找到欠拟合和过拟合之间的最佳平衡点。
- en: '[Figure 9-6](#figure9-6) shows another example of overfitting, this time in
    classifying two categories of two-dimensional (2D) points.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9-6](#figure9-6)展示了另一个过拟合的例子，这次是在对二维（2D）点进行分类时出现的。'
- en: '![F09006](Images/F09006.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![F09006](Images/F09006.png)'
- en: 'Figure 9-6: A likely case of overfitting. (Inspired by Bullinaria 2015.)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图9-6：一个可能的过拟合情况。（灵感来源于Bullinaria 2015。）
- en: On the left side of [Figure 9-6](#figure9-6), we have one circular point deep
    in square territory, resulting in a complicated boundary curve. We call this kind
    of isolated point an *outlier*, and it’s natural to treat it with suspicion. Maybe
    this is the result of a measuring or recording error, or maybe it’s just one very
    unusual piece of perfectly valid data. Getting more data would give us a better
    sense of which case describes this oddity, but if all we have is this one set
    of data to work with, we need to decide what to do. By drawing the boundary to
    accommodate this one data point, we risk misclassifying some future data points
    as blue circles, even though they were solidly inside the brown square region,
    because they landed on the blue side of this strange boundary curve. It might
    be better to prefer a simpler curve like that on the right of [Figure 9-6](#figure9-6),
    and accept this one point as an error.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图9-6](#figure9-6)的左侧，我们看到一个圆形点位于正方形区域的深处，导致了一个复杂的边界曲线。我们称这种孤立的点为*异常点*，并且自然会对它保持怀疑态度。也许这是由于测量或记录错误造成的，或者这只是一个非常不寻常但完全有效的数据点。获取更多的数据可以帮助我们更好地理解这种异常现象的情况，但如果我们只有这一组数据，我们需要决定如何处理它。通过绘制边界来适应这个数据点，我们有可能错误地将一些未来的数据点归类为蓝色圆形，尽管它们实际上位于棕色正方形区域内部，因为它们落在了这个奇怪的边界曲线的蓝色一侧。也许更好的做法是选择[图9-6](#figure9-6)右侧的简单曲线，并接受这个点为一个错误。
- en: Now that we’ve seen what overfitting looks like, let’s look at how we can prevent
    it from happening.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了过拟合的样子，让我们看看如何防止它的发生。
- en: Early Stopping
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 早期停止
- en: Generally speaking, when we start training our model, we are underfitting. The
    model won’t have seen enough examples yet to figure out how to handle them properly,
    so its rules are general and vague.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，当我们开始训练模型时，通常是欠拟合的。模型还没有看到足够的例子来弄清楚如何正确处理它们，因此它的规则是广泛和模糊的。
- en: As we train more and the model refines its boundaries, the training and validation
    errors both typically drop. To discuss this, let’s repeat [Figure 9-1](#figure9-1)
    for convenience here as [Figure 9-7](#figure9-7).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 随着训练的进行，模型不断调整边界，训练误差和验证误差通常都会下降。为了方便讨论，我们在这里重复[图9-1](#figure9-1)，作为[图9-7](#figure9-7)。
- en: '![F09007](Images/F09007.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![F09007](Images/F09007.png)'
- en: 'Figure 9-7: A repeat of [Figure 9-1](#figure9-1) for convenience'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图9-7：为了方便起见，重复[图9-1](#figure9-1)
- en: At some point, we’ll find that although the training error is continuing to
    drop, the validation error is starting to rise (it may go flat for a while first).
    Now we’re overfitting. The training error is dropping because we’re getting more
    and more details right. But we’re now tuning our results too much to the training
    data, and the generalization error (or its estimate, the validation error) is
    going up.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在某个时刻，我们会发现，虽然训练误差持续下降，但验证误差开始上升（可能首先会保持平稳一段时间）。现在我们进入了过拟合阶段。训练误差在下降，因为我们越来越多地正确地处理了细节。但我们现在过度调整了结果，以适应训练数据，导致泛化误差（或其估计值——验证误差）上升。
- en: 'From this analysis we can come up with a good guiding principle: *when we start
    overfitting, stop training*. That is, when we get to around 28 epochs in [Figure
    9-7](#figure9-7), and we find that the validation error is going up even as training
    error is dropping, we should stop training. This technique of ending training
    just as the validation error starts to rise is called *early stopping*, since
    we’re stopping our training process before the training error has reached zero.
    It may be helpful to think of this idea as *last-minute stopping*, since we’re
    training for as long as we can, only stopping when we’ve found the best representation
    of our data without overfitting.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个分析中，我们可以得出一个好的指导原则：*当我们开始过拟合时，停止训练*。也就是说，当我们在[图 9-7](#figure9-7)中达到大约28个epoch，并且发现验证误差开始上升，而训练误差仍在下降时，我们应该停止训练。这种在验证误差开始上升时结束训练的技巧叫做*提前停止*，因为我们在训练误差尚未降到零时就停止了训练过程。把这个想法称为*最后时刻停止*可能更有帮助，因为我们尽可能地训练，直到找到没有过拟合的最佳数据表示，然后才停止训练。
- en: In practice, our error measurements are rarely as smooth as the idealized curves
    in [Figure 9-7](#figure9-7). They tend to be noisy and may even go the “wrong”
    way for short periods, so it can be hard to find the exact right place to stop.
    Most library functions for early stopping offer a few variables that let us tell
    them to implicitly smooth out these error curves so they can detect when the validation
    error really is rising and not just experiencing a momentary increase.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们的误差测量通常不像[图 9-7](#figure9-7)中的理想曲线那样平滑。它们往往会有噪声，甚至可能在短时间内出现“错误”的变化方向，因此很难找到完全正确的停止时机。大多数用于提前停止的库函数都提供了几个变量，允许我们让它们隐式地平滑这些误差曲线，以便在验证误差真正上升时能检测到，而不是仅仅经历短暂的增加。
- en: Regularization
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 正则化
- en: We always want to squeeze as much information as we can out of our training
    data, stopping just short of overfitting. Early stopping ends learning when the
    validation error starts rising, but what if there was a way to delay that phenomenon,
    so we can train longer and continue to push down both training and validation
    errors?
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们总是希望尽可能从训练数据中提取信息，直到接近过拟合为止。提前停止在验证误差开始上升时结束学习，但如果有办法延迟这一现象，让我们可以训练更久，并继续降低训练误差和验证误差呢？
- en: By analogy, consider cooking a turkey in the oven. If we just put the turkey
    in a pan and cook it on high heat, the outside eventually starts to burn. But
    say we want to cook the turkey for longer, without burning it. One way to do this
    is to wrap it in aluminum foil. The foil delays the onset of burning, letting
    us cook the turkey for longer.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 类比来说，考虑在烤箱里烤火鸡。如果我们只是将火鸡放在一个锅里，并用高温烹饪，它的外部最终会烧焦。但假设我们想要把火鸡烤得更久，而又不想让它烧焦。一个方法是将其包裹在铝箔中。铝箔延缓了烧焦的发生，使我们可以烤得更久。
- en: The techniques that delay the onset of overfitting are collectively known as
    *regularization methods*, or simply *regularization*. Remember that the computer
    doesn’t know that it’s overfitting. When we ask it to learn from the training
    data, it learns from that data as well as it can. It doesn’t know when it crosses
    the line from “good knowledge of the input data” to “overly specific knowledge
    of this particular input data,” so it’s up to us to manage the issue.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 延迟过拟合发生的技术统称为*正则化方法*，或简称*正则化*。请记住，计算机并不知道它正在过拟合。当我们要求它从训练数据中学习时，它会尽可能地从数据中学习。它并不知道何时从“对输入数据的良好理解”跨越到“对这个特定输入数据的过度特定理解”，所以管理这个问题完全由我们来处理。
- en: A popular way to perform regularization, or delay the start of overfitting,
    is to limit the values of the parameters used by the classifier. Conceptually,
    the core argument for why this staves off overfitting is that by keeping all of
    the parameters to small numbers, we prevent any one of them from dominating (Domke
    2008). This makes it harder for the classifier to become dependent on specialized,
    narrow idiosyncrasies.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的正则化方法，或者说延迟过拟合开始的方法，是限制分类器所使用的参数的值。从概念上讲，为什么这种方法能够延迟过拟合的核心理由是，通过将所有参数的值保持在较小的数字范围内，我们防止了任何一个参数占主导地位（Domke
    2008）。这使得分类器不容易依赖于特殊的、狭窄的特征。
- en: To see this, think back to our example of remembering people’s names. When we
    memorized the name of Walter, who wore a walrus mustache, that one piece of information
    dominated everything else we remembered. The other facts we could have learned
    from looking at him included that he was a man, he was almost six feet tall, he
    had long gray hair, he had a big smile and a low voice, he wore a dark-red shirt
    with brown buttons, and so on. But instead, we focused on his mustache, and ignored
    all these other useful cues. Later, when we saw a completely different person
    with a walrus mustache, that one feature dominated all the others and we mistook
    that person for Walter.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这一点，回想一下我们记住人名的例子。当我们记住了沃尔特这个名字时，他有一副海象胡子，这个信息主导了我们记住的所有其他信息。我们还可以从他的外貌中学到其他的事实，比如他是个男性，身高接近六英尺，长着灰色的长发，有着大大的微笑和低沉的声音，穿着一件深红色的衬衫，扣子是棕色的，等等。但相反，我们专注于他的胡子，忽略了其他所有有用的线索。后来，当我们看到另一个拥有海象胡子的完全不同的人时，那一特征主导了其他所有特征，我们把那个人误认为是沃尔特。
- en: If we force all of the features we notice to have values in roughly the same
    range, then “has a walrus mustache” doesn’t get the chance to dominate, and the
    other features continue to matter when we remember the name of a new person. Regularization
    techniques make sure that no one parameter, or no small set of parameters, dominates
    all the others.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们强制要求我们注意到的所有特征都在大致相同的范围内，那么“有海象胡子”就不会有机会主导，其他特征在我们记住新人的名字时仍然会发挥作用。正则化技术确保没有任何一个参数，或者一小组参数，能够主导其他所有参数。
- en: Note that we’re not trying to set all the parameters to the *same* value, which
    would make them useless. We’re just trying to make sure they’re all in roughly
    the same range. Pushing the parameters down to small values allows us to learn
    longer and extract more information from our training data before overfitting
    occurs.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们并不是试图将所有参数设置为*相同*的值，这样会使它们变得毫无用处。我们只是想确保它们都在大致相同的范围内。将参数推到较小的值可以让我们学习更长时间，并在发生过拟合之前从训练数据中提取更多信息。
- en: The best amount of regularization to apply varies from one learner and dataset
    to the next, so we usually have to try out a few values and see what works best.
    We specify the amount of regularization to apply with a hyperparameter that’s
    traditionally written as a lowercase Greek λ (lambda), though sometimes other
    letters are used. Most commonly, larger values of λ mean more regularization.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 应用正则化的最佳量因学习器和数据集的不同而有所变化，因此我们通常需要尝试几个值，看看哪一个最有效。我们通过一个超参数来指定应用的正则化量，传统上这个超参数写作小写希腊字母λ（lambda），不过有时也使用其他字母。通常来说，较大的λ值意味着更多的正则化。
- en: Keeping the parameter values small also usually means that the classifier’s
    boundary curves don’t get as complex and wiggly as they otherwise could. We can
    use the regularization parameter λ to choose how complex we want our boundary
    to be. High values give us smooth boundaries, whereas low values let the boundary
    fit more precisely to the data it’s looking at.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 保持参数值较小通常意味着分类器的边界曲线不会像它本来可能那样复杂和波动。我们可以使用正则化参数λ来选择我们希望边界有多复杂。较高的值给我们平滑的边界，而较低的值让边界更加精确地拟合它所看的数据。
- en: In later chapters we’ll work with learning architectures that have multiple
    layers of processing. Such systems can use additional, specialized regularization
    techniques called *dropout*, *batchnorm*, *layer norm*, and *weight regularization*
    that can help control overfitting on those types of architectures. All of these
    methods are designed to prevent any elements of the network from dominating the
    results.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在后面的章节中，我们将处理具有多层处理结构的学习架构。这些系统可以使用额外的、专门的正则化技术，如*dropout*、*batchnorm*、*layer
    norm*和*weight regularization*，这些技术有助于控制这些类型架构的过拟合。所有这些方法的设计目的是防止网络的任何元素主导最终结果。
- en: Bias and Variance
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 偏差与方差
- en: The statistical terms *bias* and *variance* are intimately related to underfitting
    and overfitting, and often they come up when those topics are discussed. We can
    say that bias measures the tendency of a system to consistently learn the wrong
    things, and variance measures its tendency to learn irrelevant details (Domingos
    2015). Another way to think of these is that a large amount of bias means that
    a system is prejudiced toward a particular kind of result, and a large amount
    of variance means that the answers returned by the system are too specific to
    the data.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 统计术语*偏差*和*方差*与欠拟合和过拟合密切相关，通常在讨论这些话题时会涉及到。我们可以说，偏差衡量的是一个系统持续学习错误事物的倾向，而方差衡量的是它学习无关细节的倾向（Domingos
    2015）。另一种理解方式是，大量的偏差意味着系统偏向于某种特定的结果，而大量的方差意味着系统返回的答案过于针对特定数据。
- en: We’re going to take a graphical approach to these two ideas by discussing them
    in terms of 2D curves. These curves might be the solutions to a regression problem,
    like our earlier task of setting the tempo for a store’s background music over
    time. Or the curves could be the boundary curves between two regions of the plane,
    as in a classification problem. The ideas of bias and variance are not limited
    to any one type of algorithm, or to 2D data. But we’ll stick to 2D curves because
    we can draw and interpret them. Let’s focus on finding a good fit to an underlying
    noisy curve and see how the ideas of bias and variance let us describe how our
    algorithms behave.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过讨论二维曲线的方式来图解这两个概念。这些曲线可能是回归问题的解，就像我们之前的任务一样：为商店的背景音乐设定节奏，随时间变化。或者这些曲线可能是平面上两个区域之间的边界曲线，如分类问题中的情况。偏差和方差的概念并不限于任何特定类型的算法或二维数据。但我们将坚持使用二维曲线，因为我们可以画出并解读它们。让我们专注于找到一个合适的拟合曲线，来描述如何通过偏差和方差的概念来说明我们的算法行为。
- en: Matching the Underlying Data
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 匹配基础数据
- en: Let’s suppose that an atmospheric researcher friend of ours has come to us for
    some help. She’s measured the wind speed at a certain spot at the top of a mountain,
    at the same time, every day, for several months. Her measured data is in [Figure
    9-8](#figure9-8).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的一个大气研究员朋友来向我们寻求帮助。她在几个月内每天同一时间，在一座山顶的某个位置测量了风速。她测量的数据见[图9-8](#figure9-8)。
- en: '![F09008](Images/F09008.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![F09008](Images/F09008.png)'
- en: 'Figure 9-8: Wind speed over time as measured by an atmospheric scientist. In
    this data there’s a clear underlying curve, but there’s also plenty of noise (base
    curve inspired by Macskassy 2008).'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图9-8：大气科学家测量的风速随时间变化的数据。在这些数据中，有一条明显的基础曲线，但也有很多噪音（基础曲线灵感来自Macskassy 2008）。
- en: She believes that the data she’s measured is the sum of an *idealized curve*,
    which is the same from year to year, and *noise*, which accounts for unpredictable
    day-to-day fluctuations. The data she measured is called a *noisy curve*, since
    it’s the sum of the idealized curve and the noise. [Figure 9-9](#figure9-9) shows
    the idealized curve and the noise that, when added together, make [Figure 9-8](#figure9-8).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 她认为她所测得的数据是一个*理想化曲线*和*噪音*的总和，理想化曲线每年都保持不变，而噪音则解释了日常波动的不可预测性。她测得的数据被称为*噪声曲线*，因为它是理想化曲线和噪音的叠加。[图9-9](#figure9-9)展示了理想化曲线和噪音，二者叠加在一起形成[图9-8](#figure9-8)中的数据。
- en: '![F09009](Images/F09009.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![F09009](Images/F09009.png)'
- en: 'Figure 9-9: The data from [Figure 9-8](#figure9-8) split into two pieces. Left:
    The underlying “idealized” curve we seek. Right: The noise that nature added to
    the idealized curve to give us the noisy, measured data. Note that the two graphs
    have different vertical scales.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图9-9：将[图9-8](#figure9-8)中的数据分成两部分。左侧：我们要寻找的基础“理想化”曲线。右侧：大自然添加到理想化曲线上的噪音，导致我们得到噪声的、测量的数据。请注意，这两个图的垂直尺度不同。
- en: Our atmospheric researcher believes that she has a good model for describing
    the noise (maybe it follows the uniform or Gaussian distributions we saw in Chapter
    2). But her description of the noise is statistical, so she can’t use it to fix
    her day-to-day measurements. In other words, if she knew the exact values of the
    noise on the right of [Figure 9-9](#figure9-9), she could subtract them from the
    measurements in [Figure 9-8](#figure9-8) to get at her goal, the clean curve on
    the left of [Figure 9-9](#figure9-9). But she doesn’t know those noise values.
    She has a statistical model that can generate lots of curves like those on the
    right of [Figure 9-9](#figure9-9), but she doesn’t have the specific values that
    correspond to her data.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的大气研究员认为她有一个很好的模型来描述噪声（也许它遵循我们在第二章中看到的均匀分布或高斯分布）。但是她对噪声的描述是统计性的，因此她不能用它来修正她的日常测量。换句话说，如果她知道[图
    9-9](#figure9-9)右侧噪声的确切值，她可以将其从[图 9-8](#figure9-8)中的测量值中减去，从而获得她的目标，即[图 9-9](#figure9-9)左侧的干净曲线。但她不知道这些噪声值。她有一个统计模型，可以生成很多像[图
    9-9](#figure9-9)右侧那样的曲线，但她没有与她的数据对应的具体值。
- en: Here’s one approach to cleaning up the noisy data. We can go back to the noisy
    data in [Figure 9-8](#figure9-8) and try to fit a smooth curve to it (Bishop 2006).
    By choosing the complexity of the curve to be wiggly enough to follow the data,
    but not so wiggly that it tries to match each point exactly, we hope to get a
    pretty fair match to the general shape of the curve, which is a good starting
    point for finding the underlying smooth curve.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一种清理噪声数据的方法。我们可以回到[图 9-8](#figure9-8)中的噪声数据，并尝试将一条平滑曲线拟合到它上面（Bishop 2006）。通过选择曲线的复杂性，使其足够曲折以跟随数据，但又不至于曲折到精确匹配每个点，我们希望能得到一个相当好的匹配，反映曲线的大致形状，这是找到潜在平滑曲线的一个良好起点。
- en: There are many ways to fit a smooth curve to noisy data. [Figure 9-10](#figure9-10)
    shows one such curve. The little wiggle at the right end is typical of the sort
    of curve we used, which tends to jump around a bit near the edges of the dataset.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多方法可以将平滑曲线拟合到噪声数据上。[图 9-10](#figure9-10)展示了其中一条这样的曲线。右端的小波动是我们使用的曲线的典型特征，它在数据集的边缘附近往往会有一些波动。
- en: That doesn’t look too far off. But can we do better?
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来还不错。但我们能做得更好吗？
- en: Let’s apply the ideas of bias and variance to the problem of finding the idealized
    curve. The idea is inspired by the method of bootstrapping that we discussed in
    Chapter 2, but we won’t actually use the bootstrapping technique.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将偏差和方差的概念应用于寻找理想曲线的问题。这个想法源于我们在第二章讨论的自助法（bootstrapping）方法，但我们实际上不会使用自助法技巧。
- en: '![F09010](Images/F09010.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![F09010](Images/F09010.png)'
- en: 'Figure 9-10: Fitting our noisy data with a curve using a curve-fitting algorithm'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9-10：使用曲线拟合算法拟合我们的噪声数据
- en: Let’s make 50 versions of the original noisy data, but each version contains
    just 30 points selected randomly, without replacement. The first five of these
    reduced datasets are shown in [Figure 9-11](#figure9-11).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们制作原始噪声数据的 50 个版本，但每个版本仅包含 30 个随机选取的点，且不重复选择。这些缩小的数据集的前五个版本显示在[图 9-11](#figure9-11)中。
- en: '![F09011](Images/F09011.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![F09011](Images/F09011.png)'
- en: 'Figure 9-11: Five of the 50 smaller versions of our noisy starting data. Each
    version consists of 30 samples chosen from the original data without replacement.
    These are the first five versions, with the chosen points shown as green dots,
    and the original, noisy data shown in gray for reference.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9-11：我们原始噪声数据的 50 个小版本中的五个。每个版本由从原始数据中随机选择的 30 个样本组成，且不重复选择。这些是前五个版本，所选点显示为绿色点，原始的噪声数据为灰色，以供参考。
- en: Let’s try matching each of these sets of points with simple curves and then
    complex curves, and compare the results in terms of bias and variance.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试用简单的曲线和复杂的曲线来匹配这些数据点集，并根据偏差和方差来比较结果。
- en: High Bias, Low Variance
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高偏差，低方差
- en: We’ll first fit our data with simple, smooth curves. Because we’ve selected
    these qualities ahead of time, we expect that all of our resulting curves will
    look about the same. The curves that fit our five sets of data in [Figure 9-11](#figure9-11)
    are shown in [Figure 9-12](#figure9-12).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先用简单的平滑曲线来拟合数据。由于我们提前选择了这些特征，我们预计所有的拟合曲线看起来都会差不多。拟合我们在[图 9-11](#figure9-11)中五个数据集的曲线如[图
    9-12](#figure9-12)所示。
- en: '![F09012](Images/F09012.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![F09012](Images/F09012.png)'
- en: 'Figure 9-12: Fitting simple curves to our first five sets of points in [Figure
    9-11](#figure9-11)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9-12：将简单曲线拟合到我们在[图 9-11](#figure9-11)中的前五个数据点集
- en: As expected, the curves are all simple and similar. Because these curves are
    very similar to one another, we say that this collection of curves is showing
    a *high bias*. The *bias* here refers to the predetermined preference for a simple
    shape. Because the curves are so simple, each one lacks the flexibility to pass
    through more than a few of its points at the most.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，这些曲线都非常简单且相似。由于这些曲线彼此非常相似，我们说这组曲线表现出*高偏差*。这里的*偏差*指的是对简单形状的预定偏好。因为这些曲线非常简单，所以每条曲线都缺乏通过多个点的灵活性，最多只能经过其中的一些点。
- en: The *variance* refers to how much the curves vary, or differ, from one to the
    next. To see the variance of these high-bias curves, we can draw all 50 curves
    on top of one another, as in [Figure 9-13](#figure9-13).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*方差*指的是曲线之间的差异或变化程度。为了查看这些高偏差曲线的方差，我们可以将所有50条曲线叠加在一起，如[图9-13](#figure9-13)所示。'
- en: '![F09013](Images/F09013.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![F09013](Images/F09013.png)'
- en: 'Figure 9-13: The curves for all 50 of our 30-point samples from the original
    data, overlaid on one another'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图9-13：所有50个30点样本的曲线，叠加在一起
- en: As expected, the curves are quite similar. We say that they demonstrate low
    variance.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，曲线非常相似。我们说它们展示了低方差。
- en: To sum up, this collection of curves has a high bias, because they all have
    about the same shape, and a low variance, because the individual curves aren’t
    being influenced much by the data.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，这组曲线具有高偏差，因为它们的形状差不多，而且具有低方差，因为个别曲线不受数据的太多影响。
- en: Low Bias, High Variance
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 低偏差，高方差
- en: Now let’s try reducing our constraint that the curves need to be simple. That
    lets us fit complex curves to our data, so that each one comes closer to matching
    its green points. [Figure 9-14](#figure9-14) shows these curves applied to our
    first five sets of data. Compared to [Figure 9-12](#figure9-12), these curves
    are much wigglier, with multiple hills and valleys. Though they still don’t pass
    directly through too many points, they come a lot closer to them.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试减少曲线需要简单的约束条件。这使我们能够拟合复杂的曲线，使每条曲线更接近其绿色点。[图9-14](#figure9-14)展示了这些曲线应用于我们前五组数据的情况。与[图9-12](#figure9-12)相比，这些曲线更加弯曲，拥有多个山峰和谷底。尽管它们仍然不会直接经过太多点，但它们比之前更接近这些点。
- en: '![F09014](Images/F09014.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![F09014](Images/F09014.png)'
- en: 'Figure 9-14: The complex curves created for the first five sets of points from
    our noisy data'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图9-14：我们从噪声数据的前五组点生成的复杂曲线
- en: Since the shapes of these curves are more complex and flexible, they’re more
    influenced by the data than by any starting assumptions. Because we are placing
    fewer constraints on the curve shapes, we say that the collection has *low bias*.
    On the other hand, they’re quite different from one another. We can see this by
    drawing all 50 curves on top of one another, as shown in [Figure 9-15](#figure9-15).
    Because the curves veer off wildly at the start and end, we also show an expanded
    vertical scale that covers those big swings.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些曲线的形状更复杂且更灵活，它们比任何初始假设更受数据的影响。因为我们对曲线形状施加的约束更少，所以我们说这组曲线具有*低偏差*。另一方面，这些曲线彼此差异很大。我们可以通过将所有50条曲线叠加在一起来看出这一点，如[图9-15](#figure9-15)所示。由于曲线在开始和结束时剧烈偏离，因此我们还展示了覆盖这些大幅波动的扩展垂直坐标轴。
- en: '![F09015](Images/F09015.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![F09015](Images/F09015.png)'
- en: 'Figure 9-15: The complex curves fit to our 50 sets of data. The plot on the
    right shows the entire vertical scale of the curves.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图9-15：拟合到我们50组数据的复杂曲线。右侧的图显示了曲线的整个垂直尺度。
- en: These curves don’t all follow the same shape, so they have low bias. Furthermore,
    they are quite different from one another, and each is strongly influenced by
    its data, so the collection has *high variance*.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这些曲线并不完全遵循相同的形状，因此它们具有低偏差。此外，它们彼此之间差异很大，每条曲线都受到其数据的强烈影响，因此这组曲线具有*高方差*。
- en: Comparing Curves
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 比较曲线
- en: Let’s recap our curve fitting experiment so far.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下迄今为止的曲线拟合实验。
- en: Our atmospheric scientist asked us for a curve that matches the underlying idealized
    curve in her data. We created 50 small sets of points, randomly extracted from
    her original, noisy data. When we fit simple, smooth curves to those sets of points,
    the curves consistently missed most of the data points. That set of curves had
    a high bias, or a predisposition to a particular result (smooth and simple). The
    curves were not much influenced by the data they were intended to match, so that
    set of curves had a low variance.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的气象学家要求我们提供一条符合她数据中理想化曲线的曲线。我们从她原始的噪声数据中随机提取了50个小点集。当我们为这些点集拟合简单光滑的曲线时，曲线始终未能准确匹配大多数数据点。这组曲线具有高偏差，或者说是对特定结果（光滑且简单）的偏向。曲线受它们要匹配的数据影响较小，因此这组曲线具有低方差。
- en: On the other hand, when we fit complex and wiggly curves to these sets of points,
    the curves were able to fit to the data and came much closer to most of the points.
    Because they were influenced more by the data than by any predisposition to a
    particular shape, that set of curves had a low bias. But the adaptability of the
    curves means that they were all significantly different from one another. In other
    words, that set of curves had a high variance.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，当我们为这些点集拟合复杂且波动的曲线时，曲线能够适应数据，并且与大多数点更接近。由于它们更受数据的影响，而非对特定形状的偏向，这组曲线具有低偏差。但曲线的适应性意味着它们彼此之间差异很大。换句话说，这组曲线具有高方差。
- en: So, the first set had high bias and low variance, and the second set has low
    bias and high variance.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，第一个集合具有高偏差和低方差，而第二个集合具有低偏差和高方差。
- en: Ideally, we’d like curves with a low bias (so we’re not imposing our preconceived
    ideas on their possible shapes), and low variance (so our different curves all
    create roughly the same match to the original, noisy data). Unfortunately, in
    most real situations, as either measure goes down, the other goes up. This means
    it’s up to us to find the best *bias-variance tradeoff* for each specific situation.
    We’ll come back to this issue in a moment.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们希望得到具有低偏差（这样我们不会对其可能的形状加上先入为主的想法）和低方差（这样我们不同的曲线都能大致匹配原始噪声数据）的曲线。不幸的是，在大多数实际情况中，随着其中一个指标的下降，另一个指标会上升。这意味着我们需要找到每种特定情况的最佳*偏差-方差权衡*。我们稍后会回到这个问题。
- en: Notice that bias and variance are properties of *families*, or collections,
    of curves. It doesn’t make sense to discuss the bias and variance of a single
    curve. Bias and variance often come up in machine learning discussions as ways
    to describe the complexity or power of a model or algorithm.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，偏差和方差是*一系列*曲线或曲线集合的属性。讨论单一曲线的偏差和方差是没有意义的。偏差和方差在机器学习讨论中常常用来描述模型或算法的复杂性或能力。
- en: We can now see how bias and variance help us describe underfitting and overfitting.
    In the beginning of training, as the system tries to find the right way to represent
    the training data, it’s producing general rules, or underfitting. If these rules
    are boundaries between classes of data, they have the form of curves. If we train
    on multiple similar but different datasets, we’ll see curves that are simple in
    shape and like one another. That is, they have high bias and low variance.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以看到，偏差和方差如何帮助我们描述欠拟合和过拟合。在训练开始时，系统试图找到表示训练数据的正确方式，它会生成一般规则，或者说是欠拟合。如果这些规则是数据类之间的边界，它们呈曲线形状。如果我们在多个相似但不同的数据集上进行训练，我们会看到形状简单且相似的曲线。也就是说，它们具有高偏差和低方差。
- en: Later in training, the curves for each dataset are more complicated. There are
    fewer preconditions on their shape, so they have low bias, and they can closely
    match the training data, so they have high variance. When we let a system train
    for too long, the high-variance curves start following the input data too tightly,
    causing overfitting.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练的后期，每个数据集的曲线变得更加复杂。它们的形状没有太多的前提条件，因此具有较低的偏差，并且可以与训练数据紧密匹配，因此具有较高的方差。当我们让系统训练得过长时，高方差的曲线开始过度跟随输入数据，导致过拟合。
- en: '[Figure 9-16](#figure9-16) shows the tradeoff of bias and variance graphically.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-16](#figure9-16)展示了偏差和方差的权衡关系图。'
- en: '![F09016](Images/F09016.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![F09016](Images/F09016.png)'
- en: 'Figure 9-16: Top row: Four curves we’d like to match. Middle row: Using curves
    with high bias and low variance. Bottom row: Curves with low bias and high variance.
    The far-right image in the bottom two rows shows the four curves superimposed.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图9-16：最上排：我们希望匹配的四条曲线。中排：使用高偏差和低方差的曲线。底排：使用低偏差和高方差的曲线。底部两行最右侧的图像显示了四条曲线的叠加。
- en: In the middle row, high bias gives us nice, simple curves (which avoid overfitting),
    but their low variance means they can’t match the data very well. In the bottom
    row, low bias lets the curves better match the data, but their high variance means
    that the curves can match too well (which risks overfitting).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在中排，高偏差给我们带来简单的曲线（可以避免过拟合），但它们的低方差意味着它们不能很好地匹配数据。在底排，低偏差让曲线更好地匹配数据，但它们的高方差意味着曲线可能过度匹配（从而有过拟合的风险）。
- en: In general, neither bias nor variance is inherently better or worse than the
    other, so we shouldn’t always be tempted to find, say, the solution with the lowest
    possible bias or variance. In some applications, high bias or high variance may
    be acceptable. For instance, if we know that our training set is absolutely representative
    of all future data, then we don’t care about the variance and instead aim for
    the lowest possible bias, since matching that training set perfectly is just what
    we want. On the other hand, if we know that our training set is not a good representative
    of future data (but it’s the best we have at the moment), we may not care about
    bias, since matching this lousy dataset isn’t important, but we want the lowest
    variance we can get so that we have the best chance of at least doing something
    reasonable on future data.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，偏差和方差没有谁天生优于谁，因此我们不应该总是试图找到例如最低偏差或方差的解决方案。在某些应用中，高偏差或高方差可能是可以接受的。例如，如果我们知道我们的训练集完全代表了所有未来的数据，那么我们不在乎方差，而是追求最低的偏差，因为完美匹配训练集正是我们所需要的。另一方面，如果我们知道我们的训练集并不能很好地代表未来的数据（但这是我们目前最好的选择），我们可能不关心偏差，因为匹配这个糟糕的数据集并不重要，但我们希望得到最低的方差，以便在未来数据上至少能做出一些合理的预测。
- en: In general, we need to find the right balance between these two measures in
    a way that works best for the goals of any particular project, given the specific
    algorithm and data we’re working with.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，我们需要在这两个度量之间找到合适的平衡，使其最好地服务于特定项目的目标，考虑到我们正在使用的具体算法和数据。
- en: Fitting a Line with Bayes’ Rule
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用贝叶斯法则拟合直线
- en: Bias and variance are a useful way to characterize how well a family of curves
    fits their data. Recalling our discussion of the frequentist and Bayesian philosophies
    from Chapter 4, we can say that bias and variance are inherently frequentist ideas.
    That’s because the notions of bias and variance rely on drawing multiple values
    from a source of data. We don’t rely too much on any single curve. Instead, we
    use averaging of all the curves to find the “true” answer that each curve approximates.
    Those ideas fit the frequentist approach very well.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差和方差是描述一组曲线如何拟合数据的有用方式。回顾我们在第四章讨论的频率主义和贝叶斯哲学，我们可以说偏差和方差本质上是频率主义的概念。这是因为偏差和方差的概念依赖于从数据源中提取多个值。我们不依赖于任何单一曲线，而是通过对所有曲线进行平均，来找到每条曲线所逼近的“真实”答案。这些观点非常适合频率主义方法。
- en: By contrast, the Bayesian approach to fitting data asserts that the results
    can only be described in a probabilistic way. We list out all the ways to match
    our data that we think are possible and attach a probability to each one. As we
    gather more data, we gradually eliminate some of those descriptions and thereby
    make the remaining ones more probable, but we never get to a single, absolute
    answer.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，贝叶斯方法拟合数据的方式主张，结果只能用概率的方式来描述。我们列出所有我们认为可能的匹配数据的方式，并为每种方式附加一个概率。随着我们收集更多的数据，我们逐渐淘汰一些描述，从而使剩余的描述变得更加可能，但我们永远无法得到一个唯一的、绝对的答案。
- en: Let’s see this in practice. Our discussion is based on a visualization from
    *Pattern Recognition and Machine Learning* (Bishop 2006). We’ll use Bayes’ Rule
    to find a nice approximation of the noisy data atmospheric data we saw in [Figure
    9-8](#figure9-8). Rather than fit a complicated curve to our data, we restrict
    ourselves to straight lines. That’s only because doing so lets us show everything
    with 2D plots and diagrams. In fact, we stick to lines that are *mostly horizontal*.
    Again, this is just so we can draw nice diagrams that don’t require higher dimensions.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在实际操作中看看这个方法。我们的讨论基于《模式识别与机器学习》（Bishop 2006）中的一个可视化图示。我们将使用贝叶斯定理来找到对我们在[图9-8](#figure9-8)中看到的带噪声大气数据的良好近似。我们并不打算为数据拟合一条复杂的曲线，而是限制自己使用直线。这仅仅是因为这样做可以让我们用二维图形和图表展示所有内容。事实上，我们坚持使用*大部分水平*的直线。再说一次，这只是为了方便绘制不需要高维度的漂亮图示。
- en: The method for curve fitting with Bayes’ Rule can use complex curves, or sheets
    in space, or even shapes with hundreds of dimensions. We will use straight lines
    that are mosty horizontal only because that choice keeps the pictures simple.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 使用贝叶斯定理进行曲线拟合的方法可以使用复杂的曲线、空间中的平面，甚至具有数百个维度的形状。我们仅使用*大部分水平*的直线，只因为这种选择让图示保持简单。
- en: We’ll be working with multiple lines at once, so it would be great to find a
    compact way to represent different groups of lines without drawing them all.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将同时处理多行，因此找到一种紧凑的方式来表示不同组的线条而不需要绘制它们所有的内容会非常有帮助。
- en: The trick will be to describe every line with two numbers. The first tells us
    how much the line is tilted from being perfectly horizontal, which gives us a
    line in any orientation. The second number tells us how much to move the line
    up and down.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 诀窍是用两个数字来描述每条直线。第一个数字告诉我们直线与水平线的倾斜程度，这样可以得到任何方向的直线。第二个数字告诉我们该如何上下移动直线。
- en: The first number is the *slope*. A horizontal line has a slope of 0\. As the
    line rotates clockwise, as in [Figure 9-17](#figure9-17), the slope increases.
    As the line rotates counterclockwise, the slope decreases.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个数字是*斜率*。水平线的斜率为0。当直线顺时针旋转时，如[图9-17](#figure9-17)所示，斜率增加。当直线逆时针旋转时，斜率减少。
- en: When the line is perfectly diagonal, the slope is either 1 or –1\. As it rotates
    to steeper orientations, the slope increases quickly until it reaches infinity
    for a perfectly vertical line. We can take steps to avoid this problem, but it
    only makes the discussion more complicated. So for the sake of simplicity, we
    will limit our attention to lines that have a slope between –2 and 2, which lie
    in the green zone in [Figure 9-17](#figure9-17).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 当直线完全对角时，斜率为1或–1。当它旋转到更陡峭的角度时，斜率迅速增加，直到对于完全垂直的直线达到无穷大。我们可以采取措施来避免这个问题，但这只会让讨论变得更复杂。因此，为了简化起见，我们将只关注斜率在–2到2之间的直线，这些直线位于[图9-17](#figure9-17)中的绿色区域。
- en: '![f09017](Images/f09017.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![f09017](Images/f09017.png)'
- en: 'Figure 9-17: A horizontal line has a slope of 0\. As the line rotates counterclockwise,
    the slope increases. As it rotates clockwise, the slope decreases. We will only
    use lines with slopes that fall in the light-green region.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图9-17：水平线的斜率为0。当直线逆时针旋转时，斜率增加。当直线顺时针旋转时，斜率减少。我们只会使用斜率位于浅绿色区域的直线。
- en: The second number that describes a line is the *Y intercept*. This merely moves
    the whole line up or down as a whole. This number tells us the value of the line
    when X is zero. In other words, it’s the value of the line as it crosses, or intercepts,
    the Y axis. [Figure 9-18](#figure9-18) illustrates the idea. Again for simplicity
    we’ll restrict our focus to lines with a Y intercept in the range [–2, 2], tinted
    green in [Figure 9-18](#figure9-18).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 描述一条直线的第二个数字是*Y轴截距*。它只是将整条直线上下平移。这个数字告诉我们当X为零时直线的值。换句话说，它是直线与Y轴交点的值。[图9-18](#figure9-18)展示了这一概念。为了简化起见，我们将重点关注Y截距在[–2,
    2]范围内的直线，这些直线在[图9-18](#figure9-18)中被绿色标出。
- en: '![f09018](Images/f09018.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![f09018](Images/f09018.png)'
- en: 'Figure 9-18: The Y intercept tells us the Y value of the line when it crosses
    the Y axis, regardless of its slope. We’ll just use lines that have a Y intercept
    between –2 and 2.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图9-18：Y截距告诉我们直线与Y轴交点时的Y值，无论其斜率如何。我们将只使用Y截距在–2到2之间的直线。
- en: Given any line, we can measure its orientation to get a value for its slope,
    and observe where it crosses the Y axis to get the value of the Y intercept. That’s
    everything we need to describe the line. We can show this as a point in a new
    2D grid where the axes are labeled *slope* and *Y intercept*. Let’s call this
    an *SI* diagram, standing for slope-intercept. A normal diagram will be just an
    XY diagram. We can also say that the SI diagram plots lines in *SI space*, and
    the XY diagram shows lines in *XY space*(also called *Cartesian space*).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 给定任意一条直线，我们可以测量其方向以得到斜率的值，并观察它与Y轴的交点以得到Y截距的值。这就是描述直线所需的所有信息。我们可以将其显示为一个新二维网格中的点，其中坐标轴标记为*斜率*和*Y截距*。我们将此称为*SI*图，表示斜率-截距。普通的图将是一个XY图。我们还可以说，SI图绘制的是*SI空间*中的直线，而XY图则显示的是*XY空间*（也叫*笛卡尔空间*）中的直线。
- en: '[Figure 9-19](#figure9-19) shows a few lines in both XY and SI diagrams.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-19](#figure9-19) 显示了XY和SI图中的几条线。'
- en: Mathematicians call these two ways of looking at the same thing a *dual representation*,
    and there’s a lot to be said about such things. We’ll stick to just what we need
    for our discussion of fitting a line using Bayes’ Rule.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 数学家称这两种看待同一事物的方式为*对偶表示*，这样的事物还有很多可以讨论的地方。我们只会讨论与使用贝叶斯定理拟合直线相关的内容。
- en: '![f09019](Images/f09019.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![f09019](Images/f09019.png)'
- en: 'Figure 9-19: Left: Three lines in XY space. Right: Each line drawn as a dot
    in SI space.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9-19：左：XY空间中的三条直线。右：每条直线作为一个点绘制在SI空间中。
- en: Something interesting happens when we arrange a set of points in SI space along
    a line. When we draw their corresponding lines in XY space, they all meet at the
    same XY point. [Figure 9-20](#figure9-20) shows this in action. This is true any
    time our SI points lie on a line.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在SI空间中将一组点沿一条直线排列时，发生了一些有趣的事情。当我们在XY空间中绘制它们的对应直线时，它们都会在同一个XY点交汇。[图 9-20](#figure9-20)展示了这一现象。这在任何时候，如果我们的SI点位于一条直线上时，都是成立的。
- en: '![f09020](Images/f09020.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![f09020](Images/f09020.png)'
- en: 'Figure 9-20: Two examples of placing dots in SI space along a line. Their corresponding
    lines will always meet at a single point in XY space.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9-20：在SI空间中沿直线放置数据点的两个示例。它们对应的直线将在XY空间中的单一点交汇。
- en: As we look for the best line to fit our data, we know it probably won’t be able
    to go through all the points. But we’d like it to come close. So instead of placing
    dots in SI space, let’s assign every possible line a probability from 0 to 1,
    indicating how likely it is to be the line we’re looking for. [Figure 9-21](#figure9-21)
    shows the idea (in [Figure 9-21](#figure9-21), and the figures to come, we scale
    up the probability values as needed so that they’re easier to read).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们寻找最适合数据的最佳直线时，我们知道它可能无法通过所有数据点。但我们希望它尽可能接近。因此，我们不在SI空间中放置数据点，而是给每一条可能的直线分配一个从0到1的概率，表示它成为我们所需直线的可能性。[图
    9-21](#figure9-21) 展示了这个概念（在[图 9-21](#figure9-21)及随后的图中，我们根据需要放大概率值，以便更容易阅读）。
- en: '![f09021](Images/f09021.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![f09021](Images/f09021.png)'
- en: 'Figure 9-21: Left: A point in XY space. Middle: Every point in the SI graph
    is assigned a probability from 0 to 1 (blue to purple), telling us how close that
    line comes to the point. We show some representative lines with black dots. Right:
    The black dots in the middle figure drawn as their lines in XY space. Note that
    they all pass through our original red dot, or come close to it.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9-21：左：XY空间中的一个点。中：SI图中的每个点都分配一个从0到1的概率（从蓝到紫），告诉我们该直线与点的接近程度。我们展示了一些代表性的直线，使用黑点表示。右：中间图中的黑点作为它们在XY空间中的直线绘制。注意，它们都通过我们原始的红点，或者接近它。
- en: 'Let’s now return to the problem we want to solve: finding the best straight
    line approximation for a noisy set of data. Let’s start with an arbitrary, broad
    Gaussian bump in SI space as a prior, as in the top left of [Figure 9-22](#figure9-22).
    This says that any line might be our answer, but those in the bright purple region
    are the most likely. We’ve chosen some points in this graph according to their
    probabilities, and drawn them in the upper right. We’re getting a lot of very
    different lines, confirming our very vague prior when it comes to choosing lines.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们回到我们要解决的问题：为一个带噪声的数据集找到最佳的直线近似。我们从SI空间中的一个任意、宽泛的高斯凸起作为先验开始，如[图 9-22](#figure9-22)左上角所示。这意味着任何一条直线都可能是我们的答案，但处于亮紫色区域的直线最为可能。我们根据这些点的概率选择了几个点，并将它们绘制在右上角。我们得到了很多非常不同的直线，这确认了我们在选择直线时非常模糊的先验。
- en: '![f09022](Images/f09022.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![f09022](Images/f09022.png)'
- en: 'Figure 9-22: Top left: A prior in SI space, along with some points chosen from
    that probability distribution. Top right: Those points drawn as lines in XY space.
    Second and third rows: Like the top row, but with smaller priors.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9-22：左上：在SI空间中的先验，以及从该概率分布中选择的一些点。右上：这些点在XY空间中绘制为直线。第二行和第三行：与第一行类似，但先验变得更小。
- en: We can see in [Figure 9-22](#figure9-22) that as the prior becomes smaller,
    we get a more refined choice of lines. So we’d hope that Bayes’ Rule will follow
    this kind of change, and give us a small posterior (or prior) resulting in a small
    collection of lines that can fit our data.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在[图 9-22](#figure9-22)中看到，随着先验变得更小，我们得到的直线选择变得更加精细。因此，我们希望贝叶斯法则能够跟随这种变化，给我们一个较小的后验（或先验），从而得到一小组能够拟合我们数据的直线。
- en: Now we’re ready to use Bayes’ Rule to match our data! [Figure 9-23](#figure9-23)
    shows the process.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备好使用贝叶斯法则来匹配我们的数据了！[图 9-23](#figure9-23)展示了这一过程。
- en: '![f09023](Images/f09023.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![f09023](Images/f09023.png)'
- en: 'Figure 9-23: Fitting a straight line through our data with Bayes’ Rule (figure
    inspired by Bishop 2006)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9-23：使用贝叶斯法则拟合我们的数据的直线（图形灵感来自Bishop 2006）
- en: Let’s walk through what’s happening in [Figure 9-23](#figure9-23) row by row.
    In row 1, we show our prior, or our starting guess of the distribution of straight
    lines that will fit our data. We arbitrarily chose a Gaussian with its center
    in the middle. This prior means that we’re guessing that our data is mostly likely
    fit by a straight line that is horizontal and has a Y intercept of 0\. That is,
    it’s the X axis itself. But the Gaussian goes all the way out to the edges (it
    doesn’t quite reach 0 anywhere in the diagram), so any of our available lines
    are possible. We could look at the data and pick a better starting prior, but
    this one is simple and, because it has at least some probability for every line
    we’ll consider as a candidate, it’s an acceptable start.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐行查看[图 9-23](#figure9-23)中发生的情况。在第1行中，我们展示了我们的先验，或者说是我们对将拟合我们数据的直线分布的初步猜测。我们任意选择了一个中心在中间的高斯分布。这个先验意味着我们猜测我们的数据最可能被一条水平的直线拟合，其Y截距为0，即它就是X轴本身。但高斯分布延伸到边缘（它在图中任何地方都没有完全达到0），因此我们考虑的任何直线都是可能的。我们可以查看数据并选择一个更好的初始先验，但这个先验简单，而且因为它对每一条我们考虑作为候选的直线都有一定的概率，所以它是一个可以接受的起点。
- en: The image on the right of row 1 shows 20 lines picked at random from this prior,
    with more probable lines being more likely to be picked.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 第1行右侧的图像显示了从该先验中随机选取的20条直线，其中更有可能的直线被选中的概率更高。
- en: The left image in row 2 shows our noisy dataset, and a point picked at random,
    shown in red. The likelihood diagram for all lines that go through (or near) that
    point is shown to its right. Now we apply Bayes’ Rule, and multiply the prior
    in row 1 by the likelihood in row 2\.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 第2行左侧的图像显示了我们的噪声数据集，以及随机选取的一个点，标记为红色。所有通过（或接近）该点的直线的似然图显示在其右侧。现在我们应用贝叶斯法则，将第1行中的先验与第2行中的似然相乘。
- en: The result is the left figure in row 3\. This is the posterior, or the result
    of multiplying each point in the prior (how likely we thought that line was),
    with the corresponding point in the new point’s likelihood (how likely it is that
    each line fits this piece of data). We’re not showing the Bayes’ Rule step of
    dividing by the evidence because we’re scaling our pictures to span the whole
    range of colors, so this is really a scaled version of the posterior. For simplicity,
    let’s refer to it as the posterior anyway.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是第3行左侧的图像。这是后验，或者说是将先验中的每个点（我们认为该直线有多可能）与新点的似然（每条直线与该数据点拟合的可能性）相乘的结果。我们没有展示贝叶斯法则中除以证据的步骤，因为我们正在缩放图片以覆盖整个颜色范围，所以这实际上是后验的一个缩放版本。为了简化，我们还是称之为后验。
- en: Notice that the posterior on line 3 is a new 2D distribution, represented by
    a new blob. To its right we see another 20 lines drawn at random from that distribution.
    We can see a big empty space near the top of the figure that wasn’t there in row
    1\. The system has learned from this one step of Bayes’ Rule that none of the
    lines that pass through that space are likely to match the data we’ve seen. The
    posterior from line 3 becomes our new prior for the next data point that comes
    along.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，第3行的后验是一个新的二维分布，由一个新的“云”表示。在其右侧，我们看到从该分布中随机绘制的另外20条直线。我们可以看到图像顶部的一个大空白区域，这在第1行中并不存在。系统从贝叶斯法则的这一单步学习到，没有任何穿过该空白区域的直线能够与我们看到的数据匹配。第3行的后验成为下一步数据点的新的先验。
- en: In row 4 we pick a new data point from the input, again shown in red. To its
    right is the likelihood for lines with respect to this point. In row 5 we apply
    Bayes’ Rule again and multiply our prior (the posterior from line 3) with the
    likelihood from row 4 to get a new posterior. Notice that the posterior has shrunk
    in size, telling us that the collection of lines that probably fit both points
    is smaller than the collection that fits just the first one. To the right, we
    show lines drawn from this distribution. Notice how much they’ve grouped together
    in the same general direction as the two points we just learned from.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在第4行，我们从输入中选择一个新的数据点，再次显示为红色。其右侧是该点对应的直线的似然性。在第5行，我们再次应用贝叶斯定理，并将我们的先验（来自第3行的后验）与第4行的似然性相乘，以得到新的后验。注意，后验的大小变小了，这告诉我们，适合两个点的直线集合比仅适合第一个点的集合要小。右侧显示了从这个分布中绘制的直线。注意它们如何聚集在与我们刚刚学习的两个点相似的方向上。
- en: We repeat the process again with a new point and likelihood in line 6, and a
    new posterior and set of lines in line 7\. The lines from this posterior are looking
    very similar, and the trend seems to be approaching a good fit to our data. By
    using more and more points, we get an increasingly limited range of probable lines.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次使用第6行中的新点和似然性进行处理，并在第7行得到新的后验和一组新的直线。来自该后验的直线非常相似，趋势似乎正趋向于与我们的数据良好拟合。通过使用越来越多的点，我们得到了一个越来越有限的可能直线范围。
- en: We can see from this example why Bayes’ Rule is so useful in training a learning
    system. Think of our training data as the points on the curve, and the evolving
    prior as the output from our system. As we provide the system with more samples
    (in this case, points), the system is able to fine-tune itself to deliver the
    response we’re looking for.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个例子中，我们可以看出贝叶斯定理在训练学习系统中的重要性。可以把我们的训练数据看作曲线上的点，把不断演变的先验看作系统的输出。当我们向系统提供更多的样本（在这种情况下是点）时，系统能够自我调整，以提供我们所期望的响应。
- en: It might be tempting to look at the lines in the bottom right of [Figure 9-21](#figure9-21)
    and apply our ideas of bias and variance to them, but that’s not thinking like
    a Bayesian. In the Bayesian framework, these aren’t a family of lines that approximate
    some true answer that we can discover by using various forms of averaging. Instead,
    a Bayesian sees all of these lines as accurate and correct, but with different
    probabilities. Computing the bias and variance of lines drawn from this collection
    is possible, but not meaningful in a Bayesian sense.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 看着[图9-21](#figure9-21)右下角的这些直线，可能会让人想将偏差和方差的概念应用于它们，但那并不是贝叶斯思维方式。在贝叶斯框架中，这些不是一组近似某个真实答案的直线，我们也不能通过使用各种形式的平均来发现这个真实答案。相反，贝叶斯认为所有这些直线都是准确和正确的，只是具有不同的概率。计算从这个集合中绘制出的直线的偏差和方差是可能的，但在贝叶斯意义上并没有实际意义。
- en: Both the frequentist and Bayesian approaches let us fit lines (or curves) to
    data. They just take very different attitudes and use different mechanisms, giving
    us two different ways to find good answers to our problem.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 频率派方法和贝叶斯方法都允许我们将直线（或曲线）拟合到数据上。它们采用了非常不同的态度和机制，给我们提供了两种不同的方式来找到问题的好答案。
- en: Summary
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we looked at a few ways in which a learning system can fail
    to generalize. When a learning system underperforms because the curves are not
    good fits to the data, we are underfitting. When a learning system underperforms
    on new data, but excels on the training data, we are overfitting: the system has
    learned too many of the quirks and idiosyncrasies of the training data. We saw
    how we can prevent overfitting by watching training and validation performance
    and using regularization methods. We ended the chapter by considering bias and
    variance’s relationship to overfitting, and seeing how we can fit a straight line
    to noisy data using Bayes’ Rule.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们探讨了学习系统无法进行良好泛化的几种方式。当学习系统由于曲线与数据不匹配而表现不佳时，我们称之为欠拟合。当学习系统在新数据上表现不佳，但在训练数据上表现出色时，我们称之为过拟合：系统已经学会了训练数据中的许多独特之处和特殊性。我们看到如何通过观察训练和验证的表现并使用正则化方法来防止过拟合。我们在本章的结尾讨论了偏差和方差与过拟合的关系，并通过贝叶斯定理看到了如何为噪声数据拟合一条直线。
- en: In the next chapter, we’ll look at data and how we can properly prepare it for
    our learning systems.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨数据以及如何为我们的学习系统正确地准备数据。
