- en: '**17'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**17'
- en: ENCODER- AND DECODER-STYLE TRANSFORMERS**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器和解码器型Transformer**
- en: '![Image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/common.jpg)'
- en: What are the differences between encoder-and decoder-based language transformers?
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器和解码器型语言Transformer之间有什么区别？
- en: Both encoder- and decoder-style architectures use the same self-attention layers
    to encode word tokens. The main difference is that encoders are designed to learn
    embeddings that can be used for various predictive modeling tasks such as classification.
    In contrast, decoders are designed to generate new texts, for example, to answer
    user queries.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器和解码器型架构都使用相同的自注意力层来编码单词标记。主要区别在于，编码器旨在学习可用于各种预测建模任务（例如分类）的嵌入。相比之下，解码器旨在生成新文本，例如回答用户查询。
- en: This chapter starts by describing the original transformer architecture consisting
    of an encoder that processes input text and a decoder that produces translations.
    The subsequent sections then describe how models like BERT and RoBERTa utilize
    only the encoder to understand context and how the GPT architectures emphasize
    decoder-only mechanisms for text generation.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章首先描述了由编码器和解码器组成的原始Transformer架构，编码器处理输入文本，解码器生成翻译。接下来的部分描述了像BERT和RoBERTa这样的模型如何仅使用编码器来理解上下文，以及GPT架构如何强调仅使用解码器机制进行文本生成。
- en: '**The Original Transformer**'
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**原始Transformer**'
- en: The original transformer architecture introduced in [Chapter 16](ch16.xhtml)
    was developed for English-to-French and English-to-German language translation.
    It utilized both an encoder and a decoder, as illustrated in [Figure 17-1](ch17.xhtml#ch17fig1).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第16章](ch16.xhtml)中介绍的原始Transformer架构是为英语到法语和英语到德语的语言翻译而开发的。它利用了编码器和解码器，如[图17-1](ch17.xhtml#ch17fig1)所示。
- en: '![Image](../images/17fig01.jpg)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/17fig01.jpg)'
- en: '*Figure 17-1: The original transformer architecture*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*图17-1：原始的Transformer架构*'
- en: In [Figure 17-1](ch17.xhtml#ch17fig1), the input text (that is, the sentences
    of the text to be translated) is first tokenized into individual word tokens,
    which are then encoded via an embedding layer before they enter the encoder part
    (see [Chapter 1](ch01.xhtml) for more on embeddings). After a positional encoding
    vector is added to each embedded word, the embeddings go through a multi-head
    self-attention layer. This layer is followed by an addition step, indicated by
    a plus sign (+) in [Figure 17-1](ch17.xhtml#ch17fig1), which performs a layer
    normalization and adds the original embeddings via a skip connection, also known
    as a *residual* or *shortcut* connection. Following this is a LayerNorm block,
    short for *layer normalization*, which normalizes the activations of the previous
    layer to improve the stability of the neural network’s training. The addition
    of the original embeddings and the layer normalization steps are often summarized
    as the *Add & Norm step*. Finally, after entering the fully connected network—a
    small, multilayer perceptron consisting of two fully connected layers with a nonlinear
    activation function in between—the outputs are again added and normalized before
    they are passed to a multi-head self-attention layer of the decoder.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图17-1](ch17.xhtml#ch17fig1)中，输入文本（即需要翻译的文本句子）首先被分词为单个单词标记，然后通过嵌入层进行编码，之后进入编码器部分（有关嵌入的更多信息，请参见[第1章](ch01.xhtml)）。在每个嵌入的单词上加上位置编码向量后，嵌入经过多头自注意力层。此层后面是一个加法步骤，如[图17-1](ch17.xhtml#ch17fig1)中所示，通过跳跃连接（也称为*残差*或*快捷*连接）执行层归一化并添加原始嵌入。接下来是一个LayerNorm模块，简称*层归一化*，它对前一层的激活进行归一化，以提高神经网络训练的稳定性。原始嵌入的加法和层归一化步骤通常被总结为*加法与归一化步骤*。最后，在进入全连接网络之后——一个由两个全连接层和一个非线性激活函数组成的小型多层感知机——输出再次被加法和归一化，然后传递到解码器的多头自注意力层。
- en: 'The decoder in [Figure 17-1](ch17.xhtml#ch17fig1) has a similar overall structure
    to the encoder. The key difference is that the inputs and outputs are different:
    the encoder receives the input text to be translated, while the decoder generates
    the translated text.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[图17-1](ch17.xhtml#ch17fig1)中的解码器与编码器的总体结构相似。关键区别在于输入和输出的不同：编码器接收待翻译的输入文本，而解码器生成翻译后的文本。'
- en: '***Encoders***'
  id: totrans-12
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***编码器***'
- en: The encoder part in the original transformer, as illustrated in [Figure 17-1](ch17.xhtml#ch17fig1),
    is responsible for understanding and extracting the relevant information from
    the input text. It then outputs a continuous representation (embedding) of the
    input text, which is passed to the decoder. Finally, the decoder generates the
    translated text (target language) based on the continuous representation received
    from the encoder.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 原始变换器中的编码器部分，如[图17-1](ch17.xhtml#ch17fig1)所示，负责理解和提取输入文本中的相关信息。然后，它输出输入文本的连续表示（嵌入），该表示被传递给解码器。最后，解码器基于从编码器接收到的连续表示生成翻译后的文本（目标语言）。
- en: Over the years, various encoder-only architectures have been developed based
    on the encoder module of the original transformer model outlined earlier. One
    notable example is BERT, which stands for bidirectional encoder representations
    from transformers.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，基于原始变换器模型的编码器模块，已开发出多种仅编码器架构。其中一个显著的例子是BERT，它代表了来自变换器的双向编码器表示。
- en: As noted in [Chapter 14](ch14.xhtml), BERT is an encoder-only architecture based
    on the transformer’s encoder module. The BERT model is pretrained on a large text
    corpus using masked language modeling and next-sentence prediction tasks. [Figure
    17-2](ch17.xhtml#ch17fig2) illustrates the masked language modeling pretraining
    objective used in BERT-style transformers.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第14章](ch14.xhtml)所述，BERT是基于变换器编码器模块的仅编码器架构。BERT模型通过掩蔽语言建模和下一句预测任务在大型文本语料库上进行预训练。[图17-2](ch17.xhtml#ch17fig2)展示了BERT风格变换器中使用的掩蔽语言建模预训练目标。
- en: '![Image](../images/17fig02.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/17fig02.jpg)'
- en: '*Figure 17-2: BERT randomly masks 15 percent of the input tokens during pretraining.*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*图17-2：BERT在预训练过程中随机掩蔽15%的输入标记。*'
- en: As [Figure 17-2](ch17.xhtml#ch17fig2) demonstrates, the main idea behind masked
    language modeling is to mask (or replace) random word tokens in the input sequence
    and then train the model to predict the original masked tokens based on the surrounding
    context.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图17-2](ch17.xhtml#ch17fig2)所示，掩蔽语言建模的主要思路是掩蔽（或替换）输入序列中的随机单词标记，然后训练模型根据周围的上下文预测原始被掩蔽的标记。
- en: 'In addition to the masked language modeling pretraining task illustrated in
    [Figure 17-2](ch17.xhtml#ch17fig2), the next-sentence prediction task asks the
    model to predict whether the original document’s sentence order of two randomly
    shuffled sentences is correct. For example, say that two sentences, in random
    order, are separated by the [SEP] token (*SEP* is short for *separate*). The brackets
    are a part of the token’s notation and are used to make it clear that this is
    a special token as opposed to a regular word in the text. BERT-style transformers
    also use a [CLS] token. The [CLS] token serves as a placeholder token for the
    model, prompting the model to return a *True* or *False* label indicating whether
    the sentences are in the correct order:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在[图17-2](ch17.xhtml#ch17fig2)中展示的掩蔽语言建模预训练任务外，下一句预测任务要求模型预测原文中两个随机打乱句子的顺序是否正确。例如，假设两个句子以随机顺序排列，并且它们之间由[SEP]标记分隔（*SEP*是*separate*的缩写）。方括号是该标记的符号的一部分，用于明确表示这是一个特殊标记，而不是文本中的普通单词。BERT风格的变换器还使用[CLS]标记。[CLS]标记充当模型的占位符，提示模型返回*True*或*False*标签，表示句子顺序是否正确：
- en: “[CLS] Toast is a simple yet delicious food. [SEP] It’s often served with butter,
    jam, or honey.”
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “[CLS] 吐司是一种简单而美味的食物。[SEP] 它通常与黄油、果酱或蜂蜜一起食用。”
- en: “[CLS] It’s often served with butter, jam, or honey. [SEP] Toast is a simple
    yet delicious food.”
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “[CLS] 它通常与黄油、果酱或蜂蜜一起食用。[SEP] 吐司是一种简单而美味的食物。”
- en: The masked language and next-sentence pretraining objectives allow BERT to learn
    rich contextual representations of the input texts, which can then be fine-tuned
    for various downstream tasks like sentiment analysis, question answering, and
    named entity recognition. It’s worth noting that this pretraining is a form of
    self-supervised learning (see [Chapter 2](ch02.xhtml) for more details on this
    type of learning).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 掩蔽语言和下一句预训练目标使BERT能够学习输入文本的丰富上下文表示，这些表示可以进一步微调以用于各种下游任务，如情感分析、问答和命名实体识别。值得注意的是，这种预训练是一种自监督学习（有关此类学习的更多细节，请参见[第2章](ch02.xhtml)）。
- en: RoBERTa, which stands for robustly optimized BERT approach, is an improved version
    of BERT. It maintains the same overall architecture as BERT but employs several
    training and optimization improvements, such as larger batch sizes, more training
    data, and eliminating the next-sentence prediction task. These changes have resulted
    in RoBERTa achieving better performance on various natural language understanding
    tasks than BERT.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: RoBERTa（即稳健优化的 BERT 方法）是 BERT 的改进版。它保持与 BERT 相同的总体架构，但采用了若干训练和优化改进措施，如更大的批量大小、更多的训练数据，并去除了下一句预测任务。这些变化使得
    RoBERTa 在各种自然语言理解任务中超过了 BERT，取得了更好的表现。
- en: '***Decoders***'
  id: totrans-24
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***解码器***'
- en: Coming back to the original transformer architecture outlined in [Figure 17-1](ch17.xhtml#ch17fig1),
    the multi-head self-attention mechanism in the decoder is similar to the one in
    the encoder, but it is masked to prevent the model from attending to future positions,
    ensuring that the predictions for position *i* can depend only on the known outputs
    at positions less than *i*. As illustrated in [Figure 17-3](ch17.xhtml#ch17fig3),
    the decoder generates the output word by word.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 回到[图 17-1](ch17.xhtml#ch17fig1)中概述的原始 Transformer 架构，解码器中的多头自注意力机制与编码器中的类似，但它被掩蔽，以防止模型关注未来的位置，从而确保对位置
    *i* 的预测只能依赖于位置小于 *i* 的已知输出。正如[图 17-3](ch17.xhtml#ch17fig3)所示，解码器逐字生成输出词。
- en: '![Image](../images/17fig03.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/17fig03.jpg)'
- en: '*Figure 17-3: The next-sentence prediction task used in the original transformer*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 17-3：原始 Transformer 中使用的下一句预测任务*'
- en: This masking (shown explicitly in [Figure 17-3](ch17.xhtml#ch17fig3), although
    it occurs internally in the decoder’s multi-head self-attention mechanism) is
    essential to maintaining the transformer model’s autoregressive property during
    training and inference. This autoregressive property ensures that the model generates
    output tokens one at a time and uses previously generated tokens as context for
    generating the next word token.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这种掩蔽（在[图 17-3](ch17.xhtml#ch17fig3)中明确显示，尽管它在解码器的多头自注意力机制中是内部发生的）对于在训练和推理过程中保持
    Transformer 模型的自回归特性至关重要。这个自回归特性确保模型一次生成一个输出标记，并将之前生成的标记作为上下文来生成下一个单词标记。
- en: Over the years, researchers have built upon the original encoder-decoder transformer
    architecture and developed several decoder-only models that have proven highly
    effective in various natural language processing tasks. The most notable models
    include the GPT family, which we briefly discussed in [Chapter 14](ch14.xhtml)
    and in various other chapters throughout the book.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，研究人员在原始的编码器-解码器 Transformer 架构的基础上，发展了几种仅包含解码器的模型，这些模型在各种自然语言处理任务中已被证明非常有效。最著名的模型包括
    GPT 系列，我们在[第14章](ch14.xhtml)以及书中其他章节中简要讨论过这些模型。
- en: '*GPT* stands for *generative pretrained transformer*. The GPT series comprises
    decoder-only models pretrained on large-scale unsupervised text data and fine-tuned
    for specific tasks such as text classification, sentiment analysis, question answering,
    and summarization. The GPT models, including at the time of writing GPT-2, GPT-3,
    and GPT-4, have shown remarkable performance in various benchmarks and are currently
    the most popular architecture for natural language processing.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*GPT* 代表 *生成式预训练变换器*。GPT 系列由仅包含解码器的模型组成，这些模型在大规模无监督文本数据上进行预训练，并针对特定任务（如文本分类、情感分析、问答和摘要）进行微调。GPT
    模型，包括撰写时的 GPT-2、GPT-3 和 GPT-4，在各种基准测试中表现出色，目前是自然语言处理领域最流行的架构。'
- en: One of the most notable aspects of GPT models is their emergent properties.
    Emergent properties are the abilities and skills that a model develops due to
    its next-word prediction pretraining. Even though these models were taught only
    to predict the next word, the pretrained models are capable of text summarization,
    translation, question answering, classification, and more. Furthermore, these
    models can perform new tasks without updating the model parameters via in-context
    learning, which we’ll discuss in more detail in [Chapter 18](ch18.xhtml).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: GPT 模型最显著的特点之一是其涌现特性。涌现特性是指模型由于下一词预测的预训练而发展出的能力和技能。尽管这些模型只被训练预测下一个词，经过预训练的模型却能够进行文本摘要、翻译、问答、分类等任务。此外，这些模型可以通过上下文学习在不更新模型参数的情况下执行新的任务，我们将在[第18章](ch18.xhtml)中更详细地讨论这一点。
- en: '**Encoder-Decoder Hybrids**'
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**编码器-解码器混合体**'
- en: Next to the traditional encoder and decoder architectures, there have been advancements
    in the development of new encoder-decoder models that leverage the strengths of
    both components. These models often incorporate novel techniques, pretraining
    objectives, or architectural modifications to enhance their performance in various
    natural language processing tasks. Some notable examples of these new encoder-decoder
    models include BART and T5.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 除了传统的编码器和解码器架构外，新的编码器-解码器模型的开发也取得了进展，这些模型结合了两个组件的优势。这些模型通常采用新颖的技术、预训练目标或架构修改，以提升其在各种自然语言处理任务中的表现。一些值得注意的新型编码器-解码器模型包括BART和T5。
- en: Encoder-decoder models are typically used for natural language processing tasks
    that involve understanding input sequences and generating output sequences, often
    with different lengths and structures. They are particularly good at tasks where
    there is a complex mapping between the input and output sequences and where it
    is crucial to capture the relationships between the elements in both sequences.
    Some common use cases for encoder-decoder models include text translation and
    summarization.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器-解码器模型通常用于涉及理解输入序列和生成输出序列的自然语言处理任务，这些任务的输出序列往往具有不同的长度和结构。这些模型特别适用于输入与输出序列之间存在复杂映射关系的任务，尤其是在需要捕捉两个序列中元素之间关系的情况下。编码器-解码器模型的常见应用场景包括文本翻译和摘要生成。
- en: '**Terminology**'
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**术语**'
- en: All of these methods—encoder-only, decoder-only, and encoder-decoder models—are
    sequence-to-sequence models (often abbreviated as *seq2seq*). While we refer to
    BERT-style methods as “encoder-only,” the description may be misleading since
    these methods also *decode* the embeddings into output tokens or text during pretraining.
    In other words, both encoder-only and decoder-only architectures perform decoding.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些方法——仅编码器、仅解码器和编码器-解码器模型——都是序列到序列模型（通常缩写为*seq2seq*）。虽然我们称BERT风格的方法为“仅编码器”，但这一描述可能会误导，因为这些方法在预训练过程中也会将嵌入解码为输出令牌或文本。换句话说，编码器-解码器架构和仅解码器架构都进行解码。
- en: However, the encoder-only architectures, in contrast to decoder-only and encoder-decoder
    architectures, don’t decode in an autoregressive fashion. *Autoregressive decoding*
    refers to generating output sequences one token at a time, conditioning each token
    on the previously generated tokens. Encoder-only models do not generate coherent
    output sequences in this manner. Instead, they focus on understanding the input
    text and producing task-specific outputs, such as labels or token predictions.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与仅解码器和编码器-解码器架构不同，编码器-仅架构并不以自回归的方式进行解码。*自回归解码*指的是一次生成一个输出令牌，并根据先前生成的令牌来调整每个令牌的生成。仅编码器模型并不像这样生成连贯的输出序列。相反，它们专注于理解输入文本并产生任务特定的输出，如标签或令牌预测。
- en: '**Contemporary Transformer Models**'
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**现代Transformer模型**'
- en: In brief, encoder-style models are popular for learning embeddings used in classification
    tasks, encoder-decoder models are used in generative tasks where the output heavily
    relies on the input (for example, translation and summarization), and decoder-only
    models are used for other types of generative tasks, including Q&A. Since the
    first transformer architecture emerged, hundreds of encoder-only, decoder-only,
    and encoder-decoder hybrids have been developed, as diagrammed in [Figure 17-4](ch17.xhtml#ch17fig4).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，编码器风格的模型通常用于学习用于分类任务的嵌入，编码器-解码器模型用于生成性任务，其中输出严重依赖输入（例如翻译和摘要生成），而仅解码器模型则用于其他类型的生成性任务，包括问答。自从第一个Transformer架构问世以来，已经开发出了数百种编码器-解码器、仅编码器、仅解码器混合型模型，如[图17-4](ch17.xhtml#ch17fig4)所示。
- en: '![Image](../images/17fig04.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/17fig04.jpg)'
- en: '*Figure 17-4: Some of the most popular large language transformers organized
    by architecture type and developer*'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*图17-4：按架构类型和开发者分类的一些最受欢迎的大型语言Transformer模型*'
- en: While encoder-only models have gradually become less popular, decoder-only models
    like GPT have exploded in popularity, thanks to breakthroughs in text generation
    via GPT-3, ChatGPT, and GPT-4\. However, encoder-only models are still very useful
    for training predictive models based on text embeddings as opposed to generating
    texts.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然仅编码器模型逐渐变得不那么流行，但像GPT这样的仅解码器模型却因通过GPT-3、ChatGPT和GPT-4在文本生成方面的突破而大爆发。然而，仅编码器模型在训练基于文本嵌入的预测模型时仍然非常有用，而不是用于文本生成。
- en: '**Exercises**'
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**练习**'
- en: '**17-1.** As discussed in this chapter, BERT-style encoder models are pretrained
    using masked language modeling and next-sentence prediction pretraining objectives.
    How could we adopt such a pretrained model for a classification task (for example,
    predicting whether a text has a positive or negative sentiment)?'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**17-1.** 如本章所述，BERT风格的编码器模型通过掩蔽语言模型和下一句预测预训练目标进行预训练。我们如何将这样一个预训练模型应用于分类任务（例如，预测文本是正面还是负面情感）？'
- en: '**17-2.** Can we fine-tune a decoder-only model like GPT for classification?'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**17-2.** 我们能否对仅解码器模型，如GPT，进行微调以用于分类？'
- en: '**References**'
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: 'The Bahdanau attention mechanism for RNNs: Dzmitry Bahdanau, Kyunghyun Cho,
    and Yoshua Bengio, “Neural Machine Translation by Jointly Learning to Align and
    Translate” (2014), *[https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473)*.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bahdanau注意力机制用于RNN：Dzmitry Bahdanau、Kyunghyun Cho和Yoshua Bengio，“通过联合学习对齐和翻译的神经机器翻译”（2014年），*[https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473)*。
- en: 'The original BERT paper, which popularized encoder-style transformers with
    a masked word and a next-sentence prediction pre-training objective: Jacob Devlin
    et al., “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”
    (2018), *[https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)*.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最初的BERT论文，它通过掩蔽词和下一句预测预训练目标推广了编码器风格的变换器：Jacob Devlin等，“BERT：用于语言理解的深度双向变换器预训练”（2018年），*[https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)*。
- en: 'RoBERTa improves upon BERT by optimizing training procedures, using larger
    training datasets, and removing the next-sentence prediction task: Yinhan Liu
    et al., “RoBERTa: A Robustly Optimized BERT Pretraining Approach” (2019), *[https://arxiv.org/abs/1907.11692](https://arxiv.org/abs/1907.11692)*.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RoBERTa通过优化训练程序、使用更大的训练数据集并移除下一句预测任务对BERT进行了改进：Yinhan Liu等，“RoBERTa：一种强健优化的BERT预训练方法”（2019年），*[https://arxiv.org/abs/1907.11692](https://arxiv.org/abs/1907.11692)*。
- en: 'The BART encoder-decoder architecture: Mike Lewis et al., “BART: Denoising
    Sequence-to-Sequence Pre-training for Natural Language Generation, Translation,
    and Comprehension” (2018), *[https://arxiv.org/abs/1910.13461](https://arxiv.org/abs/1910.13461)*.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BART编码器-解码器架构：Mike Lewis等，“BART：用于自然语言生成、翻译和理解的去噪序列到序列预训练”（2018年），*[https://arxiv.org/abs/1910.13461](https://arxiv.org/abs/1910.13461)*。
- en: 'The T5 encoder-decoder architecture: Colin Raffel et al., “Exploring the Limits
    of Transfer Learning with a Unified Text-to-Text Transformer” (2019), *[https://arxiv.org/abs/1910.10683](https://arxiv.org/abs/1910.10683)*.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: T5编码器-解码器架构：Colin Raffel等，“通过统一的文本到文本变换器探索迁移学习的极限”（2019年），*[https://arxiv.org/abs/1910.10683](https://arxiv.org/abs/1910.10683)*。
- en: 'The paper proposing the first GPT architecture: Alec Radford et al., “Improving
    Language Understanding by Generative Pre-Training” (2018), *[https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)*.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提出首个GPT架构的论文：Alec Radford等，“通过生成预训练提高语言理解”（2018年），*[https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)*。
- en: 'The GPT-2 model: Alec Radford et al., “Language Models Are Unsupervised Multitask
    Learners” (2019), *[https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe](https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe)*.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-2模型：Alec Radford等，“语言模型是无监督多任务学习者”（2019年），*[https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe](https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe)*。
- en: 'The GPT-3 model: Tom B. Brown et al., “Language Models Are Few-Shot Learners”
    (2020), *[https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)*.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-3模型：Tom B. Brown等，“语言模型是少量示例学习者”（2020年），*[https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)*。
