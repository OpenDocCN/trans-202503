- en: '3'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '3'
- en: RESOURCE LIMITING
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 资源限制
- en: '![image](../images/common01.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/common01.jpg)'
- en: The process isolation work we did in [Chapter 2](ch02.xhtml#ch02) was very important,
    as a process cannot generally affect what it cannot “see.” However, our process
    can see the host’s CPU, memory, and networking, so it is possible for a process
    to prevent other processes from running correctly by using too much of these resources,
    not leaving enough room for others. In this chapter, we will see how to guarantee
    that a process uses only its allocated CPU, memory, and network resources, ensuring
    that we can divide up our resources accurately. This will help when we move on
    to container orchestration because it will provide Kubernetes with certainty about
    the resources available on each host when it schedules a container.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第二章](ch02.xhtml#ch02)中做的进程隔离工作非常重要，因为一个进程通常无法影响它看不见的东西。然而，我们的进程可以看到主机的 CPU、内存和网络，因此，进程有可能通过过度使用这些资源，导致其他进程无法正确运行，无法为其他进程留下足够的空间。在本章中，我们将看到如何保证进程仅使用其分配的
    CPU、内存和网络资源，从而确保我们可以准确划分资源。这将在我们进行容器编排时有所帮助，因为它将为 Kubernetes 提供有关每个主机可用资源的确定性，从而在调度容器时进行决策。
- en: 'CPU, memory, and network are important, but there’s one more really important
    shared resource: storage. However, in a container orchestration environment like
    Kubernetes, storage is distributed, and limits need to be applied at the level
    of the whole cluster. For this reason, our discussion of storage must wait until
    we introduce distributed storage in [Chapter 15](ch15.xhtml#ch15).'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: CPU、内存和网络是重要的，但还有一个非常重要的共享资源：存储。然而，在像 Kubernetes 这样的容器编排环境中，存储是分布式的，限制需要在整个集群层面应用。因此，我们对存储的讨论必须等到我们在[第十五章](ch15.xhtml#ch15)引入分布式存储时才开始。
- en: CPU Priorities
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CPU 优先级
- en: 'We’ll need to look at CPU, memory, and network separately, as the effect of
    applying limits is different in each case. Let’s begin by looking at how to control
    CPU usage. To understand CPU limits, we first need to look at how the Linux kernel
    decides which process to run and for how long. In the Linux kernel, the *scheduler*
    keeps a list of all of the processes. It also tracks which processes are ready
    to run and how much time each process has received lately. This allows it to create
    a prioritized list so that it can choose the process that will run next. The scheduler
    is designed to be as fair as possible (it’s even known as the Completely Fair
    Scheduler); thus, it tries to give all processes a chance to run. However, it
    does accept outside input on which of these processes are more important than
    others. This prioritization is made up of two parts: the scheduling policy, and
    the priority of each process within that policy.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要分别查看 CPU、内存和网络，因为应用限制的效果在每种情况下不同。让我们首先看看如何控制 CPU 使用。为了理解 CPU 限制，我们首先需要了解
    Linux 内核是如何决定运行哪个进程以及运行多长时间的。在 Linux 内核中，*调度器*会维护一个所有进程的列表。它还会追踪哪些进程准备好运行，以及每个进程最近运行了多长时间。这使得它能够创建一个优先级列表，从而选择下一个要运行的进程。调度器的设计尽量公平（它被称为完全公平调度器）；因此，它会尽力给所有进程提供运行的机会。然而，它也接受外部输入，决定哪些进程比其他进程更为重要。这个优先级划分由两个部分组成：调度策略，以及在该策略下每个进程的优先级。
- en: Real-Time and Non-Real-Time Policies
  id: totrans-7
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实时和非实时策略
- en: The scheduler supports several different policies, but for our purposes we can
    group them into real-time policies and non-real-time policies. The term *real-time*
    means that some real-world event is critical to the process that creates a deadline.
    The process needs to complete its processing before this deadline expires, or
    something bad will happen. For example, the process might be collecting data from
    an embedded hardware device. In that case, the process must read the data before
    the hardware buffer overflows. A real-time process is typically not extremely
    CPU intensive, but when it needs the CPU, it cannot wait, so all processes under
    a real-time policy are higher priority than any process under a non-real-time
    policy. Let’s explore this on an example Linux system.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 调度器支持几种不同的策略，但就我们的目的而言，我们可以将它们分为实时策略和非实时策略。术语*实时*意味着某些现实世界的事件对进程至关重要，并且需要在特定的最后期限前完成处理。如果进程在最后期限过后还没有完成处理，就会发生不良后果。例如，进程可能在从嵌入式硬件设备收集数据。在这种情况下，进程必须在硬件缓冲区溢出之前读取数据。实时进程通常不会非常占用
    CPU，但当它需要 CPU 时，不能等待，因此所有处于实时策略下的进程都比任何处于非实时策略下的进程优先级更高。让我们通过一个示例 Linux 系统来探讨这个问题。
- en: '**NOTE**'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*The example repository for this book is at* [https://github.com/book-of-kubernetes/examples](https://github.com/book-of-kubernetes/examples).
    *See “Running Examples” on [page xx](ch00.xhtml#ch00lev1sec2) for details on getting
    set up.*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*本书的示例仓库位于* [https://github.com/book-of-kubernetes/examples](https://github.com/book-of-kubernetes/examples)。
    *有关设置的详细信息，请参见[第 xx 页](ch00.xhtml#ch00lev1sec2)的“运行示例”部分。*'
- en: 'The Linux `ps` command tells us the specific policy that applies to each process.
    Run this command on *host01* from this chapter’s examples:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Linux的`ps`命令告诉我们每个进程适用的具体策略。在*host01*上运行此命令，以查看本章示例：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `-o` flag provides `ps` with a custom list of output fields, including
    the scheduling policy *class* (`CLS`) and two numeric priority fields: `RTPRIO`
    and `NI`.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '`-o`标志为`ps`提供了一个自定义的输出字段列表，包括调度策略*class*（`CLS`）和两个数字优先级字段：`RTPRIO`和`NI`。'
- en: Looking at the `CLS` field first, lots of processes are listed as `TS`, which
    stands for “time-sharing” and is the default non-real-time policy. This includes
    commands we run ourselves (like the `ps` command we ran) as well as important
    Linux system processes like `systemd`. However, we also see processes with policy
    `FF` for first in–first out (FIFO) and policy `RR` for round-robin. These are
    real-time processes, and as such, they have priority over all non-real-time policies
    in the system. Real-time processes in the list include `watchdog`, which detects
    system lockups and thus might need to preempt other processes, and `multipathd`,
    which watches for device changes and must be able to configure those devices before
    other processes get a chance to talk to them.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 首先查看`CLS`字段，许多进程列出为`TS`，表示“时间共享”，这是默认的非实时策略。这包括我们自己运行的命令（例如我们运行的`ps`命令）以及重要的
    Linux 系统进程，如`systemd`。然而，我们也看到具有`FF`策略（先进先出，FIFO）和`RR`策略（轮转）的进程。这些是实时进程，因此它们的优先级高于系统中所有非实时策略的进程。列表中的实时进程包括`watchdog`（用于检测系统死锁，因此可能需要抢占其他进程）和`multipathd`（用于监视设备更改，并且必须在其他进程有机会与设备通信之前配置设备）。
- en: In addition to the class, the two numeric priority fields tell us how processes
    are prioritized within the policy. Not surprisingly, the `RTPRIO` field means
    “real-time priority” and applies only to real-time processes. The `NI` field is
    the “nice” level of the process and applies only to non-real-time processes. For
    historical reasons, the nice level runs from –20 (least nice, or highest priority)
    to 19 (nicest, lowest priority).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 除了类之外，两个数字优先级字段还告诉我们进程在策略中的优先级。不出所料，`RTPRIO`字段表示“实时优先级”，仅适用于实时进程。`NI`字段是进程的“nice”级别，仅适用于非实时进程。由于历史原因，nice级别从-20（最不友好，或最高优先级）到19（最友好，最低优先级）。
- en: Setting Process Priorities
  id: totrans-16
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 设置进程优先级
- en: Linux allows us to set the priority for processes we start. Let’s try to use
    priorities to control CPU usage. We’ll run a program called `stress` that is designed
    to exercise our system. Let’s use a containerized version of `stress` using CRI-O.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Linux 允许我们为启动的进程设置优先级。我们来尝试通过优先级控制 CPU 使用。我们将运行一个名为`stress`的程序，它旨在对我们的系统进行压力测试。我们将使用基于
    CRI-O 的容器化版本的`stress`。
- en: 'As before, we need to define YAML files for the Pod and container to tell `crictl`
    what to run. The Pod YAML shown in [Listing 3-1](ch03.xhtml#ch03list1) is almost
    the same as the BusyBox example in [Chapter 2](ch02.xhtml#ch02); only the name
    is different:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如之前所述，我们需要为 Pod 和容器定义 YAML 文件，以告诉`crictl`该运行什么。[清单 3-1](ch03.xhtml#ch03list1)中显示的
    Pod YAML 与[第 2 章](ch02.xhtml#ch02)中的 BusyBox 示例几乎相同，唯一不同的是名称：
- en: '*po-nolim.yaml*'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*po-nolim.yaml*'
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '*Listing 3-1: BusyBox Pod*'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 3-1：BusyBox Pod*'
- en: 'The container YAML has more changes compared to the BusyBox example. In addition
    to using a different container image, one that already has `stress` installed,
    we also need to provide arguments to `stress` to tell it to exercise a single
    CPU:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 容器的 YAML 相比于 BusyBox 示例有更多的更改。除了使用不同的容器镜像，即已经安装了`stress`的镜像外，我们还需要向`stress`提供参数，告诉它只使用一个
    CPU：
- en: '*co-nolim.yaml*'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*co-nolim.yaml*'
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'CRI-O is already installed on `host01`, so it just takes a few commands to
    start this container. First, we’ll pull the image:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '`host01`上已安装CRI-O，因此只需几条命令即可启动这个容器。首先，我们将拉取镜像：'
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then, we can run a container from the image:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以从镜像运行一个容器：
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `crictl ps` command is just to check that our container is running as expected.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '`crictl ps`命令只是用来检查我们的容器是否按预期运行。'
- en: 'The `stress` program is now running on our system, and we can see the current
    priority and CPU usage. We want the current CPU usage, so we’ll use `top`:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`stress`程序已在我们的系统上运行，我们可以查看当前的优先级和 CPU 使用情况。我们想查看当前的 CPU 使用情况，因此我们将使用`top`：
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `pgrep` command looks up the process IDs (PIDs) for `stress`; there are
    two because `stress` forked a separate process for the CPU exercise we requested.
    This CPU worker is using up 100 percent of one CPU; fortunately, our VM has two
    CPUs, so it’s not overloaded.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`pgrep`命令查找了`stress`的进程ID（PID）；有两个PID，因为`stress`为我们请求的CPU负载操作创建了一个独立的进程。这个CPU工作进程占用了一个CPU的100%；幸运的是，我们的虚拟机有两个CPU，所以它并没有超载。'
- en: 'We started this process with default priority, so it has a nice value of `0`,
    as shown in the `NI` column. What happens if we change that priority? Let’s find
    out using `renice`:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以默认优先级启动了这个进程，因此它的nice值为`0`，如`NI`列所示。如果我们改变这个优先级会发生什么呢？让我们使用`renice`来找出答案：
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `ps` command used previously expected the PIDs to be separated with a comma,
    whereas the `renice` command expects the PIDs to be separated with a space; fortunately,
    `pgrep` can handle both.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 之前使用的`ps`命令期望PID通过逗号分隔，而`renice`命令期望PID通过空格分隔；幸运的是，`pgrep`可以同时处理这两种情况。
- en: 'We have successfully changed the priority of the process:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经成功地改变了进程的优先级：
- en: '[PRE7]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The new nice value is `19`, meaning that our process is lower priority than
    before. However, the `stress` program is still using 100 percent of one CPU! What’s
    going on here? The problem is that priority is only a relative measurement. If
    nothing else needs the CPU, as is true in this case, even a lower-priority process
    can use as much as it wants.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 新的nice值是`19`，意味着我们的进程比之前的优先级低。然而，`stress`程序仍然占用了一个CPU的100%！这是怎么回事？问题在于优先级只是一个相对的度量。如果没有其他程序需要CPU（在这种情况下是这样），即使是低优先级的进程也可以尽可能多地使用CPU。
- en: This arrangement may seem to be what we want. After all, if the CPU is available,
    shouldn’t we want our application components to be able to use it? Unfortunately,
    even though that sounds reasonable, it’s not suitable for our containerized applications
    for two main reasons. First, a container orchestration environment like Kubernetes
    works best when a container can be allocated to any host with enough resources
    to run it. It’s not reasonable for us to know the relative priority of every single
    container in our Kubernetes cluster, especially when we consider that a single
    Kubernetes cluster can be *multitenant*, meaning multiple separate applications
    or teams might be using a single cluster. Second, without some idea of how much
    CPU a particular container will use, Kubernetes cannot know which hosts are full
    and which ones have more room available. We don’t want to get into a situation
    in which multiple containers on the same host all become busy at the same time,
    because they will fight for the available CPU cores, and the whole host will slow
    down.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这种安排可能看起来是我们想要的。毕竟，如果CPU可用，我们是不是希望我们的应用组件能够使用它？不幸的是，尽管这听起来很合理，但由于两个主要原因，它并不适合我们的容器化应用。首先，像Kubernetes这样的容器编排环境在容器能够被分配到任何具有足够资源的主机上时效果最佳。我们不可能了解Kubernetes集群中每个容器的相对优先级，特别是当我们考虑到一个Kubernetes集群可能是*多租户*的，即多个独立的应用或团队可能在同一个集群中使用时。第二，Kubernetes如果没有某个容器将使用多少CPU的概念，就无法知道哪些主机已经满载，哪些主机还有空余空间。如果多个容器在同一台主机上同时变得繁忙，它们将争夺可用的CPU核心，整个主机将变慢，这是我们不希望发生的情况。
- en: Linux Control Groups
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Linux 控制组
- en: As we saw in the last section, process prioritization will not help a container
    orchestration environment like Kubernetes know what host to use when scheduling
    a new container, because even low-priority processes can get a lot of CPU time
    when the CPU is idle. And because our Kubernetes cluster might be multitenant,
    the cluster can’t just trust each container to promise to use only a certain amount
    of CPU. First, that would allow one process to affect another negatively, either
    maliciously or accidentally. Second, processes don’t really control their own
    scheduling; they get CPU time when the Linux kernel decides to give them CPU time.
    We need a different solution for controlling CPU utilization.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一节看到的，进程优先级调整不会帮助像Kubernetes这样的容器编排环境了解在调度新容器时应该使用哪个主机，因为即使是低优先级进程在CPU空闲时也能获得大量的CPU时间。而且由于我们的Kubernetes集群可能是多租户的，集群不能仅仅依赖每个容器承诺只使用一定量的CPU。首先，这样可能会导致一个进程负面影响到另一个进程，无论是恶意的还是意外的。其次，进程并不真正控制自己的调度；它们在Linux内核决定分配CPU时间时才获得CPU时间。我们需要一种不同的解决方案来控制CPU的使用。
- en: To find the answer, we can take an approach used by real-time processing. As
    we mentioned in the previous section, a real-time process is typically not compute
    intensive, but when it needs the CPU, it needs it immediately. To ensure that
    all real-time processes get the CPU they need, it is common to reserve a slice
    of the CPU time for each process. Even though our container processes are non-real-time,
    we can use the same strategy. If we can configure our containers so that they
    can use no more than their allocated slice of the CPU time, Kubernetes will be
    able to calculate how much space is available on each host and will be able to
    schedule containers onto hosts with sufficient space.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到答案，我们可以采用实时处理所使用的一种方法。正如我们在前一部分提到的，实时进程通常不需要大量计算，但当它需要 CPU 时，它需要立即获取。为了确保所有实时进程都能获得它们需要的
    CPU，通常会为每个进程保留一部分 CPU 时间。即使我们的容器进程不是实时的，我们也可以使用相同的策略。如果我们能配置容器，使其只能使用分配的 CPU 时间片，Kubernetes
    将能够计算每个主机上可用的空间，并能够将容器调度到有足够空间的主机上。
- en: To manage container use of CPU cores, we will use *control groups*. Control
    groups (cgroups) are a feature of the Linux kernel that manage process resource
    utilization. Each resource type, such as CPU, memory, or a block device, can have
    an entire hierarchy of cgroups associated with it. After a process is in a cgroup,
    the kernel automatically applies the controls from that group.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了管理容器对 CPU 核心的使用，我们将使用 *控制组*。控制组（cgroups）是 Linux 内核的一个特性，用于管理进程资源的使用。每种资源类型，如
    CPU、内存或块设备，都可以有一个与之关联的 cgroup 层级结构。进程进入 cgroup 后，内核会自动应用该组的控制。
- en: 'The creation and configuration of cgroups is handled through a specific kind
    of filesystem, similar to the way that Linux reports information on the system
    through the */proc* filesystem. By default, the filesystem for cgroups is located
    at */sys/fs/cgroup*:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: cgroup 的创建和配置是通过一种特定的文件系统处理的，类似于 Linux 通过 */proc* 文件系统报告系统信息的方式。默认情况下，cgroup
    的文件系统位于 */sys/fs/cgroup*：
- en: '[PRE8]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Each of the entries in */sys/fs/cgroup* is a different resource that can be
    limited. If we look in one of those directories, we can begin to see what controls
    can be applied. For example, for *cpu*:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*/sys/fs/cgroup* 中的每一项都是可以限制的不同资源。如果我们查看其中一个目录，我们可以开始看到可以应用的控制。例如，对于*cpu*：'
- en: '[PRE9]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The `-F` flag on `ls` adds a slash character to directories, which enables us
    to begin to see the hierarchy. Each of those subdirectories (*init.scope*, *system.slice*,
    and *user.slice*) is a separate CPU cgroup, and each has its own set of configuration
    files that apply to processes in that cgroup.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`ls` 命令上的 `-F` 标志会为目录添加斜杠字符，这使我们可以开始看到层级结构。每个子目录（*init.scope*、*system.slice*
    和 *user.slice*）都是一个独立的 CPU cgroup，每个都有一组适用于该 cgroup 中进程的配置文件。'
- en: CPU Quotas with cgroups
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 cgroups 的 CPU 配额
- en: 'To understand the contents of this directory, let’s see how we can use cgroups
    to limit the CPU usage of our `stress` container. We’ll begin by checking its
    CPU usage again:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这个目录的内容，让我们看看如何使用 cgroups 来限制 `stress` 容器的 CPU 使用情况。我们将重新检查它的 CPU 使用情况：
- en: '[PRE10]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'If you don’t still see `stress` running, start it up again using the commands
    from earlier in this chapter. Next, let’s explore what CPU cgroup our `stress`
    CPU process is in. We can do this by finding its PID inside a file within the
    */sys/fs/cgroup/cpu* hierarchy:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仍然没有看到 `stress` 正在运行，使用本章前面提到的命令重新启动它。接下来，让我们探索 `stress` CPU 进程所在的 CPU cgroup。我们可以通过在
    */sys/fs/cgroup/cpu* 层级中的文件内查找其 PID 来做到这一点：
- en: '[PRE11]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The `stress` process is part of the *system.slice* hierarchy, and is in a subdirectory
    created by `runc`, which is one of the internal components of CRI-O. This is really
    convenient, as it means we don’t need to create our own cgroup and move this process
    into it. It is also no accident; as we’ll see in a moment, CRI-O supports CPU
    limits on containers, so it naturally needs to create a cgroup for each container
    it runs. In fact, the cgroup is named after the container ID.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '`stress` 进程属于 *system.slice* 层级，并位于由 `runc` 创建的子目录中，`runc` 是 CRI-O 的内部组件之一。这非常方便，因为这意味着我们不需要创建自己的
    cgroup 并将此进程移入其中。这也不是偶然的；正如我们稍后将看到的，CRI-O 支持对容器设置 CPU 限制，因此它自然需要为每个运行的容器创建一个 cgroup。实际上，cgroup
    的名称是以容器 ID 命名的。'
- en: 'Let’s move into the directory for our container’s cgroup:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进入容器 cgroup 的目录：
- en: '[PRE12]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We use the container ID variable we saved earlier to change into the appropriate
    directory. As soon as we’re in this directory, we can see that it has the same
    configuration files as the root of the hierarchy */sys/fs/cgroup/cpu*:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用之前保存的容器ID变量进入适当的目录。一旦进入该目录，我们可以看到它具有与*/sys/fs/cgroup/cpu*根目录相同的配置文件：
- en: '[PRE13]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The *cgroup.procs* file lists the processes in this control group:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '*cgroup.procs*文件列出了这个控制组中的进程：'
- en: '[PRE14]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This directory has many other files, but we are mostly interested in three:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这个目录还有许多其他文件，但我们主要关心三个文件：
- en: '***cpu.shares*** Slice of the CPU relative to this cgroup’s peers'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '***cpu.shares*** 这个cgroup相对于同级cgroup所占的CPU份额'
- en: '***cpu.cfs_period_us*** Length of a period, in microseconds'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '***cpu.cfs_period_us*** 一个周期的长度，以微秒为单位'
- en: '***cpu.cfs_quota_us*** CPU time during a period, in microseconds'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '***cpu.cfs_quota_us*** 一个周期内的CPU时间，以微秒为单位'
- en: 'We’ll look at how Kubernetes uses *cpu.shares* in [Chapter 14](ch14.xhtml#ch14).
    For now, we need a way to get our instance under control so that it doesn’t overwhelm
    our system. To do that, we’ll set an absolute quota on this container. First,
    let’s see the value of *cpu.cfs_period_us*:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将查看Kubernetes如何在[第14章](ch14.xhtml#ch14)中使用*cpu.shares*。现在，我们需要一种方法来控制我们的实例，避免它对系统造成过载。为此，我们将为这个容器设置一个绝对配额。首先，让我们查看*cpu.cfs_period_us*的值：
- en: '[PRE15]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The period is set to 100,000 μs, or 0.1 seconds. We can use this number to
    figure out what quota to set in order to limit the amount of CPU the `stress`
    container can use. At the moment, there is no quota:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 该周期设置为100,000微秒，或者0.1秒。我们可以利用这个数字来计算应该设置什么样的配额，以限制`stress`容器能使用的CPU量。目前，没有设置配额：
- en: '[PRE16]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We can set a quota by just updating the *cpu.cfs_quota_us* file:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需更新*cpu.cfs_quota_us*文件即可设置配额：
- en: '[PRE17]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This provides the processes in this cgroup with 50,000 μs of CPU time per 100,000
    μs, which averages out to 50 percent of a CPU. The processes are immediately affected,
    as we can confirm:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这为该cgroup中的进程提供了每100,000微秒50,000微秒的CPU时间，平均分配为50%的CPU。进程会立即受到影响，正如我们可以确认的那样：
- en: '[PRE18]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Your listing might not show exactly 50 percent CPU usage, because the period
    during which the `top` command measures CPU usage might not align perfectly with
    the kernel’s scheduling period. But on average, our `stress` container now cannot
    use more than 50 percent of one CPU.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 你的清单可能不会显示出精确的50% CPU使用率，因为`top`命令测量CPU使用情况的周期可能与内核的调度周期不完全对齐。但平均来看，我们的`stress`容器现在无法使用超过50%的单个CPU。
- en: 'Before we move on, let’s stop the `stress` container:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们先停止`stress`容器：
- en: '[PRE19]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: CPU Quota with CRI-O and crictl
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用CRP-O和crictl设置CPU配额
- en: It would be tiresome to have to go through the process of finding the cgroup
    location in the filesystem and updating the CPU quota for every container in order
    to control CPU usage. Fortunately, we can specify the quota in our `crictl` YAML
    files, and CRI-O will enforce it for us. Let’s look at an example that was installed
    into */opt* when we set up this example virtual machine.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果每次都需要在文件系统中找到cgroup位置并更新每个容器的CPU配额来控制CPU使用，这将是一件繁琐的事情。幸运的是，我们可以在`crictl`的YAML文件中指定配额，CRI-O会为我们强制执行。让我们看看一个安装在*/opt*中的例子，当我们设置这个虚拟机时，配置也已安装。
- en: 'The Pod configuration is only slightly different from [Listing 3-1](ch03.xhtml#ch03list1).
    We add a `cgroup_parent` setting so that we can control where CRI-O creates the
    cgroup, which will make it easier to find the cgroup to see the configuration:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Pod配置与[清单3-1](ch03.xhtml#ch03list1)只有略微的不同。我们添加了`cgroup_parent`设置，这样可以控制CRI-O创建cgroup的位置，这将使我们更容易找到cgroup并查看其配置：
- en: '*po-clim.yaml*'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '*po-clim.yaml*'
- en: '[PRE20]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The container configuration is where we include the CPU limits. Our `stress1`
    container will be allotted only 10 percent of a CPU:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 容器配置是我们包含CPU限制的地方。我们的`stress1`容器将只分配10%的CPU：
- en: '*co-clim.yaml*'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*co-clim.yaml*'
- en: '[PRE21]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The value for `cpu_period` corresponds with the file *cpu.cfs_period_us* and
    provides the length of the period during which the quota applies. The value for
    `cpu_quota` corresponds with the file *cpu.cfs_quota_us*. Dividing the quota by
    the period, we can determine that this will set a CPU limit of 10 percent. Let’s
    go ahead and launch this `stress` container with its CPU limit:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`cpu_period`的值对应于文件*cpu.cfs_period_us*，并提供配额适用的周期长度。`cpu_quota`的值对应于文件*cpu.cfs_quota_us*。通过将配额除以周期，我们可以确定这将设置一个10%的CPU限制。现在，让我们启动这个带有CPU限制的`stress`容器：'
- en: '[PRE22]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Our container is immediately restricted to 10 percent of a CPU:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的容器立即被限制为10%的CPU使用：
- en: '[PRE23]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: As in our earlier example, the CPU usage shown is a snapshot during the time
    that `top` was running, so it might not match the limit exactly, but over the
    long term, this process will use no more than its allocated CPU.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如同我们之前的例子所示，显示的 CPU 使用率是在 `top` 运行期间的快照，因此可能不会完全匹配限制，但从长远来看，这个进程不会超过其分配的 CPU
    使用量。
- en: 'We can inspect the cgroup to confirm that CRI-O put it in the place we specified
    and automatically configured the CPU quota:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以检查 cgroup 来确认 CRI-O 是否将其放置在我们指定的位置，并自动配置了 CPU 配额：
- en: '[PRE24]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: CRI-O created a new cgroup parent *pod.slice* for our container, created a cgroup
    within it specific to the container, and configured its CPU quota without us having
    to lift a finger.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: CRI-O 为我们的容器创建了一个新的 cgroup 父级 *pod.slice*，在其中为容器创建了一个特定的 cgroup，并配置了它的 CPU 配额，而我们无需动手。
- en: 'We don’t need this container any longer, so let’s remove it:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不再需要这个容器了，所以让我们把它移除：
- en: '[PRE25]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: With these commands we stop and then delete first the container, then the Pod.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些命令，我们先停止容器，再删除容器，最后删除 Pod。
- en: Memory Limits
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 内存限制
- en: Memory is another important resource for a process. If a system doesn’t have
    sufficient memory to meet a request, the allocation of memory will fail. This
    usually causes the process to behave badly or to fail entirely. Of course, most
    Linux systems use *swap space* to write memory contents to disk temporarily, which
    allows the system memory to appear larger than it is but also reduces system performance.
    It’s a big enough concern that the Kubernetes team discourages having swap enabled
    in a cluster.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 内存是进程的另一个重要资源。如果系统没有足够的内存来满足请求，内存分配将失败。这通常会导致进程出现异常行为或完全失败。当然，大多数 Linux 系统使用
    *交换空间* 将内存内容暂时写入磁盘，这使得系统内存看起来比实际更大，但也会降低系统性能。这个问题足够重要，以至于 Kubernetes 团队不鼓励在集群中启用交换空间。
- en: Also, even if we could use swap, we don’t want one process grabbing all the
    resident memory and making other processes very slow. As a result, we need to
    limit the memory usage of our processes so that they cooperate with one another.
    We also need to have a clear maximum for memory usage so that Kubernetes can reliably
    ensure that a host has enough available memory before scheduling a new container
    onto a host.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，即使我们能够使用交换空间，我们也不希望某个进程占用所有常驻内存，从而使其他进程变得非常缓慢。因此，我们需要限制进程的内存使用，以便它们能够相互协作。我们还需要为内存使用设置一个明确的最大值，以便
    Kubernetes 可以可靠地确保主机在调度新容器到主机之前有足够的可用内存。
- en: 'Linux systems, like other variants of Unix, have traditionally had to deal
    with multiple users who are sharing scarce resources. For this reason, the kernel
    supports limits on system resources, including CPU, memory, number of child processes,
    and number of open files. We can set these limits from the command line using
    the `ulimit` command. For example, one type of limit is a limit on “virtual memory.”
    This includes not only the amount of RAM a process has in resident memory but
    also any swap space it is using. Here’s an example of a `ulimit` command limiting
    virtual memory:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Linux 系统与其他 Unix 变种一样，传统上需要处理多个共享稀缺资源的用户。因此，内核支持对系统资源的限制，包括 CPU、内存、子进程数量和打开文件数。我们可以通过命令行使用
    `ulimit` 命令来设置这些限制。例如，一种限制类型是“虚拟内存”限制。它不仅包括进程在常驻内存中使用的 RAM，还包括它使用的任何交换空间。以下是一个限制虚拟内存的
    `ulimit` 命令示例：
- en: '[PRE26]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The `-v` switch specifies a limit on virtual memory. The parameter is in bytes,
    so 262144 places a virtual memory limit of 256MiB on each additional process we
    start from this shell session. Setting a virtual memory limit is a total limit;
    it allows us to ensure that a process can’t use swap to get around the limit.
    We can verify the limit was applied by pulling some data into memory:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`-v` 开关指定了虚拟内存的限制。参数以字节为单位，因此 262144 将对我们从这个 shell 会话启动的每个附加进程设置一个 256MiB 的虚拟内存限制。设置虚拟内存限制是一个总的限制；它可以确保进程不能通过交换空间绕过限制。我们可以通过将一些数据加载到内存中来验证限制是否已应用：'
- en: '[PRE27]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This command reads from */dev/zero* and tries to keep the first 500MiB of zeros
    it finds in memory. However, at some point, when the `tail` command tries to allocate
    more space to hold the zeros it is getting from `head`, it fails because of the
    limit.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令从 */dev/zero* 中读取数据，并尝试将它找到的前 500MiB 零字节保持在内存中。然而，当 `tail` 命令尝试分配更多空间来存放从
    `head` 获取的零字节时，它因为达到限制而失败。
- en: Thus, Unix limits give us the ability to control memory usage for our processes,
    but they won’t provide everything we need for containers, for a couple of reasons.
    First, Unix limits can be applied only to individual processes or to an entire
    user. Neither of those provide what we need, as a container is really a *group*
    of processes. A container’s initial process might create many child processes,
    and all processes in a container need to live within the same limit. At the same
    time, applying limits to an entire user doesn’t really help us in a container
    orchestration environment like Kubernetes, because from the perspective of the
    operating system, all of the containers belong to the same user. Second, when
    it comes to CPU limits, the only thing that regular Unix limits can do is limit
    the maximum CPU time our process gets before it is terminated. That isn’t the
    kind of limit we need for sharing the CPU between long-running processes.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Unix 限制使我们能够控制进程的内存使用，但由于一些原因，它们无法提供容器所需的所有功能。首先，Unix 限制只能应用于单个进程或整个用户。这两者都不能满足我们的需求，因为容器实际上是一个*进程组*。容器的初始进程可能会创建许多子进程，并且容器中的所有进程都需要在相同的限制下运行。同时，将限制应用于整个用户并不能真正帮助我们在像
    Kubernetes 这样的容器编排环境中，因为从操作系统的角度来看，所有容器都属于同一个用户。其次，关于 CPU 限制，常规的 Unix 限制唯一能做的就是限制进程在被终止之前获得的最大
    CPU 时间。这不是我们在共享 CPU 给长时间运行的进程时所需要的限制类型。
- en: Instead of using traditional Unix limits, we’ll use cgroups again, this time
    to limit the memory available to a process. We’ll use the same `stress` container
    image, this time with a child process that tries to allocate lots of memory.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将不再使用传统的 Unix 限制，而是再次使用 cgroups，这次是为了限制进程可用的内存。我们将使用相同的 `stress` 容器镜像，这次包含一个尝试分配大量内存的子进程。
- en: 'If we were to try to apply a memory limit to this `stress` container after
    starting it, we would find that the kernel won’t let us, because it will have
    already grabbed too much memory. So instead we’ll apply it immediately in the
    YAML configuration. As before, we need a Pod:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在启动 `stress` 容器后尝试应用内存限制，我们会发现内核不允许这么做，因为它已经占用了过多内存。因此，我们将立即在 YAML 配置中应用它。和之前一样，我们需要一个
    Pod：
- en: '*po-mlim.yaml*'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '*po-mlim.yaml*'
- en: '[PRE28]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This is identical to the Pod we used for CPU limit, but the name is different
    to avoid a collision. As we did earlier, we are asking CRI-O to put the cgroup
    into *pod.slice* so that we can find it easily.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们用于 CPU 限制的 Pod 相同，但为了避免冲突，名称不同。就像我们之前做的那样，我们要求 CRI-O 将 cgroup 放入*pod.slice*，这样我们就可以轻松找到它。
- en: 'We also need a container definition:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要一个容器定义：
- en: '*co-mlim.yaml*'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '*co-mlim.yaml*'
- en: '[PRE29]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The new resource limit is `memory_limit_in_bytes`, which we set to 256MiB ➋.
    We keep the CPU quota in there ➌ because continuously trying to allocate memory
    is going to use a lot of CPU. Finally, in the `args` section, we tell `stress`
    to try to allocate 512MB of memory ➊.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 新的资源限制是 `memory_limit_in_bytes`，我们将其设置为 256MiB ➋。我们保持 CPU 配额 ➌，因为持续尝试分配内存将消耗大量
    CPU。最后，在 `args` 部分，我们告诉 `stress` 尝试分配 512MB 的内存 ➊。
- en: 'We can run this using similar `crictl` commands to what we’ve previously used:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用与之前相同的 `crictl` 命令运行它：
- en: '[PRE30]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'If we tell `crictl` to list containers, everything seems okay:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们告诉 `crictl` 列出容器，所有情况看起来都正常：
- en: '[PRE31]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This reports that the container is in a `Running` state. However, behind the
    scenes, `stress` is struggling to allocate memory. We can see this if we print
    out the log messages coming from the `stress` container:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明容器处于 `Running` 状态。然而，在背后，`stress` 正在努力分配内存。如果我们打印出来自 `stress` 容器的日志信息，我们就可以看到这一点：
- en: '[PRE32]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Stress is reporting that its memory allocation process is being continuously
    killed by the “out of memory.”
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Stress 报告说其内存分配进程正在被 “内存不足” 持续终止。
- en: 'And we can see the kernel reporting that the `oom_reaper` is indeed the reason
    that the processes are being killed:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到内核报告显示 `oom_reaper` 确实是导致进程被终止的原因：
- en: '[PRE33]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The `OOM killer` is the same feature Linux uses when the whole system is low
    on memory and it needs to kill one or more processes to protect the system. In
    this case, it is sending `SIGKILL` to the process to keep the cgroup under its
    memory limit. `SIGKILL` is a message to the process that it should immediately
    terminate without any cleanup.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '`OOM killer` 是 Linux 在整个系统内存不足时使用的功能，它需要终止一个或多个进程来保护系统。在这种情况下，它通过发送 `SIGKILL`
    信号终止进程，以确保 cgroup 在其内存限制下。`SIGKILL` 是一种信号，通知进程立即终止，且不进行任何清理。'
- en: '**WHY USE THE OOM KILLER?**'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么使用 OOM Killer？**'
- en: When we used regular limits to control memory, an attempt to exceed our limits
    caused the memory allocation to fail, but the kernel didn’t use the OOM killer
    to kill our process. Why the difference? The answer is that this is the nature
    of containers. As we look at architecting reliable systems using containerized
    microservices, we’ll see that a container is supposed to be quick to start and
    quick to scale. This means that each individual container in our application is
    intentionally just not very important. This further means that the idea that one
    of our containers could be killed unexpectedly is not really a concern. Add to
    that the fact that not checking for memory allocation errors is one of the most
    common bugs, so it’s considered safer simply to kill the process.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用常规限制来控制内存时，超出限制会导致内存分配失败，但内核不会使用 OOM 杀手来终止我们的进程。为什么会有这种差异？答案在于容器的本质。当我们设计使用容器化微服务的可靠系统时，我们会发现，容器应该是快速启动和快速扩展的。这意味着应用中的每个单独容器本质上并不太重要。这也意味着，一个容器可能会被意外终止，通常不会引发太大关注。再加上不检查内存分配错误是最常见的
    bug 之一，因此直接终止进程被认为是更安全的做法。
- en: That said, it’s worth noting that it is possible to turn off the OOM killer
    for a cgroup. However, rather than having the memory allocation fail, the effect
    is to just pause the process until other processes in the group free up memory.
    That’s actually worse, as now we have a process that isn’t officially killed but
    isn’t doing anything useful either.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，值得注意的是，确实可以为某个 cgroup 关闭 OOM 杀手。然而，与其让内存分配失败，效果是将进程暂停，直到该组中的其他进程释放内存。实际上，这样更糟，因为现在我们有一个既没有被正式终止，又没有执行任何有用操作的进程。
- en: 'Before we move on, let’s put this continuously failing `stress` container out
    of its misery:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们先把这个不断失败的`stress`容器解脱出来：
- en: '[PRE34]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Stopping and removing the container and Pod prevents the `stress` container
    from wasting CPU by continually trying to restart the memory allocation process.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 停止并移除容器和 Pod 可以防止`stress`容器浪费 CPU，不断尝试重启内存分配过程。
- en: Network Bandwidth Limits
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网络带宽限制
- en: In this chapter, we’ve moved from resources that are easy to limit to resources
    that are more difficult to limit. We started with CPU, where the kernel is wholly
    in charge of which process gets CPU time and how much time it gets before being
    preempted. Then we looked at memory, where the kernel doesn’t have the ability
    to force a process to give up memory, but at least the kernel can control whether
    a memory allocation is successful, or it can kill a process that requests too
    much memory.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们从易于限制的资源转向了更难限制的资源。我们从 CPU 开始，内核完全负责哪个进程获得 CPU 时间以及在被抢占之前能获得多少时间。接着我们看了内存，内核没有能力强制进程放弃内存，但至少内核可以控制内存分配是否成功，或者它可以终止请求过多内存的进程。
- en: Now we’re moving on to network bandwidth, for which control is even more difficult
    to exert for two important reasons. First, network devices don’t really “sum up”
    like CPU or memory, so we’ll need to limit usage at the level of each individual
    network device. Second, our system can’t really control what is sent to it across
    the network; we can only completely control *egress* bandwidth, the traffic that
    is sent on a given network device.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们开始讨论网络带宽，控制网络带宽比控制 CPU 或内存更为困难，原因有两个。首先，网络设备不像 CPU 或内存那样可以“合并”，因此我们需要在每个独立的网络设备层面上进行限制。其次，我们的系统实际上无法控制通过网络发送给它的数据；我们只能完全控制*出口*带宽，即通过特定网络设备发送的流量。
- en: '**PROPER NETWORK MANAGEMENT**'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**正确的网络管理**'
- en: To have a completely reliable cluster, merely controlling egress traffic is
    clearly insufficient. A process that downloads a large file is going to saturate
    the available bandwidth just as much as one that uploads lots of data. However,
    we really can’t control what comes into our host via a given network interface,
    at least not at the host level. If we really want to manage network bandwidth,
    we need to handle that kind of thing at a switch or a router. For example, it
    is very common to divide up the physical network into virtual local area networks
    (VLANs). One VLAN might be an administration network used for auditing, logging,
    and for administrators to ensure that they can log in. We might also reserve another
    VLAN for important container traffic, or use traffic shaping to ensure that important
    packets get through. As long as we perform this kind of configuration at the switch,
    we can typically allow the remaining bandwidth to be “best effort.”
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现一个完全可靠的集群，仅仅控制出站流量显然是不够的。一个下载大文件的进程将和一个上传大量数据的进程一样占用可用带宽。然而，我们实际上无法控制通过特定网络接口进入我们主机的流量，至少在主机层面上是无法控制的。如果我们真的想要管理网络带宽，我们需要在交换机或路由器上处理这类问题。例如，将物理网络划分为虚拟局域网（VLAN）是很常见的做法。一个VLAN可能是用于审计、日志记录以及供管理员登录使用的管理网络。我们还可能为重要的容器流量预留另一个VLAN，或者使用流量整形确保重要数据包能够通过。只要我们在交换机上执行这种配置，通常可以允许剩余带宽以“最佳努力”方式传输。
- en: Although Linux does provide some cgroup capability for network interfaces, these
    would only help us prioritize and classify network traffic. For this reason, rather
    than using cgroups to control egress traffic, we’re going to directly configure
    the Linux kernel’s *traffic control* capabilities. We’ll test network performance
    using `iperf3`, apply a limit to outgoing traffic, and then test again. In this
    chapter’s examples, *host02* with IP address `192.168.61.12` was set up automatically
    with an `iperf3` server running so that we can send data to it from *host01*.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Linux确实为网络接口提供了一些cgroup功能，但这些仅有助于我们优先处理和分类网络流量。因此，与其使用cgroups来控制出站流量，我们将直接配置Linux内核的*流量控制*功能。我们将使用`iperf3`来测试网络性能，应用出站流量限制，然后再次进行测试。在本章的示例中，具有IP地址`192.168.61.12`的*host02*已自动设置并运行`iperf3`服务器，以便我们可以从*host01*向其发送数据。
- en: 'Let’s begin by seeing the egress bandwidth we can get on an unlimited interface:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先查看在没有限制的接口上可以获得的出站带宽：
- en: '[PRE35]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This example shows gigabit network speeds. Depending on how you’re running
    the examples, you might see lower or higher figures. Now that we have a baseline,
    we can use `tc` to set a quota going out. You’ll want to choose a quota that makes
    sense given your bandwidth; most likely enforcing a 100Mb cap will work:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子展示了千兆网速。根据你运行示例的方式，你可能会看到更低或更高的数值。现在我们有了基准，我们可以使用`tc`设置一个出站流量配额。你需要选择一个符合你带宽的配额；最有可能的是，设置100Mb的上限将会有效：
- en: '[PRE36]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The name of the network interface may be different on different systems, so
    we use `ip addr` to identify which interface we want to control. Then, we use
    `tc` to actually apply the limit. The token `tbf` in the command stands for *token
    bucket filter*. With a token bucket filter, every packet consumes tokens. The
    bucket refills with tokens over time, but if at any point the bucket is empty,
    packets are queued until tokens are available. By controlling the size of the
    bucket and the rate at which it refills, it is very easy for the kernel to place
    a bandwidth limit.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 网络接口的名称在不同的系统上可能不同，因此我们使用`ip addr`来确定我们要控制的接口。然后，我们使用`tc`来实际应用限制。命令中的`token
    tbf`代表*令牌桶过滤器*。使用令牌桶过滤器时，每个数据包都会消耗令牌。桶会随着时间的推移不断地重新填充令牌，但如果桶在任何时候为空，数据包会被排队，直到有令牌可用。通过控制桶的大小和桶填充的速率，内核能够轻松地设置带宽限制。
- en: 'Now that we’ve applied a limit to this interface, let’s see it in action by
    running the exact same `iperf3` command again:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对这个接口应用了限制，让我们通过再次运行完全相同的`iperf3`命令来查看其效果：
- en: '[PRE37]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: As expected, we are now limited to 100Mbps on this interface.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期所示，我们现在在这个接口上限速到100Mbps。
- en: Of course, in this case, we limited the bandwidth available on this network
    interface for everyone on the system. To use this ability properly to control
    bandwidth usage, we need to target the limits more precisely. However, in order
    to do that, we need to isolate a process to its own set of network interfaces,
    which is the subject of the next chapter.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在这种情况下，我们限制了系统上每个人使用的网络接口的带宽。要正确使用这种功能来控制带宽使用，我们需要更精确地设定限制。然而，为了做到这一点，我们需要将一个进程隔离到其自己的一组网络接口中，这将是下一章的主题。
- en: Final Thoughts
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结思考
- en: Ensuring that a process doesn’t cause problems for other processes on the system
    includes making sure that it fairly shares system resources such as CPU, memory,
    and network bandwidth. In this chapter, we looked at how Linux provides control
    groups (cgroups) that manage CPU and memory limits and traffic control capabilities
    that manage network interfaces. As we create a Kubernetes cluster and deploy containers
    to it, we’ll see how Kubernetes uses these underlying Linux kernel features to
    ensure that containers are scheduled on hosts with sufficient resources and that
    containers are well behaved on those hosts.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 确保一个进程不会给系统上的其他进程带来问题，包括确保它公平共享CPU、内存和网络带宽等系统资源。在本章中，我们看到Linux提供了控制组（cgroups）来管理CPU和内存限制，以及管理网络接口的流量控制能力。当我们创建一个Kubernetes集群并部署容器到其中时，我们将看到Kubernetes如何利用这些底层Linux内核功能来确保容器被调度到具有足够资源的主机上，并确保这些主机上的容器行为良好。
- en: 'We’ve now moved through some of the most important elements of process isolation
    provided by a container runtime, but there are two types of isolation that we
    haven’t explored yet: network isolation and storage isolation. In the next chapter,
    we’ll look at how Linux network namespaces are used to make each container appear
    to have its own set of network interfaces, complete with separate IP addresses
    and ports. We’ll also look at how traffic from those separate container interfaces
    flows through our system so that containers can talk to one another and to the
    rest of the network.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了容器运行时提供的一些最重要的进程隔离元素，但还有两种隔离类型我们尚未探讨：网络隔离和存储隔离。在下一章中，我们将看看Linux网络命名空间是如何被用来让每个容器看起来拥有自己的一组网络接口，包括独立的IP地址和端口。我们还将探讨这些单独容器接口的流量如何在我们的系统中流动，以便容器之间可以互相通信并与网络的其余部分进行通信。
