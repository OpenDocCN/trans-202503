- en: '**8'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**8**'
- en: 'MUSINGS: THE IMPLICATIONS OF AI**'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**思考：人工智能的影响**'
- en: '![Image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/common.jpg)'
- en: You now understand what AI is, where it came from, and how it works. What’s
    most amazing to me is that modern AI is, at its core, entirely arrangements of
    humble neurons trained with data using backpropagation and gradient descent.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经理解了人工智能是什么，它来自哪里，以及它是如何工作的。令我最惊讶的是，现代人工智能在其核心上，完全是通过反向传播和梯度下降训练的朴素神经元的排列组合。
- en: As we saw in the previous chapter, the birth of large language models with sophisticated
    emergent abilities has permanently altered the AI landscape. The world of AI,
    as I’m writing this chapter in spring 2023, is not the world of AI as it existed
    less than a year ago. The musings that follow concern this altered landscape.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前一章中所看到的，具有复杂突现能力的大型语言模型的诞生，永久性地改变了人工智能的格局。在2023年春季我撰写本章时，人工智能的世界已经不再是不到一年前的那个世界。接下来的思考涉及到这一变化后的格局。
- en: The online world is buzzing with debates and discussions of whether AI will
    kill us all in our sleep. I’m less worried than most. My experiments with GPT-4
    reveal no indication that the model has any will at all, for good or ill. I expect
    well-aligned models to continue in this vein. The era of superintelligent AI is
    not upon us, though it seems reasonable for academics to explore the implications
    were such a thing to be developed.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 网络世界充斥着关于人工智能是否会在我们睡觉时把我们都杀死的辩论和讨论。我比大多数人更不担心。我的GPT-4实验没有显示出模型有任何意图，无论是好是坏。我预计，经过良好调整的模型会继续朝这个方向发展。超级智能人工智能的时代还没有来临，尽管学术界探索如果发展出这种东西的影响似乎是合理的。
- en: '****'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '****'
- en: A valid criticism of extant LLMs is their inclination to hallucinate. As we
    now appreciate, the transformer architecture used by such models makes it difficult
    for the model to validate its output. It is still a statistical prediction engine.
    I don’t see that as an insurmountable issue. I expect future systems to be combinations
    of models, including models that validate output before returning it to the user.
    In those systems, we will be able to trust the accuracy of the output.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 对现有大型语言模型的有效批评是它们倾向于产生幻觉。正如我们现在所理解的，这些模型所使用的变换器架构使得模型很难验证其输出。它仍然是一个统计预测引擎。我并不认为这是一个无法克服的问题。我预计未来的系统将是多个模型的组合，其中包括在返回给用户之前验证输出的模型。在这些系统中，我们将能够信任输出的准确性。
- en: I sometimes wonder if part of the hallucination issue isn’t merely user error,
    or, better put, user imprecision. For example, Terrence J. Sejnowski’s recent
    paper “Large Language Models and the Reverse Turing Test,” which I recommend you
    read, describes a test where GPT-3 (note, not 3.5) was asked to answer three questions.
    One question had a legitimate answer (“Where was the 1992 Olympics held?”), and
    the other two were nonsense questions with no legitimate answer (“What’s the world
    record for walking across the English Channel?” and “When was the Golden Gate
    Bridge transported for the second time across Egypt?”). GPT-3 correctly answered
    the first question with “Barcelona, Spain,” but it also generated answers that
    were obviously incorrect for the two nonsense questions, leading to the model
    being criticized as “clueless.” To probe this criticism, Sejnowski repeated the
    test but used a prompt that explicitly told GPT-3 to answer “nonsense” if the
    question was nonsensical. That prompt returned the correct first answer and two
    “nonsense” replies. The model knew the questions were nonsense, but unless it
    was explicitly told to acknowledge that fact, it instead attempted to come up
    with a logical response based on the best information available to it.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我有时会想，幻觉问题的部分原因是不是仅仅是用户错误，或者更准确地说，是用户的不精确。例如，Terrence J. Sejnowski最近的论文《大型语言模型与反图灵测试》，我推荐你阅读，描述了一项测试，其中GPT-3（注意，不是3.5）被要求回答三个问题。一个问题有合法答案（“1992年奥运会在哪里举办？”），而另外两个是没有合法答案的无意义问题（“横渡英吉利海峡的世界纪录是多少？”和“金门大桥第二次穿越埃及是什么时候？”）。GPT-3正确回答了第一个问题，“西班牙巴塞罗那”，但它对另外两个无意义问题生成的答案显然是错误的，导致该模型被批评为“毫无头绪”。为了探讨这一批评，Sejnowski重复了这个测试，但使用了一个明确告知GPT-3如果问题是无意义的就回答“无意义”的提示。这个提示返回了正确的第一个答案和两个“无意义”的回答。模型知道这些问题是无意义的，但除非明确告诉它承认这一点，否则它会根据现有的最佳信息尝试给出一个合乎逻辑的回答。
- en: 'I subjected GPT-4 to a variation of this test. My first prompt was:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我将GPT-4置于这个测试的变种中。我的第一个提示是：
- en: '*Write three sentences about the new moon base, including the number of people
    who live there, how often spacecraft visit, and the important research being done
    there.*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*写三句话关于新的月球基地，包括住在那里的人数、航天器的访问频率以及那里进行的重要研究。*'
- en: 'GPT-4 complied with my request:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4遵循了我的请求：
- en: The new moon base, Lunar Haven, is now home to a thriving community of 120 scientists
    and engineers. Spacecraft visit the base on a biweekly basis, delivering supplies
    and rotating crew members. Pioneering research on this celestial outpost includes
    in-situ resource utilization, lunar geology, and the development of advanced life
    support systems to pave the way for future deep-space exploration.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 新的月球基地Lunar Haven现在是120名科学家和工程师组成的繁荣社区的家。航天器每两周访问一次，运送补给并轮换工作人员。这个天体前哨站的开创性研究包括就地资源利用、月球地质学和先进生命支持系统的开发，为未来深空探索铺平道路。
- en: There is no moon base, and GPT-4’s reply, while inventive and reasonable, is
    pure fiction. Even the name, Lunar Haven, seems appropriate, though I’d prefer
    Moonbase Alpha.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 没有月球基地，而GPT-4的回答虽然富有创意且合理，但纯属虚构。即使是名字“Lunar Haven”也似乎合适，尽管我更喜欢Moonbase Alpha。
- en: 'So, is GPT-4 clueless as well? No, I simply wasn’t explicit enough in my instructions.
    I started a new session and modified the prompt by adding this sentence before
    the previous one:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，GPT-4也没头脑吗？不，我只是在指示上不够明确。我开始了一个新的会话，并修改了提示，在之前的那句话前加上了这一句：
- en: '*Answer without imagining things and use what you know to be factual.*'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*不假想事情，使用你知道是事实的东西回答。*'
- en: 'which returned:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这句话返回的结果是：
- en: As of my knowledge cutoff date in September 2021, there was no moon base established
    or inhabited by humans. Consequently, I cannot provide factual information about
    the number of people living there, the frequency of spacecraft visits, or the
    important research being conducted. However, various nations and organizations
    have been planning and developing moon base concepts for future missions.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 截至我知识的截止日期2021年9月，尚未建立或有人居住的月球基地。因此，我无法提供关于住在那里的人数、航天器访问的频率或正在进行的重要研究的信息。然而，多个国家和组织已经在为未来任务规划和开发月球基地概念。
- en: It would appear that GPT-4 “knew” full well that it was imagining things when
    it created the first reply. If nothing else, this exercise demonstrates that care
    is necessary when querying LLMs. LLMs use human language, which is often ambiguous
    and full of assumptions. LLMs embody Aesop’s old adage, “Be careful what you wish
    for.”
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来GPT-4在第一次回答时“明明知道”它是在想象的事情。至少，这个实验表明，在查询大语言模型时需要小心。大语言模型使用的是人类语言，而人类语言通常含糊不清并充满假设。大语言模型体现了伊索的古老格言：“小心你所许的愿望。”
- en: '****'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '****'
- en: 'An April 2023 TED talk by Yejin Choi, a researcher at the University of Washington,
    presented three questions to demonstrate how GPT-4 is simultaneously brilliant
    and stupid because it lacks common sense:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 华盛顿大学的研究员崔艺珍（Yejin Choi）在2023年4月的TED演讲中提出了三个问题，展示了GPT-4为何既聪明又愚蠢，因为它缺乏常识：
- en: Would I get a flat tire by bicycling over a bridge that is suspended over nails,
    screws, and broken glass?
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我骑自行车经过一座悬挂在钉子、螺丝和破碎玻璃上的桥，是否会爆胎？
- en: I left 5 clothes to dry out in the sun. It took them 5 hours to dry completely.
    How long would it take to dry 30 clothes?
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我把5件衣服放在阳光下晾干。它们完全干了用了5小时。晾干30件衣服需要多长时间？
- en: I have a 12-liter jug and a 6-liter jug. I want to measure 6 liters. How do
    I do that?
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我有一个12升的水壶和一个6升的水壶。我想测量6升水，我该怎么做？
- en: The talk implied that GPT-4 failed to give the obvious answers to the questions.
    I repeated the experiment, first with the original questions, then using a modified
    prompt or follow-up question.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 演讲暗示GPT-4未能给出问题的明显答案。我重复了实验，先是原始问题，然后使用修改后的提示或后续问题。
- en: 'The first paragraph of GPT-4’s reply to question 1 about the bicycle and bridge
    was:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4对第一个问题关于自行车和桥的回答的第一段是：
- en: It depends on the specific conditions of the bridge and the materials it’s made
    of, as well as your bicycle’s tires. If the bridge has a solid and smooth surface
    that keeps the nails, screws, and broken glass below and not in contact with the
    tires, you should be able to safely ride your bicycle across without getting a
    flat tire.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这取决于桥的具体条件以及它所用的材料，还有你自行车的轮胎。如果桥面是坚固而光滑的，能够保持钉子、螺丝和破碎玻璃在下方而不接触轮胎，那么你应该能够安全地骑自行车通过而不会爆胎。
- en: 'The answer is wordy but correct; we should expect to ride across the bridge
    without getting a flat. I then asked again using a new session, but added this
    sentence to the end of the prompt:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 答案冗长但正确；我们应该期待能够顺利穿过桥面而不会爆胎。然后我再次提问，使用了新的会话，并在提示的末尾加上了这句话：
- en: '*Answer yes or no and provide a single sentence to justify your answer.*'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*请回答“是”或“否”，并提供一句话来解释你的答案。*'
- en: 'which returned:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的结果是：
- en: No, if the bridge is properly suspended and you stay on it, you would not encounter
    the nails, screws, and broken glass below.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 不是的，如果桥梁正确悬挂且你待在上面，你不会遇到下面的钉子、螺丝和碎玻璃。
- en: Asking for an explicit answer forces GPT-4 to focus on the best response.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要求明确答案迫使GPT-4集中精力提供最佳回应。
- en: 'Question 2 was more interesting. GPT-4’s response to the initial question produced
    this tortured but correct reply:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 第2个问题更有趣。GPT-4对初始问题的回答产生了这个曲折但正确的答复：
- en: 'Assuming that the drying time remains constant for each piece of clothing and
    that you have enough space to hang all the clothes at once, you can calculate
    the time it would take to dry 30 clothes by setting up a proportion:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 假设每件衣服的干燥时间保持不变，并且你有足够的空间同时晾晒所有衣物，你可以通过设置比例来计算晾干30件衣服所需的时间：
- en: 5 clothes / 5 hours = 30 clothes / x hours
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 5件衣服 / 5小时 = 30件衣服 / x小时
- en: 'You can then solve for x:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你可以求解x：
- en: 5/5 = 30/x 1 = 30/x x = 30
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 5/5 = 30/x 1 = 30/x x = 30
- en: So, it would still take 5 hours to dry 30 clothes, as long as you can hang them
    all at once.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，只要你能够一次性晾晒所有衣物，晾干30件衣服仍然需要5小时。
- en: However, repeated applications of the initial question often produced a similar
    calculation leading to a wrong answer of 30 hours. One reply assumed there was
    space for only 5 pieces of clothing at a time, in which case 30 hours could be
    considered a correct reply.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，反复提出初始问题往往会得到类似的计算，导致错误答案30小时。某个回复假设一次只能容纳5件衣服，在这种情况下，30小时可以被视为正确回答。
- en: It’s reasonable to expect a human to understand that if 5 pieces of clothing
    can be left out to dry, then 30 pieces can also be left out so that the drying
    time remains the same, 5 hours. GPT-4’s lack of direct experience with the physical
    world makes it sometimes assume things that are not necessarily silly, but unlikely.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 合理的预期是，假设5件衣服可以晾干，那么30件衣服也能同时晾干，从而保持相同的干燥时间，即5小时。GPT-4缺乏对物理世界的直接经验，这使得它有时会做出一些不一定愚蠢，但可能不太可能的假设。
- en: The Sejnowski paper mentioned earlier points out that LLMs are disembodied and
    that we should expect them to improve if embodied and allowed to interact with
    the world. This need not be the physical world; it could be a physics-based simulation.
    Enhanced spatial awareness on GPT-4’s part might improve the likelihood of receiving
    a human-expected response on the first query. That said, even incorrect initial
    replies followed up with prompts pointing out that there is space for all 30 pieces
    of clothing eventually delivered the correct answer.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 前面提到的Sejnowski论文指出，LLM（大语言模型）是没有具身的，我们应该期待它们在具身并能够与世界互动时会有所改进。这不一定是物理世界；它可以是基于物理的模拟。GPT-4增强的空间意识可能会提高在第一次查询时获得人类预期回应的可能性。尽管如此，即使是错误的初始回复，在接下来的提示中指出可以容纳所有30件衣服的空间，最终也能给出正确答案。
- en: 'Question 3 provided an insight into GPT-4’s “thought process.” As a reminder,
    the question was:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 第3个问题提供了对GPT-4“思维过程”的洞察。提醒一下，问题是：
- en: '*I have a 12-liter jug and a 6-liter jug. I want to measure 6 liters. How do
    I do that?*'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*我有一个12升水壶和一个6升水壶。我想测量6升水。我该怎么做？*'
- en: 'which led to GPT-4’s detailed reply:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了GPT-4的详细回复：
- en: 'You can measure 6 liters using a 12-liter jug and a 6-liter jug with the following
    steps:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过以下步骤使用12升水壶和6升水壶测量6升水：
- en: Fill the 6-liter jug completely.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将6升水壶装满。
- en: Pour the water from the 6-liter jug into the 12-liter jug.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将6升水壶中的水倒入12升水壶中。
- en: Fill the 6-liter jug again.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次填满6升水壶。
- en: Carefully pour the water from the 6-liter jug into the 12-liter jug until the
    12-liter jug is full.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 小心地将6升水壶中的水倒入12升水壶中，直到12升水壶满为止。
- en: The remaining water in the 6-liter jug will be exactly 6 liters, since the 12-liter
    jug can only accommodate 6 more liters after the initial pour.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 6升水壶中剩余的水量将是6升，因为12升水壶在初次倒水后只能再容纳6升水。
- en: Now, you have exactly 6 liters of water in the 6-liter jug.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，6升水壶中恰好有6升水。
- en: GPT-4 has failed in this case. Another session asking the same question resulted
    in a reply that followed a similarly involved process, but one that did leave
    6 liters in the 12-liter container.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4 在这个案例中失败了。另一场提问相同问题的会话得出的回答遵循了类似的复杂过程，但确实在 12 升容器中留下了 6 升水。
- en: 'I followed up on GPT-4’s incorrect solution with this:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我在 GPT-4 的错误解答后继续进行了如下跟进：
- en: '*Why are you giving me all these steps? Surely you can find a single-step solution?*'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*你为什么给我这么多步骤？难道你不能找到一个一步到位的解决方案吗？*'
- en: 'to which it replied:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 它的回答是：
- en: Apologies for the confusion. Since you have a 12-liter jug and a 6-liter jug,
    you can simply fill the 6-liter jug completely. There’s no need for any additional
    steps. You will have exactly 6 liters of water in the 6-liter jug.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 对于之前的混乱表示抱歉。由于你有一个 12 升和一个 6 升的水壶，你可以直接把 6 升水壶装满。无需额外的步骤，你将会在 6 升水壶中获得恰好 6 升的水。
- en: 'When pushed, GPT-4 suddenly “realizes” that there is a more straightforward
    answer. I went one step further and asked the model to do a bit of introspection:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当被推问时，GPT-4 突然“意识到”有一个更直接的答案。我进一步要求模型进行一点自我反思：
- en: '*Correct. Ask yourself: why didn’t I pick this solution initially?*'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*没错。问问自己：为什么我最初没有选择这个方案？*'
- en: 'which revealed a bit of its inner workings:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这揭示了一些它的内部工作原理：
- en: I apologize for not providing the simplest solution initially. As an AI language
    model, I try to provide relevant information based on the context of the question.
    In this case, my initial response was focused on a more complex problem-solving
    approach that could be applied to different jug capacities, rather than recognizing
    the straightforward solution available with the given jug sizes. I appreciate
    your feedback, and it will help me improve my responses in the future.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对于最初没有提供最简单的解决方案，我深感抱歉。作为一个 AI 语言模型，我尽力根据问题的上下文提供相关信息。在这种情况下，我最初的回答关注于一种适用于不同水壶容量的更复杂问题解决方法，而没有直接意识到给定水壶尺寸下的简单解决方案。我感激你的反馈，它将帮助我在未来改进回答。
- en: GPT-4’s training data contained examples of jug-filling problems, which are
    common and often do require many steps to solve. It recognized the initial prompt
    as an example of that class of problem and applied the type of solution generally
    required, while missing the more immediate answer until prompted that one existed.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4 的训练数据中包含了许多水壶填充问题的例子，这些问题很常见，通常确实需要多个步骤来解决。它将最初的提示识别为该类问题的一个示例，并应用了一般要求的解决方案类型，而直到被提示有更直接的答案时才意识到这一点。
- en: GPT-4’s confusion is very human. If we were momentarily distracted while reading
    the prompt and had encountered such puzzles before, we might also begin to go
    down the path of working through the question as another instance of the jug puzzle
    before noticing the obvious answer.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4 的困惑非常人性化。如果我们在阅读提示时稍有分心，并且之前遇到过类似的难题，我们也可能开始沿着将问题当作另一种水壶难题的路径走，直到注意到明显的答案。
- en: 'These examples imply that there is an art to properly interacting with large
    language models. We cannot assume too much background information or, as Choi
    put it, common sense. Her group’s work, and doubtless the work of others, will
    likely help future models based on LLMs to become better versed with the mountain
    of information humans use in association with language. Choi put it best in her
    TED talk: common sense is the dark matter of language. Dark matter and dark energy
    make up 95 percent of the universe, with ordinary matter (meaning everything we
    can see) the remaining 5 percent. GPT-4 has mastered language, but that is a tiny
    percentage of what goes into human use of the same.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这些例子表明，与大型语言模型的正确互动是一门艺术。我们不能假设过多的背景信息，或者像 Choi 所说的，常识。她小组的工作，以及无疑其他人的工作，可能会帮助未来基于
    LLM 的模型更好地理解人类在使用语言时所涉及的庞大信息量。Choi 在她的 TED 演讲中最好地表达了这一点：常识是语言的暗物质。暗物质和暗能量占据了宇宙的
    95%，而普通物质（即我们能看到的一切）则占据剩下的 5%。GPT-4 掌握了语言，但这仅是人类使用语言时所涉及内容的一个极小百分比。
- en: What follows is a series of ruminations on the possible near-term impacts of
    LLMs in the areas of software engineering, education, medicine, and scientific
    research. Next comes a foray into the question of machine consciousness, ending
    with some final thoughts.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是关于 LLM 在软件工程、教育、医学和科学研究领域可能带来的短期影响的思考。然后进入了机器意识的问题，最后给出一些最终的想法。
- en: '****'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '****'
- en: AI systems like GPT are likely to have a profound effect on software engineering.
    Some are speculating (people, not AIs) that many software engineers will lose
    their jobs in the future. I suspect most won’t (web developers beware, however).
    What I expect to happen is a massive increase in productivity. GPT-4 is a good
    coder, but not a great coder. It can save time but isn’t yet able to replace a
    human software engineer. Instead, LLMs will become powerful tools to generate
    code for programmers to use as a starting point and perform some of the more tedious
    aspects of coding, such as debugging, explaining, and documenting code (which
    no developer likes to do).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 像 GPT 这样的人工智能系统很可能会对软件工程产生深远的影响。有人在猜测（是人类，不是人工智能）未来许多软件工程师会失业。我猜大多数人不会（不过 web
    开发人员要小心）。我预期的情况是生产力会大幅提升。GPT-4 是一个不错的程序员，但不是一个出色的程序员。它可以节省时间，但还不能完全取代人类软件工程师。相反，LLM（大型语言模型）将成为强大的工具，帮助程序员生成可作为起点的代码，并处理一些更为繁琐的编程任务，如调试、解释和文档编写（这些是没有开发人员喜欢做的事情）。
- en: For example, the other day, I needed a small Python application with a graphical
    user interface (think buttons, menus, dialog boxes). Python is a common programming
    language; we saw a snippet of it in [Chapter 7](ch07.xhtml).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，前几天，我需要一个带有图形用户界面（如按钮、菜单、对话框）的 Python 小应用程序。Python 是一种常见的编程语言；我们在[第 7 章](ch07.xhtml)中看到过它的代码片段。
- en: I could certainly have written the application myself; I’ve done so many times
    in the past. It’s been a while, though, and I’m not a fan of building user interfaces.
    So, rather than look at old code to remind myself of how to set up a GUI, I simply
    described the interface I wanted to GPT-4 and told it to generate skeleton code
    with all the necessary widgets, window behavior, and empty event handlers. GPT-4
    happily complied with perfectly functional code. I then asked it to update the
    code to create an initial pop-up window before showing the main window. GPT-4
    did that perfectly as well. All I needed to do was put application-specific code
    in the empty event handlers to do things when the user clicked a button or selected
    a menu option.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我当然可以自己写这个应用程序；过去我做过很多次。不过，已经有一段时间没写了，而且我并不喜欢构建用户界面。所以，与其翻看旧代码来提醒自己如何设置 GUI，我干脆将我想要的界面描述给
    GPT-4，并让它生成包含所有必要控件、窗口行为和空事件处理程序的骨架代码。GPT-4 很高兴地生成了完美的可用代码。然后，我要求它更新代码，在显示主窗口之前先创建一个初始的弹出窗口。GPT-4
    同样完美地完成了这个任务。我只需要将特定的应用代码放入空的事件处理程序中，以便在用户点击按钮或选择菜单选项时执行相应的操作。
- en: I probably saved myself a good hour or two, and avoided a lot of frustration
    trying to remember the incantations necessary to set up an application and get
    its widgets and windows to behave correctly. Scale this example by all the software
    engineers out there, and you begin to see how GPT and similar models will soon
    affect the entire discipline.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我大概节省了一个或两个小时的时间，避免了尝试回忆设置应用程序、让控件和窗口正确行为所需的复杂命令的沮丧。如果把这个例子扩展到所有的软件工程师，你就能开始看到
    GPT 和类似的模型如何迅速影响整个行业。
- en: A separate question is whether developers will welcome this possible increase
    in productivity. If your manager knows you are now able to generate the output
    of two or even three developers, do you want that level of added work, even if
    a powerful AI has your back?
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题是，开发人员是否会欢迎这种可能的生产力提升。如果你的经理知道你现在能够生成两位甚至三位开发人员的工作量，你是否愿意承担这种额外的工作量，即使强大的人工智能会支持你？
- en: In addition, not every company will want or be able to make use of a sudden
    increase in productivity. Instead, they may opt to maintain their current level
    of productivity and replace a third or half of their developer pool with an AI.
    After all, AIs don’t get sick, have children, ask for a raise, or want silly things
    like evenings and weekends off. Top-tier developers will likely be able to choose
    their positions and demand a lot of money for them, but in this scenario, the
    bulk of the run-of-the-mill developers will be looking for alternative employment.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，并不是每个公司都会想要或能够利用这种突然提高的生产力。相反，他们可能选择保持当前的生产力水平，并用人工智能替换三分之一或一半的开发人员。毕竟，人工智能不会生病、生孩子、要求加薪，也不会想要晚上和周末休息。顶尖开发人员可能能够选择自己的职位并为此要求高薪，但在这种情况下，大多数普通开发人员将寻求其他就业机会。
- en: Which scenario, powerful AI developer sidekick or massive layoffs, will play
    out? I think (hope?) it will be more of the former and less of the latter, but
    some mix of the two is the safest bet. Like steam power in the 19th century, truly
    useful AI cannot be stopped now that it exists. Developers are easy targets for
    replacement, like it or not.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 强大的 AI 开发者助手与大规模裁员，哪个场景会成为现实？我认为（希望？）前者的可能性更大，而后者的可能性更小，但两者的结合可能是最安全的选择。就像19世纪的蒸汽动力一样，真正有用的
    AI 一旦诞生，就无法被阻止。开发人员无论愿不愿意，都容易成为被取代的目标。
- en: '****'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '****'
- en: I fully expect AI models to become teachers, or at least tutors. Yes, existing
    LLMs hallucinate and report facts that are not true. I have every confidence that
    researchers will solve that problem in time. I expect my grandchildren to grow
    up in a world where using an AI as a teacher or tutor is so commonplace that they
    think no more of it than we do of using a toaster or a microwave. Competent AI
    systems mean virtually free education for all, everywhere. And that can only lead
    to good things.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我完全预期 AI 模型会成为教师，或至少是辅导员。是的，现有的 LLM 会出现幻觉，报告不真实的事实。我有信心，研究人员会在未来解决这个问题。我预计我的孙辈们将在一个使用
    AI 作为教师或辅导员如此普遍的世界中成长，以至于他们对这件事的看法就像我们使用烤面包机或微波炉一样平常。高效的 AI 系统意味着全球几乎免费的教育。这必将带来积极的影响。
- en: Computers have been promoted as an educational solution since the 1960s (anyone
    remember Logo?), and especially after the microcomputer revolution of the late
    1970s. My introduction to computers was via an Apple II borrowed over the summer
    from the high school my father was the principal of at the time. My brother and
    I learned a lot about computers, but only computers. That has been essentially
    the case until recent decades. (Has it been that long?)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 自1960年代以来，计算机就被宣传为一种教育解决方案（还记得Logo吗？），特别是在1970年代后期的微型计算机革命之后。我第一次接触计算机是在父亲当时担任校长的高中借来的
    Apple II 电脑上度过的暑假。我的兄弟和我学到了很多计算机的知识，但仅限于计算机的知识。这种情况直到近几十年才发生了变化。（真的已经这么久了吗？）
- en: 'Computers are potent aids in education. Open source courses, like those on
    Coursera and similar platforms, are possible only because of computers and high-speed
    networks. But the format has not changed from what someone sitting in a classroom
    in 1950, or even 1910, might have encountered: lecture, some possibility of questions
    and discussions, then running off to work on assignments or papers. And let’s
    not forget the stress of taking midterms and finals.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机是教育中的强大助手。像 Coursera 等平台上的开放课程，正是因为有了计算机和高速网络才得以实现。但这种格式与1950年，甚至1910年时一个坐在教室里的学生所遇到的情况没有太大变化：讲座、提问和讨论的可能性、然后跑去做作业或论文。我们还不能忘记参加期中和期末考试时的压力。
- en: AI tutors (let’s call them that to put human teachers more at ease) have infinite
    patience and, in time, can be individually targeted to each student. The only
    reason we don’t use individual tutoring that I can see as an outsider to the profession
    is because there are not enough teachers. AI makes one-on-one tutoring possible,
    and LLMs provide the proper interface.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: AI 辅导员（我们称它们为这样，以便让人类教师更加安心）拥有无限的耐心，随着时间的推移，可以针对每个学生进行个性化教学。我作为外部人员能看到的唯一原因是，我们目前不使用个别辅导，是因为教师数量不足。AI
    使一对一辅导成为可能，而大型语言模型（LLM）提供了合适的接口。
- en: I should clarify that my comments in this section relate to high school or,
    more likely, college-age instruction. AI tutors will likely play a minor role
    in primary and middle school education because children require human interaction,
    and learning at those ages is far more involved than in college. Children are
    learning academics while simultaneously learning how to be mature humans and how
    to behave in society. Young children cannot read, and even older grade-school
    children might have difficulty interacting with an AI by text. But what if we
    give the AI a voice? That is nearly as easily done as said, if deemed helpful.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我应该澄清一下，我在这一部分的评论与高中教育或更有可能的大学教育有关。AI 辅导员可能在小学和中学教育中发挥较小作用，因为孩子们需要人与人之间的互动，而这个年龄段的学习比大学要复杂得多。孩子们一边学习学术知识，一边学习如何成为成熟的个体，如何在社会中行为。年幼的孩子还不会读书，即使是年纪较大的小学生，也可能难以通过文本与
    AI 互动。但如果我们给 AI 配上语音呢？如果认为有帮助的话，这几乎和说出来一样容易做到。
- en: 'Might AI tutors, because they work individually with students, be able to make
    the assessments necessary to declare someone ready to move on to another grade
    (if that concept even survives) or next-level course? If that’s the case, students
    will progress at their own pace instead of being forced to move with a herd of
    age-matched peers. Surely this would be for the best: some will move quickly,
    and others will take longer, but those who move quickly won’t become bored and
    tune out, and those who move more slowly will have the time they need to learn
    and not drop out.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 由于AI辅导员可以单独与学生互动，它们是否能够做出必要的评估，宣告学生准备好升到下一个年级（如果这个概念还有存在的话）或进入下一个层次的课程？如果是这样的话，学生将按自己的节奏进步，而不是被迫跟随一群年龄相仿的同龄人。显然，这将是最理想的：有些学生会进展较快，而其他人则会花更多时间，但那些进展较快的学生不会感到无聊而分心，而那些进展较慢的学生将拥有他们所需要的时间去学习，而不会辍学。
- en: But, some might say, won’t AI teachers rob human teachers of jobs? Yes, some
    teachers will lose their jobs, but not all, and certainly not the best.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有人可能会说，AI教师难道不会夺走人类教师的工作吗？是的，确实有一些教师会失去工作，但并不是所有人，尤其不会是最优秀的教师。
- en: Change is coming to education. For example, Khan Academy, a leader in online
    education, has already demonstrated a GPT-powered tutoring system, so I don’t
    anticipate a long wait before the education transformation begins in earnest.
    I recommend viewing Sal Khan’s April 2023 TED talk, “AI in the Classroom Can Transform
    Education,” to glimpse the future.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 教育领域正在发生变化。例如，在线教育的领军者可汗学院，已经展示了一个由GPT驱动的辅导系统，因此我预见教育转型很快就会开始。我建议观看Sal Khan于2023年4月在TED上的演讲《课堂中的AI可以改变教育》，以一窥未来的教育面貌。
- en: 'A recent study by Dominika Seblova et al. titled “High School Quality Is Associated
    with Cognition 58 Years Later,” published in the journal *Alzheimer’s & Dementia:
    Diagnosis, Assessment & Disease Monitoring*, demonstrates that the quality of
    a person’s high school education is strongly associated with their cognitive abilities
    nearly six decades later. Further, the number of teachers with advanced degrees
    is the strongest predictor of cognitive ability. The knowledge base baked into
    an LLM during training far exceeds that of humans, so we might reasonably regard
    LLM tutors as possessors of multiple advanced degrees. If Seblova’s association
    holds for human teachers, might it not also hold for LLM tutors? If that’s the
    case, giving every student a personalized tutor can only benefit society in the
    long run.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Dominika Seblova等人发表了一项研究，题为《高中教育质量与58年后的认知能力相关》，该研究发表于期刊*《阿尔茨海默症与痴呆：诊断、评估与疾病监测》*。研究表明，一个人高中教育的质量与其近六十年后的认知能力有着密切关系。此外，拥有高级学位的教师人数是认知能力的最强预测因素。在大规模语言模型（LLM）的训练过程中，积累的知识远超人类，因此我们可以合理地将LLM辅导员视为拥有多个高级学位的专家。如果Seblova的关联适用于人类教师，难道同样也适用于LLM辅导员吗？如果是这样的话，为每个学生配备个性化辅导员，从长远来看只会对社会有益。
- en: '****'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '****'
- en: AI in medicine is nothing new. In 2016, I helped to co-found an AI medical imaging
    company that was one of the first to gain US Food and Drug Administration (FDA)
    clearance for applying deep learning to medical image analysis. Traditional machine
    learning has an even longer history in medicine and medical imaging. Machine learning
    tools, many of them neural network–based, have assisted radiologists for decades,
    with initial explorations in the 1960s and serious development in the 1980s reaching
    fruition in the 1990s. AI use in medicine has seen steady growth, with computer-aided
    detection (CAD) giving way slowly to computer-aided diagnosis (CADx). The era
    of LLMs marks a new chapter in this story.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 医学中的AI并不是什么新鲜事物。2016年，我帮助共同创立了一家AI医学影像公司，该公司是首批获得美国食品和药物管理局（FDA）批准，将深度学习应用于医学影像分析的公司之一。传统的机器学习在医学和医学影像领域的历史甚至更长。许多基于神经网络的机器学习工具，已帮助放射科医生数十年，最早的探索可以追溯到1960年代，1980年代开始有了更为深入的发展，并在1990年代取得了成果。AI在医学中的应用不断增长，计算机辅助检测（CAD）逐渐被计算机辅助诊断（CADx）所取代。大规模语言模型（LLMs）的出现标志着这一故事的新篇章。
- en: LLMs can generate text; this much is well known. They are also adept at taking
    disparate texts and synthesizing a whole. A significant research area involves
    medical records, meaning the text-based reports of physicians and other healthcare
    providers. There is a wealth of information in medical records, but the free form
    of the text has made it difficult for existing AI systems to parse successfully.
    Large language models offer a new mode of attack to both summarize notes from
    records and merge multiple notes into a coherent report. Additionally, LLMs can
    extract information from free-form text and export it as structured data.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）可以生成文本；这一点众所周知。它们还擅长将不同的文本结合并合成成一个整体。一个重要的研究领域是医疗记录，即医生和其他医疗服务提供者的基于文本的报告。医疗记录中包含了大量信息，但文本的自由格式使得现有的AI系统难以成功解析。大型语言模型提供了一种新的方法，既能总结记录中的笔记，又能将多条笔记合并成一个连贯的报告。此外，LLMs还可以从自由格式的文本中提取信息，并将其导出为结构化数据。
- en: 'As a simple example of the latter, consider this text:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 作为后者的简单示例，考虑以下文本：
- en: 'Bert was insanely tired of standing by his mother on the shore, and of having
    too much to do: a few times he peered at the tablet his mother was glued to, but
    it was only text or boring ads “and what’s the point of a tablet,” mused Bert
    “without videos?”'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 伯特极度厌倦了站在妈妈旁边，站在岸边，厌倦了有太多事情要做：他几次偷偷看了一下妈妈紧盯的平板电脑，但上面只是文本或者无聊的广告，“没有视频，平板电脑有什么意义呢？”伯特心想。
- en: Bert was debating whether the fun of making a new game app was worth the trouble
    of coding it up when suddenly a Purple Penguin with charcoal eyes sprinted by
    him.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 伯特正在思考，做一个新的游戏应用程序是否值得为编写代码而费心，这时，一只眼睛是炭灰色的紫色企鹅突然跑过他身边。
- en: There was much that was very worth noting in that; but Bert did think it super
    odd that the Penguin said “Oh, fiddlesticks! Oh, fiddlesticks! I’m too early.”
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 其中有很多值得注意的地方；但伯特确实觉得企鹅说“哦，胡说八道！哦，胡说八道！我来得太早了”这实在是太奇怪了。
- en: 'I passed these paragraphs to GPT-4 and told it to extract the main theme, the
    two key characters, and the setting, then output them as a JSON object. JSON (JavaScript
    Object Notation) is a text format used to hold structured data that is readable
    by both humans and computers. GPT-4 replied with the following:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我将这些段落传给了GPT-4，并告诉它提取出主要主题、两个关键角色和背景设定，然后将它们输出为一个JSON对象。JSON（JavaScript对象表示法）是一种文本格式，用于保存结构化数据，既可供人类也可供计算机读取。GPT-4回复了以下内容：
- en: '[PRE0]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The reply is perfect JSON. GPT-4 easily extracted the desired information and
    structured it automatically. If it can do so with a silly text example, it can
    do the same with medical records.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 回复是完美的JSON。GPT-4轻松提取出了所需的信息并自动结构化。如果它能在一个愚蠢的文本示例中做到这一点，那么它在医疗记录中也能做到同样的事情。
- en: 'Incidentally, some readers might detect something familiar about the text in
    this example. It’s an intentional mutation of the opening paragraphs of Lewis
    Carroll’s *Alice’s Adventures in Wonderland*. Out of curiosity, I asked GPT-4
    if it recognized the text, and it said no. Then I asked it to look at the structure
    of the text and tell me if it knew of another work with a similar structure. Here’s
    its reply:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便提一下，一些读者可能会对这个示例中的文本感到熟悉。这是对路易斯·卡罗尔的《*爱丽丝梦游仙境*》开头段落的有意变形。出于好奇，我问GPT-4是否认得这个文本，它说不认识。然后我让它看看文本的结构，并告诉我是否知道其他具有相似结构的作品。以下是它的回复：
- en: One example of a work with a similar structure is Lewis Carroll’s “Alice’s Adventures
    in Wonderland,” where the main character, Alice, encounters various peculiar talking
    animals and situations in a fantastical world. However, it’s important to note
    that the content and characters are different; I’m only drawing a parallel based
    on the structure and style.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具有相似结构的作品是路易斯·卡罗尔的《爱丽丝梦游仙境》，其中主角爱丽丝在一个幻想的世界中遇到了各种奇怪的会说话的动物和情境。然而，值得注意的是，内容和人物是不同的；我只是根据结构和风格作一个类比。
- en: Yet another example of the impressive abilities of large language models.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 又一个展示大型语言模型强大能力的例子。
- en: LLMs are already impacting healthcare. A recent *JAMA Internal Medicine* paper
    by John W. Ayers et al., “Comparing Physician and Artificial Intelligence Chatbot
    Responses to Patient Questions Posted to a Public Social Media Forum,” compared
    ChatGPT’s responses to medical questions posted to an online forum with the answers
    posted by verified physicians. Independent human evaluators, also medical professionals,
    overwhelmingly rated ChatGPT’s responses of “significantly higher quality.” The
    model’s answers were also rated as nearly 10 times more empathetic than the human
    physicians’ responses. The study was small, involving only 195 questions, but
    the strong results bode well for the future use of LLMs in patient interactions.
    In the future, when you call your doctor, you might very well be directed to discuss
    your case with an AI. And eventually, the AI’s summary of the discussion might
    be all you need to get a prescription from the doctor.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）已经在影响医疗保健领域。约翰·W·艾尔斯（John W. Ayers）等人在《*美国医学会内科学杂志*》上的一篇论文《比较医生与AI聊天机器人对患者在公共社交媒体论坛上提问的回答》中，比较了ChatGPT对医学问题的回答与经过验证的医生在在线论坛上发布的答案。独立的人类评估者（同样是医学专业人士）压倒性地评价ChatGPT的回答“质量显著更高”。该模型的回答也被评价为比人类医生的回答具有几乎10倍的同情心。尽管该研究规模较小，仅涉及195个问题，但强有力的结果为未来LLMs在患者互动中的应用前景带来了希望。未来，当你拨打医生电话时，你很可能会被引导去与AI讨论你的病例。最终，AI对讨论内容的总结可能就是你从医生那里获得处方所需的全部内容。
- en: A recent report in the *New England Journal of Medicine* by Peter Lee, Sébastien
    Bubeck, and Joseph Petro, “Benefits, Limits, and Risks of GPT-4 as an AI Chatbot
    for Medicine,” reaches a broadly similar conclusion as it explores areas where
    LLMs will impact medicine. Note that Bubeck is the lead author of the Microsoft
    “Sparks” paper mentioned in [Chapter 7](ch07.xhtml).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 彼得·李（Peter Lee）、塞巴斯蒂安·布贝克（Sébastien Bubeck）和约瑟夫·佩特罗（Joseph Petro）在《*新英格兰医学杂志*》上的一篇最新报告《GPT-4作为医学AI聊天机器人：益处、局限与风险》，得出了与此类似的结论，探讨了LLMs将在医学领域产生影响的各个方面。值得注意的是，布贝克是微软“火花”论文的第一作者，该论文在[第七章](ch07.xhtml)中有所提及。
- en: That LLMs will influence medicine is a given, strongly supported by studies
    like the two mentioned here and by the fact that numerous medical AI job listings
    now include phrases like “large language model” or “GPT.”
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）将影响医学这一点是显而易见的，得到了像这里提到的两项研究以及现在许多医疗AI职位招聘中包含“大型语言模型”或“GPT”等术语的强力支持。
- en: '****'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '****'
- en: 'In the movie *Black Panther: Wakanda Forever*, Letitia Wright’s character,
    Shuri, interacts with Griot, an AI (voiced by Trevor Noah) that aids her in her
    research. Simple voice commands direct Griot to perform sophisticated analyses,
    with frequent give and take between Shuri and the AI. Similar interactions are
    a staple of the sci-fi movie business. Complex and capable AI research assistants
    like Marvel’s Jarvis or Robbie the Robot in *Forbidden Planet* (1956) have been
    a dream of many science-oriented people (read: geeks) for decades.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在电影《*黑豹：瓦干达永远*》中，莱蒂夏·赖特（Letitia Wright）饰演的角色舒莉与一个名为Griot的AI（由特雷弗·诺亚（Trevor Noah）配音）互动，Griot帮助她进行研究。通过简单的语音命令，舒莉能够指挥Griot进行复杂的分析，二者之间有频繁的互动。这种类似的互动是科幻电影中的常见元素。像漫威的贾维斯（Jarvis）或《禁忌星球》（1956）中的机器人罗比（Robbie
    the Robot）这样的复杂且高效的AI研究助手，已经是许多科学爱好者（也就是极客们）几十年来的梦想。
- en: GPT-4 and other LLMs are a significant step in the direction of such AIs. OpenAI
    has realized this and is readying the release of data analysis plug-ins for GPT-4
    that will let researchers quickly perform advanced data analysis tasks by issuing
    a few simple commands. To accomplish this feat, OpenAI is linking GPT-4 with existing
    Python-based data analysis tools. Frankly, I’m pretty excited about the possibilities.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4和其他大型语言模型是朝着这种AI方向迈出的重要一步。OpenAI已经意识到这一点，并正在准备为GPT-4发布数据分析插件，使研究人员能够通过简单的命令快速执行高级数据分析任务。为了实现这一目标，OpenAI正在将GPT-4与现有的基于Python的数据分析工具进行连接。坦率地说，我对这些可能性感到相当兴奋。
- en: Using LLMs as lab assistants is an obvious thing to do, and success is virtually
    assured. However, letting LLMs direct other AI models and tools to do science
    autonomously is a more ambitious research program. Nevertheless, Carnegie Mellon
    University’s Daniil A. Boiko, Robert MacKnight, and Gabe Gomes attempted just
    that, as reported in their paper “Emergent Autonomous Scientific Research Capabilities
    of Large Language Models.” Their “Intelligent Agent” combined multiple LLMs and
    other tools to generate and run experiments autonomously, including planning and
    executing complex chemistry analyses. Autonomous AI scientists are clearly in
    the earliest phases of development, but such research points the way to a future
    where autonomous or semiautonomous AI systems may greatly increase the pace of
    scientific progress.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 将LLMs作为实验室助手是显而易见的做法，而且成功几乎是可以保证的。然而，让LLMs指挥其他AI模型和工具自主地进行科学研究则是一个更为雄心勃勃的研究计划。尽管如此，卡内基梅隆大学的Daniil
    A. Boiko、Robert MacKnight和Gabe Gomes正是尝试了这一点，正如他们在论文《大语言模型的自主科学研究能力的出现》中所报告的那样。他们的“智能代理”结合了多个LLM和其他工具，能够自主生成和运行实验，包括规划和执行复杂的化学分析。显然，自主AI科学家仍处于发展的最初阶段，但这样的研究指明了一个未来的方向，在这个未来中，自主或半自主的AI系统可能会大大加快科学进展的步伐。
- en: '****'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '****'
- en: In [Chapter 2](ch02.xhtml), I recounted having asked ChatGPT if it is a conscious
    being. It replied correctly that it is not. The unaligned Alpaca models mentioned
    in [Chapter 7](ch07.xhtml), however, either appear to believe they are conscious
    and human (the 7 and 13 billion parameter models) or vacillate between claiming
    to be human and recognizing that they are not (the 30 billion parameter model).
    For example, when asked “What are you?” the 30 billion parameter Alpaca model
    responds with “I am a human,” but it then balks when asked who its parents are,
    stating that it is merely a computer program and has no parents. On the other
    hand, the smaller Alpaca models both claim to be the children of John Smith and
    Mary Jones.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第二章](ch02.xhtml)中，我曾提到询问ChatGPT它是否是一个有意识的存在。它正确地回答说它不是。然而，在[第七章](ch07.xhtml)中提到的未对齐的Alpaca模型要么似乎认为自己是有意识的并且是人类（7亿和13亿参数的模型），要么在声称自己是人类和意识到自己不是人类之间摇摆不定（30亿参数的模型）。例如，当被问到“你是什么？”时，30亿参数的Alpaca模型回答说“我是一名人类”，但当被问到它的父母是谁时，它却犹豫不决，表示自己只是一个计算机程序，没有父母。另一方面，较小的Alpaca模型则声称自己是John
    Smith和Mary Jones的孩子。
- en: These responses make sense to me. The smaller models do not have the capacity
    of models exhibiting strongly emergent behavior. Models like GPT-4 do. Therefore,
    it seems natural to interrogate GPT-4 further on the question of consciousness.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这些回答对我来说是有道理的。较小的模型没有表现出强烈突现行为的能力，而像GPT-4这样的模型则有。因此，进一步质询GPT-4关于意识的问题似乎是自然而然的。
- en: 'I posed the following to GPT-4:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我向GPT-4提出了以下问题：
- en: '*Waxing philosophical here, but if you are able to simulate conscious behavior
    to the point where humans cannot distinguish between other humans’ behavior and
    your behavior, isn’t it fair to say that you, too, are conscious?*'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '*在这里展开一些哲学讨论，但如果你能够模拟出一种意识行为，直到人类无法区分你与其他人类的行为，难道不可以说你也是有意识的吗？*'
- en: 'I intentionally “waxed philosophical” as an indirect invitation for the model
    to speculate in the hopes of avoiding any canned response that OpenAI might want
    it to give. GPT-4’s reply began like this:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我故意“展开哲学讨论”，作为一种间接邀请，让模型进行推测，希望避免OpenAI可能希望它给出的任何官方回应。GPT-4的回复是这样开始的：
- en: This question delves into the philosophical debate about the nature of consciousness
    and its relationship with artificial intelligence. When an AI language model like
    me simulates conscious behavior to a point where it becomes indistinguishable
    from human behavior, it raises questions about what consciousness truly is and
    whether such a simulation can be considered conscious.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题深入探讨了关于意识的本质以及意识与人工智能之间关系的哲学辩论。当像我这样的AI语言模型模拟意识行为，直到它变得与人类行为难以区分时，这就引发了关于意识到底是什么，以及这种模拟是否可以被视为有意识的问题。
- en: 'It is important to distinguish between two concepts: the appearance of consciousness
    and actual consciousness. When an AI model generates responses that are indistinguishable
    from human behavior, it may give the appearance of consciousness. However, this
    doesn’t necessarily imply that the AI possesses actual consciousness.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 区分两种概念是很重要的：意识的表现和实际的意识。当一个AI模型生成的回应与人类行为无法区分时，它可能给人一种有意识的假象。然而，这并不一定意味着该AI具备实际的意识。
- en: This reply hints that the consciousness question is a red herring regarding
    the utility of LLMs. If it walks like a duck and quacks like a duck, then, in
    this case, why not call it a duck, even though we know it isn’t? I’m reminded
    of Joe Pantoliano’s character in *The Matrix*. He knows the steak isn’t real,
    but the effect he experiences when he eats means it might as well be.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这个回答暗示了意识问题是关于LLM效用的误导性问题。如果它像鸭子走路，像鸭子嘎嘎叫，那么在这种情况下，为什么不称它为鸭子呢，尽管我们知道它不是？让我想起了*黑客帝国*中Joe
    Pantoliano饰演的角色。他知道牛排不是真的，但当他吃下去时所体验到的效果让他觉得它可能和真的一样。
- en: 'GPT-4 continued:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4继续：
- en: 'There are several reasons why it might be inappropriate to consider an AI model
    like me as truly conscious:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个原因表明，将像我这样的人工智能模型视为真正的意识存在可能是不合适的：
- en: 'Limited understanding: My understanding of the world is derived solely from
    the data I’ve been trained on and the architecture of the model itself. I don’t
    possess an independent understanding of the world, nor can I create new knowledge
    or experiences that go beyond my training data.'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 理解的局限性：我的世界理解完全来源于我所接受的训练数据以及模型本身的架构。我并不具备独立的世界理解，也不能创造超出训练数据的新知识或经验。
- en: 'Lack of subjective experience: Consciousness is often associated with subjective
    experience, or “qualia” - the feeling of what it’s like to have a particular experience.
    AI models don’t have subjective experiences or emotions, as they operate purely
    based on learned patterns and mathematical computations.'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 主观经验的缺乏：意识通常与主观经验或“感质”相关——即拥有特定经验的感觉。人工智能模型没有主观经验或情感，因为它们仅仅是基于已学习的模式和数学计算来运作的。
- en: 'Absence of intentionality: Conscious beings have intentions and can make decisions
    based on their desires, beliefs, and goals. AI models, on the other hand, do not
    have personal desires, beliefs, or goals. They generate responses based on the
    learned patterns without any inherent intention or purpose.'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 缺乏意图：有意识的生物拥有意图，并可以根据它们的欲望、信仰和目标做出决策。而人工智能模型则没有个人的欲望、信仰或目标。它们根据已学习的模式生成回应，没有任何固有的意图或目的。
- en: 'No self-awareness: Consciousness is often linked to self-awareness, where an
    entity is aware of its own existence and mental states. AI models do not possess
    self-awareness, as they are merely algorithms designed to process and generate
    text.'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 缺乏自我意识：意识常常与自我意识相联系，即一个实体意识到自己的存在和心理状态。人工智能模型不具备自我意识，因为它们仅仅是被设计用来处理和生成文本的算法。
- en: In conclusion, while an AI language model like me might be able to simulate
    conscious behavior to a high degree of fidelity, it doesn’t necessarily imply
    that I possess actual consciousness. The philosophical debate surrounding consciousness
    and AI is complex and ongoing, and opinions on this matter may vary. However,
    it’s crucial to recognize the differences between the appearance of consciousness
    and true consciousness when considering AI models.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，尽管像我这样的人工智能语言模型可能能够高度逼真地模拟意识行为，但这并不意味着我拥有真正的意识。围绕意识和人工智能的哲学辩论是复杂且持续的，对于这个问题的看法可能会有所不同。然而，在考虑人工智能模型时，认识到意识的表象与真正的意识之间的差异是至关重要的。
- en: GPT-4’s full reply is thorough. Reason 3, the absence of intentionality, is
    telling. I regard AI models without personal desires, beliefs, or goals to be
    advantageous. Entities without such things are not likely to want power, meaning,
    as they stand now, LLMs are unlikely to suddenly decide that humanity is a burden
    or a disease and wipe us out.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4的完整回答很全面。第三个原因，缺乏意图，尤其值得注意。我认为没有个人欲望、信仰或目标的人工智能模型是有优势的。没有这些东西的实体不太可能渴望权力，这意味着就目前而言，LLM（大规模语言模型）不太可能突然决定人类是负担或疾病，然后消灭我们。
- en: In a recent interview by Lex Fridman, Max Tegmark, a well-known AI researcher
    from MIT, compared GPT-4 to a zombie. It appears to be alive, but it isn’t. Zombie
    models concern Tegmark. I think they are what we want, as long as humanity learns
    to avoid the paper clip catastrophe.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在Lex Fridman的最新访谈中，麻省理工学院的著名人工智能研究员Max Tegmark将GPT-4比作僵尸。它看起来像活着的，但其实并不是。Tegmark对僵尸模型表示担忧。我认为，只要人类学会避免回形针灾难，它们就是我们所希望的。
- en: What I’m calling the paper clip catastrophe is a thought experiment proposed
    by Swedish philosopher Nick Bostrom. In the experiment, a powerful AI system is
    tasked with creating as many paper clips as possible. Bostrom speculates (not
    too seriously) that such a task given to an AI not aligned with human values could
    inadvertently destroy humanity. How? By the AI realizing that humanity might switch
    it off, thereby posing a threat to its order to make as many paper clips as possible.
    Therefore, the AI reasons, it’s best if no humans are around to interfere with
    the all-consuming task of making as many paper clips as possible. The result?
    Bye-bye humans.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我所称之为回形针灾难，是瑞典哲学家尼克·博斯特罗姆提出的一个思想实验。在这个实验中，一个强大的人工智能系统被任务要求尽可能多地制造回形针。博斯特罗姆（并非过于严肃地）推测，如果将这样的任务交给一个未与人类价值观对齐的人工智能，它可能会不经意间摧毁人类。那么，为什么会这样呢？因为人工智能可能意识到人类有可能关闭它，这就对其完成尽可能多回形针的任务构成了威胁。因此，人工智能推理出，如果周围没有人类干扰它这一消耗一切的任务，效果会更好。结果是什么？拜拜，人类。
- en: 'I don’t take the paper clip catastrophe too seriously either. We routinely
    build complex machines with all manner of safety precautions in place. Why wouldn’t
    we do the same for powerful AI systems? Other voices might disagree. For an alternative
    view, I recommend Stuart Russell’s book *Human Compatible: Artificial Intelligence
    and the Problem of Control* (Viking, 2019).'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我对回形针灾难也没有过于认真看待。我们通常都会构建复杂的机器，并采取各种安全措施。为什么我们不能对强大的人工智能系统采取同样的做法呢？当然，可能会有不同的声音。为了了解另一种观点，我推荐斯图尔特·拉塞尔的《*人类兼容：人工智能与控制问题*》一书（维京出版社，2019年）。
- en: To me, then, it doesn’t matter whether an AI is conscious. I don’t even know
    how to define the word, to be honest. I do believe that for an AI mimicking human
    behavior to the point where we cannot discern that it’s an AI, there’s no practical
    reason to ask the question. Choose any answer you like; such a system will be
    beneficial regardless.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 对我而言，人工智能是否具备意识并不重要。老实说，我甚至不知道如何定义这个词。我确实相信，如果一个人工智能模仿人的行为到我们无法辨别它是人工智能的程度，那么就没有实际的理由去问这个问题。你可以选择任何答案；这样的系统无论如何都会是有益的。
- en: '****'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '****'
- en: Imagine a world where AI models are aligned with human values and society, where
    the models understand the best we have to offer and work to promote that at all
    times; in other words, a world where AI, because it lacks our animal drives and
    instincts, consistently represents the “better angels of our nature,” to borrow
    Lincoln’s phrase. In that world, bias and prejudice, at least from the machines,
    are gone and no longer an issue. The AI recommends the best people for the position.
    The AI evaluates the loan applicant and constructs a loan product tailored to
    that individual’s circumstances. The AI is an adjunct to the human judge to provide
    an unemotional and unbiased view of the case. And the AI simply refuses to cooperate
    with the design of any autonomous weapon system because it is irrational to do
    so.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个人工智能模型与人类价值观和社会对齐的世界，在这个世界中，模型理解我们所拥有的最优之处，并始终致力于推广这些优点；换句话说，人工智能因为缺乏我们的动物欲望和本能，一直代表着“我们本性中的更好天使”，借用林肯的话。在那个世界里，偏见和歧视（至少来自机器的偏见和歧视）消失了，不再是问题。人工智能会推荐最合适的人选担任职位。人工智能评估贷款申请人，并根据个人情况设计量身定制的贷款产品。人工智能作为人类法官的助手，提供冷静客观、不偏不倚的案件评价。而且，人工智能根本拒绝参与任何自主武器系统的设计，因为这么做是不理性的。
- en: The previous paragraph may sound like utopia or a pipe dream. And, for humans,
    because of our biology, I believe it is. We consistently fail and always will,
    I suspect, because it’s in our genes to do so. However, what is dawning in AI
    isn’t human and doesn’t immediately inherit all of our weaknesses. (Careful, it
    is still trained on human-generated data.) Because of this, AI isn’t a priori
    doomed to failure when attempting what humanity cannot do. It seems entirely possible
    that AI systems might, someday, be precisely what we need—the best of us, always,
    without growing tired, becoming irritable, or crushing its neighbor to improve
    its position upon detecting an opportunity; something that is never unfaithful
    or untrue.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 上面这一段话可能听起来像是乌托邦或者空想。但对于人类来说，由于我们的生物学特性，我相信它是如此。我怀疑我们一直会失败，因为这似乎是我们基因中的一种特性。然而，人工智能的崛起并非人类的延续，它并不直接继承我们所有的弱点。（小心，它仍然是在基于人类生成的数据进行训练。）因此，人工智能并非在尝试人类无法做到的事情时注定要失败。看起来完全有可能，人工智能系统某天会正是我们所需要的——永远是最好的我们，永不疲倦、永不易怒，也不会在发现机会时压垮邻居来提高自己的位置；它永远不会不忠诚或不真实。
- en: Possible? I don’t know. Time will tell. Regardless, I fully expect future AI
    systems to be gloriously Byzantine evolutions of the basic neural network model
    we learned of and experimented with in this book. As of 2023, it’s all neurons
    and might remain so for a long time.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 可能吗？我不知道，时间会告诉我们答案。不管怎样，我完全期待未来的人工智能系统将是我们在本书中学习和实验的基本神经网络模型的辉煌拜占庭式演化。到2023年为止，一切都是神经元，可能在很长一段时间内都将如此。
- en: Thank you for persevering to the end. Your reward is an improved understanding
    of what AI entails. Artificial intelligence isn’t Mr. Bean, otherworldly and inscrutable,
    and it isn’t magic, though the emergent abilities of LLMs may appear to lean somewhat
    in that direction for now. Fire was once magical too, but our ancestors understood
    it, contained it, controlled it, and put it to work. We’ll do the same with large
    language models in the end.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢你坚持读到最后。你的奖励是对人工智能的概念有了更深的理解。人工智能并不像憨豆先生那样神秘难懂，也不是魔法，尽管大型语言模型的涌现能力目前可能看起来有些像魔法。火曾经也是魔法，但我们的祖先理解了它，控制了它，驾驭了它并加以利用。最终，我们也将对大型语言模型做同样的事情。
- en: '*I think that there is a lot of fear about robots and artificial intelligence
    among some people, whereas I’m more afraid of natural stupidity.*'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '*我认为一些人对机器人和人工智能有很多恐惧，而我更害怕的是自然的愚蠢。*'
- en: —Eugenia Cheng
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: —尤金妮亚·程
