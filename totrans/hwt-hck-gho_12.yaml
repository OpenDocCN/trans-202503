- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sticky Shell
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/book_art/chapterart.png)'
  prefs: []
  type: TYPE_IMG
- en: Persistence takes on a whole new dimension when dealing with a volatile and
    renewable infrastructure like Kubernetes. Containers and nodes tend to be treated
    as immutable and disposable objects that can vanish anytime, anywhere.
  prefs: []
  type: TYPE_NORMAL
- en: This volatility is further aggravated on AWS machines by the use of special
    types called *spot instances*. At about 40 percent of the regular price, companies
    can spawn a spot instance of almost any type available. The catch is that AWS
    has the power to reclaim the machine whenever it needs the compute power back.
    While this setup seems ideal for a Kubernetes cluster, where containers can be
    automatically moved to healthy machines and new nodes respawned in a matter of
    seconds, it does pose new challenges for reliable long-term backdoors.
  prefs: []
  type: TYPE_NORMAL
- en: Persistence used to be about backdooring binaries, running secret shells on
    machines, and planting Secure Shell (SSH) keys. None of these options provide
    stable, long-term access in a world where the average lifetime of a machine is
    a few hours.
  prefs: []
  type: TYPE_NORMAL
- en: The good news is using 100 percent spot instances for a cluster poses such a
    heavy risk that no serious company sets up such clusters—at least not to process
    critical workloads. If AWS suddenly spikes in reclaims, the cluster might fail
    to scale fast enough to meet customer demand. For this reason, a common strategy
    for cost-effective resilience is to have a stable part of critical workloads scheduled
    on a minimal base of regular instances and absorb traffic fluctuations with spot
    instances.
  prefs: []
  type: TYPE_NORMAL
- en: A lazy way to backdoor such a fluctuating infrastructure is to locate this set
    of precious machines—they’re usually the oldest ones in the cluster—and backdoor
    them using the old-fashioned methods. We could set up a cron job that regularly
    pulls and executes a reverse shell. We could use *binary planting*, where we replace
    common tools like `ls`, Docker, and SSHD with variants that execute distant code,
    grant root privileges, and perform other mischievous actions. We could insert
    a *rootkit*, which counts as any modification to the system (libraries, kernel
    structures, and so on) that allows or maintains access (check out a sample rootkit
    on Linux at [https://github.com/croemheld/lkm-rootkit/](https://github.com/croemheld/lkm-rootkit/)).
  prefs: []
  type: TYPE_NORMAL
- en: In [Listing 9-1](#listing9-1), we retrieve machines and order them by their
    creation timestamp.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 9-1: Finding the oldest nodes to locate the stable section of the cluster'
  prefs: []
  type: TYPE_NORMAL
- en: Each node supports different services, so backdooring a dozen of these nodes
    should give us at least a few days of guaranteed access. The shell will then automatically
    disappear with the node, burying any evidence of our shenanigans. It’s the perfect
    crime.
  prefs: []
  type: TYPE_NORMAL
- en: But what if a few days isn’t enough time to find a way to Gretsch Politico’s
    network? Can we persist longer somehow? We are, after all, in a setup that could
    adapt and heal itself. Wouldn’t it be magical if it healed our backdoor with it?
  prefs: []
  type: TYPE_NORMAL
- en: If we start thinking of our backdoor as a container or a pod, then maybe we
    can leverage the dark wizardry of Kubernetes to ensure that at least one copy
    is always up and running somewhere. The risk of such an ambition cannot be taken
    lightly, however. Kubernetes offers a ridiculous level of insights and metrics
    about all its components, so using an actual Kubernetes pod for our backdoor will
    make it a bit tricky to stay under the radar.
  prefs: []
  type: TYPE_NORMAL
- en: Persistence is always a game of trade-offs. Should we sacrifice stealth for
    more durable access or keep a very low profile and accept losing our hard-won
    shell at the slightest turbulence? To each their own opinion about the subject,
    which will depend on several factors like their confidence in the anonymity of
    the attacking infrastructure, the target’s security level, their risk appetite,
    and so forth.
  prefs: []
  type: TYPE_NORMAL
- en: 'This ostensibly impossible quandary has one obvious solution, though: multiple
    backdoors with different properties. We’ll have both a stable-yet-somewhat-plain
    backdoor and the stealthy-but-volatile shell. The first backdoor will consist
    of a pod cleverly hidden in plain sight that acts as our main center of operations.
    The pod will regularly beacon back home, looking for commands to execute. This
    also provides direct internet connection, which our current shell lacks. Whenever
    it gets destroyed for whatever reason, Kube will hurry to bring it back to life.
    Parallel to the first backdoor, we’ll drop another, stealthier program that hibernates
    until we send a predefined signal. This gives us a secret way back into the system
    should our first backdoor get busted by a curious admin.'
  prefs: []
  type: TYPE_NORMAL
- en: 'These multiple backdoors should not share any indicator of compromise: they
    will contact different IPs, use different techniques, run different containers,
    and be completely isolated from each other. An investigator who finds one seed
    with certain attributes should not be able to leverage this information to find
    other backdoors. The demise of one should not, in theory, put the others at risk.'
  prefs: []
  type: TYPE_NORMAL
- en: Stable Access
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The stable backdoor will be able to, for instance, run on a select few of the
    hundreds of nodes available. This rogue container will be a slim image that loads
    and executes a file at boot time. We’ll use *Alpine*, a minimal distribution of
    about 5MB commonly used to spin up containers.
  prefs: []
  type: TYPE_NORMAL
- en: In [Listing 9-2](#listing9-2), we start by writing the Dockerfile to download
    and run an arbitrary file within an Alpine container.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 9-2: A Dockerfile to build a container that downloads and runs an executable
    after booting'
  prefs: []
  type: TYPE_NORMAL
- en: Since MXR Ads is such a big fan of S3, we pull the future binary from an S3
    bucket we own, which we’ve treacherously called amazon-cni-plugin-essentials (more
    on the name later).
  prefs: []
  type: TYPE_NORMAL
- en: The binary (also called an *agent*) can be any of your favorite custom or boilerplate
    reverse shells. Some hackers may not even mind running a vanilla meterpreter agent
    on a Linux box. As stated in Chapter 1, the attacking framework we’ve built is
    reliable and stable, and few companies bother to invest in costly endpoint detection
    response solutions to protect their Linux servers, especially in ephemeral machines
    in a Kubernetes cluster. That makes off-the-shelf exploitation frameworks like
    Metasploit a reasonable option.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, we’ll stay on the side of caution and take a few seconds to build
    a reliable payload unlikely to trip over hidden wires.
  prefs: []
  type: TYPE_NORMAL
- en: We head to our lab and generate a stageless vanilla HTTPS meterpreter. A stageless
    payload is one that is fully self-contained and doesn’t need to download additional
    code from the internet to start. The meterpreter is directly injected into the
    executable *.text* section of the ELF/PE binary of our choosing (provided the
    template file has enough space for it). In [Listing 9-3](#listing9-3), we choose
    the */bin/ls* binary as a template and sneak the reverse shell into it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 9-3: Embedding a meterpreter inside a regular */bin/ls* executable'
  prefs: []
  type: TYPE_NORMAL
- en: Simple enough. Now, instead of running this file from disk like any classic
    binary, we would like to trigger its execution exclusively from memory to thwart
    potential security solutions. Had the payload been a regular shellcode instead
    of a literal binary file, we would only have needed to copy it to a read/write/execute
    memory page and then jump to the first byte of the payload.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, since our `meterpreter_reverse_https` payload produces a full ELF
    binary file, reflectively loading it in memory requires a bit of extra work: we
    have to manually load imported DLLs and resolve local offsets. Check the resources
    at the end of the chapter for more on how to handle this. Thankfully, Linux 3.17
    introduced a syscall tool that provides a much quicker way of achieving the same
    result: *memfd*.'
  prefs: []
  type: TYPE_NORMAL
- en: This syscall creates a virtual file that lives entirely in memory and behaves
    like any regular disk file. Using the virtual file’s symbolic link */proc/self/fd/<id>*,
    we can open the virtual file, alter it, truncate it, and, of course, execute it!
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the five main steps to carry out this operation:'
  prefs: []
  type: TYPE_NORMAL
- en: Encrypt the vanilla meterpreter payload using an XOR operation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Store the result in an S3 bucket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Craft a stager that will download the encrypted payload over HTTPS on the target
    machine.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decrypt the payload in memory and initialize an “anonymous” file using the memfd
    syscall.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy the decrypted payload into this memory-only file and then execute it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Listing 9-4](#listing9-4) is an abridged walkthrough of the main steps our
    stager will take—as usual, the full code is hosted on GitHub.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 9-4: High-level actions of the stager'
  prefs: []
  type: TYPE_NORMAL
- en: That’s about it. We don’t need to do any obscure offset calculations, library
    hot-loading, patching of procedure linkage table (PLT) sections, or other hazardous
    tricks. We have a reliable stager that executes a file exclusively in memory and
    that is guaranteed to work on any recent Linux distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'We compile the code and then upload it to S3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, to further enhance the web of deceit, when we build the container’s
    image and push it to our own AWS ECR registry (ECR is the equivalent of Docker
    Hub on AWS), we do so under the guise of a legitimate Amazon container, amazon-k8s-cni:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The names of the fake container (amazon-k8s-cni) and S3 bucket (amazon-cni-plugin-essentials)
    are not arbitrary choices. EKS runs a copy of a similar container on every single
    node to manage the network configuration of pods and nodes, as we can see if we
    grab a list of pods from any running cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: These pods named aws-node-*xxxx* are running the official `amazon-k8s-cni` image
    hosted on AWS’s own repository.
  prefs: []
  type: TYPE_NORMAL
- en: These pods were created by a *DaemonSet* object, a Kubernetes resource that
    maintains at least one copy of a given pod constantly running on all (or some)
    nodes. Each of these aws-node pods is assigned a service account with read-only
    access to all namespaces, nodes, and pods. And to top it all off, they all automatically
    mount */var/run/docker.sock*, giving them root privileges on the host. It is the
    perfect cover.
  prefs: []
  type: TYPE_NORMAL
- en: We will spawn an almost exact copy of this DaemonSet. Unlike the real one, however,
    this new DaemonSet will fetch its `amazon-k8s-cni` pod image from our own ECR
    repository. A DaemonSet runs by default on all machines. We do not want to end
    up with thousands of reverse shells phoning home at once, so we will only target
    a few nodes—for instance, the three bearing the “kafka-broker-collector” label.
    This is a good population size for our evil DaemonSet.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command displays machine names along with their labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We have chosen our targets. Our payload is locked and ready. The next step is
    to create the DaemonSet object.
  prefs: []
  type: TYPE_NORMAL
- en: No need to go looking for the YAML definition of a DaemonSet; we just dump the
    DaemonSet used by the legitimate aws-node, update the container image field so
    it points to our own repository, alter the display name (aws-node-cni instead
    of aws-node), change the container port to avoid conflict with the existing DaemonSet,
    and finally add the label selector to match kafka-broker-collector. In [Listing
    9-5](#listing9-5), we resubmit the newly changed file for scheduling.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 9-5: Creating our own fake DaemonSet'
  prefs: []
  type: TYPE_NORMAL
- en: After a few `sed` commands, we have our updated manifest ready to be pushed
    to the API server.
  prefs: []
  type: TYPE_NORMAL
- en: 'Meanwhile, we head back to our Metasploit container to set up a listener serving
    a payload of type `meterpreter_reverse_https` on port 443, as shown next. This
    payload type is, of course, the same one we used in the `msfvenom` command at
    the beginning of this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We push this updated manifest to the cluster, which will create the DaemonSet
    object along with the three reverse shell containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Awesome. Nodes can break down and pods can be wiped out, but so long as there
    are nodes bearing the label kafka-collector-broker, our evil containers will be
    scheduled on them time and time again, resurrecting our backdoor. After all, who
    will dare question Amazon-looking pods obviously related to a critical component
    of the EKS cluster? Security by obscurity may not be a winning defense strategy,
    but it’s a golden rule in the offensive world.
  prefs: []
  type: TYPE_NORMAL
- en: The Stealthy Backdoor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our stable backdoor is very resilient and will survive node termination, but
    it’s a bit loud. The pod and DaemonSet are constantly running and visible on the
    cluster. We therefore complement this backdoor with a stealthier one that only
    fires up occasionally.
  prefs: []
  type: TYPE_NORMAL
- en: We set up a cron job at the cluster level that runs every day at 10 AM to bring
    a pod to life. We’ll use a different AWS account than the one present in the DaemonSet
    to make sure we’re not sharing data or techniques between our backdoors. [Listing
    9-6](#listing9-6) shows the manifest file of the cron job.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 9-6: The cron job for our stealthy backdoor'
  prefs: []
  type: TYPE_NORMAL
- en: This cron job loads the `amazon-metrics-collector` image from yet another AWS
    account we control. This Docker image has a thicker structure and may even pass
    for a legit metrics job (see [Listing 9-7](#listing9-7)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 9-7: A Dockerfile installing a number of packages and executing a script
    on startup'
  prefs: []
  type: TYPE_NORMAL
- en: Behind the façade of useless packages and dozens of dummy lines of code, deep
    inside *init.sh*, we place an instruction that downloads and executes our custom
    script hosted on S3\. At first, this remote script will be a harmless dummy `echo`
    command. The moment we want to activate this backdoor to regain access to the
    system, we overwrite the file on S3 with our custom meterpreter. It’s a sort of
    dormant shell that we only use in case of emergency.
  prefs: []
  type: TYPE_NORMAL
- en: This setup, however, will not completely solve the original problem of visibility.
    Once we activate our shell, we will have a pod constantly running on the system,
    visible to every Kube admin.
  prefs: []
  type: TYPE_NORMAL
- en: One optimization is to avoid executing our custom stager directly on the foreign
    container metrics-collector pod. Instead, we will use this pod to contact the
    Docker socket that we so conveniently mounted and instruct it to start yet another
    container on the host, which will in time load the meterpreter agent. The metrics-collector
    pod, having done its job, can gracefully terminate, while our shell remains running
    unhindered in its own second container.
  prefs: []
  type: TYPE_NORMAL
- en: This second container will be completely invisible to Kubernetes since it is
    not attached to an existing object like a ReplicaSet or DaemonSet, but was defiantly
    created by Docker on a node. This container will silently continue running in
    privileged mode with minimal supervision. [Listing 9-8](#listing9-8) gives the
    three `curl` commands to pull, create, and start such a container through the
    Docker API. This script should be loaded and executed by the amazon-metrics-collector
    container we defined earlier.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 9-8: A script to pull a new Docker image, create the container, and
    start it'
  prefs: []
  type: TYPE_NORMAL
- en: To further conceal our rogue container, we smuggle it among the many *pause*
    *containers* that are usually running on any given node. The pause container plays
    a key role in the Kubernetes architecture, as it’s the container that inherits
    all the namespaces assigned to a pod and shares them with the containers inside.
    There are as many pause containers as there are pods, so one more will hardly
    raise an eyebrow.
  prefs: []
  type: TYPE_NORMAL
- en: At this stage, we have a pretty solid foothold on the Kubernetes cluster. We
    could go on spinning processes on random nodes in case someone destroys our Kube
    resources, but hopefully by that time we’ll already have finished our business
    anyway.
  prefs: []
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For more information about meterpreter payloads, search for the article “Deep
    Dive into Stageless Meterpreter Payloads” by OJ Reeves on [https://blog.rapid7.com/](https://blog.rapid7.com/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For a thorough article about the power of `memcpy` and `mprotect` for shellcode
    execution, see “Make Stack Executable Again” by Shivam Shrirao: [http://bit.ly/3601dxh](http://bit.ly/3601dxh).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The ReflectiveELFLoader by @nsxz provides a proof of concept: [https://github.com/nsxz/ReflectiveELFLoader/](https://github.com/nsxz/ReflectiveELFLoader/).
    The code is well documented but requires some knowledge of ELF headers; see [https://0x00sec.org/t/dissecting-and-exploiting-elf-files/7267/](https://0x00sec.org/t/dissecting-and-exploiting-elf-files/7267/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A compilation of memory-only execution methods on Linux can be found at [http://bit.ly/35YMiTY](http://bit.ly/35YMiTY).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Memfd was introduced in Linux kernel 3.17\. See the manual page for`memfd_create`:
    [http://bit.ly/3aeig27](http://bit.ly/3aeig27).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information about DaemonSets, see the Kubernetes documentation:[http://bit.ly/2TBkmD8](http://bit.ly/2TBkmD8).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For help with Docker, see the API docs: [https://dockr.ly/2QKr1ck](https://dockr.ly/2QKr1ck).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
