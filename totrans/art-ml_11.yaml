- en: '**8'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PARAMETRIC METHODS**
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Recall the term *regression function*, first introduced in [Section 1.6](ch01.xhtml#ch01lev6)
    and denoted by *r*(*t*). It’s the mean *Y* in the subpopulation defined by the
    condition *X* = *t*. The example we gave then involved bike ridership data:'
  prefs: []
  type: TYPE_NORMAL
- en: A regression function has as many arguments as we have features. Let’s take
    humidity as a second feature, for instance. To predict ridership for a day with
    temperature 28 and humidity 0.51, we would use the mean ridership in our dataset,
    among days in which temperature and humidity are approximately 28 and 0.51\. In
    regression function notation, that’s *r*(28, 0.51).
  prefs: []
  type: TYPE_NORMAL
- en: Basically, ML methods all are techniques to estimate the regression function
    from sample data. With k-NN, we would estimate *r*(28, 0.51) in the bike ridership
    example by calculating the mean ridership among the days in the neighborhood of
    (28,0.51). With trees, we would plug (28,0.51) into our tree, follow the proper
    branches, and then calculate the mean ridership in the resulting leaf node, which
    acts like a neighborhood.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have not made any assumptions about the shape of the regression function
    graph. In this chapter, we will assume the shape is that of a straight line, or
    planes and so on in higher dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: The so-called *linear model* is quite old, a couple of centuries old, actually.
    It can work fairly well in “easy” prediction applications, and even in some “advanced”
    ones. Indeed, we will see in [Section 8.13](ch08.xhtml#ch08lev13) that a variant
    of the linear model can often outperform more sophisticated ML models.
  prefs: []
  type: TYPE_NORMAL
- en: The linear model should thus be in every analyst’s toolkit. But an even more
    compelling reason to know the linear model is that it forms the basis of some
    of the most popular and powerful ML algorithms, including the LASSO, support vector
    machines, and neural networks, which we will cover in the succeeding chapters
    of this book.
  prefs: []
  type: TYPE_NORMAL
- en: '8.1 Motivating Example: The Baseball Player Data'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ll soon introduce the `qe*`-series function for linear models, `qeLin()`.
    But to understand what it does, let’s start with a simple setting in which we
    have only one feature and use it to motivate the concept of a linear model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall the dataset `mlb` in [Section 1.8](ch01.xhtml#ch01lev8) that is included
    with `regtools`. Let’s restrict attention to just heights and weights of the players:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Here, *X* and *Y* will be height and weight, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '***8.1.1 A Graph to Guide Our Intuition***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: So, we are predicting weight from height. In the *r*() notation, that means
    that if we wish to predict the weight of a new player whose height is 71 inches,
    we need to estimate *r*(71). This is the mean weight of all players in the subpopulation
    of players having height 71.
  prefs: []
  type: TYPE_NORMAL
- en: 'We don’t know population values, as we only have a sample from the population.
    (As noted earlier, we consider our data to be a sample from the population of
    all players, past, present, and future.) How, then, can we estimate *r*(71)? The
    natural estimate is the analogous sample quantity, the mean weight of all height
    71 players in our sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Recalling that the “hat” notation means “estimate of,” we have that ![Image](../images/rcap1.jpg)(71)
    = 190.3596\. With deft usage of R’s `tapply()` function, we can get all the estimated
    *r*() values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This says, “Group the weight values by height, and find the mean weight in
    each group.” By the way, note that the heights are available as the names of the
    weight items:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s plot the estimated mean weights against height:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 8-1](ch08.xhtml#ch08fig01) shows the result.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch08fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 8-1: Estimated regression function, weight vs. height*'
  prefs: []
  type: TYPE_NORMAL
- en: Remarkably, the points seem to nearly lie on a straight line. This suggests
    a model for *r*(*t*),
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch08equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: for some unknown values of the slope *m* and intercept *b* that we will estimate
    from the data. We are assuming that the graph of *r*(*t*) is *some* straight line,
    though we don’t know which one—that is, we don’t know *b* and *m*. This is the
    linear model.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind *r*(*t*) is the *mean Y* for the subpopulation *X* = *t*, so we
    are modeling *mean Y* and not *Y* itself. We are not saying [Equation 8.1](ch08.xhtml#ch08equ01)
    gives us the weight of individual players, though we do use the equation as the
    basis of our predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '***8.1.2 View as Dimension Reduction***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If [Equation 8.1](ch08.xhtml#ch08equ01) is a valid model, we have greatly simplified
    our problem.
  prefs: []
  type: TYPE_NORMAL
- en: Ordinarily, we would need to estimate many different values of *r*(*t*), such
    as those for *t* equal to 68, 69, 70, 71, 72, 73, and so on, say, 15 or 20 of
    them. But with the above model, *we need to estimate only two numbers*, *m* and
    *b*. As such, this is a form of dimension reduction.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2 The lm() Function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Assuming the linear model (again, we’ll address its validity shortly), we can
    use R’s `lm()` function to estimate *m* and *b*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: So, ![Image](../images/unch08equ01.jpg) and ![Image](../images/unch08equ02.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see what this call is saying:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Here we are requesting that R fit a linear model to our data frame `hw`, predicting
    weight. The dot (.) means “all other columns,” which, in this case, is just the
    height column.
  prefs: []
  type: TYPE_NORMAL
- en: 'To predict the weight of a new player of height 71, we would compute:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch08equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'But hey, we should have the computer do this computation rather than do it
    by hand:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The slight discrepancy is due to a roundoff error in the computation by hand,
    where our data was given only to a few digits.
  prefs: []
  type: TYPE_NORMAL
- en: '8.3 Wrapper for lm() in the qe*-Series: qeLin()'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `lm()` function is so basic in R that everyone should see it at least once,
    so we used it in the last section. But for simplicity and uniformity, we will
    use its `qe*`-series wrapper, `qeLin()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how to do the above computations in `qeLin()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Most applications have more than just one feature. We cover the general case
    next.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4 Use of Multiple Features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can, and typically do, fit the model to more than one feature.
  prefs: []
  type: TYPE_NORMAL
- en: '***8.4.1 Example: Baseball Player, Continued***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Say we add in age, so our linear model is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch08equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For the purpose of terminology (used here and later), let’s write this as
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch08equ04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where *D* is an artificial variable that is always equal to 1\. Then we say
    that mean weight is a *linear combination* of the variables *D*, *height*, and
    *age*. This is just a term meaning that to get mean weight, we multiply each of
    the three variables *D*, *height*, and *age* by the corresponding coefficients
    *b*, *m*[1], and *m*[2] and sum up the result.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now are using columns 4, 5, and 6 of `mlb`, so we fit the model as follows,
    say, for age 28:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '***8.4.2 Beta Notation***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Since the reader of this book will likely see other discussions, say, on the
    web, it should be mentioned that it’s traditional to use the Greek letter *β*
    for the coefficients. For instance, [Equation 8.3](ch08.xhtml#ch08equ03) would
    be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch08equ05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We will estimate *β*[0], *β*[1], and *β*[2] from our sample data, as seen in
    [Section 8.4.4](ch08.xhtml#ch08lev4sec4). And, recalling that we use the hat notation
    for estimates, our estimated coefficients will be denoted by ![Image](../images/unch08equ03.jpg)
    and ![Image](../images/unch08equ04.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: '***8.4.3 Example: Airbnb Data***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The short-term housing firm Airbnb makes available voluminous rental data. Here
    we look at some data from San Francisco.^([1](footnote.xhtml#ch8fn1)) (The dataset
    used here, from February 1, 2019, appears to no longer be available.) It will
    not only provide another example of the linear model, but it also will illustrate
    some data-cleaning issues.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4.3.1 Data Preparation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: After downloading the data and reading it into R (details not shown), we had
    a data frame `Abb`, which still required a lot of attention.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many of the features are textual, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We will treat the topic of text data later in this book but removed it for this
    example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another problem is that prices include dollar signs and commas, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Dealing with such issues tends to take up a remarkably large portion of a data
    scientist’s job. Here we wrote a function to convert a column `d` of such numbers
    to the proper form, using a couple of R’s character string manipulation facilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'And, not surprisingly, this dataset seems to have its share of erroneous entries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: For instance, areas of 1 and 2 square feet are listed, obviously incorrect.
    We will not pursue this further here, but clearly we would have a lot more work
    to do if this were not merely an example for the book.
  prefs: []
  type: TYPE_NORMAL
- en: 'After data cleaning, the data frame looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We are now ready to perform the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: '***8.4.4 Applying the Linear Model***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here is the call, omitting the square footage and weekly price columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: As is common in R, the estimated coefficients are displayed here in *scientific
    notation*, in which, for instance, 1.605326*e* + 03 = 1.605326 × 10³ = 1605.326\.
    So, for instance, ![Image](../images/unch08equ05.jpg) is about −4,486, ![Image](../images/unch08equ06.jpg)
    is about −444, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Note that `lm()` (via its wrapper `qeLin()`) has converted the ZIP code feature,
    an R factor, to dummy variables. Recall that typically we have one fewer dummy
    than the number of categories—in this case, the number of ZIP codes. R leaves
    out the first one here, which is 94102.
  prefs: []
  type: TYPE_NORMAL
- en: Since our focus in this book is on prediction rather than causal interpretation,
    the estimated coefficients are of lesser interest. Furthermore, one must be very
    careful in interpreting coefficients. Nevertheless, some comments regarding the
    coefficients are in order, next.
  prefs: []
  type: TYPE_NORMAL
- en: 8.5 Dimension Reduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s discuss this fundamental ML topic in the context of linear models and
    the Airbnb example in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: '***8.5.1 Which Features Are Important?***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As noted in [Section 3.1.1](ch03.xhtml#ch03lev1sec1), there are more than 40,000
    ZIP codes in the United States; this is typically far too many to use directly.
    In San Francisco, the number is manageable, but still we may wish to drop the
    ones that seem unimportant to our predictions.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, as real estate agents say, “Location, location, location.”
    ZIP code should matter a lot, and the estimated coefficients at least seem to
    confirm this. For instance, according to the coefficient estimates given earlier,
    a property in ZIP code 94105, on average, commands a price premium of about $1,012,
    while one in 94107 will, on average, cost about $285 below market, holding all
    other variables fixed. But what are the terms *premium* and *cost less* relative
    to here? Since ZIP code 94102 was omitted, we see that 94105 costs about $1,012
    more on average than 94102—the ZIP code term in ![Image](../images/rcap1.jpg)(*t*)
    would be about 1 · 1012 for a property in that ZIP code, while it would be 0 for
    one in 94102, since there is no dummy variable for that ZIP code. Similarly, 94107
    runs about $444 below 94102, and so on. In other words, 94102 becomes the baseline
    ZIP code.
  prefs: []
  type: TYPE_NORMAL
- en: 'But . . . note the phrasing above: “the estimated coefficients at least *seem*
    to confirm this.” After all, we are working with *estimates* of finite accuracy.
    This is a vital point to take into account, which we will do next.'
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the amount of `security_deposit` seems not to matter much
    at all, so we should consider dropping it from our analysis. Recall that having
    more features means less bias but more variance. Since the effect of a security
    deposit in prediction values seems small, dropping this feature should add very
    little bias. The same statement holds for the features `minimum_nights` and `maximum_nights`.
  prefs: []
  type: TYPE_NORMAL
- en: '***8.5.2 Statistical Significance and Dimension Reduction***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the previous section, we suggested several features to drop from our analysis.
    But we did so only on the basis of a “feeling.” It is natural to desire some magic
    formula that will determine which features to retain and which to remove. But
    alas, as has been explained in this book, no such magic formula exists. We have
    cited a few methods, such as cross-validation and PCA, that are commonly used,
    but again, these are not magic, foolproof solutions.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we look at the use of *statistical significance* for dimension
    reduction in parametric models. *We do not recommend it*, and it is less favored
    than in the past, but it is still popular among many analysts. Thus, it is imperative
    to cover the technique here.
  prefs: []
  type: TYPE_NORMAL
- en: First, we will need to introduce a new R generic function ([Section 1.5.1](ch01.xhtml#ch01lev5sec1)).
    In addition to `print()`, `plot()`, and `predict()`, another common generic function
    in R is `summary()`. It does what its name implies; that is, it provides a summary
    of the object.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that a generic function is tailored to the class of the object at hand.
    What is the class of our object here, `linout`?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'So, if we make the call `summary(linout)`, the R interpreter will first check
    for a function `summary.qeLin()`. Since the `qeML` package has no such function,
    the interpreter will next look for `summary.lm()`, which does exist. Let’s see
    what the function gives us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: One particular type of information computed here is standard errors, discussed
    next.
  prefs: []
  type: TYPE_NORMAL
- en: 8.5.2.1 Standard Errors
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: You can see above that a *standard error* is reported for each estimated coefficient
    ![Image](../images/unch08equ07.jpg). It’s the estimated standard deviation of
    ![Image](../images/unch08equ07.jpg) over all possible samples for whatever population
    is being sampled. This gives us an idea as to how accurate ![Image](../images/unch08equ07.jpg)
    is, using the following reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: If the standard error is small, it says that if we had had a different set of
    sample data from the given population, ![Image](../images/unch08equ07.jpg) probably
    would have come out to about the same value as what we got. In other words, we
    can treat ![Image](../images/unch08equ07.jpg) as representative.
  prefs: []
  type: TYPE_NORMAL
- en: We can form an approximate 95 percent confidence interval (CI) for *β*[*i*]
    by adding and subtracting 1.96 times the standard error of ![Image](../images/unch08equ07.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, consider the dummy variable `zipcode94134`. The estimated beta
    coefficient for this variable is −$1,370\. This is relative to whichever ZIP code
    is the base, meaning the one for which there is no dummy variable. (Recall from
    [Section 1.4](ch01.xhtml#ch01lev4) that with a categorical feature, we have one
    fewer dummy than the number of categories.) As noted earlier, the omitted ZIP
    code is 94102\. So, for a given security deposit, guest policy, and so on, this
    neighborhood is estimated to be more than $1,000 cheaper than the baseline. But
    look at the CI:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch08equ06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The CI suggests that this neighborhood actually could be hundreds of dollars
    more *expensive* than the base.
  prefs: []
  type: TYPE_NORMAL
- en: 8.5.2.2 Significance Tests
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: That last example suggests that the status of `zipcode94134` as a predictor
    of rent is inconclusive. We should thus seriously consider dropping it from our
    model. Remember the notion of the Bias-Variance Trade-off means that if a feature
    is not very helpful, then including it in our model may degrade our predictive
    ability.
  prefs: []
  type: TYPE_NORMAL
- en: But let’s consider another ZIP code, say, 94132\. Here the CI is
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch08equ07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: which is entirely in negative territory. For this reason, it is flagged with
    an asterisk.
  prefs: []
  type: TYPE_NORMAL
- en: What do all those asterisks mean? Why are there double asterisks for some coefficients?
    Our focus in this book is not on statistics, but it is important for the reader
    to have at least an overview of the situation since it is common to use the asterisks
    as a guide for dimension reduction.
  prefs: []
  type: TYPE_NORMAL
- en: Roughly speaking, if the CI does not contain 0, the coefficient is flagged with
    an asterisk. It rates *two* asterisks if 0 is well outside the interval and three
    if the CI is far, far away from 0\. A coefficient with one asterisk is termed
    *significant* (that is, significantly different from 0); one with two asterisks
    is called *highly significant*, and three asterisks wins a coefficient the accolade
    *very highly significant*.
  prefs: []
  type: TYPE_NORMAL
- en: Well then, what constitutes “well outside the interval” and “far, far away from
    0”? This is determined by the p-value. A p-value under 0.05 is significant, and
    it is highly or very highly significant if it is under 0.01 or 0.001, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: The p-value is a certain probability whose convoluted definition we will skip.
    (Recall this term from [Section 5.5](ch05.xhtml#ch05lev5).) Suffice it to say,
    under this approach to dimension reduction, one discards any feature with no asterisks,
    such as `zipcode94134`, and retains the others in the model. If one wants to exercise
    a little more caution, one might retain only the coefficients with at least two
    asterisks.
  prefs: []
  type: TYPE_NORMAL
- en: Today, many analysts, including myself, consider this approach to be flawed.
    Let’s see why. The short answer is that p-values are too dependent on the number
    of data points *n*. Actually, the standard error is inversely proportional to
    ![Image](../images/unch08equ08.jpg). This has quite an implication, as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose, hypothetically, that the estimated coefficient for, say, `zipcode94132`
    had been 1.4, with a standard error of 0.9\. That would give us a confidence interval
    of:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch08equ08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This contains 0, hence no asterisks. And that’s probably a good thing, since
    this feature seems to have no real predictive power: being in that ZIP code makes
    an estimated difference in rent of only a dollar or so.'
  prefs: []
  type: TYPE_NORMAL
- en: 'But what if we were fortunate to have 25 times as much data? Then ![Images](../images/unch08equ08.jpg)
    would increase by a factor of 5, so the standard error would shrink by a factor
    of 5, coming out at approximately 0.18\. It would change somewhat, as would the
    coefficient estimate 1.4, but in rough terms our CI would now be something like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch08equ09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Ah, now it’s significant! Yay! But . . . the estimated coefficient would still
    be something like $1.40—less than $2! That variable can hardly help us predict
    rent. In other words, the so-called significant nature of that feature could really
    lead us astray.
  prefs: []
  type: TYPE_NORMAL
- en: Use of significance tests and p-values is frowned upon by many statisticians
    (including this author).^([2](footnote.xhtml#ch8fn2)) The tests are especially
    unreliable in prediction applications. With large datasets, *every* feature will
    be declared “very highly significant” (three asterisks) regardless of whether
    the feature has substantial predictive power. A feature with a very small regression
    coefficient could be declared “significant,” in spite of being essentially useless
    as a predictor.
  prefs: []
  type: TYPE_NORMAL
- en: '8.5.2.3 Pitfall: NA Values and Impact on n'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As shown above, this dataset also includes a number of NA values. We didn’t
    have to deal with this directly, since `lm()`, which `qeLin()` wraps, automatically
    restricts its computations to complete cases. Nevertheless, as noted in [Section
    4.1](ch04.xhtml#ch04lev1), if a dataset contains many NAs, this is yet another
    reason to seek dimension reduction, as doing so may increase the number of complete
    cases. This means less variance, which is very desirable. Some experimentation
    here reveals that removal of the most NA-prone features, beyond the ones we’ve
    already deleted, does not help increase data size in this particular case, but
    it is an important general principle.
  prefs: []
  type: TYPE_NORMAL
- en: '8.5.2.4 Pitfall: Difficulty of Forming Holdout Sets with Many-Level Categoricals'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In our earlier Airbnb analysis, problems occur if we form a holdout set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Of course, since the holdout set is chosen randomly, there may be a different
    result each time. But we see that in each of our two tries here, we had at least
    one error, “factor zipcode has new levels.” What is happening here?
  prefs: []
  type: TYPE_NORMAL
- en: The problem is that certain ZIP codes, such as 94014, appear in only a few data
    points. Apparently there were no 94014 cases in each training set here, so lm()
    *was “surprised” to see one in the holdout set.*
  prefs: []
  type: TYPE_NORMAL
- en: The only solution would be to remove all cases with 94014 (and possibly others)
    from the data before running `qeLin()`.
  prefs: []
  type: TYPE_NORMAL
- en: 8.6 Least Squares and Residuals
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Even though the computational details underlying `lm()` are beyond the scope
    of this book, it’s important to have a rough idea of what is involved, as similar
    computations will arise later in the book. This will bring in the notion of *least
    squares*. That, in turn, will lead to the idea of *residuals*, which are important
    in their own right.
  prefs: []
  type: TYPE_NORMAL
- en: For simplicity, let’s consider the context of [Section 8.2](ch08.xhtml#ch08lev2)
    here. The quantities ![Image](../images/unch08equ09.jpg) and ![Image](../images/unch08equ10.jpg)
    and so on are computed using the famous *ordinary least squares (OLS)* method,
    which works as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine that after we compute ![Image](../images/unch08equ09.jpg) and ![Image](../images/unch08equ10.jpg),
    we go back and “predict” the weight of the first player in our sample data. As
    implied by the quotation marks, this would be silly; after all, we already know
    the weight of the first player, 180:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: But think through this exercise anyway. It will turn out to be the basis for
    how things work, both for linear models and all the ML methods in the remainder
    of the book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our predicted value would be ![Image](../images/unch08equ11.jpg). Thus our
    prediction error would be:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/unch08equ12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This is the *residual* for that row in the dataset. (Recall that this was briefly
    mentioned in [Section 6.3.2](ch06.xhtml#ch06lev3sec2).) We’ll square that error
    rather than using it in its raw form, as we will be summing errors and don’t want
    positive and negative ones to cancel. Now we “predict” all the other data points
    as well, and add up the squared errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch08equ10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now here is the point: the way `lm()` finds ![Image](../images/unch08equ10.jpg)
    and ![Image](../images/unch08equ09.jpg) is to set them to whatever values minimize
    the sum of squares ([Equation 8.10](ch08.xhtml#ch08equ10)). In other words, think
    of the expression as a function of two variables, ![Image](../images/unch08equ10.jpg)
    and ![Image](../images/unch08equ09.jpg), and then minimize the expression with
    respect to those two variables. (Readers who know calculus may have spotted the
    fact that we set the two derivatives equal to 0 and solve for ![Image](../images/unch08equ10.jpg)
    and ![Image](../images/unch08equ09.jpg).)'
  prefs: []
  type: TYPE_NORMAL
- en: Since we are minimizing a sum of squares, the estimated coefficients are said
    to be the *least squares estimates*. (The word *ordinary* is often added, as ordinary
    least squares is distinct from some variants that we will not discuss here.)
  prefs: []
  type: TYPE_NORMAL
- en: '8.7 Diagnostics: Is the Linear Model Valid?'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*All models are wrong, but some are useful.*'
  prefs: []
  type: TYPE_NORMAL
- en: —George Box, famous early statistician
  prefs: []
  type: TYPE_NORMAL
- en: The linearity assumption is pretty strong. When is it appropriate? Let’s take
    a closer look.
  prefs: []
  type: TYPE_NORMAL
- en: '***8.7.1 Exactness?***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The reader may ask, “How can the linear model in [Equation 8.1](ch08.xhtml#ch08equ01)
    be valid?” Yes, the points in [Figure 8-1](ch08.xhtml#ch08fig01) look like they
    are kind of on a straight line, but not exactly so. There are two important answers:'
  prefs: []
  type: TYPE_NORMAL
- en: As the quote from George Box points out, no model is *exactly* correct. Commonly
    used physics models ignore things like air resistance and friction, and even models
    accounting for such things still don’t reflect all possible factors. A linear
    approximation to the regression function *r*(*t*) may do a fine job in prediction
    even if the model is not perfect.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even if [Equation 8.1](ch08.xhtml#ch08equ01) were exactly correct, the points
    in [Figure 8-1](ch08.xhtml#ch08fig01) would not lie exactly on the line. Remember,
    *r*(71), for instance, is only the *mean* weight of all players of height 71\.
    Most individual players of that height are heavier or lighter than that value,
    so their data points will not fall exactly on that line and, in fact, may be far
    from it in some cases. And the same point holds for the mean weights that we plotted
    in [Figure 8-1](ch08.xhtml#ch08fig01); each of those means were based on only
    a few players.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the way, classical linear model methodology makes some assumptions beyond
    linearity, such as *Y* having a normal distribution in each subpopulation. But
    these are not relevant to our prediction context. (Actually, even for statistical
    inference, the normality assumption is not important in large samples.)
  prefs: []
  type: TYPE_NORMAL
- en: '***8.7.2 Diagnostic Methods***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Over the years, analysts have developed a number of methods to check the validity
    of the linear model. Several are described in my book *Statistical Regression
    and Classification: From Linear Models to Machine Learning* (CRC Press, 2017).'
  prefs: []
  type: TYPE_NORMAL
- en: Again, since we are interested in prediction rather than causal analysis, we
    will not cover this material here. As long as the outcome variable is an increasing
    or decreasing function of the features—for example, mean human weight is an increasing
    function of height—a linear model should do fairly well in prediction-oriented
    applications. With linear polynomial models (see [Section 8.11](ch08.xhtml#ch08lev11)),
    this can be refined.
  prefs: []
  type: TYPE_NORMAL
- en: 8.8 The R-Squared Value(s)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recall that the estimated coefficients are calculated by minimizing the sum
    of squared differences between actual and predicted *Y* values (see [Section 8.6](ch08.xhtml#ch08lev6)).
    *R*² is the squared correlation between actual and predicted *Y*.
  prefs: []
  type: TYPE_NORMAL
- en: It can be shown that this can be interpreted as the proportion of variation
    in *Y* due to *X*. (As always, *X* refers collectively to all our features.) As
    such, we have 0 ≤ *R*² ≤ 1, with a value of 1 meaning that *X* perfectly predicts
    *Y*. However, there is a big problem here, as we are predicting the same data
    that we used to estimate our prediction machine, the regression coefficients.
    If we are overfitting, then *R*² will be overly optimistic.
  prefs: []
  type: TYPE_NORMAL
- en: This, of course, is the motivation for using holdout data. Thus `qeLin()` reports
    not only the standard *R*² but also the *R*² calculated on the holdout set (stored
    in the `holdoutR2` component of the `qeLin()` return value). The latter is more
    reliable. Furthermore, if there is a large discrepancy between the two, it suggests
    that we are overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Most linear regression software libraries also report the *adjusted R*² value.
    The word *adjusted* here alludes to the fact that the formula attempts to correct
    for overfitting. The `qeLin()` reports this too, and again a large discrepancy
    between this value and the first *R*² value suggests we are overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: '8.9 Classification Applications: The Logistic Model'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The linear model is designed for regression applications. What about classification?
    A generalization of the linear model, unsurprisingly called the *generalized linear
    model*, handles that. Here we will present one form of the model, the *logistic*
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Recall the discussion at the beginning of [Chapter 2](ch02.xhtml), which pointed
    out that in classification settings, where *Y* is either 1 or 0, the regression
    function becomes the probability of *Y* = 1 for the given subpopulation. If we
    fit a purely linear model with `lm()`, the estimated regression values may be
    outside the interval [0,1] and thus not represent a probability. We could, of
    course, truncate any value predicted by `lm()` to [0,1], but the *logistic* model
    provides a better approach.
  prefs: []
  type: TYPE_NORMAL
- en: The model takes its name from the logistic function *l*(*t*) = 1/(1 + *e*^(−*t*)).
    Since that function takes values in (0,1), it is suitable for modeling a probability.
    We still use a linear form but run the form through the logistic function to squeeze
    it into (0,1) for probability modeling.
  prefs: []
  type: TYPE_NORMAL
- en: 'Say we wish to predict gender from height. Our model might be:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/unch08equ13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here *β*[0] and *β*[1] are, again, population values, which we estimate from
    our data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The logistic and linear models are basically similar: in the linear model,
    *β**[i]* is the impact that the *i**^(th)* feature has on mean *Y*, while in the
    logit case, *β**[i]* is the impact that the *i**^(th)* feature has on the probability
    that *Y* = 1\. (Some analysts view logit in terms of a linear model of the *log-odds
    ratio*, log(*P*(*Y* = 1| *X*) / [1 − *P*(*Y* = 1|*X*)]).)'
  prefs: []
  type: TYPE_NORMAL
- en: The logistic model is often called the *logit* model for simplicity.
  prefs: []
  type: TYPE_NORMAL
- en: '***8.9.1 The glm() and qeLogit() Functions***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In R, the standard function for the generalized linear model is `glm()`. In
    the logistic case, that function is wrapped by the `qeML` package function `qeLogit()`.
    The call form for the latter is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The first three arguments are as in the other `qe*`-series functions. The last
    argument, `yesYVal`, is needed in the 2-class case. It specifies the value of
    *Y* that we wish to be coded as *Y* = 1.
  prefs: []
  type: TYPE_NORMAL
- en: '***8.9.2 Example: Telco Churn Data***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In [Section 2.2](ch02.xhtml#ch02lev2), we used k-NN to analyze some customer
    retention data. Let’s revisit that data, now using a logistic model. Recall that
    the `Churn` variable has values `'Yes'` and `'No'`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Our model is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/unch08equ14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s predict a new case that is similar to, say, the 333rd one in our dataset
    but with a different gender:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We guess that the customer will stay put—that is, not jump to another service
    provider—with a jump probability of only about 23 percent.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also get a warning message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: This is a technical issue, occurring when the features are highly correlated.
    Here `glm()` has actually skipped over some of the features that are in essence
    redundant.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes `glm()` will give us a warning message like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Again, this is a technical issue, which we will not pursue here. The reader
    may proceed as usual.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, a warning that one cannot ignore is “failed to converge.”
    This will not happen with `lm()`, the R function wrapped by our `qeLinear()`,
    but it may occasionally occur with logit. This is usually remedied by performing
    some dimension reduction.
  prefs: []
  type: TYPE_NORMAL
- en: '***8.9.3 Multiclass Case***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If there are more than two classes, we have two options. For concreteness, consider
    the vertebrae data from [Section 2.3](ch02.xhtml#ch02lev3). There we had three
    classes, DH, NO, and SL. For now, just consider using `glm()` directly rather
    than its wrapper, `qeLogit()`.
  prefs: []
  type: TYPE_NORMAL
- en: '**One vs. All (OVA) method** Here we run `glm()` once for each class. We first
    run logit with DH playing the role of *Y*. Then we do this with NO serving as
    *Y* and then finally the same for SL. That gives us three sets of coefficients—in
    fact, three return objects from `glm()`, say, `DHout`, `NOout`, and `SLout`. Then
    when predicting a new case `newx`, we run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This gives us three probabilities. We take as our prediction whichever class
    has the highest probability.
  prefs: []
  type: TYPE_NORMAL
- en: '**All vs. All (AVA) method** Here we run `glm()` once for each *pair* of classes.
    First, we restrict the data to just DH and NO, putting SL cases aside for the
    moment, and take *Y* as DH. Then we focus on just DH and SL, taking *Y* to be
    DH again. Finally, we put DH aside for the moment, running with NO and SL and
    taking *Y* to be NO. Again, that would give us three objects output from `glm()`.'
  prefs: []
  type: TYPE_NORMAL
- en: Then we would call `predict()` three times on `newx`. Say the outcome with the
    first object is less than 0.5\. That means between DH and NO, we would predict
    this new case to be NO—that is, NO “wins.” We do this on all three objects, and
    whichever class wins the most often is our predicted class.
  prefs: []
  type: TYPE_NORMAL
- en: The `qeLogit()` function uses the OVA approach. Since `qeLogit()` is a wrapper
    for `glm()`, we do not see the actions of the latter, and they are used only as
    intermediate internal computations. However, if desired, the results of the calls
    to `glm()` can be accessed in the `glOuts` component of the object returned by
    `qeLogit()`.
  prefs: []
  type: TYPE_NORMAL
- en: '***8.9.4 Example: Fall Detection Data***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This dataset is included in `qeML`, originally from Kaggle.^([3](footnote.xhtml#ch8fn3))
    From the site:'
  prefs: []
  type: TYPE_NORMAL
- en: Falls are a serious public health problem and possibly life threatening for
    people in fall risk groups. We develop an automated fall detection system with
    wearable motion sensor units fitted to the subjects’ body at six different positions.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are six activity types, thus six classes, coded 0 (Standing), 1 (Walking),
    2 (Sitting), 3 (Falling), 4 (Cramps), and 5 (Running). Let’s see how well we can
    predict the class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: We are correctly predicting only about 40 percent of the cases with our logit
    model, but that’s better than the 28 percent correct we’d get just guessing every
    case to be of Class 3, the most common class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s predict a hypothetical new case, say, like the first row in the data
    but with `BP` equal to 28:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Such a case would result in a prediction of Class 2, with a probability of about
    23 percent.
  prefs: []
  type: TYPE_NORMAL
- en: 8.10 Bias and Variance in Linear/Generalized Linear Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As discussed in [Chapter 3](ch03.xhtml), the more features we use, the smaller
    the bias but the larger the variance. With parametric models, such as those in
    this chapter, the larger variance comes in the form of less-stable estimates of
    the coefficients, which, in turn, make later predictions less stable.
  prefs: []
  type: TYPE_NORMAL
- en: Once again, note that high variance for a coefficient estimate means that the
    value of that estimate will vary a lot from one sample to another. That large
    oscillation, in turn, means that the estimated coefficient vector will be less
    likely to be near the true population value.
  prefs: []
  type: TYPE_NORMAL
- en: Here we present a concrete example illustrating the fact that the variance increases
    with the complexity of the model.
  prefs: []
  type: TYPE_NORMAL
- en: '***8.10.1 Example: Bike Sharing Data***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We can make that point about instability of predictions more concrete using
    the `regtools` function `stdErrPred()`. This function finds the standard error
    of a predicted value obtained from `lm()`. Recall from [Section 8.5.2.1](ch08.xhtml#ch08lev5sec2sec1)
    that the standard error of an estimator is the estimated standard deviation of
    that estimator. A larger standard error thus means more variability of the estimator
    from one sample to another.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll fit two models, one with a smaller feature set and the other with a somewhat
    larger one, and then do the same prediction on each; just as an example, we’ll
    predict the third data point in the dataset. We will print out the two predictions
    and, most important, the standard error of the two predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: So the prediction from the larger feature set has a larger standard error. The
    standard error is the standard deviation of an estimator—in this case, our estimate
    of prediction accuracy. So here we see the Bias-Variance Trade-off in action.
    The larger model, though more detailed and thus less biased, does have a larger
    variance.
  prefs: []
  type: TYPE_NORMAL
- en: Does that mean that we should use the smaller feature set? No. In order to see
    if we’ve hit the switchover point, we’d need to use cross-validation. But the
    reader should keep this concrete illustration of the trade-off in mind.
  prefs: []
  type: TYPE_NORMAL
- en: 8.11 Polynomial Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Surprisingly, one can use linear regression methods to model nonlinear effects.
    We’ll see how in this section. Why is this important?
  prefs: []
  type: TYPE_NORMAL
- en: A polynomial model can often match or even outperform many of the more glamorous
    ML models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Polynomials will play an important role in our chapter on support vector machines
    ([Chapter 10](ch10.xhtml)), and even in our treatment of neural networks ([Chapter
    11](ch11.xhtml)), where there is a surprising connection to polynomials.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***8.11.1 Motivation***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We’ve used the example of programmer and engineer wages earlier in this book
    (see [Section 3.2.3](ch03.xhtml#ch03lev2sec3)). Consider the graph of wage income
    against age shown in [Figure 8-2](ch08.xhtml#ch08fig02). There seems to be a steep
    rise in a worker’s 20s, then a long leveling off, with a hint even of a decline
    after age 55 or so. This is definitely not a linear relationship.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch08fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 8-2: Wage income vs. age*'
  prefs: []
  type: TYPE_NORMAL
- en: Or consider [Figure 8-3](ch08.xhtml#ch08fig03), for the bike sharing data, graphing
    total ridership against temperature. The nonlinear relationship is even clearer
    here. (We seem to see two groups here, possibly for the registered and casual
    riders.) No surprise, of course—people don’t want to go bike riding if the weather
    is too cold or too hot—but again, the point is that a linear model would seem
    questionable.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch08fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 8-3: Ridership vs. temperature*'
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, these nonlinear effects actually *can* be accommodated with linear
    models.
  prefs: []
  type: TYPE_NORMAL
- en: '***8.11.2 Modeling Nonlinearity with a Linear Model***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Starting simple, suppose in the bike rental data we wish to predict total ridership
    `tot`, using temperature `temp` as our sole feature, but want a quadratic model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch08equ11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This is still a linear model! Sure, there is a squared term there for `temp`,
    so we say the model is nonlinear *in terms of* `temp`. But it is still linear
    in `b`, `c`, and `d`. We are modeling mean `tot` as a linear combination of three
    things: 1, `temp`, and `temp`² (thinking of *b* as *b* × 1).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we could simply add a `temp`² column and call `qeLin()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: We see that ![Image](../images/unch08equ15.jpg) and ![Image](../images/unch08equ16.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: But this is inconvenient. Not only would we need to add that squared column
    manually, but we also would have to remember to add it later to new cases that
    we wish to predict.
  prefs: []
  type: TYPE_NORMAL
- en: 'Worse, we would need to add the *cross-product* terms. Say we are predicting
    total ridership from both temperature and humidity. In that case, [Equation 8.11](ch08.xhtml#ch08equ11)
    would include a product of these two features, becoming:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch08equ12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If we have a lot of features, adding these terms manually would become a real
    nuisance.
  prefs: []
  type: TYPE_NORMAL
- en: A more subtle problem concerns dummy variables. Since 0² = 0 and 1² = 1, we
    see that the square of any dummy is itself. So adding squared terms to our model
    for dummies would be redundant.
  prefs: []
  type: TYPE_NORMAL
- en: 'To avoid such issues, the `qeML` package has the `qePolyLin()` function, which
    takes care of these things for us automatically. Its basic call form is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The argument `deg` is the degree of the polynomial, and `maxInteractDeg` is
    the maximum interaction degree term; for instance, *temp* × *hum* is considered
    to be of degree 2 in [Equation 8.12](ch08.xhtml#ch08equ12). Of course, if we have
    just a single feature, there are no interaction terms.
  prefs: []
  type: TYPE_NORMAL
- en: 'It gives the same fit as the one we got above manually (of course). Let’s again
    predict `tot` from `temp`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s take a look at the resulting estimated coefficients:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Prediction is as usual, for example, for a 12-degree day:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s see if the quadratic model predicts better:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Yes, the MAPE value was smaller for the quadratic model, though as always, it
    must be added that we should use `replicMeans()` to be sure (see [Section 3.2.2](ch03.xhtml#ch03lev2sec2)).
  prefs: []
  type: TYPE_NORMAL
- en: '***8.11.3 Polynomial Logistic Regression***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Recall that the logit model starts with a linear combination of the features
    and then runs it through the logistic function *l*(*t*) = 1/(1 + *e*^(−*t*)) to
    squeeze it into [0,1]. That means we can add polynomial terms as in the linear
    model. The `regtools` function `qePolyLog()` does this.
  prefs: []
  type: TYPE_NORMAL
- en: '***8.11.4 Example: Programmer and Engineer Wages***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let’s predict occupation by applying nonpolynomial logit first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'About 35 percent right, which is not too bad, considering there are 6 classes.
    But maybe a quadratic model—that is, adding terms such as the squares of income
    and age—would improve things. Let’s see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: This is a slight improvement. But is it a sampling accident? We could use the
    `qeCompare()` function to compare different degrees while using many holdout sets
    to address sampling issues (see [Section 8.13](ch08.xhtml#ch08lev13)).
  prefs: []
  type: TYPE_NORMAL
- en: 8.12 Blending the Linear Model with Other Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Problems tend to occur with k-NN around the fringes of a dataset. As a simple
    concrete example, let’s again consider predicting human weight from height in
    the Major League Baseball Player data from [Section 1.8](ch01.xhtml#ch01lev8).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a summary of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Suppose we wish to predict the weight of a new player whose height is 68.2 and
    use k-NN. This height is on the low end of our training data, so most of the nearest
    neighbors will be taller than this. Taller people tend to be heavier, and the
    dataset neighbors of our new point will mostly be taller than this new player
    and thus likely heavier. The result will be that our prediction will be biased
    upward; we will tend to predict a larger weight than this player’s true value.
    Similarly, if our new case has height, say, 81.5, our prediction will be biased
    downward.
  prefs: []
  type: TYPE_NORMAL
- en: One remedy is to fit a linear model within the neighborhood. Say we are predicting
    a new case `x` and use *k* = 25 neighbors. Then, instead of averaging the weights
    of those 25 neighboring players, we invoke `lm()` on that neighborhood data. We
    then predict `x` from the output of `lm()`. The linearity of this process will
    result in more realistic predictions at the fringes of the data.
  prefs: []
  type: TYPE_NORMAL
- en: The point, then, is that instead of using the mean to smooth the data in a neighborhood,
    we could use `lm()`. Or, if we are worried about the effects of outliers, we might
    try `median()`.
  prefs: []
  type: TYPE_NORMAL
- en: There is an argument called `smoothingFtn` in the `regtools` function `kNN()`
    (which is wrapped by `qeKNN()`) that lets us specify some kind of smoothing other
    than the usual mean. The default is `smoothingFtn = mean`; to use median smoothing,
    we would specify `smoothingFtn = median`. For linear smoothing, we would use `smoothingFtn
    = loclin`.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that decision trees (and thus random forests and tree-based boosting
    as well) form neighborhoods too. They thus are subject to the same problem—that
    is, bias in neighborhoods that are near the edges of the data. Thus the same local-linear
    idea could be applied here. The CRAN package `grf` does this; it is wrapped by
    `qeRFgrf()`.
  prefs: []
  type: TYPE_NORMAL
- en: 8.13 The qeCompare() Function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In our experiment in [Section 8.11.4](ch08.xhtml#ch08lev11sec4), a quadratic
    model did appear to help, with a slightly lower OME than that of the ordinary
    logit. An extensive investigation would involve `fineTuning()`, with cross-validation
    trials, and possibly exploring polynomial degrees other than 1 or 2.
  prefs: []
  type: TYPE_NORMAL
- en: But remember, “qe” stands for “quick and easy.” The `qeML` function `qeCompare()`
    can be used for quick comparisons between models. (Of course, for large datasets,
    it won’t be so quick.)
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use it to compare ordinary logistic regression with a quadratic version
    for the vertebrae data. While we are at it, let’s compare to the other methods
    we’ve had in the book so far.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: What happened here?
  prefs: []
  type: TYPE_NORMAL
- en: Here we generated 100 random holdout sets (of size 73, the default for this
    dataset). The same holdout set is used for all methods. (The `qeCompare()` function
    has an optional random-number `seed` argument, but we’ve taken the default value
    of 9999.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are using default hyperparameters for each of the functions. The `qeCompare()`
    function has an optional `opts` argument to set non-default values, such as `nTree`
    for `qeRF()`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We found OME values for each method.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The quadratic logit not only outperformed the ordinary logit, but it also turned
    out to be the best of the bunch! Yes, it outperformed the fancy ML methods. Of
    course, each method was run with the default hyperparameters, and things could
    change with other values.
  prefs: []
  type: TYPE_NORMAL
- en: '***8.13.1 Need for Caution Regarding Polynomial Models***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The polynomial degree is a hyperparameter. In our example of predicting occupation
    in [Section 8.11.4](ch08.xhtml#ch08lev11sec4), we might, say, try a cubic (degree-3)
    model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: or even set degree to 4, 5, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: However, with higher and higher degrees, we do need to watch for over-fitting,
    as the number of terms increases very rapidly with degree. Let’s illustrate that
    point by checking the number of *β* coefficients in each model for varying degree,
    as shown in [Table 8-1](ch08.xhtml#ch8tab1). This requires some digging into the
    output objects. It would be a distraction to explain that here, but for the interested
    reader, this is in the component `$glmOuts[[1]]`. One must then exclude the intercept
    term.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 8-1:** Complexity of Polynomial Models'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Degree** | **Coefficients** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 6 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 22 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 50 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 90 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 143 |'
  prefs: []
  type: TYPE_TB
- en: Remember, these are the values of *p*, our number of features, after we add
    in polynomial terms. Our sample size *n* remains at 20,090\. At some point while
    increasing the degree, we will overfit.
  prefs: []
  type: TYPE_NORMAL
- en: If we follow the rough rule of thumb that we need to have ![Image](../images/prootn.jpg),
    this suggests a limit of something like *p* < 141, corresponding to using at most
    a degree-4 model. But this is, after all, only a rule of thumb. It may be the
    case that, say, OME starts increasing after just a degree-2 model. The reader
    is urged to try polynomials of various degrees, on this dataset and others, noting
    the OME values that result.
  prefs: []
  type: TYPE_NORMAL
- en: 'We note the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Polynomial models may be able to hold their own against, or even outperform,
    their more esoteric ML counterparts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use of polynomial models is attractive in that there is only one hyperparameter
    (the degree), and also because the calculation (in the case of `qePolyLin()`)
    is noniterative and thus has no convergence issues.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As with any ML method, one must always keep in mind the possibility of overfitting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unfortunately, many treatments of ML overlook polynomial models. But they can
    be quite powerful and should definitely be in the analyst’s toolkit.
  prefs: []
  type: TYPE_NORMAL
- en: 8.14 What’s Next
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The linear model is the oldest form of ML. As we have seen, it still can be
    quite powerful, outperforming “modern” ML methods in some cases. But in some settings,
    it can be made even better by, oddly, “shrinking” ![Image](../images/betacap.jpg)
    This is the topic of the next chapter.
  prefs: []
  type: TYPE_NORMAL
