<html><head></head><body>
<section>&#13;
<header>&#13;
<h1 class="chapter">&#13;
<span class="ChapterNumber"><span epub:type="pagebreak" title="37" id="Page_37"/>3</span><br/>&#13;
<span class="ChapterTitle">Evaluating Your Architecture</span>&#13;
</h1>&#13;
</header>&#13;
<p class="ChapterIntro"><span class="DropCap">A</span> big red flag is raised for me when people talk about the phases of their modernization plans in terms of which <em>technologies</em> they are going to use rather than what <em>value</em> they will add. This distinction is usually a pretty clear sign that they assume anything new must be better and more advanced than what they already have.</p>&#13;
<p>It may seem picky to focus on language, but communication is an essential part of keeping modernization on track. Teams tend to move in the direction they are looking. If we talk about what we’re doing in terms of technical choices, users’ needs get lost. The best way to find value is by focusing on their needs.</p>&#13;
<p>I always keep in mind three principles when developing a strategy around a new legacy system. The tour of history in Chapters 1 and 2 laid them out in detail:</p>&#13;
<ul>&#13;
<li>Modernizations should be based on adding value, not chasing new technology.</li>&#13;
<li><span epub:type="pagebreak" title="38" id="Page_38"/>Familiar interfaces help speed up adoption.</li>&#13;
<li>People gain awareness of interfaces and technology through their networks, not necessarily by popularity.</li>&#13;
</ul>&#13;
<p>But for most organizations, the conversation around modernization begins with failure. No one would invest the time and effort if the system were humming along just fine. The term <em>legacy modernization</em> itself is a little misleading. Plenty of old systems exist that no one gives a thought to changing because they just work.</p>&#13;
<p>So the last thing you need to consider when developing a plan of attack is the exact nature of the failure that is driving the desire to modernize in the first place. In all likelihood, you’re dealing with one or more of the following issues: technical debt, poor performance, or instability.</p>&#13;
<h2 id="h1-501188c03-0001">Problem 1: Technical Debt</h2>&#13;
<p class="BodyFirst">Old systems don’t need to be modernized simply because they are old. Lots of technology has not fundamentally changed in decades. Moving to the latest and greatest thing can sometimes cause more problems than it solves.</p>&#13;
<p>The following situations might warrant modernization:</p>&#13;
<ul>&#13;
<li>The code is difficult to understand. It references decisions or architectural choices that are no longer relevant, and institutional memory has been lost.</li>&#13;
<li>Qualified engineering candidates are rare.</li>&#13;
<li>Hardware replacement parts are difficult to find.</li>&#13;
<li>The technology can no longer perform its function efficiently.</li>&#13;
</ul>&#13;
<p>The terms <em>legacy</em> and <em>technical debt</em> are frequently conflated. They are different concepts, although a system can show signs of both problems.</p>&#13;
<p><em>Legacy</em> refers to an old system. Its design patterns are relatively consistent, but they are out-of-date. Upgrading the capacity of the underlying <span epub:type="pagebreak" title="39" id="Page_39"/>infrastructure results in performance increases. New engineers are difficult to onboard because of the skills gap between the technology they know and the technology with which the legacy system was built.</p>&#13;
<p><em>Technical debt</em>, by contrast, can (and does) happen at any age. It’s a product of subpar trade-offs: partial migrations, quick patches, and out-of-date or unnecessary dependencies. Technical debt is most likely to happen when assumptions or requirements have changed and the organization resorts to a quick fix rather than budgeting the time and resources to adapt. Unlike legacy systems, performance issues in this case are usually a byproduct of inefficient code instead of out-of-date infrastructure. Upgrading the infrastructure—increasing memory and cores or adding servers—doesn’t always produce equal increases in performance.</p>&#13;
<p>Systems with substantial technical debt also make it difficult to onboard new engineers, but in this case, the difficulty is because the application’s internal logic doesn’t make sense. Perhaps the documentation is out-of-date, or levels of abstraction are piled up on top of one another, or functions are named unintuitively.</p>&#13;
<p>Managing technical debt is about restoring consistency. A good way to approach the challenge is to run a product discovery exercise as if you were going to build a completely new system, but don’t actually build one! Instead, use this new vision to excavate and refocus the current system.</p>&#13;
<p>As time passes, requirements naturally change. As requirements change, usage patterns change, and the organization and design that is most efficient also changes. Use product discovery to redefine what your MVP is, and then find where that MVP is in the existing code. How are these sets of functions and features organized? How would you organize them today?</p>&#13;
<p>Another useful exercise to run when dealing with technical debt is to compare the technology available when the system was originally built to the technology we would use for those same requirements today. I employ this technique a lot when dealing with systems written in COBOL. For all that people talk about COBOL dying off, it is good at certain tasks. The <span epub:type="pagebreak" title="40" id="Page_40"/>problem with most old COBOL systems is that they were designed at a time when COBOL was the <em>only</em> option. If the goal is to get rid of COBOL, I start by sorting which parts of the system are in COBOL because COBOL is good at performing that task, and which parts are in COBOL because there were no other tools available. Once we have that mapping, we start by pulling the latter off into separate services that are written and designed using the technology we would choose for that task today.</p>&#13;
<h2 id="h1-501188c03-0002">Example: The General Ledger</h2>&#13;
<p class="BodyFirst">One such debt-heavy system was designed as a <em>general ledger</em> for a large healthcare organization. It is a complex system involving multiple mainframes working together. It processes requests from still more mainframes that back other systems that need to issue payments. The general ledger’s core function is to authorize and issue payments from an organization to third parties. The system, therefore, must make sure the organization has the funds to issue the payment, that the request is valid, that the request is not a duplicate, and that the circumstances of the request comply with all relevant regulations. In addition, this system also tracks money owed to the organization, sends requests to remind debtors to pay, and generates reports for various stakeholders.</p>&#13;
<p>The current system organizes code based on division—for example, Loans and Accounts Payable are different applications within the system despite having overlapping requirements—and is written in COBOL or the Assembly language specific to the mainframe that typically runs its jobs. Overall, the system looks something like <a href="#figure3-1" id="figureanchor3-1">Figure 3-1</a>.</p>&#13;
<p>It’s easy to see how this system evolved this way. The organization is large with a lot of money to spend, and when computers were first being introduced to the market, it took advantage of them right away (hence, the Assembly). The organization migrated paper processes to digital processes largely without changing them and maintains the original process boundaries within the technology.</p>&#13;
<span epub:type="pagebreak" title="41" id="Page_41"/><figure>&#13;
<img src="../Images/f03001.png" alt="f03001"/>&#13;
<figcaption><p><a id="figure3-1">Figure 3-1</a>: The applications that talk to the general ledger</p></figcaption>&#13;
</figure>&#13;
<p class="BodyContinued">Back then, computers were “extras,” big experimental toys to make things faster, and not every business unit felt the new machines would add value to their process. The final system ended up divided by business unit because the adoption of technology was gradual, unit by unit.</p>&#13;
<p>But today, computers are the default, so this is not the way we would build such a system. We might preserve the mapping of applications to divisions, but we would build shared services that reflected their shared requirements. Some features play to COBOL’s strengths of processing large amounts of financial data accurately, but COBOL doesn’t necessarily bring much to the table when generating reports or sending out mailings.</p>&#13;
<p>In modernizing this system, I would identify the appropriate shared services and then select one to build. The ideal situation is when I can identify an application that needs only one of the proposed shared services. We build that service and rewrite that application to use it. Then we go back and find an application that needs that shared service plus another shared service on our list. We build the second shared service and rewrite the application to use both.</p>&#13;
<p>However, rarely can applications in large systems be arranged in order of ascending complexity in that manner. More likely, we will have <span epub:type="pagebreak" title="42" id="Page_42"/>to pull out one shared service and rewrite each application one by one, before pulling out a second shared service and rewriting each application one by one. This can be frustrating, but it’s important not to increase load on a new service before we have enough experience with it to know what normal behavior looks like.</p>&#13;
<h2 id="h1-501188c03-0003">Problem 2: Performance Issues</h2>&#13;
<p class="BodyFirst">Performance issues are actually one of the nicer problems to have with legacy systems. Few organizations are motivated to do anything about legacy systems until they start affecting the business side and work starts to slow down. Sometimes this is because the system itself has slowed down, but more likely, the system’s performance has remained pretty static and literally everything around it has gotten faster.</p>&#13;
<p>Normally, the issues of how long something should take and how many resources it needs to do the job are highly subjective. People tend to accept the current state as fine, especially if they have limited experience with other systems. If the organization believes its system is having performance issues, the hard work of figuring out what “better” is has already been done for you. A system cannot have performance issues unless the organization that owns it has defined expectations.</p>&#13;
<p>This book will repeat the message of trade-offs over and over again. No changes made to existing systems are free. Changes that improve one characteristic of a system often make something else harder. Teams that are good at legacy modernization know how to identify the trade-offs and negotiate the best possible deal. You have to pick a goal or a characteristic to optimize on and set budgets for all other characteristics so you know how much you’re willing to give up before you start losing value.</p>&#13;
<p>Is it worth losing some accuracy to make things faster? Is it worth migrating to managed services when that makes testing locally more difficult? When an organization has decided its system has performance issues, it is easier to answer these questions. The organization must <span epub:type="pagebreak" title="43" id="Page_43"/>have some expectation of how fast performance should be or how much money it should spend to satisfy requirements.</p>&#13;
<p>Once performance requirements are defined, the task of evaluating the legacy system and developing a strategy becomes about listing all the steps in a given task and identifying performance bottlenecks. With that mapped out, you can prioritize improvements, starting with the areas where the most gains can be realized.</p>&#13;
<p>Tackling each bottleneck should not require eliminating it completely. If you can do that, great, but in most cases, you’ll find that what you would need to invest to eliminate it is not worth the boost in performance. Don’t underestimate the power of 5 percent, 10 percent, and 20 percent performance gains. As long as your approach to reaching those gains moves the system toward a better overall state, a 5 percent gain can pay interest as the project moves forward. Other changes may turn that 5 percent into a 30 percent or 50 percent gain later.</p>&#13;
<p>That being said, don’t throw out engineering best practices and good architecture just to patch something up and get a performance boost. You can spot such solutions because they often avoid touching what is obviously the real problem. The people who propose these solutions are often frustrated by the system’s problems and overwhelmed by the possibility of investing months or years in incremental improvement. They argue against the 5 percent change that makes the system better because they believe a 5 percent improvement will never be enough. Instead, they propose a solution that offers a much larger performance gain, but that compounds the root cause or makes it more difficult to fix later. Here’s one example of what I mean. We had a system where multiple services needed access to a giant unstructured data store. The data had grown to a size that deleting some of it from the data store was such a resource-intensive process, it affected the performance of normal reads and writes.</p>&#13;
<p>The problem was the unstructured nature of the data and the fact that so many services needed to access it at one time, but that is a hard problem to solve. The process of breaking up the data, structuring it <span epub:type="pagebreak" title="44" id="Page_44"/>appropriately, and migrating services over would take months, if not years. Instead, the engineers on the project wanted to build a garbage collection service that would run deletes during low traffic periods when the performance hit wasn’t as big an issue.</p>&#13;
<p>What’s the problem with this approach? To begin with, creating a new service is no small amount of work, and once created, it has to be maintained, monitored, tested, and scaled. On top of that, the new service is an abstraction to perform a potentially dangerous operation outside the normal flow of events. What triggers this service, and how do we know the job it’s running is correct? Adding a new service just increases the overall complexity of the system to take advantage of a temporary situation. As load continues to increase, those low-traffic windows will be smaller and harder to find.</p>&#13;
<p>Adding this system, if it worked, could produce a huge gain in performance that would buy the organization time to fix the real problem. Certainly that was the intention of the engineers who were proposing it. But it’s also possible that once such a bandage was in place, the organization would lose interest in fixing the real problem, and this team would have accomplished nothing more than resetting the clock on the time bomb.</p>&#13;
<p>The smarter thing to do would be to look for the baby steps toward breaking up the data that would have produced those 5 or 10 percent gains. Such gains add up if you find enough of them.</p>&#13;
<p>Large problems are always tackled by breaking them down into smaller problems. Solve enough small problems, and eventually the large problem collapses and can be resolved.</p>&#13;
<h2 id="h1-501188c03-0004">Example: Case Flow Management</h2>&#13;
<p class="BodyFirst">Software built to manage an application through multistaged approval processes are performance battlegrounds as they age. Here’s an example of a system where we could increase its output just by finding enough bottlenecks that could be whittled down. The technology behind this <span epub:type="pagebreak" title="45" id="Page_45"/>application approval process is reasonably good, but some parts of the process are automated, and some are manual. Some parts are digital, and some are still on paper. Some parts were digitalized recently and some 20 years ago. Everyone agrees that the system would be better if the remaining parts that could be automated were automated, if the paper parts of the process were digitalized, and if the older components of the system were brought up to speed, but that’s a long list of improvements.</p>&#13;
<p>Not all of the highest priority tasks actually affect the time it takes to process an application. For example, at one point in the process, the applicant must sign a consent form authorizing the organization to run a background check. Although the paper form could be replaced with a simple web form or an integration with a third-party service, this part of the application process is often done in parallel with processing the rest of the application. Therefore, digitalizing that step does not actually speed up the total processing time of a single application.</p>&#13;
<p>Other seemingly irrelevant issues could make a much bigger difference. Cases were being sent to the background-check service in batches. If one application within that batch had a problem, all the applications in that batch had to wait for it to be resolved before moving on. Simply reconfiguring jobs into batches of one could save a lot of time.</p>&#13;
<p>Instead of looking at the purely technical improvements to the system, the team decreased the processing time for an average application by tracing the application’s path. They had already done the hard work of determining a better system meant faster application turnaround, and they structured their approach around optimizing for that.</p>&#13;
<h2 id="h1-501188c03-0005">Problem 3: Stability Issues</h2>&#13;
<p class="BodyFirst">On the other hand, some legacy systems perform their core functions within the parameters the organization needs to be successful, but they are unstable. They are not too slow; they produce the correct result and within the resources the organizations has available for the task, but <span epub:type="pagebreak" title="46" id="Page_46"/>there are frequent “surprises,” such as outages with bizarre black-swan-style root causes or routine upgrades that sometimes go very poorly. Ongoing development work is stopped because unforeseen technical conflicts pop up and need to be resolved.</p>&#13;
<p>In 1983, Charles Perrow coined the term <em>normal accidents</em> to describe systems that were so prone to failure, no amount of safety procedures could eliminate accidents entirely. According to Perrow, normal accidents are not the product of bad technology or incompetent staff. Systems that experience normal accidents display two important characteristics.</p>&#13;
<p><b>They are tightly coupled</b>. When two separate components are dependent on each other, they are said to be coupled. In tightly coupled situations, there’s a high probability that changes with one component will affect the other. For example, if a change to one code base requires a corresponding change to another code base, the two repositories are tightly coupled. Loosely coupled components, on the other hand, are ones where changes made to one component don’t necessarily affect the other.</p>&#13;
<p>Tightly coupled systems produce cascading effects. One change creates a response in another part of the system, which creates a response in another part of the system. Like a domino effect, parts of the system start executing without a human operator telling them to do so. If the system is simple, it is possible to anticipate how failure will happen and prevent it, which leads to the second characteristic of systems that experience normal accidents.</p>&#13;
<p><b>They are complex</b>. Big systems are often complex, but not all complex systems are big. Signs of complexity in software include the number of direct dependencies and the depth of the dependency tree, the number of integrations, the hierarchy of users and ability to delegate, the number of edge cases the system must control for, the amount of input from untrusted sources, the amount of legal variety in that input, and so on, and so forth. Computer systems naturally grow more complex as they age, because as they age, we tend to add more and <span epub:type="pagebreak" title="47" id="Page_47"/>more features to them, which increases at least a few of these characteristics. Computer systems also tend to start off tightly coupled and may in fact stay that way if priority is not given to refactoring the code occasionally.</p>&#13;
<p>Tightly coupled and complex systems are prone to failure because the coupling produces cascading effects, and the complexity makes the direction and course of those cascades impossible to predict.</p>&#13;
<p>If your goal is to reduce failures or minimize security risks, your best bet is to start by evaluating your system on those two characteristics: Where are things tightly coupled, and where are things complex? Your goal should not be to eliminate all complexity and all coupling; there will be trade-offs in each specific instance.</p>&#13;
<p>Suppose you have three services that need to access the same data. If you configure them to talk to the same database, they are tightly coupled (<a href="#figure3-2" id="figureanchor3-2">Figure 3-2</a>).</p>&#13;
<figure>&#13;
<img src="../Images/f03002.png" alt="f03002"/>&#13;
<figcaption><p><a id="figure3-2">Figure 3-2</a>: Tightly coupled services</p></figcaption>&#13;
</figure>&#13;
<p><span epub:type="pagebreak" title="48" id="Page_48"/>Such coupling creates a few potential problems. To begin with, any of the three services could make a change to the data that breaks the other two services. Any changes to the database schema have to be coordinated across all three services. By sharing a database, you lose the scaling benefit of having three separate services, because as load increases on one service, it is passed down to the database, and the other services see a dip in performance.</p>&#13;
<p>However, giving each service its own database trades those problems for other potential problems. You now must figure out how to keep the data between the three separate databases consistent.</p>&#13;
<p>Loosening up the coupling of two components usually ends with the creation of additional abstraction layers, which raises complexity on the system. Minimizing the complexity of systems tends to mean more reuse of common components, which tightens couplings. It’s not about transforming your legacy system into something that is completely simple and uncoupled, it’s about being strategic as to where you are coupled and where you are complex and to what degree. Places of complexity are areas where the human operators make the most mistakes and have the greatest probability of misunderstanding. Places of tight coupling are areas of acceleration where effects both good and bad will move faster, which means less time for intervention.</p>&#13;
<p>Once you have identified the parts of the system where there is tight coupling and where there is complexity, study the role those areas have played in past problems. Will changing the ratio of complexity to coupling make those problems better or worse?</p>&#13;
<p>A helpful way to think about this is to classify the types of failures you’ve seen so far. Problems that are caused by human beings failing to read something, understand something, or check something are usually improved by minimizing complexity. Problems that are caused by failures in monitoring or testing are usually improved by loosening the coupling (and thereby creating places for automated testing). Remember also that an incident can include both elements, so be thoughtful <span epub:type="pagebreak" title="49" id="Page_49"/>in your analysis. A human operator may have made a mistake to trigger an incident, but if that mistake was impossible to discover because the logs weren’t granular enough, minimizing complexity will not pay off as much as changing the coupling.</p>&#13;
<h2 id="h1-501188c03-0006">Example: Custom Configuration</h2>&#13;
<p class="BodyFirst">Consider an organization that wanted to increase the power of custom configurations on its monolithic application. It built a configuration service that would allow its software engineers to set flags through the monolith’s code (<a href="#figure3-3" id="figureanchor3-3">Figure 3-3</a>). The application sends requests to the service with the identity of the user to fetch the appropriate configuration value. Since those values rarely change, more than 90 percent of the requests are handled by a cache. If the cache fails, the request moves on to a simple web service that immediately retries the cache before ultimately going back to the database to retrieve the configuration setting. The database is separate from the monolith’s database, but it runs on the same virtual machine (VM). Traffic directly from the application connects with the monolith’s database. The custom configuration database uses about 1 percent of the VM resources.</p>&#13;
<p>When the service receives the configuration value from the database, it updates the cache and sends the data back to the monolith. The data </p>&#13;
<figure>&#13;
<img src="../Images/f03003.png" alt="f03003"/>&#13;
<figcaption><p><a id="figure3-3">Figure 3-3</a>: Requests moving through the custom configuration service</p></figcaption>&#13;
</figure>&#13;
<p class="BodyContinued"><span epub:type="pagebreak" title="50" id="Page_50"/>on custom configurations is stored in a key-value style, with the key being the identity of the user and the value being a dictionary with all relevant configuration settings. Because possible customizations are almost infinite, these dictionaries do not have standard schemas. If a user has no configuration value set for a given flag, it is not present at all in the dictionary. The cache preserves this structure.</p>&#13;
<p>In general, this service performs well for the organization, but it has quirks that are difficult for engineers to reproduce and even harder to diagnose. A few problems have been traced back to cache stampedes. Users rarely change values after setting them, but in the rare cases where the cache does need to be updated, the whole dictionary is affected.</p>&#13;
<p>How can we think of this part of the system in terms of complexity and coupling? The monolith’s behavior is coupled to the configuration service. If the configuration service goes down, the monolith either cannot fulfill requests or falls back to a default value that might completely change the user’s experience. If the configuration service experiences partial outages, the monolith’s behavior becomes wildly unpredictable.</p>&#13;
<p>Hosting the databases on the same VM creates coupling between the monolith and the configuration service. If the monolith’s database has performance issues, the configuration service’s database feels them, and vice versa. However, in this case, fixing that issue by moving the configuration service’s database to its own VM might not bring much value. If the monolith’s database is having problems, the product itself is likely down, making the performance of this service largely irrelevant. Since the service uses only 1 percent of the VM’s resources, it is unlikely that it will affect the monolith without first triggering pages to the engineer on call. We might want to separate them for the sake of right-scaling, but that increases the number of VMs we’re paying for and doesn’t necessarily buy us much more than cosmetic improvements on our architectural diagram.</p>&#13;
<p>On the complexity side, the data structure was probably a poor design choice. When the monolith makes a request, it does not need every value set for the user, only the value relevant at that moment. If the key in the <span epub:type="pagebreak" title="51" id="Page_51"/>key-value store was user ID plus flag ID, the data could be flat, which would mitigate the risk of cache stampedes. On the other hand, we could keep the data structured as is and change the monolith’s assumptions so that it requests the user’s dictionary only once and stores the data returned in memory for the lifetime of the session. That solution minimizes the coupling between the monolith and the service, but it increases complexity. We need to understand how much data we would be storing in memory at any one point, and at what level that becomes problematic. We need to define a time to live and how to implement it. Will we want to make sure all the users’ requests are directed to the same server, or should we just assume that if a session is live, all servers in the application cluster will query the configuration service at least once and store the same data in their memory?</p>&#13;
<h2 id="h1-501188c03-0007">Stages of a Modernization Plan</h2>&#13;
<p class="BodyFirst">One day during a one-on-one, an engineer on my team confessed to feeling that we had approached our work on one legacy system completely wrong. I had recently brought a new engineer onto the team and given her explicit instructions to tear through the system’s testing suites. Although the tests were comprehensive and the coverage was good, they were brittle, poorly organized, and difficult to make sense of. That was a reflection of the system’s overall design, so the new engineer set about refactoring huge parts of how the code was organized, making it easier to test and the tests more reliable.</p>&#13;
<p>Looking at the new engineer’s contributions, my engineer knew this configuration was better. For months, we had been working on this system. She was kicking herself for not looking at the problem the way the newcomer did. “We were too pragmatic,” she said. “We just conformed to the system’s existing patterns when we should have redone it.”</p>&#13;
<p>I disagreed. What my engineer had forgotten was that when we took on this system, it was unstable. Things would frequently go wrong <span epub:type="pagebreak" title="52" id="Page_52"/>silently. Errors weren’t properly handled or logged. Performance was an issue.</p>&#13;
<p>It was good to learn how to have that kind of technical vision the new engineer displayed. I certainly wasn’t going to discourage my team from studying her contributions, but it was right to be pragmatic in the beginning. When you first take on a legacy system, you can’t possibly understand it well enough to make big changes right away. As part of those pragmatic changes, we also invested a lot of time documenting and researching the system. Truth be told, the new engineer’s first assignment was a series of small, pragmatic changes designed to help her get to know the system too, but by that point, my engineers knew the system so well, they were able to onboard her much quicker. She tore through those assignments in a matter of days.</p>&#13;
<p>“How do you think handling a major refactoring at the same time that we were having regular incidents would have affected you?” I asked.</p>&#13;
<p>“It would have been really stressful.”</p>&#13;
<p>So stressful, in fact, that it would have compromised the team’s judgment. These are the kinds of situations where people become frustrated and start convincing themselves that the best thing to do is throw the whole thing out and build it from scratch.</p>&#13;
<p>When both observability and testing are lacking on your legacy system, observability comes first. Tests tell you only what <em>won’t</em> fail; monitoring tells you what <em>is</em> failing. Our new engineer had the freedom to alter huge swaths of the system because the work the team had done rolling out better monitoring meant when her changes were deployed, we could spot problems quickly.</p>&#13;
<p>But the real lesson here is that modernization plans evolve as they progress. The first stage is one of evaluation. This doesn’t necessarily mean you should stop everything and produce big complicated plans, but you should focus on low-hanging fruit of immediate issues with pragmatic fixes. Use these small tasks to focus your investigation of the system itself. Get to know it and its quirks. Where are your blind spots in <span epub:type="pagebreak" title="53" id="Page_53"/>terms of monitoring? How easy is it to change things, test them, and be confident that they will work? Where are the gaps where the official documentation says things work this way, but they don’t? How much dead code is there? And so on, and so forth.</p>&#13;
<p>When your team knows the system well enough, you can expand the scope to look at issues across the system. Are things organized the way they should be? Is there a better technology to incorporate now, perhaps a different programming language or a new tool?</p>&#13;
<p>On particularly large systems, it is a good idea to make this an iterative multilevel process. In other words, pick one part of the large system and focus on that. Look at small pragmatic issues, and then look at more global issues within the component. Take a further step back and look for those global issues elsewhere in the system itself before deciding on an approach to them. Zoom back down to fix the component’s global issues and move on to the next component. Continue this local-global-superglobal routine until the system is where you need it to be.</p>&#13;
<p>The deeper your team understands the system and its quirks, the more predictable the system’s behavior is on a day-to-day basis and the easier it is to make big changes.</p>&#13;
<h2 id="h1-501188c03-0008">No Silver Bullets</h2>&#13;
<p class="BodyFirst">The only real rule of modernizing legacy systems is that there are no silver bullets. The rest of this chapter outlines different styles of organizing development activities. You will likely use all of them at different points on a large project.</p>&#13;
<p>The key thing to remember is that this is a toolkit. You break down large problems into smaller problems, and you choose the tool that gives you the highest probability of success with that specific problem. Sure, you may use some methods more often than others, but every large-scale legacy system has at least one square peg to contend with. It’s impossible to finish the job if all you know how to do is solve for round holes.</p>&#13;
<h2 id="h1-501188c03-0009"><span epub:type="pagebreak" title="54" id="Page_54"/>Full Rewrite</h2>&#13;
<p class="BodyFirst">A <em>full rewrite</em> is exactly what it sounds like: you start over with the intention of building a totally new system. The trouble with this approach is what do you do with the old system while you’re building the new one? Some organizations choose to put the old system on “maintenance mode” and give it only the resources for patches and fixes necessary to keep the lights on. If the new project falls behind schedule (and it almost certainly will), the old system continues to degrade. If the new project fails and is subsequently canceled, the gap between the old system and operational excellence has widened significantly in the meantime.</p>&#13;
<p>The longer the new system takes to get up and running, the longer users and the business side of the organization have to wait for new features. Neglecting business needs breaks trust with engineering, making it more difficult for engineering to secure resources in the future.</p>&#13;
<p>On the other hand, if you continue development on the old system while building a new system, keeping design decisions in sync between the two teams is a considerable challenge. If those systems handle data, and almost all computer systems do, migrating the data over from one to another poses a huge challenge.</p>&#13;
<p>Another consideration is the people involved. Who gets to work on the new system, and who takes on the maintenance tasks of the old system? If the old system is written in an obsolete technology relevant only to that particular system, the team maintaining the old system is essentially sitting around waiting to be fired. And don’t kid yourself, they know it. So if the people maintaining the old system are not participating in the creation of the new system, you should expect that they are also looking for new jobs. If they leave before your new system is operational, you lose both their expertise and their institutional knowledge.</p>&#13;
<p>That being said, lots of little parts within a big modernization project are not improved much by any kind of iteration. If you have an interface <span epub:type="pagebreak" title="55" id="Page_55"/>written in ActionScript, it’s probably better to just rewrite it and push it into production as a full replacement.</p>&#13;
<h2 id="h1-501188c03-0010">Iteration in Place</h2>&#13;
<p class="BodyFirst">If you have a working system, sometimes the simplest thing to do is to iterate it until it looks the way you want. This works well with managing technical debt, but you can also use it for situations when you want to redo the architecture. A fair amount of prep work is necessary to make iteration in place work. You will need to set up monitoring. At a minimum, you should have some way to track errors in the application layer and search logs, but the tooling here grows more sophisticated every year. The better you can identify what normal looks like on your legacy system, the easier it is to iterate in place safely.</p>&#13;
<p>Another area to make sure you have a mature approach is testing. Tests should run automatically, without needing a human being to follow test cases manually. Tests should also be multilevel, testing both the small units of code and whole processes end to end. Good tests take skill to write, and entire books have been written on the subject, so I won’t attempt to summarize them in a few paragraphs here. The most relevant guide for legacy modernizations is Michael Feathers’ <em>Working Effectively with Legacy Code</em>.</p>&#13;
<p>Finally, make sure your team can recover from failures quickly. This is an engineering best practice generally, but it’s especially important if you’re making changes to production systems. If you’ve never restored from a backup, you don’t actually have backups. If you’ve never failed over to another region, you don’t actually have failovers. If you’ve never rolled back a deploy, you don’t have a mature deploy pipeline.</p>&#13;
<p>If you have a good monitoring strategy, have a good testing strategy, and can roll back changes quickly, you will be able to change almost anything about your legacy system with confidence.</p>&#13;
<p><span epub:type="pagebreak" title="56" id="Page_56"/>Although it might seem risky, consider iteration in place to be the default approach. It is most likely to produce successful results in the greatest number of situations.</p>&#13;
<h2 id="h1-501188c03-0011">Split in Place</h2>&#13;
<p class="BodyFirst"><em>Split in place</em> is a variant of iteration in place specific to breaking up systems. This can mean moving from a monolithic structure to a service-oriented one, but it can also mean taking two components that are tightly coupled and uncoupling them. The difference from iteration in place is that you finish splitting things off by integrating them back. In other words, when you pull off a service from a monolith, that service will likely still need to receive inputs from and send outputs to the monolith. So you build the separate service and ultimately connect it to the monolith before moving on to the next service. You keep doing this over and over (breaking off services and integrating them back) until you’ve broken the whole project into small service-based sets of code.</p>&#13;
<h2 id="h1-501188c03-0012">Blue-Green</h2>&#13;
<p class="BodyFirst">A familiar pattern for deploys, the <em>blue-green technique</em> involves running two components in parallel and slowly draining traffic off from one and over to the other. The big benefit to doing this is that it’s easy to undo if something goes wrong. Often with technology, increasing load reveals problems that were not otherwise found in testing. Legacy systems have both the blessing and the curse of an existing pool of users and activity. The system that replaces them has a narrow grace period with which to fix those mistakes discovered under high load. Blue-green deployments allow the new system to ease into the full load of the old system gradually, and you can fix problems before the load exacerbates them.</p>&#13;
<h2 id="h1-501188c03-0013"><span epub:type="pagebreak" title="57" id="Page_57"/>The Hard Cutoff</h2>&#13;
<p class="BodyFirst">The <em>hard cutoff</em> is a deployment strategy where the new system or component replaces the old all at once. It is one of the riskiest strategies in the modernization toolbox.</p>&#13;
<p>A hard cutoff is sometimes done in stages, usually by environment or region. An organization might deploy to a low-traffic region first, monitor for issues, and then deploy to a higher-traffic region. This gives the organization some of the benefits of blue-green deploys in that it can stop the update (and ideally roll it back) midstream, but this method is not as accurate as blue-green deploys. The difference between environments and regions might not be completely predictable, and problems might escape notice.</p>&#13;
<p>If you don’t have multiple regions or are working with software designed to be installed by the user and have no control over how many users get access to the new version, you may not have a choice. Alpha and beta testing groups help in the latter case; making sure you can undo any change (either through restoring from backup or reverting/rolling back commands in the version control system) helps in the former case.</p>&#13;
<h2 id="h1-501188c03-0014">Putting It Together</h2>&#13;
<p class="BodyFirst">Good planning is less about controlling every detail and more about setting expectations across the organization. Your plan will define what it means to modernize your legacy system, what the goals are, and what value will be delivered and when. Specifically, your plan should focus on answering the following questions:</p>&#13;
<ul>&#13;
<li>What problem are we trying to solve by modernizing?</li>&#13;
<li>What small pragmatic changes will help us learn more about the system?</li>&#13;
<li><span epub:type="pagebreak" title="58" id="Page_58"/>What can we iterate on?</li>&#13;
<li>How will we spot problems after we deploy changes?</li>&#13;
</ul>&#13;
<p>Next, we’ll look at how to move out of the planning stage and into facing the problems that will make implementation hard. </p>&#13;
</section>&#13;
</body></html>