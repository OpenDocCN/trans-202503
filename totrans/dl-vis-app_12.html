<html><head></head><body><div id="sbo-rt-content"><section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" title="221" id="Page_221"/>10</span><br/>
<span class="ChapterTitle">Data Preparation</span></h1>
</header>
<figure class="opener">
<img src="Images/chapterart.png" alt="" width="206" height="206"/>
</figure>
<p class="ChapterIntro">Machine learning algorithms can only work as well as the data they’re trained on. In the real world, our data can come from noisy sensors, computer programs with bugs, or even incomplete or inaccurate transcriptions of paper records. We always need to look at our data and fix any problems before we use it.</p>
<p>A rich body of methods has been developed for just this job. They’re referred to as techniques for <em>data preparation</em>, or <em>data cleaning</em>. The idea is to process our data before<em> </em>learning from it so that our learning systems can use the data most efficiently. </p>
<p>We also want to make sure that the data itself is well suited to machine learning, which may mean adjusting it, for example, by scaling numbers, or combining categories. This work is essential because the particular way the data is structured, and the numerical ranges it spans, can have a strong effect on the information an algorithm can extract from it.</p>
<p><span epub:type="pagebreak" title="222" id="Page_222"/>Our goal in this chapter is to see how we can adjust the data we’re given, without changing its meaning, to get the most efficient and effective learning process. We begin with techniques to confirm that our data is clean and ready for training. We then consider methods for examining the data itself, and for making sure that we’ve got it in the best form for machine learning. This can involve doing simple things like replacing strings with numbers or taking more interesting actions like scaling the data. Finally, we look at ways to reduce the size of our training data. This lets our algorithms run and learn more quickly.</p>
<h2 id="h1-500723c10-0001">Basic Data Cleaning</h2>
<p class="BodyFirst">Let’s start by considering some simple ways to ensure that our data is well cleaned. The idea is to make sure that we’re starting out with data that has no blanks, incorrect entries, or other errors.</p>
<p>If our data is in textual form, then we want to make sure there are no typographical errors, misspellings, embedded unprintable characters, or other such corruptions. For example, if we have a collection of animal photos along with a text file describing them and our system is case-sensitive, then we want to make sure that every giraffe is labeled as <span class="CustomCharStyle">giraffe</span> and not <span class="CustomCharStyle">girafe</span> or <span class="CustomCharStyle">Giraffe</span>, and we want to avoid other typos or variants like <span class="CustomCharStyle">beautiful giraffe</span> or <span class="CustomCharStyle">giraffe-extra tall</span>. Every reference to a giraffe needs to use the identical string.</p>
<p>We should also look for other common-sense things. We want to remove any accidental duplicates in our training data, because they will skew our idea of what data we’re working with. If we accidentally include a single piece of data multiple times, our learner will interpret it as multiple, different samples that just happen to have the same value, and thus that sample may have more influence than it should.</p>
<p>We also want to make sure we don’t have any typographical errors, like missing a decimal point so we specify a value of 1,000 rather than 1.000, or putting two minus signs in front of a number rather than just one. It’s not uncommon to find some hand-composed databases with blanks or question marks in them, signifying that people didn’t have any data to enter. Some computer-generated databases can include a code like <code>NaN</code> (not a number), which is a placeholder indicating that the computer wanted to print a number but didn’t have a valid number to show. More troubling, sometimes when people are missing data for a numerical field, they enter something like 0 or –1. We have to find and fix all such issues before we start learning from the data. </p>
<p>We also need to make sure that the data is in a format that will be properly interpreted by the software we’re giving it to. For example, we can use a format known as <em>scientific notation</em> to write very large and very small numbers. The problem is that such notation has no official format. Different programs use slightly different forms for this type of output, and other programs that read that data (like the library functions we often use in deep learning) can misinterpret forms they’re not expecting. For example, in <span epub:type="pagebreak" title="223" id="Page_223"/>scientific notation, the value 0.007 is commonly printed out as <code>7e-3</code> or <code>7E-3</code>. When we provide the sequence <code>7e-3</code> as an input, a program may interpret it as (7 × <em>e</em>) – 3, where <em>e </em>is Euler’s constant, which has a value of about 2.7. The result is that the computer thinks that <code>7e-3</code> means that we’re asking it to first multiply the values of 7 and <em>e </em>together, and then subtract 3, giving us about 16 rather than 0.007. We need to catch these sorts of things so that our programs properly interpret their inputs.</p>
<p>We also want to look for missing data. If a sample is missing data for one or more features, we might be able to patch the holes manually or algorithmically, but it might be better to simply remove the sample altogether. This is a subjective call that we usually make on a case-by-case basis.</p>
<p>Lastly, we want to identify any pieces of data that seem dramatically different from all the others. Some of these <em>outliers</em> might be mere typos, like a forgotten decimal point. Others might be the result of human error, like a misdirected copy and paste, or when someone forgot to delete an entry from a spreadsheet. When we don’t know if an outlier is a real piece of data or an error of some kind, we have to use our judgment to decide whether to leave it in or remove it manually. This is a subjective decision that depends entirely on what our data represents, how well we understand it, and what we want to do with it.</p>
<p>Though these steps may seem straightforward, in practice, carrying them out can be a major effort depending on the size and complexity of our data and how messed up it is when we first get it. Many tools are available to help us clean data. Some are stand-alone, and others are built into machine-learning libraries. Commercial services will also clean data for a fee.</p>
<p>It’s useful to keep in mind this classic computing motto: <em>garbage in, garbage out</em>. In other words, our results are only as good as our starting data, so it’s vital that we start with the best data available, which means working hard to make it as clean as we possibly can.</p>
<p>Now that we’ve taken care of the essential small stuff, let’s turn our attention to making the data well suited for learning. </p>
<h2 id="h1-500723c10-0002">The Importance of Consistency</h2>
<p class="BodyFirst">Preparing numbers for learning means applying <em>transformations</em> to them, without changing the relationships among them that we care about. We cover several such transformations later in this chapter, where we might scale all the numbers to a given range or eliminate some superfluous data so that the learner has less work to do. When we do these things, we must always obey a vital principle: any time we modify our training data in some way, <em>we must also modify all future data the same way</em>. </p>
<p>Let’s look at why this is so important. When we make any changes to our training data, we typically modify or combine the values in ways that are designed to improve the computer’s learning efficiency or accuracy. <a href="#figure10-1" id="figureanchor10-1">Figure 10-1</a> shows the idea visually.</p>
<span epub:type="pagebreak" title="224" id="Page_224"/><figure>
<img src="Images/F10001.png" alt="F10001" width="446" height="384"/>
<figcaption><p><a id="figure10-1">Figure 10-1</a>: The flow of preprocessing for training and evaluating</p></figcaption>
</figure>
<p>As the figure shows, we typically determine any needed transformations by looking at the entirety of the training set. We transform that data to train our learner, and we also use the same transformation for all new data that comes after we’ve released our system to the world. The key point is that we must apply the <em>identical</em> modifications<em> </em>to all new data before we give it to our algorithm for as long as our system is in use. This step must not be skipped.</p>
<p>The fact that we need to reuse the same transformation on all data we evaluate pops up again and again in machine learning, often in subtle ways. Let’s first look at the problem in a general way with a visual example. </p>
<p>Suppose we want to teach a classifier how to distinguish pictures of cows from pictures of zebras. We can collect a huge number of photos of both animals to use as training data. What most obviously distinguishes pictures of these two animals are their different black-and-white markings. To make sure our learner pays attention to these elements, we may decide to crop each photo to isolate the animal’s hide, and then we train with those isolated patches of texture. These cropped photos are all that the learner sees. <a href="#figure10-2" id="figureanchor10-2">Figure 10-2</a> shows a couple of samples.</p>
<figure>
<img src="Images/F10002.png" alt="F10002" width="370" height="148"/>
<figcaption><p><a id="figure10-2">Figure 10-2</a>: Left: A patch of texture from a cow. Right: A patch of texture from a zebra.</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="225" id="Page_225"/>Suppose that we’ve trained our system and deployed it, but we forget to tell people about this preprocessing step of cropping each image to just the texture. Without knowing this vital information, a typical user might give our system complete pictures of cows and zebras, like those in <a href="#figure10-3" id="figureanchor10-3">Figure 10-3</a>, and ask the system to identify the animal in each one.</p>
<figure>
<img src="Images/F10003.png" alt="F10003" width="678" height="210"/>
<figcaption><p><a id="figure10-3">Figure 10-3</a>: Left: A photo of a cow. Right: A photo of a zebra. If we trained our system on the isolated patterns of <a href="#figure10-2">Figure 10-2</a>, it could be misled by all the extra details in the photos.</p></figcaption>
</figure>
<p>Humans can pick out the hide patterns from these photos. A computer, on the other hand, can be misled by the legs, the heads, the ground, and other details, thus reducing its ability to give us good results. The difference between the prepared data of <a href="#figure10-2">Figure 10-2</a> and the unprepared data of <a href="#figure10-3">Figure 10-3</a> can result in a system that performs beautifully on our training data but gives lousy results in the real world. To avoid this, all new data, like that in <a href="#figure10-3">Figure 10-3</a>, must be cropped to produce inputs that are just like the training data in <a href="#figure10-2">Figure 10-2</a>.</p>
<p>Forgetting to transform new data in the same way as we transformed the training data is an easy mistake to make but usually causes our algorithms to underperform, sometimes to the point of becoming useless. The rule to remember is this: we determine how to modify our training data, then we modify it, and then we <em>remember how we modified it</em>. Any time we deal with more data, we must first <em>modify that data in the identical way </em>that the training data was modified. We’ll come back to this idea later and see how it’s used in practice.</p>
<h2 id="h1-500723c10-0003">Types of Data</h2>
<p class="BodyFirst">Typical databases contain different types of data: floating-point numbers, strings, integers that refer to categories, and so on. We’ll treat each of these data types<em> </em>in its own way, so it’s useful to distinguish them and give each unique type its own name. The most common naming system is based on whether a kind of data can be sorted. Though we rarely use explicit sorting when we do deep learning, this naming system is still convenient and widely used.  </p>
<p>Recall that each sample is a list of values, each of which is called a <em>feature</em>. Each feature in a sample can be either of two general varieties: <em>numerical</em> or <em>categorical</em>.</p>
<p><span epub:type="pagebreak" title="226" id="Page_226"/>Numerical data is simply a number, either floating-point or integer. We also call this <em>quantitative</em> data. Numerical, or quantitative, data can be sorted just by using its values.</p>
<p>Categorical data is just about anything else, though often it’s a string that describes a label such as <span class="CustomCharStyle">cow</span> or <span class="CustomCharStyle">zebra</span>. The two types of categorical data correspond to data that can be naturally sorted and that which can’t.</p>
<p><em>Ordinal</em> data is categorical data that has a known order (hence the name), so we can sort it. Strings can be sorted alphabetically, but we can also sort them by meaning. For example, we can think of the rainbow colors as ordinal, because they have a natural ordering in which they appear in a rainbow, from <span class="CustomCharStyle">red</span> to <span class="CustomCharStyle">orange</span> on to <span class="CustomCharStyle">violet</span>. To sort the names of colors by rainbow order, we need to use a program that understands the orders of colors in a rainbow. Another example of ordinal data are strings that describe a person at different ages, such as <span class="CustomCharStyle">infant</span>, <span class="CustomCharStyle">teenager</span>, and <span class="CustomCharStyle">elderly</span>. These strings also have a natural order, so we can sort them as well, again by some kind of custom routine.</p>
<p><em>Nominal</em> data is categorical data without a natural ordering. For example, a list of desktop items such as <span class="CustomCharStyle">paper clip</span>, <span class="CustomCharStyle">stapler</span>, and <span class="CustomCharStyle">pencil sharpener</span> has no natural or built-in ordering, nor does a collection of pictures of clothing, like socks, shirts, gloves, and bowler hats. We can turn nominal data into ordinal data just by defining an order and sticking to it. For example, we can assert that the order of clothing should be from head to toes, so our previous example would have the order <span class="CustomCharStyle">bowler hats</span>, <span class="CustomCharStyle">shirts</span>, <span class="CustomCharStyle">gloves</span>, and <span class="CustomCharStyle">socks</span>, thereby turning our pictures into ordinal data. The order we create for nominal data doesn’t have to make any particular kind of sense, it just has to be defined and then used consistently.</p>
<p>Machine learning algorithms require numbers as input, so we convert string data (and any other nonnumerical data) into numbers before we learn from it. Taking strings as an example, we could make a list of all the strings in the training data, and assign each one a unique number starting with 0. Many libraries provide built-in routines to create and apply this transformation.</p>
<h2 id="h1-500723c10-0004">One-Hot Encoding</h2>
<p class="BodyFirst">Sometimes it’s useful to turn integers into lists. For instance, we might have a classifier with 10 classes where class 3 might be <span class="CustomCharStyle">toaster</span> and class 7 might be <span class="CustomCharStyle">ball-point pen</span>, and so on. When we manually assign a label to a photo of one of these objects, we consult this list and give it the correct number. When the system makes a prediction, it gives us back a list of 10 numbers. Each number represents the system’s confidence that the input belongs to the corresponding class.</p>
<p>This means we are comparing our label (an integer) with the classifier’s output (a list). When we build classifiers, it makes sense to compare lists to lists, so we need a way to turn our label into a list.</p>
<p>That’s easily done. Our list form of the label is just the list we want from the output. Let’s suppose that we’re labeling a picture of a toaster. We want <span epub:type="pagebreak" title="227" id="Page_227"/>the system’s output to be a list of ten values, with a 1 in the slot for class 3, corresponding to complete certainty that this is a toaster, and a 0 in every other slot, indicating complete certainty that the image is none of those other things. So, the list form of our label is the very same thing: ten numbers, all 0, except for a 1 in slot 3.</p>
<p>Converting a label like 3 or 7 into this kind of list is called <em>one-hot encoding</em>, referring to only one entry in the list being “hot,” or marked. The list itself is sometimes called a <em>dummy variable</em>. When we provide class labels to the system during training, we usually provide this one-hot encoded list, or dummy variable, rather than a single integer.</p>
<p>Let’s see this in action. <a href="#figure10-4" id="figureanchor10-4">Figure 10-4</a>(a) shows the eight colors in the original 1903 box of Crayola Crayons (Crayola 2016). Let’s suppose these colors appear as strings in our data. The one-hot labels that we provide to the system as our labels are shown in the rightmost column.</p>
<figure>
<img src="Images/F10004.png" alt="F10004" width="694" height="360"/>
<figcaption><p><a id="figure10-4">Figure 10-4</a>: One-hot encoding for the original eight Crayola colors in 1903. (a) The original eight strings. (b) Each string is assigned a value from 0 to 7. (c) Each time the string appears in our data, we replace it with a list of eight numbers, all of which are 0 except for a 1 in the position corresponding to that string’s value.</p></figcaption>
</figure>
<p>So far, we’ve converted data in one form to another form. Now let’s look at some transformations that actually change the values in our data.</p>
<h2 id="h1-500723c10-0005">Normalizing and Standardizing</h2>
<p class="BodyFirst">We often work with samples whose features span different numerical ranges. For instance, suppose we collected data on a herd of African bush elephants. Our data describes each elephant with four values:</p>
<ol class="decimal">
<li value="1">Age in hours (0, 420,000)</li>
<li value="2">Weight in tons (0, 7)</li>
<li value="3">Tail length in centimeters (120, 155)</li>
<li value="4">Age relative to the historical mean age, in hours (−210,000, 210,000)</li>
</ol>
<p><span epub:type="pagebreak" title="228" id="Page_228"/>These are significantly different ranges of numbers. Generally speaking, because of the numerical nature of the algorithms we use, larger numbers may influence a learning program more than smaller ones. The values in feature 4 are not only large, but they also can be negative.</p>
<p>For the best learning behavior, we want all of our data to be roughly comparable, or to fit in roughly the same range of numbers. </p>
<h3 id="h2-500723c10-0001">Normalization</h3>
<p class="BodyFirst">A common first step in transforming our data is to <em>normalize</em> each feature. The word <em>normal</em> is used in everyday life to mean “typical,” but it also has specialized technical meanings in different fields. In this context, we use the word in its statistical sense. We say that when we scale data into some specific range, the data has been <em>normalized.</em> The most popular choice of ranges for normalization are [−1,1] and [0,1], depending on the data and what it means (it doesn’t make sense to speak of negative apples or ages, for instance). Every machine learning library offers a routine to do this, but we have to remember to call it. </p>
<p><a href="#figure10-5" id="figureanchor10-5">Figure 10-5</a> shows a 2D dataset that we’ll use for demonstration. We’ve chosen a guitar because the shape helps us see what happens to the points as we move them around. We also added colors strictly as a visual aid, only to help us see how the points move. The colors have no other meaning.</p>
<figure>
<img src="Images/F10005.png" alt="F10005" width="543" height="524"/>
<figcaption><p><a id="figure10-5">Figure 10-5</a>: A guitar shape made of 232 points</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="229" id="Page_229"/>Typically, these points are the results of measurements, say the age and weights of some people, or the tempo and volume of a song. To keep things generic, let’s call the two features x and y.</p>
<p><a href="#figure10-6" id="figureanchor10-6">Figure 10-6</a> shows the results of normalizing each feature in our guitar-shaped data to the range [−1,1] on each axis. </p>
<figure>
<img src="Images/F10006.png" alt="F10006" width="488" height="485"/>
<figcaption><p><a id="figure10-6">Figure 10-6</a>: The data of <a href="#figure10-5">Figure 10-5</a> after normalization to the range [−1,1] on each axis. The skewing of the shape is due to it being stretched more along the Y axis than the X.</p></figcaption>
</figure>
<p>In <a href="#figure10-6">Figure 10-6</a>, the x values are scaled from −1 to 1, and the y values are independently scaled from −1 to 1. The guitar shape resulting from this operation has skewed a little bit because it’s been stretched more vertically than horizontally. This happens any time the different dimensions of the starting data span different ranges. In our case, the x data originally spanned the range of about [–1, 0] and the y data spanned about [–0.5, 0.2]. When we adjusted the values, we had to stretch the y values apart more than the x values, causing the skewing we see in <a href="#figure10-6">Figure 10-6</a>. </p>
<h3 id="h2-500723c10-0002">Standardization</h3>
<p class="BodyFirst">Another common operation involves <em>standardizing</em> each feature. This is a two-step process. First, we add (or subtract) a fixed value to all the data for each feature so that the mean value of every feature is 0 (this step is also called <em>mean normalization </em>or <em>mean subtraction</em>). In our 2D data, this moves the entire dataset left-right and up-down so that the mean value is sitting right on (0,0). Then, instead of normalizing or scaling each feature to lie between −1 and 1, we scale it so that it has a standard deviation of 1 (this step is also called <em>variance normalization</em>). Recall from Chapter 2 that this means about 68 percent of the values in that feature lie in the range of −1 to 1. </p>
<p><span epub:type="pagebreak" title="230" id="Page_230"/>In our 2D example, the x values are stretched or compressed horizontally until about 68 percent of the data is between −1 and 1 on the X axis, and then the y values are stretched or compressed vertically until the same thing is true on the Y axis. This necessarily means that points will land outside of the range [−1, 1] on each axis, so our results are different than what we get from normalization. <a href="#figure10-7" id="figureanchor10-7">Figure 10-7</a> shows the application of standardization to our starting data in <a href="#figure10-5">Figure 10-5</a>.</p>
<figure>
<img src="Images/F10007.png" alt="F10007" width="489" height="504"/>
<figcaption><p><a id="figure10-7">Figure 10-7</a>: The data of <a href="#figure10-5">Figure 10-5</a> after standardization</p></figcaption>
</figure>
<p>Here again we see that when the original shape doesn’t fit a normal distribution, a transformation like standardization can skew or otherwise distort the shape of the original data. Most libraries offer routines to normalize or standardize any or all of our features in one call. This makes it convenient to satisfy some algorithms that require their input to be normalized or standardized.</p>
<h3 id="h2-500723c10-0003">Remembering the Transformation</h3>
<p class="BodyFirst">Both normalization and standardization routines are controlled by parameters that tell them how to do their jobs. Most library routines analyze the data to find these parameters and then use them to apply the transformation. Because it’s so important to transform future data with the same operations, these library calls always give us a way to hang onto these parameters so we can apply the same transformations again later.</p>
<p>In other words, when we later receive a new batch of data to evaluate, either to evaluate our system’s accuracy or to make real predictions out in the field, we do <em>not</em> analyze that data to find new normalizing or <span epub:type="pagebreak" title="231" id="Page_231"/>standardizing transformations. Instead, we apply the same normalizing or standardizing steps that we determined for the training data.</p>
<p>A consequence of this step is that the newly transformed data is almost never itself normalized or standardized. That is, it won’t be in the range [−1,1] on both axes, or it won’t have its average at (0,0) and contain 68 percent of its data in the range [−1,1] on each axis. That’s fine. What’s important is that we’re using the same transformation. If the new data isn’t quite normalized or standardized, so be it.</p>
<h2 id="h1-500723c10-0006">Types of Transformations</h2>
<p class="BodyFirst">Some transformations are <em>univariate</em>, which means they work on just one feature at a time, each independent of the others (the name comes from combining <em>uni</em>, for one, with <em>variate</em>, which means the same as variable or feature). Others are <em>multivariate</em>, meaning they work on many features simultaneously.</p>
<p>Let’s consider normalization. This is usually implemented as a univariate transformer that treats each feature as a separate set of data to be manipulated. That is, if it’s scaling 2D points to the range [0,1], it would scale all the x values to that range, and then independently scale all the y values. The two sets of features don’t interact in any way, so how the X axis gets scaled does not depend at all on the y values, and vice versa. <a href="#figure10-8" id="figureanchor10-8">Figure 10-8</a> shows this ideal visually for a normalizer applied to data with three features.</p>
<figure>
<img src="Images/F10008.png" alt="F10008" width="525" height="198"/>
<figcaption><p><a id="figure10-8">Figure 10-8</a>: When we apply a univariate transformation, each feature is transformed independently of the others. Here we are normalizing three features to the range [0,1]. (a) The starting ranges of three features. (b) Each of the three ranges is independently shifted and stretched to the range [0,1].</p></figcaption>
</figure>
<p>By contrast, a multivariate algorithm looks at multiple features at a time and treats them as a group. The most extreme (and most common) version of this process is to handle all of the features simultaneously. If we scale our three colored bars in a multivariate way, we move and stretch them all as a group until they collectively<em> </em>fill the range [0,1], as illustrated in <a href="#figure10-9" id="figureanchor10-9">Figure 10-9</a>.</p>
<p>We can apply many transformations in either a univariate or multivariate way. We choose based on our data and application. For instance, the univariate version made sense in <a href="#figure10-6">Figure 10-6</a> when we scaled our x and y samples because they’re essentially independent. But suppose our features are temperature measurements made at different times over the course of different days? We probably want to scale all the features together so that, as a collection, they span the range of temperatures we’re working with.</p>
<span epub:type="pagebreak" title="232" id="Page_232"/><figure>
<img src="Images/F10009.png" alt="F10009" width="655" height="247"/>
<figcaption><p><a id="figure10-9">Figure 10-9</a>: When we apply a multivariate transformation, we treat multiple features simultaneously. Here we are again normalizing to the range [0,1]. (a) The starting ranges of three features. (b) The bars are shifted and stretched as a group so that their collective minimum and maximum values span the range [0,1].</p></figcaption>
</figure>
<h3 id="h2-500723c10-0004">Slice Processing</h3>
<p class="BodyFirst">Given a dataset, we need to think about how we select the data we want to transform. There are three approaches, depending on whether we think of <em>slicing</em>, or extracting, our data by sample, by feature, or by element. These approaches are respectively called <em>samplewise</em>, <em>featurewise</em>, and <em>elementwise</em> processing.</p>
<p>Let’s look at them in that order. For this discussion, let’s assume that each sample in our dataset is a list of numbers. We can arrange the whole dataset in a 2D grid, where each row holds a sample and each element in that row is a feature. <a href="#figure10-10" id="figureanchor10-10">Figure 10-10</a> shows the setup. </p>
<figure>
<img src="Images/F10010.png" alt="F10010" width="372" height="159"/>
<figcaption><p><a id="figure10-10">Figure 10-10</a>: Our database for the coming discussion is a 2D grid. Each row is a sample that contains multiple features, which make up the columns.</p></figcaption>
</figure>
<h3 id="h2-500723c10-0005">Samplewise Processing</h3>
<p class="BodyFirst">The samplewise approach is appropriate when all of our features are aspects of the same thing. For example, suppose our input data contains little snippets of audio, such as a person speaking into a cell phone. Then the features in each sample are the amplitude of the audio at successive moments, as in <a href="#figure10-11" id="figureanchor10-11">Figure 10-11</a>.</p>
<p><span epub:type="pagebreak" title="233" id="Page_233"/>If we want to scale this data to the range [0,1], it makes sense to scale all the features in a single sample so the loudest parts are set to 1 and the quietest parts to 0. Thus, we process each sample, one at a time, independent of the other samples. </p>
<figure>
<img src="Images/F10011.png" alt="F10011" width="372" height="159"/>
<figcaption><p><a id="figure10-11">Figure 10-11</a>: Each sample consists of a series of measurements of a short audio waveform. Each feature gives us an instantaneous measurement of the volume of the sound at that moment.</p></figcaption>
</figure>
<h3 id="h2-500723c10-0006">Featurewise Processing</h3>
<p class="BodyFirst">The featurewise approach is appropriate when our samples represent essentially different things.</p>
<p>Suppose we’ve taken a variety of weather measurements each evening, recording the temperature, rainfall, wind speed, and humidity. This gives us four features per sample, as in <a href="#figure10-12" id="figureanchor10-12">Figure 10-12</a>.</p>
<figure>
<img src="Images/F10012.png" alt="F10012" width="845" height="509"/>
<figcaption><p><a id="figure10-12">Figure 10-12</a>: When we process our data featurewise, we analyze each column independently. Top three lines: The original data. Middle line: The range. Bottom line: The scaled data.</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="234" id="Page_234"/>It doesn’t make sense to scale this data on a samplewise basis, because the units and measurements are incompatible. We can’t compare the wind speed and the humidity on an equal footing. But we can analyze all of the humidity values together, and the same is true for all the values for temperature, rainfall, and humidity. In other words, we modify each feature in turn.</p>
<p>When we process data featurewise, each column of feature values is sometimes called a <em>fibre</em>.</p>
<h3 id="h2-500723c10-0007">Elementwise Processing</h3>
<p class="BodyFirst">The elementwise approach treats each element in the grid of <a href="#figure10-10">Figure 10-10</a> as an independent entity and applies the same transformation to every element in the grid independently. This is useful, for example, when all of our data represents the same kind of thing, but we want to change its units. For instance, suppose that each sample corresponds to a family with eight members and contains the heights of each of the eight people. Our measurement team reported their heights in inches, but we want the heights in millimeters.</p>
<p>We need only multiply every entry in the grid by 25.4 to convert inches to millimeters. It doesn’t matter if we think of this as working across rows or along columns, since every element is handled the same way.</p>
<p>We do this frequently when we work with images. Image data often arrives with each pixel in the range [0,255]. We often apply an elementwise scaling operation to divide every pixel value in the entire input by 255, giving us data from 0 to 1.</p>
<p>Most libraries allow us to apply transformations using any of these interpretations.</p>
<h2 id="h1-500723c10-0007">Inverse Transformations</h2>
<p class="BodyFirst">We’ve been looking at different transformations that we can apply to our data. However, sometimes we want to undo, or <em>invert</em>, those steps so we can more easily compare our results to our original data. </p>
<p>For example, suppose we work for the traffic department of a city that has one major highway. Our city is far north, so the temperature often drops below freezing. The city managers have noticed that the traffic density seems to vary with temperature, with more people staying home on the coldest days. In order to plan for roadwork and other construction, the managers want to know how many cars they can predict for each morning’s rush-hour commute based on the temperature. Because it takes some time to measure and process the data, we decide to measure the temperature at midnight each evening and then predict how many cars will be on the road between 7 and 8 <span class="SmallCaps">am</span> the next morning. We’re going to start using our system in the middle of winter, so we expect temperatures both above and below freezing (0° Celsius). </p>
<p>For a few months, we measure the temperature at every midnight, and we count the total number of cars passing a particular marker on the road between 7 and 8 <span class="SmallCaps">am</span> the next morning. The raw data is shown in <a href="#figure10-13" id="figureanchor10-13">Figure 10-13</a>.</p>
<p>We want to give this data to a machine-learning system that will learn the connection between temperature and traffic density. After deployment, we feed <span epub:type="pagebreak" title="235" id="Page_235"/>in a sample consisting of one feature, describing the temperature in degrees, and we get back a real number telling us the predicted number of cars on the road.</p>
<figure>
<img src="Images/F10013.png" alt="F10013" width="635" height="454"/>
<figcaption><p><a id="figure10-13">Figure 10-13</a>: Each midnight we measure the temperature, and then the following morning, we measure the number of cars on the road between 7 and 8 <span class="SmallCaps">am</span>.</p></figcaption>
</figure>
<p>Let’s suppose that the regression algorithm we’re using works best when its input data is scaled to the range [0,1]. We can normalize the data to [0,1] on both axes, as in <a href="#figure10-14" id="figureanchor10-14">Figure 10-14</a>.</p>
<figure>
<img src="Images/F10014.png" alt="F10014" width="635" height="439"/>
<figcaption><p><a id="figure10-14">Figure 10-14</a>: Normalizing both ranges to [0,1] makes the data more amenable for training.</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="236" id="Page_236"/>This looks just like <a href="#figure10-13">Figure 10-13</a>, only now both our scales (and data) run from 0 to 1. </p>
<p>We’ve stressed the importance of remembering this transformation so we can apply it to future data. Let’s look at those mechanics in three steps. For convenience, let’s use an object-oriented philosophy, where our transformations are carried out by objects that remember their own parameters.</p>
<p>The first of our three steps is to create a transformer object for each axis. This object is capable of performing this transformation (also called a <em>mapping</em>). </p>
<p>Second, let’s give that object our input data to analyze. It finds the smallest and largest values and uses them to create the transformation that shifts and scales our input data to the range [0,1]. We’ll give the temperature data to the first transformer and the vehicle count data to the second transformer. </p>
<p>So far, we’ve only created the transformers, but we haven’t applied them. Nothing has changed in any of our data.</p>
<p><a href="#figure10-15" id="figureanchor10-15">Figure 10-15</a> shows the idea.</p>
<figure>
<img src="Images/f10015.png" alt="f10015" width="386" height="229"/>
<figcaption><p><a id="figure10-15">Figure 10-15</a>: Building transformation objects. Left: The temperature data is fed to a transformation object, represented by a blue rectangle. Right: We also build a yellow transformer for the car counts.</p></figcaption>
</figure>
<p>The third step is to give our data to the transform objects again, but this time, we tell them to apply the transformation they have already computed. The result is a new set of data that has been transformed to the range [0,1]. <a href="#figure10-16" id="figureanchor10-16">Figure 10-16</a> shows the idea.</p>
<p>Now we’re ready to learn. We give our transformed data to our learning algorithm and let it figure out the relationship between the inputs and the outputs, as shown schematically in <a href="#figure10-17" id="figureanchor10-17">Figure 10-17</a>.</p>
<p>Let’s assume that we’ve trained our system, and it’s doing a good job of predicting car counts from temperature data.</p>
<p>The next day, we deploy our system on a web page for the city managers. On the first night, the manager on duty measures a midnight temperature of −10° Celsius. She opens up our application, finds the input box for the temperature, types in −10, and hits the big “Predict Traffic” button.</p>
<span epub:type="pagebreak" title="237" id="Page_237"/><figure>
<img src="Images/F10016.png" alt="F10016" width="386" height="355"/>
<figcaption><p><a id="figure10-16">Figure 10-16</a>: Each feature is modified by the transformation we previously computed for it. The output of the transformations goes into our learning system.</p></figcaption>
</figure>
<figure>
<img src="Images/F10017.png" alt="F10017" width="386" height="380"/>
<figcaption><p><a id="figure10-17">Figure 10-17</a>: A schematic view of learning from our transformed features and targets</p></figcaption>
</figure>
<p>Uh-oh, something’s gone wrong. We can’t just feed −10 into our trained system, because as <a href="#figure10-17">Figure 10-17</a> shows, it’s expecting a number in the range of 0 to 1. We need to transform the data somehow. The only way that makes sense is to apply the same transformation that we applied to the temperatures when we trained our system. For example, if in our original dataset −10 became the value 0.29, then if the temperature is −10 tonight, we should enter 0.29, not –10. </p>
<p>Here’s where we see the value of saving our transformation as an object. We can simply tell that object to take the same transformation that <span epub:type="pagebreak" title="238" id="Page_238"/>it applied to our training data and now apply it to this new piece of data. If −10 turned into 0.29 during training, any new input of −10 turns into 0.29 during deployment as well.</p>
<p>Let’s suppose that we correctly give the temperature 0.29 to the system, and it produces a traffic density of 0.32. This corresponds to the value of some number of cars transformed by our car transformation. But that value is between 0 and 1, because that was the range of the data we trained on representing car counts. How do we undo that transformation and turn it into a number of cars?</p>
<p>In any machine learning library, every transformation object comes with a routine to <em>invert</em>, or undo, its transformation, providing us with an <em>inverse transformation</em>. In this case, it inverts the normalizing transformation it’s been applying so far. If the object transformed 39 cars into the normalized value 0.32, then the inverse transformation turns the normalized value 0.32 back into 39 cars. This is the value we print out to the city manager. <a href="#figure10-18" id="figureanchor10-18">Figure 10-18</a> shows these steps.</p>
<figure>
<img src="Images/F10018.png" alt="F10018" width="166" height="553"/>
<figcaption><p><a id="figure10-18">Figure 10-18</a>: When we feed a new temperature to our system, we transform it using the transformation we computed for our temperature data, turning it into a number from 0 to 1. The value that comes out is then run through the inverse of the transformation we computed for the car data, turning it from a scaled number into a number of cars.</p></figcaption>
</figure>
<p>One thing that can seemingly go wrong here is if we get new samples outside of the original input range. Suppose we get a surprisingly cold temperature reading one night of −50° Celsius, which is far below the <span epub:type="pagebreak" title="239" id="Page_239"/>minimum value in our original data. The result is that the transformed value is a negative number, outside of our [0,1] range. The same thing can happen if we get a very hot night, giving us a positive temperature that transforms to a value greater than 1, which is again outside of [0,1].</p>
<p>Both situations are fine. Our desire for scaling inputs to [0,1] is to make training go as efficiently as possible, and also to keep numerical issues in check. Once the system is trained, we can give it any values we want as input, and it calculates a corresponding output. Of course, we still have to pay attention to our data. If the system predicts a negative number of cars for tomorrow, we don’t want to make plans based on that number.</p>
<h2 id="h1-500723c10-0008">Information Leakage in Cross-Validation</h2>
<p class="BodyFirst">We’ve seen how to build the transformation from the training set and then retain that transformation and apply it, unchanged, to all additional data. If we don’t follow this policy carefully, we can get <em>information leakage</em>, where information that doesn’t belong in our transformation accidentally sneaks (or leaks) into it, affecting the transformation. This means that we don’t transform the data the way we intend. Worse, this leakage can lead to the system getting an unfair advantage when it evaluates our test data, giving us an overinflated measure of accuracy. We may conclude that our system is performing well enough to be deployed, only to be disappointed when it has much worse performance when used for real.</p>
<p>Information leakage is a challenging problem because many of its causes can be subtle. As an example, let’s see how information leakage can affect the process of cross-validation, which we discussed in Chapter 8. Modern libraries give us convenient routines that provide fast and correct implementations of cross-validation, so we don’t have to write our own code to do it. But let’s look under the hood anyway. We’ll see how a seemingly reasonable approach leads to information leakage, and then how we can fix it. Seeing this in action will help us get better at preventing, spotting, and fixing information leakage in our own systems and algorithms. </p>
<p>Recall that in cross-validation, we set aside one fold (or section) of the starting training set to serve as a temporary validation set. Then we build a new learner and train it with the remaining data. When we’re done training, we evaluate the learner using the saved fold as the validation set. This means that each time through the loop, we have a new training set (the starting data without the samples in the selected fold). If we’re going to apply a transformation to our data, we need to build it from just the data that’s being used as the training set for that learner. We then apply that transformation to the current training set, and we apply <em>the same transformation </em>to the current validation set. The key thing to remember is that because in cross-validation we create a new training set and validation set each time through the loop, we need to build a new transformation each time through the loop as well.</p>
<p>Let’s see what happens if we do it incorrectly. <a href="#figure10-19" id="figureanchor10-19">Figure 10-19</a> shows our starting set of samples at the left. They’re analyzed to produce a <span epub:type="pagebreak" title="240" id="Page_240"/>transformation (shown by the red circle), which is then applied by a transformation routine (marked T). To show the change, we colored the transformed samples red, like the transformation.</p>
<figure>
<img src="Images/f10019.png" alt="f10019" width="694" height="349"/>
<figcaption><p><a id="figure10-19">Figure 10-19</a>: A <em>wrong</em> way to do cross-validation: building one transform based on all the original training data</p></figcaption>
</figure>
<p>Then we enter the cross-validation process. Here the loop is “unrolled,” so we’re showing several instances of the training sessions, each associated with a different fold. Each time through the loop, we remove a fold, train on the remaining samples, and then test with the validation fold and create a score.</p>
<p>The problem here is that when we analyzed the input data to build the transformation, we included the data in every fold in our analysis. To see why this is a problem, let’s look more closely at what’s going on with a simpler and more specific scenario.</p>
<p>Suppose that the transformation we want to apply is scaling all the features in the training set, as a group, to the range 0 to 1. That is, we’ll do samplewise multivariate normalization. Let’s say that in the very first fold, the smallest and largest feature values are 0.01 and 0.99. In the other folds, the largest and smallest values occupy smaller ranges. <a href="#figure10-20" id="figureanchor10-20">Figure 10-20</a> shows the range of data contained in each of the five folds. We’re going to analyze data in all the folds and build our transformation from that.</p>
<p>In <a href="#figure10-20">Figure 10-20</a>, our dataset is shown at the left, split into five folds. Inside each box, we show the range of values in that fold, with 0 at the left and 1 at the right. The top fold has features running from 0.01 to 0.99. The other folds have values that are well within this range. When we analyze all the folds as a group, the range of the first fold dominates, so we only stretch the whole dataset by a little bit.</p>
<span epub:type="pagebreak" title="241" id="Page_241"/><figure>
<img src="Images/F10020.png" alt="F10020" width="694" height="201"/>
<figcaption><p><a id="figure10-20">Figure 10-20</a>: A <em>wrong</em><b> </b>way to transform our data for cross-validation is to transform everything at once before the loop.</p></figcaption>
</figure>
<p>Now let’s proceed with the cross-validation loop. Our input data is the stack of five transformed folds at the far right of <a href="#figure10-20">Figure 10-20</a>. Let’s start by extracting the first fold and setting it aside; then we can train with the rest of the data, and validate. But we’ve done something bad here, because <em>our training data’s transformation was influenced by the validation data</em>. This is a violation of our basic principle that we create the transform using only the values in the training data. But here we used what is now the validation data when we computed our transform. We say that information has <em>leaked</em> from this step’s validation data into the transformation parameters, where it doesn’t belong.</p>
<p>The right way to build the transformation for the training data is to remove the validation data from the samples, then<em> </em>build the transformation from the remaining data, and then apply that transformation to both the training and validation data. <a href="#figure10-21" id="figureanchor10-21">Figure 10-21</a> shows this visually.</p>
<figure>
<img src="Images/f10021.png" alt="f10021" width="694" height="227"/>
<figcaption><p><a id="figure10-21">Figure 10-21</a>: The proper way to transform our data for cross-validation is to first remove the fold samples and then compute the transformation from the data that remains. </p></figcaption>
</figure>
<p>Now we can apply that transformation to both the training set and the validation data. Note that here the validation data ends up way outside the range [0,1], which is no problem, because that data really is more extreme than the training set.</p>
<p>To fix our cross-validation process, we need to use this scheme in the loop and compute a new transformation for every training set. <a href="#figure10-22" id="figureanchor10-22">Figure 10-22</a> shows the right way to proceed.</p>
<span epub:type="pagebreak" title="242" id="Page_242"/><figure>
<img src="Images/F10022.png" alt="F10022" width="694" height="407"/>
<figcaption><p><a id="figure10-22">Figure 10-22</a>: The proper way to do cross-validation</p></figcaption>
</figure>
<p>For each fold we want to use as a validation set, we analyze the starting samples with that fold removed and then apply the resulting transform to both the training set and the validation set in the fold. The different colors show that each time through the loop, we build and apply a different transformation.</p>
<p>We’ve discussed information leakage in the context of cross-validation because it’s a great example of this tricky topic. Happily, the cross-validation routines in modern libraries all do the right thing, so we don’t have to worry about this problem ourselves when we use library routines. But this doesn’t take the responsibility off of us when we write our own code. Information leakage is often subtle, and it can creep into our programs in unexpected ways. It’s important that we always think carefully about possible sources of information leakage when we build and apply transformations.</p>
<h2 id="h1-500723c10-0009">Shrinking the Dataset</h2>
<p class="BodyFirst">We’ve been looking at ways to adjust the numbers in our data, and how to select the numbers that go into each transformation. Now let’s look at a different kind of transformation, designed not just to manipulate the data, but to actually compress it. We will literally create a new dataset that’s smaller than the original training set, typically by removing or combining features in each sample.</p>
<p>This has two advantages: improved speed and accuracy when training. It stands to reason that the less data we need to process during training, the faster our training goes. By going faster, we mean we can pack <span epub:type="pagebreak" title="243" id="Page_243"/>in more learning in a given amount of time, resulting in a more accurate system.</p>
<p>Let’s look at a few ways to shrink our dataset.</p>
<h3 id="h2-500723c10-0008">Feature Selection</h3>
<p class="BodyFirst">If we’ve collected features in our data that are redundant, irrelevant, or otherwise not helpful, then we should eliminate them so that we don’t waste time on them. This process is called <em>feature selection</em>, or sometimes, <em>feature filtering</em>.</p>
<p>Let’s consider an example where some data is actually superfluous. Suppose we’re hand-labeling images of elephants by entering their size, species, and other characteristics into a database. For some reason nobody can quite remember, we also have a field for the number of heads. Elephants have only one head, so that field’s going to be nothing but 1’s. So that data is not just useless, it also slows us down. We ought to remove that field from our data.</p>
<p>We can generalize this idea to removing features that are <em>almost </em>useless, that contribute very little, or that simply make the least contribution to getting the right answer. Let’s continue with our collection of elephant images. We have values for each animal’s height, weight, last known latitude and longitude, trunk length, ear size, and so on. But for this (imaginary) species, the trunk length and ear size may be closely correlated. If so, we can remove (or <em>filter out</em>) either one and still get the benefit of the information they each represent.</p>
<p>Many libraries offer tools that can estimate the impact of removing each field from a database. We can then use this information to guide us in simplifying our database and speeding our learning without sacrificing more accuracy than we’re willing to give up. Because removing a feature is a transformation, any features we remove from our training set must be removed from all future data as well.</p>
<h3 id="h2-500723c10-0009">Dimensionality Reduction</h3>
<p class="BodyFirst">Another approach to reducing the size of our dataset is combining features, so one feature can do the work of two or more. This is called <em>dimensionality reduction</em>, where <em>dimensionality</em> refers to the number of features. </p>
<p>The intuition here is that some of the features in our data might be closely related without being entirely redundant. If the relationship is strong, we might be able to combine those two features into just one new one. An everyday example of this is the <em>body mass index (BMI)</em>. This is a single number that combines a person’s height and weight. Some measurements of a person’s health can be computed with just the BMI. For example, charts that help people decide if they need to lose weight can be conveniently indexed by age and BMI (CDC 2017).</p>
<p>Let’s look at a tool that automatically determines how to select and combine features to make the smallest impact on our results.</p>
<h2 id="h1-500723c10-0010"><span epub:type="pagebreak" title="244" id="Page_244"/>Principal Component Analysis</h2>
<p class="BodyFirst"><em>Principal component analysis (PCA)</em>, is a mathematical technique for reducing the dimensionality of data. Let’s get a visual feel for PCA by looking at what it does to our guitar data. </p>
<p><a href="#figure10-23" id="figureanchor10-23">Figure 10-23</a> shows our starting guitar data again. As before, the colors of the dots are just to make it easier to track them in the following figures as the data is manipulated, and don’t have any other meaning.</p>
<figure>
<img src="Images/F10023.png" alt="F10023" width="482" height="468"/>
<figcaption><p><a id="figure10-23">Figure 10-23</a>: The starting data for our discussion of PCA </p></figcaption>
</figure>
<p>Our goal is to crunch this two-dimensional data down to one-dimensional data. That is, we combine each set of paired x and y values to create a single new number based on both of them, just as BMI is a single number that combines a person’s height and weight. </p>
<p>Let’s start by standardizing the data. <a href="#figure10-24" id="figureanchor10-24">Figure 10-24</a> shows this combination of setting each dimension to have a mean of 0 and a standard deviation of 1, as we saw before.</p>
<p>We already know that we’re going to try to reduce this 2D data to just 1D. To get a feel for the idea before we actually apply it, let’s go through the process with one key step missing, and then we’ll put that step back in. </p>
<p>To get started, let’s draw a horizontal line on the X axis. We’ll call this the <em>projection line</em>. Then we’ll <em>project</em>, or move, each data point to its closest spot on the projection line. Because our line is horizontal, we only need to move our points up or down to find their closest point on the projection line.</p>
<span epub:type="pagebreak" title="245" id="Page_245"/><figure>
<img src="Images/F10024.png" alt="F10024" width="453" height="468"/>
<figcaption><p><a id="figure10-24">Figure 10-24</a>: Our input data after standardizing</p></figcaption>
</figure>
<p>The results of projecting the data in <a href="#figure10-24">Figure 10-24</a> onto a horizontal projection line are shown in <a href="#figure10-25" id="figureanchor10-25">Figure 10-25</a>.</p>
<figure>
<img src="Images/F10025.png" alt="F10025" width="453" height="468"/>
<figcaption><p><a id="figure10-25">Figure 10-25</a>: We project each data point of the guitar by moving it to its nearest point on the projection line. For clarity, we’re showing the path taken by only about 25 percent of the points. </p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="246" id="Page_246"/>The results after all the points are processed is shown in <a href="#figure10-26" id="figureanchor10-26">Figure 10-26</a>.</p>
<figure>
<img src="Images/F10026.png" alt="F10026" width="453" height="468"/>
<figcaption><p><a id="figure10-26">Figure 10-26</a>: The result of <a href="#figure10-25">Figure 10-25</a> after all the points have been moved to the projection line. Each point now is described only by its x coordinate, resulting in a one-dimensional dataset.</p></figcaption>
</figure>
<p>This is the one-dimensional dataset we were after, because the points only differ by their x value (the y value is always 0, so it’s irrelevant). But this would be a lousy way to combine our features, because all we did was throw away the y values. It’s like computing BMI by simply using the weight and ignoring the height.</p>
<p>To improve the situation, let’s include the step we skipped. Instead of using a horizontal projection line, we rotate the line around until it’s passing through the direction of maximum variance. Think of this as the line that, after projection, has the largest range of points. Any library routine that implements PCA finds this line for us automatically. <a href="#figure10-27" id="figureanchor10-27">Figure 10-27</a> shows this line for our guitar data.</p>
<span epub:type="pagebreak" title="247" id="Page_247"/><figure>
<img src="Images/F10027.png" alt="F10027" width="443" height="457"/>
<figcaption><p><a id="figure10-27">Figure 10-27</a>: The thick black line is the line of maximum variance through our original data. This is our projection line.</p></figcaption>
</figure>
<p>Now we continue just like before. We project each point onto this projection line by moving it to its closest point on the line. As before, we do this by moving perpendicular to the line until we intersect it. <a href="#figure10-28" id="figureanchor10-28">Figure 10-28</a> shows this process.</p>
<figure>
<img src="Images/F10028.png" alt="F10028" width="443" height="457"/>
<figcaption><p><a id="figure10-28">Figure 10-28</a>: We project the guitar data onto the projection line by moving each point to its closest point on the line. For clarity, we’re showing the path taken by only about 25 percent of the points.</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="248" id="Page_248"/>The projected points are shown in <a href="#figure10-29" id="figureanchor10-29">Figure 10-29</a>. Note that they all lie on the line of maximum variance that we found in <a href="#figure10-27">Figure 10-27</a>.</p>
<figure>
<img src="Images/f10029.png" alt="f10029" width="433" height="447"/>
<figcaption><p><a id="figure10-29">Figure 10-29</a>: The points of the guitar dataset projected onto the line of maximum variance</p></figcaption>
</figure>
<p>For convenience, we can rotate this line of points to lie on the X axis, as shown in <a href="#figure10-30" id="figureanchor10-30">Figure 10-30</a>. Now the y coordinate is irrelevant again, and we have 1D data that includes information about each point from both its original x and y values.</p>
<figure>
<img src="Images/F10030.png" alt="F10030" width="433" height="447"/>
<figcaption><p><a id="figure10-30">Figure 10-30</a>: Rotating the points of <a href="#figure10-29">Figure 10-29</a> into a horizontal position</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="249" id="Page_249"/>Although the straight line of points in <a href="#figure10-30">Figure 10-30</a> looks roughly like the line of points in <a href="#figure10-26">Figure 10-26</a>, they’re different, because the points are distributed differently along the X axis. In other words, they have different values, because they were computed by projecting onto a tilted line, rather than a horizontal one. <a href="#figure10-31" id="figureanchor10-31">Figure 10-31</a> shows the two projections together. The PCA result is not just longer, but the points are also distributed differently.</p>
<figure>
<img src="Images/F10031.png" alt="F10031" width="694" height="308"/>
<figcaption><p><a id="figure10-31">Figure 10-31</a>: Comparing the points created by projection to y = 0 in <a href="#figure10-26">Figure 10-26</a> (top) and the PCA algorithm in <a href="#figure10-30">Figure 10-30</a> (bottom) </p></figcaption>
</figure>
<p>All of the steps we just discussed are carried out automatically by a machine learning library when we call its PCA routine.</p>
<p>The beauty of the 1D data created by this projection step is that every point’s single value (its x value) is a combination of the 2D data it started with. We reduced the dimensionality of our dataset by one dimension, but we did so while retaining as much information as we could. Our learning algorithms now only have to process one feature rather than two, so they’ll run faster. Of course, we’ve thrown some information away, so the accuracy may suffer. The trick to using PCA effectively is to choose dimensions that can be combined while still staying within the performance goals of our project.</p>
<p>If we have 3D data, we can imagine placing a plane in the middle of the cloud of samples and projecting our data down onto the plane. The library’s job is to find the best orientation of that plane. This takes our data from 3D to 2D. If we want to go all the way down to 1D, we can imagine projecting the data onto a line through the volume of points. In practice, we can use this technique in problems with any number of dimensions, where we may reduce the dimensionality of the data by tens or more.</p>
<p>The critical questions for this kind of algorithm include: How many dimensions should we try to compress? Which dimensions should be combined? How they should get combined? We usually use the letter <em>k </em>to stand for the number of dimensions remaining in our data after PCA has done its work. So, in our guitar example, <em>k </em>was 1. We can call <em>k </em>a parameter of the algorithm, though usually we call it a hyperparameter<b> </b>of the entire learning <span epub:type="pagebreak" title="250" id="Page_250"/>system. As we’ve seen, the letter <em>k </em>is used for lots of different algorithms in machine learning, which is unfortunate; it’s important to pay attention to the context when we see references to <em>k</em>.</p>
<p>Compressing too little means our training and evaluation steps are going to be inefficient, but compressing too much means we risk eliminating important information that we should have kept. To pick the best value for the hyperparameter <em>k</em>, we usually try out a few different values to see how they do and then pick the one that seems to work best. We can automate this search using the techniques of <em>hyperparameter searching</em> provided by many libraries.</p>
<p>As always, whatever PCA transformations we use to compress our training data must also be used in the same way for all future data.</p>
<h3 id="h2-500723c10-0010">PCA for Simple Images</h3>
<p class="BodyFirst">Images are an important and special kind of data. Let’s apply PCA to a simple set of images.</p>
<p><a href="#figure10-32" id="figureanchor10-32">Figure 10-32</a> shows a set of six images, perhaps drawn from a huge dataset of tens of thousands of such pictures. If these grayscale images are 1,000 pixels on a side, each contains 1,000 × 1,000, or 1 million, pixels. Is there any better way to represent them than with a million numbers each?</p>
<figure>
<img src="Images/F10032.png" alt="F10032" width="844" height="117"/>
<figcaption><p><a id="figure10-32">Figure 10-32</a>: Six images we’d like to represent</p></figcaption>
</figure>
<p>Let’s start by observing that each image in <a href="#figure10-32">Figure 10-32</a> can be re-created from the three images in <a href="#figure10-33" id="figureanchor10-33">Figure 10-33</a>, each scaled by a different amount and then added together.</p>
<figure>
<img src="Images/F10033.png" alt="F10033" width="406" height="118"/>
<figcaption><p><a id="figure10-33">Figure 10-33</a>: We can create all six images in <a href="#figure10-32">Figure 10-32</a> by scaling these three by different amounts and adding the results.</p></figcaption>
</figure>
<p>For example, we can reconstruct the first image in <a href="#figure10-32">Figure 10-32</a> by adding 20 percent of the circle, 70 percent of the vertical box, and 40 percent of the horizontal box. We often call these scaling factors the <em>weights</em>. The weights for each of the six starting images are shown in <a href="#figure10-34" id="figureanchor10-34">Figure 10-34</a>.</p>
<span epub:type="pagebreak" title="251" id="Page_251"/><figure>
<img src="Images/F10034.png" alt="F10034" width="844" height="274"/>
<figcaption><p><a id="figure10-34">Figure 10-34</a>: The weights to use when scaling the three images in <a href="#figure10-33">Figure 10-33</a> to recover the images in <a href="#figure10-32">Figure 10-32</a></p></figcaption>
</figure>
<p><a href="#figure10-35" id="figureanchor10-35">Figure 10-35</a> shows the process of scaling and combining the components to recover the first original image.</p>
<p>In general, we can represent any image of this type with the three weights of the component images that make the best match. To reconstruct any of the input images, we need the three simpler pictures (1 million values each) plus the three numbers for that specific image. If we had 1,000 images, storing each one would take a total of 1,000 megabytes. But using this compressed form, we need a total of only 3.001 megabytes.</p>
<figure>
<img src="Images/F10035.png" alt="F10035" width="694" height="130"/>
<figcaption><p><a id="figure10-35">Figure 10-35</a>: Recovering the first image in <a href="#figure10-32">Figure 10-32</a> by scaling the images in <a href="#figure10-33">Figure 10-33</a></p></figcaption>
</figure>
<p>For these simple images, it’s easy to find the components as three geometric shapes. But when we have more realistic pictures, this won’t generally be possible. </p>
<p>The good news is that we can use the projection technique we discussed earlier. Instead of projecting a collection of points to create a new set of points on a line, we can project a collection of images to create a new image. This is a more abstract process than the one we followed with the guitar points, but the concept is the same. Let’s get a feeling for how PCA handles images by skipping the mechanics and focusing on the results.</p>
<p>Consider again the six starting images of <a href="#figure10-32">Figure 10-32</a>. Remember that these are grayscale images, not vector drawings. Let’s ask PCA to find a grayscale image that comes closest to representing all the images, in the same way that our diagonal line came closest to representing all the points in our guitar. Then we can represent each starting image as a sum of this image, scaled by an appropriate amount, plus whatever is left over. <a href="#figure10-36" id="figureanchor10-36">Figure 10-36</a> shows this process.</p>
<span epub:type="pagebreak" title="252" id="Page_252"/><figure>
<img src="Images/F10036.png" alt="F10036" width="842" height="383"/>
<figcaption><p><a id="figure10-36">Figure 10-36</a>: Running PCA on the images from <a href="#figure10-32">Figure 10-32</a> </p></figcaption>
</figure>
<p>The starting images from 10-32 are shown at the top of <a href="#figure10-36">Figure 10-36</a>.</p>
<p>Now we ask PCA to find an image that corresponds to the line in <a href="#figure10-28">Figure 10-28</a>. That is, an image that, in some sense, captures something from all of the inputs. Let’s say it found the picture at the left in <a href="#figure10-36">Figure 10-36</a>, showing the two bars and the circle all in black and overlaid. We’ll call this the <em>shared image</em>, which isn’t a formal term but is useful here.</p>
<p>Let’s now represent each image at the top of <a href="#figure10-36">Figure 10-36</a> as a combination of a scaled version of the shared image, and some other image. To do this, we find the lightest pixel in each input image, and scale the shared image to that intensity. That scaling factor is shown at the top of the copies of the common image in the middle row, where the copies have been scaled in intensity by that amount. If we subtract each scaled common image from the source image above it in the figure, we get their difference. We could write this as “source – common = difference,” or equivalently, “source = common + difference,” which is shown in the figure.</p>
<p>We can then run PCA again, this time on the bottom row of <a href="#figure10-36">Figure 10-36</a>. Again, it projects these six images to create a new image that is the best match for all of them. As before, we can represent each picture as the sum of a scaled version of what’s common, plus whatever is left. <a href="#figure10-37" id="figureanchor10-37">Figure 10-37</a> shows the idea. In this demonstration, we’re supposing that PCA created an image of the two overlapping boxes as the best matching image. </p>
<span epub:type="pagebreak" title="253" id="Page_253"/><figure>
<img src="Images/F10037.png" alt="F10037" width="844" height="385"/>
<figcaption><p><a id="figure10-37">Figure 10-37</a>: Running PCA on the bottom row of <a href="#figure10-36">Figure 10-36</a> </p></figcaption>
</figure>
<p>Something interesting happened in two images on the bottom row. Let’s look at the second column from the left. The image we want to match on the top row is a circle and horizontal box, but we’re trying to match it with a scaled pair of crossed boxes. To match the top image, we need to add in some of the circle, but subtract the vertical box that we just introduced. That just means setting the corresponding data in the bottom image to negative values. These are perfectly valid numbers to place into our data, though we have to be careful if we try to display this image directly. If we reconstruct the top image by adding up the two images beneath it, the negative values in the red region in the bottom row cancel out the positive values in the vertical box in the middle row, so the sum of these images matches the circle and horizontal box at the top. The same reasoning applies to the horizontal box in the rightmost column.</p>
<p><a href="#figure10-38" id="figureanchor10-38">Figure 10-38</a> summarizes the two steps we’ve seen so far.</p>
<p>We took only two steps here, but we can repeat this process dozens or hundreds of times. </p>
<p>To represent each starting image, we only need the collection of common images and the weight we assigned to each. Since the common images are shared by all the images, we can consider them a shared resource. Then each image can be completely described by a reference to this shared resource, and the list of weights to be applied to the common images. </p>
<span epub:type="pagebreak" title="254" id="Page_254"/><figure>
<img src="Images/F10038.png" alt="F10038" width="844" height="536"/>
<figcaption><p><a id="figure10-38">Figure 10-38</a>: Representing each starting image (top row) as the sum of two scaled component images (second and third rows), plus whatever’s left (bottom row)</p></figcaption>
</figure>
<p>Each of the common images is called a <em>component</em>, and the one we create at every step is the <em>principal</em> component (since it’s the best of all possible components, in the same way that the line in <a href="#figure10-28">Figure 10-28</a> was the best line). We find these principal components by analyzing the input images. Hence, the name <em>principal component analysis</em>.</p>
<p>The more components we include, the more accurately each reconstructed image will match its original. We usually aim to produce enough components so that each reconstructed image has all the qualities we care about in its original. </p>
<p>In this discussion, most of the weights were positive, but we also saw some component images that should be subtracted, not added, and thus they produce weights with negative values. This is so that the final pixels, when all the components are summed together, have the desired values. </p>
<p>How well did we do with just two component images? <a href="#figure10-39" id="figureanchor10-39">Figure 10-39</a> shows the original six images and our reconstructed images using the sum of the middle two rows of <a href="#figure10-38">Figure 10-38</a>.</p>
<p>The matches aren’t perfect, but they’re a good start, particularly for just two components. So, we have a common pool of two images (requiring 1 million numbers each), and then each image itself can be described with just two numbers. The beauty of this scheme is that our algorithms never need to see the common images. We just need those for finding the weights that describe each input image (and to reconstruct the images, if we want). As far as the learning algorithm is concerned, each image is described by <span epub:type="pagebreak" title="255" id="Page_255"/>only two numbers. This means our algorithms consume less memory and run more quickly. </p>
<figure>
<img src="Images/F10039.png" alt="F10039" width="839" height="264"/>
<figcaption><p><a id="figure10-39">Figure 10-39</a>: Our starting six images (top), and the reconstructed images from <a href="#figure10-35">Figure 10-35</a>c (bottom)</p></figcaption>
</figure>
<p>We skipped a step in this example: normally when we use PCA, we standardize all the images first. That’s included in our next example.</p>
<h3 id="h2-500723c10-0011">PCA for Real Images</h3>
<p class="BodyFirst">The images in the last section were contrived for simplicity. Now we’ll apply PCA to real pictures.</p>
<p>Let’s begin with the six pictures of huskies shown in <a href="#figure10-40" id="figureanchor10-40">Figure 10-40</a>. To make our processing easier to see, these images are only 45 × 64 pixels. These were aligned by hand so that the eyes and nose are in about the same location in each image. This way each pixel in each image has a good chance of representing the same part of a dog as the corresponding pixel in the other images. For instance, a pixel just below the center is likely to part of a nose, one near the upper corners is likely to be an ear, and so on.</p>
<figure>
<img src="Images/f10040.png" alt="f10040" width="694" height="155"/>
<figcaption><p><a id="figure10-40">Figure 10-40</a>: Our starting set of huskies</p></figcaption>
</figure>
<p>A database of six dogs isn’t a lot of training data, so let’s enlarge our database using the idea of <em>data augmentation, </em>a common strategy for <em>amplifying </em>or <em>enlarging </em>a dataset. In this case, let’s run through our set of six images in random order over and over. Each time through, we’ll pick an image, make a copy, randomly shift it horizontally and vertically up to 10 percent on each axis, rotate it up to five degrees clockwise or counterclockwise, and maybe flip it left to right. Then we append that transformed image to our training set. <a href="#figure10-41" id="figureanchor10-41">Figure 10-41</a> shows the results of the first two passes through <span epub:type="pagebreak" title="256" id="Page_256"/>our six dogs. We used this technique of creating variations to build a training set of 4,000 dog images.</p>
<figure>
<img src="Images/F10041.png" alt="F10041" width="694" height="359"/>
<figcaption><p><a id="figure10-41">Figure 10-41</a>: Each row shows a set of new images created by shifting, rotating, and perhaps horizontally flipping each of our input images. </p></figcaption>
</figure>
<p>Since we’d like to run PCA on these images, our first step is to standardize them. This means we analyze the same pixel in each of our 4,000 images and adjust the collection to have zero mean and unit variance (Turk and Pentland 1991). The standardized versions of our first six generated dogs are shown in <a href="#figure10-42" id="figureanchor10-42">Figure 10-42</a>.</p>
<figure>
<img src="Images/F10042.png" alt="F10042" width="694" height="156"/>
<figcaption><p><a id="figure10-42">Figure 10-42</a>: Our first six huskies after standardization</p></figcaption>
</figure>
<p>Since 12 images fit nicely into a figure, let’s begin by arbitrarily asking PCA to find the 12 images that, when added together with appropriate weights, best reconstruct the input images. Each of the projections (or components) found by PCA is technically known as an <em>eigenvector</em>, from the German <em>eigen </em>meaning “own” (or roughly “self”), and <em>vector </em>from the mathematical name for this kind of object. When we create eigenvectors of particular types of things, it’s common to create a playful name by combining the prefix eigen with the object we’re processing. Hence, <a href="#figure10-43" id="figureanchor10-43">Figure 10-43</a> shows our 12 <em>eigendogs</em>.</p>
<span epub:type="pagebreak" title="257" id="Page_257"/><figure>
<img src="Images/F10043.png" alt="F10043" width="694" height="380"/>
<figcaption><p><a id="figure10-43">Figure 10-43</a>: The 12 eigendogs produced by PCA</p></figcaption>
</figure>
<p>Looking at the eigendogs tells us a lot about how PCA is analyzing our images. The first eigendog is a big smudge that is darker roughly where most of the dogs appear in the image. This is the single image that comes closest to approximating every input image. The second eigendog gives us refinements to the first, capturing some of the left-right shading differences.</p>
<p>The next eigendog provides additional detail, and so it goes through all 12 eigendogs. So, the first eigendog captures the broadest, most common features, and each additional eigendog lets us recover a little more detail.</p>
<p>PCA is able not only to create the eigendogs of <a href="#figure10-43">Figure 10-43</a>, but also to take as input any picture, and tell us the weight to apply to each eigendog image so that, when the weighted images are added together, we get the best approximation to the input image.</p>
<p>Let’s see how well we can recover our original images by combining these 12 eigendogs with their corresponding weights. <a href="#figure10-44" id="figureanchor10-44">Figure 10-44</a> shows the weights that PCA found for each input image. We create the reconstructed dogs by scaling each eigendog image from <a href="#figure10-43">Figure 10-43</a> with its corresponding weight, and then adding the results together.</p>
<span epub:type="pagebreak" title="258" id="Page_258"/><figure>
<img src="Images/f10044.png" alt="f10044" width="694" height="375"/>
<figcaption><p><a id="figure10-44">Figure 10-44</a>: Reconstructing our original inputs from a set of 12 eigendogs. Top row: The reconstructed dogs. Bottom row: The weights applied to the eigendogs of <a href="#figure10-43">Figure 10-43</a> to build the image directly above. Notice that the vertical scales on the bottom row are not all the same.</p></figcaption>
</figure>
<p>The recovered dogs in <a href="#figure10-44">Figure 10-44</a> are not great. We’ve asked PCA to represent all 4,000 images in our training set with just 12 pictures. It did its best, but these results are pretty blurry. They do seem to be on the right track, though.</p>
<p>Let’s try using 100 eigendogs. The first 12 eigendog images look just like those in <a href="#figure10-43">Figure 10-43</a>, but then they get more complicated and detailed. The results of reconstructing our first set of 6 dogs are shown in <a href="#figure10-45" id="figureanchor10-45">Figure 10-45</a>.</p>
<figure>
<img src="Images/F10045.png" alt="F10045" width="694" height="375"/>
<figcaption><p><a id="figure10-45">Figure 10-45</a>: Reconstructing our original inputs from a set of 100 eigendogs </p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="259" id="Page_259"/>That’s better! They’re starting to look like dogs. But it seems that 100 eigendogs is still not enough.</p>
<p>Let’s crank up our number of eigendogs to 500 and try again. <a href="#figure10-46" id="figureanchor10-46">Figure 10-46</a> shows the results.</p>
<figure>
<img src="Images/F10046.png" alt="F10046" width="694" height="375"/>
<figcaption><p><a id="figure10-46">Figure 10-46</a>: Reconstructing our original inputs from a set of 500 eigendogs </p></figcaption>
</figure>
<p>These are looking pretty great. They are all easily recognized as the 6 standardized dogs in <a href="#figure10-42">Figure 10-42</a>. They’re not perfect, but considering that we’re adding together different amounts of 500 shared images, we’ve done a fine job of matching the original images. There’s nothing special about these first 6 images. If we look at any of the 4,000 images in our database, they all look this good. We could keep increasing the number of eigendogs, and the results would continue to improve, with the images getting increasingly sharper and less noisy.</p>
<p>In each plot of the weights, the eigendog images that get the most weight are the ones at the start, which capture the big structures. As we work our way down the list, each new eigendog is generally weighted a little less than the one before, so it contributes less to the overall result.</p>
<p>The value of PCA here is not that we can make images that look just like the starting set, but rather, that we can use the eigendogs’ representation to reduce the amount of data our deep learning system has to process. This is illustrated in <a href="#figure10-47" id="figureanchor10-47">Figure 10-47</a>. Our set of input dogs goes into PCA, which generates a set of eigendogs. Then each dog we’d like to classify goes into PCA again, which gives us the weights for that image. Those are the values that go into the classifier. </p>
<p>As we mentioned earlier, rather than training our categorizer on all of the pixels from each image, we can train it on just that image’s 100 or 500 weights. The categorizer never sees a full image of a million pixels. It never even sees the eigendogs. It just gets a list of the weights for each image, and that’s the data it uses for analysis and prediction during training. When we <span epub:type="pagebreak" title="260" id="Page_260"/>want to classify a new image, we provide just its weights, and the computer gives us back a class. This can save a lot of computation, which translates to a savings in time, and perhaps increased quality of final results.</p>
<figure>
<img src="Images/F10047.png" alt="F10047" width="844" height="690"/>
<figcaption><p><a id="figure10-47">Figure 10-47</a>: In the top row we first we use PCA to build a set of eigendogs, and then in the bottom row we find the weights for each input to our classifier, which only uses those weights to find the input’s class. </p></figcaption>
</figure>
<p>To summarize, the data we hand the classifier is not each input image, but its weights. The classifier then proceeds to work out which breed of dog it’s looking at based just on those weights. Often we need only a few hundred weights to represent input samples with many thousands, or even millions, of features. </p>
<h2 id="h1-500723c10-0011">Summary</h2>
<p class="BodyFirst">In this chapter, we looked at ways to prepare data. We saw that it’s important to inspect our data before we do anything with it and make sure that it’s clean. Once our data is clean, we can transform it to better fit our learning algorithms in a number of ways. These transformations are built from the training data only. It’s important to remember that any transforms we <span epub:type="pagebreak" title="261" id="Page_261"/>apply to the training data must then be applied to every additional sample we give to our algorithm, from validation and test data to deployment data provided by real-world users.</p>
<p>In the next chapter, we’ll dig into classifiers and survey some of the most important algorithms for the job. </p>
</section>
</div></body></html>