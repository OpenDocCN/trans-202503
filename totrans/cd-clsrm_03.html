<html><head></head><body>
<div>&#13;
<header>&#13;
<h1 class="ChapterStart"><a id="_idTextAnchor041"/>3</h1>&#13;
<h1 class="ChapterTitle"><a id="_idTextAnchor042"/>Computer Science History</h1>&#13;
</header>&#13;
<p class="ChapterIntroFirst">In this chapter, you’ll learn the technical details of every ­logical layer inside your computer, from what you see on your monitor to the electronic circuits that move bits of data. Learning this information is an exercise in <em class="EmphasisItalic">decomposition</em>. You’ll break down a highly complex system, the computer, into some of its smaller parts to understand them. Students familiar with the end-to-end operations of a computer will have deeper technical insights and appreciation of computer systems in general.</p>&#13;
<p class="ChapterIntroNext">As we move down each layer of logic, we’ll travel through history, going back to times when those layers were primarily how humans worked with computers. We’ll go from the point-and-click interfaces we use today back to when programming required flipping electric switches and soldering circuits.  </p>&#13;
<p class="Body">You’ll learn how each innovation hides the complexity of the layer below it and the importance of <em class="EmphasisItalic">abstractions</em>, names and symbols that allow users to interact with computers without having to know the complex details of how they operate. Students should appreciate how each generation of programmers developed abstractions that made computers more accessible to the generations that followed.</p>&#13;
<p class="Body">Students should realize how much they don’t know and can’t know about these immensely complex computing systems. The modern computer is a vast ecosystem of solutions built up through generations of innovators. Also, students should acknowledge that all people are ignorant in some areas of computer science and approach the subject with personal humility, sensitivity to peers who aren’t aware of certain facts, and deep gratitude for everyone who has contributed to making computer science more accessible for all.</p>&#13;
<h2 class="HeadA"><a id="_idTextAnchor043"/>The User Interface</h2>&#13;
<p class="BodyFirst">Most of us are familiar with the icons that represent certain programs. But how often do we think about the real-world objects these icons are based on? Figure 3-1 shows some of the many icons that identify various applications.</p>&#13;
<figure>&#13;
<p class="AnchoredGraphic"><img alt="" class="_idGenObjectAttribute-1" src="image/CitC03_01_Metaphors.png"/></p>&#13;
<figcaption class="Caption">Figure 3-1: The many metaphors used to abstract away computational complexity</figcaption>&#13;
</figure>&#13;
<p class="Body">For example, paper folders represent file locations, cogs and gears represent settings, and postal envelopes represent electronic mail. These icons use objects from the physical world to interface with algorithms and information architectures that are incomprehensibly complex to the human mind. These technical complexities become accessible to us through an interface that presents only the abstraction: a single icon we can click without a second thought. </p>&#13;
<p class="Body">So what does the Save icon do on the computer? When we click this icon, we send a command to the software we’re using, which in turn communicates with the computer’s operating system to save the file. The operating system takes the contents of what we’re saving from working memory and sends a command to the hard drive controllers to store the contents in its long-term memory along with a directory address. The long-term memory hardware, such as a hard drive or flash drive, commits the contents in bytes of bits stored along various physical locations within the device. </p>&#13;
<p class="Body">This complex, highly technical chain of events is completely abstracted away from the end user. The user simply clicks a disk icon, watches an indicator spin, and moves on to initiating the next mind-bafflingly complex sequence of processes hidden behind this wall of abstractions.</p>&#13;
<aside epub:type="sidebar">&#13;
<div class="top hr"><hr/></div>&#13;
<section class="feature1">&#13;
<h2 class="BoxHead">Computational Collectibles</h2>&#13;
<p class="BoxBodyFirst">How will the icons in use today be perceived generations from now when the real-world items they represent are ancient history? A popular meme on social media has a father showing his son a decades-old 3.25-inch floppy disk, to which the son replies, “Neat! You 3D-printed a Save icon!” In fact, a 2013 TeachHUB survey of 1,000 K–5th graders found only 14 percent of them knew what the Save icon was in real life. Like the technology it represents, this knowledge is rapidly becoming obsolete. Ask your students about these icons. Are they aware of their real-world counterparts? Show them how these programs emulate the functions these older innovations provided.</p>&#13;
<p class="BoxBody">Long before electronic computers, humans were using all sorts of devices to cognitively offload computational problems. Curating a collection of these now obsolete devices can provide your students with many opportunities for hands-on exploratory activities. Here are a few examples of artifacts you can bring into the classroom for students to see how many abstract computational concepts are modeled in real-world devices.</p>&#13;
<p class="BoxListPlainA"><strong class="EmphasisBoldBox">Mechanical computing</strong> For early mechanical devices, a <em class="EmphasisItalicBox">suanpan</em>, or Chinese abacus, is great for children to practice cardinality and learn number places. For advanced students, the device is useful for exploring the bi-quinary numeral encoding scheme and hexadecimal number system. There are algorithms students can practice with the device for division and evaluating polynomials.</p>&#13;
<p class="BoxListPlainA">Other computational devices to consider are astrolabes for charting the stars and sextants for navigation, both tools for computing trigonometry problems. The slide rule is a mechanical analog computer that was indispensable for calculating exponents, roots, and logarithms. Slide rules often came with a book of algorithms for calculations and tables of precalculated results for equations.</p>&#13;
<p class="BoxListPlainB"><strong class="EmphasisBoldBox">E-waste</strong> The accelerating pace of technology makes it seem like today’s top-of-the-line computer will be worthless junk in just a few years. Rescue some electronic waste from the landfill to let your students explore. An old desktop computer that still boots up can provide a great class project for learning about the main components of computers. Spend half an hour taking the computer apart and explaining the purpose of the hard drive, CPU, motherboard, video card, networking card, RAM, and other parts. Then spend the second half of class putting it all back together and enjoy the sense of wonder when it boots up again at the end. Punch cards, vacuum tubes, and giant hard drives that carry less memory than a modern, thumbnail-sized SD card can also illustrate how far computing has advanced over the years.</p>&#13;
<p class="BoxListPlainCLast"><strong class="EmphasisBoldBox">3D printing</strong> The growing 3D printing frontier of innovation is revealing new ideas in mechanical computation. Designers are modeling hand-cranked gear-driven calculators and re-creating antique computational toys. Some contraptions, like <em class="EmphasisItalicBox">Turing Tumble</em>, use marbles to flip switches and mechanically<a id="_idTextAnchor044"/> simulate Boolean operations. You can find many other manipulatives online and bring them into your classroom to bring computational concepts to life.</p>&#13;
<div class="bottom hr"><hr/></div>&#13;
</section>&#13;
</aside><p class="Body">What might surprise your students is that behind these abstractions is <em class="EmphasisItalic">another</em> layer of abstractions. These icons might trigger programming functions in the code with names like <span class="Literal _idGenCharOverride-1">loadContactList()</span>, <span class="Literal _idGenCharOverride-1">checkForNewMessages()</span>, or <span class="Literal _idGenCharOverride-1">plotNavigation­BetweenTwoPoints()</span>, themselves representations that abstract away complexity for easy use. Figure 3-2 shows the many levels of coding abstractions, starting with the user interface (UI) and descending to the machine’s architecture.</p>&#13;
<figure>&#13;
<p class="AnchoredGraphic"><img alt="" class="_idGenObjectAttribute-2" src="image/CitC03_02_LanguageLevelsDiagram.png"/></p>&#13;
<figcaption class="Caption">Figure 3-2: The levels of code and languages between the user and the computer’s hardware</figcaption>&#13;
</figure>&#13;
<p class="Body">In this diagram, we see many levels of abstraction between the user and the computer’s hardware. As we move down the levels, the code examples grow increasingly challenging to understand because the syntax more closely conforms to the computer’s architecture and hardware configuration. In the next section, we’ll look at the code just beneath the UI.</p>&#13;
<h2 class="HeadA"><a id="_idTextAnchor045"/>High-Level Code</h2>&#13;
<p class="BodyFirst">Just below the UI is the <em class="EmphasisItalic">high-level code</em>, code where the syntax is more legible to humans than the code further down the stack. This is the code your students will work with most often in class and in the professional world. Listing 3-1 shows an elegant bit of high-level code that draws polygons in the web browser. <em class="EmphasisItalic">Comments</em>, human-readable annotations the computer ignores, are included after each <span class="Literal _idGenCharOverride-1">//</span> in the code to explain some of the functions.</p>&#13;
<pre>var drawPolygon = function(context, x, y, radius, sides, color) {&#13;
  //context is our cursor&#13;
  //translate moves the cursor to coordinates x,y&#13;
  context.translate(x,y);&#13;
  //Move the cursor out the length of the radius&#13;
  context.moveTo(radius,0);&#13;
  //Calculate the angle between sides&#13;
  var a = (Math.PI * 2)/sides;&#13;
  //For each side...&#13;
  for (var i = 1; i &lt; sides; i++) {&#13;
  //Draw a line between x, y coordinates&#13;
  //according to the calculated angle&#13;
    context.lineTo(radius*Math.cos(a*i),radius*Math.sin(a*i));&#13;
  }&#13;
  //Close the drawing path.&#13;
  context.closePath();&#13;
  //Set the fill color&#13;
  context.fillStyle = color;&#13;
  //and fill with that color&#13;
  context.fill();&#13;
}</pre>&#13;
<p class="Listing">Listing 3-1: High-level JavaScript code for rendering polygons</p>&#13;
<p class="Body">What we see in Listing 3-1 are many high-level functions performing what might appear to be some very simple operations. A cursor is in the context, and using <span class="Literal _idGenCharOverride-1">translate()</span> and <span class="Literal _idGenCharOverride-1">moveTo()</span> functions, we position the drawing point. Then we use the <span class="Literal _idGenCharOverride-1">Math.cos()</span> and <span class="Literal _idGenCharOverride-1">Math.sin()</span> functions to calculate the start and end point angles for each side. Finally, we draw the lines with the <span class="Literal _idGenCharOverride-1">lineTo()</span> function and fill the shape with the requested color using the <span class="Literal _idGenCharOverride-1">fill()</span> function. In just 11 lines, we perform some fairly complex calculations to draw any polygon in any color requested. </p>&#13;
<p class="Body">What we <em class="EmphasisItalic">don’t</em> see here—what has been abstracted away from us—is all the messy detail of how the computer executes these functions. For example, the <span class="Literal _idGenCharOverride-1">lineTo()</span> function tells the computer to draw a line between point A and point B. But the computer must calculate where those points are in the canvas, where that canvas is in the browser window, where that browser window is on the desktop, and what the dimensions of the desktop are. Then it has to interface with the monitor and graphics card before changing the color of each pixel between them to our chosen color. Incredible amounts of math are going on behind the scenes of this code, from the monitor down to the electronic circuits, that we don’t have to think about. The high-level code insulates the programmer from having to worry about the machine the code is running on. Instead, it lets programmers focus on what their program is trying to accomplish; however, the computer still needs code to communicate with the hardware.</p>&#13;
<h2 class="HeadA"><a id="_idTextAnchor046"/>Low-Level Code</h2>&#13;
<p class="BodyFirst">In the many layers of interfaces between the human and the computers’ circuits, <em class="EmphasisItalic">low-level code</em> is where the hardware-specific operations are defined. Low-level code is often machine specific and will reference particular memory addresses, storage peripherals, or processor functions. As a result, it can also be quite difficult to read and requires a deep familiarity with the hardware architecture your computer uses.</p>&#13;
<h3 class="HeadB"><a id="_idTextAnchor047"/>Assembly Language</h3>&#13;
<p class="BodyFirst">Even low-level code offers some human readability. <em class="EmphasisItalic">Assembly language</em> is low-level code but still uses symbolic commands. It’s different in that it must operate very strictly within the computer architecture in which it executes. Listing 3-2 shows a function in assembly language that will add one to the number it’s given and return the result.</p>&#13;
<pre>def add_one(n)&#13;
  pushq %rbp       &#13;
  movq  %rsp, %rbp&#13;
  addl  $1, %edi   &#13;
  movl  %edi, %eax&#13;
  popq  %rbp&#13;
  retq&#13;
end</pre>&#13;
<p class="Listing">Listing 3-2: Assembly language code for adding one to a number</p>&#13;
<p class="Body">The <span class="Literal _idGenCharOverride-1">def add_one(n)</span> line defines the name of the function that accepts the argument <span class="Literal _idGenCharOverride-1">n</span>. The <span class="Literal _idGenCharOverride-1">pushq</span> and <span class="Literal _idGenCharOverride-1">movq</span> lines set up a new stack for the function to run on specific registers in this hardware architecture, addresses for specific locations in the hardware’s memory. The <span class="Literal _idGenCharOverride-1">addl</span> line adds a long integer: the first argument adds one and the second, <span class="Literal _idGenCharOverride-1">%edi</span>, refers to the register holding the value of <span class="Literal _idGenCharOverride-1">n</span>. The <span class="Literal _idGenCharOverride-1">movl</span> line moves the new value for <span class="Literal _idGenCharOverride-1">n</span> from the <span class="Literal _idGenCharOverride-1">%edi</span> register into the register holding the return value, <span class="Literal _idGenCharOverride-1">%eax</span>. Finally, the <span class="Literal _idGenCharOverride-1">popq</span> and <span class="Literal _idGenCharOverride-1">retq</span> lines free the memory and return the computer to where it was in the program before the function was called. </p>&#13;
<p class="Body">The equivalent function in high-level programming code would look something like this: <span class="Literal _idGenCharOverride-1">n = n +1;</span> or <span class="Literal _idGenCharOverride-1">n++;</span>. It takes eight lines of cryptic assembly language code to accomplish what we can do in one line of high-level code. Similarly, the high-level code example in Listing 3-1 could draw any polygon in just nine lines of code, whereas the same task in assembly code would take many more lines than that.</p>&#13;
<p class="Body">It takes a lot of assembly code to tell the computer where to specifically store and retrieve each piece of data. Figure 3-3 is an iconic photo of Margaret Hamilton, director of the Software Engineering Division at the MIT Instrumentation Laboratory at the time, standing next to a printout of the assembly code for the Apollo Guidance Computer (AGC), alongside the LEGO figure honoring her.</p>&#13;
<figure>&#13;
<p class="AnchoredGraphic"><img alt="" class="_idGenObjectAttribute-3" src="image/CitC03_03_Margaret_Hamilton_LegoComparison.png"/></p>&#13;
<figcaption class="Caption">Figure 3-3: Margaret Hamilton (left) during her time as lead Apollo flight software engineer, standing next to listings of the actual Apollo Guidance Computer (AGC) source code (Photo: Draper Laboratory, 1969). Reconstruction of the iconic photo (right) from the “Women of NASA” LEGO set.</figcaption>&#13;
</figure>&#13;
<p class="Body">This stack of assembly language code is as tall as the programmer. When we write high-level code, it’s important to appreciate that there are extensive libraries of assembly code like this making each function call possible. On the human side, this photo of Margaret Hamilton is iconic for how it puts a relatable human face on something as technically complex as flying to the moon. Even in the complex code, programmers find ways to convey personality and levity. Listing 3-3 shows some sample lines of assembly code from the Apollo code repository. After each hash mark (<span class="Literal _idGenCharOverride-1">#</span>) symbol are <em class="EmphasisItalic">comments</em>, which are explanations of the code for human benefit that the computer won’t read.</p>&#13;
<pre>FLAGORGY TC       INTPRET   #  DIONYSIAN FLAG WAVING&#13;
         BZF      P63SPOT4  #  BRANCH IF ANTENNA ALREADY IN POSITION 1&#13;
        &#13;
         CAF      CODE500   #  ASTRONAUT:     PLEASE CRANK THE&#13;
         TC       BANKCALL  #                 SILLY THING AROUND&#13;
         CADR     GOPERF1                  &#13;
         TCF      GOTOP00H  #  TERMINATE&#13;
         TCF      P63SPOT3  #  PROCEED        SEE IF HE'S LYING&#13;
&#13;
         TC       POSTJUMP  #  OFF TO SEE THE WIZARD ...&#13;
         CADR     BURNBABY&#13;
&#13;
         CAF      V06N43*   # ASTRONAUT:  NOW LOOK WHERE TO ENDED UP</pre>&#13;
<p class="Listing">Listing 3-3: Sample code from the Apollo computer</p>&#13;
<p class="Body">There’s a lot of humor to be found among the cryptic commands in Listing 3-3. The <span class="Literal _idGenCharOverride-1">GOTOP00H</span> command is a reference to Winnie the Pooh, which was the name of the root program. The <span class="Literal _idGenCharOverride-1">FLAGORGY</span> command, probably an alert for erratic behavior, has a comment referencing Dionysus, the god of wine and fertility, who is also the antonym of Apollonian. The comment <span class="Literal _idGenCharOverride-1">PLEASE CRANK THE SILLY THING AROUND</span> describes the intent of the <span class="Literal _idGenCharOverride-1">CODE500</span> message if the antenna isn’t in its proper position , and the <span class="Literal _idGenCharOverride-1">SEE IF HE'S LYING</span> verifies the position again. Just before ignition, we see <span class="Literal _idGenCharOverride-1">OFF TO SEE THE WIZARD ...</span>,<span class="Literal _idGenCharOverride-1"> </span>followed by the command <span class="Literal _idGenCharOverride-1">BURNBABY</span> and <span class="Literal _idGenCharOverride-1">V06N43*</span> references when the lunar lander should be on the moon, with the comment <span class="Literal _idGenCharOverride-1">NOW LOOK WHERE TO ENDED UP</span>. You can share this code with your students to highlight the human side of coding. Even when the code can be a matter of life and death in a mission to get astronauts safely to the moon, there is room for levity and personal expression.</p>&#13;
<aside epub:type="sidebar">&#13;
<div class="top hr"><hr/></div>&#13;
<section class="feature1">&#13;
<h2 class="BoxHead">Computer Science in Media</h2>&#13;
<p class="BoxBodyFirst">Books and films allow students to take a break from the hard, logical problems of computer science and engage with the subject’s softer, human side. Here are some ways you can use popular media to have fun and connect with serious social topics:</p>&#13;
<p class="BoxListPlainA"><strong class="EmphasisBoldBox">Social issues</strong> In this chapter, you learn that women have contributed heavily to computer science. So why are companies now struggling to fill the gender gap in information technology jobs? A great book on this subject is Marie Hicks’s <em class="EmphasisItalicBox">Programmed Inequality: How Britain Discarded Women Technologists and Lost Its Edge in Computing</em>. The book covers how computer programming went from “women’s work” in the 1940s to being taken over by men and masculinized when it became lucrative in the 1960s. Other resources include films like <em class="EmphasisItalicBox">Hidden Figures</em>, which is about the African American women working at NASA’s West Area Computing Unit. In addition, <em class="EmphasisItalicBox">The Imitation Game</em> is about gay British cryptanalyst and forbearer of modern computing Alan Turing. It explores the experiences of minorities working in computer science fields.</p>&#13;
<p class="BoxListPlainB"><strong class="EmphasisBoldBox">Current events</strong> Have regular discussions about current and developing events in computing. What laws are being passed? What data has been compromised? Ask your students about the implications of emergent technologies, such as computer vision, machine learning, and autonomous vehicles. What jobs might be lost due to automation? What new opportunities might open up? These are the kinds of topics effective digital citizens need to be aware of.</p>&#13;
<p class="BoxListPlainC"><strong class="EmphasisBoldBox">Having fun</strong> Many seemingly complex concepts were discovered by people at play. Search for US Navy Rear Admiral Grace Hopper videos. You’ll find some delightful lectures and late-night talk-show interviews of her ­talking about computing power in nanoseconds and picoseconds with lengths of wire and pepper packets representing each. The 2017 book <em class="EmphasisItalicBox">A Mind at Play: How Claude Shannon Invented the Information Age</em>, in a very down-to-earth way, explains the complex computer science concepts discovered by a man who spent much of his life building playthings, such as maze-running mechanical mice, juggling robots, and calculating pi from spilt toothpicks. </p>&#13;
<p class="BoxBodyLast">Just make sure you review all media before sharing it with the class, in case you need to skip scenes or bleep inappropriate language.</p>&#13;
<div class="bottom hr"><hr/></div>&#13;
</section>&#13;
</aside><p class="Body">As cryptic as the assembly language code is, even it abstracts away complexity. The memory registers that the code references in the computer are labels, and the commands it executes are named for human understanding. Even these instructions must be further translated into information the computer can decipher.</p>&#13;
<h3 class="HeadB"><a id="_idTextAnchor048"/>Machine Code</h3>&#13;
<p class="BodyFirst">Although assembly code is hardware-specific and written to work with certain memory addresses in the computer architecture in which it runs, it’s still working with human-friendly abstractions and manipulating blocks of data. At the lowest level, programming code manipulates the most discrete units of information in the computer, the <em class="EmphasisItalic">bits</em>, which are either one or zero. <em class="EmphasisItalic">Machine code</em>, a strictly numerical programming language, is used at this level to work with these bits. </p>&#13;
<p class="Body">Reading machine language code is like reading the atoms in a DNA molecule. Listing 3-4 shows an example of binary machine code used to store a text string. You can imagine the challenges of working in such an opaque syntax.</p>&#13;
<pre>01001000 01100101 01101100 01101100 01101111&#13;
00100000 01010111 01101111 01110010 01101100 01100100&#13;
</pre>&#13;
<p class="Listing">Listing 3-4: <span class="LiteralCaption">"Hello World"</span> in ASCII binary code</p>&#13;
<p class="Body">Bill Gates and Paul Allen wrote a version of the BASIC programming language for the 1975 MITS Altair 8800, an early microcomputer that was widely popular despite being meant only for hobbyists. Gates and Allen had to load their BASIC interpreter into the machine using a set of binary commands. In Figure 3-4, you can see that, instead of a monitor, the computer had only lights and switches for binary inputs and outputs.</p>&#13;
<figure>&#13;
<p class="AnchoredGraphic"><img alt="" class="_idGenObjectAttribute-4" src="image/CitC03_04_Altair8800.png"/></p>&#13;
<figcaption class="Caption">Figure 3-4: Altair 8800 computer (Photo: National Museum of American History) </figcaption>&#13;
</figure>&#13;
<p class="Body">When describing the CPU as speaking in ones and zeros, yet another layer of complexity is abstracted away. Even the ones and zeros are abstractions representing the amount of electricity in a circuit.</p>&#13;
<h2 class="HeadA"><a id="_idTextAnchor049"/>Circuits</h2>&#13;
<p class="BodyFirst">Ones and zeros, the bits of data that make up the strings of machine code, represent “on” and “off” settings inside the computer. The computer’s CPU, which performs all calculations, is a microchip with one to many <em class="EmphasisItalic">integrated circuits (IC)</em>, which are microchips that contain sets of electronic circuits. Each IC is filled with billions of <em class="EmphasisItalic">transistors</em>. The electrical state of each transistor determines whether a bit is on or off—one or zero. If you look carefully, you’ll likely find a symbol on your computer that combines both values for a bit on one of its buttons, as in Figure 3-5.</p>&#13;
<figure>&#13;
<p class="AnchoredGraphic"><img alt="" class="_idGenObjectAttribute-5" src="image/CitC03_05_powerbuttonicon.png"/></p>&#13;
<figcaption class="Caption">Figure 3-5: Computer power button icon</figcaption>&#13;
</figure>&#13;
<p class="Body">Before the Nobel Prize–winning invention of the transistor, computers used <em class="EmphasisItalic">vacuum tubes</em>, which were circuits that resembled lightbulbs about the size of your thumb. They were large and energy-hungry produced a lot of heat, and burned out often. The first electronic general-purpose computer, the ENIAC, was made in 1946. It used 20,000 vacuum tubes, occupied 1,800 square feet, and weighed 30 tons. In comparison, today’s cell phones, which use ICs, have thousands of times more processing power than the ENIAC. </p>&#13;
<p class="Body">The first computer program was written for ENIAC by six women mathematicians: Kathleen McNulty, Frances Bilas, Betty Jean Jennings, Elizabeth Snyder, Ruth Lichterman, and Marlyn Wescoff Meltzer. It involved setting switches and plugs for various binary commands and values. At the time, the word <em class="EmphasisItalic">computer</em> referred to the job title of someone who crunched numbers. Only later did it become the name of the tool that would replace this occupation.</p>&#13;
<p class="Body">In Figure 3-6, you can get an idea of the size and complexity of the ENIAC’s interface with its many lights and switchboards representing the binary inputs and outputs.</p>&#13;
<figure>&#13;
<p class="AnchoredGraphic"><img alt="" class="_idGenObjectAttribute-6" src="image/CitC03_06_ENIAC.png"/></p>&#13;
<figcaption class="Caption">Figure 3-6: Betty Jennings (left) and Frances Bilas (right) operating the ENIAC’s main control panel (Photo: ARL Technical Library, U. S. Army Photo)</figcaption>&#13;
</figure>&#13;
<p class="Body">Even transistors are abstractions. They represent logical operations.  The first digital computer wouldn’t have been possible without the 1936 paper “A Symbolic Analysis of Relay and Switching Circuits,” the master’s thesis of an MIT student named Claude Elwood Shannon. In this milestone document, Shannon demonstrates that electric switches could be used to perform <em class="EmphasisItalic">Boolean algebra</em>, a branch of algebra in which operations manipulate true and false values. Boolean algebra was introduced by George Boole in his 1847 book, <em class="EmphasisItalic">The Mathematical Analysis of Logic</em>, and discussed in his 1854 book, <em class="EmphasisItalic">An Investigation of the Laws of Thought on Which Are Founded the Mathematical Theories of Logic and Probabilities</em>. Figure 3-7 shows some examples of <em class="EmphasisItalic">logic gates</em>, which model Boolean logic in a way that can be translated into circuitry.</p>&#13;
<figure>&#13;
<p class="AnchoredGraphic"><img alt="" class="_idGenObjectAttribute-7" src="image/CitC03_07_LogicGates.png"/></p>&#13;
<figcaption class="Caption">Figure 3-7: Logic gates</figcaption>&#13;
</figure>&#13;
<p class="Body">The lines on the left of the symbols represent binary inputs and those on the right binary outputs. For example, the AND gate accepts two inputs and both must be 1 for the output to be 1. So 0 <span class="Literal _idGenCharOverride-1">and</span> 0, 1 <span class="Literal _idGenCharOverride-1">and</span> 0, and 0 <span class="Literal _idGenCharOverride-1">and</span> 1 will all output 0, whereas 1 <span class="Literal _idGenCharOverride-1">and</span> 1 outputs 1. The OR gate returns 1 if either input is 1. So 1 <span class="Literal _idGenCharOverride-1">or</span> 0, 0 <span class="Literal _idGenCharOverride-1">or</span> 1, and 1 <span class="Literal _idGenCharOverride-1">or</span> 1 will output 1, while 0 <span class="Literal _idGenCharOverride-1">or</span> 0 outputs 0. The NOT gate inverts any input, so an input of <span class="Literal _idGenCharOverride-1">not</span> 1 outputs 0 and <span class="Literal _idGenCharOverride-1">not</span> 0 outputs 1. Logic gates can be combined into complex configurations to model logical processes, which can then be constructed on a circuit board with the appropriate components. </p>&#13;
<p class="Body">This information explained the hardware piece of the computer puzzle. But before computer scientists could engineer machines that could automate logic, there were others who first imagined that such a thing was even possible.</p>&#13;
<h2 class="HeadA"><a id="_idTextAnchor050"/>Envisioning Thought Machines</h2>&#13;
<p class="BodyFirst">Around the same time Claude Shannon was figuring out how to perform logical operations using electric circuits, Alan Turing, a polymath whose codebreaking skills saved millions of lives in World War II, was deciphering how discrete logical operations could combine into a computing system. In his paper “On Computable Numbers, with an Application to the Entscheidungsproblem,” Turing describes a hypothetical <em class="EmphasisItalic">Turing machine</em>. This machine could be a person or machine that reads symbols from a potentially infinite strip of tape; stores the <em class="EmphasisItalic">state</em> of the machine in a <em class="EmphasisItalic">register</em>, a reference to the human computer’s state of mind; looks up those symbols in an instruction table; prints an output; and moves to a new position along the tape according to the instructions. In other words, he described a very primitive CPU capable of processing a computer program. For this and other achievements, he is often regarded as the father of modern computer science.</p>&#13;
<aside epub:type="sidebar">&#13;
<div class="top hr"><hr/></div>&#13;
<section class="feature1">&#13;
<h2 class="BoxHead">Artificial Intelligence</h2>&#13;
<p class="BoxBodyFirst">In 1950, Alan Turing proposed a method of testing whether a machine was as intelligent as a human being, which is now known as the <em class="EmphasisItalicBox">Turing test</em>. In the modern version of this test, a human judge would chat online with two individuals, one an actual human and the other a computer program. If, after sufficient inquiry, the judge couldn’t distinguish which was human and which was the artificial intelligence (AI), the AI would pass the test. Ask your students what they think about this method. Is this a good way of measuring AI?</p>&#13;
<p class="BoxBody">Many philosophical, technological, and ethical dimensions exist on the issue of AI in our society. Here are some additional concepts you might bring into the discussion:</p>&#13;
<p class="BoxListPlainA"><strong class="EmphasisBoldBox">Philosophical</strong> Among the philosophical challenges to the Turing test is John Searle’s 1980 <em class="EmphasisItalicBox">Chinese room thought experiment</em>. In this experiment, an English speaker is placed in a room with a book of instructions written in English. A Chinese speaker outside the room writes queries in Chinese and slips them through the door. The person in the room looks up the symbols in the instructions book, writes down symbols the book instructs them to, and slips the answer out the door. The Chinese speaker gets a response that makes sense to them, but the English speaker has no understanding of the symbols they’re processing. What are the implications of this thought experiment for the Turing test?</p>&#13;
<p class="BoxListPlainB"><strong class="EmphasisBoldBox">Technological</strong> In computer software, you find the programmatic equivalent of this thought experiment in <em class="EmphasisItalicBox">chatbots</em>, programs that scan sentences for keywords that a user inputs, look up those words in a table, and output a predefined reply. It’s possible to converse with a wide variety of chatbots online. Many are open source, allowing you to download the lookup tables and share them with your students to show them how they work. Ask your students whether this is like the Chinese room experiment. Is this intelligence? </p>&#13;
<p class="BoxListPlainB"><strong class="EmphasisBoldBox">Cybersecurity</strong> Chatbots find the most success when their context is constrained. For example, a chatbot specialized in, and restricted to, only talking about 1980s sitcoms will be more convincing than a chatbot expected to talk on any subject. In fact, researchers have released chatbots into text-based massively multiplayer online role-playing games (MMORPGS) and have successfully passed them off as human to other players. Similarly, society is increasingly becoming aware of “bots” on social media, humans partnered with computers to deceive and influence people online. To protect themselves online when someone they don’t know contacts them, how can students evaluate whether that someone is legitimate or not?</p>&#13;
<p class="BoxListPlainB"><strong class="EmphasisBoldBox">Ethical</strong> Another context in which chatbots can thrive is when interacting with young children. With limited vocabularies and life experiences, children are more likely to accept chatbots as living things and be more open to making friends with them. Ask your students what the ethical implications are for this. Would it be acceptable for a software developer to write a chatbot app and market it to young children as a virtual friend? What could the impacts be on children who make friends with chatbots? How would you teach children to distinguish between real people and AI? </p>&#13;
<p class="BoxListPlainCLast"><strong class="EmphasisBoldBox">Future trends</strong> As the bots get more sophisticated, they’ll get better at deceiving older, more experienced humans. How should society prepare for this eventuality?</p>&#13;
<div class="bottom hr"><hr/></div>&#13;
</section>&#13;
</aside><p class="Body">Inventors dreamed of having machines perform cognitively taxing tasks long before Turing. Over the course of several decades in the 1800s, English polymath Charles Babbage proposed and attempted the construction of a mechanical calculator, which he called the Difference Engine. Later he constructed a general-purpose mechanical computer, which he called the Analytical Engine. Neither invention was successfully constructed in his lifetime. But the Analytical Engine had a memory store, the equivalent of a CPU, and was programmable with punch cards.</p>&#13;
<p class="Body">Ada Lovelace, the daughter of the poet Lord Byron, was a mathematician and writer who described her approach as “poetical science.” She was also a longtime friend of Babbage’s, who called her “the Enchantress of Numbers.” Ada was fascinated by the Difference Engine. When translating a foreign paper on Babbage’s proposed Analytical Engine to English, she supplemented the paper with extensive notes, even including a detailed algorithm to calculate the Bernoulli number sequence in the engine. Because of this algorithm, Lovelace is widely considered the world’s first computer programmer. Figure 3-8 shows a watercolor portrait of her that was adopted by the Ada Initiative, an organization focused on increasing the participation of women in open source technology and culture.</p>&#13;
<figure>&#13;
<p class="AnchoredGraphic"><img alt="" class="_idGenObjectAttribute-8" src="image/CitC03_08_Ada_Lovelace_portrait.png"/></p>&#13;
<figcaption class="Caption">Figure 3-8: Circa-1840 portrait of Ada Lovelace by Alfred Edward Chalon</figcaption>&#13;
</figure>&#13;
<p class="Body">In her writings, Lovelace is clearly enchanted with the Analytical Engine. She marvels at how it manifests abstract mental processes in machine operations. She envisions a powerful language in machine code that will produce faster, more accurate analysis for the human race. She also references the engine’s predecessor, the <em class="EmphasisItalic">Jacquard loom</em>, which used punch cards to program fabric designs, saying the Analytical Engine weaved algebraic expressions the way the loom weaved flowers and leaves. In fact, when we look further back in time, we find that the weaving of logical expressions shares some of the same challenges and frustrations as weaving fabrics. </p>&#13;
<h2 class="HeadA"><a id="_idTextAnchor051"/>Ancient History</h2>&#13;
<p class="BodyFirst">On a trip to Peru, I had the good fortune of visiting a mountain village where I learned of the importance of weaving in their culture. Without a written language, for thousands of years the people relied on <em class="EmphasisItalic">ideograms</em>, images used to communicate identifications of food, resources, warnings, or people. The local women wove the ideograms on tapestries, which we can think of as abstractions of the real-life things they represent, just like the icons in a computer’s UI. </p>&#13;
<p class="Body">In the village, they had recently purchased two simple looms, which required programming by hand. Figure 3-9 shows the programming code for the loom in the string arrangements with inputs of string that will later become design outputs.</p>&#13;
<figure>&#13;
<p class="AnchoredGraphic"><img alt="" class="_idGenObjectAttribute-9" src="image/CitC03_09_PeruLoom.png"/></p>&#13;
<figcaption class="Caption">Figure 3-9: A loom in Peru</figcaption>&#13;
</figure>&#13;
<p class="Body">This loom image shows a lot of complexity, and we can imagine how daunting a challenge programming it would be. Watching the women weave by hand was like watching a computer lay down pixels line by line and bit by bit. One woman explained that the new looms were faster, but it was frustrating when they set them up incorrectly and the patterns came out wrong. This is similar to the frustrations of programming when the software executes quickly, but getting the programming logic correct can be a challenge. In this village without electricity or running water, people were successfully taking on complex challenges that involved computational thinking and abstraction without a computer anywhere in sight. </p>&#13;
<aside epub:type="sidebar">&#13;
<div class="top hr"><hr/></div>&#13;
<section class="feature1">&#13;
<h2 class="BoxHead">Quantifying Exponential Growth in Computing</h2>&#13;
<p class="BoxBodyFirst">There’s an ancient fable about the origins of the game chess and the awesome power of exponential growth. In the fable, a king is so enthralled with the game that he offers its inventor a prize of their choosing. The inventor requests a grain of wheat for the first square on the board, two for the second, four for the third, eight, 16, and so on for the 64 squares. The king thinks this a paltry prize and orders the treasurer to fulfill it. The treasurer does the addition, 1 + 2 + 4 + 8 + 16 +. . . + 9,223,372,036,854,775,808, which totals 18,446,744,073,709,551,615—a sum greater than the number of sand grains on Earth.</p>&#13;
<p class="BoxBody">For nearly half a century people have witnessed a similar exponential rate of growth in computing. In a phenomenon known as <em class="EmphasisItalicBox">Moore’s law</em>, the number of transistors in an integrated circuit has doubled about every two years. That’s about 24 doublings since the 1970s, and that number isn’t even halfway across the chessboard’s 64 squares. In other words, if Moore’s law continues to hold true, society will continue to see incredible gains in processing power in the future.</p>&#13;
<p class="BoxBody">Chess is an excellent way to understand how processing power produces real-world results. Chess programs are fairly straightforward. They’re composed of algorithms that search every possible move far out into the future and choose the best one. For this reason, improvements in chess playing programs highly depend on increased computing power. IBM’s Bernstein program had a chess rating of 800 in 1957, making it a very weak player. With a rating of 2,700 in 1996, Deep Blue became the first computer to beat Garry Kasparov, the top chess player in the world. It’s estimated that every doubling of computer processing power translates to an increase of 80 to 100 points in software chess rating. In 2018, the chess program Stockfish 10 achieved a 3,600 chess rating. Chess enthusiasts often speculate on when a computer will break 4,000 or if the game will eventually be “solved,” where every possible outcome is mapped out.</p>&#13;
<p class="BoxBodyLast">Another way to see the effects of exponential growth in processing power is computer animation. Share samples of computer graphics throughout the decades with your students and note how blocky, pixelated graphics evolve into current photorealistic marvels. Pixar Studios came up with the idea for the film <em class="EmphasisItalicBox">Toy Story</em> in the late 1980s. But the studio predicted it would need to wait five years for computing power to reach what the animators estimated was needed to make the film. What can your students imagine doing with the projected computing power of the future?</p>&#13;
<div class="bottom hr"><hr/></div>&#13;
</section>&#13;
</aside>&#13;
<h2 class="HeadA"><a id="_idTextAnchor052"/>Summary</h2>&#13;
<p class="BodyFirst">In this chapter, we explored the many layers and innovations that make modern computing systems possible. With the UI as the starting point, we traversed down through the layers that included high-level programming code, low-level assembly language code, machine code, circuits, conceptual innovations like Boolean logic, and precursors to the computer, like programmable looms. Concurrently, we traveled back through time and met a few of the many innovators in computer science, such as Margaret Hamilton and her team at NASA; the ENIAC programmers; Claude Shannon; Alan Turing; Charles Babbage; Ada Lovelace; and the weavers from indigenous villages. All of these individuals are human beings with whom your students can identify, providing models from which students can see themselves working in computer science.</p>&#13;
<p class="Body">Additionally, by making students aware of the vast number of experts and innovations it took to make modern computers possible, you’ll teach them to respect the subject matter and understand that no one person can hope to know it all. Students should realize that it’s best to engage the subject with a personal humility and sensitivity to others, recognizing that everyone has blind spots when it comes to computer technologies and innovations. When students understand that computer science is complex for everyone and that the history of the field is the story of making computers more accessible to others over time through abstraction, they’ll hopefully find the subject more approachable.</p>&#13;
<p class="Body">In this history we saw how early computer science, which was built on circuit boards and abstract research papers, barely resembles the programming environments we work in today. Yet, as we will learn, the foundational elements of programming have remained the same over the intervening decades, and we don’t need modern computers to learn computer science. </p>&#13;
<p class="Body">In the next chapter, we will discover the many ways you can explore the basic building blocks of computer programming in the classroom without involving computers.</p>&#13;
</div>&#13;
</body></html>