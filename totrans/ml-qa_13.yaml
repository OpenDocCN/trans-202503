- en: '**11'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**11'
- en: CALCULATING THE NUMBER OF PARAMETERS**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**计算参数数量**'
- en: '![Image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/common.jpg)'
- en: How do we compute the number of parameters in a convolutional neural network,
    and why is this information useful?
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何计算卷积神经网络中的参数数量，这些信息为什么有用？
- en: Knowing the number of parameters in a model helps gauge the model’s size, which
    affects storage and memory requirements. The following sections will explain how
    to compute the convolutional and fully connected layer parameter counts.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 了解模型中的参数数量有助于评估模型的大小，这直接影响存储和内存需求。接下来的部分将解释如何计算卷积层和全连接层的参数数量。
- en: '**How to Find Parameter Counts**'
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**如何查找参数数量**'
- en: Suppose we are working with a convolutional network that has two convolutional
    layers with kernel size 5 and kernel size 3, respectively. The first convolutional
    layer has 3 input channels and 5 output channels, and the second one has 5 input
    channels and 12 output channels. The stride of these convolutional layers is 1\.
    Furthermore, the network has two pooling layers, one with a kernel size of 3 and
    a stride of 2, and another with a kernel size of 5 and a stride of 2\. It also
    has two fully connected hidden layers with 192 and 128 hidden units each, where
    the output layer is a classification layer for 10 classes. The architecture of
    this network is illustrated in [Figure 11-1](ch11.xhtml#ch11fig1).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在处理一个卷积网络，该网络有两个卷积层，分别具有 5 和 3 的卷积核大小。第一个卷积层有 3 个输入通道和 5 个输出通道，第二个卷积层有
    5 个输入通道和 12 个输出通道。这些卷积层的步长为 1。此外，网络有两个池化层，一个池化层的卷积核大小为 3，步长为 2，另一个池化层的卷积核大小为 5，步长为
    2。它还具有两个全连接隐藏层，每个隐藏层分别有 192 和 128 个隐藏单元，输出层是用于 10 类分类的分类层。该网络的架构如 [图 11-1](ch11.xhtml#ch11fig1)
    所示。
- en: '![Image](../images/11fig01.jpg)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/11fig01.jpg)'
- en: '*Figure 11-1: A convolutional neural network with two convolutional and two
    fully connected layers*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-1：具有两个卷积层和两个全连接层的卷积神经网络*'
- en: What is the number of trainable parameters in this convolutional network? We
    can approach this problem from left to right, computing the number of parameters
    for each layer and then summing up these counts to obtain the total number of
    parameters. Each layer’s number of trainable parameters consists of weights and
    bias units.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这个卷积网络中的可训练参数数量是多少？我们可以从左到右处理这个问题，逐层计算每个层的参数数量，然后将这些数量相加以获得总的参数数量。每一层的可训练参数由权重和偏置单元组成。
- en: '***Convolutional Layers***'
  id: totrans-10
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***卷积层***'
- en: In a convolutional layer, the number of weights depends on the kernel’s width
    and height and the number of input and output channels. The number of bias units
    depends on the number of output channels only. To illustrate the computation step
    by step, suppose we have a kernel width and height of 5, one input channel, and
    one output channel, as illustrated in [Figure 11-2](ch11.xhtml#ch11fig2).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积层中，权重的数量取决于卷积核的宽度和高度，以及输入和输出通道的数量。偏置单元的数量仅取决于输出通道的数量。为了逐步演示计算，假设我们有一个宽度和高度为
    5 的卷积核，一个输入通道和一个输出通道，如 [图 11-2](ch11.xhtml#ch11fig2) 所示。
- en: '![Image](../images/11fig02.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/11fig02.jpg)'
- en: '*Figure 11-2: A convolutional layer with one input channel and one output channel*'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-2：具有一个输入通道和一个输出通道的卷积层*'
- en: In this case, we have 26 parameters, since we have 5 *×* 5 = 25 weights via
    the kernel plus the bias unit. The computation to determine an output value or
    pixel *z* is *z* = *b* + *∑[j] w[j] x[j]*, where *x[j]* represents an input pixel,
    *w[j]* represents a weight parameter of the kernel, and *b* is the bias unit.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们有 26 个参数，因为我们通过卷积核得到 5 *×* 5 = 25 个权重，加上偏置单元。计算输出值或像素 *z* 的公式为 *z*
    = *b* + *∑[j] w[j] x[j]*，其中 *x[j]* 代表输入像素，*w[j]* 代表卷积核的权重参数，*b* 是偏置单元。
- en: Now, suppose we have three input channels, as illustrated in [Figure 11-3](ch11.xhtml#ch11fig3).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设我们有三个输入通道，如 [图 11-3](ch11.xhtml#ch11fig3) 所示。
- en: '![Image](../images/11fig03.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/11fig03.jpg)'
- en: '*Figure 11-3: A convolutional layer with three input channels and one output
    channel*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-3：具有三个输入通道和一个输出通道的卷积层*'
- en: 'In that case, we compute the output value by performing the aforementioned
    operation, *∑[j] w[j] x[j]*, for each input channel and then add the bias unit.
    For three input channels, this would involve three different kernels with three
    sets of weights:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们通过对每个输入通道执行上述操作 *∑[j] w[j] x[j]* 来计算输出值，然后加上偏置单元。对于三个输入通道，这将涉及三个不同的卷积核，每个卷积核有一组权重：
- en: '![Image](../images/f0071-01.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0071-01.jpg)'
- en: Since we have three sets of weights (*w*^((1)), *w*^((2)), and *w*^((3)) for
    *j* = [1, . . . , 25]), we have 3 *×* 25 + 1 = 76 parameters in this convolutional
    layer.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有三组权重（*w*^((1))、*w*^((2)) 和 *w*^((3))，对于 *j* = [1, . . . , 25]），因此在该卷积层中有3
    *×* 25 + 1 = 76个参数。
- en: We use one kernel for each output channel, where each kernel is unique to a
    given output channel. Thus, if we extend the number of output channels from one
    to five, as shown in [Figure 11-4](ch11.xhtml#ch11fig4), we extend the number
    of parameters by a factor of 5\. In other words, if the kernel for one output
    channel has 76 parameters, the 5 kernels required for the five output channels
    will have 5 *×* 76 = 380 parameters.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为每个输出通道使用一个卷积核，其中每个卷积核是专门针对特定输出通道的。因此，如果我们将输出通道的数量从一个扩展到五个，如[图 11-4](ch11.xhtml#ch11fig4)所示，参数数量将增加五倍。换句话说，如果一个输出通道的卷积核有76个参数，那么需要为五个输出通道准备的五个卷积核将有5
    *×* 76 = 380个参数。
- en: '![Image](../images/11fig04.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/11fig04.jpg)'
- en: '*Figure 11-4: A convolutional layer with three input channels and five output
    channels*'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-4：具有三个输入通道和五个输出通道的卷积层*'
- en: Returning to the neural network architecture illustrated in [Figure 11-1](ch11.xhtml#ch11fig1)
    at the beginning of this section, we compute the number of parameters in the convolutional
    layers based on the kernel size and number of input and output channels. For example,
    the first convolutional layer has three input channels, five output channels,
    and a kernel size of 5\. Thus, its number of parameters is 5 *×* (5 *×* 5 *×*
    3) + 5 = 380\. The second convolutional layer, with five input channels, 12 output
    channels, and a kernel size of 3, has 12 *×* (3 *×* 3 *×* 5) + 12 = 552 parameters.
    Since the pooling layers do not have any trainable parameters, we can count 380
    + 552 = 932 for the convolutional part of this architecture.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 回到本节开始时展示的[图 11-1](ch11.xhtml#ch11fig1)中的神经网络架构，我们基于卷积核的大小以及输入和输出通道的数量来计算卷积层中的参数。例如，第一个卷积层有三个输入通道、五个输出通道和一个大小为5的卷积核。因此，它的参数数量为5
    *×* (5 *×* 5 *×* 3) + 5 = 380。第二个卷积层有五个输入通道、12个输出通道和一个大小为3的卷积核，其参数数量为12 *×* (3
    *×* 3 *×* 5) + 12 = 552。由于池化层没有可训练的参数，我们可以将卷积部分的参数数目计算为380 + 552 = 932。
- en: Next, let’s see how we can compute the number of parameters of fully connected
    layers.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看如何计算全连接层的参数数量。
- en: '***Fully Connected Layers***'
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***全连接层***'
- en: Counting the number of parameters in a fully connected layer is relatively straightforward.
    A fully connected node connects each input node to each output node, so the number
    of weights is the number of inputs times the number of outputs plus the bias units
    added to the output. For example, if we have a fully connected layer with five
    inputs and three outputs, as shown in [Figure 11-5](ch11.xhtml#ch11fig5), we have
    5 *×* 3 = 15 weights and three bias units, that is, 18 parameters total.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 计算全连接层的参数数量相对简单。每个全连接节点将每个输入节点与每个输出节点连接，因此权重的数量是输入数量乘以输出数量，再加上添加到输出的偏置单元。例如，如果我们有一个具有五个输入和三个输出的全连接层，如[图
    11-5](ch11.xhtml#ch11fig5)所示，那么我们有5 *×* 3 = 15个权重和三个偏置单元，也就是总共18个参数。
- en: '![Image](../images/11fig05.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/11fig05.jpg)'
- en: '*Figure 11-5: A fully connected layer with five inputs and three outputs*'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-5：具有五个输入和三个输出的全连接层*'
- en: 'Returning once more to the neural network architecture illustrated in [Figure
    11-1](ch11.xhtml#ch11fig1), we can now calculate the parameters in the fully connected
    layers as follows: 192 *×* 128 + 128 = 24,704 in the first fully connected layer
    and 128 *×* 10 + 10 = 1,290 in the second fully connected layer, the output layer.
    Hence, we have 24,704 + 1,290 = 25,994 in the fully connected part of this network.
    After adding the 932 parameters from the convolutional layers and the 25,994 parameters
    from the fully connected layers, we can conclude that this network’s total number
    of parameters is 26,926.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 再次回到在[图 11-1](ch11.xhtml#ch11fig1)中展示的神经网络架构，我们现在可以计算全连接层中的参数，如下所示：在第一个全连接层中，192
    *×* 128 + 128 = 24,704，在第二个全连接层（输出层）中，128 *×* 10 + 10 = 1,290。因此，这个网络全连接部分的总参数为24,704
    + 1,290 = 25,994。将卷积层的932个参数与全连接层的25,994个参数相加，我们可以得出该网络的总参数数为26,926。
- en: As a bonus, interested readers can find PyTorch code to compute the number of
    parameters programmatically in the *supplementary/q11-conv-size* subfolder at
    *[https://github.com/rasbt/MachineLearning-QandAI-book](https://github.com/rasbt/MachineLearning-QandAI-book)*.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 作为额外的内容，感兴趣的读者可以在*[https://github.com/rasbt/MachineLearning-QandAI-book](https://github.com/rasbt/MachineLearning-QandAI-book)*的*supplementary/q11-conv-size*子文件夹中找到用于程序计算参数数量的PyTorch代码。
- en: '**Practical Applications**'
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**实际应用**'
- en: Why do we care about the number of parameters at all? First, we can use this
    number to estimate a model’s complexity. As a rule of thumb, the more parameters
    there are, the more training data we’ll need to train the model well.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们关心参数的数量呢？首先，我们可以使用这个数字来估算模型的复杂度。作为经验法则，参数越多，我们就需要更多的训练数据来有效训练模型。
- en: The number of parameters also lets us estimate the size of the neural network,
    which in turn helps us estimate whether the network can fit into GPU memory. Although
    the memory requirement during training often exceeds the model size due to the
    additional memory required for carrying out matrix multiplications and storing
    gradients, model size gives us a ballpark sense of whether training the model
    on a given hardware setup is feasible.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 参数的数量还可以让我们估算神经网络的大小，进而帮助我们判断网络是否可以适配到GPU内存中。尽管在训练过程中，由于进行矩阵乘法和存储梯度所需的额外内存，内存需求通常会超过模型大小，但模型大小仍然可以帮助我们大致判断在给定硬件配置下训练该模型是否可行。
- en: '**Exercises**'
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**练习**'
- en: '**11-1.** Suppose we want to optimize the neural network using a plain stochastic
    gradient descent (SGD) optimizer or the popular Adam optimizer. What are the respective
    numbers of parameters that need to be stored for SGD and Adam?'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**11-1.** 假设我们想使用普通的随机梯度下降（SGD）优化器或流行的Adam优化器来优化神经网络。那么，SGD和Adam分别需要存储多少参数？'
- en: '**11-2.** Suppose we’re adding three batch normalization (BatchNorm) layers:
    one after the first convolutional layer, one after the second convolutional layer,
    and another one after the first fully connected layer (we typically do not want
    to add BatchNorm layers to the output layer). How many additional parameters do
    these three BatchNorm layers add to the model?'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**11-2.** 假设我们添加了三个批量归一化（BatchNorm）层：一个在第一个卷积层后，一个在第二个卷积层后，还有一个在第一个全连接层后（通常我们不希望在输出层添加BatchNorm层）。这三个BatchNorm层将会给模型增加多少额外的参数？'
