- en: '12'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '12'
- en: CONTAINER RUNTIME
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 容器运行时
- en: '![image](../images/common01.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/common01.jpg)'
- en: In the previous chapter, we saw how the control plane manages and monitors the
    state of the cluster. However, it is the container runtime, especially the `kubelet`
    service, that creates, starts, stops, and deletes containers to actually bring
    the cluster to the desired state. In this chapter, we’ll explore how `kubelet`
    is configured in our cluster and how it operates.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们了解了控制平面如何管理和监控集群的状态。然而，实际上是容器运行时，特别是 `kubelet` 服务，负责创建、启动、停止和删除容器，以实际将集群带入所需状态。本章中，我们将探索
    `kubelet` 如何在我们的集群中配置及其运作方式。
- en: As part of this exploration, we’ll address how `kubelet` manages to host the
    control plane while also being dependent on it. Finally, we’ll look at node maintenance
    in a Kubernetes cluster, including how to shut down a node for maintenance, issues
    that can prevent a node from working correctly, how the cluster behaves if a node
    suddenly becomes unavailable, and how the node behaves when it loses its cluster
    connection.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 作为这一探索的一部分，我们将讨论 `kubelet` 如何在依赖于控制平面的同时，也承载着控制平面的功能。最后，我们将探讨 Kubernetes 集群中的节点维护，包括如何为维护关闭节点、可能导致节点无法正常工作的问题、当节点突然变得不可用时集群如何表现，以及节点在失去集群连接时的行为。
- en: Node Service
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 节点服务
- en: The primary service that turns a regular host into a Kubernetes node is `kubelet`.
    Because of its criticality to a Kubernetes cluster, we’ll look in detail at how
    it is configured and how it behaves.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 将普通主机转化为 Kubernetes 节点的主要服务是 `kubelet`。由于它对 Kubernetes 集群的关键性，我们将详细探讨它是如何配置的，以及它的行为方式。
- en: '**CONTAINERD AND CRI-O**'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**CONTAINERD 和 CRI-O**'
- en: 'The examples for this chapter provide automated scripts to launch a cluster
    using either of two container runtimes: `containerd` and CRI-O. We’ll primarily
    use the `containerd` installation, though we’ll briefly look at the configuration
    difference. The CRI-O cluster is there to allow you to experiment with a separate
    container runtime. It also illustrates the fact that `kubelet` hides this difference
    from the rest of the cluster, as the rest of the cluster configuration is unaffected
    by a container runtime change.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的示例提供了自动化脚本，用于通过两种容器运行时之一启动集群：`containerd` 和 CRI-O。我们将主要使用 `containerd` 安装，但也会简要介绍配置差异。CRI-O
    集群的设置允许你尝试使用一个独立的容器运行时。它还展示了 `kubelet` 隐藏这一差异的事实，因为集群的其他配置不受容器运行时更改的影响。
- en: We installed `kubelet` as a package on all of our nodes when we set up our cluster
    in [Chapter 6](ch06.xhtml#ch06), and the automation has been setting it up similarly
    for each chapter thereafter.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第 6 章](ch06.xhtml#ch06)中设置集群时，已将 `kubelet` 作为软件包安装到所有节点上，之后的各章中，自动化过程也同样为每个章节设置了它。
- en: '**NOTE**'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*The example repository for this book is at* [https://github.com/book-of-kubernetes/examples](https://github.com/book-of-kubernetes/examples).
    *See “Running Examples” on [page xx](ch00.xhtml#ch00lev1sec2) for details on getting
    set up.*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*本书的示例代码仓库在* [https://github.com/book-of-kubernetes/examples](https://github.com/book-of-kubernetes/examples)。*详细设置方法请见“运行示例”部分，[第
    xx 页](ch00.xhtml#ch00lev1sec2)。*'
- en: 'The `kubelet` package also includes a system service. Our operating system
    is using `systemd` to run services, so we can get service information using `systemctl`:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubelet` 包还包含一个系统服务。我们的操作系统使用 `systemd` 来运行服务，因此我们可以使用 `systemctl` 获取服务信息：'
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The first time `kubelet` started, it didn’t have the configuration needed to
    join the cluster. When we ran `kubeadm`, it created the file *10-kubeadm.conf*
    shown in the preceding output. This file configures the `kubelet` service for
    the cluster by setting command line parameters.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次启动 `kubelet` 时，它没有加入集群所需的配置。当我们运行 `kubeadm` 时，它创建了前面输出中显示的文件 *10-kubeadm.conf*。该文件通过设置命令行参数来为集群配置
    `kubelet` 服务。
- en: '[Listing 12-1](ch12.xhtml#ch12list1) gives us a look at the command line parameters
    that are passed to the `kubelet` service.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 12-1](ch12.xhtml#ch12list1) 展示了传递给 `kubelet` 服务的命令行参数。'
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '*Listing 12-1: Kubelet command line*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 12-1：Kubelet 命令行*'
- en: The `pgrep kubelet` embedded command outputs the process ID of the `kubelet`
    service. We then use this to print the command line of the process using the */proc*
    Linux virtual filesystem. We use `strings` to print this file rather than `cat`
    because each separate command line parameter is null-terminated and `strings`
    turns this into a nice multiline display.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '`pgrep kubelet`嵌入命令输出`kubelet`服务的进程ID。我们接着使用该ID通过*/proc* Linux虚拟文件系统打印进程的命令行。我们使用`strings`来打印该文件，而不是`cat`，因为每个单独的命令行参数都是以空字符结尾，`strings`会将其转换为良好的多行显示格式。'
- en: 'The `kubelet` service needs three main groups of configuration options: *cluster
    configuration*, *container runtime configuration*, and *network configuration*.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubelet`服务需要三个主要的配置选项组：*集群配置*、*容器运行时配置*和*网络配置*。'
- en: Kubelet Cluster Configuration
  id: totrans-20
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Kubelet 集群配置
- en: The cluster configuration options tell `kubelet` how to communicate with the
    cluster and how to authenticate. When `kubelet` starts for the first time, it
    uses the `bootstrap-kubeconfig` shown in [Listing 12-1](ch12.xhtml#ch12list1)
    to find the cluster, verify the server certificate, and authenticate using the
    bootstrap token we discussed in [Chapter 11](ch11.xhtml#ch11). This bootstrap
    token is used to submit a Certificate Signing Request (CSR) for this new node.
    The `kubelet` then downloads the signed client certificate from the API server
    and stores it in */etc/kubernetes/kubelet.conf*, the location specified by the
    `kubeconfig` option. This *kubelet.conf* file follows the same format that is
    used to configure `kubectl` to talk to the API server, as we saw in [Chapter 11](ch11.xhtml#ch11).
    After *kubelet.conf* has been written, the bootstrap file is deleted.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 集群配置选项告诉`kubelet`如何与集群通信以及如何进行身份验证。当`kubelet`第一次启动时，它使用[清单 12-1](ch12.xhtml#ch12list1)中显示的`bootstrap-kubeconfig`来查找集群，验证服务器证书，并使用我们在[第
    11 章](ch11.xhtml#ch11)中讨论的引导令牌进行身份验证。这个引导令牌用于提交此新节点的证书签名请求（CSR）。然后，`kubelet`从API服务器下载签名的客户端证书，并将其存储在*/etc/kubernetes/kubelet.conf*中，这是由`kubeconfig`选项指定的位置。此*kubelet.conf*文件遵循与配置`kubectl`与API服务器通信相同的格式，正如我们在[第
    11 章](ch11.xhtml#ch11)中看到的那样。在*kubelet.conf*写入后，引导文件会被删除。
- en: 'The */var/lib/kubelet/config.yaml* file specified in [Listing 12-1](ch12.xhtml#ch12list1)
    also contains important configuration information. To pull metrics from `kubelet`,
    we need to set it up with its own server certificate, not just a client certificate,
    and we need to configure how it authenticates its own clients. Here is the relevant
    content from the configuration file, created by `kubeadm`:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在[清单 12-1](ch12.xhtml#ch12list1)中指定的*/var/lib/kubelet/config.yaml*文件也包含了重要的配置内容。为了从`kubelet`拉取度量信息，我们需要为其配置自己的服务器证书，而不仅仅是客户端证书，并且需要配置它如何验证自己的客户端。以下是由`kubeadm`创建的配置文件中的相关内容：
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `authentication` section tells `kubelet` not to allow anonymous requests,
    but to allow both webhook bearer tokens as well as any client certificates signed
    by the cluster certificate authority. The YAML resource file we installed for
    the metrics server includes a ServiceAccount that is used in its Deployment, so
    it is automatically injected with credentials that it can use to authenticate
    to `kubelet` instances, as we saw in [Chapter 11](ch11.xhtml#ch11).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '`authentication`部分告诉`kubelet`不允许匿名请求，但允许webhook承载令牌以及由集群证书颁发机构签名的任何客户端证书。我们为度量服务器安装的YAML资源文件包括一个ServiceAccount，该账户在其部署中使用，因此它会自动注入凭证，供其用来向`kubelet`实例进行身份验证，正如我们在[第
    11 章](ch11.xhtml#ch11)中看到的那样。'
- en: Kubelet Container Runtime Configuration
  id: totrans-25
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Kubelet 容器运行时配置
- en: The container runtime configuration options tell `kubelet` how to connect to
    the container runtime so that `kubelet` can manage containers on the local machine.
    Because `kubelet` expects the runtime to support the Container Runtime Interface
    (CRI) standard, only a couple of settings are needed, as shown in [Listing 12-1](ch12.xhtml#ch12list1).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 容器运行时配置选项告诉`kubelet`如何连接到容器运行时，以便`kubelet`能够管理本地机器上的容器。由于`kubelet`期望运行时支持容器运行时接口（CRI）标准，因此只需要几个设置，如[清单
    12-1](ch12.xhtml#ch12list1)所示。
- en: The first key setting is `container-runtime`, which can be set to either `remote`
    or `docker`. Kubernetes predates the separation of the Docker engine from the
    `containerd` runtime, so it had legacy support for Docker that used a *shim* to
    emulate the standard CRI interface. Because we are using `containerd` directly
    and not via the Docker shim or Docker engine, we set this to `remote`.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个关键设置是 `container-runtime`，可以设置为 `remote` 或 `docker`。Kubernetes 诞生于 Docker
    引擎与 `containerd` 运行时分离之前，因此它对 Docker 有遗留支持，使用 *shim* 来模拟标准的 CRI 接口。因为我们直接使用 `containerd`，而不是通过
    Docker shim 或 Docker 引擎，所以我们将其设置为 `remote`。
- en: Next, we specify the path to the container runtime using the `container-runtime-endpoint`
    setting. The value in this case is */run/containerd/containerd.sock*. The `kubelet`
    connects to this Unix socket to send CRI requests and receive status.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用 `container-runtime-endpoint` 设置指定容器运行时的路径。此情况下的值是 */run/containerd/containerd.sock*。`kubelet`
    连接到这个 Unix 套接字以发送 CRI 请求并接收状态。
- en: 'The `container-runtime-endpoint` command line setting is the only difference
    needed to switch the cluster between `containerd` and CRI-O. Additionally, it
    is automatically detected by `kubeadm` when the node is initialized, so the only
    difference in the automated scripts is to install CRI-O rather than `containerd`
    prior to installing Kubernetes. If we look at the command line for `kubelet` in
    our CRI-O cluster, we see only one change in the command line options:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '`container-runtime-endpoint` 命令行设置是切换集群在 `containerd` 和 CRI-O 之间所需的唯一差异。此外，当节点初始化时，`kubeadm`
    会自动检测到它，因此自动化脚本中的唯一差异是在安装 Kubernetes 之前安装 CRI-O，而不是 `containerd`。如果我们查看 CRI-O
    集群中 `kubelet` 的命令行选项，我们会看到命令行选项中只有一个变化：'
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The rest of the command line options are identical to our `containerd` cluster.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的命令行选项与我们的 `containerd` 集群相同。
- en: 'Finally, we have one more setting that is relevant to the container runtime:
    `pod-infra-container-image`. This specifies the Pod infrastructure image. We saw
    this image in [Chapter 2](ch02.xhtml#ch02) in the form of a `pause` process that
    was the owner of Linux namespaces created for our containers. In this case, this
    `pause` process will come from the container image `k8s.gcr.io/pause:3.4.1`.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还有一个与容器运行时相关的设置：`pod-infra-container-image`。此设置指定 Pod 基础设施镜像。我们在[第2章](ch02.xhtml#ch02)中以
    `pause` 进程的形式看到了这个镜像，它是为我们的容器创建的 Linux 命名空间的所有者。在这种情况下，这个 `pause` 进程将来自容器镜像 `k8s.gcr.io/pause:3.4.1`。
- en: It’s highly convenient to have a separate container to own the namespaces that
    are shared between the containers in a Pod. Because the `pause` process doesn’t
    really do anything, it is very reliable and isn’t likely to crash, so it can continue
    to own these shared namespaces even if the other containers in the Pod terminate
    unexpectedly.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有一个单独的容器来管理 Pod 中容器之间共享的命名空间是非常方便的。因为 `pause` 进程实际上什么都不做，它非常可靠，不容易崩溃，所以即使 Pod
    中的其他容器意外终止，它也能继续管理这些共享的命名空间。
- en: 'The `pause` image clocks in at around 300kb, as we can see by running `crictl`
    on one of our nodes:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`pause` 镜像大约有 300KB，如我们在其中一个节点上运行 `crictl` 所见：'
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Additionally, the `pause` process uses practically no CPU, so the effect on
    our nodes of having an extra process for every Pod is minimal.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，`pause` 进程几乎不占用 CPU，因此每个 Pod 为每个节点增加一个额外进程对节点的影响很小。
- en: Kubelet Network Configuration
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Kubelet 网络配置
- en: Network configuration helps `kubelet` integrate itself into the cluster and
    to integrate Pods into the overall cluster network. As we saw in [Chapter 8](ch08.xhtml#ch08),
    the actual Pod network setup is performed by a network plug-in, but the `kubelet`
    has a couple of important roles as well.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 网络配置帮助 `kubelet` 将其集成到集群中，并将 Pods 集成到整体的集群网络中。正如我们在[第8章](ch08.xhtml#ch08)中看到的，实际的
    Pod 网络设置是由网络插件执行的，但 `kubelet` 也有几个重要的角色。
- en: 'Our `kubelet` command line includes one option relevant to the network configuration:
    `node-ip`. It’s an optional flag, and if it is not present, `kubelet` will try
    to determine the IP address it should use to communicate with the API server.
    However, specifying the flag directly is useful because it guarantees that our
    cluster works in cases for which nodes have multiple network interfaces (such
    as the Vagrant configuration in this book’s examples, where a separate internal
    network is used for cluster communication).'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`kubelet`命令行包括一个与网络配置相关的选项：`node-ip`。这是一个可选标志，如果没有提供，`kubelet`将尝试确定它应该使用的IP地址与API服务器进行通信。然而，直接指定该标志是有用的，因为它可以确保我们的集群在节点有多个网络接口的情况下正常工作（例如本书示例中的Vagrant配置，其中使用一个单独的内部网络进行集群通信）。
- en: 'In addition to this one command line option, `kubeadm` places two important
    network settings in */var/lib/kubelet/config.yaml*:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这一行命令行选项外，`kubeadm`还将两个重要的网络设置放入*/var/lib/kubelet/config.yaml*：
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: These settings are used to provide the */etc/resolv.conf* file to all containers.
    The `clusterDNS` entry provides the IP address of this DNS server, whereas the
    `clusterDomain` entry provides a default domain for searches so that we can distinguish
    between hostnames inside the cluster and hostnames on external networks.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这些设置用于将*/etc/resolv.conf*文件提供给所有容器。`clusterDNS`条目提供了该DNS服务器的IP地址，而`clusterDomain`条目提供了一个默认的搜索域，以便我们区分集群内部的主机名和外部网络上的主机名。
- en: 'Let’s take a quick look at how these values are provided to the Pod. We’ll
    begin by creating a Pod:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速查看这些值是如何提供给Pod的。我们将从创建一个Pod开始：
- en: '[PRE6]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'After a few seconds, when the Pod is running, we can get a shell:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 几秒钟后，当Pod正在运行时，我们可以获取一个shell：
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Notice that */etc/resolv.conf* is a separately mounted file in our container:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，*/etc/resolv.conf*是我们容器中单独挂载的文件：
- en: '[PRE8]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Its contents reflect the `kubelet` configuration:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 其内容反映了`kubelet`的配置：
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This DNS configuration points to the DNS server that is part of the Kubernetes
    cluster core components, enabling the Service lookup we saw in [Chapter 9](ch09.xhtml#ch09).
    Depending on the DNS configuration in your network, you might see other items
    in the `search` list beyond what is shown here.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这个DNS配置指向Kubernetes集群核心组件的一部分DNS服务器，使得我们在[第9章](ch09.xhtml#ch09)中看到的服务查找成为可能。根据你网络中的DNS配置，你可能会在`search`列表中看到其他项目，而不仅仅是这里显示的内容。
- en: 'While we’re here, note also that */run/secrets/kubernetes.io/serviceaccount*
    is also a separately mounted directory in our container. This directory contains
    the ServiceAccount information we saw in [Chapter 11](ch11.xhtml#ch11) to enable
    authentication with the API server from within a container:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 同时请注意，*/run/secrets/kubernetes.io/serviceaccount*也是我们容器中单独挂载的目录。这个目录包含了我们在[第11章](ch11.xhtml#ch11)中看到的ServiceAccount信息，用于在容器内与API服务器进行身份验证：
- en: '[PRE10]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In this case, the mounted directory is of type `tmpfs` because `kubelet` has
    created an in-memory filesystem to hold the authentication information.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，挂载的目录是`tmpfs`类型，因为`kubelet`已经创建了一个内存文件系统来存储认证信息。
- en: 'Let’s finish by exiting the shell session and deleting the Pod (we no longer
    need it):'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过退出shell会话并删除Pod来结束操作（我们不再需要它）：
- en: '[PRE11]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This cleanup will make upcoming Pod listings clearer as we look at how the
    cluster reacts when a node stops working. Before we do that, we have one more
    key mystery to solve: how `kubelet` can host the control plane and also depend
    on it.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这次清理将使得接下来的Pod列表更加清晰，因为我们将查看当一个节点停止工作时集群的反应。在此之前，我们还有一个关键的谜题需要解决：`kubelet`如何同时托管控制平面并依赖于它。
- en: Static Pods
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 静态Pods
- en: We have something of a chicken-or-egg problem with creating our cluster. We
    want `kubelet` to manage the control plane components as Pods because that makes
    it easier to monitor, maintain, and update the control plane components. However,
    `kubelet` is dependent on the control plane to determine what containers to run.
    The solution is for `kubelet` to support static Pod definitions that it pulls
    from the filesystem and runs automatically prior to having its control plane connection.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 创建我们的集群时，我们遇到了一种“先鸡还是先蛋”的问题。我们希望`kubelet`能够管理控制平面组件作为 Pods，因为这样可以更容易地监控、维护和更新控制平面组件。然而，`kubelet`依赖于控制平面来决定运行哪些容器。解决方案是让`kubelet`支持静态Pod定义，它从文件系统中拉取并在建立控制平面连接之前自动运行。
- en: 'This static Pod configuration is handled in */var/lib/kubelet/config.yaml*:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这个静态Pod配置在*/var/lib/kubelet/config.yaml*中处理：
- en: '[PRE12]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'If we look in */etc/kubernetes/manifests*, we see a number of YAML files. These
    files were placed by `kubeadm` and define the Pods necessary to run the control
    plane components for this node:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看 */etc/kubernetes/manifests*，我们会看到多个 YAML 文件。这些文件是由 `kubeadm` 放置的，定义了运行此节点控制平面组件所必需的
    Pods：
- en: '[PRE13]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As expected, we see a YAML file for each of the three essential control plane
    services we discussed in [Chapter 11](ch11.xhtml#ch11). We also see a Pod definition
    for `etcd`, the component that stores the cluster’s state and helps elect a leader
    for our highly available cluster. We’ll look at `etcd` in more detail in [Chapter
    16](ch16.xhtml#ch16).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，我们看到每个我们在[第11章](ch11.xhtml#ch11)讨论过的三个关键控制平面服务都有一个 YAML 文件。我们还看到一个 `etcd`
    的 Pod 定义，`etcd` 是存储集群状态并帮助选举领导者以确保我们集群高可用的组件。我们将在[第16章](ch16.xhtml#ch16)中更详细地了解
    `etcd`。
- en: 'Each of these files contains a Pod definition just like the ones we’ve already
    seen:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这些文件中的每一个都包含一个 Pod 定义，类似于我们已经看到的：
- en: '[PRE14]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The `kubelet` service continually monitors this directory for any changes, and
    updates the corresponding static Pod accordingly, which makes it possible for
    `kubeadm` to upgrade the cluster’s control plane on a rolling basis without any
    downtime.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubelet` 服务持续监控此目录中的任何变化，并相应地更新对应的静态 Pod，这使得 `kubeadm` 能够在不中断的情况下，按滚动方式升级集群的控制平面。'
- en: Cluster add-ons like Calico and Longhorn could also be run using this directory,
    but they instead use a DaemonSet to have the cluster run a Pod on each node. This
    makes sense, as a DaemonSet can be managed once for the whole cluster, guaranteeing
    a consistent configuration across all nodes.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 集群附加组件如 Calico 和 Longhorn 也可以使用这个目录运行，但它们使用 DaemonSet 来确保集群在每个节点上运行一个 Pod。这是有道理的，因为
    DaemonSet 可以一次性管理整个集群，确保所有节点之间的配置一致。
- en: 'This static Pod directory is different on our three control plane nodes, *host01*
    through *host03*, compared to our “normal” node, *host04*. To make *host04* a
    normal node, `kubeadm` omits the control plane static Pod files from */etc/kubernetes/manifests*:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这个静态 Pod 目录在我们的三个控制平面节点 *host01* 到 *host03* 与我们的“普通”节点 *host04* 上有所不同。为了将 *host04*
    设为普通节点，`kubeadm` 会在 */etc/kubernetes/manifests* 中省略控制平面的静态 Pod 文件：
- en: '[PRE15]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note that this command is run from *host04*, our sole normal node in this cluster.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个命令是在 *host04* 上执行的，它是我们集群中唯一的普通节点。
- en: Node Maintenance
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 节点维护
- en: The controller manager component of the control plane continuously monitors
    nodes to ensure that they are still connected and healthy. The `kubelet` service
    has the responsibility of reporting node information, including node memory consumption,
    disk consumption, and connection to the underlying container runtime. If a node
    becomes unhealthy, the control plane will shift Pods to other nodes to maintain
    the requested scale for Deployments, and will not schedule any new Pods to the
    node until it is healthy again.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 控制平面中的控制器管理器组件持续监控节点，以确保它们仍然连接且健康。`kubelet` 服务负责报告节点信息，包括节点内存使用、磁盘使用和与底层容器运行时的连接。如果一个节点变得不健康，控制平面将把
    Pods 移动到其他节点，以维持部署的预期规模，并且在节点恢复健康之前，不会向该节点调度任何新的 Pods。
- en: Node Draining and Cordoning
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 节点排空与隔离
- en: If we know that we need to perform maintenance on a node, such as a reboot,
    we can tell the cluster to transfer Pods off of the node and mark the node as
    unscheduleable. We do this using the `kubectl drain` command.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们知道需要对某个节点进行维护，比如重启，我们可以告诉集群将 Pods 从该节点迁移，并将该节点标记为不可调度。我们通过使用 `kubectl drain`
    命令来实现这一点。
- en: 'To see an example, let’s create a Deployment with eight Pods, making it likely
    that each of our nodes will get a Pod:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，我们创建一个有八个 Pods 的 Deployment，这样每个节点很可能会获得一个 Pod：
- en: '[PRE16]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'If we allow enough time for startup, we can see that the Pods are distributed
    across the nodes:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们给足够的启动时间，我们可以看到 Pods 被分配到各个节点上：
- en: '[PRE17]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: To minimize the size of our test cluster, our normal node `host04` is small
    in terms of resources, so in this example it gets only one of the Pods. But that’s
    sufficient to see what happens when we shut down the node. This process is somewhat
    random, so if you don’t see any Pods allocated to `host04`, you can delete the
    Deployment and try again or scale it down and then back up, as we do in the next
    example.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最小化我们的测试集群大小，我们的普通节点 `host04` 资源较少，因此在这个例子中它只获得一个 Pod。但这足以看到当我们关闭节点时会发生什么。这个过程有一定的随机性，所以如果你没有看到任何
    Pods 分配到 `host04`，你可以删除 Deployment 重新尝试，或者像我们在下一个例子中那样缩小后再放大。
- en: 'To shut down the node, we use the `kubectl drain` command:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 要关闭节点，我们使用 `kubectl drain` 命令：
- en: '[PRE18]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We need to provide the `--ignore-daemonsets` option because all of our nodes
    have Calico and Longhorn DaemonSets, and of course, those Pods cannot be transferred
    to another node.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要提供`--ignore-daemonsets`选项，因为我们所有的节点都运行着Calico和Longhorn DaemonSets，当然，这些Pod无法迁移到其他节点。
- en: 'The eviction will take a little time. When it’s complete, we can see that the
    Deployment has created a Pod on another node, which keeps our Pod count at eight:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 驱逐过程会花费一些时间。完成后，我们可以看到部署在另一个节点上创建了一个Pod，这样我们的Pod数量保持在八个：
- en: '[PRE19]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Additionally, the node has been *cordoned*, thus no more Pods will be scheduled
    on it:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，节点已被*隔离*，因此将不会再有Pod被调度到该节点上：
- en: '[PRE20]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'At this point, it is safe to stop `kubelet` or the container runtime, to reboot
    the node, or even to delete it from Kubernetes entirely:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，停止`kubelet`或容器运行时、重启节点，甚至完全从Kubernetes中删除该节点都是安全的：
- en: '[PRE21]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This deletion removes the node information from the cluster’s storage, but
    because the node still has a valid client certificate and all its configuration,
    a simple restart of the `kubelet` service on `host04` will add it back to the
    cluster. First let’s restart `kubelet`:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 该删除操作会从集群的存储中移除节点信息，但由于该节点仍然拥有有效的客户端证书和所有配置，简单地重启`host04`上的`kubelet`服务将把它重新加入集群。首先让我们重启`kubelet`：
- en: '[PRE22]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Be sure to do this on `host04`. Next, back on `host01`, if we wait for `kubelet`
    on `host04` to finish cleaning up from its previous run and to reinitialize, we
    can see it return in the list of nodes:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 请确保在`host04`上执行此操作。接下来，在`host01`上，如果我们等待`host04`上的`kubelet`完成上次运行的清理并重新初始化，我们会看到它重新出现在节点列表中：
- en: '[PRE23]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Note that the cordon has been removed and `host04` no longer shows a status
    that includes `SchedulingDisabled`. This is one way to remove the cordon. The
    other is to do it directly using `kubectl uncordon`.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，隔离已经被移除，`host04`不再显示包含`SchedulingDisabled`的状态。这是移除隔离的一种方式，另一种方式是直接使用`kubectl
    uncordon`命令。
- en: Unhealthy Nodes
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 不健康节点
- en: Kubernetes will also shift Pods on a node automatically if the node becomes
    unhealthy as a result of resource constraints such as insufficient memory or disk
    space. Let’s simulate a low-memory condition on `host04` so that we can see this
    in action.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果节点因内存不足或磁盘空间等资源限制变得不健康，Kubernetes还会自动将Pod迁移到其他节点。让我们模拟`host04`上内存不足的情况，以便观察这一过程。
- en: 'First, we’ll need to reset the scale of our `debug` Deployment to ensure that
    new Pods are allocated onto `host04`:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要重置`debug`部署的规模，以确保新的Pod被分配到`host04`上：
- en: '[PRE24]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We first scale the Deployment all the way down, and then we scale it back up.
    This way, we get more chances to schedule at least one Pod on `host04`. As soon
    as the Pods have had a chance to settle, we see Pods on `host04` again:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将部署的规模缩减到最小，然后再将其扩大。这样，我们有更多机会将至少一个Pod调度到`host04`上。一旦Pod有机会稳定下来，我们会看到`host04`上再次有Pod：
- en: '[PRE25]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We can check the current statistics for our nodes using `kubectl top`:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`kubectl top`来检查当前节点的统计信息：
- en: '[PRE26]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We have 2GB total on `host04`, and currently we’re using more than 500MiB. By
    default, `kubelet` will evict Pods when there is less than 100MiB of memory remaining.
    We could try to use up memory on the node to get below that default threshold,
    but it’s chancy because using up so much memory could make our node behave badly.
    Instead, let’s update the eviction limit. To do this, we’ll add lines to */var/lib/kubelet/config.yaml*
    and then restart `kubelet`.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`host04`总共有2GB内存，目前已使用超过500MiB。默认情况下，当剩余内存少于100MiB时，`kubelet`会驱逐Pod。我们可以尝试使用节点上的内存直到低于这个默认阈值，但这很冒险，因为大量使用内存可能会导致节点行为不正常。相反，我们可以更新驱逐限制。为此，我们将向*/var/lib/kubelet/config.yaml*添加几行，然后重启`kubelet`。'
- en: 'Here’s the additional configuration we’ll add to our `kubelet` config file:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们将添加到`kubelet`配置文件中的额外配置：
- en: '*node-evict.yaml*'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*node-evict.yaml*'
- en: '[PRE27]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This tells `kubelet` to start evicting Pods if it has less than 1,900MiB available.
    For nodes in our example cluster, that will happen right away. Let’s apply this
    change:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这会告诉`kubelet`，如果可用内存少于1,900MiB，它将开始驱逐Pod。在我们的示例集群中的节点上，这将立即发生。让我们应用这一更改：
- en: '[PRE28]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Be sure to run these commands on `host04`. The first command adds additional
    lines to the `kubelet` config file. The second command restarts `kubelet` so that
    it picks up the change.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 请确保在`host04`上执行这些命令。第一条命令会向`kubelet`配置文件中添加额外的行。第二条命令会重启`kubelet`，以便它加载这个更改。
- en: 'If we check on the node status for `host04`, it will appear to still be ready:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们检查`host04`的节点状态，它似乎仍然是就绪的：
- en: '[PRE29]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'However, the node’s event log makes clear what is happening:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，节点的事件日志清楚地显示了发生了什么：
- en: '[PRE30]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The node starts evicting Pods, and the cluster automatically creates new Pods
    on other nodes as needed to stay at the desired scale:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 节点开始驱逐 Pod，并且集群会根据需要在其他节点上自动创建新的 Pod 以保持所需的规模：
- en: '[PRE31]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Pods allocated to `host04` show `OutOfMemory`, and they have been replaced with
    Pods on other nodes. The Pods are stopped on the node, but unlike the previous
    case for which we drained the node, the Pods are not automatically terminated.
    Even if the node recovers from its low-memory situation, the Pods will continue
    to show up in the list of Pods, stuck in the `OutOfMemory` state, until `kubelet`
    is restarted.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 分配给`host04`的 Pod 显示为`OutOfMemory`，它们已被其他节点上的 Pod 替换。这些 Pod 在节点上被停止，但不像前面我们排空节点的情况，这些
    Pod 不会自动终止。即使节点从低内存状态恢复，这些 Pod 仍将显示在 Pod 列表中，处于`OutOfMemory`状态，直到重新启动`kubelet`。
- en: Node Unreachable
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 节点不可达
- en: We have one more case to look at. In our previous two examples, `kubelet` could
    communicate with the control plane to update its status, allowing the control
    plane to act accordingly. But what happens if there is a network issue or sudden
    power failure and the node loses its connection to the cluster without being able
    to report that it is shutting down? In that case, the cluster will record the
    node status as unknown, and after a timeout, it will start shifting Pods onto
    other nodes.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还有一个案例要讨论。在我们之前的两个示例中，`kubelet`可以与控制平面通信以更新其状态，使控制平面能够相应地采取行动。但是如果出现网络问题或突然断电，并且节点失去与集群的连接而无法报告正在关闭，会发生什么情况呢？在这种情况下，集群将记录节点状态为未知，并在超时后开始将
    Pod 转移到其他节点。
- en: 'Let’s simulate this. We’ll begin by restoring `host04` to its proper working
    order:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们模拟一下这种情况。我们将从恢复`host04`到正常工作状态开始：
- en: '[PRE32]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Be sure to run these commands on `host04`. The first command removes the two
    lines we added to the `kubelet` config, whereas the second restarts `kubelet`
    to pick up the change. We now can rescale our Deployment again so that it is redistributed:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 确保在`host04`上运行这些命令。第一个命令删除我们添加到`kubelet`配置中的两行，而第二个命令重新启动`kubelet`以应用更改。现在我们可以再次调整我们的部署，以便重新分配：
- en: '[PRE33]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: As before, after you’ve run these commands, allow a few minutes for the Pods
    to settle. Then, use kubectl get pods -o wide to verify that at least one Pod
    was allocated to `host04`.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前一样，在运行这些命令后，请等待几分钟以使 Pod 稳定下来。然后，使用`kubectl get pods -o wide`来验证至少有一个 Pod
    分配到了`host04`。
- en: 'We’re now ready to forcibly disconnect `host04` from the cluster. We’ll do
    this by adding a firewall rule:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备强制断开`host04`与集群的连接。我们将通过添加防火墙规则来执行此操作：
- en: '[PRE34]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Be sure to run this on `host04`. The first command tells the firewall to drop
    all traffic coming from the IP address `192.168.61.10`, which is the highly available
    IP that is shared by all three control plane nodes. The second command tells the
    firewall to drop all traffic going to that same IP address.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 确保在`host04`上运行此命令。第一个命令告诉防火墙丢弃所有来自 IP 地址`192.168.61.10`的流量，这是所有三个控制平面节点共享的高可用
    IP 地址。第二个命令告诉防火墙丢弃所有发送到同一 IP 地址的流量。
- en: 'After a minute or so, `host04` will show a state of `NotReady`:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 大约一分钟后，`host04`将显示为`NotReady`状态：
- en: '[PRE35]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'And if we wait a few minutes, the Pods on `host04` will be shown as `Terminating`
    because the cluster gives up on those Pods and shifts them to other nodes:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如果等待几分钟，`host04`上的 Pod 将显示为`Terminating`，因为集群放弃了这些 Pod 并将它们转移到其他节点：
- en: '[PRE36]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'However, because `kubelet` on `host04` can’t connect to the control plane,
    it is unaware that it should be shutting down its Pods. If we check to see what
    containers are running on `host04`, we still see multiple containers:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，因为`host04`上的`kubelet`无法连接到控制平面，它不知道它应该关闭其 Pod。如果我们检查在`host04`上运行哪些容器，我们仍然会看到多个容器在运行：
- en: '[PRE37]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Not only are the Pods still running, but because of the way we cut off the connection,
    they are still able to communicate with the rest of the cluster. This is very
    important. Kubernetes will do its best to run the number of instances requested
    and to respond to errors, but it can only do that based on the information it
    has available. In this case, because `kubelet` on `host04` can’t talk to the control
    plane, Kubernetes has no way of knowing that the Pods are still running. When
    building applications for a distributed system like a Kubernetes cluster, you
    should recognize that some types of errors can have surprising results, like partial
    network connectivity or a different number of instances compared to what is specified.
    In more advanced application architectures that include rolling updates, this
    can even lead to cases in which old versions of application components are still
    running unexpectedly. Be sure to build applications that are resilient in the
    face of these kinds of surprising behaviors.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅Pods仍在运行，而且由于我们切断连接的方式，它们仍然能够与集群的其余部分进行通信。这一点非常重要。Kubernetes会尽力运行请求的实例数量并响应错误，但它只能基于它所拥有的信息来执行此操作。在这种情况下，由于`host04`上的`kubelet`无法与控制平面通信，Kubernetes无法知道Pods仍然在运行。在为像Kubernetes集群这样的分布式系统构建应用时，你应该认识到某些类型的错误可能会导致意想不到的结果，比如部分网络连接或与指定实例数量不同的情况。在包含滚动更新的更高级的应用架构中，这甚至可能导致旧版本的应用组件意外运行。确保构建能够应对这些意外行为的具有弹性的应用。
- en: Final Thoughts
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最后的思考
- en: Ultimately, to have a Kubernetes cluster, we need nodes that can run containers,
    and that means instances of `kubelet` connected to the control plane and a container
    runtime. In this chapter, we’ve inspected how to configure `kubelet` and how the
    cluster behaves when nodes leave or enter the cluster, either intentionally or
    through an outage.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，要拥有一个 Kubernetes 集群，我们需要能够运行容器的节点，这意味着需要连接到控制平面和容器运行时的`kubelet`实例。在本章中，我们检查了如何配置`kubelet`以及当节点离开或加入集群时，集群的行为——无论是故意的还是由于故障。
- en: One of the key themes of this chapter is the way that Kubernetes acts to keep
    the specified number of Pods running, even in the face of node issues. In the
    next chapter, we’ll see how that monitoring extends inside the container to its
    processes, ensuring that the processes run as expected. We’ll see how to specify
    probes that allow Kubernetes to monitor containers, and how the cluster responds
    when a container is unhealthy.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的一个关键主题是Kubernetes如何在节点出现问题时仍然保持指定数量的Pods运行。在下一章中，我们将看到如何将监控扩展到容器内部及其进程，确保进程按预期运行。我们将看到如何指定探针以允许Kubernetes监控容器，以及当容器不健康时集群如何响应。
