<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" epub:prefix="index: http://www.index.com/" lang="en" xml:lang="en">
<head>
<title>Chapter 10: Infrastructure</title>
<link href="NSTemplate_v1.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:c2f206e1-36e8-4f89-b533-508263d6ec16" name="Adept.expected.resource"/>
</head>
<body epub:type="bodymatter chapter">
<section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" id="Page_91" title="91"/>10</span><br/>
<span class="ChapterTitle">Infrastructure</span></h1>
</header>
<figure class="graphic">
<img alt="g10001" src="image_fi/502680c10/g10001.png"/></figure>

<p class="ChapterIntro"><span class="dropCap">O</span>ne of the non-obvious parts about any software project, particularly a project that is being worked on by more than just one or two people, is the infrastructure that you need to actually build the product. <em>Infrastructure</em> can refer to a number of things, including:</p>
<ol class="none">
<li><span class="RunInHead">Building</span>  How do you take the code that random engineers are constantly submitting and build the product? What if the product needs to run on various different devices and not just one? And where do you store all of these builds for testing, debugging, and releasing purposes?</li>
<li><span class="RunInHead">Testing</span>  How do you test the product once it’s built? And how do you test it continuously so that you can catch bugs that have crept in before they cause serious problems (and while you can more easily trace them back to when they were first submitted so that you can find and fix them)?</li>
<li><span class="RunInHead">Source code control</span>  Where do you store all of the code? And how do you allow a team of people to make simultaneous changes to the same source code files?</li>
<li><span class="RunInHead">Release</span>  How do you actually ship the product to the devices that need it?</li>
</ol>
<p>Android needed people dedicated to solving these infrastructure problems.</p>
<h2 id="h1-502680c10-0001"><span epub:type="pagebreak" id="Page_92" title="92"/>Joe Onorato and the Build</h2>
<p class="BodyFirst">In the beginning, Android builds were cobbled together by a fragile and time-consuming system that built all of the constituent pieces for the kernel, the platform, the apps, and everything in between. This system was fine in the early days when there wasn’t much to build, but Android was getting too big for it to work any longer. So in the Spring of 2006, Joe Onorato attacked the problem.</p>
<p>Joe figures he was destined to be a programmer, since both of his parents were MIT grads. “They met at the Tech Model Railroad Club;<sup class="FootnoteReference"><a href="#c10-footnote-1" id="c10-footnoteref-1">1</a></sup> it was love at first chat. It was pretty much obvious that I was going to be a computer scientist.”</p>
<p>In high school, Joe worked on the yearbook with his friend Jeff Hamilton (a future Be, PalmSource, and Android colleague), making the first Jostens<sup class="FootnoteReference"><a href="#c10-footnote-2" id="c10-footnoteref-2">2</a></sup> yearbook that was entirely digital. Their system included a custom search algorithm and a digitizing system that simplified publishing while decreasing the cost for the students. Joe later worked (again, with Jeff) at Be, and then PalmSource, on operating system projects that were similar to what he would work on later at Android.</p>
<p>In late 2005, Joe wasn’t excited about where PalmSource was going, so he reached out to a former colleague from Be. That person knew Swetland and got Joe routed over to the Android team. Joe got an offer, but wasn’t sure what he was signing up for, so the recruiter got him in touch with Andy. After assurances of confidentiality, Andy told Joe, “We’re going to make the best phone ever.” That’s when Joe joined the Android team. </p>
<p>Joe worked on several projects in those early times, including the framework and the UI toolkit. But in the Spring of 2006, he saw that the build system needed a serious restructuring.</p>
<p>“We had a big recursive<sup class="FootnoteReference"><a href="#c10-footnote-3" id="c10-footnoteref-3">3</a></sup> make build system, and I was like, ‘Let’s have a real build system.’ It was somewhat controversial: is it even possible?” Fortunately, Joe had experience from Be. Be used a similar build system, which was written by a group of people including future Android engineer <span epub:type="pagebreak" id="Page_93" title="93"/>Jean-Baptiste Quéru (who was known to the team as “JBQ”). Joe remembered, “I think some of the Danger folks [who had also worked at Be] had left before that happened and thought that was an impossible thing to do. How could you have one make file that knows about everything? Like it’s going to get all confused. But . . . it worked.”</p>
<p>Joe dove in and made the build system work for Android, speeding it up and making it more robust in the process. The whole project took a couple months, resulting in a system called Total Dependency Awareness.</p>
<h2 id="h1-502680c10-0002">Ed Heyl and Android Infrastructure</h2>
<blockquote class="Epigraph" epub:type="epigraph">
<p class="Epigraph">The first monkey lab was my laptop and seven Dream devices. I wrote some scripts and tools to beat the shit out of them till they crashed.</p>
<p class="EpigraphSource">—Ed Heyl</p>
</blockquote>
<p class="BodyFirst">The build system that Joe wrote worked sufficiently for a while. But as the team and the number of code submissions grew, there was a need for a system that could automatically build the product as developers submitted their changes. For example, if someone submits code that causes a bug, it’s better to be able to build and test the product with just that change than to wait until after many other changes have piled on top of it, obscuring the root cause of the problem. </p>
<p>In September of 2007, to get the build and test infrastructure under control, the team brought in Ed Heyl, who was then working at Microsoft.</p>
<p>In college Ed studied computer science but couldn’t wait to graduate. “I was looking to get out as fast as possible and get into the workforce. I did okay in school . . . but I excelled at work.”</p>
<p>Ed joined Apple in 1987, where he worked for five years. “The company was in a really weird state. They were still making all their money off the Apple II, but all the mindshare was going into Mac.” A few years later, Ed joined the Taligent<sup class="FootnoteReference"><a href="#c10-footnote-4" id="c10-footnoteref-4">4</a></sup> spin-out, followed by General Magic soon after, “right when they did their IPO. It set the record for IPO gain, and then nose-dived <span epub:type="pagebreak" id="Page_94" title="94"/>in the months after. The company itself was not very healthy at that time. All the people were already kind of disenchanted. There was so much hype building up to the IPO that there was a lot of letdown.”</p>
<p>Ed lasted at General Magic for about ten months, then joined WebTV. He stayed through the acquisition by Microsoft and another ten years until joining the Android team. At WebTV and Microsoft, Ed worked with future Android people, including Andy Rubin, Steve Horowitz, Mike Cleron, and Andy McFadden. </p>
<p>Ed started on the Android team around the time that the Android SDK was first released, in October of 2007. At the time Ed joined, Android already had an automated build system called <em>Launch Control</em>. Three times per day, it would take whatever code had been submitted and build it, producing a result that was then available for the automated testing system.</p>
<p>Launch Control was better than nothing, but it was nowhere near what Android needed. “It was something for QA to test, as opposed to a dashboard to show the state of the world. There wasn’t a lot of traceability. Continuous integration<sup class="FootnoteReference"><a href="#c10-footnote-5" id="c10-footnoteref-5">5</a></sup> tries to build and test as much as it can to give you as many data points as it can.”</p>
<p>The team needed a system that would build and test far more often. It also needed to scale up. At the time, it was only building for a single device: Sooner. But soon the team would have Dream devices (which launched with 1.0 as the G1), and the system would have to build for multiple targets. 
</p><p>Ed started on his own, but eventually led a team of people who worked on the build. Ed said, “It was Dave Bort that took it and actually made it good enough to base products on. Made it really solid, with a good design and a good layout of how things worked. Dave Bort took it from a good but sloppy build system to a product.</p>
<p>“At the same time he reorganized the build system, he reorganized the whole source tree. He set all of the fundamentals in place for open source and architectural level things. Even though he worked on the build system, <span epub:type="pagebreak" id="Page_95" title="95"/>it was architectural; it rippled through the whole system. He laid all that groundwork. He basically got Android ready for open source.”</p>
<h2 id="h1-502680c10-0003">Testing, Testing</h2>
<p class="BodyFirst">Another area that had to be figured out was testing. How do you verify that all of the random bits of software landing in the build constantly from different engineers on different parts of the system are not actually breaking things? It’s necessary, in any software system, to have some kind of automated test framework,<sup class="FootnoteReference"><a href="#c10-footnote-6" id="c10-footnoteref-6">6</a></sup> to catch problems quickly. Android didn’t have automated testing at that time, so Ed got some monkeys to do it.</p>
<p>“At WebTV, we had this thing called the monkey,<sup class="FootnoteReference"><a href="#c10-footnote-7" id="c10-footnoteref-7">7</a></sup> which would find links on web pages and just go nuts surfing everywhere. </p>
<p>“I can’t remember if Dianne had already done it [for the Android platform], or whether we were talking to her about it and she did it. But she put in the system for randomization and event injection into the framework, which we call ‘monkey’ today.</p>
<p>“I built the first monkey lab, which was my laptop and seven Dream devices. I wrote some scripts and tools to beat the shit out of them till they crashed, grabbed the crash [report] and put them back to work. I’d analyze those reports and I’d summarize them all. So every day we could have the number of events that it would handle, and what crashes it hit. Jason Parks and I, and eventually Evan Millar, hooked up a set of tools to help create our first stability numbers. That ended up living for years and years, as bad as it <span epub:type="pagebreak" id="Page_96" title="96"/>was. It was just Python<sup class="FootnoteReference"><a href="#c10-footnote-8" id="c10-footnoteref-8">8</a></sup> scripts analyzing bug reports and writing out HTML reports. In late 2008, I hired Bruce Gay [also from Microsoft]. He took that and turned it into a real lab environment.”<sup class="FootnoteReference"><a href="#c10-footnote-9" id="c10-footnoteref-9">9</a></sup></p>
<p>Bruce grew the lab over the years from an initial set of seven devices to more than 400. He said there were some unanticipated problems to resolve over that time. “One day I walked into the monkey lab to hear a voice say, ‘911—What’s your emergency?’” That situation resulted in Dianne adding a new function to the API, <code>isUserAMonkey()</code>, which is used to gate actions that monkeys shouldn’t take during tests (including dialing the phone and resetting the device).</p>
<p>Early monkey tests would run for up to 3,000 input events before crashing. By 1.0, the number was up around 5,000. Bruce said, “‘Passing’ was 125K events. It took us a few years to meet that goal.”</p>
<figure>
<img alt="" class="" src="image_fi/502680c10/f10001.png"/>
<figcaption><p>The monkey test lab in May of 2009 (photo courtesy Brian Swetland)</p></figcaption>
</figure>
<p><span epub:type="pagebreak" id="Page_97" title="97"/>Romain Guy talked about how critical monkey testing was in the run up to 1.0. “We used to rely on the monkey a lot back then. Every night we would run those monkey tests and every morning we had a lot of crashes to fix. Our goal was to get the monkey number up; how long can we run the monkey without crashing? Because they were crashing everywhere, from the widgets down to the kernel or SurfaceFlinger.<sup class="FootnoteReference"><a href="#c10-footnote-10" id="c10-footnoteref-10">10</a></sup> Especially once we switched to the touchscreen, things were a lot more complicated.”</p>
<p>In addition to monkey tests, other people on the team were working on different kinds of tests to verify that the platform had the correct behavior. Evan Millar, who joined the team out of grad school in early 2007, worked on early performance testing frameworks, timing how long it took for applications to launch. He also worked on an early system of automated testing called Puppet Master, which allowed test scripts to drive the UI (opening windows, clicking on buttons), measuring correctness against golden images.<sup class="FootnoteReference"><a href="#c10-footnote-11" id="c10-footnoteref-11">11</a></sup> The results were mixed, given the difficulty of comparing against golden images, in addition to the asynchronous nature of the tests and the platform. A test script would request a particular UI action, like clicking a button or launching an application, but it might take a while for the platform to process that event, making correctness-testing tricky and error-prone.</p>
<p>Chiu-Ki Chan dealt with some of these inherent difficulties in testing when she joined the Maps team after stints on the services and Android Market teams. She had been working on a system to automate testing of the maps app, but was increasingly frustrated with the difficulties of testing her app on a system that not designed for testing. She said, “Testing? There was no such thing as testing.”</p>
<p>An important part of overall Android testing is the Compatibility Test Suite (CTS). This was a system built initially by external contractors (managed <span epub:type="pagebreak" id="Page_98" title="98"/>by Patrick Brady<sup class="FootnoteReference"><a href="#c10-footnote-12" id="c10-footnoteref-12">12</a></sup>). CTS tests are important because they not only test specific pieces of functionality in the system and catch regressions<sup class="FootnoteReference"><a href="#c10-footnote-13" id="c10-footnoteref-13">13</a></sup> when tests fail, but they are required for partners to pass as well, guaranteeing that the Android devices they ship conform to Android’s defined platform behavior. For example, if there is a test that colors the screen white and tests that the result is, in fact, white pixels, it should be impossible for a device to reinterpret “white” as red and still pass that test. </p>
<h2 id="h1-502680c10-0004">Lean Infrastructure</h2>
<p class="BodyFirst">Android build, test, and release infrastructure, like much of the rest of Android, was created by a small team with limited resources. This was a conscious decision about where to invest limited budget given the priorities of getting the product out the door. Ed Heyl said, “We had no idea whether what we were doing was going to be successful or not. We were just trying to make a new device and be relevant. Apple was getting all the mindshare, Microsoft was not going to let go, and they were actually in the best position at that point. So everything was of the mindset: whatever we can do to make forward progress. We did not prioritize investing in really good solutions, it was just ‘we gotta get this going, prove that we can deliver and iterate.’ We never stopped and said we really need to invest in a build infrastructure, Python scripts are not going to get us very far, so we should really think about how we’re going to use the Google back-end infrastructure. We never stopped to think about that. It was just full steam ahead.</p>
<p>“If it was part of the core product, we invested more into it. But if it was just test, or build, it was minimum stuff to get it going. That’s the way we operated.”</p>
</section>
<section class="footnotes">
<aside class="FootnoteEntry"><p><sup class="FootnoteReference"><a href="#c10-footnoteref-1" id="c10-footnote-1">1.</a></sup>  The club is a hacker community at MIT that dates back to the 1940s.</p></aside>
<aside class="FootnoteEntry"><p><sup class="FootnoteReference"><a href="#c10-footnoteref-2" id="c10-footnote-2">2.</a></sup>  Jostens sells school-oriented memorabilia like class rings and yearbooks.</p></aside>
<aside class="FootnoteEntry"><p><sup class="FootnoteReference"><a href="#c10-footnoteref-3" id="c10-footnote-3">3.</a></sup>  Definition: recursion: See [recursion]. 
Recursion is a common technique in software, in which a given function calls itself. A very simple example is that the sum of all integers up to some given integer <em>x</em> can be solved by adding <em>x</em> to the sum of all integers up to (<em>x</em> – 1). Recursion is a powerful technique, but can be tricky to think through, and to ensure that it will actually terminate.</p></aside>
<aside class="FootnoteEntry"><p><sup class="FootnoteReference"><a href="#c10-footnoteref-4" id="c10-footnote-4">4.</a></sup>  Taligent was a company formed by Apple and IBM with the goal of providing a new operating system, at a time when Apple was trying to come up with a successor to the aging MacOS. Taligent eventually failed and Apple continued its attempts internally before eventually acquiring Steve Jobs’s NeXT Computer and adopting NeXTSTEP OS instead.</p></aside>
<aside class="FootnoteEntry"><p><sup class="FootnoteReference"><a href="#c10-footnoteref-5" id="c10-footnote-5">5.</a></sup>  Continuous Integration, or CI, is the practice in software development of integrating all of a team’s changes as often as possible for building and testing. It helps maintain a constant measure of the quality and stability of the product so that it doesn’t spin out of control before someone notices.</p></aside>
<aside class="FootnoteEntry"><p><sup class="FootnoteReference"><a href="#c10-footnoteref-6" id="c10-footnote-6">6.</a></sup>  Ideally, you have tests that run automatically, all the time, to make sure that changes don’t break things. Manual testing is much more expensive, time-consuming, and infrequent, so automated testing is preferable.</p></aside>
<aside class="FootnoteEntry"><p><sup class="FootnoteReference"><a href="#c10-footnoteref-7" id="c10-footnote-7">7.</a></sup>  Bruce Gay, who eventually ran the monkey lab, said the name came from the Infinite Monkey Theorem, which states that an infinite set of monkeys hitting keys on a keyboard will eventually produce the works of Shakespeare. Which seems like a slightly different goal than finding crashes in an OS. 
The monkey didn’t just come from WebTV; Dianne also used a monkey system at PalmSource. 
Andy Hertzfeld’s enjoyable book, <em>Revolution in The Valley: The Insanely Great Story of How the Mac Was Made</em>, also talks about monkeys, which apparently have a long history in platform testing. The original Mac had a desktop utility also called “The Monkey” that would similarly generate random input events to pound on the system and test its robustness. Who knew monkeys were so useful, so ubiquitous, so good at testing, and so very random?</p></aside>
<aside class="FootnoteEntry"><p><sup class="FootnoteReference"><a href="#c10-footnoteref-8" id="c10-footnote-8">8.</a></sup>  Python is a programming language, used for many things including small utility programs like the ones Ed is describing here.</p></aside>
<aside class="FootnoteEntry"><p><sup class="FootnoteReference"><a href="#c10-footnoteref-9" id="c10-footnote-9">9.</a></sup>  The monkey testing lab continues to be an important part of Android testing. Somewhere in a quiet lab are racks of devices being hammered by a horde of virtual monkeys until they crash, at which point they collect a log of the crash and file a bug. 
Damn monkeys.</p></aside>
<aside class="FootnoteEntry"><p><sup class="FootnoteReference"><a href="#c10-footnoteref-10" id="c10-footnote-10">10.</a></sup>  SurfaceFlinger is part of the low-level graphics system, which is described in <span class="xref" itemid="xref_target_Chapter 11">Chapter 11</span> (“Graphics”).</p></aside>
<aside class="FootnoteEntry"><p><sup class="FootnoteReference"><a href="#c10-footnoteref-11" id="c10-footnote-11">11.</a></sup>  Golden image testing works by saving a visual result from some known-correct run, then comparing that image to future runs. Some variation is typically allowed to account for minor differences that don’t indicate failure. This technique tends to be effective for very low-level tests (for example, verifying that a graphics API can draw shapes consistently), but can be rather fragile the more that’s involved with each test, because so much variation can be introduced that is not indicative of failure.</p></aside>
<aside class="FootnoteEntry"><p><sup class="FootnoteReference"><a href="#c10-footnoteref-12" id="c10-footnote-12">12.</a></sup>  Patrick went on to become the VP of Android Auto.</p></aside>
<aside class="FootnoteEntry"><p><sup class="FootnoteReference"><a href="#c10-footnoteref-13" id="c10-footnote-13">13.</a></sup>  Regression is a term often used in software testing. Tests are used to catch failures in software in general. A regression is a new failure on existing code; the software used to work (and the test used to pass), but now there is a failure in the software that is causing the test to fail. This tends to indicate that code checked in recently is buggy (or the test is flaky and reports failures randomly, which is depressingly more common than you would hope).</p></aside>
</section>
</body>
</html>