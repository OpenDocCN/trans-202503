<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><h2 class="h2" id="ch28"><span epub:type="pagebreak" id="page_185"/><strong><span class="big">28</span><br/>THE K IN K-FOLD CROSS-VALIDATION</strong></h2>&#13;
<div class="image1"><img src="../images/common.jpg" alt="Image" width="252" height="252"/></div>&#13;
<p class="noindent"><em>k</em>-fold cross-validation is a common choice for evaluating machine learning classifiers because it lets us use all training data to simulate how well a machine learning algorithm might perform on new data. What are the advantages and disadvantages of choosing a large <em>k</em>?</p>&#13;
<p class="indent">We can think of <em>k</em>-fold cross-validation as a workaround for model evaluation when we have limited data. In machine learning model evaluation, we care about the generalization performance of our model, that is, how well it performs on new data. In <em>k</em>-fold cross-validation, we use the training data for model selection and evaluation by partitioning it into <em>k</em> validation rounds and folds. If we have <em>k</em> folds, we have <em>k</em> iterations, leading to <em>k</em> different models, as illustrated in <a href="ch28.xhtml#ch28fig1">Figure 28-1</a>.</p>&#13;
<div class="image"><img id="ch28fig1" src="../images/28fig01.jpg" alt="Image" width="852" height="522"/></div>&#13;
<p class="figcap"><em>Figure 28-1: An example of</em> k <em>-fold cross-validation for model evaluation where</em> k <em>= 5</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_186"/>Using <em>k</em>-fold cross-validation, we usually evaluate the performance of a particular hyperparameter configuration by computing the average performance over the <em>k</em> models. This performance reflects or approximates the performance of a model trained on the complete training dataset after evaluation.</p>&#13;
<p class="indent">The following sections cover the trade-offs of selecting values for <em>k</em> in <em>k</em>-fold cross-validation and address the challenges of large <em>k</em> values and their computational demands, especially in deep learning contexts. We then discuss the core purposes of <em>k</em> and how to choose an appropriate value based on specific modeling needs.</p>&#13;
<h3 class="h3" id="ch00lev140"><strong>Trade-offs in Selecting Values for k</strong></h3>&#13;
<p class="noindent">If <em>k</em> is too large, the training sets are too similar between the different rounds of cross-validation. The <em>k</em> models are thus very similar to the model we obtain by training on the whole training set. In this case, we can still leverage the advantage of <em>k</em>-fold cross-validation: evaluating the performance for the entire training set via the held-out validation fold in each round. (Here, we obtain the training set by concatenating all <em>k</em> – 1 training folds in a given iteration.) However, a disadvantage of a large <em>k</em> is that it is more challenging to analyze how the machine learning algorithm with the particular choice of hyperparameter setting behaves on different training datasets.</p>&#13;
<p class="indent">Besides the issue of too-similar datasets, running <em>k</em>-fold cross-validation with a large value of <em>k</em> is also computationally more demanding. A larger <em>k</em> is more expensive since it increases both the number of iterations and the training set size at each iteration. This is especially problematic if we work with relatively large models that are expensive to train, such as contemporary deep neural networks.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_187"/>A common choice for <em>k</em> is typically 5 or 10, for practical and historical reasons. A study by Ron Kohavi (see “<a href="ch28.xhtml#ch00lev143">References</a>” at the end of this chapter) found that <em>k</em> = 10 offers a good bias and variance trade-off for classical machine learning algorithms, such as decision trees and naive Bayes classifiers, on a handful of small datasets.</p>&#13;
<p class="indent">For example, in 10-fold cross-validation, we use 9/10 (90 percent) of the data for training in each round, whereas in 5-fold cross-validation, we use only 4/5 (80 percent) of the data, as shown in <a href="ch28.xhtml#ch28fig2">Figure 28-2</a>.</p>&#13;
<div class="image"><img id="ch28fig2" src="../images/28fig02.jpg" alt="Image" width="851" height="348"/></div>&#13;
<p class="figcap"><em>Figure 28-2: A comparison of 5-fold and 10-fold cross-validation</em></p>&#13;
<p class="indent">However, this does not mean large training sets are bad, since they can reduce the pessimistic bias of the performance estimate (mostly a good thing) if we assume that the model training can benefit from more training data. (See <a href="ch05.xhtml#ch5fig1">Figure 5-1</a> on <a href="ch05.xhtml#ch5fig1">page 24</a> for an example of a learning curve.)</p>&#13;
<p class="indent">In practice, both a very small and a very large <em>k</em> may increase variance. For instance, a larger <em>k</em> makes the training folds more similar to each other since a smaller proportion is left for the held-out validation sets. Since the training folds are more similar, the models in each round will be more similar. In practice, we may observe that the variance of the held-out validation fold scores is more similar for larger values of <em>k</em>. On the other hand, when <em>k</em> is large, the validation sets are small, so they may contain more random noise or be more susceptible to quirks of the data, leading to more variation in the validation scores across the different folds. Even though the models themselves are more similar (since the training sets are more similar), the validation scores may be more sensitive to the particularities of the small validation sets, leading to higher variance in the overall cross-validation score.</p>&#13;
<h3 class="h3" id="ch00lev141"><strong>Determining Appropriate Values for k</strong></h3>&#13;
<p class="noindent">When deciding upon an appropriate value of <em>k</em>, we are often guided by computational performance and conventions. However, it’s worthwhile to define the purpose and context of using <em>k</em>-fold cross-validation. For example, if we care primarily about approximating the predictive performance of the final model, using a large <em>k</em> makes sense. This way, the training folds are <span epub:type="pagebreak" id="page_188"/>very similar to the combined training dataset, yet we still get to evaluate the model on all data points via the validation folds.</p>&#13;
<p class="indent">On the other hand, if we care to evaluate how sensitive a given hyperparameter configuration and training pipeline is to different training datasets, then choosing a smaller number for <em>k</em> makes more sense.</p>&#13;
<p class="indent">Since most practical scenarios consist of two steps—tuning hyperparameters and evaluating the performance of a model—we can also consider a two-step procedure. For instance, we can use a smaller <em>k</em> during hyperparameter tuning. This will help speed up the hyperparameter search and probe the hyperparameter configurations for robustness (in addition to the average performance, we can also consider the variance as a selection criterion). Then, after hyperparameter tuning and selection, we can increase the value of <em>k</em> to evaluate the model.</p>&#13;
<p class="indent">However, reusing the same dataset for model selection and evaluation introduces biases, and it is usually better to use a separate test set for model evaluation. Also, nested cross-validation may be preferred as an alternative to <em>k</em>-fold cross-validation.</p>&#13;
<h3 class="h3" id="ch00lev142"><strong>Exercises</strong></h3>&#13;
<p class="number1"><strong>28-1.</strong> Suppose we want to provide a model with as much training data as possible. We consider using <em>leave-one-out cross-validation (LOOCV)</em>, a special case of <em>k</em>-fold cross-validation where <em>k</em> is equal to the number of training examples, such that the validation folds contain only a single data point. A colleague mentions that LOOCV is defective for discontinuous loss functions and performance measures such as classification accuracy. For instance, for a validation fold consisting of only one example, the accuracy is always either 0 (0 percent) or 1 (99 percent). Is this really a problem?</p>&#13;
<p class="number1"><strong>28-2.</strong> This chapter discussed model selection and model evaluation as two use cases of <em>k</em>-fold cross-validation. Can you think of other use cases?</p>&#13;
<h3 class="h3" id="ch00lev143"><strong>References</strong></h3>&#13;
<ul>&#13;
<li class="noindent">For a longer and more detailed explanation of why and how to use <em>k</em>-fold cross-validation, see my article: “Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning” (2018), <em><a href="https://arxiv.org/abs/1811.12808">https://arxiv.org/abs/1811.12808</a></em>.</li>&#13;
<li class="noindent">The paper that popularized the recommendation of choosing <em>k</em> = 5 and <em>k</em> = 10: Ron Kohavi, “A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection” (1995), <em><a href="https://dl.acm.org/doi/10.5555/1643031.1643047">https://dl.acm.org/doi/10.5555/1643031.1643047</a></em>.</li>&#13;
</ul>&#13;
</div>
</div>
</body></html>