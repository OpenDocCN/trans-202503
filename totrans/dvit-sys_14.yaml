- en: '14'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '14'
- en: LEVERAGING SHARED MEMORY IN THE MULTICORE ERA
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在多核时代利用共享内存
- en: '*The world is changed.*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*世界改变了。*'
- en: '*I feel it in the silica.*'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '*我在硅中感受到它。*'
- en: '*I feel it in the transistor.*'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*我在晶体管中感受到了它。*'
- en: '*I see it in the core.*'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*我在核心中看到了它。*'
- en: '*–With apologies to Galadriel*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*——向加拉德丽尔致歉*'
- en: 'Lord of the Rings: Fellowship of the Ring'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 《指环王：护戒使者》
- en: '![image](../images/common.jpg)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/common.jpg)'
- en: Until now, our discussion of architecture has focused on a purely single-CPU
    world. But the world has changed. Today’s CPUs have multiple *cores*, or compute
    units. In this chapter, we discuss multicore architectures, and how to leverage
    them to speed up the execution of programs.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论的架构集中在纯单CPU世界中。但世界已经发生了变化。今天的CPU拥有多个*核心*或计算单元。在本章中，我们将讨论多核架构，并介绍如何利用它们加速程序的执行。
- en: '**Note CPUS, PROCESSORS, AND CORES**'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：CPU、处理器与核心**'
- en: 'In many instances in this chapter, the terms *processor* and *CPU* are used
    interchangeably. At a fundamental level, a *processor* is any circuit that performs
    some computation on external data. Based on this definition, the *central processing
    unit* (CPU) is an example of a processor. A processor or a CPU with multiple compute
    cores is referred to as a *multicore processor* or a *multicore CPU*. A *core*
    is a compute unit that contains many of the components that make up the classical
    CPU: an ALU, registers, and a bit of cache. Although a *core* is different from
    a processor, it is not unusual to see these terms used interchangeably in the
    literature (especially if the literature originated at a time when multicore processors
    were still considered novel).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的许多实例中，*处理器*和*中央处理器（CPU）*这两个术语是可以互换使用的。从根本上讲，*处理器*是执行外部数据计算的任何电路。根据这个定义，*中央处理单元（CPU）*是一个处理器的例子。具有多个计算核心的处理器或CPU被称为*多核处理器*或*多核CPU*。*核心*是一个计算单元，包含构成经典CPU的许多组件：算术逻辑单元（ALU）、寄存器和一些缓存。虽然*核心*与处理器不同，但在文献中看到这些术语互换使用并不罕见（尤其是在文献出现在多核处理器还被认为是新兴技术的时期）。
- en: 'In 1965, the founder of Intel, Gordon Moore, estimated that the number of transistors
    in an integrated circuit would double every year. His prediction, now known as
    *Moore’s Law*, was later revised to transistor counts doubling every *two* years.
    Despite the evolution of electronic switches from Bardeen’s transistor to the
    tiny chip transistors that are currently used in modern computers, Moore’s Law
    has held true for the past 50 years. However, the turn of the millennium saw processor
    design hit several critical performance walls:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 1965年，英特尔创始人戈登·摩尔估计，集成电路中的晶体管数量每年会翻一番。他的预测，现在被称为*摩尔定律*，后来被修正为晶体管数量每*两年*翻一番。尽管电子开关从巴尔登的晶体管演变为现代计算机中使用的微型芯片晶体管，摩尔定律在过去50年里依然有效。然而，千年之交，处理器设计遇到了几个关键的性能瓶颈：
- en: 'The *memory wall*: Improvements in memory technology did not keep pace with
    improvements in clock speed, resulting in memory becoming a bottleneck to performance.
    As a result, continuously speeding up the execution of a CPU no longer improves
    its overall system performance.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*内存墙*：内存技术的进步没有跟上时钟速度的提升，导致内存成为性能的瓶颈。因此，持续加速CPU的执行不再能改善其整体系统性能。'
- en: 'The *power wall*: Increasing the number of transistors on a processor necessarily
    increases that processor’s temperature and power consumption, which in turn increases
    the required cost to power and cool the system. With the proliferation of multicore
    systems, power is now the dominant concern in computer system design.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*功率墙*：增加处理器上的晶体管数量必然会提高该处理器的温度和功耗，从而增加为系统提供电力和散热的成本。随着多核系统的普及，电力已成为计算机系统设计中的主要问题。'
- en: The power and memory walls caused computer architects to change the way they
    designed processors. Instead of adding more transistors to increase the speed
    at which a CPU executes a single stream of instructions, architects began adding
    multiple *compute cores* to a CPU. Compute cores are simplified processing units
    that contain fewer transistors than traditional CPUs and are generally easier
    to create. Combining multiple cores on one CPU allows the CPU to execute *multiple*
    independent streams of instructions at once.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 电力和内存瓶颈促使计算机架构师改变了设计处理器的方式。与其增加更多晶体管来提高 CPU 执行单一指令流的速度，架构师开始在 CPU 中增加多个*计算核心*。计算核心是简化的处理单元，包含比传统
    CPU 更少的晶体管，且通常更容易制造。在一个 CPU 上组合多个核心可以让 CPU 同时执行*多个*独立的指令流。
- en: '**Warning MORE CORES != BETTER**'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**警告：更多核心 != 更好**'
- en: It may be tempting to assume that all cores are equal and that the more cores
    a computer has, the better it is. This is not necessarily the case! For example,
    *graphics processing unit* (GPU) cores have even fewer transistors than CPU cores,
    and are specialized for particular tasks involving vectors. A typical GPU can
    have 5,000 or more GPU cores. However, GPU cores are limited in the types of operations
    that they can perform and are not always suitable for general-purpose computing
    like the CPU core. Computing with GPUs is known as *manycore* computing. In this
    chapter, we concentrate on *multicore* computing. See [Chapter 15](ch15.xhtml#ch15)
    for a discussion of manycore computing.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 可能会让人产生误解，认为所有核心都是相同的，并且计算机核心越多，性能就越好。但事实并非如此！例如，*图形处理单元*（GPU）核心比 CPU 核心的晶体管还要少，并且专门用于涉及向量的特定任务。一个典型的
    GPU 可能有 5000 个或更多 GPU 核心。然而，GPU 核心在它们能够执行的操作类型上是有限制的，并且并不总是适合像 CPU 核心那样的通用计算。使用
    GPU 进行计算被称为*多核*计算。在本章中，我们集中讨论*多核*计算。有关多核计算的讨论，请参见[第15章](ch15.xhtml#ch15)。
- en: 'Taking a Closer Look: How Many Cores?'
  id: totrans-18
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 更深入的探讨：多少个核心？
- en: 'Almost all modern computer systems have multiple cores, including small devices
    like the Raspberry Pi.^([1](ch14.xhtml#fn14_1)) Identifying the number of cores
    on a system is critical for accurately measuring the performance of multicore
    programs. On Linux and macOS computers, the `lscpu` command provides a summary
    of a system’s architecture. In the following example, we show the output of the
    `lscpu` command when run on a sample machine (some output is omitted to emphasize
    the key features):'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有现代计算机系统都有多个核心，包括像树莓派这样的微型设备。^([1](ch14.xhtml#fn14_1)) 确定系统上的核心数量对于准确衡量多核程序的性能至关重要。在
    Linux 和 macOS 计算机上，`lscpu` 命令提供了系统架构的摘要。在以下示例中，我们展示了在一台样本机器上运行 `lscpu` 命令时的输出（部分输出被省略，以强调关键特性）：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `lscpu` command gives a lot of useful information, including the type of
    processors, the core speed, and the number of cores. To calculate the number of
    *physical* (or actual) cores on a system, multiply the number of sockets by the
    number of cores per socket. The sample `lscpu` output shown in the preceding example
    reveals that the system has one socket with four cores per socket, or four physical
    cores in total.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '`lscpu` 命令提供了很多有用的信息，包括处理器类型、核心速度和核心数量。要计算系统中的*物理*（或实际）核心数，需将插槽数量与每个插槽的核心数相乘。前面示例中显示的
    `lscpu` 输出表明，该系统有一个插槽，每个插槽有四个核心，总共有四个物理核心。'
- en: HYPERTHREADING
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 超线程
- en: 'At first glance, it may appear that the system in the previous example has
    eight cores in total. After all, this is what the “CPU(s)” field seems to imply.
    However, that field actually indicates the number of *hyperthreaded* (logical)
    cores, not the number of physical cores. Hyperthreading, or simultaneous multithreading
    (SMT), enables the efficient processing of multiple threads on a single core.
    Although hyperthreading can decrease the overall runtime of a program, performance
    on hyperthreaded cores does not scale at the same rate as on physical cores. However,
    if one task idles (e.g., due to a control hazard, see “Pipelining Hazards: Control
    Hazards” on [page 279](ch05.xhtml#lev2_106)), another task can still utilize the
    core. In short, hyperthreading was introduced to improve *process throughput*
    (which measures the number of processes that complete in a given unit of time)
    rather than *process speedup* (which measures the amount of runtime improvement
    of an individual process). Much of our discussion of performance in the coming
    chapter will focus on speedup.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 刚开始看，可能会觉得前面的示例系统总共有八个核心。毕竟，“CPU(s)”字段似乎暗示了这一点。然而，这个字段实际上表示的是*超线程*（逻辑）核心的数量，而不是物理核心的数量。超线程技术，或称为同时多线程（SMT），使得在单个核心上可以高效地处理多个线程。虽然超线程可以减少程序的总体运行时间，但超线程核心的性能提升并不像物理核心那样按相同比例增长。然而，如果某个任务处于空闲状态（例如，由于控制冒险，参见[第279页](ch05.xhtml#lev2_106)的“流水线冒险：控制冒险”部分），另一个任务仍然可以使用该核心。简而言之，超线程的引入是为了提高*进程吞吐量*（即在给定时间内完成的进程数量），而不是*进程加速*（即衡量单个进程的运行时间改善）。在接下来的章节中，我们将重点讨论加速。
- en: 14.1 Programming Multicore Systems
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.1 编程多核系统
- en: Most of the common languages that programmers know today were created prior
    to the multicore age. As a result, many languages cannot *implicitly* (or automatically)
    employ multicore processors to speed up the execution of a program. Instead, programmers
    must specifically write software to leverage the multiple cores on a system.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 今天大多数程序员所熟知的常见编程语言是在多核时代之前创建的。因此，许多语言无法*隐式*（或自动）利用多核处理器来加速程序的执行。相反，程序员必须专门编写软件，以利用系统中的多个核心。
- en: 14.1.1 The Impact of Multicore Systems on Process Execution
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.1.1 多核系统对进程执行的影响
- en: Recall that a process can be thought of as an abstraction of a running program
    (see “Processes” on [page 624](ch13.xhtml#lev1_100)). Each process executes in
    its own virtual address space. The operating system (OS) schedules processes for
    execution on the CPU; a *context switch* occurs when the CPU changes which process
    it currently executes.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，进程可以被视为正在运行的程序的抽象（参见[第624页](ch13.xhtml#lev1_100)的“进程”部分）。每个进程在其自己的虚拟地址空间中执行。操作系统（OS）将进程安排在CPU上执行；当CPU切换到执行其他进程时，就会发生*上下文切换*。
- en: '[Figure 14-1](ch14.xhtml#ch14fig1) illustrates how five example processes may
    execute on a single-core CPU.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[图14-1](ch14.xhtml#ch14fig1)展示了五个示例进程如何在单核CPU上执行。'
- en: '![image](../images/14fig01.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/14fig01.jpg)'
- en: '*Figure 14-1: An execution time sequence for five processes as they share a
    single CPU core*'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*图14-1：五个进程在共享单个CPU核心时的执行时间序列*'
- en: The horizontal axis is time, with each time slice taking one unit of time. A
    box indicates when a process is using the single-core CPU. Assume that each process
    executes for one full time slice before a context switch occurs. So, Process 1
    uses the CPU during time steps T1 and T3.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 水平轴表示时间，每个时间片占用一个单位时间。一个框表示进程正在使用单核CPU的时段。假设每个进程在发生上下文切换之前会执行完整的时间片。因此，进程1在时间步骤T1和T3期间使用CPU。
- en: In this example, the order of process execution is P1, P2, P1, P2, P4, P2, P3,
    P4, P5, P3, P5\. We take a moment here to distinguish between two measures of
    time. The *CPU time* measures the amount of time a process takes to execute on
    a CPU. In contrast, the *wall-clock time* measures the amount of time a human
    perceives a process takes to complete. The wall-clock time is often significantly
    longer than the CPU time, due to context switches. For example, Process 1’s CPU
    time requires two time units, whereas its wall-clock time is three time units.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，进程执行的顺序是P1、P2、P1、P2、P4、P2、P3、P4、P5、P3、P5。我们在这里花点时间区分两种时间度量。*CPU时间*衡量一个进程在CPU上执行的时间。相比之下，*实时时间*衡量一个人感知到的进程完成所需的时间。由于上下文切换的存在，实时时间通常比CPU时间长得多。例如，进程1的CPU时间需要两个时间单位，而其实时时间需要三个时间单位。
- en: When the total execution time of one process overlaps with another, the processes
    are running *concurrently* with each other. Operating systems employed concurrency
    in the single-core era to give the illusion that a computer can execute many things
    at once (e.g., you can have a calculator program, a web browser, and a word processing
    document all open at the same time). In truth, each process executes serially
    and the operating system determines the order in which processes execute and complete
    (which often differs in subsequent runs); see “Multiprogramming and Context Switching”
    on [page 625](ch13.xhtml#lev2_221).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个进程的总执行时间与另一个进程重叠时，进程是*并发*执行的。操作系统在单核时代使用并发性，给人一种计算机可以同时执行许多任务的假象（例如，你可以同时打开计算器程序、网页浏览器和文字处理文档）。实际上，每个进程是串行执行的，操作系统决定了进程执行和完成的顺序（通常在后续运行中有所不同）；参见[第625页](ch13.xhtml#lev2_221)的“多任务处理与上下文切换”。
- en: Returning to the example, observe that Process 1 and Process 2 run concurrently
    with each other, since their executions overlap at time points T2–T4\. Likewise,
    Process 2 runs concurrently with Process 4, because their executions overlap at
    time points T4–T6\. In contrast, Process 2 does *not* run concurrently with Process
    3, because they share no overlap in their execution; Process 3 only starts running
    at time T7, whereas Process 2 completes at time T6.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 回到这个例子，观察到进程1和进程2是并发执行的，因为它们的执行在T2到T4时间点重叠。同样，进程2与进程4并发执行，因为它们的执行在T4到T6时间点重叠。相反，进程2与进程3*不*并发执行，因为它们的执行没有重叠；进程3仅在T7时开始执行，而进程2在T6时完成。
- en: A multicore CPU enables the OS to schedule a different process to each available
    core, allowing processes to execute *simultaneously*. The simultaneous execution
    of instructions from processes running on multiple cores is referred to as *parallel
    execution*. [Figure 14-2](ch14.xhtml#ch14fig2) shows how our example processes
    might execute on a dual-core system.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 多核CPU使操作系统能够将不同的进程调度到每个可用的核心，从而允许进程*同时*执行。多个核心上运行的进程指令同时执行被称为*并行执行*。[图14-2](ch14.xhtml#ch14fig2)展示了我们示例中的进程如何在双核系统上执行。
- en: '![image](../images/14fig02.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/14fig02.jpg)'
- en: '*Figure 14-2: An execution time sequence for five processes, extended to include
    two CPU cores (one in dark gray, the other in light gray)*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*图14-2：五个进程的执行时间序列，扩展为包括两个CPU核心（一个为深灰色，另一个为浅灰色）*'
- en: In this example, the two CPU cores are colored differently. Suppose that the
    process execution order is again P1, P2, P1, P2, P4, P2, P3, P4, P5, P3, P5\.
    The presence of multiple cores enables certain processes to execute *sooner*.
    For example, during time unit T1, the first core executes Process 1 while the
    second core executes Process 2\. At time T2, the first core executes Process 2
    while the second executes Process 1\. Thus, Process 1 finishes executing after
    time T2, whereas Process 2 finishes executing at time T3.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，两个CPU核心的颜色不同。假设进程执行顺序再次为P1、P2、P1、P2、P4、P2、P3、P4、P5、P3、P5。多个核心的存在使得某些进程能够*更早*执行。例如，在时间单位T1时，第一个核心执行进程1，而第二个核心执行进程2。到时间T2，第一个核心执行进程2，而第二个核心执行进程1。因此，进程1在T2时间后完成执行，而进程2在T3时间完成执行。
- en: Note that the parallel execution of multiple processes increases just the number
    of processes that execute at any one time. In [Figure 14-2](ch14.xhtml#ch14fig2),
    all the processes complete execution by time unit T7\. However, each individual
    process still requires the same amount of CPU time to complete as shown in [Figure
    14-1](ch14.xhtml#ch14fig1). For example, Process 2 requires three time units regardless
    of execution on a single or multicore system (i.e., its *CPU time* remains the
    same). A multicore processor increases the *throughput* of process execution,
    or the number of processes that can complete in a given period of time. Thus,
    even though the CPU time of an individual process remains unchanged, its wall-clock
    time may decrease.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，多个进程的并行执行仅增加了在任意时刻执行的进程数量。在[图14-2](ch14.xhtml#ch14fig2)中，所有进程都在时间单位T7前完成执行。然而，每个进程仍然需要相同的CPU时间来完成，如[图14-1](ch14.xhtml#ch14fig1)所示。例如，进程2无论是在单核系统还是多核系统上执行，都需要三个时间单位（即其*CPU时间*保持不变）。多核处理器增加了进程执行的*吞吐量*，即在给定时间内可以完成的进程数量。因此，即使单个进程的CPU时间保持不变，其墙钟时间可能会减少。
- en: 14.1.2 Expediting Process Execution with Threads
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.1.2 使用线程加速进程执行
- en: One way to speed up the execution of a single process is to decompose it into
    lightweight, independent execution flows called *threads*. [Figure 14-3](ch14.xhtml#ch14fig3)
    shows how a process’s virtual address space changes when it is multithreaded with
    two threads. While each thread has its own private allocation of call stack memory,
    all threads *share* the program data, instructions, and the heap allocated to
    the multithreaded process.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 加速单个进程执行的一种方法是将其分解为轻量级、独立的执行流，称为*线程*。[图14-3](ch14.xhtml#ch14fig3)展示了当一个进程被两个线程多线程化时，其虚拟地址空间如何变化。尽管每个线程都有自己的私有调用栈内存分配，但所有线程都*共享*多线程进程的程序数据、指令和堆内存。
- en: '![image](../images/14fig03.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/14fig03.jpg)'
- en: '*Figure 14-3: Comparing the virtual address space of a single-threaded and
    a multithreaded process with two threads*'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*图14-3：比较单线程进程和有两个线程的多线程进程的虚拟地址空间*'
- en: The OS schedules threads in the same manner as it schedules processes. On a
    multicore processor, the OS can speed up the execution of a multithreaded program
    by scheduling the different threads to run on separate cores. The maximum number
    of threads that can execute in parallel is equal to the number of physical cores
    on the system. If the number of threads exceeds the number of physical cores,
    the remaining threads must wait their turn to execute (similar to how processes
    execute on a single core).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统调度线程的方式与调度进程的方式相同。在多核处理器上，操作系统通过将不同的线程调度到不同的核心上来加速多线程程序的执行。可以并行执行的最大线程数等于系统中物理核心的数量。如果线程数超过物理核心数，剩余的线程必须等待轮到它们执行（类似于进程在单核上执行的方式）。
- en: 'An Example: Scalar Multiplication'
  id: totrans-45
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例：标量乘法
- en: As an initial example of how to use multithreading to speed up an application,
    consider the problem of performing scalar multiplication of an array `array` and
    some integer `s`. In scalar multiplication, each element in the array is scaled
    by multiplying the element with `s`.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 作为如何使用多线程加速应用程序的初步示例，考虑对数组`array`和某个整数`s`进行标量乘法的问题。在标量乘法中，数组中的每个元素都通过与`s`相乘来进行缩放。
- en: 'A serial implementation of a scalar multiplication function follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 一个标量乘法函数的串行实现如下：
- en: '[PRE1]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Suppose that `array` has *N* total elements. To create a multithreaded version
    of this application with *t* threads, it is necessary to:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 假设`array`有*N*个元素。要创建一个具有*t*个线程的多线程版本应用程序，必须：
- en: 1\. Create *t* threads.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 创建*t*个线程。
- en: 2\. Assign each thread a subset of the input array (i.e., *N*/*t* elements).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 给每个线程分配输入数组的一个子集（即*N*/*t*个元素）。
- en: 3\. Instruct each thread to multiply the elements in its array subset by `s`.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 指示每个线程将其数组子集中的元素与`s`相乘。
- en: Suppose that the serial implementation of `scalar_multiply` spends 60 seconds
    multiplying an input array of 100 million elements. To build a version that executes
    with *t* = 4 threads, we assign each thread one fourth of the total input array
    (25 million elements).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 假设`scalar_multiply`的串行实现需要60秒来乘以一个包含1亿个元素的输入数组。为了构建一个使用*t* = 4个线程执行的版本，我们将总输入数组的四分之一（2500万个元素）分配给每个线程。
- en: '[Figure 14-4](ch14.xhtml#ch14fig4) shows what happens when we run four threads
    on a single core. As before, the execution order is left to the operating system.
    In this scenario, assume that the thread execution order is Thread 1, Thread 3,
    Thread 2, Thread 4\. On a single-core processor (represented by the squares),
    each thread executes sequentially. Thus, the multithreaded process running on
    one core will still take 60 seconds to run (perhaps a little longer, given the
    overhead of creating threads).'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[图14-4](ch14.xhtml#ch14fig4)显示了在单核上运行四个线程时的情况。如前所述，执行顺序由操作系统决定。在这个场景下，假设线程的执行顺序为线程1、线程3、线程2、线程4。在单核处理器（由方块表示）上，每个线程按顺序执行。因此，运行在一个核心上的多线程过程仍然需要60秒才能运行（考虑到创建线程的开销，可能稍微长一些）。'
- en: '![image](../images/14fig04.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/14fig04.jpg)'
- en: '*Figure 14-4: Running four threads on a single-core CPU*'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*图14-4：在单核CPU上运行四个线程*'
- en: Now suppose that we run our multithreaded process on a dual-core system. [Figure
    14-5](ch14.xhtml#ch14fig5) shows the result. Again, assume *t* = 4 threads, and
    that the thread execution order is Thread 1, Thread 3, Thread 2, Thread 4\. Our
    two cores are represented by shaded squares. Since the system is dual-core, Thread
    1 and Thread 3 execute in parallel during time step T1\. Threads 2 and 4 then
    execute in parallel during time step T2\. Thus, the multithreaded process that
    originally took 60 seconds to run now runs in 30 seconds.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们在双核系统上运行我们的多线程进程。[图 14-5](ch14.xhtml#ch14fig5) 显示了结果。再次假设 *t* = 4 个线程，线程执行顺序为
    线程 1、线程 3、线程 2、线程 4。我们的两个核心由阴影方块表示。由于系统是双核，线程 1 和线程 3 在时间步 T1 内并行执行。然后，线程 2 和线程
    4 在时间步 T2 内并行执行。因此，原本需要 60 秒运行的多线程进程现在只需 30 秒。
- en: '![image](../images/14fig05.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/14fig05.jpg)'
- en: '*Figure 14-5: Running four threads on a dual-core CPU*'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 14-5：在双核 CPU 上运行四个线程*'
- en: Finally, suppose that the multithreaded process (*t* = 4) is run on a quad-core
    CPU. [Figure 14-6](ch14.xhtml#ch14fig6) shows one such execution sequence. Each
    of the four cores in [Figure 14-6](ch14.xhtml#ch14fig6) is shaded differently.
    On the quad-core system, each thread executes in parallel during time slice T1\.
    Thus, on a quad-core CPU, the multithreaded process that originally took 60 seconds
    now runs in 15 seconds.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，假设多线程进程（*t* = 4）在四核 CPU 上运行。[图 14-6](ch14.xhtml#ch14fig6) 显示了这样的执行序列。[图 14-6](ch14.xhtml#ch14fig6)中的四个核心分别用不同的阴影表示。在四核系统中，每个线程在时间片
    T1 内并行执行。因此，在四核 CPU 上，原本需要 60 秒的多线程进程现在只需 15 秒。
- en: '![image](../images/14fig06.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/14fig06.jpg)'
- en: '*Figure 14-6: Running four threads on a quad-core CPU*'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 14-6：在四核 CPU 上运行四个线程*'
- en: In general, if the number of threads matches the number of cores (*c*) and the
    operating system schedules each thread to run on a separate core in parallel,
    then the multithreaded process should run in approximately 1/*c* of the time.
    Such linear speedup is ideal, but not frequently observed in practice. For example,
    if there are many other processes (or multithreaded processes) waiting to use
    the CPU, they will all compete for the limited number of cores, resulting in *resource
    contention* among the processes. If the number of specified threads exceeds the
    number of CPU cores, each thread must wait its turn to run. We explore other factors
    that often prevent linear speedup in “Measuring the Performance of Parallel Programs”
    on [page 709](ch14.xhtml#lev1_108).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，如果线程数与核心数（*c*）匹配，并且操作系统将每个线程调度到不同的核心并行执行，那么多线程进程的运行时间大约是 1/*c*。这种线性加速是理想的，但在实践中并不常见。例如，如果有很多其他进程（或多线程进程）在等待使用
    CPU，它们会争夺有限的核心数量，从而导致进程之间的*资源争用*。如果指定的线程数超过了 CPU 核心数，每个线程就必须等待轮到它执行。我们将在“[页面 709](ch14.xhtml#lev1_108)”的“测量并行程序性能”一节中探讨其他经常阻碍线性加速的因素。
- en: 14.2 Hello Threading! Writing Your First Multithreaded Program
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.2 你好，线程！编写你的第一个多线程程序
- en: In this section, we examine the ubiquitous POSIX thread library *Pthreads*.
    POSIX is an acronym for Portable Operating System Interface. It is an IEEE standard
    that specifies how UNIX systems look, act, and feel. The POSIX threads API is
    available on almost all UNIX-like operating systems, each of which meets the standard
    in its entirety or to some great degree. So, if you write parallel code using
    POSIX threads on a Linux machine, it will certainly work on other Linux machines,
    and it will likely work on machines running macOS or other UNIX variants.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 本节我们将讨论无处不在的 POSIX 线程库 *Pthreads*。POSIX 是可移植操作系统接口的缩写，它是一个 IEEE 标准，规定了 UNIX
    系统的外观、行为和感觉。POSIX 线程 API 几乎可以在所有类 UNIX 操作系统上使用，每个系统都完全或在很大程度上遵循该标准。因此，如果你在 Linux
    机器上使用 POSIX 线程编写并行代码，它肯定可以在其他 Linux 机器上运行，并且很可能也能在运行 macOS 或其他 UNIX 变种的机器上运行。
- en: Let’s begin by analyzing an example “Hello World” Pthreads program.^([2](ch14.xhtml#fn14_2))
    For brevity, we have excluded error handling in the listing, though the downloadable
    version contains sample error handling.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从分析一个“Hello World”Pthreads 程序开始。^([2](ch14.xhtml#fn14_2)) 为了简洁，我们在列表中省略了错误处理，尽管可下载版本包含了示例错误处理。
- en: '[PRE2]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Let’s examine this program in smaller components. Notice the inclusion of the
    `pthread.h` header file, which declares `pthread` types and functions. Next, the
    `HelloWorld` function defines the *thread function* that we later pass to `pthread_create`.
    A thread function is analogous to a `main` function for a worker (created) thread—a
    thread begins execution at the start of its thread function and terminates when
    it reaches the end. Each thread executes the thread function using its private
    execution state (i.e., its own stack memory and register values). Note also that
    the thread function is of type `void *`. Specifying an *anonymous pointer* in
    this context allows programmers to write thread functions that deal with arguments
    and return values of different types (see “The void * Type and Type Recasting”
    on [page 222](ch02.xhtml#lev2_38)). Lastly, in the `main` function, the main thread
    initializes the program state before creating and joining the worker threads.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这个程序分解成更小的组件。注意包括了`pthread.h`头文件，它声明了`pthread`类型和函数。接下来，`HelloWorld`函数定义了*线程函数*，我们稍后将其传递给`pthread_create`。线程函数类似于工作线程（创建线程）的`main`函数——线程从线程函数的开始处开始执行，直到到达末尾时终止。每个线程使用其私有的执行状态（即它自己的堆栈内存和寄存器值）执行线程函数。还需注意，线程函数的类型为`void
    *`。在此上下文中指定*匿名指针*，允许程序员编写可以处理不同类型参数和返回值的线程函数（参见[第222页](ch02.xhtml#lev2_38)的《void
    *类型和类型重casting》）。最后，在`main`函数中，主线程初始化程序状态后，创建并连接工作线程。
- en: 14.2.1 Creating and Joining Threads
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.2.1 创建和连接线程
- en: 'The program first starts as a single-threaded process. As it executes the `main`
    function, it reads the number of threads to create, and it allocates memory for
    two arrays: `thread_array` and `thread_ids`. The `thread_array` array contains
    the set of addresses for each thread created. The `thread_ids` array stores the
    set of arguments that each thread is passed. In this example, each thread is passed
    the address of its rank (or ID, represented by `thread_ids[i]`).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 该程序首先作为单线程进程启动。在执行`main`函数时，它读取要创建的线程数，并为两个数组分配内存：`thread_array`和`thread_ids`。`thread_array`数组包含为每个创建的线程分配的地址集合。`thread_ids`数组存储传递给每个线程的参数集合。在此示例中，每个线程都会传递它的等级（或ID，表示为`thread_ids[i]`）的地址。
- en: 'After all the preliminary variables are allocated and initialized, the `main`
    thread executes the two major steps of multithreading:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有初始变量分配和初始化之后，`main`线程执行多线程的两个主要步骤：
- en: The *creation* step, in which the main thread spawns one or more worker threads.
    After being spawned, each worker thread runs within its own execution context
    concurrently with the other threads and processes on the system.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*创建*步骤，其中主线程生成一个或多个工作线程。每个工作线程在生成后，在其自己的执行上下文中并发运行，与系统中的其他线程和进程共同执行。'
- en: The *join* step, in which the main thread waits for all the workers to complete
    before proceeding as a single-thread process. Joining a thread that has terminated
    frees the thread’s execution context and resources. Attempting to join a thread
    that *hasn’t* terminated blocks the caller until the thread terminates, similar
    to the semantics of the `wait` function for processes (see “exit and wait” on
    [page 635](ch13.xhtml#lev2_226)).
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*连接*步骤，其中主线程等待所有工作线程完成，然后继续作为单线程进程。连接一个已终止的线程会释放该线程的执行上下文和资源。尝试连接一个*未*终止的线程会阻塞调用者，直到该线程终止，类似于进程的`wait`函数的语义（参见[第635页](ch13.xhtml#lev2_226)的《exit和wait》）。'
- en: 'The Pthreads library offers a `pthread_create` function for creating threads
    and a `pthread_join` function for joining them. The `pthread_create` function
    has the following signature:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Pthreads库提供了`pthread_create`函数用于创建线程，`pthread_join`函数用于连接线程。`pthread_create`函数的签名如下：
- en: '[PRE3]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The function takes a pointer to a thread struct (of type `pthread_t`), a pointer
    to an attribute struct (normally set to `NULL`), the name of the function the
    thread should execute, and the array of arguments to pass to the thread function
    when it starts.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数接受一个指向线程结构体（类型为`pthread_t`）的指针，一个指向属性结构体的指针（通常设置为`NULL`），线程应执行的函数名称，以及启动时传递给线程函数的参数数组。
- en: 'The Hello World program calls `pthread_create` in the `main` function using:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Hello World程序在`main`函数中使用`pthread_create`调用：
- en: '[PRE4]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这里：
- en: '`&thread_array[i]` contains the address of thread *i*. The `pthread_create`
    function allocates a `pthread_t` thread object and stores its address at this
    location, enabling the programmer to reference the thread later (e.g., when joining
    it).'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`&thread_array[i]` 包含线程 *i* 的地址。`pthread_create` 函数分配一个 `pthread_t` 线程对象，并将其地址存储在此位置，使程序员能够稍后引用该线程（例如，在合并时）。'
- en: '`NULL` specifies that the thread should be created with default attributes.
    In most programs, it is safe to leave this second parameter as `NULL`.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NULL` 表示线程应该使用默认属性创建。在大多数程序中，将这个第二个参数留空为 `NULL` 是安全的。'
- en: '`HelloWorld` names the thread function that the created thread should execute.
    This function behaves like the “main” function for the thread. For an arbitrary
    thread function (e.g., `function`), its prototype must match the form `void *
    function(void *)`.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`HelloWorld` 是创建的线程应该执行的线程函数的名称。这个函数就像线程的“主”函数。对于一个任意线程函数（例如，`function`），它的原型必须匹配
    `void * function(void *)` 这种形式。'
- en: '`&thread_ids[i]` specifies the address of the arguments to be passed to thread
    *i*. In this case, `thread_ids[i]` contains a single `long` representing the thread’s
    ID. Since the last argument to `pthread_create` must be a pointer, we pass the
    *address* of the thread’s ID.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`&thread_ids[i]` 指定要传递给线程 *i* 的参数的地址。在这种情况下，`thread_ids[i]` 包含一个表示线程 ID 的单个
    `long` 类型的值。由于 `pthread_create` 的最后一个参数必须是一个指针，因此我们传递线程 ID 的 *地址*。'
- en: 'To generate several threads that execute the `HelloWorld` thread function,
    the program assigns each thread a unique ID and creates each thread within a `for`
    loop:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成多个执行 `HelloWorld` 线程函数的线程，程序为每个线程分配一个唯一的 ID，并在 `for` 循环中创建每个线程：
- en: '[PRE5]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The OS schedules the execution of each created thread; the user cannot make
    any assumption on the order in which the threads will execute.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统调度每个创建的线程的执行；用户无法对线程执行的顺序做出任何假设。
- en: 'The `pthread_join` function suspends the execution of its caller until the
    thread it references terminates. Its signature is:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`pthread_join` 函数会挂起调用者的执行，直到它引用的线程终止。其函数签名为：'
- en: '[PRE6]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `pthread_join` takes as input a `pthread_t` struct, indicating which thread
    to wait on, and an optional pointer argument that specifies where the thread’s
    return value should be stored.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`pthread_join` 以 `pthread_t` 结构体为输入，指示要等待的线程，并且有一个可选的指针参数，指定线程的返回值应存储的位置。'
- en: 'The Hello World program calls `pthread_join` in `main` using:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`Hello World` 程序在 `main` 中使用以下代码调用 `pthread_join`：'
- en: '[PRE7]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This line indicates that the main thread must wait on the termination of thread
    `t`. Passing `NULL` as the second argument indicates that the program does not
    use the thread’s return value.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这一行表示主线程必须等待线程 `t` 的终止。将 `NULL` 作为第二个参数传递表示程序不使用线程的返回值。
- en: 'In the previous program, `main` calls `pthread_join` in a loop because *all*
    of the worker threads need to terminate before the `main` function proceeds to
    clean up memory and terminate the process:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的程序中，`main` 在循环中调用 `pthread_join`，因为 *所有* 工作线程需要在 `main` 函数继续清理内存并终止进程之前终止：
- en: '[PRE8]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 14.2.2 The Thread Function
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.2.2 线程函数
- en: 'In the previous program, each spawned thread prints out `Hello world! I am
    thread n`, where `n` is the thread’s unique ID. After the thread prints out its
    message, it terminates. Let’s take a closer look at the `HelloWorld` function:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的程序中，每个生成的线程都会打印 `Hello world! I am thread n`，其中 `n` 是线程的唯一标识符。在线程打印出消息后，它会终止。让我们更仔细地看看
    `HelloWorld` 函数：
- en: '[PRE9]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Recall that `pthread_create` passes the arguments to the thread function using
    the `thread_args` parameter. In the `pthread_create` function in `main`, the Hello
    World program specified that this parameter is in fact the thread’s ID. Note that
    the parameter to `HelloWorld` must be declared as a generic or anonymous pointer
    (`void *`) (see “The void * Type and Type Recasting” on [page 126](ch02.xhtml#lev2_38)).
    The Pthreads library uses `void *` to make `pthread_create` more general purpose
    by not prescribing a parameter type. As a programmer, the `void *` is mildly inconvenient
    given that it must be recast before use. Here, we *know* the parameter is of type
    `long *` because that’s what we passed to `pthread_create` in `main`. Thus, we
    can safely cast the value as a `long *` and dereference the pointer to access
    the `long` value. Many parallel programs follow this structure.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，`pthread_create`通过`thread_args`参数将参数传递给线程函数。在`main`中的`pthread_create`函数中，Hello
    World程序指定该参数实际上是线程的ID。注意，`HelloWorld`的参数必须声明为通用或匿名指针（`void *`）（参见[第126页](ch02.xhtml#lev2_38)的《void
    *类型与类型重 casting》）。Pthreads库使用`void *`使得`pthread_create`更加通用，不对参数类型进行限制。作为程序员，使用`void
    *`有些不方便，因为在使用之前必须进行类型转换。在这里，我们*知道*该参数是`long *`类型，因为我们在`main`中传递给`pthread_create`的就是这个类型。因此，我们可以安全地将其转换为`long
    *`并解引用指针以访问`long`值。许多并行程序遵循这种结构。
- en: 'Similar to the thread function’s parameter, the Pthreads library avoids prescribing
    the thread function’s return type by specifying another `void *`: the programmer
    is free to return any pointer from the thread function. If the program needs to
    access the thread’s return value, it can retrieve it via the second argument to
    `pthread_join`. In our example, the thread has no need to return a value, so it
    simply returns a `NULL` pointer.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 与线程函数的参数类似，Pthreads库通过指定另一个`void *`来避免规定线程函数的返回类型：程序员可以自由地从线程函数中返回任何指针。如果程序需要访问线程的返回值，它可以通过`pthread_join`的第二个参数来获取。在我们的示例中，线程不需要返回值，因此它只是返回一个`NULL`指针。
- en: 14.2.3 Running the Code
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.2.3 运行代码
- en: 'The command that follows shows how to use GCC to compile the program. Building
    a Pthreads application requires that the `-lpthread` linker flag be passed to
    GCC to ensure that the Pthreads functions and types are accessible:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的命令展示了如何使用GCC编译程序。构建一个Pthreads应用程序需要将`-lpthread`链接器标志传递给GCC，以确保Pthreads函数和类型是可访问的：
- en: '[PRE10]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Running the program without a command line argument results in a usage message:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有命令行参数的情况下运行程序会显示用法信息：
- en: '[PRE11]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Running the program with four threads yields the following output:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 使用四个线程运行程序会产生以下输出：
- en: '[PRE12]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Notice that each thread prints its unique ID number. In this run, thread 1’s
    output displays first, followed by threads 2, 3, and 0\. If we run the program
    again, we may see the output displayed in a different order:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，每个线程都会打印其唯一的ID号码。在此次运行中，线程1的输出最先显示，其次是线程2、3和0。如果我们再次运行程序，可能会看到输出的顺序发生变化：
- en: '[PRE13]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Recall that the operating system’s scheduler determines the thread execution
    order. From a user’s perspective, the order is *effectively random* due to being
    influenced by many factors that vary outside the user’s control (e.g., available
    system resources, the system receiving input, or OS scheduling). Since all threads
    are running concurrently with one another and each thread executes a call to `printf`
    (which prints to `stdout`), the first thread that prints to `stdout` will have
    its output show up first. Subsequent executions may (or may not) result in different
    output.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，操作系统的调度器决定了线程的执行顺序。从用户的角度来看，由于受到许多用户无法控制的因素（例如可用系统资源、系统接收输入或操作系统调度等）的影响，顺序是*实际上是随机的*。由于所有线程都是并发执行的，并且每个线程都会调用`printf`（它会打印到`stdout`），因此，第一个打印到`stdout`的线程的输出将首先显示。后续的执行可能（或可能不会）导致不同的输出。
- en: '**Warning THREAD EXECUTION ORDER**'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**警告 线程执行顺序**'
- en: You should *never* make any assumptions about the order in which threads will
    execute. If the correctness of your program requires that threads run in a particular
    order, you must add synchronization (see “Synchronizing Threads” on [page 686](ch14.xhtml#lev1_107))
    to your program to prevent threads from running when they shouldn’t.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 你*永远*不要对线程执行的顺序做出任何假设。如果程序的正确性要求线程按特定顺序执行，你必须为程序添加同步机制（参见[第686页](ch14.xhtml#lev1_107)的《线程同步》），以防止线程在不该运行的时候执行。
- en: 14.2.4 Revisiting Scalar Multiplication
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.2.4 再次探讨标量乘法
- en: 'Let’s explore how to create a multithreaded implementation of the scalar multiplication
    program from “An Example: Scalar Multiplication” on [page 675](ch14.xhtml#lev3_113).
    Recall that our general strategy for parallelizing `scalar_multiply` is to create
    multiple threads, assign each thread a subset of the input array, and instruct
    each thread to multiply the elements in its array subset by `s`.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索如何从“示例：标量乘法”中创建一个多线程实现程序，见[第675页](ch14.xhtml#lev3_113)。回顾我们并行化`scalar_multiply`的一般策略是创建多个线程，分配每个线程一个输入数组的子集，并指示每个线程将其数组子集中的元素与`s`相乘。
- en: The following is a thread function that accomplishes this task. Notice that
    we have moved `array`, `length`, and `s` to the global scope of the program.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个线程函数，它完成了这个任务。注意，我们已将`array`、`length`和`s`移到程序的全局范围。
- en: '[PRE14]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let’s break this down into parts. Recall that the first step is to assign each
    thread a component of the array. The following lines accomplish this task:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这部分内容分解成几个步骤。回想一下，第一步是为每个线程分配数组的一个部分。以下几行代码实现了这一任务：
- en: '[PRE15]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The variable `chunk` stores the number of elements that each thread is assigned.
    To ensure that each thread gets roughly the same amount of work, we first set
    the chunk size to the number of elements divided by the number of threads, or
    `length / nthreads`.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 变量`chunk`存储每个线程分配的元素数量。为了确保每个线程的工作量大致相同，我们首先将块大小设置为元素数量除以线程数量，即`length / nthreads`。
- en: Next, we assign each thread a distinct range of elements to process. Each thread
    computes its range’s `start` and `end` index using the `chunk` size and its unique
    thread ID.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们为每个线程分配一个不同的元素范围进行处理。每个线程使用`chunk`大小和其唯一的线程ID来计算其范围的`start`和`end`索引。
- en: For example, with four threads (with IDs 0–3) operating over an array with 100
    million elements, each thread is responsible for processing a 25 million element
    `chunk`. Incorporating the thread ID assigns each thread a unique subset of the
    input.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，使用四个线程（ID为0到3）操作一个包含1亿个元素的数组，每个线程负责处理一个2500万个元素的`chunk`。通过引入线程ID，将为每个线程分配输入数据的唯一子集。
- en: 'The next two lines account for the case in which `length` is not evenly divisible
    by the number of threads:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的两行代码考虑了`length`不能被线程数量整除的情况：
- en: '[PRE16]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Suppose that we specified three rather than four threads. The nominal chunk
    size would be 33,333,333 elements, leaving one element unaccounted for. The code
    in the previous example would assign the remaining element to the last thread.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们指定了三个线程而不是四个线程。名义上的块大小将是33,333,333个元素，剩下一个元素没有被分配。前面的示例代码会将剩余的元素分配给最后一个线程。
- en: '**Note CREATING BALANCED INPUT**'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意 创建平衡的输入**'
- en: The chunking code just shown is imperfect. In the case where the number of threads
    does not evenly divide the input, the remainder is assigned to the last thread.
    Consider a sample run in which the array has 100 elements, and 12 threads are
    specified. The nominal chunk size would be 8, and the remainder would be 4\. With
    the example code, the first 11 threads will each have 8 assigned elements, whereas
    the last thread will be assigned 12 elements. Consequently, the last thread performs
    50% more work than the other threads. A potentially better way to chunk this example
    is to have the first 4 threads process 9 elements each, whereas the last 8 threads
    process 8 elements each. This will result in better *load balancing* of the input
    across the threads.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 上面展示的分块代码并不完美。在线程数量无法整除输入数据的情况下，余数将分配给最后一个线程。考虑一个样例运行，其中数组有100个元素，指定了12个线程。名义上的块大小是8，余数是4。使用示例代码时，前11个线程每个线程将分配8个元素，而最后一个线程将分配12个元素。因此，最后一个线程的工作量是其他线程的50%。一种可能更好的分块方法是让前4个线程处理9个元素，而最后8个线程处理8个元素。这将有助于更好的*负载均衡*，使得各线程处理的输入量更加均衡。
- en: 'With an appropriate local `start` and `end` index computed, each thread is
    now ready to perform scalar multiplication on its component of the array. The
    last portion of the `scalar_multiply` function accomplishes this:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算出合适的本地`start`和`end`索引后，每个线程现在准备在其数组部分上执行标量乘法操作。`scalar_multiply`函数的最后一部分完成了这一任务：
- en: '[PRE17]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '14.2.5 Improving Scalar Multiplication: Multiple Arguments'
  id: totrans-128
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.2.5 改进标量乘法：多个参数
- en: 'A key weakness of the previous implementation is the wide use of global variables.
    Our original discussion in “Parts of Program Memory and Scope” on [page 64](ch02.xhtml#lev1_9)
    showed that, although useful, global variables should generally be avoided in
    C. To reduce the number of global variables in the program, one solution is to
    declare a `t_arg` struct as follows in the global scope:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个实现的一个关键弱点是广泛使用了全局变量。在我们在《程序内存的部分和作用域》中讨论过的内容中，虽然全局变量有用，但一般应该避免在C语言中使用全局变量。为了减少程序中的全局变量数量，一种解决方案是在全局作用域中声明一个`t_arg`结构体，如下所示：
- en: '[PRE18]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Our main function would, in addition to allocating `array` and setting local
    variables `length`, `nthreads`, and `s` (our scaling factor), allocate an array
    of `t_arg` records:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主函数除了分配`array`并设置局部变量`length`、`nthreads`和`s`（缩放因子）外，还会分配一个`t_arg`记录的数组：
- en: '[PRE19]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Later in `main`, when `pthread_create` is called, the thread’s associated `t_args`
    struct is passed as an argument:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 稍后在`main`中，当调用`pthread_create`时，线程相关的`t_args`结构体作为参数传递：
- en: '[PRE20]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Lastly, our `scalar_multiply` function would look like the following:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们的`scalar_multiply`函数将如下所示：
- en: '[PRE21]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Implementing this program fully is an exercise we leave to the reader. Please
    note that error handling has been omitted for the sake of brevity.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 完整实现此程序是留给读者的练习。请注意，出于简洁考虑，错误处理被省略了。
- en: 14.3 Synchronizing Threads
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.3 线程同步
- en: In the examples we’ve looked at thus far, each thread executes without sharing
    data with any other threads. In the scalar multiplication program, for instance,
    each element of the array is entirely independent of all the others, making it
    unnecessary for the threads to share data.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们到目前为止所看到的示例中，每个线程执行时并不与其他线程共享数据。例如，在标量乘法程序中，数组的每个元素完全独立于其他元素，因此线程之间不需要共享数据。
- en: However, a thread’s ability to easily share data with other threads is one of
    its main features. Recall that all the threads of a multithreaded process share
    the heap common to the process. In this section, we study the data sharing and
    protection mechanisms available to threads in detail.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，线程能够轻松与其他线程共享数据是其主要特性之一。回想一下，多线程进程中的所有线程共享进程的堆。在本节中，我们将详细研究线程的数据共享和保护机制。
- en: '*Thread synchronization* refers to forcing threads to execute in a particular
    order. Even though synchronizing threads can add to the runtime of a program,
    it is often necessary to ensure program correctness. In this section, we primarily
    discuss how one synchronization construct (a *mutex*) helps ensure the correctness
    of a threaded program. We conclude the section with a discussion of some other
    common synchronization constructs: *semaphores*, *barriers*, and *condition variables*.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '*线程同步*指的是强制线程按照特定顺序执行。尽管同步线程可能会增加程序的运行时间，但它通常是确保程序正确性所必需的。本节我们主要讨论一个同步构造（*互斥锁*）如何帮助确保线程程序的正确性。我们将以*信号量*、*屏障*和*条件变量*等其他常见同步构造作为结尾进行讨论。'
- en: CountSort
  id: totrans-142
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: CountSort
- en: 'Let’s study a slightly more complicated example called CountSort. The CountSort
    algorithm is a simple linear (O(*N*)) sorting algorithm for sorting a known small
    range of *R* values, where *R* is much smaller than *N*. To illustrate how CountSort
    works, consider an array `A` of 15 elements, all of which contain random values
    between 0 and 9 (10 possible values):'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们研究一个稍微复杂的例子，叫做CountSort。CountSort算法是一个简单的线性（O(*N*)）排序算法，用于排序已知的小范围*R*值，其中*R*远小于*N*。为了说明CountSort是如何工作的，考虑一个包含15个元素的数组`A`，其中所有元素的值在0到9之间（10个可能的值）：
- en: '[PRE22]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'For a particular array, CountSort works as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个特定的数组，CountSort的工作原理如下：
- en: 1\. It counts the frequency of each value in the array.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 它统计数组中每个值的频率。
- en: 2\. It overwrites the original array by enumerating each value by its frequency.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 它通过按频率枚举每个值，覆盖原始数组。
- en: After step 1, the frequency of each value is placed in a `counts` array of length
    10, where the value of `counts[i]` is the frequency of the value *i* in array
    `A`. For example, since there are three elements with value 2 in array `A`, `counts[2]`
    is 3.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在第1步之后，每个值的频率被放入一个长度为10的`counts`数组中，其中`counts[i]`的值是数组`A`中值*i*的频率。例如，由于数组`A`中有三个值为2的元素，`counts[2]`为3。
- en: 'The corresponding `counts` array for the previous example looks like the following:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 对于前一个示例，相应的`counts`数组如下所示：
- en: '[PRE23]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Note that the sum of all the elements in the `counts` array is equal to the
    length of `A`, or 15.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`counts`数组中所有元素的总和等于`A`的长度，即15。
- en: Step 2 uses the `counts` array to overwrite `A`, using the frequency counts
    to determine the set of indices in `A` that store each consecutive value in sorted
    order. So, since the `counts` array indicates that there are three elements with
    value 0 and two elements with value 1 in array `A`, the first three elements of
    the final array will be 0, and the next two will be 1.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤2使用`counts`数组来覆盖`A`，利用频率计数来确定`A`中存储每个连续值的索引集合。因此，由于`counts`数组表明数组`A`中有三个值为0的元素和两个值为1的元素，最终数组的前三个元素将是0，接下来的两个元素将是1。
- en: 'After running step 2, the final array looks like the following:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 执行步骤2后，最终数组如下所示：
- en: '[PRE24]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Following is a serial implementation of the CountSort algorithm, with the `count`
    (step 1) and `overwrite` (step 2) functions clearly delineated. For brevity, we
    do not reproduce the whole program here, though you can download the source.^([3](ch14.xhtml#fn14_3))
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是CountSort算法的串行实现，其中`count`（步骤1）和`overwrite`（步骤2）函数清晰地划分。为了简洁起见，我们在此未展示完整程序，您可以下载源代码。^([3](ch14.xhtml#fn14_3))
- en: '[PRE25]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Running this program on an array of size 15 yields the following output:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在大小为15的数组上运行此程序会产生以下输出：
- en: '[PRE26]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The second parameter to this program is a *verbose* flag, which indicates whether
    the program prints output. This is a useful option for larger arrays for which
    we may want to run the program but not necessarily print out the output.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 该程序的第二个参数是一个*verbose*标志，指示程序是否打印输出。这对于较大的数组非常有用，我们可能希望运行程序，但不一定需要打印输出结果。
- en: 'Parallelizing countElems: An Initial Attempt'
  id: totrans-160
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 并行化countElems：初步尝试
- en: CountSort consists of two primary steps, each of which benefits from being parallelized.
    In the remainder of the chapter, we primarily concentrate on the parallelization
    of step 1, or the `countElems` function. Parallelizing the `writeArray` function
    is left as an exercise for the reader.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: CountSort包含两个主要步骤，每个步骤都能通过并行化提高效率。在本章的其余部分，我们主要集中在步骤1的并行化，或`countElems`函数的并行化。至于并行化`writeArray`函数，留给读者作为练习。
- en: 'The code block that follows depicts a first attempt at creating a threaded
    `countElems` function. Parts of the code (argument parsing, error handling) are
    omitted in this example for the sake of brevity, but the full source can be downloaded.^([4](ch14.xhtml#fn14_4))
    In the code that follows, each thread attempts to count the frequency of the array
    elements in its assigned component of the global array and updates a global count
    array with the discovered counts:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的代码块展示了创建线程化`countElems`函数的首次尝试。为了简洁起见，部分代码（如参数解析、错误处理）在这个示例中被省略，但完整源代码可以下载。^([4](ch14.xhtml#fn14_4))
    在接下来的代码中，每个线程尝试计算其分配的全局数组组件中元素的频率，并用发现的计数更新全局计数数组：
- en: '[PRE27]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The `main` function looks nearly identical to our earlier sample programs:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '`main`函数看起来几乎与我们早期的示例程序相同：'
- en: '[PRE28]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: For reproducibility purposes, the random number generator is seeded with a static
    value (10) to ensure that `array` (and therefore `counts`) always contains the
    same set of numbers. An additional function (`printCounts`) prints out the contents
    of the global `counts` array. The expectation is that, regardless of the number
    of threads used, the contents of the `counts` array should always be the same.
    For brevity, error handling has been removed from the listing.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可重复性，随机数生成器使用静态值（10）进行种子初始化，以确保`array`（因此也包括`counts`）总是包含相同的数字集合。另一个函数（`printCounts`）会打印出全局`counts`数组的内容。预期无论使用多少线程，`counts`数组的内容应该始终相同。为了简洁起见，错误处理已从代码中移除。
- en: 'Compiling the program and running it with one, two, and four threads over 10
    million elements produces the following:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 编译程序并分别使用一个、两个和四个线程处理1000万个元素时，产生了以下结果：
- en: '[PRE29]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Note that the printed results change significantly on each run. In particular,
    they seem to change as we vary the number of threads! This should not happen,
    since our use of the static seed guarantees the same set of numbers every run.
    These results contradict one of the cardinal rules for threaded programs: the
    output of a program should be correct and consistent *regardless* of the number
    of threads used.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，打印的结果在每次运行时有显著变化。特别是，当我们改变线程数时，结果似乎发生了变化！这不应该发生，因为我们使用静态种子确保每次运行都产生相同的数字集合。这些结果违背了多线程程序的一个基本规则：程序的输出应该是正确且一致的，无论使用多少线程。
- en: Since our first attempt at parallelizing `countElems` doesn’t seem to be working,
    let’s delve deeper into what this program is doing and examine how we might fix
    it.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们第一次尝试并行化 `countElems` 似乎不起作用，让我们深入了解这个程序在做什么，并检查如何修复它。
- en: Data Races
  id: totrans-171
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据竞争
- en: 'To understand what’s going on, let’s consider an example run with two threads
    on two separate cores of a multicore system. Recall that the execution of any
    thread can be preempted at any time by the OS, which means that each thread could
    be running different instructions of a particular function at any given time (or
    possibly the same instruction). [Table 14-1](ch14.xhtml#ch14tab1) shows one possible
    path of execution through the `countElems` function. To better illustrate what
    is going on, we translated the line `counts[val] = counts[val] + 1` into the following
    sequence of equivalent instructions:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解发生了什么，让我们考虑一个在多核系统中两个线程在两个不同核心上运行的示例。请记住，任何线程的执行都可能随时被操作系统抢占，这意味着每个线程可能在任何时刻执行特定函数的不同指令（或可能是相同的指令）。[表
    14-1](ch14.xhtml#ch14tab1)展示了 `countElems` 函数执行的一个可能路径。为了更好地说明发生了什么，我们将 `counts[val]
    = counts[val] + 1` 这一行翻译成以下等效指令的顺序：
- en: 1\. *Read* `counts[val]` and place into a register.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. *读取* `counts[val]` 并放入寄存器。
- en: 2\. *Modify* the register by incrementing it by one.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. *修改* 寄存器，通过将其加 1。
- en: 3\. *Write* the contents of the register to `counts[val]`.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. *写入* 寄存器的内容到 `counts[val]`。
- en: This is known as the *read–modify–write* pattern. In the example shown in [Table
    14-1](ch14.xhtml#ch14tab1), each thread executes on a separate core (Thread 0
    on Core 0, Thread 1 on Core 1). We start inspecting the execution of the process
    at time step *i*, where both threads have a `val` of 1.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为 *读取-修改-写入* 模式。在[表 14-1](ch14.xhtml#ch14tab1)中所示的例子中，每个线程在不同的核心上执行（线程 0
    在核心 0 上，线程 1 在核心 1 上）。我们从时间步 *i* 开始检查进程的执行，在该时刻，两个线程的 `val` 都是 1。
- en: '**Table 14-1:** A Possible Execution Sequence of Two Threads Running `countElems`'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 14-1：** 两个线程运行 `countElems` 的可能执行顺序'
- en: '| **Time** | **Thread 0** | **Thread 1** |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| **时间** | **线程 0** | **线程 1** |'
- en: '| *i* | Read `counts[1]` and place into Core 0’s register | … |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| *i* | 读取 `counts[1]` 并放入 Core 0 的寄存器 | … |'
- en: '| *i* + 1 | Increment register by 1 | Read `counts[1]` and place into Core
    1’s register |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| *i* + 1 | 将寄存器值加 1 | 读取 `counts[1]` 并放入 Core 1 的寄存器 |'
- en: '| *i* + 2 | Overwrite `counts[1]` with contents of register | Increment register
    by 1 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| *i* + 2 | 使用寄存器内容覆盖 `counts[1]` | 将寄存器值加 1 |'
- en: '| *i* + 3 | … | Overwrite `counts[1]` with contents of register |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| *i* + 3 | … | 使用寄存器内容覆盖 `counts[1]` |'
- en: Suppose that, prior to the execution sequence in [Table 14-1](ch14.xhtml#ch14tab1),
    `counts[1]` contains the value 60\. In time step *i*, Thread 0 reads `counts[1]`
    and places the value 60 in Core 0’s register. In time step *i* + 1, while Thread
    0 increments Core 0’s register by one, the *current* value in `counts[1]` (60)
    is read into Core 1’s register by Thread 1\. In time step *i* + 2, Thread 0 updates
    `counts[1]` with the value 61 while Thread 1 increments the value stored in its
    local register (60) by one. The end result is that during time step *i* + 3, the
    value `counts[1]` is overwritten by Thread 1 with the value 61, not 62 as we would
    expect! This causes `counts[1]` to essentially “lose” an increment!
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 假设在[表 14-1](ch14.xhtml#ch14tab1)中展示的执行顺序之前，`counts[1]` 包含值 60。在时间步 *i* 时，线程
    0 读取 `counts[1]` 并将值 60 放入 Core 0 的寄存器。在时间步 *i* + 1 时，当线程 0 将 Core 0 的寄存器加 1 时，`counts[1]`
    中的 *当前* 值（60）被线程 1 读取并放入 Core 1 的寄存器。在时间步 *i* + 2 时，线程 0 用值 61 更新 `counts[1]`，而线程
    1 将存储在其本地寄存器中的值（60）加 1。最终结果是，在时间步 *i* + 3 时，`counts[1]` 的值被线程 1 用值 61 覆盖，而不是我们预期的
    62！这导致 `counts[1]` 实际上“丢失”了一个增量！
- en: We refer to the scenario in which two threads attempt to write to the same location
    in memory as a *data race* condition. More generally, a *race condition* refers
    to any scenario in which the simultaneous execution of two operations gives an
    incorrect result. Note that a simultaneous read of the `counts[1]` location would
    *not* in and of itself constitute a race condition, because values can generally
    read alone from memory without issue. It was the combination of this step with
    the writes to `counts[1]` that caused the incorrect result. This read–modify–write
    pattern is a common source of a particular type of race condition, called a *data
    race*, in most threaded programs. In our discussion of race conditions and how
    to fix them, we focus on data races.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将两个线程试图写入内存同一位置的情形称为*数据竞争*条件。更一般地，*竞争条件*是指任何两个操作的同时执行导致错误结果的情形。请注意，`counts[1]`
    位置的同时读取本身并不构成竞争条件，因为通常可以无问题地独立从内存读取值。正是这一步与对 `counts[1]` 的写操作的结合，导致了错误的结果。读取–修改–写入模式是大多数多线程程序中引发特定类型竞争条件（即*数据竞争*）的常见来源。在我们讨论竞争条件及其解决方法时，重点关注数据竞争。
- en: '**Note ATOMIC OPERATIONS**'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意 原子操作**'
- en: An operation is defined as being *atomic* if a thread perceives it as executing
    without interruption (in other words, as an “all or nothing” action). In some
    libraries, a keyword or type is used to specify that a block of computation should
    be treated as being atomic. In the previous example, the line `counts[val] = counts[val]
    + 1` (even if written as `counts[val]++`) is *not* atomic, because this line actually
    corresponds to several instructions at the machine level. A synchronization construct
    like mutual exclusion is needed to ensure that there are no data races. In general,
    all operations should be assumed to be nonatomic unless mutual exclusion is explicitly
    enforced.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个操作被定义为*原子操作*，那么线程会认为它是没有中断地执行的（换句话说，它是一个“全有或全无”的动作）。在某些库中，使用关键字或类型来指定某个计算块应该被视为原子操作。在前面的例子中，`counts[val]
    = counts[val] + 1`（即使写作 `counts[val]++`）*不是*原子操作，因为这一行实际上对应机器级别的多个指令。需要像互斥这样的同步构造来确保不会发生数据竞争。通常，除非明确强制互斥，否则所有操作都应假设为非原子操作。
- en: Keep in mind that not all execution sequences of the two threads cause a race
    condition. Consider the sample execution sequence of Threads 0 and 1 in [Table
    14-2](ch14.xhtml#ch14tab2).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，并非所有两个线程的执行顺序都会导致数据竞争问题。考虑 [表 14-2](ch14.xhtml#ch14tab2) 中线程 0 和线程 1 的示例执行顺序。
- en: '**Table 14-2:** Another Possible Execution Sequence of Two Threads Running
    `countElems`'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 14-2：** 两个线程运行 `countElems` 的另一种可能执行顺序'
- en: '| **Time** | **Thread 0** | **Thread 1** |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| **时间** | **线程 0** | **线程 1** |'
- en: '| *i* | Read `counts[1]` and place into Core 0’s register | … |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| *i* | 读取 `counts[1]` 并放入核心 0 的寄存器 | … |'
- en: '| *i* + 1 | Increment register by 1 | … |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| *i* + 1 | 将寄存器加 1 | … |'
- en: '| *i* + 2 | Overwrite `counts[1]` with contents of register | … |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| *i* + 2 | 用寄存器内容覆盖 `counts[1]` | … |'
- en: '| *i* + 3 | … | Read `counts[1]` and place into Core 1’s register |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| *i* + 3 | … | 读取 `counts[1]` 并放入核心 1 的寄存器 |'
- en: '| *i* + 4 | … | Increment register by 1 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| *i* + 4 | … | 将寄存器加 1 |'
- en: '| *i* + 5 | … | Overwrite `counts[1]` with contents of register |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| *i* + 5 | … | 用寄存器内容覆盖 `counts[1]` |'
- en: In this execution sequence, Thread 1 does not read from `counts[1]` until after
    Thread 0 updates it with its new value (61). The end result is that Thread 1 reads
    the value 61 from `counts[1]` and places it into Core 1’s register during time
    step *i* + 3, and writes the value 62 to `counts[1]` in time step *i* + 5.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个执行顺序中，线程 1 直到线程 0 用其新值（61）更新 `counts[1]` 后，才会读取 `counts[1]`。最终结果是，线程 1 在时间步长
    *i* + 3 读取 `counts[1]` 的值 61，并将其放入核心 1 的寄存器中，在时间步长 *i* + 5 将值 62 写入 `counts[1]`。
- en: To fix a data race, we must first isolate the *critical section*, or the subset
    of code that must execute *atomically* (in isolation) to ensure correct behavior.
    In threaded programs, blocks of code that update a shared resource are typically
    identified to be critical sections.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决数据竞争问题，我们必须首先隔离出*临界区*，即必须*原子性*执行的代码子集（单独执行），以确保正确的行为。在多线程程序中，更新共享资源的代码块通常被标识为临界区。
- en: 'In the `countElems` function, updates to the `counts` array should be put in
    a critical section to ensure that values are not lost due to multiple threads
    updating the same location in memory:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `countElems` 函数中，应该将对 `counts` 数组的更新放入临界区，以确保由于多个线程更新内存中的相同位置而不会丢失值：
- en: '[PRE30]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Since the fundamental problem in `countElems` is the simultaneous access of
    `counts` by multiple threads, a mechanism is needed to ensure that only one thread
    executes within the critical section at a time. Using a synchronization construct
    (like a mutex, which is covered in the next section) will force the threads to
    enter the critical section sequentially.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`countElems`中的根本问题是多个线程同时访问`counts`，因此需要一种机制来确保在任何时候只有一个线程能够在临界区内执行。使用同步构造（例如互斥锁，下一节将介绍）将强制线程按顺序进入临界区。
- en: 14.3.1 Mutual Exclusion
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.3.1 互斥
- en: '*What is the mutex? The answer is out there, and it’s looking for you,*'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '*什么是互斥锁？答案在外面，它正在寻找你，*'
- en: '*and it will find you if you want it to.*'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你希望它找到你，它就会找到你。*'
- en: —Trinity, explaining mutexes to Neo (with apologies to *The Matrix*)
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: —特里尼蒂，向尼奥解释互斥锁（向《黑客帝国》致歉）
- en: To fix the data race, let’s use a synchronization construct known as a mutual
    exclusion lock, or *mutex*. Mutual exclusion locks are a type of synchronization
    primitive that ensures that only one thread enters and executes the code inside
    the critical section at any given time.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 为了修复数据竞争，我们使用一种称为互斥锁的同步构造，或*互斥锁*。互斥锁是一种同步原语，确保在任何给定时间，只有一个线程能够进入并执行临界区内的代码。
- en: Before using a mutex, a program must first declare the mutex in memory that’s
    shared by threads (often as a global variable), and then initialize the mutex
    before the threads need to use it (typically in the `main` function).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用互斥锁之前，程序必须首先在线程共享的内存中声明互斥锁（通常作为全局变量），然后在线程需要使用它之前初始化互斥锁（通常在`main`函数中）。
- en: 'The Pthreads library defines a `pthread_mutex_t` type for mutexes. To declare
    a mutex variable, add this line:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: Pthreads库定义了一个`pthread_mutex_t`类型用于互斥锁。要声明一个互斥锁变量，请添加以下代码：
- en: '[PRE31]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'To initialize the mutex use the `pthread_mutex_init` function, which takes
    the address of a mutex and an attribute structure, typically set to `NULL`:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 要初始化互斥锁，使用`pthread_mutex_init`函数，该函数接收互斥锁的地址和一个属性结构，通常设置为`NULL`：
- en: '[PRE32]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'When the mutex is no longer needed (typically at the end of the `main` function,
    after `pthread_join`), a program should release the mutex structure by invoking
    the `pthread_mutex_destroy` function:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 当互斥锁不再需要时（通常是在`main`函数的末尾，在`pthread_join`之后），程序应通过调用`pthread_mutex_destroy`函数释放互斥锁结构：
- en: '[PRE33]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The Mutex: Locked and Loaded'
  id: totrans-213
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 互斥锁：锁定并加载
- en: The initial state of a mutex is unlocked, meaning it’s immediately usable by
    any thread. To enter a critical section, a thread must first acquire a lock. This
    is accomplished with a call to the `pthread_mutex_lock` function. After a thread
    has the lock, no other thread can enter the critical section until the thread
    with the lock releases it. If another thread calls `pthread_mutex_lock` and the
    mutex is already locked, the thread will *block* (or wait) until the mutex becomes
    available. Recall that blocking implies that the thread will not be scheduled
    to use the CPU until the condition it’s waiting for (the mutex being available)
    becomes true (see “Process State” on [page 627](ch13.xhtml#lev2_222)).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 互斥锁的初始状态是解锁的，意味着它可以立即被任何线程使用。要进入临界区，线程必须首先获得锁。这是通过调用`pthread_mutex_lock`函数实现的。在一个线程获得锁后，其他线程无法进入临界区，直到持锁的线程释放锁。如果另一个线程调用`pthread_mutex_lock`而互斥锁已被锁定，该线程将*阻塞*（或等待），直到互斥锁变为可用。请记住，阻塞意味着线程在等待的条件（即互斥锁可用）变为真之前不会被调度使用CPU（请参阅[第627页](ch13.xhtml#lev2_222)的“进程状态”）。
- en: When a thread exits the critical section it must call the `pthread_mutex_unlock`
    function to release the mutex, making it available for another thread. Thus, at
    most one thread may acquire the lock and enter the critical section at a time,
    which prevents multiple threads from *racing* to read and update shared variables.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 当线程退出临界区时，它必须调用`pthread_mutex_unlock`函数释放互斥锁，使其可供其他线程使用。因此，最多只有一个线程可以获取锁并进入临界区，这防止了多个线程*竞态*读取和更新共享变量。
- en: Having declared and initialized a mutex, the next question is where the lock
    and unlock functions should be placed to best enforce the critical section. Here
    is an initial attempt at augmenting the `countElems` function with a mutex:^([5](ch14.xhtml#fn14_5))
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在声明并初始化互斥锁后，下一个问题是应该将锁定和解锁函数放置在哪里，以最好地执行临界区。以下是一个初步的尝试，旨在使用互斥锁增强`countElems`函数：^([5](ch14.xhtml#fn14_5))
- en: '[PRE34]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The mutex initialize and destroy functions are placed in `main` around the
    thread creation and join functions:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 互斥锁的初始化和销毁函数被放置在`main`中，围绕线程创建和加入函数：
- en: '[PRE35]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Let’s recompile and run this new program while varying the number of threads:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新编译并运行这个新程序，同时改变线程数：
- en: '[PRE36]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Excellent, the output is *finally* consistent regardless of the number of threads
    used!
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了，不管使用多少线程，输出终于是一致的！
- en: 'Recall that another primary goal of threading is to reduce the runtime of a
    program as the number of threads increases (i.e., to *speed up* program execution).
    Let’s benchmark the performance of the `countElems` function. Although it may
    be tempting to use a command line utility like `time -p`, recall that invoking
    `time -p` measures the wall-clock time of the *entire* program (including the
    generation of random elements) and *not* just the running of the `countElems`
    function. In this case, it is better to use a system call like `gettimeofday`,
    which allows a user to accurately measure the wall-clock time of a particular
    section of code. Benchmarking `countElems` on 100 million elements yields the
    following run times:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，线程化的另一个主要目标是随着线程数的增加，减少程序的运行时间（即*加速*程序执行）。让我们基准测试一下`countElems`函数的性能。尽管可能会想使用类似`time
    -p`的命令行工具，然而请记住，调用`time -p`测量的是*整个*程序的挂钟时间（包括随机元素的生成），而*不是*仅仅是`countElems`函数的运行时间。在这种情况下，最好使用像`gettimeofday`这样的系统调用，它允许用户准确地测量某一代码段的挂钟时间。在100百万元素上基准测试`countElems`时，得到了以下的运行时间：
- en: '[PRE37]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Adding more threads causes the program to get *slower*! This goes against the
    goal of making programs *faster* with threads.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 添加更多线程会导致程序变得*更慢*！这与使用线程让程序变得*更快*的目标相悖。
- en: 'To understand what is going on, consider where the locks are placed in the
    `countsElems` function:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解发生了什么，考虑一下在`countsElems`函数中锁的位置：
- en: '[PRE38]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: In this example, we placed the lock around the *entirety* of the `for` loop.
    Even though this placement solves the correctness problems, it’s an extremely
    poor decision from a performance perspective—the critical section now encompasses
    the entire loop body. Placing locks in this manner guarantees that only one thread
    can execute the loop at a time, effectively serializing the program!
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将锁放置在`for`循环的*整个*范围内。尽管这种放置解决了正确性问题，但从性能角度来看，这是一个非常糟糕的决定——现在，临界区涵盖了整个循环体。以这种方式放置锁，保证了每次只有一个线程能执行循环，实际上将程序串行化了！
- en: 'The Mutex: Reloaded'
  id: totrans-229
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 互斥锁：重载版
- en: 'Let’s try another approach and place the mutex locking and unlocking functions
    within every iteration of the loop:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试另一种方法，将互斥锁的锁定和解锁函数放入循环的每次迭代中：
- en: '[PRE39]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: This may initially look like a better solution because each thread can enter
    the loop in parallel, serializing only when reaching the lock. The critical section
    is very small, encompassing only the line `counts[val] = counts[val] + 1`.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 乍一看，这可能看起来是一个更好的解决方案，因为每个线程可以并行进入循环，只有在遇到锁时才会串行化。临界区非常小，只包括那行代码`counts[val]
    = counts[val] + 1`。
- en: 'Let’s first perform a correctness check on this version of the program:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先对这个版本的程序进行正确性检查：
- en: '[PRE40]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: So far so good. This version of the program also produces consistent output
    regardless of the number of threads employed.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，程序也能根据使用的线程数产生一致的输出。
- en: 'Now, let’s look at performance:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下性能：
- en: '[PRE41]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Running this version of the code yields (amazingly enough) a *significantly
    slower* runtime!
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这个版本的代码（令人惊讶的是）结果是*明显更慢*的运行时间！
- en: 'As it turns out, locking and unlocking a mutex are expensive operations. Recall
    what was covered in the discussion on function call optimizations (see “Function
    Inlining” on [page 604](ch12.xhtml#lev1_95)): calling a function repeatedly (and
    needlessly) in a loop can be a major cause of slowdown in a program. In our prior
    use of mutexes, each thread locks and unlocks the mutex exactly once. In the current
    solution, each thread locks and unlocks the mutex *n*/*t* times, where *n* is
    the size of the array, *t* is the number of threads, and *n*/*t* is the size of
    the array component assigned to each particular thread. As a result, the cost
    of the additional mutex operations slows down the loop’s execution considerably.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，锁定和解锁互斥锁是昂贵的操作。回想一下在函数调用优化讨论中讲到的内容（参见[第604页](ch12.xhtml#lev1_95)的“函数内联”）：在循环中反复（且不必要地）调用函数可能是程序变慢的主要原因之一。在我们之前使用互斥锁的情况中，每个线程只锁定和解锁互斥锁一次。而在当前的解决方案中，每个线程锁定和解锁互斥锁的次数是*n*/*t*，其中*n*是数组的大小，*t*是线程数，*n*/*t*是分配给每个线程的数组部分的大小。因此，额外的互斥锁操作的成本大大减缓了循环的执行速度。
- en: 'The Mutex: Revisited'
  id: totrans-240
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 互斥锁：再探
- en: In addition to protecting the critical section to achieve correct behavior,
    an ideal solution would use the lock and unlock functions as little as possible,
    and reduce the critical section to the smallest possible size.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 除了保护临界区以确保行为正确外，理想的解决方案应该尽量减少使用锁定和解锁功能，并将临界区的大小缩小到最小。
- en: The original implementation satisfies the first requirement, whereas the second
    implementation tries to accomplish the second. At first glance, it appears that
    the two requirements are incompatible with each other. Is there a way to actually
    accomplish both (and while we are at it, speed up the execution of our program)?
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 原始实现满足了第一个要求，而第二个实现则尝试完成第二个要求。乍一看，似乎这两个要求是相互冲突的。是否有办法同时完成这两个目标（顺便说一下，加速我们程序的执行）？
- en: 'For the next attempt, each thread maintains a private, *local* array of counts
    on its stack. Because the array is local to each thread, a thread can access it
    without locking—there’s no risk of a race condition on data that isn’t shared
    between threads. Each thread processes its assigned subset of the shared array
    and populates its local counts array. After counting up all the values within
    its subset, each thread:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 对于下一个尝试，每个线程在其栈上维护一个私有的、*本地的*计数数组。因为该数组是每个线程的局部数组，线程可以在不加锁的情况下访问它——对于未在线程间共享的数据，不存在竞争条件。每个线程处理其分配的共享数组子集并填充其本地计数数组。在对其子集内的所有值进行计数后，每个线程：
- en: 1\. Locks the shared mutex (entering a critical section).
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 锁定共享互斥锁（进入临界区）。
- en: 2\. Adds the values from its local counts array to the shared counts array.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 将本地计数数组中的值添加到共享计数数组中。
- en: 3\. Unlocks the shared mutex (exiting the critical section).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 解锁共享互斥锁（退出临界区）。
- en: Restricting each thread to update the shared counts array only once significantly
    reduces the contention for shared variables and minimizes expensive mutex operations.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 限制每个线程仅更新一次共享计数数组，显著减少了对共享变量的争用，并最小化了昂贵的互斥锁操作。
- en: The following is our revised `countElems` function:^([6](ch14.xhtml#fn14_6))
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们修改后的`countElems`函数：^([6](ch14.xhtml#fn14_6))
- en: '[PRE42]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'This version has a few additional features:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 这个版本有一些额外的功能：
- en: The presence of `local_counts`, an array that is private to the scope of each
    thread (i.e., allocated in the thread’s stack). Like `counts`, `local_counts`
    contains `MAX` elements, given that `MAX` is the maximum value any element can
    hold in our input array.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`local_counts`的存在，这是一个属于每个线程作用域的私有数组（即在线程的栈上分配）。与`counts`类似，`local_counts`包含`MAX`个元素，因为`MAX`是输入数组中任何元素可能持有的最大值。'
- en: Each thread makes updates to `local_counts` at its own pace, without any contention
    for shared variables.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个线程根据自己的节奏更新`local_counts`，无需争用共享变量。
- en: A single call to `pthread_mutex_lock` protects each thread’s update to the global
    `counts` array, which happens only once at the end of each thread’s execution.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单次调用`pthread_mutex_lock`保护每个线程对全局`counts`数组的更新，这只发生在每个线程执行的末尾一次。
- en: In this manner, we reduce the time each thread spends in a critical section
    to just updating the shared counts array. Even though only one thread can enter
    the critical section at a time, the time each thread spends there is proportional
    to `MAX`, not *n*, the length of the global array. Since `MAX` is much less than
    *n*, we should see an improvement in performance.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们将每个线程在临界区内花费的时间减少到仅仅更新共享计数数组。尽管每次只有一个线程可以进入临界区，但每个线程在其中花费的时间与`MAX`成正比，而不是与*n*（全局数组的长度）成正比。由于`MAX`远小于*n*，我们应该会看到性能的提升。
- en: 'Let’s now benchmark this version of our code:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们对这版本的代码进行基准测试：
- en: '[PRE43]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Wow, what a difference! Our program not only computes the correct answers, but
    also executes faster as we increase the number of threads.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 哇，真是个天壤之别！我们的程序不仅计算出正确的答案，而且随着线程数的增加，执行速度也更快。
- en: 'The lesson to take away here is this: to efficiently minimize a critical section,
    use local variables to collect intermediate values. After the hard work requiring
    parallelization is over, use a mutex to safely update any shared variable(s).'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 这里要记住的教训是：为了高效地最小化临界区，使用局部变量来收集中间值。在需要并行化的繁重工作完成后，使用互斥锁安全地更新任何共享变量。
- en: Deadlock
  id: totrans-259
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 死锁
- en: In some programs, waiting threads have dependencies on one another. A situation
    called *deadlock* can arise when multiple synchronization constructs like mutexes
    are incorrectly applied. A deadlocked thread is blocked from execution by another
    thread, which *itself* is blocked on a blocked thread. Gridlock (in which cars
    in all directions cannot move forward due to being blocked by other cars) is a
    common real-world example of deadlock that occurs at busy city intersections.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在一些程序中，等待的线程之间存在依赖关系。当多个同步构造（如互斥锁）被错误使用时，可能会出现死锁的情况。死锁线程被另一个线程阻塞，该线程*本身*也被一个被阻塞的线程阻塞。死锁的常见现实例子是交通拥堵（所有方向的车辆因其他车辆的阻塞而无法前进），这通常发生在繁忙的城市交叉口。
- en: 'To illustrate a deadlock scenario in code, let’s consider an example where
    multithreading is used to implement a banking application. Each user’s account
    is defined by a balance and its own mutex (ensuring that no race conditions can
    occur when updating the balance):'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明代码中的死锁场景，假设使用多线程来实现一个银行应用程序。每个用户的账户由余额和其对应的互斥锁定义（确保在更新余额时不会发生竞争条件）：
- en: '[PRE44]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Consider the following naive implementation of a `Transfer` function that moves
    money from one bank account to another:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下一个简单的 `Transfer` 函数实现，该函数将钱从一个银行账户转移到另一个账户：
- en: '[PRE45]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Suppose that Threads 0 and 1 are executing concurrently and represent users
    A and B, respectively. Now consider the situation in which A and B want to transfer
    money to each other: A wants to transfer 20 dollars to B, while B wants to transfer
    40 dollars to A.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 假设线程 0 和线程 1 正在并发执行，并分别代表用户 A 和 B。现在考虑 A 和 B 互相转账的情况：A 想要向 B 转账 20 美元，而 B 想要向
    A 转账 40 美元。
- en: In the path of execution highlighted by [Figure 14-7](ch14.xhtml#ch14fig7),
    both threads concurrently execute the `Transfer` function. Thread 0 acquires the
    lock of `acctA` while Thread 1 acquires the lock of `acctB`. Now consider what
    happens. To continue executing, Thread 0 needs to acquire the lock on `acctB`,
    which Thread 1 holds. Likewise, Thread 1 needs to acquire the lock on `acctA`
    to continue executing, which Thread 0 holds. Since both threads are blocked on
    each other, they are in deadlock.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 14-7](ch14.xhtml#ch14fig7)所标出的执行路径中，两个线程同时执行`Transfer`函数。线程 0 获取了 `acctA`
    的锁，而线程 1 获取了 `acctB` 的锁。现在考虑发生了什么。为了继续执行，线程 0 需要获取 `acctB` 的锁，而该锁由线程 1 持有。同样，线程
    1 需要获取 `acctA` 的锁才能继续执行，而该锁由线程 0 持有。由于两个线程相互阻塞，它们处于死锁状态。
- en: '![image](../images/14fig07.jpg)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/14fig07.jpg)'
- en: '*Figure 14-7: An example of deadlock*'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 14-7：死锁的示例*'
- en: 'Although the OS provides some protection against deadlock, programmers should
    be mindful about writing code that increases the likelihood of deadlock. For example,
    the preceding scenario could have been avoided by rearranging the locks so that
    each lock/unlock pair surrounds only the balance update statement associated with
    it:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然操作系统提供了一些防止死锁的保护措施，但程序员在编写代码时应注意避免增加死锁的可能性。例如，前面的场景可以通过重新安排锁的顺序来避免，这样每一对锁/解锁操作只会围绕与其相关的余额更新语句：
- en: '[PRE46]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Deadlock is not a situation that is unique to threads. Processes (especially
    those that are communicating with one another) can deadlock with one another.
    Programmers should be mindful of the synchronization primitives they use and the
    consequences of using them incorrectly.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 死锁不仅仅是线程特有的情况。进程（尤其是相互通信的进程）也可能发生死锁。程序员应该注意他们使用的同步原语，以及错误使用这些原语的后果。
- en: 14.3.2 Semaphores
  id: totrans-272
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.3.2 信号量
- en: 'Semaphores are commonly used in operating systems and concurrent programs where
    the goal is to manage concurrent access to a pool of resources. When using a semaphore,
    the goal isn’t *who* owns what, but *how many* resources are still available.
    Semaphores are different from mutexes in several ways:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 信号量通常用于操作系统和并发程序中，其目的是管理对资源池的并发访问。在使用信号量时，目标不是*谁*拥有什么资源，而是*还有多少*资源可用。信号量与互斥锁在多个方面有所不同：
- en: Semaphores need not be in a binary (locked or unlocked) state. A special type
    of semaphore called a *counting semaphore* can range in value from 0 to some *r*,
    where *r* is the number of possible resources. Any time a resource is produced,
    the semaphore is incremented. Any time a resource is being used, the semaphore
    is decremented. When a counting semaphore has a value of 0, it means that no resources
    are available, and any other threads that attempt to acquire a resource must wait
    (e.g., block).
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信号量不必处于二进制（锁定或解锁）状态。一个特殊类型的信号量叫做*计数信号量*，它的值可以从0到某个*r*（其中*r*是可能的资源数量）。每当生产出一个资源时，信号量会递增。每当资源被使用时，信号量会递减。当计数信号量的值为0时，表示没有资源可用，任何尝试获取资源的线程都必须等待（例如，阻塞）。
- en: Semaphores can be locked by default.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信号量默认可以被锁定。
- en: While a mutex and condition variables can simulate the functionality of a semaphore,
    using a semaphore may be simpler and more efficient in some cases. Semaphores
    also have the advantage that *any* thread can unlock the semaphore (in contrast
    to a mutex, where the calling thread must unlock it).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然互斥锁和条件变量可以模拟信号量的功能，但在某些情况下，使用信号量可能会更简单、更高效。信号量的优势还在于，*任何*线程都可以解锁信号量（与互斥锁不同，互斥锁要求调用线程必须解锁它）。
- en: 'Semaphores are not part of the Pthreads library, but that does not mean that
    you cannot use them. On Linux and macOS systems, semaphore primitives can be accessed
    from `semaphore.h`, typically located in `/usr/include`. Since there is no standard,
    the function calls may differ on different systems. That said, the semaphore library
    has similar declarations to those for mutexes:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 信号量不是Pthreads库的一部分，但这并不意味着不能使用它们。在Linux和macOS系统上，信号量原语可以通过`semaphore.h`访问，通常位于`/usr/include`。由于没有标准，不同系统上的函数调用可能会有所不同。不过，信号量库的声明与互斥锁的声明类似：
- en: Declare a semaphore (type `sem_t`, e.g., `sem_t semaphore`).
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 声明一个信号量（类型为`sem_t`，例如`sem_t semaphore`）。
- en: 'Initialize a semaphore using `sem_init` (usually in `main`). The `sem_init`
    function has three parameters: the first is the address of a semaphore, the second
    is its initial state (locked or unlocked), and the third parameter indicates whether
    the semaphore should be shared with the threads of a process (e.g., with value
    0) or between processes (e.g., with value 1). This is useful because semaphores
    are commonly used for process synchronization. For example, initializing a semaphore
    with the call `sem_init(&semaphore, 1, 0)` indicates that our semaphore is initially
    locked (the second parameter is 1), and is to be shared among the threads of a
    common process (the third parameter is 0). In contrast, mutexes always start out
    unlocked. It is important to note that in macOS, the equivalent function is `sem_open`.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`sem_init`初始化信号量（通常在`main`中）。`sem_init`函数有三个参数：第一个是信号量的地址，第二个是其初始状态（锁定或解锁），第三个参数表示信号量是否应该在一个进程的线程间共享（例如，值为0）或在不同进程间共享（例如，值为1）。这很有用，因为信号量通常用于进程同步。例如，调用`sem_init(&semaphore,
    1, 0)`表示我们的信号量最初是锁定的（第二个参数为1），并且将在同一进程的线程之间共享（第三个参数为0）。相比之下，互斥锁总是从解锁状态开始。需要注意的是，在macOS中，等效的函数是`sem_open`。
- en: Destroy a semaphore using `sem_destroy` (usually in `main`). This function only
    takes a pointer to the semaphore (`sem_destroy(&semaphore)`). Note that in macOS,
    the equivalent function may be `sem_unlink` or `sem_close`.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`sem_destroy`销毁信号量（通常在`main`中）。该函数只接受一个指向信号量的指针（`sem_destroy(&semaphore)`）。请注意，在macOS中，等效的函数可能是`sem_unlink`或`sem_close`。
- en: The `sem_wait` function indicates that a resource is being used, and decrements
    the semaphore. If the semaphore’s value is greater than 0 (indicating there are
    still resources available), the function will immediately return, and the thread
    is allowed to proceed. If the semaphore’s value is already 0, the thread will
    block until a resource becomes available (i.e., the semaphore has a positive value).
    A call to `sem_wait` typically looks like `sem_wait(&semaphore)`.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sem_wait`函数表示资源正在被使用，并会递减信号量。如果信号量的值大于0（表示仍有资源可用），该函数将立即返回，并允许线程继续执行。如果信号量的值已经为0，线程将被阻塞，直到有资源可用（即信号量的值为正）。对`sem_wait`的调用通常像这样：`sem_wait(&semaphore)`。'
- en: The `sem_post` function indicates that a resource is being freed, and increments
    the semaphore. This function returns immediately. If there is a thread waiting
    on the semaphore (i.e., the semaphore’s value was previously 0), then the other
    thread will take ownership of the freed resource. A call to `sem_post` looks like
    `sem_post(&semaphore)`.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sem_post`函数表示一个资源被释放，并且会递增信号量。该函数会立即返回。如果有线程在等待该信号量（即信号量的值之前为0），那么其他线程将获得释放的资源。`sem_post`函数的调用形式为`sem_post(&semaphore)`。'
- en: 14.3.3 Other Synchronization Constructs
  id: totrans-283
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.3.3 其他同步构造
- en: Mutexes and semaphores are not the only example of synchronization constructs
    that can be used in the context of multithreaded programs. In this subsection
    we will briefly discuss the barrier and condition variable synchronization constructs,
    which are both part of the Pthreads library.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 互斥锁和信号量并不是在多线程程序中使用的唯一同步构造。在这一小节中，我们将简要讨论屏障和条件变量同步构造，它们都是Pthreads库的一部分。
- en: Barriers
  id: totrans-285
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 屏障
- en: 'A *barrier* is a type of synchronization construct that forces *all* threads
    to reach a common point in execution before releasing the threads to continue
    executing concurrently. Pthreads offers a barrier synchronization primitive. To
    use Pthreads barriers, it is necessary to do the following:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '*屏障*是一种同步结构，强制*所有*线程在执行中达到一个共同点，然后才释放这些线程继续并发执行。Pthreads提供了屏障同步原语。使用Pthreads屏障时，需要执行以下步骤：'
- en: Declare a barrier global variable (e.g., `pthread_barrier_t barrier`)
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 声明一个屏障全局变量（例如，`pthread_barrier_t barrier`）
- en: Initialize the barrier in `main` (`pthread_barrier_init(&barrier)`)
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`main`中初始化屏障（`pthread_barrier_init(&barrier)`）
- en: Destroy the barrier in `main` after use (`pthread_barrier_destroy(&barrier)`)
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`main`中使用完屏障后销毁（`pthread_barrier_destroy(&barrier)`）
- en: Use the `pthread_barrier_wait` function to create a synchronization point.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`pthread_barrier_wait`函数创建一个同步点。
- en: 'The following program shows the use of a barrier in a function called `threadEx`:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 以下程序展示了在`threadEx`函数中使用屏障的示例：
- en: '[PRE47]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: In this example, no thread can start processing its assigned portion of the
    array until *every* thread has printed out the message that they are starting
    work. Without the barrier, it is possible for one thread to have finished work
    before the other threads have printed their starting work message! Notice that
    it is *still* possible for one thread to print that it is done doing work before
    another thread finishes.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，*每个*线程都无法开始处理其分配的数组部分，直到*所有*线程都打印出它们开始工作的消息。如果没有屏障，可能会有一个线程在其他线程打印开始工作消息之前就已经完成工作！注意，即使如此，仍然可能有一个线程在另一个线程完成之前打印出它已经完成工作的消息。
- en: Condition Variables
  id: totrans-294
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 条件变量
- en: Condition variables force a thread to block until a particular condition is
    reached. This construct is useful for scenarios in which a condition must be met
    before the thread does some work. In the absence of condition variables, a thread
    would have to repeatedly check to see whether the condition is met, continuously
    utilizing the CPU. Condition variables are always used in conjunction with a mutex.
    In this type of synchronization construct, the mutex enforces mutual exclusion,
    whereas the condition variable ensures that particular conditions are met before
    a thread acquires the mutex.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 条件变量强制线程在特定条件达到之前进行阻塞。这种结构对于在条件满足之前线程需要做某些工作的场景非常有用。如果没有条件变量，线程就需要反复检查条件是否满足，这样会持续占用CPU。条件变量通常与互斥锁一起使用。在这种同步结构中，互斥锁强制互斥，而条件变量确保在线程获取互斥锁之前，特定的条件已被满足。
- en: POSIX condition variables have the type `pthread_cond_t`. Like the mutex and
    barrier constructs, condition variables must be initialized prior to use and destroyed
    after use.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: POSIX条件变量的类型为`pthread_cond_t`。与互斥锁和屏障结构一样，条件变量在使用前必须初始化，并在使用后销毁。
- en: To initialize a condition variable, use the `pthread_cond_init` function. To
    destroy a condition variable, use the `pthread_cond_destroy` function.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 要初始化条件变量，使用`pthread_cond_init`函数。要销毁条件变量，使用`pthread_cond_destroy`函数。
- en: The two functions commonly invoked when using condition variables are `pthread_cond_wait`
    and `pthread_cond_signal`. Both functions require the address of a mutex in addition
    to the address of the condition variable.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 使用条件变量时常用的两个函数是`pthread_cond_wait`和`pthread_cond_signal`。这两个函数除了需要条件变量的地址外，还需要互斥锁的地址。
- en: The `pthread_cond_wait(&cond, &mutex)` function takes the addresses of a condition
    variable `cond` and a mutex `mutex` as its arguments. It causes the calling thread
    to block on the condition variable `cond` until another thread signals it (or
    “wakes” it up).
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pthread_cond_wait(&cond, &mutex)` 函数接受条件变量 `cond` 和互斥锁 `mutex` 的地址作为参数。它使调用线程在条件变量
    `cond` 上阻塞，直到另一个线程发出信号（或“唤醒”它）。'
- en: The `pthread_cond_signal(&cond)` function causes the calling thread to unblock
    (or signal) another thread that is waiting on the condition variable `cond` (based
    on scheduling priority). If no threads are currently blocked on the condition,
    then the function has no effect. Unlike `pthread_cond_wait`, the `pthread_cond_signal`
    function can be called by a thread regardless of whether it owns the mutex in
    which `pthread_cond_wait` is called.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pthread_cond_signal(&cond)` 函数使调用线程解除阻塞（或向）另一个正在等待条件变量 `cond` 的线程发出信号（根据调度优先级）。如果当前没有线程在条件变量上阻塞，则该函数没有效果。与
    `pthread_cond_wait` 不同，`pthread_cond_signal` 函数可以由任何线程调用，无论该线程是否拥有 `pthread_cond_wait`
    所调用的互斥锁。'
- en: Condition Variable Example
  id: totrans-301
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 条件变量示例
- en: Traditionally, condition variables are most useful when a subset of threads
    are waiting on another set to complete some action. In the following example,
    we use multiple threads to simulate a set of farmers collecting eggs from a set
    of chickens. “Chicken” and “Farmer” represent two separate classes of threads.
    The full source of this program can be downloaded;^([7](ch14.xhtml#fn14_7)) note
    that this listing excludes many comments/error handling for brevity.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，条件变量在某些线程集合需要等待另一些线程完成某个动作时最为有用。在以下示例中，我们使用多个线程来模拟一组农夫从一组鸡那里收集鸡蛋。“鸡”和“农夫”代表两种不同的线程类型。此程序的完整源代码可以下载；^([7](ch14.xhtml#fn14_7))
    注意，这个列表省略了许多注释和错误处理以简化。
- en: 'The `main` function creates a shared variable `num_eggs` (which indicates the
    total number of eggs available at any given time), a shared `mutex` (which is
    used whenever a thread accesses `num_eggs`), and a shared condition variable `eggs`.
    It then creates two Chicken and two Farmer threads:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '`main` 函数创建了一个共享变量 `num_eggs`（表示任何时刻可用的鸡蛋总数），一个共享的 `mutex`（每当线程访问 `num_eggs`
    时使用该互斥锁），以及一个共享的条件变量 `eggs`。然后它创建了两个鸡线程和两个农夫线程：'
- en: '[PRE48]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Each Chicken thread is responsible for laying a certain number of eggs:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 每个鸡线程负责下一个数量的鸡蛋：
- en: '[PRE49]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: To lay an egg, a Chicken thread sleeps for a while, acquires the mutex and updates
    the total number of available eggs by one. Prior to releasing the mutex, the Chicken
    thread “wakes up” a sleeping Farmer (presumably by squawking). The Chicken thread
    repeats the cycle until it has laid all the eggs it intends to (`total_eggs`).
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 为了下蛋，鸡线程会先睡一会，获得互斥锁，并将可用鸡蛋的总数增加一。在释放互斥锁之前，鸡线程会“唤醒”一个正在休眠的农夫（假设是通过叫声）。鸡线程重复这个周期，直到它下完所有预定的鸡蛋（`total_eggs`）。
- en: 'Each Farmer thread is responsible for collecting `total_eggs` eggs from the
    set of chickens (presumably for their breakfast):'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 每个农夫线程负责从一组鸡中收集 `total_eggs` 个鸡蛋（假设是为了早餐）：
- en: '[PRE50]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Each Farmer thread acquires the mutex prior to checking the shared `num_eggs`
    variable to see whether any eggs are available (`*num_eggs == 0`). While there
    aren’t any eggs available, the Farmer thread blocks (i.e., takes a nap).
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 每个农夫线程在检查共享的 `num_eggs` 变量之前，先获得互斥锁，以查看是否有可用的鸡蛋（`*num_eggs == 0`）。当没有鸡蛋可用时，农夫线程会阻塞（即小睡一会）。
- en: After the Farmer thread “wakes up” due to a signal from a Chicken thread, it
    checks to see that an egg is still available (another Farmer could have grabbed
    it first) and if so, the Farmer “collects” an egg (decrementing `num_eggs` by
    one) and releases the mutex.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 当农夫线程因鸡线程的信号“醒来”后，它会检查是否仍有鸡蛋可用（另一个农夫线程可能先拿走了一个），如果有，农夫线程会“收集”一个鸡蛋（将 `num_eggs`
    减少 1），然后释放互斥锁。
- en: In this manner, the Chicken and Farmer work in concert to lay/collect eggs.
    Condition variables ensure that no Farmer thread collects an egg until it is laid
    by a Chicken thread.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，鸡线程和农夫线程协作下蛋和收集鸡蛋。条件变量确保在鸡线程下蛋之前，农夫线程不能收集鸡蛋。
- en: Broadcasting
  id: totrans-313
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 广播
- en: 'Another function used with condition variables is `pthread_cond_broadcast`,
    which is useful when multiple threads are blocked on a particular condition. Calling
    `pthread_cond_broadcast(&cond)` wakes up *all* threads that are blocked on condition
    `cond`. In this next example, we show how condition variables can implement the
    barrier construct discussed previously:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个与条件变量一起使用的函数是`pthread_cond_broadcast`，当多个线程被阻塞在特定条件下时，它非常有用。调用`pthread_cond_broadcast(&cond)`会唤醒*所有*在条件`cond`上被阻塞的线程。在下一个示例中，我们展示了如何通过条件变量实现前面讨论的屏障构造：
- en: '[PRE51]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: The function `threadEx_v2` has identical functionality to `threadEx`. In this
    example, the condition variable is named `barrier`. As each thread acquires the
    lock, it increments `n_reached`, the number of threads that have reached that
    point. While the number of threads that have reached the barrier is less than
    the total number of threads, the thread waits on the condition variable `barrier`
    and mutex `mutex`.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`threadEx_v2`与`threadEx`具有相同的功能。在此示例中，条件变量名为`barrier`。每个线程在获取锁后，都会递增`n_reached`，即到达该点的线程数量。当到达屏障的线程数少于总线程数时，线程会在条件变量`barrier`和互斥锁`mutex`上等待。
- en: However, when the last thread reaches the barrier, it calls `pthread_cond _broadcast(&barrier)`,
    which releases *all* the other threads that are waiting on the condition variable
    `barrier`, enabling them to continue execution.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当最后一个线程到达屏障时，它会调用`pthread_cond_broadcast(&barrier)`，这会释放*所有*等待条件变量`barrier`的其他线程，允许它们继续执行。
- en: This example is useful for illustrating the `pthread_cond_broadcast` function;
    however, it is best to use the Pthreads barrier primitive whenever barriers are
    necessary in a program.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例对于说明`pthread_cond_broadcast`函数非常有用；然而，在程序中需要屏障时，最好使用Pthreads屏障原语。
- en: One question that students tend to ask is if the `while` loop around the call
    to `pthread_cond_wait` in the `farmer` and `threadEx_v2` code can be replaced
    with an `if` statement. This `while` loop is in fact absolutely necessary for
    two main reasons. First, the condition may change prior to the woken thread arriving
    to continue execution. The `while` loop enforces that the condition be retested
    one last time. Second, the `pthread_cond_wait` function is vulnerable to *spurious
    wakeups*, in which a thread is erroneously woken up even though the condition
    may not be met. The `while` loop is in fact an example of a *predicate loop*,
    which forces a final check of the condition variable before releasing the mutex.
    The use of predicate loops is therefore correct practice when using condition
    variables.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 学生们常问的一个问题是，`farmer`和`threadEx_v2`代码中对`pthread_cond_wait`调用的`while`循环是否可以被`if`语句替换。事实上，这个`while`循环是绝对必要的，主要有两个原因。首先，条件可能在唤醒线程到达继续执行之前发生变化。`while`循环强制最后一次检查条件。其次，`pthread_cond_wait`函数容易发生*虚假唤醒*，即线程即使条件未满足，也会被错误地唤醒。`while`循环实际上是一个*谓词循环*的例子，它在释放互斥锁之前强制对条件变量进行最终检查。因此，在使用条件变量时，使用谓词循环是正确的做法。
- en: 14.4 Measuring the Performance of Parallel Programs
  id: totrans-320
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.4 衡量并行程序的性能
- en: So far, we have used the `gettimeofday` function to measure the amount of time
    it takes for programs to execute. In this section, we discuss how to measure how
    well a parallel program performs in comparison to a serial program as well as
    other topics related to measuring the performance of parallel programs.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经使用`gettimeofday`函数来衡量程序执行所需的时间。在本节中，我们讨论如何衡量并行程序相对于串行程序的表现，以及与衡量并行程序性能相关的其他主题。
- en: 14.4.1 Parallel Performance Basics
  id: totrans-322
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.4.1 并行性能基础
- en: Speedup
  id: totrans-323
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 加速比
- en: 'Suppose that a program takes *T*[c] time to execute on *c* cores. Thus, the
    serial version of the program would take *T*[1] time. The speedup of the program
    on *c* cores is then expressed by this equation:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个程序在*c*个核心上执行需要 *T*[c] 时间。因此，程序的串行版本将在 *T*[1] 时间内完成。程序在 *c* 个核心上的加速比通过以下公式表示：
- en: '![image](../images/equ0709-01.jpg)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/equ0709-01.jpg)'
- en: If a serial program takes 60 seconds to execute, while its parallel version
    takes 30 seconds on 2 cores, the corresponding speedup is 2\. Likewise if that
    program takes 15 seconds on 4 cores, the speedup is 4\. In an ideal scenario,
    a program running on *n* cores with *n* total threads has a speedup of *n*.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个串行程序需要60秒执行，而其并行版本在2个核心上只需要30秒，那么对应的加速比为2。同样，如果该程序在4个核心上只需15秒，那么加速比为4。在理想情况下，运行在*n*个核心上的程序，使用*n*个总线程时，理想加速比为*n*。
- en: If the speedup of a program is greater than 1, it indicates that the parallelization
    yielded some improvement. If the speedup is less than 1, then the parallel solution
    is in fact slower than the serial solution. It is possible for a program to have
    a speedup greater than *n* (for example, as a side effect of additional caches
    reducing accesses to memory). Such cases are referred to as *superlinear speedup*.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个程序的加速比大于 1，说明并行化带来了某些改进。如果加速比小于 1，则表示并行解决方案实际上比串行解决方案还要慢。一个程序的加速比可能大于 *n*（例如，额外的缓存减少了对内存的访问）。这种情况被称为*超线性加速*。
- en: Efficiency
  id: totrans-328
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 效率
- en: Speedup doesn’t factor in the number of cores—it is simply the ratio of the
    serial time to the parallel time. For example, if a serial program takes 60 seconds,
    but a parallel program takes 30 seconds on four cores, it still gets a speedup
    of 2\. However, that metric doesn’t capture the fact that it ran on four cores.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 加速比不考虑核心的数量——它仅仅是串行时间与并行时间的比率。例如，如果一个串行程序需要 60 秒，但一个并行程序在四个核心上只需要 30 秒，那么它的加速比是
    2。然而，这个指标并没有反映出它是在哪四个核心上运行的。
- en: 'To measure the speedup per core, use efficiency:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 为了衡量每个核心的加速比，使用效率：
- en: '![image](../images/equ0710-01.jpg)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/equ0710-01.jpg)'
- en: Efficiency typically varies from 0 to 1\. An efficiency of 1 indicates that
    the cores are being used perfectly. If efficiency is close to 0, then there is
    little to no benefit to parallelism, as the additional cores do not improve performance.
    If efficiency is greater than 1, it indicates superlinear speedup.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 效率通常在 0 到 1 之间变化。效率为 1 表示核心被完美利用。如果效率接近 0，那么并行性几乎没有任何好处，因为额外的核心并没有提高性能。如果效率大于
    1，则表示超线性加速。
- en: Let’s revisit the previous example in which a serial program takes 60 seconds.
    If the parallel version takes 30 seconds on two cores, then its efficiency is
    1 (or 100%). If instead the program takes 30 seconds on four cores, then the efficiency
    drops to 0.5 (or 50%).
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾之前的例子，一个串行程序需要 60 秒。如果并行版本在两个核心上运行需要 30 秒，那么它的效率是 1（或 100%）。如果程序在四个核心上运行需要
    30 秒，那么效率降至 0.5（或 50%）。
- en: Parallel Performance in the Real World
  id: totrans-334
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 现实世界中的并行性能
- en: In an ideal world, speedup is linear. For each additional compute unit, a parallel
    program should achieve a commensurate amount of speedup. However, this scenario
    rarely occurs in the real world. Most programs contain a necessarily serial component
    that exists due to inherent dependencies in the code. The longest set of dependencies
    in a program is referred to as its *critical path*. Reducing the length of a program’s
    critical path is an important first step in its parallelization. Thread synchronization
    points and (for programs running on multiple compute nodes) communication overhead
    between processes are other components in the code that can limit a program’s
    parallel performance.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在理想情况下，加速比是线性的。对于每个额外的计算单元，并行程序应该实现相应的加速。然而，在现实世界中，这种情况很少发生。大多数程序包含一个必须是串行的部分，因为代码中存在固有的依赖关系。程序中依赖关系最长的部分被称为它的*临界路径*。减少程序临界路径的长度是并行化的一个重要第一步。线程同步点以及（对于在多个计算节点上运行的程序）进程之间的通信开销是其他可能限制程序并行性能的代码组件。
- en: '**Warning NOT ALL PROGRAMS ARE GOOD CANDIDATES FOR PARALLELISM!**'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '**警告：并非所有程序都适合并行化！**'
- en: The length of the critical path can make some programs downright *hard* to parallelize.
    As an example, consider the problem of generating the *n*th Fibonacci number.
    Since every Fibonacci number is dependent on the two before it, parallelizing
    this program efficiently is very difficult!
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 临界路径的长度可能使得某些程序完全*难以*并行化。以生成第*n*个斐波那契数为例。由于每个斐波那契数依赖于前两个数，因此高效地并行化这个程序是非常困难的！
- en: 'Consider the parallelization of the `countElems` function of the CountSort
    algorithm from earlier in this chapter. In an ideal world, we would expect the
    speedup of the program to be linear with respect to the number of cores. However,
    let’s measure its runtime (in this case, running on a quad-core system with eight
    logical threads):'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑本章前面提到的 CountSort 算法中的 `countElems` 函数的并行化。在理想情况下，我们期望程序的加速比与核心数量成线性关系。然而，来看看它的运行时间（在这种情况下，运行在一个四核系统上，拥有八个逻辑线程）：
- en: '[PRE52]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[Table 14-3](ch14.xhtml#ch14tab3) shows the speedup and efficiency for these
    multithreaded executions.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 14-3](ch14.xhtml#ch14tab3)显示了这些多线程执行的加速比和效率。'
- en: '**Table 14-3:** Performance Benchmarks'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 14-3：** 性能基准'
- en: '| Number of threads | 2 | 4 | 8 |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| 线程数 | 2 | 4 | 8 |'
- en: '| Speedup | 1.68 | 2.36 | 3.08 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| 加速比 | 1.68 | 2.36 | 3.08 |'
- en: '| Efficiency | 0.84 | 0.59 | 0.39 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| 效率 | 0.84 | 0.59 | 0.39 |'
- en: We have 84% efficiency with two cores, but the core efficiency falls to 39%
    with eight cores. Notice that the ideal speedup of eight was not met. One reason
    for this is that the overhead of assigning work to threads and the serial update
    to the `counts` array starts dominating performance at higher numbers of threads.
    Second, resource contention by the eight threads (remember this is a quad-core
    processor) reduces core efficiency.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在使用两个核心时有84%的效率，但在使用八个核心时，核心效率降至39%。注意，八个核心的理想加速比并未实现。造成这种情况的一个原因是，随着线程数增加，分配工作给线程以及对`counts`数组的串行更新的开销开始主导性能。其次，八个线程的资源竞争（记住这是一个四核处理器）降低了核心效率。
- en: Amdahl’s Law
  id: totrans-346
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 阿姆达尔定律
- en: In 1967, Gene Amdahl, a leading computer architect at IBM, predicted that the
    maximum speedup that a computer program can achieve is limited by the size of
    its necessarily serial component (now referred to as Amdahl’s Law). More generally,
    Amdahl’s Law states that for every program, there is a component that can be sped
    up (i.e., the fraction of a program that can be optimized or parallelized, *P*),
    and a component that *cannot* be sped up (i.e., the fraction of a program that
    is inherently serial, or *S*). Even if the time needed to execute the optimizable
    or parallelizable component *P* is reduced to zero, the serial component *S* will
    exist, and will come to eventually dominate performance. Since *S* and *P* are
    fractions, note that *S* + *P* = 1.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 1967年，IBM的著名计算机架构师吉恩·阿姆达尔（Gene Amdahl）预测，一个计算机程序能够实现的最大加速比是由其必然是串行部分的大小所限制的（现称为阿姆达尔定律）。更一般地，阿姆达尔定律指出，对于每个程序，都有一个可以加速的部分（即，可以优化或并行化的程序部分，*P*），还有一个*不能*加速的部分（即，程序中固有的串行部分，或*S*）。即使优化或并行化的部分*P*的执行时间减少到零，串行部分*S*仍然存在，最终将主导程序的性能。由于*S*和*P*是分数，注意*S*
    + *P* = 1。
- en: Consider a program that executes on one core in time *T*[1]. Then, the fraction
    of the program execution that is necessarily serial takes *S* × *T*[1] time to
    run, and the parallelizable fraction of program execution (*P* = 1 *– S*) takes
    *P* × *T*[1] to run.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个程序，它在一个核心上执行，所需时间为*T*[1]。那么，程序执行中必定是串行的部分需要*T*[1]的*S*时间，而可以并行化的部分（*P* =
    1 *– S*）需要*T*[1]的*P*时间来执行。
- en: 'When the program executes on *c* cores, the serial fraction of the code still
    takes *S* × *T*[1] time to run (all other conditions being equal), but the parallelizable
    fraction can be divided into *c* cores. Thus, the maximum improvement for the
    parallel processor with *c* cores to run the same job is:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 当程序在*c*个核心上执行时，串行部分的代码仍然需要*S* × *T*[1]时间来运行（在其他条件相同的情况下），但可以并行化的部分可以被划分到*c*个核心中。因此，平行处理器在*c*个核心上执行相同任务的最大改进是：
- en: '![image](../images/equ0711-01.jpg)'
  id: totrans-350
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/equ0711-01.jpg)'
- en: As *c* increases, the execution time on the parallel processor becomes dominated
    by the serial fraction of the program.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 随着*c*的增加，平行处理器的执行时间开始受到程序串行部分的主导。
- en: To understand the impact of Amdahl’s law, consider a program that is 90% parallelizable
    and executes in 10 seconds on 1 core. In our equation, the parallelizable component
    (*P*) is 0.9, while the serial component (*S*) is 0.1\. [Table 14-4](ch14.xhtml#ch14tab4)
    depicts the corresponding total time on *c* cores (*T*[*c*]) according to Amdahl’s
    Law, and the associated speedup.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解阿姆达尔定律的影响，考虑一个程序，该程序可以并行化90%，并且在1个核心上执行需要10秒。在我们的方程中，可并行化部分（*P*）为0.9，而串行部分（*S*）为0.1。[表14-4](ch14.xhtml#ch14tab4)显示了根据阿姆达尔定律，在*c*个核心上执行的总时间（*T*[*c*]）以及相关的加速比。
- en: '**Table 14-4:** The Effect of Amdahl’s Law on a 10-Second Program that is 90%
    Parallelizable'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 14-4：** 阿姆达尔定律对一个可以并行化90%的10秒程序的影响'
- en: '| **Number of cores** | **Serial time (s)** | **Parallel time (s)** | **Total
    time (***T*[*c*] **s)** | **Speedup (over one core)** |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| **核心数量** | **串行时间（秒）** | **并行时间（秒）** | **总时间（*T*[*c*] 秒）** | **加速比（相对于单核）**
    |'
- en: '| 1 | 1 | 9 | 10 | 1 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 9 | 10 | 1 |'
- en: '| 10 | 1 | 0.9 |   1.9 | 5.26 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 1 | 0.9 |   1.9 | 5.26 |'
- en: '| 100 | 1 | 0.09 |   1.09 | 9.17 |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 1 | 0.09 |   1.09 | 9.17 |'
- en: '| 1000 | 1 | 0.009 |   1.009 | 9.91 |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| 1000 | 1 | 0.009 |   1.009 | 9.91 |'
- en: Observe that, over time, the serial component of the program begins to dominate,
    and the effect of adding more and more cores seems to have little to no effect.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 观察到，随着时间的推移，程序的串行部分开始主导，而增加更多核心的效果似乎变得微乎其微。
- en: 'A more formal way to look at this requires incorporating Amdahl’s calculation
    for *T*[c] into the equation for speedup:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 以更正式的方式来看待这个问题，需要将阿姆达尔对于*T*[c]的计算纳入加速比方程中：
- en: '![image](../images/equ0712-01.jpg)'
  id: totrans-361
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/equ0712-01.jpg)'
- en: Taking the limit of this equation shows that as the number of cores (*c*) approaches
    infinity, speedup approaches 1/*S*. In the example shown in [Table 14-4](ch14.xhtml#ch14tab4),
    speedup approaches 1/0.1, or 10.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对这个方程取极限可以看出，当核心数（*c*）趋近于无穷大时，加速比趋近于1/*S*。在[表14-4](ch14.xhtml#ch14tab4)中显示的例子中，加速比趋近于1/0.1，或者10。
- en: As another example, consider a program where *P* = 0.99\. In other words, 99%
    of the program is parallelizable. As *c* approaches infinity, the serial time
    starts to dominate the performance (in this example, *S* = 0.01). Thus, speedup
    approaches 1/0.01 or 100\. In other words, even with a million cores, the maximum
    speedup achievable by this program is only 100.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 作为另一个例子，考虑一个程序，其中*P* = 0.99。换句话说，99%的程序是可并行化的。随着*c*趋近于无穷大，串行时间开始主导性能（在这个例子中，*S*
    = 0.01）。因此，加速比趋近于1/0.01，或者100。换句话说，即使有一百万个核心，这个程序的最大加速比也只有100。
- en: 'ALL IS NOT LOST: THE LIMITS OF AMDAHL’S LAW'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 一切并未失去：阿姆达尔定律的局限性
- en: 'When learning about Amdahl’s Law, it’s important to consider the *intentions*
    of its originator, Gene Amdahl. In his own words, the law was proposed to demonstrate
    “the continued validity of the single processor approach, and the weakness of
    the multiple processor approach in terms of application to real problems and their
    attendant irregularities.”^([8](ch14.xhtml#fn14_8)) In his 1967 paper Amdahl expanded
    on this concept, writing: “For over a decade prophets have voiced the contention
    that the organization of a single computer has reached its limits, and that truly
    significant advances can be made only by interconnection of a multiplicity of
    computers in such a manner as to permit cooperative solution.” Subsequent work
    challenged some of the key assumptions made by Amdahl. Read about the Gustafson–Barsis
    Law in the next subsection for a discussion on the limits of Amdahl’s Law and
    a different argument on how to think about the benefits of parallelism.'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习阿姆达尔定律时，考虑其创始人基恩·阿姆达尔（Gene Amdahl）的*意图*非常重要。用他自己的话说，这一定律是为了展示“单处理器方法的持续有效性，以及多处理器方法在应用于实际问题及其伴随的不规则性时的弱点。”^([8](ch14.xhtml#fn14_8))
    在他1967年的论文中，阿姆达尔进一步阐述了这一概念，写道：“十多年来，许多预言家声称，单一计算机的组织已经达到了极限，只有通过以某种方式将多台计算机互联，才能实现真正重大的进步，从而允许合作解决问题。”
    随后的研究挑战了阿姆达尔提出的一些关键假设。在接下来的子章节中，阅读关于古斯塔夫森–巴尔西斯定律的内容，讨论阿姆达尔定律的局限性，并提出一种不同的观点来思考并行性带来的好处。
- en: 14.4.2 Advanced Topics
  id: totrans-366
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.4.2 高级话题
- en: Gustafson–Barsis Law
  id: totrans-367
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 古斯塔夫森–巴尔西斯定律
- en: In 1988, John L. Gustafson, a computer scientist and researcher at Sandia National
    Labs, wrote a paper called “Reevaluating Amdahl’s Law.”^([9](ch14.xhtml#fn14_9))
    In this paper, Gustafson calls to light a critical assumption that was made about
    the execution of a parallel program that is not always true.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 1988年，计算机科学家及桑迪亚国家实验室的研究员约翰·L·古斯塔夫森（John L. Gustafson）撰写了一篇名为《重新评估阿姆达尔定律》的论文。^([9](ch14.xhtml#fn14_9))
    在这篇论文中，古斯塔夫森揭示了一个关于并行程序执行的关键假设，这个假设并不总是成立。
- en: Specifically, Amdahl’s law implies that the number of compute cores *c* and
    the fraction of a program that is parallelizable *P* are independent of each other.
    Gustafson notes that this “is virtually never the case.” While benchmarking a
    program’s performance by varying the number of cores on a fixed set of data is
    a useful academic exercise, in the real world, more cores (or processors, as examined
    in our discussion of distributed memory) are added as the problem grows large.
    “It may be most realistic,” Gustafson writes, “to assume run time, not problem
    size, is constant.”
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，阿姆达尔定律假设计算核心的数量*c*与程序中可并行化部分*P*是彼此独立的。古斯塔夫森指出，这“几乎从来都不是这样”。虽然通过在固定数据集上变化核心数量来基准测试程序的性能是一个有用的学术练习，但在现实世界中，随着问题规模的增大，更多的核心（或处理器，正如我们在分布式内存讨论中所探讨的）被添加进来。古斯塔夫森写道：“最现实的假设是，假设运行时间，而非问题规模，是恒定的。”
- en: Therefore, according to Gustafson, it is most accurate to say that “The amount
    of work that can be done in parallel varies linearly with the number of processors.”
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，根据古斯塔夫森的观点，最准确的表述是：“并行处理的工作量与处理器数量呈线性关系。”
- en: Consider a *parallel* program that takes time *T*[c] to run on a system with
    *c* cores. Let *S* represent the fraction of the program execution that is necessarily
    serial and takes *S* × *T*[c] time to run. Thus, the parallelizable fraction of
    the program execution, *P* = 1 *– S*, takes *P* × *T*[c] time to run on *c* cores.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个*并行*程序，它在具有*c*核心的系统上运行时需要时间*T*[c]。设*S*表示程序执行中必须串行执行的部分，占用*S* × *T*[c]的时间。因此，程序中可并行化的部分，即*P*
    = 1 *– S*，在*c*核心上运行时需要*P* × *T*[c]的时间。
- en: 'When the same program is run on just one core, the serial fraction of the code
    still takes *S* × *T*[c] (assuming all other conditions are equal). However, the
    parallelizable fraction (which was divided between *c* cores) now has to be executed
    by just one core to run serially and takes *P* × *T*[c] × *c* time. In other words,
    the parallel component will take *c* times as long on a single-core system. It
    follows that the scaled speedup would be:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 当同一个程序在单个核心上运行时，代码的串行部分仍然需要*S* × *T*[c]（假设其他条件相同）。然而，原本分配给*c*个核心的并行部分，现在必须由单个核心执行串行操作，并且需要*P*
    × *T*[c] × *c*的时间。换句话说，在单核系统上，并行部分的执行时间将是*c*倍。因此，缩放加速比将是：
- en: '![image](../images/equ0713-01.jpg)'
  id: totrans-373
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/equ0713-01.jpg)'
- en: This shows that the scaled speedup increases linearly with the number of compute
    units.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明，缩放加速随着计算单元数量的增加而线性增加。
- en: Consider our prior example in which 99% of a program is parallelizable (i.e.,
    *P* = 0.99). Applying the scaled speedup equation, the theoretical speedup on
    100 processors would be 99.01\. On 1,000 processors, it would be 990.01\. Notice
    that the efficiency stays constant at *P*.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑我们之前的例子，其中99%的程序是可并行化的（即，*P* = 0.99）。应用缩放加速方程，使用100个处理器时的理论加速比为99.01，使用1,000个处理器时为990.01。请注意，效率保持在*P*不变。
- en: As Gustafson concludes, “speedup should be measured by scaling the problem to
    the number of processors, not by fixing a problem size.” Gustafson’s result is
    notable because it shows that it is possible to get increasing speedup by updating
    the number of processors. As a researcher working in a national supercomputing
    facility, Gustafson was more interested in doing *more work* in a constant amount
    of time. In several scientific fields, the ability to analyze more data usually
    leads to higher accuracy or fidelity of results. Gustafson’s work showed that
    it was possible to get large speedups on large numbers of processors, and revived
    interest in parallel processing.^([10](ch14.xhtml#fn14_10))
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 正如Gustafson所总结的，“加速比应通过将问题规模扩展到处理器数量来衡量，而不是通过固定问题规模。”Gustafson的结果值得注意，因为它表明通过更新处理器数量是可以实现加速的增加的。作为一名在国家超级计算设施工作的研究员，Gustafson更关注在恒定时间内做*更多的工作*。在多个科学领域中，分析更多数据通常会提高结果的准确性或可信度。Gustafson的工作表明，在大量处理器上获得大规模加速是可能的，并重新激发了人们对并行处理的兴趣。^([10](ch14.xhtml#fn14_10))
- en: Scalability
  id: totrans-377
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 可扩展性
- en: We describe a program as *scalable* if we see improving (or constant) performance
    as we increase the number of resources (cores, processors) or the problem size.
    Two related concepts are *strong scaling* and *weak scaling*. It is important
    to note that “weak” and “strong” in this context do not indicate the *quality*
    of a program’s scalability, but are simply different ways to measure scalability.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称一个程序为*可扩展*，如果随着资源（核心、处理器）或问题规模的增加，我们看到性能在提升（或保持不变）。两个相关的概念是*强扩展*和*弱扩展*。需要注意的是，"弱"和"强"在这里并不表示程序可扩展性的*质量*，而只是衡量可扩展性的不同方式。
- en: We say that a program is *strongly scalable* if increasing the number of cores/processing
    units on a *fixed* problem size yields an improvement in performance. A program
    displays strong linear scalability if, when run on *n* cores, the speedup is also
    *n*. Of course, Amdahl’s Law guarantees that after some point, adding additional
    cores makes little sense.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 我们说一个程序是*强可扩展*的，如果在*固定*问题规模下增加核心/处理单元的数量会提高性能。如果在*n*个核心上运行时加速比也是*n*，则该程序表现为强线性可扩展性。当然，阿姆达尔定律保证，在某个时刻，添加更多核心几乎没有意义。
- en: We say that a program is *weakly scalable* if increasing the size of the data
    at the same rate as the number of cores (i.e., if there is a fixed data size per
    core/processor) results in constant or an improvement in performance. We say a
    program displays weak linear scalability if we see an improvement of *n* if the
    work per core is scaled up by a factor of *n*.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在增加数据大小的速度与核心数相同的情况下（即每个核心/处理器有固定的数据大小）能够保持性能不变或有所提升，我们就说一个程序是*弱可扩展*的。如果当每个核心的工作量按因子*n*增加时，程序表现出弱线性可扩展性，我们就说这个程序表现出了弱线性可扩展性。
- en: General Advice Regarding Measuring Performance
  id: totrans-381
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 关于性能测量的一般建议
- en: We conclude our discussion on performance with some notes about benchmarking
    and performance on hyperthreaded cores.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在性能讨论的最后，附上一些关于基准测试和超线程核心性能的说明。
- en: '**Run a program multiple times when benchmarking.**   In many of the examples
    shown thus far in this book, we run a program only once to get a sense of its
    runtime. However, this is not sufficient for formal benchmarks. Running a program
    once is *never* an accurate measure of a program’s true runtime! Context switches
    and other running processes can temporarily cause the runtime to radically fluctuate.
    Therefore, it is always best to run a program several times and report an average
    runtime together with as many details as feasible, including number of runs, observed
    variability of the measurements (e.g., error bars, minimum, maximum, median, standard
    deviation) and conditions under which the measurements were taken.'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '**在进行基准测试时多次运行程序。** 在本书前面的许多示例中，我们仅运行一次程序来感知其运行时间。然而，这对正式的基准测试来说是不够的。只运行一次程序*从来*无法准确测量程序的真实运行时间！上下文切换和其他运行中的进程可能会导致运行时间发生剧烈波动。因此，最好的做法是多次运行程序，并报告平均运行时间，同时尽可能提供更多的细节，包括运行次数、测量的波动性（例如误差条、最小值、最大值、中位数、标准差）以及测量时的条件。'
- en: '**Be careful where you measure timing.**   The `gettimeofday` function is useful
    in helping to accurately measure the time a program takes to run. However, it
    can also be abused. Even though it may be tempting to place the `gettimeofday`
    call around only the thread creation and joining component in `main`, it is important
    to consider what exactly you are trying to time. For example, if a program reads
    in an external data file as a necessary part of its execution, the time for file
    reading should likely be included in the program’s timing.'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '**小心你测量时间的位置。** `gettimeofday`函数在帮助准确测量程序运行时间方面很有用，但也可能被滥用。尽管将`gettimeofday`调用仅放在`main`函数中的线程创建和加入部分可能很诱人，但你必须考虑你到底要测量什么。例如，如果一个程序在执行过程中需要读取外部数据文件，那么文件读取的时间应该包含在程序的计时中。'
- en: '**Be aware of the impact of hyperthreaded cores.**   As discussed in “Taking
    a Closer Look: How Many Cores?” on [page 671](ch14.xhtml#lev3_112) and “Multicore
    and Hardware Multithreading” on [page 283](ch05.xhtml#lev2_108), hyperthreaded
    (logical) cores are capable of executing multiple threads on a single core. In
    a quad-core system with two logical threads per core, we say there are eight hyperthreaded
    cores on the system. Running a program in parallel on eight logical cores in many
    cases yields better wall time than running a program on four cores. However, due
    to the resource contention that usually occurs with hyperthreaded cores, you may
    see a dip in core efficiency and nonlinear speedup.'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意超线程核心的影响。** 如在[第671页](ch14.xhtml#lev3_112)的《仔细观察：多少个核心？》和[第283页](ch05.xhtml#lev2_108)的《多核与硬件多线程》中讨论的那样，超线程（逻辑）核心能够在单个核心上执行多个线程。在一个具有每个核心两个逻辑线程的四核系统中，我们可以说系统有八个超线程核心。在八个逻辑核心上并行运行程序，通常比在四个核心上运行程序更能节省墙面时间。然而，由于超线程核心通常会发生资源竞争，你可能会看到核心效率下降以及非线性加速现象。'
- en: '**Beware of resource contention.**   When benchmarking, it’s always important
    to consider what *other* processes and threaded applications are running on the
    system. If your performance results ever look a bit strange, it is worth quickly
    running `top` to see whether there are any other users also running resource-intensive
    tasks on the same system. If so, try using a different system to benchmark (or
    wait until the system is not so heavily used).'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '**警惕资源竞争。** 在进行基准测试时，始终要考虑系统上正在运行的*其他*进程和线程应用程序。如果你的性能结果看起来有些异常，快速运行`top`命令检查是否有其他用户在同一系统上运行资源密集型任务是非常值得的。如果是这样，尝试使用不同的系统进行基准测试（或者等到系统不那么繁忙时再进行）。'
- en: 14.5 Cache Coherence and False Sharing
  id: totrans-387
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.5 缓存一致性与虚假共享
- en: 'Multicore caches can have profound implications on a multithreaded program’s
    performance. First, however, let’s quickly review some of the basic concepts related
    to cache design (see “CPU Caches” on page 1299 for more details):'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 多核缓存可能对多线程程序的性能产生深远的影响。然而，在此之前，让我们快速回顾一些与缓存设计相关的基本概念（更多细节见第1299页的“CPU缓存”）：
- en: Data/instructions are not transported *individually* to the cache. Instead,
    data is transferred in *blocks*, and block sizes tend to get larger at lower levels
    of the memory hierarchy.
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据/指令不会*单独*传输到缓存中。相反，数据是以*块*的形式传输的，并且块的大小通常在内存层次结构的较低级别变得更大。
- en: Each cache is organized into a series of sets, with each set having a number
    of lines. Each line holds a single block of data.
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个缓存被组织成一系列的集合，每个集合有若干行。每行存储一个数据块。
- en: The individual bits of a memory address are used to determine the set, tag,
    and block offset of the cache to which to write a block of data.
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存地址的各个比特用于确定缓存的集合、标签和块偏移量，以将数据块写入缓存。
- en: A *cache hit* occurs when the desired data block exists in the cache. Otherwise,
    a *cache miss* occurs, and a lookup is performed on the next lower level of the
    memory hierarchy (which can be cache or main memory).
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*缓存命中*发生在所需数据块存在于缓存中时。否则，发生*缓存未命中*，并且会在内存层次结构的下一级（可能是缓存或主内存）进行查找。'
- en: The *valid bit* indicates if a block at a particular line in the cache is safe
    to use. If the valid bit is set to 0, the data block at that line cannot be used
    (e.g., the block could contain data from an exited process).
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*有效位*表示缓存中特定行的块是否可以安全使用。如果有效位设置为0，则该行的数据显示块不能使用（例如，该块可能包含来自已退出进程的数据）。'
- en: Information is written to cache/memory based on two main strategies. In the
    *write-through* strategy, the data is written to cache and main memory simultaneously.
    In the *write-back* strategy, data is written only to cache and gets written to
    lower levels in the hierarchy after the block is evicted from the cache.
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信息是根据两种主要策略写入缓存/内存的。在*直写*策略中，数据同时写入缓存和主内存。在*回写*策略中，数据只写入缓存，并在缓存块被逐出后写入更低级别的内存。
- en: 14.5.1 Caches on Multicore Systems
  id: totrans-395
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.5.1 多核系统中的缓存
- en: 'Recall that in shared memory architectures each core can have its own cache
    (see “Looking Ahead: Caching on Multicore Processors” on [page 581](ch11.xhtml#lev1_91))
    and that multiple cores can share a common cache. [Figure 14-8](ch14.xhtml#ch14fig8)
    depicts an example dual-core CPU. Even though each core has its own local L1 cache,
    the cores share a common L2 cache.'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，在共享内存架构中，每个核心可以有自己的缓存（参见[第581页](ch11.xhtml#lev1_91)的“展望：多核处理器中的缓存”），并且多个核心可以共享一个公共缓存。[图14-8](ch14.xhtml#ch14fig8)展示了一个示例的双核CPU。尽管每个核心都有自己的本地L1缓存，但核心之间共享一个公共的L2缓存。
- en: '![image](../images/14fig08.jpg)'
  id: totrans-397
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/14fig08.jpg)'
- en: '*Figure 14-8: An example dual-core CPU with separate L1 caches and a shared
    L2 cache*'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '*图14-8：一个示例的双核CPU，具有独立的L1缓存和共享的L2缓存*'
- en: Multiple threads in a single executable may execute separate functions. Without
    a *cache coherency* strategy (see “Cache Coherency” on [page 583](ch11.xhtml#lev2_198))
    to ensure that each cache maintains a consistent view of shared memory, it is
    possible for shared variables to be updated inconsistently. As an example, consider
    the dual-core processor in [Figure 14-8](ch14.xhtml#ch14fig8), where each core
    is busy executing separate threads concurrently. The thread assigned to Core 0
    has a local variable `x`, whereas the thread executing on Core 1 has a local variable
    `y`, and both threads have shared access to a global variable `g`. [Table 14-5](ch14.xhtml#ch14tab5)
    shows one possible path of execution.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 单个可执行文件中的多个线程可能执行不同的函数。如果没有*缓存一致性*策略（参见[第583页](ch11.xhtml#lev2_198)的“缓存一致性”），确保每个缓存都能保持共享内存的一致视图，那么共享变量可能会被不一致地更新。例如，考虑[图14-8](ch14.xhtml#ch14fig8)中的双核处理器，其中每个核心并行执行不同的线程。分配给核心0的线程有一个本地变量`x`，而在核心1上执行的线程有一个本地变量`y`，并且两个线程都可以共享访问一个全局变量`g`。[表14-5](ch14.xhtml#ch14tab5)展示了一个可能的执行路径。
- en: '**Table 14-5:** Problematic Data Sharing Due to Caching'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '**表14-5：由于缓存引起的问题数据共享**'
- en: '| **Time** | **Core 0** | **Core 1** |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| **时间** | **核心0** | **核心1** |'
- en: '| 0 | `g = 5` | (other work) |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| 0 | `g = 5` | (其他工作) |'
- en: '| 1 | (other work) | `y = g*4` |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| 1 | (其他工作) | `y = g*4` |'
- en: '| 2 | `x += g` | `y += g*2` |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| 2 | `x += g` | `y += g*2` |'
- en: 'Suppose that the initial value of `g` is 10, and the initial values of `x`
    and `y` are both 0\. What is the final value of `y` at the end of this sequence
    of operations? Without cache coherence, this is a very difficult question to answer
    given that there are at least three stored values of `g`: one in Core 0’s L1 cache,
    one in Core 1’s L1 cache, and a separate copy of `g` stored in the shared L2 cache.'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 假设`g`的初始值为 10，`x`和`y`的初始值都为 0。那么，在这段操作序列结束时，`y`的最终值是多少？如果没有缓存一致性，这是一个非常难以回答的问题，因为至少有三个`g`的存储值：一个在
    Core 0 的 L1 缓存中，一个在 Core 1 的 L1 缓存中，另一个是存储在共享的 L2 缓存中的`g`副本。
- en: '[Figure 14-9](ch14.xhtml#ch14fig9) shows one possible erroneous result after
    the sequence of operations in [Table 14-5](ch14.xhtml#ch14tab5) completes. Suppose
    that the L1 caches implement a write-back policy. When the thread executing on
    Core 0 writes the value 5 to `g`, it updates only the value of `g` in Core 0’s
    L1 cache. The value of `g` in Core 1’s L1 cache still remains 10, as does the
    copy in the shared L2 cache. Even if a write-through policy is implemented, there
    is no guarantee that the copy of `g` stored in Core 1’s L1 cache gets updated!
    In this case, `y` will have the final value of `60`.'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 14-9](ch14.xhtml#ch14fig9) 显示了在[表 14-5](ch14.xhtml#ch14tab5)操作序列完成后可能出现的一个错误结果。假设
    L1 缓存实现了写回策略。当在 Core 0 上执行的线程将值 5 写入`g`时，它仅更新 Core 0 L1 缓存中的`g`值。Core 1 的 L1 缓存中的`g`值仍然是
    10，且共享 L2 缓存中的副本也不变。即使实现了写穿透策略，也不能保证存储在 Core 1 L1 缓存中的`g`副本会被更新！在这种情况下，`y`的最终值将是
    60。'
- en: '![image](../images/14fig09.jpg)'
  id: totrans-407
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/14fig09.jpg)'
- en: '*Figure 14-9: A problematic update to caches that do not employ cache coherency*'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 14-9：一个没有采用缓存一致性策略的缓存更新问题*'
- en: A cache coherence strategy invalidates or updates cached copies of shared values
    in other caches when a write to the shared data value is made in one cache. The
    *modified shared invalid* (MSI) protocol (discussed in detail in “The MSI Protocol”
    on [page 584](ch11.xhtml#lev2_199)) is one example of an invalidating cache coherency
    protocol.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 一种缓存一致性策略是在一个缓存中对共享数据值进行写入时，使其他缓存中的该共享值的缓存副本无效或更新。*修改的共享无效*（MSI）协议（在《MSI 协议》一节中详细讨论，参见[第
    584 页](ch11.xhtml#lev2_199)）就是一种无效化缓存一致性协议的例子。
- en: A common technnique for implementing MSI is snooping. Such a *snoopy cache*
    “snoops” on the memory bus for possible write signals. If the snoopy cache detects
    a write to a shared cache block, it invalidates its line containing that cache
    block. The end result is that the only valid version of the block is in the cache
    that is written to, whereas *all other* copies of the block in other caches are
    marked as invalid.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 实现 MSI 的一种常见技术是嗅探。这样的*嗅探缓存*在内存总线上“嗅探”可能的写入信号。如果嗅探缓存检测到对共享缓存块的写入，它会使包含该缓存块的行无效。最终结果是，块的唯一有效版本存在于写入的缓存中，而*所有其他*缓存中的该块副本都会被标记为无效。
- en: Employing the MSI protocol with snoooping would yield the correct final assignment
    of `30` to variable `y` in the previous example.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 使用嗅探的 MSI 协议将会导致前述例子中变量`y`最终赋值为`30`。
- en: 14.5.2 False Sharing
  id: totrans-412
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.5.2 错误共享
- en: Cache coherence guarantees correctness, but it can potentially harm performance.
    Recall that when the thread updates `g` on Core 0, the snoopy cache invalidates
    not only `g`, but the *entire cache line* that `g` is a part of.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存一致性保证了正确性，但它可能会影响性能。回想一下，当线程在 Core 0 上更新`g`时，嗅探缓存不仅使`g`无效，还使包含`g`的*整个缓存行*无效。
- en: 'Consider our initial attempt at parallelizing the `countElems` function of
    the CountSort algorithm.⁴ For convenience, the function is reproduced here:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑我们最初尝试并行化 CountSort 算法中的`countElems`函数。⁴ 为了方便，这里重新列出该函数：
- en: '[PRE53]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'In our previous discussion of this function (see “Data Races” on [page 691](ch14.xhtml#lev3_116)),
    we pointed out how data races can cause the `counts` array to not populate with
    the correct set of counts. Let’s see what happens if we attempt to *time* this
    function. We add timing code to `main` using `getimeofday` as before.⁶ Benchmarking
    the initial version of `countElems` as just shown on 100 million elements yields
    the following times:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前讨论这个函数时（参见《数据竞争》一节，见[第 691 页](ch14.xhtml#lev3_116)），我们指出了数据竞争如何导致`counts`数组没有正确填充。让我们看看如果我们尝试*计时*这个函数会发生什么。我们使用`getimeofday`在`main`中添加计时代码，像以前一样。⁶
    在 1 亿个元素上对`countElems`的初始版本进行基准测试，得到了以下时间：
- en: '[PRE54]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Even without any synchronization constructs, this version of the program *still
    gets slower* as the number of threads increases!
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 即使没有任何同步构造，随着线程数的增加，这个版本的程序*仍然会变慢*！
- en: To understand what is going on, let’s revisit the `counts` array. This holds
    the frequency of occurrence of each number in our input array. The maximum value
    is determined by the variable `MAX`. In our example program, `MAX` is set to 10\.
    In other words, the `counts` array takes up 40 bytes of space.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解发生了什么，让我们重新审视 `counts` 数组。这个数组存储了输入数组中每个数字的出现频率。最大值由变量 `MAX` 决定。在我们的示例程序中，`MAX`
    被设置为 10。换句话说，`counts` 数组占用了 40 字节的空间。
- en: 'Recall that the cache details on a Linux system (see “Looking Ahead: Caching
    on Multicore Processors” on [page 581](ch11.xhtml#lev1_91)) are located in the
    `/sys/devices/system/cpu/` directory. Each logical core has its own `cpu` subdirectory
    called `cpuk`, where `k` indicates the *k*th logical core. Each `cpu` subdirectory
    in turn has separate `index` directories that indicate the caches available to
    that core.'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 请回忆，在 Linux 系统上的缓存详情（参见《展望未来：多核处理器上的缓存》章节，第 [581 页](ch11.xhtml#lev1_91)）位于 `/sys/devices/system/cpu/`
    目录中。每个逻辑核心都有自己的 `cpu` 子目录，名为 `cpuk`，其中 `k` 表示第 *k* 个逻辑核心。每个 `cpu` 子目录又有单独的 `index`
    目录，指示该核心可用的缓存。
- en: 'The `index` directories contain files with numerous details about each logical
    core’s caches. The contents of a sample `index0` directory are shown here (`index0`
    typically corresponds to a Linux system’s L1 cache):'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '`index` 目录包含关于每个逻辑核心缓存的众多详细信息。一个示例 `index0` 目录的内容如下所示（`index0` 通常对应于 Linux
    系统的 L1 缓存）：'
- en: '[PRE55]'
  id: totrans-422
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'To discover the cache line size of the L1 cache, use this command:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 要发现 L1 缓存的缓存行大小，可以使用以下命令：
- en: '[PRE56]'
  id: totrans-424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: The output reveals that the L1 cache line size for the machine is 64 bytes.
    In other words, the 40-byte `counts` array fits *within one cache line*.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示机器的 L1 缓存行大小为 64 字节。换句话说，40 字节的 `counts` 数组完全可以*放在一个缓存行内*。
- en: Recall that with invalidating cache coherence protocols like MSI, every time
    a program updates a shared variable, the *entire cache line in other caches storing
    the variable is invalidated*. Let’s consider what happens when two threads execute
    the preceding function. One possible path of execution is shown in [Table 14-6](ch14.xhtml#ch14tab6)
    (assuming that each thread is assigned to a separate core, and the variable `x`
    is local to each thread).
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 请回忆，在像 MSI 这样的失效缓存一致性协议中，每当程序更新一个共享变量时，*存储该变量的其他缓存中整个缓存行都会被使无效*。让我们考虑当两个线程执行上述函数时会发生什么。可能的执行路径如
    [表 14-6](ch14.xhtml#ch14tab6) 所示（假设每个线程被分配到一个单独的核心，并且变量 `x` 对每个线程都是局部的）。
- en: '**Table 14-6:** A Possible Execution Sequence of Two Threads Running `countElems`'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 14-6：** 两个线程执行 `countElems` 的可能执行序列'
- en: '| **Time** | **Thread 0** | **Thread 1** |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| **时间** | **线程 0** | **线程 1** |'
- en: '| *i* | Reads `array[x]` | … |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| *i* | 读取 `array[x]` | … |'
- en: '|  | (1) |  |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '|  | (1) |  |'
- en: '| *i* + 1 | Increments `counts[1]` (**invalidates** | Reads `array[x]` (4)
    |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| *i* + 1 | 增加 `counts[1]` (**使无效** | 读取 `array[x]` (4) |'
- en: '|  | **cache line**) |  |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '|  | **缓存行**) |  |'
- en: '| *i* + 2 | Reads `array[x]` (6) | Increments `counts[4]` (**invalidates**
    |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '| *i* + 2 | 读取 `array[x]` (6) | 增加 `counts[4]` (**使无效** |'
- en: '|  |  | **cache line**) |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '|  |  | **缓存行**) |'
- en: '| *i* + 3 | Increments `counts[6]` (**invalidates** | Reads `array[x]` (2)
    |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '| *i* + 3 | 增加 `counts[6]` (**使无效** | 读取 `array[x]` (2) |'
- en: '|  | **cache line**) |  |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '|  | **缓存行**) |  |'
- en: '| *i* + 4 | Reads `array[x]` (3) | Increments `counts[2]` (**invalidates**
    |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| *i* + 4 | 读取 `array[x]` (3) | 增加 `counts[2]` (**使无效** |'
- en: '|  |  | **cache line**) |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '|  |  | **缓存行**) |'
- en: '| *i* + 5 | Increments `counts[3]` (**invalidates** | … |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| *i* + 5 | 增加 `counts[3]` (**使无效** | … |'
- en: '|  | **cache line**) |  |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '|  | **缓存行**) |  |'
- en: During time step *i*, Thread 0 reads the value at `array[x]` in its part of
    the array, which is a 1 in this example. During time steps *i* + 1 to *i* + 5,
    each thread reads a value from `array[x]`. Note that each thread is looking at
    different components of the array. Not only that, each read of `array` in our
    sample execution yields unique values (so no race conditions in this sample execution
    sequence!). After reading the value from `array[x]`, each thread increments the
    associated value in `counts`.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间步 *i* 中，线程 0 读取它自己部分的 `array[x]` 的值，在这个示例中是 1。在时间步 *i* + 1 到 *i* + 5 之间，每个线程都会读取一个
    `array[x]` 的值。请注意，每个线程查看的是数组的不同部分。不仅如此，我们的示例执行中，每次读取 `array` 都得到唯一的值（因此在这个示例执行序列中没有竞争条件！）。在读取
    `array[x]` 的值后，每个线程都会增加 `counts` 中相应的值。
- en: Recall that the `counts` array *fits on a single cache line* in our L1 cache.
    As a result, every write to `counts` invalidates the *entire line* in *every other
    L1 cache*. The end result is that, despite updating *different* memory locations
    in `counts`, any cache line containing `counts` is *invalidated* with *every update*
    to `counts`!
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，`counts` 数组*适配到一个单独的缓存行*，存储在我们的 L1 缓存中。因此，每次对 `counts` 的写操作都会使*每个其他 L1 缓存中的整行*失效。最终结果是，尽管更新了
    `counts` 中的*不同*内存位置，但任何包含 `counts` 的缓存行都会在对 `counts`的*每次更新*时被*失效*！
- en: The invalidation forces all L1 caches to update the line with a “valid” version
    from L2\. The repeated invalidation and overwriting of lines from the L1 cache
    is an example of *thrashing*, where repeated conflicts in the cache cause a series
    of misses.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存行的失效会迫使所有 L1 缓存更新该行，并从 L2 获取一个“有效”的版本。L1 缓存的反复失效和重写行是*抖动*的一个例子，其中缓存中的重复冲突导致一系列的未命中。
- en: The addition of more cores makes the problem worse, given that now more L1 caches
    are invalidating the line. As a result, adding additional threads slows down the
    runtime, despite the fact that each thread is accessing different elements of
    the `counts` array! This is an example of *false sharing*, or the illusion that
    individual elements are being shared by multiple cores. In the previous example,
    it appears that all the cores are accessing the same elements of `counts`, even
    though this is not the case.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 增加更多核心会使问题更加严重，因为现在更多的 L1 缓存会使缓存行失效。因此，尽管每个线程访问的是 `counts` 数组中不同的元素，但增加额外线程会使运行速度变慢！这是一个*伪共享*的例子，或者说是多个核心共享单个元素的错觉。在前一个例子中，所有核心似乎都在访问
    `counts` 中的相同元素，尽管事实并非如此。
- en: 14.5.3 Fixing False Sharing
  id: totrans-445
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.5.3 修复伪共享
- en: One way to fix an instance of false sharing is to pad the array (in our case
    `counts`) with additional elements so that it doesn’t fit in a single cache line.
    However, padding can waste memory, and may not eliminate the problem from all
    architectures (consider the scenario in which two different machines have different
    L1 cache sizes). In most cases, writing code to support different cache sizes
    is generally not worth the gain in performance.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 修复伪共享问题的一种方法是用额外的元素填充数组（在我们这个例子中是 `counts`），使其不再适配单个缓存行。然而，填充可能浪费内存，并且可能无法解决所有架构中的问题（考虑到两台不同机器的
    L1 缓存大小不同）。在大多数情况下，编写支持不同缓存大小的代码通常不值得为了性能提升而付出代价。
- en: A better solution is to have threads write to *local storage* whenever possible.
    Local storage in this context refers to memory that is *local* to a thread. The
    following solution reduces false sharing by choosing to perform updates to a locally
    declared version of `counts` called `local_counts`.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的解决方案是尽可能让线程写入*本地存储*。在这个上下文中，本地存储是指*仅限线程使用*的内存。以下解决方案通过选择对名为 `local_counts`
    的本地声明版本进行更新，从而减少伪共享。
- en: Let’s revisit the final version of our `countElems` function:⁶
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下 `countElems` 函数的最终版本：⁶
- en: '[PRE57]'
  id: totrans-449
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The use of `local_counts` to accumulate frequencies in lieu of `counts` is
    the major source of reduction of false sharing in this example:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `local_counts` 来累积频率，代替 `counts` 是本例中减少伪共享的主要原因：
- en: '[PRE58]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Since cache coherence is meant to maintain a consistent view of shared memory,
    the invalidations trigger only on *writes* to *shared values* in memory. Since
    `local_counts` is not shared among the different threads, a write to it will not
    invalidate its associated cache line.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 由于缓存一致性旨在保持共享内存的一致视图，只有对内存中的*共享值*进行*写操作*时，失效才会触发。由于 `local_counts` 在不同线程之间没有共享，因此对它的写操作不会使其关联的缓存行失效。
- en: 'In the last component of the code, the mutex enforces correctness by ensuring
    that only one thread updates the shared `counts` array at a time:'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码的最后一个部分，互斥锁通过确保一次只有一个线程更新共享的 `counts` 数组来保证正确性：
- en: '[PRE59]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Since `counts` is located on a single cache line, it will still get invalidated
    with every write. The difference is that the penalty here is at most `MAX` × *t*
    writes vs. *n* writes, where *n* is the length of our input array, and *t* is
    the number of threads employed.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 `counts` 位于单个缓存行中，每次写操作仍然会使其失效。不同之处在于，这里的惩罚最多是 `MAX` × *t* 次写操作，而不是 *n* 次写操作，其中
    *n* 是输入数组的长度，*t* 是使用的线程数。
- en: 14.6 Thread Safety
  id: totrans-456
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.6 线程安全
- en: So far, we have covered synchronization constructs that programmers can use
    to ensure that their multithreaded programs are consistent and correct regardless
    of the number of threads employed. However, it is not always safe to make the
    assumption that standard C library functions can be used “as is” in the context
    of any multithreaded application. Not all functions in the C library are *thread
    safe*, or capable of being run by multiple threads while guaranteeing a correct
    result without unintended side effects. To ensure that the programs *we* write
    are thread safe, it is important to use synchronization primitives like mutexes
    and barriers to enforce that multithreaded programs are consistent and correct
    regardless of how the number of threads varies.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了程序员可以使用的同步构造，以确保他们的多线程程序无论线程数量如何，都是一致且正确的。然而，在任何多线程应用程序的上下文中，并不总是安全地假设可以“直接”使用标准C库函数。并非所有C库中的函数都是*线程安全*的，或者说并不是所有函数都能在多个线程中运行时，保证正确的结果且不会产生意外的副作用。为了确保我们编写的程序是线程安全的，使用同步原语（如互斥锁和屏障）强制执行多线程程序的一致性和正确性是非常重要的，无论线程数量如何变化。
- en: Another closely related concept related to thread safety is re-entrancy. All
    thread safe code is re-entrant; however, not all re-entrant code is thread safe.
    A function is *re-entrant* if it can be re-executed/partially executed by a function
    without causing issue. By definition, re-entrant code ensures that accesses to
    the global state of a program always result in that global state remaining consistent.
    While re-entrancy is often (incorrectly) used as a synonym for thread safety,
    there are special cases for which re-entrant code is not thread safe.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 与线程安全密切相关的另一个概念是可重入性。所有线程安全的代码都是可重入的；然而，并非所有可重入的代码都是线程安全的。如果一个函数能够被另一个函数重新执行或部分执行而不会引发问题，则该函数是*可重入*的。按定义，可重入的代码确保对程序全局状态的访问始终保持全局状态的一致性。虽然可重入性常常（错误地）作为线程安全的同义词使用，但实际上有一些特殊情况，导致可重入的代码并非线程安全。
- en: When writing multithreaded code, verify that the C library functions used are
    indeed thread safe. Fortunately, the list of thread-unsafe C library functions
    is fairly small. The Open Group kindly maintains a list of thread unsafe functions.^([11](ch14.xhtml#fn14_11))
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写多线程代码时，需要验证所使用的C库函数是否确实是线程安全的。幸运的是，线程不安全的C库函数列表相对较小。Open Group友好地维护了一份线程不安全函数的列表。^([11](ch14.xhtml#fn14_11))
- en: 14.6.1 Fixing Issues of Thread Safety
  id: totrans-460
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.6.1 解决线程安全问题
- en: Synchronization primitives are the most common way to fix issues related to
    thread safety. However, unknowingly using thread-unsafe C library functions can
    cause subtle issues. Let’s look at a slightly modified version of our `countsElem`
    function called `countElemsStr`, which attempts to count the frequency of digits
    in a given string, where each digit is separated by spaces. The following program
    has been edited for brevity; the full source of this program is available online.^([12](ch14.xhtml#fn14_12))
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 同步原语是解决线程安全问题最常见的方法。然而，未知地使用线程不安全的C库函数可能会导致一些微妙的问题。让我们看一下稍微修改过的`countsElem`函数版本，称为`countElemsStr`，该版本尝试计算给定字符串中数字的频率，数字之间用空格分隔。以下程序已编辑为简洁版；该程序的完整源代码可以在线获得。^([12](ch14.xhtml#fn14_12))
- en: '[PRE60]'
  id: totrans-462
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: The `countElemsStr` function uses the `strtok` function (as examined in our
    discussion in “strtok, strtok_r” on [page 100](ch02.xhtml#lev3_22)) to parse each
    digit (stored in `token`) in the string, before converting it to an integer and
    making the associated updates in the `counts` array.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: '`countElemsStr`函数使用`strtok`函数（如我们在“strtok, strtok_r”部分讨论中所述，见[第100页](ch02.xhtml#lev3_22)）来解析字符串中的每个数字（存储在`token`中），然后将其转换为整数，并在`counts`数组中进行相关更新。'
- en: 'Compiling and running this program on 100,000 elements yields the following
    output:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 在100,000个元素上编译并运行该程序时，输出如下：
- en: '[PRE61]'
  id: totrans-465
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Now, let’s take a look at a multithreaded version of `countElemsStr`:^([13](ch14.xhtml#fn14_13))
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下`countElemsStr`的多线程版本：^([13](ch14.xhtml#fn14_13))
- en: '[PRE62]'
  id: totrans-467
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: In this version of the program, each thread processes a separate section of
    the string referenced by `input_str`. The `local_counts` array ensures that the
    bulk of the write operations occur to local storage. A mutex is employed to ensure
    that no two threads write to the shared variable `counts`.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个版本的程序中，每个线程处理由`input_str`引用的字符串的不同部分。`local_counts`数组确保大部分写操作发生在本地存储中。使用互斥锁来确保没有两个线程同时写入共享变量`counts`。
- en: 'However, compiling and running this program yields the following results:'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，编译并运行该程序会产生以下结果：
- en: '[PRE63]'
  id: totrans-470
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Even though mutex locks are used around accesses to the `counts` array, the
    results from separate runs are radically different. This issue arises because
    the `countsElemsStr` function is not thread safe, because the string library function
    `strtok` is *not thread safe*! Visiting the OpenGroup website^(11) confirms that
    `strtok` is on the list of thread-unsafe functions.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在访问 `counts` 数组时使用了互斥锁，但不同运行的结果仍然截然不同。这个问题的根源在于 `countsElemsStr` 函数不是线程安全的，因为字符串库函数
    `strtok` 是*不线程安全的*！访问 OpenGroup 网站^(11)可以确认 `strtok` 在不安全线程函数的列表中。
- en: To fix this issue, it suffices to replace `strtok` with its thread-safe alternative,
    `strtok_r`. In the latter function, a pointer is used as the last parameter to
    help the thread keep track of where in the string it is parsing. Here is the fixed
    function with `strtok_r`:^([14](ch14.xhtml#fn14_14))
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 为了修复这个问题，只需将 `strtok` 替换为其线程安全的替代函数 `strtok_r`。在后者函数中，使用一个指针作为最后一个参数，帮助线程跟踪当前正在解析的字符串位置。以下是使用
    `strtok_r` 的修复后的函数：^([14](ch14.xhtml#fn14_14))
- en: '[PRE64]'
  id: totrans-473
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'The only change in this version of the code is the declaration of the character
    pointer `saveptr` and replacing all instances of `strtok` with `strtok_r`. Rerunning
    the code with these changes yields the following output:'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 这个版本的代码唯一的变化是声明了字符指针 `saveptr`，并将所有 `strtok` 的实例替换为 `strtok_r`。在这些更改下重新运行代码，输出如下：
- en: '[PRE65]'
  id: totrans-475
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Now the program produces the same result for every run. The use of `saveptr`
    in conjunction with `strtok_r` ensures that each thread can independently track
    their location when parsing the string.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，程序每次运行都产生相同的结果。使用 `saveptr` 与 `strtok_r` 一起确保了每个线程都能独立跟踪它们在解析字符串时的位置。
- en: The takeaway from this section is that one should always check the list of thread-unsafe
    functions in C^(11) when writing multithreaded applications. Doing so can save
    the programmer a lot of heartache and frustration when writing and debugging threaded
    applications.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的要点是，在编写多线程应用程序时，应该始终检查 C 语言中的线程不安全函数列表^(11)。这样做可以帮助程序员避免在编写和调试多线程应用时遇到许多痛苦和挫折。
- en: 14.7 Implicit Threading with OpenMP
  id: totrans-478
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.7 使用 OpenMP 的隐式线程化
- en: Thus far, we have presented shared memory programming using POSIX threads. Although
    Pthreads are great for simple applications, they become increasingly difficult
    to use as programs themselves become more complex. POSIX threads are an example
    of *explicit parallel programming* of threads, requiring a programmer to specify
    exactly what each thread is required to do and when each thread should start and
    stop.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经介绍了使用 POSIX 线程进行共享内存编程。尽管 Pthreads 非常适合简单应用，但随着程序本身变得越来越复杂，它们的使用变得越来越困难。POSIX
    线程是*显式并行编程*的一个例子，需要程序员明确指定每个线程要做什么，以及每个线程何时开始和停止。
- en: With Pthreads, it can also be challenging to *incrementally* add parallelism
    to an existing sequential program. That is, one must often rewrite the program
    entirely to use threads, which is often not desirable when attempting to parallelize
    a large, existing codebase.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Pthreads 时，将并行性*渐进式*地添加到现有的顺序程序中也是一个挑战。也就是说，通常需要彻底重写程序以使用线程，而在尝试并行化一个庞大的现有代码库时，这往往是不太理想的做法。
- en: The Open Multiprocessing (OpenMP) library implements an *implicit* alternative
    to Pthreads. OpenMP is built in to GCC and other popular compilers such as LLVM
    and Clang, and can be used with the C, C++, and Fortran programming languages.
    A key advantage of OpenMP is that it enables programmers to parallelize components
    of existing, sequential C code by adding *pragmas* (special compiler directives)
    to parts of the code. Pragmas specific to OpenMP begin with `#pragma omp`.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: Open Multiprocessing（OpenMP）库实现了一个*隐式*的 Pthreads 替代方案。OpenMP 内置于 GCC 和其他流行的编译器（如
    LLVM 和 Clang）中，并可以与 C、C++ 和 Fortran 编程语言一起使用。OpenMP 的一个关键优势是，它使程序员能够通过向现有的顺序 C
    代码中添加*指令*（特殊的编译器指令）来实现并行化。专门用于 OpenMP 的指令以 `#pragma omp` 开头。
- en: Detailed coverage of OpenMP is outside the scope of this book, but we do cover
    some common pragmas and show how several can be used in the context of some sample
    applications.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 本书并未详细介绍 OpenMP，但我们涵盖了一些常见的指令，并展示了如何在一些示例应用的上下文中使用它们。
- en: 14.7.1 Common Pragmas
  id: totrans-483
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.7.1 常用的指令
- en: 'Here are some of the most commonly used pragmas in OpenMP programs:'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 OpenMP 程序中最常用的一些指令：
- en: '#pragma omp parallel  This pragma creates a team of threads and has each thread
    run the code in its scope (usually a function call) on each thread. An invocation
    of this pragma is usually equivalent to an invocation of the `pthread_create`
    and `pthread_join` function pairing discussed in “Creating and Joining Threads”
    on [page 679](ch14.xhtml#lev2_236). The pragma may have a number of clauses, including
    the following:'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: '#pragma omp parallel  此指令创建一个线程组，并让每个线程在其作用域内（通常是一个函数调用）执行代码。调用该指令通常等同于在《创建和加入线程》的[第679页](ch14.xhtml#lev2_236)中讨论的
    `pthread_create` 和 `pthread_join` 函数组合。该指令可以有多个子句，包括以下内容：'
- en: num_threads  Specifies the number of threads to create.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: num_threads  指定要创建的线程数。
- en: private  A list of variables that should be private (or local) to each thread.
    Variables that should be private to a thread can also be declared within the scope
    of the pragma (see below for an example). Each thread gets its own copy of each
    variable.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: private  一个变量列表，指定这些变量应该是每个线程私有的（或局部的）。那些应该是线程私有的变量也可以在指令的作用域内声明（参见下文示例）。每个线程都获得每个变量的一个副本。
- en: shared  A listing of variables that should be shared among the threads. There
    is one copy of the variable that is shared among all threads.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: shared  一个变量列表，指定这些变量应该在所有线程之间共享。该变量有一个副本，所有线程共享。
- en: default  Indicates whether the determination of which variables should be shared
    is left up to the compiler. In most cases, we want to use `default(none)` and
    specify explicitly which variables should be shared and which should be private.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: default  指示是否让编译器决定哪些变量应当共享。在大多数情况下，我们希望使用 `default(none)` 并明确指定哪些变量应该共享，哪些应该是私有的。
- en: '#pragma omp for  Specifies that each thread execute a subset of iterations
    of a `for` loop. Although the scheduling of the loops is up to the system, the
    default is usually the “chunking” method first discussed in “Revisiting Scalar
    Multiplication” on [page 682](ch14.xhtml#lev2_239). This is a *static* form of
    scheduling: each thread gets an assigned chunk, and then processes the iterations
    in its chunk. However, OpenMP also makes *dynamic* scheduling easy. In dynamic
    scheduling, each thread gets a number of iterations, and requests a new set upon
    completing processing their iteration. The scheduling policy can be set using
    the following clause:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: '#pragma omp for  指定每个线程执行 `for` 循环的一个子集。虽然循环的调度由系统决定，默认方法通常是“块化”方法，这在《重新审视标量乘法》的[第682页](ch14.xhtml#lev2_239)中首次讨论。这是一种*静态*调度形式：每个线程得到一个指定的块，然后处理该块中的迭代。然而，OpenMP
    也使得*动态*调度变得容易。在动态调度中，每个线程得到一定数量的迭代，并在处理完自己的迭代后请求一个新的迭代集。调度策略可以使用以下子句设置：'
- en: schedule(dynamic)  Specifies that a *dynamic* form of scheduling should be used.
    While this is advantageous in some cases, the static (default) form of scheduling
    is usually faster.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: schedule(dynamic)  指定应使用*动态*调度方式。虽然在某些情况下这样做有优势，但静态（默认）调度方式通常更快。
- en: '#pragma omp parallel for  This pragma is a combination of the `omp parallel`
    and the `omp for` pragmas. Unlike the `omp for` pragma, the `omp parallel for`
    pragma also generates a team of threads before assigning each thread a set of
    iterations of the loop.'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: '#pragma omp parallel for  此指令是 `omp parallel` 和 `omp for` 指令的组合。与 `omp for`
    指令不同，`omp parallel for` 指令在分配每个线程一组循环迭代之前，还会生成一个线程组。'
- en: '#pragma omp critical  This pragma is used to specify that the code under its
    scope should be treated as a *critical section*—that is, only one thread should
    execute the section of code at a time to ensure correct behavior.'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: '#pragma omp critical  此指令用于指定其作用域内的代码应该作为*临界区*处理——即，只有一个线程可以在任何时刻执行该段代码，以确保正确的行为。'
- en: 'There are also several *functions* that a thread can access that are often
    useful for execution. For example:'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些*函数*，线程可以访问它们，这些函数在执行时通常非常有用。例如：
- en: omp_get_num_threads  Returns the number of threads in the current team that
    is being executed.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: omp_get_num_threads  返回当前线程组中正在执行的线程数。
- en: omp_set_num_threads  Sets the number of threads that a team should have.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: omp_set_num_threads  设置一个线程组应该拥有的线程数。
- en: omp_get_thread_num  Returns the identifier of the calling thread.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: omp_get_thread_num  返回调用线程的标识符。
- en: '**Warning THE OMP PARALLEL FOR DIRECTIVE WORKS ONLY WITH FOR LOOPS!**'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: '**警告：OMP PARALLEL FOR 指令仅适用于 FOR 循环！**'
- en: Keep in mind that the `omp parallel for` pragma works *only* with `for` loops.
    Other types of loops, such as `while` loops and `do`–`while` loops, are not supported.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，`omp parallel for` 指令*仅*对 `for` 循环有效。其他类型的循环，如 `while` 循环和 `do`–`while`
    循环不受支持。
- en: '14.7.2 Hello Threading: OpenMP Flavored'
  id: totrans-500
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.7.2 Hello 线程：OpenMP 风格
- en: 'Let’s revisit our “Hello World” program,² now using OpenMP instead of Pthreads:'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新审视我们的“Hello World”程序²，现在使用 OpenMP 代替 Pthreads：
- en: '[PRE66]'
  id: totrans-502
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Note that the OpenMP program is *much* shorter than the Pthreads version. To
    access the OpenMP library functions, we include the header file `omp.h`. The `omp
    parallel num_threads(nthreads)` pragma in `main` creates a set of threads, where
    each thread calls the `HelloWorld` function. The clause `num _threads(nthreads)`
    specifies that a total of `nthreads` should be generated. The pragma also joins
    each created thread back to a single-threaded process. In other words, all the
    low-level work of creating and joining threads is *abstracted* away from the programmer
    and is accomplished with the inclusion of just one pragma. For this reason, OpenMP
    is considered an *implicit threading* library.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，OpenMP 程序比 Pthreads 版本*短得多*。为了访问 OpenMP 库函数，我们包含了头文件 `omp.h`。`main` 中的 `omp
    parallel num_threads(nthreads)` 指令创建了一组线程，每个线程都会调用 `HelloWorld` 函数。`num_threads(nthreads)`
    子句指定应该生成总共 `nthreads` 个线程。该指令还将每个创建的线程重新合并回单线程进程。换句话说，所有的低级线程创建和合并工作都被*抽象*化，程序员只需包含一个指令即可完成。因此，OpenMP
    被认为是一个*隐式线程*库。
- en: OpenMP also abstracts away the need to explicitly manage thread IDs. In the
    context of `HelloWorld`, the `omp_get_thread_num` function extracts the unique
    ID associated with the thread that is running it.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: OpenMP 还抽象化了显式管理线程 ID 的需求。在 `HelloWorld` 的上下文中，`omp_get_thread_num` 函数提取与正在运行该线程相关的唯一
    ID。
- en: Compiling the code
  id: totrans-505
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 编译代码
- en: 'Let’s compile and run this program by passing the `-fopenmp` flag to the compiler,
    which signals that we’re compiling with OpenMP:'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过将 `-fopenmp` 标志传递给编译器来编译并运行此程序，这会告诉编译器我们正在使用 OpenMP 进行编译：
- en: '[PRE67]'
  id: totrans-507
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Since the execution of threads can change with subsequent runs, rerunning this
    program results in a different sequence of messages:'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 由于线程的执行顺序可能会随着后续运行而改变，重新运行该程序会产生不同的消息顺序：
- en: '[PRE68]'
  id: totrans-509
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: This behavior is consistent with our example with Pthreads (see “Hello Threading!
    Writing Your First Multithreaded Program” on [page 677](ch14.xhtml#lev1_106)).
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 这种行为与我们在 Pthreads 中的示例一致（见“Hello 线程！编写你的第一个多线程程序”在 [第 677 页](ch14.xhtml#lev1_106)）。
- en: '14.7.3 A More Complex Example: CountSort in OpenMP'
  id: totrans-511
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.7.3 一个更复杂的例子：在 OpenMP 中的 CountSort
- en: 'A powerful advantage of OpenMP is that it enables programmers to incrementally
    parallelize their code. To see this in action, let’s parallelize the more complex
    CountSort algorithm discussed earlier in this chapter. Recall that this algorithm
    sorts arrays containing a small range of values. The main function of the serial
    program³ looks like the following:'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: OpenMP 的一个强大优势是它使程序员能够逐步将代码并行化。为了看到这一点，让我们并行化本章前面讨论的更复杂的 CountSort 算法。回想一下，该算法对包含小范围值的数组进行排序。串行程序的主函数³如下所示：
- en: '[PRE69]'
  id: totrans-513
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: The `main` function, after doing some command line parsing and generating a
    random array, calls the `countsElems` function followed by the `writeArray` function.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: '`main` 函数在进行一些命令行解析和生成随机数组后，调用 `countElems` 函数，然后是 `writeArray` 函数。'
- en: Parallelizing CountElems Using OpenMP
  id: totrans-515
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用 OpenMP 并行化 CountElems
- en: There are several ways to parallelize the preceding program. One way (shown
    in the example that follows) uses the `omp parallel` pragma in the context of
    the `countElems` and `writeArray` functions. As a result, no changes need to be
    made to the `main` function.^([15](ch14.xhtml#fn14_15))
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以并行化上述程序。一种方法（如下例所示）是在 `countElems` 和 `writeArray` 函数的上下文中使用 `omp parallel`
    指令。因此，无需对 `main` 函数进行更改。^([15](ch14.xhtml#fn14_15))
- en: 'First, let’s examine how to parallelize the `countElems` function using OpenMP:'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们来看看如何使用 OpenMP 对 `countElems` 函数进行并行化：
- en: '[PRE70]'
  id: totrans-518
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: In this version of the code, three pragmas are employed. The `#pragma omp parallel`
    pragma indicates that a team of threads should be created. The `omp_set_num_threads(nthreads)`
    line in `main` sets the default size of the thread team to be `nthreads`. If the
    `omp_set_num_threads` function is not used, then the number of threads assigned
    will equal the number of cores in the system. As a reminder, the `omp parallel`
    pragma implicitly creates threads at the beginning of the block and joins them
    at the end of the block. Braces (`{}`) are used to specify scope. The `shared`
    clause declares that the variables `counts`, `array`, and `length` are shared
    (global) among all the threads. Thus, the variables `val`, `i`, and `local[MAX]`
    are declared *locally* in each thread.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 在这版本的代码中，使用了三个指令。`#pragma omp parallel` 指令表示应创建一个线程组。`main` 中的 `omp_set_num_threads(nthreads)`
    语句设置线程组的默认大小为 `nthreads`。如果没有使用 `omp_set_num_threads` 函数，则分配的线程数将等于系统中的核心数。提醒一下，`omp
    parallel` 指令在块的开始时隐式创建线程，在块结束时合并线程。大括号 (`{}`) 用于指定作用域。`shared` 子句声明 `counts`、`array`
    和 `length` 变量在所有线程之间是共享的（全局的）。因此，变量 `val`、`i` 和 `local[MAX]` 在每个线程中被声明为*局部*变量。
- en: The next pragma is `#pragma omp for`, which parallelizes the `for` loop, splitting
    the number of iterations among the number of threads. OpenMP calculates how best
    to split up the iterations of the loop. As previously mentioned, the default strategy
    is usually a chunking method, wherein each thread gets roughly the same number
    of iterations to compute. Thus, each thread reads a component of the shared array
    `array`, and accumulates its counts in its local array `local`.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个指令是 `#pragma omp for`，它将并行化 `for` 循环，将迭代次数分配到各个线程之间。OpenMP 会计算如何最好地拆分循环的迭代次数。如前所述，默认策略通常是分块方法，即每个线程计算大致相同数量的迭代次数。因此，每个线程读取共享数组
    `array` 的一个组件，并将其计数累加到本地数组 `local` 中。
- en: The `#pragma omp critical` pragma indicates that the code in the scope of the
    critical section should be executed by exactly one thread at a time. This is equivalent
    to the mutex that was employed in the Pthreads version of this program. Here,
    each thread increments the shared `counts` array one at a time.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: '`#pragma omp critical` 指令表示在临界区范围内的代码应该由每次只有一个线程执行。这相当于在这个程序的 Pthreads 版本中使用的互斥锁。在这里，每个线程一次只递增共享的
    `counts` 数组。'
- en: 'Let’s get a sense of the performance of this function by running it with 100
    million elements:'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过使用 1 亿个元素来测试这个函数的性能：
- en: '[PRE71]'
  id: totrans-523
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: This is excellent performance, with our function getting a speedup of 2 on two
    threads, and a speedup of 3.63 on four threads. We get even better performance
    than the Pthreads implementation!
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个优秀的性能表现，我们的函数在两个线程上加速了 2 倍，在四个线程上加速了 3.63 倍。我们甚至获得了比 Pthreads 实现更好的性能！
- en: The writeArray Function in OpenMP
  id: totrans-525
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: OpenMP 中的 writeArray 函数
- en: 'Parallelizing the `writeArray` function is *much* harder. The following code
    shows one possible solution:'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `writeArray` 函数并行化要*困难*得多。下面的代码展示了一个可能的解决方案：
- en: '[PRE72]'
  id: totrans-527
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Prior to parallelizing, we made a change to this function because the old version
    of `writeArray` caused `j` to have a dependency on the previous iterations of
    the loop. In this version, each thread calculates its unique `start` value based
    on the sum of all the previous elements in `counts`.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 在并行化之前，我们对这个函数进行了修改，因为旧版本的 `writeArray` 使得 `j` 对循环的前几次迭代产生了依赖关系。在这个版本中，每个线程根据
    `counts` 中所有前面元素的总和来计算其独特的 `start` 值。
- en: When this dependency is removed, the parallelization is pretty straightforward.
    The `#pragma omp parallel for` pragma generates a team of threads and parallelizes
    the `for` loop by assigning each thread a subset of the iterations of the loop.
    As a reminder, this pragma is a combination of the `omp parallel` and the `omp
    for` pragmas (which were used in the parallelization of `countElems`).
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 当移除这个依赖关系后，并行化就变得非常直接。`#pragma omp parallel for` 指令创建了一个线程组，并通过为每个线程分配循环迭代的子集来并行化
    `for` 循环。提醒一下，这个指令是 `omp parallel` 和 `omp for` 指令的结合（它们曾被用于 `countElems` 的并行化）。
- en: A chunking approach to scheduling threads (as shown in the earlier `countElems`
    function) is not appropriate here, because it is possible that each element in
    `counts` has a radically different frequency. Therefore, the threads will not
    have equal work, resulting in some threads being assigned more work than others.
    Therefore, the `schedule(dynamic)` clause is employed, so that each thread completes
    the iteration it is assigned before requesting a new iteration from the thread
    manager.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 一种用于调度线程的分块方法（如前面的`countElems`函数所示）在这里并不合适，因为`counts`中的每个元素可能具有完全不同的频率。因此，线程之间的工作量不均等，导致一些线程被分配的工作量比其他线程更多。因此，采用了`schedule(dynamic)`子句，以便每个线程在向线程管理器请求新的迭代之前，先完成其分配的迭代任务。
- en: Since each thread is writing to distinct array locations, mutual exclusion is
    not needed for this function.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个线程都写入不同的数组位置，因此该函数不需要互斥。
- en: Notice how much cleaner the OpenMP code is than the POSIX thread implementation.
    The code is very readable and required very little modification. This is one of
    the powers of *abstraction*, in which the implementation details are hidden from
    the programmer.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，OpenMP代码比POSIX线程实现要简洁得多。代码非常易读，并且只需要很少的修改。这就是*抽象*的力量，在抽象中，程序员不需要关注实现细节。
- en: However, a necessary trade-off for abstraction is control. The programmer assumes
    that the compiler is “smart” enough to take care of the particulars of parallelization
    and thus has an easier time parallelizing their application. However, the programmer
    no longer makes detailed decisions about the particulars of that parallelization.
    Without a clear idea of how OpenMP pragmas execute under the hood, it can be difficult
    to debug an OpenMP application or know which pragma is the most appropriate to
    use at a given time.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，抽象的必要权衡是控制。程序员假设编译器足够“智能”以处理并行化的细节，从而使得并行化应用变得更加轻松。然而，程序员不再对并行化的具体细节做出详细决策。如果不了解OpenMP指令在幕后是如何执行的，调试OpenMP应用或确定何时使用最合适的指令可能会变得困难。
- en: 14.7.4 Learning More About OpenMP
  id: totrans-534
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.7.4 进一步了解OpenMP
- en: A deeper discussion of OpenMP is beyond the scope of this book, but there are
    useful free resources for learning^([16](ch14.xhtml#fn14_16)) and using^([17](ch14.xhtml#fn14_17))
    OpenMP.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的范围之外是对OpenMP的深入讨论，但有一些有用的免费资源可以帮助学习^([16](ch14.xhtml#fn14_16))和使用^([17](ch14.xhtml#fn14_17))
    OpenMP。
- en: 14.8 Summary
  id: totrans-536
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.8 总结
- en: This chapter provided an overview of multicore processors and how to program
    them. Specifically, we cover the POSIX threads (or Pthreads) library and how to
    use it to create correct multithreaded programs that speed up a single-threaded
    program’s performance. Libraries like POSIX and OpenMP utilize the *shared memory*
    model of communication, as threads share data in a common memory space.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 本章概述了多核处理器及其编程方法。具体来说，我们介绍了POSIX线程（或Pthreads）库，以及如何使用它来创建正确的多线程程序，从而加速单线程程序的性能。像POSIX和OpenMP这样的库利用了*共享内存*通信模型，因为线程在一个公共内存空间中共享数据。
- en: Key Takeaways
  id: totrans-538
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 关键要点
- en: '**Threads are the fundamental unit of concurrent programs.**   To parallelize
    a serial program, programmers utilize lightweight constructs known as *threads*.
    For a particular multithreaded process, each thread has its own allocation of
    stack memory, but shares the program data, heap and instructions of the process.
    Like processes, threads run *nondeterministically* on the CPU (i.e., the order
    of execution changes between runs, and which thread is assigned to which core
    is left up to the operating system).'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: '**线程是并发程序的基本单元。** 为了将串行程序并行化，程序员利用一种被称为*线程*的轻量级构造。对于特定的多线程进程，每个线程都有自己的堆栈内存分配，但共享该进程的程序数据、堆和指令。与进程一样，线程在CPU上*非确定性地*运行（即，执行顺序在每次运行时都会变化，且哪个线程被分配到哪个核心由操作系统决定）。'
- en: '**Synchronization constructs ensure that programs work correctly.**   A consequence
    of shared memory is that threads can accidentally overwrite data residing in shared
    memory. A *race condition* can occur whenever two operations incorrectly update
    a shared value. When that shared value is data, a special type of race condition
    called a *data race* can arise. Synchronization constructs (mutexes, semaphores,
    etc.) help to guarantee program correctness by ensuring that threads execute one
    at a time when updating shared variables.'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: '**同步构造确保程序正确运行。** 共享内存的一个后果是线程可能会不小心覆盖共享内存中的数据。每当两个操作错误地更新一个共享值时，就可能发生*竞态条件*。当这个共享值是数据时，一种特殊类型的竞态条件——*数据竞争*——就会出现。同步构造（互斥锁、信号量等）通过确保线程在更新共享变量时逐个执行，从而帮助保证程序的正确性。'
- en: '**Be mindful when using synchronization constructs.**   Synchronization inherently
    introduces points of serial computation in an otherwise parallel program. It is
    therefore important to be aware of *how* one uses synchronization concepts. The
    set of operations that must run atomically is referred to as a *critical section*.
    If a critical section is too big, the threads will execute serially, yielding
    no improvement in runtime. Use synchronization constructs sloppily, and situations
    like *deadlock* may inadvertently arise. A good strategy is to have threads employ
    local variables as much as possible and update shared variables only when necessary.'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用同步构造时要小心。** 同步本质上会在一个本应并行的程序中引入串行计算的点。因此，了解如何使用同步概念非常重要。必须原子性执行的一组操作称为*临界区*。如果临界区过大，线程将串行执行，无法提高运行时性能。若不小心使用同步构造，可能会无意中导致像*死锁*这样的情况发生。一个好的策略是尽可能让线程使用局部变量，仅在必要时更新共享变量。'
- en: '**Not all components of a program are parallelizable.**   Some programs necessarily
    have large serial components that can hinder a multithreaded program’s performance
    on multiple cores (e.g., *Amdahl’s Law*). Even when a high percentage of a program
    is parallelizable, speedup is rarely linear. Readers are also encouraged to look
    at other metrics such as efficiency and scalability when ascertaining the performance
    of their programs.'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: '**并非所有程序的组件都可以并行化。** 一些程序必然包含大量的串行部分，这会影响多核环境下多线程程序的性能（例如，*阿姆达尔定律*）。即使一个程序的大部分是可以并行化的，加速也很少是线性的。读者还被鼓励在评估程序性能时，参考其他指标，如效率和可扩展性。'
- en: Further Reading
  id: totrans-543
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: This chapter is meant to give a taste of concurrency topics with threads; it
    is by no means exhaustive. To learn more about programming with POSIX threads
    and OpenMP, check out the excellent tutorials on Pthreads^([18](ch14.xhtml#fn14_18))
    and OpenMP^([19](ch14.xhtml#fn14_19)) by Blaise Barney from Lawrence Livermore
    National Labs. For automated tools for debugging parallel programs, readers are
    encouraged to check out the Helgrind^([20](ch14.xhtml#fn14_20)) and DRD^([21](ch14.xhtml#fn14_21))
    Valgrind tools.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 本章旨在提供并发话题的概览，使用线程作为示例；内容并不详尽。要深入了解使用POSIX线程和OpenMP的编程，建议查阅Blaise Barney在劳伦斯·利弗莫尔国家实验室提供的关于Pthreads^([18](ch14.xhtml#fn14_18))和OpenMP^([19](ch14.xhtml#fn14_19))的优秀教程。对于并行程序调试的自动化工具，读者可以查阅Helgrind^([20](ch14.xhtml#fn14_20))和DRD^([21](ch14.xhtml#fn14_21))
    Valgrind工具。
- en: In the final chapter of the book, we give a high-level overview of other common
    parallel architectures and how to program them.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的最后一章，我们将提供其他常见并行架构的高层次概述，以及如何编程实现这些架构。
- en: Notes
  id: totrans-546
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意事项
- en: '[1.](ch14.xhtml#rfn14_1) *[https://www.raspberrypi.org/](https://www.raspberrypi.org/)*'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: '[1.](ch14.xhtml#rfn14_1) *[https://www.raspberrypi.org/](https://www.raspberrypi.org/)*'
- en: '[2.](ch14.xhtml#rfn14_2) Available at *[https://diveintosystems.org/book/C14-SharedMemory/_attachments/hellothreads.c](https://diveintosystems.org/book/C14-SharedMemory/_attachments/hellothreads.c)*.'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: '[2.](ch14.xhtml#rfn14_2) 可在 *[https://diveintosystems.org/book/C14-SharedMemory/_attachments/hellothreads.c](https://diveintosystems.org/book/C14-SharedMemory/_attachments/hellothreads.c)*
    上获得。'
- en: '[3.](ch14.xhtml#rfn14_3) Available at *[https://diveintosystems.org/book/C14-SharedMemory/_attachments/countSort.c](https://diveintosystems.org/book/C14-SharedMemory/_attachments/countSort.c)*.'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: '[3.](ch14.xhtml#rfn14_3) 可在 *[https://diveintosystems.org/book/C14-SharedMemory/_attachments/countSort.c](https://diveintosystems.org/book/C14-SharedMemory/_attachments/countSort.c)*
    上获得。'
- en: '[4.](ch14.xhtml#rfn14_4) Available at *[https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElems_p.c](https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElems_p.c)*.'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: '[4.](ch14.xhtml#rfn14_4) 可在 *[https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElems_p.c](https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElems_p.c)*
    上获得。'
- en: '[5.](ch14.xhtml#rfn14_5) The full source can be downloaded from *[https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElems_p_v2.c](https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElems_p_v2.c)*.'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: '[5.](ch14.xhtml#rfn14_5) 完整源代码可从 *[https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElems_p_v2.c](https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElems_p_v2.c)*
    下载。'
- en: '[6.](ch14.xhtml#rfn14_6) The full source code for this final program can be
    accessed at *[https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElems_p_v3.c](https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElems_p_v3.c)*.'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: '[6.](ch14.xhtml#rfn14_6) 此最终程序的完整源代码可以在 *[https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElems_p_v3.c](https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElems_p_v3.c)*
    获取。'
- en: '[7.](ch14.xhtml#rfn14_7) Available at *[https://diveintosystems.org/book/C14-SharedMemory/_attachments/layeggs.c](https://diveintosystems.org/book/C14-SharedMemory/_attachments/layeggs.c)*.'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: '[7.](ch14.xhtml#rfn14_7) 可在 *[https://diveintosystems.org/book/C14-SharedMemory/_attachments/layeggs.c](https://diveintosystems.org/book/C14-SharedMemory/_attachments/layeggs.c)*
    获取。'
- en: '[8.](ch14.xhtml#rfn14_8) Gene Amdahl. “Validity of the single processor approach
    to achieving large scale computing capabilities,” *Proceedings of the April 18-20,
    1967, Spring Joint Computer Conference*, pp. 483–485, ACM, 1967.'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: '[8.](ch14.xhtml#rfn14_8) Gene Amdahl. “单处理器方法在实现大规模计算能力中的有效性，” *《1967年4月18-20日春季联合计算机会议论文集》*，第483–485页，ACM，1967年。'
- en: '[9.](ch14.xhtml#rfn14_9) John Gustafson, “Reevaluating Amdahl’s law,” *Communications
    of the ACM* 31(5), pp. 532–533, 1988.'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: '[9.](ch14.xhtml#rfn14_9) John Gustafson, “重新评估阿姆达尔定律，” *《ACM通讯》* 31(5)，第532–533页，1988年。'
- en: '[10.](ch14.xhtml#rfn14_10) Caroline Connor, “Movers and Shakers in HPC: John
    Gustafson,” *HPC Wire*, *[http://www.hpcwire.com/hpcwire/2010-10-20/movers_and_shakers_in_hpc_john_gustafson.html](http://www.hpcwire.com/hpcwire/2010-10-20/movers_and_shakers_in_hpc_john_gustafson.html)*.'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: '[10.](ch14.xhtml#rfn14_10) Caroline Connor, “HPC领域的推动者与变革者：John Gustafson，”
    *HPC Wire*，*[http://www.hpcwire.com/hpcwire/2010-10-20/movers_and_shakers_in_hpc_john_gustafson.html](http://www.hpcwire.com/hpcwire/2010-10-20/movers_and_shakers_in_hpc_john_gustafson.html)*。'
- en: '[11.](ch14.xhtml#rfn14_11) *[http://pubs.opengroup.org/onlinepubs/009695399/functions/xsh_chap02_09.html](http://pubs.opengroup.org/onlinepubs/009695399/functions/xsh_chap02_09.html)*'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: '[11.](ch14.xhtml#rfn14_11) *[http://pubs.opengroup.org/onlinepubs/009695399/functions/xsh_chap02_09.html](http://pubs.opengroup.org/onlinepubs/009695399/functions/xsh_chap02_09.html)*'
- en: '[12.](ch14.xhtml#rfn14_12) [https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElemsStr.c](https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElemsStr.c)'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: '[12.](ch14.xhtml#rfn14_12) [https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElemsStr.c](https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElemsStr.c)'
- en: '[13.](ch14.xhtml#rfn14_13) Available at *[https://diveintosystems.org/book/C14-SharedMemory/_attachmentscountElemsStr_p.c](https://diveintosystems.org/book/C14-SharedMemory/_attachmentscountElemsStr_p.c)*.'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: '[13.](ch14.xhtml#rfn14_13) 可在 *[https://diveintosystems.org/book/C14-SharedMemory/_attachmentscountElemsStr_p.c](https://diveintosystems.org/book/C14-SharedMemory/_attachmentscountElemsStr_p.c)*
    获取。'
- en: '[14.](ch14.xhtml#rfn14_14) Full source code available at [https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElemsStr_p_v2.c](https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElemsStr_p_v2.c).'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: '[14.](ch14.xhtml#rfn14_14) 完整源代码可以在 [https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElemsStr_p_v2.c](https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElemsStr_p_v2.c)
    获取。'
- en: '[15.](ch14.xhtml#rfn14_15) A full version of the program is available at *[https://diveintosystems.org/book/C14-SharedMemory/_attachments/countSort_mp.c](https://diveintosystems.org/book/C14-SharedMemory/_attachments/countSort_mp.c)*.'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: '[15.](ch14.xhtml#rfn14_15) 程序的完整版本可以在 *[https://diveintosystems.org/book/C14-SharedMemory/_attachments/countSort_mp.c](https://diveintosystems.org/book/C14-SharedMemory/_attachments/countSort_mp.c)*
    获取。'
- en: '[16.](ch14.xhtml#rfn14_16) Blaise Barney, “OpenMP,” [https://hpc.llnl.gov/tuts/openMP/](https://hpc.llnl.gov/tuts/openMP/)'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: '[16.](ch14.xhtml#rfn14_16) Blaise Barney, “OpenMP,” [https://hpc.llnl.gov/tuts/openMP/](https://hpc.llnl.gov/tuts/openMP/)'
- en: '[17.](ch14.xhtml#rfn14_17) Richard Brown and Libby Shoop, “Multicore Programming
    with OpenMP,” *CSinParallel: Parallel Computing in the Computer Science Curriculum*,
    [http://selkie.macalester.edu/csinparallel/modules/MulticoreProgramming/build/html/index.html](http://selkie.macalester.edu/csinparallel/modules/MulticoreProgramming/build/html/index.html)'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: '[17.](ch14.xhtml#rfn14_17) Richard Brown 和 Libby Shoop, “使用 OpenMP 进行多核编程，”
    *《CSinParallel: 计算机科学课程中的并行计算》*，[http://selkie.macalester.edu/csinparallel/modules/MulticoreProgramming/build/html/index.html](http://selkie.macalester.edu/csinparallel/modules/MulticoreProgramming/build/html/index.html)'
- en: '[18.](ch14.xhtml#rfn14_18) *[https://hpc-tutorials.llnl.gov/posix/](https://hpc-tutorials.llnl.gov/posix/)*'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: '[18.](ch14.xhtml#rfn14_18) *[https://hpc-tutorials.llnl.gov/posix/](https://hpc-tutorials.llnl.gov/posix/)*'
- en: '[19.](ch14.xhtml#rfn14_19) *[https://hpc.llnl.gov/tuts/openMP/](https://hpc.llnl.gov/tuts/openMP/)*'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: '[19.](ch14.xhtml#rfn14_19) *[https://hpc.llnl.gov/tuts/openMP/](https://hpc.llnl.gov/tuts/openMP/)*'
- en: '[20.](ch14.xhtml#rfn14_20) *[https://valgrind.org/docs/manual/hg-manual.html](https://valgrind.org/docs/manual/hg-manual.html)*'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: '[20.](ch14.xhtml#rfn14_20) *[https://valgrind.org/docs/manual/hg-manual.html](https://valgrind.org/docs/manual/hg-manual.html)*'
- en: '[21.](ch14.xhtml#rfn14_21) *[https://valgrind.org/docs/manual/drd-manual.html](https://valgrind.org/docs/manual/drd-manual.html)*'
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: '[21.](ch14.xhtml#rfn14_21) *[https://valgrind.org/docs/manual/drd-manual.html](https://valgrind.org/docs/manual/drd-manual.html)*'
