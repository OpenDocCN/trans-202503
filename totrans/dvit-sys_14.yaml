- en: '14'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LEVERAGING SHARED MEMORY IN THE MULTICORE ERA
  prefs: []
  type: TYPE_NORMAL
- en: '*The world is changed.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*I feel it in the silica.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*I feel it in the transistor.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*I see it in the core.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*–With apologies to Galadriel*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Lord of the Rings: Fellowship of the Ring'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Until now, our discussion of architecture has focused on a purely single-CPU
    world. But the world has changed. Today’s CPUs have multiple *cores*, or compute
    units. In this chapter, we discuss multicore architectures, and how to leverage
    them to speed up the execution of programs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note CPUS, PROCESSORS, AND CORES**'
  prefs: []
  type: TYPE_NORMAL
- en: 'In many instances in this chapter, the terms *processor* and *CPU* are used
    interchangeably. At a fundamental level, a *processor* is any circuit that performs
    some computation on external data. Based on this definition, the *central processing
    unit* (CPU) is an example of a processor. A processor or a CPU with multiple compute
    cores is referred to as a *multicore processor* or a *multicore CPU*. A *core*
    is a compute unit that contains many of the components that make up the classical
    CPU: an ALU, registers, and a bit of cache. Although a *core* is different from
    a processor, it is not unusual to see these terms used interchangeably in the
    literature (especially if the literature originated at a time when multicore processors
    were still considered novel).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In 1965, the founder of Intel, Gordon Moore, estimated that the number of transistors
    in an integrated circuit would double every year. His prediction, now known as
    *Moore’s Law*, was later revised to transistor counts doubling every *two* years.
    Despite the evolution of electronic switches from Bardeen’s transistor to the
    tiny chip transistors that are currently used in modern computers, Moore’s Law
    has held true for the past 50 years. However, the turn of the millennium saw processor
    design hit several critical performance walls:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The *memory wall*: Improvements in memory technology did not keep pace with
    improvements in clock speed, resulting in memory becoming a bottleneck to performance.
    As a result, continuously speeding up the execution of a CPU no longer improves
    its overall system performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The *power wall*: Increasing the number of transistors on a processor necessarily
    increases that processor’s temperature and power consumption, which in turn increases
    the required cost to power and cool the system. With the proliferation of multicore
    systems, power is now the dominant concern in computer system design.'
  prefs: []
  type: TYPE_NORMAL
- en: The power and memory walls caused computer architects to change the way they
    designed processors. Instead of adding more transistors to increase the speed
    at which a CPU executes a single stream of instructions, architects began adding
    multiple *compute cores* to a CPU. Compute cores are simplified processing units
    that contain fewer transistors than traditional CPUs and are generally easier
    to create. Combining multiple cores on one CPU allows the CPU to execute *multiple*
    independent streams of instructions at once.
  prefs: []
  type: TYPE_NORMAL
- en: '**Warning MORE CORES != BETTER**'
  prefs: []
  type: TYPE_NORMAL
- en: It may be tempting to assume that all cores are equal and that the more cores
    a computer has, the better it is. This is not necessarily the case! For example,
    *graphics processing unit* (GPU) cores have even fewer transistors than CPU cores,
    and are specialized for particular tasks involving vectors. A typical GPU can
    have 5,000 or more GPU cores. However, GPU cores are limited in the types of operations
    that they can perform and are not always suitable for general-purpose computing
    like the CPU core. Computing with GPUs is known as *manycore* computing. In this
    chapter, we concentrate on *multicore* computing. See [Chapter 15](ch15.xhtml#ch15)
    for a discussion of manycore computing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Taking a Closer Look: How Many Cores?'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Almost all modern computer systems have multiple cores, including small devices
    like the Raspberry Pi.^([1](ch14.xhtml#fn14_1)) Identifying the number of cores
    on a system is critical for accurately measuring the performance of multicore
    programs. On Linux and macOS computers, the `lscpu` command provides a summary
    of a system’s architecture. In the following example, we show the output of the
    `lscpu` command when run on a sample machine (some output is omitted to emphasize
    the key features):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The `lscpu` command gives a lot of useful information, including the type of
    processors, the core speed, and the number of cores. To calculate the number of
    *physical* (or actual) cores on a system, multiply the number of sockets by the
    number of cores per socket. The sample `lscpu` output shown in the preceding example
    reveals that the system has one socket with four cores per socket, or four physical
    cores in total.
  prefs: []
  type: TYPE_NORMAL
- en: HYPERTHREADING
  prefs: []
  type: TYPE_NORMAL
- en: 'At first glance, it may appear that the system in the previous example has
    eight cores in total. After all, this is what the “CPU(s)” field seems to imply.
    However, that field actually indicates the number of *hyperthreaded* (logical)
    cores, not the number of physical cores. Hyperthreading, or simultaneous multithreading
    (SMT), enables the efficient processing of multiple threads on a single core.
    Although hyperthreading can decrease the overall runtime of a program, performance
    on hyperthreaded cores does not scale at the same rate as on physical cores. However,
    if one task idles (e.g., due to a control hazard, see “Pipelining Hazards: Control
    Hazards” on [page 279](ch05.xhtml#lev2_106)), another task can still utilize the
    core. In short, hyperthreading was introduced to improve *process throughput*
    (which measures the number of processes that complete in a given unit of time)
    rather than *process speedup* (which measures the amount of runtime improvement
    of an individual process). Much of our discussion of performance in the coming
    chapter will focus on speedup.'
  prefs: []
  type: TYPE_NORMAL
- en: 14.1 Programming Multicore Systems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most of the common languages that programmers know today were created prior
    to the multicore age. As a result, many languages cannot *implicitly* (or automatically)
    employ multicore processors to speed up the execution of a program. Instead, programmers
    must specifically write software to leverage the multiple cores on a system.
  prefs: []
  type: TYPE_NORMAL
- en: 14.1.1 The Impact of Multicore Systems on Process Execution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Recall that a process can be thought of as an abstraction of a running program
    (see “Processes” on [page 624](ch13.xhtml#lev1_100)). Each process executes in
    its own virtual address space. The operating system (OS) schedules processes for
    execution on the CPU; a *context switch* occurs when the CPU changes which process
    it currently executes.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 14-1](ch14.xhtml#ch14fig1) illustrates how five example processes may
    execute on a single-core CPU.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/14fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 14-1: An execution time sequence for five processes as they share a
    single CPU core*'
  prefs: []
  type: TYPE_NORMAL
- en: The horizontal axis is time, with each time slice taking one unit of time. A
    box indicates when a process is using the single-core CPU. Assume that each process
    executes for one full time slice before a context switch occurs. So, Process 1
    uses the CPU during time steps T1 and T3.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the order of process execution is P1, P2, P1, P2, P4, P2, P3,
    P4, P5, P3, P5\. We take a moment here to distinguish between two measures of
    time. The *CPU time* measures the amount of time a process takes to execute on
    a CPU. In contrast, the *wall-clock time* measures the amount of time a human
    perceives a process takes to complete. The wall-clock time is often significantly
    longer than the CPU time, due to context switches. For example, Process 1’s CPU
    time requires two time units, whereas its wall-clock time is three time units.
  prefs: []
  type: TYPE_NORMAL
- en: When the total execution time of one process overlaps with another, the processes
    are running *concurrently* with each other. Operating systems employed concurrency
    in the single-core era to give the illusion that a computer can execute many things
    at once (e.g., you can have a calculator program, a web browser, and a word processing
    document all open at the same time). In truth, each process executes serially
    and the operating system determines the order in which processes execute and complete
    (which often differs in subsequent runs); see “Multiprogramming and Context Switching”
    on [page 625](ch13.xhtml#lev2_221).
  prefs: []
  type: TYPE_NORMAL
- en: Returning to the example, observe that Process 1 and Process 2 run concurrently
    with each other, since their executions overlap at time points T2–T4\. Likewise,
    Process 2 runs concurrently with Process 4, because their executions overlap at
    time points T4–T6\. In contrast, Process 2 does *not* run concurrently with Process
    3, because they share no overlap in their execution; Process 3 only starts running
    at time T7, whereas Process 2 completes at time T6.
  prefs: []
  type: TYPE_NORMAL
- en: A multicore CPU enables the OS to schedule a different process to each available
    core, allowing processes to execute *simultaneously*. The simultaneous execution
    of instructions from processes running on multiple cores is referred to as *parallel
    execution*. [Figure 14-2](ch14.xhtml#ch14fig2) shows how our example processes
    might execute on a dual-core system.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/14fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 14-2: An execution time sequence for five processes, extended to include
    two CPU cores (one in dark gray, the other in light gray)*'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the two CPU cores are colored differently. Suppose that the
    process execution order is again P1, P2, P1, P2, P4, P2, P3, P4, P5, P3, P5\.
    The presence of multiple cores enables certain processes to execute *sooner*.
    For example, during time unit T1, the first core executes Process 1 while the
    second core executes Process 2\. At time T2, the first core executes Process 2
    while the second executes Process 1\. Thus, Process 1 finishes executing after
    time T2, whereas Process 2 finishes executing at time T3.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the parallel execution of multiple processes increases just the number
    of processes that execute at any one time. In [Figure 14-2](ch14.xhtml#ch14fig2),
    all the processes complete execution by time unit T7\. However, each individual
    process still requires the same amount of CPU time to complete as shown in [Figure
    14-1](ch14.xhtml#ch14fig1). For example, Process 2 requires three time units regardless
    of execution on a single or multicore system (i.e., its *CPU time* remains the
    same). A multicore processor increases the *throughput* of process execution,
    or the number of processes that can complete in a given period of time. Thus,
    even though the CPU time of an individual process remains unchanged, its wall-clock
    time may decrease.
  prefs: []
  type: TYPE_NORMAL
- en: 14.1.2 Expediting Process Execution with Threads
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One way to speed up the execution of a single process is to decompose it into
    lightweight, independent execution flows called *threads*. [Figure 14-3](ch14.xhtml#ch14fig3)
    shows how a process’s virtual address space changes when it is multithreaded with
    two threads. While each thread has its own private allocation of call stack memory,
    all threads *share* the program data, instructions, and the heap allocated to
    the multithreaded process.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/14fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 14-3: Comparing the virtual address space of a single-threaded and
    a multithreaded process with two threads*'
  prefs: []
  type: TYPE_NORMAL
- en: The OS schedules threads in the same manner as it schedules processes. On a
    multicore processor, the OS can speed up the execution of a multithreaded program
    by scheduling the different threads to run on separate cores. The maximum number
    of threads that can execute in parallel is equal to the number of physical cores
    on the system. If the number of threads exceeds the number of physical cores,
    the remaining threads must wait their turn to execute (similar to how processes
    execute on a single core).
  prefs: []
  type: TYPE_NORMAL
- en: 'An Example: Scalar Multiplication'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As an initial example of how to use multithreading to speed up an application,
    consider the problem of performing scalar multiplication of an array `array` and
    some integer `s`. In scalar multiplication, each element in the array is scaled
    by multiplying the element with `s`.
  prefs: []
  type: TYPE_NORMAL
- en: 'A serial implementation of a scalar multiplication function follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Suppose that `array` has *N* total elements. To create a multithreaded version
    of this application with *t* threads, it is necessary to:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Create *t* threads.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Assign each thread a subset of the input array (i.e., *N*/*t* elements).
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Instruct each thread to multiply the elements in its array subset by `s`.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that the serial implementation of `scalar_multiply` spends 60 seconds
    multiplying an input array of 100 million elements. To build a version that executes
    with *t* = 4 threads, we assign each thread one fourth of the total input array
    (25 million elements).
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 14-4](ch14.xhtml#ch14fig4) shows what happens when we run four threads
    on a single core. As before, the execution order is left to the operating system.
    In this scenario, assume that the thread execution order is Thread 1, Thread 3,
    Thread 2, Thread 4\. On a single-core processor (represented by the squares),
    each thread executes sequentially. Thus, the multithreaded process running on
    one core will still take 60 seconds to run (perhaps a little longer, given the
    overhead of creating threads).'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/14fig04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 14-4: Running four threads on a single-core CPU*'
  prefs: []
  type: TYPE_NORMAL
- en: Now suppose that we run our multithreaded process on a dual-core system. [Figure
    14-5](ch14.xhtml#ch14fig5) shows the result. Again, assume *t* = 4 threads, and
    that the thread execution order is Thread 1, Thread 3, Thread 2, Thread 4\. Our
    two cores are represented by shaded squares. Since the system is dual-core, Thread
    1 and Thread 3 execute in parallel during time step T1\. Threads 2 and 4 then
    execute in parallel during time step T2\. Thus, the multithreaded process that
    originally took 60 seconds to run now runs in 30 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/14fig05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 14-5: Running four threads on a dual-core CPU*'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, suppose that the multithreaded process (*t* = 4) is run on a quad-core
    CPU. [Figure 14-6](ch14.xhtml#ch14fig6) shows one such execution sequence. Each
    of the four cores in [Figure 14-6](ch14.xhtml#ch14fig6) is shaded differently.
    On the quad-core system, each thread executes in parallel during time slice T1\.
    Thus, on a quad-core CPU, the multithreaded process that originally took 60 seconds
    now runs in 15 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/14fig06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 14-6: Running four threads on a quad-core CPU*'
  prefs: []
  type: TYPE_NORMAL
- en: In general, if the number of threads matches the number of cores (*c*) and the
    operating system schedules each thread to run on a separate core in parallel,
    then the multithreaded process should run in approximately 1/*c* of the time.
    Such linear speedup is ideal, but not frequently observed in practice. For example,
    if there are many other processes (or multithreaded processes) waiting to use
    the CPU, they will all compete for the limited number of cores, resulting in *resource
    contention* among the processes. If the number of specified threads exceeds the
    number of CPU cores, each thread must wait its turn to run. We explore other factors
    that often prevent linear speedup in “Measuring the Performance of Parallel Programs”
    on [page 709](ch14.xhtml#lev1_108).
  prefs: []
  type: TYPE_NORMAL
- en: 14.2 Hello Threading! Writing Your First Multithreaded Program
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we examine the ubiquitous POSIX thread library *Pthreads*.
    POSIX is an acronym for Portable Operating System Interface. It is an IEEE standard
    that specifies how UNIX systems look, act, and feel. The POSIX threads API is
    available on almost all UNIX-like operating systems, each of which meets the standard
    in its entirety or to some great degree. So, if you write parallel code using
    POSIX threads on a Linux machine, it will certainly work on other Linux machines,
    and it will likely work on machines running macOS or other UNIX variants.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin by analyzing an example “Hello World” Pthreads program.^([2](ch14.xhtml#fn14_2))
    For brevity, we have excluded error handling in the listing, though the downloadable
    version contains sample error handling.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Let’s examine this program in smaller components. Notice the inclusion of the
    `pthread.h` header file, which declares `pthread` types and functions. Next, the
    `HelloWorld` function defines the *thread function* that we later pass to `pthread_create`.
    A thread function is analogous to a `main` function for a worker (created) thread—a
    thread begins execution at the start of its thread function and terminates when
    it reaches the end. Each thread executes the thread function using its private
    execution state (i.e., its own stack memory and register values). Note also that
    the thread function is of type `void *`. Specifying an *anonymous pointer* in
    this context allows programmers to write thread functions that deal with arguments
    and return values of different types (see “The void * Type and Type Recasting”
    on [page 222](ch02.xhtml#lev2_38)). Lastly, in the `main` function, the main thread
    initializes the program state before creating and joining the worker threads.
  prefs: []
  type: TYPE_NORMAL
- en: 14.2.1 Creating and Joining Threads
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The program first starts as a single-threaded process. As it executes the `main`
    function, it reads the number of threads to create, and it allocates memory for
    two arrays: `thread_array` and `thread_ids`. The `thread_array` array contains
    the set of addresses for each thread created. The `thread_ids` array stores the
    set of arguments that each thread is passed. In this example, each thread is passed
    the address of its rank (or ID, represented by `thread_ids[i]`).'
  prefs: []
  type: TYPE_NORMAL
- en: 'After all the preliminary variables are allocated and initialized, the `main`
    thread executes the two major steps of multithreading:'
  prefs: []
  type: TYPE_NORMAL
- en: The *creation* step, in which the main thread spawns one or more worker threads.
    After being spawned, each worker thread runs within its own execution context
    concurrently with the other threads and processes on the system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *join* step, in which the main thread waits for all the workers to complete
    before proceeding as a single-thread process. Joining a thread that has terminated
    frees the thread’s execution context and resources. Attempting to join a thread
    that *hasn’t* terminated blocks the caller until the thread terminates, similar
    to the semantics of the `wait` function for processes (see “exit and wait” on
    [page 635](ch13.xhtml#lev2_226)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Pthreads library offers a `pthread_create` function for creating threads
    and a `pthread_join` function for joining them. The `pthread_create` function
    has the following signature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The function takes a pointer to a thread struct (of type `pthread_t`), a pointer
    to an attribute struct (normally set to `NULL`), the name of the function the
    thread should execute, and the array of arguments to pass to the thread function
    when it starts.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Hello World program calls `pthread_create` in the `main` function using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`&thread_array[i]` contains the address of thread *i*. The `pthread_create`
    function allocates a `pthread_t` thread object and stores its address at this
    location, enabling the programmer to reference the thread later (e.g., when joining
    it).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NULL` specifies that the thread should be created with default attributes.
    In most programs, it is safe to leave this second parameter as `NULL`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HelloWorld` names the thread function that the created thread should execute.
    This function behaves like the “main” function for the thread. For an arbitrary
    thread function (e.g., `function`), its prototype must match the form `void *
    function(void *)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`&thread_ids[i]` specifies the address of the arguments to be passed to thread
    *i*. In this case, `thread_ids[i]` contains a single `long` representing the thread’s
    ID. Since the last argument to `pthread_create` must be a pointer, we pass the
    *address* of the thread’s ID.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To generate several threads that execute the `HelloWorld` thread function,
    the program assigns each thread a unique ID and creates each thread within a `for`
    loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The OS schedules the execution of each created thread; the user cannot make
    any assumption on the order in which the threads will execute.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `pthread_join` function suspends the execution of its caller until the
    thread it references terminates. Its signature is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `pthread_join` takes as input a `pthread_t` struct, indicating which thread
    to wait on, and an optional pointer argument that specifies where the thread’s
    return value should be stored.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Hello World program calls `pthread_join` in `main` using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This line indicates that the main thread must wait on the termination of thread
    `t`. Passing `NULL` as the second argument indicates that the program does not
    use the thread’s return value.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous program, `main` calls `pthread_join` in a loop because *all*
    of the worker threads need to terminate before the `main` function proceeds to
    clean up memory and terminate the process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 14.2.2 The Thread Function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the previous program, each spawned thread prints out `Hello world! I am
    thread n`, where `n` is the thread’s unique ID. After the thread prints out its
    message, it terminates. Let’s take a closer look at the `HelloWorld` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Recall that `pthread_create` passes the arguments to the thread function using
    the `thread_args` parameter. In the `pthread_create` function in `main`, the Hello
    World program specified that this parameter is in fact the thread’s ID. Note that
    the parameter to `HelloWorld` must be declared as a generic or anonymous pointer
    (`void *`) (see “The void * Type and Type Recasting” on [page 126](ch02.xhtml#lev2_38)).
    The Pthreads library uses `void *` to make `pthread_create` more general purpose
    by not prescribing a parameter type. As a programmer, the `void *` is mildly inconvenient
    given that it must be recast before use. Here, we *know* the parameter is of type
    `long *` because that’s what we passed to `pthread_create` in `main`. Thus, we
    can safely cast the value as a `long *` and dereference the pointer to access
    the `long` value. Many parallel programs follow this structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to the thread function’s parameter, the Pthreads library avoids prescribing
    the thread function’s return type by specifying another `void *`: the programmer
    is free to return any pointer from the thread function. If the program needs to
    access the thread’s return value, it can retrieve it via the second argument to
    `pthread_join`. In our example, the thread has no need to return a value, so it
    simply returns a `NULL` pointer.'
  prefs: []
  type: TYPE_NORMAL
- en: 14.2.3 Running the Code
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The command that follows shows how to use GCC to compile the program. Building
    a Pthreads application requires that the `-lpthread` linker flag be passed to
    GCC to ensure that the Pthreads functions and types are accessible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the program without a command line argument results in a usage message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the program with four threads yields the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that each thread prints its unique ID number. In this run, thread 1’s
    output displays first, followed by threads 2, 3, and 0\. If we run the program
    again, we may see the output displayed in a different order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Recall that the operating system’s scheduler determines the thread execution
    order. From a user’s perspective, the order is *effectively random* due to being
    influenced by many factors that vary outside the user’s control (e.g., available
    system resources, the system receiving input, or OS scheduling). Since all threads
    are running concurrently with one another and each thread executes a call to `printf`
    (which prints to `stdout`), the first thread that prints to `stdout` will have
    its output show up first. Subsequent executions may (or may not) result in different
    output.
  prefs: []
  type: TYPE_NORMAL
- en: '**Warning THREAD EXECUTION ORDER**'
  prefs: []
  type: TYPE_NORMAL
- en: You should *never* make any assumptions about the order in which threads will
    execute. If the correctness of your program requires that threads run in a particular
    order, you must add synchronization (see “Synchronizing Threads” on [page 686](ch14.xhtml#lev1_107))
    to your program to prevent threads from running when they shouldn’t.
  prefs: []
  type: TYPE_NORMAL
- en: 14.2.4 Revisiting Scalar Multiplication
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let’s explore how to create a multithreaded implementation of the scalar multiplication
    program from “An Example: Scalar Multiplication” on [page 675](ch14.xhtml#lev3_113).
    Recall that our general strategy for parallelizing `scalar_multiply` is to create
    multiple threads, assign each thread a subset of the input array, and instruct
    each thread to multiply the elements in its array subset by `s`.'
  prefs: []
  type: TYPE_NORMAL
- en: The following is a thread function that accomplishes this task. Notice that
    we have moved `array`, `length`, and `s` to the global scope of the program.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s break this down into parts. Recall that the first step is to assign each
    thread a component of the array. The following lines accomplish this task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The variable `chunk` stores the number of elements that each thread is assigned.
    To ensure that each thread gets roughly the same amount of work, we first set
    the chunk size to the number of elements divided by the number of threads, or
    `length / nthreads`.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we assign each thread a distinct range of elements to process. Each thread
    computes its range’s `start` and `end` index using the `chunk` size and its unique
    thread ID.
  prefs: []
  type: TYPE_NORMAL
- en: For example, with four threads (with IDs 0–3) operating over an array with 100
    million elements, each thread is responsible for processing a 25 million element
    `chunk`. Incorporating the thread ID assigns each thread a unique subset of the
    input.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next two lines account for the case in which `length` is not evenly divisible
    by the number of threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Suppose that we specified three rather than four threads. The nominal chunk
    size would be 33,333,333 elements, leaving one element unaccounted for. The code
    in the previous example would assign the remaining element to the last thread.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note CREATING BALANCED INPUT**'
  prefs: []
  type: TYPE_NORMAL
- en: The chunking code just shown is imperfect. In the case where the number of threads
    does not evenly divide the input, the remainder is assigned to the last thread.
    Consider a sample run in which the array has 100 elements, and 12 threads are
    specified. The nominal chunk size would be 8, and the remainder would be 4\. With
    the example code, the first 11 threads will each have 8 assigned elements, whereas
    the last thread will be assigned 12 elements. Consequently, the last thread performs
    50% more work than the other threads. A potentially better way to chunk this example
    is to have the first 4 threads process 9 elements each, whereas the last 8 threads
    process 8 elements each. This will result in better *load balancing* of the input
    across the threads.
  prefs: []
  type: TYPE_NORMAL
- en: 'With an appropriate local `start` and `end` index computed, each thread is
    now ready to perform scalar multiplication on its component of the array. The
    last portion of the `scalar_multiply` function accomplishes this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '14.2.5 Improving Scalar Multiplication: Multiple Arguments'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A key weakness of the previous implementation is the wide use of global variables.
    Our original discussion in “Parts of Program Memory and Scope” on [page 64](ch02.xhtml#lev1_9)
    showed that, although useful, global variables should generally be avoided in
    C. To reduce the number of global variables in the program, one solution is to
    declare a `t_arg` struct as follows in the global scope:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Our main function would, in addition to allocating `array` and setting local
    variables `length`, `nthreads`, and `s` (our scaling factor), allocate an array
    of `t_arg` records:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Later in `main`, when `pthread_create` is called, the thread’s associated `t_args`
    struct is passed as an argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, our `scalar_multiply` function would look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Implementing this program fully is an exercise we leave to the reader. Please
    note that error handling has been omitted for the sake of brevity.
  prefs: []
  type: TYPE_NORMAL
- en: 14.3 Synchronizing Threads
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the examples we’ve looked at thus far, each thread executes without sharing
    data with any other threads. In the scalar multiplication program, for instance,
    each element of the array is entirely independent of all the others, making it
    unnecessary for the threads to share data.
  prefs: []
  type: TYPE_NORMAL
- en: However, a thread’s ability to easily share data with other threads is one of
    its main features. Recall that all the threads of a multithreaded process share
    the heap common to the process. In this section, we study the data sharing and
    protection mechanisms available to threads in detail.
  prefs: []
  type: TYPE_NORMAL
- en: '*Thread synchronization* refers to forcing threads to execute in a particular
    order. Even though synchronizing threads can add to the runtime of a program,
    it is often necessary to ensure program correctness. In this section, we primarily
    discuss how one synchronization construct (a *mutex*) helps ensure the correctness
    of a threaded program. We conclude the section with a discussion of some other
    common synchronization constructs: *semaphores*, *barriers*, and *condition variables*.'
  prefs: []
  type: TYPE_NORMAL
- en: CountSort
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Let’s study a slightly more complicated example called CountSort. The CountSort
    algorithm is a simple linear (O(*N*)) sorting algorithm for sorting a known small
    range of *R* values, where *R* is much smaller than *N*. To illustrate how CountSort
    works, consider an array `A` of 15 elements, all of which contain random values
    between 0 and 9 (10 possible values):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'For a particular array, CountSort works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. It counts the frequency of each value in the array.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. It overwrites the original array by enumerating each value by its frequency.
  prefs: []
  type: TYPE_NORMAL
- en: After step 1, the frequency of each value is placed in a `counts` array of length
    10, where the value of `counts[i]` is the frequency of the value *i* in array
    `A`. For example, since there are three elements with value 2 in array `A`, `counts[2]`
    is 3.
  prefs: []
  type: TYPE_NORMAL
- en: 'The corresponding `counts` array for the previous example looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Note that the sum of all the elements in the `counts` array is equal to the
    length of `A`, or 15.
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 uses the `counts` array to overwrite `A`, using the frequency counts
    to determine the set of indices in `A` that store each consecutive value in sorted
    order. So, since the `counts` array indicates that there are three elements with
    value 0 and two elements with value 1 in array `A`, the first three elements of
    the final array will be 0, and the next two will be 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'After running step 2, the final array looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Following is a serial implementation of the CountSort algorithm, with the `count`
    (step 1) and `overwrite` (step 2) functions clearly delineated. For brevity, we
    do not reproduce the whole program here, though you can download the source.^([3](ch14.xhtml#fn14_3))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this program on an array of size 15 yields the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The second parameter to this program is a *verbose* flag, which indicates whether
    the program prints output. This is a useful option for larger arrays for which
    we may want to run the program but not necessarily print out the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parallelizing countElems: An Initial Attempt'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: CountSort consists of two primary steps, each of which benefits from being parallelized.
    In the remainder of the chapter, we primarily concentrate on the parallelization
    of step 1, or the `countElems` function. Parallelizing the `writeArray` function
    is left as an exercise for the reader.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code block that follows depicts a first attempt at creating a threaded
    `countElems` function. Parts of the code (argument parsing, error handling) are
    omitted in this example for the sake of brevity, but the full source can be downloaded.^([4](ch14.xhtml#fn14_4))
    In the code that follows, each thread attempts to count the frequency of the array
    elements in its assigned component of the global array and updates a global count
    array with the discovered counts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The `main` function looks nearly identical to our earlier sample programs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: For reproducibility purposes, the random number generator is seeded with a static
    value (10) to ensure that `array` (and therefore `counts`) always contains the
    same set of numbers. An additional function (`printCounts`) prints out the contents
    of the global `counts` array. The expectation is that, regardless of the number
    of threads used, the contents of the `counts` array should always be the same.
    For brevity, error handling has been removed from the listing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Compiling the program and running it with one, two, and four threads over 10
    million elements produces the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the printed results change significantly on each run. In particular,
    they seem to change as we vary the number of threads! This should not happen,
    since our use of the static seed guarantees the same set of numbers every run.
    These results contradict one of the cardinal rules for threaded programs: the
    output of a program should be correct and consistent *regardless* of the number
    of threads used.'
  prefs: []
  type: TYPE_NORMAL
- en: Since our first attempt at parallelizing `countElems` doesn’t seem to be working,
    let’s delve deeper into what this program is doing and examine how we might fix
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Data Races
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'To understand what’s going on, let’s consider an example run with two threads
    on two separate cores of a multicore system. Recall that the execution of any
    thread can be preempted at any time by the OS, which means that each thread could
    be running different instructions of a particular function at any given time (or
    possibly the same instruction). [Table 14-1](ch14.xhtml#ch14tab1) shows one possible
    path of execution through the `countElems` function. To better illustrate what
    is going on, we translated the line `counts[val] = counts[val] + 1` into the following
    sequence of equivalent instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. *Read* `counts[val]` and place into a register.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. *Modify* the register by incrementing it by one.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. *Write* the contents of the register to `counts[val]`.
  prefs: []
  type: TYPE_NORMAL
- en: This is known as the *read–modify–write* pattern. In the example shown in [Table
    14-1](ch14.xhtml#ch14tab1), each thread executes on a separate core (Thread 0
    on Core 0, Thread 1 on Core 1). We start inspecting the execution of the process
    at time step *i*, where both threads have a `val` of 1.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 14-1:** A Possible Execution Sequence of Two Threads Running `countElems`'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Time** | **Thread 0** | **Thread 1** |'
  prefs: []
  type: TYPE_TB
- en: '| *i* | Read `counts[1]` and place into Core 0’s register | … |'
  prefs: []
  type: TYPE_TB
- en: '| *i* + 1 | Increment register by 1 | Read `counts[1]` and place into Core
    1’s register |'
  prefs: []
  type: TYPE_TB
- en: '| *i* + 2 | Overwrite `counts[1]` with contents of register | Increment register
    by 1 |'
  prefs: []
  type: TYPE_TB
- en: '| *i* + 3 | … | Overwrite `counts[1]` with contents of register |'
  prefs: []
  type: TYPE_TB
- en: Suppose that, prior to the execution sequence in [Table 14-1](ch14.xhtml#ch14tab1),
    `counts[1]` contains the value 60\. In time step *i*, Thread 0 reads `counts[1]`
    and places the value 60 in Core 0’s register. In time step *i* + 1, while Thread
    0 increments Core 0’s register by one, the *current* value in `counts[1]` (60)
    is read into Core 1’s register by Thread 1\. In time step *i* + 2, Thread 0 updates
    `counts[1]` with the value 61 while Thread 1 increments the value stored in its
    local register (60) by one. The end result is that during time step *i* + 3, the
    value `counts[1]` is overwritten by Thread 1 with the value 61, not 62 as we would
    expect! This causes `counts[1]` to essentially “lose” an increment!
  prefs: []
  type: TYPE_NORMAL
- en: We refer to the scenario in which two threads attempt to write to the same location
    in memory as a *data race* condition. More generally, a *race condition* refers
    to any scenario in which the simultaneous execution of two operations gives an
    incorrect result. Note that a simultaneous read of the `counts[1]` location would
    *not* in and of itself constitute a race condition, because values can generally
    read alone from memory without issue. It was the combination of this step with
    the writes to `counts[1]` that caused the incorrect result. This read–modify–write
    pattern is a common source of a particular type of race condition, called a *data
    race*, in most threaded programs. In our discussion of race conditions and how
    to fix them, we focus on data races.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note ATOMIC OPERATIONS**'
  prefs: []
  type: TYPE_NORMAL
- en: An operation is defined as being *atomic* if a thread perceives it as executing
    without interruption (in other words, as an “all or nothing” action). In some
    libraries, a keyword or type is used to specify that a block of computation should
    be treated as being atomic. In the previous example, the line `counts[val] = counts[val]
    + 1` (even if written as `counts[val]++`) is *not* atomic, because this line actually
    corresponds to several instructions at the machine level. A synchronization construct
    like mutual exclusion is needed to ensure that there are no data races. In general,
    all operations should be assumed to be nonatomic unless mutual exclusion is explicitly
    enforced.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that not all execution sequences of the two threads cause a race
    condition. Consider the sample execution sequence of Threads 0 and 1 in [Table
    14-2](ch14.xhtml#ch14tab2).
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 14-2:** Another Possible Execution Sequence of Two Threads Running
    `countElems`'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Time** | **Thread 0** | **Thread 1** |'
  prefs: []
  type: TYPE_TB
- en: '| *i* | Read `counts[1]` and place into Core 0’s register | … |'
  prefs: []
  type: TYPE_TB
- en: '| *i* + 1 | Increment register by 1 | … |'
  prefs: []
  type: TYPE_TB
- en: '| *i* + 2 | Overwrite `counts[1]` with contents of register | … |'
  prefs: []
  type: TYPE_TB
- en: '| *i* + 3 | … | Read `counts[1]` and place into Core 1’s register |'
  prefs: []
  type: TYPE_TB
- en: '| *i* + 4 | … | Increment register by 1 |'
  prefs: []
  type: TYPE_TB
- en: '| *i* + 5 | … | Overwrite `counts[1]` with contents of register |'
  prefs: []
  type: TYPE_TB
- en: In this execution sequence, Thread 1 does not read from `counts[1]` until after
    Thread 0 updates it with its new value (61). The end result is that Thread 1 reads
    the value 61 from `counts[1]` and places it into Core 1’s register during time
    step *i* + 3, and writes the value 62 to `counts[1]` in time step *i* + 5.
  prefs: []
  type: TYPE_NORMAL
- en: To fix a data race, we must first isolate the *critical section*, or the subset
    of code that must execute *atomically* (in isolation) to ensure correct behavior.
    In threaded programs, blocks of code that update a shared resource are typically
    identified to be critical sections.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `countElems` function, updates to the `counts` array should be put in
    a critical section to ensure that values are not lost due to multiple threads
    updating the same location in memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Since the fundamental problem in `countElems` is the simultaneous access of
    `counts` by multiple threads, a mechanism is needed to ensure that only one thread
    executes within the critical section at a time. Using a synchronization construct
    (like a mutex, which is covered in the next section) will force the threads to
    enter the critical section sequentially.
  prefs: []
  type: TYPE_NORMAL
- en: 14.3.1 Mutual Exclusion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*What is the mutex? The answer is out there, and it’s looking for you,*'
  prefs: []
  type: TYPE_NORMAL
- en: '*and it will find you if you want it to.*'
  prefs: []
  type: TYPE_NORMAL
- en: —Trinity, explaining mutexes to Neo (with apologies to *The Matrix*)
  prefs: []
  type: TYPE_NORMAL
- en: To fix the data race, let’s use a synchronization construct known as a mutual
    exclusion lock, or *mutex*. Mutual exclusion locks are a type of synchronization
    primitive that ensures that only one thread enters and executes the code inside
    the critical section at any given time.
  prefs: []
  type: TYPE_NORMAL
- en: Before using a mutex, a program must first declare the mutex in memory that’s
    shared by threads (often as a global variable), and then initialize the mutex
    before the threads need to use it (typically in the `main` function).
  prefs: []
  type: TYPE_NORMAL
- en: 'The Pthreads library defines a `pthread_mutex_t` type for mutexes. To declare
    a mutex variable, add this line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'To initialize the mutex use the `pthread_mutex_init` function, which takes
    the address of a mutex and an attribute structure, typically set to `NULL`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'When the mutex is no longer needed (typically at the end of the `main` function,
    after `pthread_join`), a program should release the mutex structure by invoking
    the `pthread_mutex_destroy` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The Mutex: Locked and Loaded'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The initial state of a mutex is unlocked, meaning it’s immediately usable by
    any thread. To enter a critical section, a thread must first acquire a lock. This
    is accomplished with a call to the `pthread_mutex_lock` function. After a thread
    has the lock, no other thread can enter the critical section until the thread
    with the lock releases it. If another thread calls `pthread_mutex_lock` and the
    mutex is already locked, the thread will *block* (or wait) until the mutex becomes
    available. Recall that blocking implies that the thread will not be scheduled
    to use the CPU until the condition it’s waiting for (the mutex being available)
    becomes true (see “Process State” on [page 627](ch13.xhtml#lev2_222)).
  prefs: []
  type: TYPE_NORMAL
- en: When a thread exits the critical section it must call the `pthread_mutex_unlock`
    function to release the mutex, making it available for another thread. Thus, at
    most one thread may acquire the lock and enter the critical section at a time,
    which prevents multiple threads from *racing* to read and update shared variables.
  prefs: []
  type: TYPE_NORMAL
- en: Having declared and initialized a mutex, the next question is where the lock
    and unlock functions should be placed to best enforce the critical section. Here
    is an initial attempt at augmenting the `countElems` function with a mutex:^([5](ch14.xhtml#fn14_5))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The mutex initialize and destroy functions are placed in `main` around the
    thread creation and join functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s recompile and run this new program while varying the number of threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Excellent, the output is *finally* consistent regardless of the number of threads
    used!
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that another primary goal of threading is to reduce the runtime of a
    program as the number of threads increases (i.e., to *speed up* program execution).
    Let’s benchmark the performance of the `countElems` function. Although it may
    be tempting to use a command line utility like `time -p`, recall that invoking
    `time -p` measures the wall-clock time of the *entire* program (including the
    generation of random elements) and *not* just the running of the `countElems`
    function. In this case, it is better to use a system call like `gettimeofday`,
    which allows a user to accurately measure the wall-clock time of a particular
    section of code. Benchmarking `countElems` on 100 million elements yields the
    following run times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Adding more threads causes the program to get *slower*! This goes against the
    goal of making programs *faster* with threads.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand what is going on, consider where the locks are placed in the
    `countsElems` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we placed the lock around the *entirety* of the `for` loop.
    Even though this placement solves the correctness problems, it’s an extremely
    poor decision from a performance perspective—the critical section now encompasses
    the entire loop body. Placing locks in this manner guarantees that only one thread
    can execute the loop at a time, effectively serializing the program!
  prefs: []
  type: TYPE_NORMAL
- en: 'The Mutex: Reloaded'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Let’s try another approach and place the mutex locking and unlocking functions
    within every iteration of the loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: This may initially look like a better solution because each thread can enter
    the loop in parallel, serializing only when reaching the lock. The critical section
    is very small, encompassing only the line `counts[val] = counts[val] + 1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first perform a correctness check on this version of the program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: So far so good. This version of the program also produces consistent output
    regardless of the number of threads employed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s look at performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Running this version of the code yields (amazingly enough) a *significantly
    slower* runtime!
  prefs: []
  type: TYPE_NORMAL
- en: 'As it turns out, locking and unlocking a mutex are expensive operations. Recall
    what was covered in the discussion on function call optimizations (see “Function
    Inlining” on [page 604](ch12.xhtml#lev1_95)): calling a function repeatedly (and
    needlessly) in a loop can be a major cause of slowdown in a program. In our prior
    use of mutexes, each thread locks and unlocks the mutex exactly once. In the current
    solution, each thread locks and unlocks the mutex *n*/*t* times, where *n* is
    the size of the array, *t* is the number of threads, and *n*/*t* is the size of
    the array component assigned to each particular thread. As a result, the cost
    of the additional mutex operations slows down the loop’s execution considerably.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Mutex: Revisited'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In addition to protecting the critical section to achieve correct behavior,
    an ideal solution would use the lock and unlock functions as little as possible,
    and reduce the critical section to the smallest possible size.
  prefs: []
  type: TYPE_NORMAL
- en: The original implementation satisfies the first requirement, whereas the second
    implementation tries to accomplish the second. At first glance, it appears that
    the two requirements are incompatible with each other. Is there a way to actually
    accomplish both (and while we are at it, speed up the execution of our program)?
  prefs: []
  type: TYPE_NORMAL
- en: 'For the next attempt, each thread maintains a private, *local* array of counts
    on its stack. Because the array is local to each thread, a thread can access it
    without locking—there’s no risk of a race condition on data that isn’t shared
    between threads. Each thread processes its assigned subset of the shared array
    and populates its local counts array. After counting up all the values within
    its subset, each thread:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Locks the shared mutex (entering a critical section).
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Adds the values from its local counts array to the shared counts array.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Unlocks the shared mutex (exiting the critical section).
  prefs: []
  type: TYPE_NORMAL
- en: Restricting each thread to update the shared counts array only once significantly
    reduces the contention for shared variables and minimizes expensive mutex operations.
  prefs: []
  type: TYPE_NORMAL
- en: The following is our revised `countElems` function:^([6](ch14.xhtml#fn14_6))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'This version has a few additional features:'
  prefs: []
  type: TYPE_NORMAL
- en: The presence of `local_counts`, an array that is private to the scope of each
    thread (i.e., allocated in the thread’s stack). Like `counts`, `local_counts`
    contains `MAX` elements, given that `MAX` is the maximum value any element can
    hold in our input array.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each thread makes updates to `local_counts` at its own pace, without any contention
    for shared variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A single call to `pthread_mutex_lock` protects each thread’s update to the global
    `counts` array, which happens only once at the end of each thread’s execution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this manner, we reduce the time each thread spends in a critical section
    to just updating the shared counts array. Even though only one thread can enter
    the critical section at a time, the time each thread spends there is proportional
    to `MAX`, not *n*, the length of the global array. Since `MAX` is much less than
    *n*, we should see an improvement in performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now benchmark this version of our code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Wow, what a difference! Our program not only computes the correct answers, but
    also executes faster as we increase the number of threads.
  prefs: []
  type: TYPE_NORMAL
- en: 'The lesson to take away here is this: to efficiently minimize a critical section,
    use local variables to collect intermediate values. After the hard work requiring
    parallelization is over, use a mutex to safely update any shared variable(s).'
  prefs: []
  type: TYPE_NORMAL
- en: Deadlock
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In some programs, waiting threads have dependencies on one another. A situation
    called *deadlock* can arise when multiple synchronization constructs like mutexes
    are incorrectly applied. A deadlocked thread is blocked from execution by another
    thread, which *itself* is blocked on a blocked thread. Gridlock (in which cars
    in all directions cannot move forward due to being blocked by other cars) is a
    common real-world example of deadlock that occurs at busy city intersections.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate a deadlock scenario in code, let’s consider an example where
    multithreading is used to implement a banking application. Each user’s account
    is defined by a balance and its own mutex (ensuring that no race conditions can
    occur when updating the balance):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Consider the following naive implementation of a `Transfer` function that moves
    money from one bank account to another:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Suppose that Threads 0 and 1 are executing concurrently and represent users
    A and B, respectively. Now consider the situation in which A and B want to transfer
    money to each other: A wants to transfer 20 dollars to B, while B wants to transfer
    40 dollars to A.'
  prefs: []
  type: TYPE_NORMAL
- en: In the path of execution highlighted by [Figure 14-7](ch14.xhtml#ch14fig7),
    both threads concurrently execute the `Transfer` function. Thread 0 acquires the
    lock of `acctA` while Thread 1 acquires the lock of `acctB`. Now consider what
    happens. To continue executing, Thread 0 needs to acquire the lock on `acctB`,
    which Thread 1 holds. Likewise, Thread 1 needs to acquire the lock on `acctA`
    to continue executing, which Thread 0 holds. Since both threads are blocked on
    each other, they are in deadlock.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/14fig07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 14-7: An example of deadlock*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Although the OS provides some protection against deadlock, programmers should
    be mindful about writing code that increases the likelihood of deadlock. For example,
    the preceding scenario could have been avoided by rearranging the locks so that
    each lock/unlock pair surrounds only the balance update statement associated with
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Deadlock is not a situation that is unique to threads. Processes (especially
    those that are communicating with one another) can deadlock with one another.
    Programmers should be mindful of the synchronization primitives they use and the
    consequences of using them incorrectly.
  prefs: []
  type: TYPE_NORMAL
- en: 14.3.2 Semaphores
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Semaphores are commonly used in operating systems and concurrent programs where
    the goal is to manage concurrent access to a pool of resources. When using a semaphore,
    the goal isn’t *who* owns what, but *how many* resources are still available.
    Semaphores are different from mutexes in several ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Semaphores need not be in a binary (locked or unlocked) state. A special type
    of semaphore called a *counting semaphore* can range in value from 0 to some *r*,
    where *r* is the number of possible resources. Any time a resource is produced,
    the semaphore is incremented. Any time a resource is being used, the semaphore
    is decremented. When a counting semaphore has a value of 0, it means that no resources
    are available, and any other threads that attempt to acquire a resource must wait
    (e.g., block).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semaphores can be locked by default.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While a mutex and condition variables can simulate the functionality of a semaphore,
    using a semaphore may be simpler and more efficient in some cases. Semaphores
    also have the advantage that *any* thread can unlock the semaphore (in contrast
    to a mutex, where the calling thread must unlock it).
  prefs: []
  type: TYPE_NORMAL
- en: 'Semaphores are not part of the Pthreads library, but that does not mean that
    you cannot use them. On Linux and macOS systems, semaphore primitives can be accessed
    from `semaphore.h`, typically located in `/usr/include`. Since there is no standard,
    the function calls may differ on different systems. That said, the semaphore library
    has similar declarations to those for mutexes:'
  prefs: []
  type: TYPE_NORMAL
- en: Declare a semaphore (type `sem_t`, e.g., `sem_t semaphore`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Initialize a semaphore using `sem_init` (usually in `main`). The `sem_init`
    function has three parameters: the first is the address of a semaphore, the second
    is its initial state (locked or unlocked), and the third parameter indicates whether
    the semaphore should be shared with the threads of a process (e.g., with value
    0) or between processes (e.g., with value 1). This is useful because semaphores
    are commonly used for process synchronization. For example, initializing a semaphore
    with the call `sem_init(&semaphore, 1, 0)` indicates that our semaphore is initially
    locked (the second parameter is 1), and is to be shared among the threads of a
    common process (the third parameter is 0). In contrast, mutexes always start out
    unlocked. It is important to note that in macOS, the equivalent function is `sem_open`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Destroy a semaphore using `sem_destroy` (usually in `main`). This function only
    takes a pointer to the semaphore (`sem_destroy(&semaphore)`). Note that in macOS,
    the equivalent function may be `sem_unlink` or `sem_close`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `sem_wait` function indicates that a resource is being used, and decrements
    the semaphore. If the semaphore’s value is greater than 0 (indicating there are
    still resources available), the function will immediately return, and the thread
    is allowed to proceed. If the semaphore’s value is already 0, the thread will
    block until a resource becomes available (i.e., the semaphore has a positive value).
    A call to `sem_wait` typically looks like `sem_wait(&semaphore)`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `sem_post` function indicates that a resource is being freed, and increments
    the semaphore. This function returns immediately. If there is a thread waiting
    on the semaphore (i.e., the semaphore’s value was previously 0), then the other
    thread will take ownership of the freed resource. A call to `sem_post` looks like
    `sem_post(&semaphore)`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 14.3.3 Other Synchronization Constructs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Mutexes and semaphores are not the only example of synchronization constructs
    that can be used in the context of multithreaded programs. In this subsection
    we will briefly discuss the barrier and condition variable synchronization constructs,
    which are both part of the Pthreads library.
  prefs: []
  type: TYPE_NORMAL
- en: Barriers
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'A *barrier* is a type of synchronization construct that forces *all* threads
    to reach a common point in execution before releasing the threads to continue
    executing concurrently. Pthreads offers a barrier synchronization primitive. To
    use Pthreads barriers, it is necessary to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Declare a barrier global variable (e.g., `pthread_barrier_t barrier`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Initialize the barrier in `main` (`pthread_barrier_init(&barrier)`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Destroy the barrier in `main` after use (`pthread_barrier_destroy(&barrier)`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the `pthread_barrier_wait` function to create a synchronization point.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following program shows the use of a barrier in a function called `threadEx`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: In this example, no thread can start processing its assigned portion of the
    array until *every* thread has printed out the message that they are starting
    work. Without the barrier, it is possible for one thread to have finished work
    before the other threads have printed their starting work message! Notice that
    it is *still* possible for one thread to print that it is done doing work before
    another thread finishes.
  prefs: []
  type: TYPE_NORMAL
- en: Condition Variables
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Condition variables force a thread to block until a particular condition is
    reached. This construct is useful for scenarios in which a condition must be met
    before the thread does some work. In the absence of condition variables, a thread
    would have to repeatedly check to see whether the condition is met, continuously
    utilizing the CPU. Condition variables are always used in conjunction with a mutex.
    In this type of synchronization construct, the mutex enforces mutual exclusion,
    whereas the condition variable ensures that particular conditions are met before
    a thread acquires the mutex.
  prefs: []
  type: TYPE_NORMAL
- en: POSIX condition variables have the type `pthread_cond_t`. Like the mutex and
    barrier constructs, condition variables must be initialized prior to use and destroyed
    after use.
  prefs: []
  type: TYPE_NORMAL
- en: To initialize a condition variable, use the `pthread_cond_init` function. To
    destroy a condition variable, use the `pthread_cond_destroy` function.
  prefs: []
  type: TYPE_NORMAL
- en: The two functions commonly invoked when using condition variables are `pthread_cond_wait`
    and `pthread_cond_signal`. Both functions require the address of a mutex in addition
    to the address of the condition variable.
  prefs: []
  type: TYPE_NORMAL
- en: The `pthread_cond_wait(&cond, &mutex)` function takes the addresses of a condition
    variable `cond` and a mutex `mutex` as its arguments. It causes the calling thread
    to block on the condition variable `cond` until another thread signals it (or
    “wakes” it up).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `pthread_cond_signal(&cond)` function causes the calling thread to unblock
    (or signal) another thread that is waiting on the condition variable `cond` (based
    on scheduling priority). If no threads are currently blocked on the condition,
    then the function has no effect. Unlike `pthread_cond_wait`, the `pthread_cond_signal`
    function can be called by a thread regardless of whether it owns the mutex in
    which `pthread_cond_wait` is called.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Condition Variable Example
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Traditionally, condition variables are most useful when a subset of threads
    are waiting on another set to complete some action. In the following example,
    we use multiple threads to simulate a set of farmers collecting eggs from a set
    of chickens. “Chicken” and “Farmer” represent two separate classes of threads.
    The full source of this program can be downloaded;^([7](ch14.xhtml#fn14_7)) note
    that this listing excludes many comments/error handling for brevity.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `main` function creates a shared variable `num_eggs` (which indicates the
    total number of eggs available at any given time), a shared `mutex` (which is
    used whenever a thread accesses `num_eggs`), and a shared condition variable `eggs`.
    It then creates two Chicken and two Farmer threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Each Chicken thread is responsible for laying a certain number of eggs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: To lay an egg, a Chicken thread sleeps for a while, acquires the mutex and updates
    the total number of available eggs by one. Prior to releasing the mutex, the Chicken
    thread “wakes up” a sleeping Farmer (presumably by squawking). The Chicken thread
    repeats the cycle until it has laid all the eggs it intends to (`total_eggs`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Each Farmer thread is responsible for collecting `total_eggs` eggs from the
    set of chickens (presumably for their breakfast):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Each Farmer thread acquires the mutex prior to checking the shared `num_eggs`
    variable to see whether any eggs are available (`*num_eggs == 0`). While there
    aren’t any eggs available, the Farmer thread blocks (i.e., takes a nap).
  prefs: []
  type: TYPE_NORMAL
- en: After the Farmer thread “wakes up” due to a signal from a Chicken thread, it
    checks to see that an egg is still available (another Farmer could have grabbed
    it first) and if so, the Farmer “collects” an egg (decrementing `num_eggs` by
    one) and releases the mutex.
  prefs: []
  type: TYPE_NORMAL
- en: In this manner, the Chicken and Farmer work in concert to lay/collect eggs.
    Condition variables ensure that no Farmer thread collects an egg until it is laid
    by a Chicken thread.
  prefs: []
  type: TYPE_NORMAL
- en: Broadcasting
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Another function used with condition variables is `pthread_cond_broadcast`,
    which is useful when multiple threads are blocked on a particular condition. Calling
    `pthread_cond_broadcast(&cond)` wakes up *all* threads that are blocked on condition
    `cond`. In this next example, we show how condition variables can implement the
    barrier construct discussed previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: The function `threadEx_v2` has identical functionality to `threadEx`. In this
    example, the condition variable is named `barrier`. As each thread acquires the
    lock, it increments `n_reached`, the number of threads that have reached that
    point. While the number of threads that have reached the barrier is less than
    the total number of threads, the thread waits on the condition variable `barrier`
    and mutex `mutex`.
  prefs: []
  type: TYPE_NORMAL
- en: However, when the last thread reaches the barrier, it calls `pthread_cond _broadcast(&barrier)`,
    which releases *all* the other threads that are waiting on the condition variable
    `barrier`, enabling them to continue execution.
  prefs: []
  type: TYPE_NORMAL
- en: This example is useful for illustrating the `pthread_cond_broadcast` function;
    however, it is best to use the Pthreads barrier primitive whenever barriers are
    necessary in a program.
  prefs: []
  type: TYPE_NORMAL
- en: One question that students tend to ask is if the `while` loop around the call
    to `pthread_cond_wait` in the `farmer` and `threadEx_v2` code can be replaced
    with an `if` statement. This `while` loop is in fact absolutely necessary for
    two main reasons. First, the condition may change prior to the woken thread arriving
    to continue execution. The `while` loop enforces that the condition be retested
    one last time. Second, the `pthread_cond_wait` function is vulnerable to *spurious
    wakeups*, in which a thread is erroneously woken up even though the condition
    may not be met. The `while` loop is in fact an example of a *predicate loop*,
    which forces a final check of the condition variable before releasing the mutex.
    The use of predicate loops is therefore correct practice when using condition
    variables.
  prefs: []
  type: TYPE_NORMAL
- en: 14.4 Measuring the Performance of Parallel Programs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, we have used the `gettimeofday` function to measure the amount of time
    it takes for programs to execute. In this section, we discuss how to measure how
    well a parallel program performs in comparison to a serial program as well as
    other topics related to measuring the performance of parallel programs.
  prefs: []
  type: TYPE_NORMAL
- en: 14.4.1 Parallel Performance Basics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Speedup
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Suppose that a program takes *T*[c] time to execute on *c* cores. Thus, the
    serial version of the program would take *T*[1] time. The speedup of the program
    on *c* cores is then expressed by this equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/equ0709-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If a serial program takes 60 seconds to execute, while its parallel version
    takes 30 seconds on 2 cores, the corresponding speedup is 2\. Likewise if that
    program takes 15 seconds on 4 cores, the speedup is 4\. In an ideal scenario,
    a program running on *n* cores with *n* total threads has a speedup of *n*.
  prefs: []
  type: TYPE_NORMAL
- en: If the speedup of a program is greater than 1, it indicates that the parallelization
    yielded some improvement. If the speedup is less than 1, then the parallel solution
    is in fact slower than the serial solution. It is possible for a program to have
    a speedup greater than *n* (for example, as a side effect of additional caches
    reducing accesses to memory). Such cases are referred to as *superlinear speedup*.
  prefs: []
  type: TYPE_NORMAL
- en: Efficiency
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Speedup doesn’t factor in the number of cores—it is simply the ratio of the
    serial time to the parallel time. For example, if a serial program takes 60 seconds,
    but a parallel program takes 30 seconds on four cores, it still gets a speedup
    of 2\. However, that metric doesn’t capture the fact that it ran on four cores.
  prefs: []
  type: TYPE_NORMAL
- en: 'To measure the speedup per core, use efficiency:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/equ0710-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Efficiency typically varies from 0 to 1\. An efficiency of 1 indicates that
    the cores are being used perfectly. If efficiency is close to 0, then there is
    little to no benefit to parallelism, as the additional cores do not improve performance.
    If efficiency is greater than 1, it indicates superlinear speedup.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s revisit the previous example in which a serial program takes 60 seconds.
    If the parallel version takes 30 seconds on two cores, then its efficiency is
    1 (or 100%). If instead the program takes 30 seconds on four cores, then the efficiency
    drops to 0.5 (or 50%).
  prefs: []
  type: TYPE_NORMAL
- en: Parallel Performance in the Real World
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In an ideal world, speedup is linear. For each additional compute unit, a parallel
    program should achieve a commensurate amount of speedup. However, this scenario
    rarely occurs in the real world. Most programs contain a necessarily serial component
    that exists due to inherent dependencies in the code. The longest set of dependencies
    in a program is referred to as its *critical path*. Reducing the length of a program’s
    critical path is an important first step in its parallelization. Thread synchronization
    points and (for programs running on multiple compute nodes) communication overhead
    between processes are other components in the code that can limit a program’s
    parallel performance.
  prefs: []
  type: TYPE_NORMAL
- en: '**Warning NOT ALL PROGRAMS ARE GOOD CANDIDATES FOR PARALLELISM!**'
  prefs: []
  type: TYPE_NORMAL
- en: The length of the critical path can make some programs downright *hard* to parallelize.
    As an example, consider the problem of generating the *n*th Fibonacci number.
    Since every Fibonacci number is dependent on the two before it, parallelizing
    this program efficiently is very difficult!
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the parallelization of the `countElems` function of the CountSort
    algorithm from earlier in this chapter. In an ideal world, we would expect the
    speedup of the program to be linear with respect to the number of cores. However,
    let’s measure its runtime (in this case, running on a quad-core system with eight
    logical threads):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[Table 14-3](ch14.xhtml#ch14tab3) shows the speedup and efficiency for these
    multithreaded executions.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 14-3:** Performance Benchmarks'
  prefs: []
  type: TYPE_NORMAL
- en: '| Number of threads | 2 | 4 | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| Speedup | 1.68 | 2.36 | 3.08 |'
  prefs: []
  type: TYPE_TB
- en: '| Efficiency | 0.84 | 0.59 | 0.39 |'
  prefs: []
  type: TYPE_TB
- en: We have 84% efficiency with two cores, but the core efficiency falls to 39%
    with eight cores. Notice that the ideal speedup of eight was not met. One reason
    for this is that the overhead of assigning work to threads and the serial update
    to the `counts` array starts dominating performance at higher numbers of threads.
    Second, resource contention by the eight threads (remember this is a quad-core
    processor) reduces core efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Amdahl’s Law
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In 1967, Gene Amdahl, a leading computer architect at IBM, predicted that the
    maximum speedup that a computer program can achieve is limited by the size of
    its necessarily serial component (now referred to as Amdahl’s Law). More generally,
    Amdahl’s Law states that for every program, there is a component that can be sped
    up (i.e., the fraction of a program that can be optimized or parallelized, *P*),
    and a component that *cannot* be sped up (i.e., the fraction of a program that
    is inherently serial, or *S*). Even if the time needed to execute the optimizable
    or parallelizable component *P* is reduced to zero, the serial component *S* will
    exist, and will come to eventually dominate performance. Since *S* and *P* are
    fractions, note that *S* + *P* = 1.
  prefs: []
  type: TYPE_NORMAL
- en: Consider a program that executes on one core in time *T*[1]. Then, the fraction
    of the program execution that is necessarily serial takes *S* × *T*[1] time to
    run, and the parallelizable fraction of program execution (*P* = 1 *– S*) takes
    *P* × *T*[1] to run.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the program executes on *c* cores, the serial fraction of the code still
    takes *S* × *T*[1] time to run (all other conditions being equal), but the parallelizable
    fraction can be divided into *c* cores. Thus, the maximum improvement for the
    parallel processor with *c* cores to run the same job is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/equ0711-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As *c* increases, the execution time on the parallel processor becomes dominated
    by the serial fraction of the program.
  prefs: []
  type: TYPE_NORMAL
- en: To understand the impact of Amdahl’s law, consider a program that is 90% parallelizable
    and executes in 10 seconds on 1 core. In our equation, the parallelizable component
    (*P*) is 0.9, while the serial component (*S*) is 0.1\. [Table 14-4](ch14.xhtml#ch14tab4)
    depicts the corresponding total time on *c* cores (*T*[*c*]) according to Amdahl’s
    Law, and the associated speedup.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 14-4:** The Effect of Amdahl’s Law on a 10-Second Program that is 90%
    Parallelizable'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Number of cores** | **Serial time (s)** | **Parallel time (s)** | **Total
    time (***T*[*c*] **s)** | **Speedup (over one core)** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 9 | 10 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 1 | 0.9 |   1.9 | 5.26 |'
  prefs: []
  type: TYPE_TB
- en: '| 100 | 1 | 0.09 |   1.09 | 9.17 |'
  prefs: []
  type: TYPE_TB
- en: '| 1000 | 1 | 0.009 |   1.009 | 9.91 |'
  prefs: []
  type: TYPE_TB
- en: Observe that, over time, the serial component of the program begins to dominate,
    and the effect of adding more and more cores seems to have little to no effect.
  prefs: []
  type: TYPE_NORMAL
- en: 'A more formal way to look at this requires incorporating Amdahl’s calculation
    for *T*[c] into the equation for speedup:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/equ0712-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Taking the limit of this equation shows that as the number of cores (*c*) approaches
    infinity, speedup approaches 1/*S*. In the example shown in [Table 14-4](ch14.xhtml#ch14tab4),
    speedup approaches 1/0.1, or 10.
  prefs: []
  type: TYPE_NORMAL
- en: As another example, consider a program where *P* = 0.99\. In other words, 99%
    of the program is parallelizable. As *c* approaches infinity, the serial time
    starts to dominate the performance (in this example, *S* = 0.01). Thus, speedup
    approaches 1/0.01 or 100\. In other words, even with a million cores, the maximum
    speedup achievable by this program is only 100.
  prefs: []
  type: TYPE_NORMAL
- en: 'ALL IS NOT LOST: THE LIMITS OF AMDAHL’S LAW'
  prefs: []
  type: TYPE_NORMAL
- en: 'When learning about Amdahl’s Law, it’s important to consider the *intentions*
    of its originator, Gene Amdahl. In his own words, the law was proposed to demonstrate
    “the continued validity of the single processor approach, and the weakness of
    the multiple processor approach in terms of application to real problems and their
    attendant irregularities.”^([8](ch14.xhtml#fn14_8)) In his 1967 paper Amdahl expanded
    on this concept, writing: “For over a decade prophets have voiced the contention
    that the organization of a single computer has reached its limits, and that truly
    significant advances can be made only by interconnection of a multiplicity of
    computers in such a manner as to permit cooperative solution.” Subsequent work
    challenged some of the key assumptions made by Amdahl. Read about the Gustafson–Barsis
    Law in the next subsection for a discussion on the limits of Amdahl’s Law and
    a different argument on how to think about the benefits of parallelism.'
  prefs: []
  type: TYPE_NORMAL
- en: 14.4.2 Advanced Topics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Gustafson–Barsis Law
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In 1988, John L. Gustafson, a computer scientist and researcher at Sandia National
    Labs, wrote a paper called “Reevaluating Amdahl’s Law.”^([9](ch14.xhtml#fn14_9))
    In this paper, Gustafson calls to light a critical assumption that was made about
    the execution of a parallel program that is not always true.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, Amdahl’s law implies that the number of compute cores *c* and
    the fraction of a program that is parallelizable *P* are independent of each other.
    Gustafson notes that this “is virtually never the case.” While benchmarking a
    program’s performance by varying the number of cores on a fixed set of data is
    a useful academic exercise, in the real world, more cores (or processors, as examined
    in our discussion of distributed memory) are added as the problem grows large.
    “It may be most realistic,” Gustafson writes, “to assume run time, not problem
    size, is constant.”
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, according to Gustafson, it is most accurate to say that “The amount
    of work that can be done in parallel varies linearly with the number of processors.”
  prefs: []
  type: TYPE_NORMAL
- en: Consider a *parallel* program that takes time *T*[c] to run on a system with
    *c* cores. Let *S* represent the fraction of the program execution that is necessarily
    serial and takes *S* × *T*[c] time to run. Thus, the parallelizable fraction of
    the program execution, *P* = 1 *– S*, takes *P* × *T*[c] time to run on *c* cores.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the same program is run on just one core, the serial fraction of the code
    still takes *S* × *T*[c] (assuming all other conditions are equal). However, the
    parallelizable fraction (which was divided between *c* cores) now has to be executed
    by just one core to run serially and takes *P* × *T*[c] × *c* time. In other words,
    the parallel component will take *c* times as long on a single-core system. It
    follows that the scaled speedup would be:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/equ0713-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This shows that the scaled speedup increases linearly with the number of compute
    units.
  prefs: []
  type: TYPE_NORMAL
- en: Consider our prior example in which 99% of a program is parallelizable (i.e.,
    *P* = 0.99). Applying the scaled speedup equation, the theoretical speedup on
    100 processors would be 99.01\. On 1,000 processors, it would be 990.01\. Notice
    that the efficiency stays constant at *P*.
  prefs: []
  type: TYPE_NORMAL
- en: As Gustafson concludes, “speedup should be measured by scaling the problem to
    the number of processors, not by fixing a problem size.” Gustafson’s result is
    notable because it shows that it is possible to get increasing speedup by updating
    the number of processors. As a researcher working in a national supercomputing
    facility, Gustafson was more interested in doing *more work* in a constant amount
    of time. In several scientific fields, the ability to analyze more data usually
    leads to higher accuracy or fidelity of results. Gustafson’s work showed that
    it was possible to get large speedups on large numbers of processors, and revived
    interest in parallel processing.^([10](ch14.xhtml#fn14_10))
  prefs: []
  type: TYPE_NORMAL
- en: Scalability
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We describe a program as *scalable* if we see improving (or constant) performance
    as we increase the number of resources (cores, processors) or the problem size.
    Two related concepts are *strong scaling* and *weak scaling*. It is important
    to note that “weak” and “strong” in this context do not indicate the *quality*
    of a program’s scalability, but are simply different ways to measure scalability.
  prefs: []
  type: TYPE_NORMAL
- en: We say that a program is *strongly scalable* if increasing the number of cores/processing
    units on a *fixed* problem size yields an improvement in performance. A program
    displays strong linear scalability if, when run on *n* cores, the speedup is also
    *n*. Of course, Amdahl’s Law guarantees that after some point, adding additional
    cores makes little sense.
  prefs: []
  type: TYPE_NORMAL
- en: We say that a program is *weakly scalable* if increasing the size of the data
    at the same rate as the number of cores (i.e., if there is a fixed data size per
    core/processor) results in constant or an improvement in performance. We say a
    program displays weak linear scalability if we see an improvement of *n* if the
    work per core is scaled up by a factor of *n*.
  prefs: []
  type: TYPE_NORMAL
- en: General Advice Regarding Measuring Performance
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We conclude our discussion on performance with some notes about benchmarking
    and performance on hyperthreaded cores.
  prefs: []
  type: TYPE_NORMAL
- en: '**Run a program multiple times when benchmarking.**   In many of the examples
    shown thus far in this book, we run a program only once to get a sense of its
    runtime. However, this is not sufficient for formal benchmarks. Running a program
    once is *never* an accurate measure of a program’s true runtime! Context switches
    and other running processes can temporarily cause the runtime to radically fluctuate.
    Therefore, it is always best to run a program several times and report an average
    runtime together with as many details as feasible, including number of runs, observed
    variability of the measurements (e.g., error bars, minimum, maximum, median, standard
    deviation) and conditions under which the measurements were taken.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Be careful where you measure timing.**   The `gettimeofday` function is useful
    in helping to accurately measure the time a program takes to run. However, it
    can also be abused. Even though it may be tempting to place the `gettimeofday`
    call around only the thread creation and joining component in `main`, it is important
    to consider what exactly you are trying to time. For example, if a program reads
    in an external data file as a necessary part of its execution, the time for file
    reading should likely be included in the program’s timing.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Be aware of the impact of hyperthreaded cores.**   As discussed in “Taking
    a Closer Look: How Many Cores?” on [page 671](ch14.xhtml#lev3_112) and “Multicore
    and Hardware Multithreading” on [page 283](ch05.xhtml#lev2_108), hyperthreaded
    (logical) cores are capable of executing multiple threads on a single core. In
    a quad-core system with two logical threads per core, we say there are eight hyperthreaded
    cores on the system. Running a program in parallel on eight logical cores in many
    cases yields better wall time than running a program on four cores. However, due
    to the resource contention that usually occurs with hyperthreaded cores, you may
    see a dip in core efficiency and nonlinear speedup.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Beware of resource contention.**   When benchmarking, it’s always important
    to consider what *other* processes and threaded applications are running on the
    system. If your performance results ever look a bit strange, it is worth quickly
    running `top` to see whether there are any other users also running resource-intensive
    tasks on the same system. If so, try using a different system to benchmark (or
    wait until the system is not so heavily used).'
  prefs: []
  type: TYPE_NORMAL
- en: 14.5 Cache Coherence and False Sharing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Multicore caches can have profound implications on a multithreaded program’s
    performance. First, however, let’s quickly review some of the basic concepts related
    to cache design (see “CPU Caches” on page 1299 for more details):'
  prefs: []
  type: TYPE_NORMAL
- en: Data/instructions are not transported *individually* to the cache. Instead,
    data is transferred in *blocks*, and block sizes tend to get larger at lower levels
    of the memory hierarchy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each cache is organized into a series of sets, with each set having a number
    of lines. Each line holds a single block of data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The individual bits of a memory address are used to determine the set, tag,
    and block offset of the cache to which to write a block of data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A *cache hit* occurs when the desired data block exists in the cache. Otherwise,
    a *cache miss* occurs, and a lookup is performed on the next lower level of the
    memory hierarchy (which can be cache or main memory).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *valid bit* indicates if a block at a particular line in the cache is safe
    to use. If the valid bit is set to 0, the data block at that line cannot be used
    (e.g., the block could contain data from an exited process).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Information is written to cache/memory based on two main strategies. In the
    *write-through* strategy, the data is written to cache and main memory simultaneously.
    In the *write-back* strategy, data is written only to cache and gets written to
    lower levels in the hierarchy after the block is evicted from the cache.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 14.5.1 Caches on Multicore Systems
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Recall that in shared memory architectures each core can have its own cache
    (see “Looking Ahead: Caching on Multicore Processors” on [page 581](ch11.xhtml#lev1_91))
    and that multiple cores can share a common cache. [Figure 14-8](ch14.xhtml#ch14fig8)
    depicts an example dual-core CPU. Even though each core has its own local L1 cache,
    the cores share a common L2 cache.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/14fig08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 14-8: An example dual-core CPU with separate L1 caches and a shared
    L2 cache*'
  prefs: []
  type: TYPE_NORMAL
- en: Multiple threads in a single executable may execute separate functions. Without
    a *cache coherency* strategy (see “Cache Coherency” on [page 583](ch11.xhtml#lev2_198))
    to ensure that each cache maintains a consistent view of shared memory, it is
    possible for shared variables to be updated inconsistently. As an example, consider
    the dual-core processor in [Figure 14-8](ch14.xhtml#ch14fig8), where each core
    is busy executing separate threads concurrently. The thread assigned to Core 0
    has a local variable `x`, whereas the thread executing on Core 1 has a local variable
    `y`, and both threads have shared access to a global variable `g`. [Table 14-5](ch14.xhtml#ch14tab5)
    shows one possible path of execution.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 14-5:** Problematic Data Sharing Due to Caching'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Time** | **Core 0** | **Core 1** |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | `g = 5` | (other work) |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | (other work) | `y = g*4` |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | `x += g` | `y += g*2` |'
  prefs: []
  type: TYPE_TB
- en: 'Suppose that the initial value of `g` is 10, and the initial values of `x`
    and `y` are both 0\. What is the final value of `y` at the end of this sequence
    of operations? Without cache coherence, this is a very difficult question to answer
    given that there are at least three stored values of `g`: one in Core 0’s L1 cache,
    one in Core 1’s L1 cache, and a separate copy of `g` stored in the shared L2 cache.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 14-9](ch14.xhtml#ch14fig9) shows one possible erroneous result after
    the sequence of operations in [Table 14-5](ch14.xhtml#ch14tab5) completes. Suppose
    that the L1 caches implement a write-back policy. When the thread executing on
    Core 0 writes the value 5 to `g`, it updates only the value of `g` in Core 0’s
    L1 cache. The value of `g` in Core 1’s L1 cache still remains 10, as does the
    copy in the shared L2 cache. Even if a write-through policy is implemented, there
    is no guarantee that the copy of `g` stored in Core 1’s L1 cache gets updated!
    In this case, `y` will have the final value of `60`.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/14fig09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 14-9: A problematic update to caches that do not employ cache coherency*'
  prefs: []
  type: TYPE_NORMAL
- en: A cache coherence strategy invalidates or updates cached copies of shared values
    in other caches when a write to the shared data value is made in one cache. The
    *modified shared invalid* (MSI) protocol (discussed in detail in “The MSI Protocol”
    on [page 584](ch11.xhtml#lev2_199)) is one example of an invalidating cache coherency
    protocol.
  prefs: []
  type: TYPE_NORMAL
- en: A common technnique for implementing MSI is snooping. Such a *snoopy cache*
    “snoops” on the memory bus for possible write signals. If the snoopy cache detects
    a write to a shared cache block, it invalidates its line containing that cache
    block. The end result is that the only valid version of the block is in the cache
    that is written to, whereas *all other* copies of the block in other caches are
    marked as invalid.
  prefs: []
  type: TYPE_NORMAL
- en: Employing the MSI protocol with snoooping would yield the correct final assignment
    of `30` to variable `y` in the previous example.
  prefs: []
  type: TYPE_NORMAL
- en: 14.5.2 False Sharing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Cache coherence guarantees correctness, but it can potentially harm performance.
    Recall that when the thread updates `g` on Core 0, the snoopy cache invalidates
    not only `g`, but the *entire cache line* that `g` is a part of.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider our initial attempt at parallelizing the `countElems` function of
    the CountSort algorithm.⁴ For convenience, the function is reproduced here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'In our previous discussion of this function (see “Data Races” on [page 691](ch14.xhtml#lev3_116)),
    we pointed out how data races can cause the `counts` array to not populate with
    the correct set of counts. Let’s see what happens if we attempt to *time* this
    function. We add timing code to `main` using `getimeofday` as before.⁶ Benchmarking
    the initial version of `countElems` as just shown on 100 million elements yields
    the following times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Even without any synchronization constructs, this version of the program *still
    gets slower* as the number of threads increases!
  prefs: []
  type: TYPE_NORMAL
- en: To understand what is going on, let’s revisit the `counts` array. This holds
    the frequency of occurrence of each number in our input array. The maximum value
    is determined by the variable `MAX`. In our example program, `MAX` is set to 10\.
    In other words, the `counts` array takes up 40 bytes of space.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that the cache details on a Linux system (see “Looking Ahead: Caching
    on Multicore Processors” on [page 581](ch11.xhtml#lev1_91)) are located in the
    `/sys/devices/system/cpu/` directory. Each logical core has its own `cpu` subdirectory
    called `cpuk`, where `k` indicates the *k*th logical core. Each `cpu` subdirectory
    in turn has separate `index` directories that indicate the caches available to
    that core.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `index` directories contain files with numerous details about each logical
    core’s caches. The contents of a sample `index0` directory are shown here (`index0`
    typically corresponds to a Linux system’s L1 cache):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'To discover the cache line size of the L1 cache, use this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: The output reveals that the L1 cache line size for the machine is 64 bytes.
    In other words, the 40-byte `counts` array fits *within one cache line*.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that with invalidating cache coherence protocols like MSI, every time
    a program updates a shared variable, the *entire cache line in other caches storing
    the variable is invalidated*. Let’s consider what happens when two threads execute
    the preceding function. One possible path of execution is shown in [Table 14-6](ch14.xhtml#ch14tab6)
    (assuming that each thread is assigned to a separate core, and the variable `x`
    is local to each thread).
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 14-6:** A Possible Execution Sequence of Two Threads Running `countElems`'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Time** | **Thread 0** | **Thread 1** |'
  prefs: []
  type: TYPE_TB
- en: '| *i* | Reads `array[x]` | … |'
  prefs: []
  type: TYPE_TB
- en: '|  | (1) |  |'
  prefs: []
  type: TYPE_TB
- en: '| *i* + 1 | Increments `counts[1]` (**invalidates** | Reads `array[x]` (4)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | **cache line**) |  |'
  prefs: []
  type: TYPE_TB
- en: '| *i* + 2 | Reads `array[x]` (6) | Increments `counts[4]` (**invalidates**
    |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | **cache line**) |'
  prefs: []
  type: TYPE_TB
- en: '| *i* + 3 | Increments `counts[6]` (**invalidates** | Reads `array[x]` (2)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | **cache line**) |  |'
  prefs: []
  type: TYPE_TB
- en: '| *i* + 4 | Reads `array[x]` (3) | Increments `counts[2]` (**invalidates**
    |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | **cache line**) |'
  prefs: []
  type: TYPE_TB
- en: '| *i* + 5 | Increments `counts[3]` (**invalidates** | … |'
  prefs: []
  type: TYPE_TB
- en: '|  | **cache line**) |  |'
  prefs: []
  type: TYPE_TB
- en: During time step *i*, Thread 0 reads the value at `array[x]` in its part of
    the array, which is a 1 in this example. During time steps *i* + 1 to *i* + 5,
    each thread reads a value from `array[x]`. Note that each thread is looking at
    different components of the array. Not only that, each read of `array` in our
    sample execution yields unique values (so no race conditions in this sample execution
    sequence!). After reading the value from `array[x]`, each thread increments the
    associated value in `counts`.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that the `counts` array *fits on a single cache line* in our L1 cache.
    As a result, every write to `counts` invalidates the *entire line* in *every other
    L1 cache*. The end result is that, despite updating *different* memory locations
    in `counts`, any cache line containing `counts` is *invalidated* with *every update*
    to `counts`!
  prefs: []
  type: TYPE_NORMAL
- en: The invalidation forces all L1 caches to update the line with a “valid” version
    from L2\. The repeated invalidation and overwriting of lines from the L1 cache
    is an example of *thrashing*, where repeated conflicts in the cache cause a series
    of misses.
  prefs: []
  type: TYPE_NORMAL
- en: The addition of more cores makes the problem worse, given that now more L1 caches
    are invalidating the line. As a result, adding additional threads slows down the
    runtime, despite the fact that each thread is accessing different elements of
    the `counts` array! This is an example of *false sharing*, or the illusion that
    individual elements are being shared by multiple cores. In the previous example,
    it appears that all the cores are accessing the same elements of `counts`, even
    though this is not the case.
  prefs: []
  type: TYPE_NORMAL
- en: 14.5.3 Fixing False Sharing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One way to fix an instance of false sharing is to pad the array (in our case
    `counts`) with additional elements so that it doesn’t fit in a single cache line.
    However, padding can waste memory, and may not eliminate the problem from all
    architectures (consider the scenario in which two different machines have different
    L1 cache sizes). In most cases, writing code to support different cache sizes
    is generally not worth the gain in performance.
  prefs: []
  type: TYPE_NORMAL
- en: A better solution is to have threads write to *local storage* whenever possible.
    Local storage in this context refers to memory that is *local* to a thread. The
    following solution reduces false sharing by choosing to perform updates to a locally
    declared version of `counts` called `local_counts`.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s revisit the final version of our `countElems` function:⁶
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'The use of `local_counts` to accumulate frequencies in lieu of `counts` is
    the major source of reduction of false sharing in this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Since cache coherence is meant to maintain a consistent view of shared memory,
    the invalidations trigger only on *writes* to *shared values* in memory. Since
    `local_counts` is not shared among the different threads, a write to it will not
    invalidate its associated cache line.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the last component of the code, the mutex enforces correctness by ensuring
    that only one thread updates the shared `counts` array at a time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Since `counts` is located on a single cache line, it will still get invalidated
    with every write. The difference is that the penalty here is at most `MAX` × *t*
    writes vs. *n* writes, where *n* is the length of our input array, and *t* is
    the number of threads employed.
  prefs: []
  type: TYPE_NORMAL
- en: 14.6 Thread Safety
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, we have covered synchronization constructs that programmers can use
    to ensure that their multithreaded programs are consistent and correct regardless
    of the number of threads employed. However, it is not always safe to make the
    assumption that standard C library functions can be used “as is” in the context
    of any multithreaded application. Not all functions in the C library are *thread
    safe*, or capable of being run by multiple threads while guaranteeing a correct
    result without unintended side effects. To ensure that the programs *we* write
    are thread safe, it is important to use synchronization primitives like mutexes
    and barriers to enforce that multithreaded programs are consistent and correct
    regardless of how the number of threads varies.
  prefs: []
  type: TYPE_NORMAL
- en: Another closely related concept related to thread safety is re-entrancy. All
    thread safe code is re-entrant; however, not all re-entrant code is thread safe.
    A function is *re-entrant* if it can be re-executed/partially executed by a function
    without causing issue. By definition, re-entrant code ensures that accesses to
    the global state of a program always result in that global state remaining consistent.
    While re-entrancy is often (incorrectly) used as a synonym for thread safety,
    there are special cases for which re-entrant code is not thread safe.
  prefs: []
  type: TYPE_NORMAL
- en: When writing multithreaded code, verify that the C library functions used are
    indeed thread safe. Fortunately, the list of thread-unsafe C library functions
    is fairly small. The Open Group kindly maintains a list of thread unsafe functions.^([11](ch14.xhtml#fn14_11))
  prefs: []
  type: TYPE_NORMAL
- en: 14.6.1 Fixing Issues of Thread Safety
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Synchronization primitives are the most common way to fix issues related to
    thread safety. However, unknowingly using thread-unsafe C library functions can
    cause subtle issues. Let’s look at a slightly modified version of our `countsElem`
    function called `countElemsStr`, which attempts to count the frequency of digits
    in a given string, where each digit is separated by spaces. The following program
    has been edited for brevity; the full source of this program is available online.^([12](ch14.xhtml#fn14_12))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: The `countElemsStr` function uses the `strtok` function (as examined in our
    discussion in “strtok, strtok_r” on [page 100](ch02.xhtml#lev3_22)) to parse each
    digit (stored in `token`) in the string, before converting it to an integer and
    making the associated updates in the `counts` array.
  prefs: []
  type: TYPE_NORMAL
- en: 'Compiling and running this program on 100,000 elements yields the following
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s take a look at a multithreaded version of `countElemsStr`:^([13](ch14.xhtml#fn14_13))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: In this version of the program, each thread processes a separate section of
    the string referenced by `input_str`. The `local_counts` array ensures that the
    bulk of the write operations occur to local storage. A mutex is employed to ensure
    that no two threads write to the shared variable `counts`.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, compiling and running this program yields the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Even though mutex locks are used around accesses to the `counts` array, the
    results from separate runs are radically different. This issue arises because
    the `countsElemsStr` function is not thread safe, because the string library function
    `strtok` is *not thread safe*! Visiting the OpenGroup website^(11) confirms that
    `strtok` is on the list of thread-unsafe functions.
  prefs: []
  type: TYPE_NORMAL
- en: To fix this issue, it suffices to replace `strtok` with its thread-safe alternative,
    `strtok_r`. In the latter function, a pointer is used as the last parameter to
    help the thread keep track of where in the string it is parsing. Here is the fixed
    function with `strtok_r`:^([14](ch14.xhtml#fn14_14))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'The only change in this version of the code is the declaration of the character
    pointer `saveptr` and replacing all instances of `strtok` with `strtok_r`. Rerunning
    the code with these changes yields the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Now the program produces the same result for every run. The use of `saveptr`
    in conjunction with `strtok_r` ensures that each thread can independently track
    their location when parsing the string.
  prefs: []
  type: TYPE_NORMAL
- en: The takeaway from this section is that one should always check the list of thread-unsafe
    functions in C^(11) when writing multithreaded applications. Doing so can save
    the programmer a lot of heartache and frustration when writing and debugging threaded
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: 14.7 Implicit Threading with OpenMP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Thus far, we have presented shared memory programming using POSIX threads. Although
    Pthreads are great for simple applications, they become increasingly difficult
    to use as programs themselves become more complex. POSIX threads are an example
    of *explicit parallel programming* of threads, requiring a programmer to specify
    exactly what each thread is required to do and when each thread should start and
    stop.
  prefs: []
  type: TYPE_NORMAL
- en: With Pthreads, it can also be challenging to *incrementally* add parallelism
    to an existing sequential program. That is, one must often rewrite the program
    entirely to use threads, which is often not desirable when attempting to parallelize
    a large, existing codebase.
  prefs: []
  type: TYPE_NORMAL
- en: The Open Multiprocessing (OpenMP) library implements an *implicit* alternative
    to Pthreads. OpenMP is built in to GCC and other popular compilers such as LLVM
    and Clang, and can be used with the C, C++, and Fortran programming languages.
    A key advantage of OpenMP is that it enables programmers to parallelize components
    of existing, sequential C code by adding *pragmas* (special compiler directives)
    to parts of the code. Pragmas specific to OpenMP begin with `#pragma omp`.
  prefs: []
  type: TYPE_NORMAL
- en: Detailed coverage of OpenMP is outside the scope of this book, but we do cover
    some common pragmas and show how several can be used in the context of some sample
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: 14.7.1 Common Pragmas
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here are some of the most commonly used pragmas in OpenMP programs:'
  prefs: []
  type: TYPE_NORMAL
- en: '#pragma omp parallel  This pragma creates a team of threads and has each thread
    run the code in its scope (usually a function call) on each thread. An invocation
    of this pragma is usually equivalent to an invocation of the `pthread_create`
    and `pthread_join` function pairing discussed in “Creating and Joining Threads”
    on [page 679](ch14.xhtml#lev2_236). The pragma may have a number of clauses, including
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: num_threads  Specifies the number of threads to create.
  prefs: []
  type: TYPE_NORMAL
- en: private  A list of variables that should be private (or local) to each thread.
    Variables that should be private to a thread can also be declared within the scope
    of the pragma (see below for an example). Each thread gets its own copy of each
    variable.
  prefs: []
  type: TYPE_NORMAL
- en: shared  A listing of variables that should be shared among the threads. There
    is one copy of the variable that is shared among all threads.
  prefs: []
  type: TYPE_NORMAL
- en: default  Indicates whether the determination of which variables should be shared
    is left up to the compiler. In most cases, we want to use `default(none)` and
    specify explicitly which variables should be shared and which should be private.
  prefs: []
  type: TYPE_NORMAL
- en: '#pragma omp for  Specifies that each thread execute a subset of iterations
    of a `for` loop. Although the scheduling of the loops is up to the system, the
    default is usually the “chunking” method first discussed in “Revisiting Scalar
    Multiplication” on [page 682](ch14.xhtml#lev2_239). This is a *static* form of
    scheduling: each thread gets an assigned chunk, and then processes the iterations
    in its chunk. However, OpenMP also makes *dynamic* scheduling easy. In dynamic
    scheduling, each thread gets a number of iterations, and requests a new set upon
    completing processing their iteration. The scheduling policy can be set using
    the following clause:'
  prefs: []
  type: TYPE_NORMAL
- en: schedule(dynamic)  Specifies that a *dynamic* form of scheduling should be used.
    While this is advantageous in some cases, the static (default) form of scheduling
    is usually faster.
  prefs: []
  type: TYPE_NORMAL
- en: '#pragma omp parallel for  This pragma is a combination of the `omp parallel`
    and the `omp for` pragmas. Unlike the `omp for` pragma, the `omp parallel for`
    pragma also generates a team of threads before assigning each thread a set of
    iterations of the loop.'
  prefs: []
  type: TYPE_NORMAL
- en: '#pragma omp critical  This pragma is used to specify that the code under its
    scope should be treated as a *critical section*—that is, only one thread should
    execute the section of code at a time to ensure correct behavior.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are also several *functions* that a thread can access that are often
    useful for execution. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: omp_get_num_threads  Returns the number of threads in the current team that
    is being executed.
  prefs: []
  type: TYPE_NORMAL
- en: omp_set_num_threads  Sets the number of threads that a team should have.
  prefs: []
  type: TYPE_NORMAL
- en: omp_get_thread_num  Returns the identifier of the calling thread.
  prefs: []
  type: TYPE_NORMAL
- en: '**Warning THE OMP PARALLEL FOR DIRECTIVE WORKS ONLY WITH FOR LOOPS!**'
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that the `omp parallel for` pragma works *only* with `for` loops.
    Other types of loops, such as `while` loops and `do`–`while` loops, are not supported.
  prefs: []
  type: TYPE_NORMAL
- en: '14.7.2 Hello Threading: OpenMP Flavored'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let’s revisit our “Hello World” program,² now using OpenMP instead of Pthreads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Note that the OpenMP program is *much* shorter than the Pthreads version. To
    access the OpenMP library functions, we include the header file `omp.h`. The `omp
    parallel num_threads(nthreads)` pragma in `main` creates a set of threads, where
    each thread calls the `HelloWorld` function. The clause `num _threads(nthreads)`
    specifies that a total of `nthreads` should be generated. The pragma also joins
    each created thread back to a single-threaded process. In other words, all the
    low-level work of creating and joining threads is *abstracted* away from the programmer
    and is accomplished with the inclusion of just one pragma. For this reason, OpenMP
    is considered an *implicit threading* library.
  prefs: []
  type: TYPE_NORMAL
- en: OpenMP also abstracts away the need to explicitly manage thread IDs. In the
    context of `HelloWorld`, the `omp_get_thread_num` function extracts the unique
    ID associated with the thread that is running it.
  prefs: []
  type: TYPE_NORMAL
- en: Compiling the code
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Let’s compile and run this program by passing the `-fopenmp` flag to the compiler,
    which signals that we’re compiling with OpenMP:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the execution of threads can change with subsequent runs, rerunning this
    program results in a different sequence of messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: This behavior is consistent with our example with Pthreads (see “Hello Threading!
    Writing Your First Multithreaded Program” on [page 677](ch14.xhtml#lev1_106)).
  prefs: []
  type: TYPE_NORMAL
- en: '14.7.3 A More Complex Example: CountSort in OpenMP'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A powerful advantage of OpenMP is that it enables programmers to incrementally
    parallelize their code. To see this in action, let’s parallelize the more complex
    CountSort algorithm discussed earlier in this chapter. Recall that this algorithm
    sorts arrays containing a small range of values. The main function of the serial
    program³ looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: The `main` function, after doing some command line parsing and generating a
    random array, calls the `countsElems` function followed by the `writeArray` function.
  prefs: []
  type: TYPE_NORMAL
- en: Parallelizing CountElems Using OpenMP
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: There are several ways to parallelize the preceding program. One way (shown
    in the example that follows) uses the `omp parallel` pragma in the context of
    the `countElems` and `writeArray` functions. As a result, no changes need to be
    made to the `main` function.^([15](ch14.xhtml#fn14_15))
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s examine how to parallelize the `countElems` function using OpenMP:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: In this version of the code, three pragmas are employed. The `#pragma omp parallel`
    pragma indicates that a team of threads should be created. The `omp_set_num_threads(nthreads)`
    line in `main` sets the default size of the thread team to be `nthreads`. If the
    `omp_set_num_threads` function is not used, then the number of threads assigned
    will equal the number of cores in the system. As a reminder, the `omp parallel`
    pragma implicitly creates threads at the beginning of the block and joins them
    at the end of the block. Braces (`{}`) are used to specify scope. The `shared`
    clause declares that the variables `counts`, `array`, and `length` are shared
    (global) among all the threads. Thus, the variables `val`, `i`, and `local[MAX]`
    are declared *locally* in each thread.
  prefs: []
  type: TYPE_NORMAL
- en: The next pragma is `#pragma omp for`, which parallelizes the `for` loop, splitting
    the number of iterations among the number of threads. OpenMP calculates how best
    to split up the iterations of the loop. As previously mentioned, the default strategy
    is usually a chunking method, wherein each thread gets roughly the same number
    of iterations to compute. Thus, each thread reads a component of the shared array
    `array`, and accumulates its counts in its local array `local`.
  prefs: []
  type: TYPE_NORMAL
- en: The `#pragma omp critical` pragma indicates that the code in the scope of the
    critical section should be executed by exactly one thread at a time. This is equivalent
    to the mutex that was employed in the Pthreads version of this program. Here,
    each thread increments the shared `counts` array one at a time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s get a sense of the performance of this function by running it with 100
    million elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: This is excellent performance, with our function getting a speedup of 2 on two
    threads, and a speedup of 3.63 on four threads. We get even better performance
    than the Pthreads implementation!
  prefs: []
  type: TYPE_NORMAL
- en: The writeArray Function in OpenMP
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Parallelizing the `writeArray` function is *much* harder. The following code
    shows one possible solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: Prior to parallelizing, we made a change to this function because the old version
    of `writeArray` caused `j` to have a dependency on the previous iterations of
    the loop. In this version, each thread calculates its unique `start` value based
    on the sum of all the previous elements in `counts`.
  prefs: []
  type: TYPE_NORMAL
- en: When this dependency is removed, the parallelization is pretty straightforward.
    The `#pragma omp parallel for` pragma generates a team of threads and parallelizes
    the `for` loop by assigning each thread a subset of the iterations of the loop.
    As a reminder, this pragma is a combination of the `omp parallel` and the `omp
    for` pragmas (which were used in the parallelization of `countElems`).
  prefs: []
  type: TYPE_NORMAL
- en: A chunking approach to scheduling threads (as shown in the earlier `countElems`
    function) is not appropriate here, because it is possible that each element in
    `counts` has a radically different frequency. Therefore, the threads will not
    have equal work, resulting in some threads being assigned more work than others.
    Therefore, the `schedule(dynamic)` clause is employed, so that each thread completes
    the iteration it is assigned before requesting a new iteration from the thread
    manager.
  prefs: []
  type: TYPE_NORMAL
- en: Since each thread is writing to distinct array locations, mutual exclusion is
    not needed for this function.
  prefs: []
  type: TYPE_NORMAL
- en: Notice how much cleaner the OpenMP code is than the POSIX thread implementation.
    The code is very readable and required very little modification. This is one of
    the powers of *abstraction*, in which the implementation details are hidden from
    the programmer.
  prefs: []
  type: TYPE_NORMAL
- en: However, a necessary trade-off for abstraction is control. The programmer assumes
    that the compiler is “smart” enough to take care of the particulars of parallelization
    and thus has an easier time parallelizing their application. However, the programmer
    no longer makes detailed decisions about the particulars of that parallelization.
    Without a clear idea of how OpenMP pragmas execute under the hood, it can be difficult
    to debug an OpenMP application or know which pragma is the most appropriate to
    use at a given time.
  prefs: []
  type: TYPE_NORMAL
- en: 14.7.4 Learning More About OpenMP
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A deeper discussion of OpenMP is beyond the scope of this book, but there are
    useful free resources for learning^([16](ch14.xhtml#fn14_16)) and using^([17](ch14.xhtml#fn14_17))
    OpenMP.
  prefs: []
  type: TYPE_NORMAL
- en: 14.8 Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This chapter provided an overview of multicore processors and how to program
    them. Specifically, we cover the POSIX threads (or Pthreads) library and how to
    use it to create correct multithreaded programs that speed up a single-threaded
    program’s performance. Libraries like POSIX and OpenMP utilize the *shared memory*
    model of communication, as threads share data in a common memory space.
  prefs: []
  type: TYPE_NORMAL
- en: Key Takeaways
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Threads are the fundamental unit of concurrent programs.**   To parallelize
    a serial program, programmers utilize lightweight constructs known as *threads*.
    For a particular multithreaded process, each thread has its own allocation of
    stack memory, but shares the program data, heap and instructions of the process.
    Like processes, threads run *nondeterministically* on the CPU (i.e., the order
    of execution changes between runs, and which thread is assigned to which core
    is left up to the operating system).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Synchronization constructs ensure that programs work correctly.**   A consequence
    of shared memory is that threads can accidentally overwrite data residing in shared
    memory. A *race condition* can occur whenever two operations incorrectly update
    a shared value. When that shared value is data, a special type of race condition
    called a *data race* can arise. Synchronization constructs (mutexes, semaphores,
    etc.) help to guarantee program correctness by ensuring that threads execute one
    at a time when updating shared variables.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Be mindful when using synchronization constructs.**   Synchronization inherently
    introduces points of serial computation in an otherwise parallel program. It is
    therefore important to be aware of *how* one uses synchronization concepts. The
    set of operations that must run atomically is referred to as a *critical section*.
    If a critical section is too big, the threads will execute serially, yielding
    no improvement in runtime. Use synchronization constructs sloppily, and situations
    like *deadlock* may inadvertently arise. A good strategy is to have threads employ
    local variables as much as possible and update shared variables only when necessary.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Not all components of a program are parallelizable.**   Some programs necessarily
    have large serial components that can hinder a multithreaded program’s performance
    on multiple cores (e.g., *Amdahl’s Law*). Even when a high percentage of a program
    is parallelizable, speedup is rarely linear. Readers are also encouraged to look
    at other metrics such as efficiency and scalability when ascertaining the performance
    of their programs.'
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This chapter is meant to give a taste of concurrency topics with threads; it
    is by no means exhaustive. To learn more about programming with POSIX threads
    and OpenMP, check out the excellent tutorials on Pthreads^([18](ch14.xhtml#fn14_18))
    and OpenMP^([19](ch14.xhtml#fn14_19)) by Blaise Barney from Lawrence Livermore
    National Labs. For automated tools for debugging parallel programs, readers are
    encouraged to check out the Helgrind^([20](ch14.xhtml#fn14_20)) and DRD^([21](ch14.xhtml#fn14_21))
    Valgrind tools.
  prefs: []
  type: TYPE_NORMAL
- en: In the final chapter of the book, we give a high-level overview of other common
    parallel architectures and how to program them.
  prefs: []
  type: TYPE_NORMAL
- en: Notes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[1.](ch14.xhtml#rfn14_1) *[https://www.raspberrypi.org/](https://www.raspberrypi.org/)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[2.](ch14.xhtml#rfn14_2) Available at *[https://diveintosystems.org/book/C14-SharedMemory/_attachments/hellothreads.c](https://diveintosystems.org/book/C14-SharedMemory/_attachments/hellothreads.c)*.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3.](ch14.xhtml#rfn14_3) Available at *[https://diveintosystems.org/book/C14-SharedMemory/_attachments/countSort.c](https://diveintosystems.org/book/C14-SharedMemory/_attachments/countSort.c)*.'
  prefs: []
  type: TYPE_NORMAL
- en: '[4.](ch14.xhtml#rfn14_4) Available at *[https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElems_p.c](https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElems_p.c)*.'
  prefs: []
  type: TYPE_NORMAL
- en: '[5.](ch14.xhtml#rfn14_5) The full source can be downloaded from *[https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElems_p_v2.c](https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElems_p_v2.c)*.'
  prefs: []
  type: TYPE_NORMAL
- en: '[6.](ch14.xhtml#rfn14_6) The full source code for this final program can be
    accessed at *[https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElems_p_v3.c](https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElems_p_v3.c)*.'
  prefs: []
  type: TYPE_NORMAL
- en: '[7.](ch14.xhtml#rfn14_7) Available at *[https://diveintosystems.org/book/C14-SharedMemory/_attachments/layeggs.c](https://diveintosystems.org/book/C14-SharedMemory/_attachments/layeggs.c)*.'
  prefs: []
  type: TYPE_NORMAL
- en: '[8.](ch14.xhtml#rfn14_8) Gene Amdahl. “Validity of the single processor approach
    to achieving large scale computing capabilities,” *Proceedings of the April 18-20,
    1967, Spring Joint Computer Conference*, pp. 483–485, ACM, 1967.'
  prefs: []
  type: TYPE_NORMAL
- en: '[9.](ch14.xhtml#rfn14_9) John Gustafson, “Reevaluating Amdahl’s law,” *Communications
    of the ACM* 31(5), pp. 532–533, 1988.'
  prefs: []
  type: TYPE_NORMAL
- en: '[10.](ch14.xhtml#rfn14_10) Caroline Connor, “Movers and Shakers in HPC: John
    Gustafson,” *HPC Wire*, *[http://www.hpcwire.com/hpcwire/2010-10-20/movers_and_shakers_in_hpc_john_gustafson.html](http://www.hpcwire.com/hpcwire/2010-10-20/movers_and_shakers_in_hpc_john_gustafson.html)*.'
  prefs: []
  type: TYPE_NORMAL
- en: '[11.](ch14.xhtml#rfn14_11) *[http://pubs.opengroup.org/onlinepubs/009695399/functions/xsh_chap02_09.html](http://pubs.opengroup.org/onlinepubs/009695399/functions/xsh_chap02_09.html)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[12.](ch14.xhtml#rfn14_12) [https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElemsStr.c](https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElemsStr.c)'
  prefs: []
  type: TYPE_NORMAL
- en: '[13.](ch14.xhtml#rfn14_13) Available at *[https://diveintosystems.org/book/C14-SharedMemory/_attachmentscountElemsStr_p.c](https://diveintosystems.org/book/C14-SharedMemory/_attachmentscountElemsStr_p.c)*.'
  prefs: []
  type: TYPE_NORMAL
- en: '[14.](ch14.xhtml#rfn14_14) Full source code available at [https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElemsStr_p_v2.c](https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElemsStr_p_v2.c).'
  prefs: []
  type: TYPE_NORMAL
- en: '[15.](ch14.xhtml#rfn14_15) A full version of the program is available at *[https://diveintosystems.org/book/C14-SharedMemory/_attachments/countSort_mp.c](https://diveintosystems.org/book/C14-SharedMemory/_attachments/countSort_mp.c)*.'
  prefs: []
  type: TYPE_NORMAL
- en: '[16.](ch14.xhtml#rfn14_16) Blaise Barney, “OpenMP,” [https://hpc.llnl.gov/tuts/openMP/](https://hpc.llnl.gov/tuts/openMP/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[17.](ch14.xhtml#rfn14_17) Richard Brown and Libby Shoop, “Multicore Programming
    with OpenMP,” *CSinParallel: Parallel Computing in the Computer Science Curriculum*,
    [http://selkie.macalester.edu/csinparallel/modules/MulticoreProgramming/build/html/index.html](http://selkie.macalester.edu/csinparallel/modules/MulticoreProgramming/build/html/index.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[18.](ch14.xhtml#rfn14_18) *[https://hpc-tutorials.llnl.gov/posix/](https://hpc-tutorials.llnl.gov/posix/)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[19.](ch14.xhtml#rfn14_19) *[https://hpc.llnl.gov/tuts/openMP/](https://hpc.llnl.gov/tuts/openMP/)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[20.](ch14.xhtml#rfn14_20) *[https://valgrind.org/docs/manual/hg-manual.html](https://valgrind.org/docs/manual/hg-manual.html)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[21.](ch14.xhtml#rfn14_21) *[https://valgrind.org/docs/manual/drd-manual.html](https://valgrind.org/docs/manual/drd-manual.html)*'
  prefs: []
  type: TYPE_NORMAL
