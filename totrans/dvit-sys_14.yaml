- en: '14'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '14'
- en: LEVERAGING SHARED MEMORY IN THE MULTICORE ERA
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在多核时代利用共享内存
- en: '*The world is changed.*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*世界改变了。*'
- en: '*I feel it in the silica.*'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '*我在硅中感受到它。*'
- en: '*I feel it in the transistor.*'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*我在晶体管中感受到了它。*'
- en: '*I see it in the core.*'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*我在核心中看到了它。*'
- en: '*–With apologies to Galadriel*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*——向加拉德丽尔致歉*'
- en: 'Lord of the Rings: Fellowship of the Ring'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 《指环王：护戒使者》
- en: '![image](../images/common.jpg)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/common.jpg)'
- en: Until now, our discussion of architecture has focused on a purely single-CPU
    world. But the world has changed. Today’s CPUs have multiple *cores*, or compute
    units. In this chapter, we discuss multicore architectures, and how to leverage
    them to speed up the execution of programs.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论的架构集中在纯单CPU世界中。但世界已经发生了变化。今天的CPU拥有多个*核心*或计算单元。在本章中，我们将讨论多核架构，并介绍如何利用它们加速程序的执行。
- en: '**Note CPUS, PROCESSORS, AND CORES**'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：CPU、处理器与核心**'
- en: 'In many instances in this chapter, the terms *processor* and *CPU* are used
    interchangeably. At a fundamental level, a *processor* is any circuit that performs
    some computation on external data. Based on this definition, the *central processing
    unit* (CPU) is an example of a processor. A processor or a CPU with multiple compute
    cores is referred to as a *multicore processor* or a *multicore CPU*. A *core*
    is a compute unit that contains many of the components that make up the classical
    CPU: an ALU, registers, and a bit of cache. Although a *core* is different from
    a processor, it is not unusual to see these terms used interchangeably in the
    literature (especially if the literature originated at a time when multicore processors
    were still considered novel).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的许多实例中，*处理器*和*中央处理器（CPU）*这两个术语是可以互换使用的。从根本上讲，*处理器*是执行外部数据计算的任何电路。根据这个定义，*中央处理单元（CPU）*是一个处理器的例子。具有多个计算核心的处理器或CPU被称为*多核处理器*或*多核CPU*。*核心*是一个计算单元，包含构成经典CPU的许多组件：算术逻辑单元（ALU）、寄存器和一些缓存。虽然*核心*与处理器不同，但在文献中看到这些术语互换使用并不罕见（尤其是在文献出现在多核处理器还被认为是新兴技术的时期）。
- en: 'In 1965, the founder of Intel, Gordon Moore, estimated that the number of transistors
    in an integrated circuit would double every year. His prediction, now known as
    *Moore’s Law*, was later revised to transistor counts doubling every *two* years.
    Despite the evolution of electronic switches from Bardeen’s transistor to the
    tiny chip transistors that are currently used in modern computers, Moore’s Law
    has held true for the past 50 years. However, the turn of the millennium saw processor
    design hit several critical performance walls:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 1965年，英特尔创始人戈登·摩尔估计，集成电路中的晶体管数量每年会翻一番。他的预测，现在被称为*摩尔定律*，后来被修正为晶体管数量每*两年*翻一番。尽管电子开关从巴尔登的晶体管演变为现代计算机中使用的微型芯片晶体管，摩尔定律在过去50年里依然有效。然而，千年之交，处理器设计遇到了几个关键的性能瓶颈：
- en: 'The *memory wall*: Improvements in memory technology did not keep pace with
    improvements in clock speed, resulting in memory becoming a bottleneck to performance.
    As a result, continuously speeding up the execution of a CPU no longer improves
    its overall system performance.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*内存墙*：内存技术的进步没有跟上时钟速度的提升，导致内存成为性能的瓶颈。因此，持续加速CPU的执行不再能改善其整体系统性能。'
- en: 'The *power wall*: Increasing the number of transistors on a processor necessarily
    increases that processor’s temperature and power consumption, which in turn increases
    the required cost to power and cool the system. With the proliferation of multicore
    systems, power is now the dominant concern in computer system design.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*功率墙*：增加处理器上的晶体管数量必然会提高该处理器的温度和功耗，从而增加为系统提供电力和散热的成本。随着多核系统的普及，电力已成为计算机系统设计中的主要问题。'
- en: The power and memory walls caused computer architects to change the way they
    designed processors. Instead of adding more transistors to increase the speed
    at which a CPU executes a single stream of instructions, architects began adding
    multiple *compute cores* to a CPU. Compute cores are simplified processing units
    that contain fewer transistors than traditional CPUs and are generally easier
    to create. Combining multiple cores on one CPU allows the CPU to execute *multiple*
    independent streams of instructions at once.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 电力和内存瓶颈促使计算机架构师改变了设计处理器的方式。与其增加更多晶体管来提高 CPU 执行单一指令流的速度，架构师开始在 CPU 中增加多个*计算核心*。计算核心是简化的处理单元，包含比传统
    CPU 更少的晶体管，且通常更容易制造。在一个 CPU 上组合多个核心可以让 CPU 同时执行*多个*独立的指令流。
- en: '**Warning MORE CORES != BETTER**'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**警告：更多核心 != 更好**'
- en: It may be tempting to assume that all cores are equal and that the more cores
    a computer has, the better it is. This is not necessarily the case! For example,
    *graphics processing unit* (GPU) cores have even fewer transistors than CPU cores,
    and are specialized for particular tasks involving vectors. A typical GPU can
    have 5,000 or more GPU cores. However, GPU cores are limited in the types of operations
    that they can perform and are not always suitable for general-purpose computing
    like the CPU core. Computing with GPUs is known as *manycore* computing. In this
    chapter, we concentrate on *multicore* computing. See [Chapter 15](ch15.xhtml#ch15)
    for a discussion of manycore computing.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 可能会让人产生误解，认为所有核心都是相同的，并且计算机核心越多，性能就越好。但事实并非如此！例如，*图形处理单元*（GPU）核心比 CPU 核心的晶体管还要少，并且专门用于涉及向量的特定任务。一个典型的
    GPU 可能有 5000 个或更多 GPU 核心。然而，GPU 核心在它们能够执行的操作类型上是有限制的，并且并不总是适合像 CPU 核心那样的通用计算。使用
    GPU 进行计算被称为*多核*计算。在本章中，我们集中讨论*多核*计算。有关多核计算的讨论，请参见[第15章](ch15.xhtml#ch15)。
- en: 'Taking a Closer Look: How Many Cores?'
  id: totrans-18
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 更深入的探讨：多少个核心？
- en: 'Almost all modern computer systems have multiple cores, including small devices
    like the Raspberry Pi.^([1](ch14.xhtml#fn14_1)) Identifying the number of cores
    on a system is critical for accurately measuring the performance of multicore
    programs. On Linux and macOS computers, the `lscpu` command provides a summary
    of a system’s architecture. In the following example, we show the output of the
    `lscpu` command when run on a sample machine (some output is omitted to emphasize
    the key features):'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有现代计算机系统都有多个核心，包括像树莓派这样的微型设备。^([1](ch14.xhtml#fn14_1)) 确定系统上的核心数量对于准确衡量多核程序的性能至关重要。在
    Linux 和 macOS 计算机上，`lscpu` 命令提供了系统架构的摘要。在以下示例中，我们展示了在一台样本机器上运行 `lscpu` 命令时的输出（部分输出被省略，以强调关键特性）：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `lscpu` command gives a lot of useful information, including the type of
    processors, the core speed, and the number of cores. To calculate the number of
    *physical* (or actual) cores on a system, multiply the number of sockets by the
    number of cores per socket. The sample `lscpu` output shown in the preceding example
    reveals that the system has one socket with four cores per socket, or four physical
    cores in total.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '`lscpu` 命令提供了很多有用的信息，包括处理器类型、核心速度和核心数量。要计算系统中的*物理*（或实际）核心数，需将插槽数量与每个插槽的核心数相乘。前面示例中显示的
    `lscpu` 输出表明，该系统有一个插槽，每个插槽有四个核心，总共有四个物理核心。'
- en: HYPERTHREADING
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 超线程
- en: 'At first glance, it may appear that the system in the previous example has
    eight cores in total. After all, this is what the “CPU(s)” field seems to imply.
    However, that field actually indicates the number of *hyperthreaded* (logical)
    cores, not the number of physical cores. Hyperthreading, or simultaneous multithreading
    (SMT), enables the efficient processing of multiple threads on a single core.
    Although hyperthreading can decrease the overall runtime of a program, performance
    on hyperthreaded cores does not scale at the same rate as on physical cores. However,
    if one task idles (e.g., due to a control hazard, see “Pipelining Hazards: Control
    Hazards” on [page 279](ch05.xhtml#lev2_106)), another task can still utilize the
    core. In short, hyperthreading was introduced to improve *process throughput*
    (which measures the number of processes that complete in a given unit of time)
    rather than *process speedup* (which measures the amount of runtime improvement
    of an individual process). Much of our discussion of performance in the coming
    chapter will focus on speedup.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 刚开始看，可能会觉得前面的示例系统总共有八个核心。毕竟，“CPU(s)”字段似乎暗示了这一点。然而，这个字段实际上表示的是*超线程*（逻辑）核心的数量，而不是物理核心的数量。超线程技术，或称为同时多线程（SMT），使得在单个核心上可以高效地处理多个线程。虽然超线程可以减少程序的总体运行时间，但超线程核心的性能提升并不像物理核心那样按相同比例增长。然而，如果某个任务处于空闲状态（例如，由于控制冒险，参见[第279页](ch05.xhtml#lev2_106)的“流水线冒险：控制冒险”部分），另一个任务仍然可以使用该核心。简而言之，超线程的引入是为了提高*进程吞吐量*（即在给定时间内完成的进程数量），而不是*进程加速*（即衡量单个进程的运行时间改善）。在接下来的章节中，我们将重点讨论加速。
- en: 14.1 Programming Multicore Systems
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.1 编程多核系统
- en: Most of the common languages that programmers know today were created prior
    to the multicore age. As a result, many languages cannot *implicitly* (or automatically)
    employ multicore processors to speed up the execution of a program. Instead, programmers
    must specifically write software to leverage the multiple cores on a system.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 今天大多数程序员所熟知的常见编程语言是在多核时代之前创建的。因此，许多语言无法*隐式*（或自动）利用多核处理器来加速程序的执行。相反，程序员必须专门编写软件，以利用系统中的多个核心。
- en: 14.1.1 The Impact of Multicore Systems on Process Execution
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.1.1 多核系统对进程执行的影响
- en: Recall that a process can be thought of as an abstraction of a running program
    (see “Processes” on [page 624](ch13.xhtml#lev1_100)). Each process executes in
    its own virtual address space. The operating system (OS) schedules processes for
    execution on the CPU; a *context switch* occurs when the CPU changes which process
    it currently executes.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，进程可以被视为正在运行的程序的抽象（参见[第624页](ch13.xhtml#lev1_100)的“进程”部分）。每个进程在其自己的虚拟地址空间中执行。操作系统（OS）将进程安排在CPU上执行；当CPU切换到执行其他进程时，就会发生*上下文切换*。
- en: '[Figure 14-1](ch14.xhtml#ch14fig1) illustrates how five example processes may
    execute on a single-core CPU.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[图14-1](ch14.xhtml#ch14fig1)展示了五个示例进程如何在单核CPU上执行。'
- en: '![image](../images/14fig01.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/14fig01.jpg)'
- en: '*Figure 14-1: An execution time sequence for five processes as they share a
    single CPU core*'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*图14-1：五个进程在共享单个CPU核心时的执行时间序列*'
- en: The horizontal axis is time, with each time slice taking one unit of time. A
    box indicates when a process is using the single-core CPU. Assume that each process
    executes for one full time slice before a context switch occurs. So, Process 1
    uses the CPU during time steps T1 and T3.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 水平轴表示时间，每个时间片占用一个单位时间。一个框表示进程正在使用单核CPU的时段。假设每个进程在发生上下文切换之前会执行完整的时间片。因此，进程1在时间步骤T1和T3期间使用CPU。
- en: In this example, the order of process execution is P1, P2, P1, P2, P4, P2, P3,
    P4, P5, P3, P5\. We take a moment here to distinguish between two measures of
    time. The *CPU time* measures the amount of time a process takes to execute on
    a CPU. In contrast, the *wall-clock time* measures the amount of time a human
    perceives a process takes to complete. The wall-clock time is often significantly
    longer than the CPU time, due to context switches. For example, Process 1’s CPU
    time requires two time units, whereas its wall-clock time is three time units.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，进程执行的顺序是P1、P2、P1、P2、P4、P2、P3、P4、P5、P3、P5。我们在这里花点时间区分两种时间度量。*CPU时间*衡量一个进程在CPU上执行的时间。相比之下，*实时时间*衡量一个人感知到的进程完成所需的时间。由于上下文切换的存在，实时时间通常比CPU时间长得多。例如，进程1的CPU时间需要两个时间单位，而其实时时间需要三个时间单位。
- en: When the total execution time of one process overlaps with another, the processes
    are running *concurrently* with each other. Operating systems employed concurrency
    in the single-core era to give the illusion that a computer can execute many things
    at once (e.g., you can have a calculator program, a web browser, and a word processing
    document all open at the same time). In truth, each process executes serially
    and the operating system determines the order in which processes execute and complete
    (which often differs in subsequent runs); see “Multiprogramming and Context Switching”
    on [page 625](ch13.xhtml#lev2_221).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Returning to the example, observe that Process 1 and Process 2 run concurrently
    with each other, since their executions overlap at time points T2–T4\. Likewise,
    Process 2 runs concurrently with Process 4, because their executions overlap at
    time points T4–T6\. In contrast, Process 2 does *not* run concurrently with Process
    3, because they share no overlap in their execution; Process 3 only starts running
    at time T7, whereas Process 2 completes at time T6.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: A multicore CPU enables the OS to schedule a different process to each available
    core, allowing processes to execute *simultaneously*. The simultaneous execution
    of instructions from processes running on multiple cores is referred to as *parallel
    execution*. [Figure 14-2](ch14.xhtml#ch14fig2) shows how our example processes
    might execute on a dual-core system.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/14fig02.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
- en: '*Figure 14-2: An execution time sequence for five processes, extended to include
    two CPU cores (one in dark gray, the other in light gray)*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the two CPU cores are colored differently. Suppose that the
    process execution order is again P1, P2, P1, P2, P4, P2, P3, P4, P5, P3, P5\.
    The presence of multiple cores enables certain processes to execute *sooner*.
    For example, during time unit T1, the first core executes Process 1 while the
    second core executes Process 2\. At time T2, the first core executes Process 2
    while the second executes Process 1\. Thus, Process 1 finishes executing after
    time T2, whereas Process 2 finishes executing at time T3.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Note that the parallel execution of multiple processes increases just the number
    of processes that execute at any one time. In [Figure 14-2](ch14.xhtml#ch14fig2),
    all the processes complete execution by time unit T7\. However, each individual
    process still requires the same amount of CPU time to complete as shown in [Figure
    14-1](ch14.xhtml#ch14fig1). For example, Process 2 requires three time units regardless
    of execution on a single or multicore system (i.e., its *CPU time* remains the
    same). A multicore processor increases the *throughput* of process execution,
    or the number of processes that can complete in a given period of time. Thus,
    even though the CPU time of an individual process remains unchanged, its wall-clock
    time may decrease.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: 14.1.2 Expediting Process Execution with Threads
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One way to speed up the execution of a single process is to decompose it into
    lightweight, independent execution flows called *threads*. [Figure 14-3](ch14.xhtml#ch14fig3)
    shows how a process’s virtual address space changes when it is multithreaded with
    two threads. While each thread has its own private allocation of call stack memory,
    all threads *share* the program data, instructions, and the heap allocated to
    the multithreaded process.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/14fig03.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
- en: '*Figure 14-3: Comparing the virtual address space of a single-threaded and
    a multithreaded process with two threads*'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: The OS schedules threads in the same manner as it schedules processes. On a
    multicore processor, the OS can speed up the execution of a multithreaded program
    by scheduling the different threads to run on separate cores. The maximum number
    of threads that can execute in parallel is equal to the number of physical cores
    on the system. If the number of threads exceeds the number of physical cores,
    the remaining threads must wait their turn to execute (similar to how processes
    execute on a single core).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: 'An Example: Scalar Multiplication'
  id: totrans-45
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As an initial example of how to use multithreading to speed up an application,
    consider the problem of performing scalar multiplication of an array `array` and
    some integer `s`. In scalar multiplication, each element in the array is scaled
    by multiplying the element with `s`.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 'A serial implementation of a scalar multiplication function follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Suppose that `array` has *N* total elements. To create a multithreaded version
    of this application with *t* threads, it is necessary to:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Create *t* threads.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Assign each thread a subset of the input array (i.e., *N*/*t* elements).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Instruct each thread to multiply the elements in its array subset by `s`.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that the serial implementation of `scalar_multiply` spends 60 seconds
    multiplying an input array of 100 million elements. To build a version that executes
    with *t* = 4 threads, we assign each thread one fourth of the total input array
    (25 million elements).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 14-4](ch14.xhtml#ch14fig4) shows what happens when we run four threads
    on a single core. As before, the execution order is left to the operating system.
    In this scenario, assume that the thread execution order is Thread 1, Thread 3,
    Thread 2, Thread 4\. On a single-core processor (represented by the squares),
    each thread executes sequentially. Thus, the multithreaded process running on
    one core will still take 60 seconds to run (perhaps a little longer, given the
    overhead of creating threads).'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/14fig04.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
- en: '*Figure 14-4: Running four threads on a single-core CPU*'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Now suppose that we run our multithreaded process on a dual-core system. [Figure
    14-5](ch14.xhtml#ch14fig5) shows the result. Again, assume *t* = 4 threads, and
    that the thread execution order is Thread 1, Thread 3, Thread 2, Thread 4\. Our
    two cores are represented by shaded squares. Since the system is dual-core, Thread
    1 and Thread 3 execute in parallel during time step T1\. Threads 2 and 4 then
    execute in parallel during time step T2\. Thus, the multithreaded process that
    originally took 60 seconds to run now runs in 30 seconds.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们在双核系统上运行我们的多线程进程。[图 14-5](ch14.xhtml#ch14fig5) 显示了结果。再次假设 *t* = 4 个线程，线程执行顺序为
    线程 1、线程 3、线程 2、线程 4。我们的两个核心由阴影方块表示。由于系统是双核，线程 1 和线程 3 在时间步 T1 内并行执行。然后，线程 2 和线程
    4 在时间步 T2 内并行执行。因此，原本需要 60 秒运行的多线程进程现在只需 30 秒。
- en: '![image](../images/14fig05.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/14fig05.jpg)'
- en: '*Figure 14-5: Running four threads on a dual-core CPU*'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 14-5：在双核 CPU 上运行四个线程*'
- en: Finally, suppose that the multithreaded process (*t* = 4) is run on a quad-core
    CPU. [Figure 14-6](ch14.xhtml#ch14fig6) shows one such execution sequence. Each
    of the four cores in [Figure 14-6](ch14.xhtml#ch14fig6) is shaded differently.
    On the quad-core system, each thread executes in parallel during time slice T1\.
    Thus, on a quad-core CPU, the multithreaded process that originally took 60 seconds
    now runs in 15 seconds.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，假设多线程进程（*t* = 4）在四核 CPU 上运行。[图 14-6](ch14.xhtml#ch14fig6) 显示了这样的执行序列。[图 14-6](ch14.xhtml#ch14fig6)中的四个核心分别用不同的阴影表示。在四核系统中，每个线程在时间片
    T1 内并行执行。因此，在四核 CPU 上，原本需要 60 秒的多线程进程现在只需 15 秒。
- en: '![image](../images/14fig06.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/14fig06.jpg)'
- en: '*Figure 14-6: Running four threads on a quad-core CPU*'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 14-6：在四核 CPU 上运行四个线程*'
- en: In general, if the number of threads matches the number of cores (*c*) and the
    operating system schedules each thread to run on a separate core in parallel,
    then the multithreaded process should run in approximately 1/*c* of the time.
    Such linear speedup is ideal, but not frequently observed in practice. For example,
    if there are many other processes (or multithreaded processes) waiting to use
    the CPU, they will all compete for the limited number of cores, resulting in *resource
    contention* among the processes. If the number of specified threads exceeds the
    number of CPU cores, each thread must wait its turn to run. We explore other factors
    that often prevent linear speedup in “Measuring the Performance of Parallel Programs”
    on [page 709](ch14.xhtml#lev1_108).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，如果线程数与核心数（*c*）匹配，并且操作系统将每个线程调度到不同的核心并行执行，那么多线程进程的运行时间大约是 1/*c*。这种线性加速是理想的，但在实践中并不常见。例如，如果有很多其他进程（或多线程进程）在等待使用
    CPU，它们会争夺有限的核心数量，从而导致进程之间的*资源争用*。如果指定的线程数超过了 CPU 核心数，每个线程就必须等待轮到它执行。我们将在“[页面 709](ch14.xhtml#lev1_108)”的“测量并行程序性能”一节中探讨其他经常阻碍线性加速的因素。
- en: 14.2 Hello Threading! Writing Your First Multithreaded Program
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.2 你好，线程！编写你的第一个多线程程序
- en: In this section, we examine the ubiquitous POSIX thread library *Pthreads*.
    POSIX is an acronym for Portable Operating System Interface. It is an IEEE standard
    that specifies how UNIX systems look, act, and feel. The POSIX threads API is
    available on almost all UNIX-like operating systems, each of which meets the standard
    in its entirety or to some great degree. So, if you write parallel code using
    POSIX threads on a Linux machine, it will certainly work on other Linux machines,
    and it will likely work on machines running macOS or other UNIX variants.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 本节我们将讨论无处不在的 POSIX 线程库 *Pthreads*。POSIX 是可移植操作系统接口的缩写，它是一个 IEEE 标准，规定了 UNIX
    系统的外观、行为和感觉。POSIX 线程 API 几乎可以在所有类 UNIX 操作系统上使用，每个系统都完全或在很大程度上遵循该标准。因此，如果你在 Linux
    机器上使用 POSIX 线程编写并行代码，它肯定可以在其他 Linux 机器上运行，并且很可能也能在运行 macOS 或其他 UNIX 变种的机器上运行。
- en: Let’s begin by analyzing an example “Hello World” Pthreads program.^([2](ch14.xhtml#fn14_2))
    For brevity, we have excluded error handling in the listing, though the downloadable
    version contains sample error handling.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从分析一个“Hello World”Pthreads 程序开始。^([2](ch14.xhtml#fn14_2)) 为了简洁，我们在列表中省略了错误处理，尽管可下载版本包含了示例错误处理。
- en: '[PRE2]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Let’s examine this program in smaller components. Notice the inclusion of the
    `pthread.h` header file, which declares `pthread` types and functions. Next, the
    `HelloWorld` function defines the *thread function* that we later pass to `pthread_create`.
    A thread function is analogous to a `main` function for a worker (created) thread—a
    thread begins execution at the start of its thread function and terminates when
    it reaches the end. Each thread executes the thread function using its private
    execution state (i.e., its own stack memory and register values). Note also that
    the thread function is of type `void *`. Specifying an *anonymous pointer* in
    this context allows programmers to write thread functions that deal with arguments
    and return values of different types (see “The void * Type and Type Recasting”
    on [page 222](ch02.xhtml#lev2_38)). Lastly, in the `main` function, the main thread
    initializes the program state before creating and joining the worker threads.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这个程序分解成更小的组件。注意包括了`pthread.h`头文件，它声明了`pthread`类型和函数。接下来，`HelloWorld`函数定义了*线程函数*，我们稍后将其传递给`pthread_create`。线程函数类似于工作线程（创建线程）的`main`函数——线程从线程函数的开始处开始执行，直到到达末尾时终止。每个线程使用其私有的执行状态（即它自己的堆栈内存和寄存器值）执行线程函数。还需注意，线程函数的类型为`void
    *`。在此上下文中指定*匿名指针*，允许程序员编写可以处理不同类型参数和返回值的线程函数（参见[第222页](ch02.xhtml#lev2_38)的《void
    *类型和类型重casting》）。最后，在`main`函数中，主线程初始化程序状态后，创建并连接工作线程。
- en: 14.2.1 Creating and Joining Threads
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.2.1 创建和连接线程
- en: 'The program first starts as a single-threaded process. As it executes the `main`
    function, it reads the number of threads to create, and it allocates memory for
    two arrays: `thread_array` and `thread_ids`. The `thread_array` array contains
    the set of addresses for each thread created. The `thread_ids` array stores the
    set of arguments that each thread is passed. In this example, each thread is passed
    the address of its rank (or ID, represented by `thread_ids[i]`).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 该程序首先作为单线程进程启动。在执行`main`函数时，它读取要创建的线程数，并为两个数组分配内存：`thread_array`和`thread_ids`。`thread_array`数组包含为每个创建的线程分配的地址集合。`thread_ids`数组存储传递给每个线程的参数集合。在此示例中，每个线程都会传递它的等级（或ID，表示为`thread_ids[i]`）的地址。
- en: 'After all the preliminary variables are allocated and initialized, the `main`
    thread executes the two major steps of multithreading:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有初始变量分配和初始化之后，`main`线程执行多线程的两个主要步骤：
- en: The *creation* step, in which the main thread spawns one or more worker threads.
    After being spawned, each worker thread runs within its own execution context
    concurrently with the other threads and processes on the system.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*创建*步骤，其中主线程生成一个或多个工作线程。每个工作线程在生成后，在其自己的执行上下文中并发运行，与系统中的其他线程和进程共同执行。'
- en: The *join* step, in which the main thread waits for all the workers to complete
    before proceeding as a single-thread process. Joining a thread that has terminated
    frees the thread’s execution context and resources. Attempting to join a thread
    that *hasn’t* terminated blocks the caller until the thread terminates, similar
    to the semantics of the `wait` function for processes (see “exit and wait” on
    [page 635](ch13.xhtml#lev2_226)).
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*连接*步骤，其中主线程等待所有工作线程完成，然后继续作为单线程进程。连接一个已终止的线程会释放该线程的执行上下文和资源。尝试连接一个*未*终止的线程会阻塞调用者，直到该线程终止，类似于进程的`wait`函数的语义（参见[第635页](ch13.xhtml#lev2_226)的《exit和wait》）。'
- en: 'The Pthreads library offers a `pthread_create` function for creating threads
    and a `pthread_join` function for joining them. The `pthread_create` function
    has the following signature:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Pthreads库提供了`pthread_create`函数用于创建线程，`pthread_join`函数用于连接线程。`pthread_create`函数的签名如下：
- en: '[PRE3]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The function takes a pointer to a thread struct (of type `pthread_t`), a pointer
    to an attribute struct (normally set to `NULL`), the name of the function the
    thread should execute, and the array of arguments to pass to the thread function
    when it starts.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数接受一个指向线程结构体（类型为`pthread_t`）的指针，一个指向属性结构体的指针（通常设置为`NULL`），线程应执行的函数名称，以及启动时传递给线程函数的参数数组。
- en: 'The Hello World program calls `pthread_create` in the `main` function using:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Hello World程序在`main`函数中使用`pthread_create`调用：
- en: '[PRE4]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这里：
- en: '`&thread_array[i]` contains the address of thread *i*. The `pthread_create`
    function allocates a `pthread_t` thread object and stores its address at this
    location, enabling the programmer to reference the thread later (e.g., when joining
    it).'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`&thread_array[i]` 包含线程 *i* 的地址。`pthread_create` 函数分配一个 `pthread_t` 线程对象，并将其地址存储在此位置，使程序员能够稍后引用该线程（例如，在合并时）。'
- en: '`NULL` specifies that the thread should be created with default attributes.
    In most programs, it is safe to leave this second parameter as `NULL`.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NULL` 表示线程应该使用默认属性创建。在大多数程序中，将这个第二个参数留空为 `NULL` 是安全的。'
- en: '`HelloWorld` names the thread function that the created thread should execute.
    This function behaves like the “main” function for the thread. For an arbitrary
    thread function (e.g., `function`), its prototype must match the form `void *
    function(void *)`.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`HelloWorld` 是创建的线程应该执行的线程函数的名称。这个函数就像线程的“主”函数。对于一个任意线程函数（例如，`function`），它的原型必须匹配
    `void * function(void *)` 这种形式。'
- en: '`&thread_ids[i]` specifies the address of the arguments to be passed to thread
    *i*. In this case, `thread_ids[i]` contains a single `long` representing the thread’s
    ID. Since the last argument to `pthread_create` must be a pointer, we pass the
    *address* of the thread’s ID.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`&thread_ids[i]` 指定要传递给线程 *i* 的参数的地址。在这种情况下，`thread_ids[i]` 包含一个表示线程 ID 的单个
    `long` 类型的值。由于 `pthread_create` 的最后一个参数必须是一个指针，因此我们传递线程 ID 的 *地址*。'
- en: 'To generate several threads that execute the `HelloWorld` thread function,
    the program assigns each thread a unique ID and creates each thread within a `for`
    loop:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成多个执行 `HelloWorld` 线程函数的线程，程序为每个线程分配一个唯一的 ID，并在 `for` 循环中创建每个线程：
- en: '[PRE5]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The OS schedules the execution of each created thread; the user cannot make
    any assumption on the order in which the threads will execute.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统调度每个创建的线程的执行；用户无法对线程执行的顺序做出任何假设。
- en: 'The `pthread_join` function suspends the execution of its caller until the
    thread it references terminates. Its signature is:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`pthread_join` 函数会挂起调用者的执行，直到它引用的线程终止。其函数签名为：'
- en: '[PRE6]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `pthread_join` takes as input a `pthread_t` struct, indicating which thread
    to wait on, and an optional pointer argument that specifies where the thread’s
    return value should be stored.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`pthread_join` 以 `pthread_t` 结构体为输入，指示要等待的线程，并且有一个可选的指针参数，指定线程的返回值应存储的位置。'
- en: 'The Hello World program calls `pthread_join` in `main` using:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`Hello World` 程序在 `main` 中使用以下代码调用 `pthread_join`：'
- en: '[PRE7]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This line indicates that the main thread must wait on the termination of thread
    `t`. Passing `NULL` as the second argument indicates that the program does not
    use the thread’s return value.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这一行表示主线程必须等待线程 `t` 的终止。将 `NULL` 作为第二个参数传递表示程序不使用线程的返回值。
- en: 'In the previous program, `main` calls `pthread_join` in a loop because *all*
    of the worker threads need to terminate before the `main` function proceeds to
    clean up memory and terminate the process:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的程序中，`main` 在循环中调用 `pthread_join`，因为 *所有* 工作线程需要在 `main` 函数继续清理内存并终止进程之前终止：
- en: '[PRE8]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 14.2.2 The Thread Function
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.2.2 线程函数
- en: 'In the previous program, each spawned thread prints out `Hello world! I am
    thread n`, where `n` is the thread’s unique ID. After the thread prints out its
    message, it terminates. Let’s take a closer look at the `HelloWorld` function:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的程序中，每个生成的线程都会打印 `Hello world! I am thread n`，其中 `n` 是线程的唯一标识符。在线程打印出消息后，它会终止。让我们更仔细地看看
    `HelloWorld` 函数：
- en: '[PRE9]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Recall that `pthread_create` passes the arguments to the thread function using
    the `thread_args` parameter. In the `pthread_create` function in `main`, the Hello
    World program specified that this parameter is in fact the thread’s ID. Note that
    the parameter to `HelloWorld` must be declared as a generic or anonymous pointer
    (`void *`) (see “The void * Type and Type Recasting” on [page 126](ch02.xhtml#lev2_38)).
    The Pthreads library uses `void *` to make `pthread_create` more general purpose
    by not prescribing a parameter type. As a programmer, the `void *` is mildly inconvenient
    given that it must be recast before use. Here, we *know* the parameter is of type
    `long *` because that’s what we passed to `pthread_create` in `main`. Thus, we
    can safely cast the value as a `long *` and dereference the pointer to access
    the `long` value. Many parallel programs follow this structure.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to the thread function’s parameter, the Pthreads library avoids prescribing
    the thread function’s return type by specifying another `void *`: the programmer
    is free to return any pointer from the thread function. If the program needs to
    access the thread’s return value, it can retrieve it via the second argument to
    `pthread_join`. In our example, the thread has no need to return a value, so it
    simply returns a `NULL` pointer.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: 14.2.3 Running the Code
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The command that follows shows how to use GCC to compile the program. Building
    a Pthreads application requires that the `-lpthread` linker flag be passed to
    GCC to ensure that the Pthreads functions and types are accessible:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Running the program without a command line argument results in a usage message:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Running the program with four threads yields the following output:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Notice that each thread prints its unique ID number. In this run, thread 1’s
    output displays first, followed by threads 2, 3, and 0\. If we run the program
    again, we may see the output displayed in a different order:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Recall that the operating system’s scheduler determines the thread execution
    order. From a user’s perspective, the order is *effectively random* due to being
    influenced by many factors that vary outside the user’s control (e.g., available
    system resources, the system receiving input, or OS scheduling). Since all threads
    are running concurrently with one another and each thread executes a call to `printf`
    (which prints to `stdout`), the first thread that prints to `stdout` will have
    its output show up first. Subsequent executions may (or may not) result in different
    output.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '**Warning THREAD EXECUTION ORDER**'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: You should *never* make any assumptions about the order in which threads will
    execute. If the correctness of your program requires that threads run in a particular
    order, you must add synchronization (see “Synchronizing Threads” on [page 686](ch14.xhtml#lev1_107))
    to your program to prevent threads from running when they shouldn’t.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 14.2.4 Revisiting Scalar Multiplication
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let’s explore how to create a multithreaded implementation of the scalar multiplication
    program from “An Example: Scalar Multiplication” on [page 675](ch14.xhtml#lev3_113).
    Recall that our general strategy for parallelizing `scalar_multiply` is to create
    multiple threads, assign each thread a subset of the input array, and instruct
    each thread to multiply the elements in its array subset by `s`.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: The following is a thread function that accomplishes this task. Notice that
    we have moved `array`, `length`, and `s` to the global scope of the program.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let’s break this down into parts. Recall that the first step is to assign each
    thread a component of the array. The following lines accomplish this task:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The variable `chunk` stores the number of elements that each thread is assigned.
    To ensure that each thread gets roughly the same amount of work, we first set
    the chunk size to the number of elements divided by the number of threads, or
    `length / nthreads`.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Next, we assign each thread a distinct range of elements to process. Each thread
    computes its range’s `start` and `end` index using the `chunk` size and its unique
    thread ID.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: For example, with four threads (with IDs 0–3) operating over an array with 100
    million elements, each thread is responsible for processing a 25 million element
    `chunk`. Incorporating the thread ID assigns each thread a unique subset of the
    input.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 'The next two lines account for the case in which `length` is not evenly divisible
    by the number of threads:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Suppose that we specified three rather than four threads. The nominal chunk
    size would be 33,333,333 elements, leaving one element unaccounted for. The code
    in the previous example would assign the remaining element to the last thread.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '**Note CREATING BALANCED INPUT**'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: The chunking code just shown is imperfect. In the case where the number of threads
    does not evenly divide the input, the remainder is assigned to the last thread.
    Consider a sample run in which the array has 100 elements, and 12 threads are
    specified. The nominal chunk size would be 8, and the remainder would be 4\. With
    the example code, the first 11 threads will each have 8 assigned elements, whereas
    the last thread will be assigned 12 elements. Consequently, the last thread performs
    50% more work than the other threads. A potentially better way to chunk this example
    is to have the first 4 threads process 9 elements each, whereas the last 8 threads
    process 8 elements each. This will result in better *load balancing* of the input
    across the threads.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 'With an appropriate local `start` and `end` index computed, each thread is
    now ready to perform scalar multiplication on its component of the array. The
    last portion of the `scalar_multiply` function accomplishes this:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '14.2.5 Improving Scalar Multiplication: Multiple Arguments'
  id: totrans-128
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A key weakness of the previous implementation is the wide use of global variables.
    Our original discussion in “Parts of Program Memory and Scope” on [page 64](ch02.xhtml#lev1_9)
    showed that, although useful, global variables should generally be avoided in
    C. To reduce the number of global variables in the program, one solution is to
    declare a `t_arg` struct as follows in the global scope:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Our main function would, in addition to allocating `array` and setting local
    variables `length`, `nthreads`, and `s` (our scaling factor), allocate an array
    of `t_arg` records:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Later in `main`, when `pthread_create` is called, the thread’s associated `t_args`
    struct is passed as an argument:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Lastly, our `scalar_multiply` function would look like the following:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Implementing this program fully is an exercise we leave to the reader. Please
    note that error handling has been omitted for the sake of brevity.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 14.3 Synchronizing Threads
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the examples we’ve looked at thus far, each thread executes without sharing
    data with any other threads. In the scalar multiplication program, for instance,
    each element of the array is entirely independent of all the others, making it
    unnecessary for the threads to share data.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: However, a thread’s ability to easily share data with other threads is one of
    its main features. Recall that all the threads of a multithreaded process share
    the heap common to the process. In this section, we study the data sharing and
    protection mechanisms available to threads in detail.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '*Thread synchronization* refers to forcing threads to execute in a particular
    order. Even though synchronizing threads can add to the runtime of a program,
    it is often necessary to ensure program correctness. In this section, we primarily
    discuss how one synchronization construct (a *mutex*) helps ensure the correctness
    of a threaded program. We conclude the section with a discussion of some other
    common synchronization constructs: *semaphores*, *barriers*, and *condition variables*.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: CountSort
  id: totrans-142
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Let’s study a slightly more complicated example called CountSort. The CountSort
    algorithm is a simple linear (O(*N*)) sorting algorithm for sorting a known small
    range of *R* values, where *R* is much smaller than *N*. To illustrate how CountSort
    works, consider an array `A` of 15 elements, all of which contain random values
    between 0 and 9 (10 possible values):'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'For a particular array, CountSort works as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: 1\. It counts the frequency of each value in the array.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 2\. It overwrites the original array by enumerating each value by its frequency.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: After step 1, the frequency of each value is placed in a `counts` array of length
    10, where the value of `counts[i]` is the frequency of the value *i* in array
    `A`. For example, since there are three elements with value 2 in array `A`, `counts[2]`
    is 3.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: 'The corresponding `counts` array for the previous example looks like the following:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Note that the sum of all the elements in the `counts` array is equal to the
    length of `A`, or 15.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 uses the `counts` array to overwrite `A`, using the frequency counts
    to determine the set of indices in `A` that store each consecutive value in sorted
    order. So, since the `counts` array indicates that there are three elements with
    value 0 and two elements with value 1 in array `A`, the first three elements of
    the final array will be 0, and the next two will be 1.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: 'After running step 2, the final array looks like the following:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Following is a serial implementation of the CountSort algorithm, with the `count`
    (step 1) and `overwrite` (step 2) functions clearly delineated. For brevity, we
    do not reproduce the whole program here, though you can download the source.^([3](ch14.xhtml#fn14_3))
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Running this program on an array of size 15 yields the following output:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The second parameter to this program is a *verbose* flag, which indicates whether
    the program prints output. This is a useful option for larger arrays for which
    we may want to run the program but not necessarily print out the output.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'Parallelizing countElems: An Initial Attempt'
  id: totrans-160
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: CountSort consists of two primary steps, each of which benefits from being parallelized.
    In the remainder of the chapter, we primarily concentrate on the parallelization
    of step 1, or the `countElems` function. Parallelizing the `writeArray` function
    is left as an exercise for the reader.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'The code block that follows depicts a first attempt at creating a threaded
    `countElems` function. Parts of the code (argument parsing, error handling) are
    omitted in this example for the sake of brevity, but the full source can be downloaded.^([4](ch14.xhtml#fn14_4))
    In the code that follows, each thread attempts to count the frequency of the array
    elements in its assigned component of the global array and updates a global count
    array with the discovered counts:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The `main` function looks nearly identical to our earlier sample programs:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: For reproducibility purposes, the random number generator is seeded with a static
    value (10) to ensure that `array` (and therefore `counts`) always contains the
    same set of numbers. An additional function (`printCounts`) prints out the contents
    of the global `counts` array. The expectation is that, regardless of the number
    of threads used, the contents of the `counts` array should always be the same.
    For brevity, error handling has been removed from the listing.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'Compiling the program and running it with one, two, and four threads over 10
    million elements produces the following:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Note that the printed results change significantly on each run. In particular,
    they seem to change as we vary the number of threads! This should not happen,
    since our use of the static seed guarantees the same set of numbers every run.
    These results contradict one of the cardinal rules for threaded programs: the
    output of a program should be correct and consistent *regardless* of the number
    of threads used.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Since our first attempt at parallelizing `countElems` doesn’t seem to be working,
    let’s delve deeper into what this program is doing and examine how we might fix
    it.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Data Races
  id: totrans-171
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'To understand what’s going on, let’s consider an example run with two threads
    on two separate cores of a multicore system. Recall that the execution of any
    thread can be preempted at any time by the OS, which means that each thread could
    be running different instructions of a particular function at any given time (or
    possibly the same instruction). [Table 14-1](ch14.xhtml#ch14tab1) shows one possible
    path of execution through the `countElems` function. To better illustrate what
    is going on, we translated the line `counts[val] = counts[val] + 1` into the following
    sequence of equivalent instructions:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 1\. *Read* `counts[val]` and place into a register.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 2\. *Modify* the register by incrementing it by one.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 3\. *Write* the contents of the register to `counts[val]`.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: This is known as the *read–modify–write* pattern. In the example shown in [Table
    14-1](ch14.xhtml#ch14tab1), each thread executes on a separate core (Thread 0
    on Core 0, Thread 1 on Core 1). We start inspecting the execution of the process
    at time step *i*, where both threads have a `val` of 1.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 14-1:** A Possible Execution Sequence of Two Threads Running `countElems`'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '| **Time** | **Thread 0** | **Thread 1** |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
- en: '| *i* | Read `counts[1]` and place into Core 0’s register | … |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
- en: '| *i* + 1 | Increment register by 1 | Read `counts[1]` and place into Core
    1’s register |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
- en: '| *i* + 2 | Overwrite `counts[1]` with contents of register | Increment register
    by 1 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
- en: '| *i* + 3 | … | Overwrite `counts[1]` with contents of register |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
- en: Suppose that, prior to the execution sequence in [Table 14-1](ch14.xhtml#ch14tab1),
    `counts[1]` contains the value 60\. In time step *i*, Thread 0 reads `counts[1]`
    and places the value 60 in Core 0’s register. In time step *i* + 1, while Thread
    0 increments Core 0’s register by one, the *current* value in `counts[1]` (60)
    is read into Core 1’s register by Thread 1\. In time step *i* + 2, Thread 0 updates
    `counts[1]` with the value 61 while Thread 1 increments the value stored in its
    local register (60) by one. The end result is that during time step *i* + 3, the
    value `counts[1]` is overwritten by Thread 1 with the value 61, not 62 as we would
    expect! This causes `counts[1]` to essentially “lose” an increment!
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: We refer to the scenario in which two threads attempt to write to the same location
    in memory as a *data race* condition. More generally, a *race condition* refers
    to any scenario in which the simultaneous execution of two operations gives an
    incorrect result. Note that a simultaneous read of the `counts[1]` location would
    *not* in and of itself constitute a race condition, because values can generally
    read alone from memory without issue. It was the combination of this step with
    the writes to `counts[1]` that caused the incorrect result. This read–modify–write
    pattern is a common source of a particular type of race condition, called a *data
    race*, in most threaded programs. In our discussion of race conditions and how
    to fix them, we focus on data races.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '**Note ATOMIC OPERATIONS**'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: An operation is defined as being *atomic* if a thread perceives it as executing
    without interruption (in other words, as an “all or nothing” action). In some
    libraries, a keyword or type is used to specify that a block of computation should
    be treated as being atomic. In the previous example, the line `counts[val] = counts[val]
    + 1` (even if written as `counts[val]++`) is *not* atomic, because this line actually
    corresponds to several instructions at the machine level. A synchronization construct
    like mutual exclusion is needed to ensure that there are no data races. In general,
    all operations should be assumed to be nonatomic unless mutual exclusion is explicitly
    enforced.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that not all execution sequences of the two threads cause a race
    condition. Consider the sample execution sequence of Threads 0 and 1 in [Table
    14-2](ch14.xhtml#ch14tab2).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 14-2:** Another Possible Execution Sequence of Two Threads Running
    `countElems`'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '| **Time** | **Thread 0** | **Thread 1** |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
- en: '| *i* | Read `counts[1]` and place into Core 0’s register | … |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
- en: '| *i* + 1 | Increment register by 1 | … |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
- en: '| *i* + 2 | Overwrite `counts[1]` with contents of register | … |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
- en: '| *i* + 3 | … | Read `counts[1]` and place into Core 1’s register |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
- en: '| *i* + 4 | … | Increment register by 1 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
- en: '| *i* + 5 | … | Overwrite `counts[1]` with contents of register |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
- en: In this execution sequence, Thread 1 does not read from `counts[1]` until after
    Thread 0 updates it with its new value (61). The end result is that Thread 1 reads
    the value 61 from `counts[1]` and places it into Core 1’s register during time
    step *i* + 3, and writes the value 62 to `counts[1]` in time step *i* + 5.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: To fix a data race, we must first isolate the *critical section*, or the subset
    of code that must execute *atomically* (in isolation) to ensure correct behavior.
    In threaded programs, blocks of code that update a shared resource are typically
    identified to be critical sections.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `countElems` function, updates to the `counts` array should be put in
    a critical section to ensure that values are not lost due to multiple threads
    updating the same location in memory:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Since the fundamental problem in `countElems` is the simultaneous access of
    `counts` by multiple threads, a mechanism is needed to ensure that only one thread
    executes within the critical section at a time. Using a synchronization construct
    (like a mutex, which is covered in the next section) will force the threads to
    enter the critical section sequentially.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 14.3.1 Mutual Exclusion
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*What is the mutex? The answer is out there, and it’s looking for you,*'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '*and it will find you if you want it to.*'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: —Trinity, explaining mutexes to Neo (with apologies to *The Matrix*)
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: To fix the data race, let’s use a synchronization construct known as a mutual
    exclusion lock, or *mutex*. Mutual exclusion locks are a type of synchronization
    primitive that ensures that only one thread enters and executes the code inside
    the critical section at any given time.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Before using a mutex, a program must first declare the mutex in memory that’s
    shared by threads (often as a global variable), and then initialize the mutex
    before the threads need to use it (typically in the `main` function).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 'The Pthreads library defines a `pthread_mutex_t` type for mutexes. To declare
    a mutex variable, add this line:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'To initialize the mutex use the `pthread_mutex_init` function, which takes
    the address of a mutex and an attribute structure, typically set to `NULL`:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'When the mutex is no longer needed (typically at the end of the `main` function,
    after `pthread_join`), a program should release the mutex structure by invoking
    the `pthread_mutex_destroy` function:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The Mutex: Locked and Loaded'
  id: totrans-213
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The initial state of a mutex is unlocked, meaning it’s immediately usable by
    any thread. To enter a critical section, a thread must first acquire a lock. This
    is accomplished with a call to the `pthread_mutex_lock` function. After a thread
    has the lock, no other thread can enter the critical section until the thread
    with the lock releases it. If another thread calls `pthread_mutex_lock` and the
    mutex is already locked, the thread will *block* (or wait) until the mutex becomes
    available. Recall that blocking implies that the thread will not be scheduled
    to use the CPU until the condition it’s waiting for (the mutex being available)
    becomes true (see “Process State” on [page 627](ch13.xhtml#lev2_222)).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: When a thread exits the critical section it must call the `pthread_mutex_unlock`
    function to release the mutex, making it available for another thread. Thus, at
    most one thread may acquire the lock and enter the critical section at a time,
    which prevents multiple threads from *racing* to read and update shared variables.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Having declared and initialized a mutex, the next question is where the lock
    and unlock functions should be placed to best enforce the critical section. Here
    is an initial attempt at augmenting the `countElems` function with a mutex:^([5](ch14.xhtml#fn14_5))
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The mutex initialize and destroy functions are placed in `main` around the
    thread creation and join functions:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Let’s recompile and run this new program while varying the number of threads:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Excellent, the output is *finally* consistent regardless of the number of threads
    used!
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that another primary goal of threading is to reduce the runtime of a
    program as the number of threads increases (i.e., to *speed up* program execution).
    Let’s benchmark the performance of the `countElems` function. Although it may
    be tempting to use a command line utility like `time -p`, recall that invoking
    `time -p` measures the wall-clock time of the *entire* program (including the
    generation of random elements) and *not* just the running of the `countElems`
    function. In this case, it is better to use a system call like `gettimeofday`,
    which allows a user to accurately measure the wall-clock time of a particular
    section of code. Benchmarking `countElems` on 100 million elements yields the
    following run times:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Adding more threads causes the program to get *slower*! This goes against the
    goal of making programs *faster* with threads.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand what is going on, consider where the locks are placed in the
    `countsElems` function:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: In this example, we placed the lock around the *entirety* of the `for` loop.
    Even though this placement solves the correctness problems, it’s an extremely
    poor decision from a performance perspective—the critical section now encompasses
    the entire loop body. Placing locks in this manner guarantees that only one thread
    can execute the loop at a time, effectively serializing the program!
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: 'The Mutex: Reloaded'
  id: totrans-229
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Let’s try another approach and place the mutex locking and unlocking functions
    within every iteration of the loop:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: This may initially look like a better solution because each thread can enter
    the loop in parallel, serializing only when reaching the lock. The critical section
    is very small, encompassing only the line `counts[val] = counts[val] + 1`.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first perform a correctness check on this version of the program:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: So far so good. This version of the program also produces consistent output
    regardless of the number of threads employed.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s look at performance:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Running this version of the code yields (amazingly enough) a *significantly
    slower* runtime!
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 'As it turns out, locking and unlocking a mutex are expensive operations. Recall
    what was covered in the discussion on function call optimizations (see “Function
    Inlining” on [page 604](ch12.xhtml#lev1_95)): calling a function repeatedly (and
    needlessly) in a loop can be a major cause of slowdown in a program. In our prior
    use of mutexes, each thread locks and unlocks the mutex exactly once. In the current
    solution, each thread locks and unlocks the mutex *n*/*t* times, where *n* is
    the size of the array, *t* is the number of threads, and *n*/*t* is the size of
    the array component assigned to each particular thread. As a result, the cost
    of the additional mutex operations slows down the loop’s execution considerably.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'The Mutex: Revisited'
  id: totrans-240
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In addition to protecting the critical section to achieve correct behavior,
    an ideal solution would use the lock and unlock functions as little as possible,
    and reduce the critical section to the smallest possible size.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: The original implementation satisfies the first requirement, whereas the second
    implementation tries to accomplish the second. At first glance, it appears that
    the two requirements are incompatible with each other. Is there a way to actually
    accomplish both (and while we are at it, speed up the execution of our program)?
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: 'For the next attempt, each thread maintains a private, *local* array of counts
    on its stack. Because the array is local to each thread, a thread can access it
    without locking—there’s no risk of a race condition on data that isn’t shared
    between threads. Each thread processes its assigned subset of the shared array
    and populates its local counts array. After counting up all the values within
    its subset, each thread:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Locks the shared mutex (entering a critical section).
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Adds the values from its local counts array to the shared counts array.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Unlocks the shared mutex (exiting the critical section).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Restricting each thread to update the shared counts array only once significantly
    reduces the contention for shared variables and minimizes expensive mutex operations.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: The following is our revised `countElems` function:^([6](ch14.xhtml#fn14_6))
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'This version has a few additional features:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: The presence of `local_counts`, an array that is private to the scope of each
    thread (i.e., allocated in the thread’s stack). Like `counts`, `local_counts`
    contains `MAX` elements, given that `MAX` is the maximum value any element can
    hold in our input array.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each thread makes updates to `local_counts` at its own pace, without any contention
    for shared variables.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A single call to `pthread_mutex_lock` protects each thread’s update to the global
    `counts` array, which happens only once at the end of each thread’s execution.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this manner, we reduce the time each thread spends in a critical section
    to just updating the shared counts array. Even though only one thread can enter
    the critical section at a time, the time each thread spends there is proportional
    to `MAX`, not *n*, the length of the global array. Since `MAX` is much less than
    *n*, we should see an improvement in performance.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now benchmark this version of our code:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Wow, what a difference! Our program not only computes the correct answers, but
    also executes faster as we increase the number of threads.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: 'The lesson to take away here is this: to efficiently minimize a critical section,
    use local variables to collect intermediate values. After the hard work requiring
    parallelization is over, use a mutex to safely update any shared variable(s).'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Deadlock
  id: totrans-259
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In some programs, waiting threads have dependencies on one another. A situation
    called *deadlock* can arise when multiple synchronization constructs like mutexes
    are incorrectly applied. A deadlocked thread is blocked from execution by another
    thread, which *itself* is blocked on a blocked thread. Gridlock (in which cars
    in all directions cannot move forward due to being blocked by other cars) is a
    common real-world example of deadlock that occurs at busy city intersections.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate a deadlock scenario in code, let’s consider an example where
    multithreading is used to implement a banking application. Each user’s account
    is defined by a balance and its own mutex (ensuring that no race conditions can
    occur when updating the balance):'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Consider the following naive implementation of a `Transfer` function that moves
    money from one bank account to another:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Suppose that Threads 0 and 1 are executing concurrently and represent users
    A and B, respectively. Now consider the situation in which A and B want to transfer
    money to each other: A wants to transfer 20 dollars to B, while B wants to transfer
    40 dollars to A.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: In the path of execution highlighted by [Figure 14-7](ch14.xhtml#ch14fig7),
    both threads concurrently execute the `Transfer` function. Thread 0 acquires the
    lock of `acctA` while Thread 1 acquires the lock of `acctB`. Now consider what
    happens. To continue executing, Thread 0 needs to acquire the lock on `acctB`,
    which Thread 1 holds. Likewise, Thread 1 needs to acquire the lock on `acctA`
    to continue executing, which Thread 0 holds. Since both threads are blocked on
    each other, they are in deadlock.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/14fig07.jpg)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
- en: '*Figure 14-7: An example of deadlock*'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: 'Although the OS provides some protection against deadlock, programmers should
    be mindful about writing code that increases the likelihood of deadlock. For example,
    the preceding scenario could have been avoided by rearranging the locks so that
    each lock/unlock pair surrounds only the balance update statement associated with
    it:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Deadlock is not a situation that is unique to threads. Processes (especially
    those that are communicating with one another) can deadlock with one another.
    Programmers should be mindful of the synchronization primitives they use and the
    consequences of using them incorrectly.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 14.3.2 Semaphores
  id: totrans-272
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Semaphores are commonly used in operating systems and concurrent programs where
    the goal is to manage concurrent access to a pool of resources. When using a semaphore,
    the goal isn’t *who* owns what, but *how many* resources are still available.
    Semaphores are different from mutexes in several ways:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Semaphores need not be in a binary (locked or unlocked) state. A special type
    of semaphore called a *counting semaphore* can range in value from 0 to some *r*,
    where *r* is the number of possible resources. Any time a resource is produced,
    the semaphore is incremented. Any time a resource is being used, the semaphore
    is decremented. When a counting semaphore has a value of 0, it means that no resources
    are available, and any other threads that attempt to acquire a resource must wait
    (e.g., block).
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semaphores can be locked by default.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While a mutex and condition variables can simulate the functionality of a semaphore,
    using a semaphore may be simpler and more efficient in some cases. Semaphores
    also have the advantage that *any* thread can unlock the semaphore (in contrast
    to a mutex, where the calling thread must unlock it).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 'Semaphores are not part of the Pthreads library, but that does not mean that
    you cannot use them. On Linux and macOS systems, semaphore primitives can be accessed
    from `semaphore.h`, typically located in `/usr/include`. Since there is no standard,
    the function calls may differ on different systems. That said, the semaphore library
    has similar declarations to those for mutexes:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Declare a semaphore (type `sem_t`, e.g., `sem_t semaphore`).
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Initialize a semaphore using `sem_init` (usually in `main`). The `sem_init`
    function has three parameters: the first is the address of a semaphore, the second
    is its initial state (locked or unlocked), and the third parameter indicates whether
    the semaphore should be shared with the threads of a process (e.g., with value
    0) or between processes (e.g., with value 1). This is useful because semaphores
    are commonly used for process synchronization. For example, initializing a semaphore
    with the call `sem_init(&semaphore, 1, 0)` indicates that our semaphore is initially
    locked (the second parameter is 1), and is to be shared among the threads of a
    common process (the third parameter is 0). In contrast, mutexes always start out
    unlocked. It is important to note that in macOS, the equivalent function is `sem_open`.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Destroy a semaphore using `sem_destroy` (usually in `main`). This function only
    takes a pointer to the semaphore (`sem_destroy(&semaphore)`). Note that in macOS,
    the equivalent function may be `sem_unlink` or `sem_close`.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `sem_wait` function indicates that a resource is being used, and decrements
    the semaphore. If the semaphore’s value is greater than 0 (indicating there are
    still resources available), the function will immediately return, and the thread
    is allowed to proceed. If the semaphore’s value is already 0, the thread will
    block until a resource becomes available (i.e., the semaphore has a positive value).
    A call to `sem_wait` typically looks like `sem_wait(&semaphore)`.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `sem_post` function indicates that a resource is being freed, and increments
    the semaphore. This function returns immediately. If there is a thread waiting
    on the semaphore (i.e., the semaphore’s value was previously 0), then the other
    thread will take ownership of the freed resource. A call to `sem_post` looks like
    `sem_post(&semaphore)`.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 14.3.3 Other Synchronization Constructs
  id: totrans-283
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Mutexes and semaphores are not the only example of synchronization constructs
    that can be used in the context of multithreaded programs. In this subsection
    we will briefly discuss the barrier and condition variable synchronization constructs,
    which are both part of the Pthreads library.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: Barriers
  id: totrans-285
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'A *barrier* is a type of synchronization construct that forces *all* threads
    to reach a common point in execution before releasing the threads to continue
    executing concurrently. Pthreads offers a barrier synchronization primitive. To
    use Pthreads barriers, it is necessary to do the following:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: Declare a barrier global variable (e.g., `pthread_barrier_t barrier`)
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Initialize the barrier in `main` (`pthread_barrier_init(&barrier)`)
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Destroy the barrier in `main` after use (`pthread_barrier_destroy(&barrier)`)
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the `pthread_barrier_wait` function to create a synchronization point.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following program shows the use of a barrier in a function called `threadEx`:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: In this example, no thread can start processing its assigned portion of the
    array until *every* thread has printed out the message that they are starting
    work. Without the barrier, it is possible for one thread to have finished work
    before the other threads have printed their starting work message! Notice that
    it is *still* possible for one thread to print that it is done doing work before
    another thread finishes.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: Condition Variables
  id: totrans-294
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Condition variables force a thread to block until a particular condition is
    reached. This construct is useful for scenarios in which a condition must be met
    before the thread does some work. In the absence of condition variables, a thread
    would have to repeatedly check to see whether the condition is met, continuously
    utilizing the CPU. Condition variables are always used in conjunction with a mutex.
    In this type of synchronization construct, the mutex enforces mutual exclusion,
    whereas the condition variable ensures that particular conditions are met before
    a thread acquires the mutex.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: POSIX condition variables have the type `pthread_cond_t`. Like the mutex and
    barrier constructs, condition variables must be initialized prior to use and destroyed
    after use.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: To initialize a condition variable, use the `pthread_cond_init` function. To
    destroy a condition variable, use the `pthread_cond_destroy` function.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: The two functions commonly invoked when using condition variables are `pthread_cond_wait`
    and `pthread_cond_signal`. Both functions require the address of a mutex in addition
    to the address of the condition variable.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: The `pthread_cond_wait(&cond, &mutex)` function takes the addresses of a condition
    variable `cond` and a mutex `mutex` as its arguments. It causes the calling thread
    to block on the condition variable `cond` until another thread signals it (or
    “wakes” it up).
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `pthread_cond_signal(&cond)` function causes the calling thread to unblock
    (or signal) another thread that is waiting on the condition variable `cond` (based
    on scheduling priority). If no threads are currently blocked on the condition,
    then the function has no effect. Unlike `pthread_cond_wait`, the `pthread_cond_signal`
    function can be called by a thread regardless of whether it owns the mutex in
    which `pthread_cond_wait` is called.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Condition Variable Example
  id: totrans-301
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Traditionally, condition variables are most useful when a subset of threads
    are waiting on another set to complete some action. In the following example,
    we use multiple threads to simulate a set of farmers collecting eggs from a set
    of chickens. “Chicken” and “Farmer” represent two separate classes of threads.
    The full source of this program can be downloaded;^([7](ch14.xhtml#fn14_7)) note
    that this listing excludes many comments/error handling for brevity.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: 'The `main` function creates a shared variable `num_eggs` (which indicates the
    total number of eggs available at any given time), a shared `mutex` (which is
    used whenever a thread accesses `num_eggs`), and a shared condition variable `eggs`.
    It then creates two Chicken and two Farmer threads:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Each Chicken thread is responsible for laying a certain number of eggs:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: To lay an egg, a Chicken thread sleeps for a while, acquires the mutex and updates
    the total number of available eggs by one. Prior to releasing the mutex, the Chicken
    thread “wakes up” a sleeping Farmer (presumably by squawking). The Chicken thread
    repeats the cycle until it has laid all the eggs it intends to (`total_eggs`).
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: 'Each Farmer thread is responsible for collecting `total_eggs` eggs from the
    set of chickens (presumably for their breakfast):'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Each Farmer thread acquires the mutex prior to checking the shared `num_eggs`
    variable to see whether any eggs are available (`*num_eggs == 0`). While there
    aren’t any eggs available, the Farmer thread blocks (i.e., takes a nap).
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: After the Farmer thread “wakes up” due to a signal from a Chicken thread, it
    checks to see that an egg is still available (another Farmer could have grabbed
    it first) and if so, the Farmer “collects” an egg (decrementing `num_eggs` by
    one) and releases the mutex.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: In this manner, the Chicken and Farmer work in concert to lay/collect eggs.
    Condition variables ensure that no Farmer thread collects an egg until it is laid
    by a Chicken thread.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: Broadcasting
  id: totrans-313
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Another function used with condition variables is `pthread_cond_broadcast`,
    which is useful when multiple threads are blocked on a particular condition. Calling
    `pthread_cond_broadcast(&cond)` wakes up *all* threads that are blocked on condition
    `cond`. In this next example, we show how condition variables can implement the
    barrier construct discussed previously:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: The function `threadEx_v2` has identical functionality to `threadEx`. In this
    example, the condition variable is named `barrier`. As each thread acquires the
    lock, it increments `n_reached`, the number of threads that have reached that
    point. While the number of threads that have reached the barrier is less than
    the total number of threads, the thread waits on the condition variable `barrier`
    and mutex `mutex`.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: However, when the last thread reaches the barrier, it calls `pthread_cond _broadcast(&barrier)`,
    which releases *all* the other threads that are waiting on the condition variable
    `barrier`, enabling them to continue execution.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: This example is useful for illustrating the `pthread_cond_broadcast` function;
    however, it is best to use the Pthreads barrier primitive whenever barriers are
    necessary in a program.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: One question that students tend to ask is if the `while` loop around the call
    to `pthread_cond_wait` in the `farmer` and `threadEx_v2` code can be replaced
    with an `if` statement. This `while` loop is in fact absolutely necessary for
    two main reasons. First, the condition may change prior to the woken thread arriving
    to continue execution. The `while` loop enforces that the condition be retested
    one last time. Second, the `pthread_cond_wait` function is vulnerable to *spurious
    wakeups*, in which a thread is erroneously woken up even though the condition
    may not be met. The `while` loop is in fact an example of a *predicate loop*,
    which forces a final check of the condition variable before releasing the mutex.
    The use of predicate loops is therefore correct practice when using condition
    variables.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: 14.4 Measuring the Performance of Parallel Programs
  id: totrans-320
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, we have used the `gettimeofday` function to measure the amount of time
    it takes for programs to execute. In this section, we discuss how to measure how
    well a parallel program performs in comparison to a serial program as well as
    other topics related to measuring the performance of parallel programs.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: 14.4.1 Parallel Performance Basics
  id: totrans-322
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Speedup
  id: totrans-323
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Suppose that a program takes *T*[c] time to execute on *c* cores. Thus, the
    serial version of the program would take *T*[1] time. The speedup of the program
    on *c* cores is then expressed by this equation:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/equ0709-01.jpg)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
- en: If a serial program takes 60 seconds to execute, while its parallel version
    takes 30 seconds on 2 cores, the corresponding speedup is 2\. Likewise if that
    program takes 15 seconds on 4 cores, the speedup is 4\. In an ideal scenario,
    a program running on *n* cores with *n* total threads has a speedup of *n*.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: If the speedup of a program is greater than 1, it indicates that the parallelization
    yielded some improvement. If the speedup is less than 1, then the parallel solution
    is in fact slower than the serial solution. It is possible for a program to have
    a speedup greater than *n* (for example, as a side effect of additional caches
    reducing accesses to memory). Such cases are referred to as *superlinear speedup*.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: Efficiency
  id: totrans-328
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Speedup doesn’t factor in the number of cores—it is simply the ratio of the
    serial time to the parallel time. For example, if a serial program takes 60 seconds,
    but a parallel program takes 30 seconds on four cores, it still gets a speedup
    of 2\. However, that metric doesn’t capture the fact that it ran on four cores.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: 'To measure the speedup per core, use efficiency:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/equ0710-01.jpg)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
- en: Efficiency typically varies from 0 to 1\. An efficiency of 1 indicates that
    the cores are being used perfectly. If efficiency is close to 0, then there is
    little to no benefit to parallelism, as the additional cores do not improve performance.
    If efficiency is greater than 1, it indicates superlinear speedup.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: Let’s revisit the previous example in which a serial program takes 60 seconds.
    If the parallel version takes 30 seconds on two cores, then its efficiency is
    1 (or 100%). If instead the program takes 30 seconds on four cores, then the efficiency
    drops to 0.5 (or 50%).
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: Parallel Performance in the Real World
  id: totrans-334
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In an ideal world, speedup is linear. For each additional compute unit, a parallel
    program should achieve a commensurate amount of speedup. However, this scenario
    rarely occurs in the real world. Most programs contain a necessarily serial component
    that exists due to inherent dependencies in the code. The longest set of dependencies
    in a program is referred to as its *critical path*. Reducing the length of a program’s
    critical path is an important first step in its parallelization. Thread synchronization
    points and (for programs running on multiple compute nodes) communication overhead
    between processes are other components in the code that can limit a program’s
    parallel performance.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: '**Warning NOT ALL PROGRAMS ARE GOOD CANDIDATES FOR PARALLELISM!**'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: The length of the critical path can make some programs downright *hard* to parallelize.
    As an example, consider the problem of generating the *n*th Fibonacci number.
    Since every Fibonacci number is dependent on the two before it, parallelizing
    this program efficiently is very difficult!
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the parallelization of the `countElems` function of the CountSort
    algorithm from earlier in this chapter. In an ideal world, we would expect the
    speedup of the program to be linear with respect to the number of cores. However,
    let’s measure its runtime (in this case, running on a quad-core system with eight
    logical threads):'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[Table 14-3](ch14.xhtml#ch14tab3) shows the speedup and efficiency for these
    multithreaded executions.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 14-3:** Performance Benchmarks'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: '| Number of threads | 2 | 4 | 8 |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
- en: '| Speedup | 1.68 | 2.36 | 3.08 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
- en: '| Efficiency | 0.84 | 0.59 | 0.39 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
- en: We have 84% efficiency with two cores, but the core efficiency falls to 39%
    with eight cores. Notice that the ideal speedup of eight was not met. One reason
    for this is that the overhead of assigning work to threads and the serial update
    to the `counts` array starts dominating performance at higher numbers of threads.
    Second, resource contention by the eight threads (remember this is a quad-core
    processor) reduces core efficiency.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: Amdahl’s Law
  id: totrans-346
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In 1967, Gene Amdahl, a leading computer architect at IBM, predicted that the
    maximum speedup that a computer program can achieve is limited by the size of
    its necessarily serial component (now referred to as Amdahl’s Law). More generally,
    Amdahl’s Law states that for every program, there is a component that can be sped
    up (i.e., the fraction of a program that can be optimized or parallelized, *P*),
    and a component that *cannot* be sped up (i.e., the fraction of a program that
    is inherently serial, or *S*). Even if the time needed to execute the optimizable
    or parallelizable component *P* is reduced to zero, the serial component *S* will
    exist, and will come to eventually dominate performance. Since *S* and *P* are
    fractions, note that *S* + *P* = 1.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: Consider a program that executes on one core in time *T*[1]. Then, the fraction
    of the program execution that is necessarily serial takes *S* × *T*[1] time to
    run, and the parallelizable fraction of program execution (*P* = 1 *– S*) takes
    *P* × *T*[1] to run.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: 'When the program executes on *c* cores, the serial fraction of the code still
    takes *S* × *T*[1] time to run (all other conditions being equal), but the parallelizable
    fraction can be divided into *c* cores. Thus, the maximum improvement for the
    parallel processor with *c* cores to run the same job is:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/equ0711-01.jpg)'
  id: totrans-350
  prefs: []
  type: TYPE_IMG
- en: As *c* increases, the execution time on the parallel processor becomes dominated
    by the serial fraction of the program.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: To understand the impact of Amdahl’s law, consider a program that is 90% parallelizable
    and executes in 10 seconds on 1 core. In our equation, the parallelizable component
    (*P*) is 0.9, while the serial component (*S*) is 0.1\. [Table 14-4](ch14.xhtml#ch14tab4)
    depicts the corresponding total time on *c* cores (*T*[*c*]) according to Amdahl’s
    Law, and the associated speedup.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 14-4:** The Effect of Amdahl’s Law on a 10-Second Program that is 90%
    Parallelizable'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '| **Number of cores** | **Serial time (s)** | **Parallel time (s)** | **Total
    time (***T*[*c*] **s)** | **Speedup (over one core)** |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 9 | 10 | 1 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
- en: '| 10 | 1 | 0.9 |   1.9 | 5.26 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
- en: '| 100 | 1 | 0.09 |   1.09 | 9.17 |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
- en: '| 1000 | 1 | 0.009 |   1.009 | 9.91 |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
- en: Observe that, over time, the serial component of the program begins to dominate,
    and the effect of adding more and more cores seems to have little to no effect.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: 'A more formal way to look at this requires incorporating Amdahl’s calculation
    for *T*[c] into the equation for speedup:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/equ0712-01.jpg)'
  id: totrans-361
  prefs: []
  type: TYPE_IMG
- en: Taking the limit of this equation shows that as the number of cores (*c*) approaches
    infinity, speedup approaches 1/*S*. In the example shown in [Table 14-4](ch14.xhtml#ch14tab4),
    speedup approaches 1/0.1, or 10.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: As another example, consider a program where *P* = 0.99\. In other words, 99%
    of the program is parallelizable. As *c* approaches infinity, the serial time
    starts to dominate the performance (in this example, *S* = 0.01). Thus, speedup
    approaches 1/0.01 or 100\. In other words, even with a million cores, the maximum
    speedup achievable by this program is only 100.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: 'ALL IS NOT LOST: THE LIMITS OF AMDAHL’S LAW'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: 'When learning about Amdahl’s Law, it’s important to consider the *intentions*
    of its originator, Gene Amdahl. In his own words, the law was proposed to demonstrate
    “the continued validity of the single processor approach, and the weakness of
    the multiple processor approach in terms of application to real problems and their
    attendant irregularities.”^([8](ch14.xhtml#fn14_8)) In his 1967 paper Amdahl expanded
    on this concept, writing: “For over a decade prophets have voiced the contention
    that the organization of a single computer has reached its limits, and that truly
    significant advances can be made only by interconnection of a multiplicity of
    computers in such a manner as to permit cooperative solution.” Subsequent work
    challenged some of the key assumptions made by Amdahl. Read about the Gustafson–Barsis
    Law in the next subsection for a discussion on the limits of Amdahl’s Law and
    a different argument on how to think about the benefits of parallelism.'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: 14.4.2 Advanced Topics
  id: totrans-366
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Gustafson–Barsis Law
  id: totrans-367
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In 1988, John L. Gustafson, a computer scientist and researcher at Sandia National
    Labs, wrote a paper called “Reevaluating Amdahl’s Law.”^([9](ch14.xhtml#fn14_9))
    In this paper, Gustafson calls to light a critical assumption that was made about
    the execution of a parallel program that is not always true.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, Amdahl’s law implies that the number of compute cores *c* and
    the fraction of a program that is parallelizable *P* are independent of each other.
    Gustafson notes that this “is virtually never the case.” While benchmarking a
    program’s performance by varying the number of cores on a fixed set of data is
    a useful academic exercise, in the real world, more cores (or processors, as examined
    in our discussion of distributed memory) are added as the problem grows large.
    “It may be most realistic,” Gustafson writes, “to assume run time, not problem
    size, is constant.”
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, according to Gustafson, it is most accurate to say that “The amount
    of work that can be done in parallel varies linearly with the number of processors.”
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: Consider a *parallel* program that takes time *T*[c] to run on a system with
    *c* cores. Let *S* represent the fraction of the program execution that is necessarily
    serial and takes *S* × *T*[c] time to run. Thus, the parallelizable fraction of
    the program execution, *P* = 1 *– S*, takes *P* × *T*[c] time to run on *c* cores.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: 'When the same program is run on just one core, the serial fraction of the code
    still takes *S* × *T*[c] (assuming all other conditions are equal). However, the
    parallelizable fraction (which was divided between *c* cores) now has to be executed
    by just one core to run serially and takes *P* × *T*[c] × *c* time. In other words,
    the parallel component will take *c* times as long on a single-core system. It
    follows that the scaled speedup would be:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/equ0713-01.jpg)'
  id: totrans-373
  prefs: []
  type: TYPE_IMG
- en: This shows that the scaled speedup increases linearly with the number of compute
    units.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: Consider our prior example in which 99% of a program is parallelizable (i.e.,
    *P* = 0.99). Applying the scaled speedup equation, the theoretical speedup on
    100 processors would be 99.01\. On 1,000 processors, it would be 990.01\. Notice
    that the efficiency stays constant at *P*.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: As Gustafson concludes, “speedup should be measured by scaling the problem to
    the number of processors, not by fixing a problem size.” Gustafson’s result is
    notable because it shows that it is possible to get increasing speedup by updating
    the number of processors. As a researcher working in a national supercomputing
    facility, Gustafson was more interested in doing *more work* in a constant amount
    of time. In several scientific fields, the ability to analyze more data usually
    leads to higher accuracy or fidelity of results. Gustafson’s work showed that
    it was possible to get large speedups on large numbers of processors, and revived
    interest in parallel processing.^([10](ch14.xhtml#fn14_10))
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: Scalability
  id: totrans-377
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We describe a program as *scalable* if we see improving (or constant) performance
    as we increase the number of resources (cores, processors) or the problem size.
    Two related concepts are *strong scaling* and *weak scaling*. It is important
    to note that “weak” and “strong” in this context do not indicate the *quality*
    of a program’s scalability, but are simply different ways to measure scalability.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: We say that a program is *strongly scalable* if increasing the number of cores/processing
    units on a *fixed* problem size yields an improvement in performance. A program
    displays strong linear scalability if, when run on *n* cores, the speedup is also
    *n*. Of course, Amdahl’s Law guarantees that after some point, adding additional
    cores makes little sense.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: We say that a program is *weakly scalable* if increasing the size of the data
    at the same rate as the number of cores (i.e., if there is a fixed data size per
    core/processor) results in constant or an improvement in performance. We say a
    program displays weak linear scalability if we see an improvement of *n* if the
    work per core is scaled up by a factor of *n*.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: General Advice Regarding Measuring Performance
  id: totrans-381
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We conclude our discussion on performance with some notes about benchmarking
    and performance on hyperthreaded cores.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: '**Run a program multiple times when benchmarking.**   In many of the examples
    shown thus far in this book, we run a program only once to get a sense of its
    runtime. However, this is not sufficient for formal benchmarks. Running a program
    once is *never* an accurate measure of a program’s true runtime! Context switches
    and other running processes can temporarily cause the runtime to radically fluctuate.
    Therefore, it is always best to run a program several times and report an average
    runtime together with as many details as feasible, including number of runs, observed
    variability of the measurements (e.g., error bars, minimum, maximum, median, standard
    deviation) and conditions under which the measurements were taken.'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: '**Be careful where you measure timing.**   The `gettimeofday` function is useful
    in helping to accurately measure the time a program takes to run. However, it
    can also be abused. Even though it may be tempting to place the `gettimeofday`
    call around only the thread creation and joining component in `main`, it is important
    to consider what exactly you are trying to time. For example, if a program reads
    in an external data file as a necessary part of its execution, the time for file
    reading should likely be included in the program’s timing.'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: '**Be aware of the impact of hyperthreaded cores.**   As discussed in “Taking
    a Closer Look: How Many Cores?” on [page 671](ch14.xhtml#lev3_112) and “Multicore
    and Hardware Multithreading” on [page 283](ch05.xhtml#lev2_108), hyperthreaded
    (logical) cores are capable of executing multiple threads on a single core. In
    a quad-core system with two logical threads per core, we say there are eight hyperthreaded
    cores on the system. Running a program in parallel on eight logical cores in many
    cases yields better wall time than running a program on four cores. However, due
    to the resource contention that usually occurs with hyperthreaded cores, you may
    see a dip in core efficiency and nonlinear speedup.'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: '**Beware of resource contention.**   When benchmarking, it’s always important
    to consider what *other* processes and threaded applications are running on the
    system. If your performance results ever look a bit strange, it is worth quickly
    running `top` to see whether there are any other users also running resource-intensive
    tasks on the same system. If so, try using a different system to benchmark (or
    wait until the system is not so heavily used).'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: 14.5 Cache Coherence and False Sharing
  id: totrans-387
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Multicore caches can have profound implications on a multithreaded program’s
    performance. First, however, let’s quickly review some of the basic concepts related
    to cache design (see “CPU Caches” on page 1299 for more details):'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: Data/instructions are not transported *individually* to the cache. Instead,
    data is transferred in *blocks*, and block sizes tend to get larger at lower levels
    of the memory hierarchy.
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each cache is organized into a series of sets, with each set having a number
    of lines. Each line holds a single block of data.
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The individual bits of a memory address are used to determine the set, tag,
    and block offset of the cache to which to write a block of data.
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A *cache hit* occurs when the desired data block exists in the cache. Otherwise,
    a *cache miss* occurs, and a lookup is performed on the next lower level of the
    memory hierarchy (which can be cache or main memory).
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *valid bit* indicates if a block at a particular line in the cache is safe
    to use. If the valid bit is set to 0, the data block at that line cannot be used
    (e.g., the block could contain data from an exited process).
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Information is written to cache/memory based on two main strategies. In the
    *write-through* strategy, the data is written to cache and main memory simultaneously.
    In the *write-back* strategy, data is written only to cache and gets written to
    lower levels in the hierarchy after the block is evicted from the cache.
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 14.5.1 Caches on Multicore Systems
  id: totrans-395
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Recall that in shared memory architectures each core can have its own cache
    (see “Looking Ahead: Caching on Multicore Processors” on [page 581](ch11.xhtml#lev1_91))
    and that multiple cores can share a common cache. [Figure 14-8](ch14.xhtml#ch14fig8)
    depicts an example dual-core CPU. Even though each core has its own local L1 cache,
    the cores share a common L2 cache.'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/14fig08.jpg)'
  id: totrans-397
  prefs: []
  type: TYPE_IMG
- en: '*Figure 14-8: An example dual-core CPU with separate L1 caches and a shared
    L2 cache*'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: Multiple threads in a single executable may execute separate functions. Without
    a *cache coherency* strategy (see “Cache Coherency” on [page 583](ch11.xhtml#lev2_198))
    to ensure that each cache maintains a consistent view of shared memory, it is
    possible for shared variables to be updated inconsistently. As an example, consider
    the dual-core processor in [Figure 14-8](ch14.xhtml#ch14fig8), where each core
    is busy executing separate threads concurrently. The thread assigned to Core 0
    has a local variable `x`, whereas the thread executing on Core 1 has a local variable
    `y`, and both threads have shared access to a global variable `g`. [Table 14-5](ch14.xhtml#ch14tab5)
    shows one possible path of execution.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 14-5:** Problematic Data Sharing Due to Caching'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: '| **Time** | **Core 0** | **Core 1** |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
- en: '| 0 | `g = 5` | (other work) |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
- en: '| 1 | (other work) | `y = g*4` |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
- en: '| 2 | `x += g` | `y += g*2` |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
- en: 'Suppose that the initial value of `g` is 10, and the initial values of `x`
    and `y` are both 0\. What is the final value of `y` at the end of this sequence
    of operations? Without cache coherence, this is a very difficult question to answer
    given that there are at least three stored values of `g`: one in Core 0’s L1 cache,
    one in Core 1’s L1 cache, and a separate copy of `g` stored in the shared L2 cache.'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 14-9](ch14.xhtml#ch14fig9) shows one possible erroneous result after
    the sequence of operations in [Table 14-5](ch14.xhtml#ch14tab5) completes. Suppose
    that the L1 caches implement a write-back policy. When the thread executing on
    Core 0 writes the value 5 to `g`, it updates only the value of `g` in Core 0’s
    L1 cache. The value of `g` in Core 1’s L1 cache still remains 10, as does the
    copy in the shared L2 cache. Even if a write-through policy is implemented, there
    is no guarantee that the copy of `g` stored in Core 1’s L1 cache gets updated!
    In this case, `y` will have the final value of `60`.'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/14fig09.jpg)'
  id: totrans-407
  prefs: []
  type: TYPE_IMG
- en: '*Figure 14-9: A problematic update to caches that do not employ cache coherency*'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: A cache coherence strategy invalidates or updates cached copies of shared values
    in other caches when a write to the shared data value is made in one cache. The
    *modified shared invalid* (MSI) protocol (discussed in detail in “The MSI Protocol”
    on [page 584](ch11.xhtml#lev2_199)) is one example of an invalidating cache coherency
    protocol.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: A common technnique for implementing MSI is snooping. Such a *snoopy cache*
    “snoops” on the memory bus for possible write signals. If the snoopy cache detects
    a write to a shared cache block, it invalidates its line containing that cache
    block. The end result is that the only valid version of the block is in the cache
    that is written to, whereas *all other* copies of the block in other caches are
    marked as invalid.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: Employing the MSI protocol with snoooping would yield the correct final assignment
    of `30` to variable `y` in the previous example.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: 14.5.2 False Sharing
  id: totrans-412
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Cache coherence guarantees correctness, but it can potentially harm performance.
    Recall that when the thread updates `g` on Core 0, the snoopy cache invalidates
    not only `g`, but the *entire cache line* that `g` is a part of.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider our initial attempt at parallelizing the `countElems` function of
    the CountSort algorithm.⁴ For convenience, the function is reproduced here:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'In our previous discussion of this function (see “Data Races” on [page 691](ch14.xhtml#lev3_116)),
    we pointed out how data races can cause the `counts` array to not populate with
    the correct set of counts. Let’s see what happens if we attempt to *time* this
    function. We add timing code to `main` using `getimeofday` as before.⁶ Benchmarking
    the initial version of `countElems` as just shown on 100 million elements yields
    the following times:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Even without any synchronization constructs, this version of the program *still
    gets slower* as the number of threads increases!
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: To understand what is going on, let’s revisit the `counts` array. This holds
    the frequency of occurrence of each number in our input array. The maximum value
    is determined by the variable `MAX`. In our example program, `MAX` is set to 10\.
    In other words, the `counts` array takes up 40 bytes of space.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that the cache details on a Linux system (see “Looking Ahead: Caching
    on Multicore Processors” on [page 581](ch11.xhtml#lev1_91)) are located in the
    `/sys/devices/system/cpu/` directory. Each logical core has its own `cpu` subdirectory
    called `cpuk`, where `k` indicates the *k*th logical core. Each `cpu` subdirectory
    in turn has separate `index` directories that indicate the caches available to
    that core.'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: 'The `index` directories contain files with numerous details about each logical
    core’s caches. The contents of a sample `index0` directory are shown here (`index0`
    typically corresponds to a Linux system’s L1 cache):'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-422
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'To discover the cache line size of the L1 cache, use this command:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: The output reveals that the L1 cache line size for the machine is 64 bytes.
    In other words, the 40-byte `counts` array fits *within one cache line*.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: Recall that with invalidating cache coherence protocols like MSI, every time
    a program updates a shared variable, the *entire cache line in other caches storing
    the variable is invalidated*. Let’s consider what happens when two threads execute
    the preceding function. One possible path of execution is shown in [Table 14-6](ch14.xhtml#ch14tab6)
    (assuming that each thread is assigned to a separate core, and the variable `x`
    is local to each thread).
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 14-6:** A Possible Execution Sequence of Two Threads Running `countElems`'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: '| **Time** | **Thread 0** | **Thread 1** |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
- en: '| *i* | Reads `array[x]` | … |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
- en: '|  | (1) |  |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
- en: '| *i* + 1 | Increments `counts[1]` (**invalidates** | Reads `array[x]` (4)
    |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
- en: '|  | **cache line**) |  |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
- en: '| *i* + 2 | Reads `array[x]` (6) | Increments `counts[4]` (**invalidates**
    |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
- en: '|  |  | **cache line**) |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
- en: '| *i* + 3 | Increments `counts[6]` (**invalidates** | Reads `array[x]` (2)
    |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
- en: '|  | **cache line**) |  |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
- en: '| *i* + 4 | Reads `array[x]` (3) | Increments `counts[2]` (**invalidates**
    |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
- en: '|  |  | **cache line**) |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
- en: '| *i* + 5 | Increments `counts[3]` (**invalidates** | … |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
- en: '|  | **cache line**) |  |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
- en: During time step *i*, Thread 0 reads the value at `array[x]` in its part of
    the array, which is a 1 in this example. During time steps *i* + 1 to *i* + 5,
    each thread reads a value from `array[x]`. Note that each thread is looking at
    different components of the array. Not only that, each read of `array` in our
    sample execution yields unique values (so no race conditions in this sample execution
    sequence!). After reading the value from `array[x]`, each thread increments the
    associated value in `counts`.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: Recall that the `counts` array *fits on a single cache line* in our L1 cache.
    As a result, every write to `counts` invalidates the *entire line* in *every other
    L1 cache*. The end result is that, despite updating *different* memory locations
    in `counts`, any cache line containing `counts` is *invalidated* with *every update*
    to `counts`!
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: The invalidation forces all L1 caches to update the line with a “valid” version
    from L2\. The repeated invalidation and overwriting of lines from the L1 cache
    is an example of *thrashing*, where repeated conflicts in the cache cause a series
    of misses.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: The addition of more cores makes the problem worse, given that now more L1 caches
    are invalidating the line. As a result, adding additional threads slows down the
    runtime, despite the fact that each thread is accessing different elements of
    the `counts` array! This is an example of *false sharing*, or the illusion that
    individual elements are being shared by multiple cores. In the previous example,
    it appears that all the cores are accessing the same elements of `counts`, even
    though this is not the case.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: 14.5.3 Fixing False Sharing
  id: totrans-445
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One way to fix an instance of false sharing is to pad the array (in our case
    `counts`) with additional elements so that it doesn’t fit in a single cache line.
    However, padding can waste memory, and may not eliminate the problem from all
    architectures (consider the scenario in which two different machines have different
    L1 cache sizes). In most cases, writing code to support different cache sizes
    is generally not worth the gain in performance.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: A better solution is to have threads write to *local storage* whenever possible.
    Local storage in this context refers to memory that is *local* to a thread. The
    following solution reduces false sharing by choosing to perform updates to a locally
    declared version of `counts` called `local_counts`.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: Let’s revisit the final version of our `countElems` function:⁶
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-449
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The use of `local_counts` to accumulate frequencies in lieu of `counts` is
    the major source of reduction of false sharing in this example:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Since cache coherence is meant to maintain a consistent view of shared memory,
    the invalidations trigger only on *writes* to *shared values* in memory. Since
    `local_counts` is not shared among the different threads, a write to it will not
    invalidate its associated cache line.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: 'In the last component of the code, the mutex enforces correctness by ensuring
    that only one thread updates the shared `counts` array at a time:'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Since `counts` is located on a single cache line, it will still get invalidated
    with every write. The difference is that the penalty here is at most `MAX` × *t*
    writes vs. *n* writes, where *n* is the length of our input array, and *t* is
    the number of threads employed.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: 14.6 Thread Safety
  id: totrans-456
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, we have covered synchronization constructs that programmers can use
    to ensure that their multithreaded programs are consistent and correct regardless
    of the number of threads employed. However, it is not always safe to make the
    assumption that standard C library functions can be used “as is” in the context
    of any multithreaded application. Not all functions in the C library are *thread
    safe*, or capable of being run by multiple threads while guaranteeing a correct
    result without unintended side effects. To ensure that the programs *we* write
    are thread safe, it is important to use synchronization primitives like mutexes
    and barriers to enforce that multithreaded programs are consistent and correct
    regardless of how the number of threads varies.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: Another closely related concept related to thread safety is re-entrancy. All
    thread safe code is re-entrant; however, not all re-entrant code is thread safe.
    A function is *re-entrant* if it can be re-executed/partially executed by a function
    without causing issue. By definition, re-entrant code ensures that accesses to
    the global state of a program always result in that global state remaining consistent.
    While re-entrancy is often (incorrectly) used as a synonym for thread safety,
    there are special cases for which re-entrant code is not thread safe.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: When writing multithreaded code, verify that the C library functions used are
    indeed thread safe. Fortunately, the list of thread-unsafe C library functions
    is fairly small. The Open Group kindly maintains a list of thread unsafe functions.^([11](ch14.xhtml#fn14_11))
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: 14.6.1 Fixing Issues of Thread Safety
  id: totrans-460
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Synchronization primitives are the most common way to fix issues related to
    thread safety. However, unknowingly using thread-unsafe C library functions can
    cause subtle issues. Let’s look at a slightly modified version of our `countsElem`
    function called `countElemsStr`, which attempts to count the frequency of digits
    in a given string, where each digit is separated by spaces. The following program
    has been edited for brevity; the full source of this program is available online.^([12](ch14.xhtml#fn14_12))
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-462
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: The `countElemsStr` function uses the `strtok` function (as examined in our
    discussion in “strtok, strtok_r” on [page 100](ch02.xhtml#lev3_22)) to parse each
    digit (stored in `token`) in the string, before converting it to an integer and
    making the associated updates in the `counts` array.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: 'Compiling and running this program on 100,000 elements yields the following
    output:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-465
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Now, let’s take a look at a multithreaded version of `countElemsStr`:^([13](ch14.xhtml#fn14_13))
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-467
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: In this version of the program, each thread processes a separate section of
    the string referenced by `input_str`. The `local_counts` array ensures that the
    bulk of the write operations occur to local storage. A mutex is employed to ensure
    that no two threads write to the shared variable `counts`.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
- en: 'However, compiling and running this program yields the following results:'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-470
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Even though mutex locks are used around accesses to the `counts` array, the
    results from separate runs are radically different. This issue arises because
    the `countsElemsStr` function is not thread safe, because the string library function
    `strtok` is *not thread safe*! Visiting the OpenGroup website^(11) confirms that
    `strtok` is on the list of thread-unsafe functions.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: To fix this issue, it suffices to replace `strtok` with its thread-safe alternative,
    `strtok_r`. In the latter function, a pointer is used as the last parameter to
    help the thread keep track of where in the string it is parsing. Here is the fixed
    function with `strtok_r`:^([14](ch14.xhtml#fn14_14))
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-473
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'The only change in this version of the code is the declaration of the character
    pointer `saveptr` and replacing all instances of `strtok` with `strtok_r`. Rerunning
    the code with these changes yields the following output:'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-475
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Now the program produces the same result for every run. The use of `saveptr`
    in conjunction with `strtok_r` ensures that each thread can independently track
    their location when parsing the string.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
- en: The takeaway from this section is that one should always check the list of thread-unsafe
    functions in C^(11) when writing multithreaded applications. Doing so can save
    the programmer a lot of heartache and frustration when writing and debugging threaded
    applications.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: 14.7 Implicit Threading with OpenMP
  id: totrans-478
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Thus far, we have presented shared memory programming using POSIX threads. Although
    Pthreads are great for simple applications, they become increasingly difficult
    to use as programs themselves become more complex. POSIX threads are an example
    of *explicit parallel programming* of threads, requiring a programmer to specify
    exactly what each thread is required to do and when each thread should start and
    stop.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: With Pthreads, it can also be challenging to *incrementally* add parallelism
    to an existing sequential program. That is, one must often rewrite the program
    entirely to use threads, which is often not desirable when attempting to parallelize
    a large, existing codebase.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
- en: The Open Multiprocessing (OpenMP) library implements an *implicit* alternative
    to Pthreads. OpenMP is built in to GCC and other popular compilers such as LLVM
    and Clang, and can be used with the C, C++, and Fortran programming languages.
    A key advantage of OpenMP is that it enables programmers to parallelize components
    of existing, sequential C code by adding *pragmas* (special compiler directives)
    to parts of the code. Pragmas specific to OpenMP begin with `#pragma omp`.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: Detailed coverage of OpenMP is outside the scope of this book, but we do cover
    some common pragmas and show how several can be used in the context of some sample
    applications.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: 14.7.1 Common Pragmas
  id: totrans-483
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here are some of the most commonly used pragmas in OpenMP programs:'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: '#pragma omp parallel  This pragma creates a team of threads and has each thread
    run the code in its scope (usually a function call) on each thread. An invocation
    of this pragma is usually equivalent to an invocation of the `pthread_create`
    and `pthread_join` function pairing discussed in “Creating and Joining Threads”
    on [page 679](ch14.xhtml#lev2_236). The pragma may have a number of clauses, including
    the following:'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: num_threads  Specifies the number of threads to create.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: private  A list of variables that should be private (or local) to each thread.
    Variables that should be private to a thread can also be declared within the scope
    of the pragma (see below for an example). Each thread gets its own copy of each
    variable.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
- en: shared  A listing of variables that should be shared among the threads. There
    is one copy of the variable that is shared among all threads.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
- en: default  Indicates whether the determination of which variables should be shared
    is left up to the compiler. In most cases, we want to use `default(none)` and
    specify explicitly which variables should be shared and which should be private.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
- en: '#pragma omp for  Specifies that each thread execute a subset of iterations
    of a `for` loop. Although the scheduling of the loops is up to the system, the
    default is usually the “chunking” method first discussed in “Revisiting Scalar
    Multiplication” on [page 682](ch14.xhtml#lev2_239). This is a *static* form of
    scheduling: each thread gets an assigned chunk, and then processes the iterations
    in its chunk. However, OpenMP also makes *dynamic* scheduling easy. In dynamic
    scheduling, each thread gets a number of iterations, and requests a new set upon
    completing processing their iteration. The scheduling policy can be set using
    the following clause:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
- en: schedule(dynamic)  Specifies that a *dynamic* form of scheduling should be used.
    While this is advantageous in some cases, the static (default) form of scheduling
    is usually faster.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
- en: '#pragma omp parallel for  This pragma is a combination of the `omp parallel`
    and the `omp for` pragmas. Unlike the `omp for` pragma, the `omp parallel for`
    pragma also generates a team of threads before assigning each thread a set of
    iterations of the loop.'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
- en: '#pragma omp critical  This pragma is used to specify that the code under its
    scope should be treated as a *critical section*—that is, only one thread should
    execute the section of code at a time to ensure correct behavior.'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
- en: 'There are also several *functions* that a thread can access that are often
    useful for execution. For example:'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
- en: omp_get_num_threads  Returns the number of threads in the current team that
    is being executed.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
- en: omp_set_num_threads  Sets the number of threads that a team should have.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
- en: omp_get_thread_num  Returns the identifier of the calling thread.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
- en: '**Warning THE OMP PARALLEL FOR DIRECTIVE WORKS ONLY WITH FOR LOOPS!**'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that the `omp parallel for` pragma works *only* with `for` loops.
    Other types of loops, such as `while` loops and `do`–`while` loops, are not supported.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
- en: '14.7.2 Hello Threading: OpenMP Flavored'
  id: totrans-500
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let’s revisit our “Hello World” program,² now using OpenMP instead of Pthreads:'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-502
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Note that the OpenMP program is *much* shorter than the Pthreads version. To
    access the OpenMP library functions, we include the header file `omp.h`. The `omp
    parallel num_threads(nthreads)` pragma in `main` creates a set of threads, where
    each thread calls the `HelloWorld` function. The clause `num _threads(nthreads)`
    specifies that a total of `nthreads` should be generated. The pragma also joins
    each created thread back to a single-threaded process. In other words, all the
    low-level work of creating and joining threads is *abstracted* away from the programmer
    and is accomplished with the inclusion of just one pragma. For this reason, OpenMP
    is considered an *implicit threading* library.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
- en: OpenMP also abstracts away the need to explicitly manage thread IDs. In the
    context of `HelloWorld`, the `omp_get_thread_num` function extracts the unique
    ID associated with the thread that is running it.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
- en: Compiling the code
  id: totrans-505
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Let’s compile and run this program by passing the `-fopenmp` flag to the compiler,
    which signals that we’re compiling with OpenMP:'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-507
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Since the execution of threads can change with subsequent runs, rerunning this
    program results in a different sequence of messages:'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-509
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: This behavior is consistent with our example with Pthreads (see “Hello Threading!
    Writing Your First Multithreaded Program” on [page 677](ch14.xhtml#lev1_106)).
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
- en: '14.7.3 A More Complex Example: CountSort in OpenMP'
  id: totrans-511
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A powerful advantage of OpenMP is that it enables programmers to incrementally
    parallelize their code. To see this in action, let’s parallelize the more complex
    CountSort algorithm discussed earlier in this chapter. Recall that this algorithm
    sorts arrays containing a small range of values. The main function of the serial
    program³ looks like the following:'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-513
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: The `main` function, after doing some command line parsing and generating a
    random array, calls the `countsElems` function followed by the `writeArray` function.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
- en: Parallelizing CountElems Using OpenMP
  id: totrans-515
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: There are several ways to parallelize the preceding program. One way (shown
    in the example that follows) uses the `omp parallel` pragma in the context of
    the `countElems` and `writeArray` functions. As a result, no changes need to be
    made to the `main` function.^([15](ch14.xhtml#fn14_15))
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s examine how to parallelize the `countElems` function using OpenMP:'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-518
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: In this version of the code, three pragmas are employed. The `#pragma omp parallel`
    pragma indicates that a team of threads should be created. The `omp_set_num_threads(nthreads)`
    line in `main` sets the default size of the thread team to be `nthreads`. If the
    `omp_set_num_threads` function is not used, then the number of threads assigned
    will equal the number of cores in the system. As a reminder, the `omp parallel`
    pragma implicitly creates threads at the beginning of the block and joins them
    at the end of the block. Braces (`{}`) are used to specify scope. The `shared`
    clause declares that the variables `counts`, `array`, and `length` are shared
    (global) among all the threads. Thus, the variables `val`, `i`, and `local[MAX]`
    are declared *locally* in each thread.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
- en: The next pragma is `#pragma omp for`, which parallelizes the `for` loop, splitting
    the number of iterations among the number of threads. OpenMP calculates how best
    to split up the iterations of the loop. As previously mentioned, the default strategy
    is usually a chunking method, wherein each thread gets roughly the same number
    of iterations to compute. Thus, each thread reads a component of the shared array
    `array`, and accumulates its counts in its local array `local`.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
- en: The `#pragma omp critical` pragma indicates that the code in the scope of the
    critical section should be executed by exactly one thread at a time. This is equivalent
    to the mutex that was employed in the Pthreads version of this program. Here,
    each thread increments the shared `counts` array one at a time.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s get a sense of the performance of this function by running it with 100
    million elements:'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-523
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: This is excellent performance, with our function getting a speedup of 2 on two
    threads, and a speedup of 3.63 on four threads. We get even better performance
    than the Pthreads implementation!
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
- en: The writeArray Function in OpenMP
  id: totrans-525
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Parallelizing the `writeArray` function is *much* harder. The following code
    shows one possible solution:'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-527
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Prior to parallelizing, we made a change to this function because the old version
    of `writeArray` caused `j` to have a dependency on the previous iterations of
    the loop. In this version, each thread calculates its unique `start` value based
    on the sum of all the previous elements in `counts`.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
- en: When this dependency is removed, the parallelization is pretty straightforward.
    The `#pragma omp parallel for` pragma generates a team of threads and parallelizes
    the `for` loop by assigning each thread a subset of the iterations of the loop.
    As a reminder, this pragma is a combination of the `omp parallel` and the `omp
    for` pragmas (which were used in the parallelization of `countElems`).
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
- en: A chunking approach to scheduling threads (as shown in the earlier `countElems`
    function) is not appropriate here, because it is possible that each element in
    `counts` has a radically different frequency. Therefore, the threads will not
    have equal work, resulting in some threads being assigned more work than others.
    Therefore, the `schedule(dynamic)` clause is employed, so that each thread completes
    the iteration it is assigned before requesting a new iteration from the thread
    manager.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
- en: Since each thread is writing to distinct array locations, mutual exclusion is
    not needed for this function.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
- en: Notice how much cleaner the OpenMP code is than the POSIX thread implementation.
    The code is very readable and required very little modification. This is one of
    the powers of *abstraction*, in which the implementation details are hidden from
    the programmer.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
- en: However, a necessary trade-off for abstraction is control. The programmer assumes
    that the compiler is “smart” enough to take care of the particulars of parallelization
    and thus has an easier time parallelizing their application. However, the programmer
    no longer makes detailed decisions about the particulars of that parallelization.
    Without a clear idea of how OpenMP pragmas execute under the hood, it can be difficult
    to debug an OpenMP application or know which pragma is the most appropriate to
    use at a given time.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
- en: 14.7.4 Learning More About OpenMP
  id: totrans-534
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A deeper discussion of OpenMP is beyond the scope of this book, but there are
    useful free resources for learning^([16](ch14.xhtml#fn14_16)) and using^([17](ch14.xhtml#fn14_17))
    OpenMP.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
- en: 14.8 Summary
  id: totrans-536
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This chapter provided an overview of multicore processors and how to program
    them. Specifically, we cover the POSIX threads (or Pthreads) library and how to
    use it to create correct multithreaded programs that speed up a single-threaded
    program’s performance. Libraries like POSIX and OpenMP utilize the *shared memory*
    model of communication, as threads share data in a common memory space.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
- en: Key Takeaways
  id: totrans-538
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Threads are the fundamental unit of concurrent programs.**   To parallelize
    a serial program, programmers utilize lightweight constructs known as *threads*.
    For a particular multithreaded process, each thread has its own allocation of
    stack memory, but shares the program data, heap and instructions of the process.
    Like processes, threads run *nondeterministically* on the CPU (i.e., the order
    of execution changes between runs, and which thread is assigned to which core
    is left up to the operating system).'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
- en: '**Synchronization constructs ensure that programs work correctly.**   A consequence
    of shared memory is that threads can accidentally overwrite data residing in shared
    memory. A *race condition* can occur whenever two operations incorrectly update
    a shared value. When that shared value is data, a special type of race condition
    called a *data race* can arise. Synchronization constructs (mutexes, semaphores,
    etc.) help to guarantee program correctness by ensuring that threads execute one
    at a time when updating shared variables.'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
- en: '**Be mindful when using synchronization constructs.**   Synchronization inherently
    introduces points of serial computation in an otherwise parallel program. It is
    therefore important to be aware of *how* one uses synchronization concepts. The
    set of operations that must run atomically is referred to as a *critical section*.
    If a critical section is too big, the threads will execute serially, yielding
    no improvement in runtime. Use synchronization constructs sloppily, and situations
    like *deadlock* may inadvertently arise. A good strategy is to have threads employ
    local variables as much as possible and update shared variables only when necessary.'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
- en: '**Not all components of a program are parallelizable.**   Some programs necessarily
    have large serial components that can hinder a multithreaded program’s performance
    on multiple cores (e.g., *Amdahl’s Law*). Even when a high percentage of a program
    is parallelizable, speedup is rarely linear. Readers are also encouraged to look
    at other metrics such as efficiency and scalability when ascertaining the performance
    of their programs.'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  id: totrans-543
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This chapter is meant to give a taste of concurrency topics with threads; it
    is by no means exhaustive. To learn more about programming with POSIX threads
    and OpenMP, check out the excellent tutorials on Pthreads^([18](ch14.xhtml#fn14_18))
    and OpenMP^([19](ch14.xhtml#fn14_19)) by Blaise Barney from Lawrence Livermore
    National Labs. For automated tools for debugging parallel programs, readers are
    encouraged to check out the Helgrind^([20](ch14.xhtml#fn14_20)) and DRD^([21](ch14.xhtml#fn14_21))
    Valgrind tools.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
- en: In the final chapter of the book, we give a high-level overview of other common
    parallel architectures and how to program them.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
- en: Notes
  id: totrans-546
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[1.](ch14.xhtml#rfn14_1) *[https://www.raspberrypi.org/](https://www.raspberrypi.org/)*'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
- en: '[2.](ch14.xhtml#rfn14_2) Available at *[https://diveintosystems.org/book/C14-SharedMemory/_attachments/hellothreads.c](https://diveintosystems.org/book/C14-SharedMemory/_attachments/hellothreads.c)*.'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
- en: '[3.](ch14.xhtml#rfn14_3) Available at *[https://diveintosystems.org/book/C14-SharedMemory/_attachments/countSort.c](https://diveintosystems.org/book/C14-SharedMemory/_attachments/countSort.c)*.'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
- en: '[4.](ch14.xhtml#rfn14_4) Available at *[https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElems_p.c](https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElems_p.c)*.'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
- en: '[5.](ch14.xhtml#rfn14_5) The full source can be downloaded from *[https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElems_p_v2.c](https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElems_p_v2.c)*.'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
- en: '[6.](ch14.xhtml#rfn14_6) The full source code for this final program can be
    accessed at *[https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElems_p_v3.c](https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElems_p_v3.c)*.'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
- en: '[7.](ch14.xhtml#rfn14_7) Available at *[https://diveintosystems.org/book/C14-SharedMemory/_attachments/layeggs.c](https://diveintosystems.org/book/C14-SharedMemory/_attachments/layeggs.c)*.'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
- en: '[8.](ch14.xhtml#rfn14_8) Gene Amdahl. “Validity of the single processor approach
    to achieving large scale computing capabilities,” *Proceedings of the April 18-20,
    1967, Spring Joint Computer Conference*, pp. 483–485, ACM, 1967.'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
- en: '[9.](ch14.xhtml#rfn14_9) John Gustafson, “Reevaluating Amdahl’s law,” *Communications
    of the ACM* 31(5), pp. 532–533, 1988.'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
- en: '[10.](ch14.xhtml#rfn14_10) Caroline Connor, “Movers and Shakers in HPC: John
    Gustafson,” *HPC Wire*, *[http://www.hpcwire.com/hpcwire/2010-10-20/movers_and_shakers_in_hpc_john_gustafson.html](http://www.hpcwire.com/hpcwire/2010-10-20/movers_and_shakers_in_hpc_john_gustafson.html)*.'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
- en: '[11.](ch14.xhtml#rfn14_11) *[http://pubs.opengroup.org/onlinepubs/009695399/functions/xsh_chap02_09.html](http://pubs.opengroup.org/onlinepubs/009695399/functions/xsh_chap02_09.html)*'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
- en: '[12.](ch14.xhtml#rfn14_12) [https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElemsStr.c](https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElemsStr.c)'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
- en: '[13.](ch14.xhtml#rfn14_13) Available at *[https://diveintosystems.org/book/C14-SharedMemory/_attachmentscountElemsStr_p.c](https://diveintosystems.org/book/C14-SharedMemory/_attachmentscountElemsStr_p.c)*.'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
- en: '[14.](ch14.xhtml#rfn14_14) Full source code available at [https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElemsStr_p_v2.c](https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElemsStr_p_v2.c).'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
- en: '[15.](ch14.xhtml#rfn14_15) A full version of the program is available at *[https://diveintosystems.org/book/C14-SharedMemory/_attachments/countSort_mp.c](https://diveintosystems.org/book/C14-SharedMemory/_attachments/countSort_mp.c)*.'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
- en: '[16.](ch14.xhtml#rfn14_16) Blaise Barney, “OpenMP,” [https://hpc.llnl.gov/tuts/openMP/](https://hpc.llnl.gov/tuts/openMP/)'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
- en: '[17.](ch14.xhtml#rfn14_17) Richard Brown and Libby Shoop, “Multicore Programming
    with OpenMP,” *CSinParallel: Parallel Computing in the Computer Science Curriculum*,
    [http://selkie.macalester.edu/csinparallel/modules/MulticoreProgramming/build/html/index.html](http://selkie.macalester.edu/csinparallel/modules/MulticoreProgramming/build/html/index.html)'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
- en: '[18.](ch14.xhtml#rfn14_18) *[https://hpc-tutorials.llnl.gov/posix/](https://hpc-tutorials.llnl.gov/posix/)*'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
- en: '[19.](ch14.xhtml#rfn14_19) *[https://hpc.llnl.gov/tuts/openMP/](https://hpc.llnl.gov/tuts/openMP/)*'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
- en: '[20.](ch14.xhtml#rfn14_20) *[https://valgrind.org/docs/manual/hg-manual.html](https://valgrind.org/docs/manual/hg-manual.html)*'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
- en: '[21.](ch14.xhtml#rfn14_21) *[https://valgrind.org/docs/manual/drd-manual.html](https://valgrind.org/docs/manual/drd-manual.html)*'
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
