- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bayes’ Rule
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/chapterart.png)'
  prefs: []
  type: TYPE_IMG
- en: In Chapter 3 we discussed some performance measures based on probability. As
    we dig a little deeper into probability and its use in machine learning, we find
    that there are two fundamentally different schools of thought about how to approach
    the subject.
  prefs: []
  type: TYPE_NORMAL
- en: The approach that’s most commonly taught in schools is called the *frequentist
    method*. The other approach is called the *Bayesian method,* named for Thomas
    Bayes, who originally presented the idea in the 1700s. Although it’s less well
    known, the Bayesian method is popular in machine learning. There are many reasons
    for this, but one of the most important is that it gives us a way to explicitly
    identify and use our expectations about the system we’re measuring.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter we first look at the difference between frequentist and Bayesian
    methods. We then discuss the basics of Bayesian probability, covering enough of
    it to allow us to make sense of machine learning papers and documentation that
    are based on Bayesian ideas. We focus on Bayes’ Rule, also called Bayes’ Theorem,
    the cornerstone of Bayesian statistics. Even this single rule is a substantial
    topic, so we only address it in its broadest terms (Kruschke 2014). We come back
    to comparing Bayesian and frequentist approaches at the end of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Frequentist and Bayesian Probability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In mathematics, there’s almost always more than one way to think about a problem,
    or even a whole field. Sometimes the differences between approaches are subtle,
    and sometimes they’re dramatic. Probability definitely sits in the latter camp.
    There are at least two different philosophical approaches to probability, each
    with its strengths and weaknesses. The differences between the frequentist and
    Bayesian approaches have deep philosophical roots and are often expressed in the
    nuances of the mathematics and logic that are used to build their corresponding
    theories of probability (VanderPlas 2014). This makes it difficult to discuss
    the differences without getting into a wealth of detail. Despite being so different,
    the challenge of carefully describing the distinctions between these two approaches
    to probability has been called “especially slippery” (Genovese 2004).
  prefs: []
  type: TYPE_NORMAL
- en: Our approach here is to skip the complex arguments. Instead, we describe both
    approaches in general terms so that we can get a feeling for their different goals
    and processes without diving into the details. This helps set the stage conceptually
    for our discussion of Bayes’ Rule, which is the foundation of the Bayesian approach.
  prefs: []
  type: TYPE_NORMAL
- en: The Frequentist Approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Generally speaking, a *frequentist* is a person who distrusts any specific measurement
    or observation, considering it to be only an approximation of a true, underlying
    value. For instance, if we’re frequentists, and we want to know the height of
    a mountain, we assume that each measurement we take is likely to be at least a
    little too big or too small. At the heart of this attitude is the belief that
    a true answer already exists, and that it’s our job to find it. That is, the mountain
    has some exact, well-defined height, and if we work hard enough and take enough
    observations, we’ll be able to discover that value.
  prefs: []
  type: TYPE_NORMAL
- en: To find this true value, we combine a large number of observations. Even though
    we consider each measurement to probably be inexact, we also expect each measurement
    to be an approximation of the real value. If we take a large number of measurements,
    we say that the value that comes up most *frequently* is the one that’s most *probable*.
    This focus on the most-occurring value is what gives frequentism its name. The
    true value is found by combining a large number of measurements, with the most
    frequent values having the most influence (in some cases, we can merely take an
    average of all the measurements).
  prefs: []
  type: TYPE_NORMAL
- en: When probability is first discussed in schools, the frequentist approach is
    usually the one that’s presented because it’s easy to describe, and often fits
    well with common sense.
  prefs: []
  type: TYPE_NORMAL
- en: The Bayesian Approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using the same broad brush, a *Bayesian* is a person who trusts every observation
    as an accurate measure of *something*, though it might be a slightly different
    something each time. The Bayesian attitude is that there’s no “true” value waiting
    to be found at the end of a process. Going back to our mountain example, a Bayesian
    would say that the true value of the height of the mountain is a meaningless idea.
    Instead, every measurement of the height of a mountain describes the distance
    from some point on the ground to some point near the top of the mountain, but
    they won’t be the identical two points every time. So even though every measurement
    has a different value, each one is an accurate measurement of something we could
    call the height of the mountain. Each careful measurement is just as true as the
    others—there’s not a single, definitive value out there, waiting for us to find
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, there’s only a range of possible heights for the mountain, each described
    by a probability. As we take more observations, that range of possibilities generally
    becomes more narrow, but it never shrinks to a single value. We can never state
    the height of the mountain as a number, but only as a range, where each value
    has its own probability.
  prefs: []
  type: TYPE_NORMAL
- en: Frequentists vs. Bayesians
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These two approaches to probability have led to an interesting social phenomenon.
    Some serious people working in probability believe that only the frequentist approach
    has any merit, and the Bayesian approach is a useless distraction. Other serious
    people believe exactly the other way around. Many people have less extreme, but
    still heartfelt, feelings on which approach should be considered the right way
    to think about probability. Of course, many people think that both approaches
    offer useful tools that are applicable in different situations. When we work with
    real data, our choice of how to think about probability can greatly influence
    what kinds of questions we can ask and answer (Stark and Freedman 2016).
  prefs: []
  type: TYPE_NORMAL
- en: A key feature of the Bayesian approach is that we explicitly identify our expectations
    before we start taking measurements. In our mountain example, we’d state up front
    about how high we expect the mountain to be. Some frequentists object to this,
    arguing that you should never enter an experiment with a preconceived expectation,
    or bias. Bayesians reply that bias is inevitable since it’s baked into the design
    of every experiment, influencing what we choose to measure, and how. They argue
    that it’s best to state those expectations clearly so that they can be examined
    and debated. Frequentists disagree and present counterarguments, Bayesians then
    present counter-counterarguments, and the debate goes on.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at the two techniques in action by flipping a coin and asking if
    the coin is fair, so heads and tails come up about equally often, or if it’s weighted
    to come up one way or the other more frequently. Let’s start by looking at how
    a frequentist would address the question, and then we’ll see how a Bayesian would
    go about it.
  prefs: []
  type: TYPE_NORMAL
- en: Frequentist Coin Flipping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'People often use flipping (or tossing) coins as an example when discussing
    probability (Cthaeh 2016a; Cthaeh 2016b). Coin flipping is popular because it’s
    familiar to everyone, and each flip has only two possible outcomes: heads or tails
    (we’ll ignore weird cases like the coin landing on its side). Having only two
    outcomes makes the math simple enough that we can often work things out by hand.
    Though we won’t be doing any math beyond a few little examples, coin flipping
    is still a great way to see the underlying ideas, so we use that as our running
    example here.'
  prefs: []
  type: TYPE_NORMAL
- en: We say that a *fair* coin is one that, on average, comes up heads half the time
    and tails the other half. A coin that isn’t fair we call a *rigged*, *weighted*,or
    *unfair* coin. To describe a rigged coin, we refer to its tendency to come up
    heads, or its *bias*. A coin with a bias of 0.1 comes up heads about 10 percent
    of the time, and a bias of 0.8 tells us to expect heads about 80 percent of the
    time. If a coin has a bias of 0.5, it comes up heads and tails equally often,
    and it is indistinguishable from a fair coin. We actually might say that a coin
    with a bias of 0.5, or 1/2, is the definition of a fair coin.
  prefs: []
  type: TYPE_NORMAL
- en: So, by taking lots of measurements (flips) and combining their results (heads
    or tails), we can hope to find the true answer (the coin’s bias). [Figure 4-1](#figure4-1)
    illustrates the frequentist’s approach to finding the bias of three different
    coins, using one row per coin.
  prefs: []
  type: TYPE_NORMAL
- en: '![F04001](Images/F04001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4-1: Top row: A coin with a bias of 0.2\. Middle row: A coin with a
    bias of 0.5\. Bottom row: A coin with a bias of 0.8.'
  prefs: []
  type: TYPE_NORMAL
- en: On the left of each row in [Figure 4-1](#figure4-1) we show 100 consecutive
    flips of the coin. On the right, we show a frequentist’s estimate of that coin’s
    bias after each flip. This is found by dividing the number of heads we’ve found
    up until then by the total number of flips. As usual, the frequentist looks at
    each measurement (here whether the coin is heads or tails) and considers it to
    be just one little approximation to the truth. By combining all these approximations
    (here with just a running average) we can converge on the single “true” value
    giving the bias of the coin.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian Coin Flipping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s consider how we might estimate a coin’s bias using a Bayesian point of
    view. To do so, let’s use a slightly more complicated situation that highlights
    how a Bayesian asks and answers questions.
  prefs: []
  type: TYPE_NORMAL
- en: A Motivating Example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s suppose that we have a friend who’s a deep-sea marine archaeologist. Her
    latest discovery is an ancient shipwreck that has, among its treasures, a box
    containing a marked board and a bag of two identical-looking coins. She thinks
    these were used for a betting game, and with her colleagues, she’s even reconstructed
    some of the rules.
  prefs: []
  type: TYPE_NORMAL
- en: The key element is that only one of the two seemingly identical coins is fair.
    The other coin is rigged and will come up heads two-thirds of the time (that is,
    it has a bias of 2/3). The difference between a bias of 1/2 and a bias of 2/3
    isn’t big, but it’s enough to build a game around. The rigged coin has been cleverly
    made so that we can’t tell which coin is which by looking at them, or even by
    picking them up and casually feeling them. The game involves players flipping
    these coins and trying to figure out which coin is which, along with various forms
    of bluffing and betting along the way. When the game is over, the players figure
    out which coin is rigged and which is fair by spinning both coins on their edges.
    Because of its uneven weighting, the biased coin drops sooner than the fair coin.
  prefs: []
  type: TYPE_NORMAL
- en: Our archaeologist friend wants to explore the game further, but she needs to
    know the true identities of the coins. She asks us to help sort it out. She gives
    us two envelopes, marked Fair and Rigged, and our job is to put each coin in the
    appropriate envelope. We could use the spinning test to work out which coin is
    which, but let’s do it with probabilities instead, so we can get some experience
    with thinking this way.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by picking a coin, flipping it once, seeing if it comes up heads
    or tails, and then finding out what we can do with that information. The first
    step is to select a coin. Since we can’t tell the two coins apart by looking at
    them, we have a 50 percent chance of picking up the fair coin, and the same chance
    for selecting the rigged coin. This choice sets up our big question: *Did we choose
    the fair coin?* Once we know which coin we’ve got, we can put it in its corresponding
    envelope and put the other coin in the other envelope. Let’s rephrase our question
    in terms of probabilities: *What is the probability that we picked the fair coin?*
    If we can be sure we have the fair coin, or be sure that we don’t, we’ll know
    everything we need to know.'
  prefs: []
  type: TYPE_NORMAL
- en: So, we’ve got a question and we’ve got a coin. Let’s flip. Heads! The great
    thing about reasoning with probabilities is that with just this one flip, we can
    already make a valid, quantified statement about which coin we have.
  prefs: []
  type: TYPE_NORMAL
- en: Picturing the Coin Probabilities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To draw the various probabilities involved in the coming discussion, let’s bring
    back our probability diagrams from Chapter 3\. Let’s imagine a square wall at
    which we’ll throw darts, and that every dart has an equal probability of landing
    on every point on the wall. We can paint regions of the wall in different colors
    that correspond to different outcomes. For example, if one outcome has a 75 percent
    chance of happening, and the other has a 25 percent chance, we can paint three-fourths
    of the wall blue, and the remaining quarter pink. If we throw 100 darts, we’d
    expect about 75 of them to land in the blue region, and the rest to land in the
    pink region.
  prefs: []
  type: TYPE_NORMAL
- en: Our first step was to choose a coin. Since we can’t tell one coin from the other,
    the odds of choosing the fair coin are 50:50\. To represent this, we can imagine
    splitting the wall into two regions of equal size. Let’s paint the fair region
    with flax (a kind of beige), and the rigged region with red, as in [Figure 4-2](#figure4-2).
    When we throw a dart at the wall, the odds of it landing in the fair region are
    50:50, corresponding to selecting the fair coin.
  prefs: []
  type: TYPE_NORMAL
- en: '![F04002](Images/F04002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4-2: When we pick one of the two coins at random, it’s the same as throwing
    a dart at a wall that’s been painted with two equal areas, one for the fair coin
    and one for the rigged coin.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s paint the wall in a more informative way that tells us how likely we are
    to get heads or tails from our first flip. We know that the fair coin has a 50:50
    chance of being heads or tails, so we can split the fair region into two equal
    pieces, one each for heads and tails, as in [Figure 4-3](#figure4-3).
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 4-3](#figure4-3) we also split the rigged side. Since we know from
    our friend that the rigged coin has a two-thirds chance of coming up heads, we
    assigned two-thirds of its area to heads and one-third to tails. [Figure 4-3](#figure4-3)
    summarizes everything we know about our system. It tells us the likelihood of
    picking either coin (corresponding to landing in the yellow or red zones), and
    the likelihood of getting heads or tails in each situation (from the relative
    sizes of the heads and tails regions).
  prefs: []
  type: TYPE_NORMAL
- en: '![F04003](Images/F04003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4-3: We can split up the fair and rigged regions of the wall into heads
    and tails for each coin using the information we already know about how they are
    likely to come up when flipped.'
  prefs: []
  type: TYPE_NORMAL
- en: If we throw a dart at a wall painted like [Figure 4-3](#figure4-3), our dart
    will land in a region corresponding to one of the coins and either heads or tails.
    But since we’ve already flipped the coin and observed heads, we know we landed
    in either Fair heads or Rigged heads.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember our question: What is the probability that we picked the fair coin?
    We can tighten that up by using the information that we got back heads. We’ll
    see later that the best way to phrase our question is in the form of a template
    that asks, “What is the probability that (something1) is true, given that (something2)
    is true?” In this case that becomes, “What is the probability that we have the
    fair coin, given that we saw heads?”'
  prefs: []
  type: TYPE_NORMAL
- en: We can diagram this in pictures. It’s the area of the Fair heads region compared
    to the total area that could have given us heads, which is the sum of Fair heads
    and Rigged heads. [Figure 4-4](#figure4-4) shows this ratio.
  prefs: []
  type: TYPE_NORMAL
- en: '![F04004](Images/F04004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4-4: If the coin comes up heads, how likely is it that we had the fair
    coin? It’s the size of the region where a fair coin gives us heads divided by
    all the areas combined that would give us heads.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s think about this picture for a moment. Since the Rigged heads area is
    larger than the Fair heads area, that makes it more likely that our result of
    “heads” came from landing in the rigged zone. In other words, now that we’ve seen
    that our coin came up heads, it’s a little more likely that it’s the rigged coin,
    just as a dart thrown at a wall painted as in [Figure 4-3](#figure4-3) is more
    likely to land in the Rigged heads region than the Fair heads region.
  prefs: []
  type: TYPE_NORMAL
- en: Later on, we’re going to talk about “the ways something can happen,” or “all
    of the ways that something can come about.” This means if we’re looking for some
    property to be true, we account for all of the possible events that give us that
    result. In this case, the bottom half of [Figure 4-4](#figure4-4) is the sum of
    all the ways we can get heads. In other words, we can receive heads from either
    the fair coin or from the rigged coin, so representing “all the ways to get heads”
    means combining these two possibilities.
  prefs: []
  type: TYPE_NORMAL
- en: Expressing Coin Flips as Probabilities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s rephrase [Figure 4-4](#figure4-4) using probability terms. The probability
    of getting the fair coin *and* getting heads is P(H,F) (or equivalently P(F,H)).
    The probability of getting the rigged coin *and* getting heads is P(H,R).
  prefs: []
  type: TYPE_NORMAL
- en: Now we can interpret the ratio of areas in [Figure 4-4](#figure4-4) as a probability
    statement. That diagram shows us the chance that our coin, which we know came
    up heads, is the fair coin. That’s P(F|H), which stands for “the probability that
    we have the fair coin given that we observed heads.” That is, this conditional
    probability is the answer to our question.
  prefs: []
  type: TYPE_NORMAL
- en: We can put this all together into [Figure 4-5](#figure4-5).
  prefs: []
  type: TYPE_NORMAL
- en: '![F04005](Images/F04005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4-5: Translating [Figure 4-4](#figure4-4) into the language of probability'
  prefs: []
  type: TYPE_NORMAL
- en: Can we plug numbers into this diagram and come up with an actual probability?
    Sure, in this case we can, because the situation was contrived to be simple. But
    in general, we won’t know any of these joint probabilities, and they won’t be
    easy to find out.
  prefs: []
  type: TYPE_NORMAL
- en: Not to worry. All the boxes on the right of [Figure 4-5](#figure4-5) are joint
    probabilities, and we saw in Chapter 3 that we can write any joint probability
    in two different and equivalent ways, each involving a simple probability and
    a conditional probability. Those terms are usually much easier for us to put numbers
    to. Those two approaches are repeated here as [Figure 4-6](#figure4-6) and [Figure
    4-7](#figure4-7).
  prefs: []
  type: TYPE_NORMAL
- en: '![F04006](Images/F04006.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4-6: We can write the joint probability of two events A and B as the
    conditional probability P(A|B) times the probability of B, given by P(B).'
  prefs: []
  type: TYPE_NORMAL
- en: '![F04007](Images/F04007.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4-7: We can find the joint probability of two events A and B as the
    conditional probability P(B|A) times the probability of A given by P(A).'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s write our expression for P(F|H) in [Figure 4-5](#figure4-5) without the
    colored boxes, and then replace P(H,F) with the expression in [Figure 4-7](#figure4-7),
    which tells us that we can find the joint probability of P(H,F), or landing in
    heads *and* using the fair coin, by multiplying the chance of getting heads from
    a fair coin, P(H|F), with the chance of having the fair coin in the first place,
    P(F). This change is shown [Figure 4-8](#figure4-8).
  prefs: []
  type: TYPE_NORMAL
- en: '![F04008](Images/F04008.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4-8: Our ratio of [Figure 4-5](#figure4-5) represents P(F|H), the chance
    that we have the fair coin given that it came up heads.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s do the same thing for the two other joint probabilities, replacing them
    by their expanded versions (the first of the two values is just P(H,F) again).
    [Figure 4-9](#figure4-9) shows the result.
  prefs: []
  type: TYPE_NORMAL
- en: '![F04009](Images/F04009.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4-9: We can replace the other two joint probabilities in [Figure 4-8](#figure4-8)
    with their expanded versions as well.'
  prefs: []
  type: TYPE_NORMAL
- en: Since we can generally find numbers for all of the symbolic expressions in this
    expanded version, this is a useful way to find P(F|H).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use this expression to find the chance that we just flipped the fair coin.
    We need to assign a number to each term in [Figure 4-9](#figure4-9). P(F) is the
    probability that we picked the fair coin when we started. We’ve already seen that’s
    P(F)=1/2\. P(R) is the probability that we picked the rigged coin when we started,
    which is also 1/2\. P(H|F) is the probability of getting heads given that we chose
    the fair coin. By definition, that’s 1/2\. P(H|R) is the probability of getting
    heads from the rigged coin. From what our archaeologist friend told us, that’s
    2/3.
  prefs: []
  type: TYPE_NORMAL
- en: So now we have all the numbers we need to work out the probability that we have
    the fair coin, given that we just flipped it and got heads. [Figure 4-10](#figure4-10)
    shows plugging in the numbers and cranking through the steps (following math convention,
    we perform multiplications before additions—this lets us leave out some distracting
    parentheses).
  prefs: []
  type: TYPE_NORMAL
- en: '![F04010](Images/F04010.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4-10: Finding the probability that we picked the fair coin, given that
    we just saw heads'
  prefs: []
  type: TYPE_NORMAL
- en: The result is 3/7 or about 0.43\. This is kind of remarkable. It tells us that
    after *just one flip of the coin*, we can already say in a principled way that
    there’s only a 43 percent chance we have the fair coin, and therefore a 57 percent
    chance that we have the rigged coin. That’s a 14 percent difference, from one
    flip!
  prefs: []
  type: TYPE_NORMAL
- en: As an aside, consider that a frequentist wouldn’t dare to characterize the coin
    as fair or not after just one flip, while this Bayesian approach is already describing
    the coin with specific probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Returning to our first flip, let’s suppose we received tails instead. Now we’d
    like to find P(F|T), or the probability that we have the fair coin, given that
    we saw it come up tails. Recall that the bias is the probability of coming up
    heads, so the probability of coming up tails is 1 – bias. For the fair coin, the
    chance of it coming up tails, or P(T|F), is (1 – (1/2)) = 1/2\. For the rigged
    coin, we know from our friend that the bias is 2/3, so P(T|R) is (1 – (2/3)) =
    1/3\. The chances of picking the fair and rigged coins, given by P(F) and P(R),
    are each 1/2, just as before. Let’s plug those values in and find P(F|T), the
    probability that we picked the fair coin given that it came up tails. [Figure
    4-11](#figure4-11) shows the steps. The expression for P(F|T) is like the one
    for P(F|H) with the H’s and T’s reversed.
  prefs: []
  type: TYPE_NORMAL
- en: This is an even more dramatic answer, telling us it’s 60 percent likely that
    this result of tails means that we’re flipping the fair coin (and thus it’s 40
    percent likely that we’re flipping the rigged coin). That’s a huge boost of confidence
    from just one flip! Note that the results aren’t symmetrical. If we get heads,
    we have a 43 percent probability of a fair coin, but if we get tails, we have
    a 60 percent probability of a fair coin.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve seen that we can get a lot of information from one flip, but even 60 percent
    is far from being certain. Making more flips gives us a chance to find more refined
    probabilities, and we’ll see how to do that later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '![F04011](Images/F04011.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4-11: What if we got tails on our coin flip?'
  prefs: []
  type: TYPE_NORMAL
- en: Bayes’ Rule
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s find another way to write P(F|H). In [Figure 4-5](#figure4-5), [Figure
    4-8](#figure4-8), and [Figure 4-9](#figure4-9), we saw several different ways
    to write the probability that we picked the fair coin given that we saw heads.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go back to the version in [Figure 4-8](#figure4-8) (repeated at the top
    of [Figure 4-12](#figure4-12)). Note that the bottom part of the ratio, P(H,F)
    + P(H,R), combines the probabilities for all the possible ways we could have gotten
    heads (after all, it has to come from either the fair or rigged coin). If we were
    dealing with, say, 20 coins, then we’d have to write a sum of 20 joint probabilities,
    which would make for a very messy expression. We usually use a shortcut, and write
    these combined probabilities as simply P(H), or “the probability of getting heads.”
    This implicitly means the sum of all the ways we could have gotten heads. If we
    had 20 different coins, this would be the sum of the probability of each of those
    coins giving us heads. [Figure 4-12](#figure4-12) shows this abbreviated notation.
  prefs: []
  type: TYPE_NORMAL
- en: '![F04012](Images/F04012.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4-12: The last line of [Figure 4-8](#figure4-8), but we’ve replaced
    the bottom part of the ratio with the symbol P(H)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 4-13](#figure4-13) shows this most recent version all by itself. This
    is the famous *Bayes’ Rule* or *Bayes’ Theorem* that we mentioned earlier in the
    chapter. Here we’ve adopted the mathematician’s convention that two values placed
    side by side should be multiplied.'
  prefs: []
  type: TYPE_NORMAL
- en: '![F04013](Images/F04013.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4-13: Bayes’ Rule, or Bayes’ Theorem, as it’s usually written'
  prefs: []
  type: TYPE_NORMAL
- en: Expressing this in words, we want to find P(F|H), the probability that we have
    a fair coin given that we just flipped it and saw heads. To determine that, we
    combine three pieces of information. First, P(H|F), or the probability that, if
    we do indeed have a fair coin, it will come up heads. We multiply that by P(F),
    the probability that we have a fair coin. As we’ve seen, this multiplication is
    just a more convenient way to evaluate P(H,F), or the probability that our coin
    is fair *and* that it came up heads. Lastly, we divide everything by P(H), or
    the probability that our coin will come up heads, *taking into account both the
    fair and rigged coins*. This is how likely we will be to get heads using *either*
    of these coins.
  prefs: []
  type: TYPE_NORMAL
- en: Bayes’ Theorem is usually written in the form of [Figure 4-13](#figure4-13),
    because it breaks things down into pieces that we can conveniently measure (the
    letters are often changed to better suit what’s being discussed). We just replace
    each term by its corresponding value and out pops the conditional probability
    that, given heads, we picked the fair coin. Remember that P(H) stands for the
    sum of the joint probabilities, as we saw in [Figure 4-12](#figure4-12).
  prefs: []
  type: TYPE_NORMAL
- en: 'This is why the questions we ask of Bayes’ Rule need to be in the form of a
    conditional probability: *What is the probability that (something1) is true, given
    that (something2) is true?* It’s because that’s what Bayes’ Rule provides us with.If
    we can’t express our problem in that form, then Bayes’ Rule isn’t the right tool
    for answering it.'
  prefs: []
  type: TYPE_NORMAL
- en: Discussion of Bayes’ Rule
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Bayes’ Rule can be hard to remember because there are lots of letters floating
    around, and each one has to go in the right place. But the nice thing is that
    we can quickly re-derive the rule perfectly any time we need it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s write the joint probability of F and H in both forms (that is, P(F,H)
    and P(H,F)). We know that these are both the same thing: the probability of having
    a fair coin and getting heads. Replacing them with the expanded versions as we
    did in [Figure 4-8](#figure4-8) gives us the second line of [Figure 4-14](#figure4-14).'
  prefs: []
  type: TYPE_NORMAL
- en: To get Bayes’ Rule, just divide each side by P(H), as shown in the third line.
    The result is the last line, which is Bayes’ Rule. This can be a handy way to
    re-create the rule if we need it and have forgotten it.
  prefs: []
  type: TYPE_NORMAL
- en: '![F04014](Images/F04014.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4-14: How to rediscover Bayes’ Rule if we forget, or a quick demonstration
    of why it’s true'
  prefs: []
  type: TYPE_NORMAL
- en: Each of the four terms in Bayes’ Rule has a conventional name, summarized in
    [Figure 4-15](#figure4-15).
  prefs: []
  type: TYPE_NORMAL
- en: '![F04015](Images/F04015.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4-15: The four terms in Bayes’ Rule, with their names'
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 4-15](#figure4-15) we used the traditional letters A and B, which
    stand for any kinds of events and observations. With these letters, P(A) is our
    initial estimate for whether or not we have the fair coin. Because it’s the probability
    we use for “we chose the fair coin” before, or prior to, flipping the coin, we
    call P(A) the *prior probability*, or just the *prior*.
  prefs: []
  type: TYPE_NORMAL
- en: P(B) tells us the probability of getting the result we did, which in this case
    was that the coin came up heads. We call P(B) the *evidence*. This word can be
    misleading, since sometimes this word refers to something like a fingerprint at
    a crime scene. In our context, *evidence* is the probability that event B could
    have come about *by any means at all*. Remember that the evidence is the sum of
    the probabilities for every coin we might have chosen to come up heads.
  prefs: []
  type: TYPE_NORMAL
- en: The conditional probability P(B|A) tells us the likelihoodof getting heads,
    assuming we have a fair coin. We call P(B|A) the *likelihood*.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the result of Bayes’ Rule tells us the probability that we picked the
    fair coin, given the observation of heads. Because P(A|B) is what we get at the
    end of the calculation, it’s called the *posterior probability*, or just the *posterior*.
  prefs: []
  type: TYPE_NORMAL
- en: Earlier in this chapter we said that a virtue of the Bayesian approach is that
    it lets us explicitly identify our preconceptions and expectations. Now we can
    see that we do that by choosing our prior, P(A). In general, we know the likelihood
    P(B|A) and evidence P(B) from our experimental setup, but we’ll have to guess
    at the prior P(A). This can be a problem if we run the experiment only once, because
    if our estimate of the prior is wrong, the posterior will also be wrong. We’ll
    see later that if we can run an experiment multiple times (such as by flipping
    the coin more than once), then we can use Bayes’ Rule after each flip to refine
    our initial prior into a better and better description of P(A), giving us a more
    accurate value for what we really care about, the posterior P(B|A).
  prefs: []
  type: TYPE_NORMAL
- en: We came up with a value for the prior pretty easily in our little coin-testing
    example, but in more complicated situations, choosing a good prior can be more
    complicated. Sometimes it comes down to a combination of experience, data, knowledge,
    and even just hunches about what the prior should be. Because there’s some subjective,
    or personal, aspect to our choice, picking a prior by ourselves is called *subjective
    Bayes*. On the other hand, sometimes we can use a rule or an algorithm to pick
    the prior for us. If we do so, that’s called *automatic Bayes* (Genovese 2004).
  prefs: []
  type: TYPE_NORMAL
- en: Bayes’ Rule and Confusion Matrices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Chapter 3 we looked at using the confusion matrix to help us properly understand
    the outcomes of a test. Let’s look at this idea again, but this time using Bayes’
    Rule.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rather than create some artificial, contrived example, let’s use something
    realistic and everyday. You’re the Captain of the Starship *Theseus*, on a mission
    into deep space to find rocky, uninhabited planets to mine for raw materials.
    You’ve just come across a promising rocky planet. It would be great to start mining
    it, but your orders are to never, ever mine a planet that has life on it. So the
    big question is this: Is there life on this planet?'
  prefs: []
  type: TYPE_NORMAL
- en: In your experience, most of the life on these rocky worlds is just a little
    bacteria or bit of fungus, but life is life. As protocol dictates, you send down
    a probe to investigate. The probe lands and reports “no life.”
  prefs: []
  type: TYPE_NORMAL
- en: Because no probe is perfect, we must now ask the question, “What is the probability
    that the planet contains life, given that the probe detected nothing?” This question
    is in perfect form for using Bayes’ Rule. One condition (let’s call it L) is “life
    is present,” where a positive value means the planet has life on it, and a negative
    value means the planet doesn’t have life (so we can start mining). The other condition
    (we’ll call it D) is “detected life,” where positive means the probe detected
    life, and negative means it didn’t.
  prefs: []
  type: TYPE_NORMAL
- en: 'The situation we really want to avoid is mining on a planet that has life.
    That’s a false negative: the probe reported negative, but it shouldn’t have. This
    would be terrible, since we don’t want to interfere with, much less destroy, life
    in any form. False positives are less worrisome. Those are planets that are barren,
    but the probe thought it found signs of life. The only drawback there is that
    we fail to mine a planet we otherwise could have. There’s a financial loss, but
    that’s all.'
  prefs: []
  type: TYPE_NORMAL
- en: The scientists who built our probes shared these same concerns, so they struggled
    hard to minimize the false negatives. They tried to keep down the false positives,
    too, but that wasn’t as critical.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, some planets with life might not have life everywhere, so it’s
    possible that a probe could land in a life-free region of a populated planet,
    and detect nothing. For simplicity, let’s not worry about such situations, and
    say that any incorrect results (that is, missing life that’s there, or saying
    life is present when it’s not) will be due to the probe, and not the planet.
  prefs: []
  type: TYPE_NORMAL
- en: 'The probes they sent us out with have the performance shown in [Figure 4-16](#figure4-16).
    To get these numbers, they sent their probes down onto 1,000 known planets of
    the type we’ll be wanting to mine, 101 of which were known to contain life. These
    values turn into our prior: out of every 1,000 planets, we expect life on 101
    of them.'
  prefs: []
  type: TYPE_NORMAL
- en: '![F04016](Images/F04016.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4-16: The performance of our probes'
  prefs: []
  type: TYPE_NORMAL
- en: The probe correctly reported that it found life (that is, a true positive) 100
    times out 1,000\. In other words, of the 101 planets with life, the probe missed
    life signs (a false negative) only once.
  prefs: []
  type: TYPE_NORMAL
- en: Out of the 899 empty planets, the probe correctly reported there was no life
    (a true negative) 869 times. Finally, it incorrectly reported finding life on
    a barren planet (a false positive) 30 times. All told, these aren’t bad numbers,
    since they’re skewed in favor of protecting life.
  prefs: []
  type: TYPE_NORMAL
- en: Using the letter D for “detected life” (the probe’s result), and the letter
    L for “life is present” (the reality on the ground), we can summarize these results
    in the confusion matrix of [Figure 4-17](#figure4-17). For the marginal probabilities,
    we write not-D for the probe result “not detected-life” (that is, the probe said
    there was no life), and not-L for “not life-is-present” (that is, there really
    is no life on the planet).
  prefs: []
  type: TYPE_NORMAL
- en: '![F04017](Images/F04017.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4-17: The confusion matrix that summarizes [Figure 4-16](#figure4-16),
    demonstrating the performance of our life-detecting probe. The four marginal probabilities
    are shown in the right and bottom margins.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 4-18](#figure4-18) gathers up the four marginal probabilities, plus
    two conditional probabilities that we’ll be using.'
  prefs: []
  type: TYPE_NORMAL
- en: '![F04018](Images/F04018.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4-18: Summaries of the four marginal probabilities and two conditional
    probabilities that we’ll be using based on the data in [Figure 4-17](#figure4-17)'
  prefs: []
  type: TYPE_NORMAL
- en: To find P(D|L), or the probability that the probe reported life given that there
    really *is* life, we found the number of times the probe found life (100) and
    divided it by the number of planets where life was found (101). That is, we found
    TP / (TP + FN), which we saw in Chapter 3 is called the *recall*. The value of
    100/101 is about 0.99.
  prefs: []
  type: TYPE_NORMAL
- en: To find P(not-D|L), we carried out the calculation the other way. It missed
    finding life once out of 101 planets. We found FN / (TP + FN), which we saw in
    Chapter 3 is called the *false negative rate*. It comes to 1/101, or about 0.01\.
    (For more insight into the probe’s behavior, we can also use the definitions in
    Chapter 3 to find the probe’s accuracyas 969/1000, which is 0.969, and its precision
    as 100/130, which is about 0.77.)
  prefs: []
  type: TYPE_NORMAL
- en: Now we can answer our original question. The probability that there actually
    is life, given that our probe says there isn’t, is P(L|not-D). Using Bayes’ Rule,
    we plug in the numbers from either the previous paragraph or [Figure 4-18](#figure4-18),
    giving us [Figure 4-19](#figure4-19).
  prefs: []
  type: TYPE_NORMAL
- en: '![F04019](Images/F04019.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4-19: Working out the probability that a planet has life, given that
    the probe reported that no life was detected'
  prefs: []
  type: TYPE_NORMAL
- en: This is reassuring. The probability that there’s life on that planet, given
    that our probe said there wasn’t, is about 1 in 1,000\. That’s a lot of confidence,
    but if want to be even more sure, we can send down more probes. We’ll see later
    how each successive probe can increase our confidence about whether there really
    is or isn’t any life down there.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s switch things up, and suppose that the probe instead came back with a
    positive report, telling us that it *did* detect life. That would be a financial
    loss for us, so we’d like to be sure. How confident can we be that there really
    is life on that planet? To find that, we just use Bayes’ Rule again, but this
    time we work out P(L|D), the probability of life given that the probe detected
    life. Let’s work though the numbers in [Figure 4-20](#figure4-20).
  prefs: []
  type: TYPE_NORMAL
- en: '![F04020](Images/F04020.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4-20: Working out the probability that a planet has life, given that
    the probe reported it found signs of life'
  prefs: []
  type: TYPE_NORMAL
- en: Wow. If the probe says it found life, we can be about 77 percent confident that
    there really is life, just from this one probe. This is nowhere near the level
    of confidence we got from the negative report, but that’s because the probe was
    designed to have a greater chance of reporting a false positive than a false negative.
    Since we always want to err on the side of protecting life, these are good results
    overall.
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned earlier, we can send more probes to increase our confidence
    in either result, but we’ll never get to absolute certainty either way. At some
    point, either on probe 1 or probe 10 or probe 10,000, we’ll need to make a judgment
    call about whether to mine the planet or not.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now see how sending more probes can help us increase our confidence.
  prefs: []
  type: TYPE_NORMAL
- en: Repeating Bayes’ Rule
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the preceding sections we saw how to use Bayes’ Rule to answer a question
    of the form *“What is the probability that (something1) is true, given that (something2)
    is true?”* We approached this question as a one-time event, plugging in what we
    know about the system and getting back a probability.
  prefs: []
  type: TYPE_NORMAL
- en: One event, or measurement, is not much to go on. Let’s return to our coin game
    with the two coins. Recall that one coin is fair, and one is rigged to come up
    heads more than half the time. We chose one of the two coins at random, flipped
    it, found that it came up heads, and we produced a probability that we had the
    fair coin. And that was the end of it.
  prefs: []
  type: TYPE_NORMAL
- en: But we can keep going. In this section let’s put Bayes’ Rule in the heart of
    a loop, where each new piece of data gives us a new posterior that we then use
    as the prior for the next observation. Over time, if the data is consistent, the
    prior should home in on the underlying probabilities we’re looking for.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s the basic intuition, before we get into the details. We usually know
    the likelihood, P(B|A), and the evidence, P(B), from our experimental setup, so
    those are settled. But we rarely know the prior, P(A). We need a value for this,
    so we think about the problem and take our best guess. Since this completes all
    the values needed by Bayes’ Rule, we can plug them in and get the posterior, P(A|B).
  prefs: []
  type: TYPE_NORMAL
- en: Now comes the interesting step. The posterior tells us the probability of A
    given that event B happened, but *we know event B happened*. Whether it’s a coin
    coming up heads, or a probe finding life on a planet, we know that B happened
    since we chose to compute P(A|B), rather than, say, the probability of A given
    that B did not happen. Since we know that B *has* happened, P(A|B) is just P(A).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s express this with another example to lock it down. Suppose event B is
    “the day is warm” and event A is “people are wearing sandals.” Suppose that it’s
    warm today. Then the probability that “people are wearing sandals, given that
    it’s warm” is equal to just “people are wearing sandals,” because we have already
    observed that it’s warm.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, the posterior, P(A|B), becomes P(A), which is our prior! That’s
    the key insight. When we know that B has happened, then the output of Bayes’ Rule
    gives us a new estimate of P(A). So, Bayes’ Rule gives us a way to change and
    improve our expectations, or beliefs, about the system based on what experiments
    tell us.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, we guess at P(A). Then we run an experiment. Critically, we then
    choose to compute either P(A|B) or P(A|not-B) based on whether we saw event B
    or not, as we saw in Figures 4-19 and 4-20\. This choice of which version of Bayes’
    Rule to evaluate is the magic that makes the whole loop work. Our choice of P(A|B)
    or P(A|not-B), which is determined by which outcome we actually observed, adds
    new information into our process. That new information helps us refine our understanding
    of the system we’re learning about. So, having made this choice, we plug the numbers
    into the appropriate form of Bayes’ Rule, and produce a posterior. That becomes
    the new prior. With this new P(A), we run another experiment, using Bayes’ Rule
    again, compute either P(A|B) or P(A|not-B) based on whether B happened or not,
    and update our expectations again by using that result as our new P(A), or prior,
    for the next experiment, and so on. Over time, our belief, or expectation, about
    the probability of A, or P(A), gets gradually refined from a guess to an experimentally
    supported value.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s package up this description into a loop, starting with a guess for the
    prior P(A) and then refining by running more experiments.
  prefs: []
  type: TYPE_NORMAL
- en: The Posterior-Prior Loop
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In [Figure 4-15](#figure4-15) we gave names to the terms of Bayes’ Rule. These
    aren’t the only names that are used. We also refer to the events in Bayes’ Rule
    (which we’ve been calling A and B) in terms of a hypothesis and an observation
    (sometimes abbreviated Hyp and Obs). Our *hypothesis* states something whose truth
    we want to discover (for example, “we have the fair coin”). The *observation*
    is the experimental result (for example, “we got heads”). [Figure 4-21](#figure4-21)
    shows Bayes’ Rule with these labels.
  prefs: []
  type: TYPE_NORMAL
- en: '![F04021](Images/F04021.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4-21: Writing Bayes’ Rule with descriptive labels for A and B'
  prefs: []
  type: TYPE_NORMAL
- en: In our coin-flipping example, our hypothesis is “We have selected the fair coin.”
    We ran an experiment and got an observation, which was “The coin came up heads.”
    We combine our prior P(Hyp) with the likelihood P(Obs|Hyp) of the observation
    given that hypothesis to get the joint probability of both the observation and
    hypothesis being true. We then scale that by the evidence P(Obs), or the probability
    that the observation could have come about by any means. The result is the posterior
    P(Hyp|Obs), which tells us the probability that our hypothesis is true, given
    the observation.
  prefs: []
  type: TYPE_NORMAL
- en: As promised, let’s now wrap this in a loop. We compute the posterior, and then
    (because we know that the Observation has happened) we can use that as the prior
    when we repeat the experiment. The result is a new posterior, which we can use
    as our prior the next time, and so on. Each time around the loop, our prior gets
    a little more accurate at describing the system, thanks to the inclusion of the
    results of each previous experiment.
  prefs: []
  type: TYPE_NORMAL
- en: A drawing of this loop is shown in [Figure 4-22](#figure4-22).
  prefs: []
  type: TYPE_NORMAL
- en: '![F04022](Images/F04022.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4-22: Each time we have a new observation, we combine the evidence,
    the likelihood of that observation, and the prior to compute a posterior. That
    posterior is then used as the prior when a new observation is evaluated.'
  prefs: []
  type: TYPE_NORMAL
- en: To recap, we start with a prior. This comes from analysis, experience, data,
    an algorithm, or just guesswork. Then we run an experiment (or make an observation)
    and start the loop. We combine the evidence, the likelihood of that observation,
    and the prior to select one of the two forms of Bayes’ Rule, with which we compute
    a posterior. This posterior becomes our new prior. Now, when another observation
    arrives, we enter the loop again, using our new prior.
  prefs: []
  type: TYPE_NORMAL
- en: The idea is that each time through the loop, our prior improves, from our initial
    guess toward a range of highly probable answers. The prior is improving because
    each time around the loop, the prior incorporates the latest observation, in addition
    to all the previous observations.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see this loop in action using our coin-flipping example.
  prefs: []
  type: TYPE_NORMAL
- en: The Bayes Loop in Action
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recall our archaeologist friend and her two-coin problem. Let’s generalize it
    so we can try out a number of variations and explore how to use the Bayes’ Rule
    loop shown in [Figure 4-22](#figure4-22) to answer questions.
  prefs: []
  type: TYPE_NORMAL
- en: Rather than having a single bag with a fair coin and a rigged coin, let’s suppose
    she found many such bags, where no two rigged coins had the same bias. Each bag
    is marked with the bias of its rigged coin (the bias is often written with the
    lowercase Greek θ [theta]).
  prefs: []
  type: TYPE_NORMAL
- en: She thinks that before players started a game, they’d agree on how biased they
    wanted the rigged coin to be. Then they’d pick the corresponding bag, and proceed
    as usual, picking out one of the two coins and then betting on which one had been
    picked.
  prefs: []
  type: TYPE_NORMAL
- en: Like them, we’ll first select a bag, and then a coin from the bag. Then we’ll
    determine the probability that we picked the fair coin. We can use the repeated
    form of Bayes’ Rule by flipping the coin many times, recording the heads and tails
    we get, and watching what Bayes’ Rule does with the observation, or result, of
    each flip when producing the posterior.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we make 30 flips. Even with so little data, we might see unusual events.
    For instance, we might have the fair coin and still get 25 heads and 5 tails.
    It’s very unlikely, but possible. It’s more likely that we’d get those results
    from a rigged coin with a high bias. Let’s see how Bayes’ Rule helps us decide
    which coin we have, based on multiple flips.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start by choosing the bag with a fair coin and a rigged coin with a bias
    of 0.2, which means we expect it to show 2 heads out of every 10 flips. Suppose
    we flip this coin 30 times, and only 20 percent (that is, 6) flips come up heads,
    so the other 24 flips come up tails. Do we have the fair coin or the rigged one?
    Since we’d expect 15 heads out of 30 flips from the fair coin, and 6 heads out
    of 30 flips from the rigged coin, getting 6 heads back seems like a good case
    for us having the rigged coin.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 4-23](#figure4-23) shows the result of Bayes’ Rule after each flip.
    As before, the probability that we have the fair coin is shown in flax (or beige),
    while the probability of the rigged coin is shown in red. The two probabilities
    always add up to 1.'
  prefs: []
  type: TYPE_NORMAL
- en: '![F04023](Images/F04023.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4-23: The probability that we have the fair coin is shown in flax (beige)
    through successive flips. The letter below each bar is the observation that produced
    that bar.'
  prefs: []
  type: TYPE_NORMAL
- en: To understand what this chart is telling us, look first at the letters along
    the bottom. These are either “H” or “T,” telling us the result of that flip. In
    this case, we have 24 tails, with 6 heads appearing here and there. Now consider
    the bars, starting at the left. The leftmost column shows which coin we think
    we have before we’ve flipped the coin at all, so the probabilities are both 0.5\.
    After all, the chances are equal that we picked either coin, and we haven’t flipped
    it to gain any data. The bar to the right of that shows the result of the posterior
    of Bayes’ Rule after observing tails (T) on the first flip. Since the chance of
    getting tails from the fair coin is 0.5, but the chance of tails from the rigged
    coin is 0.8, getting tails suggests that it’s more likely we have the rigged coin.
    Continuing to the right, about 80 percent of the flips are tails. That’s what
    we’d expect from the rigged coin, so its probability quickly approaches 1\. Note
    that the probability of our having the rigged coin dips about 2/3 of the way through
    the run when we get a bunch of heads close to one another, but then it goes back
    up with each new tail.
  prefs: []
  type: TYPE_NORMAL
- en: The height of the flax, or beige, block in each bar is the value of P(F), the
    probability that we selected the fair coin. After each flip, we use Bayes’ Rule
    to compute either P(F|H) or P(F|T) as appropriate. That becomes the new value
    of P(F), or our belief that we have the fair coin. We use that to compute the
    new posterior after the next flip. As we mentioned earlier, this choice is the
    key step that makes the whole loop work. After each experiment, we choose the
    version of Bayes’ Rule that returns P(F|H) or F(F|T) depending on what we observe.
    That choice is what enables us to use the posterior as a new prior, because it
    reflects what actually happened.
  prefs: []
  type: TYPE_NORMAL
- en: Toward the end, the probability that we have the fair coin is nearly 0\. It
    never gets to exactly 0, because we can never be absolutely sure that this isn’t
    a fair coin with a wildly unusual flip pattern, so that option is always has at
    least a shred of probability.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the data we got back pretty clearly revealed that we had the
    rigged coin. Let’s keep this coin and do another run. Suppose we get even fewer
    heads on the next run, perhaps only three in total, making the case for the rigged
    coin even stronger. Running this through the loop produces the results in [Figure
    4-24](#figure4-24).
  prefs: []
  type: TYPE_NORMAL
- en: '![F04024](Images/F04024.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4-24: We use the same coin with a bias of 0.2, but this time we happened
    to get only three heads in our 30 flips.'
  prefs: []
  type: TYPE_NORMAL
- en: We get to about 90 percent confidence that we’ve got the rigged coin after just
    four flips. After 30 flips, the probability of a fair coin is again almost, but
    never quite, 0.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we do another run of 30 flips, and this time we happen to get 24 heads.
    This doesn’t match either coin very well. We’d expect the fair coin to give us
    15 heads, but we’d expect only 6 heads from the rigged coin. Given just these
    two choices, the fair coin seems more likely. [Figure 4-25](#figure4-25) shows
    our results from Bayes’ Rule.
  prefs: []
  type: TYPE_NORMAL
- en: '![F04025](Images/F04025.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4-25: We use the same coin with a bias of 0.2 but this time we happened
    to get 24 heads in our 30 flips.'
  prefs: []
  type: TYPE_NORMAL
- en: Even though the fair coin should only come up heads about half the time, the
    rigged coin would come up heads only 20 percent of the time. All of those heads
    are unlikely from either coin, but they’re a lot *more* unlikely from the rigged
    coin, bolstering our confidence that we’ve got the fair coin.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve seen three different flipping results for this coin, from a pattern of
    almost all tails to a pattern of almost all heads. Let’s generalize these results
    by flipping 10 coins with different biases. Let’s create 10 different flip patterns
    for each coin, with different ratios of heads to tails. We can use Bayes’ Rule
    on each pattern for each coin, creating 100 scenarios. The results are in [Figure
    4-26](#figure4-26), where each cell is a little bar chart like those in [Figure
    4-23](#figure4-23) through [Figure 4-25](#figure4-25).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start in the bottom left corner. At this location our value on the horizontal
    axis (labeled “Rigged coin bias”) is around 0.05\. That means we’d expect this
    coin to come up heads about 1 time in 20\. Our value on the vertical axis (labeled
    “Flip sequence bias”) is also about 0.05\. This means we’re going to create an
    artificial sequence of observations, like we did earlier, where there’s a 1 in
    20 chance that each observation will be heads. In this case, the number of heads
    in the pattern of 30 observations we created for this cell in the grid matches
    the number of heads we’d expect from the rigged coin, so our confidence that the
    coin is rigged (in red) grows quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s move up three cells. Since we haven’t moved horizontally, our horizontal
    axis value is still 0.05, so we’re flipping a coin that should come up heads 1
    time in 20\. But now the vertical axis is about 0.35, so we’re looking at a pattern
    where heads are significantly more frequent. With all of those heads, it seems
    more likely that we’re getting an unusual series of flips from a fair coin, than
    a *very* unusual series of flips from the rigged coin. Our confidence that the
    coin is fair grows stronger as the number of flips increases.
  prefs: []
  type: TYPE_NORMAL
- en: '![F04026](Images/F04026.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4-26: Flipping a coin 30 times, and using repeated Bayes’ Rule applications
    to decide which coin we’re flipping. Each square reports our results for just
    one run of 30 random flips. Each row uses the same sequence of heads and tails.'
  prefs: []
  type: TYPE_NORMAL
- en: Each cell can be understood in the same way. We invent a pattern of 30 heads
    and tails, where the relative proportion of heads is given by the vertical location,
    and we ask whether that pattern is more likely to come from a fair coin, or a
    coin with a probability of heads given by its horizontal location.
  prefs: []
  type: TYPE_NORMAL
- en: In the middle of the grid, where both values are close to 0.5, it’s almost impossible
    to tell. The rigged coin comes up heads almost as often as the fair coin, and
    the patterns of heads and tails are about evenly split, so we could be flipping
    either coin. The probabilities for both stick to around 0.5\. But as we make patterns
    with fewer heads (the lower part of the figure) or more heads (the upper part),
    we can say how well that pattern matches a rigged coin with a low probability
    of coming up heads (the left side) or a high probability (the right side).
  prefs: []
  type: TYPE_NORMAL
- en: A series of 30 flips is revealing, but we can still get unusual surprises (like
    25 heads from a fair coin). If we increase the number of flips to 1,000 in each
    plot, as in [Figure 4-27](#figure4-27), such unusual sequences become rarer, and
    the patterns become clearer.
  prefs: []
  type: TYPE_NORMAL
- en: '![F04027](Images/F04027.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4-27: This chart has the same setup as [Figure 4-26](#figure4-26), but
    now we flip each coin 1,000 times.'
  prefs: []
  type: TYPE_NORMAL
- en: In the lower-left and upper right, the pattern of flips more closely matches
    the bias of the rigged coin than the fair coin, and Bayes’ Rule pushes the prior
    of “we selected the fair coin” toward 0\. In the upper left and lower right, the
    flips more closely match the fair coin, and the prior moves toward 1\.
  prefs: []
  type: TYPE_NORMAL
- en: The general lesson of [Figure 4-26](#figure4-26) and [Figure 4-27](#figure4-27)
    is that the more observations we make, the more certain we become that our hypothesis
    is either true or false. Each observation either increases or decreases our confidence.
    When the observations match up with our prior (“we have the fair coin”), our confidence
    in that prior grows. When the observations contradict that prior, our confidence
    decreases, and since there’s only one other alternative in this scenario (“we
    have the rigged coin”), that becomes more probable. Even when we have only a few
    observations, we can often gain a great deal of confidence early on.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple Hypotheses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve seen how to use Bayes’ Rule to improve a hypothesis by combining it with
    an observation, perhaps repeatedly. But there’s nothing limiting us to testing
    just a single hypothesis. We’ve been making multiple hypotheses all along, actually.
    In just the last section, we explicitly saw the two hypotheses “this coin is fair”
    and “this coin is rigged” being updated simultaneously. Since we knew the two
    probabilities had to add up to 1, knowing either one of them revealed the other,
    so we only had to keep track of one of them.
  prefs: []
  type: TYPE_NORMAL
- en: But we could explicitly calculate both probabilities if we wanted. We’d just
    use two copies of Bayes’ Rule. Suppose that a flip comes up heads. Then we can
    independently compute the conditional probability that we have a fair coin, P(F|H),
    and the conditional probability that we have a rigged coin, P(R|H). This is shown
    in [Figure 4-28](#figure4-28).
  prefs: []
  type: TYPE_NORMAL
- en: '![F04028](Images/F04028.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4-28: Calculating probabilities for two hypotheses. Line (c) explicitly
    shows how to compute P(H) in both cases.'
  prefs: []
  type: TYPE_NORMAL
- en: From [Figure 4-28](#figure4-28), we can see that the probabilities of the fair
    coin and the rigged coin are just their share of the two ways the coin can come
    up heads. If we want to track multiple hypotheses at once, we can use multiple
    copies of Bayes’ Rule in this way, updating them all after each new observation.
  prefs: []
  type: TYPE_NORMAL
- en: We can use this ability to help out our archaeologist friend again. She’s just
    found a chest with pieces for a new game, and again, the game uses bags to hold
    pairs of coins. Like before, the rigged coins in the bags have different biases.
    Because an extremely biased coin (that is, one that comes up heads much more often
    than tails, or vice versa) would be easier to spot, she thinks maybe there were
    levels of players, from newcomers to old hands. New players would play with coins
    that were highly biased, but as players became more skilled, they’d switch to
    rigged coins whose bias was closer and closer to 0.5\. Those would be harder to
    detect, leading to longer games with riskier and more complicated betting.
  prefs: []
  type: TYPE_NORMAL
- en: Since our friend wants to know all about her discovery, she emptied out all
    the coins into one big box, and has asked us to find the bias of each coin. For
    the moment, let’s suppose that there are only five possible bias values—0, 0.25,
    0.5, 0.75, and 1 (recall that a bias of 0.5 corresponds to a fair coin)—so we’ll
    create five hypotheses. We’ll number them 0 through 4, corresponding to the different
    bias values. Hypothesis 0 states, “This is the coin with bias 0,” Hypothesis 1
    states, “This is the coin with bias 0.25,” and so on, up to Hypothesis 4, which
    states, “This is the coin with bias 1.”
  prefs: []
  type: TYPE_NORMAL
- en: Now we’ll pick up a coin at random from the box, flip it repeatedly, and try
    to determine which of these hypotheses is most likely. To get started, we need
    to cook up a prior for each hypothesis. Remember that this is going to get updated
    every time through the loop, so we only need a good starting guess. Since we don’t
    know anything about the coin we’ve selected, let’s say the chance of having picked
    each one is the same, so all five prior values have a 1 in 5 chance of being right,
    or 1 / 5 = 0.2.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we could get fancier if we wanted. Each pair of coins has one that’s
    fair and one that’s rigged. Say we have 16 coins. If this is the case, then we
    have 8 fair coins, and 8 rigged coins (2 for each allowed value of the bias).
    The chance of picking a fair coin is then 8 / 16 = 0.5, whereas the chance of
    picking each rigged coin is 2 / 16 = 0.125\. This is probably a better prior,
    since it uses more of the information we already know. Starting out with a better
    prior means that our loop will home in on the high-probability solutions more
    quickly. But one of the beauties of the Bayesian approach is that we can start
    with almost any prior that’s even roughly close, and, ultimately, we get the same
    results. For simplicity, let’s use the first prior, where every hypothesis has
    a value of 0.2\.
  prefs: []
  type: TYPE_NORMAL
- en: The only thing left for us to specify is the likelihood for each coin. But we’ve
    already got those, because they *are* the bias. That is, if the coin has a bias
    of 0.2, then its likelihood of coming up heads is 0.2\. That means the likelihood
    of tails is 1 − 0.2 = 0.8.
  prefs: []
  type: TYPE_NORMAL
- en: Hypothesis 0, which says, “We have the coin with bias 0.0,” has a likelihood
    of getting heads of 0, and tails of 1\. Hypothesis 1, which says, “We have the
    coin with bias 0.2,” has a likelihood of getting heads of 0.2 (or 20 percent),
    and a likelihood of tails of 0.8 (or 80 percent). Our likelihoods are plotted
    in [Figure 4-29](#figure4-29).
  prefs: []
  type: TYPE_NORMAL
- en: Since the coins themselves don’t change as we flip them and gather observations,
    the likelihoods don’t change, either. We’ll reuse these same likelihoods over
    and over again, each time we evaluate Bayes’ Rule after getting a new observation.
  prefs: []
  type: TYPE_NORMAL
- en: '![F04029](Images/F04029.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4-29: The likelihoods for getting heads or tails for each of our five
    hypotheses'
  prefs: []
  type: TYPE_NORMAL
- en: Our goal is to flip our coin over and over and watch what happens to our five
    priors as they evolve. To show what’s happening at each flip, let’s draw the five
    prior values in red, and the five posterior values in blue. In [Figure 4-30](#figure4-30)
    we show the result of our first flip, which we’ll suppose came up heads. The five
    red bars, representing the prior for each of our five hypotheses, are all at 0.2\.
    Since the coin came up heads, we multiply each prior by its corresponding likelihood
    from the left side of [Figure 4-29](#figure4-29), which gives us our likelihoods.
    After dividing by the sum of all five probabilities for getting heads, we get
    the posterior, or the output of Bayes’ Rule, shown in blue.
  prefs: []
  type: TYPE_NORMAL
- en: '![F04030](Images/F04030.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4-30: We’re testing five hypotheses, which assert that our coin has
    a bias of 0, 0.25, 0.5, 0.75, and 1.0\. We start with a prior of 0.2 (in red)
    for each hypothesis. After one flip of the coin, which came up heads, we compute
    the posterior (in blue).'
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 4-30](#figure4-30), each pair of bars shows the prior and posterior
    value for a single hypothesis. Right away we’ve ruled out Hypothesis 0, because
    that says that the coin never comes up heads, and we just got heads. The hypothesis
    that says this coin always comes up heads is the strongest so far, because we
    just saw heads.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now make a series of flips. We’ll use a list of 100 flips that contain
    30 percent heads. That is, the flips correspond to the results we’d get from a
    coin with a bias of 0.3\. None of our five hypotheses matches this exactly, but
    Hypothesis 1 comes the closest, representing a coin with bias 0.25\. Let’s see
    how Bayes’ Rule performs. [Figure 4-31](#figure4-31) shows the results for the
    first 10 flips in the top two rows, and then takes bigger jumps in the bottom
    row.
  prefs: []
  type: TYPE_NORMAL
- en: '![F04031](Images/F04031.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4-31: The evolution of our priors (red) and posteriors (blue) in response
    to a series of flips generated by a coin with bias 0.3\. The number of the flip
    that was just evaluated is shown at the top of each graph.'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in the first two lines, the posterior computed after each flip
    (in blue) becomes the prior for the next flip (in red). We can also see that after
    the first flip (heads), Hypothesis 0 at the far left dropped to a likelihood of
    0, because that hypothesis was that we had a coin that would never come up heads.
    Then on the second flip, which happened to be tails, Hypothesis 4 went to 0, because
    that corresponded to a coin that always came up heads. That leaves just three
    hypotheses.
  prefs: []
  type: TYPE_NORMAL
- en: We can see how the three remaining options trade off the probabilities with
    each flip. As more flips come along, the number of heads comes closer to 30 percent,
    and Hypothesis 1 dominates. When we’ve reached 100 flips, the system has pretty
    much decided that Hypothesis 1 is the best one available, meaning that our coin
    is more likely to have a bias of 0.25 than any of the other choices.
  prefs: []
  type: TYPE_NORMAL
- en: If we can test 5 hypotheses, we can test 500\. [Figure 4-32](#figure4-32) shows
    500 hypotheses, each corresponding to a bias equally spaced from 0 to 1\. We’ve
    added a fourth row showing many more flips. We’ve eliminated the vertical bars
    in these charts so we can more clearly see the values of all 500 hypotheses.
  prefs: []
  type: TYPE_NORMAL
- en: '![F04032](Images/F04032.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4-32: The same situation as [Figure 4-31](#figure4-31), but now we’re
    evaluating 500 simultaneous hypotheses, each based on a coin with a slightly different
    bias'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this figure (as in the figures to come), we’re reusing the identical series
    of flip results that we used in [Figure 4-31](#figure4-31). And as we’d expect,
    the winning hypothesis is the one predicting a bias of about 0.3\. But another
    interesting thing is happening here: the posteriors are taking on the form of
    a Gaussian. Recall from Chapter 2 that a Gaussian curve is the famous bell curve
    that’s flat except for a symmetrical bump. This is a typical feature for the priors
    that evolve from the mathematics of Bayes’ Rule. It’s just another of the many
    places in statistics and probability where a Gaussian curve often emerges from
    the data.'
  prefs: []
  type: TYPE_NORMAL
- en: Notice that, as we said at the start of the chapter, Bayesian reasoning doesn’t
    zero in on one correct answer. Rather, it gradually assigns more probability to
    a smaller range of answers. The thinking is that any value in that range has its
    own probability of being the answer we seek.
  prefs: []
  type: TYPE_NORMAL
- en: If Bayes’ Rule seems to evolve so that the prior takes on the shape of a Gaussian,
    what would be the result if we *started* with priors that formed a Gaussian? Let’s
    do just that, but let’s make it even harder for the system by putting the mean
    of the prior’s bump (that is, its center) out at around 0.8\. That says that our
    belief is that the coin we’re testing is most likely to have a bias of 0.8\. This
    is far away from the value of 0.3 that we baked into our sequence of heads and
    tails. The probability at 0.3 that describes our coin starts out with a value
    of merely 0.004, so we’re asserting, through our prior, that the chance of this
    coin having a bias of 0.3 is 0.4 percent, or 4 out of 1,000\. How does the system
    respond to a prior that is so wrong, that the correct answer has only this very
    slim chance?
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 4-33](#figure4-33) shows the result.'
  prefs: []
  type: TYPE_NORMAL
- en: '![F04033](Images/F04033.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4-33: This figure has the same setup as [Figure 4-32](#figure4-32),
    only now we’re starting with a set of priors formed in a Gaussian bump centered
    at 0.8.'
  prefs: []
  type: TYPE_NORMAL
- en: Nice. Even with our poorly chosen prior, the system homed in on the proper bias
    of 0.3\. It took a while, but it got there.
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 4-34](#figure4-34) we show the priors of [Figure 4-33](#figure4-33)
    at 10 steps along the way in evaluating the first 3,000 flips, laid on top of
    one another rather than in sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that we start out with a broad prior with a mean of 0.8, but as we flip
    more and gather more observations, the prior’s mean moves toward 0.3\. The width
    of the bump also narrows, telling us the system is deciding that bias values far
    from the mean are less likely. The number of flips for each curve were chosen
    by hand so that the curves are spaced out roughly equally. Notice that as the
    system grows confident, by producing a narrow prior, the curve changes more slowly.
    In other words, the more certain the results, the more observations we need to
    make a big change to the posterior.
  prefs: []
  type: TYPE_NORMAL
- en: '![F04034](Images/F04034.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4-34: Some snapshots of the posteriors from the first 3,000 flips from
    [Figure 4-33](#figure4-33), overlaid on one another. The different colors show
    how many flips have elapsed, as given by the legend in the upper right. We can
    see the system giving more and more weight to the priors near 0.3, while reducing
    the probabilities elsewhere. The heights change in order to keep the area under
    each curve at 1.0.'
  prefs: []
  type: TYPE_NORMAL
- en: We won’t get into the details, but with some math, we can carry our increasing
    number of possible biases (and thus the number of hypotheses) to its logical extreme,
    replacing our lists of values with continuous curves, like those suggested by
    [Figure 4-34](#figure4-34). The advantage is that we can then get as precise as
    we like, finding a bias for any value rather than just the closest one in a list.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two broad camps in the field of probability: the frequentists and
    the Bayesians. The frequentist approach imagines that anything we choose to measure
    has an accurate, or true value. Each measurement is therefore only an approximation
    of that value. The Bayesian approach says that there is no single true value,
    only a range of possible values, each with its own probability. Each measurement
    is an accurate measure of something, but perhaps not what we want to measure.'
  prefs: []
  type: TYPE_NORMAL
- en: We spent most of this chapter working with the Bayesian approach. Bayesian probability
    is popular in deep learning, because it’s well suited to the nature of the kinds
    of problems we face and the kinds of questions we want to answer. The language
    of Bayesian probability is found in many of the papers, books, and documents of
    deep learning systems. At its core, it presents us with a set of tools for describing
    a measurement, not by looking for a single true number, but by finding a range
    of possible values for that measurement, each with its own probability.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if a deep learning system is helping someone write a text message
    by offering shortcuts for the next word, it usually shows several high-probability
    guesses, rather than a single best next word.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll look at some of the properties of curves and surfaces,
    which we’ll use to understand the types of errors our learning systems can make
    (and later, how to correct those errors).
  prefs: []
  type: TYPE_NORMAL
