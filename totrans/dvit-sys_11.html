<html><head></head><body>
<h2 class="h2" id="ch11"><span epub:type="pagebreak" id="page_543"/><span class="big">11</span><br/><strong>STORAGE AND THE MEMORY HIERARCHY</strong></h2>&#13;
<div class="imagec"><img alt="image" src="../images/common.jpg"/></div>&#13;
<p class="noindents">Although designing and implementing efficient algorithms is typically the <em>most</em> critical aspect of writing programs that perform well, there’s another, often overlooked factor that can have a major impact on performance: memory. Perhaps surprisingly, two algorithms with the same asymptotic performance (number of steps in the worst case) run on the same inputs might perform very differently in practice due to the organization of the hardware on which they execute. Such differences generally stem from the algorithms’ memory accesses, particularly where they store data and the kinds of patterns they exhibit when accessing it. These patterns are referred to as <em>memory locality</em>, and to achieve the best performance, a program’s access patterns need to align well with the hardware’s memory arrangement.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_544"/>For example, consider the following two variations of a function for averaging the values in an <em>N</em> × <em>N</em> matrix. Despite both versions accessing the same memory locations an equal number of times (<em>N</em><sup>2</sup>), Variation 1 executes about five times faster on real systems than Variation 2. The difference arises from the patterns in which they access those memory locations. Toward the end of this chapter we analyze this example using the memory profiling tool <em>Cachegrind</em>.</p>&#13;
<p class="margnote">Variation 1</p>&#13;
<p class="programs">float averageMat_v1(int **mat, int n) {<br/>&#13;
    int i, j, total = 0;<br/>&#13;
<br/>&#13;
    for (i = 0; i &lt; n; i++) {<br/>&#13;
        for (j = 0; j &lt; n; j++) {<br/>&#13;
            // Note indexing: [i][j]<br/>&#13;
            total += mat[i][j];<br/>&#13;
        }<br/>&#13;
    }<br/>&#13;
<br/>&#13;
    return (float) total / (n * n);<br/>&#13;
}</p>&#13;
<p class="margnote">Variation 2</p>&#13;
<p class="programs">float averageMat_v2(int **mat, int n) {<br/>&#13;
    int i, j, total = 0;<br/>&#13;
<br/>&#13;
    for (i = 0; i &lt; n; i++) {<br/>&#13;
        for (j = 0; j &lt; n; j++) {<br/>&#13;
            // Note indexing: [j][i]<br/>&#13;
            total += mat[j][i];<br/>&#13;
        }<br/>&#13;
    }<br/>&#13;
<br/>&#13;
    return (float) total / (n * n);<br/>&#13;
}</p>&#13;
<p class="indent">Storage locations like registers, CPU caches, main memory, and files on disk all feature remarkably different access times, transfer rates, and storage capacities. When programming a high-performance application, it’s important to consider where data is stored and how frequently the program accesses each device’s data. For example, accessing a slow disk once as the program starts is rarely a major concern. On the other hand, accessing the disk frequently will slow down the program considerably.</p>&#13;
<p class="indent">This chapter characterizes a diverse set of memory devices and describes how they’re organized in a modern PC. With that context, we’ll see how a collection of varied memory devices can be combined to exploit the locality found in a typical program’s memory access patterns.</p>&#13;
<h3 class="h3" id="lev1_86"><span epub:type="pagebreak" id="page_545"/>11.1 The Memory Hierarchy</h3>&#13;
<p class="noindent">As we explore modern computer storage, a common pattern emerges: devices with higher capacities offer lower performance. Said another way, systems use devices that are fast and devices that store a large amount of data, but no single device does both. This trade-off between performance and capacity is known as the <em>memory hierarchy</em>, and <a href="ch11.xhtml#ch11fig1">Figure 11-1</a> depicts the hierarchy visually.</p>&#13;
<p class="indent">Storage devices similarly trade cost and storage density: faster devices are more expensive, both in terms of bytes per dollar and operational costs (e.g., energy usage). Consider that even though caches provide great performance, the cost (and manufacturing challenges) of building a CPU with a large enough cache to forego main memory makes such a design infeasible. Practical systems must utilize a combination of devices to meet the performance and capacity requirements of programs, and a typical system today incorporates most, if not all, of the devices described in <a href="ch11.xhtml#ch11fig1">Figure 11-1</a>.</p>&#13;
<div class="imagec" id="ch11fig1"><img alt="image" src="../images/11fig01.jpg"/></div>&#13;
<p class="figcap"><em>Figure 11-1: The memory hierarchy</em></p>&#13;
<p class="indent">The reality of the memory hierarchy is unfortunate for programmers, who would prefer to not worry about the performance implications of where their data resides. For example, when declaring an integer <em>in most applications</em>, a programmer ideally wouldn’t need to agonize over the differences between data stored in a cache or main memory. Requiring a programmer to micromanage which type of memory each variable occupies would be burdensome, although it may occasionally be worth the effort for certain small, performance-critical sections of code.</p>&#13;
<p class="indent">Note that <a href="ch11.xhtml#ch11fig1">Figure 11-1</a> categorizes <em>cache</em> as single entity, but most systems contain multiple levels of caches that form their own smaller hierarchy. For example, CPUs commonly incorporate a very small and fast <em>level one</em> (L1) <span epub:type="pagebreak" id="page_546"/>cache, which sits relatively close to the ALU, and a larger and slower <em>level two</em> (L2) cache that resides farther away. Many multicore CPUs also share data between cores in a larger <em>level three</em> (L3) cache. Although the differences between the cache levels may matter to performance-conscious applications, this book considers just a single level of caching for simplicity.</p>&#13;
<p class="indent">Though this chapter primarily focuses on data movement between registers, CPU caches, and main memory, the next section characterizes common storage devices across the memory hierarchy. We examine disks and their role in the bigger picture of memory management later, in “Virtual Memory” on <a href="ch13.xhtml#lev1_101">page 639</a>.</p>&#13;
<h3 class="h3" id="lev1_87">11.2 Storage Devices</h3>&#13;
<p class="noindent">Systems designers classify devices in the memory hierarchy according to how programs access their data. <em>Primary storage</em> devices can be accessed directly by a program on the CPU. That is, the CPU’s assembly instructions encode the exact location of the data that the instructions should retrieve. Examples of primary storage include CPU registers and main memory (RAM), which assembly instructions reference directly (e.g., in IA32 assembly as <span class="literal">%reg</span> and <span class="literal">(%reg)</span>, respectively).</p>&#13;
<p class="indent">In contrast, CPU instructions cannot directly refer to <em>secondary storage</em> devices. To access the contents of a secondary storage device, a program must first request that the device copy its data into primary storage (typically memory). The most familiar types of secondary storage devices are disk devices (e.g., hard disk drives and solid-state drives), which persistently store file data. Other examples include floppy disks, magnetic tape cartridges, or even remote file servers.</p>&#13;
<p class="indent">Even though you may not have considered the distinction between primary and secondary storage in these terms before, it’s likely that you have encountered their differences in programs already. For example, after declaring and assigning ordinary variables (primary storage), a program can immediately use them in arithmetic operations. When working with file data (secondary storage), the program must read values from the file into memory variables before it can access them (see “File Input/Output” on <a href="ch02.xhtml#lev2_33">page 117</a>).</p>&#13;
<p class="indent">Several other important criteria for classifying memory devices arise from their performance and capacity characteristics. The three most interesting measures are:</p>&#13;
<div class="ul-none">&#13;
<p class="ul-noindent"><strong>Capacity</strong>  The amount of data a device can store. Capacity is typically measured in bytes.</p>&#13;
<p class="ul-noindent"><strong>Latency</strong>  The amount of time it takes for a device to respond with data after it has been instructed to perform a data retrieval operation. Latency is typically measured in either fractions of a second (e.g., milliseconds or nanoseconds) or CPU cycles.</p>&#13;
<p class="ul-noindent"><strong>Transfer rate</strong>  The amount of data that can be moved between the device and main memory over some interval of time. Transfer rate is also known as <em>throughput</em> and is typically measured in bytes per second.</p>&#13;
</div>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_547"/>Exploring the variety of devices in a modern computer reveals a huge disparity in device performance across all three of these measures. The performance variance primarily arises from two factors: <em>distance</em> and <em>variations in the technologies</em> used to implement the devices.</p>&#13;
<p class="indent">Distance contributes because, ultimately, any data that a program wants to use must be available to the CPU’s arithmetic components (e.g., the ALU) for processing. CPU designers place registers close to the ALU to minimize the time it takes for a signal to propagate between the two. Thus, while registers can store only a few bytes and there aren’t many of them, the values stored are available to the ALU almost immediately! In contrast, secondary storage devices like disks transfer data to memory through various controller devices that are connected by longer wires. The extra distance and intermediate processing slows down secondary storage considerably.</p>&#13;
<div class="g-box">&#13;
<p class="box-title">GRACE HOPPER’S “NANOSECONDS”</p>&#13;
<p class="noindentt">When speaking to an audience, computing pioneer and US Navy Admiral Grace Hopper frequently handed out 11.8-inch strands of wire to audience members. These strands represented the maximum distance that an electrical signal travels in one nanosecond and were called “Grace Hopper nano-seconds.” She used them to describe the latency limitations of satellite communication and to demonstrate why computing devices need to be small in order to be fast. Recordings of Grace Hopper presenting her nanoseconds are available on YouTube.<sup><a href="ch11.xhtml#fn11_1" id="rfn11_1">1</a></sup></p>&#13;
</div>&#13;
<p class="indent">The underlying technology also significantly affects device performance. Registers and caches are built from relatively simple circuits, consisting of just a few logic gates. Their small size and minimal complexity ensures that electrical signals can propagate through them quickly, reducing their latencies. On the opposite end of the spectrum, traditional hard disks contain spinning magnetic platters that store hundreds of gigabytes. Although they offer dense storage, their access latency is relatively high due to the requirements of mechanically aligning and rotating components into the correct positions.</p>&#13;
<p class="indent">The remainder of this section examines the details of primary and secondary storage devices and analyzes their performance characteristics.</p>&#13;
<h4 class="h4" id="lev2_187">11.2.1 Primary Storage</h4>&#13;
<p class="noindent">Primary storage devices consist of <em>random access memory</em> (RAM), which means the time it takes to access data is not affected by the data’s location in the device. That is, RAM doesn’t need to worry about things like moving parts into the correct position or rewinding tape spools. There are two widely used types of RAM, <em>static RAM</em> (SRAM) and <em>dynamic RAM</em> (DRAM), and both play an important role in modern computers. <a href="ch11.xhtml#ch11tab1">Table 11-1</a> characterizes the performance measures of common primary storage devices and the types of RAM they use.</p>&#13;
<p class="tabcap" id="ch11tab1"><span epub:type="pagebreak" id="page_548"/><strong>Table 11-1:</strong> Primary Storage Device Characteristics of a Typical 2022 Workstation</p>&#13;
<table class="line">&#13;
<colgroup>&#13;
<col style="width:20%"/>&#13;
<col style="width:30%"/>&#13;
<col style="width:30%"/>&#13;
<col style="width:20%"/>&#13;
</colgroup>&#13;
<tr class="borderb">&#13;
<td style="vertical-align: top"><p class="tab"><strong>Device</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>Capacity</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>Approx. Latency</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>RAM Type</strong></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">Register</p></td>&#13;
<td style="vertical-align: top"><p class="tab">4–8 bytes</p></td>&#13;
<td style="vertical-align: top"><p class="tab">&lt; 1 ns</p></td>&#13;
<td style="vertical-align: top"><p class="tab">SRAM</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">CPU cache</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1–32 megabytes</p></td>&#13;
<td style="vertical-align: top"><p class="tab">5 ns</p></td>&#13;
<td style="vertical-align: top"><p class="tab">SRAM</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">Main memory</p></td>&#13;
<td style="vertical-align: top"><p class="tab">4–64 gigabytes</p></td>&#13;
<td style="vertical-align: top"><p class="tab">100 ns</p></td>&#13;
<td style="vertical-align: top"><p class="tab">DRAM</p></td>&#13;
</tr>&#13;
</table>&#13;
<p class="indent">SRAM stores data in small electrical circuits (e.g., latches—see “RS Latch” on <a href="ch05.xhtml#lev3_49">page 257</a>). SRAM is typically the fastest type of memory, and designers integrate it directly into a CPU to build registers and caches. SRAM is relatively expensive in its cost to build, cost to operate (e.g., power consumption), and in the amount of space it occupies. Collectively, those costs limit the amount of SRAM storage that a CPU can include.</p>&#13;
<p class="indent">DRAM stores data using electrical components called <em>capacitors</em> that hold an electrical charge. It’s called “dynamic” because a DRAM system must frequently refresh the charge of its capacitors to maintain a stored value. Modern systems use DRAM to implement main memory on modules that connect to the CPU via a high-speed interconnect called the <em>memory bus</em>.</p>&#13;
<p class="indent"><a href="ch11.xhtml#ch11fig2">Figure 11-2</a> illustrates the positions of primary storage devices relative to the memory bus. To retrieve a value from memory, the CPU puts the address of the data it would like to retrieve on the memory bus and signals that the memory modules should perform a read. After a short delay, the memory module sends the value stored at the requested address across the bus to the CPU.</p>&#13;
<div class="imagec" id="ch11fig2"><img alt="image" src="../images/11fig02.jpg"/></div>&#13;
<p class="figcap"><em>Figure 11-2: Primary storage and memory bus architecture</em></p>&#13;
<p class="indent">Even though the CPU and main memory are physically just a few inches away from each other, data must travel through the memory bus when it moves between the CPU and main memory. The extra distance and circuitry between them increases the latency and reduces the transfer rate of main <span epub:type="pagebreak" id="page_549"/>memory relative to on-CPU storage. As a result, the memory bus is sometimes referred to as the <em>von Neumann bottleneck</em>. Of course, despite its lower performance, main memory remains an essential component because it stores several orders of magnitude more data than can fit on the CPU. Consistent with other forms of storage, there’s a clear trade-off between capacity and speed.</p>&#13;
<p class="indent"><em>CPU cache</em> (pronounced “cash”) occupies the middle ground between registers and main memory, both physically and in terms of its performance and capacity characteristics. A CPU cache typically stores a few kilobytes to megabytes of data directly on the CPU, but physically, caches are not quite as close to the ALU as registers. Thus, caches are faster to access than main memory, but they require a few more cycles than registers to make data available for computation.</p>&#13;
<p class="indent">Rather than the programmer explicitly loading values into the cache, control circuitry within the CPU automatically stores a subset of the main memory’s contents in the cache. CPUs strategically control which subset of main memory they store in caches so that as many memory requests as possible can be serviced by the (much higher performance) cache. Later sections of this chapter describe the design decisions that go into cache construction and the algorithms that should govern which data they store.</p>&#13;
<p class="indent">Real systems incorporate multiple levels of caches that behave like their own miniature version of the memory hierarchy. That is, a CPU might have a very small and fast <em>L1 cache</em> that stores a subset of a slightly larger and slower <em>L2 cache</em>, which in turns stores a subset of a larger and slower <em>L3 cache</em>. The remainder of this section describes a system with just a single cache, but the interaction between caches on a real system behaves much like the interaction between a single cache and main memory detailed later.</p>&#13;
<p class="note"><strong><span class="black">Note</span></strong></p>&#13;
<p class="note1">If you’re curious about the sizes of the caches and main memory on your system, the <span class="literal">lscpu</span> command prints information about the CPU (including its cache capacities). Running <span class="literal">free -m</span> shows the system’s main memory capacity in megabytes.</p>&#13;
<h4 class="h4" id="lev2_188">11.2.2 Secondary Storage</h4>&#13;
<p class="noindent">Physically, secondary storage devices connect to a system even farther away from the CPU than main memory. Compared to most other computer equipment, secondary storage devices have evolved dramatically over the years, and they continue to exhibit more diverse designs than other components. The iconic punch card<sup><a href="ch11.xhtml#fn11_2" id="rfn11_2">2</a></sup> allowed a human operator to store data by making small holes in a thick piece of paper, similar to an index card. Punch cards, whose design dates back to the US census of 1890, faithfully stored user data (often programs) through the 1960s and into the 1970s.</p>&#13;
<p class="indent">A tape drive<sup><a href="ch11.xhtml#fn11_3" id="rfn11_3">3</a></sup> stores data on a spool of magnetic tape. Although they generally offer good storage density (lots of information in a small size) for a low cost, tape drives are slow to access because they must wind the spool to the correct location. Although most computer users don’t encounter them <span epub:type="pagebreak" id="page_550"/>often anymore, tape drives are still frequently used for bulk storage operations (e.g., large data backups) in which reading the data back is expected to be rare. Modern tape drives arrange the magnetic tape spool into small cartridges for ease of use.</p>&#13;
<div class="imagec" id="ch11fig3"><img alt="image" src="../images/11fig03.jpg"/></div>&#13;
<p class="figcap"><em>Figure 11-3: Example photos of (a) a punch card, (b) a magnetic tape spool, and (c) a variety of floppy disk sizes. Images from Wikipedia.</em></p>&#13;
<p class="indent">Removable media like floppy disks<sup><a href="ch11.xhtml#fn11_4" id="rfn11_4">4</a></sup> and optical discs<sup><a href="ch11.xhtml#fn11_5" id="rfn11_5">5</a></sup> are another popular form of secondary storage. Floppy disks contain a spindle of magnetic recording media that rotates over a disk head that reads and writes its contents. <a href="ch11.xhtml#ch11fig3">Figure 11-3</a> shows photos of a punch card, a tape drive, and a floppy disk. Optical discs (e.g., CD, DVD, and Blu-ray) store information via small indentations on the disc. The drive reads a disc by shining a laser at it, and the presence or absence of indentations causes the beam to reflect (or not), encoding zeros and ones.</p>&#13;
<h5 class="h5" id="lev3_82">11.2.2.1 Modern Secondary Storage</h5>&#13;
<p class="tabcap" id="ch11tab2"><strong>Table 11-2:</strong> Secondary Storage Device Characteristics of a Typical 2022 Workstation</p>&#13;
<table class="line">&#13;
<colgroup>&#13;
<col style="width:20%"/>&#13;
<col style="width:30%"/>&#13;
<col style="width:20%"/>&#13;
<col style="width:30%"/>&#13;
</colgroup>&#13;
<tr class="borderb">&#13;
<td style="vertical-align: top"><p class="tab"><strong>Device</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>Capacity</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>Latency</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>Transfer Rate</strong></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">Flash disk</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.5–2 terabytes</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.1–1 ms</p></td>&#13;
<td style="vertical-align: top"><p class="tab">200–3,000 megabytes/second</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">Traditional hard disk</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.5–10 terabytes</p></td>&#13;
<td style="vertical-align: top"><p class="tab">5–10 ms</p></td>&#13;
<td style="vertical-align: top"><p class="tab">100–200 megabytes/second</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">Remote network server</p></td>&#13;
<td style="vertical-align: top"><p class="tab">Varies considerably</p></td>&#13;
<td style="vertical-align: top"><p class="tab">20–200 ms</p></td>&#13;
<td style="vertical-align: top"><p class="tab">Varies considerably</p></td>&#13;
</tr>&#13;
</table>&#13;
<p class="noindent"><a href="ch11.xhtml#ch11tab2">Table 11-2</a> characterizes the secondary storage devices commonly available to workstations today. <a href="ch11.xhtml#ch11fig4">Figure 11-4</a> displays how the path from secondary storage to main memory generally passes through several intermediate device controllers. For example, a typical hard disk connects to a Serial ATA controller, which connects to the system I/O controller, which in turn connects to the memory bus. These intermediate devices make disks easier to <span epub:type="pagebreak" id="page_551"/>use by abstracting the disk communication details from the OS and programmer. However, they also introduce transfer delays as data flows through the additional devices.</p>&#13;
<div class="imagec" id="ch11fig4"><img alt="image" src="../images/11fig04.jpg"/></div>&#13;
<p class="figcap"><em>Figure 11-4: Secondary storage and I/O bus architecture</em></p>&#13;
<p class="indent">The two most common secondary storage devices today are <em>hard disk drives</em> (HDDs) and flash-based <em>solid-state drives</em> (SSDs). A hard disk consists of a few flat, circular platters made from a material that allows for magnetic recording. The platters rotate quickly, typically at speeds between 5,000 and 15,000 revolutions per minute. As the platters spin, a small mechanical arm with a disk head at the tip moves across the platter to read or write data on concentric tracks (regions of the platter located at the same diameter).</p>&#13;
<p class="indent"><a href="ch11.xhtml#ch11fig5">Figure 11-5</a> illustrates the major components of a hard disk.<sup><a href="ch11.xhtml#fn11_6" id="rfn11_6">6</a></sup> Before accessing data, the disk must align the disk head with the track that contains the desired data. Alignment requires extending or retracting the arm until the head sits above the track. Moving the disk arm is called <em>seeking</em>, and because it requires mechanical motion, seeking introduces a small <em>seek time</em> delay to accessing data (a few milliseconds). When the arm is in the correct position, the disk must wait for the platter to rotate until the disk head is directly above the location that stores the desired data. This introduces another short delay (a few more milliseconds) known as <em>rotational latency</em>. Thus, due to their mechanical characteristics, hard disks exhibit significantly higher access latencies than the primary storage devices described earlier.</p>&#13;
<div class="imagec" id="ch11fig5"><span epub:type="pagebreak" id="page_552"/><img alt="image" src="../images/11fig05.jpg"/></div>&#13;
<p class="figcap"><em>Figure 11-5: The major components of a hard disk drive</em></p>&#13;
<p class="indent">In the past few years, SSDs, which have no moving parts (and thus lower latency), have quickly risen to prominence. They are known as solid-state drives because they don’t rely on mechanical movement. Although several solid-state technologies exist, flash memory<sup><a href="ch11.xhtml#fn11_7" id="rfn11_7">7</a></sup> reigns supreme in commercial SSD devices. The technical details of flash memory are beyond the scope of this book, but it suffices to say that flash-based devices allow for reading, writing, and erasing data at speeds faster than traditional hard disks. Though they don’t yet store data as densely as their mechanical counterparts, they’ve largely replaced spinning disks in most consumer devices like laptops.</p>&#13;
<h3 class="h3" id="lev1_88">11.3 Locality</h3>&#13;
<p class="noindent">Because memory devices vary considerably in their performance characteristics and storage capacities, modern systems integrate several forms of storage. Luckily, most programs exhibit common memory access patterns, known as <em>locality</em>, and designers build hardware that exploits good locality to automatically move data into an appropriate storage location. Specifically, a system improves performance by moving the subset of data that a program is actively using into storage that lives close to the CPU’s computation circuitry (e.g., in a register or CPU cache). As necessary data moves up the hierarchy toward the CPU, unused data moves farther away to slower storage until the program needs it.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_553"/>To a system designer, building a system that exploits locality represents an abstraction problem. The system provides an abstract view of memory devices such that it appears to programmers as if they have the sum of all memory capacities with the performance characteristics of fast on-chip storage. Of course, providing this rosy illusion to users can’t be accomplished perfectly, but by exploiting program locality, modern systems achieve good performance for most well-written programs.</p>&#13;
<p class="indent">Systems primarily exploit two forms of locality:</p>&#13;
<div class="ul-none">&#13;
<p class="ul-noindent"><strong>Temporal locality</strong>  Programs tend to access the same data repeatedly over time. That is, if a program has used a variable recently, it’s likely to use that variable again soon.</p>&#13;
<p class="ul-noindent"><strong>Spatial locality</strong>  Programs tend to access data that is nearby other, previously accessed data. “Nearby” here refers to the data’s memory address. For example, if a program accesses data at addresses <em>N</em> and <em>N</em> + 4, it’s likely to access <em>N</em> + 8 soon.</p>&#13;
</div>&#13;
<h4 class="h4" id="lev2_189">11.3.1 Locality Examples in Code</h4>&#13;
<p class="noindent">Fortunately, common programming patterns exhibit both forms of locality quite frequently. Take the following function, for example:</p>&#13;
<p class="programs">/* Sum up the elements in an integer array of length len. */<br/>&#13;
int sum_array(int *array, int len) {<br/>&#13;
    int i;<br/>&#13;
    int sum = 0;<br/>&#13;
<br/>&#13;
    for (i = 0; i &lt; len; i++) {<br/>&#13;
        sum += array[i];<br/>&#13;
    }<br/>&#13;
<br/>&#13;
    return sum;<br/>&#13;
}</p>&#13;
<p class="indent">In this code, the repetitive nature of the <span class="literal">for</span> loop introduces temporal locality for <span class="literal">i</span>, <span class="literal">len</span>, <span class="literal">sum</span>, and <span class="literal">array</span> (the base address of the array), as the program accesses each of these variables within every loop iteration. Exploiting this temporal locality allows a system to load each variable from main memory into the CPU cache only once. Every subsequent access can be serviced out of the significantly faster cache.</p>&#13;
<p class="indent">Accesses to the array’s contents also benefit from spatial locality. Even though the program accesses each array element only once, a modern system loads more than one <span class="literal">int</span> at a time from memory to the CPU cache. That is, accessing the first array index fills the cache with not only the first integer, but also the next few integers after it, too. Exactly <em>how many</em> additional integers get moved into the cache depends on the cache’s <em>block size</em>—the amount of data transferred into the cache at once.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_554"/>For example, with a 16-byte block size, a system copies four integers from memory to the cache at a time. Thus, accessing the first integer incurs the relatively high cost of accessing main memory, but the accesses to the next three are served out of cache, even if the program has never accessed them previously.</p>&#13;
<p class="indent">In many cases, a programmer can help a system by intentionally writing code that exhibits good locality patterns. For example, consider the nested loops that access every element of an <em>N</em> × <em>N</em> matrix (this same example appeared in this chapter’s introduction):</p>&#13;
<p class="margnote">Version 1</p>&#13;
<p class="programs">float averageMat_v1(int **mat, int n) {<br/>&#13;
    int i, j, total = 0;<br/>&#13;
<br/>&#13;
    for (i = 0; i &lt; n; i++) {<br/>&#13;
        for (j = 0; j &lt; n; j++) {<br/>&#13;
            // Note indexing: [i][j]<br/>&#13;
            total += mat[i][j];<br/>&#13;
        }<br/>&#13;
    }<br/>&#13;
<br/>&#13;
    return (float) total / (n * n);<br/>&#13;
}</p>&#13;
<p class="margnote">Version 2</p>&#13;
<p class="programs">float averageMat_v2(int **mat, int n) {<br/>&#13;
    int i, j, total = 0;<br/>&#13;
<br/>&#13;
    for (i = 0; i &lt; n; i++) {<br/>&#13;
        for (j = 0; j &lt; n; j++) {<br/>&#13;
            // Note indexing: [j][i]<br/>&#13;
            total += mat[j][i];<br/>&#13;
        }<br/>&#13;
    }<br/>&#13;
<br/>&#13;
    return (float) total / (n * n);<br/>&#13;
}</p>&#13;
<p class="indent">In both versions, the loop variables (<span class="literal">i</span> and <span class="literal">j</span>) and the accumulator variable (<span class="literal">total</span>) exhibit good temporal locality because the loops repeatedly use them in every iteration. Thus, when executing this code, a system would store those variables in fast on-CPU storage locations to provide good performance.</p>&#13;
<p class="indent">However, due to the row-major order organization of a matrix in memory (see “Two-Dimensional Array Memory Layout” on <a href="ch02.xhtml#lev3_13">page 86</a>), the first version of the code executes about five times faster than the second version. The disparity arises from the difference in spatial locality—the first version accesses the matrix’s values sequentially in memory (i.e., in order of consecutive memory addresses). Thus, it benefits from a system that loads <span epub:type="pagebreak" id="page_555"/>large blocks from memory into the cache because it pays the cost of going to memory only once for every block of values.</p>&#13;
<p class="indent">The second version accesses the matrix’s values by repeatedly jumping between rows across nonsequential memory addresses. It <em>never</em> reads from the same cache block in subsequent memory accesses, so it looks to the cache like the block isn’t needed. Thus, it pays the cost of going to memory for every matrix value it reads.</p>&#13;
<p class="indent">This example illustrates how a programmer can affect the system-level costs of a program’s execution. Keep these principles in mind when writing high-performance applications, particularly those that access arrays in a regular pattern.</p>&#13;
<h4 class="h4" id="lev2_190">11.3.2 From Locality to Caches</h4>&#13;
<p class="noindent">To help illustrate how the concepts of temporal and spatial locality enable cache designs, we’ll adopt an example scenario with familiar real-world objects: books. Suppose that Fiona does all of her homework at a desk in her dorm room, and the desk has a small amount of space that can store only three books. Just outside her room she keeps a bookshelf, which has much more space than the desk. Finally, across campus her college has a library with a huge variety of books. The “book storage hierarchy” in this example might look something like <a href="ch11.xhtml#ch11fig6">Figure 11-6</a>. Given this scenario, we’ll explore how locality can help guide which storage location Fiona should use to store her books.</p>&#13;
<div class="imagec" id="ch11fig6"><img alt="image" src="../images/11fig06.jpg"/></div>&#13;
<p class="figcap"><em>Figure 11-6: A hypothetical book storage hierarchy</em></p>&#13;
<h4 class="h4" id="lev2_191">11.3.3 Temporal Locality</h4>&#13;
<p class="noindent">Temporal locality suggests that, if there’s a book Fiona uses frequently, she should keep it as close to her desk as possible. If she occasionally needs to move it to the shelf to clear up temporary work space, the cost isn’t too high, but it would be silly to take a book back to the library if she’s just going to need it again the next day. The inverse is also true: if there’s a book taking up valuable space on her desk or shelf, and she hasn’t used it for quite a while, that book seems like a good candidate for returning to the library.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_556"/>So, which books should Fiona move to her precious desk space? In this example, real students would probably look at their upcoming assignments and select the books that they expect to be most useful. In other words, to make the best storage decision, they would ideally need information about <em>future usage</em>.</p>&#13;
<p class="indent">Unfortunately, hardware designers haven’t discovered how to build circuits that can predict the future. As an alternative to prediction, one could instead imagine a system that asks the programmer or user to inform the system in advance how a program will use data so that it’s placement could be optimized. Such a strategy may work well in specialized applications (e.g., large databases) that exhibit <em>very</em> regular access patterns. However, in a general-purpose system like a personal computer, requiring advance notice from the user is too large a burden—many users would not want to (or would be unable to) provide enough detail to help the system make good decisions.</p>&#13;
<p class="indent">Thus, instead of relying on future access information, systems look to the past as a predictor of what will <em>likely</em> happen in the future. Applying this idea to the book example suggests a relatively simple (but still quite effective) strategy for governing book storage spaces:</p>&#13;
<ul>&#13;
<li class="noindent">When Fiona needs to use a book, she retrieves it from wherever it currently is and moves it to her desk.</li>&#13;
<li class="noindent">If the desk is already full, she moves the book that she used <em>least recently</em> (that is, the book that has been sitting on the desk untouched for the longest amount of time) to her shelf.</li>&#13;
<li class="noindent">If the shelf is full, she returns the shelf’s least recently used book to the library to free up space.</li>&#13;
</ul>&#13;
<p class="indent">Even though this scheme may not be perfect, the simplicity makes it attractive. All it requires is the ability to move books between storage locations and a small amount of metainformation regarding the order in which books were previously used. Furthermore, this scheme captures the two initial temporal locality objectives well:</p>&#13;
<ul>&#13;
<li class="noindent">Frequently used books are likely to remain on the desk or shelf, preventing unnecessary trips to the library.</li>&#13;
<li class="noindent">Infrequently used books eventually become the least recently used book, at which point returning them to the library makes sense.</li>&#13;
</ul>&#13;
<p class="indent">Applying this strategy to primary storage devices looks remarkably similar to the book example: as data is loaded into CPU registers from main memory, make room for it in the CPU cache. If the cache is already full, make room in the cache by <em>evicting</em> the least recently used cache data to main memory. In the following caching section, we’ll explore the details of how such mechanisms are built in to modern caching systems.</p>&#13;
<h4 class="h4" id="lev2_192">11.3.4 Spatial Locality</h4>&#13;
<p class="noindent">Spatial locality suggests that, when making a trip to the library, Fiona should retrieve more than one book to reduce the likelihood of future library trips. <span epub:type="pagebreak" id="page_557"/>Specifically, she should retrieve additional books that are “nearby” the one she needs because those that are nearby seem like good candidates for books that might otherwise turn into additional library visits.</p>&#13;
<p class="indent">Suppose that she’s taking a literature course on the topic of Shakespeare’s histories. If in the first week of the course she’s assigned to read <em>Henry VI, Part I</em>, when she finds herself in the library to retrieve it, she’s likely to also find Parts II and III close by on the shelves. Even if she doesn’t yet know whether the course will assign those other two parts, it’s not unreasonable to think that she <em>might</em> need them. That is, the likelihood of needing them is much higher than a random book in the library, specifically because they are nearby the book she does need.</p>&#13;
<p class="indent">In this scenario, the likelihood increases due to the way libraries arrange books on shelves, and programs similarly organize data in memory. For example, a programming construct like an array or a <span class="literal">struct</span> stores a collection of related data in a contiguous region of memory. When iterating over consecutive elements in an array, there is clearly a spatial pattern in the accessed memory addresses. Applying these spatial locality lessons to primary storage devices implies that, when retrieving data from main memory, the system should also retrieve the data immediately surrounding it.</p>&#13;
<p class="indent">In the next section, we’ll characterize cache characteristics and describe mechanisms for the hardware to make identifying and exploiting locality happen automatically.</p>&#13;
<h3 class="h3" id="lev1_89">11.4 CPU Caches</h3>&#13;
<p class="noindent">Having characterized storage devices and recognized the important patterns of temporal and spatial locality, we’re ready to explore how CPU caches are designed and implemented. A <em>cache</em> is a small, fast storage device on a CPU that holds limited subsets of main memory. Caches face several important design questions: <em>Which</em> subsets of a program’s memory should the cache hold? <em>When</em> should the cache copy a subset of a program’s data from main memory to the cache, or vice versa? <em>How</em> can a system determine whether a program’s data is present in the cache?</p>&#13;
<p class="indent">Before exploring these challenging questions, we need to introduce some cache behavior and terminology. Recall that when accessing data in memory, a program first computes the data’s memory address (see “Instruction Structure” on <a href="ch07.xhtml#lev2_116">page 298</a>). Ideally, the data at the desired address already resides in the cache, allowing the program to skip accessing main memory altogether. To maximize performance, the hardware simultaneously sends the desired address to <em>both</em> the cache and main memory. Because the cache is faster and closer to the ALU, the cache responds much more quickly than memory. If the data is present in the cache (a <em>cache hit</em>), the cache hardware cancels the pending memory access because the cache can serve the data faster than memory.</p>&#13;
<p class="indent">Otherwise, if the data isn’t in the cache (a <em>cache miss</em>), the CPU has no choice but to wait for memory to retrieve it. Critically though, when the request to main memory completes, the CPU loads the retrieved data into <span epub:type="pagebreak" id="page_558"/>the cache so that subsequent requests for the same address (which are likely thanks to temporal locality) can be serviced quickly from the cache. Even if the memory access that misses is <em>writing</em> to memory, the CPU still loads the value into the cache on a miss because it’s likely that the program will attempt to access the same location again in the future.</p>&#13;
<p class="indent">When loading data into a cache after a miss, a CPU often finds that the cache doesn’t have enough free space available. In such cases, the cache must first <em>evict</em> some resident data to make room for the new data that it’s loading in. Because a cache stores subsets of data copied from main memory, evicting cached data that has been modified requires the cache to update the contents of main memory prior to evicting data from the cache.</p>&#13;
<p class="indent">To provide all the aforementioned functionality, cache designers employ one of three designs. This section begins by examining <em>direct-mapped caches</em>, which are less complex than the other designs.</p>&#13;
<h4 class="h4" id="lev2_193">11.4.1 Direct-Mapped Caches</h4>&#13;
<p class="noindent">A direct-mapped cache divides its storage space into units called <em>cache lines</em>. Depending on the size of a cache, it might hold dozens, hundreds, or even thousands of cache lines. In a direct-mapped cache, each cache line is independent of all the others and contains two important types of information: a <em>cache data block</em> and <em>metadata</em>.</p>&#13;
<p class="indent">A <em>cache data block</em> (often shortened to <em>cache block</em>) stores a subset of program data from main memory. Cache blocks store multibyte chunks of program data to take advantage of spatial locality. The size of a cache block determines the unit of data transfer between the cache and main memory. That is, when loading a cache with data from memory, the cache always receives a chunk of data the size of a cache block. Cache designers balance a trade-off in choosing a cache’s block size. Given a fixed storage budget, a cache can store more smaller blocks or fewer larger blocks. Using larger blocks improves performance for programs that exhibit good spatial locality, whereas having more blocks gives a cache the opportunity to store a more diverse subset of memory. Ultimately, which strategy provides the best performance depends on the workload of applications. Since general-purpose CPUs can’t assume much about a system’s applications, a typical CPU cache today uses middle-of-the-road block sizes ranging from 16 to 64 bytes.</p>&#13;
<p class="indent"><em>Metadata</em> stores information about the contents of the cache line’s data block. A cache line’s metadata does <em>not</em> contain program data. Instead, it maintains bookkeeping information for the cache line (e.g., to help identify which subset of memory the cache line’s data block holds).</p>&#13;
<p class="indent">When a program attempts to access a memory address, a cache must know where to look to find the corresponding data, check whether the desired data is available at that cache location, and if so, retrieve a portion of the stored cache block to the requesting application. The following steps walk through the details of this process for finding data in a cache and retrieving it.</p>&#13;
<h5 class="h5" id="lev3_83"><span epub:type="pagebreak" id="page_559"/>Locating Cached Data</h5>&#13;
<p class="noindent">A cache must be able to quickly determine whether the subset of memory corresponding to a requested address currently resides in the cache. To answer that question, a cache must first determine which cache line(s) to check. In a direct-mapped cache, each address in memory corresponds to <em>exactly</em> one cache line. This restriction explains the <em>direct-mapped</em> name—it maps every memory address directly to one cache line.</p>&#13;
<p class="indent"><a href="ch11.xhtml#ch11fig7">Figure 11-7</a> shows how memory addresses map to cache lines in a small direct-mapped cache with four cache lines and a 32-byte cache block size. Recall that a cache’s block size represents the smallest unit of data transfer between a cache and main memory. Thus, every memory address falls within one 32-byte range, and each range maps to one cache line.</p>&#13;
<div class="imagec" id="ch11fig7"><img alt="image" src="../images/11fig07.jpg"/></div>&#13;
<p class="figcap"><em>Figure 11-7: An example mapping of memory addresses to cache lines in a four-line direct-mapped cache with 32-byte cache blocks</em></p>&#13;
<p class="indent">Note that although each region of memory maps to only one cache line, many memory ranges map to the <em>same</em> cache line. All of the memory regions that map the same cache line (i.e., chunks of the same color in <a href="ch11.xhtml#ch11fig7">Figure 11-7</a>) compete for space in the same cache line, so only one region of each color can reside in the cache at a time.</p>&#13;
<p class="indent">A cache maps a memory address to a cache line using a portion of the bits in the memory address. To spread data more evenly among cache lines, caches use bits taken from the <em>middle</em> of the memory address, known as the <em>index</em> portion of the address, to determine which line the address maps to. The number of bits used as the index (which varies) determines how many lines a cache will hold. <a href="ch11.xhtml#ch11fig8">Figure 11-8</a> shows the index portion of a memory address referring to a cache line.</p>&#13;
<div class="imagec" id="ch11fig8"><span epub:type="pagebreak" id="page_560"/><img alt="image" src="../images/11fig08.jpg"/></div>&#13;
<p class="figcap"><em>Figure 11-8: The middle index portion of a memory address identifies a cache line.</em></p>&#13;
<p class="indent">Using the middle of the address reduces competition for the same cache line when program data is clustered together, which is often the case for programs that exhibit good locality. That is, programs tend to store variables nearby one another in one of a few locations (e.g., on the stack or heap). Such clustered variables share the same high-order address bits. Thus, indexing with the high-order bits would cause the clustered variables to all map to the same cache lines, leaving the rest of the cache unused. By using bits from the middle of the address, caches spread data more evenly among the available cache lines.</p>&#13;
<h5 class="h5" id="lev3_84">Identifying Cache Contents</h5>&#13;
<p class="noindent">Next, having located the appropriate cache line, the cache must determine whether that line holds the requested address. Since multiple memory ranges map to the same cache line, the cache examines the line’s metadata to answer two important questions: Does this cache line hold a valid subset of memory? If so, which of the many subsets of memory that map to this cache line does it currently hold?</p>&#13;
<p class="indent">To answer these questions, each cache line’s metadata includes a valid bit and a tag. The <em>valid bit</em> is a single bit that indicates whether a line is currently storing a valid subset of memory (if valid is set to 1). An invalid line (if valid is set to 0) never produces a cache hit because no data has been loaded into it. Invalid lines effectively represent free space in the cache.</p>&#13;
<p class="indent">In addition to a valid bit, each cache line’s metadata stores a <em>tag</em> that uniquely identifies which subset of memory the line’s cache block holds. The tag field stores the high-order bits of the address range stored in the cache line and allows a cache line to track where in memory its data block came from. In other words, because many memory subsets map to the same cache line (those with the same index bits), the tag records which of those subsets is currently present in the cache line.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_561"/>For a cache lookup to produce a hit, the tag field stored in the cache line must exactly match the tag portion (upper bits) of the program’s requested memory address. A tag mismatch indicates that a cache line’s data block does not contain the requested memory, even if the line stores valid data. <a href="ch11.xhtml#ch11fig9">Figure 11-9</a> illustrates how a cache divides a memory address into a tag and an index, uses the index bits to select a target cache line, verifies a line’s valid bit, and checks the line’s tag for a match.</p>&#13;
<div class="imagec" id="ch11fig9"><img alt="image" src="../images/11fig09.jpg"/></div>&#13;
<p class="figcap"><em>Figure 11-9: After using the requested memory address’s index bits to locate the proper cache line, the cache simultaneously verifies the line’s valid bit and checks its tag against the requested address’s tag. If the line is valid with a matching tag, the lookup succeeds as a hit.</em></p>&#13;
<h5 class="h5" id="lev3_85">Retrieving Cached Data</h5>&#13;
<p class="noindent">Finally, after using the program’s requested memory address to find the appropriate cache line and verifying that the line holds a valid subset of memory containing that address, the cache sends the requested data to the CPU’s components that need it. Because a cache line’s data block size (e.g., 64 bytes) is typically much larger than the amount of data that programs request (e.g., 4 bytes), caches use the low-order bits of the requested address as an <em>offset</em> into the cached data block. <a href="ch11.xhtml#ch11fig10">Figure 11-10</a> depicts how the offset portion of an address identifies which bytes of a cache block the program expects to retrieve.</p>&#13;
<div class="imagec" id="ch11fig10"><img alt="image" src="../images/11fig10.jpg"/></div>&#13;
<p class="figcap"><span epub:type="pagebreak" id="page_562"/><em>Figure 11-10: Given a cache data block, the offset portion of an address identifies which bytes the program wants to retrieve.</em></p>&#13;
<h5 class="h5" id="lev3_86">Memory Address Division</h5>&#13;
<p class="noindent">The <em>dimensions</em> of a cache dictate how many bits to interpret as the offset, index, and tag portions of a memory address. Equivalently, the number of bits in each portion of an address imply what the dimensions of a cache must be. In determining which bits belong to each portion of an address, it’s helpful to consider the address from right to left (i.e., from least to most significant bit).</p>&#13;
<p class="indent">The rightmost portion of the address is the <em>offset</em>, and its length depends on a cache’s block size dimension. The offset portion of an address must contain enough bits to refer to every possible byte within a cache data block. For example, suppose that a cache stores 32-byte data blocks. Because a program might come along asking for any of those 32 bytes, the cache needs enough offset bits to describe exactly which of the 32 possible positions the program might want. In this case, it would need five bits for the offset because five bits are necessary to represent 32 unique values (log<sub>2</sub> 32 = 5). In the reverse direction, a cache that uses four bits for the offset must store 16-byte data blocks (2<sup>4</sup> = 16).</p>&#13;
<p class="indent">The <em>index</em> portion of the address begins immediately to the left of the offset. To determine the number of index bits, consider the number of lines in the cache, given that the index needs enough bits to uniquely identify every cache line. Using similar logic to the offset, a cache with 1,024 lines needs 10 bits for the index (log<sub>2</sub> 1,024 = 10). Likewise, a cache that uses 12 bits for the index must have 4,096 lines (2<sup>12</sup> = 4,096).</p>&#13;
<div class="imagec" id="ch11fig11"><span epub:type="pagebreak" id="page_563"/><img alt="image" src="../images/11fig11.jpg"/></div>&#13;
<p class="figcap"><em>Figure 11-11: The index portion of an address uniquely identifies a cache line, and the offset portion uniquely identifies a position in the line’s data block.</em></p>&#13;
<p class="indent">The remaining address bits form the tag. Because the tag must uniquely identify the subset of memory contained within a cache line, the tag must use <em>all</em> of the remaining, unclaimed bits of the address. For example, if a machine uses 32-bit addresses, a cache with 5 offset bits and 10 index bits uses the remaining 32 – 15 = 17 bits of the address to represent the tag.</p>&#13;
<h5 class="h5" id="lev3_87">Direct-Mapped Read Examples</h5>&#13;
<p class="noindent">Consider a CPU with the following characteristics:</p>&#13;
<ul>&#13;
<li class="noindent">16-bit memory addresses</li>&#13;
<li class="noindent">a direct-mapped cache with 128 cache lines</li>&#13;
<li class="noindent">32-byte cache data blocks.</li>&#13;
</ul>&#13;
<p class="indent">The cache starts empty (all lines are invalid), as shown in <a href="ch11.xhtml#ch11fig12">Figure 11-12</a>.</p>&#13;
<div class="imagec" id="ch11fig12"><img alt="image" src="../images/11fig12.jpg"/></div>&#13;
<p class="figcap"><em>Figure 11-12: An empty direct-mapped example cache</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_564"/>Suppose that a program running on this CPU accesses the following memory locations (see <a href="ch11.xhtml#ch11fig13">Figures 11-13</a> through <a href="ch11.xhtml#ch11fig16">11-16</a>):</p>&#13;
<div class="number">&#13;
<p class="number">1. Read from address 1010000001100100.</p>&#13;
<p class="number">2. Read from address 1010000001100111.</p>&#13;
<p class="number">3. Read from address 1001000000100000.</p>&#13;
<p class="number">4. Read from address 1111000001100101.</p>&#13;
</div>&#13;
<p class="indent">To put the entire sequence together, follow these steps when tracing the behavior of a cache:</p>&#13;
<div class="number">&#13;
<p class="number">1. Divide the requested address into three portions, from right (low-order bits) to left (high-order bits): an offset within the cache data block, an index into the appropriate cache line, and a tag to identify which subset of memory the line stores.</p>&#13;
<p class="number">2. Index into the cache using the middle portion of the requested address to find the cache line to which the address maps.</p>&#13;
<p class="number">3. Check the cache line’s valid bit. When invalid, the program can’t use a cache line’s contents (cache miss), regardless of what the tag might be.</p>&#13;
<p class="number">4. Check the cache line’s tag. If the address’s tag matches the cache line’s tag and the line is valid, the cache line’s data block holds the data the program is looking for (cache hit). Otherwise, the cache must load the data from main memory at the identified index (cache miss).</p>&#13;
<p class="number">5. On a hit, use the low-order offset bits of the address to extract the program’s desired data from the stored block. (Not shown in this example.)</p>&#13;
</div>&#13;
<h5 class="h5" id="lev3_88">Address Division</h5>&#13;
<p class="noindent">Begin by determining how to divide the memory addresses into their <em>offset</em>, <em>index</em>, and <em>tag</em> portions. Consider the address portions from low-order to high-order bits (right to left):</p>&#13;
<div class="ul-none">&#13;
<p class="ul-indent"><em>Offset</em>: A 32-byte block size implies that the rightmost five bits of the address (log<sub>2</sub> 32 = 5) comprise the offset portion. With five bits, the offset can uniquely identify any of the 32 bytes in block.</p>&#13;
<p class="ul-indent"><em>Index</em>: A cache with 128 lines implies that the next seven bits of the address (log<sub>2</sub> 128 = 7) comprise the index portion. With seven bits, the index can uniquely identify each cache line.</p>&#13;
<p class="ul-indent"><span epub:type="pagebreak" id="page_565"/><em>Tag</em>: The tag consists of any remaining address bits that don’t belong to the offset or index. Here, the address has four remaining bits left that form the tag (16 – (5 + 7) = 4).</p>&#13;
</div>&#13;
<div class="imagec" id="ch11fig13"><img alt="image" src="../images/11fig13.jpg"/></div>&#13;
<p class="figcap"><em>Figure 11-13: Read from address 1010000001100100. Index 0000011 (line 3) is invalid, so the request misses and the cache loads data from main memory.</em></p>&#13;
<div class="imagec" id="ch11fig14"><img alt="image" src="../images/11fig14.jpg"/></div>&#13;
<p class="figcap"><em>Figure 11-14: Read from address 1010000001100111. Index 0000011 (line 3) is valid, and the tag (1010) matches, so the request hits. The cache yields data beginning at byte 7 (offset 0b00111) of its data block.</em></p>&#13;
<div class="imagec" id="ch11fig15"><img alt="image" src="../images/11fig15.jpg"/></div>&#13;
<p class="figcap"><span epub:type="pagebreak" id="page_566"/><em>Figure 11-15: Read from address 1001000000100000. Index 0000001 (line 1) is invalid, so the request misses and the cache loads data from main memory.</em></p>&#13;
<div class="imagec" id="ch11fig16"><img alt="image" src="../images/11fig16.jpg"/></div>&#13;
<p class="figcap"><em>Figure 11-16: Read from address 1111000001100101. Index 0000011 (line 3) is valid, but the tag doesn’t match, so the request misses and the cache loads data from main memory.</em></p>&#13;
<h5 class="h5" id="lev3_89">Writing to Cached Data</h5>&#13;
<p class="noindent">So far, this section has primarily considered memory read operations for which a CPU performs lookups in the cache. Caches must also allow programs to store values, and they support store operations with one of two strategies.</p>&#13;
<p class="indent">In a <em>write-through cache</em>, a memory write operation modifies the value in the cache and simultaneously updates the contents of main memory. That is, a write operation <em>always</em> synchronizes the contents of the cache and main memory immediately.</p>&#13;
<p class="indent">In a <em>write-back cache</em>, a memory write operation modifies the value stored in the cache’s data block, but it does <em>not</em> update main memory. Thus, after <span epub:type="pagebreak" id="page_567"/>updating the cache’s data, a write-back cache’s contents differ from the corresponding data in main memory.</p>&#13;
<p class="indent">To identify cache blocks whose contents differ from their main memory counterparts, each line in a write-back cache stores an additional bit of metadata, known as a <em>dirty bit</em>. When evicting the data block from a dirty cache line, the cache block’s data must first be written back to main memory to synchronize their contents. <a href="ch11.xhtml#ch11fig17">Figure 11-17</a> shows a direct-mapped cache that includes a dirty bit to mark lines that must be written to memory upon eviction.</p>&#13;
<div class="imagec" id="ch11fig17"><img alt="image" src="../images/11fig17.jpg"/></div>&#13;
<p class="figcap"><em>Figure 11-17: Cache extended with a dirty bit</em></p>&#13;
<p class="indent">As usual, the difference between the designs reveals a trade-off. Write-through caches are less complex than write-back caches, and they avoid storing extra metadata in the form of a dirty bit for each line. On the other hand, write-back caches reduce the cost of repeated writes to the same location in memory.</p>&#13;
<p class="indent">For example, suppose that a program frequently updates the same variable without that variable’s memory ever being evicted from the cache. A write-through cache writes to main memory on every update, even though each subsequent update is just going to overwrite the previous one, whereas a write-back cache writes to memory only when eventually evicting the cache block. Because amortizing the cost of a memory access across many writes significantly improves performance, most modern caches opt for a write-back design.</p>&#13;
<h5 class="h5" id="lev3_90">Direct-Mapped Write Examples (Write-Back)</h5>&#13;
<p class="noindent">Writes to the cache behave like reads, except they also set the modified cache line’s dirty bit. When evicting a dirty cache line, the cache must write the modified data block to memory before discarding it.</p>&#13;
<p class="indent">Suppose that the previously described example scenario continues with two additional memory accesses (see <a href="ch11.xhtml#ch11fig18">Figures 11-18</a> and <a href="ch11.xhtml#ch11fig19">11-19</a>):</p>&#13;
<div class="number">&#13;
<p class="number">5. Write to address: 1111000001100000.</p>&#13;
<p class="number">6. Write to address: 1010000001100100.</p>&#13;
</div>&#13;
<div class="imagec" id="ch11fig18"><img alt="image" src="../images/11fig18.jpg"/></div>&#13;
<p class="figcap"><span epub:type="pagebreak" id="page_568"/><em>Figure 11-18: Write to address 1111000001100000. Index 0000011 (line 3) is valid, and the tag (1111) matches, so the request hits. Because this access is a write, the cache sets the line’s dirty bit to 1.</em></p>&#13;
<div class="imagec" id="ch11fig19"><img alt="image" src="../images/11fig19.jpg"/></div>&#13;
<p class="figcap"><em>Figure 11-19: Write to address 1010000001100100. Index 0000011 (line 3) is valid, but the tag doesn’t match, so the request misses. Because the target line is both valid and dirty, the cache must save the existing data block to main memory before loading the new one. This access is a write, so the cache sets the newly loaded line’s dirty bit to 1.</em></p>&#13;
<p class="indent">In the fourth and sixth memory accesses of the example, the cache evicts data because two memory regions are competing for the same cache line. Next, we’ll explore a different cache design that aims to reduce this type of competition.</p>&#13;
<h4 class="h4" id="lev2_194">11.4.2 Cache Misses and Associative Designs</h4>&#13;
<p class="noindent">Cache designers aim to maximize a cache’s hit rate to ensure that as many memory requests as possible can avoid going to main memory. Even though <span epub:type="pagebreak" id="page_569"/>locality provides hope for achieving a good hit rate, real caches can’t expect to hit on every access for a variety of reasons:</p>&#13;
<div class="ul-none">&#13;
<p class="ul-indent"><strong>Compulsory</strong> or <strong>cold-start misses</strong>: If a program has never accessed a memory location (or any location near it), it has little hope of finding that location’s data in the cache. Thus, programs often cannot avoid cache misses when first accessing new memory addresses.</p>&#13;
<p class="ul-indent"><strong>Capacity misses</strong>: A cache stores a subset of main memory, and ideally it stores <em>exactly</em> the subset of memory that a program is actively using. However, if a program is actively using more memory than fits in the cache, it can’t possibly find <em>all</em> of the data it wants in the cache, leading to <em>capacity misses</em>.</p>&#13;
<p class="ul-indent"><strong>Conflict misses</strong>: To reduce the complexity of finding data, some cache designs limit where in the cache data can reside, and those restrictions can lead to <em>conflict misses</em>. For example, even if a direct-mapped cache is not 100% full, a program might end up with the addresses of two frequently used variables mapping to the same cache location. In such cases, each access to one of those variables evicts the other from the cache as they compete for the same cache line.</p>&#13;
</div>&#13;
<p class="indent">The relative frequency of each miss type depends on a program’s memory access pattern. In general though, without increasing the cache size, a cache’s design mainly affects its conflict miss rate. Although direct-mapped caches are less complex than other designs, they suffer the most from conflicts.</p>&#13;
<p class="indent">The alternative to a direct-mapped cache is an <em>associative</em> cache. An associative design gives a cache the flexibility to choose among more than one location to store a region of memory. Intuitively, having more storage location options reduces the likelihood of conflicts but also increases complexity due to more locations needing to be checked on every access.</p>&#13;
<p class="indent">A <em>fully associative</em> cache allows any memory region to occupy any cache location. Fully associative caches offer the most flexibility, but they also have the highest lookup and eviction complexity because every location needs to be simultaneously considered during any operation. Although fully associative caches are valuable in some small, specialized applications (e.g., translation look-aside buffers—see “Making Page Accesses Faster” on <a href="ch13.xhtml#lev3_110">page 655</a>), their high complexity makes them generally unfit for a general-purpose CPU cache.</p>&#13;
<p class="indent"><em>Set associative</em> caches occupy the middle ground between direct-mapped and fully associative designs, which makes them well suited for general-purpose CPUs. In a set associative cache, every memory region maps to exactly one <em>cache set</em>, but each set stores multiple cache lines. The number of lines allowed in a set is a fixed dimension of a cache, and set associative caches typically store two to eight lines per set.</p>&#13;
<h4 class="h4" id="lev2_195"><span epub:type="pagebreak" id="page_570"/>11.4.3 Set Associative Caches</h4>&#13;
<p class="noindent">A set associative design offers a good compromise between complexity and conflicts. The number of lines in a set limits how many places a cache needs to check during a lookup, and multiple memory regions that map to the same set don’t trigger conflict misses unless the entire set fills.</p>&#13;
<p class="indent">In a set associative cache, the <em>index</em> portion of a memory address maps the address to one set of cache lines. When performing an address lookup, the cache simultaneously checks every line in the set. <a href="ch11.xhtml#ch11fig20">Figure 11-20</a> illustrates the tag and valid bit checks in a two-way set associative cache.</p>&#13;
<p class="indent">If any of a set’s valid lines contains a tag that matches the address’s tag portion, the matching line completes the lookup. When the lookup narrows the search to just one cache line, it proceeds like a direct-mapped cache: the cache uses the address’s <em>offset</em> to send the desired bytes from the line’s cache block to the CPU’s arithmetic components.</p>&#13;
<div class="imagec" id="ch11fig20"><img alt="image" src="../images/11fig20.jpg"/></div>&#13;
<p class="figcap"><em>Figure 11-20: Valid bit verification and tag matching in a two-way set associative cache</em></p>&#13;
<p class="indent">The additional flexibility of multiple cache lines in a set reduces conflicts, but it also introduces a new wrinkle: when loading a value into a cache (and when evicting data already resident in the cache), the cache must decide <em>which</em> of the line options to use.</p>&#13;
<p class="indent">To help solve this selection problem, caches turn to the idea of locality. Specifically, temporal locality suggests that recently used data is likely to be used again. Therefore, caches adopt the same strategy that the previous section used to manage our example bookcase: when deciding which line in a set to evict, choose the least recently used (LRU) line. LRU is known as a <em>cache replacement policy</em> because it governs the cache’s eviction mechanism.</p>&#13;
<p class="indent">The LRU policy requires each set to store additional bits of metadata to identify which line of the set was used least recently. As the number of <span epub:type="pagebreak" id="page_571"/> lines in a set increases, so does the number of bits required to encode the LRU status of the set. These extra metadata bits contribute to the “higher complexity” of set associative designs compared to simpler direct-mapped variants.</p>&#13;
<p class="indent"><a href="ch11.xhtml#ch11fig21">Figure 11-21</a> illustrates a two-way set associative cache, meaning each set contains two lines. With just two lines, each set requires one LRU metadata bit to keep track of which line was least recently used. In the figure, an LRU value of zero indicates the leftmost line was least recently used, and a value of one means the rightmost line was least recently used.</p>&#13;
<div class="imagec" id="ch11fig21"><img alt="image" src="../images/11fig21.jpg"/></div>&#13;
<p class="figcap"><em>Figure 11-21: A two-way set associative cache in which each set stores one bit of LRU metadata to inform eviction decisions</em></p>&#13;
<p class="note"><strong><span class="black">Warning</span> LRU BIT INTERPRETATION</strong></p>&#13;
<p class="note-w"><a href="ch11.xhtml#ch11fig21">Figure 11-21</a>’s choice that zero means “left” and one means “right” is arbitrary. The interpretation of LRU bits varies across caches. If you’re asked to work with caches on an assignment, don’t assume the assignment is using the same LRU encoding scheme!</p>&#13;
<h5 class="h5" id="lev3_91">Set Associative Cache Examples</h5>&#13;
<p class="noindent">Consider a CPU with the following characteristics:</p>&#13;
<ul>&#13;
<li class="noindent">16-bit memory addresses.</li>&#13;
<li class="noindent">A two-way set associative cache with 64 sets. Note that making a cache two-way set associative doubles its storage capacity (two lines per set), so this example halves the number of sets so that it stores the same number of lines as the earlier direct-mapped example.</li>&#13;
<li class="noindent">32-byte cache blocks.</li>&#13;
<li class="noindent">An LRU cache replacement policy that indicates whether the leftmost line of the set was least recently used (LRU = 0) or the rightmost line of the set was least recently used (LRU = 1).</li>&#13;
</ul>&#13;
<p class="indent">Initially, the cache is empty (all lines invalid and LRU bits 0), as shown in <a href="ch11.xhtml#ch11fig22">Figure 11-22</a>.</p>&#13;
<div class="imagec" id="ch11fig22"><img alt="image" src="../images/11fig22.jpg"/></div>&#13;
<p class="figcap"><span epub:type="pagebreak" id="page_572"/><em>Figure 11-22: An empty two-way set associative example cache</em></p>&#13;
<p class="indent">Suppose that a program running on this CPU accesses the following memory locations (same as the direct-mapped example) (see <a href="ch11.xhtml#ch11fig23">Figures 11-23</a> through <a href="ch11.xhtml#ch11fig28">11-28</a>):</p>&#13;
<div class="number">&#13;
<p class="number">1. Read from address 1010000001100100.</p>&#13;
<p class="number">2. Read from address 1010000001100111.</p>&#13;
<p class="number">3. Read from address 1001000000100000.</p>&#13;
<p class="number">4. Read from address 1111000001100101.</p>&#13;
<p class="number">5. Write to address 1111000001100000.</p>&#13;
<p class="number">6. Write to address 1010000001100100.</p>&#13;
</div>&#13;
<p class="indent">Begin by determining how to divide the memory addresses into their <em>offset</em>, <em>index</em>, and <em>tag</em> portions. Consider the address portions from low-order to high-order bits (right to left):</p>&#13;
<div class="ul-none">&#13;
<p class="ul-indent"><em>Offset</em>: A 32-byte block size implies that the rightmost five bits of the address (log<sub>2</sub> 32 = 5) comprise the offset portion. Five bits allows the offset to uniquely identify any of the bytes in a block.</p>&#13;
<p class="ul-indent"><em>Index</em>: A 64-set cache implies that the next six bits of the address (log<sub>2</sub> 64 = 6) comprise the index portion. Six bits allows the index to uniquely identify each set in the cache.</p>&#13;
<p class="ul-indent"><em>Tag</em>: The tag consists of any remaining bits of the address that don’t belong to the offset or index. Here, the address has five remaining bits left over for the tag (16 – (5 + 6) = 5).</p>&#13;
</div>&#13;
<div class="imagec" id="ch11fig23"><img alt="image" src="../images/11fig23.jpg"/></div>&#13;
<p class="figcap"><span epub:type="pagebreak" id="page_573"/><em>Figure 11-23: Read from address 1010000001100100. Both lines at index 000011 (set 3) are invalid, so the request misses, and the cache loads data from main memory. The set’s LRU bit is 0, so the cache loads data into the left line and updates the LRU bit to 1.</em></p>&#13;
<div class="imagec" id="ch11fig24"><img alt="image" src="../images/11fig24.jpg"/></div>&#13;
<p class="figcap"><em>Figure 11-24: Read from address 1010000001100111. The left line at index 000011 (set 3) holds a matching tag, so the request hits.</em></p>&#13;
<div class="imagec" id="ch11fig25"><img alt="image" src="../images/11fig25.jpg"/></div>&#13;
<p class="figcap"><em>Figure 11-25: Read from address 1001000000100000. Both lines at index 000001 (set 1) are invalid, so the request misses, and the cache loads data from main memory. The set’s LRU bit is 0, so the cache loads data into the left line and updates the LRU bit to 1.</em></p>&#13;
<div class="imagec" id="ch11fig26"><img alt="image" src="../images/11fig26.jpg"/></div>&#13;
<p class="figcap"><span epub:type="pagebreak" id="page_574"/><em>Figure 11-26: Read from address 1111000001100101. At index 000011 (set 3), one line’s tag doesn’t match, and the other line is invalid, so the request misses. The set’s LRU bit is 1, so the cache loads data into the right line and updates the LRU bit to 0.</em></p>&#13;
<div class="imagec" id="ch11fig27"><img alt="image" src="../images/11fig27.jpg"/></div>&#13;
<p class="figcap"><em>Figure 11-27: Write to address 1111000001100000. The right line at index 000011 (set 3) is valid and holds a matching tag, so the request hits. Because this access is a write, the cache sets the line’s dirty bit to 1. The LRU bit remains 0 to indicate that the left line remains least recently used.</em></p>&#13;
<div class="imagec" id="ch11fig28"><img alt="image" src="../images/11fig28.jpg"/></div>&#13;
<p class="figcap"><span epub:type="pagebreak" id="page_575"/><em>Figure 11-28: Write to address 1010000001100100. The left line at index 000011 (set 3) is valid and holds a matching tag, so the request hits. Because this access is a write, the cache sets the line’s dirty bit to 1. After accessing the left line, the cache sets the line’s LRU bit to 1.</em></p>&#13;
<p class="indent">In this example, the same memory access sequence that produced two conflict misses with a direct-mapped cache suffers from no conflicts with a two-way set associative cache.</p>&#13;
<h3 class="h3" id="lev1_90">11.5 Cache Analysis and Valgrind</h3>&#13;
<p class="noindent">Because caches significantly influence program performance, most systems provide profiling tools to measure a program’s use of the cache. One such tool is Valgrind’s <span class="literal">cachegrind</span> mode, which this section uses to evaluate cache performance.</p>&#13;
<p class="indent">Consider the following program that generates a random <em>N</em> × <em>N</em> matrix:</p>&#13;
<p class="programs">#include &lt;stdio.h&gt;<br/>&#13;
#include &lt;stdlib.h&gt;<br/>&#13;
#include &lt;sys/time.h&gt;<br/>&#13;
#include &lt;time.h&gt;<br/>&#13;
<span epub:type="pagebreak" id="page_576"/>int **genRandomMatrix(int n, int max) {<br/>&#13;
    int i, j;<br/>&#13;
    int **mat = malloc(n * sizeof(int *));<br/>&#13;
<br/>&#13;
    for (i = 0; i &lt; n; i++) {<br/>&#13;
        mat[i] = malloc(n * sizeof(int));<br/>&#13;
<br/>&#13;
        for (j = 0; j &lt; n; j++) {<br/>&#13;
            mat[i][j] = 1 + rand() % max;<br/>&#13;
        }<br/>&#13;
    }<br/>&#13;
<br/>&#13;
    return mat;<br/>&#13;
}<br/>&#13;
<br/>&#13;
void free_all(int **mat, int n) {<br/>&#13;
    int i;<br/>&#13;
<br/>&#13;
    for (i = 0; i &lt; n; i++) {<br/>&#13;
        free(mat[i]);<br/>&#13;
    }<br/>&#13;
<br/>&#13;
    free(mat);<br/>&#13;
}<br/>&#13;
<br/>&#13;
int main(int argc, char **argv) {<br/>&#13;
    int i, n;<br/>&#13;
    int **matrix;<br/>&#13;
<br/>&#13;
    if (argc != 2) {<br/>&#13;
        fprintf(stderr, "usage: %s &lt;n&gt;\n", argv[0]);<br/>&#13;
        fprintf(stderr, "where &lt;n&gt; is the dimension of the matrix\n");<br/>&#13;
        return 1;<br/>&#13;
    }<br/>&#13;
<br/>&#13;
    n = strtol(argv[1], NULL, 10);<br/>&#13;
    srand(time(NULL));<br/>&#13;
<br/>&#13;
    matrix = genRandomMatrix(n, 100);<br/>&#13;
<br/>&#13;
    free_all(matrix, n);<br/>&#13;
    return 0;<br/>&#13;
}</p>&#13;
<p class="indent">Prior sections in this chapter introduced two functions for averaging every element of a matrix. They differ only in the way they index into the matrix:</p>&#13;
<span epub:type="pagebreak" id="page_577"/>&#13;
<p class="margnote">Version 1</p>&#13;
<p class="programs">float averageMat_v1(int **mat, int n) {<br/>&#13;
    int i, j, total = 0;<br/>&#13;
<br/>&#13;
    for (i = 0; i &lt; n; i++) {<br/>&#13;
        for (j = 0; j &lt; n; j++) {<br/>&#13;
            // Note indexing: [i][j]<br/>&#13;
            total += mat[i][j];<br/>&#13;
        }<br/>&#13;
    }<br/>&#13;
<br/>&#13;
    return (float) total / (n * n);<br/>&#13;
}</p>&#13;
<p class="margnote">Version 2</p>&#13;
<p class="programs">float averageMat_v2(int **mat, int n) {<br/>&#13;
    int i, j, total = 0;<br/>&#13;
<br/>&#13;
    for (i = 0; i &lt; n; i++) {<br/>&#13;
        for (j = 0; j &lt; n; j++) {<br/>&#13;
            // Note indexing: [j][i]<br/>&#13;
            total += mat[j][i];<br/>&#13;
        }<br/>&#13;
    }<br/>&#13;
<br/>&#13;
    return (float) total / (n * n);<br/>&#13;
}</p>&#13;
<p class="indent">This section uses cache profiling tools to quantify the differences between them.</p>&#13;
<h4 class="h4" id="lev2_196">11.5.1 A First Cut: Theoretical Analysis and Benchmarking</h4>&#13;
<p class="noindent">A theoretical analysis based on locality and the memory hierarchy suggests that the first version exhibits better spatial locality (on matrix <span class="literal">mat</span>) due to the fact that <span class="literal">mat</span> is stored in row-major order in memory (see the section “Two-Dimensional Array Memory Layout” on <a href="ch02.xhtml#lev3_13">page 86</a>). The second solution has poor spatial locality because each element in the matrix is visited in column-major order. Recall that data is loaded into a cache in <em>blocks</em>. Traversing the matrix in column-major order will likely lead to more cache misses, resulting in poorer performance.</p>&#13;
<p class="indent">Let’s modify the main function to include calls to the <span class="literal">gettimeofday</span> function to accurately measure the difference in performance between the two versions:</p>&#13;
<p class="programs">int main(int argc, char** argv) {<br/>&#13;
   /* Validate command line parameters. */<br/>&#13;
   if (argc != 2) {<br/>&#13;
       fprintf(stderr, "usage: %s &lt;n&gt;\n", argv[0]);<br/>&#13;
<span epub:type="pagebreak" id="page_578"/>&#13;
       fprintf(stderr, "where &lt;n&gt; is the dimension of the matrix\n");<br/>&#13;
       return 1;<br/>&#13;
   }<br/>&#13;
<br/>&#13;
   /* Declare and initialize variables. */<br/>&#13;
   int i;<br/>&#13;
   float res;<br/>&#13;
   double timer;<br/>&#13;
   int n = strtol(argv[1], NULL, 10);<br/>&#13;
   srand(time(NULL));<br/>&#13;
   struct timeval tstart, tend;<br/>&#13;
   int ** matrix = genRandomMatrix(n, 100);<br/>&#13;
<br/>&#13;
   /* Time version 1. */<br/>&#13;
   gettimeofday(&amp;tstart, NULL);<br/>&#13;
   res = averageMat_v1(matrix, n);<br/>&#13;
   gettimeofday(&amp;tend, NULL);<br/>&#13;
   timer = tend.tv_sec - tstart.tv_sec + (tend.tv_usec - tstart.tv_usec)/1.e6;<br/>&#13;
   printf("v1 average is: %.2f; time is %g\n", res, timer);<br/>&#13;
<br/>&#13;
   /* Time version 2. */<br/>&#13;
   gettimeofday(&amp;tstart, NULL);<br/>&#13;
   res = averageMat_v2(matrix, n);<br/>&#13;
   gettimeofday(&amp;tend, NULL);<br/>&#13;
   timer = tend.tv_sec - tstart.tv_sec + (tend.tv_usec - tstart.tv_usec)/1.e6;<br/>&#13;
   printf("v2 average is: %.2f; time is %g\n", res, timer);<br/>&#13;
<br/>&#13;
   /* Clean up. */<br/>&#13;
   free_all(matrix, n);<br/>&#13;
   return 0;<br/>&#13;
}</p>&#13;
<p class="indent">Compiling the code and running it yields the following result (note that the times will vary based on the machine on which it’s run):</p>&#13;
<p class="programs">$ <span class="codestrong1">gcc -o cachex cachex.c</span><br/>&#13;
$ <span class="codestrong1">./cachex 5000</span><br/>&#13;
v1 average is: 50.49; time is 0.053641<br/>&#13;
v2 average is: 50.49; time is 0.247644</p>&#13;
<p class="indent">That’s a big difference! In essence, the solution using row-major order is 4.61 times faster than the second one!</p>&#13;
<h4 class="h4" id="lev2_197">11.5.2 Cache Analysis in the Real World: Cachegrind</h4>&#13;
<p class="noindent">Theoretically analyzing the two solutions and then running them verifies that the first version is faster than the second. However, it doesn’t confirm the details of the cache analysis. Fortunately, the Valgrind suite of tools can help. Earlier in the book, we discussed how Valgrind can help find memory <span epub:type="pagebreak" id="page_579"/>leaks in a program (see “Debugging Memory with Valgrind” on <a href="ch03.xhtml#lev1_22">page 168</a>). This section describes Cachegrind, Valgrind’s cache simulator. Cachegrind enables a programmer to study how a program or particular function affects the cache.</p>&#13;
<p class="indent">Cachegrind simulates how a program interacts with the computer’s cache hierarchy. In many cases, Cachegrind can autodetect the cache organization of a machine. In the cases that it cannot, Cachegrind still simulates the first level (L1) cache and the last level (LL) cache. It assumes the first level cache has two independent components: the instruction cache and the data cache. The reason for this is that the last level cache has the most important implications for runtime. L1 caches also have the lowest level of associativity, so it’s important to ensure that programs interact well with it. These assumptions match the structure of most modern machines.</p>&#13;
<p class="indent">Cachegrind collects and outputs the following information:</p>&#13;
<ul>&#13;
<li class="noindent">Instruction cache reads (<span class="literal">Ir</span>)</li>&#13;
<li class="noindent">L1 instruction cache read misses (<span class="literal">I1mr</span>) and LL cache instruction read misses (<span class="literal">ILmr</span>)</li>&#13;
<li class="noindent">Data cache reads (<span class="literal">Dr</span>)</li>&#13;
<li class="noindent">D1 cache read misses (<span class="literal">D1mr</span>) and LL cache data misses (<span class="literal">DLmr</span>)</li>&#13;
<li class="noindent">Data cache writes (<span class="literal">Dw</span>)</li>&#13;
<li class="noindent">D1 cache write misses (<span class="literal">D1mw</span>) and LL cache data write misses (<span class="literal">DLmw</span>)</li>&#13;
</ul>&#13;
<p class="indent">Note that D1 total access is computed by <span class="literal">D1</span> = <span class="literal">D1mr</span> + <span class="literal">D1mw</span> and LL total access is given by <span class="literal">ILmr</span> + <span class="literal">DLmr</span> + <span class="literal">DLmw</span>.</p>&#13;
<p class="indent">Let’s see how well version 1 of the code operates under Cachegrind. To run it, execute Valgrind on the compiled code with the following command:</p>&#13;
<p class="programs">$ <span class="codestrong1">valgrind --tool=cachegrind ./cachex 1000</span></p>&#13;
<p class="indent">In this invocation, Valgrind’s <span class="literal">cachegrind</span> tool acts as a wrapper around the <span class="literal">cachex</span> executable. Choosing a smaller matrix size for Cachegrind aids in the speed of execution. Cachegrind outputs information about the number of cache hits and misses in the overall program:</p>&#13;
<p class="programs">==28657== Cachegrind, a cache and branch-prediction profiler<br/>&#13;
==28657== Copyright (C) 2002-2015, and GNU GPL'd by Nicholas Nethercote et al.<br/>&#13;
==28657== Using Valgrind-3.11.0 and LibVEX; rerun with -h for copyright info<br/>&#13;
==28657== Command: ./cachex 1000<br/>&#13;
==28657==<br/>&#13;
--28657-- warning: L3 cache found, using its data for the LL simulation.<br/>&#13;
average is: 50.49; time is 0.080304<br/>&#13;
average is: 50.49; time is 0.09733<br/>&#13;
==28657==<br/>&#13;
==28657== I   refs:      122,626,329<br/>&#13;
==28657== I1  misses:          1,070<br/>&#13;
==28657== LLi misses:          1,053<br/>&#13;
==28657== I1  miss rate:        0.00%<br/>&#13;
<span epub:type="pagebreak" id="page_580"/>==28657== LLi miss rate:        0.00%<br/>&#13;
==28657==<br/>&#13;
==28657== D   refs:       75,292,076  (56,205,598 rd   + 19,086,478 wr)<br/>&#13;
==28657== D1  misses:      1,192,118  ( 1,129,099 rd   +     63,019 wr)<br/>&#13;
==28657== LLd misses:         64,399  (     1,543 rd   +     62,856 wr)<br/>&#13;
==28657== D1  miss rate:         1.6% (       2.0%     +        0.3%  )<br/>&#13;
==28657== LLd miss rate:         0.1% (       0.0%     +        0.3%  )<br/>&#13;
==28657==<br/>&#13;
==28657== LL refs:         1,193,188  ( 1,130,169 rd   +     63,019 wr)<br/>&#13;
==28657== LL misses:          65,452  (     2,596 rd   +     62,856 wr)<br/>&#13;
==28657== LL miss rate:          0.0% (       0.0%     +        0.3%  )</p>&#13;
<p class="indent">However, this analysis is interested <em>specifically</em> in the hits and misses for the two versions of this averaging function. To view that information, use the Cachegrind tool <span class="literal">cg_annotate</span>. Running Cachegrind should have produced a file in the current working directory that looks similar to <span class="literal">cachegrind.out.n</span>, where <span class="literal">n</span> is some process ID number. To run <span class="literal">cg_annotate</span>, type in the following command (replacing <span class="literal">cachegrind.out.28657</span> with the name of the output file):</p>&#13;
<p class="programs">$ <span class="codestrong1">cg_annotate cachegrind.out.28657</span><br/>&#13;
<br/>&#13;
I1 cache:         32768 B, 64 B, 8-way associative<br/>&#13;
D1 cache:         32768 B, 64 B, 8-way associative<br/>&#13;
LL cache:         8388608 B, 64 B, 16-way associative<br/>&#13;
Command:          ./cachex 1000<br/>&#13;
Data file:        cachegrind.out.28657<br/>&#13;
Events recorded:  Ir I1mr ILmr Dr D1mr DLmr Dw D1mw DLmw<br/>&#13;
Events shown:     Ir I1mr ILmr Dr D1mr DLmr Dw D1mw DLmw<br/>&#13;
Event sort order: Ir I1mr ILmr Dr D1mr DLmr Dw D1mw DLmw<br/>&#13;
Thresholds:       0.1 100 100 100 100 100 100 100 100<br/>&#13;
Include dirs:<br/>&#13;
User annotated:<br/>&#13;
Auto-annotation:  off<br/>&#13;
<br/>&#13;
 ----------------------------------------------------------------------------<br/>&#13;
         Ir  I1mr  ILmr         Dr      D1mr  DLmr         Dw   D1mw   DLmw<br/>&#13;
 ----------------------------------------------------------------------------<br/>&#13;
122,626,329 1,070 1,053 56,205,598 1,129,099 1,543 19,086,478 63,019 62,856  PROG TOTALS<br/>&#13;
<br/>&#13;
 ----------------------------------------------------------------------------<br/>&#13;
        Ir I1mr ILmr         Dr      D1mr DLmr        Dw   D1mw   DLmw  file:function<br/>&#13;
 ----------------------------------------------------------------------------<br/>&#13;
14,009,017    3    3  9,005,008    62,688    0     1,004      0      0  averageMat_v1<br/>&#13;
14,009,017    0    0  9,005,008 1,062,996    0     1,004      0      0  averageMat_v2</p>&#13;
<p class="indent">We’ve edited the output from this command slightly to focus on the two versions of the average function. This output shows that version 2 yields <span epub:type="pagebreak" id="page_581"/>1,062,996 data misses, compared to only <span class="literal">62688</span> misses in version 1. Cachegrind provides solid proof that our analysis is correct!</p>&#13;
<h3 class="h3" id="lev1_91">11.6 Looking Ahead: Caching on Multicore Processors</h3>&#13;
<p class="noindent">So far our discussion of caching has focused on a single level of cache memory on a single-core processor. Modern processors, however, are multicore with several levels of cache memory. Typically, each core maintains its own private cache memory at the highest level(s) of the memory hierarchy and shares a single cache with all cores at lower levels. <a href="ch11.xhtml#ch11fig29">Figure 11-29</a> shows an example of the memory hierarchy on a four-core processor in which each core contains a private level 1 (L1) cache, and the level 2 (L2) cache is shared by all four cores.</p>&#13;
<div class="imagec" id="ch11fig29"><img alt="image" src="../images/11fig29.jpg"/></div>&#13;
<p class="figcap"><em>Figure 11-29: An example memory hierarchy on a multicore processor. Each of the four cores has its own private L1 cache, and all four cores share a single L2 cache that they access through a shared bus. The multicore processor connects to RAM via the memory bus.</em></p>&#13;
<p class="indent">Recall that higher levels of the memory hierarchy are faster to access and smaller in capacity than lower levels of the memory hierarchy. Thus, an L1 cache is smaller and faster than an L2 cache, which in turn is smaller and faster than RAM. Also recall that cache memory stores a copy of a value from a lower level in the memory hierarchy; a value stored in a L1 cache is a copy of the same value stored in the L2 cache, which is a copy of the same value stored in RAM. As a result, higher levels of the memory hierarchy serve as caches for lower levels. Thus, for the example in <a href="ch11.xhtml#ch11fig29">Figure 11-29</a>, the L2 cache is a cache of RAM contents, and each core’s L1 cache is a cache of the L2 cache contents.</p>&#13;
<p class="indent">Each core in a multicore processor simultaneously executes an independent stream of instructions, often from separate programs. Providing each core a private L1 cache allows the core to store copies of the data and instructions exclusively from the instruction stream it’s executing in its fastest cache memory. In other words, each core’s L1 cache stores a copy of only <span epub:type="pagebreak" id="page_582"/>those blocks of memory that are from its execution stream as opposed to competing for space in a single L1 cache shared by all cores. This design yields a higher hit rate in each core’s private L1 cache (in its fastest cache memory) than one in which all cores share a single L1 cache.</p>&#13;
<p class="indent">Today’s processors often include more than two levels of cache. Three levels are common in desktop systems, with the highest level (L1) typically split into two separate L1 caches, one for program instructions and the other for program data. Lower-level caches are usually <em>unified caches</em>, meaning that they store both program data and instructions. Each core usually maintains a private L1 cache and shares a single L3 cache with all cores. The L2 cache layer, which sits between each core’s private L1 cache and the shared L3 cache, varies substantially in modern CPU designs. The L2 may be a private L2 cache, may be shared by all cores, or may be a hybrid organization with multiple L2 caches, each shared by a subset of cores.</p>&#13;
<div class="g-box">&#13;
<p class="box-title">PROCESSOR AND CACHE INFORMATION IN LINUX SYSTEMS</p>&#13;
<p class="noindentt">If you’re curious about your CPU’s design, there are several ways to obtain information about a processor and its cache organization on your system. For example, the <span class="literal">lscpu</span> command displays information about the processor, including its number of cores and the levels and sizes of its caches:</p>&#13;
<p class="programs">$ lscpu<br/>&#13;
...<br/>&#13;
CPU(s):                          12<br/>&#13;
Thread(s) per core:              2<br/>&#13;
Core(s) per socket:              6<br/>&#13;
Socket(s):                       1<br/>&#13;
...<br/>&#13;
L1d cache:                       192 KiB<br/>&#13;
L1i cache:                       384 KiB<br/>&#13;
L2 cache:                        3 MiB<br/>&#13;
L3 cache:                        16 MiB</p>&#13;
<p class="noindentt">This output shows that there are six total cores (the number of <span class="literal">Socket(s)</span> multiplied by the <span class="literal">Core(s) per socket</span>), and that each core is two-way hyper- threaded (<span class="literal">Thread(s) per core</span>) to make the six physical cores appear as 12 CPUs to the operating system (see ”Multicore and Hardware Multithreading” in <a href="ch05.xhtml#ch05">Chapter 5</a> for more information on hardware multithreading). Additionally, the output shows that there are three levels of cache (<span class="literal">L1</span>, <span class="literal">L2</span>, and <span class="literal">L3</span>), and that there are two separate L1 caches, one for caching data (<span class="literal">L1d</span>) and another for caching instructions (<span class="literal">L1i</span>).</p>&#13;
<p class="noindentt">In addition to <span class="literal">lscpu</span>, files in the <span class="literal">/proc</span> and <span class="literal">/sys</span> filesystems contain information about the processor. For example, the command <span class="literal">cat /proc/cpuinfo</span> outputs information about the processor, and the following command lists information about the caches for a specific processor core (note that these files are named in terms of a core’s hyperthreaded CPUs, and in this example <span class="literal">cpu0</span> and <span class="literal">cpu6</span> are the two hyperthreaded CPUs on core 0).</p>&#13;
<span epub:type="pagebreak" id="page_583"/>&#13;
<p class="programs">$ ls /sys/devices/system/cpu/cpu0/cache<br/>&#13;
index0/  index1/  index2/  index3/</p>&#13;
<p class="noindentt">This output indicates that core 0 has four caches (<span class="literal">index0</span> to <span class="literal">index3</span>). To see the details of each cache, examine the index directory’s <span class="literal">type</span>, <span class="literal">level</span>, and <span class="literal">shared</span> <span class="literal">_cpu_list</span> files:</p>&#13;
<p class="programs">$ cat /sys/devices/system/cpu/cpu0/cache/index*/type<br/>&#13;
Data<br/>&#13;
Instruction<br/>&#13;
Unified<br/>&#13;
Unified<br/>&#13;
$ cat /sys/devices/system/cpu/cpu0/cache/index*/level<br/>&#13;
1<br/>&#13;
1<br/>&#13;
2<br/>&#13;
3<br/>&#13;
$ cat /sys/devices/system/cpu/cpu0/cache/index*/shared_cpu_list<br/>&#13;
0,6<br/>&#13;
0,6<br/>&#13;
0,6<br/>&#13;
0-11</p>&#13;
<p class="noindent">The <span class="literal">type</span> output indicates that core 0 has separate data and instruction caches as well as two other unified caches. Correlating the <span class="literal">level</span> output with the <span class="literal">type</span> output reveals that the data and instruction caches are both L1 caches, whereas the unified caches are L2 and L3 caches, respectively. The <span class="literal">shared_cpu_list</span> further shows that the L1 and L2 caches are private to core 0 (shared only by CPU <span class="literal">0</span> and <span class="literal">6</span>, the two hyperthread CPUs on core 0), and that the L3 cache is shared by all six cores (by all 12 hyperthreaded CPUs, <span class="literal">0-11</span>).</p>&#13;
</div>&#13;
<h4 class="h4" id="lev2_198">11.6.1 Cache Coherency</h4>&#13;
<p class="noindent">Because programs typically exhibit a high degree of locality of reference, it is advantageous for each core to have its own L1 cache to store copies of the data and instructions from the instruction stream it executes. However, multiple L1 caches can result in <em>cache coherency</em> problems. Problems with cache coherency arise when the value of a copy of a block of memory stored in one core’s L1 cache is different than the value of a copy of the same block stored in another core’s L1 cache. This situation occurs when one core writes to a block cached in its L1 cache that is also cached in other core’s L1 caches. Because a cache block contains a copy of memory contents, the system needs to maintain a coherent single value of the memory contents across all copies of the cached block. implement a <em>cache-coherence protocol</em> to ensure a coherent view of memory that can be cached and accessed by multiple cores. A cache coherency protocol ensures that any core accessing a memory location sees the most recently modified value of that memory location rather than seeing an older (stale) copy of the value that may be stored in its L1 cache.</p>&#13;
<h4 class="h4" id="lev2_199"><span epub:type="pagebreak" id="page_584"/>11.6.2 The MSI Protocol</h4>&#13;
<p class="noindent">There are many different cache coherency protocols. Here, we discuss the details of one example, the MSI protocol. The <em>MSI protocol</em> (Modified, Shared, Invalid) adds three flags (or bits) to each cache line. A flag’s value is either clear (0) or set (1). The values of the three flags encode the state of its data block with respect to cache coherency with other cached copies of the block, and their values trigger cache coherency actions on read or write accesses to the data block in the cache line. The three flags used by the MSI protocol are:</p>&#13;
<ul>&#13;
<li class="noindent">The <em>M</em> flag that, if set, indicates the block has been modified, meaning that this core has written to its copy of the cached value.</li>&#13;
<li class="noindent">The <em>S</em> flag that, if set, indicates that the block is unmodified and can be safely shared, meaning that multiple L1 caches may safely store a copy of the block and read from their copy.</li>&#13;
<li class="noindent">The <em>I</em> flag that, if set, indicates if the cached block is invalid or contains stale data (is an older copy of the data that does not reflect the current value of the block of memory).</li>&#13;
</ul>&#13;
<p class="indent">The MSI protocol is triggered on read and write accesses to cache entries.</p>&#13;
<p class="indent">On a read access:</p>&#13;
<ul>&#13;
<li class="noindent">If the cache block is in the M or S state, then the cached value is used to satisfy the read (its copy’s value is the most current value of the block of memory).</li>&#13;
<li class="noindent">If the cache block is in the I state, then the cached copy is out of date with a newer version of the block, and the block’s new value needs to be loaded into the cache line before the read can be satisfied.&#13;
<p class="indent">If another core’s L1 cache stores the new value (it stores the value with the M flag set indicating that it stores a modified copy of the value), that other core must first write its value back to the lower level (e.g., to the L2 cache). After performing the write-back, it clears the M flag with the cache line (its copy and the copy in the lower-level are now consistent) and sets the S bit to indicate that the block in this cache line is in a state that can be safely cached by other cores (the L1 block is consistent with its copy in the L2 cache and the core read the current value of the block from this L1 copy).</p>&#13;
<p class="indent">The core that initiated the read access on an line with the I flag set can then load the new value of the block into its cache line. It clears the I flag indicating that the block is now valid and stores the new value of the block, sets the S flag indicating that the block can be safely shared (it stores the latest value and is consistent with other cached copies), and clears the M flag indicating that the L1 block’s value matches that of the copy stored in the L2 cache (a read does not modify the L1 cached copy of the memory).</p></li>&#13;
</ul>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_585"/>On a write access:</p>&#13;
<ul>&#13;
<li class="noindent">If the block is in the M state, then write to the cached copy of the block. No changes to the flags are needed (the block remains in the M state).</li>&#13;
<li class="noindent">If the block is in the I or the S state, then notify other cores that the block is being written to (modified). Other L1 caches that have the block stored in the S state need to clear the S bit and set the I bit on their block (their copies of the block are now out of date with the copy that is being written to by the other core). If another L1 cache has the block in the M state, it will write its block back to the lower level and set its copy to I. The core writing will then load the new value of the block into its L1 cache, set the M flag (its copy will be modified by the write), and clear the I flags (its copy is now valid), and write to the cached block.</li>&#13;
</ul>&#13;
<p class="indent"><a href="ch11.xhtml#ch11fig30">Figure 11-30</a> through <a href="ch11.xhtml#ch11fig32">Figure 11-32</a> step through an example of the MSI protocol applied to ensure coherency of read and write accesses to a block of memory that is cached in two core’s private L1 caches. In <a href="ch11.xhtml#ch11fig30">Figure 11-30</a> our example starts with the shared data block copied into both core’s L1 cache with the S flag set, meaning that the L1 cached copies are the same as the value of the block in the L2 cache (all copies store the current value of the block, 6). At this point, both core 0 and core 1 can safely read from the copy stored in their L1 caches without triggering coherency actions (the S flag indicates that their shared copy is up to date).</p>&#13;
<div class="imagec" id="ch11fig30"><img alt="image" src="../images/11fig30.jpg"/></div>&#13;
<p class="figcap"><em>Figure 11-30: At the start, both cores have a copy of the block in their private L1 caches with the S flag set (in Shared mode)</em></p>&#13;
<p class="indent">If core 0 next writes to the copy of the block stored in its L1 cache, its L1 cache controller notifies the other L1 caches to invalidate their copy of the block. Core 1’s L1 cache controller then clears the S flag and sets the I flag on its copy, indicating that its copy of the block is stale. Core 0 writes to its copy of the block in its L1 cache (changing its value to 7 in our example) and sets the M flag and clears the S flag on the cache line to indicate that its copy has been modified and stores the current value of the block. At this point, <span epub:type="pagebreak" id="page_586"/>the copy in the L2 cache and in core 1’s L1 cache are stale. The resulting cache state is shown in <a href="ch11.xhtml#ch11fig31">Figure 11-31</a>.</p>&#13;
<div class="imagec" id="ch11fig31"><img alt="image" src="../images/11fig31.jpg"/></div>&#13;
<p class="figcap"><em>Figure 11-31: The resulting state of the caches after Core 0 writes to its copy of the block</em></p>&#13;
<p class="indent">At this point, core 0 can safely read from its copy of the cached block because its copy is in the M state, meaning that it stores the most recently written value of the block.</p>&#13;
<p class="indent">If core 1 next reads from the memory block, the I flag on its L1 cached copy indicates that its L1 copy of the block is stale and cannot be used to satisfy the read. Core 1’s L1 cache controller must first load the new value of the block into the L1 cache before the read can be satisfied. To achieve this, core 0’s L1 cache controller must first write its modified value of the block back to the L2 cache, so that core 1’s L1 cache can read the new value of the block into its L1 cache. The result of these actions, (shown in <a href="ch11.xhtml#ch11fig32">Figure 11-32</a>), is that core 0 and core 1’s L1 cached copies of the block are now both stored in the S state, indicating that each core’s L1 copy is up to data and can be safely used to satisfy subsequent reads to the block.</p>&#13;
<div class="imagec" id="ch11fig32"><img alt="image" src="../images/11fig32.jpg"/></div>&#13;
<p class="figcap"><em>Figure 11-32: The resulting state of the caches after Core 1 next reads the block</em></p>&#13;
<h4 class="h4" id="lev2_200"><span epub:type="pagebreak" id="page_587"/>11.6.3 Implementing Cache Coherency Protocols</h4>&#13;
<p class="noindent">To implement a cache coherency protocol, a processor needs some mechanism to identify when accesses to the other cores’ L1 cache contents require coherency state changes involving the other cores’ L1 cache contents. One way this mechanism is implemented is through <em>snooping</em> on a bus that is shared by all L1 caches. A snooping cache controller listens (or snoops) on the bus for reads or writes to blocks that it caches. Because every read and write request is in terms of a memory address, a snooping L1 cache controller can identify any read or write from another L1 cache for a block it stores, and can then respond appropriately based on the coherency protocol. For example, it can set the I flag on a cache line when it snoops a write to the same address by another L1 cache. This example is how a <em>write-invalidate protocol</em> would be implemented with snooping.</p>&#13;
<p class="indent">MSI and other similar protocols such as MESI and MOESI are write-invalidate protocols; that is, protocols that invalidate copies of cached entries on writes. Snooping can also be used by write-update cache coherency protocols, where the new value of a data is snooped from the bus and applied to update all copies stored in other L1 caches.</p>&#13;
<p class="indent">Instead of snooping, a directory-based cache coherence mechanism can be used to trigger cache coherency protocols. This method scales better than snooping due to performance limitations of multiple cores sharing a single bus. However, directory-based mechanisms require more state to detect when memory blocks are shared, and are slower than snooping.</p>&#13;
<h4 class="h4" id="lev2_201">11.6.4 More About Multicore Caching</h4>&#13;
<p class="noindent">The benefits to performance of each core of a multicore processor having its own separate cache(s) at the highest levels of the memory hierarchy, which are used to store copies of only the program data and instructions that it executes, is worth the added extra complexity of the processor needing to implement a cache coherency protocol.</p>&#13;
<p class="indent">Although cache coherency solves the memory coherency problem on multicore processors with separate L1 caches, there is another problem that can occur as a result of cache coherency protocols on multicore processors. This problem, called <em>false sharing</em>, may occur when multiple threads of a single multithreaded parallel program are running simultaneously across the multiple cores and are accessing memory locations that are near to those accessed by other threads. In section 14.5, we discuss the false sharing problem and some solutions to it.</p>&#13;
<p class="indent">For more information and details about hardware caching on multicore processors, including different protocols and how they are implemented, refer to a computer architecture textbook.<sup><a href="ch11.xhtml#fn11_8" id="rfn11_8">8</a></sup></p>&#13;
<h3 class="h3" id="lev1_92">11.7 Summary</h3>&#13;
<p class="noindent">This chapter explored the characteristics of computer storage devices and their trade-offs with respect to key measures like access latency, storage capacity, <span epub:type="pagebreak" id="page_588"/>transfer latency, and cost. Because devices embody many disparate design and performance trade-offs, they naturally form a memory hierarchy, which arranges them according to their capacity and access time. At the top of the hierarchy, primary storage devices like CPU caches and main memory quickly provide data directly to the CPU, but their capacity is limited. Lower in the hierarchy, secondary storage devices like solid-state drives and hard disks offer dense bulk storage at the cost of performance.</p>&#13;
<p class="indent">Because modern systems require both high capacity and good performance, system designers build computers with multiple forms of storage. Crucially, the system must manage which storage device holds any particular chunk of data. Systems aim to store data that’s being actively used in faster storage devices, and they relegate infrequently used data to slower storage devices.</p>&#13;
<p class="indent">To determine which data is being used, systems rely on program data access patterns known as <em>locality</em>. Programs exhibit two important types of locality: <em>temporal locality</em>, whereby programs tend to access the same data repeatedly over time, and <em>spatial locality</em>, whereby programs tend to access data that is nearby other, previously accessed data.</p>&#13;
<p class="indent">Locality serves as the basis for CPU caches, which store a small subset of main memory in fast storage directly on the CPU chip. When a program attempts to access main memory, the CPU first checks for the data in the cache; if it finds the data there, it can avoid the more costly trip to main memory.</p>&#13;
<p class="indent">When a program issues a request to read or write memory, it provides the address of the memory location that it wants to access. CPU caches use three sections of the bits in a memory address to identify which subset of main memory a cache line stores. The middle <em>index</em> bits of an address map the address to a storage location in the cache, the high-order <em>tag</em> bits uniquely identify which subset of memory the cache location stores, and the low-order <em>offset</em> bits identify which bytes of stored data the program wants to access.</p>&#13;
<p class="indent">Finally, this chapter concluded by demonstrating how the Cachegrind tool can enable cache performance profiling for a running program. Cachegrind simulates a program’s interaction with the cache hierarchy and collects statistics about a program’s use of the cache (e.g., the hit and miss rates).</p>&#13;
<h3 class="h3" id="lev1_93">Notes</h3>&#13;
<p class="fnote"><a href="ch11.xhtml#rfn11_1" id="fn11_1">1.</a> <em><a href="https://www.youtube.com/watch?v=9eyFDBPk4Yw">https://www.youtube.com/watch?v=9eyFDBPk4Yw</a></em></p>&#13;
<p class="fnote"><a href="ch11.xhtml#rfn11_2" id="fn11_2">2.</a> <em><a href="https://en.wikipedia.org/wiki/Punched_card">https://en.wikipedia.org/wiki/Punched_card</a></em></p>&#13;
<p class="fnote"><a href="ch11.xhtml#rfn11_3" id="fn11_3">3.</a> <em><a href="https://en.wikipedia.org/wiki/Magnetic_tape_data_storage">https://en.wikipedia.org/wiki/Magnetic_tape_data_storage</a></em></p>&#13;
<p class="fnote"><a href="ch11.xhtml#rfn11_4" id="fn11_4">4.</a> <em><a href="https://en.wikipedia.org/wiki/Floppy_disk">https://en.wikipedia.org/wiki/Floppy_disk</a></em></p>&#13;
<p class="fnote"><a href="ch11.xhtml#rfn11_5" id="fn11_5">5.</a> <em><a href="https://en.wikipedia.org/wiki/Optical_disc">https://en.wikipedia.org/wiki/Optical_disc</a></em></p>&#13;
<p class="fnote"><a href="ch11.xhtml#rfn11_6" id="fn11_6">6.</a> <em><a href="https://en.wikipedia.org/wiki/Hard_disk_drive">https://en.wikipedia.org/wiki/Hard_disk_drive</a></em></p>&#13;
<p class="fnote"><a href="ch11.xhtml#rfn11_7" id="fn11_7">7.</a> <em><a href="https://en.wikipedia.org/wiki/Flash_memory">https://en.wikipedia.org/wiki/Flash_memory</a></em></p>&#13;
<p class="fnote"><a href="ch11.xhtml#rfn11_8" id="fn11_8">8.</a> One suggestion is “Computer Organization and Design: The Hardware and Software Interface,” by David A. Patterson and John L. Hennessy.</p>&#13;
</body></html>