<html><head></head><body>
<section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" title="595" id="Page_595"/>11</span><br/>
<span class="ChapterTitle">SIMD Instructions</span></h1>
</header>
<figure class="opener">
<img src="image_fi/book_art/chapterart.png" alt=""/>
</figure>
<p class="ChapterIntro">This chapter discusses the <em>vector instructions</em> on the x86-64. This special class of instructions provides parallel processing, traditionally known as <em>single-instruction, multiple-data (</em><em>SIMD)</em> instructions because, quite literally, a single instruction operates on several pieces of data concurrently. As a result of this concurrency, SIMD instructions can often execute several times faster (in theory, as much as 32 to 64 times faster) than the comparable <em>single-instruction, single-data (SISD),</em> or <em>scalar,</em> instructions that compose the standard x86-64 instruction set.</p>
<p>The x86-64 actually provides three sets of vector instructions: the Multimedia Extensions (MMX) instruction set, the Streaming SIMD Extensions (SSE) instruction set, and the Advanced Vector Extensions (AVX) instruction <span epub:type="pagebreak" title="596" id="Page_596"/>set. This book does not consider the MMX instructions as they are obsolete (SSE equivalents exist for the MMX instructions).</p>
<p>The x86-64 vector instruction set (SSE/AVX) is almost as large as the scalar instruction set. A whole book could be written about SSE/AVX programming and algorithms. However, this is not that book; SIMD and parallel algorithms are an advanced subject beyond the scope of this book, so this chapter settles for introducing a fair number of SSE/AVX instructions and leaves it at that.</p>
<p>This chapter begins with some prerequisite information. First, it begins with a discussion of the x86-64 vector architecture and streaming data types. Then, it discusses how to detect the presence of various vector instructions (which are not present on all x86-64 CPUs) by using the <code>cpuid</code> instruction. Because most vector instructions require special memory alignment for data operands, this chapter also discusses MASM segments. </p>
<h2 id="h1-501089c11-0001">	11.1	The SSE/AVX Architectures</h2>
<p class="BodyFirst">Let’s begin by taking a quick look at the SSE and AVX features in the x64-86 CPUs. The SSE and AVX instructions have several variants: the original SSE, plus SSE2, SSE3, SSE3, SSE4 (SSE4.1 and SSE4.2), AVX, AVX2 (AVX and AVX2 are sometimes called AVX-256), and AVX-512. SSE3 was introduced along with the Pentium 4F (Prescott) CPU, Intel’s first 64-bit CPU. Therefore, you can assume that all Intel 64-bit CPUs support the SSE3 and earlier SIMD instructions.</p>
<p>The SSE/AVX architectures have three main generations:</p>
<ul>
<li>The SSE architecture, which (on 64-bit CPUs) provided sixteen 128-bit XMM registers supporting integer and floating-point data types</li>
<li>The AVX/AVX2 architecture, which supported sixteen 256-bit YMM registers (also supporting integer and floating-point data types)</li>
<li>The AVX-512 architecture, which supported up to thirty-two 512-bit ZMM registers</li>
</ul>
<p>As a general rule, this chapter sticks to AVX2 and earlier instructions in its examples. Please see the Intel and AMD CPU manuals for a discussion of the additional instruction set extensions such as AVX-512. This chapter does not attempt to describe every SSE or AVX instruction. Most streaming instructions have very specialized purposes and aren’t particularly useful in generic applications.</p>
<h2 id="h1-501089c11-0002">	11.2	Streaming Data Types</h2>
<p class="BodyFirst">The SSE and AVX programming models support two basic data types: scalars and vectors. <em>Scalars</em> hold one single- or double-precision floating-point value. <em>Vectors</em> hold multiple floating-point or integer values (between 2 and 32 values, depending on the scalar data type of byte, word, dword, qword, <span epub:type="pagebreak" title="597" id="Page_597"/>single precision, or double precision, and the register and memory size of 128 or 256 bits).</p>
<p>The XMM registers (XMM0 to XMM15) can hold a single 32-bit floating-point value (a scalar) or four single-precision floating-point values (a vector). The YMM registers (YMM0 to YMM15) can hold eight single-precision (32-bit) floating-point values (a vector); see <a href="#figure11-1" id="figureanchor11-1">Figure 11-1</a>.</p>
<figure>
<img src="image_fi/501089c11/f11001.png" alt="f11001" class=""/>
<figcaption><p><a id="figure11-1">Figure 11-1</a>: Packed and scalar single-precision floating-point data type</p></figcaption>
</figure>
<p>The XMM registers can hold a single double-precision scalar value or a vector containing a pair of double-precision values. The YMM registers can hold a vector containing four double-precision floating-point values, as shown in <a href="#figure11-2" id="figureanchor11-2">Figure 11-2</a>.</p>
<figure>
<img src="image_fi/501089c11/f11002.png" alt="f11002" class=""/>
<figcaption><p><a id="figure11-2">Figure 11-2</a>: Packed and scalar double-precision floating-point type</p></figcaption>
</figure>
<p>The XMM registers can hold 16 byte values (YMM registers can hold 32 byte values), allowing the CPU to perform 16 (32) byte-sized computations with one instruction (<a href="#figure11-3" id="figureanchor11-3">Figure 11-3</a>).</p>
<figure>
<img src="image_fi/501089c11/f11003.png" alt="f11003" class=""/>
<figcaption><p><a id="figure11-3">Figure 11-3</a>: Packed byte data type</p></figcaption>
</figure>
<p>The XMM registers can hold eight word values (YMM registers can hold sixteen word values), allowing the CPU to perform eight (sixteen) 16-bit word-sized integer computations with one instruction (<a href="#figure11-4" id="figureanchor11-4">Figure 11-4</a>).</p>
<span epub:type="pagebreak" title="598" id="Page_598"/><figure>
<img src="image_fi/501089c11/f11004.png" alt="f11004" class=""/>
<figcaption><p><a id="figure11-4">Figure 11-4</a>: Packed word data type</p></figcaption>
</figure>
<p>The XMM registers can hold four dword values (YMM registers can hold eight dword values), allowing the CPU to perform four (eight) 32-bit dword-sized integer computations with one instruction (<a href="#figure11-5" id="figureanchor11-5">Figure 11-5</a>).</p>
<figure>
<img src="image_fi/501089c11/f11005.png" alt="f11005" class=""/>
<figcaption><p><a id="figure11-5">Figure 11-5</a>: Packed double-word data type</p></figcaption>
</figure>
<p>The XMM registers can hold two qword values (YMM registers can hold four qword values), allowing the CPU to perform two (four) 64-bit qword computations with one instruction (<a href="#figure11-6" id="figureanchor11-6">Figure 11-6</a>).</p>
<figure>
<img src="image_fi/501089c11/f11006.png" alt="f11006" class=""/>
<figcaption><p><a id="figure11-6">Figure 11-6</a>: Packed quad-word data type</p></figcaption>
</figure>
<p>Intel’s documentation calls the vector elements in an XMM and a YMM register <em>lanes</em>. For example, a 128-bit XMM register has 16 bytes. Bits 0 to 7 are lane 0, bits 8 to 15 are lane 1, bits 16 to 23 are lane 2, . . . , and bits 120 to 127 are lane 15. A 256-bit YMM register has 32 byte-sized lanes, and a 512-bit ZMM register has 64 byte-sized lanes.</p>
<p>Similarly, a 128-bit XMM register has eight word-sized lanes (lanes 0 to 7). A 256-bit YMM register has sixteen word-sized lanes (lanes 0 to 15). On AVX-512-capable CPUs, a ZMM register (512 bits) has thirty-two word-sized lanes, numbered 0 to 31.</p>
<p>An XMM register has four dword-sized lanes (lanes 0 to 3); it also has four single-precision (32-bit) floating-point lanes (also numbered 0 to 3). A YMM register has eight dword or single-precision lanes (lanes 0 to 7). An AVX2 ZMM register has sixteen dword or single-precision-sized lanes (numbers 0 to 15).</p>
<p><span epub:type="pagebreak" title="599" id="Page_599"/>XMM registers support two qword-sized lanes (or two double-precision lanes), numbered 0 to 1. As expected, a YMM register has twice as many (four lanes, numbered 0 to 3), and an AVX2 ZMM register has four times as many lanes (0 to 7).</p>
<p>Several SSE/AVX instructions refer to various lanes within these registers. In particular, the shuffle and unpack instructions allow you to move data between lanes in SSE and AVX operands. See <span class="xref" itemid="xref_target_“The Shuffle and Unpack Instructions” on page 625">“The Shuffle and Unpack Instructions” on page 625</span> for examples of lane usage.</p>
<h2 id="h1-501089c11-0003">	11.3	Using cpuid to Differentiate Instruction Sets</h2>
<p class="BodyFirst">Intel introduced the 8086 (and shortly thereafter, the 8088) microprocessor in 1978. With almost every succeeding CPU generation, Intel added new instructions to the instruction set. Until this chapter, this book has used instructions that are generally available on all x86-64 CPUs (Intel and AMD). This chapter presents instructions that are available only on later-model x86-64 CPUs. To allow programmers to determine which CPU their applications were using so they could dynamically avoid using newer instructions on older processors, Intel introduced the <code>cpuid</code> instruction. </p>
<p>The <code>cpuid</code> instruction expects a single parameter (called a <em>leaf</em> function) passed in the EAX register. It returns various pieces of information about the CPU in different 32-bit registers based on the value passed in EAX. An application can test the return information to see if certain CPU features are available.</p>
<p>As Intel introduced new instructions, it changed the behavior of <code>cpuid</code> to reflect those changes. Specifically, Intel changed the range of values a program could legally pass in EAX to <code>cpuid</code>; this is known as the <em>highest function supported</em>. As a result, some 64-bit CPUs accept only values in the range 0h to 05h. The instructions this chapter discusses may require passing values in the range 0h to 07h. Therefore, the first thing you have to do when using <code>cpuid</code> is to verify that it accepts EAX = 07h as a valid parameter. </p>
<p>To determine the highest function supported, you load EAX with 0 or 8000_0000h and execute the <code>cpuid</code> instruction (all 64-bit CPUs support these two function values). The return value is the maximum you can pass to <code>cpuid</code> in EAX. The Intel and AMD documentation (also see <a href="https://en.wikipedia.org/wiki/CPUID" class="LinkURL">https://en.wikipedia.org/wiki/CPUID</a>) will list the values <code>cpuid</code> returns for various CPUs; for the purposes of this chapter, we need only verify that the highest function supported is 01h (which is true for all 64-bit CPUs) or 07h for certain instructions.</p>
<p>In addition to providing the highest function supported, the <code>cpuid</code> instruction with EAX = 0h (or 8000_0002h) also returns a 12-character vendor ID in the EBX, ECX, and EDX registers. For x86-64 chips, this will be either of the following:</p>
<ul>
<li>GenuineIntel (EBX is 756e_6547h, EDX is 4965_6e69h, and ECX is 6c65_746eh) </li>
<li>AuthenticAMD (EBX is 6874_7541h, EDX is 6974_6E65h, and ECX is 444D_4163h)</li>
</ul>
<p><span epub:type="pagebreak" title="600" id="Page_600"/>To determine if the CPU can execute most SSE and AVX instructions, you must execute <code>cpuid</code> with EAX = 01h and test various bits placed in the ECX register. For a few of the more advanced features (advanced bit-manipulation functions and AVX2 instructions), you’ll need to execute <code>cpuid</code> with EAX = 07h and check the results in the EBX register. The <code>cpuid</code> instruction (with EAX = 1) returns an interesting SSE/AVX feature flag in the following bits in ECX, as shown in <a href="#table11-1" id="tableanchor11-1">Table 11-1</a>; with EAX = 07h, it returns the bit manipulation or AVX2 flag in EBX, as shown in <a href="#table11-2" id="tableanchor11-2">Table 11-2</a>. If the bit is set, the CPU supports the specific instruction(s).</p>
<figure>
<figcaption class="TableTitle"><p><a id="table11-1">Table 11-1</a>: Intel <code>cpuid</code> Feature Flags (EAX = 1)</p></figcaption>
<table id="table-501089c11-0001" border="1">
<thead>
<tr>
<td><b>Bit</b></td>
<td><b>ECX</b></td>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>SSE3 support</td>
</tr>
<tr>
<td>1</td>
<td>PCLMULQDQ support</td>
</tr>
<tr>
<td>9</td>
<td>SSSE3 support</td>
</tr>
<tr>
<td>19</td>
<td>CPU supports SSE4.1 instructions</td>
</tr>
<tr>
<td>20</td>
<td>CPU supports SSE4.2 instructions</td>
</tr>
<tr>
<td>28</td>
<td>Advanced Vector Extensions</td>
</tr>
</tbody>
</table>
</figure>
<figure>
<figcaption class="TableTitle"><p><a id="table11-2">Table 11-2</a>: Intel <code>cpuid</code> Extended Feature Flags (EAX = 7, ECX = 0)</p></figcaption>
<table id="table-501089c11-0002" border="1">
<thead>
<tr>
<td><b>Bit</b></td>
<td><b>EBX</b></td>
</tr>
</thead>
<tbody>
<tr>
<td>3</td>
<td>Bit Manipulation Instruction Set 1</td>
</tr>
<tr>
<td>5</td>
<td>Advanced Vector Extensions 2 (AVX2)</td>
</tr>
<tr>
<td>8</td>
<td>Bit Manipulation Instruction Set 2</td>
</tr>
</tbody>
</table>
</figure>
<p><a href="#listing11-1" id="listinganchor11-1">Listing 11-1</a> queries the vendor ID and basic feature flags on a CPU.</p>
<pre><code>; Listing 11-1
 
; CPUID Demonstration.

        option  casemap:none

nl          =       10

            .const
ttlStr      byte    "Listing 11-1", 0

            .data
maxFeature  dword   ?
VendorID    byte    14 dup (0)

            .code
            externdef printf:proc

<span epub:type="pagebreak" title="601" id="Page_601"/>; Return program title to C++ program:

            public  getTitle
getTitle    proc
            lea     rax, ttlStr
            ret
getTitle    endp

; Used for debugging:

print       proc
            push    rax
            push    rbx
            push    rcx
            push    rdx
            push    r8
            push    r9
            push    r10
            push    r11

            push    rbp
            mov     rbp, rsp
            sub     rsp, 40
            and     rsp, -16

            mov     rcx, [rbp + 72]   ; Return address
            call    printf

            mov     rcx, [rbp + 72]
            dec     rcx
skipTo0:    inc     rcx
            cmp     byte ptr [rcx], 0
            jne     skipTo0
            inc     rcx
            mov     [rbp + 72], rcx

            leave
            pop     r11
            pop     r10
            pop     r9
            pop     r8
            pop     rdx
            pop     rcx
            pop     rbx
            pop     rax
            ret
print       endp

; Here is the "asmMain" function.

            public  asmMain
asmMain     proc
            push    rbx
            push    rbp
            mov     rbp, rsp
<span epub:type="pagebreak" title="602" id="Page_602"/>            sub     rsp, 56         ; Shadow storage

            xor     eax, eax
            cpuid
            mov     maxFeature, eax
            mov     dword ptr VendorID, ebx 
            mov     dword ptr VendorID[4], edx 
            mov     dword ptr VendorID[8], ecx

            lea     rdx, VendorID
            mov     r8d, eax
            call    print
            byte    "CPUID(0): Vendor ID='%s',  "
            byte    "max feature=0%xh", nl, 0

; Leaf function 1 is available on all CPUs that support
; CPUID, no need to test for it. 

            mov     eax, 1
            cpuid
            mov     r8d, edx
            mov     edx, ecx
            call    print
            byte    "cpuid(1), ECX=%08x, EDX=%08x", nl, 0

; Most likely, leaf function 7 is supported on all modern CPUs
; (for example, x86-64), but we'll test its availability nonetheless.

            cmp     maxFeature, 7
            jb      allDone

            mov     eax, 7
            xor     ecx, ecx
            cpuid
            mov     edx, ebx
            mov     r8d, ecx
            call    print
            byte    "cpuid(7), EBX=%08x, ECX=%08x", nl, 0

allDone:    leave
            pop     rbx
            ret     ; Returns to caller
asmMain     endp
            end</code></pre>
<p class="CodeListingCaption"><a id="listing11-1">Listing 11-1</a>: <code>cpuid</code> demonstration program</p>
<p>On an old MacBook Pro Retina with an Intel i7-3720QM CPU, running under Parallels, you get the following output:</p>
<pre><code>C:\&gt;<b>build listing11-1</b>

C:\&gt;<b>echo off</b>
 Assembling: listing11-1.asm
c.cpp

<span epub:type="pagebreak" title="603" id="Page_603"/>C:\&gt;<b>listing11-1</b>
Calling Listing 11-1:
CPUID(0): Vendor ID='GenuineIntel', max feature=0dh
cpuid(1), ECX=ffba2203, EDX=1f8bfbff
cpuid(7), EBX=00000281, ECX=00000000
Listing 11-1 terminated</code></pre>
<p>This CPU supports SSE3 instructions (bit 0 of ECX is 1), SSE4.1 and SSE4.2 instructions (bits 19 and 20 of ECX are 1), and the AVX instructions (bit 28 is 1). Those, largely, are the instructions this chapter describes. Most modern CPUs will support these instructions (the i7-3720QM was released by Intel in 2012). The processor doesn’t support some of the more interesting extended features on the Intel instruction set (the extended bit-manipulation instructions and the AVX2 instruction set). Programs using those instructions will not execute on this (ancient) MacBook Pro. </p>
<p>Running this on a more recent CPU (an iMac Pro 10-core Intel Xeon W-2150B) produces the following output:</p>
<pre><code>C:\&gt;<b>listing11-1</b>
Calling Listing 11-1:
CPUID(0): Vendor ID='GenuineIntel', max feature=016h
cpuid(1), ECX=fffa3203, EDX=1f8bfbff
cpuid(7), EBX=d09f47bb, ECX=00000000
Listing 11-1 terminated</code></pre>
<p>As you can see, looking at the extended feature bits, the newer Xeon CPU does support these additional instructions. The code fragment in <a href="#listing11-2" id="listinganchor11-2">Listing 11-2</a> provides a quick modification to <a href="#listing11-1">Listing 11-1</a> that tests for the availability of the BMI1 and BMI2 bit-manipulation instruction sets (insert the following code right before the <code>allDone</code> label in <a href="#listing11-1">Listing 11-1</a>).</p>
<pre><code>; Test for extended bit manipulation instructions 
; (BMI1 and BMI2):

            and     ebx, 108h       ; Test bits 3 and 8
            cmp     ebx, 108h       ; Both must be set
            jne     Unsupported
            call    print
            byte    "CPU supports BMI1 &amp; BMI2", nl, 0
            jmp     allDone 

Unsupported:
            call    print
            byte    "CPU does not support BMI1 &amp; BMI2 "
            byte    "instructions", nl, 0

allDone:    leave
            pop     rbx
            ret     ; Returns to caller
asmMain     endp</code></pre>
<p class="CodeListingCaption"><a id="listing11-2">Listing 11-2</a>: Test for BMI1 and BMI2 instruction sets</p>
<p><span epub:type="pagebreak" title="604" id="Page_604"/>Here’s the build command and program output on the Intel i7-3720QM CPU:</p>
<pre><code>C:\&gt;<b>build listing11-2</b>

C:\&gt;<b>echo off</b>
 Assembling: listing11-2.asm
c.cpp

C:\&gt;<b>listing11-2</b>
Calling Listing 11-2:
CPUID(0): Vendor ID='GenuineIntel', max feature=0dh
cpuid(1), ECX=ffba2203, EDX=1f8bfbff
cpuid(7), EBX=00000281, ECX=00000000
CPU does not support BMI1 &amp; BMI2 instructions
Listing 11-2 terminated</code></pre>
<p>Here’s the same program running on the iMac Pro (Intel Xeon W-2150B):</p>
<pre><code>C:\&gt;<b>listing11-2</b>
Calling Listing 11-2:
CPUID(0): Vendor ID='GenuineIntel', max feature=016h
cpuid(1), ECX=fffa3203, EDX=1f8bfbff
cpuid(7), EBX=d09f47bb, ECX=00000000
CPU supports BMI1 &amp; BMI2
Listing 11-2 terminated</code></pre>
<h2 id="h1-501089c11-0004">	11.4	Full-Segment Syntax and Segment Alignment</h2>
<p class="BodyFirst">As you will soon see, SSE and AVX memory data require alignment on 16-, 32-, and even 64-byte boundaries. Although you can use the <code>align</code> directive to align data (see <span class="xref" itemid="xref_target_“MASM Support for Data Alignment” in Chapter 3">“MASM Support for Data Alignment” in Chapter 3</span>), it doesn’t work beyond 16-byte alignment when using the simplified segment directives presented thus far in this book. If you need alignment beyond 16 bytes, you have to use MASM full-segment declarations.</p>
<p>If you want to create a segment with complete control over segment attributes, you need to use the <code>segment</code> and <code>ends</code> directives.<sup class="FootnoteReference"><a id="c11-footnoteref-1" href="#c11-footnote-1">1</a></sup> The generic syntax for a segment declaration is as follows:</p>
<pre><code><var>segname</var>  segment <var>readonly</var> <var>alignment</var> '<var>class</var>'
         statements
<var>segname</var>  ends</code></pre>
<p><var>segname</var> is an identifier. This is the name of the segment (which must also appear before the closing <code>ends</code> directive). It need not be unique; you can have several segment declarations that share the same name. MASM will <span epub:type="pagebreak" title="605" id="Page_605"/>combine segments with the same name when emitting code to the object file. Avoid the segment names <code>_TEXT</code>, <code>_DATA</code>, <code>_BSS</code>, and <code>_CONST</code>, as MASM uses these names for the <code>.code</code>, <code>.data</code>, <code>.data?</code>, and <code>.const</code> directives, respectively. </p>
<p>The <var>readonly</var> option is either blank or the MASM-reserved word <code>readonly</code>. This is a hint to MASM that the segment will contain read-only (constant) data. If you attempt to (directly) store a value into a variable that you declare in a read-only segment, MASM will complain that you cannot modify a read-only segment. </p>
<p>The <var>alignment</var> option is also optional and allows you to specify one of the following options:</p>
<ul>
<li><code>byte</code></li>
<li><code>word</code></li>
<li><code>dword</code></li>
<li><code>para</code></li>
<li><code>page</code></li>
<li><code>align(</code><var>n</var><code>)</code> (<var>n</var> is a constant that must be a power of 2)</li>
</ul>
<p>The alignment options tell MASM that the first byte emitted for this particular segment must appear at an address that is a multiple of the alignment option. The <code>byte</code>, <code>word</code>, and <code>dword</code> reserved words specify 1-, 2-, or 4-byte alignments. The <code>para</code> alignment option specifies paragraph alignment (16 bytes). The <code>page</code> alignment option specifies an address alignment of 256 bytes. Finally, the <code>align(</code><var>n</var><code>)</code> alignment option lets you specify any address alignment that is a power of 2 (1, 2, 4, 8, 16, 32, and so on).</p>
<p>The default segment alignment, if you don’t explicitly specify one, is paragraph alignment (16 bytes). This is also the default alignment for the simplified segment directives (<code>.code</code>, <code>.data</code>, <code>.data?</code>, and <code>.const</code>). </p>
<p>If you have some (SSE/AVX) data objects that must start at an address that is a multiple of 32 or 64 bytes, then creating a new data segment with 64-byte alignment is what you want. Here’s an example of such a segment:</p>
<pre><code>dseg64  segment align(64)
obj64   oword   0, 1, 2, 3   ; Starts on 64-byte boundary
b       byte    0            ; Messes with alignment
        align   32           ; Sets alignment to 32 bytes
obj32   oword   0, 1         ; Starts on 32-byte boundary
dseg64  ends</code></pre>
<p>The optional <var>class</var> field is a string (delimited by apostrophes and single quotes) that is typically one of the following names: <code>CODE</code>, <code>DATA</code>, or <code>CONST</code>. Note that MASM and the Microsoft linker will combine segments that have the same class name even if their segment names are different.</p>
<p>This chapter presents examples of these segment declarations as they are needed. </p>
<h2 id="h1-501089c11-0005"><span epub:type="pagebreak" title="606" id="Page_606"/>	11.5	SSE, AVX, and AVX2 Memory Operand Alignment</h2>
<p class="BodyFirst">SSE and AVX instructions typically allow access to a variety of memory operand sizes. The so-called scalar instructions, which operate on single data elements, can access byte-, word-, dword-, and qword-sized memory operands. In many respects, these types of memory accesses are similar to memory accesses by the non-SIMD instructions. The SSE, AVX, and AVX2 instruction set extensions also access <em>packed</em> or <em>vector</em> operands in memory. Unlike with the scalar memory operands, stringent rules limit the access of packed memory operands. This section discusses those rules.</p>
<p>The SSE instructions can access up to 128 bits of memory (16 bytes) with a single instruction. Most multi-operand SSE instructions can specify an XMM register or a 128-bit memory operand as their source (second) operand. As a general rule, these memory operands must appear on a 16-byte-aligned address in memory (that is, the LO 4 bits of the memory address must contain 0s). </p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p>	Almost all SSE, AVX, and AVX2 instructions will generate a memory alignment fault if you attempt to access a 128-bit object at an address that is not 16-byte-aligned. Always ensure that your SSE packed operands are properly aligned.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>Because segments have a default alignment of <code>para</code> (16 bytes), you can easily ensure that any 16-byte packed data objects are 16-byte-aligned by using the <code>align</code> directive:</p>
<pre><code>align 16</code></pre>
<p>MASM will report an error if you attempt to use <code>align 16</code> in a segment you’ve defined with the <code>byte</code>, <code>word</code>, or <code>dword</code> alignment type. It will work properly with <code>para</code>, <code>page</code>, or any <code>align(</code><var>n</var><code>)</code> option where <var>n</var> is greater than or equal to 16.</p>
<p>If you are using AVX instructions to access 256-bit (32-byte) memory operands, you must ensure that those memory operands begin on a 32-byte address boundary. Unfortunately, <code>align 32</code> won’t work, because the default segment alignment is <code>para</code> (16-byte) alignment, and the segment’s alignment must be greater than or equal to the operand field of any <code>align</code> directives appearing within that segment. Therefore, to be able to define 256-bit variables usable by AVX instructions, you must explicitly define a (data) segment that is aligned on a (minimum) 32-byte boundary, such as the following:</p>
<pre><code>avxData    segment  align(32)
           align    32    ; This is actually redundant here
someData   oword    0, 1  ; 256 bits of data
             .
             .
             .
avxData    ends</code></pre>
<p><span epub:type="pagebreak" title="607" id="Page_607"/>Though it’s somewhat redundant to say this, it’s so important it’s worth repeating: </p>
<blockquote class="review">
<p class="Blockquote">Almost all AVX/AVX2 instructions will generate an alignment fault if you attempt to access a 256-bit object at an address that is not 32-byte-aligned. Always ensure that your AVX packed operands are properly aligned.</p></blockquote>
<p>If you are using the AVX2 extended instructions with 512-bit memory operands, you must ensure that those operands appear on an address in memory that is a multiple of 64 bytes. As for AVX instructions, you will have to define a segment that has an alignment greater than or equal to 64 bytes, such as this:</p>
<pre><code>avx2Data   segment  align(64)
someData   oword    0, 1, 2, 3  ; 512 bits of data
             .
             .
             .
avx2Data   ends</code></pre>
<p>Forgive the redundancy, but it’s important to remember:</p>
<blockquote class="review">
<p class="Blockquote">Almost all AVX-512 instructions will generate an alignment fault if you attempt to access a 512-bit object at an address that is not 64-byte-aligned. Always ensure that your AVX-512 packed operands are properly aligned.</p></blockquote>
<p>If you’re using SSE, AVX, and AVX2 data types in the same application, you can create a single data segment to hold all these data values by using a 64-byte alignment option for the single section, instead of a segment for each data type size. Remember, the segment’s alignment has to be <em>greater than</em> or equal to the alignment required by the specific data type. Therefore, a 64-byte alignment will work fine for SSE and AVX/AVX2 variables, as well as AVX-512 variables:</p>
<pre><code>SIMDData   segment  align(64)
sseData    oword    0    ; 64-byte-aligned is also 16-byte-aligned
           align    32   ; Alignment for AVX data
avxData    oword    0, 1 ; 32 bytes of data aligned on 32 bytes
           align    64
avx2Data   oword    0, 1, 2, 3  ; 64 bytes of data
             .
             .
             .
SIMDData   ends</code></pre>
<p>If you specify an alignment option that is much larger than you need (such as 256-byte <code>page</code> alignment), you might unnecessarily waste memory.</p>
<p>The <code>align</code> directive works well when your SSE, AVX, and AVX2 data values are static or global variables. What happens when you want to create local variables on the stack or dynamic variables on the heap? Even if your program adheres to the Microsoft ABI, you’re guaranteed only 16-byte <span epub:type="pagebreak" title="608" id="Page_608"/>alignment on the stack upon entry to your program (or to a procedure). Similarly, depending on your heap management functions, there is no guarantee that a <code>malloc</code> (or similar) function returns an address that is properly aligned for SSE, AVX, or AVX2 data objects. </p>
<p>Inside a procedure, you can allocate storage for a 16-, 32-, or 64-byte-aligned variable by over-allocating the storage, adding the size minus 1 of the object to the allocated address, and then using the <code>and</code> instruction to zero out LO bits of the address (4 bits for 16-byte-aligned objects, 5 bits for 32-byte-aligned objects, and 6 bits for 64-byte-aligned objects). Then you reference the object by using this pointer. The following sample code demonstrates how to do this:</p>
<pre><code>sseproc     proc
sseptr      equ     &lt;[rbp - 8]&gt;
avxptr      equ     &lt;[rbp - 16]&gt;
avx2ptr     equ     &lt;[rbp - 24]&gt;
            push    rbp
            mov     rbp, rsp
            sub     rsp, 160

; Load RAX with an address 64 bytes
; above the current stack pointer. A
; 64-byte-aligned address will be somewhere
; between RSP and RSP + 63.

            lea     rax, [rsp + 63]

; Mask out the LO 6 bits of RAX. This
; generates an address in RAX that is
; aligned on a 64-byte boundary and is
; between RSP and RSP + 63:

            and     rax, -64 ; 0FFFF...FC0h

; Save this 64-byte-aligned address as
; the pointer to the AVX2 data:

            mov     avx2ptr, rax

; Add 64 to AVX2's address. This skips
; over AVX2's data. The address is also
; 64-byte-aligned (which means it is
; also 32-byte-aligned). Use this as
; the address of AVX's data:

            add     rax, 64
            mov     avxptr, rax

; Add 32 to AVX's address. This skips
; over AVX's data. The address is also
; 32-byte-aligned (which means it is
; also 16-byte-aligned). Use this as
<span epub:type="pagebreak" title="609" id="Page_609"/>; the address of SSE's data:

            add     rax, 32
            mov     sseptr, rax
             .
             . <var>Code that accesses the</var>
             . <var>AVX2, AVX, and SSE data</var>
             . <var>areas using avx2ptr</var>,
             . <var>avxptr, and sseptr</var>

            leave
            ret
sseproc     endp</code></pre>
<p>For data you allocate on the heap, you do the same thing: allocate extra storage (up to twice as many bytes minus 1), add the size of the object minus 1 (15, 31, or 63) to the address, and then mask the newly formed address with –64, –32, or –16 to produce a 64-, 32-, or 16-byte-aligned object, respectively.</p>
<h2 id="h1-501089c11-0006">	11.6	SIMD Data Movement Instructions</h2>
<p class="BodyFirst">The x86-64 CPUs provide a variety of data move instructions that copy data between (SSE/AVX) registers, load registers from memory, and store register values to memory. The following subsections describe each of these instructions.</p>
<h3 id="h2-501089c11-0001">11.6.1	The (v)movd and (v)movq Instructions</h3>
<p class="BodyFirst">For the SSE instruction set, the <code>movd</code> (<em>move dword</em>) and <code>movq</code> (<em>move qword</em>) instructions copy the value from a 32- or 64-bit general-purpose register or memory location into the LO dword or qword of an XMM register:<sup class="FootnoteReference"><a id="c11-footnoteref-2" href="#c11-footnote-2">2</a></sup></p>
<pre><code>movd <var>xmm</var><sub><em>n</em></sub>, <var>reg</var><span class="SubscriptLiteral">32</span>/<var>mem</var><span class="SubscriptLiteral">32</span>
movq <var>xmm</var><sub><em>n</em></sub>, <var>reg</var><span class="SubscriptLiteral">64</span>/<var>mem</var><span class="SubscriptLiteral">64</span></code></pre>
<p>These instructions zero-extend the value to remaining HO bits in the XMM register, as shown in Figures 11-7 and 11-8.</p>
<figure>
<img src="image_fi/501089c11/f11007.png" alt="f11007" class=""/>
<figcaption><p><a id="figure11-7">Figure 11-7</a>: Moving a 32-bit value from memory to an XMM register (with zero extension)</p></figcaption>
</figure>
<span epub:type="pagebreak" title="610" id="Page_610"/><figure>
<img src="image_fi/501089c11/f11008.png" alt="f11008" class=""/>
<figcaption><p><a id="figure11-8">Figure 11-8</a>: Moving a 64-bit value from memory to an XMM register (with zero extension)</p></figcaption>
</figure>
<p>The following instructions store the LO 32 or 64 bits of an XMM register into a dword or qword memory location or general-purpose register:</p>
<pre><code>movd <var>reg</var><sub>32</sub>/<var>mem</var><sub>32</sub>, <var>xmm</var><span class="SubscriptLiteral">n</span>
movq <var>reg</var><sub>64</sub>/<var>mem</var><sub>64</sub>, <var>xmm</var><span class="SubscriptLiteral">n</span></code></pre>
<p>The <code>movq</code> instruction also allows you to copy data from the LO qword of one XMM register to another, but for whatever reason, the <code>movd</code> instruction does not allow two XMM register operands: </p>
<pre><code>movq <var>xmm</var><span class="SubscriptLiteral">n</span>, <var>xmm</var><span class="SubscriptLiteral">n</span></code></pre>
<p>For the AVX instructions, you use the following instructions:<sup class="FootnoteReference"><a id="c11-footnoteref-3" href="#c11-footnote-3">3</a></sup></p>
<pre><code>vmovd <var>xmm</var><span class="SubscriptLiteral">n</span>, <var>reg</var><sub>32</sub>/<var>mem</var><sub>32</sub>
vmovd <var>reg</var><sub>32</sub>/<var>mem</var><sub>32</sub>, <var>xmm</var><span class="SubscriptLiteral">n</span>
vmovq <var>xmm</var><span class="SubscriptLiteral">n</span>, <var>reg</var><sub>64</sub>/<var>mem</var><sub>64</sub>
vmovq <var>reg</var><sub>64</sub>/<var>mem</var><sub>64</sub>, <var>xmm</var><span class="SubscriptLiteral">n</span></code></pre>
<p>The instructions with the XMM destination operands also zero-extend their values into the HO bits (up to bit 255, unlike the standard SSE instructions that do not modify the upper bits of the YMM registers).</p>
<p>Because the <code>movd</code> and <code>movq</code> instructions access 32- and 64-bit values in memory (rather than 128-, 256-, or 512-bit values), these instructions do not require their memory operands to be 16-, 32-, or 64-byte-aligned. Of course, the instructions may execute faster if their operands are dword (<code>movd</code>) or qword (<code>movq</code>) aligned in memory.</p>
<h3 id="h2-501089c11-0002">11.6.2	The (v)movaps, (v)movapd, and (v)movdqa Instructions</h3>
<p class="BodyFirst">The <code>movaps</code> (<em>move aligned, packed single</em>), <code>movapd</code> (<em>move aligned, packed double</em>), and <code>movdqa</code> (<em>move double quad-word aligned</em>) instructions move 16 bytes of data between memory and an XMM register or between two XMM registers. The AVX versions (with the <code>v</code> prefix) move 16 or 32 bytes between memory and an XMM or a YMM register or between two XMM or YMM registers (moves involving XMM registers zero out the HO bits of the corresponding YMM <span epub:type="pagebreak" title="611" id="Page_611"/>register). The memory locations must be aligned on a 16-byte or 32-byte boundary (respectively), or the CPU will generate an unaligned access fault. </p>
<p>All three <code>mov*</code> instructions load 16 bytes into an XMM register and are, in theory, interchangeable. In practice, Intel may optimize the operations for the type of data they move (single-precision floating-point values, double-precision floating-point values, or integer values), so it’s always a good idea to choose the appropriate instruction for the data type you are using (see<span class="xref" itemid="xref_target_ “Performance Issues and the SIMD Move Instructions” on page 622"> “Performance Issues and the SIMD Move Instructions” on page 622</span> for an explanation). Likewise, all three <code>vmov*</code> instructions load 16 or 32 bytes into an XMM or a YMM register and are interchangeable.</p>
<p>These instructions take the following forms:</p>
<pre><code>movaps <var>xmm</var><span class="SubscriptLiteral">n</span>, <var>mem</var><sub>128</sub>    vmovaps <var>xmm</var><span class="SubscriptLiteral">n</span>, <var>mem</var><sub>128</sub>    vmovaps <var>ymm</var><span class="SubscriptLiteral">n</span>, <var>mem</var><sub>256</sub>
movaps <var>mem</var><sub>128</sub>, <var>xmm</var><span class="SubscriptLiteral">n</span>    vmovaps <var>mem</var><sub>128</sub>, <var>xmm</var><span class="SubscriptLiteral">n</span>    vmovaps <var>mem</var><sub>256</sub>, <var>ymm</var><span class="SubscriptLiteral">n</span>
movaps <var>xmm</var><span class="SubscriptLiteral">n</span>, <var>xmm</var><span class="SubscriptLiteral">n</span>     vmovaps <var>xmm</var><span class="SubscriptLiteral">n</span>, <var>xmm</var><span class="SubscriptLiteral">n</span>     vmovaps <var>ymm</var><span class="SubscriptLiteral">n</span>, <var>ymm</var><span class="SubscriptLiteral">n</span>
movapd <var>xmm</var><span class="SubscriptLiteral">n</span>, <var>mem</var><sub>128</sub>    vmovapd <var>xmm</var><span class="SubscriptLiteral">n</span>, <var>mem</var><sub>128</sub>    vmovapd <var>ymm</var><span class="SubscriptLiteral">n</span>, <var>mem</var><sub>256</sub>
movapd <var>mem</var><sub>128</sub>, <var>xmm</var><span class="SubscriptLiteral">n</span>    vmovapd <var>mem</var><sub>128</sub>, <var>xmm</var><span class="SubscriptLiteral">n</span>    vmovapd <var>mem</var><sub>256</sub>, <var>ymm</var><span class="SubscriptLiteral">n</span>
movapd <var>xmm</var><span class="SubscriptLiteral">n</span>, <var>xmm</var><span class="SubscriptLiteral">n</span>     vmovapd <var>xmm</var><span class="SubscriptLiteral">n</span>, <var>xmm</var><span class="SubscriptLiteral">n</span>     vmovapd <var>ymm</var><span class="SubscriptLiteral">n</span>, <var>ymm</var><span class="SubscriptLiteral">n</span>
movdqa <var>xmm</var><span class="SubscriptLiteral">n</span>, <var>mem</var><sub>128</sub>    vmovdqa <var>xmm</var><span class="SubscriptLiteral">n</span>, <var>mem</var><sub>128</sub>    vmovdqa <var>ymm</var><span class="SubscriptLiteral">n</span>, <var>mem</var><sub>256</sub>
movdqa <var>mem</var><sub>128</sub>, <var>xmm</var><span class="SubscriptLiteral">n</span>    vmovdqa <var>mem</var><sub>128</sub>, <var>xmm</var><span class="SubscriptLiteral">n</span>    vmovdqa <var>mem</var><sub>256</sub>, <var>ymm</var><span class="SubscriptLiteral">n</span>
movdqa <var>xmm</var><span class="SubscriptLiteral">n</span>, <var>xmm</var><span class="SubscriptLiteral">n</span>     vmovdqa <var>xmm</var><span class="SubscriptLiteral">n</span>, <var>xmm</var><span class="SubscriptLiteral">n</span>     vmovdqa <var>ymm</var><span class="SubscriptLiteral">n</span>, <var>ymm</var><span class="SubscriptLiteral">n</span></code></pre>
<p>The <var>mem</var><span class="SubscriptLiteral">128</span> operand should be a vector (array) of four single-precision floating-point values for the <code>(v)movaps</code> instruction; it should be a vector of two double-precision floating-point values for the <code>(v)movapd</code> instruction; it should be a 16-byte value (16 bytes, 8 words, 4 dwords, or 2 qwords) when using the <code>(v)movdqa</code> instruction. If you cannot guarantee that the operands are aligned on a 16-byte boundary, use the <code>movups</code>, <code>movupd</code>, or <code>movdqu</code> instructions, instead (see the next section).</p>
<p>The <var>mem</var><span class="SubscriptLiteral">256</span> operand should be a vector (array) of eight single-precision floating-point values for the <code>vmovaps</code> instruction; it should be a vector of four double-precision floating-point values for the <code>vmovapd</code> instruction; it should be a 32-byte value (32 bytes, 16 words, 8 dwords, or 4 qwords) when using the <code>vmovdqa</code> instruction. If you cannot guarantee that the operands are 32-byte-aligned, use the <code>vmovups</code>, <code>vmovupd</code>, or <code>vmovdqu</code> instructions instead.</p>
<p>Although the physical machine instructions themselves don’t particularly care about the data type of the memory operands, MASM’s assembly syntax certainly does care. You will need to use operand type coercion if the instruction doesn’t match one of the following types:</p>
<ul>
<li>The <code>movaps</code> instruction allows <code>real4</code>, <code>dword</code>, and <code>oword</code> operands.</li>
<li>The <code>movapd</code> instruction allows <code>real8</code>, <code>qword</code>, and <code>oword</code> operands.</li>
<li>The <code>movdqa</code> instruction allows only <code>oword</code> operands.</li>
<li>The <code>vmovaps</code> instruction allows <code>real4</code>, <code>dword</code>, and <code>ymmword ptr</code> operands (when using a YMM register).</li>
<li>The <code>vmovapd</code> instruction allows <code>real8</code>, <code>qword</code>, and <code>ymmword ptr</code> operands (when using a YMM register).</li>
<li>The <code>vmovdqa</code> instruction allows only <code>ymmword ptr</code> operands (when using a YMM register).</li>
</ul>
<p><span epub:type="pagebreak" title="612" id="Page_612"/>Often you will see <code>memcpy</code> (<em>memory copy</em>) functions use the <code>(v)movapd</code> instructions for very high-performance operations. See Agner Fog’s website at <a href="https://www.agner.org/optimize/" class="LinkURL">https://www.agner.org/optimize/</a> for more details. </p>
<h3 id="h2-501089c11-0003">11.6.3	The (v)movups, (v)movupd, and (v)movdqu Instructions</h3>
<p class="BodyFirst">When you cannot guarantee that packed data memory operands lie on a 16- or 32-byte address boundary, you can use the <code>(v)movups</code> (<em>move unaligned packed single-precision</em>), <code>(v)movupd</code> (<em>move unaligned packed</em> <em>double-precision</em>), and <code>(v)movdqu</code> (<em>move double quad-word</em> <em>unaligned</em>) instructions to move data between XMM or YMM registers and memory. </p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p>	These instructions typically run slower than their aligned equivalents. Therefore, you should use the aligned instructions if you are moving data between XMM or YMM registers or know the memory operands lie on 16-byte-aligned or 32-byte-aligned addresses.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>As for the aligned moves, all the unaligned moves do the same thing: copying 16 (32) bytes of data to and from memory. The convention for the various data types is the same as it is for the aligned data movement instructions.</p>
<h3 id="h2-501089c11-0004">11.6.4	Performance of Aligned and Unaligned Moves</h3>
<p class="BodyFirst">Listings <a href="#listing11-3">11-3</a> and <a href="#listing11-4">11-4</a> provide sample programs that demonstrate the performance of the <code>mova*</code> and <code>movu*</code> instructions using aligned and unaligned memory accesses.</p>
<pre><code>; Listing 11-3
 
; Performance test for packed versus unpacked
; instructions. This program times aligned accesses.

        option  casemap:none

nl          =       10

            .const
ttlStr      byte    "Listing 11-3", 0

dseg        segment align(64) 'DATA'

; Aligned data types:

            align   64
alignedData byte    64 dup (0)
dseg        ends
            
            .code
            externdef printf:proc
            
<span epub:type="pagebreak" title="613" id="Page_613"/>; Return program title to C++ program:

            public  getTitle
getTitle    proc
            lea     rax, ttlStr
            ret
getTitle    endp

; Used for debugging:

print       proc

; Print code removed for brevity.
; See Listing 11-1 for actual code.

print       endp

; Here is the "asmMain" function.

            public  asmMain
asmMain     proc
            push    rbx
            push    rbp
            mov     rbp, rsp
            sub     rsp, 56         ; Shadow storage
            
            call    print
            byte    "Starting", nl, 0
            
            mov     rcx, 4000000000 ; 4,000,000,000
            lea     rdx, alignedData
            mov     rbx, 0
rptLp:      mov     rax, 15
rptLp2:     movaps  xmm0, xmmword ptr [rdx + rbx * 1]
            movapd  xmm0, real8 ptr   [rdx + rbx * 1]
            movdqa  xmm0, xmmword ptr [rdx + rbx * 1]
            vmovaps ymm0, ymmword ptr [rdx + rbx * 1]
            vmovapd ymm0, ymmword ptr [rdx + rbx * 1]
            vmovdqa ymm0, ymmword ptr [rdx + rbx * 1]
            vmovaps zmm0, zmmword ptr [rdx + rbx * 1]
            vmovapd zmm0, zmmword ptr [rdx + rbx * 1]
            
            dec     rax
            jns     rptLp2

            dec     rcx
            jnz     rptLp
            
            call    print
            byte    "Done", nl, 0
             
allDone:    leave
            pop     rbx
<span epub:type="pagebreak" title="614" id="Page_614"/>            ret     ; Returns to caller
asmMain     endp
            end</code></pre>
<p class="CodeListingCaption"><a id="listing11-3">Listing 11-3</a>: Aligned memory-access timing code</p>
<pre><code>; Listing 11-4
 
; Performance test for packed versus unpacked
; instructions. This program times unaligned accesses. 

        option  casemap:none

nl          =       10

            .const
ttlStr      byte    "Listing 11-4", 0

dseg        segment align(64) 'DATA'

; Aligned data types:

            align   64
alignedData byte    64 dup (0)
dseg        ends

            .code
            externdef printf:proc
            
; Return program title to C++ program:

            public  getTitle
getTitle    proc
            lea     rax, ttlStr
            ret
getTitle    endp

; Used for debugging:

print       proc

; Print code removed for brevity.
; See Listing 11-1 for actual code.

print       endp

; Here is the "asmMain" function.

            public  asmMain
asmMain     proc
            push    rbx
            push    rbp
            mov     rbp, rsp
<span epub:type="pagebreak" title="615" id="Page_615"/>            sub     rsp, 56         ; Shadow storage
            
            call    print
            byte    "Starting", nl, 0
            
            mov     rcx, 4000000000 ; 4,000,000,000
            lea     rdx, alignedData
rptLp:      mov     rbx, 15
rptLp2:
            movups  xmm0, xmmword ptr [rdx + rbx * 1]
            movupd  xmm0, real8 ptr   [rdx + rbx * 1]
            movdqu  xmm0, xmmword ptr [rdx + rbx * 1]
            vmovups ymm0, ymmword ptr [rdx + rbx * 1]
            vmovupd ymm0, ymmword ptr [rdx + rbx * 1]
            vmovdqu ymm0, ymmword ptr [rdx + rbx * 1]
            vmovups zmm0, zmmword ptr [rdx + rbx * 1]
            vmovupd zmm0, zmmword ptr [rdx + rbx * 1]
            dec     rbx
            jns     rptLp2

            dec     rcx
            jnz     rptLp
            
            call    print
            byte    "Done", nl, 0
             
allDone:    leave
            pop     rbx
            ret     ; Returns to caller
asmMain     endp
            end</code></pre>
<p class="CodeListingCaption"><a id="listing11-4">Listing 11-4</a>: Unaligned memory-access timing code</p>
<p>The code in <a href="#listing11-3">Listing 11-3</a> took about 1 minute and 7 seconds to execute on a 3GHz Xeon W CPU. The code in <a href="#listing11-4">Listing 11-4</a> took 1 minute and 55 seconds to execute on the same processor. As you can see, there is sometimes an advantage to accessing SIMD data on an aligned address boundary.</p>
<h3 id="h2-501089c11-0005">11.6.5	The (v)movlps and (v)movlpd Instructions</h3>
<p class="BodyFirst">The <code>(v)movl*</code> instructions and <code>(v)movh*</code> instructions (from the next section) might look like normal move instructions. Their behavior is similar to many other SSE/AVX move instructions. However, they were designed to support packing and unpacking floating-point vectors. Specifically, these instructions allow you to merge two pairs of single-precision or a pair of double-precision floating-point operands from two different sources into a single XMM register.</p>
<p>The <code>(v)movlps</code> instructions use the following syntax:</p>
<pre><code>movlps  <var>xmm</var><sub>dest</sub>, <var>mem</var><sub>64</sub>
movlps  <var>mem</var><sub>64</sub>,  <var>xmm</var><sub>src</sub>
vmovlps <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>, <var>mem</var><sub>64</sub>
vmovlps <var>mem</var><sub>64</sub>,  <var>xmm</var><sub>src</sub></code></pre>
<p><span epub:type="pagebreak" title="616" id="Page_616"/>The <code>movlps </code><var>xmm</var><span class="SubscriptLiteral">dest</span><code>, </code><var>mem</var><span class="SubscriptLiteral">64</span> form copies a pair of single-precision floating-point values into the two LO 32-bit lanes of a destination XMM register, as shown in <a href="#figure11-9" id="figureanchor11-9">Figure 11-9</a>. This instruction leaves the HO 64 bits unchanged.</p>
<figure>
<img src="image_fi/501089c11/f11009.png" alt="f11009" class=""/>
<figcaption><p><a id="figure11-9">Figure 11-9</a>: <span class="LiteralInCaption"><code>movlps</code></span> instruction</p></figcaption>
</figure>
<p>The <code>movlps </code><var>mem</var><span class="SubscriptLiteral">64</span><code>, </code><var>xmm</var><span class="SubscriptLiteral">src</span> form copies the LO 64 bits (the two LO single-precision lanes) from the XMM source register to the specified memory location. Functionally, this is equivalent to the <code>movq</code> or <code>movsd</code> instructions (as it copies 64 bits to memory), though this instruction might be slightly faster if the LO 64 bits of the XMM register actually contain two single-precision values (see <span class="xref" itemid="xref_target_“Performance Issues and the SIMD Move Instructions” on page 622">“Performance Issues and the SIMD Move Instructions” on page 622</span> for an explanation).</p>
<p>The <code>vmovlps</code> instruction has three operands: a destination XMM register, a source XMM register, and a source (64-bit) memory location. This instruction copies the two single-precision values from the memory location into the LO 64 bits of the destination XMM register. It copies the HO 64 bits of the source register (which also hold two single-precision values) into the HO 64 bits of the destination register. <a href="#figure11-10" id="figureanchor11-10">Figure 11-10</a> shows the operation. Note that this instruction merges the pair of operands with a single instruction.</p>
<figure>
<img src="image_fi/501089c11/f11010.png" alt="f11010" class=""/>
<figcaption><p><a id="figure11-10">Figure 11-10</a>: <span class="LiteralInCaption"><code>vmovlps</code></span> instruction</p></figcaption>
</figure>
<p>Like <code>movsd</code>, the <code>movlpd</code> (<em>move low packed double</em>) instruction copies the LO 64 bits (a double-precision floating-point value) of the source operand to the LO 64 bits of the destination operand. The difference is that the <code>movlpd</code> instruction doesn’t zero-extend the value when moving data from memory into an XMM register, whereas the <code>movsd</code> instruction will zero-extend the <span epub:type="pagebreak" title="617" id="Page_617"/>value into the upper 64 bits of the destination XMM register. (Neither the <code>movsd</code> nor <code>movlpd</code> will zero-extend when copying data between XMM registers; of course, zero extension doesn’t apply when storing data to memory.)<sup class="FootnoteReference"><a id="c11-footnoteref-4" href="#c11-footnote-4">4</a></sup></p>
<h3 id="h2-501089c11-0006">11.6.6	The movhps and movhpd Instructions</h3>
<p class="BodyFirst">The <code>movhps</code> and <code>movhpd</code> instructions move a 64-bit value (either two single-precision floats in the case of <code>movhps</code>, or a single double-precision value in the case of <code>movhpd</code>) into the HO quad word of a destination XMM register. <a href="#figure11-11" id="figureanchor11-11">Figure 11-11</a> shows the operation of the <code>movhps</code> instruction; <a href="#figure11-12" id="figureanchor11-12">Figure 11-12</a> shows the <code>movhpd</code> instruction.</p>
<figure>
<img src="image_fi/501089c11/f11011.png" alt="f11011" class=""/>
<figcaption><p><a id="figure11-11">Figure 11-11</a>: <span class="LiteralInCaption"><code>movhps</code></span> instruction</p></figcaption>
</figure>
<figure>
<img src="image_fi/501089c11/f11012.png" alt="f11012" class=""/>
<figcaption><p><a id="figure11-12">Figure 11-12</a>: <span class="LiteralInCaption"><code>movhpd</code></span> instruction</p></figcaption>
</figure>
<p>The <code>movhps</code> and <code>movhpd</code> instructions can also store the HO quad word of an XMM register into memory. The allowable syntax is shown here:</p>
<pre><code>movhps <var>xmm</var><span class="SubscriptLiteral">n</span>, <var>mem</var><sub>64</sub>
movhps <var>mem</var><sub>64</sub>, <var>xmm</var><span class="SubscriptLiteral">n</span>
movhpd <var>xmm</var><span class="SubscriptLiteral">n</span>, <var>mem</var><sub>64</sub>
movhpd <var>mem</var><sub>64</sub>, <var>xmm</var><span class="SubscriptLiteral">n</span></code></pre>
<p>These instructions do not affect bits 128 to 255 of the YMM registers (if present on the CPU).</p>
<p>You would normally use a <code>movlps</code> instruction followed by a <code>movhps</code> instruction to load four single-precision floating-point values into an XMM register, taking the floating-point values from two different data sources (similarly, you could use the <code>movlpd</code> and <code>movhpd</code> instructions to load a pair of double-precision values into a single XMM register from different sources). <span epub:type="pagebreak" title="618" id="Page_618"/>Conversely, you could also use this instruction to split a vector result in half and store the two halves in different data streams. This is probably the intended purpose of this instruction. Of course, if you can use it for other purposes, have at it.</p>
<p>MASM (version 14.15.26730.0, at least) seems to require <code>movhps</code> operands to be a 64-bit data type and does not allow <code>real4</code> operands.<sup class="FootnoteReference"><a id="c11-footnoteref-5" href="#c11-footnote-5">5</a></sup> Therefore, you may have to explicitly coerce an array of two <code>real4</code> values with <code>qword ptr</code> when using this instruction:</p>
<pre><code>r4m         real4   1.0, 2.0, 3.0, 4.0
r8m         real8   1.0, 2.0
              .
              .
              .
            movhps  xmm0, qword ptr r4m2
            movhpd  xmm0, r8m</code></pre>
<h3 id="h2-501089c11-0007">11.6.7	The vmovhps and vmovhpd Instructions</h3>
<p class="BodyFirst">Although the AVX instruction extensions provide <code>vmovhps</code> and <code>vmovhpd</code> instructions, they are not a simple extension of the SSE <code>movhps</code> and <code>movhpd</code> instructions. The syntax for these instructions is as follows:</p>
<pre><code>vmovhps <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>, <var>mem</var><sub>64</sub>
vmovhps <var>mem</var><sub>64</sub>,  <var>xmm</var><sub>src</sub>
vmovhpd <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>, <var>mem</var><sub>64</sub>
vmovhpd <var>mem</var><sub>64</sub>,  <var>xmm</var><sub>src</sub></code></pre>
<p>The instructions that store data into a 64-bit memory location behave similarly to the <code>movhps</code> and <code>movhpd</code> instructions. The instructions that load data into an XMM register have two source operands. They load a full 128 bits (four single-precision values or two double-precision values) into the destination XMM register. The HO 64 bits come from the memory operand; the LO 64 bits come from the LO quad word of the source XMM register, as <a href="#figure11-13" id="figureanchor11-13">Figure 11-13</a> shows. These instructions also zero-extend the value into the upper 128 bits of the (overlaid) YMM register.</p>
<figure>
<img src="image_fi/501089c11/f11013.png" alt="f11013" class=""/>
<figcaption><p><a id="figure11-13">Figure 11-13</a>: <span class="LiteralInCaption"><code>vmovhpd</code></span> and <span class="LiteralInCaption"><code>vmovhps</code></span> instructions</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="619" id="Page_619"/>Unlike for the <code>movhps</code> instruction, MASM properly accepts <code>real4</code> source operands for the <code>vmovhps</code> instruction:</p>
<pre><code>r4m         real4   1.0, 2.0, 3.0, 4.0
r8m         real8   1.0, 2.0
              .
              .
              .
            vmovhps xmm0, xmm1, r4m
            vmovhpd xmm0, xmm1, r8m</code></pre>
<h3 id="h2-501089c11-0008">11.6.8	The movlhps and vmovlhps Instructions</h3>
<p class="BodyFirst">The <code>movlhps</code> instruction moves a pair of 32-bit single-precision floating-point values from the LO qword of the source XMM register into the HO 64 bits of a destination XMM register. It leaves the LO 64 bits of the destination register unchanged. If the destination register is on a CPU that supports 256-bit AVX registers, this instruction also leaves the HO 128 bits of the overlaid YMM register unchanged.</p>
<p>The syntax for these instructions is as follows:</p>
<pre><code>movlhps  <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>
vmovlhps <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub></code></pre>
<p>You cannot use this instruction to move data between memory and an XMM register; it transfers data only between XMM registers. No double-precision version of this instruction exists.</p>
<p>The <code>vmovlhps</code> instruction is similar to <code>movlhps</code>, with the following differences:</p>
<ul>
<li><code>vmovlhps</code> requires three operands: two source XMM registers and a destination XMM register.</li>
<li><code>vmovlhps</code> copies the LO quad word of the first source register into the LO quad word of the destination register.</li>
<li><code>vmovlhps</code> copies the LO quad word of the second source register into bits 64 to 127 of the destination register.</li>
<li><code>vmovlhps</code> zero-extends the result into the upper 128 bits of the overlaid YMM register.</li>
</ul>
<p>There is no <code>vmovlhpd</code> instruction.</p>
<h3 id="h2-501089c11-0009">11.6.9	The movhlps and vmovhlps Instructions</h3>
<p class="BodyFirst">The <code>movhlps</code> instruction has the following syntax:</p>
<pre><code>movhlps <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub></code></pre>
<p><span epub:type="pagebreak" title="620" id="Page_620"/>The <code>movhlps</code> instruction copies the pair of 32-bit single-precision floating-point values from the HO qword of the source operand to the LO qword of the destination register, leaving the HO 64 bits of the destination register unchanged (this is the converse of <code>movlhps</code>). This instruction copies data only between XMM registers; it does not allow a memory operand.</p>
<p>The <code>vmovhlps</code> instruction requires three XMM register operands; here is its syntax:</p>
<pre><code>vmovhlps <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub></code></pre>
<p>This instruction copies the HO 64 bits of the first source register into the HO 64 bits of the destination register, copies the HO 64 bits of the second source register into bits 0 to 63 of the destination register, and finally, zero-extends the result into the upper bits of the overlaid YMM register.</p>
<p>There are no <code>movhlpd</code> or <code>vmovhlpd</code> instructions.</p>
<h3 id="h2-501089c11-0010">11.6.10	The (v)movshdup and (v)movsldup Instructions</h3>
<p class="BodyFirst">The <code>movshdup</code> instruction moves the two odd-index single-precision floating-point values from the source operand (memory or XMM register) and duplicates each element into the destination XMM register, as shown in <a href="#figure11-14" id="figureanchor11-14">Figure 11-14</a>. </p>
<figure>
<img src="image_fi/501089c11/f11014.png" alt="f11014" class=""/>
<figcaption><p><a id="figure11-14">Figure 11-14</a>: <span class="LiteralInCaption"><code>movshdup</code></span> and <span class="LiteralInCaption"><code>vmovshdup</code></span> instructions</p></figcaption>
</figure>
<p>This instruction ignores the single-precision floating-point values at even-lane indexes into the XMM register. The <code>vmovshdup</code> instruction works the same way but on YMM registers, copying four single-precision values rather than two (and, of course, zeroing the HO bits). The syntax for these instructions is shown here:</p>
<pre><code>movshdup  <var>xmm</var><sub>dest</sub>, <var>mem</var><sub>128</sub>/<var>xmm</var><sub>src</sub>
vmovshdup <var>xmm</var><sub>dest</sub>, <var>mem</var><sub>128</sub>/<var>xmm</var><sub>src</sub>
vmovshdup <var>ymm</var><sub>dest</sub>, <var>mem</var><sub>256</sub>/<var>ymm</var><sub>src</sub></code></pre>
<p>The <code>movsldup</code> instruction works just like the <code>movshdup</code> instruction, except it copies and duplicates the two single-precision values at even indexes in <span epub:type="pagebreak" title="621" id="Page_621"/>the source XMM register to the destination XMM register. Likewise, the <code>vmovsldup</code> instruction copies and duplicates the four double-precision values in the source YMM register at even indexes, as shown in <a href="#figure11-15" id="figureanchor11-15">Figure 11-15</a>.</p>
<figure>
<img src="image_fi/501089c11/f11015.png" alt="f11015" class=""/>
<figcaption><p><a id="figure11-15">Figure 11-15</a>: <span class="LiteralInCaption"><code>movsldup</code></span> and <span class="LiteralInCaption"><code>vmovsldup</code></span> instructions</p></figcaption>
</figure>
<p>The syntax is as follows:</p>
<pre><code>movsldup  <var>xmm</var><sub>dest</sub>, <var>mem</var><sub>128</sub>/<var>xmm</var><sub>src</sub>
vmovsldup <var>xmm</var><sub>dest</sub>, <var>mem</var><sub>128</sub>/<var>xmm</var><sub>src</sub>
vmovsldup <var>ymm</var><sub>dest</sub>, <var>mem</var><sub>256</sub>/<var>ymm</var><sub>src</sub></code></pre>
<h3 id="h2-501089c11-0011">11.6.11	The (v)movddup Instruction</h3>
<p class="BodyFirst">The <code>movddup</code> instruction copies and duplicates a double-precision value from the LO 64 bits of an XMM register or a 64-bit memory location into the LO 64 bits of a destination XMM register; then it also duplicates this value into bits 64 to 127 of that same destination register, as shown in <a href="#figure11-16" id="figureanchor11-16">Figure 11-16</a>.</p>
<figure>
<img src="image_fi/501089c11/f11016.png" alt="f11016" class=""/>
<figcaption><p><a id="figure11-16">Figure 11-16</a>: <span class="LiteralInCaption"><code>movddup</code></span> instruction behavior</p></figcaption>
</figure>
<p>This instruction does not disturb the HO 128 bits of a YMM register (if applicable). The syntax for this instruction is as follows:</p>
<pre><code>movddup <var>xmm</var><sub>dest</sub>, <var>mem</var><sub>64</sub>/<var>xmm</var><sub>src</sub></code></pre>
<p>The <code>vmovddup</code> instruction operates on an XMM or a YMM destination register and an XMM or a YMM source register or 128- or 256-bit memory location. The 128-bit version works just like the <code>movddup</code> instruction except it zeroes the HO bits of the destination YMM register. The 256-bit version copies a pair of double-precision values at even indexes (0 and 2) in the <span epub:type="pagebreak" title="622" id="Page_622"/>source value to their corresponding indexes in the destination YMM register and duplicates those values at the odd indexes in the destination, as <a href="#figure11-17" id="figureanchor11-17">Figure 11-17</a> shows.</p>
<figure>
<img src="image_fi/501089c11/f11017.png" alt="f11017" class=""/>
<figcaption><p><a id="figure11-17">Figure 11-17</a>: <span class="LiteralInCaption"><code>vmovddup</code></span> instruction behavior</p></figcaption>
</figure>
<p>Here is the syntax for this instruction:</p>
<pre><code>movddup  <var>xmm</var><sub>dest</sub>, <var>mem</var><sub>64</sub>/<var>xmm</var><sub>src</sub>
vmovddup <var>ymm</var><sub>dest</sub>, <var>mem</var><sub>256</sub>/<var>ymm</var><sub>src</sub></code></pre>
<h3 id="h2-501089c11-0012">11.6.12	The (v)lddqu Instruction</h3>
<p class="BodyFirst">The <code>(v)lddqu</code> instruction is operationally identical to <code>(v)movdqu</code>. You can sometimes use this instruction to improve performance if the (memory) source operand is not aligned properly and crosses a cache line boundary in memory. For more details on this instruction and its performance limitations, refer to the Intel or AMD documentation (specifically, the optimization manuals).</p>
<p>These instructions always take the following form:</p>
<pre><code>lddqu  <var>xmm</var><sub>dest</sub>, <var>mem</var><sub>128</sub>
vlddqu <var>xmm</var><sub>dest</sub>, <var>mem</var><sub>128</sub>
vlddqu <var>ymm</var><sub>dest</sub>, <var>mem</var><sub>256</sub></code></pre>
<h3 id="h2-501089c11-0013">11.6.13	Performance Issues and the SIMD Move Instructions</h3>
<p class="BodyFirst">When you look at the SSE/AVX instructions’ semantics at the programming model level, you might question why certain instructions appear in the instruction set. For example, the <code>movq</code>, <code>movsd</code>, and <code>movlps</code> instructions can all load 64 bits from a memory location into the LO 64 bits of an XMM register. Why bother doing this? Why not have a single instruction that copies the 64 bits from a quad word in memory to the LO 64 bits of an XMM register (be it a 64-bit integer, a pair of 32-bit integers, a 64-bit double-precision floating-point value, or a pair of 32-bit single-precision floating-point values)? The answer lies in the term <em>microarchitecture</em>.</p>
<p>The x86-64 <em>macroarchitecture</em> is the programming model that a software engineer sees. In the macroarchitecture, an XMM register is a 128-bit resource that, at any given time, could hold a 128-bit array of bits (or an integer), a pair of 64-bit integer values, a pair of 64-bit double-precision floating-point values, a set of four single-precision floating-point values, <span epub:type="pagebreak" title="623" id="Page_623"/>a set of four double-word integers, eight words, or 16 bytes. All these data types overlay one another, just like the 8-, 16-, 32-, and 64-bit general-purpose registers overlay one another (this is known as <em>aliasing</em>). If you load two double-precision floating-point values into an XMM register and then modify the (integer) word at bit positions 0 to 15, you’re also changing those same bits (0 to 15) in the double-precision value in the LO qword of the XMM register. The semantics of the x86-64 programming model require this.</p>
<p>At the microarchitectural level, however, there is no requirement that the CPU use the same physical bits in the CPU for integer, single-precision, and double-precision values (even when they are aliased to the same register). The microarchitecture could set aside a separate set of bits to hold integers, single-precision, and double-precision values for a single register. So, for example, when you use the <code>movq</code> instruction to load 64 bits into an XMM register, that instruction might actually copy the bits into the underlying integer register (without affecting the single-precision or double-precision subregisters). Likewise, <code>movlps</code> would copy a pair of single-precision values into the single-precision register, and <code>movsd</code> would copy a double-precision value into the double-precision register (<a href="#figure11-18" id="figureanchor11-18">Figure 11-18</a>). These separate subregisters (integer, single-precision, and double-precision) could be connected directly to the arithmetic or logical unit that handles their specific data types, making arithmetic and logical operations on those subregisters more efficient. As long as the data is sitting in the appropriate subregister, everything works smoothly.</p>
<figure>
<img src="image_fi/501089c11/f11018.png" alt="f11018" class=""/>
<figcaption><p><a id="figure11-18">Figure 11-18</a>: Register aliasing at the microarchitectural level</p></figcaption>
</figure>
<p>However, what happens if you use <code>movq</code> to load a pair of single-precision floating-point values into an XMM register and then try to perform a single-precision vector operation on those two values? At the macroarchitectural level, the two single-precision values are sitting in the appropriate bit positions of the XMM register, so this has to be a legal operation. At the microarchitectural level, however, those two single-precision floating-point values are sitting in the integer subregister, not the single-precision subregister. The underlying microarchitecture has to note that the values are in the wrong subregister and move them to the appropriate (single-precision) subregister before performing the single-precision arithmetic or logical operation. This may introduce a slight delay (while the microarchitecture moves the data around), which is why you should always pick the appropriate move instructions for your data types.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<span epub:type="pagebreak" title="624" id="Page_624"/><h2><span class="NoteHead">Note</span></h2>
<p>	There is no guarantee that your programs will run faster by using the appropriate instructions for your data type, but at least they won’t run slower.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<h3 id="h2-501089c11-0014">11.6.14	Some Final Comments on the SIMD Move Instructions</h3>
<p class="BodyFirst">The SIMD data movement instructions are a confusing bunch. Their syntax is inconsistent, many instructions duplicate the actions of other instructions, and they have some perplexing irregularity issues. Someone new to the x86-64 instruction set might ask, “Why was the instruction set designed this way?” Why, indeed? </p>
<p>The answer to that question is historical. The SIMD instructions did not exist on the earliest x86 CPUs. Intel added the MMX instruction set to the Pentium-series CPUs. At that time (the early 1990s), current technology allowed Intel to add only a few additional instructions, and the MMX registers were limited to 64 bits in size. Furthermore, software engineers and computer systems designers were only beginning to explore the multimedia capabilities of modern computers, so it wasn’t entirely clear which instructions (and data types) were necessary to support the type of software we see several decades later. As a result, the earliest SIMD instructions and data types were limited in scope.</p>
<p>As time passed, CPUs gained additional silicon resources, and software/systems engineers discovered new uses for computers (and new algorithms to run on those computers), so Intel (and AMD) responded by adding new SIMD instructions to support these more modern multimedia applications. The original MMX instructions, for example, supported only integer data types, so Intel added floating-point support in the SSE instruction set, because multimedia applications needed real data types. Then Intel extended the integer types from 64 bits to 128, 256, and even 512 bits. With each extension, Intel (and AMD) had to retain the older instruction set extensions in order to allow preexisting software to run on the new CPUs. </p>
<p>As a result, the newer instruction sets kept piling on new instructions that did the same work as the older ones (with some additional capabilities). This is why instructions like <code>movaps</code> and <code>vmovaps</code> have considerable overlap in their functionality. If the CPU resources had been available earlier (for example, to put 256-bit YMM registers on the CPU), there would have been almost no need for the <code>movaps</code> instruction—the <code>vmovaps</code> could have done all the work.<sup class="FootnoteReference"><a id="c11-footnoteref-6" href="#c11-footnote-6">6</a></sup></p>
<p>In theory, we could create an architecturally elegant variant of the x86-64 by starting over from scratch and designing a minimal instruction set that handles all the activities of the current x86-64 without all the kruft and kludges present in the existing instruction set. However, such a CPU would lose the primary advantage of the x86-64: the ability to run decades of software written for the Intel architecture. The cost of being able to run all this old software is that assembly language programmers (and compiler writers) have to deal with all these irregularities in the instruction set.</p>
<h2 id="h1-501089c11-0007"><span epub:type="pagebreak" title="625" id="Page_625"/>	11.7	The Shuffle and Unpack Instructions</h2>
<p class="BodyFirst">The SSE/AVX <em>shuffle and unpack instructions</em> are variants of the move instructions. In addition to moving data around, these instructions can also rearrange the data appearing in different lanes of the XMM and YMM registers.</p>
<h3 id="h2-501089c11-0015">11.7.1	The (v)pshufb Instructions</h3>
<p class="BodyFirst">The <code>pshufb</code> instruction was the first packed byte shuffle SIMD instruction (it first appeared with the MMX instruction set). Because of its origin, its syntax and behavior are a bit different from the other shuffle instructions in the instruction set. The syntax is the following:</p>
<pre><code>pshufb <var>xmm</var><sub>dest</sub>, <var>xmm</var>/<code>mem</code><sub>128</sub></code></pre>
<p>The first (destination) operand is an XMM register whose byte lanes <code>pshufb</code> will shuffle (rearrange). The second operand (either an XMM register or a 128-bit oword memory location) is an array of 16 byte values holding indexes that control the shuffle operation. If the second operand is a memory location, that oword value must be aligned on a 16-byte boundary.</p>
<p>Each byte (lane) in the second operand selects a value for the corresponding byte lane in the first operand, as shown in <a href="#figure11-19" id="figureanchor11-19">Figure 11-19</a>.</p>
<figure>
<img src="image_fi/501089c11/f11019.png" alt="f11019" class=""/>
<figcaption><p><a id="figure11-19">Figure 11-19</a>: Lane index correspondence for <span class="LiteralInCaption"><code>pshufb</code></span> instruction</p></figcaption>
</figure>
<p>The 16-byte indexes in the second operand each take the form shown in <a href="#figure11-20" id="figureanchor11-20">Figure 11-20</a>. </p>
<figure>
<img src="image_fi/501089c11/f11020.png" alt="f11020" class=""/>
<figcaption><p><a id="figure11-20">Figure 11-20</a>: <var>phsufb</var> byte index</p></figcaption>
</figure>
<p>The <code>pshufb</code> instruction ignores bits 4 to 6 in an index byte. Bit 7 is the clear bit; if this bit contains a 1, the <code>pshufb</code> instruction ignores the lane index bits and stores a 0 into the corresponding byte in XMM<sub>dest</sub>. If the clear bit contains a 0, the <code>pshufb</code> instruction does a shuffle operation.</p>
<p>The <code>pshufb</code> shuffle operation takes place on a lane-by-lane basis. The instruction first makes a temporary copy of XMM<sub>dest</sub>. Then for each index byte (whose HO bit is 0), the <code>pshufb</code> copies the lane specified by the LO 4 bits of the index from the XMM<sub>dest</sub> lane that matches the index’s lane, as <span epub:type="pagebreak" title="626" id="Page_626"/>shown in <a href="#figure11-21" id="figureanchor11-21">Figure 11-21</a>. In this example, the index appearing in lane 6 contains the value 00000011b. This selects the value in lane 3 of the temporary (original XMM<sub>dest</sub>) value and copies it to lane 6 of XMM<sub>dest</sub>. The <code>pshufb</code> instruction repeats this operation for all l6 lanes.</p>
<figure>
<img src="image_fi/501089c11/f11021.png" alt="f11021" class=""/>
<figcaption><p><a id="figure11-21">Figure 11-21</a>: Shuffle operation</p></figcaption>
</figure>
<p>The AVX instruction set extensions introduced the <code>vpshufb</code> instruction. Its syntax is the following:</p>
<pre><code>vpshufb <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>, <var>xmm</var><sub>index</sub>/<var>mem</var><sub>128</sub>
vpshufb <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src</sub>, <var>ymm</var><sub>index</sub>/<var>mem</var><sub>256</sub></code></pre>
<p>The AVX variant adds a source register (rather than using XMM<sub>dest</sub> as both the source and destination registers), and, rather than creating a temporary copy of XMM<sub>dest </sub>prior to the operation and picking the values from that copy, the <code>vpshufb</code> instructions select the source bytes from the XMM<sub>src</sub> register. Other than that, and the fact that these instructions zero the HO bits of YMM<sub>dest</sub>, the 128-bit variant operates identically to the SSE <code>pshufb</code> instruction. </p>
<p>The AVX instruction allows you to specify 256-bit YMM registers in addition to 128-bit XMM registers.<sup class="FootnoteReference"><a id="c11-footnoteref-7" href="#c11-footnote-7">7</a></sup> </p>
<h3 id="h2-501089c11-0016">11.7.2	The (v)pshufd Instructions</h3>
<p class="BodyFirst">The SSE extensions first introduced the <code>pshufd</code> instruction. The AVX extensions added the <code>vpshufd</code> instruction. These instructions shuffle dwords in XMM and YMM registers (<em>not</em> double-precision values) similarly to the <code>(v)pshufb</code> instructions. However, the shuffle index is specified differently from <code>(v)pshufb</code>. The syntax for the <code>(v)pshufd</code> instructions is as follows:</p>
<pre><code>pshufd  <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>, <var>imm</var><sub>8</sub>
vpshufd <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>, <var>imm</var><sub>8</sub>
vpshufd <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src</sub>/<var>me</var>m<sub>256</sub>, <var>imm</var><sub>8</sub></code></pre>
<p><span epub:type="pagebreak" title="627" id="Page_627"/>The first operand (XMM<sub>dest </sub>or YMM<sub>dest</sub>) is the destination operand where the shuffled values will be stored. The second operand is the source from which the instruction will select the double words to place in the destination register; as usual, if this is a memory operand, you must align it on the appropriate (16- or 32-byte) boundary. The third operand is an 8-bit immediate value that specifies the indexes for the double words to select from the source operand. </p>
<p>For the <code>(v)pshufd</code> instructions with an XMM<sub>dest</sub> operand, the imm<sub>8</sub> operand has the encoding shown in <a href="#table11-3" id="tableanchor11-3">Table 11-3</a>. The value in bits 0 to 1 selects a particular dword from the source operand to place in dword 0 of the XMM<sub>dest</sub> operand. The value in bits 2 to 3 selects a dword from the source operand to place in dword 1 of the XMM<sub>dest</sub> operand. The value in bits 4 to 5 selects a dword from the source operand to place in dword 2 of the XMM<sub>dest</sub> operand. Finally, the value in bits 6 to 7 selects a dword from the source operand to place in dword 3 of the XMM<sub>dest</sub> operand.</p>
<figure>
<figcaption class="TableTitle"><p><a id="table11-3">Table 11-3</a>: <code>(v)pshufd</code> imm<sub>8 </sub>Operand Values</p></figcaption>
<table id="table-501089c11-0003" border="1">
<thead>
<tr>
<td><b>Bit positions</b></td>
<td><b>Destination lane </b></td>
</tr>
</thead>
<tbody>
<tr>
<td>0 to 1</td>
<td>0</td>
</tr>
<tr>
<td>2 to 3</td>
<td>1</td>
</tr>
<tr>
<td>4 to 5</td>
<td>2</td>
</tr>
<tr>
<td>6 to 7</td>
<td>3</td>
</tr>
</tbody>
</table>
</figure>
<p>The difference between the 128-bit <code>pshufd</code> and <code>vpshufd</code> instructions is that <code>pshufd</code> leaves the HO 128 bits of the underlying YMM register unchanged and <code>vpshufd</code> zeroes the HO 128 bits of the underlying YMM register.</p>
<p>The 256-bit variant of <code>vpshufd</code> (when using YMM registers as the source and destination operands) still uses an 8-bit immediate operand as the index value. Each 2-bit index value manipulates two dword values in the YMM registers. Bits 0 to 1 control dwords 0 and 4, bits 2 to 3 control dwords 1 and 5, bits 4 to 5 control dwords 2 and 6, and bits 6 to 7 control dwords 3 and 7, as shown in <a href="#table11-4" id="tableanchor11-4">Table 11-4</a>.</p>
<figure>
<figcaption class="TableTitle"><p><a id="table11-4">Table 11-4</a>: Double-Word Transfers for <code>vpshufd</code> YMM<sub>dest</sub>, YMM<sub>src</sub>/mem<sub>src</sub>, imm<sub>8</sub></p></figcaption>
<table id="table-501089c11-0004" border="1">
<thead>
<tr>
<td><b>Index</b></td>
<td><b>YMM/mem<sub>src </sub>[index] copied into</b></td>
<td><b>YMM/mem<sub>src </sub>[index + 4] copied into</b></td>
</tr>
</thead>
<tbody>
<tr>
<td>Bits 0 to 1 of imm<sub>8</sub></td>
<td>YMM<sub>dest</sub>[0] </td>
<td>YMM<sub>dest</sub>[4] </td>
</tr>
<tr>
<td>Bits 2 to 3 of imm<sub>8</sub></td>
<td>YMM<sub>dest</sub>[1] </td>
<td>YMM<sub>dest</sub>[5] </td>
</tr>
<tr>
<td>Bits 4 to 5 of imm<sub>8</sub></td>
<td>YMM<sub>dest</sub>[2] </td>
<td>YMM<sub>dest</sub>[6] </td>
</tr>
<tr>
<td>Bits 6 to 7 of imm<sub>8</sub></td>
<td>YMM<sub>dest</sub>[3] </td>
<td>YMM<sub>dest</sub>[7] </td>
</tr>
</tbody>
</table>
</figure>
<p>The 256-bit version is slightly less flexible as it copies two dwords at a time, rather than one. It processes the LO 128 bits exactly the same way as <span epub:type="pagebreak" title="628" id="Page_628"/>the 128-bit version of the instruction; it also copies the corresponding lanes in the upper 128 bits of the source to the YMM destination register by using the same shuffle pattern. Unfortunately, you can’t independently control the HO and LO halves of the YMM register by using the <code>vpshufd</code> instruction. If you really need to shuffle dwords independently, you can use <code>vshufb</code> with appropriate indexes that copy 4 bytes (in place of a single dword).</p>
<h3 id="h2-501089c11-0017">11.7.3	The (v)pshuflw and (v)pshufhw Instructions</h3>
<p class="BodyFirst">The <code>pshuflw</code> and <code>vpshuflw</code> and the <code>pshufhw</code> and <code>vpshufhw</code> instructions provide support for 16-bit word shuffles within an XMM or a YMM register. The syntax for these instructions is the following:</p>
<pre><code>pshuflw  <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>, <var>imm</var><sub>8</sub>
pshufhw  <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>, <var>imm</var><sub>8</sub>

vpshuflw <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>, <var>imm</var><sub>8</sub>
vpshufhw <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>, <var>imm</var><sub>8</sub>

vpshuflw <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src</sub>/<var>mem</var><sub>256</sub>, <var>imm</var><sub>8</sub>
vpshufhw <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src</sub>/<var>mem</var><sub>256</sub>, <var>imm</var><sub>8</sub></code></pre>
<p>The 128-bit <code>lw</code> variants copy the HO 64 bits of the source operand to the same positions in the XMM<sub>dest</sub> operand. Then they use the index (imm<sub>8</sub>) operand to select word lanes 0 to 3 in the LO qword of the XMM<sub>src</sub>/mem<sub>128</sub> operand to move to the LO 4 lanes of the destination operand. For example, if the LO 2 bits of imm<sub>8</sub> are 10b, then the <code>pshuflw</code> instruction copies lane 2 from the source into lane 0 of the destination operand (<a href="#figure11-22" id="figureanchor11-22">Figure 11-22</a>). Note that <code>pshuflw</code> does not modify the HO 128 bits of the overlaid YMM register, whereas <code>vpshuflw</code> zeroes those HO bits. </p>
<figure>
<img src="image_fi/501089c11/f11022.png" alt="f11022" class=""/>
<figcaption><p><a id="figure11-22">Figure 11-22</a>: <span class="LiteralInCaption"><code>(v)pshuflw </code></span><var>xmm</var><span class="LiteralInCaption"><code>, </code></span><var>xmm</var><span class="LiteralInCaption"><code>/</code></span><var>mem</var><span class="LiteralInCaption"><code>, </code></span><var>imm</var><span class="SubscriptLiteral">8</span> operation</p></figcaption>
</figure>
<p>The 256-bit <code>vpshuflw</code> instruction (with a YMM destination register) copies two pairs of words at a time—one pair in the HO 128 bits and one pair in the LO 128 bits of the YMM destination register and 256-bit source locations, as shown in <a href="#figure11-23" id="figureanchor11-23">Figure 11-23</a>. The index (imm<sub>8</sub>) selection is the same for the LO and HO 128 bits.</p>
<span epub:type="pagebreak" title="629" id="Page_629"/><figure>
<img src="image_fi/501089c11/f11023.png" alt="f11023" class=""/>
<figcaption><p><a id="figure11-23">Figure 11-23</a>: <code>vpshuflw</code> <var>ymm</var><span class="LiteralInCaption"><code>, </code></span><var>ymm</var><span class="LiteralInCaption"><code>/</code></span><var>mem</var><span class="LiteralInCaption"><code>, </code></span><var>imm</var><span class="SubscriptLiteral">8</span> operation</p></figcaption>
</figure>
<p>The 128-bit <code>hw</code> variants copy the LO 64 bits of the source operand to the same positions in the destination operand. Then they use the index operand to select words 4 to 7 (indexed as 0 to 3) in the 128-bit source operand to move to the HO four word lanes of the destination operand (<a href="#figure11-24" id="figureanchor11-24">Figure 11-24</a>).</p>
<figure>
<img src="image_fi/501089c11/f11024.png" alt="f11024" class=""/>
<figcaption><p><a id="figure11-24">Figure 11-24</a>: <span class="LiteralInCaption"><code>(v)pshufhw</code></span> operation</p></figcaption>
</figure>
<p>The 256-bit <code>vpshufhw</code> instruction (with a YMM destination register) copies two pairs of words at a time—one in the HO 128 bits and one in the LO 128 bits of the YMM destination register and 256-bit source locations, as shown in <a href="#figure11-25" id="figureanchor11-25">Figure 11-25</a>.</p>
<figure>
<img src="image_fi/501089c11/f11025.png" alt="f11025" class=""/>
<figcaption><p><a id="figure11-25">Figure 11-25</a>: <span class="LiteralInCaption"><code>vpshufhw</code></span> operation</p></figcaption>
</figure>
<h3 id="h2-501089c11-0018"><span epub:type="pagebreak" title="630" id="Page_630"/>11.7.4	The shufps and shufpd Instructions</h3>
<p class="BodyFirst">The shuffle instructions (<code>shufps</code> and <code>shufpd</code>) extract single- or double-precision values from the source operands and place them in specified positions in the destination operand. The third operand, an 8-bit immediate value, selects which values to extract from the source to move into the destination register. The syntax for these two instructions is as follows:</p>
<pre><code>shufps <var>xmm</var><sub>src1/dest</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>, <var>imm</var><sub>8</sub>
shufpd <var>xmm</var><sub>src1/dest</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>, <var>imm</var><sub>8</sub></code></pre>
<p>For the <code>shufps</code> instruction, the second source operand is an 8-bit immediate value that is actually a four-element array of 2-bit values. </p>
<p>imm<sub>8</sub> bits 0 and 1 select a single-precision value from one of the four lanes in the XMM<sub>src1/dest</sub> operand to store into lane 0 of the destination operation. Bits 2 and 3 select a single-precision value from one of the four lanes in the XMM<sub>src1/dest</sub> operand to store into lane 1 of the destination operation (the destination operand is also XMM<sub>src1/dest</sub>).</p>
<p>imm<sub>8</sub> bits 4 and 5 select a single-precision value from one of the four lanes in the XMM<sub>src2</sub>/mem<sub>src2</sub> operand to store into lane 2 of the destination operation. Bits 6 and 7 select a single-precision value from one of the four lanes in the XMM<sub>src2</sub>/mem<sub>src2</sub> operand to store into lane 3 of the destination operation.</p>
<p><a href="#figure11-26" id="figureanchor11-26">Figure 11-26</a> shows the operation of the <code>shufps</code> instruction.</p>
<figure>
<img src="image_fi/501089c11/f11026.png" alt="f11026" class=""/>
<figcaption><p><a id="figure11-26">Figure 11-26</a>: <span class="LiteralInCaption"><code>shufps</code></span> operation</p></figcaption>
</figure>
<p>For example, the instruction</p>
<pre><code>shufps xmm0, xmm1, 0E4h  ; 0E4h = 11 10 01 00</code></pre>
<p class="BodyContinued">loads XMM0 with the following single-precision values:</p>
<ul>
<li><span epub:type="pagebreak" title="631" id="Page_631"/>XMM0[0 to 31] from XMM0[0 to 32]</li>
<li>XMM0[32 to 63] from XMM0[32 to 63]</li>
<li>XMM0[64 to 95] from XMM1[63 to 95]</li>
<li>XMM0[96 to 127] from XMM1[96 to 127]</li>
</ul>
<p>If the second operand (XMM<sub>src2</sub>/mem<sub>src2</sub>) is the same as the first operand (XMM<sub>src1/dest</sub>), it’s possible to rearrange the four single-precision values in the XMM<sub>dest</sub> register (which is probably the source of the instruction name <em>shuffle</em>).</p>
<p>The <code>shufpd</code> instruction works similarly, shuffling double-precision values. As there are only two double-precision values in an XMM register, it takes only a single bit to choose between the values. Likewise, as there are only two double-precision values in the destination register, the instruction requires only two (single-bit) array elements to choose the destination. As a result, the third operand, the imm<sub>8</sub> value, is actually just a 2-bit value; the instruction ignores bits 2 to 7 in the imm<sub>8</sub> operand. Bit 0 of the imm<sub>8</sub> operand selects either lane 0 and bits 0 to 63 (if it is 0) or lane 1 and bits 64 to 127 (if it is 1) from the XMM<sub>src1/dest</sub> operand to place into lane 0 and bits 0 to 63 of XMM<sub>dest</sub>. Bit 1 of the imm<sub>8</sub> operand selects either lane 0 and bits 0 to 63 (if it is 0) or lane 1 and bits 64 to 127 (if it is 1) from the XMM<sub>src</sub>/mem<sub>128</sub> operand to place into lane 1 and bits 64 to 127 of XMM<sub>dest</sub>. <a href="#figure11-27" id="figureanchor11-27">Figure 11-27</a> shows this operation.</p>
<figure>
<img src="image_fi/501089c11/f11027.png" alt="f11027" class=""/>
<figcaption><p><a id="figure11-27">Figure 11-27</a>: <span class="LiteralInCaption"><code>shufpd</code></span> operation</p></figcaption>
</figure>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p>	These instructions do not modify the upper 128 bits of any overlaid YMM register. </p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<h3 id="h2-501089c11-0019"><span epub:type="pagebreak" title="632" id="Page_632"/>11.7.5	The vshufps and vshufpd Instructions</h3>
<p class="BodyFirst">The <code>vshufps</code> and <code>vshufpd</code> instructions are similar to <code>shufps</code> and <code>shufpd</code>. They allow you to shuffle the values in 128-bit XMM registers or 256-bit YMM registers.<sup class="FootnoteReference"><a id="c11-footnoteref-8" href="#c11-footnote-8">8</a></sup> The <code>vshufps</code> and <code>vshufpd</code> instructions have four operands: a destination XMM or YMM register, two source operands (src<sub>1</sub> must be an XMM or a YMM register, and src<sub>2</sub> can be an XMM or a YMM register or a 128- or 256-bit memory location), and an imm<sub>8</sub> operand. Their syntax is the following:</p>
<pre><code>vshufps <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>, <var>imm</var><sub>8</sub>
vshufpd <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>, <var>imm</var><sub>8</sub>

vshufps <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub>, <var>imm</var><sub>8</sub>
vshufpd <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub>, <var>imm</var><sub>8</sub></code></pre>
<p>Whereas the SSE shuffle instructions use the destination register as an implicit source operand, the AVX shuffle instructions allow you to specify explicit destination and source operands (they can all be different, or all the same, or any combination thereof).</p>
<p>For the 256-bit <code>vshufps</code> instructions, the imm<sub>8</sub> operand is an array of four 2-bit values (bits 0:1, 2:3, 4:5, and 6:7). These 2-bit values select one of four single-precision values from the source locations, as described in <a href="#table11-5" id="tableanchor11-5">Table 11-5</a>. </p>
<figure>
<figcaption class="TableTitle"><p><a id="table11-5">Table 11-5</a>: <code>vshufps</code> Destination Selection</p></figcaption>
<table id="table-501089c11-0005" border="1">
<thead>
<tr>
<td/>
<td><b>Destination</b></td>
<td colspan="4"><b>imm<sub>8</sub> value</b></td>
</tr>
<tr>
<td><b>imm<sub>8</sub> bits</b></td>
<td/>
<td><b>00</b></td>
<td><b>01</b></td>
<td><b>10</b></td>
<td><b>11</b></td>
</tr>
</thead>
<tbody>
<tr>
<td>76 54 32 <b>10</b></td>
<td>Dest[0 to 31]</td>
<td>Src<sub>1</sub>[0 to 31]</td>
<td>Src<sub>1</sub>[32 to 63]</td>
<td>Src<sub>1</sub>[64 to 95]</td>
<td>Src<sub>1</sub>[96 to 127]</td>
</tr>
<tr>
<td/>
<td>Dest[128 to 159]</td>
<td>Src<sub>1</sub>[128 to 159]</td>
<td>Src<sub>1</sub>[160 to 191]</td>
<td>Src<sub>1</sub>[192 to 223]</td>
<td>Src<sub>1</sub>[224 to 255]</td>
</tr>
<tr>
<td>76 54 <b>32</b> 10</td>
<td>Dest[32 to 63]</td>
<td>Src<sub>1</sub>[0 to 31]</td>
<td>Src<sub>1</sub>[32 to 63]</td>
<td>Src<sub>1</sub>[64 to 95]</td>
<td>Src<sub>1</sub>[96 to 127]</td>
</tr>
<tr>
<td/>
<td>Dest[160 to 191]</td>
<td>Src<sub>1</sub>[128 to 159]</td>
<td>Src<sub>1</sub>[160 to 191]</td>
<td>Src<sub>1</sub>[192 to 223]</td>
<td>Src<sub>1</sub>[224 to 255]</td>
</tr>
<tr>
<td>76 <b>54</b> 32 10</td>
<td>Dest[64 to 95]</td>
<td>Src<sub>2</sub>[0 to 31]</td>
<td>Src<sub>2</sub>[32 to 63]</td>
<td>Src<sub>2</sub>[64 to 95]</td>
<td>Src<sub>2</sub>[96 to 127]</td>
</tr>
<tr>
<td/>
<td>Dest[192 to 223]</td>
<td>Src<sub>2</sub>[128 to 159]</td>
<td>Src<sub>2</sub>[160 to 191]</td>
<td>Src<sub>2</sub>[192 to 223]</td>
<td>Src<sub>2</sub>[224 to 255]</td>
</tr>
<tr>
<td><b>76</b> 54 32 10</td>
<td>Dest[96 to 127]</td>
<td>Src<sub>2</sub>[0 to 31]</td>
<td>Src<sub>2</sub>[32 to 63]</td>
<td>Src<sub>2</sub>[64 to 95]</td>
<td>Src<sub>2</sub>[96 to 127]</td>
</tr>
<tr>
<td/>
<td>Dest[224 to 255]</td>
<td>Src<sub>2</sub>[128 to 159]</td>
<td>Src<sub>2</sub>[160 to 191]</td>
<td>Src<sub>2</sub>[192 to 223]</td>
<td>Src<sub>2</sub>[224 to 255]</td>
</tr>
</tbody>
</table>
</figure>
<p>If both source operands are the same, you can shuffle around the single-precision values in any order you choose (and if the destination and both source operands are the same, you can arbitrarily shuffle the dwords within that register).</p>
<p><span epub:type="pagebreak" title="633" id="Page_633"/>The <code>vshufps</code> instruction also allows you to specify XMM and 128-bit memory operands. In this form, it behaves quite similarly to the <code>shufps</code> instruction except that you get to specify two different 128-bit source operands (rather than only one 128-bit source operand), and it zeroes the HO 128 bits of the corresponding YMM register. If the destination operand is different from the first source operand, this can be useful. If the <code>vshufps</code>’s first source operand is the same XMM register as the destination operand, you should use the <code>shufps</code> instruction as its machine encoding is shorter.</p>
<p>The <code>vshufpd</code> instruction is an extension of <code>shufpd</code> to 256 bits (plus the addition of a second source operand). As there are four double-precision values present in a 256-bit YMM register, <code>vshufpd</code> needs 4 bits to select the source indexes (rather than the 2 bits that <code>shufpd</code> requires). <a href="#table11-6" id="tableanchor11-6">Table 11-6</a> describes how <code>vshufpd</code> copies the data from the source operands to the destination operand.</p>
<figure>
<figcaption class="TableTitle"><p><a id="table11-6">Table 11-6</a>: <code>vshufpd</code> Destination Selection</p></figcaption>
<table id="table-501089c11-0006" border="1">
<thead>
<tr>
<td/>
<td><b>Destination</b></td>
<td colspan="2"><b>imm<sub>8</sub> value</b></td>
</tr>
<tr>
<td><b>imm<sub>8</sub> bits</b></td>
<td/>
<td><b>0</b></td>
<td><b>1</b></td>
</tr>
</thead>
<tbody>
<tr>
<td>7654 3 2 1 <b>0</b></td>
<td>Dest[0 to 63]</td>
<td>Src<sub>1</sub>[0 to 63]</td>
<td>Src<sub>1</sub>[64 to 127]</td>
</tr>
<tr>
<td>7654 3 2 <b>1</b> 0</td>
<td>Dest[64 to 127]</td>
<td>Src<sub>2</sub>[0 to 63]</td>
<td>Src<sub>2</sub>[64 to 127]</td>
</tr>
<tr>
<td>7654 3 <b>2</b> 1 0</td>
<td>Dest[128 to 191]</td>
<td>Src<sub>1</sub>[128 to 191]</td>
<td>Src<sub>1</sub>[192 to 255]</td>
</tr>
<tr>
<td>7654 <b>3</b> 2 1 0</td>
<td>Dest[192 to 255]</td>
<td>Src<sub>2</sub>[128 to 191]</td>
<td>Src<sub>2</sub>[192 to 255]</td>
</tr>
</tbody>
</table>
</figure>
<p>Like the <code>vshufps</code> instruction, <code>vshufpd</code> also allows you to specify XMM registers if you want a three-operand version of <code>shufpd</code>.</p>
<h3 id="h2-501089c11-0020">11.7.6	The (v)unpcklps, (v)unpckhps, (v)unpcklpd, and (v)unpckhpd Instructions</h3>
<p class="BodyFirst">The unpack (and merge) instructions are a simplified variant of the shuffle instructions. These instructions copy single- and double-precision values from fixed locations in their source operands and insert those values into fixed locations in the destination operand. They are, essentially, shuffle instructions without the imm<sub>8</sub> operand and with fixed shuffle patterns.</p>
<p>The <code>unpcklps</code> and <code>unpckhps</code> instructions choose half their single-precision operands from one of two sources, merge these values (interleaving them), and then store the merged result into the destination operand (which is the same as the first source operand). The syntax for these two instructions is as follows:</p>
<pre><code>unpcklps <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>
unpckhps <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub></code></pre>
<p>The XMM<sub>dest</sub> operand serves as both the first source operand and the destination operand. The XMM<sub>src</sub>/mem<sub>128</sub> operand is the second source operand.</p>
<p><span epub:type="pagebreak" title="634" id="Page_634"/>The difference between the two is the way they select their source operands. The <code>unpcklps</code> instruction copies the two LO single-precision values from the source operand to bit positions 32 to 63 (dword 1) and 96 to 127 (dword 3). It leaves dword 0 in the destination operand alone and copies the value originally in dword 1 to dword 2 in the destination. <a href="#figure11-28" id="figureanchor11-28">Figure 11-28</a> diagrams this operation.</p>
<figure>
<img src="image_fi/501089c11/f11028.png" alt="f11028" class=""/>
<figcaption><p><a id="figure11-28">Figure 11-28</a>: <span class="LiteralInCaption"><code>unpcklps</code></span> instruction operation</p></figcaption>
</figure>
<p>The <code>unpckhps</code> instruction copies the two HO single-precision values from the two sources to the destination register, as shown in <a href="#figure11-29" id="figureanchor11-29">Figure 11-29</a>.</p>
<figure>
<img src="image_fi/501089c11/f11029.png" alt="f11029" class=""/>
<figcaption><p><a id="figure11-29">Figure 11-29</a>: <span class="LiteralInCaption"><code>unpckhps</code></span> instruction operation</p></figcaption>
</figure>
<p>The <code>unpcklpd</code> and <code>unpckhpd</code> instructions do the same thing as <code>unpcklps</code> and <code>unpckhps</code> except, of course, they operate on double-precision values rather than single-precision values. Figures 11-30 and 11-31 show the operation of these two instructions.</p>
<figure>
<img src="image_fi/501089c11/f11030.png" alt="f11030" class=""/>
<figcaption><p><a id="figure11-30">Figure 11-30</a>: <span class="LiteralInCaption"><code>unpcklpd</code></span> instruction operation</p></figcaption>
</figure>
<span epub:type="pagebreak" title="635" id="Page_635"/><figure>
<img src="image_fi/501089c11/f11031.png" alt="f11031" class=""/>
<figcaption><p><a id="figure11-31">Figure 11-31</a>: <span class="LiteralInCaption"><code>unpckhpd</code></span> instruction operation</p></figcaption>
</figure>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p>	These instructions do not modify the upper 128 bits of any overlaid YMM register.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>The <code>vunpcklps</code>, <code>vunpckhps</code>, <code>vunpcklpd</code>, and <code>vunpckhpd</code> instructions have the following syntax:</p>
<pre><code>vunpcklps <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>
vunpckhps <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>

vunpcklps <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub>
vunpckhps <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub></code></pre>
<p>They work similarly to the non-<code>v</code> variants, with a couple of differences:</p>
<ul>
<li>The AVX variants support using the YMM registers as well as the XMM registers.</li>
<li>The AVX variants require three operands. The first (destination) and second (source<sub>1</sub>) operands must be XMM or YMM registers. The third (source<sub>2</sub>) operand can be an XMM or a YMM register or a 128- or 256-bit memory location. The two-operand form is just a special case of the three-operand form, where the first and second operands specify the same register name.</li>
<li>The 128-bit variants zero out the HO bits of the YMM register rather than leaving those bits unchanged.</li>
</ul>
<p>Of course, the AVX instructions with the YMM registers interleave twice as many single- or double-precision values. The interleaving extension happens in the intuitive way, with <code>vunpcklps</code> (<a href="#figure11-32" id="figureanchor11-32">Figure 11-32</a>):</p>
<ul>
<li>The single-precision values in source<sub>1</sub>, bits 0 to 31, are first written to bits 0 to 31 of the destination.</li>
<li>The single-precision values in source<sub>2</sub>, bits 0 to 31, are written to bits 32 to 63 of the destination.</li>
<li>The single-precision values in source<sub>1</sub>, bits 32 to 63, are written to bits 64 to 95 of the destination.</li>
<li>The single-precision values in source<sub>2</sub>, bits 32 to 63, are written to bits 96 to 127 of the destination.</li>
<li><span epub:type="pagebreak" title="636" id="Page_636"/>The single-precision values in source<sub>1</sub>, bits 128 to 159, are first written to bits 128 to 159 of the destination.</li>
<li>The single-precision values in source<sub>2</sub>, bits 128 to 159, are written to bits 160 to 191 of the destination.</li>
<li>The single-precision values in source<sub>1</sub>, bits 160 to 191, are written to bits 192 to 223 of the destination.</li>
<li>The single-precision values in source<sub>2</sub>, bits 160 to 191, are written to bits 224 to 256 of the destination.</li>
</ul>
<figure>
<img src="image_fi/501089c11/f11032.png" alt="f11032" class=""/>
<figcaption><p><a id="figure11-32">Figure 11-32</a>: <span class="LiteralInCaption"><code>vunpcklps</code></span> instruction operation</p></figcaption>
</figure>
<p>The <code>vunpckhps</code> instruction (<a href="#figure11-33" id="figureanchor11-33">Figure 11-33</a>) does the following:</p>
<ul>
<li>The single-precision values in source<sub>1</sub>, bits 64 to 95, are first written to bits 0 to 31 of the destination.</li>
<li>The single-precision values in source<sub>2</sub>, bits 64 to 95, are written to bits 32 to 63 of the destination.</li>
<li>The single-precision values in source<sub>1</sub>, bits 96 to 127, are written to bits 64 to 95 of the destination.</li>
<li>The single-precision values in source<sub>2</sub>, bits 96 to 127, are written to bits 96 to 127 of the destination.</li>
</ul>
<figure>
<img src="image_fi/501089c11/f11033.png" alt="f11033" class=""/>
<figcaption><p><a id="figure11-33">Figure 11-33</a>: <span class="LiteralInCaption"><code>vunpckhps</code></span> instruction operation</p></figcaption>
</figure>
<p>Likewise, <code>vunpcklpd</code> and <code>vunpckhpd</code> move double-precision values.</p>
<h3 id="h2-501089c11-0021"><span epub:type="pagebreak" title="637" id="Page_637"/>11.7.7	The Integer Unpack Instructions</h3>
<p class="BodyFirst">The <code>punpck*</code> instructions provide a set of integer unpack instructions to complement the floating-point variants. These instructions appear in <a href="#table11-7" id="tableanchor11-7">Table 11-7</a>.</p>
<figure>
<figcaption class="TableTitle"><p><a id="table11-7">Table 11-7</a>: Integer Unpack Instructions</p></figcaption>
<table id="table-501089c11-0007" border="1">
<thead>
<tr>
<td>Instruction</td>
<td>Description</td>
</tr>
</thead>
<tbody>
<tr>
<td><code>punpcklbw</code></td>
<td>Unpacks low bytes to words</td>
</tr>
<tr>
<td><code>punpckhbw</code></td>
<td>Unpacks high bytes to words</td>
</tr>
<tr>
<td><code>punpcklwd</code></td>
<td>Unpacks low words to dwords</td>
</tr>
<tr>
<td><code>punpckhwd</code></td>
<td>Unpacks high words to dwords</td>
</tr>
<tr>
<td><code>punpckldq</code></td>
<td>Unpacks low dwords to qwords</td>
</tr>
<tr>
<td><code>punpckhdq</code></td>
<td>Unpacks high dwords to qwords</td>
</tr>
<tr>
<td><code>punpcklqdq</code></td>
<td>Unpacks low qwords to owords (double qwords)</td>
</tr>
<tr>
<td><code>punpckhqdq</code></td>
<td>Unpacks high qwords to owords (double qwords)</td>
</tr>
</tbody>
</table>
</figure>
<h4 id="h3-501089c11-0001">11.7.7.1	The punpck* Instructions</h4>
<p class="BodyFirst">The <code>punpck*</code> instructions extract half the bytes, words, dwords, or qwords from two different sources and merge these values into a destination SSE register. The syntax for these instructions is shown here:</p>
<pre><code>punpcklbw  <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>
punpcklbw  <var>xmm</var><sub>dest</sub>, <var>mem</var><sub>src</sub>
punpckhbw  <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>
punpckhbw  <var>xmm</var><sub>dest</sub>, <var>mem</var><sub>src</sub>
punpcklwd  <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>
punpcklwd  <var>xmm</var><sub>dest</sub>, <var>mem</var><sub>src</sub>
punpckhwd  <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>
punpckhwd  <var>xmm</var><sub>dest</sub>, <var>mem</var><sub>src</sub>
punpckldq  <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>
punpckldq  <var>xmm</var><sub>dest</sub>, <var>mem</var><sub>src</sub>
punpckhdq  <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>
punpckhdq  <var>xmm</var><sub>dest</sub>, <var>mem</var><sub>src</sub>
punpcklqdq <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>
punpcklqdq <var>xmm</var><sub>dest</sub>, <var>mem</var><sub>src</sub>
punpckhqdq <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>
punpckhqdq <var>xmm</var><sub>dest</sub>, <var>mem</var><sub>src</sub></code></pre>
<p>Figures 11- 34 through 11-41 show the data transfers for each of these instructions.</p>
<span epub:type="pagebreak" title="638" id="Page_638"/><figure>
<img src="image_fi/501089c11/f11034.png" alt="f11034" class=""/>
<figcaption><p><a id="figure11-34">Figure 11-34</a>: <span class="LiteralInCaption"><code>punpcklbw</code></span> instruction operation</p></figcaption>
</figure>
<figure>
<img src="image_fi/501089c11/f11035.png" alt="f11035" class=""/>
<figcaption><p><a id="figure11-35">Figure 11-35</a>: <span class="LiteralInCaption"><code>punpckhbw</code></span> operation</p></figcaption>
</figure>
<figure>
<img src="image_fi/501089c11/f11036.png" alt="f11036" class=""/>
<figcaption><p><a id="figure11-36">Figure 11-36</a>: <span class="LiteralInCaption"><code>punpcklwd</code></span> operation</p></figcaption>
</figure>
<span epub:type="pagebreak" title="639" id="Page_639"/><figure>
<img src="image_fi/501089c11/f11037.png" alt="f11037" class=""/>
<figcaption><p><a id="figure11-37">Figure 11-37</a>: <span class="LiteralInCaption"><code>punpckhwd</code></span> operation</p></figcaption>
</figure>
<figure>
<img src="image_fi/501089c11/f11038.png" alt="f11038" class=""/>
<figcaption><p><a id="figure11-38">Figure 11-38</a>: <span class="LiteralInCaption"><code>punpckldq</code></span> operation</p></figcaption>
</figure>
<figure>
<img src="image_fi/501089c11/f11039.png" alt="f11039" class=""/>
<figcaption><p><a id="figure11-39">Figure 11-39</a>: <span class="LiteralInCaption"><code>punpckhdq</code></span> operation</p></figcaption>
</figure>
<span epub:type="pagebreak" title="640" id="Page_640"/><figure>
<img src="image_fi/501089c11/f11040.png" alt="f11040" class=""/>
<figcaption><p><a id="figure11-40">Figure 11-40</a>: <span class="LiteralInCaption"><code>punpcklqdq</code></span> operation</p></figcaption>
</figure>
<figure>
<img src="image_fi/501089c11/f11041.png" alt="f11041" class=""/>
<figcaption><p><a id="figure11-41">Figure 11-41</a>: <span class="LiteralInCaption"><code>punpckhqdq</code></span> operation</p></figcaption>
</figure>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p>	These instructions do not modify the upper 128 bits of any overlaid YMM register.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<h4 id="h3-501089c11-0002">11.7.7.2	The vpunpck* SSE Instructions</h4>
<p class="BodyFirst">The AVX <code>vpunpck*</code> instructions provide a set of AVX integer unpack instructions to complement the SSE variants. These instructions appear in <a href="#table11-8" id="tableanchor11-8">Table 11-8</a>.</p>
<figure>
<figcaption class="TableTitle"><p><a id="table11-8">Table 11-8</a>: AVX Integer Unpack Instructions</p></figcaption>
<table id="table-501089c11-0008" border="1">
<thead>
<tr>
<td><b>Instruction</b></td>
<td><b>Description</b></td>
</tr>
</thead>
<tbody>
<tr>
<td><code>vpunpcklbw</code></td>
<td>Unpacks low bytes to words</td>
</tr>
<tr>
<td><code>vpunpckhbw</code></td>
<td>Unpacks high bytes to words</td>
</tr>
<tr>
<td><code>vpunpcklwd</code></td>
<td>Unpacks low words to dwords</td>
</tr>
<tr>
<td><code>vpunpckhwd</code></td>
<td>Unpacks high words to dwords</td>
</tr>
<tr>
<td><code>vpunpckldq</code></td>
<td>Unpacks low dwords to qwords</td>
</tr>
<tr>
<td><code><span epub:type="pagebreak" title="641" id="Page_641"/>vpunpckhdq</code></td>
<td>Unpacks high dwords to qwords</td>
</tr>
<tr>
<td><code>vpunpcklqdq</code></td>
<td>Unpacks low qwords to owords (double qwords)</td>
</tr>
<tr>
<td><code>vpunpckhqdq</code></td>
<td>Unpacks high qwords to owords (double qwords)</td>
</tr>
</tbody>
</table>
</figure>
<p>The <code>vpunpck*</code> instructions extract half the bytes, words, dwords, or qwords from two different sources and merge these values into a destination AVX or SSE register. Here is the syntax for the SSE forms of these instructions:</p>
<pre><code>vpunpcklbw  <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>
vpunpckhbw  <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>
vpunpcklwd  <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>
vpunpckhwd  <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>
vpunpckldq  <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>
vpunpckhdq  <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>
vpunpcklqdq <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>
vpunpckhqdq <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub></code></pre>
<p>Functionally, the only difference between these AVX instructions (<code>vunpck*</code>) and the SSE (<code>unpck*</code>) instructions is that the SSE variants leave the upper bits of the YMM AVX registers (bits 128 to 255) unchanged, whereas the AVX variants zero-extend the result to 256 bits. See Figures 11-34 through 11-41 for a description of the operation of these instructions. </p>
<h4 id="h3-501089c11-0003">11.7.7.3	The vpunpck* AVX Instructions</h4>
<p class="BodyFirst">The AVX <code>vunpck*</code> instructions also support the use of the AVX YMM registers, in which case the unpack and merge operation extends from 128 bits to 256 bits. The syntax for these instructions is as follows:</p>
<pre><code>vpunpcklbw  <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub>
vpunpckhbw  <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub>
vpunpcklwd  <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub>
vpunpckhwd  <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub>
vpunpckldq  <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub>
vpunpckhdq  <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub>
vpunpcklqdq <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub>
vpunpckhqdq <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub></code></pre>
<h3 id="h2-501089c11-0022">11.7.8	The (v)pextrb, (v)pextrw, (v)pextrd, and (v)pextrq Instructions</h3>
<p class="BodyFirst">The <code>(v)pextrb</code>, <code>(v)pextrw</code>, <code>(v)pextrd</code>, and <code>(v)pextrq</code> instructions extract a byte, word, dword, or qword from a 128-bit XMM register and copy this data to a general-purpose register or memory location. The syntax for these instructions is the following:</p>
<pre><code>pextrb  <var>reg</var><sub>32</sub>, <var>xmm</var><sub>src</sub>, <var>imm</var><sub>8</sub>   ; imm<sub>8</sub> = 0 to 15
pextrb  <var>reg</var><sub>64</sub>, <var>xmm</var><sub>src</sub>, <var>imm</var><sub>8</sub>   ; imm<sub>8</sub> = 0 to 15
pextrb  <var>mem</var><sub>8</sub>, <var>xmm</var><sub>src</sub>, <var>imm</var><sub>8</sub>    ; imm<sub>8</sub> = 0 to 15
<span epub:type="pagebreak" title="642" id="Page_642"/>vpextrb <var>reg</var><sub>32</sub>, <var>xmm</var><sub>src</sub>, <var>imm</var><sub>8</sub>  ; imm<sub>8</sub> = 0 to 15
vpextrb <var>reg</var><sub>64</sub>, <var>xmm</var><sub>src</sub>, <var>imm</var><sub>8</sub>  ; imm<sub>8</sub> = 0 to 15
vpextrb <var>mem</var><sub>8</sub>, <var>xmm</var><sub>src</sub>, <var>imm</var><sub>8</sub>   ; imm<sub>8</sub> = 0 to 15

pextrw  <var>reg</var><sub>32</sub>, <var>xmm</var><sub>src</sub>, <var>imm</var><sub>8</sub>  ; imm<sub>8</sub> = 0 to 7
pextrw  <var>reg</var><sub>64</sub>, <var>xmm</var><sub>src</sub>, <var>imm</var><sub>8</sub>  ; imm<sub>8</sub> = 0 to 7
pextrw  <var>mem</var><sub>16</sub>, <var>xmm</var><sub>src</sub>, <var>imm</var><sub>8</sub>  ; imm<sub>8</sub> = 0 to 7
vpextrw <var>reg</var><sub>32</sub>, <var>xmm</var><sub>src</sub>, <var>imm</var><sub>8</sub>  ; imm<sub>8</sub> = 0 to 7
vpextrw <var>reg</var><sub>64</sub>, <var>xmm</var><sub>src</sub>, <var>imm</var><sub>8</sub>  ; imm<sub>8</sub> = 0 to 7
vpextrw <var>mem</var><sub>16</sub>, <var>xmm</var><sub>src</sub>, <var>imm</var><sub>8</sub>  ; imm<sub>8</sub> = 0 to 7

pextrd  <var>reg</var><sub>32</sub>, <var>xmm</var><sub>src</sub>, <var>imm</var><sub>8</sub>  ; imm<sub>8</sub> = 0 to 3
pextrd  <var>mem</var><sub>32</sub>, <var>xmm</var><sub>src</sub>, <var>imm</var><sub>8</sub>  ; imm<sub>8</sub> = 0 to 3
vpextrd <var>mem</var><sub>64</sub>, <var>xmm</var><sub>src</sub>, <var>imm</var><sub>8</sub>  ; imm<sub>8</sub> = 0 to 3
vpextrd <var>reg</var><sub>32</sub>, <var>xmm</var><sub>src</sub>, <var>imm</var><sub>8</sub>  ; imm<sub>8</sub> = 0 to 3
vpextrd <var>reg</var><sub>64</sub>, <var>xmm</var><sub>src</sub>, <var>imm</var><sub>8</sub>  ; imm<sub>8</sub> = 0 to 3
vpextrd <var>mem</var><sub>32</sub>, <var>xmm</var><sub>src</sub>, <var>imm</var><sub>8</sub>  ; imm<sub>8</sub> = 0 to 3

pextrq  <var>reg</var><sub>64</sub>, <var>xmm</var><sub>src</sub>, <var>imm</var><sub>8</sub>  ; imm<sub>8</sub> = 0 to 1
pextrq  <var>mem</var><sub>64</sub>, <var>xmm</var><sub>src</sub>, <var>imm</var><sub>8</sub>  ; imm<sub>8</sub> = 0 to 1
vpextrq <var>reg</var><sub>64</sub>, <var>xmm</var><sub>src</sub>, <var>imm</var><sub>8</sub>  ; imm<sub>8</sub> = 0 to 1
vpextrq <var>mem</var><sub>64</sub>, <var>xmm</var><sub>src</sub>, <var>imm</var><sub>8</sub>  ; imm<sub>8</sub> = 0 to 1</code></pre>
<p>The byte and word instructions expect a 32- or 64-bit general-purpose register as their destination (first operand) or a memory location that is the same size as the instruction (that is, <code>pextrb</code> expects a byte-sized memory operand, <code>pextrw</code> expects a word-sized operand, and so on). The source (second) operand is a 128-bit XMM register. The index (third) operand is an 8-bit immediate value that specifies an index (lane number). These instructions fetch the byte, word, dword, or qword in the lane specified by the 8-bit immediate value and copy that value into the destination operand. The double-word and quad-word variants require a 32-bit or 64-bit general-purpose register, respectively. If the destination operand is a 32- or 64-bit general-purpose register, the instruction zero-extends the value to 32 or 64 bits, if necessary.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p>	These instructions do not support extracting data from the upper 128 bits of a YMM register.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<h3 id="h2-501089c11-0023">11.7.9	The (v)pinsrb, (v)pinsrw, (v)pinsrd, and (v)pinsrq Instructions</h3>
<p class="BodyFirst">The <code>(v)pinsr{b,w,d,q}</code> instructions take a byte, word, dword, or qword from a general-purpose register or memory location and store that data to a lane of an XMM register. The syntax for these instructions is the following:<sup class="FootnoteReference"><a id="c11-footnoteref-9" href="#c11-footnote-9">9</a></sup></p>
<pre><code>pinsrb  <var>xmm</var><sub>dest</sub>, <var>reg</var><sub>32</sub>, <var>imm</var><sub>8</sub>         <span class="NegativeCodeSpace" style="margin-left: .1em;"> </span>; imm<sub>8</sub> = 0 to 15
pinsrb  <var>xmm</var><sub>dest</sub>, <var>mem</var><sub>8</sub>, <var>imm</var><sub>8</sub>           ; imm<sub>8</sub> = 0 to 15
vpinsrb <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src2</sub>, <var>reg</var><sub>32</sub>, <var>imm</var><sub>8</sub>  <span class="NegativeCodeSpace" style="margin-left: -.6em;"> </span>; imm<sub>8</sub> = 0 to 15
vpinsrb <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src2</sub>, <var>mem</var><sub>8</sub>, <var>imm</var><sub>8</sub>   <span class="NegativeCodeSpace" style="margin-left: -.8em;"> </span>; imm<sub>8</sub> = 0 to 15

<span epub:type="pagebreak" title="643" id="Page_643"/>pinsrw  <var>xmm</var><span class="SubscriptLiteral">dest</span>, <var>reg</var><span class="SubscriptLiteral">32</span>, <var>imm</var><span class="SubscriptLiteral">8</span>         <span class="NegativeCodeSpace" style="margin-left: .-1em;"> </span>; imm<sub><em>8</em></sub> = 0 to 7
pinsrw  <var>xmm</var><sub>dest</sub>, <var>mem</var><sub>16</sub>, <var>imm</var><sub>8</sub>         <span class="NegativeCodeSpace"> </span>; imm<sub>8</sub> = 0 to 7
vpinsrw <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src2</sub>, <var>reg</var><sub>32</sub>, <var>imm</var><sub>8</sub> <span class="NegativeCodeSpace"> </span>; imm<sub>8</sub> = 0 to 7
vpinsrw <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src2</sub>, <var>mem</var><sub>16</sub>, <var>imm</var><sub>8</sub> <span class="NegativeCodeSpace"> </span>; imm<sub>8</sub> = 0 to 7

pinsrd  <var>xmm</var><sub>dest</sub>, <var>reg</var><sub>32</sub>, <var>imm</var><sub>8</sub>          ; imm<sub>8</sub> = 0 to 3
pinsrd  <var>xmm</var><sub>dest</sub>, <var>mem</var><sub>32</sub>, <var>imm</var><sub>8</sub>          ; imm<sub>8</sub> = 0 to 3
vpinsrd <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src2</sub>, <var>reg</var><sub>32</sub>, <var>imm</var><sub>8</sub> <span class="NegativeCodeSpace"> </span>; imm<sub>8</sub> = 0 to 3
vpinsrd <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src2</sub>, <var>mem</var><sub>32</sub>, <var>imm</var><sub>8</sub> <span class="NegativeCodeSpace"> </span>; imm<sub>8</sub> = 0 to 3

pinsrq  <var>xmm</var><sub>dest</sub>, <var>reg</var><sub>64</sub>, <var>imm</var><sub>8</sub>          ; imm<sub>8</sub> = 0 to 1
pinsrq  <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src2</sub>, <var>mem</var><sub>64</sub>, <var>imm</var><sub>8</sub> <span class="NegativeCodeSpace"> </span>; imm<sub>8</sub> = 0 to 1
vpinsrq <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src2</sub>, <var>reg</var><sub>64</sub>, <var>imm</var><sub>8</sub> <span class="NegativeCodeSpace"> </span>; imm<sub>8</sub> = 0 to 1
vpinsrq <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src2</sub>, <var>mem</var><sub>64</sub>, <var>imm</var><sub>8</sub> <span class="NegativeCodeSpace"> </span>; imm<sub>8</sub> = 0 to 1</code></pre>
<p>The destination (first) operand is a 128-bit XMM register. The <code>pinsr*</code> instructions expect a memory location or a 32-bit general-purpose register as their source (second) operand (except the <code>pinsrq</code> instructions, which require a 64-bit register). The index (third) operand is an 8-bit immediate value that specifies an index (lane number). </p>
<p>These instructions fetch a byte, word, dword, or qword from the general-purpose register or memory location and copy that to the lane in the XMM register specified by the 8-bit immediate value. The <code>pinsr{b,w,d,q}</code> instructions leave any HO bits in the underlying YMM register unchanged (if applicable). </p>
<p>The <code>vpinsr{b,w,d,q}</code> instructions copy the data from the XMM source register into the destination register and then copy the byte, word, dword, or quad word to the specified location in the destination register. These instructions zero-extend the value throughout the HO bits of the underlying YMM register.</p>
<h3 id="h2-501089c11-0024">11.7.10	The (v)extractps and (v)insertps Instructions</h3>
<p class="BodyFirst">The <code>extractps</code> and <code>vextractps</code> instructions are functionally equivalent to <code>pextrd</code> and <code>vpextrd</code>. They extract a 32-bit (single-precision floating-point) value from an XMM register and move it into a 32-bit general-purpose register or a 32-bit memory location. The syntax for the <code>(v)extractps</code> instructions is shown here:</p>
<pre><code>extractps  <var>reg</var><sub>32</sub>, <var>xmm</var><sub>src</sub>, <var>imm</var><sub>8</sub>
extractps  <var>mem</var><sub>32</sub>, <var>xmm</var><sub>src</sub>, <var>imm</var><sub>8</sub>
vextractps <var>reg</var><sub>32</sub>, <var>xmm</var><sub>src</sub>, <var>imm</var><sub>8</sub>
vextractps <var>mem</var><sub>32</sub>, <var>xmm</var><sub>src</sub>, <var>imm</var><sub>8</sub></code></pre>
<p>The <code>insertps</code> and <code>vinsertps</code> instructions insert a 32-bit floating-point value into an XMM register and, optionally, zero out other lanes in the XMM register. The syntax for these instructions is as follows:</p>
<pre><code>insertps  <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>, <var>imm</var><sub>8</sub>
insertps  <var>xmm</var><sub>dest</sub>, <var>mem</var><sub>32</sub>, <var>imm</var><sub>8</sub>
vinsertps <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>, <var>imm</var><sub>8</sub>
vinsertps <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>mem</var><sub>32</sub>, <var>imm</var><sub>8</sub></code></pre>
<p><span epub:type="pagebreak" title="644" id="Page_644"/>For the <code>insertps</code> and <code>vinsertps</code> instructions, the imm<sub>8</sub> operand has the fields listed in <a href="#table11-9" id="tableanchor11-9">Table 11-9</a>.</p>
<figure>
<figcaption class="TableTitle"><p><a id="table11-9">Table 11-9</a>: imm<sub>8</sub> Bit Fields for <code>insertps</code> and <code>vinsertps</code> Instructions</p></figcaption>
<table id="table-501089c11-0009" border="1">
<thead>
<tr>
<td><b>Bit(s)</b></td>
<td><b>Meaning</b></td>
</tr>
</thead>
<tbody>
<tr>
<td>6 to 7</td>
<td>(Only if the source operand is an XMM register): Selects the 32-bit lane from the source XMM register (0, 1, 2, or 3). If the source operand is a 32-bit memory location, the instruction ignores this field and uses the full 32 bits from memory.</td>
</tr>
<tr>
<td>4 to 5</td>
<td>Specifies the lane in the destination XMM register in which to store the single-precision value.</td>
</tr>
<tr>
<td>3</td>
<td>If set, zeroes lane 3 of XMM<sub>dest</sub>.</td>
</tr>
<tr>
<td>2</td>
<td>If set, zeroes lane 2 of XMM<sub>dest</sub>.</td>
</tr>
<tr>
<td>1</td>
<td>If set, zeroes lane 1 of XMM<sub>dest</sub>.</td>
</tr>
<tr>
<td>0</td>
<td>If set, zeroes lane 0 of XMM<sub>dest</sub>.</td>
</tr>
</tbody>
</table>
</figure>
<p>On CPUs with the AVX extensions, <code>insertps</code> does not modify the upper bits of the YMM registers; <code>vinsertps</code> zeroes the upper bits.</p>
<p>The <code>vinsertps</code> instruction first copies the XMM<sub>src1</sub> register to XMM<sub>dest </sub>before performing the insertion operation. The HO bits of the corresponding YMM register are set to 0.</p>
<p>The x86-64 does not provide <code>(v)extractpd</code> or <code>(v)insertpd</code> instructions.</p>
<h2 id="h1-501089c11-0008">	11.8	SIMD Arithmetic and Logical Operations</h2>
<p class="BodyFirst">The SSE and AVX instruction set extensions provide a variety of scalar and vector arithmetic and logical operations. </p>
<p><span class="xref" itemid="xref_target_“SSE Floating-Point Arithmetic” in Chapter 6">“SSE Floating-Point Arithmetic” in Chapter 6</span> has already covered floating-point arithmetic using the scalar SSE instruction set, so this section does not repeat that discussion. Instead, this section covers the <em>vector</em> (or <em>packed</em>) arithmetic and logical instructions.</p>
<p>The vector instructions perform multiple operations in parallel on the different data lanes in an SSE or AVX register. Given two source operands, a typical SSE instruction will calculate two double-precision floating-point results, two quad-word integer calculations, four single-precision floating-point operations, four double-word integer calculations, eight word integer calculations, or sixteen byte calculations, simultaneously. The AVX registers (YMM) double the number of lanes and therefore double the number of concurrent calculations.</p>
<p><a href="#figure11-42" id="figureanchor11-42">Figure 11-42</a> shows how the SSE and AVX instructions perform concurrent calculations; a value is taken from the same lane in two source locations, the calculation is performed, and the instruction stores the result to the same lane in the destination location. This process happens simultaneously for each lane in the source and destination operands. For example, if a pair of <span epub:type="pagebreak" title="645" id="Page_645"/>XMM registers contains four single-precision floating-point values, a SIMD packed floating-point addition instruction would add the single-precision values in the corresponding lanes of the source operands and store the single-precision sums into the corresponding lanes of the destination XMM register.</p>
<figure>
<img src="image_fi/501089c11/f11042.png" alt="f11042" class=""/>
<figcaption><p><a id="figure11-42">Figure 11-42</a>: SIMD concurrent arithmetic and logical operations</p></figcaption>
</figure>
<p>Certain operations—for example, logical AND, ANDN (<em>and not</em>), OR, and XOR—don’t have to be broken into lanes, because those operations perform the same result regardless of the instruction size. The lane size is a single bit. Therefore, the corresponding SSE/AVX instructions operate on their entire operands without regard for a lane size.</p>
<h2 id="h1-501089c11-0009">	11.9	The SIMD Logical (Bitwise) Instructions</h2>
<p class="BodyFirst">The SSE and AVX instruction set extensions provide the logical operations shown in <a href="#table11-10" id="tableanchor11-10">Table 11-10</a> (using C/C++ bitwise operator syntax).</p>
<figure>
<figcaption class="TableTitle"><p><a id="table11-10">Table 11-10</a>: SSE/AVX Logical Instructions</p></figcaption>
<table id="table-501089c11-0010" border="1">
<thead>
<tr>
<td><b>Operation</b></td>
<td><b>Description</b></td>
</tr>
</thead>
<tbody>
<tr>
<td><code>andpd</code></td>
<td>dest = dest and source (128-bit operands)</td>
</tr>
<tr>
<td><code>vandpd</code></td>
<td>dest = source1 and source2 (128-bit or 256-bit operands)</td>
</tr>
<tr>
<td><code>andnpd</code></td>
<td>dest = dest and ~source (128-bit operands)</td>
</tr>
<tr>
<td><code>vandnpd</code></td>
<td>dest = source1 and ~source2 (128-bit or 256-bit operands)</td>
</tr>
<tr>
<td><code>orpd</code></td>
<td>dest = dest | source (128-bit operands)</td>
</tr>
<tr>
<td><code>vorpd</code></td>
<td>dest = source1 | source2 (128-bit or 256-bit operands)</td>
</tr>
<tr>
<td><code>xorpd</code></td>
<td>dest = dest ^ source (128-bit operands)</td>
</tr>
<tr>
<td><code>vxorpd</code></td>
<td>dest = source1 ^ source2 (128-bit or 256-bit operands)</td>
</tr>
</tbody>
</table>
</figure>
<p><span epub:type="pagebreak" title="646" id="Page_646"/>The syntax for these instructions is the following:</p>
<pre><code>andpd   <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>
vandpd  <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>
vandpd  <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub>

andnpd  <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>
vandnpd <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>
vandnpd <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub>

orpd    <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>
vorpd   <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>
vorpd   <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub>

xorpd   <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>
vxorpd  <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>
vxorpd  <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub></code></pre>
<p>The SSE instructions (without the <code>v</code> prefix) leave the HO bits of the underlying YMM register unchanged (if applicable). The AVX instructions (with the <code>v</code> prefix) that have 128-bit operands will zero-extend their result into the HO bits of the YMM register.</p>
<p>If the (second) source operand is a memory location, it must be aligned on an appropriate boundary (for example, 16 bytes for mem<sub>128</sub> values and 32 bytes for mem<sub>256</sub> values). Failure to do so will result in a runtime memory alignment fault.</p>
<h3 id="h2-501089c11-0025">11.9.1	The (v)ptest Instructions</h3>
<p class="BodyFirst">The <code>ptest</code> instruction (<em>packed test</em>) is similar to the standard integer <code>test</code> instruction. The <code>ptest</code> instruction performs a logical AND between the two operands and sets the zero flag if the result is 0. The <code>ptest</code> instruction sets the carry flag if the logical AND of the second operand with the inverted bits of the first operand produces 0. The <code>ptest</code> instruction supports the following syntax:</p>
<pre><code>ptest  <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>
vptest <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>
vptest <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub></code></pre>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p>	The <code>ptest</code> instruction is available only on CPUs that support the SSE4.1 instruction set (and later) extensions; <code>vptest</code> requires AVX support. The 128-bit SSE (<code>ptest</code>) and AVX (<code>vptest</code>) instructions do exactly the same thing, but the SSE encoding is more efficient.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<h3 id="h2-501089c11-0026">11.9.2	The Byte Shift Instructions</h3>
<p class="BodyFirst">The SSE and AVX instruction set extensions also support a set of logical and arithmetic shift instructions. The first two to consider are <code>pslldq</code> and <code>psrldq</code>. Although they begin with a <code>p</code>, suggesting they are packed (vector) <span epub:type="pagebreak" title="647" id="Page_647"/>instructions, these instructions really are just 128-bit logical shift-left and shift-right instructions. Their syntax is as follows:</p>
<pre><code>pslldq  <var>xmm</var><sub>dest</sub>, <var>imm</var><sub>8</sub>
vpslldq <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>, <var>imm</var><sub>8</sub>
vpslldq <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src</sub>, <var>imm</var><sub>8</sub>
psrldq  <var>xmm</var><sub>dest</sub>, <var>imm</var><sub>8</sub>
vpsrldq <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>, <var>imm</var><sub>8</sub>
vpsrldq <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src</sub>, <var>imm</var><sub>8</sub></code></pre>
<p>The <code>pslldq</code> instruction shifts its destination XMM register to the left by the number of <em>bytes</em> specified by the imm<sub>8</sub> operand. This instruction shifts 0s into the vacated LO bytes.</p>
<p>The <code>vpslldq</code> instruction takes the value in the source register (XMM or YMM), shifts that value to the left by imm<sub>8</sub> bytes, and then stores the result into the destination register. For the 128-bit variant, this instruction zero-extends the result into bits 128 to 255 of the underlying YMM register (on AVX-capable CPUs).</p>
<p>The <code>psrldq</code> and <code>vpsrldq</code> instructions operate similarly to <code>(v)pslldq</code> except, of course, they shift their operands to the right rather than to the left. These are logical shift-right operations, so they shift 0s into the HO bytes of their operand, and bits shifted out of bit 0 are lost.</p>
<p>The <code>pslldq</code> and <code>psrldq</code> instructions shift <em>bytes</em> rather than bits. For example, many SSE instructions produce byte masks 0 or 0FFh, representing Boolean results. These instructions shift the equivalent of a bit in one of these byte masks by shifting whole bytes at a time.</p>
<h3 id="h2-501089c11-0027">11.9.3	The Bit Shift Instructions</h3>
<p class="BodyFirst">The SSE/AVX instruction set extensions also provide vector bit shift operations that work on two or more integer lanes, concurrently. These instructions provide word, dword, and qword variants of the logical shift-left, logical shift-right, and arithmetic shift-right operations, using the syntax</p>
<pre><code><var>shift</var>  <var>xmm</var><sub>dest</sub>, <var>imm</var><sub>8</sub>
<var>shift</var>  <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>
<var>vshift</var> <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>, <var>imm</var><sub>8</sub>
<var>vshift</var> <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>, <var>mem</var><sub>128</sub>
<var>vshift</var> <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src</sub>, <var>imm</var><sub>8</sub>
<var>vshift</var> <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src</sub>, <var>xmm</var>/<var>mem</var><sub>128</sub></code></pre>
<p class="BodyContinued">where <var>shift</var> = <code>psllw</code>, <code>pslld</code>, <code>psllq</code>, <code>psrlw</code>, <code>psrld</code>, <code>psrlq</code>, <code>psraw</code>, or <code>psrad</code>, and <var>vshift</var> = <code>vpsllw</code>, <code>vpslld</code>, <code>vpsllq</code>, <code>vpsrlw</code>, <code>vpsrld</code>, <code>vpsrlq</code>, <code>vpsraw</code>, <code>vpsrad</code>, or <code>vpsraq</code>.</p>
<p>The <code>(v)psl*</code> instructions shift their operands to the left; the <code>(v)psr*</code> instructions shift their operands to the right. The <code>(v)psll*</code> and <code>(v)psrl*</code> instructions are logical shift instructions and shift 0s into the bits vacated by the shift. Any bits shifted out of the operand are lost. The <code>(v)psra*</code> instructions are arithmetic shift-right instructions. They replicate the HO bit in each lane when shifting that lane’s bits to the right; all bits shifted out of the LO bit are lost.</p>
<p><span epub:type="pagebreak" title="648" id="Page_648"/>The SSE two-operand instructions treat their first operand as both the source and destination operand. The second operand specifies the number of bits to shift (which is either an 8-bit immediate constant or a value held in an XMM register or a 128-bit memory location). Regardless of the shift count’s size, only the LO 4, 5, or 6 bits of the count are meaningful (depending on the lane size).</p>
<p>The AVX three-operand instructions specify a separate source and destination register for the shift operation. These instructions take the value from the source register, shift it the specified number of bits, and store the shifted result into the destination register. The source register remains unmodified (unless, of course, the instruction specifies the same register for the source and destination operands). For the AVX instructions, the source and destination registers can be XMM (128-bit) or YMM (256-bit) registers. The third operand is either an 8-bit immediate constant, an XMM register, or a 128-bit memory location. The third operand specifies the bit shift count (the same as the SSE instructions). You specify an XMM register for the count even when the source and destination registers are 256-bit YMM registers.</p>
<p>The <code>w</code> suffix instructions shift 16-bit operands (eight lanes for 128-bit destination operands, sixteen lanes for 256-bit destinations). The <code>d</code> suffix instructions shift 32-bit dword operands (four lanes for 128-bit destination operands, eight lanes for 256-bit destination operands). The <code>q</code> suffix instructions shift 64-bit operands (two lanes for 128-bit operands, four lanes for 256-bit operands).</p>
<h2 id="h1-501089c11-0010">	11.10	The SIMD Integer Arithmetic Instructions</h2>
<p class="BodyFirst">The SSE and AVX instruction set extensions deal mainly with floating-point calculations. They do, however, include a set of signed and unsigned integer arithmetic operations. This section describes the SSE/AVX integer arithmetic instructions.</p>
<h3 id="h2-501089c11-0028">11.10.1	SIMD Integer Addition</h3>
<p class="BodyFirst">The SIMD integer addition instructions appear in <a href="#table11-11" id="tableanchor11-11">Table 11-11</a>. These instructions do not affect any flags and thus do not indicate when an overflow (signed or unsigned) occurs during the execution of these instructions. The program itself must ensure that the source operands are all within the appropriate range before performing an addition. If carry occurs during an addition, the carry is lost.</p>
<figure>
<figcaption class="TableTitle"><p><a id="table11-11">Table 11-11</a>: SIMD Integer Addition Instructions</p></figcaption>
<table id="table-501089c11-0011" border="1">
<thead>
<tr>
<td><b>Instruction</b></td>
<td><b>Operands</b></td>
<td><b>Description</b></td>
</tr>
</thead>
<tbody>
<tr>
<td><code>paddb</code></td>
<td><var>xmm</var><sub>dest</sub>, <var>xmm</var>/<var>mem</var><sub>128</sub></td>
<td>16-lane byte addition</td>
</tr>
<tr>
<td><code>vpaddb</code></td>
<td><var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub></td>
<td>16-lane byte addition</td>
</tr>
<tr>
<td><code>vpaddb</code></td>
<td><var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub></td>
<td>32-lane byte addition</td>
</tr>
<tr>
<td><code>paddw</code></td>
<td><var>xmm</var><sub>dest</sub>, <var>xmm</var>/<var>mem</var><sub>128</sub></td>
<td>8-lane word addition</td>
</tr>
<tr>
<td><code>vpaddw</code></td>
<td><var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub></td>
<td>8-lane word addition</td>
</tr>
<tr>
<td><code><span epub:type="pagebreak" title="649" id="Page_649"/>vpaddw</code></td>
<td><var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub></td>
<td>16-lane word addition</td>
</tr>
<tr>
<td><code>paddd</code></td>
<td><var>xmm</var><sub>dest</sub>, <var>xmm</var>/<var>mem</var><sub>128</sub></td>
<td>4-lane dword addition</td>
</tr>
<tr>
<td><code>vpaddd</code></td>
<td><var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub></td>
<td>4-lane dword addition</td>
</tr>
<tr>
<td><code>vpaddd</code></td>
<td><var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub></td>
<td>8-lane dword addition</td>
</tr>
<tr>
<td><code>paddq</code></td>
<td><var>xmm</var><sub>dest</sub>, <var>xmm</var>/<var>mem</var><sub>128</sub></td>
<td>2-lane qword addition</td>
</tr>
<tr>
<td><code>vpaddq</code></td>
<td><var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub></td>
<td>2-lane qword addition</td>
</tr>
<tr>
<td><code>vpaddq</code></td>
<td><var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub></td>
<td>4-lane qword addition</td>
</tr>
</tbody>
</table>
</figure>
<p>These addition instructions are known as <em>vertical additions</em> because if we stack the two source operands on top of each other (on a printed page), the lane additions occur vertically (one source lane is directly above the second source lane for the corresponding addition operation). </p>
<p>The packed additions ignore any overflow from the addition operation, keeping only the LO byte, word, dword, or qword of each addition. As long as overflow is never possible, this is not an issue. However, for certain algorithms (especially audio and video, which commonly use packed addition), truncating away the overflow can produce bizarre results. </p>
<p>A cleaner solution is to use <em>saturation arithmetic</em>. For unsigned addition, saturation arithmetic <em>clips</em> (or <em>saturates</em>) an overflow to the largest possible value that the instruction’s size can handle. For example, if the addition of two byte values exceeds 0FFh, saturation arithmetic produces 0FFh—the largest possible unsigned 8-bit value (likewise, saturation subtraction would produce 0 if underflow occurs). For signed saturation arithmetic, clipping occurs at the largest positive and smallest negative values (for example, 7Fh/+127 for positive values and 80h/–128 for negative values).</p>
<p>The x86 SIMD instructions provide both signed and unsigned saturation arithmetic, though the operations are limited to 8- and 16-bit quantities.<sup class="FootnoteReference"><a id="c11-footnoteref-10" href="#c11-footnote-10">10</a></sup> The instructions appear in <a href="#table11-12" id="tableanchor11-12">Table 11-12</a>.</p>
<figure>
<figcaption class="TableTitle"><p><a id="table11-12">Table 11-12</a>: SIMD Integer Saturation Addition Instructions</p></figcaption>
<table id="table-501089c11-0012" border="1">
<thead>
<tr>
<td><b>Instruction</b></td>
<td><b>Operands</b></td>
<td><b>Description</b></td>
</tr>
</thead>
<tbody>
<tr>
<td><code>paddsb</code></td>
<td><var>xmm</var><sub>dest</sub>, <var>xmm</var>/<var>mem</var><sub>128</sub></td>
<td>16-lane byte signed saturation addition</td>
</tr>
<tr>
<td><code>vpaddsb</code></td>
<td><var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub></td>
<td>16-lane byte signed saturation addition</td>
</tr>
<tr>
<td><code>vpaddsb</code></td>
<td><var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub></td>
<td>32-lane byte signed saturation addition</td>
</tr>
<tr>
<td><code>paddsw</code></td>
<td><var>xmm</var><sub>dest</sub>, <var>xmm</var>/<var>mem</var><sub>128</sub></td>
<td>8-lane word signed saturation addition</td>
</tr>
<tr>
<td><code>vpaddsw</code></td>
<td><var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub></td>
<td>8-lane word signed saturation addition</td>
</tr>
<tr>
<td><code>vpaddsw</code></td>
<td><var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub></td>
<td>16-lane word signed saturation addition</td>
</tr>
<tr>
<td><code>paddusb</code></td>
<td><var>xmm</var><sub>dest</sub>, <var>xmm</var>/<var>mem</var><sub>128</sub></td>
<td>16-lane byte unsigned saturation addition</td>
</tr>
<tr>
<td><code><span epub:type="pagebreak" title="650" id="Page_650"/>vpaddusb</code></td>
<td><var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub></td>
<td>16-lane byte unsigned saturation addition</td>
</tr>
<tr>
<td><code>vpaddusb</code></td>
<td><var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub></td>
<td>32-lane byte unsigned saturation addition</td>
</tr>
<tr>
<td><code>paddusw</code></td>
<td><var>xmm</var><sub>dest</sub>, <var>xmm</var>/<var>mem</var><sub>128</sub></td>
<td>8-lane word unsigned saturation addition</td>
</tr>
<tr>
<td><code>vpaddusw</code></td>
<td><var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub></td>
<td>8-lane word unsigned saturation addition</td>
</tr>
<tr>
<td><code>vpaddusw</code></td>
<td><var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub></td>
<td>16-lane word unsigned saturation addition</td>
</tr>
</tbody>
</table>
</figure>
<p>As usual, both <code>padd*</code> and <code>vpadd*</code> instructions accept 128-bit XMM registers (sixteen 8-bit additions or eight 16-bit additions). The <code>padd*</code> instructions leave the HO bits of any corresponding YMM destination undisturbed; the <code>vpadd*</code> variants clear the HO bits. Also note that the <code>padd*</code> instructions have only two operands (the destination register is also a source), whereas the <code>vpadd*</code> instructions have two source operands and a single destination operand. The <code>vpadd*</code> instructions with the YMM register provide double the number of parallel additions.</p>
<h3 id="h2-501089c11-0029">11.10.2	Horizontal Additions</h3>
<p class="BodyFirst">The SSE/AVX instruction sets also support three <em>horizontal addition</em> instructions, listed in <a href="#table11-13" id="tableanchor11-13">Table 11-13</a>.</p>
<figure>
<figcaption class="TableTitle"><p><a id="table11-13">Table 11-13</a>: Horizontal Addition Instructions</p></figcaption>
<table id="table-501089c11-0013" border="1">
<thead>
<tr>
<td><b>Instruction</b></td>
<td><b>Description</b></td>
</tr>
</thead>
<tbody>
<tr>
<td><code>(v)</code><code>phaddw</code></td>
<td>16-bit (word) horizontal add</td>
</tr>
<tr>
<td><code>(v)</code><code>phaddd</code></td>
<td>32-bit (dword) horizontal add</td>
</tr>
<tr>
<td><code>(v)</code><code>phaddsw</code></td>
<td>16-bit (word) horizontal add and saturate</td>
</tr>
</tbody>
</table>
</figure>
<p>The horizontal addition instructions add adjacent words or dwords in their two source operands and store the sum of the result into a destination lane, as shown in <a href="#figure11-43" id="figureanchor11-43">Figure 11-43</a>. </p>
<figure>
<img src="image_fi/501089c11/f11043.png" alt="f11043" class=""/>
<figcaption><p><a id="figure11-43">Figure 11-43</a>: Horizontal addition operation</p></figcaption>
</figure>
<p class="BodyFirst"><span epub:type="pagebreak" title="651" id="Page_651"/>The <code>phaddw</code> instruction has the following syntax:</p>
<pre><code>phaddw <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub></code></pre>
<p>It computes the following:</p>
<pre><code>temp[0 to 15]    = <var>xmm</var><sub>dest</sub>[0 to 15]        + <var>xmm</var><sub>dest</sub>[16 to 31]
temp[16 to 31]   = <var>xmm</var><sub>dest</sub>[32 to 47]       + <var>xmm</var><sub>dest</sub>[48 to 63]
temp[32 to 47]   = <var>xmm</var><sub>dest</sub>[64 to 79]       + <var>xmm</var><sub>dest</sub>[80 to 95]
temp[48 to 63]   = <var>xmm</var><sub>dest</sub>[96 to 111]      + <var>xmm</var><sub>dest</sub>[112 to 127]
temp[64 to 79]   = <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>[0 to 15]   + <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>[16 to 31]
temp[80 to 95]   = <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>[32 to 47]  + <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>[48 to 63]
temp[96 to 111]  = <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>[64 to 79]  + <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>[80 to 95]
temp[112 to 127] = <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>[96 to 111] + <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>[112 to 127]
<var>xmm</var><sub>dest</sub> = temp</code></pre>
<p>As is the case with most SSE instructions, <code>phaddw</code> does not affect the HO bits of the corresponding YMM destination register, only the LO 128 bits.</p>
<p>The 128-bit <code>vphaddw</code> instruction has the following syntax:</p>
<pre><code>vphaddw <var>xmm</var><span class="SubscriptLiteral">dest</span>, <var>xmm</var><span class="SubscriptLiteral">src1</span>, <var>xmm</var><span class="SubscriptLiteral">src2</span>/<var>mem</var><span class="SubscriptLiteral">128</span></code></pre>
<p>It computes the following:</p>
<pre><code><var>xmm</var><sub>dest</sub>[0 to 15]    = <var>xmm</var><sub>src1</sub>[0 to 15]         + <var>xmm</var><sub>src1</sub>[16 to 31]
<var>xmm</var><sub>dest</sub>[16 to 31]   = <var>xmm</var><sub>src1</sub>[32 to 47]        + <var>xmm</var><sub>src1</sub>[48 to 63]
<var>xmm</var><sub>dest</sub>[32 to 47]   = <var>xmm</var><sub>src1</sub>[64 to 79]        + <var>xmm</var><sub>src1</sub>[80 to 95]
<var>xmm</var><sub>dest</sub>[48 to 63]   = <var>xmm</var><sub>src1</sub>[96 to 111]       + <var>xmm</var><sub>src1</sub>[112 to 127]
<var>xmm</var><sub>dest</sub>[64 to 79]   = <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>[0 to 15]   + <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>[16 to 31]
<var>xmm</var><sub>dest</sub>[80 to 95]   = <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>[32 to 47]  + <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>[48 to 63]
<var>xmm</var><sub>dest</sub>[96 to 111]  = <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>[64 to 79]  + <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>[80 to 95]
<var>xmm</var><sub>dest</sub>[111 to 127] = <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>[96 to 111] + <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>[112 to 127]</code></pre>
<p>The <code>vphaddw</code> instruction zeroes out the HO 128 bits of the corresponding YMM destination register.</p>
<p>The 256-bit <code>vphaddw</code> instruction has the following syntax:</p>
<pre><code>vphaddw <var>ymm</var><span class="SubscriptLiteral">dest</span>, <var>ymm</var><span class="SubscriptLiteral">src1</span>, <var>ymm</var><span class="SubscriptLiteral">src2</span>/<var>mem</var><span class="SubscriptLiteral">256</span></code></pre>
<p><code>vphaddw</code> does not simply extend the 128-bit version in the intuitive way. Instead, it mixes up computations as follows (where <code>SRC1</code> is YMM<sub>src1</sub> and <code>SRC2</code> is YMM<sub>src2</sub>/mem<sub>256</sub>):</p>
<pre><code><var>ymm</var><sub>dest</sub>[0 to 15]    = SRC1[16 to 31]   + SRC1[0 to 15]
<var>ymm</var><sub>dest</sub>[16 to 31]   = SRC1[48 to 63]   + SRC1[32 to 47]
<var>ymm</var><sub>dest</sub>[32 to 47]   = SRC1[80 to 95]   + SRC1[64 to 79]
<var>ymm</var><sub>dest</sub>[48 to 63]   = SRC1[112 to 127] + SRC1[96 to 111]
<var>ymm</var><sub>dest</sub>[64 to 79]   = SRC2[16 to 31]   + SRC2[0 to 15]
<var>ymm</var><sub>dest</sub>[80 to 95]   = SRC2[48 to 63]   + SRC2[32 to 47]
<var>ymm</var><sub>dest</sub>[96 to 111]  = SRC2[80 to 95]   + SRC2[64 to 79]
<var>ymm</var><sub>dest</sub>[112 to 127] = SRC2[112 to 127] + SRC2[96 to 111]
<var>ymm</var><sub>dest</sub>[128 to 143] = SRC1[144 to 159] + SRC1[128 to 143]
<var>ymm</var><sub>dest</sub>[144 to 159] = SRC1[176 to 191] + SRC1[160 to 175]
<span epub:type="pagebreak" title="652" id="Page_652"/><var>ymm</var><sub>dest</sub>[160 to 175] = SRC1[208 to 223] + SRC1[192 to 207]
<var>ymm</var><sub>dest</sub>[176 to 191] = SRC1[240 to 255] + SRC1[224 to 239]
<var>ymm</var><sub>dest</sub>[192 to 207] = SRC2[144 to 159] + SRC2[128 to 143]
<var>ymm</var><sub>dest</sub>[208 to 223] = SRC2[176 to 191] + SRC2[160 to 175]
<var>ymm</var><sub>dest</sub>[224 to 239] = SRC2[208 to 223] + SRC2[192 to 207]
<var>ymm</var><sub>dest</sub>[240 to 255] = SRC2[240 to 255] + SRC2[224 to 239]</code></pre>
<h3 id="h2-501089c11-0030">11.10.3	Double-Word–Sized Horizontal Additions</h3>
<p class="BodyFirst">The <code>phaddd</code> instruction has the following syntax:</p>
<pre><code>phaddd <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub></code></pre>
<p>It computes the following:</p>
<pre><code>temp[0 to 31]   = <var>xmm</var><sub>dest</sub>[0 to 31]       + <var>xmm</var><sub>dest</sub>[32 to 63]
temp[32 to 63]  = <var>xmm</var><sub>dest</sub>[64 to 95]      + <var>xmm</var><sub>dest</sub>[96 to 127]
temp[64 to 95]  = <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>[0 to 31]  + <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>[32 to 63]
temp[96 to 127] = <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>[64 to 95] + <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>[96 to 127]
<var>xmm</var><sub>dest</sub> = temp</code></pre>
<p>The 128-bit <code>vphaddd</code> instruction has this syntax:</p>
<pre><code>vphaddd <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub></code></pre>
<p>It computes the following:</p>
<pre><code><var>xmm</var><span class="SubscriptLiteral">dest</span>[0 to 31]     = <var>xmm</var><span class="SubscriptLiteral">src1</span>[0 to 31]        + <var>xmm</var><span class="SubscriptLiteral">src1</span>[32 to 63]
<var>xmm</var><span class="SubscriptLiteral">dest</span>[32 to 63]    = <var>xmm</var><span class="SubscriptLiteral">src1</span>[64 to 95]       + <var>xmm</var><span class="SubscriptLiteral">src1</span>[96 to 127]
<var>xmm</var><span class="SubscriptLiteral">dest</span>[64 to 95]    = <var>xmm</var><span class="SubscriptLiteral">src2</span>/<var>mem</var><span class="SubscriptLiteral">128</span>[0 to 31]  + <var>xmm</var><span class="SubscriptLiteral">src2</span>/<var>mem</var><span class="SubscriptLiteral">128</span>[32 to 63]
<var>xmm</var><span class="SubscriptLiteral">dest</span>[96 to 127]   = <var>xmm</var><span class="SubscriptLiteral">src2</span>/<var>mem</var><span class="SubscriptLiteral">128</span>[64 to 95] + <var>xmm</var><span class="SubscriptLiteral">src2</span>/<var>mem</var><span class="SubscriptLiteral">128</span>[96 to 127]
(<var>ymm</var><span class="SubscriptLiteral">dest</span>[128 to 255] = 0)</code></pre>
<p>Like <code>vphaddw</code>, the 256-bit <code>vphaddd</code> instruction has the following syntax:</p>
<pre><code>vphaddd <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub></code></pre>
<p>It calculates the following:</p>
<pre><code><var>ymm</var><sub>dest</sub>[0 to 31]    = <var>ymm</var><sub>src1</sub>[32 to 63]         + <var>ymm</var><sub>src1</sub>[0 to 31]
<var>ymm</var><sub>dest</sub>[32 to 63]   = <var>ymm</var><sub>src1</sub>[96 to 127]        + <var>ymm</var><sub>src1</sub>[64 to 95]
<var>ymm</var><sub>dest</sub>[64 to 95]   = <var>ymm</var><sub>src2</sub>/mem<sub>128</sub>[32 to 63]   + <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>[0 to 31]
<var>ymm</var><sub>dest</sub>[96 to 127]  = <var>ymm</var><sub>src2</sub>/mem<sub>128</sub>[96 to 127]  + <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>[64 to 95]
<var>ymm</var><sub>dest</sub>[128 to 159] = <var>ymm</var><sub>src1</sub>[160 to 191]       + <var>ymm</var><sub>src1</sub>[128 to 159]
<var>ymm</var><sub>dest</sub>[160 to 191] = <var>ymm</var><sub>src1</sub>[224 to 255]       + <var>ymm</var><sub>src1</sub>[192 to 223]
<var>ymm</var><sub>dest</sub>[192 to 223] = <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>[160 to 191] + <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>[128 to 159]
<var>ymm</var><sub>dest</sub>[224 to 255] = <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>[224 to 255] + <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>[192 to 223]</code></pre>
<p>If an overflow occurs during the horizontal addition, <code>(v)phaddw</code> and <code>(v)phaddd</code> simply ignore the overflow and store the LO 16 or 32 bits of the result into the destination location. </p>
<p><span epub:type="pagebreak" title="653" id="Page_653"/>The <code>(v)phaddsw</code> instructions take the following forms:</p>
<pre><code>phaddsw  <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>
vphaddsw <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>
vphaddsw <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub></code></pre>
<p>The <code>(v)phaddsw</code> instruction (<em>horizontal signed integer add with saturate, word</em>) is a slightly different form of <code>(v)phaddw</code>: rather than storing only the LO bits into the result in the destination lane, this instruction saturates the result. <em>Saturation</em> means that any (positive) overflow results in the value 7FFFh, regardless of the actual result. Likewise, any negative underflow results in the value 8000h. </p>
<p>Saturation arithmetic works well for audio and video processing. If you were using standard (wraparound/modulo) addition when adding two sound samples together, the result would be horrible clicking sounds. Saturation, on the other hand, simply produces a clipped audio signal. While this is not ideal, it sounds considerably better than the results from modulo arithmetic. Similarly, for video processing, saturation produces a washed-out (white) color versus the bizarre colors that result from modulo arithmetic.</p>
<p>Sadly, there is no horizontal add with saturation for double-word operands (for example, to handle 24-bit audio).</p>
<h3 id="h2-501089c11-0031">11.10.4	SIMD Integer Subtraction</h3>
<p class="BodyFirst">The SIMD integer subtraction instructions appear in <a href="#table11-14" id="tableanchor11-14">Table 11-14</a>. As for the SIMD addition instructions, they do not affect any flags; any carry, borrow, overflow, or underflow information is lost. These instructions subtract the second source operand from the first source operand (which is also the destination operand for the SSE-only instructions) and store the result into the destination operand.</p>
<figure>
<figcaption class="TableTitle"><p><a id="table11-14">Table 11-14</a>: SIMD Integer Subtraction Instructions</p></figcaption>
<table id="table-501089c11-0014" border="1">
<thead>
<tr>
<td><b>Instruction</b></td>
<td><b>Operands</b></td>
<td><b>Description</b></td>
</tr>
</thead>
<tbody>
<tr>
<td><code>psubb</code></td>
<td><var>xmm</var><sub>dest</sub>, <var>xmm</var>/<var>mem</var><sub>128</sub></td>
<td>16-lane byte subtraction</td>
</tr>
<tr>
<td><code>vpsubb</code></td>
<td><var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>, <var>xmm</var>/<var>mem</var><sub>128</sub></td>
<td>16-lane byte subtraction</td>
</tr>
<tr>
<td><code>vpsubb</code></td>
<td><var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src</sub>, <var>ymm</var>/<var>mem</var><sub>256</sub></td>
<td>32-lane byte subtraction</td>
</tr>
<tr>
<td><code>psubw</code></td>
<td><var>xmm</var><sub>dest</sub>, <var>xmm</var>/<var>mem</var><sub>128</sub></td>
<td>8-lane word subtraction</td>
</tr>
<tr>
<td><code>vpsubw</code></td>
<td><var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>, <var>xmm</var>/<var>mem</var><sub>128</sub></td>
<td>8-lane word subtraction</td>
</tr>
<tr>
<td><code>vpsubw</code></td>
<td><var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src</sub>, <var>ymm</var>/<var>mem</var><sub>256</sub></td>
<td>16-lane word subtraction</td>
</tr>
<tr>
<td><code>psubd</code></td>
<td><var>xmm</var><sub>dest</sub>, <var>xmm</var>/<var>mem</var><sub>128</sub></td>
<td>4-lane dword subtraction</td>
</tr>
<tr>
<td><code>vpsubd</code></td>
<td><var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>, <var>xmm</var>/<var>mem</var><sub>128</sub></td>
<td>4-lane dword subtraction</td>
</tr>
<tr>
<td><code>vpsubd</code></td>
<td><var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src</sub>, <var>ymm</var>/<var>mem</var><sub>256</sub></td>
<td>8-lane dword subtraction</td>
</tr>
<tr>
<td><code>psubq</code></td>
<td><var>xmm</var><sub>dest</sub>, <var>xmm</var>/<var>mem</var><sub>128</sub></td>
<td>2-lane qword subtraction</td>
</tr>
<tr>
<td><code>vpsubq</code></td>
<td><var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>, <var>xmm</var>/<var>mem</var><sub>128</sub></td>
<td>2-lane qword subtraction</td>
</tr>
<tr>
<td><code>vpsubq</code></td>
<td><var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src</sub>, <var>ymm</var>/<var>mem</var><sub>256</sub></td>
<td>4-lane qword subtraction</td>
</tr>
</tbody>
</table>
</figure>
<p><span epub:type="pagebreak" title="654" id="Page_654"/>The <code>(v)phsubw</code>, <code>(v)phsubd</code>, and <code>(v)phsubsw</code> horizontal subtraction instructions work just like the horizontal addition instructions, except (of course) they compute the difference of the two source operands rather than the sum. See the previous sections for details on the horizontal addition instructions.</p>
<p>Likewise, there is a set of signed and unsigned byte and word saturating subtraction instructions (see <a href="#table11-15" id="tableanchor11-15">Table 11-15</a>). For the signed instructions, the byte-sized instructions saturate positive overflow to 7Fh (+127) and negative underflow to 80h (–128). The word-sized instructions saturate to 7FFFh (+32,767) and 8000h (–32,768). The unsigned saturation instructions saturate to 0FFFFh (+65,535) and 0.</p>
<figure>
<figcaption class="TableTitle"><p><a id="table11-15">Table 11-15</a>: SIMD Integer Saturating Subtraction Instructions</p></figcaption>
<table id="table-501089c11-0015" border="1">
<thead>
<tr>
<td><b>Instruction</b></td>
<td><b>Operands</b></td>
<td><b>Description</b></td>
</tr>
</thead>
<tbody>
<tr>
<td><code>psubsb</code></td>
<td><var>xmm</var><sub>dest</sub>, <var>xmm</var>/<var>mem</var><sub>128</sub></td>
<td>16-lane byte signed saturation subtraction</td>
</tr>
<tr>
<td><code>vpsubsb</code></td>
<td><var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>, <var>xmm</var>/<var>mem</var><sub>128</sub></td>
<td>16-lane byte signed saturation subtraction</td>
</tr>
<tr>
<td><code>vpsubsb</code></td>
<td><var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src</sub>, <var>ymm</var>/<var>mem</var><sub>256</sub></td>
<td>32-lane byte signed saturation subtraction</td>
</tr>
<tr>
<td><code>psubsw</code></td>
<td><var>xmm</var><sub>dest</sub>, <var>xmm</var>/<var>mem</var><sub>128</sub></td>
<td>8-lane word signed saturation subtraction</td>
</tr>
<tr>
<td><code>vpsubsw</code></td>
<td><var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>, <var>xmm</var>/<var>mem</var><sub>128</sub></td>
<td>8-lane word signed saturation subtraction</td>
</tr>
<tr>
<td><code>vpsubsw</code></td>
<td><var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src</sub>, <var>ymm</var>/<var>mem</var><sub>256</sub></td>
<td>16-lane word signed saturation subtraction</td>
</tr>
<tr>
<td><code>psubusb</code></td>
<td><var>xmm</var><sub>dest</sub>, <var>xmm</var>/<var>mem</var><sub>128</sub></td>
<td>16-lane byte unsigned saturation subtraction</td>
</tr>
<tr>
<td><code>vpsubusb</code></td>
<td><var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>, <var>xmm</var>/<var>mem</var><sub>128</sub></td>
<td>16-lane byte unsigned saturation subtraction</td>
</tr>
<tr>
<td><code>vpsubusb</code></td>
<td><var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src</sub>, <var>ymm</var>/<var>mem</var><sub>256</sub></td>
<td>32-lane byte unsigned saturation subtraction</td>
</tr>
<tr>
<td><code>psubusw</code></td>
<td><var>xmm</var><sub>dest</sub>, <var>xmm</var>/<var>mem</var><sub>128</sub></td>
<td>8-lane word unsigned saturation subtraction</td>
</tr>
<tr>
<td><code>vpsubusw</code></td>
<td><var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>, <var>xmm</var>/<var>mem</var><sub>128</sub></td>
<td>8-lane word unsigned saturation subtraction</td>
</tr>
<tr>
<td><code>vpsubusw</code></td>
<td><var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src</sub>, <var>ymm</var>/<var>mem</var><sub>256</sub></td>
<td>16-lane word unsigned saturation subtraction</td>
</tr>
</tbody>
</table>
</figure>
<h3 id="h2-501089c11-0032">11.10.5	SIMD Integer Multiplication</h3>
<p class="BodyFirst">The SSE/AVX instruction set extensions <em>somewhat</em> support multiplication. Lane-by-lane multiplication requires that the result of an operation on two <em>n</em>-bit values fits in <em>n</em> bits, but <em>n </em>× <em>n</em> multiplication can produce a 2×<em>n</em>-bit result. So a lane-by-lane multiplication operation creates problems as overflow is lost. The basic packed integer multiplication multiplies a pair of lanes and stores the LO bits of the result in the destination lane. For extended arithmetic, packed integer multiplication instructions produce the HO bits of the result.</p>
<p>The instructions in <a href="#table11-16" id="tableanchor11-16">Table 11-16</a> handle 16-bit multiplication operations. The <code>(v)pmullw</code> instruction multiplies the 16-bit values appearing in the lanes of the source operand and stores the LO word of the result into the corresponding destination lane. This instruction is applicable to both signed and unsigned values. The <code>(v)pmulhw</code> instruction computes the product of <span epub:type="pagebreak" title="655" id="Page_655"/>two signed word values and stores the <em>HO word</em> of the result into the destination lanes. For unsigned operands, <code>(v)pmulhuw</code> performs the same task. By executing both <code>(v)pmullw</code> and <code>(v)pmulh(u)w</code> with the same operands, you can compute the full 32-bit result of a 16×16-bit multiplication. (You can use the <code>punpck*</code> instructions to merge the results into 32-bit integers.)</p>
<figure>
<figcaption class="TableTitle"><p><a id="table11-16">Table 11-16</a>: SIMD 16-Bit Packed Integer Multiplication Instructions</p></figcaption>
<table id="table-501089c11-0016" border="1">
<thead>
<tr>
<td><b>Instruction</b></td>
<td><b>Operands</b></td>
<td><b>Description</b></td>
</tr>
</thead>
<tbody>
<tr>
<td><code>pmullw</code></td>
<td><var>xmm</var><sub>dest</sub>, <var>xmm</var>/<var>mem</var><sub>128</sub></td>
<td>8-lane word multiplication, producing the LO word of the product</td>
</tr>
<tr>
<td><code>vpmullw</code></td>
<td><var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>, <var>xmm</var>/<var>mem</var><sub>128</sub></td>
<td>8-lane word multiplication, producing the LO word of the product</td>
</tr>
<tr>
<td><code>vpmullw</code></td>
<td><var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src</sub>, <var>ymm</var>/<var>mem</var><sub>256</sub></td>
<td>16-lane word multiplication, producing the LO word of the product</td>
</tr>
<tr>
<td><code>pmulhuw</code></td>
<td><var>xmm</var><sub>dest</sub>, <var>xmm</var>/<var>mem</var><sub>128</sub></td>
<td>8-lane word unsigned multiplication, producing the HO word of the product</td>
</tr>
<tr>
<td><code>vpmulhuw</code></td>
<td><var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>, <var>xmm</var>/<var>mem</var><sub>128</sub></td>
<td>8-lane word unsigned multiplication, producing the HO word of the product</td>
</tr>
<tr>
<td><code>vpmulhuw</code></td>
<td><var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src</sub>, <var>ymm</var>/<var>mem</var><sub>256</sub></td>
<td>16-lane word unsigned multiplication, producing the HO word of the product</td>
</tr>
<tr>
<td><code>pmulhw</code></td>
<td><var>xmm</var><sub>dest</sub>, <var>xmm</var>/<var>mem</var><sub>128</sub></td>
<td>8-lane word signed multiplication, producing the HO word of the product</td>
</tr>
<tr>
<td><code>vpmulhw</code></td>
<td><var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>, <var>xmm</var>/<var>mem</var><sub>128</sub></td>
<td>8-lane word signed multiplication, producing the HO word of the product</td>
</tr>
<tr>
<td><code>vpmulhw</code></td>
<td><var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src</sub>, <var>ymm</var>/<var>mem</var><sub>256</sub></td>
<td>16-lane word signed multiplication, producing the HO word of the product</td>
</tr>
</tbody>
</table>
</figure>
<p><a href="#table11-17" id="tableanchor11-17">Table 11-17</a> lists the 32- and 64-bit versions of the packed multiplication instructions. There are no <code>(v)pmulhd</code> or <code>(v)pmulhq</code> instructions; see <code>(v)pmuludq</code> and <code>(v)pmuldq</code> to handle 32- and 64-bit packed multiplication.</p>
<figure>
<figcaption class="TableTitle"><p><a id="table11-17">Table 11-17</a>: SIMD 32- and 64-Bit Packed Integer Multiplication Instructions</p></figcaption>
<table id="table-501089c11-0017" border="1">
<thead>
<tr>
<td><b>Instruction</b></td>
<td><b>Operands</b></td>
<td><b>Description</b></td>
</tr>
</thead>
<tbody>
<tr>
<td><code>pmulld</code></td>
<td><var>xmm</var><sub>dest</sub>, <var>xmm</var>/<var>mem</var><sub>128</sub></td>
<td>4-lane dword multiplication, producing the LO dword of the product</td>
</tr>
<tr>
<td><code>vpmulld</code></td>
<td><var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>, <var>xmm</var>/<var>mem</var><sub>128</sub></td>
<td>4-lane dword multiplication, producing the LO dword of the product</td>
</tr>
<tr>
<td><code>vpmulld</code></td>
<td><var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src</sub>, <var>ymm</var>/<var>mem</var><sub>256</sub></td>
<td>8-lane dword multiplication, producing the LO dword of the product</td>
</tr>
<tr>
<td><code>vpmullq</code></td>
<td><var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>, <var>xmm</var>/<var>mem</var><sub>128</sub></td>
<td>2-lane qword multiplication, producing the LO qword of the product</td>
</tr>
<tr>
<td><code>vpmullq</code></td>
<td><var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src</sub>, <var>ymm</var>/<var>mem</var><sub>256</sub></td>
<td>4-lane qword multiplication, producing the LO qword of the product (available on only AVX-512 CPUs)</td>
</tr>
</tbody>
</table>
</figure>
<p><span epub:type="pagebreak" title="656" id="Page_656"/>At some point along the way, Intel introduced <code>(v)pmuldq</code> and <code>(v)pmuludq</code> to perform signed and unsigned 32×32-bit multiplications, producing a 64-bit result. The syntax for these instructions is as follows:</p>
<pre><code>pmuldq   <var>xmm</var><sub>dest</sub>, <var>xmm</var>/<var>mem</var><sub>128</sub>
vpmuldq  <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var>/<var>mem</var><sub>128</sub>
vpmuldq  <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var>/<var>mem</var><sub>256</sub>

pmuludq  <var>xmm</var><sub>dest</sub>, <var>xmm</var>/<var>mem</var><sub>128</sub>
vpmuludq <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var>/<var>mem</var><sub>128</sub>
vpmuludq <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var>/<var>mem</var><sub>256</sub></code></pre>
<p>The 128-bit variants multiply the double words appearing in lanes 0 and 2 and store the 64-bit results into qword lanes 0 and 1 (dword lanes 0 and 1 and 2 and 3). On CPUs with AVX registers,<sup class="FootnoteReference"><a id="c11-footnoteref-11" href="#c11-footnote-11">11</a></sup> <code>pmuldq</code> and <code>pmuludq</code> do not affect the HO 128 bits of the YMM register. The <code>vpmuldq</code> and <code>vpmuludq</code> instructions zero-extend the result to 256 bits. The 256-bit variants multiply the double words appearing in lanes 0, 2, 4, and 6, producing 64-bit results that they store in qword lanes 0, 1, 2, and 3 (dword lanes 0 and 1, 2 and 3, 4 and 5, and 6 and 7 ). </p>
<p>The <code>pclmulqdq</code> instruction provides the ability to multiply two qword values, producing a 128-bit result. Here is the syntax for this instruction:</p>
<pre><code>pclmulqdq  <var>xmm</var><sub>dest</sub>, <var>xmm</var>/<var>mem</var><sub>128</sub>, <var>imm</var><sub>8</sub>
vpclmulqdq <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>, <var>imm</var><sub>8</sub></code></pre>
<p>These instructions multiply a pair of qword values found in XMM<sub>dest</sub> and XMM<sub>src</sub> and leave the 128-bit result in XMM<sub>dest</sub>. The imm<sub>8</sub> operand specifies which qwords to use as the source operands. <a href="#table11-18" id="tableanchor11-18">Table 11-18</a> lists the possible combinations for <code>pclmulqdq</code>. <a href="#table11-19" id="tableanchor11-19">Table 11-19</a> lists the combinations for <code>vpclmulqdq</code>.</p>
<figure>
<figcaption class="TableTitle"><p><a id="table11-18">Table 11-18</a>: imm<sub>8</sub> Operand Values for <code>pclmulqdq</code> Instruction</p></figcaption>
<table id="table-501089c11-0018" border="1">
<thead>
<tr>
<td><b>imm<sub>8</sub></b></td>
<td><b>Result</b></td>
</tr>
</thead>
<tbody>
<tr>
<td>00h</td>
<td>XMM<sub>dest</sub> = XMM<sub>dest</sub>[0 to 63]     * XMM/mem<sub>128</sub>[0 to 63] </td>
</tr>
<tr>
<td>01h</td>
<td>XMM<sub>dest</sub> = XMM<sub>dest</sub>[64 to 127] * XMM/mem<sub>128</sub>[0 to 63]</td>
</tr>
<tr>
<td>10h</td>
<td>XMM<sub>dest</sub> = XMM<sub>dest</sub>[0 to 63]     * XMM/mem<sub>128</sub>[64 to 127]</td>
</tr>
<tr>
<td>11h</td>
<td>XMM<sub>dest</sub> = XMM<sub>dest</sub>[64 to 127] * XMM/mem<sub>128</sub>[64 to 127]</td>
</tr>
</tbody>
</table>
</figure>
<figure>
<figcaption class="TableTitle"><p><a id="table11-19">Table 11-19</a>: imm<sub>8</sub> Operand Values for <code>vpclmulqdq</code> Instruction</p></figcaption>
<table id="table-501089c11-0019" border="1">
<thead>
<tr>
<td><b>imm<sub>8</sub></b></td>
<td><b>Result</b></td>
</tr>
</thead>
<tbody>
<tr>
<td>00h</td>
<td>XMM<sub>dest</sub> = XMM<sub>src1</sub>[0 to 63]     * XMM<sub>src2</sub>/mem<sub>128</sub>[0 to 63] </td>
</tr>
<tr>
<td>01h</td>
<td>XMM<sub>dest</sub> = XMM<sub>src1</sub>[64 to 127] * XMM<sub>src2</sub>/mem<sub>128</sub>[0 to 63]</td>
</tr>
<tr>
<td>10h</td>
<td>XMM<sub>dest</sub> = XMM<sub>src1</sub>[0 to 63]     * XMM<sub>src2</sub>/mem<sub>128</sub>[64 to 127]</td>
</tr>
<tr>
<td>11h</td>
<td>XMM<sub>dest</sub> = XMM<sub>src1</sub>[64 to 127] * XMM<sub>src2</sub>/mem<sub>128</sub>[64 to 127]</td>
</tr>
</tbody>
</table>
</figure>
<p><span epub:type="pagebreak" title="657" id="Page_657"/>As usual, <code>pclmulqdq</code> leaves the HO 128 bits of the corresponding YMM destination register unchanged, while <code>vpcmulqdq</code> zeroes those bits.</p>
<h3 id="h2-501089c11-0033">11.10.6	SIMD Integer Averages</h3>
<p class="BodyFirst">The <code>(v)pavgb</code> and <code>(v)pavgw</code> instructions compute the average of two sets of bytes or words. These instructions sum the value in the byte or word lanes of their source and destination operands, divide the result by 2, round the results, and leave the averaged results sitting in the destination operand lanes. The syntax for these instructions is shown here:</p>
<pre><code>pavgb  <var>xmm</var><sub>dest</sub>, <var>xmm</var>/<var>mem</var><sub>128</sub>
vpavgb <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>
vpavgb <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub>
pavgw  <var>xmm</var><sub>dest</sub>, <var>xmm</var>/<var>mem</var><sub>128</sub>
vpavgw <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>
vpavgw <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub></code></pre>
<p>The 128-bit <code>pavgb</code> and <code>vpavgb</code> instructions compute 16 byte-sized averages (for the 16 lanes in the source and destination operands). The 256-bit variant of the <code>vpavgb</code> instruction computes 32 byte-sized averages.</p>
<p>The 128-bit <code>pavgw</code> and <code>vpavgw</code> instructions compute eight word-sized averages (for the eight lanes in the source and destination operands). The 256-bit variant of the <code>vpavgw</code> instruction computes 16 byte-sized averages.</p>
<p>The <code>vpavgb</code> and <code>vpavgw</code> instructions compute the average of the first XMM or YMM source operand and the second XMM, YMM, or mem source operand, storing the average in the destination XMM or YMM register.</p>
<p>Unfortunately, there are no <code>(v)pavgd</code> or <code>(v)pavgq</code> instructions. No doubt, these instructions were originally intended for mixing 8- and 16-bit audio or video streams (or photo manipulation), and the x86-64 CPU designers never felt the need to extend this beyond 16 bits (even though 24-bit audio is common among professional audio engineers).</p>
<h3 id="h2-501089c11-0034">11.10.7	SIMD Integer Minimum and Maximum</h3>
<p class="BodyFirst">The SSE4.1 instruction set extensions added eight packed integer <em>minimum</em> and <em>maximum</em> instructions, as shown in <a href="#table11-20" id="tableanchor11-20">Table 11-20</a>. These instructions scan the lanes of a pair of 128- or 256-bit operands and copy the maximum or minimum value from that lane to the same lane in the destination operand.</p>
<figure>
<figcaption class="TableTitle"><p><a id="table11-20">Table 11-20</a>: SIMD Minimum and Maximum Instructions</p></figcaption>
<table id="table-501089c11-0020" border="1">
<thead>
<tr>
<td><b>Instruction</b></td>
<td><b>Description</b></td>
</tr>
</thead>
<tbody>
<tr>
<td><code>(v)</code><code>pmaxsb</code></td>
<td>Destination byte lanes set to the maximum value of the two signed byte values found in the corresponding source lanes.</td>
</tr>
<tr>
<td><code>(v)</code><code>pmaxsw</code></td>
<td>Destination word lanes set to the maximum value of the two signed word values found in the corresponding source lanes.</td>
</tr>
<tr>
<td><code><span epub:type="pagebreak" title="658" id="Page_658"/>(v)</code><code>pmaxsd</code></td>
<td>Destination dword lanes set to the maximum value of the two signed dword values found in the corresponding source lanes.</td>
</tr>
<tr>
<td><code>v</code><code>pmaxsq</code></td>
<td>Destination qword lanes set to the maximum value of the two signed qword values found in the corresponding source lanes. (AVX-512 required for this instruction.)</td>
</tr>
<tr>
<td><code>(v)</code><code>pmaxub</code></td>
<td>Destination byte lanes set to the maximum value of the two unsigned byte values found in the corresponding source lanes.</td>
</tr>
<tr>
<td><code>(v)</code><code>pmaxuw</code></td>
<td>Destination word lanes set to the maximum value of the two unsigned word values found in the corresponding source lanes.</td>
</tr>
<tr>
<td><code>(v)</code><code>pmaxud</code></td>
<td>Destination dword lanes set to the maximum value of the two unsigned dword values found in the corresponding source lanes.</td>
</tr>
<tr>
<td><code>v</code><code>pmaxuq</code></td>
<td>Destination qword lanes set to the maximum value of the two unsigned qword values found in the corresponding source lanes. (AVX-512 required for this instruction.)</td>
</tr>
<tr>
<td><code>(v)</code><code>pminsb</code></td>
<td>Destination byte lanes set to the minimum value of the two signed byte values found in the corresponding source lanes.</td>
</tr>
<tr>
<td><code>(v)</code><code>pminsw</code></td>
<td>Destination word lanes set to the minimum value of the two signed word values found in the corresponding source lanes.</td>
</tr>
<tr>
<td><code>(v)</code><code>pminsd</code></td>
<td>Destination dword lanes set to the minimum value of the two signed dword values found in the corresponding source lanes.</td>
</tr>
<tr>
<td><code>v</code><code>pminsq</code></td>
<td>Destination qword lanes set to the minimum value of the two signed qword values found in the corresponding source lanes. (AVX-512- required for this instruction.)</td>
</tr>
<tr>
<td><code>(v)</code><code>pminub</code></td>
<td>Destination byte lanes set to the minimum value of the two unsigned byte values found in the corresponding source lanes.</td>
</tr>
<tr>
<td><code>(v)</code><code>pminuw</code></td>
<td>Destination word lanes set to the minimum value of the two unsigned word values found in the corresponding source lanes.</td>
</tr>
<tr>
<td><code>(v)</code><code>pminud</code></td>
<td>Destination dword lanes set to the minimum value of the two unsigned dword values found in the corresponding source lanes.</td>
</tr>
<tr>
<td><code>v</code><code>pminuq</code></td>
<td>Destination qword lanes set to the minimum value of the two unsigned qword values found in the corresponding source lanes. (AVX-512 required for this instruction.)</td>
</tr>
</tbody>
</table>
</figure>
<p>The generic syntax for these instructions is as follows:<sup class="FootnoteReference"><a id="c11-footnoteref-12" href="#c11-footnote-12">12</a></sup></p>
<pre><code>pm<var>xxyz</var>  <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>
vpm<var>xxyz</var> <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>
vpm<var>xxyz</var> <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub></code></pre>
<p>The SSE instructions compute the minimum or maximum of the corresponding lanes in the source and destination operands and store the minimum or maximum result into the corresponding lanes in the destination register. The AVX instructions compute the minimum or maximum of the <span epub:type="pagebreak" title="659" id="Page_659"/>values in the same lanes of the two source operands and store the minimum or maximum result into the corresponding lanes of the destination register.</p>
<h3 id="h2-501089c11-0035">11.10.8	SIMD Integer Absolute Value</h3>
<p class="BodyFirst">The SSE/AVX instruction set extensions provide three sets of instructions for computing the absolute values of signed byte, word, and double-word integers: <code>(v)pabsb</code>, <code>(v)pabsw</code>, and <code>(v)pabsd</code>.<sup class="FootnoteReference"><a id="c11-footnoteref-13" href="#c11-footnote-13">13</a></sup> The syntax for these instructions is the following:</p>
<pre><code>pabsb  <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>
vpabsb <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>
vpabsb <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src</sub>/<var>mem</var><sub>256</sub>

pabsw  <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>
vpabsw <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>
vpabsw <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src</sub>/<var>mem</var><sub>256</sub>

pabsd  <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>
vpabsd <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>
vpabsd <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src</sub>/<var>mem</var><sub>256</sub></code></pre>
<p>When operating on a system that supports AVX registers, the SSE <code>pabsb</code>, <code>pabsw</code>, and <code>pabsd</code> instructions leave the upper bits of the YMM registers unmodified. The 128-bit versions of the AVX instructions (<code>vpabsb</code>, <code>vpabsw</code>, and <code>vpabsd</code>) zero-extend the result through the upper bits.</p>
<h3 id="h2-501089c11-0036">11.10.9	SIMD Integer Sign Adjustment Instructions</h3>
<p class="BodyFirst">The <code>(v)psignb</code>, <code>(v)psignw</code>, and <code>(v)psignd </code>instructions apply the sign found in a source lane to the corresponding destination lane. The algorithm works as follows:</p>
<pre><code>if source lane value is less than zero then
    negate the corresponding destination lane
else if source lane value is equal to zero
    set the corresponding destination lane to zero
else 
    leave the corresponding destination lane unchanged</code></pre>
<p>The syntax for these instructions is the following:</p>
<pre><code>psignb  <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>
vpsignb <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>
vpsignb <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub>

psignw  <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>
vpsignw <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>
vpsignw <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub>

<span epub:type="pagebreak" title="660" id="Page_660"/>psignd  <var>xmm</var><sub><em>dest</em></sub>, <var>xmm</var><sub><em>src</em></sub>/<var>mem</var><sub><em>128</em></sub>
vpsignd <var>xmm</var><sub><em>dest</em></sub>, <var>xmm</var><sub><em>src1</em></sub>, <var>xmm</var><sub><em>src2</em></sub>/<var>mem</var><sub><em>128</em></sub>
vpsignd <var>ymm</var><sub><em>dest</em></sub>, <var>ymm</var><sub><em>src1</em></sub>, <var>ymm</var><sub><em>src2</em></sub>/<var>mem</var><sub><em>256</em></sub></code></pre>
<p>As usual, the 128-bit SSE instructions leave the upper bits of the YMM register unchanged (if applicable), and the 128-bit AVX instructions zero-extend the result into the upper bits of the YMM register.</p>
<h3 id="h2-501089c11-0037">11.10.10	SIMD Integer Comparison Instructions</h3>
<p class="BodyFirst">The <code>(v)pcmpeqb</code>, <code>(v)pcmpeqw</code>, <code>(v)pcmpeqd</code>, <code>(v)pcmpeqq</code>, <code>(v)pcmpgtb</code>, <code>(v)pcmpgtw</code>, <code>(v)pcmpgtd</code>, and <code>(v)pcmpgtq</code> instructions provide packed signed integer comparisons. These instructions compare corresponding bytes, word, dwords, or qwords (depending on the instruction suffix) in the various lanes of their operands.<sup class="FootnoteReference"><a id="c11-footnoteref-14" href="#c11-footnote-14">14</a></sup> They store the result of the comparison instruction in the corresponding destination lanes.</p>
<h4 id="h3-501089c11-0004">11.10.10.1	SSE Compare-for-Equality Instructions</h4>
<p class="BodyFirst">The syntax for the SSE <em>compare-for-equality</em> instructions (<code>pcmpeq*</code>) is shown here:</p>
<pre><code>pcmpeqb <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>  ; Compares 16 bytes
pcmpeqw <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>  ; Compares 8 words
pcmpeqd <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>  ; Compares 4 dwords
pcmpeqq <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>  ; Compares 2 qwords</code></pre>
<p>These instructions compute</p>
<pre><code><var>xmm</var><sub>dest</sub>[<var>lane</var>] = <var>xmm</var><sub>dest</sub>[<var>lane</var>] == <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>[<var>lane</var>]</code></pre>
<p class="BodyContinued">where <var>lane</var> varies from 0 to 15 for <code>pcmpeqb</code>, 0 to 7 for <code>pcmpeqw</code>, 0 to 3 for <code>pcmpeqd</code>, and 0 to 1 for <code>pcmpeqq</code>. The <code>==</code> operator produces a value of all 1 bits if the two values in the same lane are equal; it produces all 0 bits if the values are not equal.</p>
<h4 id="h3-501089c11-0005">11.10.10.2	SSE Compare-for-Greater-Than Instructions</h4>
<p class="BodyFirst">The following is the syntax for the SSE <em>compare-for-greater-than</em> instructions (<code>pcmpgt*</code>):</p>
<pre><code>pcmpgtb <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>  ; Compares 16 bytes
pcmpgtw <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>  ; Compares 8 words
pcmpgtd <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>  ; Compares 4 dwords
pcmpgtq <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>  ; Compares 2 qwords</code></pre>
<p>These instructions compute</p>
<pre><code><var>xmm</var><sub>dest</sub>[<var>lane</var>] = <var>xmm</var><sub>dest</sub>[<var>lane</var>] &gt; <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>[<var>lane</var>]</code></pre>
<p class="BodyContinued"><span epub:type="pagebreak" title="661" id="Page_661"/>where <var>lane</var> is the same as for the compare-for-equality instructions, and the <code>&gt;</code> operator produces a value of all 1 bits if the signed integer in the XMM<sub>dest</sub> lane is greater than the signed value in the corresponding XMM<sub>src</sub>/MEM<sub>128</sub> lane. </p>
<p>On AVX-capable CPUs, the SSE packed integer comparisons preserve the value in the upper bits of the underlying YMM register.</p>
<h4 id="h3-501089c11-0006">11.10.10.3	AVX Comparison Instructions</h4>
<p class="BodyFirst">The 128-bit variants of these instructions have the following syntax:</p>
<pre><code>vpcmpeqb <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>  ; Compares 16 bytes
vpcmpeqw <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>  ; Compares 8 words
vpcmpeqd <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>  ; Compares 4 dwords
vpcmpeqq <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>  ; Compares 2 qwords

vpcmpgtb <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>  ; Compares 16 bytes
vpcmpgtw <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>  ; Compares 8 words
vpcmpgtd <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>  ; Compares 4 dwords
vpcmpgtq <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>  ; Compares 2 qwords</code></pre>
<p>These instructions compute as follows:</p>
<pre><code><var>xmm</var><sub>dest</sub>[<var>lane</var>] = <var>xmm</var><sub>src1</sub>[<var>lane</var>] == <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>[<var>lane</var>]
<var>xmm</var><sub>dest</sub>[<var>lane</var>] = <var>xmm</var><sub>src1</sub>[<var>lane</var>] &gt;  <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>[<var>lane</var>]</code></pre>
<p>These AVX instructions write 0s to the upper bits of the underlying YMM register.</p>
<p>The 256-bit variants of these instructions have the following syntax:</p>
<pre><code>vpcmpeqb <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub>  ; Compares 32 bytes
vpcmpeqw <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub>  ; Compares 16 words
vpcmpeqd <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub>  ; Compares 8 dwords
vpcmpeqq <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub>  ; Compares 4 qwords

vpcmpgtb <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub>  ; Compares 32 bytes
vpcmpgtw <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub>  ; Compares 16 words
vpcmpgtd <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub>  ; Compares 8 dwords
vpcmpgtq <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub>  ; Compares 4 qwords</code></pre>
<p>These instructions compute as follows:</p>
<pre><code><var>ymm</var><sub>dest</sub>[<var>lane</var>] = <var>ymm</var><sub>src1</sub>[<var>lane</var>] == <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub>[<var>lane</var>]
<var>ymm</var><sub>dest</sub>[<var>lane</var>] = <var>ymm</var><sub>src1</sub>[<var>lane</var>] &gt;  <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub>[<var>lane</var>]</code></pre>
<p>Of course, the principal difference between the 256- and the 128-bit instructions is that the 256-bit variants support twice as many byte (32), word (16), dword (8), and qword (4) signed-integer lanes.</p>
<h4 id="h3-501089c11-0007"><span epub:type="pagebreak" title="662" id="Page_662"/>11.10.10.4	Compare-for-Less-Than Instructions</h4>
<p class="BodyFirst">There are no packed <em>compare-for-less-than</em> instructions. You can synthesize a less-than comparison by reversing the operands and using a greater-than comparison. That is, if <em>x</em> &lt; <em>y</em>, then it is also true that <em>y</em> &gt; <em>x</em>. If both packed operands are sitting in XMM or YMM registers, swapping the registers is relatively easy (especially when using the three-operand AVX instructions). If the second operand is a memory operand, you must first load that operand into a register so you can reverse the operands (a memory operand must always be the second operand).</p>
<h4 id="h3-501089c11-0008">11.10.10.5	Using Packed Comparison Results</h4>
<p class="BodyFirst">The question remains of what to do with the result you obtain from a packed comparison. SSE/AVX packed signed integer comparisons do not affect condition code flags (because they compare multiple values and only one of those comparisons could be moved into the flags). Instead, the packed comparisons simply produce Boolean results. You can use these results with the packed AND instructions (<code>pand</code>, <code>vpand</code>, <code>pandn</code>, and <code>vpandn</code>), the packed OR instructions (<code>por</code> and <code>vpor</code>), or the packed XOR instructions (<code>pxor</code> and <code>vpxor</code>) to mask or otherwise modify other packed data values. Of course, you could also extract the individual lane values and test them (via a conditional jump). The following section describes a straightforward way to achieve this.</p>
<h4 id="h3-501089c11-0009">11.10.10.6	The (v)pmovmskb Instructions</h4>
<p class="BodyFirst">The <code>(v)pmovmskb</code> instruction extracts the HO bit from all the bytes in an XMM or YMM register and stores the 16 or 32 bits (respectively) into a general-purpose register. These instructions set all HO bits of the general-purpose register to 0 (beyond those needed to hold the mask bits). The syntax is</p>
<pre><code>pmovmskb  <var>reg</var>, <var>xmm</var><sub>src</sub>
vpmovmskb <var>reg</var>, <var>xmm</var><sub>src</sub>
vpmovmskb <var>reg</var>, <var>ymm</var><sub>src</sub></code></pre>
<p class="BodyContinued">where <code>reg</code> is any 32-bit or 64-bit general-purpose integer register. The semantics for the <code>pmovmskb</code> and <code>vpmovmskb</code> instructions with an XMM source register are the same, but the encoding of <code>pmovmskb</code> is more efficient.</p>
<p>The <code>(v)pmovmskb</code> instruction copies the sign bits from each of the byte lanes into the corresponding bit position of the general-purpose register. It copies bit 7 from the XMM register (the sign bit for lane 0) into bit 0 of the destination register; it copies bit 15 from the XMM register (the sign bit for lane 1) into bit 1 of the destination register; it copies bit 23 from the XMM register (the sign bit for lane 2) into bit 2 of the destination register; and so on.</p>
<p>The 128-bit instructions fill only bits 0 through 15 of the destination register (zeroing out all other bits). The 256-bit form of the <code>vpmovmskb</code> instruction fills bits 0 through 31 of the destination register (zeroing out HO bits if you specify a 64-bit register).</p>
<p><span epub:type="pagebreak" title="663" id="Page_663"/>You can use the <code>pmovmskb</code> instruction to extract a single bit from each byte lane in an XMM or a YMM register after a <code>(v)pcmpeqb</code> or <code>(v)pcmpgtb</code> instruction. Consider the following code sequence:</p>
<pre><code>pcmpeqb  xmm0, xmm1
pmovmskb eax,  xmm0</code></pre>
<p>After the execution of these two instructions, EAX bit 0 will be 1 or 0 if byte 0 of XMM0 was equal, or not equal, to byte 0 of XMM1, respectively. Likewise, EAX bit 1 will contain the result of comparing byte 1 of XMM0 to XMM1, and so on for each of the following bytes (up to bit 15, which compares 16-byte values in XMM0 and XMM1).</p>
<p>Unfortunately, there are no <code>pmovmskw</code>, <code>pmovmskd</code>, and <code>pmovmsq</code> instructions. You can achieve the same result as <code>pmovmskw</code> by using the following code sequence:</p>
<pre><code>pcmpeqw  xmm0, xmm1
pmovmskb eax, xmm0
mov      cl, 0     ; Put result here
shr      ax, 1     ; Shift out lane 7 result
rcl      cl, 1     ; Shift bit into CL
shr      ax, 1     ; Ignore this bit
shr      ax, 1     ; Shift out lane 6 result
rcl      cl, 1     ; Shift lane 6 result into CL
shr      ax, 1     ; Ignore this bit
shr      ax, 1     ; Shift out lane 5 result
rcl      cl, 1     ; Shift lane 5 result into CL
shr      ax, 1     ; Ignore this bit
shr      ax, 1     ; Shift out lane 4 result
rcl      cl, 1     ; Shift lane 4 result into CL
shr      ax, 1     ; Ignore this bit
shr      ax, 1     ; Shift out lane 3 result
rcl      cl, 1     ; Shift lane 3 result into CL
shr      ax, 1     ; Ignore this bit
shr      ax, 1     ; Shift out lane 2 result
rcl      cl, 1     ; Shift lane 2 result into CL
shr      ax, 1     ; Ignore this bit
shr      ax, 1     ; Shift out lane 1 result
rcl      cl, 1     ; Shift lane 1 result into CL
shr      ax, 1     ; Ignore this bit
shr      ax, 1     ; Shift out lane 0 result
rcl      cl, 1     ; Shift lane 0 result into CL</code></pre>
<p>Because <code>pcmpeqw</code> produces a sequence of words (which contain either 0000h or 0FFFFh) and <code>pmovmskb</code> expects byte values, <code>pmovmskb</code> produces twice as many results as we expect, and every odd-numbered bit that <code>pmovmskb</code> produces is a duplicate of the preceding even-numbered bit (because the inputs are either 0000h or 0FFFFh). This code grabs every odd-numbered bit (starting with bit 15 and working down) and skips over the even-numbered bits. While this code is easy enough to follow, it is rather long and slow. If <span epub:type="pagebreak" title="664" id="Page_664"/>you’re willing to live with an 8-bit result for which the lane numbers don’t match the bit numbers, you can use more efficient code:</p>
<pre><code>pcmpeqw  xmm0, xmm1
pmovmskb eax, xmm0
shr      al, 1     ; Move odd bits to even positions
and      al, 55h   ; Zero out the odd bits, keep even bits
and      ah, 0aah  ; Zero out the even bits, keep odd bits
or       al, ah    ; Merge the two sets of bits</code></pre>
<p>This interleaves the lanes in the bit positions as shown in <a href="#figure11-44" id="figureanchor11-44">Figure 11-44</a>. Usually, it’s easy enough to work around this rearrangement in the software. Of course, you can also use a 256-entry lookup table (see <span class="xref" itemid="xref_target_Chapter 10">Chapter 10</span>) to rearrange the bits however you desire. Of course, if you’re just going to test the individual bits rather than use them as some sort of mask, you can directly test the bits that <code>pmovmskb</code> leaves in EAX; you don’t have to coalesce them into a single byte.</p>
<figure>
<img src="image_fi/501089c11/f11044.png" alt="f11044" class=""/>
<figcaption><p><a id="figure11-44">Figure 11-44</a>: Merging bits from <span class="LiteralInCaption"><code>pcmpeqw</code></span></p></figcaption>
</figure>
<p>When using the double-word or quad-word packed comparisons, you could also use a scheme such as the one provided here for <code>pcmpeqw</code>. However, the floating-point mask move instructions (see <span class="xref" itemid="xref_target_“The (v)movmskps, (v)movmskpd Instructions” on page 676">“The (v)movmskps, (v)movmskpd Instructions” on page 676</span>) do the job more efficiently by breaking the rule about using SIMD instructions that are appropriate for the data type.</p>
<h3 id="h2-501089c11-0038">11.10.11	Integer Conversions</h3>
<p class="BodyFirst">The SSE and AVX instruction set extensions provide various instructions that convert integer values from one form to another. There are zero- and sign-extension instructions that convert from a smaller value to a larger one. Other instructions convert larger values to smaller ones. This section covers these instructions.</p>
<h4 id="h3-501089c11-0010"><span epub:type="pagebreak" title="665" id="Page_665"/>11.10.11.1	Packed Zero-Extension Instructions</h4>
<p class="BodyFirst">The <em>move with zero-extension</em> instructions perform the conversions appearing in <a href="#table11-21" id="tableanchor11-21">Table 11-21</a>.</p>
<figure>
<figcaption class="TableTitle"><p><a id="table11-21">Table 11-21</a>: SSE4.1 and AVX Packed Zero-Extension Instructions</p></figcaption>
<table id="table-501089c11-0021" border="1">
<thead>
<tr>
<td><b>Syntax</b></td>
<td><b>Description</b></td>
</tr>
</thead>
<tbody>
<tr>
<td><code>pmovzxbw </code><var>xmm</var><sub>dest</sub><code>, </code><var>xmm</var><sub>src</sub><code>/</code><var>mem</var><sub>64</sub></td>
<td>Zero-extends a set of eight byte values in the LO 8 bytes of XMM<sub>src</sub>/mem<sub>64 </sub>to word values in XMM<sub>dest</sub>.</td>
</tr>
<tr>
<td><code>pmovzxbd </code><var>xmm</var><sub>dest</sub><code>, </code><var>xmm</var><sub>src</sub><code>/</code><var>mem</var><sub>32</sub></td>
<td>Zero-extends a set of four byte values in the LO 4 bytes of XMM<sub>src</sub>/mem<sub>32</sub> to dword values in XMM<sub>dest</sub>.</td>
</tr>
<tr>
<td><code>pmovzxbq </code><var>xmm</var><sub>dest</sub><code>, </code><var>xmm</var><sub>src</sub><code>/</code><var>mem</var><sub>16</sub></td>
<td>Zero-extends a set of two byte values in the LO 2 bytes of XMM<sub>src</sub>/mem<sub>16</sub> to qword values in XMM<sub>dest</sub>.</td>
</tr>
<tr>
<td><code>pmovzxwd </code><var>xmm</var><sub>dest</sub><code>, </code><var>xmm</var><sub>src</sub><code>/</code><var>mem</var><sub>64</sub></td>
<td>Zero-extends a set of four word values in the LO 8 bytes of XMM<sub>src</sub>/mem<sub>64</sub> to dword values in XMM<sub>dest</sub>.</td>
</tr>
<tr>
<td><code>pmovzxwq </code><var>xmm</var><sub>dest</sub><code>, </code><var>xmm</var><sub>src</sub><code>/</code><var>mem</var><sub>32</sub></td>
<td>Zero-extends a set of two word values in the LO 4 bytes of XMM<sub>src</sub>/mem<sub>32</sub> to qword values in XMM<sub>dest</sub>.</td>
</tr>
<tr>
<td><code>pmovzxdq </code><var>xmm</var><sub>dest</sub><code>, </code><var>xmm</var><sub>src</sub><code>/</code><var>mem</var><sub>64</sub></td>
<td>Zero-extends a set of two dword values in the LO 8 bytes of XMM<sub>src</sub>/mem<sub>64</sub> to qword values in XMM<sub>dest</sub>.</td>
</tr>
</tbody>
</table>
</figure>
<p>A set of comparable AVX instructions also exists (same syntax, but with a <code>v</code> prefix on the instruction mnemonics). The difference, as usual, is that the SSE instructions leave the upper bits of the YMM register unchanged, whereas the AVX instructions store 0s into the upper bits of the YMM registers.</p>
<p>The AVX2 instruction set extensions double the number of lanes by allowing the use of the YMM registers. They take similar operands to the SSE/AVX instructions (substituting YMM for the destination register and doubling the size of the memory locations) and process twice the number of lanes to produce sixteen words, eight dwords, or four qwords in a YMM destination register. See <a href="#table11-22" id="tableanchor11-22">Table 11-22</a> for details.</p>
<figure>
<figcaption class="TableTitle"><p><a id="table11-22">Table 11-22</a>: AVX2 Packed Zero-Extension Instructions</p></figcaption>
<table id="table-501089c11-0022" border="1">
<thead>
<tr>
<td><b>Syntax</b></td>
<td><b>Description</b></td>
</tr>
</thead>
<tbody>
<tr>
<td><code>v</code><code>pmovzxbw </code><var>ymm</var><sub>dest</sub><code>, </code><var>xmm</var><sub>src</sub><code>/</code><var>mem</var><sub>128</sub></td>
<td>Zero-extends a set of sixteen byte values in the LO 16 bytes of XMM<sub>src</sub>/mem<sub>128 </sub>to word values in YMM<sub>dest</sub>.</td>
</tr>
<tr>
<td><code>v</code><code>pmovzxbd </code><var>ymm</var><sub>dest</sub><code>, </code><var>xmm</var><sub>src</sub><code>/</code><var>mem</var><sub>64</sub></td>
<td>Zero-extends a set of eight byte values in the LO 8 bytes of XMM<sub>src</sub>/mem<sub>64</sub> to dword values in YMM<sub>dest</sub>.</td>
</tr>
<tr>
<td><code>v</code><code>pmovzxbq </code><var>ymm</var><sub>dest</sub><code>, </code><var>xmm</var><sub>src</sub><code>/</code><var>mem</var><sub>32</sub></td>
<td>Zero-extends a set of four byte values in the LO 4 bytes of XMM<sub>src</sub>/mem<sub>32</sub> to qword values in YMM<sub>dest</sub>.</td>
</tr>
<tr>
<td><code>v</code><code>pmovzxwd </code><var>ymm</var><sub>dest</sub><code>, </code><var>xmm</var><sub>src</sub><code>/</code><var>mem</var><sub>128</sub></td>
<td>Zero-extends a set of eight word values in the LO 16 bytes of XMM<sub>src</sub>/mem<sub>128</sub> to dword values in YMM<sub>dest</sub>.</td>
</tr>
<tr>
<td><code>v</code><code>pmovzxwq </code><var>ymm</var><sub>dest</sub><code>, </code><var>xmm</var><sub>src</sub><code>/</code><var>mem</var><sub>64</sub></td>
<td>Zero-extends a set of four word values in the LO 8 bytes of XMM<sub>src</sub>/mem<sub>64</sub> to qword values in YMM<sub>dest</sub>.</td>
</tr>
<tr>
<td><code>v</code><code>pmovzxdq </code><var>ymm</var><sub>dest</sub><code>, </code><var>xmm</var><sub>src</sub><code>/</code><var>mem</var><sub>128</sub></td>
<td>Zero-extends a set of four dword values in the LO 16 bytes of XMM<sub>src</sub>/mem<sub>128 </sub>to qword values in YMM<sub>dest</sub>.</td>
</tr>
</tbody>
</table>
</figure>
<h4 id="h3-501089c11-0011"><span epub:type="pagebreak" title="666" id="Page_666"/>11.10.11.2	Packed Sign-Extension Instructions</h4>
<p class="BodyFirst">The SSE/AVX/AVX2 instruction set extensions provide a comparable set of instructions that sign-extend byte, word, and dword values. <a href="#table11-23" id="tableanchor11-23">Table 11-23</a> lists the SSE packed sign-extension instructions.</p>
<figure>
<figcaption class="TableTitle"><p><a id="table11-23">Table 11-23</a>: SSE Packed Sign-Extension Instructions</p></figcaption>
<table id="table-501089c11-0023" border="1">
<thead>
<tr>
<td><b>Syntax</b></td>
<td><b>Description</b></td>
</tr>
</thead>
<tbody>
<tr>
<td><code>pmovsxbw </code><var>xmm</var><sub>dest</sub><code>, </code><var>xmm</var><sub>src</sub><code>/</code><var>mem</var><sub>64</sub></td>
<td>Sign-extends a set of eight byte values in the LO 8 bytes of XMM<sub>src</sub>/mem<sub>64</sub> to word values in XMM<sub>dest</sub>.</td>
</tr>
<tr>
<td><code>pmovsxbd </code><var>xmm</var><sub>dest</sub><code>, </code><var>xmm</var><sub>src</sub><code>/</code><var>mem</var><sub>32</sub></td>
<td>Sign-extends a set of four byte values in the LO 4 bytes of XMM<sub>src</sub>/mem<sub>32</sub> to dword values in XMM<sub>dest</sub>.</td>
</tr>
<tr>
<td><code>pmovsxbq </code><var>xmm</var><sub>dest</sub><code>, </code><var>xmm</var><sub>src</sub><code>/</code><var>mem</var><sub>16</sub></td>
<td>Sign-extends a set of two byte values in the LO 2 bytes of XMM<sub>src</sub>/mem<sub>16</sub> to qword values in XMM<sub>dest</sub>.</td>
</tr>
<tr>
<td><code>pmovsxwd </code><var>xmm</var><sub>dest</sub><code>, </code><var>xmm</var><sub>src</sub><code>/</code><var>mem</var><sub>64</sub></td>
<td>Sign-extends a set of four word values in the LO 8 bytes of XMM<sub>src</sub>/mem<sub>64</sub> to dword values in XMM<sub>dest</sub>.</td>
</tr>
<tr>
<td><code>pmovsxwq </code><var>xmm</var><sub>dest</sub><code>, </code><var>xmm</var><sub>src</sub><code>/</code><var>mem</var><sub>32</sub></td>
<td>Sign-extends a set of two word values in the LO 4 bytes of XMM<sub>src</sub>/mem<sub>32</sub> to qword values in XMM<sub>dest</sub>.</td>
</tr>
<tr>
<td><code>pmovsxdq </code><var>xmm</var><sub>dest</sub><code>, </code><var>xmm</var><sub>src</sub><code>/mem</code><sub>64</sub></td>
<td>Sign-extends a set of two dword values in the LO 8 bytes of XMM<sub>src</sub>/mem<sub>64</sub> to qword values in XMM<sub>dest</sub>.</td>
</tr>
</tbody>
</table>
</figure>
<p>A set of corresponding AVX instructions also exists (whose mnemonics have the <code>v</code> prefix). As usual, the difference between the SSE and AVX instructions is that the SSE instructions leave the upper bits of the YMM register unchanged (if applicable), and the AVX instructions store 0s into those upper bits.</p>
<p>AVX2-capable processors also allow a YMM<sub>dest</sub> destination register, which doubles the number of (output) values the instruction can handle; see <a href="#table11-24" id="tableanchor11-24">Table 11-24</a>.</p>
<figure>
<figcaption class="TableTitle"><p><a id="table11-24">Table 11-24</a>: AVX Packed Sign-Extension Instructions</p></figcaption>
<table id="table-501089c11-0024" border="1">
<thead>
<tr>
<td><b>Syntax</b></td>
<td><b>Description</b></td>
</tr>
</thead>
<tbody>
<tr>
<td><code>v</code><code>pmovsxbw </code><var>ymm</var><sub>dest</sub><code>, </code><var>xmm</var><sub>src</sub><code>/</code><var>mem</var><sub>128</sub></td>
<td>Sign-extends a set of sixteen byte values in the LO 16 bytes of XMM<sub>src</sub>/mem<sub>128</sub> to word values in YMM<sub>dest</sub>.</td>
</tr>
<tr>
<td><code>v</code><code>pmovsxbd </code><var>ymm</var><sub>dest</sub><code>, </code><var>xmm</var><sub>src</sub><code>/</code><var>mem</var><sub>64</sub></td>
<td>Sign-extends a set of eight byte values in the LO 8 bytes of XMM<sub>src</sub>/mem<sub>64</sub> to dword values in YMM<sub>dest</sub>.</td>
</tr>
<tr>
<td><code>v</code><code>pmovsxbq </code><var>ymm</var><sub>dest</sub><code>, </code><var>xmms</var><sub>rc</sub><code>/</code><var>mem</var><sub>32</sub></td>
<td>Sign-extends a set of four byte values in the LO 4 bytes of XMM<sub>src</sub>/mem<sub>32</sub> to qword values in YMM<sub>dest</sub>.</td>
</tr>
<tr>
<td><code>v</code><code>pmovsxwd </code><var>ymm</var><sub>dest</sub><code>, </code><var>xmm</var><sub>src</sub><code>/</code><var>mem</var><sub>128</sub></td>
<td>Sign-extends a set of eight word values in the LO 16 bytes of XMM<sub>src</sub>/mem<sub>128</sub> to dword values in YMM<sub>dest</sub>.</td>
</tr>
<tr>
<td><code>v</code><code>pmovsxwq </code><var>ymm</var><sub>dest</sub><code>, </code><var>xmm</var><sub>src</sub><code>/</code><var>mem</var><sub>64</sub></td>
<td>Sign-extends a set of four word values in the LO 8 bytes of XMM<sub>src</sub>/mem<sub>64</sub> to qword values in YMM<sub>dest</sub>.</td>
</tr>
<tr>
<td><code>v</code><code>pmovsxdq </code><var>ymm</var><sub>dest</sub><code>, </code><var>xmm</var><sub>src</sub><code>/</code><var>mem</var><sub>128</sub></td>
<td>Sign-extends a set of four dword values in the LO 16 bytes of XMM<sub>src</sub>/mem<sub>128</sub> to qword values in YMM<sub>dest</sub>.</td>
</tr>
</tbody>
</table>
</figure>
<h4 id="h3-501089c11-0012"><span epub:type="pagebreak" title="667" id="Page_667"/>11.10.11.3	Packed Sign Extension with Saturation</h4>
<p class="BodyFirst">In addition to converting smaller signed or unsigned values to a larger format, the SSE/AVX/AVX2-capable CPUs have the ability to convert large values to smaller values via saturation; see <a href="#table11-25" id="tableanchor11-25">Table 11-25</a>. </p>
<figure>
<figcaption class="TableTitle"><p><a id="table11-25">Table 11-25</a>: SSE Packed Sign-Extension with Saturation Instructions</p></figcaption>
<table id="table-501089c11-0025" border="1">
<thead>
<tr>
<td><b>Syntax</b></td>
<td><b>Description</b></td>
</tr>
</thead>
<tbody>
<tr>
<td><code>packsswb </code><var>xmm</var><sub>dest</sub><code>, </code><var>xmm</var><sub>src</sub><code>/</code><var>mem</var><sub>128</sub></td>
<td>Packs sixteen signed word values (from two 128-bit sources) into sixteen byte lanes in a 128-bit destination register using signed saturation.</td>
</tr>
<tr>
<td><code>packuswb</code> <var>xmm</var><sub>dest</sub><code>, </code><var>xmm</var><sub>src</sub><code>/</code><var>mem</var><sub>128</sub></td>
<td>Packs sixteen unsigned word values (from two 128-bit sources) into sixteen byte lanes in a 128-bit destination register using unsigned saturation.</td>
</tr>
<tr>
<td><code>packssdw</code> <var>xmm</var><sub>dest</sub><code>, </code><var>xmm</var><sub>src</sub><code>/</code><var>mem</var><sub>128</sub></td>
<td>Packs eight signed dword values (from two 128-bit sources) into eight word values in a 128-bit destination register using signed saturation.</td>
</tr>
<tr>
<td><code>packusdw</code> <var>xmm</var><sub>dest</sub><code>, </code><var>xmm</var><sub>src</sub><code>/</code><var>mem</var><sub>128</sub></td>
<td>Packs eight unsigned dword values (from two 128-bit sources) into eight word values in a 128-bit destination register using unsigned saturation.</td>
</tr>
</tbody>
</table>
</figure>
<p>The saturate operation checks its operand to see if the value exceeds the range of the result (–128 to +127 for signed bytes, 0 to 255 for unsigned bytes, –32,768 to +32,767 for signed words, and 0 to 65,535 for unsigned words). When saturating to a byte, if the signed source value is less than –128, byte saturation sets the value to –128. When saturating to a word, if the signed source value is less than –32,786, signed saturation sets the value to –32,768. Similarly, if a signed byte or word value exceeds +127 or +32,767, then saturation replaces the value with +127 or +32,767, respectively. For unsigned operations, saturation limits the value to +255 (for bytes) or +65,535 (for words). Unsigned values are never less than 0, so unsigned saturation clips values to only +255 or +65,535.</p>
<p>AVX-capable CPUs provide 128-bit variants of these instructions that support three operands: two source operands and an independent destination operand. These instructions (mnemonics the same as the SSE instructions, with a <code>v</code> prefix) have the following syntax:</p>
<pre><code>vpacksswb  <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>
vpackuswb  <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>
vpackssdw  <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>
vpackusdw  <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub></code></pre>
<p>These instructions are roughly equivalent to the SSE variants, except that these instructions use XMM<sub>src1</sub> as the first source operand rather than XMM<sub>dest</sub> (which the SSE instructions use). Also, the SSE instructions do not modify the upper bits of the YMM register (if present on the CPU), whereas the AVX instructions store 0s into the upper YMM register bits.</p>
<p><span epub:type="pagebreak" title="668" id="Page_668"/>AVX2-capable CPUs also allow the use of the YMM registers (and 256-bit memory locations) to double the number of values the instruction can saturate (see <a href="#table11-26" id="tableanchor11-26">Table 11-26</a>). Of course, don’t forget to check for AVX2 (and AVX) compatibility before using these instructions.</p>
<figure>
<figcaption class="TableTitle"><p><a id="table11-26">Table 11-26</a>: AVX Packed Sign-Extension with Saturation Instructions</p></figcaption>
<table id="table-501089c11-0026" border="1">
<thead>
<tr>
<td><b>Syntax</b></td>
<td><b>Description</b></td>
</tr>
</thead>
<tbody>
<tr>
<td><code>v</code><code>packsswb </code><var>ymm</var><sub>dest</sub><code>, </code><var>ymm</var><sub>src1</sub><code>, </code><var>ymm</var><sub>src2</sub><code>/</code><var>mem</var><sub>256</sub></td>
<td>Packs 32 signed word values (from two 256-bit sources) into 32 byte lanes in a 256-bit destination register using signed saturation.</td>
</tr>
<tr>
<td><code>v</code><code>packuswb</code> <var>ymm</var><sub>dest</sub><code>, </code><var>ymm</var><sub>src1</sub><code>, </code><var>ymm</var><sub>src2</sub><code>/</code><var>mem</var><sub>256</sub></td>
<td>Packs 32 unsigned word values (from two 256-bit sources) into 32 byte lanes in a 256-bit destination register using unsigned saturation.</td>
</tr>
<tr>
<td><code>v</code><code>packssdw</code> <var>ymm</var><sub>dest</sub><code>, </code><var>ymm</var><sub>src1</sub><code>, </code><var>ymm</var><sub>src2</sub><code>/</code><var>mem</var><sub>256</sub></td>
<td>Packs 16 signed dword values (from two 256-bit sources) into 16 word values in a 256-bit destination register using signed saturation.</td>
</tr>
<tr>
<td><code>v</code><code>packusdw</code> <var>ymm</var><sub>dest</sub><code>, </code><var>ymm</var><sub>src1</sub><code>, </code><var>ymm</var><sub>src2</sub><code>/</code><var>mem</var><sub>256</sub></td>
<td>Packs 16 unsigned dword values (from two 256-bit sources) into 16 word values in a 256-bit destination register using unsigned saturation.</td>
</tr>
</tbody>
</table>
</figure>
<h2 id="h1-501089c11-0011">	11.11	SIMD Floating-Point Arithmetic Operations</h2>
<p class="BodyFirst">The SSE and AVX instruction set extensions provide packed arithmetic equivalents for all the scalar floating-point instructions in <span class="xref" itemid="xref_target_“SSE Floating-Point Arithmetic” in Chapter 6">“SSE Floating-Point Arithmetic” in Chapter 6</span>. This section does not repeat the discussion of the scalar floating-point operations; see <span class="xref" itemid="xref_target_Chapter 6">Chapter 6</span> for more details. </p>
<p>The 128-bit SSE packed floating-point instructions have the following generic syntax (where <var>instr</var> is one of the floating-point instructions in <a href="#table11-27" id="tableanchor11-27">Table 11-27</a>):</p>
<pre><code><var>instr</var>ps <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>
<var>instr</var>pd <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub></code></pre>
<p>The <em>packed single</em> (<code>*ps</code>) instructions perform four single-precision floating-point operations simultaneously. The <em>packed double</em> (<code>*pd</code>) instructions perform two double-precision floating-point operations simultaneously. As is typical for SSE instructions, these packed arithmetic instructions compute</p>
<pre><code><var>xmm</var><sub>dest</sub>[<var>lane</var>] = <var>xmm</var><sub>dest</sub>[<var>lane</var>] <var>op</var> <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>[<var>lane</var>]</code></pre>
<p class="BodyContinued">where <var>lane</var> varies from 0 to 3 for packed single-precision instructions and from 0 to 1 for packed double-precision instructions. <var>op</var> represents the operation (such as addition or subtraction). When the SSE instructions are executed on a CPU that supports the AVX extensions, the SSE instructions leave the upper bits of the AVX register unmodified.</p>
<p><span epub:type="pagebreak" title="669" id="Page_669"/>The 128-bit AVX packed floating-point instructions have this syntax:<sup class="FootnoteReference"><a id="c11-footnoteref-15" href="#c11-footnote-15">15</a></sup></p>
<pre><code>v<var>instr</var>ps <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub> ; For dyadic operations
v<var>instr</var>pd <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub> ; For dyadic operations
v<var>instr</var>ps <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>         <span class="NegativeCodeSpace" style="margin-left: -.1em;"> </span>; For monadic operations
v<var>instr</var>pd <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>         <span class="NegativeCodeSpace" style="margin-left: -.1em;"> </span>; For monadic operations</code></pre>
<p>These instructions compute</p>
<pre><code><var>xmm</var><sub>dest</sub>[<var>lane</var>] = <var>xmm</var><sub>src1</sub>[<var>lane</var>] <var>op</var> <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>[<var>lane</var>]</code></pre>
<p class="BodyContinued">where <var>op</var> corresponds to the operation associated with the specific instruction (for example, <code>vaddps</code> does a packed single-precision addition). These 128-bit AVX instructions clear the HO bits of the underlying YMM<sub>dest</sub> register.</p>
<p>The 256-bit AVX packed floating-point instructions have this syntax:</p>
<pre><code>v<var>instr</var>ps <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub> ; For dyadic operations
v<var>instr</var>pd <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub> ; For dyadic operations
v<var>instr</var>ps <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src</sub>/<var>mem</var><sub>256</sub>         <span class="NegativeCodeSpace" style="margin-left: -.1em;"> </span>; For monadic operations
v<var>instr</var>pd <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src</sub>/<var>mem</var><sub>256</sub>         <span class="NegativeCodeSpace" style="margin-left: -.1em;"> </span>; For monadic operations</code></pre>
<p>These instructions compute</p>
<pre><code><var>ymm</var><sub>dest</sub>[<var>lane</var>] = <var>ymm</var><sub>src1</sub>[<var>lane</var>] <var>op</var> <var>ymm</var><sub>src</sub>/<var>mem</var><sub>256</sub>[<var>lane</var>]</code></pre>
<p class="BodyContinued">where <var>op</var> corresponds to the operation associated with the specific instruction (for example, <code>vaddps</code> is a packed single-precision addition). Because these instructions operate on 256-bit operands, they compute twice as many lanes of data as the 128-bit instructions. Specifically, they simultaneously compute eight single-precision (the <code>v*ps</code> instructions) or four double-precision results (the <code>v*pd</code> instructions).</p>
<p><a href="#table11-27">Table 11-27</a> provides the list of SSE/AVX packed instructions.</p>
<figure>
<figcaption class="TableTitle"><p><a id="table11-27">Table 11-27</a>: Floating-Point Arithmetic Instructions</p></figcaption>
<table id="table-501089c11-0027" border="1">
<thead>
<tr>
<td><b>Instruction</b></td>
<td><b>Lanes</b></td>
<td><b>Description</b></td>
</tr>
</thead>
<tbody>
<tr>
<td><code>addps</code></td>
<td>4</td>
<td>Adds four single-precision floating-point values</td>
</tr>
<tr>
<td><code>addpd</code></td>
<td>2</td>
<td>Adds two double-precision floating-point values</td>
</tr>
<tr>
<td><code>vaddps</code></td>
<td>4/8</td>
<td>Adds four (128-bit/XMM operands) or eight (256-bit/YMM operands) single-precision values</td>
</tr>
<tr>
<td><code>vaddpd</code></td>
<td>2/4</td>
<td>Adds two (128-bit/XMM operands) or four (256-bit/YMM operands) double-precision values</td>
</tr>
<tr>
<td><code>subps</code></td>
<td>4</td>
<td>Subtracts four single-precision floating-point values</td>
</tr>
<tr>
<td><code>subpd</code></td>
<td>2</td>
<td>Subtracts two double-precision floating-point values</td>
</tr>
<tr>
<td><code><span epub:type="pagebreak" title="670" id="Page_670"/>vsubps</code></td>
<td>4/8</td>
<td>Subtracts four (128-bit/XMM operands) or eight (256-bit/YMM operands) single-precision values</td>
</tr>
<tr>
<td><code>vsubpd</code></td>
<td>2/4</td>
<td>Subtracts two (128-bit/XMM operands) or four (256-bit/YMM operands) double-precision values</td>
</tr>
<tr>
<td><code>mulps</code></td>
<td>4</td>
<td>Multiplies four single-precision floating-point values</td>
</tr>
<tr>
<td><code>mulpd</code></td>
<td>2</td>
<td>Multiplies two double-precision floating-point values</td>
</tr>
<tr>
<td><code>vmulps</code></td>
<td>4/8</td>
<td>Multiplies four (128-bit/XMM operands) or eight (256-bit/YMM operands) single-precision values</td>
</tr>
<tr>
<td><code>vmulpd</code></td>
<td>2/4</td>
<td>Multiplies two (128-bit/XMM operands) or four (256-bit/YMM operands) double-precision values</td>
</tr>
<tr>
<td><code>divps</code></td>
<td>4</td>
<td>Divides four single-precision floating-point values</td>
</tr>
<tr>
<td><code>divpd</code></td>
<td>2</td>
<td>Divides two double-precision floating-point values</td>
</tr>
<tr>
<td><code>vdivps</code></td>
<td>4/8</td>
<td>Divides four (128-bit/XMM operands) or eight (256-bit/YMM operands) single-precision values</td>
</tr>
<tr>
<td><code>vdivpd</code></td>
<td>2/4</td>
<td>Divides two (128-bit/XMM operands) or four (256-bit/YMM operands) double-precision values</td>
</tr>
<tr>
<td><code>maxps</code></td>
<td>4</td>
<td>Computes the maximum of four pairs of single-precision floating-point values</td>
</tr>
<tr>
<td><code>maxpd</code></td>
<td>2</td>
<td>Computes the maximum of two pairs of double-precision floating-point values</td>
</tr>
<tr>
<td><code>vmaxps</code></td>
<td>4/8</td>
<td>Computes the maximum of four (128-bit/XMM operands) or eight (256-bit/YMM operands) pairs of single-precision values</td>
</tr>
<tr>
<td><code>vmaxpd</code></td>
<td>2/4</td>
<td>Computes the maximum of two (128-bit/XMM operands) or four (256-bit/YMM operands) pairs of double-precision values</td>
</tr>
<tr>
<td><code>minps</code></td>
<td>4</td>
<td>Computes the minimum of four pairs of single-precision floating-point values</td>
</tr>
<tr>
<td><code>minpd</code></td>
<td>2</td>
<td>Computes the minimum of two pairs of double-precision floating-point values</td>
</tr>
<tr>
<td><code>vminps</code></td>
<td>4/8</td>
<td>Computes the minimum of four (128-bit/XMM operands) or eight (256-bit/YMM operands) pairs of single-precision values</td>
</tr>
<tr>
<td><code>vminpd</code></td>
<td>2/4</td>
<td>Computes the minimum of two (128-bit/XMM operands) or four (256-bit/YMM operands) pairs of double-precision values</td>
</tr>
<tr>
<td><code>sqrtps</code></td>
<td>4</td>
<td>Computes the square root of four single-precision floating-point values</td>
</tr>
<tr>
<td><code>sqrtpd</code></td>
<td>2</td>
<td>Computes the square root of two double-precision floating-point values</td>
</tr>
<tr>
<td><code>vsqrtps</code></td>
<td>4/8</td>
<td>Computes the square root of four (128-bit/XMM operands) or eight (256-bit/YMM operands) single-precision values</td>
</tr>
<tr>
<td><code>vsqrtpd</code></td>
<td>2/4</td>
<td>Computes the square root of two (128-bit/XMM operands) or four (256-bit/YMM operands) double-precision values</td>
</tr>
<tr>
<td><code>rsqrtps</code></td>
<td>4</td>
<td>Computes the approximate reciprocal square root of four single-precision floating-point values<sup class="FootnoteReference"><a id="c11-footnoteref-16" href="#c11-footnote-16">*</a></sup></td>
</tr>
<tr>
<td><code>vrsqrtps</code></td>
<td>4/8</td>
<td>Computes the approximate reciprocal square root of four (128-bit/XMM operands) or eight (256-bit/YMM operands) single-precision values</td>
</tr>
</tbody>
<tfoot>
<tr>
<td colspan="3"><p class="TableFootnote"><sup class="FootnoteReference"><a id="c11-footnote-16" href="#c11-footnoteref-16">*</a></sup> The relative error is ≤ 1.5 × 2<sup>-12</sup>.</p></td>
</tr>
</tfoot>
</table>
</figure>
<p><span epub:type="pagebreak" title="671" id="Page_671"/>The SSE/AVX instruction set extensions also include floating-point horizontal addition and subtraction instructions. The syntax for these instructions is as follows:</p>
<pre><code>haddps  <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>
vhaddps <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>
vhaddps <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub>
haddpd  <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>
vhaddpd <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>
vhaddpd <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub>

hsubps  <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>
vhsubps <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>
vhsubps <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub>
hsubpd  <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>
vhsubpd <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>
vhsubpd <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub></code></pre>
<p>As for the integer horizontal addition and subtraction instructions, these instructions add or subtract the values in adjacent lanes in the same register and store the result in the destination register (lane 2), as shown in <a href="#figure11-43">Figure 11-43</a>.</p>
<h2 id="h1-501089c11-0012">	11.12	SIMD Floating-Point Comparison Instructions</h2>
<p class="BodyFirst">Like the integer packed comparisons, the SSE/AVX floating-point comparisons compare two sets of floating-point values (either single- or double-precision, depending on the instruction’s syntax) and store a resulting Boolean value (all 1 bits for true, all 0 bits for false) into the destination lane. However, the floating-point comparisons are far more comprehensive than those of their integer counterparts. Part of the reason is that floating-point arithmetic is more complex; however, an ever-increasing silicon budget for the CPU designers is also responsible for this. </p>
<h3 id="h2-501089c11-0039">11.12.1	SSE and AVX Comparisons</h3>
<p class="BodyFirst">There are two sets of basic floating-point comparisons: <code>(v)cmpps</code>, which compares a set of packed single-precision values, and <code>(v)cmppd</code>, which compares a set of packed double-precision values. Instead of encoding the comparison type into the mnemonic, these instructions use an imm<sub>8</sub> operand whose value specifies the type of comparison. The generic syntax for these instructions is as follows:</p>
<pre><code>cmpps  <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>, <var>imm</var><sub>8</sub>
vcmpps <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>, <var>imm</var><sub>8</sub>
vcmpps <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub>, <var>imm</var><sub>8</sub>

cmppd  <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src</sub>/<var>mem</var><sub>128</sub>, <var>imm</var><sub>8</sub>
vcmppd <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>, <var>imm</var><sub>8</sub>
vcmppd <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub>, <var>imm</var><sub>8</sub></code></pre>
<p>The imm<sub>8</sub> operand specifies the type of the comparison. There are 32 possible comparisons, as listed in <a href="#table11-28" id="tableanchor11-28">Table 11-28</a>.</p>
<figure>
<figcaption class="TableTitle"><p><a id="table11-28">Table 11-28</a>: imm<sub>8</sub> Values for <code>cmpps</code> and <code>cmppd</code> Instructions<sup class="FootnoteReference"><a id="c11-footnoteref-17" href="#c11-footnote-17">†</a></sup><span epub:type="pagebreak" title="672" id="Page_672"/></p></figcaption>
<table id="table-501089c11-0028" border="1">
<thead>
<tr>
<td><b>imm<sub>8</sub></b></td>
<td><b>Description</b></td>
<td colspan="4"><b>Result</b></td>
<td><b>Signal</b></td>
</tr>
<tr>
<td/>
<td/>
<td><b>A &lt; B</b></td>
<td><b>A = B</b></td>
<td><b>A &gt; B</b></td>
<td><b>Unord</b></td>
<td/>
</tr>
</thead>
<tbody>
<tr>
<td>00h</td>
<td>EQ, ordered, quiet</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>No</td>
</tr>
<tr>
<td>01h</td>
<td>LT, ordered, signaling</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>Yes</td>
</tr>
<tr>
<td>02h</td>
<td>LE, ordered, signaling</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>Yes</td>
</tr>
<tr>
<td>03h</td>
<td>Unordered, quiet</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>No</td>
</tr>
<tr>
<td>04h</td>
<td>NE, unordered, quiet</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>No</td>
</tr>
<tr>
<td>05h</td>
<td>NLT, unordered, signaling</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>Yes</td>
</tr>
<tr>
<td>06h</td>
<td>NLE, unordered, signaling</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>Yes</td>
</tr>
<tr>
<td>07h</td>
<td>Ordered, quiet</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>No</td>
</tr>
<tr>
<td>08h</td>
<td>EQ, unordered, quiet</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>No</td>
</tr>
<tr>
<td>09h</td>
<td>NGE, unordered, signaling</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>Yes</td>
</tr>
<tr>
<td>0Ah</td>
<td>NGT, unordered, signaling</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>Yes</td>
</tr>
<tr>
<td>0Bh</td>
<td>False, ordered, quiet</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>No</td>
</tr>
<tr>
<td>0Ch</td>
<td>NE, ordered, quiet</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>No</td>
</tr>
<tr class="TableHighlight">
<td class="TableHighlight">0Dh</td>
<td class="TableHighlight">GE, ordered, signaling</td>
<td class="TableHighlight">0</td>
<td class="TableHighlight">1</td>
<td class="TableHighlight">1</td>
<td class="TableHighlight">0</td>
<td class="TableHighlight">Yes</td>
</tr>
<tr class="TableHighlight">
<td class="TableHighlight">0Eh</td>
<td class="TableHighlight">GT, ordered, signaling</td>
<td class="TableHighlight">0</td>
<td class="TableHighlight">0</td>
<td class="TableHighlight">1</td>
<td class="TableHighlight">0</td>
<td class="TableHighlight">Yes</td>
</tr>
<tr>
<td>0Fh</td>
<td>True, unordered, quiet</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>No</td>
</tr>
<tr>
<td>10h</td>
<td>EQ, ordered, signaling</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>Yes</td>
</tr>
<tr>
<td>11h</td>
<td>LT, ordered, quiet</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>No</td>
</tr>
<tr>
<td>12h</td>
<td>LE, ordered, quiet</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>No</td>
</tr>
<tr>
<td>13h</td>
<td>Unordered, signaling</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>Yes</td>
</tr>
<tr>
<td>14h</td>
<td>NE, unordered, signaling</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>Yes</td>
</tr>
<tr>
<td>15h</td>
<td>NLT, unordered, quiet</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>No</td>
</tr>
<tr>
<td>16h</td>
<td>NLE, unordered, quiet</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>No</td>
</tr>
<tr>
<td>17h</td>
<td>Ordered, signaling</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>Yes</td>
</tr>
<tr>
<td>18h</td>
<td>EQ, unordered, signaling</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>Yes</td>
</tr>
<tr class="TableHighlight">
<td class="TableHighlight">19h</td>
<td class="TableHighlight">NGE, unordered, quiet</td>
<td class="TableHighlight">1</td>
<td class="TableHighlight">0</td>
<td class="TableHighlight">0</td>
<td class="TableHighlight">1</td>
<td class="TableHighlight">No</td>
</tr>
<tr class="TableHighlight">
<td class="TableHighlight">1Ah</td>
<td class="TableHighlight">NGT, unordered, quiet</td>
<td class="TableHighlight">1</td>
<td class="TableHighlight">1</td>
<td class="TableHighlight">0</td>
<td class="TableHighlight">1</td>
<td class="TableHighlight">No</td>
</tr>
<tr>
<td>1Bh</td>
<td>False, ordered, signaling</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>Yes</td>
</tr>
<tr>
<td>1Ch</td>
<td>NE, ordered, signaling</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>Yes</td>
</tr>
<tr>
<td>1Dh</td>
<td>GE, ordered, quiet</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>No</td>
</tr>
<tr>
<td>1Eh</td>
<td>GT, ordered, quiet</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>No</td>
</tr>
<tr>
<td>1Fh</td>
<td>True, unordered, signaling</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>Yes</td>
</tr>
</tbody>
<tfoot>
<tr>
<td colspan="7"><p class="TableFootnote"><sup class="FootnoteReference"><a id="c11-footnote-17" href="#c11-footnoteref-17">†</a></sup> The darker shaded entries are available only on CPUs that support AVX extensions.</p></td>
</tr>
</tfoot>
</table>
</figure>
<p><span epub:type="pagebreak" title="673" id="Page_673"/>The “true” and “false” comparisons always store true or false into the destination lanes. For the most part, these comparisons aren’t particularly useful. The <code>pxor</code>, <code>xorps</code>, <code>xorpd</code>, <code>vxorps</code>, and <code>vxorpd</code> instructions are probably better for setting an XMM or a YMM register to 0. Prior to AVX2, using a true comparison was the shortest instruction that would set all bits in an XMM or a YMM register to 1, though <code>pcmpeqb</code> is commonly used as well (be aware of microarchitectural inefficiencies with this latter instruction).</p>
<p>Note that non-AVX CPUs do not implement the GT, GE, NGT, and NGE instructions. On these CPUs, use the inverse operation (for example, NLT for GE) or swap the operands and use the opposite condition (as was done for the packed integer comparisons).</p>
<h3 id="h2-501089c11-0040">11.12.2	Unordered vs. Ordered Comparisons</h3>
<p class="BodyFirst">The unordered relationship is true when at least one of the two source operands being compared is a NaN; the ordered relationship is true when neither source operand is a NaN. Having ordered and unordered comparisons allows you to pass error conditions through comparisons as false or true, depending on how you interpret the final Boolean results appearing in the lanes. Unordered results, as their name implies, are incomparable. When you compare two values, one of which is not a number, you must always treat the result as a failed comparison. </p>
<p>To handle this situation, you use an ordered or unordered comparison to force the result to be false or true, the opposite of what you ultimately expect when using the comparison result. For example, suppose you are comparing a sequence of values and want the resulting masks to be true if all the comparisons are valid (for example, you’re testing to see if all the src<sub>1</sub> values are greater than the corresponding src<sub>2</sub> values). You would use an ordered comparison in this situation that would force a particular lane to false if one of the values being compared is NaN. On the other hand, if you’re checking to see if all the conditions are false after the comparison, you’d use an unordered comparison to force the result to true if any of the values are NaN.</p>
<h3 id="h2-501089c11-0041">11.12.3	Signaling and Quiet Comparisons</h3>
<p class="BodyFirst">The signaling comparisons generate an invalid arithmetic operation exception (IA) when an operation produces a quiet NaN. The quiet comparisons do not throw an exception and reflect only the status in the MXCSR (see <span class="xref" itemid="xref_target_“SSE MXCSR Register” in Chapter 6">“SSE MXCSR Register” in Chapter 6</span>). Note that you can also mask signaling exceptions in the MXCSR register; you must explicitly set the IM (<em>invalid operation mask</em>, bit 7) in the MXCSR to 0 if you want to allow exceptions.</p>
<h3 id="h2-501089c11-0042">11.12.4	Instruction Synonyms</h3>
<p class="BodyFirst">MASM supports the use of certain synonyms so you don’t have to memorize the 32 encodings. <a href="#table11-29" id="tableanchor11-29">Table 11-29</a> lists these synonyms. In this table, <em>x1</em> denotes the destination operand (XMM<sub><em>n</em></sub> or YMM<sub><em>n</em></sub>), and <em>x2</em> denotes the source operand (XMM<sub><em>n</em></sub>/mem<sub>128</sub> or YMM<sub><em>n</em></sub>/mem<sub>256</sub>, as appropriate).</p>
<figure>
<figcaption class="TableTitle"><p><a id="table11-29">Table 11-29</a>: Synonyms for Common Packed Floating-Point Comparisons<span epub:type="pagebreak" title="674" id="Page_674"/></p></figcaption>
<table id="table-501089c11-0029" border="1">
<thead>
<tr>
<td><b>Synonym</b></td>
<td><b>Instruction</b></td>
<td><b>Synonym</b></td>
<td><b>Instruction</b></td>
</tr>
</thead>
<tbody>
<tr>
<td><code>cmpeqps </code><var>x1</var><code>, </code><var>x2</var></td>
<td><code>cmpps </code><var>x1</var><code>, </code><var>x2</var><code>, 0</code></td>
<td><code>cmpeqpd </code><var>x1</var><code>, </code><var>x2</var></td>
<td><code>cmppd </code><var>x1</var><code>, </code><var>x2</var><code>, 0</code></td>
</tr>
<tr>
<td><code>cmpltps </code><var>x1</var><code>, </code><var>x2</var></td>
<td><code>cmpps </code><var>x1</var><code>, </code><var>x2</var><code>, 1</code></td>
<td><code>cmpltpd </code><var>x1</var><code>, </code><var>x2</var></td>
<td><code>cmppd </code><var>x1</var><code>, </code><var>x2</var><code>, 1</code></td>
</tr>
<tr>
<td><code>cmpleps </code><var>x1</var><code>, </code><var>x2</var></td>
<td><code>cmpps </code><var>x1</var><code>, </code><var>x2</var><code>, 2</code></td>
<td><code>cmplepd </code><var>x1</var><code>, </code><var>x2</var></td>
<td><code>cmppd </code><var>x1</var><code>, </code><var>x2</var><code>, 2</code></td>
</tr>
<tr>
<td><code>cmpunordps </code><var>x1</var><code>, </code><var>x2</var></td>
<td><code>cmpps </code><var>x1</var><code>, </code><var>x2</var><code>, 3</code></td>
<td><code>cmpunordpd </code><var>x1</var><code>, </code><var>x2</var></td>
<td><code>cmppd x1, x2, 3</code></td>
</tr>
<tr>
<td><code>cmpneqps </code><var>x1</var><code>, </code><var>x2</var></td>
<td><code>cmpps </code><var>x1</var><code>, </code><var>x2</var><code>, 4</code></td>
<td><code>cmpneqpd </code><var>x1</var><code>, </code><var>x2</var></td>
<td><code>cmppd </code><var>x1</var><code>, </code><var>x2</var><code>, 4</code></td>
</tr>
<tr>
<td><code>cmpnltps </code><var>x1</var><code>, </code><var>x2</var></td>
<td><code>cmpps </code><var>x1</var><code>, </code><var>x2</var><code>, 5</code></td>
<td><code>cmpnltpd </code><var>x1</var><code>, </code><var>x2</var></td>
<td><code>cmppd </code><var>x1</var><code>, </code><var>x2</var><code>, 5</code></td>
</tr>
<tr>
<td><code>cmpnleps </code><var>x1</var><code>, </code><var>x2</var></td>
<td><code>cmpps </code><var>x1</var><code>, </code><var>x2</var><code>, 6</code></td>
<td><code>cmpnlepd </code><var>x1</var><code>, </code><var>x2</var></td>
<td><code>cmppd </code><var>x1</var><code>, </code><var>x2</var><code>, 6</code></td>
</tr>
<tr>
<td><code>cmpordps </code><var>x1</var><code>, </code><var>x2</var></td>
<td><code>cmpps </code><var>x1</var><code>, </code><var>x2</var><code>, 7</code></td>
<td><code>cmpordpd </code><var>x1</var><code>, </code><var>x2</var></td>
<td><code>cmppd </code><var>x1</var><code>, </code><var>x2</var><code>, 7</code></td>
</tr>
</tbody>
</table>
</figure>
<p>The synonyms allow you to write instructions such as</p>
<pre><code>cmpeqps  xmm0, xmm1</code></pre>
<p class="BodyContinued">rather than</p>
<pre><code>cmpps  xmm0, xmm1, 0       ; Compare xmm0 to xmm1 for equality</code></pre>
<p>Obviously, using the synonym makes the code much easier to read and understand. There aren’t synonyms for all the possible comparisons. To create readable synonyms for the instructions MASM doesn’t support, you can use a macro (or a more readable symbolic constant). For more information on macros, see <span class="xref" itemid="xref_target_Chapter 13">Chapter 13</span>.</p>
<h3 id="h2-501089c11-0043">11.12.5	AVX Extended Comparisons</h3>
<p class="BodyFirst">The AVX versions of these instructions allow three register operands: a destination XMM or YMM register, a source XMM or YMM register, and a source XMM or YMM register or 128-bit or 256-bit memory location (followed by the imm<sub>8</sub> operand specifying the type of the comparison). The basic syntax is the following:</p>
<pre><code>vcmpps <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>, <var>imm</var><sub>8</sub>
vcmpps <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub>, <var>imm</var><sub>8</sub>

vcmppd <var>xmm</var><sub>dest</sub>, <var>xmm</var><sub>src1</sub>, <var>xmm</var><sub>src2</sub>/<var>mem</var><sub>128</sub>, <var>imm</var><sub>8</sub>
vcmppd <var>ymm</var><sub>dest</sub>, <var>ymm</var><sub>src1</sub>, <var>ymm</var><sub>src2</sub>/<var>mem</var><sub>256</sub>, <var>imm</var><sub>8</sub></code></pre>
<p>The 128-bit <code>vcmpps</code> instruction compares the four single-precision floating-point values in each lane of the XMM<sub>src1</sub> register against the values in the corresponding XMM<sub>src2</sub>/mem<sub>128</sub> lanes and stores the true (all 1 bits) or false (all 0 bits) result into the corresponding lane of the XMM<sub>dest</sub> register. The 256-bit <code>vcmpps</code> instruction compares the eight single-precision floating-point values in each lane of the YMM<sub>src1</sub> register against the values in the corresponding YMM<sub>src2</sub>/mem<sub>256</sub> lanes and stores the true or false result into the corresponding lane of the YMM<sub>dest</sub> register.</p>
<p><span epub:type="pagebreak" title="675" id="Page_675"/>The <code>vcmppd</code> instructions compare the double-precision values in the two lanes (128-bit version) or four lanes (256-bit version) and store the result into the corresponding lane of the destination register.</p>
<p>As for the SSE compare instructions, the AVX instructions provide synonyms that eliminate the need to memorize 32 imm<sub>8</sub> values. <a href="#table11-30" id="tableanchor11-30">Table 11-30</a> lists the 32 instruction synonyms.</p>
<figure>
<figcaption class="TableTitle"><p><a id="table11-30">Table 11-30</a>: AVX Packed Compare Instructions</p></figcaption>
<table id="table-501089c11-0030" border="1">
<thead>
<tr>
<td><b>imm<sub>8</sub></b></td>
<td><b>Instruction</b></td>
</tr>
</thead>
<tbody>
<tr>
<td>00h</td>
<td><code>vcmpeqps</code> or <code>vcmpeqpd</code></td>
</tr>
<tr>
<td>01h</td>
<td><code>vcmpltps</code> or <code>vcmpltpd</code></td>
</tr>
<tr>
<td>02h</td>
<td><code>vcmpleps</code> or <code>vcmplepd</code></td>
</tr>
<tr>
<td>03h</td>
<td><code>vcmpunordps</code> or <code>vcmpunordpd</code></td>
</tr>
<tr>
<td>04h</td>
<td><code>vcmpneqps</code> or <code>vcmpneqpd</code></td>
</tr>
<tr>
<td>05h</td>
<td><code>vcmpltps</code> or <code>vcmpltpd</code></td>
</tr>
<tr>
<td>06h</td>
<td><code>vcmpleps</code> or <code>vcmplepd</code></td>
</tr>
<tr>
<td>07h</td>
<td><code>vcmpordps</code> or <code>vcmpordpd</code></td>
</tr>
<tr>
<td>08h</td>
<td><code>vcmpeq_uqps</code> or <code>vcmpeq_uqpd</code></td>
</tr>
<tr>
<td>09h</td>
<td><code>vcmpngeps</code> or <code>vcmpngepd</code></td>
</tr>
<tr>
<td>0Ah</td>
<td><code>vcmpngtps</code> or <code>vcmpngtpd</code></td>
</tr>
<tr>
<td>0Bh</td>
<td><code>vcmpfalseps</code> or <code>vcmpfalsepd</code></td>
</tr>
<tr>
<td>0Ch</td>
<td><code>vcmpneq_oqps</code> or <code>vcmpneq_oqpd</code></td>
</tr>
<tr>
<td>0Dh</td>
<td><code>vcmpgeps</code> or <code>vcmpgepd</code></td>
</tr>
<tr>
<td>0Eh</td>
<td><code>vcmpgtps</code> or <code>vcmpgtpd</code></td>
</tr>
<tr>
<td>0Fh</td>
<td><code>vcmptrueps</code> or <code>vcmptruepd</code></td>
</tr>
<tr>
<td>10h</td>
<td><code>vcmpeq_osps</code> or <code>vcmpeq_ospd</code></td>
</tr>
<tr>
<td>11h</td>
<td><code>vcmplt_oqps</code> or <code>vcmplt_oqpd</code></td>
</tr>
<tr>
<td>12h</td>
<td><code>vcmple_oqps</code> or <code>vcmple_oqpd</code></td>
</tr>
<tr>
<td>13h</td>
<td><code>vcmpunord_sps</code> or <code>vcmpunord_spd</code></td>
</tr>
<tr>
<td>14h</td>
<td><code>vcmpneq_usps</code> or <code>vcmpneq_uspd</code></td>
</tr>
<tr>
<td>15h</td>
<td><code>vcmpnlt_uqps</code> or <code>vcmpnlt_uqpd</code></td>
</tr>
<tr>
<td>16h</td>
<td><code>vcmpnle_uqps</code> or <code>vcmpnle_uqpd</code></td>
</tr>
<tr>
<td>17h</td>
<td><code>vcmpord_sps</code> or <code>vcmpord_spd</code></td>
</tr>
<tr>
<td>18h</td>
<td><code>vcmpeq_usps</code> or <code>vcmpeq_uspd</code></td>
</tr>
<tr>
<td>19h</td>
<td><code>vcmpnge_uqps</code> or <code>vcmpnge_uqpd</code></td>
</tr>
<tr>
<td>1Ah</td>
<td><code>vcmpngt_uqps</code> or <code>vcmpngt_uqpd</code></td>
</tr>
<tr>
<td>1Bh</td>
<td><code>vcmpfalse_osps</code> or <code>vcmpfalse_ospd</code></td>
</tr>
<tr>
<td>1Ch</td>
<td><code>vcmpneq_osps</code> or <code>vcmpneq_ospd</code></td>
</tr>
<tr>
<td><span epub:type="pagebreak" title="676" id="Page_676"/>1Dh</td>
<td>vcmpge_oqps or <code>vcmpge_oqpd</code></td>
</tr>
<tr>
<td>1Eh</td>
<td><code>vcmpgt_oqps</code> or <code>vcmpgt_oqpd</code></td>
</tr>
<tr>
<td>1Fh</td>
<td><code>vcmptrue_usps</code> or <code>vcmptrue_uspd</code></td>
</tr>
</tbody>
</table>
</figure>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p>	The <code>vcmpfalse*</code> instructions always set the destination lanes to false (0 bits), and the <code>vcmptrue*</code> instructions always set the destination lanes to true (1 bits).</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<h3 id="h2-501089c11-0044">11.12.6	Using SIMD Comparison Instructions</h3>
<p class="BodyFirst">As for the integer comparisons (see <span class="xref" itemid="xref_target_“Using Packed Comparison Results” on page 662">“Using Packed Comparison Results” on page 662</span>), the floating-point comparison instructions produce a vector of Boolean results that you use to mask further operations on data lanes. You can use the packed logical instructions (<code>pand</code> and <code>vpand</code>, <code>pandn</code> and <code>vpandn</code>, <code>por</code> and <code>vpor</code>, and <code>pxor</code> and <code>vpxor</code>) to manipulate these results. You could extract the individual lane values and test them with a conditional jump, though this is definitely not the SIMD way of doing things; the following section describes one way to extract these masks.</p>
<h3 id="h2-501089c11-0045">11.12.7	The (v)movmskps, (v)movmskpd Instructions</h3>
<p class="BodyFirst">The <code>movmskps</code> and <code>movmskpd</code> instructions extract the sign bits from their packed single- and double-precision floating-point source operands and store these bits into the LO 4 (or 8) bits of a general-purpose register. The syntax is</p>
<pre><code>movmskps  <var>reg</var>, <var>xmm</var><sub>src</sub>
movmskpd  <var>reg</var>, <var>xmm</var><sub>src</sub> 
vmovmskps <var>reg</var>, <var>ymm</var><sub>src</sub>
vmovmskpd <var>reg</var>, <var>ymm</var><sub>src</sub></code></pre>
<p class="BodyContinued">where <var>reg</var> is any 32-bit or 64-bit general-purpose integer register.</p>
<p>The <code>movmskps</code> instruction extracts the sign bits from the four single-precision floating-point values in the XMM source register and copies these bits to the LO 4 bits of the destination register, as shown in <a href="#figure11-45" id="figureanchor11-45">Figure 11-45</a>.</p>
<p>The <code>movmskpd</code> instruction copies the sign bits from the two double-precision floating-point values in the source XMM register to bits 0 and 1 of the destination register, as <a href="#figure11-46" id="figureanchor11-46">Figure 11-46</a> shows.</p>
<p>The <code>vmovmskps</code> instruction extracts the sign bits from the four and eight single-precision floating-point values in the XMM and YMM source register and copies these bits to the LO 4 and 8 bits of the destination register. <a href="#figure11-47" id="figureanchor11-47">Figure 11-47</a> shows this operation with a YMM source register.</p>
<span epub:type="pagebreak" title="677" id="Page_677"/><figure>
<img src="image_fi/501089c11/f11045.png" alt="f11045" class=""/>
<figcaption><p><a id="figure11-45">Figure 11-45</a>: <span class="LiteralInCaption"><code>movmskps</code></span> operation</p></figcaption>
</figure>
<figure>
<img src="image_fi/501089c11/f11046.png" alt="f11046" class=""/>
<figcaption><p><a id="figure11-46">Figure 11-46</a>: <span class="LiteralInCaption"><code>movmskpd</code></span> operation</p></figcaption>
</figure>
<figure>
<img src="image_fi/501089c11/f11047.png" alt="f11047" class=""/>
<figcaption><p><a id="figure11-47">Figure 11-47</a>: <span class="LiteralInCaption"><code>vmovmskps</code></span> operation</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="678" id="Page_678"/>The <code>vmovmskpd</code> instruction copies the sign bits from the four double-precision floating-point values in the source YMM register to bits 0 to 3 of the destination register, as shown in <a href="#figure11-48" id="figureanchor11-48">Figure 11-48</a>.</p>
<figure>
<img src="image_fi/501089c11/f11048.png" alt="f11048" class=""/>
<figcaption><p><a id="figure11-48">Figure 11-48</a>: <span class="LiteralInCaption"><code>vmovmskpd</code></span> operation</p></figcaption>
</figure>
<p>This instruction, with an XMM source register, will copy the sign bits from the two double-precision floating-point values into bits 0 and 1 of the destination register. In all cases, these instructions zero-extend the results into the upper bits of the general-purpose destination register. Note that these instructions do not allow memory operands.</p>
<p>Although the stated data type for these instructions is packed single-precision and packed double-precision, you will also use these instructions on 32-bit integers (<code>movmskps</code> and <code>vmovmskps</code>) and 64-bit integers (<code>movmskpd</code> and <code>vmovmskpd</code>). Specifically, these instructions are perfect for extracting 1-bit Boolean values from the various lanes after one of the (dword or qword) packed integer comparisons as well as after the single- or double-precision floating-point comparisons (remember that although the packed floating-point comparisons compare floating-point values, their results are actually integer values). </p>
<p>Consider the following instruction sequence:</p>
<pre><code>         cmpeqpd  xmm0, xmm1
         movmskpd rax,  xmm0      ; Moves 2 bits into RAX
         lea      rcx,  jmpTable
         jmp      qword ptr [rcx][rax*8]

jmpTable qword    nene
         qword    neeq
         qword    eqne
         qword    eqeq</code></pre>
<p>Because <code>movmskpd</code> extracts 2 bits from XMM0 and stores them into RAX, this code can use RAX as an index into a jump table to select four different branch labels. The code at label <code>nene</code> executes if both comparisons produce not equal; label <code>neeq</code> is the target when the lane 0 values are equal but the lane 1 values are not equal. Label <code>eqne</code> is the target when the lane 0 values are not equal but the lane 1 values are equal. Finally, label <code>eqeq</code> is where this code branches when both sets of lanes contain equal values.</p>
<h2 id="h1-501089c11-0013"><span epub:type="pagebreak" title="679" id="Page_679"/>	11.13	Floating-Point Conversion Instructions</h2>
<p class="BodyFirst">Previously, I described several instructions to convert data between various scalar floating-point and integer formats (see <span class="xref" itemid="xref_target_“SSE Floating-Point Conversions” in Chapter 6">“SSE Floating-Point Conversions” in Chapter 6</span>). Variants of these instructions also exist for packed data conversions. <a href="#table11-31" id="tableanchor11-31">Table 11-31</a> lists many of these instructions you will commonly use.</p>
<figure>
<figcaption class="TableTitle"><p><a id="table11-31">Table 11-31</a>: SSE Conversion Instructions</p></figcaption>
<table id="table-501089c11-0031" border="1">
<thead>
<tr>
<td><b>Instruction syntax</b></td>
<td><b>Description</b></td>
</tr>
</thead>
<tbody>
<tr>
<td><code>cvtdq2pd </code><var>xmm</var><sub>dest</sub><code>, </code><var>xmm</var><sub>src</sub><code>/</code><var>mem</var><sub>64</sub></td>
<td>Converts two packed signed double-word integers from XMM<sub>src</sub>/mem<sub>64</sub> to two packed double-precision floating-point values in XMM<sub>dest</sub>. If YMM register is present, this instruction leaves the HO bits unchanged.</td>
</tr>
<tr>
<td><code>vcvtdq2pd </code><var>xmm</var><sub>dest</sub><code>, </code><var>xmm</var><sub>src</sub><code>/</code><var>mem</var><sub>64</sub></td>
<td>(AVX) Converts two packed signed double-word integers from XMM<sub>src</sub>/mem<sub>64</sub> to two packed double-precision floating-point values in XMM<sub>dest</sub>. This instruction stores 0s into the HO bits of the underlying YMM register.</td>
</tr>
<tr>
<td><code>vcvtdq2pd </code><var>ymm</var><sub>dest</sub><code>, </code><var>xmm</var><sub>src</sub><code>/</code><var>mem</var><sub>128</sub></td>
<td>(AVX) Converts four packed signed double-word integers from XMM<sub>src</sub>/mem<sub>128</sub> to four packed double-precision floating-point values in YMM<sub>dest</sub>.</td>
</tr>
<tr>
<td><code>cvtdq2ps </code><var>xmm</var><sub>dest</sub><code>, </code><var>xmm</var><sub>src</sub><code>/</code><var>mem</var><sub>128</sub></td>
<td>Converts four packed signed double-word integers from XMM<sub>src</sub>/mem<sub>128</sub> to four packed single-precision floating-point values in XMM<sub>dest</sub>. If YMM register is present, this instruction leaves the HO bits unchanged.</td>
</tr>
<tr>
<td><code>vcvtdq2ps </code><var>xmm</var><sub>dest</sub><code>, </code><var>xmm</var><sub>src</sub><code>/</code><var>mem</var><sub>128</sub></td>
<td>(AVX) Converts four packed signed double-word integers from XMM<sub>src</sub>/mem<sub>128</sub> to four packed single-precision floating-point values in XMM<sub>dest</sub>. If YMM register is present, this instruction writes 0s to the HO bits.</td>
</tr>
<tr>
<td><code>vcvtdq2ps </code><var>ymm</var><sub>dest</sub><code>, </code><var>ymm</var><sub>src</sub><code>/</code><var>mem</var><sub>256</sub></td>
<td>(AVX) Converts eight packed signed double-word integers from YMM<sub>src</sub>/mem<sub>256</sub> to eight packed single-precision floating-point values in YMM<sub>dest</sub>. If YMM register is present, this instruction writes 0s to the HO bits.</td>
</tr>
<tr>
<td><code>cvtpd2dq </code><var>xmm</var><sub>dest</sub><code>, </code><var>xmm</var><sub>src</sub><code>/</code><var>mem</var><sub>128</sub></td>
<td>Converts two packed double-precision floating-point values from XMM<sub>src</sub>/mem<sub>128</sub> to two packed signed double-word integers in XMM<sub>dest</sub>. If YMM register is present, this instruction leaves the HO bits unchanged. The conversion from floating-point to integer uses the current SSE rounding mode.</td>
</tr>
<tr>
<td><code>vcvtpd2dq </code><var>xmm</var><sub>dest</sub><code>, </code><var>xmm</var><sub>src</sub><code>/</code><var>mem</var><sub>128</sub></td>
<td>(AVX) Converts two packed double-precision floating-point values from XMM<sub>src</sub>/mem<sub>128</sub> to two packed signed double-word integers in XMM<sub>dest</sub>. This instruction stores 0s into the HO bits of the underlying YMM register. The conversion from floating-point to integer uses the current AVX rounding mode.</td>
</tr>
<tr>
<td><code>vcvtpd2dq </code><var>xmm</var><sub>dest</sub><code>, </code><var>ymm</var><sub>src</sub><code>/</code><var>mem</var><sub>256</sub></td>
<td>(AVX) Converts four packed double-precision floating-point values from YMM<sub>src</sub>/mem<sub>256</sub> to four packed signed double-word integers in XMM<sub>dest</sub>. The conversion of floating-point to integer uses the current AVX rounding mode.</td>
</tr>
<tr>
<td><code><span epub:type="pagebreak" title="680" id="Page_680"/>cvtpd2ps </code><var>xmm</var><sub>dest</sub><code>, </code><var>xmm</var><sub>src</sub><code>/</code><var>mem</var><sub>128</sub></td>
<td>Converts two packed double-precision floating-point values from XMM<sub>src</sub>/mem<sub>128</sub> to two packed single-precision floating-point values in XMM<sub>dest</sub>. If YMM register is present, this instruction leaves the HO bits unchanged.</td>
</tr>
<tr>
<td><code>vcvtpd2ps </code><var>xmm</var><sub>dest</sub><code>, </code><var>xmm</var><sub>src</sub><code>/</code><var>mem</var><sub>128</sub></td>
<td>(AVX) Converts two packed double-precision floating-point values from XMM<sub>src</sub>/mem<sub>128</sub> to two packed single-precision floating-point values in XMM<sub>dest</sub>. This instruction stores 0s into the HO bits of the underlying YMM register.</td>
</tr>
<tr>
<td><code>vcvtpd2ps </code><var>xmm</var><sub>dest</sub><code>, </code><var>ymm</var><sub>src</sub><code>/</code><var>mem</var><sub>256</sub></td>
<td>(AVX) Converts four packed double-precision floating-point values from YMM<sub>src</sub>/mem<sub>256</sub> to four packed single-precision floating-point values in YMM<sub>dest</sub>.</td>
</tr>
<tr>
<td><code>cvtps2dq </code><var>xmm</var><sub>dest</sub><code>, </code><var>xmm</var><sub>src</sub><code>/</code><var>mem</var><sub>128</sub></td>
<td>Converts four packed single-precision floating-point values from XMM<sub>src</sub>/mem<sub>128</sub> to four packed signed double-word integers in XMM<sub>dest</sub>. If YMM register is present, this instruction leaves the HO bits unchanged. The conversion of floating-point to integer uses the current SSE rounding mode.</td>
</tr>
<tr>
<td><code>vcvtps2dq </code><var>xmm</var><sub>dest</sub><code>, </code><var>xmm</var><sub>src</sub><code>/</code><var>mem</var><sub>128</sub></td>
<td>(AVX) Converts four packed single-precision floating-point values from XMM<sub>src</sub>/mem<sub>128</sub> to four packed signed double-word integers in XMM<sub>dest</sub>. This instruction stores 0s into the HO bits of the underlying YMM register. The conversion of floating-point to integer uses the current AVX rounding mode.</td>
</tr>
<tr>
<td><code>vcvtps2dq </code><var>ymm</var><sub>dest</sub><code>, </code><var>ymm</var><sub>src</sub><code>/</code><var>mem</var><sub>256</sub></td>
<td>(AVX) Converts eight packed single-precision floating-point values from YMM<sub>src</sub>/mem<sub>256</sub> to eight packed signed double-word integers in YMM<sub>dest</sub>. The conversion of floating-point to integer uses the current AVX rounding mode.</td>
</tr>
<tr>
<td><code>cvtps2pd </code><var>xmm</var><sub>dest</sub><code>, </code><var>xmm</var><sub>src</sub><code>/</code><var>mem</var><sub>64</sub></td>
<td>Converts two packed single-precision floating-point values from XMM<sub>src</sub>/mem<sub>64</sub> to two packed double-precision values in XMM<sub>dest</sub>. If YMM register is present, this instruction leaves the HO bits unchanged.</td>
</tr>
<tr>
<td><code>vcvtps2pd </code><var>xmm</var><sub>dest</sub><code>, </code><var>xmm</var><sub>src</sub><code>/</code><var>mem</var><sub>64</sub></td>
<td>(AVX) Converts two packed single-precision floating-point values from XMM<sub>src</sub>/mem<sub>64</sub> to two packed double-precision values in XMM<sub>dest</sub>. This instruction stores 0s into the HO bits of the underlying YMM register.</td>
</tr>
<tr>
<td><code>vcvtps2pd </code><var>ymm</var><sub>dest</sub><code>, </code><var>xmm</var><sub>src</sub><code>/</code><var>mem</var><sub>128</sub></td>
<td>(AVX) Converts four packed single-precision floating-point values from XMM<sub>src</sub>/mem<sub>128</sub> to four packed double-precision values in YMM<sub>dest</sub>.</td>
</tr>
<tr>
<td><code>cvttpd2dq </code><var>xmm</var><sub>dest</sub><code>, </code><var>xmm</var><sub>src</sub><code>/</code><var>mem</var><sub>128</sub></td>
<td>Converts two packed double-precision floating-point values from XMM<sub>src</sub>/mem<sub>128</sub> to two packed signed double-word integers in XMM<sub>dest</sub> using truncation. If YMM register is present, this instruction leaves the HO bits unchanged.</td>
</tr>
<tr>
<td><code>vcvttpd2dq </code><var>xmm</var><sub>dest</sub><code>, </code><var>xmm</var><sub>src</sub><code>/</code><var>mem</var><sub>128</sub></td>
<td>(AVX) Converts two packed double-precision floating-point values from XMM<sub>src</sub>/mem<sub>128</sub> to two packed signed double-word integers in XMM<sub>dest</sub> using truncation. This instruction stores 0s into the HO bits of the underlying YMM register.</td>
</tr>
<tr>
<td><code>vcvttpd2dq </code><var>xmm</var><sub>dest</sub><code>, </code><var>ymm</var><sub>src</sub><code>/</code><var>mem</var><sub>256</sub></td>
<td>(AVX) Converts four packed double-precision floating-point values from YMM<sub>src</sub>/mem<sub>256</sub> to four packed signed double-word integers in XMM<sub>dest</sub> using truncation. </td>
</tr>
<tr>
<td><code>cvttps2dq </code><var>xmm</var><sub>dest</sub><code>, </code><var>xmm</var><sub>src</sub><code>/</code><var>mem</var><sub>128</sub></td>
<td>Converts four packed single-precision floating-point values from XMM<sub>src</sub>/mem<sub>128</sub> to four packed signed double-word integers in XMM<sub>dest</sub> using truncation. If YMM register is present, this instruction leaves the HO bits unchanged.</td>
</tr>
<tr>
<td><code><span epub:type="pagebreak" title="681" id="Page_681"/>vcvttps2dq </code><var>xmm</var><sub>dest</sub><code>, </code><var>xmm</var><sub>src</sub><code>/</code><var>mem</var><sub>128</sub></td>
<td>(AVX) Converts four packed single-precision floating-point values from XMM<sub>src</sub>/mem<sub>128</sub> to four packed signed double-word integers in XMM<sub>dest</sub> using truncation. This instruction stores 0s into the HO bits of the underlying YMM register.</td>
</tr>
<tr>
<td><code>vcvttps2dq </code><var>ymm</var><sub>dest</sub><code>, </code><var>ymm</var><sub>src</sub><code>/</code><var>mem</var><sub>256</sub></td>
<td>(AVX) Converts eight packed single-precision floating-point values from YMM<sub>src</sub>/mem<sub>256</sub> to eight packed signed double-word integers in YMM<sub>dest</sub> using truncation. </td>
</tr>
</tbody>
</table>
</figure>
<h2 id="h1-501089c11-0014">	11.14	Aligning SIMD Memory Accesses</h2>
<p class="BodyFirst">Most SSE and AVX instructions require their memory operands to be on a 16-byte (SSE) or 32-byte (AVX) boundary, but this is not always possible. The easiest way to handle unaligned memory addresses is to use instructions that don’t require aligned memory operands, like <code>movdqu</code>, <code>movups</code>, and <code>movupd</code>. However, the performance hit of using unaligned data movement instructions often defeats the purpose of using SSE/AVX instructions in the first place.</p>
<p>Instead, the trick to aligning data for use by SIMD instructions is to process the first few data items by using standard general-purpose registers until you reach an address that is aligned properly. For example, suppose you want to use the <code>pcmpeqb</code> instruction to compare blocks of 16 bytes in a large array of bytes. <code>pcmpeqb</code> requires its memory operands to be at 16-byte-aligned addresses, so if the memory operand is not already 16-byte-aligned, you can process the first 1 to 15 bytes in the array by using standard (non-SSE) instructions until you reach an appropriate address for <code>pcmpeqb</code>; for example:</p>
<pre><code>cmpLp:  mov  al, [rsi]
        cmp  al, someByteValue
        je   foundByte
        inc  rsi
        test rsi, 0Fh
        jnz  cmpLp
 <var>Use SSE instructions here, as RSI is now 16-byte-aligned</var> </code></pre>
<p>ANDing RSI with 0Fh produces a 0 result (and sets the zero flag) if the LO 4 bits of RSI contain 0. If the LO 4 bits of RSI contain 0, the address it contains is aligned on a 16-byte boundary.<sup class="FootnoteReference"><a id="c11-footnoteref-18" href="#c11-footnote-18">16</a></sup></p>
<p>The only drawback to this approach is that you must process as many as 15 bytes individually until you get an appropriate address. That’s 6 × 15, or 90, machine instructions. However, for large blocks of data (say, more than about 48 or 64 bytes), you amortize the cost of the single-byte comparisons, and this approach isn’t so bad.</p>
<p>To improve the performance of this code, you can modify the initial address so that it begins at a 16-byte boundary. ANDing the value in RSI <span epub:type="pagebreak" title="682" id="Page_682"/>(in this particular example) with 0FFFFFFFFFFFFFFF0h (–16) modifies RSI so that it holds the address of the start of the 16-byte block containing the original address:<sup class="FootnoteReference"><a id="c11-footnoteref-19" href="#c11-footnote-19">17</a></sup></p>
<pre><code>           and  rsi, -16</code></pre>
<p>To avoid matching unintended bytes before the start of the data structure, we can create a mask to cover the extra bytes. For example, suppose that we’re using the following instruction sequence to rapidly compare 16 bytes at a time:</p>
<pre><code>           sub      rsi, 16
cmpLp:     add      rsi, 16
           movdqa   xmm0, xmm2   ; XMM2 contains bytes to test
           pcmpeqb  xmm0, [rsi]
           pmovmskb eax, xmm0
           ptest    eax, eax
           jz       cmpLp</code></pre>
<p>If we use the AND instruction to align the RSI register prior to the execution of this code, we might get false results when we compare the first 16 bytes. To solve this, we can create a mask that will eliminate any bits from unintended comparisons. To create this mask, we start with all 1 bits and zero out any bits corresponding to addresses from the beginning of the 16-byte block to the first actual data item we’re comparing. This mask can be calculated using the following expression:</p>
<pre><code>-1 &lt;&lt; (startAdrs &amp; 0xF)  ; Note: -1 is all 1 bits</code></pre>
<p>This creates 0 bits in the locations before the data to compare and 1 bit thereafter (for the first 16 bytes). We can use this mask to zero out the undesired bit results from the <code>pmovmskb</code> instruction. The following code snippet demonstrates this technique:</p>
<pre><code>           mov    rcx, rsi
           and    rsi, -16   ; Align to 16 bits
           and    ecx, 0fH   ; Strip out offset of start of data
           mov    ebx, -1    ; 0FFFFFFFFh – all 1 bits
           shl    ebx, cl    ; Create mask

; Special case for the first 1 to 16 bytes:

           movdqa   xmm0, xmm2
           pcmpeqb  xmm0, [rsi]
           pmovmskb eax, xmm0
           and      eax, ebx
           jnz      foundByte
<span epub:type="pagebreak" title="683" id="Page_683"/>cmpLp:     add      rsi, 16
           movdqa   xmm0, xmm2   ; XMM2 contains bytes to test
           pcmpeqb  xmm0, [rsi]
           pmovmskb eax, xmm0
           test     eax, eax
           jz       cmpLp
foundByte:
<var> Do whatever needs to be done when the block of 16 bytes</var>
<var>   contains at least one match between the bytes in XMM2</var>
<var>   and the data at RSI </var></code></pre>
<p>Suppose, for example, that the address is already aligned on a 16-byte boundary. ANDing that value with 0Fh produces 0. Shifting –1 to the left zero positions produces –1 (all 1 bits). Later, when the code logically ANDs this with the mask obtained after the <code>pcmpeqb</code> and <code>pmovmskb</code> instructions, the result does not change. Therefore, the code tests all 16 bytes (as we would want if the original address is 16-byte-aligned).</p>
<p>When the address in RSI has the value 0001b in the LO 4 bits, the actual data starts at offset 1 into the 16-byte block. So, we want to ignore the first byte when comparing the values in XMM2 against the 16 bytes at [RSI]. In this case, the mask is 0FFFFFFFEh, which is all 1s except for a 0 in bit 0. After the comparison, if bit 0 of EAX contains a 1 (meaning the bytes at offset 0 match), the AND operation eliminates this bit (replacing it with 0) so it doesn’t affect the comparison. Likewise, if the starting offset into the block is 2, 3, . . . , 15, the <code>shl</code> instruction modifies the bit mask in EBX to eliminate bytes at those offsets from consideration in the first compare operation. The result is that it takes only 11 instructions to do the same work as (up to) 90+ instructions in the original (byte-by-byte comparison) example.</p>
<h2 id="h1-501089c11-0015">	11.15	Aligning Word, Dword, and Qword Object Addresses</h2>
<p class="BodyFirst">When aligning non-byte-sized objects, you increment the pointer by the size of the object (in bytes) until you obtain an address that is 16- (or 32-) byte-aligned. However, this works only if the object size is 2, 4, or 8 (because any other value will likely miss addresses that are multiples of 16).</p>
<p>For example, you can process the first several elements of an array of word objects (where the first element of the array appears at an even address in memory) on a word-by-word basis, incrementing the pointer by 2, until you obtain an address that is divisible by 16 (or 32). Note, though, that this scheme works only if the array of objects begins at an address that is a multiple of the element size. For example, if an array of word values begins at an odd address in memory, you will not be able to get an address that is divisible by 16 or 32 with a series of additions by 2, and you would not be able to use SSE/AVX instructions to process this data without first moving it to another location in memory that is properly aligned.</p>
<h2 id="h1-501089c11-0016"><span epub:type="pagebreak" title="684" id="Page_684"/>	11.16	Filling an XMM Register with Several Copies of the Same Value</h2>
<p class="BodyFirst">For many SIMD algorithms, you will want multiple copies of the same value in an XMM or a YMM register. You can use the <code>(v)movddup</code>, <code>(v)movshdup</code>, <code>(v)pinsd</code>, <code>(v)pinsq</code>, and <code>(v)pshufd</code> instructions for single-precision and double-precision floating-point values. For example, if you have a single-precision floating-point value, <code>r4var</code>, in memory and you want to replicate it throughout XMM0, you could use the following code:</p>
<pre><code>movss  xmm0, r4var
pshufd xmm0, xmm0, 0    ; Lanes 3, 2, 1, and 0 from lane 0</code></pre>
<p>To copy a pair of double-precision floating-point values from <code>r8var</code> into XMM0, you could use:</p>
<pre><code>movsd  xmm0, r8var
pshufd xmm0, xmm0, 44h  ; Lane 0 to lanes 0 and 2, 1 to 1, and 3</code></pre>
<p>Of course, <code>pshufd</code> is really intended for double-word integer operations, so additional latency (time) may be involved in using <code>pshufd</code> immediately after <code>movsd</code> or <code>movss</code>. Although <code>pshufd</code> allows a memory operand, that operand must be a 16-byte-aligned 128-bit-memory operand, so it’s not useful for directly copying a floating-point value through an XMM register.</p>
<p>For double-precision floating-point values, you can use <code>movddup</code> to duplicate a single 64-bit float in the LO bits of an XMM register into the HO bits:</p>
<pre><code>movddup xmm0, r8var</code></pre>
<p>The <code>movddup</code> instruction allows unaligned 64-bit memory operands, so it’s probably the best choice for duplicating double-precision values.</p>
<p>To copy byte, word, dword, or qword integer values throughout an XMM register, the <code>pshufb</code>, <code>pshufw</code>, <code>pshufd</code>, or <code>pshufq</code> instructions are a good choice. For example, to replicate a single byte throughout XMM0, you could use the following sequence:</p>
<pre><code>movzx  eax, byteToCopy
movd   xmm0, eax
pxor   xmm1, xmm1   ; Mask to copy byte 0 throughout
pshufb xmm0, xmm1</code></pre>
<p>The XMM1 operand is an array of bytes containing masks used to copy data from locations in XMM0 onto itself. The value 0 copies byte 0 in XMM0 throughout all the other bits in XMM0. This same code can be used to copy words, dwords, and qwords by simply changing the mask value <span epub:type="pagebreak" title="685" id="Page_685"/>in XMM1. Or you could use the <code>pshuflw</code> or <code>pshufd</code> instructions to do the job. Here’s another variant that replicates a byte throughout XMM0:</p>
<pre><code>movzx     eax, byteToCopy
mov       ah, al
movd      xmm0, eax
punpcklbw xmm0, xmm0    ; Copy bytes 0 and 1 to 2 and 3
pshufd    xmm0, xmm0, 0 ; Copy LO dword throughout</code></pre>
<h2 id="h1-501089c11-0017">	11.17	Loading Some Common Constants Into XMM and YMM Registers</h2>
<p class="BodyFirst">No SSE/AVX instructions let you load an immediate constant into a register. However, you can use a couple of idioms (tricks) to load certain common constant values into an XMM or a YMM register. This section discusses some of these idioms.</p>
<p>Loading 0 into an SSE/AVX register uses the same idiom that general-purpose integer registers employ: exclusive-OR the register with itself. For example, to set all the bits in XMM0 to 0s, you would use the following instruction:</p>
<pre><code>pxor xmm0, xmm0</code></pre>
<p>To set all the bits in an XMM or a YMM register to 1, you can use the <code>pcmpeqb</code> instruction, as follows:</p>
<pre><code>pcmpeqb xmm0, xmm0</code></pre>
<p>Because any given XMM or YMM register is equal to itself, this instruction stores 0FFh in all the bytes of XMM0 (or whatever XMM or YMM register you specify).</p>
<p>If you want to load the 8-bit value 01h into all 16 bytes of an XMM register, you can use the following code (this comes from Intel):</p>
<pre><code>pxor    xmm0, xmm0
pcmpeqb xmm1, xmm1
psubb   xmm0, xmm1   ; 0 - (-1) is (1)</code></pre>
<p>You can substitute <code>psubw</code> or <code>psubd</code> for <code>psubb</code> in this example if you want to create 16- or 32-bit results (for example, four 32-bit dwords in XMM0, each containing the value 00000001h).</p>
<p>If you would like the 1 bit in a different bit position (rather than bit 0 of each byte), you can use the <code>pslld</code> instruction after the preceding sequence to reposition the bits. For example, if you want to load the XMM0 register with 8080808080808080h, you could use the following instruction sequence:</p>
<pre><code>pxor    xmm0, xmm0
pcmpeqb xmm1, xmm1
<span epub:type="pagebreak" title="686" id="Page_686"/>psubb   xmm0, xmm1
pslld   xmm0, 7         ; 01h -&gt; 80h in each byte</code></pre>
<p>Of course, you can supply a different immediate constant to <code>pslld</code> to load each byte in the register with 02h, 04h, 08h, 10h, 20h, or 40h.</p>
<p>Here’s a neat trick you can use to load 2<sup><em>n</em></sup> – 1 (all 1 bits up to the <em>n</em>th bit in a number) into all the lanes on an SSE/AVX register:<sup class="FootnoteReference"><a id="c11-footnoteref-20" href="#c11-footnote-20">18</a></sup></p>
<pre><code>; For 16-bit lanes:

pcmpeqd  xmm0, xmm0     ; Set all bits to 1
psrlw    xmm0, 16 - <var>n</var>   ; Clear top 16 - <var>n</var> bits of xmm0

; For 32-bit lanes:

pcmpeqd  xmm0, xmm0     ; Set all bits to 1
psrld    xmm0, 32 - <var>n</var>   ; Clear top 16 - <var>n</var> bits of xmm0

; For 64-bit lanes:

pcmpeqd  xmm0, xmm0     ; Set all bits to 1
psrlq    xmm0, 64 - <var>n</var>   ; Clear top 16 - <var>n</var> bits of xmm0</code></pre>
<p>You can also load the inverse (NOT(2<sup><em>n</em></sup> – 1), all 1 bits in bit position <em>n</em> through the end of the register) by shifting to the left rather than the right:</p>
<pre><code>; For 16-bit lanes:

pcmpeqd  xmm0, xmm0     ; Set all bits to 1
psllw    xmm0, <var>n</var>        ; Clear bottom <var>n</var> bits of xmm0

; For 32-bit lanes:

pcmpeqd  xmm0, xmm0     ; Set all bits to 1
pslld    xmm0, <var>n</var>        ; Clear bottom <var>n</var> bits of xmm0

; For 64-bit lanes:

pcmpeqd  xmm0, xmm0     ; Set all bits to 1
psllq    xmm0, <var>n</var>        ; Clear bottom <var>n</var> bits of xmm0</code></pre>
<p>Of course, you can also load a “constant” into an XMM or a YMM register by putting that constant into a memory location (preferably 16- or 32-byte-aligned) and then using a <code>movdqu</code> or <code>movdqa</code> instruction to load that value into a register. Do keep in mind, however, that such an operation can be relatively slow if the data in memory does not appear in cache. Another possibility, if the constant is small enough, is to load the constant into a 32- or 64-bit integer register and use <code>movd</code> or <code>movq</code> to copy that value into an XMM register.</p>
<h2 id="h1-501089c11-0018"><span epub:type="pagebreak" title="687" id="Page_687"/>	11.18	Setting, Clearing, Inverting, and Testing a Single Bit in an SSE Register</h2>
<p class="BodyFirst">Here’s another set of tricks suggested by Raymond Chen (<a href="https://blogs.msdn.microsoft.com/oldnewthing/20141222-00/?p=43333/" class="LinkURL">https://blogs.msdn.microsoft.com/oldnewthing/20141222-00/?p=43333/</a>) to set, clear, or test an individual bit in an XMM register.</p>
<p>To set an individual bit (bit <em>n</em>, assuming that <em>n</em> is a constant) with all other bits cleared, you can use the following macro:</p>
<pre><code>; setXBit - Sets bit <var>n</var> in SSE register xReg.

setXBit  macro   xReg, n
         pcmpeqb xReg, xReg   ; Set all bits in xReg
         psrlq   xReg, 63     ; Set both 64-bit lanes to 01h
         if      n lt 64
         psrldq  xReg, 8      ; Clear the upper lane
         else
         pslldq  xReg, 8      ; Clear the lower lane
         endif
         if      (n and 3fh) ne 0
         psllq   xReg, (n and 3fh)
         endif
         endm</code></pre>
<p>Once you can fill an XMM register with a single set bit, you can use that register’s value to set, clear, invert, or test that bit in another XMM register. For example, to set bit <em>n</em> in XMM1, without affecting any of the other bits in XMM1, you could use the following code sequence:</p>
<pre><code>setXBit xmm0, <var>n</var>      ; Set bit <var>n</var> in XMM1 to 1 without
por     xmm1, xmm0   ; affecting any other bits</code></pre>
<p>To clear bit <em>n</em> in an XMM register, you use the same sequence but substitute the <code>vpandn</code> (AND NOT) instruction for the <code>por</code> instruction:</p>
<pre><code>setXBit xmm0, <var>n</var>            ; Clear bit <var>n</var> in XMM1 without
vpandn  xmm1, xmm0, xmm1   ; affecting any other bits</code></pre>
<p>To invert a bit, simply substitute <code>pxor</code> for <code>por</code> or <code>vpandn</code>:</p>
<pre><code>setXBit xmm0, <var>n</var>      ; Invert bit <var>n</var> in XMM1 without
pxor    xmm1, xmm0   ; affecting any other bits</code></pre>
<p>To test a bit to see if it is set, you have a couple of options. If your CPU supports the SSE4.1 instruction set extensions, you can use the <code>ptest</code> instruction:</p>
<pre><code>setXBit xmm0, <var>n</var>      ; Test bit <var>n</var> in XMM1
ptest   xmm1, xmm0
jnz     bitNisSet    ; Fall through if bit <var>n</var> is clear</code></pre>
<p><span epub:type="pagebreak" title="688" id="Page_688"/>If you have an older CPU that doesn’t support the <code>ptest</code> instruction, you can use <code>pmovmskb</code> as follows:</p>
<pre><code>; Remember, psllq shifts bits, not bytes.
; If bit <var>n</var> is not in bit position 7 of a given
; byte, then move it there. For example, if <var>n</var> = 0, then
; (7 - (0 and 7)) is 7, so psllq moves bit 0 to bit 7.

movdqa   xmm0, xmm1
if       7 - (<var>n</var> and 7)
psllq    xmm0, 7 - (<var>n</var> and 7)
endif

; Now that the desired bit to test is sitting in bit position
; 7 of *some* byte, use pmovmskb to extract all bit 7s into AX:

pmovmskb eax, xmm0

; Now use the (integer) test instruction to test that bit:

test    ax, 1 shl (<var>n</var> / 8)
jnz     bitNisSet</code></pre>
<h2 id="h1-501089c11-0019">	11.19	Processing Two Vectors by Using a Single Incremented Index</h2>
<p class="BodyFirst">Sometimes your code will need to process two blocks of data simultaneously, incrementing pointers into both blocks during the execution of the loop. </p>
<p>One easy way to do this is to use the scaled-indexed addressing mode. If R8 and R9 contain pointers to the data you want to process, you can walk along both blocks of data by using code such as the following:</p>
<pre><code>          dec rcx
blkLoop:  inc rcx
          mov eax, [r8][rcx * 4]
          cmp eax, [r9][rcx * 4]
          je  theyreEqual
          cmp eax, sentinelValue
          jne blkLoop</code></pre>
<p>This code marches along through the two dword arrays comparing values (to search for an equal value in the arrays at the same index). This loop uses four registers: EAX to compare the two values from the arrays, the two pointers to the arrays (R8 and R9), and then the RCX index register to step through the two arrays. </p>
<p>It is possible to eliminate RCX from this loop by incrementing the R8 and R9 registers in this loop (assuming it’s okay to modify the values in R8 and R9):</p>
<pre><code>          sub r8, 4
          sub r9, 4
blkLoop:  add r8, 4
<span epub:type="pagebreak" title="689" id="Page_689"/>          add r9, 4
          mov eax, [r8]
          cmp eax, [r9]
          je  theyreEqual
          cmp eax, sentinelValue
          jne blkLoop</code></pre>
<p>This scheme requires an extra <code>add</code> instruction in the loop. If the execution speed of this loop is critical, inserting this extra addition could be a deal breaker.</p>
<p>There is, however, a sneaky trick you can use so that you have to increment only a single register on each iteration of the loop:</p>
<pre><code>          sub r9, r8            ; R9 = R9 - R8
          sub r8, 4
blkLoop:  add r8, 4
          mov eax, [r8]
          cmp eax, [r9][r8 * 1] ; Address = R9 + R8
          je  theyreEqual
          cmp eax, sentinelValue
          jne blkLoop</code></pre>
<p>The comments are there because they explain the trick being used. At the beginning of the code, you subtract the value of R8 from R9 and leave the result in R9. In the body of the loop, you compensate for this subtraction by using the <code>[r9][r8 * 1]</code> scaled-indexed addressing mode (whose effective address is the sum of R8 and R9, thus restoring R9 to its original value, at least on the first iteration of the loop). Now, because the <code>cmp</code> instruction’s memory address is the sum of R8 and R9, adding 4 to R8 also adds 4 to the effective address used by the <code>cmp</code> instruction. Therefore, on each iteration of the loop, the <code>mov</code> and <code>cmp</code> instructions look at successive elements of their respective arrays, yet the code has to increment only a single pointer.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p>	In this example, you always use the <code>*</code><code> </code><code>1</code> scale factor on the scaled-indexed addressing mode. Adjusting for the size of the operand (4 bytes) happens when adding 4 to the R8 register.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>This scheme works especially well when processing SIMD arrays with SSE and AVX instructions because the XMM and YMM registers are 16 and 32 bytes each, so you can’t use normal scaling factors (1, 2, 4, or 8) to index into an array of packed data values. You wind up having to add 16 (or 32) to your pointers when stepping through the arrays, thus losing one of the benefits of the scaled-indexed addressing mode. For example:</p>
<pre><code>; Assume R9 and R8 point at (32-byte-aligned) arrays of 20 double values.
; Assume R10 points at a (32-byte-aligned) destination array of 20 doubles.

          sub     r9, r8     ; R9 = R9 - R8
          sub     r10, r8    ; R10 = R10 – R8
          sub     r8, 32
<span epub:type="pagebreak" title="690" id="Page_690"/>          mov     ecx, 5     ; Vector with 20 (5 * 4) double values
addLoop:  add     r8, 32
          vmovapd ymm0, [r8]
          vaddpd  ymm0, ymm0, [r9][r8 * 1] ; Address = R9 + R8
          vmovapd [r10][r8 * 1], ymm0      ; Address = R10 + R8
          dec     ecx
          jnz     addLoop</code></pre>
<h2 id="h1-501089c11-0020">	11.20	Aligning Two Addresses to a Boundary</h2>
<p class="BodyFirst">The <code>vmovapd</code> and <code>vaddpd</code> instructions from the preceding example require their memory operands to be 32-byte-aligned or you will get a general protection fault (memory access violation). If you have control over the placement of the arrays in memory, you can specify an alignment for the arrays. If you have no control over the data’s placement in memory, you have two options: working with the unaligned data regardless of the performance loss, or moving the data to a location where it is properly aligned.</p>
<p>If you must work with unaligned data, you can substitute an unaligned move for an aligned move (for example, <code>vmovupd</code> for <code>vmovdqa</code>) or load the data into a YMM register by using an unaligned move and then operate on the data in that register by using your desired instruction. For example:</p>
<pre><code>addLoop:  add     r8, 32
          vmovupd ymm0, [r8]
          vmovupd ymm1, [r9][r8 * 1]  ; Address = R9 + R8
          vaddpd  ymm0, ymm0, ymm1
          vmovupd [r10][r8 * 1], ymm0 ; Address = R10 + R8
          dec     ecx
          jnz     addLoop</code></pre>
<p>Sadly, the <code>vaddpd</code> instruction does not support unaligned access to memory, so you must load the value from the second array (pointed at by R9) into another register (YMM1) before the packed addition operation. This is the drawback to unaligned access: not only are unaligned moves slower, but you also may need to use additional registers and instructions to deal with unaligned data.</p>
<p>Moving the data to a memory location whose alignment you can control is an option when you have a data operand you will be using over and over again in the future. Moving data is an expensive operation; however, if you have a standard block of data you’re going to compare against many other blocks, you can amortize the cost of moving that block to a new location over all the operations you need to do. </p>
<p>Moving the data is especially useful when one (or both) of the data arrays appears at an address that is not an integral multiple of the sub-elements’s size. For example, if you have an array of dwords that begin at an odd address, you will never be able to align a pointer to that array’s data to a 16-byte boundary without moving the data.</p>
<h2 id="h1-501089c11-0021"><span epub:type="pagebreak" title="691" id="Page_691"/>	11.21	Working with Blocks of Data Whose Length Is Not a Multiple of the SSE/AVX Register Size</h2>
<p class="BodyFirst">Using SIMD instructions to march through a large data set processing 2, 4, 8, 16, or 32 values at a time often allows a SIMD algorithm (a <em>vectorized</em> algorithm) to run an order of magnitude faster than the SISD (scalar) algorithm. However, two boundary conditions create problems: the start of the data set (when the starting address might not be properly aligned) and the end of the data set (when there might not be a sufficient number of array elements to completely fill an XMM or a YMM register). I’ve addressed the issues with the start of the data set (misaligned data) already. This section takes a look at the latter problem.</p>
<p>For the most part, when you run out of data at the end of the array (and the XMM and YMM registers need more for a packed operation), you can use the same technique given earlier for aligning a pointer: load more data than is necessary into the register and mask out the unneeded results. For example, if only 8 bytes are left to process in a byte array, you can load 16 bytes, do the operation, and ignore the results from the last 8 bytes. In the comparison loop examples I’ve been using through these past sections, you could do the following:</p>
<pre><code>movdqa   xmm0, [r8]
pcmpeqd  xmm0, [r9]
pmovmskb eax, xmm0
and      eax, 0ffh     ; Mask out the last 8 compares
cmp      eax, 0ffh
je       matchedData</code></pre>
<p>In most cases, accessing data beyond the end of the data structures (either the data pointed at by R8, R9, or both in this example) is harmless. However, as you saw in <span class="xref" itemid="xref_target_“Memory Access and 4K Memory Management Unit Pages” in Chapter 3">“Memory Access and 4K Memory Management Unit Pages” in Chapter 3</span>, if that extra data happens to cross a memory management unit page, and that new page doesn’t allow read access, the CPU will generate a general protection fault (memory access or segmentation fault). Therefore, unless you know that valid data follows the array in memory (at least to the extent the instruction references), you shouldn’t access that memory area; doing so could crash your software.</p>
<p>This problem has two solutions. First, you can align memory accesses on an address boundary that is the same size as the register (for example, 16-byte alignment for XMM registers). Accessing data beyond the end of the data structure with an SSE/AVX instruction will not cross a page boundary (because 16-byte accesses aligned on 16-byte boundaries will always fall within the same MMU page, and ditto for 32-byte accesses on 32-byte boundaries).</p>
<p>The second solution is to examine the memory address prior to accessing memory. While you cannot access the new page without possibly triggering <span epub:type="pagebreak" title="692" id="Page_692"/>an access fault,<sup class="FootnoteReference"><a id="c11-footnoteref-21" href="#c11-footnote-21">19</a></sup> you can check the address itself and see if accessing 16 (or 32) bytes at that address will access data in a new page. If it would, you can take some precautions before accessing the data on the next page. For example, rather than continuing to process the data in SIMD mode, you could drop down to SISD mode and finish processing the data to the end of the array by using standard scalar instructions.</p>
<p>To test if a SIMD access will cross an MMU page boundary, supposing that R9 contains the address at which you’re about to access 16 bytes in memory using an SSE instruction, use code like the following:</p>
<pre><code>mov  eax, r9d
and  eax, 0fffh
cmp  eax, 0ff0h
ja   willCrossPage</code></pre>
<p>Each MMU page is 4KB long and is situated on a 4KB address boundary in memory. Therefore, the LO 12 bits of an address provide an index into the MMU page associated with that address. The preceding code checks whether the address has a page offset greater than 0FF0h (4080). If so, then accessing 16 bytes starting at that address will cross a page boundary. Check for a value of 0FE0h if you need to check for a 32-byte access. </p>
<h2 id="h1-501089c11-0022">	11.22	Dynamically Testing for a CPU Feature</h2>
<p class="BodyFirst">At the beginning of this chapter, I mentioned that when testing the CPU feature set to determine which extensions it supports, the best solution is to dynamically select a set of functions based on the presence or absence of certain capabilities. To demonstrate dynamically testing for, and using (or avoiding), certain CPU features—specifically, testing for the presence of AVX extensions—I’ll modify (and expand) the <code>print</code> procedure that I’ve been using in examples up to this point. </p>
<p>The <code>print</code> procedure I’ve been using is very convenient, but it doesn’t preserve any SSE or AVX registers that a call to <code>printf()</code> could (legally) modify. A generic version of <code>print</code> should preserve the volatile XMM and YMM registers as well as general-purpose registers. </p>
<p>The problem is that you cannot write a generic version of <code>print</code> that will run on all CPUs. If you preserve the XMM registers only, the code will run on any x86-64 CPU. However, if the CPU supports the AVX extensions and the program uses YMM0 to YMM5, the print routine will preserve only the LO 128 bits of those registers, as they are aliased to the corresponding XMM registers. If you save the volatile YMM registers, that code will crash on a CPU that doesn’t support the AVX extensions. So, the trick is to write code that will dynamically determine whether the CPU has the AVX registers and preserve them if they are present, and otherwise preserve only the SSE registers.</p>
<p><span epub:type="pagebreak" title="693" id="Page_693"/>The easy way to do this, and probably the most appropriate solution for the <code>print</code> function, is to simply stick the <code>cpuid</code> instruction inside <code>print</code> and test the results immediately before preserving (and restoring) the registers. Here’s a code fragment that demonstrates how this could be done:</p>
<pre><code>AVXSupport  =     10000000h              ; Bit 28

print       proc

; Preserve all the volatile registers
; (be nice to the assembly code that
; calls this procedure):

            push    rax
            push    rbx                  ; CPUID messes with EBX
            push    rcx
            push    rdx
            push    r8
            push    r9
            push    r10
            push    r11

; Reserve space on the stack for the AVX/SSE registers.
; Note: SSE registers need only 96 bytes, but the code
; is easier to deal with if we reserve the full 128 bytes
; that the AVX registers need and ignore the extra 64
; bytes when running SSE code.

            sub     rsp, 192

; Determine if we have to preserve the YMM registers:

            mov     eax, 1
            cpuid
            test    ecx, AVXSupport      ; Test bits 19 and 20
            jnz     preserveAVX

; No AVX support, so just preserve the XXM0 to XXM3 registers:

            movdqu  xmmword ptr [rsp + 00], xmm0
            movdqu  xmmword ptr [rsp + 16], xmm1
            movdqu  xmmword ptr [rsp + 32], xmm2
            movdqu  xmmword ptr [rsp + 48], xmm3
            movdqu  xmmword ptr [rsp + 64], xmm4
            movdqu  xmmword ptr [rsp + 80], xmm5
            jmp     restOfPrint

; YMM0 to YMM3 are considered volatile, so preserve them:

preserveAVX: 
            vmovdqu ymmword ptr [rsp + 000], ymm0
            vmovdqu ymmword ptr [rsp + 032], ymm1
            vmovdqu ymmword ptr [rsp + 064], ymm2
            vmovdqu ymmword ptr [rsp + 096], ymm3
<span epub:type="pagebreak" title="694" id="Page_694"/>            vmovdqu ymmword ptr [rsp + 128], ymm4
            vmovdqu ymmword ptr [rsp + 160], ymm5

restOfPrint:
        <var>The rest of the print function goes here</var></code></pre>
<p>At the end of the <code>print</code> function, when it’s time to restore everything, you could do another test to determine whether to restore XMM or YMM registers.<sup class="FootnoteReference"><a id="c11-footnoteref-22" href="#c11-footnote-22">20</a></sup></p>
<p>For other functions, when you might not want the expense of <code>cpuid</code> (and preserving all the registers it stomps on) incurred on every function call, the trick is to write <em>three</em> functions: one for SSE CPUs, one for AVX CPUs, and a special function (that you call only once) that selects which of these two you will call in the future. The bit of magic that makes this efficient is <em>indirection</em>. You won’t directly call any of these functions. Instead, you’ll initialize a pointer with the address of the function to call and indirectly call one of these three functions by using the pointer. For the current example, we’ll name this pointer <code>print</code> and initialize it with the address of the third function, <code>choosePrint</code>:</p>
<pre><code>          .data
print     qword   choosePrint</code></pre>
<p>Here’s the code for <code>choosePrint</code>:</p>
<pre><code>; On first call, determine if we support AVX instructions
; and set the "print" pointer to point at print_AVX or
; print_SSE:
 
choosePrint proc
            push    rax             ; Preserve registers that get
            push    rbx             ; tweaked by CPUID
            push    rcx
            push    rdx
            
            mov     eax, 1
            cpuid
            test    ecx, AVXSupport ; Test bit 28 for AVX
            jnz     doAVXPrint
            
            lea     rax, print_SSE  ; From now on, call
            mov     print, rax      ; print_SSE directly

; Return address must point at the format string
; following the call to this function! So we have
; to clean up the stack and JMP to print_SSE.

            pop     rdx
            pop     rcx
            pop     rbx
<span epub:type="pagebreak" title="695" id="Page_695"/>            pop     rax
            jmp     print_SSE
            
doAVXPrint: lea     rax, print_AVX  ; From now on, call
            mov     print, rax      ; print_AVX directly
            
; Return address must point at the format string
; following the call to this function! So we have
; to clean up the stack and JMP to print_AUX.

            pop     rdx
            pop     rcx
            pop     rbx
            pop     rax
            jmp     print_AVX

choosePrint endp</code></pre>
<p>The <code>print_SSE</code> procedure runs on CPUs without AVX support, and the <code>print_AVX</code> procedure runs on CPUs with AVX support. The <code>choosePrint</code> procedure executes the <code>cpuid</code> instruction to determine whether the CPU supports the AVX extensions; if so, it initializes the <code>print</code> pointer with the address of the <code>print_AVX</code> procedure, and if not, it stores the address of <code>print_SSE</code> into the <code>print</code> variable.</p>
<p><code>choosePrint</code> is not an explicit initialization procedure you must call prior to calling <code>print</code>. The <code>choosePrint</code> procedure executes only <em>once</em> (assuming you call it via the <code>print</code> pointer rather than calling it directly). After the first execution, the <code>print</code> pointer contains the address of the CPU-appropriate print function, and <code>choosePrint</code> no longer executes.</p>
<p>You call the <code>print</code> pointer just as you would make any other call to <code>print</code>; for example:</p>
<pre><code>call print
byte "Hello, world!", nl, 0</code></pre>
<p>After setting up the <code>print</code> pointer, <code>choosePrint</code> must transfer control to the appropriate print procedure (<code>print_SSE</code> or <code>print_AVX</code>) to do the work the user is expecting. Because preserved register values are sitting on the stack, and the actual print routines expect only a return address, <code>choosePrint</code> will first restore all the (general-purpose) registers it saved and then jump to (not call) the appropriate print procedure. It does a jump, rather than a call, because the return address pointing to the format string is already sitting on the top of the stack. On return from the <code>print_SSE</code> or <code>print_AVX</code> procedure, control will return to whomever called <code>choosePrint</code> (via the <code>print</code> pointer).</p>
<p><a href="#listing11-5" id="listinganchor11-5">Listing 11-5</a> shows the complete <code>print</code> function, with <code>print_SSE</code> and <code>print_AVX</code>, and a simple main program that calls <code>print</code>. I’ve extended <code>print</code> to accept <span epub:type="pagebreak" title="696" id="Page_696"/>arguments in R10 and R11 as well as in RDX, R8, and R9 (this function reserves RCX to hold the address of the format string following the call to <code>print</code>).</p>
<pre><code>; Listing 11-5
 
; Generic print procedure and dynamically
; selecting CPU features.

        option  casemap:none

nl          =       10

; SSE4.2 feature flags (in ECX):

SSE42       =       00180000h       ; Bits 19 and 20
AVXSupport  =       10000000h       ; Bit 28

; CPUID bits (EAX = 7, EBX register)

AVX2Support  =      20h             ; Bit 5 = AVX

            .const
ttlStr      byte    "Listing 11-5", 0

            .data
            align   qword
print       qword   choosePrint     ; Pointer to print function

; Floating-point values for testing purposes:

fp1         real8   1.0
fp2         real8   2.0
fp3         real8   3.0
fp4         real8   4.0
fp5         real8   5.0
            
            .code
            externdef printf:proc
            
; Return program title to C++ program:

            public  getTitle
getTitle    proc
            lea     rax, ttlStr
            ret
getTitle    endp

***************************************************************

; print - "Quick" form of printf that allows the format string to
;         follow the call in the code stream. Supports up to five
;         additional parameters in RDX, R8, R9, R10, and R11.

<span epub:type="pagebreak" title="697" id="Page_697"/>; This function saves all the Microsoft ABI–volatile,
; parameter, and return result registers so that code
; can call it without worrying about any registers being
; modified (this code assumes that Windows ABI treats
; YMM4 to YMM15 as nonvolatile).

; Of course, this code assumes that AVX instructions are
; available on the CPU.

; Allows up to 5 arguments in:

;  RDX - Arg #1
;  R8  - Arg #2
;  R9  - Arg #3
;  R10 - Arg #4
;  R11 - Arg #5

; Note that you must pass floating-point values in
; these registers, as well. The printf function
; expects real values in the integer registers. 

; There are two versions of this function, one that
; will run on CPUs without AVX capabilities (no YMM
; registers) and one that will run on CPUs that
; have AVX capabilities (YMM registers). The difference
; between the two is which registers they preserve
; (print_SSE preserves only XMM registers and will
; run properly on CPUs that don't have YMM register
; support; print_AVX will preserve the volatile YMM
; registers on CPUs with AVX support).

; On first call, determine if we support AVX instructions
; and set the "print" pointer to point at print_AVX or
; print_SSE:

choosePrint proc
            push    rax             ; Preserve registers that get
            push    rbx             ; tweaked by CPUID
            push    rcx
            push    rdx
            
            mov     eax, 1
            cpuid
            test    ecx, AVXSupport ; Test bit 28 for AVX
            jnz     doAVXPrint
            
            lea     rax, print_SSE  ; From now on, call
            mov     print, rax      ; print_SSE directly

; Return address must point at the format string
; following the call to this function! So we have
; to clean up the stack and JMP to print_SSE.

            pop     rdx
            pop     rcx
<span epub:type="pagebreak" title="698" id="Page_698"/>            pop     rbx
            pop     rax
            jmp     print_SSE
            
doAVXPrint: lea     rax, print_AVX  ; From now on, call
            mov     print, rax      ; print_AVX directly
            
; Return address must point at the format string
; following the call to this function! So we have
; to clean up the stack and JMP to print_AUX.

            pop     rdx
            pop     rcx
            pop     rbx
            pop     rax
            jmp     print_AVX

choosePrint endp

; Version of print that will preserve volatile
; AVX registers (YMM0 to YMM3):

print_AVX   proc

; Preserve all the volatile registers
; (be nice to the assembly code that
; calls this procedure):

            push    rax
            push    rbx
            push    rcx
            push    rdx
            push    r8
            push    r9
            push    r10
            push    r11
            
; YMM0 to YMM7 are considered volatile, so preserve them:

            sub     rsp, 256
            vmovdqu ymmword ptr [rsp + 000], ymm0
            vmovdqu ymmword ptr [rsp + 032], ymm1
            vmovdqu ymmword ptr [rsp + 064], ymm2
            vmovdqu ymmword ptr [rsp + 096], ymm3
            vmovdqu ymmword ptr [rsp + 128], ymm4
            vmovdqu ymmword ptr [rsp + 160], ymm5
            vmovdqu ymmword ptr [rsp + 192], ymm6
            vmovdqu ymmword ptr [rsp + 224], ymm7
            
            push    rbp

returnAdrs  textequ &lt;[rbp + 328]&gt;

            mov     rbp, rsp
            sub     rsp, 128
<span epub:type="pagebreak" title="699" id="Page_699"/>            and     rsp, -16
            
; Format string (passed in RCX) is sitting at
; the location pointed at by the return address,
; load that into RCX:

            mov     rcx, returnAdrs

; To handle more than 3 arguments (4 counting
; RCX), you must pass data on stack. However, to the
; print caller, the stack is unavailable, so use
; R10 and R11 as extra parameters (could be just
; junk in these registers, but pass them just
; in case):

            mov     [rsp + 32], r10
            mov     [rsp + 40], r11
            call    printf
            
; Need to modify the return address so
; that it points beyond the zero-terminating byte.
; Could use a fast strlen function for this, but
; printf is so slow it won't really save us anything.
            
            mov     rcx, returnAdrs
            dec     rcx
skipTo0:    inc     rcx
            cmp     byte ptr [rcx], 0
            jne     skipTo0
            inc     rcx
            mov     returnAdrs, rcx
            
            leave
            vmovdqu ymm0, ymmword ptr [rsp + 000]
            vmovdqu ymm1, ymmword ptr [rsp + 032]
            vmovdqu ymm2, ymmword ptr [rsp + 064]
            vmovdqu ymm3, ymmword ptr [rsp + 096]
            vmovdqu ymm4, ymmword ptr [rsp + 128]
            vmovdqu ymm5, ymmword ptr [rsp + 160]
            vmovdqu ymm6, ymmword ptr [rsp + 192]
            vmovdqu ymm7, ymmword ptr [rsp + 224]
            add     rsp, 256
            pop     r11
            pop     r10
            pop     r9
            pop     r8
            pop     rdx
            pop     rcx
            pop     rbx
            pop     rax
            ret
print_AVX   endp

; Version that will run on CPUs without
; AVX support and will preserve the
<span epub:type="pagebreak" title="700" id="Page_700"/>; volatile SSE registers (XMM0 to XMM3):

print_SSE   proc

; Preserve all the volatile registers
; (be nice to the assembly code that
; calls this procedure):

            push    rax
            push    rbx
            push    rcx
            push    rdx
            push    r8
            push    r9
            push    r10
            push    r11
            
; XMM0 to XMM3 are considered volatile, so preserve them:

            sub     rsp, 128
            movdqu  xmmword ptr [rsp + 00],  xmm0
            movdqu  xmmword ptr [rsp + 16],  xmm1
            movdqu  xmmword ptr [rsp + 32],  xmm2
            movdqu  xmmword ptr [rsp + 48],  xmm3
            movdqu  xmmword ptr [rsp + 64],  xmm4
            movdqu  xmmword ptr [rsp + 80],  xmm5
            movdqu  xmmword ptr [rsp + 96],  xmm6
            movdqu  xmmword ptr [rsp + 112], xmm7
            
            push    rbp

returnAdrs  textequ &lt;[rbp + 200]&gt;

            mov     rbp, rsp
            sub     rsp, 128
            and     rsp, -16
            
; Format string (passed in RCX) is sitting at
; the location pointed at by the return address,
; load that into RCX:

            mov     rcx, returnAdrs
            
; To handle more than 3 arguments (4 counting
; RCX), you must pass data on stack. However, to the
; print caller, the stack is unavailable, so use
; R10 and R11 as extra parameters (could be just
; junk in these registers, but pass them just
; in case):

            mov     [rsp + 32], r10
            mov     [rsp + 40], r11
            call    printf
            
; Need to modify the return address so
<span epub:type="pagebreak" title="701" id="Page_701"/>; that it points beyond the zero-terminating byte.
; Could use a fast strlen function for this, but
; printf is so slow it won't really save us anything.
            
            mov     rcx, returnAdrs
            dec     rcx
skipTo0:    inc     rcx
            cmp     byte ptr [rcx], 0
            jne     skipTo0
            inc     rcx
            mov     returnAdrs, rcx
            
            leave
            movdqu  xmm0, xmmword ptr [rsp + 00] 
            movdqu  xmm1, xmmword ptr [rsp + 16] 
            movdqu  xmm2, xmmword ptr [rsp + 32] 
            movdqu  xmm3, xmmword ptr [rsp + 48] 
            movdqu  xmm4, xmmword ptr [rsp + 64] 
            movdqu  xmm5, xmmword ptr [rsp + 80] 
            movdqu  xmm6, xmmword ptr [rsp + 96] 
            movdqu  xmm7, xmmword ptr [rsp + 112] 
            add     rsp, 128
            pop     r11
            pop     r10
            pop     r9
            pop     r8
            pop     rdx
            pop     rcx
            pop     rbx
            pop     rax
            ret
print_SSE   endp 
            
***************************************************************
            
; Here is the "asmMain" function.
        
            public  asmMain
asmMain     proc
            push    rbx
            push    rsi
            push    rdi
            push    rbp
            mov     rbp, rsp
            sub     rsp, 56         ; Shadow storage

; Trivial example, no arguments:

            call    print
            byte    "Hello, world!", nl, 0
            
; Simple example with integer arguments:

            mov     rdx, 1          ; Argument #1 for printf
            mov     r8, 2           ; Argument #2 for printf
<span epub:type="pagebreak" title="702" id="Page_702"/>            mov     r9, 3           ; Argument #3 for printf
            mov     r10, 4          ; Argument #4 for printf
            mov     r11, 5          ; Argument #5 for printf
            call    print
            byte    "Arg 1=%d, Arg2=%d, Arg3=%d "
            byte    "Arg 4=%d, Arg5=%d", nl, 0
            
; Demonstration of floating-point operands. Note that
; args 1, 2, and 3 must be passed in RDX, R8, and R9.
; You'll have to load parameters 4 and 5 into R10 and R11.

            mov     rdx, qword ptr fp1
            mov     r8,  qword ptr fp2
            mov     r9,  qword ptr fp3
            mov     r10, qword ptr fp4
            mov     r11, qword ptr fp5
            call    print
            byte    "Arg1=%6.1f, Arg2=%6.1f, Arg3=%6.1f "
            byte    "Arg4=%6.1f, Arg5=%6.1f ", nl, 0
                         
allDone:    leave
            pop     rdi
            pop     rsi
            pop     rbx
            ret     ; Returns to caller
asmMain     endp
            end</code></pre>
<p class="CodeListingCaption"><a id="listing11-5">Listing 11-5</a>: Dynamically selected print procedure</p>
<p>Here’s the build command and output for the program in <a href="#listing11-5">Listing 11-5</a>:</p>
<pre><code>C:\&gt;<b>build listing11-5</b>

C:\&gt;<b>echo off</b>
 Assembling: listing11-5.asm
c.cpp

C:\&gt;<b>listing11-5</b>
Calling Listing 11-5:
Hello, World!
Arg 1=1, Arg2=2, Arg3=3 Arg 4=4, Arg5=5
Arg1=   1.0, Arg2=   2.0, Arg3=   3.0 Arg4=   4.0, Arg5=   5.0
Listing 11-5 terminated</code></pre>
<h2 id="h1-501089c11-0023">	11.23	The MASM Include Directive</h2>
<p class="BodyFirst">As you’ve seen already, including the source code for the <code>print</code> procedure in every sample listing in this book wastes a lot of space. Including the new version from the previous section in every listing would be impractical. In <span class="xref" itemid="xref_target_Chapter 15">Chapter 15</span>, I discuss include files, libraries, and other functionality you can use to break large projects into manageable pieces. In the meantime, however, it’s worthwhile to discuss the MASM <code>include</code> directive so this book can eliminate a lot of unnecessary code duplication in sample programs.</p>
<p><span epub:type="pagebreak" title="703" id="Page_703"/>The MASM <code>include</code> directive uses the following syntax:</p>
<pre><code>include  <var>source_filename</var></code></pre>
<p class="BodyContinued">where <var>source_filename</var> is the name of a text file (generally in the same directory of the source file containing this <code>include</code> directive). MASM will take the source file and insert it into the assembly at the point of the <code>include</code> directive, exactly as though the text in that file had appeared in the source file being assembled.</p>
<p>For example, I have extracted all the source code associated with the new print procedure (the <code>choosePrint</code>, <code>print_AVX</code>, and <code>print_SSE</code> procedures, and the <code>print</code> qword variable), and I’ve inserted them into the <em>print.inc</em> source file.<sup class="FootnoteReference"><a id="c11-footnoteref-23" href="#c11-footnote-23">21</a></sup> In listings that follow in this book, I’ll simply place the following directive in the code in place of the print function:</p>
<pre><code>include print.inc</code></pre>
<p>I’ve also put the <code>getTitle</code> procedure into its own header file (<em>getTitle.inc</em>) to be able to remove that common code from sample listings.</p>
<h2 id="h1-501089c11-0024">	11.24	And a Whole Lot More</h2>
<p class="BodyFirst">This chapter doesn’t even begin to describe all the various SSE, AVX, AVX2, and AVX512 instructions. As already mentioned, most of the SIMD instructions have a specific purpose (such as interleaving or deinterleaving bytes associated with video or audio information) that aren’t very useful outside their particular problem domain. Other instructions (at least, as this book was being written) are sufficiently new that they won’t execute on many CPUs in use today. If you’re interested in learning about more of the SIMD instructions, check out the information in the next section.</p>
<h2 id="h1-501089c11-0025">	11.25	For More Information</h2>
<p class="BodyFirst">For more information about the <code>cpuid</code> instruction on AMD CPUs, see the 2010 AMD document “CPUID Specification” (<a href="https://www.amd.com/system/files/TechDocs/25481.pdf" class="LinkURL">https://www.amd.com/system/files/TechDocs/25481.pdf</a>). For Intel CPUs, check out “Intel Architecture and Processor Identification with CPUID Model and Family Numbers” (<a href="https://software.intel.com/en-us/articles/intel-architecture-and-processor-identification-with-cpuid-model-and-family-numbers/" class="LinkURL">https://software.intel.com/en-us/articles/intel-architecture-and-processor-identification-with-cpuid-model-and-family-numbers/</a>).</p>
<p>Microsoft’s website (particularly the Visual Studio documentation) has additional information on the MASM <code>segment</code> directive and x86-64 segments. A search for <em>MASM Segment Directive</em> on the internet, for example, brought up the page <a href="https://docs.microsoft.com/en-us/cpp/assembler/masm/segment?view=msvc-160/" class="LinkURL">https://docs.microsoft.com/en-us/cpp/assembler/masm/segment?view=msvc-160/</a>.</p>
<p>The complete discussion of all the SIMD instructions can be found in Intel’s documentation: <em>Intel® 64 and IA-32 Architectures Software Developer’s Manual, </em>Volume 2:<em> Instruction Set Reference</em>.</p>
<p><span epub:type="pagebreak" title="704" id="Page_704"/>You can easily find this documentation online at Intel’s website; for example:</p>
<ul>
<li><em/><a href="https://software.intel.com/en-us/articles/intel-sdm/" class="LinkURL">https://software.intel.com/en-us/articles/intel-sdm/</a></li>
<li><em/><a href="https://software.intel.com/content/www/us/en/develop/download/intel-64-and-ia-32-architectures-sdm-combined-volumes-1-2a-2b-2c-2d-3a-3b-3c-3d-and-4.html" class="LinkURL">https://software.intel.com/content/www/us/en/develop/download/intel-64-and-ia-32-architectures-sdm-combined-volumes-1-2a-2b-2c-2d-3a-3b-3c-3d-and-4.html</a></li>
</ul>
<p>AMD’s variant can be found at <a href="https://www.amd.com/system/files/TechDocs/40332.pdf" class="LinkURL">https://www.amd.com/system/files/TechDocs/40332.pdf</a>.</p>
<p>Although this chapter has presented many of the SSE/AVX/AVX2 instructions and what they do, it has not spent much time describing how you would use these instructions in a typical program. You can easily find lots of useful high-performance algorithms that use SSE and AVX instructions on the internet. The following URLs provide some examples:</p>
<p class="ListHead"><b>Tutorials on SIMD programming</b></p>
<ul>
<li>SSE Arithmetic, by Stefano Tommesani, <a href="https://tommesani.com/index.php/2010/04/24/sse-arithmetic/" class="LinkURL">https://tommesani.com/index.php/2010/04/24/sse-arithmetic/</a></li>
<li><em/>x86/x64 SIMD Instruction List, <a href="https://www.officedaytime.com/simd512e/" class="LinkURL">https://www.officedaytime.com/simd512e/</a></li>
<li><em/>Basics of SIMD Programming, Sony Computer Entertainment, <a href="http://ftp.cvut.cz/kernel/people/geoff/cell/ps3-linux-docs/CellProgrammingTutorial/BasicsOfSIMDProgramming.html" class="LinkURL">http://ftp.cvut.cz/kernel/people/geoff/cell/ps3-linux-docs/CellProgrammingTutorial/BasicsOfSIMDProgramming.html</a></li>
</ul>
<p class="ListHead"><b>Sorting algorithms</b></p>
<ul>
<li>“A Novel Hybrid Quicksort Algorithm Vectorized Using AVX-512 on Intel Skylake,” by Berenger Bramas, <a href="https://arxiv.org/pdf/1704.08579.pdf" class="LinkURL">https://arxiv.org/pdf/1704.08579.pdf</a></li>
<li>“Register Level Sort Algorithm on Multi-Core SIMD Processors” by Tian Xiaochen et al., <a href="http://olab.is.s.u-tokyo.ac.jp/~kamil.rocki/xiaochen_rocki_IA3_SC13.pdf" class="LinkURL">http://olab.is.s.u-tokyo.ac.jp/~kamil.rocki/xiaochen_rocki_IA3_SC13.pdf</a></li>
<li>“Fast Quicksort Implementation Using AVX Instructions” by Shay Gueron and Vlad Krasnov, <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1009.7773&amp;rep=rep1&amp;type=pdf" class="LinkURL">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1009.7773&amp;rep=rep1&amp;type=pdf</a></li>
</ul>
<p class="ListHead"><b>Search algorithms</b></p>
<ul>
<li>“SIMD-Friendly Algorithms for Substring Searching” by Wojciech Mula, <a href="http://0x80.pl/articles/simd-strfind.html" class="LinkURL">http://0x80.pl/articles/simd-strfind.html</a></li>
<li>“Fast Multiple String Matching Using Streaming SIMD Extensions Technology” by Simone Faro and M. Oğuzhan Külekci, <a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1041.3831&amp;rep=rep1&amp;type=pdf" class="LinkURL">https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1041.3831&amp;rep=rep1&amp;type=pdf</a></li>
<li>“k-Ary Search on Modern Processors” by Benjamin Schlegel et al., <a href="https://event.cwi.nl/damon2009/DaMoN09-KarySearch.pdf" class="LinkURL">https://event.cwi.nl/damon2009/DaMoN09-KarySearch.pdf</a></li>
</ul>
<h2 id="h1-501089c11-0026"><span epub:type="pagebreak" title="705" id="Page_705"/>	11.26	Test Yourself</h2>
<ol class="decimal">
<li value="1">How can you determine whether a particular SSE or AVX feature is available on the CPU?</li>
<li value="2">Why is it important to check the manufacturer of the CPU? </li>
<li value="3">What EAX setting do you use with <code>cpuid</code> to obtain the feature flags? </li>
<li value="4">What feature flag bit tells you that the CPU supports SSE4.2 instructions? </li>
<li value="5">What is the name of the default segment used by the following directives?
<ol class="lower-alpha">
<li value="1"><code>.code</code> </li>
<li value="2"><code>.data</code></li>
<li value="3"><code>.data?</code> </li>
<li value="4"><code>.const</code> </li>
</ol></li>
<li value="6">What is the default segment alignment?</li>
<li value="7">How would you create a data segment aligned on a 64-byte boundary?</li>
<li value="8">Which instruction set extensions support the YMM<em>x</em> registers? </li>
<li value="9">What is a lane?</li>
<li value="10">What is the difference between a scalar instruction and a vector instruction? </li>
<li value="11">SSE memory operands (XMM) must usually be aligned on what memory boundary?</li>
<li value="12">AVX memory operands (YMM) must usually be aligned on what memory boundary? </li>
<li value="13">AVX-512 memory operands (ZMM) must usually be aligned on what memory boundary? </li>
<li value="14">What instruction would you use to move the data from a 32-bit general-purpose integer register into the LO 32 bits of an XMM and a YMM register? </li>
<li value="15">What instruction would you use to move the data from a 64-bit general-purpose integer register into the LO 64 bits of an XMM and a YMM register? </li>
<li value="16">What three instructions would you use to load 16 bytes from an aligned memory location into an XMM register?</li>
<li value="17">What three instructions would you use to load 16 bytes from an arbitrary memory address into an XMM register?</li>
<li value="18">If you want to move the HO 64 bits of an XMM register into the HO 64 bits of another XMM register without affecting the LO 64 bits of the destination, what instruction would you use? </li>
<li value="19"><span epub:type="pagebreak" title="706" id="Page_706"/>If you want to duplicate a double-precision value in the LO 64 bits of an XMM register in the two qwords (LO and HO) of another XMM register, what instruction would you use?</li>
<li value="20">Which instruction would you use to rearrange the bytes in an XMM register? </li>
<li value="21">Which instruction would you use to rearrange the dword lanes in an XMM register?</li>
<li value="22">Which instructions would you use to extract bytes, words, dwords, or qwords from an XMM register and move them into a general-purpose register?</li>
<li value="23">Which instructions would you use to take a byte, word, dword, or qword in a general-purpose register and insert it somewhere in an XMM register?</li>
<li value="24">What does the <code>andnpd</code> instruction do? </li>
<li value="25">Which instruction would you use to shift the bytes in an XMM register one byte position to the left (8 bits)? </li>
<li value="26">Which instruction would you use to shift the bytes in an XMM register one byte position to the right (8 bits)? </li>
<li value="27">If you want to shift the two qwords in an XMM register <em>n</em> bit positions to the left, what instruction would you use?</li>
<li value="28">If you want to shift the two qwords in an XMM register <em>n</em> bit positions to the right, what instruction would you use?</li>
<li value="29">What happens in a <code>paddb</code> instruction when a sum will not fit into 8 bits? </li>
<li value="30">What is the difference between a vertical addition and a horizontal addition? </li>
<li value="31">Where does the <code>pcmpeqb</code> instruction put the result of the comparison? How does it indicate the result is true?</li>
<li value="32">There is no <code>pcmpltq</code> instruction. Explain how to compare lanes in a pair of XMM registers for the less-than condition.</li>
<li value="33">What does the <code>pmovmskb</code> instruction do?</li>
<li value="34">How many simultaneous additions are performed by the following?
<ol class="lower-alpha">
<li value="1"><code>addps</code> </li>
<li value="2"><code>addpd</code> </li>
</ol></li>
<li value="35">If you have a pointer to data in RAX and want to force that address to be aligned on a 16-byte boundary, what instruction would you use?</li>
<li value="36">How can you set all the bits in the XMM0 register to 0? </li>
<li value="37">How can you set all the bits in the XMM1 register to 1?</li>
<li value="38">What directive do you use to insert the content of a source file into the current source file during assembly?</li>
</ol>
<section class="footnotes">
<aside class="FootnoteEntry"><p><sup class="FootnoteReference"><a id="c11-footnote-1" href="#c11-footnoteref-1">1.</a></sup> Yes, MASM uses the same directive, <span class="LiteralFootnote"><code>ends</code></span>, for ending both structures and segments.</p></aside>
<aside class="FootnoteEntry"><p><sup class="FootnoteReference"><a id="c11-footnote-2" href="#c11-footnoteref-2">2.</a></sup> <span class="LiteralFootnote"><code>xmm</code></span><span class="SubscriptLiteral">n</span> represents XMM0 through XMM15.</p></aside>
<aside class="FootnoteEntry"><p><sup class="FootnoteReference"><a id="c11-footnote-3" href="#c11-footnoteref-3">3.</a></sup> <span class="LiteralFootnote"><code>ymm</code></span><span class="SubscriptLiteral">n</span> represents YMM0 through YMM15.</p></aside>
<aside class="FootnoteEntry"><p><sup class="FootnoteReference"><a id="c11-footnote-4" href="#c11-footnoteref-4">4.</a></sup> The <span class="LiteralFootnote"><code>vmovlps</code></span> and <span class="LiteralFootnote"><code>vmovlpd</code></span> instructions will zero-extend to the HO bits of the corresponding YMM register, regardless of what happens in the XMM register.</p></aside>
<aside class="FootnoteEntry"><p><sup class="FootnoteReference"><a id="c11-footnote-5" href="#c11-footnoteref-5">5.</a></sup> This is probably a bug. It may be corrected in later versions of MASM.</p></aside>
<aside class="FootnoteEntry"><p><sup class="FootnoteReference"><a id="c11-footnote-6" href="#c11-footnoteref-6">6.</a></sup> Other than, of course, the issue of zeroing and preserving the HO bits of YMM registers when operating on 128-bit data sets.</p></aside>
<aside class="FootnoteEntry"><p><sup class="FootnoteReference"><a id="c11-footnote-7" href="#c11-footnoteref-7">7.</a></sup> The AVX-512 extensions also allow the use of 512-bit ZMM registers for the <span class="LiteralFootnote"><code>vshufb</code></span> instruction.</p></aside>
<aside class="FootnoteEntry"><p><sup class="FootnoteReference"><a id="c11-footnote-8" href="#c11-footnoteref-8">8.</a></sup> They also allow you to shuffle values in ZMM registers. However, this book is largely ignoring the AVX-512 instruction set extensions. See the Intel and AMD documentation if you are interested in using the 512-bit variants of these instructions.</p></aside>
<aside class="FootnoteEntry"><p><sup class="FootnoteReference"><a id="c11-footnote-9" href="#c11-footnoteref-9">9.</a></sup> Intel and AMD’s documentation swap the second and third operands. This book uses the Intel syntax.</p></aside>
<aside class="FootnoteEntry"><p><sup class="FootnoteReference"><a id="c11-footnote-10" href="#c11-footnoteref-10">10.</a></sup> 8-bit addition is generally sufficient for video, with 16-bit addition certainly sufficient for high-end video encoding. 16-bit saturation is suitable for normal audio, though high-end audio requires 24-bit arithmetic.</p></aside>
<aside class="FootnoteEntry"><p><sup class="FootnoteReference"><a id="c11-footnote-11" href="#c11-footnoteref-11">11.</a></sup> Where the <span class="LiteralFootnote"><code>SSE4_1</code></span> feature flag for the legacy 128-bit version is set. See the Intel documentation for full details.</p></aside>
<aside class="FootnoteEntry"><p><sup class="FootnoteReference"><a id="c11-footnote-12" href="#c11-footnoteref-12">12.</a></sup> <var>xx</var> = <span class="LiteralFootnote"><code>ax</code></span> or <span class="LiteralFootnote"><code>in</code></span>, <var>y</var> = <span class="LiteralFootnote"><code>s</code></span> or <span class="LiteralFootnote"><code>u</code></span>, and <var>z</var> = <span class="LiteralFootnote"><code>b</code></span>, <span class="LiteralFootnote"><code>w</code></span>, <span class="LiteralFootnote"><code>d</code></span>, or <span class="LiteralFootnote"><code>q</code></span>.</p></aside>
<aside class="FootnoteEntry"><p><sup class="FootnoteReference"><a id="c11-footnote-13" href="#c11-footnoteref-13">13.</a></sup> The AVX-512 instruction set actually includes a fourth set of absolute value instructions (<span class="LiteralFootnote"><code>vpvasq</code></span>); see the Intel documentation for more details.</p></aside>
<aside class="FootnoteEntry"><p><sup class="FootnoteReference"><a id="c11-footnote-14" href="#c11-footnoteref-14">14.</a></sup> Qword comparisons are available only on CPUs that support the SSE4.1 instruction set extensions.</p></aside>
<aside class="FootnoteEntry"><p><sup class="FootnoteReference"><a id="c11-footnote-15" href="#c11-footnoteref-15">15.</a></sup> Dyadic operations have two operands; for example, addition is dyadic: x + y. Monadic operations have a single operand; for example, <span class="LiteralFootnote"><code>sqrt(x)</code></span>.</p></aside>

<aside class="FootnoteEntry"><p><sup class="FootnoteReference"><a id="c11-footnote-18" href="#c11-footnoteref-18">16.</a></sup> Logically AND with the value 1Fh for 32-byte alignment.</p></aside>
<aside class="FootnoteEntry"><p><sup class="FootnoteReference"><a id="c11-footnote-19" href="#c11-footnoteref-19">17.</a></sup> One nice feature of the two’s complement numbering system is that negating a power of 2 produces all 1 bits except for the LO log<sub>2</sub>(<span class="LiteralFootnote"><code>pwrOf2</code></span>) bits of the number. For example, –32 has 0s in the LO 5 bits, –16 has 0s in the LO 4 bits, –8 has 0s in the LO 3 bits, –4 has 0s in the LO 2 bits, and –2 has a 0 in the LO bit.</p></aside>
<aside class="FootnoteEntry"><p><sup class="FootnoteReference"><a id="c11-footnote-20" href="#c11-footnoteref-20">18.</a></sup> Suggested by Raymond Chen at <a href="https://blogs.msdn.microsoft.com/oldnewthing/" class="LinkURL">https://blogs.msdn.microsoft.com/oldnewthing/</a>.</p></aside>
<aside class="FootnoteEntry"><p><sup class="FootnoteReference"><a id="c11-footnote-21" href="#c11-footnoteref-21">19.</a></sup> As far as I know, at least while this is being written, there is no convenient way to test a byte in memory to see if it is accessible without causing a fault; in theory, you could put in an exception handler, but triggering and handling the exception is far too expensive to consider.</p></aside>
<aside class="FootnoteEntry"><p><sup class="FootnoteReference"><a id="c11-footnote-22" href="#c11-footnoteref-22">20.</a></sup> You could save the <span class="LiteralFootnote"><code>cpuid</code></span> results and just test the flag, if that is more convenient for you.</p></aside>
<aside class="FootnoteEntry"><p><sup class="FootnoteReference"><a id="c11-footnote-23" href="#c11-footnoteref-23">21.</a></sup> <em>.inc</em> is the typical suffix MASM programmers use for include files.</p></aside>
</section>
</section>
</body></html>