<html><head></head><body><div id="sbo-rt-content"><section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" title="181" id="Page_181"/>8</span><br/>
<span class="ChapterTitle">Training and Testing</span></h1>
</header>
<figure class="opener">
<img src="Images/chapterart.png" alt="" width="206" height="206"/>
</figure>
<p class="ChapterIntro">In this chapter we’ll look at <em>training</em>, the process of taking a system that’s been initialized with default or random values and gradually improving it so that it’s tuned to the data we want to understand. When we’re done training, we can estimate how well our system will evaluate new data it hasn’t seen before, a process known as <em>testing</em>.</p>
<p>We illustrate the ideas in this chapter using a supervised classifier, which we teach with labeled data. Most of the techniques we discuss are general and can be applied to almost all types of learners.</p>
<h2 id="h1-500723c08-0001"><span epub:type="pagebreak" title="182" id="Page_182"/>Training</h2>
<p class="BodyFirst">When we train a classifier with supervised learning, every sample has an associated label describing the class we’ve manually assigned to it. The collection of all the samples we’re going to learn from, along with their labels, is called the <em>training set</em>. We’re going to present each of the samples in our training set to the classifier, one at a time. For each sample, we give the system the sample’s features and ask it to predict its class.</p>
<p>If the prediction is correct (that is, it matches the label we assigned), then we move on to the next sample. If the prediction is wrong, we feed the classifier’s output and the correct label back to the classifier. Using algorithms that we’ll see in later chapters, we modify the classifier’s internal parameters so that it’s more likely to predict the correct label if it sees this sample again.</p>
<p><a href="#figure8-1" id="figureanchor8-1">Figure 8-1</a> shows this idea visually. We use the classifier to get a prediction and compare that to the label. If they disagree, we update the classifier. Then we move on to the next sample.</p>
<figure>
<img src="Images/f08001.png" alt="f08001" width="844" height="443"/>
<figcaption><p><a id="figure8-1">Figure 8-1</a>: A block diagram of training a classifier</p></figcaption>
</figure>
<p>As our training process runs through this loop, one sample at a time, the classifier’s internal variables are nudged toward values that do an increasingly good job of predicting labels. Each time we run through the entire training set, we say that we’ve trained for one <em>epoch</em>. We usually run the system through many epochs so the system sees every sample many times. Typically, we keep training as long as the system is still learning and improving its performance on the training data, but we might stop if we run out of time, or if we run into problems like those we discuss later in this chapter and in Chapter 9.</p>
<p><span epub:type="pagebreak" title="183" id="Page_183"/>Let’s now look at how to measure the classifier’s accuracy at predicting correct labels.</p>
<h2 id="h1-500723c08-0002">Testing the Performance</h2>
<p class="BodyFirst">We begin with a system whose parameters are initialized with random numbers. Then we teach it using the samples in the training data. Once the system has been <em>released</em>, or <em>deployed</em>, into the real world, it encounters new, <em>real-world data</em> (or <em>deployment data</em>, <em>release data</em>, or <em>user data</em>). We’d like to know how well our classifier will perform on real-world data before we deploy it. We may not need perfect accuracy, but we usually want the system to meet or exceed some quality threshold that we already have in mind. How can we estimate the quality of our system’s predictions before it’s released?</p>
<p>We need our system to do really well on the training data, but if we judge the system’s accuracy based on just this data, we’ll usually be misled. This is an important principle in practice, so let’s look at it more detail.</p>
<p>Suppose we’re going to use our supervised classifier to process pictures of dogs. For every image, it will assign a label identifying that dog’s breed. Our goal is to put the system online so people can drag a picture of their dog onto their browser, and have it come back either with the dog’s breed, or the catch-all “mixed breed.”</p>
<p>To train our system, let’s collect 1,000 photos of different purebred dogs, each labeled by an expert. Using <a href="#figure8-1">Figure 8-1</a>, we can show the system all 1,000 pictures, and then we show it all of them again, and again, over and over, one epoch after another. When doing so, we usually scramble the order of the images in each epoch so they don’t always arrive in an identical sequence. If our system is designed well, it gradually starts producing more and more accurate results, until it’s perhaps correctly identifying the breed of the dog in 99 percent of these training pictures. </p>
<p>This does <em>not</em> mean that our system is going to be 99 percent correct when we put it up on the web. The problem is that the system might be exploiting subtle relationships in the training data that aren’t true for data in general. For example, suppose our images of poodles look like <a href="#figure8-2" id="figureanchor8-2">Figure 8-2</a>.</p>
<p>When we assembled our training set, we didn’t notice that all of the poodles had a little bob at the end of their tails and that none of the other dogs did. But the system noticed. That little idiosyncrasy in the data gave the system a way to easily classify poodles: instead of looking at the size of the dog’s legs, the shape of its nose, and other features, the system could just look for the bob on the end of the tail. Using that rule, it would correctly classify all of our training images of poodles. We sometimes say that the system is doing what we asked for (“identify poodles”), but not what we want (“based on most of the features of the dog in the picture, determine if it’s a poodle”). We often say that the system has learned to <em>cheat</em>, though that might be unfair. What it learned was a shortcut that gave us the results we asked for.</p>
<span epub:type="pagebreak" title="184" id="Page_184"/><figure>
<img src="Images/f08002.png" alt="f08002" width="694" height="471"/>
<figcaption><p><a id="figure8-2">Figure 8-2</a>: Training data for a system to identify dog breeds from pictures. Top row: Our input poodle images. Bottom row: The feature that our system learned to identify a photo as a poodle is highlighted in red.</p></figcaption>
</figure>
<p>For another example, suppose that all the pictures of Yorkshire terriers (or Yorkies) in our training data were taken when the dogs were sitting on a couch, as in <a href="#figure8-3" id="figureanchor8-3">Figure 8-3</a>. We hadn’t noticed this, nor another important fact: none of the other pictures had couches in them. The system may learn that if there’s a couch in the image, it can immediately classify the image as a picture of a Yorkshire terrier. This rule works perfectly for our training data.</p>
<figure>
<img src="Images/f08003.png" alt="f08003" width="694" height="267"/>
<figcaption><p><a id="figure8-3">Figure 8-3</a>: Top row: Three Yorkies on couches. Bottom row: Our system has learned to recognize the couch, shown in red.</p></figcaption>
</figure>
<p>Suppose we then deploy our system and someone submits a picture of a Great Dane standing in front of a holiday decoration of big white balls on a string, or their Siberian husky on a couch, as in <a href="#figure8-4" id="figureanchor8-4">Figure 8-4</a>. Our system notices the white ball at the end of the Great Dane’s tail and says that it’s a <span epub:type="pagebreak" title="185" id="Page_185"/>poodle, and it sees the couch, ignores the dog, and reports that the husky is a Yorkie.</p>
<figure>
<img src="Images/f08004.png" alt="f08004" width="694" height="481"/>
<figcaption><p><a id="figure8-4">Figure 8-4</a>: Top: A Great Dane is standing in front of a holiday display of white balls on a string, and a Siberian husky is lying on a couch. Bottom: The system sees the white ball on the end of the Great Dane’s tail and tells us that the dog is a poodle, and it notices the couch and classifies the dog on it as a Yorkie.</p></figcaption>
</figure>
<p>This isn’t just a theoretical concern. A famous example of this phenomenon describes a meeting in the 1960s where a presenter was demonstrating an early machine-learning system (Muehlhauser 2011). The details of the data are murky, but it seems that they had photos of stands of trees with a camouflaged tank in their midst, and stands of trees with no tank. The presenter claimed the system could pick out the image with the tank without fail. That would have been an incredible feat for the time.</p>
<p>At the end of the talk, an audience member stood up and observed that the photos with the tanks in them were all taken on sunny days, whereas the photos without the tanks were all taken on cloudy days. It seemed likely that the system had merely distinguished bright skies from dark skies, so the impressive (and accurate) results had nothing to do with tanks at all.</p>
<p>This is why looking at the performance on the training data isn’t good enough to predict performance in the real world. The system might learn about some weird idiosyncrasies in the training data and then use that as a rule, only to be foiled by new data that doesn’t happen to have those quirks. This is known formally as <em>overfitting</em>, though we often refer casually to it simply as <em>cheating.</em> We’ll look at overfitting more closely in Chapter 9.</p>
<p>We’ve seen that we need some measure other than performance on the training set to predict how well our system is going to do if we deploy it. It <span epub:type="pagebreak" title="186" id="Page_186"/>would be great if there was an algorithm or formula that would take our trained classifier and tell us how good it is, but there isn’t. There’s no way for us to know how our system will perform without trying it out and seeing. Like natural scientists who must run experiments to see what actually happens in the real world, we also must run experiments to see how well our systems perform.</p>
<h3 id="h2-500723c08-0001">Test Data</h3>
<p class="BodyFirst">The best way anyone has found to determine how well a system will do on new, unseen data is to give it new, unseen data and see how well it does. There’s no shortcut to this kind of experimental verification.</p>
<p>We call this unseen set of data points, or samples, the <em>test data</em> or a <em>test set</em>. Like the training data, we hope that the test data is representative of the data that we’re going to see once our system has been released. The typical process is to train the system using the training data until it’s doing as well as we think it can do. Then we evaluate it on the test data, and that tells us how well it’s likely to do in the real world.</p>
<p>If the system’s performance on the test data isn’t good enough, we need to improve it. Since training on more data is almost always a good way to improve performance, it’s a usually good idea to gather more data and train again. Another benefit of getting more data is that we can diversify our training set. For example, we might find dogs other than poodles with bobs on their tail, or we might find dogs other than Yorkies on couches. Then our classifier would have to find other ways to identify those dogs, and we’d avoid making mistakes due to overfitting.</p>
<p>An <em>essential</em> rule of the training and testing process is that <em>we never learn from the test data</em>. As tempting as it might be for us to put the test data into the training set so that the system has even more examples to learn from, doing so ruins the test data’s value as an objective way to measure the accuracy of our system. The problem with learning from the test data is that it just becomes part of the training set. That means we’re right back where we were before: the system can all too easily key in on idiosyncrasies in the test data. If we then use the test data to see how well the classifier works, it might predict the correct label for each sample, but it could be cheating. If we learn from the test data, it loses its special and valuable quality as a way to measure the performance of the system on new data.</p>
<p>For this reason, we split the test data off from the training data before we even begin to train, and we hold it aside. We only come back to the test data when training is over, and then we use it just one time to evaluate the quality of our system. If the system doesn’t do well enough on the test set, we can’t just train some more and then test again. Think of the test set as being like the final exam questions in a class: once they’ve been seen, they can’t be used again. If our system doesn’t perform well on the test data, we must start all over again with a system initialized with random values. Then we can train with more data, or for a longer time period. When training is done, we can use the test set again, because this newly trained system has never seen it before. If it again doesn’t perform well enough, we must start our training all over. </p>
<p><span epub:type="pagebreak" title="187" id="Page_187"/>This is important enough to repeat: we must never let the system see the test data in any way prior to its single use when training has completed.</p>
<p>The problem of accidentally learning from the test data has its own name: <em>data leakage</em>, also called <em>data contamination</em>, or <em>contaminated data</em>. We have to constantly look out for this, because as our training procedures and classifiers become more sophisticated, data leakage can sneak in wearing different (and hard-to-notice) disguises. Data leakage can be avoided by practicing <em>data hygiene</em>: always make sure the test data is kept separate and that it is only used once, when training has been completed.  </p>
<p>We often create the test data by splitting our original data collection into two pieces: the training set and the test set. We commonly set up this split to give about 75 percent of the samples to the training set. Often samples are chosen randomly for each set, but more sophisticated algorithms can try to make sure that each collection is a good approximation of the complete input data. Most machine-learning libraries offer routines to perform this splitting process for us.</p>
<p><a href="#figure8-5" id="figureanchor8-5">Figure 8-5</a> shows the idea.</p>
<figure>
<img src="Images/f08005.png" alt="f08005" width="364" height="242"/>
<figcaption><p><a id="figure8-5">Figure 8-5</a>: Splitting our input examples into a training set and a test set. The split is often about 75:25 or 70:30.</p></figcaption>
</figure>
<h3 id="h2-500723c08-0002">Validation Data</h3>
<p class="BodyFirst">In our discussion up to this point, we trained the system for a while, then we stopped and evaluated its performance using the test set. If the performance wasn’t good enough, we started training all over again.</p>
<p>There’s nothing wrong with that strategy except that it is a slow way to work. In practice, we often want a rough estimate of the system’s performance as we go along, so we can stop training when we think the system is going to give us the performance we want from the test set.</p>
<p>To make this estimate, we split the input data into three sets, rather than the two we’ve seen so far. We call this new set the <em>validation data</em>, or <em>validation set</em>. The validation data is yet another chunk of data that’s meant to be a good proxy of the real-world data we’ll see when we deploy the system. We typically make these three sets by assigning about 60 percent of the original data to the training set, 20 percent to the validation set, and the remaining 20 percent to the test set. <a href="#figure8-6" id="figureanchor8-6">Figure 8-6</a> shows the idea.</p>
<span epub:type="pagebreak" title="188" id="Page_188"/><figure>
<img src="Images/f08006.png" alt="f08006" width="402" height="243"/>
<figcaption><p><a id="figure8-6">Figure 8-6</a>: Splitting our input data into a training set, a validation set, and a test set</p></figcaption>
</figure>
<p>Our new process will be to train the system for an epoch, running through the entire training set, and then we estimate its performance by asking it to make predictions for the validation set. We do this after every epoch, so we’re reusing the validation set. This causes data leakage, but we only use the validation data for informal estimates. We use the system’s performance on the validation set to get a general sense of how well it’s learning over time. When we think the system is doing well enough to deploy, we use the one-time test set to get a reliable performance estimate.</p>
<p>The validation set is also helpful when we use automated search techniques to try out many values of hyperparameters. Recall that hyperparameters are variables that we set before we run our system to control how it operates, such as how much it should update its internal values after an error, or even how complex our classifier should be. For each variation, we train on the training set and evaluate the system’s performance on the validation set. As we mentioned, we don’t learn from the validation set, but we do use it repeatedly. The results from the validation set are just an estimate of how well the system is doing, so we can decide when to stop training. When we think that performance is up to par, we break out the test set and use it once in order to get a reliable estimate of the system’s accuracy.</p>
<p>This gives us a convenient way to repeatedly try out different hyperparameters and then choose the best ones based on how they do on the validation set. </p>
<p>This approach to trying out different sets of hyperparameters is based on running a loop. Let’s look at a simplified version of that loop now. </p>
<p>To run our loop, we select a set of hyperparameters, train our system, and then evaluate its performance with the validation set. This estimates how well the system trained with those hyperparameters predicts new data. Next, we set that system aside and create a new system, initialized, as always, with random values. We apply the next set of hyperparameters, train, and use the validation set to evaluate this system’s performance. We repeat this process over and over, once for each set of hyperparameters. When we’ve run through all sets of hyperparameters, we select the system that seemed to provide the most accurate results, run the test set through it, and discover how good its predictions really are.</p>
<p><a href="#figure8-7" id="figureanchor8-7">Figure 8-7</a> shows this whole process graphically.</p>
<span epub:type="pagebreak" title="189" id="Page_189"/><figure>
<img src="Images/f08007.png" alt="f08007" width="517" height="979"/>
<figcaption><p><a id="figure8-7">Figure 8-7</a>: We use the validation set when we try out lots of different hyperparameter sets. Note that we still keep a separate test set, which we use just before deployment.</p></figcaption>
</figure>
<p>When the loop is done, we may be tempted to use the results from the validation data as our final evaluation of the system. After all, the classifier didn’t learn from that data, since it was only used for testing. It may seem <span epub:type="pagebreak" title="190" id="Page_190"/>that we can save ourselves the trouble of making a separate test set and then run it through the system to get a performance estimate.</p>
<p>But that would be working with leaked data, which would distort our conclusions. The source of this leakage is a bit sneaky and subtle, like many data contamination issues. The problem is that although the classifier didn’t learn from the validation data, our whole training and evaluation system did, because it used that data to pick the best hyperparameters for the classifier. In other words, even though the classifier didn’t explicitly learn from the validation data, that data influenced our choice of classifier. We chose a classifier that did best on the validation data, so we <em>already know</em> that it’s going to do a good job with it. In other words, our knowledge of the classifier’s performance on the validation data “leaked” into our selection process.</p>
<p>If this seems subtle or tricky, it is. This sort of thing is easy to overlook or miss, which is why we have to be vigilant about data contamination. Otherwise we risk thinking our system is better than it really is, and thus we deploy a system that isn’t good enough for our intended use. To get a good estimate for how our system performs on brand-new data that it has never seen before, there’s no shortcut: we need to test it on brand-new data that it has never seen before. That’s why we always save the test set for the very end.</p>
<h2 id="h1-500723c08-0003">Cross-Validation</h2>
<p class="BodyFirst">In the last section, we took almost half our training data and set it aside for validation and testing. That’s fine when we have lots and lots of data. But what if our sample set is small and we can’t get more data? Maybe we’re working with photos of Pluto and its moons taken by the New Horizons spacecraft during its 2015 flyby, and we want to build a classifier we can install on future spacecraft to identify what kind of terrain they’re looking at. Our dataset is limited and it’s not going to get bigger: there are no new close-up photos of Pluto coming anytime soon. Every image that we have is precious, and we want to learn all we can from every photo we have. Setting some images aside just to determine how good our classifier is would be a huge price to pay.</p>
<p>If we’re willing to accept an estimate of the system’s performance, rather than a reliable measure of it, then we don’t have to set aside a test set. We can indeed train on every piece of input data and still predict our performance on new data. The catch is that we’re only going to get back an estimate of the system’s accuracy, so it won’t be as reliable a measure as we’d get by using a real test set, but when samples are precious, that tradeoff can be worth it.</p>
<p>The technique that does this job is called <em>cross-validation</em> or <em>rotation validation</em>. There are different types of cross-validation algorithms, but they all share the same basic structure (Schneider 1997). We’re going to look at a version that doesn’t require us to create a dedicated test set. </p>
<p>The core idea is that we can run a loop that repeatedly trains the same system from scratch and then tests it. Each time through we’ll split the <span epub:type="pagebreak" title="191" id="Page_191"/>entire input data into a one-time training set and a one-time validation set. The key thing is that we’ll construct these sets differently each time through the loop. This lets us use all of our data for training (though, as we’ll see, not all at the same time).</p>
<p>We begin by building a fresh instance of our classifier. We split the input data into a temporary training set and temporary validation set. We train our system on the temporary training set and evaluate it with the temporary test set. This gives us a score for the classifier’s performance. Now we go through the loop again, but this time, we split the training data into different temporary training and test sets. When we’ve done this for every iteration through the loop, the average of all the scores is our estimate for the overall performance of our classifier.</p>
<p>A visual summary of cross-validation is shown in <a href="#figure8-8" id="figureanchor8-8">Figure 8-8</a>.</p>
<figure>
<img src="Images/f08008.png" alt="f08008" width="555" height="755"/>
<figcaption><p><a id="figure8-8">Figure 8-8</a>: Using cross-validation to evaluate our system’s performance</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="192" id="Page_192"/>By using cross-validation, we get to train with all of our training data (though not all of it on every pass through the loop), yet we still get an objective measurement of the system’s quality from a held-out test set. This algorithm doesn’t have data leakage issues because each time through the loop, we create a new classifier, and the temporary test set for that classifier contains data that is brand-new and unseen with respect to <em>that specific</em> classifier, so it’s fair to use it to evaluate that classifier’s performance. The penalty for this technique is that our final estimate of the system’s accuracy is not as reliable as what we’d get from a held-out test set. </p>
<p>A variety of different algorithms are available for constructing the temporary training and validation sets. Let’s look at a popular approach.</p>
<h2 id="h1-500723c08-0004">k-Fold Cross-Validation</h2>
<p class="BodyFirst">Perhaps the most popular way to build the temporary datasets for cross-validation is called <em>k-fold cross-validation</em>. The letter <em>k </em>here is not the first letter of a word. Instead, it stands for an integer (for example, we might run “2-fold cross-validation” or “5-fold cross-validation”). Typically, the value of <em>k </em>is the number of times we want to go through the loop of <a href="#figure8-8">Figure 8-8</a>.</p>
<p>The algorithm starts before the cross-validation loop begins. We take our training data and split it up into a series of equal-sized groups. Every sample is placed into exactly one group, and all groups are the same size (well, we allow one smaller group at the end if we can’t split the input into equal-sized pieces).</p>
<p>It would have been great if these groups were each called something like “group,” or “equal-sized piece,” but the word that’s come to describe this idea is <em>fold.</em> This word is used here in an unusual sense to mean the section of a page <em>between </em>creases (or ends). To picture this, imagine writing all the samples in the training set on a long piece of paper and then folding that up into a fixed number of equal pieces. Each time we bend the paper we’re making a crease, and the material between the creases is called a fold. <a href="#figure8-9" id="figureanchor8-9">Figure 8-9</a> shows the idea.</p>
<figure>
<img src="Images/f08009.png" alt="f08009" width="700" height="281"/>
<figcaption><p><a id="figure8-9">Figure 8-9</a>: Creating the folds for <em>k</em>-fold cross-validation. Here we have four creases and five folds.</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="193" id="Page_193"/>Let’s build equal-sized folds from our training data. We can flatten out <a href="#figure8-9">Figure 8-9</a> to create the more typical picture of five folds, shown in <a href="#figure8-10" id="figureanchor8-10">Figure 8-10</a>.</p>
<figure>
<img src="Images/f08010.png" alt="f08010" width="365" height="426"/>
<figcaption><p><a id="figure8-10">Figure 8-10</a>: Splitting our training set into five equally sized folds, named Fold 1 through Fold 5</p></figcaption>
</figure>
<p>Let’s use these five folds to see how the loop proceeds. The first time through the loop, we treat the samples in folds 2 through 5 as our temporary training set, and the samples in fold 1 as our temporary test set. That is, we train the classifier with the samples in folds 2 through 5 and then evaluate it with the samples in fold 1.</p>
<p>The next time through the loop, starting with a fresh classifier initialized with random numbers, we use the samples in folds 1, 3, 4, and 5 for our temporary training set, and the samples in fold 2 for our temporary test set. We train and test as usual with these two sets, and continue with the remaining folds. <a href="#figure8-11" id="figureanchor8-11">Figure 8-11</a> shows the idea visually.</p>
<figure>
<img src="Images/f08011.png" alt="f08011" width="838" height="202"/>
<figcaption><p><a id="figure8-11">Figure 8-11</a>: In each pass through the loop, we choose one fold for testing (in blue) and use the others (in red) for training. If we go through the loop more than five times, we repeat the pattern.</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="194" id="Page_194"/>We can choose to repeat the loop as many times as we like, just repeating the cycle of fold selections (or mixing up the data so the sets always have different contents).</p>
<p>In an optional final step, we can train a fresh classifier with all of our data. This means we can’t get an estimate for its performance. But if we watch the training carefully, and look out for overfitting (discussed in the next chapter), we can usually assume that the system that was trained on all the data is at least as good as the worst performance we obtained from cross-validation (and we hope it will be at least a little bit better).</p>
<p>This makes cross-validation a great option when our data is limited. We do have to repeat the train-test cycle many times, and our final performance measure is only an estimate, both of which are downsides, but we gain the ability to train with all of our data, squeezing every little bit of information out of our input set and using it to make our classifier better.</p>
<p>We discussed <em>k</em>-fold cross-validation with a classifier, but the algorithm is broadly applicable to almost any kind of learner. </p>
<h2 id="h1-500723c08-0005">Summary</h2>
<p class="BodyFirst">This chapter was all about training a deep learning system and deciding if it’s working well enough to be deployed. We focused on a classifier, but the general thinking holds for any such system. We saw that we split the data into two pieces: a training set and a test set. We learned about the problems of overfitting and data leakage, and we also saw how we can use a validation set to get a rough idea of how well the system is learning after every epoch. Finally, we looked at cross validation, a technique typically used with small dataset, to estimate a system’s performance.</p>
<p>In the next chapter, we’ll take a closer look at overfitting and underfitting.</p>
</section>
</div></body></html>