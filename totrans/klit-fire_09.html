<html><head></head><body>
<section>&#13;
<header>&#13;
<h1 class="chapter">&#13;
<span class="ChapterNumber"><span epub:type="pagebreak" title="181" id="Page_181"/>9</span><br/>&#13;
<span class="ChapterTitle">How to Finish</span>&#13;
</h1>&#13;
</header>&#13;
<p class="ChapterIntro"><span class="DropCap">W</span>hen I was working for the United Nations (UN), my boss at the time would regularly start conversations with the words “When the website is done . . .” to which I would respond, “The website is never done.” I count among my accomplishments the fact that by the time I left the UN, people had bought in to the agile, iterative process that treats software as a living thing that needs to be constantly maintained and improved. They stopped saying “When the website is done . . .”</p>&#13;
<p>Technology is never done, but modernization projects can be. This chapter covers how to define success in a way that makes it clear when the modernization effort is completed and what to do next. In the beginning of a project, what success looks like can seem obvious, but often some people in an organization make assumptions about what success means that are different from the assumptions of others. Getting everyone on the same page and keeping everyone on the same page is critical to ensuring that the project crosses the finish line.</p>&#13;
<h2 id="h1-501188c09-0001"><span epub:type="pagebreak" title="182" id="Page_182"/>Revealing Assumptions</h2>&#13;
<p class="BodyFirst">A fair number of broken systems end up that way because the units of the organization involved in the implementation saw their roles and how they contributed to the larger picture differently from one another. Any kind of modernization, rearchitecting, or rethinking of an existing technical system is a long game. Work will stretch on for months or years. For that reason, it is essential when you work on these projects that everyone on the team is able to answer this question: How do we know if it’s getting better?</p>&#13;
<p>The team should know the long-term answer to that, but they should also know what better looks like within days or weeks from where they are now. This kind of work is a slog. To do it well, you have to maintain your own resilience. You have to be mindful of how people behave when they think a project is going poorly. The only thing harder than managing your own doubts is dealing with sabotage from colleagues who don’t understand how much progress is being made because their expectations of what improvement will look like is different from other members of the team.</p>&#13;
<h2 id="h1-501188c09-0002">Approach 1: Success Criteria</h2>&#13;
<p class="BodyFirst">Success doesn’t happen quickly or all at once, which begs the question: How do you know whether a project is moving in the right direction? When I set success criteria with my teams (and usually my boss or other significant stakeholders), we first determine the time frame for evaluation. If two people agree that shipping a feature is an indication of success, they can still come into conflict if they disagree on the timeline. Shipping something in three months is not the same thing as shipping it in a week. For the person who believes the feature should be shipped in a week, a deploy much later feels like a warning sign. For the person who believes it should be shipped in a year, the same deploy is validation.</p>&#13;
<p>If you are familiar with <em>objectives and key results (OKRs),</em> success criteria can take on a similar shape. First, you define your goal, and then, you <span epub:type="pagebreak" title="183" id="Page_183"/>define how you’ll know that you’ve reached your goal. Except, OKRs usually focus on signs that the goal is completed, and success criteria should focus on signs that you’re heading in the right direction.</p>&#13;
<p>The value of this strategy is that the criteria chosen indicates the approach that will be taken without anyone arguing about the approach. If the success criteria are all about implementing new features, it’s unlikely the team is going to prioritize resolving any technical debt. If the success criteria are instead about decreasing the number of errors or speeding up minimum time to recover, the team has to focus on improving existing code. Whenever you can avoid having people argue about principles, philosophies, and other generalities of good engineering versus bad engineering, take advantage of it.</p>&#13;
<p>The same goal can have wildly different success criteria depending on the team’s model. For example, a consulting model would focus on the client’s ability to absorb and adopt process and best practices, not deploys. Consultants don’t have much control over deploys, and the only way they get control is by not being consultants anymore. As software engineers, it is easy to fall into the trap of thinking that effective work will always look like writing code, but sometimes you get much closer to your desired outcome by teaching others to do the job rather than doing it yourself.</p>&#13;
<h3 id="h2-501188c09-0001">Example: Adding Continuous Integration/Continuous Deploy</h3>&#13;
<ol class="none">&#13;
<li>Goal: Move service on to its own deploy pipeline.</li>&#13;
<li>Timeline: One quarter.</li>&#13;
<li>Success Criteria:</li>&#13;
</ol>&#13;
<ul>&#13;
<li>Time to deploy drops by 20 percent.</li>&#13;
<li>Any single person on the Service team can initiate and manage a deploy.</li>&#13;
<li>Number of deploys in a week goes up.</li>&#13;
</ul>&#13;
<h2 id="h1-501188c09-0003"><span epub:type="pagebreak" title="184" id="Page_184"/>Approach 2: Diagnosis-Policy-Actions</h2>&#13;
<p class="BodyFirst">Developed by Richard Rumelt, this approach draws on the same information as success criteria but frames it a bit differently.<sup class="FootnoteReference"><a id="c09-footnoteref-1" href="#c09-footnote-1">1</a></sup> Information is represented in three segments: diagnosis, policy, and actions. <em>Diagnosis</em> is a definition of a problem being fixed. <em>Policy</em> refers to the boundaries of potential solutions—the rules about what the solution should involve doing or shouldn’t involve doing. Finally, <em>actions</em> are the steps the team will take to solve the problem without violating their policy.</p>&#13;
<p>What’s useful about Rumelt’s approach is that it is more focused on what you’re going to do and how you’re going to do it. This might be better in situations when there is little consensus around what success should look like and no single authority to make that decision, but you may find this approach more difficult if your team struggles with road maps. The challenges of legacy modernization can be varied, intertwined, and politically complex. Teams might not agree on what success looks like, and they might also disagree on which tasks need to be executed to untangle problems. Rumelt’s approach is better for situations when it is easier to reach agreement on the steps forward and their order, but harder to reach consensus on what signs of improvement will look like.</p>&#13;
<h3 id="h2-501188c09-0002">Example: Upgrading a Database</h3>&#13;
<ol class="none">&#13;
<li>Diagnosis: Our database software is several versions out of date. The vendor won’t provide support anymore.</li>&#13;
<li>Policy: We want to use blue-green deploys. We will never just turn one database version off and the other on all at once.</li>&#13;
<li><span epub:type="pagebreak" title="185" id="Page_185"/>Actions:</li>&#13;
</ol>&#13;
<ul>&#13;
<li>We will back up our data before each upgrade.</li>&#13;
<li>We will upgrade to 3.2 on this date.</li>&#13;
<li>We will update to 3.3 on this date.</li>&#13;
</ul>&#13;
<h2 id="h1-501188c09-0004">Comparison</h2>&#13;
<p class="BodyFirst">In terms of defining success, success criteria and diagnosis-policy-actions have different strengths and weaknesses. Success criteria connects modernization activities more directly to the value add they can demonstrate. It affords more flexibility in exactly what the team does by not prescribing a specific approach or set of tasks. It is an excellent exercise to run with bosses and any other oversight forces that might be inclined to micromanage a team. How to do something should be the decision of the people actually entrusted to do it. For that reason, the diagnosis-policy-actions approach is too detail oriented to help a team manage up. If the set of actions needs to be changed later, the team might be reluctant to do it and seem inconsistent in front of senior leadership. Discretion is so critical to success; don’t forfeit your team’s right to it by presenting implementation details for feedback if that’s not requested. What leadership needs to know is what outcomes you’re pushing for.</p>&#13;
<p>On the other hand, sometimes you know what better looks like but have no idea which set of actions will get you there. Research and experimentation might fail to signal a clear winner among competing road maps. One side of an organization might say this one thing needs to be fixed first, while another side might argue a completely different fix takes priority. If the situation can be resolved only by executive decision, the diagnosis-policy-action approach is a better fit. The same flexibility that makes success criteria effective at adapting to new information and changing tactics will create confusion when the team is undecided about the day-to-day work.</p>&#13;
<h2 id="h1-501188c09-0005"><span epub:type="pagebreak" title="186" id="Page_186"/>Marking Time</h2>&#13;
<p class="BodyFirst">Defining what success looks like helps keep people on the same page, but since the success criteria and diagnosis-policy-actions approaches cut challenges into smaller accomplishments, you also need to stop people from losing faith in the significance of those small accomplishments. If the team feels like what they’ve succeeded in doing was not worth the time they invested in it, the effectiveness of your definition of success will be diminished.</p>&#13;
<p>What’s to your advantage here is that the perception of time is just as variable as perception of risk. Finding a way to mark time is about finding a way to pull people out of day-to-day frustrations that slow down time and help them focus on the larger picture. My favorite way of marking time is bullet journaling. I have a book where every day I write down five things I am going to work on and how long I think they will take. Throughout the day, I check off those tasks as I complete them and jot down little notes with significant details. During slow periods, I often doodle in the margins or decorate pages with stickers I’ve gotten from vendors.</p>&#13;
<p>Whenever I flip two or three weeks back in my bullet journal, I am shocked by how much has changed. I’ve gotten so much more done than I realized. The tasks I’ve completed feel like months’ worth of work. Sometimes I am shocked when looking only one week back in time.</p>&#13;
<p>Just as humans are terrible judges of probability, we’re also terrible judges of time. What feels like ages might only be a few days. By marking time, we can realign our emotional perception of how things are going. Find some way to record what you worked on and when so the team can easily go back and get a full picture of how far they’ve come.</p>&#13;
<p>You might be tempted to say, “Oh, well, they can do that with our project management system.” Except, project management tools are geared toward presenting one stream of work at a time. Marking time is more effective because of the more complete a picture it paints of one specific <span epub:type="pagebreak" title="187" id="Page_187"/>point in people’s lives. Knowing that such-and-such ticket was closed on a certain day doesn’t necessarily take me back to that moment. Sometimes a date is just a date. When you mark time, do so in a way that evokes memory, that highlights the feeling of distance between that moment and where the team is now.</p>&#13;
<p>Bullet journaling is effective for me because each page is a snapshot of everything that is on my mind at the time. I record work projects, personal projects, events and social activities, holidays, and illnesses. Anything that I expect to take up a large part of the day, I write down. Looking back on a project’s progress with that information gives it a sense of context that I hadn’t considered before. Once I consider it, I realize that I have not been standing in one place mindlessly banging my head against a wall. Bit by bit, piece by piece, I have made things better.</p>&#13;
<h2 id="h1-501188c09-0006">Postmortems on Success</h2>&#13;
<p class="BodyFirst">For software engineers, a <em>postmortem</em> is a review done after an outage that explores the timeline of the failure, contributing factors, and the ultimate resolution in detail. We typically tack on the prefix “blameless” when referring to postmortems to emphasize that the purpose of doing a postmortem is to understand what went wrong, not to assign blame to a particular group or individual who made a mistake.</p>&#13;
<p>But postmortems are not specific to failure. If your modernization plan includes modeling your approach after another successful project, consider doing a postmortem on that project’s success instead. Remember that we tend to think of failure as bad luck and success as skill. We do postmortems on failure because we’re likely to see them as complex scenarios with a variety of contributing factors. We assume that success happens for simple, straightforward reasons.</p>&#13;
<p>In reality, success is no more or less complex than failure. You should use the same methodology to learn from success that you use to learn from failure.</p>&#13;
<h3 id="h2-501188c09-0003"><span epub:type="pagebreak" title="188" id="Page_188"/>Postmortem vs. Retrospective</h3>&#13;
<p class="BodyFirst">Right now you might be thinking to yourself that your organization does do postmortems on success, you just call them <em>retrospectives</em>.</p>&#13;
<p>And, that’s true. The post-sprint or post-launch retrospective does ask many of the same questions as a postmortem. However, I have yet to work with a technical organization that treated retrospectives and postmortems the same. In practice, retrospectives are much more informal. They do not generate reports for others to read—at least, not that I’ve ever seen. I do not know of any organizations that post their retrospectives online for the public to read. I’ve been in a lot of retrospectives where we’ve captured great information and had deep soul-searching conversations, but rarely have I seen that output leave the whiteboard and be shared with other teams.</p>&#13;
<p>As an industry, we <em>reflect</em> on success but <em>study</em> failure. Sometimes obsessively. I’m suggesting that if you’re modeling your project after another team or another organization’s success, you should devote some time to actually researching how that success came to be in the first place.</p>&#13;
<h3 id="h2-501188c09-0004">Running Postmortems</h3>&#13;
<p class="BodyFirst">Before discussing writing postmortems on success, it might be useful to give an overview of how postmortems are run in technical organizations in general. The conventional postmortem format has a couple characteristics that are impractical outside incident response. For example, an outage happens quickly and ideally is resolved within hours, so creating a detailed timeline of events for that is an easier task than it would be for a project that has run for months.</p>&#13;
<p>A traditional postmortem describes the impact of the outage. The team discusses and documents what went well, what went poorly, and where people felt they got lucky. As mentioned, the postmortem often includes a detailed timeline of events around the incident response. These timelines break down what happened, when it happened, and who did what. Postmortems also do not usually reference people by name to protect them from <span epub:type="pagebreak" title="189" id="Page_189"/>blame. Instead, monikers like “SRE 1” or “Software Engineer 2” are used to identify individuals and the actions they took. The postmortem concludes with actionable steps for improvement. How can the organization improve the things that went poorly or build on the things that went well?</p>&#13;
<p>Postmortems are written by reviewing communications and interviewing team members. Then a final review meeting is held to present and verify the gathered information with the full team.</p>&#13;
<p>When running a postmortem on success, you have to weigh the investment of time and energy it will take to get that level of detail when the timeline stretches out over months instead of hours. Such efforts can become cumbersome and bureaucratic fast. Traditional postmortems are written by the software engineers who responded to the incident. These are people you want working on software, not writing reports.</p>&#13;
<p>For that reason, postmortems on success should be run like their lighter-weight cousin the retrospective, but documented with the philosophy of traditional postmortems. The value of the postmortem is not its level of detail, but the role it plays in knowledge sharing. Postmortems are about helping people not involved in the incident avoid the same mistakes. The best postmortems are also distributed outside the organization to cultivate trust through transparency.</p>&#13;
<p>Postmortems establish a record about what really happened and how specific actions affected the outcome. They do not document failure; they provide context. Postmortems on success should serve a similar purpose. Why was a specific approach or technique successful? Did the final strategy look like what the team had planned at the start? Your timeline in a postmortem for success should be built around these questions: How did the organization execute on the original strategy, how did the strategy change, when did those changes happen, and what triggered them?</p>&#13;
<p>Even the biggest successes have challenges that could have gone better and places where good fortune saved the day. Documenting those helps people evaluate the suitability of your approach for their own problems and ultimately reproduce your success.</p>&#13;
<p><span epub:type="pagebreak" title="190" id="Page_190"/>If you’re convinced the strategy from another organization will work for your issue, don’t wait for those engineers to start doing postmortems. You can gather most of the information you need by taking people out for coffee. If there’s an organization whose success you want to copy, spend a couple weeks interviewing people about their strategy using the postmortem’s key questions.</p>&#13;
<ol class="none">&#13;
<li>What went well?</li>&#13;
<li>What could have gone better?</li>&#13;
<li>Where did you get lucky?</li>&#13;
</ol>&#13;
<h2 id="h1-501188c09-0007">The Tale of Two War Rooms</h2>&#13;
<p class="BodyFirst">I examine the contributing factors to success before finalizing a strategy, because I’ve seen organizations learn the wrong lessons from success. For example, an organization I was working with had a project that was hopelessly behind schedule. Not only did team members not have a realistic end date for even the smallest part of the project, they did not know why the project kept getting delayed in the first place. It was a large project that required a couple different organizational units to work together and share information. My consulting team was just rolling off a successful project that had faced a similar challenge. The organization had heard that on our project, we had arranged for representatives from each organizational unit to work out of the same room. For several months, instead of reporting to their offices or cubicles every day, this group sat down together in a large conference room with their laptops. This was a war room, not a meeting. Over time, the conference room looked more like an open plan co-working space.</p>&#13;
<p>The organization with the failing project decided to copy our strategy without talking to us or learning much about how this war room worked. A large conference room was booked for a month, and representatives <span epub:type="pagebreak" title="191" id="Page_191"/>from each affected operational unit were pulled in and told they had to work in that conference room together.</p>&#13;
<p>The organization didn’t see the same results that we had, and eventually leadership representatives reached out and asked us why. Having neglected to research our success, they were forced to research their own failure again.</p>&#13;
<p>I understood their frustrations. After all, they had followed our process to the letter and gotten different results. Or at least, that’s what the situation looked like from their perspective, but when they asked for my thoughts, I noticed they had missed one critical component of that war room.</p>&#13;
<p>They put the wrong people in it.</p>&#13;
<h2 id="h1-501188c09-0008">Working Groups vs. Committees</h2>&#13;
<p class="BodyFirst">Throughout this book, I have suggested structures that involve one small group of people advising, planning for, and, at times, delegating work to a larger team. These are working groups. It has become popular as of late to use the term <em>working group</em> instead of the word <em>committee</em> because committee just sounds bureaucratic to most people. Committees have a reputation for not getting anything done or, worse, for building through compromise. A camel, the old expression goes, is a horse built by committee.</p>&#13;
<p>Working groups do not have this baggage, so people will often say they want to start a working group and turn around and start a committee instead—as if simply changing the name of a structure somehow makes the structure a more effective tool. Worse still, most people can no longer tell the difference between a working group and a committee. That was what the leaders had missed when they copied our successful strategy. The people in the war room of our project were a working group. The people in the war room of the failed project were a committee.</p>&#13;
<p>What exactly is the difference?</p>&#13;
<p><span epub:type="pagebreak" title="192" id="Page_192"/>Working groups relax hierarchy to allow people to solve problems across organizational units, whereas committees both reflect and reinforce those organizational boundaries and hierarchies. Our war room was made up of software engineers and network administrators. We brought people who had to work together to implement the project into the same room to work next to one another instead of communicating over email, through bosses, and scheduling any number of conference calls.</p>&#13;
<p>The failed war room, on the other hand, was made up of executives. Rather than bringing together people who are peers but report up to different chains of command, this war room just mimicked the existing structure and the existing politics of the organization. Worse, the day to day of most of these executives consisted almost entirely of meetings. Instead of working shoulder to shoulder with colleagues, they used the war room as a place to throw their belongings as they ran in and out of other conference rooms to conduct business as usual.</p>&#13;
<p>Working groups are internally facing; the customers for a working group are the members of the working group itself. People join working groups to share their knowledge and experience with peers. They are effective because they establish a space for cross-organizational collaboration and troubleshooting. People on the implementation layer of an organization can bring their challenges to peers who have already experienced those challenges and hear their stories or bounce ideas off them. Sometimes this results in recommendations for leadership, but the primary purpose of a working group is to troubleshoot and evangelize across an organization or industry. Working groups are typically initiated and staffed by people on the implementation layer of the organization.</p>&#13;
<p>A committee is formed to advise an audience external to the committee itself, typically someone in senior leadership. Whereas the working group is open to those who consider themselves operating in the same space as the working group’s topic area, committees are selected by the entity they will be advising and typically closed off to everyone else. That external authority decides the committee’s scope and goals. The <span epub:type="pagebreak" title="193" id="Page_193"/>committee reports up to that external authority and exists solely for its benefit.</p>&#13;
<p>Committees also tend to have a lot of procedure around them, but the absence of chairpeople and <em>Robert’s Rules of Order</em> does not make a committee a working group.</p>&#13;
<p>The model can have a fair amount of variation. A Code Yellow team, for example, might not be self-selecting, and ignoring its advice might have serious consequences, but the point of a Code Yellow team is to reorganize and redistribute resources temporarily across an organization. That team ultimately reports up to a leader who is closer to a peer than an executive. The important takeaway is working groups relax organizational boundaries while committees reinforce them.</p>&#13;
<p>There is little value in having senior managers represent their units in a war room. Since they are not implementing the technology themselves, they cannot speak to what compromises would unblock the project without going back to their own engineers. Our war room was successful because it shortened the distance those conversations had to travel. The failed war room added a game of telephone on top of the existing barriers to communication. One could never be sure that the message coming out of that war room was accurate or if the manager representing you had misunderstood an implementation detail.</p>&#13;
<p>Funny story: I would, from time to time, run war rooms filled with senior executives. It was a tactic I resorted to when the organization was in such a state of panic that senior leadership members were micromanaging their teams. Software engineers can’t fix problems if they spend all their time in meetings giving their managers updates about the problems. The boot has to be moved off their necks.</p>&#13;
<p>In that situation, my team would typically run two war rooms: one where the engineers were solving problems together and one outfitted with lots of fancy dashboards where I was babysitting the senior executives. The most valuable skill a leader can have is knowing when to get out of the way.</p>&#13;
<h2 id="h1-501188c09-0009"><span epub:type="pagebreak" title="194" id="Page_194"/>Success Is Not Obvious If Undefined</h2>&#13;
<p class="BodyFirst">Modernization projects without a clear definition of what success looks like will find themselves with a finish line that only moves further back the closer they get to it. Don’t assume that success is obvious. Different members of your team may have different and competing visions of what better looks like. Everyone needs to be able to explain how they know that their efforts are moving the project forward for people to be able to work together. Projects that need something as extreme as a legacy modernization effort have no shortage of problems to solve. By defining success, you keep the finish line from moving.</p>&#13;
<p>In the next chapter, I will explain how to maintain software on a day-to-day basis to avoid having to run legacy modernizations in the first place. The finish line of a modernization effort need not be that everything is fixed and the technology is now perfect. I have yet to encounter a system that could be described that way, and I have worked for organizations as young as six months and as old as 200 years. All technology is imperfect, so the goal of legacy modernization should not be restoring mythical perfection but bringing the system to a state where it is possible to maintain modern best practices around security, stability, and availability.</p>&#13;
</section>&#13;
<section class="footnotes">&#13;
<aside class="FootnoteEntry">&#13;
<p><sup class="FootnoteReference"><a id="c09-footnote-1" href="#c09-footnoteref-1">1.</a></sup> 	Richard P. Rumelt, <em>Good Strategy Bad Strategy: The Difference and Why It Matters</em> (New York: Crown Business, 2011).</p>&#13;
</aside>&#13;
</section>&#13;
</body></html>