- en: '**7'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**7'
- en: MULTI-GPU TRAINING PARADIGMS**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 多GPU训练范式**
- en: '![Image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/common.jpg)'
- en: What are the different multi-GPU training paradigms, and what are their respective
    advantages and disadvantages?
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 有哪些不同的多GPU训练范式，它们各自的优缺点是什么？
- en: 'Multi-GPU training paradigms can be categorized into two groups: dividing data
    for parallel processing with multiple GPUs and dividing the model among multiple
    GPUs to handle memory constraints when the model size surpasses that of a single
    GPU. Data parallelism falls into the first category, while model parallelism and
    tensor parallelism fall into the second category. Techniques like pipeline parallelism
    borrow ideas from both categories. In addition, current software implementations
    such as DeepSpeed, Colossal AI, and others blend multiple approaches into a hybrid
    technique.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 多GPU训练范式可以分为两类：一类是将数据划分进行并行处理，另一类是将模型划分到多个GPU上，以应对当模型大小超出单个GPU内存时的内存限制。数据并行属于第一类，而模型并行和张量并行属于第二类。像流水线并行这样的技术则融合了这两类思想。此外，当前的软件实现，如DeepSpeed、Colossal
    AI等，也将多种方法融合成一种混合技术。
- en: This chapter introduces several training paradigms and provides advice on which
    to use in practice.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了几种训练范式，并提供了在实践中使用哪些范式的建议。
- en: '**NOTE**'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*This chapter primarily uses the term* GPUs *to describe the hardware utilized
    for parallel processing. However, the same concepts and techniques discussed can
    be applied to other specialized hardware devices, such as tensor processing units
    (TPUs) or other accelerators, depending on the specific architecture and requirements
    of the system.*'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章主要使用“GPU”一词来描述用于并行处理的硬件。然而，讨论的相同概念和技术也可以应用于其他专用硬件设备，如张量处理单元（TPU）或其他加速器，具体取决于系统的架构和需求。*'
- en: '**The Training Paradigms**'
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**训练范式**'
- en: The following sections discuss the model parallelism, data parallelism, tensor
    parallelism, and sequence parallelism multi-GPU training paradigms.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 以下各节讨论了模型并行、数据并行、张量并行和序列并行的多GPU训练范式。
- en: '***Model Parallelism***'
  id: totrans-10
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***模型并行***'
- en: '*Model parallelism*, or *inter-op parallelism*, is a technique in which different
    sections of a large model are placed on different GPUs and are computed sequentially,
    with intermediate results passed between the devices. This allows for the training
    and execution of models that might not fit entirely on a single device, but it
    can require intricate coordination to manage the dependencies between different
    parts of the model.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*模型并行*，或称*操作间并行*，是一种将大型模型的不同部分放置在不同GPU上的技术，这些部分按顺序计算，并在设备之间传递中间结果。这使得训练和执行可能无法完全适配单个设备的模型成为可能，但它可能需要复杂的协调来管理模型不同部分之间的依赖关系。'
- en: Model parallelism is perhaps the most intuitive form of parallelization across
    devices. For example, for a simple neural network that consists of only two layers—a
    hidden layer and an output layer—we can keep one layer on one GPU and the other
    layer on another GPU. Of course, this can scale to an arbitrary number of layers
    and GPUs.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 模型并行可能是最直观的设备间并行化形式。例如，对于一个仅包含两个层——隐藏层和输出层的简单神经网络，我们可以将一层放在一个GPU上，另一层放在另一个GPU上。当然，这可以扩展到任意数量的层和GPU。
- en: This is a good strategy for dealing with limited GPU memory where the complete
    network does not fit into one GPU. However, there are more efficient ways of using
    multiple GPUs, such as tensor parallelism, because the chain-like structure (layer
    1 on GPU 1 *→* layer 2 on GPU 2 *→* . . .) in model parallelism introduces a bottleneck.
    In other words, a major disadvantage of model parallelism is that the GPUs have
    to wait for each other. They cannot efficiently work in parallel, as they depend
    on one other’s outputs.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这是处理有限GPU内存的一个好策略，尤其是当整个网络无法完全加载到一个GPU时。然而，还有更高效的多GPU使用方式，如张量并行，因为模型并行中的链式结构（层1在GPU
    1上*→* 层2在GPU 2上*→* . . .）会引入瓶颈。换句话说，模型并行的一个主要缺点是，GPU必须互相等待，无法高效并行工作，因为它们依赖于彼此的输出。
- en: '***Data Parallelism***'
  id: totrans-14
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***数据并行***'
- en: '*Data parallelism* has been the default mode for multi-GPU training for several
    years. Here, we divide a minibatch into smaller microbatches. Each GPU then processes
    a microbatch separately to compute the loss and loss gradients for the model weights.
    After the individual devices process the microbatches, the gradients are combined
    to compute the weight update for the next round.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*数据并行*已经成为多GPU训练的默认模式好几年了。在这种模式下，我们将一个小批次分割成更小的微批次。然后每个GPU分别处理一个微批次，以计算模型权重的损失和损失梯度。在每个设备处理完微批次后，梯度会被合并，计算出下一轮的权重更新。'
- en: An advantage of data parallelism over model parallelism is that the GPUs can
    run in parallel. Each GPU processes a portion of the training minibatch, that
    is, a microbatch. However, a caveat is that each GPU requires a full copy of the
    model. This is obviously not feasible if we have large models that don’t fit into
    the GPU’s VRAM.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并行相对于模型并行的一个优势是，GPU可以并行运行。每个GPU处理训练小批次的一部分，也就是一个微批次。然而，一个警告是，每个GPU都需要完整的模型副本。如果我们有无法放入GPU显存的大型模型，显然这是不可行的。
- en: '***Tensor Parallelism***'
  id: totrans-17
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***张量并行***'
- en: '*Tensor parallelism*, or *intra-op parallelism*, is a more efficient form of
    model parallelism. Here, the weight and activation matrices are spread across
    the devices instead of distributing whole layers across devices: the individual
    matrices are split, so we split an individual matrix multiplication across GPUs.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*张量并行*，或称为*操作内并行*，是一种更高效的模型并行形式。在这种方式下，权重矩阵和激活矩阵被分布到多个设备上，而不是将整个层分布到不同设备：每个矩阵被拆分，因此我们可以将单个矩阵乘法分配到多个GPU上进行计算。'
- en: We can implement tensor parallelism using basic principles of linear algebra;
    we can split a matrix multiplication across two GPUs in a row- or column-wise
    fashion, as illustrated in [Figure 7-1](ch07.xhtml#ch7fig1) for two GPUs. (This
    concept can be extended to an arbitrary number of GPUs.)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用线性代数的基本原理实现张量并行；我们可以按行或按列的方式将矩阵乘法拆分到两个GPU上，如[图 7-1](ch07.xhtml#ch7fig1)所示。
    （这一概念可以扩展到任意数量的GPU。）
- en: '![Image](../images/07fig01.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/07fig01.jpg)'
- en: '*Figure 7-1: Tensor parallelism for distributing matrix multiplication across
    different devices*'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7-1：用于分布矩阵乘法的张量并行*'
- en: Like model parallelism, tensor parallelism allows us to work around memory limitations.
    At the same time, it also lets us execute operations in parallel, similar to data
    parallelism.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 和模型并行一样，张量并行让我们可以绕过内存限制。同时，它也允许我们并行执行操作，类似于数据并行。
- en: A small weakness of tensor parallelism is that it can result in high communication
    overhead between the multiple GPUs across which the matrices are split or sharded.
    For instance, tensor parallelism requires frequent synchronization of the model
    parameters across devices, which can slow down the overall training process.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 张量并行的一个小缺点是，它可能会导致多个GPU之间的高通信开销，尤其是在矩阵被拆分或分片时。例如，张量并行要求频繁地同步设备间的模型参数，这可能会减慢整个训练过程。
- en: '[Figure 7-2](ch07.xhtml#ch7fig2) compares model, data, and tensor parallelism.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7-2](ch07.xhtml#ch7fig2)比较了模型、数据和张量并行。'
- en: '![Image](../images/07fig02.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/07fig02.jpg)'
- en: '*Figure 7-2: A comparison of model, data, and tensor parallelism*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7-2：模型、数据和张量并行的比较*'
- en: In model parallelism, we put different layers onto different GPUs to work around
    GPU memory limitations. In data parallelism, we split a batch across GPUs to train
    copies of the model in parallel, averaging gradients for the weight update afterward.
    In tensor parallelism, we split matrices (inputs and weights) across different
    GPUs for parallel processing when models are too large to fit into GPU memory.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型并行中，我们将不同的层放置到不同的GPU上，以绕过GPU内存的限制。在数据并行中，我们将一个批次分割到多个GPU上并行训练模型的副本，随后对梯度进行平均以更新权重。在张量并行中，我们将矩阵（输入和权重）分布到不同的GPU上进行并行处理，当模型太大而无法放入GPU内存时，这种方式尤其有效。
- en: '***Pipeline Parallelism***'
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***流水线并行***'
- en: In *pipeline parallelism*, activations are passed during the forward pass, as
    in model parallelism. The twist is that the gradients of the input tensor are
    passed backward to prevent the devices from being idle. In a sense, pipeline parallelism
    is a sophisticated hybrid version of data and model parallelism.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在*流水线并行*中，激活值在前向传播时传递，就像在模型并行中一样。不同的是，输入张量的梯度会反向传递，以避免设备空闲。从某种意义上说，流水线并行是数据并行和模型并行的复杂混合版本。
- en: We can think of pipeline parallelism as a form of model parallelism that tries
    to minimize the sequential computation bottleneck, enhancing the parallelism between
    the individual layers sitting on different devices. However, pipeline parallelism
    also borrows ideas from data parallelism, such as splitting minibatches further
    into microbatches.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将管道并行性视为一种模型并行性，它试图最小化顺序计算瓶颈，增强不同设备上单个层之间的并行性。然而，管道并行性也借鉴了数据并行性的一些思想，例如将小批量进一步划分为微批次。
- en: Pipeline parallelism is definitely an improvement over model parallelism, though
    it is not perfect and there will be idle bubbles. A further disadvantage of pipeline
    parallelism is that it may require significant effort to design and implement
    the pipeline stages and associated communication patterns. Additionally, the performance
    gains it generates may not be as substantial as those from other parallelization
    techniques, such as pure data parallelism, especially for small models or in cases
    where the communication overhead is high.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 管道并行性无疑是对模型并行性的改进，尽管它并不完美，仍然会有空闲气泡。管道并行性的一个进一步缺点是，它可能需要大量努力来设计和实现管道阶段及其相关的通信模式。此外，它所产生的性能提升可能不如其他并行化技术（如纯数据并行性）那么显著，特别是对于小模型或在通信开销较高的情况下。
- en: For modern architectures that are too large to fit into GPU memory, it is more
    common nowadays to use a blend of data parallelism and tensor parallelism techniques
    instead of pipeline parallelism.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些过大以至于无法装入GPU内存的现代架构，现在更常用的是数据并行性和张量并行性技术的结合，而不是管道并行性。
- en: '***Sequence Parallelism***'
  id: totrans-33
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***序列并行性***'
- en: '*Sequence parallelism* aims to address computational bottlenecks when working
    with long sequences using transformer-based LLMs. More specifically, one shortcoming
    of transformers is that the self-attention mechanism (the original scaled-dot
    product attention) scales quadratically with the input sequence length. There
    are, of course, more efficient alternatives to the original attention mechanism
    that scale linearly.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*序列并行性*旨在解决使用基于变换器的LLM处理长序列时的计算瓶颈。更具体地说，变换器的一个缺点是自注意力机制（原始的缩放点积注意力）随着输入序列长度的增加，计算量呈二次增长。当然，也有比原始注意力机制更高效的替代方案，能够线性扩展。'
- en: However, these efficient self-attention mechanisms are less popular, and most
    people still prefer the original scaled-dot product attention mechanism as of
    this writing. Sequence parallelism, illustrated in [Figure 7-3](ch07.xhtml#ch7fig3),
    splits the input sequence into smaller chunks to be distributed across GPUs, which
    aims to reduce computation memory constraints of self-attention mechanisms.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些高效的自注意力机制并不太流行，到目前为止，大多数人仍然更喜欢原始的缩放点积注意力机制。序列并行性，如[图7-3](ch07.xhtml#ch7fig3)所示，将输入序列拆分成较小的块，分配到多个GPU上，旨在减少自注意力机制的计算内存限制。
- en: '![Image](../images/07fig03.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/07fig03.jpg)'
- en: '*Figure 7-3: Sequence parallelism divides long inputs among GPUs.*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7-3：序列并行性将长输入划分到多个GPU上。*'
- en: How does sequence parallelism relate to the multi-GPU techniques discussed earlier?
    Sequence parallelism deals specifically with sequential data, tensor parallelism
    deals with the model’s internal structure, and data parallelism deals with how
    the training data is divided. Theoretically, since each of these parallelism strategies
    addresses a different aspect of the computational challenge, they can thus be
    combined in various ways to optimize the training or inference process. Sequence
    parallelism is not as well studied as other parallelization techniques, however.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 序列并行性如何与前面讨论的多GPU技术相关联？序列并行性专门处理顺序数据，张量并行性处理模型的内部结构，而数据并行性则处理训练数据的划分。理论上，由于这些并行策略各自解决计算挑战的不同方面，因此可以通过多种方式结合使用，以优化训练或推理过程。然而，序列并行性并不像其他并行化技术那样得到充分研究。
- en: While sequence parallelism appears useful in practice, it also introduces additional
    communication overheads similar to the aforementioned parallelism techniques.
    Like data parallelism, it requires us to duplicate the model and make sure it
    fits into the device memory. Another of its disadvantages (depending on the implementation)
    for multi-GPU training of transformers is that breaking up the input sequence
    into smaller subsequences can decrease the model’s accuracy (mainly when the model
    is applied to longer sequences).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管序列并行性在实践中似乎有用，但它也带来了与上述并行化技术类似的额外通信开销。像数据并行性一样，它要求我们复制模型并确保它适配设备内存。对于变换器的多GPU训练，另一个缺点（取决于实现）是将输入序列分割成更小的子序列可能会降低模型的准确性（主要是在应用于较长序列时）。
- en: '**Recommendations**'
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**建议**'
- en: Practical recommendations depend on the context. If we train small models that
    fit onto a single GPU, then data parallelism strategies may be the most efficient.
    Performance gains from pipeline parallelism may not be as significant as those
    from other parallelization techniques, such as data parallelism, especially for
    small models or in cases where the communication overhead is high.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 实际建议取决于具体的情况。如果我们训练的小模型可以适应单个GPU，那么数据并行性策略可能是最有效的。流水线并行性带来的性能提升可能不如其他并行化技术，如数据并行性，尤其是在小模型或通信开销较高的情况下。
- en: If models are too large to fit into the memory of a single GPU, we need to explore
    model or tensor parallelism. Tensor parallelism is naturally more efficient; the
    GPUs can work in parallel since there is no sequential dependency as in model
    parallelism.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型太大，无法适配单个GPU的内存，我们需要探索模型或张量并行性。张量并行性天生更高效；因为没有像模型并行性那样的顺序依赖，GPU可以并行工作。
- en: Modern multi-GPU strategies also typically combine data parallelism and tensor
    parallelism.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现代的多GPU策略通常也结合了数据并行性和张量并行性。
- en: '**Exercises**'
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**练习**'
- en: '**7-1.** Suppose we are implementing our own version of tensor parallelism,
    which works great when we train our model with a standard stochastic gradient
    descent optimizer. However, when we try the Adam optimizer by Diederik P. Kingma
    and Jimmy Ba, we encounter an out-of-memory device. What problem might explain
    this issue?'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**7-1.** 假设我们正在实现自己版本的张量并行性，当我们使用标准的随机梯度下降优化器训练模型时，它运行得很好。然而，当我们尝试使用Diederik
    P. Kingma和Jimmy Ba提出的Adam优化器时，遇到了设备内存不足的问题。是什么原因可能解释了这个问题？'
- en: '**7-2.** Suppose we don’t have access to a GPU and are considering using data
    parallelism on the CPU. Is this a good idea?'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**7-2.** 假设我们没有GPU可用，并且考虑在CPU上使用数据并行性。这是一个好主意吗？'
- en: '**References**'
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: 'The original paper on the Adam optimizer: Diederik P. Kingma and Jimmy Ba,
    “Adam: A Method for Stochastic Optimization” (2014), *[https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)*.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '关于Adam优化器的原始论文：Diederik P. Kingma和Jimmy Ba，《Adam: 一种用于随机优化的方法》（2014年），*[https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)*。'
- en: 'For more on DeepSpeed and Colossal-AI for multi-GPU training: *[https://github.com/microsoft/DeepSpeed](https://github.com/microsoft/DeepSpeed)*
    and *[https://github.com/hpcaitech/ColossalAI](https://github.com/hpcaitech/ColossalAI)*.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关DeepSpeed和Colossal-AI在多GPU训练中的更多信息：*[https://github.com/microsoft/DeepSpeed](https://github.com/microsoft/DeepSpeed)*
    和 *[https://github.com/hpcaitech/ColossalAI](https://github.com/hpcaitech/ColossalAI)*。
- en: 'Pipeline parallelism tutorials and research by the DeepSpeed team: *[https://www.deepspeed.ai/tutorials/pipeline](https://www.deepspeed.ai/tutorials/pipeline)*
    and Yanping Huang et al., “GPipe: Efficient Training of Giant Neural Networks
    Using Pipeline Parallelism” (2018), *[https://arxiv.org/abs/1811.06965](https://arxiv.org/abs/1811.06965)*.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'DeepSpeed团队的流水线并行教程和研究：*[https://www.deepspeed.ai/tutorials/pipeline](https://www.deepspeed.ai/tutorials/pipeline)*
    和 Yanping Huang等人，《GPipe: 使用流水线并行高效训练巨型神经网络》（2018年），*[https://arxiv.org/abs/1811.06965](https://arxiv.org/abs/1811.06965)*。'
- en: 'The paper proposing sequence parallelism for transformer-based language models:
    Shenggui Li et al., “Sequence Parallelism: Long Sequence Training from [a] System[s]
    Perspective” (2022), *[https://arxiv.org/abs/2105.13120](https://arxiv.org/abs/2105.13120)*.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提出序列并行性用于基于变换器的语言模型的论文：Shenggui Li等人，《序列并行性：从[系统]的角度看长序列训练》（2022年），*[https://arxiv.org/abs/2105.13120](https://arxiv.org/abs/2105.13120)*。
- en: 'The scaled-dot product attention mechanism was proposed with the original transformer
    architecture: Ashish Vaswani et al., “Attention Is All You Need” (2017), *[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)*.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展点积注意力机制是在原始的 Transformer 架构中提出的：Ashish Vaswani 等人，“Attention Is All You Need”（2017），*[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)*。
- en: 'A survey covering alternatives to the original self-attention mechanism that
    scale linearly: Yi Tay et al., “Efficient Transformers: A Survey” (2020), *[https://arxiv.org/abs/2009.06732](https://arxiv.org/abs/2009.06732)*.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '介绍了替代原始自注意力机制的方案，并且这些方案具有线性扩展性：Yi Tay 等人，“Efficient Transformers: A Survey”（2020），*[https://arxiv.org/abs/2009.06732](https://arxiv.org/abs/2009.06732)*。'
- en: 'A survey covering additional techniques to improve the training efficiency
    of transformers: Bohan Zhuang et al., “A Survey on Efficient Training of Transformers”
    (2023), *[https://arxiv.org/abs/2302.01107](https://arxiv.org/abs/2302.01107)*.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍了提高 Transformer 训练效率的额外技术：Bohan Zhuang 等人，“A Survey on Efficient Training
    of Transformers”（2023），*[https://arxiv.org/abs/2302.01107](https://arxiv.org/abs/2302.01107)*。
- en: 'Modern multi-GPU strategies typically combine data parallelism and tensor parallelism.
    Popular examples include DeepSpeed stages 2 and 3, described in this tutorial
    on the zero redundancy optimizer: *[https://www.deepspeed.ai/tutorials/zero/](https://www.deepspeed.ai/tutorials/zero/)*.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现代多 GPU 策略通常结合数据并行性和张量并行性。流行的例子包括 DeepSpeed 阶段 2 和 3，这些在零冗余优化器的教程中有描述：*[https://www.deepspeed.ai/tutorials/zero/](https://www.deepspeed.ai/tutorials/zero/)*。
