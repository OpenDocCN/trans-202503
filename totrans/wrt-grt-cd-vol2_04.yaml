- en: '**4**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**COMPILER OPERATION AND CODE GENERATION**'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/common01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In order to write HLL code that produces efficient machine code, you must first
    understand how compilers and linkers translate high-level source statements into
    executable machine code. Complete coverage of compiler theory is beyond the scope
    of this book; however, this chapter explains the basics of the translation process
    so you can understand and work within the limitations of HLL compilers. We’ll
    cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The different types of input files programming languages use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Differences between compilers and interpreters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How typical compilers process source files to produce executable programs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The process of optimization and why compilers cannot produce the best possible
    code for a given source file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different types of output files that compilers produce
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common object file formats, such as COFF and ELF
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory organization and alignment issues that affect the size and efficiency
    of executable files a compiler produces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How linker options can affect the efficiency of your code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This material provides the foundation for all the chapters that follow, and
    is crucial to helping a compiler produce the best possible code. We’ll begin with
    a discussion of file formats used by programming languages.
  prefs: []
  type: TYPE_NORMAL
- en: '**4.1 File Types That Programming Languages Use**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A typical program can take many forms. A *source file* is a human-readable form
    that a programmer creates and supplies to a language translator (such as a compiler).
    A typical compiler translates the source file or files into an *object code* file.
    A *linker program* combines separate object modules to produce a relocatable or
    executable file. Finally, a *loader* (usually the operating system) loads the
    executable file into memory and makes the final modifications to the object code
    prior to execution. Note that the modifications are made to the object code that
    is now in memory; the actual file on the disk does not get modified. These are
    not the only types of files that language processing systems manipulate, but they
    are typical. To fully understand compiler limitations, it’s important to know
    how the language processor deals with each of these file types. We’ll look at
    source files first.
  prefs: []
  type: TYPE_NORMAL
- en: '**4.2 Source Files**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Traditionally, source files contain pure ASCII or Unicode text (or some other
    character set) that a programmer has created with a text editor. One advantage
    to using pure text files is that a programmer can manipulate a source file using
    any program that processes text files. For example, a program that counts the
    number of lines in an arbitrary text file will also count the number of source
    lines in a program. Because there are hundreds of little filter programs that
    manipulate text files, maintaining source files in a pure text format is a good
    approach. This format is sometimes called *plain vanilla text*.
  prefs: []
  type: TYPE_NORMAL
- en: '**4.2.1 Tokenized Source Files**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Some language processing systems (especially interpreters) maintain their source
    files in a *tokenized* form. Tokenized source files generally use special single-byte
    *token* values to compress reserved words and other lexical elements in the source
    language, and thus they are often smaller than text source files. Furthermore,
    interpreters that operate on tokenized code are generally an order of magnitude
    faster than interpreters that operate on pure text, because processing strings
    of single-byte tokens is far more efficient than recognizing reserved word strings.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, the tokenized file from the interpreter consists of a sequence of
    bytes that map directly to strings such as `if` and `print` in the source file.
    So, by using a table of strings and a little extra logic, you can decipher a tokenized
    program to produce the original source code. (Usually, you lose any extra whitespace
    you inserted into the source file, but that’s about the only difference.) Many
    of the original BASIC interpreters found on early PC systems worked this way.
    You’d type a line of BASIC source code into the interpreter, and the interpreter
    would immediately tokenize that line and store the tokenized form in memory. Later,
    when you executed the `LIST` command, the interpreter would *detokenize* the source
    code in memory to produce the listing.
  prefs: []
  type: TYPE_NORMAL
- en: On the flip side, tokenized source files often use a proprietary format. This
    means they can’t take advantage of general-purpose text-manipulation tools like
    `wc` (word count), `entab`, and `detab` (which count the number of lines, words,
    and characters in a text file; replace spaces with tabs; and replace tabs with
    spaces, respectively).
  prefs: []
  type: TYPE_NORMAL
- en: To overcome this limitation, most languages that operate on tokenized files
    enable you to detokenize a source file to produce a standard text file. (They
    also allow you to retokenize a source file, given an input text file.) You then
    run the resulting text file through some filter program, and retokenize the output
    of the filter program to produce a new tokenized source file. Although this takes
    considerable work, it allows language translators that work with tokenized files
    to take advantage of various text-based utility programs.
  prefs: []
  type: TYPE_NORMAL
- en: '**4.2.2 Specialized Source Files**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Some programming languages, such as Embarcadero’s Delphi and Free Pascal’s comparable
    Lazarus program, do not use a traditional text-based file format at all. Instead,
    they often use graphical elements like flowcharts and forms to represent the instructions
    the program is to perform. Other examples are the Scratch programming language,
    which allows you to write simple programs using graphical elements on a bitmapped
    display, and the Microsoft Visual Studio and Apple Xcode integrated development
    environments (IDEs), which both allow you to specify a screen layout using graphical
    operations rather than a text-based source file.
  prefs: []
  type: TYPE_NORMAL
- en: '**4.3 Types of Computer Language Processors**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Computer language processing systems generally fall into one of four categories:
    pure interpreters, interpreters, compilers, and incremental compilers. These systems
    differ in how they process the source program and execute the result, which affects
    their respective efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: '**4.3.1 Pure Interpreters**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Pure interpreters* operate directly on a text source file and tend to be very
    inefficient. They continuously scan the source file (usually an ASCII text file),
    processing it as string data. Recognizing *lexemes* (language components such
    as reserved words, literal constants, and the like) consumes time. Indeed, many
    pure interpreters spend more time processing the lexemes (that is, performing
    *lexical analysis*) than they do actually executing the program. Because the actual
    on-the-fly execution of the lexeme takes only a little additional effort beyond
    the lexical analysis, pure interpreters tend to be the smallest of the computer
    language processing programs. For this reason, pure interpreters are popular when
    you need a very compact language processor. They are also popular for scripting
    languages and very high-level languages that let you manipulate the language’s
    source code as string data during program execution.'
  prefs: []
  type: TYPE_NORMAL
- en: '**4.3.2 Interpreters**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: An *interpreter* executes some representation of a program’s source file at
    runtime. This representation isn’t necessarily a text file in human-readable form.
    As noted in the previous section, many interpreters operate on tokenized source
    files in order to avoid lexical analysis during execution. Some interpreters read
    a text source file as input and translate the input file to a tokenized form prior
    to execution. This allows programmers to work with text files in their favorite
    editor while enjoying the fast execution of a tokenized format. The only costs
    are an initial delay to tokenize the source file (which is unnoticeable on most
    modern machines) and the fact that it may not be possible to execute strings containing
    program statements.
  prefs: []
  type: TYPE_NORMAL
- en: '**4.3.3 Compilers**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A *compiler* translates a source program in text form into executable machine
    code. This is a complex process, particularly in optimizing compilers. There are
    a couple of things to note about the code a compiler produces. First, a compiler
    produces machine instructions that the underlying CPU can execute directly. Therefore,
    the CPU doesn’t waste any cycles decoding the source file while executing the
    program—all of the CPU’s resources are dedicated to executing the machine code.
    Thus, the resulting program generally runs many times faster than an interpreted
    version does. Of course, some compilers do a better job of translating HLL source
    code into machine code than other compilers, but even low-quality compilers do
    a better job than most interpreters.
  prefs: []
  type: TYPE_NORMAL
- en: A compiler’s translation from source code to machine code is a one-way function.
    In contrast to interpreters, it is very difficult, if not impossible, to reconstruct
    the original source file if you’re given only the machine code output from a program.
  prefs: []
  type: TYPE_NORMAL
- en: '**4.3.4 Incremental Compilers**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: An *incremental compiler* is a cross between a compiler and an interpreter.
    There are many different types of incremental compilers, but in general, they
    operate like an interpreter in that they do not compile the source file directly
    into machine code; instead, they translate the source code into an intermediate
    form. Unlike the tokenized code from interpreters, however, this intermediate
    form usually is not strongly correlated to the original source file. The intermediate
    form is generally the machine code for a *virtual machine language*—“virtual”
    in that there is no real CPU that can execute this code. However, it is easy to
    write an interpreter that can execute it. Because interpreters for virtual machines
    (VMs) are usually much more efficient than interpreters for tokenized code, executing
    VM code is usually much faster than executing a list of tokens in an interpreter.
    Languages like Java use this compilation technique, along with a *Java bytecode
    engine* (an interpreter program), to interpretively execute the Java “machine
    code” (see [Figure 4-1](ch04.xhtml#ch4fig1)). The big advantage to VM execution
    is that the VM code is portable; that is, programs running on the virtual machine
    can execute anywhere an interpreter is available. True machine code, by contrast,
    executes only on the CPU (family) for which it was written. Generally, interpreted
    VM code runs about 2 to 10 times faster than interpreted code (tokenized), and
    pure machine code runs about 2 to 10 times faster than interpreted VM code.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/04fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-1: The JBC interpreter*'
  prefs: []
  type: TYPE_NORMAL
- en: In an attempt to improve the performance of programs compiled via an incremental
    compiler, many vendors (particularly Java systems vendors) have turned to a technique
    known as *just-in-time (JIT) compilation*. The concept is based on the fact that
    the time spent in interpretation is largely consumed by fetching and deciphering
    the VM code at runtime. This interpretation occurs repeatedly as the program executes.
    JIT compilation translates the VM code to actual machine code whenever it encounters
    a VM instruction for the first time. This spares the interpreter from repeating
    the interpretation process the next time it encounters the same statement in the
    program (for example, in a loop). Although JIT compilation is nowhere near as
    good as a true compiler, it can typically improve the performance of a program
    by a factor of two to five.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Older compilers and some freely available compilers compile the source code
    to assembly language, and then a separate compiler, known as an* assembler, *assembles
    this output to the desired machine code. Most modern and highly efficient compilers
    skip this step altogether. See “Compiler Output” on [page 67](ch04.xhtml#page_67)
    for more on this subject.*'
  prefs: []
  type: TYPE_NORMAL
- en: Of the four categories of computer language processors just described, this
    chapter will focus on compilers. By understanding how a compiler generates machine
    code, you can choose appropriate HLL statements to generate better, more efficient
    machine code. If you want to improve the performance of programs written with
    an interpreter or incremental compiler instead, the best approach is to use an
    optimizing compiler to process your application. For example, GNU provides a compiler
    for Java that produces optimized machine code rather than interpreted Java bytecode
    (JBC); the resulting executable files run much faster than interpreted JBC or
    even JIT-compiled bytecode.
  prefs: []
  type: TYPE_NORMAL
- en: '**4.4 The Translation Process**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A typical compiler is broken down into several logical components called *phases*.
    Although their exact number and names may vary somewhat among different compilers,
    the five most common phases are *lexical analysis*, *syntax analysis*, *intermediate
    code generation*, *native code generation*, and, for compilers that support it,
    *optimization*.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 4-2](ch04.xhtml#ch4fig2) shows how the compiler logically arranges
    these phases to translate source code in the HLL into machine (object) code.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/04fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-2: Phases of compilation*'
  prefs: []
  type: TYPE_NORMAL
- en: Although [Figure 4-2](ch04.xhtml#ch4fig2) suggests that the compiler executes
    these phases sequentially, most compilers do not. Instead, the phases tend to
    execute in parallel, with each phase doing a small amount of work, passing its
    output to the next phase, and then waiting for input from the previous phase.
    In a typical compiler, the *parser* (the syntax analysis phase) is probably the
    closest thing you’ll find to the main program or the master process. The parser
    usually drives the compilation process in that it calls the *scanner* (lexical
    analysis phase) to obtain input and calls the intermediate code generator to process
    its own output. The intermediate code generator may (optionally) call the optimizer
    and then call the native code generator. The native code generator may (optionally)
    call the optimizer as well. The output from the native code generation phase is
    the executable code. After the native code generator/optimizer emits some code,
    execution returns to the intermediate code generator, then to the parser, which
    requests more input from the scanner, starting the whole process over.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Other compiler organizations are possible. Some compilers, for example, allow
    the user to choose whether the compiler runs the optimization phase, while others
    don’t have an optimization phase at all. Similarly, some compilers dispense with
    intermediate code generation and directly call a native code generator. Some compilers
    include additional phases that process object modules compiled at different times.*'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, although [Figure 4-2](ch04.xhtml#ch4fig2) doesn’t accurately depict the
    typical (parallel) execution path, the *data flow* it shows is correct. The scanner
    reads the source file, translates it to a different form, and then passes this
    translated data on to the parser. The parser accepts its input from the scanner,
    translates that input to a different form, and then passes this new data to the
    intermediate code generator. Similarly, the remaining phases read their input
    from the previous phase, translate the input to a (possibly) different form, and
    then pass that input on to the next phase. The compiler writes the output of its
    last phase to the executable object file.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a closer look at each phase of the code translation process.
  prefs: []
  type: TYPE_NORMAL
- en: '**4.4.1 Scanning (Lexical Analysis)**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The scanner (aka the *lexical analyzer*, or *lexer*) is responsible for reading
    the character/string data found in the source file and breaking up this data into
    tokens that represent the lexical items, or lexemes, in the source file. As mentioned
    previously, lexemes are the character sequences in the source file that we would
    recognize as atomic components of the language. For example, a scanner for the
    C language would recognize substrings like `if` and `while` as C reserved words.
    The scanner would not, however, pick out the “if ” within the identifier `ifReady`
    and treat it as a reserved word. Instead, the scanner considers the context in
    which a reserved word is used so that it can differentiate between reserved words
    and identifiers. For each lexeme, the scanner creates a small data package—a token—and
    passes it on to the parser. A token typically contains several values:'
  prefs: []
  type: TYPE_NORMAL
- en: A small integer that uniquely identifies the token’s class (whether it’s a reserved
    word, identifier, integer constant, operator, or character string literal)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another value that differentiates the token within a class (for example, this
    value would indicate which reserved word the scanner has processed)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any other attributes the scanner might associate with the lexeme
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Do not confuse this reference to a token with the compressed-style tokens
    in an interpreter discussed previously. In this context, tokens are simply a variable-sized
    data structure that holds information associated with a lexeme for the interpreter/compiler.*'
  prefs: []
  type: TYPE_NORMAL
- en: When the scanner sees the character string `12345` in the source file, for example,
    it might identify the token’s class as a literal constant, the token’s second
    value as an integer typed constant, and the token’s attribute as the numeric equivalent
    of the string (that is, twelve thousand, three hundred, forty-five). [Figure 4-3](ch04.xhtml#ch4fig3)
    demonstrates what this token might look like in memory.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/04fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-3: A token for the lexeme "12345"*'
  prefs: []
  type: TYPE_NORMAL
- en: The token’s enumerated value is `345` (indicating an integer constant), the
    token class’s value is `5` (indicating a literal constant), the token’s attribute
    value is `12345` (the numeric form of the lexeme), and the lexeme string is `"12345"`
    as returned by the scanner. Different code sequences in the compiler can refer
    to this token data structure as appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: Strictly speaking, the lexical analysis phase is optional. A parser could work
    directly with the source file. However, tokenization makes the compilation process
    more efficient, because it allows the parser to deal with tokens as integer values
    rather than as string data. Because most CPUs can handle small integer values
    much more efficiently than string data, and because the parser has to refer to
    the token data multiple times during processing, lexical analysis saves considerable
    time during compilation. Generally, pure interpreters are the only language processors
    that rescan each token during parsing, and this is one major reason why they are
    so slow (compared to, say, an interpreter that stores the source file in a tokenized
    form to avoid constantly processing a pure-text source file).
  prefs: []
  type: TYPE_NORMAL
- en: '**4.4.2 Parsing (Syntax Analysis)**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The parser is the part of the compiler that is responsible for checking whether
    the source program is syntactically (and semantically) correct. If there’s an
    error in the source file, it’s usually the parser that discovers and reports it.
    The parser is also responsible for reorganizing the token stream (that is, the
    source code) into a more complex data structure that represents the meaning or
    semantics of the program. The scanner and parser generally process the source
    file in a linear fashion from the beginning to the end of the file, and the compiler
    usually reads the source file only once. Later phases, however, need to refer
    to the body of the source program in a more ad hoc way. By building up a data
    structure representation of the source code (often called an *abstract syntax
    tree*, or *AST*), the parser enables the code generation and optimization phases
    to easily reference different parts of the program.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 4-4](ch04.xhtml#ch4fig4) shows how a compiler might represent the expression
    `12345+6` using three nodes in an AST (`43` is the value for the addition operator
    and `7` is the subclass representing arithmetic operators).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/04fig04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-4: A portion of an abstract syntax tree*'
  prefs: []
  type: TYPE_NORMAL
- en: '**4.4.3 Intermediate Code Generation**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The intermediate code generation phase is responsible for translating the AST
    representation of the source file into a quasi–machine code form. There are two
    reasons compilers typically translate a program into an intermediate form rather
    than converting it directly to native machine code.
  prefs: []
  type: TYPE_NORMAL
- en: First, the compiler’s optimization phase can do certain types of optimizations,
    such as common subexpression elimination, much more easily on this intermediate
    form.
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, many compilers, known as *cross-compilers*, generate executable machine
    code for several different CPUs. By breaking the code generation phase into two
    pieces—the intermediate code generator and the native code generator—the compiler
    writer can move all the CPU-independent activities into the intermediate code
    generation phase and write this code only once. This simplifies the native code
    generation phase. That is, because the compiler needs only one intermediate code
    generation phase but may need separate native code generation phases for each
    CPU the compiler supports, moving as much of the CPU-independent code as possible
    into the intermediate code generator will reduce the size of the native code generators.
    For the same reason, the optimization phase is often broken into two components
    (refer back to [Figure 4-2](ch04.xhtml#ch4fig2)): a CPU-independent component
    (the part following the intermediate code generator) and a CPU-dependent component.'
  prefs: []
  type: TYPE_NORMAL
- en: Some language systems, such as Microsoft’s VB.NET and C#, actually emit the
    intermediate code as the output of the compiler (in the .NET system, Microsoft
    calls this code *Common Intermediate Language*, or *CIL*). Native code generation
    and optimization are actually handled by the Microsoft *Common Language Runtime
    (CLR)* system, which performs JIT compilation on the CIL code the .NET compilers
    produce.
  prefs: []
  type: TYPE_NORMAL
- en: '**4.4.4 Optimization**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The optimization phase, which follows intermediate code generation, translates
    the intermediate code into a more efficient form. This generally involves eliminating
    unnecessary entries from the AST. For example, the compiler’s optimizer might
    transform the following intermediate code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'to something like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: If there are no more references to `i` and `j`, the optimizer can eliminate
    all references to them. Indeed, if `k` is never used again, the optimizer can
    replace these two instructions with the single instruction `add 5 to m`. Note
    that this type of transformation is valid on nearly all CPUs. Therefore, this
    type of transformation/optimization is perfect for the first optimization phase.
  prefs: []
  type: TYPE_NORMAL
- en: '**4.4.4.1 The Problem with Optimization**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Transforming intermediate code “into a more efficient form” is not a well-defined
    process—what makes one form of a program more efficient than another? The primary
    definition of efficiency is that the program minimizes the use of some system
    resource, usually memory (space) or CPU cycles (speed). A compiler’s optimizer
    could manage other resources, but space and speed are the principal considerations
    for programmers. But even if we consider only these two facets of optimization,
    describing the “optimal” result is difficult. The problem is that optimizing for
    one goal (say, better performance) may create conflicts with another optimization
    goal (such as reduced memory usage). For this reason, the optimization process
    is usually a matter of compromise, where you make trade-offs and sacrifice certain
    subgoals (for example, running certain sections of the code a little slower) in
    order to create a reasonable result (like creating a program that doesn’t consume
    too much memory).
  prefs: []
  type: TYPE_NORMAL
- en: '**4.4.4.2 Optimization’s Effect on Compile Time**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: You might think that it’s possible to set a single goal (for example, highest
    possible performance) and optimize strictly for that. However, the compiler must
    also be capable of producing an executable result in a reasonable amount of time.
    The optimization process is an example of what complexity theory calls an *NP-complete
    problem*. These are problems that are, as far as we know, intractable; that is,
    you cannot produce a guaranteed correct result (for example, an optimal version
    of a program) without first computing all possibilities and choosing from among
    them. Unfortunately, the time generally required to solve an NP-complete problem
    increases exponentially with the size of the input, which in the case of compiler
    optimization means roughly the number of lines of source code.
  prefs: []
  type: TYPE_NORMAL
- en: This means that in the worst case, producing a truly optimal program would take
    longer than it was worth. Adding one line of source code could approximately *double*
    the amount of time it takes to compile and optimize the code. Adding two lines
    could *quadruple* the amount of time. In fact, a full guaranteed optimization
    of a modern application could take longer than the known lifetime of the universe.
  prefs: []
  type: TYPE_NORMAL
- en: For all but the smallest source files (a few dozen lines), a perfect optimizer
    would take far too long to be of any practical value (and such optimizers have
    been written; search online for “superoptimizers” for details). For this reason,
    compiler optimizers rarely produce a truly optimal program. They simply produce
    the best result they can, given the limited amount of CPU time the user is willing
    to allow for the process.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Languages that rely on JIT compilation (such as Java, C#, and VB.Net) move
    part of the optimization phase to runtime. Therefore, the optimizer’s performance
    has a direct impact on the application’s runtime. Because the JIT compiler system
    is running concurrently with the application, it cannot spend considerable time
    optimizing the code without having a huge impact on runtime. This is why languages
    such as Java and C#, even when ultimately compiled to low-level machine code,
    rarely perform as well as highly optimized code compiled by traditional languages
    such as C/C++ and Pascal.*'
  prefs: []
  type: TYPE_NORMAL
- en: Rather than trying all possibilities and choosing the best result, modern optimizers
    use heuristics and case-based algorithms to determine the transformations they
    will apply to the machine code they produce. You need to be aware of these techniques
    so you can write your HLL code in a manner that allows an optimizer to easily
    process it and produce better machine code.
  prefs: []
  type: TYPE_NORMAL
- en: '**4.4.4.3 Basic Blocks, Reducible Code, and Optimization**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Understanding how the compiler organizes the intermediate code (to output better
    machine code in later phases) is very important if you want to be able to help
    the optimizer do its job more efficiently. As control flows through the program,
    the optimizer keeps track of variable values in a process known as *data flow
    analysis (DFA)*. After careful DFA, a compiler can determine where a variable
    is uninitialized, when the variable contains certain values, when the program
    no longer uses the variable, and (just as importantly) when the compiler simply
    doesn’t know anything about the variable’s value. For example, consider the following
    Pascal code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'A good optimizer will replace this code with something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In fact, the compiler probably would not generate code for the last two statements;
    instead, it would substitute the value `0` for `i` and `6` for `path` in later
    references. If this seems impressive to you, note that some compilers can even
    track constant assignments and expressions through nested function calls and complex
    expressions.
  prefs: []
  type: TYPE_NORMAL
- en: Although a complete description of how a compiler analyzes data flow is beyond
    the scope of this book, you should have a basic understanding of the process,
    because a sloppily written program can thwart the compiler’s optimization abilities.
    Great code works synergistically with the compiler, not against it.
  prefs: []
  type: TYPE_NORMAL
- en: Some compilers can do some truly amazing things when it comes to optimizing
    high-level code. However, optimization is an inherently slow process. As noted
    earlier, it is an intractable problem. Fortunately, most programs don’t require
    full optimization. Even if it runs a little slower than the optimal program, a
    good approximation is an acceptable compromise when compared to intractable compilation
    times.
  prefs: []
  type: TYPE_NORMAL
- en: The major concession to compilation time that compilers make during optimization
    is that they search for only so many possible improvements to a section of code
    before they move on. Therefore, if your programming style confuses the compiler,
    it may not be able to generate an optimal (or even close to optimal) executable
    because it has too many possibilities to consider. The trick, then, is to learn
    how compilers optimize the source file so you can accommodate them.
  prefs: []
  type: TYPE_NORMAL
- en: 'To analyze data flow, compilers divide the source code into sequences known
    as *basic blocks*—machine instructions into and out of which there are no branches
    except at the beginning and end. For example, consider the following C code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This code snippet contains five basic blocks. Basic block 1 starts with the
    beginning of the source code. A basic block ends at the point where there is a
    jump into or out of the sequence of instructions. Basic block 1 ends at the call
    to the `f()` function. Basic block 2 starts with the statement following the call
    to the `f()` function, then ends at the beginning of the `if` statement because
    the `if` can transfer control to either of two locations. The `else` clause terminates
    basic block 3\. It also marks the beginning of basic block 4 because there is
    a jump (from the `if`’s `then` clause) to the first statement following the `else`
    clause. Basic block 4 ends not because the code transfers control somewhere else,
    but because there is a jump from basic block 2 to the first statement that begins
    basic block 5 (from the `if`’s `then` section). Basic block 5 ends with a call
    to the C `printf()` function.
  prefs: []
  type: TYPE_NORMAL
- en: The easiest way to determine where the basic blocks begin and end is to consider
    the assembly code that the compiler will generate. Wherever there is a conditional
    branch/jump, unconditional jump, or call instruction, a basic block will end.
    Note, however, that the basic block includes the instruction that transfers control
    to a new location. A new basic block begins immediately after the instruction
    that transfers control to a new location. Also note that the target label of any
    conditional branch, unconditional jump, or call instruction begins a basic block.
  prefs: []
  type: TYPE_NORMAL
- en: Basic blocks make it easy for the compiler to track what’s happening to variables
    and other program objects. As the compiler processes each statement, it can (symbolically)
    track the values that a variable will hold based upon their initial values and
    the computations on them within the basic block.
  prefs: []
  type: TYPE_NORMAL
- en: A problem occurs when the paths from two basic blocks join into a single code
    stream. For example, at the end of basic block 3 in the current example, the compiler
    could easily determine that the variable `j` contains zero because code in the
    basic block assigns the value `0` to `j` and then makes no other assignments to
    `j`. Similarly, at the end of basic block 3, the program knows that `j` contains
    the value `j0 + x0` (assuming `j0` represents the initial value of `j` upon entry
    into the basic block and `x0` represents the initial value of `x` upon entry into
    the block). But when the paths merge at the beginning of basic block 4, the compiler
    probably can’t determine whether `j` will contain zero or the value `j0 + x0`.
    This means the compiler has to note that `j`’s value could be either of two different
    values at this point.
  prefs: []
  type: TYPE_NORMAL
- en: While keeping track of two possible values that a variable might contain at
    a given point is easy for a decent optimizer, it’s not hard to imagine a situation
    where the compiler would have to keep track of many different possible values.
    In fact, if you have several `if` statements that the code executes sequentially,
    and each path through these `if` statements modifies a given variable, then the
    number of possible values for each variable doubles with each `if` statement.
    In other words, the number of possibilities increases exponentially with the number
    of `if` statements in a code sequence. At some point, the compiler cannot keep
    track of all the possible values a variable might contain, so it has to stop monitoring
    that information for the given variable. When this happens, there are fewer optimization
    possibilities that the compiler can consider.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, although loops, conditional statements, `switch/case` statements,
    and procedure/function calls can increase the number of possible paths through
    the code exponentially, in practice compilers have few problems with typical well-written
    programs. This is because as paths from basic blocks converge, programs often
    make new assignments to their variables (thereby eliminating the old values the
    compiler was tracking). Compilers generally assume that programs rarely assign
    a different value to a variable along every distinct path in the program, and
    their internal data structures reflect this. Keep in mind that if you violate
    this assumption, the compiler may lose track of variable values and generate inferior
    code as a result.
  prefs: []
  type: TYPE_NORMAL
- en: Poorly structured programs can create control flow paths that confuse the compiler,
    reducing the opportunities for optimization. Good programs produce *reducible
    flow graphs*, pictorial depictions of the control flow path. [Figure 4-5](ch04.xhtml#ch4fig5)
    is a flow graph for the previous code fragment.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/04fig05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-5: An example flow graph*'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, arrows connect the end of each basic block with the beginning
    of the basic block into which they transfer control. In this particular example,
    all of the arrows flow downward, but this isn’t always the case. Loops, for example,
    transfer control backward in the flow graph. As another example, consider the
    following Pascal code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 4-6](ch04.xhtml#ch4fig6) shows the flow graph for this simple code
    fragment.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/04fig06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-6: Flow graph for a while loop*'
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned, flow graphs in well-structured programs are *reducible*. Although
    a complete description of what constitutes a reducible flow graph is beyond the
    scope of this book, any program that consists only of structured control statements
    (`if`, `while`, `repeat..until`, and so on) and avoids `goto` statements will
    be reducible. This is an important point because compiler optimizers generally
    do a much better job when working on reducible programs. In contrast, programs
    that are not reducible tend to confuse them.
  prefs: []
  type: TYPE_NORMAL
- en: What makes reducible programs easier for optimizers to deal with is that their
    basic blocks can be collapsed in an outline fashion, with enclosing blocks inheriting
    properties (for example, which variables the block modifies) from the enclosed
    blocks. By processing the source file this way, the optimizer can deal with a
    small number of basic blocks rather than a large number of statements. This hierarchical
    approach to optimization is more efficient and allows the optimizer to maintain
    more information about the program’s state. Furthermore, the exponential time
    complexity of the optimization problem works for us in this case. By reducing
    the number of blocks the code has to deal with, you dramatically decrease the
    amount of work the optimizer must do. Again, the exact details of how the compiler
    achieves this are not important here. The takeaway is that making your programs
    reducible enables the optimizer to do its job more effectively. Attempts to “optimize”
    your code by sticking in lots of `goto` statements—to avoid duplicating code and
    executing unnecessary tests—may actually work against you. While you may save
    a few bytes or a few cycles in the immediate area you’re working on, the end result
    might confuse the compiler enough that it cannot optimize as well, causing an
    overall loss of efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: '**4.4.4.4 Common Compiler Optimizations**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[Chapter 12](ch12.xhtml#ch12) will provide complete definitions and examples
    of common compiler optimizations in programming contexts where compilers typically
    use them. But for now, here’s a quick preview of the basic types:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Constant folding**'
  prefs: []
  type: TYPE_NORMAL
- en: Constant folding computes the value of constant expressions or subexpressions
    at compile time rather than at runtime. See “Constant Folding” on [page 397](ch12.xhtml#page_397)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: '**Constant propagation**'
  prefs: []
  type: TYPE_NORMAL
- en: Constant propagation replaces a variable with a constant value if the compiler
    determines that the program assigned that constant to the variable earlier in
    the code. See “Constant Propagation” on [page 400](ch12.xhtml#page_400) for more
    information.
  prefs: []
  type: TYPE_NORMAL
- en: '**Dead code elimination**'
  prefs: []
  type: TYPE_NORMAL
- en: Dead code elimination removes the object code associated with a particular source
    code statement when the program will never use the result of that statement, or
    when a conditional block will never be `true`. See “Dead Code Elimination” on
    [page 404](ch12.xhtml#page_404) for more information.
  prefs: []
  type: TYPE_NORMAL
- en: '**Common subexpression elimination**'
  prefs: []
  type: TYPE_NORMAL
- en: Frequently, part of an expression will appear elsewhere in the current function;
    this is known as a *subexpression*. If the values of the variables in a subexpression
    haven’t changed, the program does not need to recompute them everywhere the subexpression
    appears. The program can simply save the subexpression’s value on the first evaluation
    and then use it for every other occurrence of the subexpression. See “Common Subexpression
    Elimination” on [page 410](ch12.xhtml#page_410) for more information.
  prefs: []
  type: TYPE_NORMAL
- en: '**Strength reduction**'
  prefs: []
  type: TYPE_NORMAL
- en: Often, the CPU can directly compute a value using a different operator than
    the source code specifies. For example, a `shift` instruction can implement multiplication
    or division by a constant that is a power of 2, and a bitwise `and` instruction
    can compute certain modulo (remainder) operations (the `shift` and `and` instructions
    generally execute much faster than the multiply and divide instructions). Most
    compiler optimizers are good at recognizing such operations and replacing the
    more expensive computation with a less expensive sequence of machine instructions.
    See “Strength Reduction” on [page 417](ch12.xhtml#page_417) for more information.
  prefs: []
  type: TYPE_NORMAL
- en: '**Induction**'
  prefs: []
  type: TYPE_NORMAL
- en: In many expressions, particularly those appearing within a loop, the value of
    one variable in the expression is completely dependent upon some other variable.
    Frequently, the compiler can eliminate the computation of the new value or merge
    the two computations into one for the duration of that loop. See “Induction” on
    [page 422](ch12.xhtml#page_422) for more information.
  prefs: []
  type: TYPE_NORMAL
- en: '**Loop invariants**'
  prefs: []
  type: TYPE_NORMAL
- en: The optimizations so far have all been techniques a compiler can use to improve
    code that is already well written. Handling loop invariants, by contrast, is a
    compiler optimization for fixing bad code. A *loop invariant* is an expression
    that does not change on each iteration of some loop. An optimizer can compute
    the result of such a calculation just once, outside the loop, and then use the
    computed value within the loop’s body. Many optimizers are smart enough to discover
    loop invariant calculations and can use *code motion* to move the invariant calculation
    outside the loop. See “Loop Invariants” on [page 427](ch12.xhtml#page_427) for
    more information.
  prefs: []
  type: TYPE_NORMAL
- en: Good compilers can perform many other optimizations, but these are the standard
    optimizations that any decent compiler should be able to do.
  prefs: []
  type: TYPE_NORMAL
- en: '**4.4.4.5 Compiler Optimization Control**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: By default, most compilers do very little or no optimization unless you explicitly
    tell them to. This might seem counterintuitive; after all, we generally want compilers
    to produce the best possible code for us. However, there are many definitions
    of “optimal,” and no single compiler output is going to satisfy every possible
    one.
  prefs: []
  type: TYPE_NORMAL
- en: 'You might argue that some sort of optimization, even if it’s not the particular
    type you’re interested in, is better than none at all. However, there are a few
    reasons why no optimization is a compiler’s default state:'
  prefs: []
  type: TYPE_NORMAL
- en: Optimization is a slow process. You get quicker turnaround times on compiles
    when you have the optimizer turned off. This can be a big help during rapid edit-compile-test
    cycles.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many debuggers don’t work properly with optimized code, and you have to turn
    off optimization in order to use a debugger on your application (this also makes
    analyzing the compiler output much easier).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most compiler defects occur in the optimizer. By emitting unoptimized code,
    you’re less likely to encounter defects in the compiler (then again, the compiler’s
    author is less likely to be notified about defects in the compiler, too).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most compilers provide command-line options that let you control the types of
    optimization the compiler performs. Early C compilers under Unix used command-line
    arguments like `-O`, `-O1`, and `-O2`. Many later compilers (C and otherwise)
    have adopted this strategy, if not exactly the same command-line options.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re wondering why a compiler might offer multiple options to control optimization
    rather than just a single option (optimization or no optimization), remember that
    “optimal” means different things to different people. Some people might want code
    that is optimized for space; others might want code that is optimized for speed
    (and those two optimizations could be mutually exclusive in a given situation).
    Some people might want to optimize their files but don’t want the compiler to
    take forever to process them, so they’d be willing to compromise with a small
    set of fast optimizations. Others might want to control optimization for a specific
    member of a CPU family (such as the Core i9 processor in the 80x86 family). Furthermore,
    some optimizations are “safe” (that is, they always produce correct code) only
    if the program is written in a certain way. You certainly don’t want to enable
    such optimizations unless the programmer guarantees that they’ve written their
    code accordingly. Finally, for programmers who are writing their HLL code carefully,
    some optimizations the compiler performs may actually produce *inferior* code,
    in which case the ability to specify optimizations is very handy. For these reasons
    and more, most modern compilers provide considerable flexibility over the types
    of optimizations they perform.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the Microsoft Visual C++ compiler. It provides the following command-line
    options to control optimization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: GCC has a comparable—though much longer—list, which you can view by specifying
    `-v --help` on the GCC command line. Most of the individual optimization flags
    begin with `-f`. You can also use `-On`, where n is a single digit integer value,
    to specify different levels of optimization. Take care when using `-O3` (or higher),
    as doing so may perform some unsafe optimizations in certain cases.
  prefs: []
  type: TYPE_NORMAL
- en: '**4.4.5 Compiler Benchmarking**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One real-world constraint on our ability to produce great code is that different
    compilers provide a wildly varying set of optimizations. Even the same optimizations
    performed by two different compilers can differ greatly in effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, several websites have benchmarked various compilers. (One good
    example is Willus.com.) Simply search online for a topic like “compiler benchmarks”
    or “compiler comparisons” and have fun.
  prefs: []
  type: TYPE_NORMAL
- en: '**4.4.6 Native Code Generation**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The native code generation phase is responsible for translating the intermediate
    code into machine code for the target CPU. An 80x86 native code generator, for
    example, might translate the intermediate code sequence given previously into
    something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The second optimization phase, which takes place after native code generation,
    handles machine idiosyncrasies that don’t exist on all machines. For example,
    an optimizer for a Pentium II processor might replace an instruction of the form
    `add(1, eax);` with the instruction `inc(eax);`. Optimizers for later CPUs might
    do just the opposite. Optimizers for certain 80x86 processors might arrange the
    sequence of instructions one way to maximize parallel execution of the instructions
    in a superscalar CPU, while an optimizer targeting a different (80x86) CPU might
    arrange the instructions differently.
  prefs: []
  type: TYPE_NORMAL
- en: '**4.5 Compiler Output**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The previous section stated that compilers typically produce machine code as
    their output. Strictly speaking, this is neither necessary nor even that common.
    Most compiler output is not code that a given CPU can directly execute. Some compilers
    emit assembly language source code, which requires further processing by an assembler
    prior to execution. Other compilers produce an object file, which is similar to
    executable code but is not directly executable. Still other compilers actually
    produce source code output that requires further processing by a different HLL
    compiler. I’ll discuss these different output formats and their advantages and
    disadvantages in this section.
  prefs: []
  type: TYPE_NORMAL
- en: '**4.5.1 Emitting HLL Code as Compiler Output**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Some compilers actually emit output that is source code for a different high-level
    programming language (see [Figure 4-7](ch04.xhtml#ch4fig7)). For example, many
    compilers (including the original C++ compiler) emit C code as their output. Indeed,
    compiler writers who emit some high-level source code from their compiler frequently
    choose the C programming language.
  prefs: []
  type: TYPE_NORMAL
- en: Emitting HLL code as compiler output offers several advantages. The output is
    human-readable and generally easy to verify. The HLL code emitted is often portable
    across various platforms; for example, if a compiler emits C code, you can usually
    compile that output on several different machines because C compilers exist for
    most platforms. Finally, by emitting HLL code, a translator can rely on the optimizer
    of the target language’s compiler, thereby saving the effort of writing its own
    optimizer. In other words, emitting HLL code allows a compiler writer to create
    a less complex code generator module and rely on the robustness of some other
    compiler for the most complex part of the compilation process.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/04fig07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-7: Emission of HLL code by a compiler*'
  prefs: []
  type: TYPE_NORMAL
- en: Emitting HLL code has several disadvantages, too. First and foremost, this approach
    usually takes more processing time than directly generating executable code. To
    produce an executable file, a second, otherwise unnecessary, compiler might be
    needed. Worse, the output of that second compiler might need to be processed further
    by another compiler or assembler, exacerbating the problem. Another disadvantage
    is that in HLL code it’s difficult to embed debugging information that a debugger
    program can use. Perhaps the most fundamental problem with this approach, however,
    is that HLLs are usually an abstraction of the underlying machine. Therefore,
    it could be quite difficult for a compiler to emit statements in an HLL that efficiently
    map to low-level machine code.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, compilers that emit HLL statements as their output are translating
    a *very high-level language (VHLL)* into a lower-level language. For example,
    C is often considered to be a fairly low-level HLL, which is one reason why it’s
    a popular output format for many compilers. Projects that have attempted to create
    a special, portable, low-level language specifically for this purpose have never
    been enormously popular. Check out any of the “C- -” projects on the internet
    for examples of such systems.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to write efficient code by analyzing compiler output, you’ll probably
    find it more difficult to work with compilers that output HLL code. With a standard
    compiler, all you have to learn is the particular machine code statements that
    your compiler produces. However, when a compiler emits HLL statements as its output,
    learning to write great code with that compiler is more difficult. You need to
    understand both how the main language emits the HLL statements and how the second
    compiler translates the code into machine code.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, compilers that produce HLL code as their output are either experimental
    compilers for VHLLs, or compilers that attempt to translate legacy code in an
    older language to a more modern language (for example, FORTRAN to C). As a result,
    expecting those compilers to emit efficient code is generally asking too much.
    Thus, you’d probably be wise to avoid a compiler that emits HLL statements. A
    compiler that directly generates machine code (or assembly language code) is more
    likely to produce smaller and faster-running executables.
  prefs: []
  type: TYPE_NORMAL
- en: '**4.5.2 Emitting Assembly Language as Compiler Output**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Many compilers will emit human-readable assembly language source files rather
    than binary machine code files (see [Figure 4-8](ch04.xhtml#ch4fig8)). Probably
    the most famous example is the FSF/GNU GCC compiler suite, which emits assembly
    language output for the FSF/GNU assembler Gas. Like compilers that emit HLL source
    code, emitting assembly language has some advantages and disadvantages.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/04fig08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-8: Emission of assembly code by a compiler*'
  prefs: []
  type: TYPE_NORMAL
- en: The principal disadvantages to emitting assembly code are similar to the downsides
    of emitting HLL source output. First, you have to run a second language translator
    (namely the assembler) to produce the actual object code for execution. Second,
    some assemblers may not allow the embedding of debugging metadata that allows
    a debugger to work with the original source code (though many assemblers do support
    this capability). These two disadvantages turn out to be minimal if a compiler
    emits code for an appropriate assembler. For example, Gas is very fast and supports
    the insertion of debug information for use by source-level debuggers. Therefore,
    the FSF/GNU compilers don’t suffer as a result of emitting Gas output.
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of assembly language output, particularly for our purposes, is
    that it’s easy to read the compiler’s output and determine which machine instructions
    the compiler emits. Indeed, this book uses this compiler facility to analyze compiler
    output. Emitting assembly code frees the compiler writer from having to worry
    about several different object code output formats—the underlying assembler handles
    that—which allows the compiler writer to create a more portable compiler. True,
    the assembler has to be capable of generating code for different operating systems,
    but you only need to repeat this exercise once for each object file format, rather
    than once for each format multiplied by the number of compilers you write. The
    FSF/GNU compiler suite has taken good advantage of the Unix philosophy of using
    small tools that chain together to accomplish larger, more complicated tasks—that
    is, minimize redundancy.
  prefs: []
  type: TYPE_NORMAL
- en: Another advantage of compilers that can emit assembly language output is that
    they generally allow you to embed *inline assembly language* statements in the
    HLL code. This lets you insert a few machine instructions directly into time-critical
    sections of your code without the hassle of having to create a separate assembly
    language program and link its output to your HLL program.
  prefs: []
  type: TYPE_NORMAL
- en: '**4.5.3 Emitting Object Files as Compiler Output**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Most compilers translate the source language into an object file format, an
    intermediate file format that contains machine instructions and binary runtime
    data along with certain metadata. This metadata allows a linker/loader program
    to combine various object modules to produce a complete executable. This in turn
    allows programmers to link *library modules* and other object modules that they’ve
    written and compiled separately from their main application module.
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of object file output is that you don’t need a separate compiler
    or assembler to convert the compiler’s output to object code form, which saves
    a little time during compilation. Note, however, that a linker program must still
    process the object file output, which consumes a little time after compilation.
    Nevertheless, linkers are generally quite fast, so it’s usually more cost-effective
    to compile a single module and link it with several previously compiled modules
    than it is to compile all the modules together to form an executable file.
  prefs: []
  type: TYPE_NORMAL
- en: Object modules are binary files and do not contain human-readable data, so it’s
    a bit more difficult to analyze compiler output in this format than in the others
    we’ve discussed. Fortunately, there are utility programs that will disassemble
    the output of an object module into a human-readable form. The result isn’t as
    easy to read as straight assembly compiler output, but you can still do a reasonably
    good job.
  prefs: []
  type: TYPE_NORMAL
- en: Because object files are challenging to analyze, many compiler writers provide
    an option to emit assembly code instead of object code. This handy feature makes
    analysis much easier, so we’ll use it with various compilers throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*The section “Object File Formats” on [page 71](ch04.xhtml#page_71) provides
    a detailed look at the elements of an object file, focusing on COFF (Common Object
    File Format).*'
  prefs: []
  type: TYPE_NORMAL
- en: '**4.5.4 Emitting Executable Files as Compiler Output**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Some compilers directly emit an executable output file. These compilers are
    often very fast, producing almost instantaneous turnaround during the edit-compile-run-test-debug
    cycle. Unfortunately, their output is often the most difficult to read and analyze,
    requiring the use of a debugger or disassembler program and a lot of manual work.
    Nevertheless, the fast turnaround makes these compilers popular, so later in this
    book, we’ll look at how to analyze the executable files they produce.
  prefs: []
  type: TYPE_NORMAL
- en: '**4.6 Object File Formats**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As previously noted, object files are among the most popular forms of compiler
    output. Even though it is possible to create a proprietary object file format—one
    that only a single compiler and its associated tools can use—most compilers emit
    code using one or more standardized object file formats. This allows different
    compilers to share the same set of object file utilities, including linkers, librarians,
    dump utilities, and disassemblers. Examples of common object file formats include:
    OMF (Object Module Format), COFF (Common Object File Format), PE/COFF (Microsoft’s
    Portable Executable variant on COFF), and ELF (Executable and Linkable Format).
    There are several variants of these file formats, as well as many altogether different
    object file formats.'
  prefs: []
  type: TYPE_NORMAL
- en: Most programmers understand that object files represent the machine code that
    an application executes, but they often don’t realize the impact of the object
    file’s organization on their application’s performance and size. Although you
    don’t need to have detailed knowledge of an object file’s internal representation
    to write great code, having a basic understanding will help you organize your
    source files to better take advantage of the way compilers and assemblers generate
    code for your applications.
  prefs: []
  type: TYPE_NORMAL
- en: An object file usually begins with a header that comprises the first few bytes
    of the file. This header contains certain *signature information* that identifies
    the file as a valid object file, along with several other values that define the
    location of various data structures in the file. Beyond the header, an object
    file is usually divided into several sections, each containing application data,
    machine instructions, symbol table entries, relocation data, and other metadata
    (data about the program). In some cases, the actual code and data represent only
    a small part of the entire object code file.
  prefs: []
  type: TYPE_NORMAL
- en: To get a feeling for how object files are structured, it’s worthwhile to look
    at a specific object file format in detail. I’ll use COFF in the following discussion
    because most object file formats (for example, ELF and PE/COFF) are based on,
    or very similar to, COFF. The basic layout of a COFF file is shown in [Figure
    4-9](ch04.xhtml#ch4fig9), after which I’ll describe each section in turn.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/04fig09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-9: Layout of a COFF file*'
  prefs: []
  type: TYPE_NORMAL
- en: '**4.6.1 The COFF File Header**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'At the beginning of every COFF file is a *COFF file header*. Here are the definitions
    that Microsoft Windows and Linux use for the COFF header structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The Linux *coff.h* header file uses traditional Unix names for these fields;
    the Microsoft *winnt.h* header file uses (arguably) more readable names. Here’s
    a summary of each field in the header, with Unix names to the left of the slash
    and Microsoft equivalents to the right:'
  prefs: []
  type: TYPE_NORMAL
- en: f_magic/Machine
  prefs: []
  type: TYPE_NORMAL
- en: Identifies the system for which this COFF file was created. In the original
    Unix definition, this value identified the particular Unix port for which the
    code was created. Today’s operating systems define this value somewhat differently,
    but the bottom line is that this value is a signature that specifies whether the
    COFF file contains data or machine instructions that are appropriate for the current
    operating system and CPU.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 4-1](ch04.xhtml#ch4tab1) provides the encodings for the `f_magic/Machine`
    field.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 4-1:** `f_magic/Machine` Field Encoding'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Value** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| 0x14c | Intel 386 |'
  prefs: []
  type: TYPE_TB
- en: '| 0x8664 | x86-64 |'
  prefs: []
  type: TYPE_TB
- en: '| 0x162 | MIPS R3000 |'
  prefs: []
  type: TYPE_TB
- en: '| 0x168 | MIPS R10000 |'
  prefs: []
  type: TYPE_TB
- en: '| 0x169 | MIPS little endian WCI v2 |'
  prefs: []
  type: TYPE_TB
- en: '| 0x183 | old Alpha AXP |'
  prefs: []
  type: TYPE_TB
- en: '| 0x184 | Alpha AXP |'
  prefs: []
  type: TYPE_TB
- en: '| 0x1a2 | Hitachi SH3 |'
  prefs: []
  type: TYPE_TB
- en: '| 0x1a3 | Hitachi SH3 DSP |'
  prefs: []
  type: TYPE_TB
- en: '| 0x1a6 | Hitachi SH4 |'
  prefs: []
  type: TYPE_TB
- en: '| 0x1a8 | Hitachi SH5 |'
  prefs: []
  type: TYPE_TB
- en: '| 0x1c0 | ARM little endian |'
  prefs: []
  type: TYPE_TB
- en: '| 0x1c2 | Thumb |'
  prefs: []
  type: TYPE_TB
- en: '| 0x1c4 | ARMv7 |'
  prefs: []
  type: TYPE_TB
- en: '| 0x1d3 | Matsushita AM33 |'
  prefs: []
  type: TYPE_TB
- en: '| 0x1f0 | PowerPC little endian |'
  prefs: []
  type: TYPE_TB
- en: '| 0x1f1 | PowerPC with floating-point support |'
  prefs: []
  type: TYPE_TB
- en: '| 0x200 | Intel IA64 |'
  prefs: []
  type: TYPE_TB
- en: '| 0x266 | MIPS16 |'
  prefs: []
  type: TYPE_TB
- en: '| 0x268 | Motorola 68000 series |'
  prefs: []
  type: TYPE_TB
- en: '| 0x284 | Alpha AXP 64-bit |'
  prefs: []
  type: TYPE_TB
- en: '| 0x366 | MIPS with FPU |'
  prefs: []
  type: TYPE_TB
- en: '| 0x466 | MIPS16 with FPU |'
  prefs: []
  type: TYPE_TB
- en: '| 0xebc | EFI bytecode |'
  prefs: []
  type: TYPE_TB
- en: '| 0x8664 | AMD AMD64 |'
  prefs: []
  type: TYPE_TB
- en: '| 0x9041 | Mitsubishi M32R little endian |'
  prefs: []
  type: TYPE_TB
- en: '| 0xaa64 | ARM64 little endian |'
  prefs: []
  type: TYPE_TB
- en: '| 0xc0ee | CLR pure MSIL |'
  prefs: []
  type: TYPE_TB
- en: f_nscns/NumberOfSections
  prefs: []
  type: TYPE_NORMAL
- en: Specifies how many segments (sections) are present in the COFF file. A linker
    program can iterate through a set of section headers (described a little later)
    using this value.
  prefs: []
  type: TYPE_NORMAL
- en: f_timdat/TimeDateStamp
  prefs: []
  type: TYPE_NORMAL
- en: Contains a Unix-style timestamp (number of seconds since January 1, 1970) value
    specifying the file’s creation date and time.
  prefs: []
  type: TYPE_NORMAL
- en: f_symptr/PointerToSymbolTable
  prefs: []
  type: TYPE_NORMAL
- en: Contains a file offset value (that is, the number of bytes from the beginning
    of the file) that specifies where the *symbol table* begins in the file. The symbol
    table is a data structure that specifies the names and other information about
    all external, global, and other symbols used by the code in the COFF file. Linkers
    use the symbol table to resolve external references. This symbol table information
    may also appear in the final executable file for use by a symbolic debugger.
  prefs: []
  type: TYPE_NORMAL
- en: f_nsyms/NumberOfSymbols
  prefs: []
  type: TYPE_NORMAL
- en: The number of entries in the symbol table.
  prefs: []
  type: TYPE_NORMAL
- en: f_opthdr/SizeOfOptionalHeader
  prefs: []
  type: TYPE_NORMAL
- en: Specifies the size of the optional header that immediately follows the file
    header (that is, the first byte of the optional header immediately follows the
    `f_flags/Characteristics` field in the file header structure). A linker or other
    object code manipulation program would use the value in this field to determine
    where the optional header ends and the section headers begin in the file. The
    section headers immediately follow the optional header, but the optional header’s
    size isn’t fixed. Different implementations of a COFF file can have different
    optional header structures. If the optional header is not present in a COFF file,
    the `f_opthdr/SizeOfOptionalHeader` field will contain zero, and the first section
    header will immediately follow the file header.
  prefs: []
  type: TYPE_NORMAL
- en: f_flags/Characteristics
  prefs: []
  type: TYPE_NORMAL
- en: A small bitmap that specifies certain Boolean flags, such as whether the file
    is executable, whether it contains symbol information, and whether it contains
    line number information (for use by debuggers).
  prefs: []
  type: TYPE_NORMAL
- en: '**4.6.2 The COFF Optional Header**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The COFF optional header contains information pertinent to executable files.
    This header may not be present if the file contains object code that is not executable
    (because of unresolved references). Note, however, that this optional header is
    always present in Linux COFF and Microsoft PE/COFF files, even when the file is
    not executable. The Windows and Linux structures for this optional file header
    take the following forms in C.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The first thing to notice is that these structures are not identical. The Microsoft
    version has considerably more information than the Linux version. The `f_opthdr/SizeOfOptionalHeader`
    field exists in the file header to determine the actual size of the optional header.
  prefs: []
  type: TYPE_NORMAL
- en: magic/Magic
  prefs: []
  type: TYPE_NORMAL
- en: Provides yet another signature value for the COFF file. This signature value
    identifies the file type (that is, COFF) rather than the system under which it
    was created. Linkers use the value of this field to determine if they are truly
    operating on a COFF file (instead of some arbitrary file that would confuse the
    linker).
  prefs: []
  type: TYPE_NORMAL
- en: vstamp/MajorLinkerVersion/MinorLinkerVersion
  prefs: []
  type: TYPE_NORMAL
- en: Specifies the version number of the COFF format so that a linker written for
    an older version of the file format won’t try to process files intended for newer
    linkers.
  prefs: []
  type: TYPE_NORMAL
- en: tsize/SizeOfCode
  prefs: []
  type: TYPE_NORMAL
- en: Attempts to specify the size of the code section found in the file. If the COFF
    file contains more than one code section, the value of this field is undefined,
    although it usually specifies the size of the first code/text section in the COFF
    file.
  prefs: []
  type: TYPE_NORMAL
- en: dsize/SizeOfInitializedData
  prefs: []
  type: TYPE_NORMAL
- en: Specifies the size of the data segment appearing in this COFF file. Once again,
    this field is undefined if there are two or more data sections in the file. Usually,
    this field specifies the size of the first data section if there are multiple
    data sections.
  prefs: []
  type: TYPE_NORMAL
- en: bsize/SizeOfUninitializedData
  prefs: []
  type: TYPE_NORMAL
- en: Specifies the size of the *block started by symbol (BSS)* section—the uninitialized
    data section—in the COFF file. As for the text and data sections, this field is
    undefined if there are two or more BSS sections; in such cases this field usually
    specifies the size of the first BSS section in the file.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*See “Pages, Segments, and File Size” on [page 81](ch04.xhtml#page_81) for
    more on BSS sections.*'
  prefs: []
  type: TYPE_NORMAL
- en: entry/AddressOfEntryPoint
  prefs: []
  type: TYPE_NORMAL
- en: Contains the starting address of the executable program. Like other pointers
    in the COFF file header, this field is actually an offset into the file; it is
    not an actual memory address.
  prefs: []
  type: TYPE_NORMAL
- en: text_start/BaseOfCode
  prefs: []
  type: TYPE_NORMAL
- en: Specifies a file offset into the COFF file where the code section begins. If
    there are two or more code sections, this field is undefined, but it generally
    specifies the offset to the first code section in the COFF file.
  prefs: []
  type: TYPE_NORMAL
- en: data_start/BaseOfData
  prefs: []
  type: TYPE_NORMAL
- en: Specifies a file offset into the COFF file where the data section begins. If
    there are two or more data sections, this field is undefined, but it generally
    specifies the offset to the first data section in the COFF file.
  prefs: []
  type: TYPE_NORMAL
- en: There is no need for a `bss_start/StartOfUninitializedData` field. The COFF
    file format assumes that the operating system’s program loader will automatically
    allocate storage for a BSS section when the program loads into memory. There is
    no need to consume space in the COFF file for uninitialized data (however, “Executable
    File Formats” on [page 80](ch04.xhtml#page_80) describes how some compilers actually
    merge BSS and DATA sections together for performance reasons).
  prefs: []
  type: TYPE_NORMAL
- en: The optional file header structure is actually a throwback to the *a.out* format,
    an older object file format used in Unix systems. This is why it doesn’t handle
    multiple text/code and data sections, even though COFF allows them.
  prefs: []
  type: TYPE_NORMAL
- en: The remaining fields in the Windows variant of the optional header hold values
    that Windows linkers allow programmers to specify. While their purposes will likely
    be clear to anyone who has manually run Microsoft’s linker from a command line,
    those are not important here. What is important is that COFF does not require
    a specific data structure for the optional header. Different implementations of
    COFF (such as Microsoft’s) may freely extend the definition of the optional header.
  prefs: []
  type: TYPE_NORMAL
- en: '**4.6.3 COFF Section Headers**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The section headers follow the optional header in a COFF file. Unlike the file
    and optional headers, a COFF file may contain multiple section headers. The `f_nscns/NumberOfSections`
    field in the file header specifies the exact number of section headers (and, therefore,
    sections) found in the COFF file. Keep in mind that the first section header does
    not begin at a fixed offset in the file. Because the optional header’s size is
    variable (and, in fact, could even be 0 if it isn’t present), you have to add
    the `f_opthdr/SizeOfOptionalHeader` field in the file header to the size of the
    file header to get the starting offset of the first section header. Section headers
    are a fixed size, so once you have the address of the first section header you
    can easily compute the address of any other by multiplying the desired section
    header number by the section header size and adding the result to the base offset
    of the first section header.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the C struct definitions for Windows and Linux section headers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: If you inspect these two structures closely, you’ll find that they are roughly
    equivalent (the only structural difference is that Windows overloads the physical
    address field, which in Linux is always equivalent to the `VirtualAddress` field,
    to hold a `VirtualSize` field).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a summary of each field:'
  prefs: []
  type: TYPE_NORMAL
- en: s_name/Name
  prefs: []
  type: TYPE_NORMAL
- en: Specifies the name of the section. As is apparent in the Linux definition, this
    field is limited to eight characters and, accordingly, section names will be a
    maximum of eight characters long. (Usually, if a source file specifies a longer
    name, the compiler/assembler will truncate it to 8 characters when creating the
    COFF file.) If the section name is exactly eight characters long, those eight
    characters will consume all 8 bytes of this field and there will be no zero-terminating
    byte. If the section name is shorter than eight characters, a zero-terminating
    byte will follow the name. The value of this field is often something like `.text`,
    `CODE`, `.data`, or `DATA`. Note, however, that the name does not define the segment’s
    type. You could create a code/text section and name it `DATA`; you could also
    create a data section and name it `.text` or `CODE`. The `s_flags/Characteristics`
    field determines the actual type of this section.
  prefs: []
  type: TYPE_NORMAL
- en: s_paddr/PhysicalAddress/VirtualSize
  prefs: []
  type: TYPE_NORMAL
- en: Not used by most tools. Under Unix-like operating systems (such as Linux), this
    field is usually set to the same value as the `VirtualAddress` field. Different
    Windows tools set this field to different values (including zero); the linker/loader
    seems to ignore whatever value appears here.
  prefs: []
  type: TYPE_NORMAL
- en: s_vaddr/VirtualAddress
  prefs: []
  type: TYPE_NORMAL
- en: Specifies the section’s loading address in memory (that is, its virtual memory
    address). Note that this is a runtime memory address, not an offset into the file.
    The program loader uses this value to determine where to load the section into
    memory.
  prefs: []
  type: TYPE_NORMAL
- en: s_size/SizeOfRawData
  prefs: []
  type: TYPE_NORMAL
- en: Specifies the size, in bytes, of the section.
  prefs: []
  type: TYPE_NORMAL
- en: s_scnptr/PointerToRawData
  prefs: []
  type: TYPE_NORMAL
- en: Provides the file offset to the start of the section’s data in the COFF file.
  prefs: []
  type: TYPE_NORMAL
- en: s_relptr/PointerToRelocations
  prefs: []
  type: TYPE_NORMAL
- en: Provides a file offset to the relocation list for this particular section.
  prefs: []
  type: TYPE_NORMAL
- en: s_lnnoptr/PointerToLinenumbers
  prefs: []
  type: TYPE_NORMAL
- en: Contains a file offset to the line number records for the current section.
  prefs: []
  type: TYPE_NORMAL
- en: s_nreloc/NumberOfRelocations
  prefs: []
  type: TYPE_NORMAL
- en: Specifies the number of *relocation entries* found at that file offset. Relocation
    entries are small structures that provide file offsets to address data in the
    section’s data area that must be patched when the file is loaded into memory.
    We won’t discuss these relocation entries in this book, but if you’re interested
    in more details, see the references at the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: s_nlnno/NumberOfLinenumbers
  prefs: []
  type: TYPE_NORMAL
- en: Specifies how many line number records can be found at that offset. Line number
    information is used by debuggers and is beyond the scope of this chapter. Again,
    see the references at the end of this chapter if you’re interested in more information
    about the line number entries.
  prefs: []
  type: TYPE_NORMAL
- en: s_flags/Characteristics
  prefs: []
  type: TYPE_NORMAL
- en: A bitmap that specifies the characteristics of this section. In particular,
    this field will tell you whether the section requires relocation, whether it contains
    code, whether it is read-only, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '**4.6.4 COFF Sections**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The section headers provide a directory that describes the actual data and code
    found in the object file. The `s_scnptr/PointerToRawData` field contains a file
    offset to where the raw binary data or code is sitting in the file, and the `s_size/SizeOfRawData`
    field specifies the length of the section’s data. Due to relocation requirements,
    the data actually appearing in the section block may not be an exact representation
    of the data that the operating system loads into memory. This is because many
    instruction operand addresses and pointer values appearing in the section may
    need to be *patched* to relocate the file based on where the OS loads it into
    memory. The relocation list (which is separate from the section’s data) contains
    offsets into the section where the OS must patch the relocatable addresses. The
    OS performs this patching when loading the section’s data from disk.
  prefs: []
  type: TYPE_NORMAL
- en: Although the bytes in a COFF section may not be an exact representation of the
    data in memory at runtime, the COFF format requires that all of the bytes in the
    section *map* to the corresponding address in memory. This allows the loader to
    copy the section’s data directly from the file into sequential memory locations.
    The relocation operation never inserts or deletes bytes in a section; it only
    changes the values of certain bytes in the section. This requirement helps simplify
    the system loader and improves application performance because the operating system
    doesn’t have to move large blocks of memory around when loading the application
    into memory. The drawback to this scheme is that the COFF format misses the opportunity
    to compress redundant data appearing in the section’s data area. The COFF designers
    felt it was more important to emphasize performance over space in their design.
  prefs: []
  type: TYPE_NORMAL
- en: '**4.6.5 The Relocation Section**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The relocation section in the COFF file contains the offsets to the pointers
    in the COFF sections that must be relocated when the system loads those sections’
    code or data into memory.
  prefs: []
  type: TYPE_NORMAL
- en: '**4.6.6 Debugging and Symbolic Information**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The last three sections shown in [Figure 4-9](ch04.xhtml#ch4fig9) contain information
    that debuggers and linkers use. One section contains line number information that
    a debugger uses to correlate lines of source code with the executable machine
    code instructions. The symbol table and string table sections hold the public
    and external symbols for the COFF file. Linkers use this information to resolve
    external references between object modules; debuggers use this information to
    display symbolic variable and function names during debugging.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*This book doesn’t provide a complete description of the COFF file format,
    but you’ll definitely want to dig deeper into it and other object code formats
    (ELF, MACH-O, OMF, and so on) if you’re interested in writing applications such
    as assemblers, compilers, and linkers. To study this area further, see the references
    at the end of this chapter.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**4.7 Executable File Formats**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most operating systems use a special file format for executable files. Often,
    the executable file format is similar to the object file format, the principal
    difference being that there are usually no unresolved external references in the
    executable file.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to machine code and binary data, executable files contain other
    metadata, including debugging information, linkage information for dynamically
    linked libraries, and details about how the operating system should load different
    sections of the file into memory. Depending on the CPU and OS, the executable
    files may also contain relocation information so that the OS can patch absolute
    addresses when it loads the file into memory. Object code files contain the same
    information, so it’s no surprise that the executable file formats used by many
    operating systems are similar to their object file formats.
  prefs: []
  type: TYPE_NORMAL
- en: The Executable and Linkable Format (ELF), employed by Linux, QNX, and other
    Unix-like operating systems, is very typical of a combined object file format
    and executable format. Indeed, the name of the format suggests its dual nature.
    As another example, Microsoft’s PE file format is a straightforward variant of
    the COFF format. The similarity between the object and executable file formats
    allows the OS designer to share code between the loader (responsible for executing
    the program) and linker applications. Given this similarity, there’s little reason
    to discuss the specific data structures found in an executable file, as doing
    so would largely repeat the information from the previous sections.
  prefs: []
  type: TYPE_NORMAL
- en: However, there’s one very practical difference in the layout of these two types
    of files worth mentioning. Object files are usually designed to be as small as
    possible, while executable files are usually designed to load into memory as fast
    as possible, even if this means that they’re larger than absolutely necessary.
    It may seem paradoxical that a larger file could load into memory faster than
    a smaller file; however, the OS might load only a small part of the executable
    file into memory at one time if it supports virtual memory. As we’ll discuss next,
    a well-designed executable file format can take advantage of this fact by laying
    out the data and machine instructions in the file to reduce virtual memory overhead.
  prefs: []
  type: TYPE_NORMAL
- en: '**4.7.1 Pages, Segments, and File Size**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Virtual memory subsystems and memory protection schemes generally operate on
    *pages* in memory. A page on a typical processor is usually between 1KB and 64KB
    in size. Whatever the size, a page is the smallest unit of memory to which you
    can apply discrete protection features (such as whether the data in that page
    is read-only, read/write, or executable). In particular, you cannot mix read-only/executable
    code with read/write data in the same page—the two must appear in separate pages
    in memory. Using the 80x86 CPU family as an example, pages in memory are 4KB each.
    Therefore, the minimum amount of code space and the minimum amount of data space
    we can allocate to a process is 8KB if we have read/write data and we want to
    place the machine instructions in read-only memory. In fact, most programs contain
    several segments or sections (as you saw previously with object files) to which
    we can apply individual protection rights, and each section will require a unique
    set of one or more pages in memory that are not shared with any of the other sections.
    A typical program has four or more sections in memory: code or text, static data,
    uninitialized data, and stack are the most common. In addition, many compilers
    also generate heap segments, linkage segments, read-only segments, constant data
    segments, and application-named data segments (see [Figure 4-10](ch04.xhtml#ch4fig10)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/04fig10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-10: Typical segments found in memory*'
  prefs: []
  type: TYPE_NORMAL
- en: Because operating systems map segments to pages, a segment will always require
    some number of bytes that is a multiple of the page size. For example, if a program
    has a segment that contains only a single byte of data, that segment will still
    consume 4,096 bytes on an 80x86 processor. Similarly, if an 80x86 application
    consists of six different segments, that application will consume at least 24KB
    in memory, regardless of the number of machine instructions and data bytes the
    program uses and regardless of the executable file’s size.
  prefs: []
  type: TYPE_NORMAL
- en: Many executable file formats (such as ELF and PE/COFF) provide an option in
    memory for a BSS section where a programmer can place uninitialized static variables.
    Because the values are uninitialized, there is no need to clutter the executable
    file with random data values for each of these variables. Therefore, the BSS section
    in some executable file formats is just a small stub that tells the OS loader
    the size of the BSS section. This way, you can add new uninitialized static variables
    to your application without affecting the executable file’s size. When you increase
    the amount of BSS data, the compiler simply adjusts a value to tell the loader
    how many bytes to reserve for the uninitialized variables. Were you to add those
    same variables to an initialized data section, the size of the executable file
    would grow with each byte of data that you added. Obviously, saving space on your
    mass storage device is a good thing to do, so using BSS sections to reduce your
    executable file sizes is a useful optimization.
  prefs: []
  type: TYPE_NORMAL
- en: The one thing that many people tend to forget, however, is that a BSS section
    still requires main memory at runtime. Even though the executable file size may
    be smaller, each byte of data you declare in your program translates to 1 byte
    of data in memory. Some programmers have the mistaken impression that the executable’s
    file size is indicative of the amount of memory that the program consumes. This,
    however, isn’t necessarily true, as our BSS example shows. A given application’s
    executable file might consist of only 600 bytes, but if that program uses four
    different sections, with each section consuming a 4KB page in memory, the program
    will require 16,384 bytes of memory when the OS loads it into memory. This is
    because the underlying memory protection hardware requires the OS to allocate
    whole pages of memory to a given process.
  prefs: []
  type: TYPE_NORMAL
- en: '**4.7.2 Internal Fragmentation**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Another reason an executable file might be smaller than an application’s *execution
    memory footprint* (the amount of memory the application consumes at runtime) is
    *internal fragmentation*. Internal fragmentation occurs when you must allocate
    sections of memory in fixed-sized chunks even though you might need only a portion
    of each chunk (see [Figure 4-11](ch04.xhtml#ch4fig11)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/04fig11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-11: Internal fragmentation*'
  prefs: []
  type: TYPE_NORMAL
- en: Remember that each section in memory consumes an integral number of pages, even
    if that section’s data size is not a multiple of the page size. All bytes from
    the last data/code byte in a section to the end of the page holding that byte
    are wasted; this is internal fragmentation. Some executable file formats allow
    you to pack each section without padding it to some multiple of the page size.
    However, as you’ll soon see, there may be a performance penalty for packing sections
    together in this fashion, so some executable formats don’t do it.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, don’t forget that an executable file’s size does not include any data
    (including data objects on the heap and values placed on the CPU’s stack) allocated
    dynamically at runtime. As you can see, an application can actually consume much
    more memory than the executable file’s size.
  prefs: []
  type: TYPE_NORMAL
- en: Programmers commonly compete to see who can write the smallest “Hello World”
    program using their favorite language. Assembly language programmers are especially
    guilty of bragging about how much smaller they can write this program in assembly
    than they can in C or some other HLL. This is a fun mental challenge, but whether
    the program’s executable file is 600 or 16,000 bytes long, the chances are pretty
    good that the program will consume exactly the same amount of memory at runtime
    once the operating system allocates four or five pages for the program’s different
    sections. While writing the world’s shortest “Hello World” application might win
    bragging rights, in real-world terms such an application saves almost nothing
    at runtime due to internal fragmentation.
  prefs: []
  type: TYPE_NORMAL
- en: '**4.7.3 Reasons to Optimize for Space**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This is not to suggest that optimizing for space isn’t worthwhile. Programmers
    who write great code consider all the machine resources their application uses,
    and they avoid wasting those resources. However, attempting to take this process
    to an extreme is a waste of effort. Once you’ve gotten a given section below 4,096
    bytes (on an 80x86 or other CPU with a 4KB page size), additional optimizations
    save you nothing. Remember, the *allocation granularity*—that is, the minimum
    allocation block size—is 4,096 bytes. If you have a section with 4,097 bytes of
    data, it’s going to consume 8,192 bytes at runtime. In that case, it would behoove
    you to reduce that section by 1 byte (thereby saving 4,096 bytes at runtime).
    However, if you have a data section that consumes 16,380 bytes, attempting to
    reduce its size by 4,092 bytes in order to reduce the file size is going to be
    difficult unless the data organization was very bad to begin with.
  prefs: []
  type: TYPE_NORMAL
- en: Note that most operating systems allocate disk space in clusters (or blocks)
    that are often comparable to (or even larger than) the page size for the memory
    management unit in the CPU. Therefore, if you shrink an executable’s file size
    down to 700 bytes in an attempt to save disk space (an admirable goal, even given
    the gargantuan size of modern disk drive subsystems), the savings won’t be as
    great as you’d expect. That 700-byte application, for example, is still going
    to consume a minimum of one block on the disk’s surface. All you achieve by reducing
    your application’s code or data size is to waste that much more space in the disk
    file—subject, of course, to section/block allocation granularity.
  prefs: []
  type: TYPE_NORMAL
- en: For larger executable files, those larger than the disk block size, internal
    fragmentation has less impact with respect to wasted space. If an executable file
    packs the data and code sections without any wasted space between the sections,
    then internal fragmentation occurs only at the end of the file, in the very last
    disk block. Assuming that file sizes are random (even distribution), then internal
    fragmentation will waste approximately one-half of a disk block per file (that
    is, an average of 2KB per file when the disk block size is 4KB). For a very small
    file, one that is less than 4KB in size, this might represent a significant amount
    of the file’s space. For larger applications, however, the wasted space becomes
    insignificant. So it would seem that as long as an executable file packs all the
    sections of the program sequentially in the file, the file will be as small as
    possible. But is this really desirable?
  prefs: []
  type: TYPE_NORMAL
- en: Assuming all things are equal, having smaller executable files is a good thing.
    However, all things aren’t always equal, so sometimes creating the smallest possible
    executable file isn’t really best. To understand why, recall the earlier discussion
    of the operating system’s virtual memory subsystem. When an OS loads an application
    into memory for execution, it doesn’t actually have to read the entire file. Instead,
    the operating system’s paging system can load only those pages needed to start
    the application. This usually consists of the first page of executable code, a
    page of memory to hold stack-based data, and, possibly, some data pages. In theory,
    an application could begin execution with as few as two or three pages of memory
    and bring in the remaining pages of code and data *on demand* (as the application
    requests the data or code found in those pages). This is known as *demand-paged
    memory management*. In practice, most operating systems actually preload pages
    for efficiency reasons (maintaining a working set of pages in memory). However,
    operating systems generally don’t load the entire executable file into memory;
    instead, they load various blocks as the application requires them. As a result,
    the effort needed to load a page of memory from a file can dramatically affect
    a program’s performance. Is there some way, then, to organize the executable file
    to improve performance when the OS uses demand-paged memory management? Yes—if
    you make the file a little larger.
  prefs: []
  type: TYPE_NORMAL
- en: The trick to improving performance is to organize the executable file’s blocks
    to match the memory page layout. This means that sections in memory should be
    aligned on page-sized boundaries in the executable file. It also means that disk
    blocks should be the size of, or a multiple of the size of, a disk sector or block.
    This being the case, the virtual memory management system can rapidly copy a single
    block on the disk into a single page of memory, update any necessary relocation
    values, and continue program execution. On the other hand, if a page of data is
    spread across two blocks on the disk and is not aligned on a disk block boundary,
    the OS has to read two blocks (rather than one) from disk into an internal buffer
    and then copy the page of data from that buffer to the destination page where
    it belongs. This extra work can be very time-consuming and hamper application
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, some compilers will actually pad the executable file to ensure
    that each section in the executable file begins on a block boundary that the virtual
    memory management subsystem can map directly to a page in memory. Compilers that
    employ this technique often produce much larger executable file sizes than those
    that don’t. This is especially true if the executable file contains a large amount
    of BSS (uninitialized) data that a packed file format can represent very compactly.
  prefs: []
  type: TYPE_NORMAL
- en: Because some compilers produce packed files at the expense of execution time,
    while others produce expanded files that load and run faster, it’s dangerous to
    compare the quality of compilers based on the size of the executable files they
    produce. The best way to determine the quality of a compiler’s output is by directly
    analyzing that output, not by using a weak metric such as output file size.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Analyzing compiler output is the subject of the very next chapter, so if you’re
    interested in the topic, keep reading.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**4.8 Data and Code Alignment in an Object File**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As I pointed out in *WGC1*, aligning data objects on an address boundary that
    is “natural” for that object’s size can improve performance. It’s also true that
    aligning the start of a procedure’s code or the starting instruction of a loop
    on some nice boundary can improve performance. Compiler writers are well aware
    of this fact and will often emit *padding bytes* in the data or code stream to
    align data or code sequences on an appropriate boundary. However, note that the
    linker is free to move sections of code around when linking two object files to
    produce a single executable result.
  prefs: []
  type: TYPE_NORMAL
- en: Sections are generally aligned to a page boundary in memory. For a typical application,
    the text/code section will begin on a page boundary, the data section will begin
    on a different page boundary, the BSS section (if it exists) will begin on its
    own page boundary, and so on. However, this doesn’t imply that each and every
    section associated with a section header in the object files starts on its own
    page in memory. The linker program will combine sections that have the same name
    into a single section in the executable file. So, for example, if two different
    object files both contain a `.text` segment, the linker will combine them into
    a single `.text` section in the final executable file. By combining sections that
    have the same name, the linker avoids wasting a large amount of memory to internal
    fragmentation.
  prefs: []
  type: TYPE_NORMAL
- en: How does the linker respect the alignment requirements of each of the sections
    it combines? The answer, of course, depends on exactly what object file format
    and OS you’re using, but it’s usually found in the object file format itself.
    For example, in a Windows PE/COFF file the `IMAGE_OPTIONAL_HEADER32` structure
    contains a field named `SectionAlignment`. This field specifies the address boundary
    that the linker and OS must respect when combining sections and loading the section
    into memory. Under Windows, the `SectionAlignment` field in the PE/COFF optional
    header will usually contain 32 or 4,096 bytes. The 4KB value, of course, will
    align a section to a 4KB page boundary in memory. The alignment value of 32 was
    probably chosen because this is a reasonable cache line value (see *WGC1* for
    a discussion of cache lines). Other values are certainly possible—an application
    programmer can usually specify section alignment values by using linker (or compiler)
    command-line parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '**4.8.1 Choosing a Section Alignment Size**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Within each section, a compiler, assembler, or other code-generation tool can
    guarantee any alignment that is a submultiple of the section’s alignment. For
    example, if the section’s alignment value is 32, then alignments of 1, 2, 4, 8,
    16, and 32 are possible within that section. Larger alignment values are not possible.
    If a section’s alignment value is 32 bytes, you cannot guarantee alignment within
    that section on a 64-byte boundary, because the OS or linker will respect only
    the section’s alignment value and it can place that section on any boundary that
    is a multiple of 32 bytes. And about half of those won’t be 64-byte boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps less obvious, but just as true, is the fact that you cannot align an
    object within a section on a boundary that is not a submultiple of the section’s
    alignment. For example, a section with a 32-byte alignment value will not allow
    an alignment of 5 bytes. True, you could guarantee that the offset of some object
    within the section would be a multiple of 5; however, if the starting memory address
    of the section is not a multiple of 5, then the address of the object you attempted
    to align might not fall on a multiple of 5 bytes. The only solution is to pick
    a section alignment value that is some multiple of 5.
  prefs: []
  type: TYPE_NORMAL
- en: Because memory addresses are binary values, most language translators and linkers
    limit alignment values to a power of 2 that is less than or equal to some maximum
    value, usually the memory management unit’s page size. Many languages restrict
    the alignment value to a small power of 2 (such as 32, 64, or 256).
  prefs: []
  type: TYPE_NORMAL
- en: '**4.8.2 Combining Sections**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When a linker combines two sections, it has to respect the alignment values
    associated with each section because the application may depend on that alignment
    for correct operation. Therefore, a linker or other program that combines sections
    in object files can’t simply concatenate the data for the two sections when building
    the combined section.
  prefs: []
  type: TYPE_NORMAL
- en: When combining two sections, a linker might have to add padding bytes between
    the sections if one or both of the lengths is not a multiple of the sections’
    alignment. For example, if two sections have an alignment value of 32, and one
    section is 37 bytes long and the other section is 50 bytes long, the linker will
    have to add 27 bytes of padding between the first and second sections, or it will
    have to add 14 bytes of padding between the second section and the first (the
    linker usually gets to choose in which order it places the sections in the combined
    file).
  prefs: []
  type: TYPE_NORMAL
- en: The situation gets a bit more complicated if the alignment values are not the
    same for the two sections. When a linker combines two sections, it has to ensure
    that the alignment requests are met for the data in both sections. If the alignment
    value of one section is a multiple of the other section’s alignment value, then
    the linker can simply choose the larger of the two alignment values. For example,
    if the alignment values are always powers of 2 (as most linkers require), then
    the linker can simply choose the larger of the two alignment values for the combined
    section.
  prefs: []
  type: TYPE_NORMAL
- en: If one section’s alignment value is not a multiple of the other’s, then the
    only way to guarantee the alignment requirements of both sections when combining
    them is to use an alignment value that is a product of the two values (or, better
    yet, the *least common multiple* of the two values). For example, combining a
    section aligned on a 32-byte boundary with one aligned on a 5-byte boundary requires
    an alignment value of 160 bytes (5 × 32). Because of the complexities of combining
    two such sections, most linkers require section sizes to be small powers of 2,
    which guarantees that the larger segment alignment value is always a multiple
    of the smaller alignment value.
  prefs: []
  type: TYPE_NORMAL
- en: '**4.8.3 Controlling the Section Alignment**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You typically use linker options to control the section alignment within your
    programs. For example, with the Microsoft *link.exe* program, the `/ALIGN:value`
    command-line parameter tells the linker to align all sections in the output file
    to the specified boundary (which must be a power of 2). GNU’s *ld* linker program
    lets you specify a section alignment by using the `BLOCK(value)` option in a linker
    script file. The macOS linker (`ld`) provides a `-segalign value` command-line
    option you can use to specify section alignment. The exact command and possible
    values are specific to the linker; however, almost every modern linker allows
    you to specify the section alignment properties. See your linker’s documentation
    for details.
  prefs: []
  type: TYPE_NORMAL
- en: 'One word of caution about setting the section alignment: more often than not,
    a linker will require that all sections in a given file be aligned on the same
    boundary (a power of 2). Therefore, if you have different alignment requirements
    for all your sections, then you’ll need to choose the largest alignment value
    for all the sections in your object file.'
  prefs: []
  type: TYPE_NORMAL
- en: '**4.8.4 Aligning Sections Within Library Modules**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Section alignment can have a very big impact on the size of your executable
    files if you use a lot of short library routines. Suppose, for example, that you’ve
    specified an alignment size of 16 bytes for the sections associated with the object
    files appearing in a library. Each library function that the linker processes
    will be placed on a 16-byte boundary. If the functions are small (fewer than 16
    bytes in length), the space between the functions will be unused when the linker
    creates the final executable. This is another form of internal fragmentation.
  prefs: []
  type: TYPE_NORMAL
- en: To understand why you would want to align the code (or data) in a section on
    a given boundary, think about how cache lines work (see *WGC1* for a refresher).
    By aligning the start of a function on a cache line, you may be able to slightly
    increase the execution speed of that function, as it may generate fewer cache
    misses during execution. For this reason, many programmers like to align all their
    functions at the start of a cache line. Although the size of a cache line varies
    from CPU to CPU, a typical cache line is 16 to 64 bytes long, so many compilers,
    assemblers, and linkers will attempt to align code and data to one of these boundaries.
    On the 80x86 processor, there are some other benefits to 16-byte alignment, so
    many 80x86-based tools default to a 16-byte section alignment for object files.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider, for example, the following short HLA (High-Level Assembly) program,
    processed by Microsoft tools, that calls two relatively small library routines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Here is the source code for the `bits.reverse32()` library function. Note that
    this source file also includes the `bits.reverse16()` and `bits.reverse8()` functions
    (to conserve space, the bodies of these functions do not appear below). Although
    their operation is not pertinent to our discussion, note that these functions
    swap the values in the HO (high-order) and LO (low-order) bit positions. Because
    these three functions appear in a single source file, any program that includes
    one of these functions will automatically include all three (because of the way
    compilers, assemblers, and linkers work).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The Microsoft *dumpbin.exe* tool allows you to examine the various fields of
    an *.obj* or *.exe* file. Running `dumpbin` with the `/headers` command-line option
    on the *bitcnt.obj* and *reverse.obj* files (produced for the HLA standard library)
    tells us that each of the sections is aligned to a 16-byte boundary. Therefore,
    when the linker combines the *bitcnt.obj* and *reverse.obj* data with the sample
    program given earlier, it will align the `bits.cnt()` function in the *bitcnt.obj*
    file on a 16-byte boundary, and the three functions in the *reverse.obj* file
    on a 16-byte boundary. (Note that it will not align each function in the file
    on a 16-byte boundary. That task is the responsibility of the tool that created
    the object file, if such alignment is desired.) By using the *dumpbin.exe* program
    with the `/disasm` command-line option on the executable file, you can see that
    the linker has honored these alignment requests (note that an address that is
    aligned on a 16-byte boundary will have a `0` in the LO hexadecimal digit):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The exact operation of this program really isn’t important (after all, it doesn’t
    actually do anything useful). The takeaway is how the linker inserts extra bytes
    (`$cc`, the `int 3` instruction) before a group of one or more functions appearing
    in a source file to ensure that they are aligned on the specified boundary.
  prefs: []
  type: TYPE_NORMAL
- en: In this particular example, the `bits.cnt()` function is actually 64 bytes long,
    and the linker inserted only 3 bytes in order to align it to a 16-byte boundary.
    This percentage of waste—the number of padding bytes compared to the size of the
    function—is quite low. However, if you have a large number of small functions,
    the wasted space can become significant (as with the default exception handler
    in this example that has only two instructions). When creating your own library
    modules, you’ll need to weigh the inefficiencies of extra space for padding against
    the small performance gains you’ll obtain by using aligned code.
  prefs: []
  type: TYPE_NORMAL
- en: Object code dump utilities (like *dumpbin.exe*) are quite useful for analyzing
    object code and executable files in order to determine attributes such as section
    size and alignment. Linux (and most Unix-like systems) provides the comparable
    `objdump` utility. I’ll discuss these tools in the next chapter, as they are great
    for analyzing compiler output.
  prefs: []
  type: TYPE_NORMAL
- en: '**4.9 How Linkers Affect Code**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The limitations of object file formats such as COFF and ELF have a big impact
    on the quality of code that compilers can generate. Because of how object file
    formats are designed, linkers and compilers often have to inject extra code into
    an executable file that wouldn’t be necessary otherwise. In this section we’ll
    explore some of the problems that generic object code formats like COFF and ELF
    inflict on the executable code.
  prefs: []
  type: TYPE_NORMAL
- en: One problem with generic object file formats like COFF and ELF is that they
    were not designed to produce efficient executable files for specific CPUs. Instead,
    they were created to support a wide variety of CPUs and to make it easy to link
    together object modules. Unfortunately, their versatility often prevents them
    from creating the best possible object files.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perhaps the biggest problem with the COFF and ELF formats is that relocation
    values in the object file must apply to 32- and 64-bit pointers in the object
    code. This creates problems, for example, when an instruction encodes a displacement
    or address value with less than 32 (64) bits. On some processors, such as the
    80x86, displacements smaller than 32 bits are so small (for example, the 80x86’s
    8-bit displacement) that you would never use them to refer to code outside the
    current object module. However, on some RISC processors, such as the PowerPC or
    ARM, displacements are much larger (26 bits in the case of the PowerPC branch
    instruction). This can lead to code kludges like the function stub generation
    that GCC produces for external function calls. Consider the following C program
    and the PowerPC code that GCC emits for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The compiler must generate the `L_printf$stub` stub because it doesn’t know
    how far away the actual `printf()` routine will be when the linker adds it to
    the final executable file. It’s unlikely that `printf()` would be sitting outside
    the ±32MB range that the PowerPC’s 24-bit branch displacement supports (extended
    to 26 bits), but it’s not guaranteed. If `printf()` is part of a shared library
    that is dynamically linked in at runtime, it very well could be outside this range.
    Therefore, the compiler has to make the safe choice and use a 32-bit displacement
    for the address of the `printf()` function. Unfortunately, PowerPC instructions
    don’t support a 32-bit displacement, because all PowerPC instructions are 32 bits
    long. A 32-bit displacement would leave no room for the instruction’s opcode.
    Therefore, the compiler has to store a 32-bit pointer to the `printf()` routine
    in a variable and jump indirect through that variable. Accessing a 32-bit memory
    pointer on the PowerPC takes quite a bit of code if you don’t already have the
    pointer’s address in a register, hence all the extra code following the `L_printf$stub`
    label.
  prefs: []
  type: TYPE_NORMAL
- en: If the linker were able to adjust 26-bit displacements rather than just 32-bit
    values, there would be no need for the `L_printf$stub` routine or the `L_printf$lazy_ptr`
    pointer variable. Instead, the `bl L_printf$stub` instruction would be able to
    branch directly to the `printf()` routine (assuming it’s not more than ±32MB away).
    Because single program files generally don’t contain more than 32MB of machine
    instructions, there would rarely be the need to go through the gymnastics this
    code does in order to call an external routine.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, there is nothing you can do about the object file format; you’re
    stuck with whatever format the OS specifies (which is usually a variant of COFF
    or ELF on modern 32-bit and 64-bit machines). However, you can work within those
    limitations.
  prefs: []
  type: TYPE_NORMAL
- en: If you expect your code to run on a CPU like the PowerPC or ARM (or some other
    RISC processor) that cannot encode 32-bit displacements directly within instructions,
    you can optimize by avoiding cross-module calls as much as possible. While it’s
    not good programming practice to create monolithic applications, where all the
    source code appears in one source file (or is processed by a single compilation),
    there’s really no need to place all of your own functions in separate source modules
    and compile each one separately from the others—particularly if these routines
    make calls to one another. By placing a set of common routines your code uses
    into a single compilation unit (source file), you allow the compiler to optimize
    the calls among these functions and avoid all the stub generation on processors
    like the PowerPC. This is not a suggestion to simply move all of your external
    functions into a single source file. The code is better only if the functions
    in a module call one another or share other global objects. If the functions are
    completely independent of one another and are called only by code external to
    the compilation unit, then you’ve saved nothing because the compiler may still
    need to generate stub routines in the external code.
  prefs: []
  type: TYPE_NORMAL
- en: '**4.10 For More Information**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Aho, Alfred V., Monica S. Lam, Ravi Sethi, and Jeffrey D. Ullman. *Compilers:
    Principles, Techniques, and Tools*. 2nd ed. Essex, UK: Pearson Education Limited,
    1986.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Gircys, Gintaras. *Understanding and Using COFF*. Sebastopol, CA: O’Reilly
    Media, 1988.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Levine, John R. *Linkers and Loaders*. San Diego: Academic Press, 2000.'
  prefs: []
  type: TYPE_NORMAL
