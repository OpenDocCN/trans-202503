<html><head></head><body><div id="sbo-rt-content"><section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" title="565" id="Page_565"/>20</span><br/>
<span class="ChapterTitle">Attention and Transformers</span></h1>
</header>
<figure class="opener">
<img src="Images/chapterart.png" alt="" width="206" height="206"/>
</figure>
<p class="ChapterIntro">In Chapter 19 we looked at how to use RNNs to handle sequential data. Though powerful, RNNs have a few drawbacks. Because all of the information about an input is represented in a single piece of state memory, or context vector, the networks inside each recurrent cell need to work hard to compress everything that’s needed into the available space. And no matter how large we make the state memory, we can always get an input that exceeds what the memory can hold, so something necessarily gets lost.</p>
<p>Another problem is that an RNN must be trained and used one word at a time. This can be a slow way to work, particularly with large databases.</p>
<p>An alternative approach is based on a small network called an <em>attention network</em>, which doesn’t have a state memory and can be trained and used in parallel. Attention networks can be combined into larger structures called <em>transformers</em>, which are capable of serving as language models that can perform tasks like translation. The building blocks of transformers can be used in other architectures that provide even more powerful language models, including generators.</p>
<p><span epub:type="pagebreak" title="566" id="Page_566"/>In this chapter, we start with a more powerful way to represent words rather than as single numbers, and then build our way up to attention and modern architectures that use transformer blocks to perform many NLP tasks. </p>
<h2 id="h1-500723c20-0001">Embedding</h2>
<p class="BodyFirst">In Chapter 19 we promised to improve our word descriptions beyond a single number. The value of this change is that it allows us to manipulate the representations of words in meaningful ways. For example, we can find a word that is like another word, or we can blend two words to find one that’s in between them. This concept is key to developing attention, and then transformers.</p>
<p>The technique is called <em>word embedding </em>(or <em>token embedding </em>when we use it on the more general idea of a token). It’s a bit abstract, so let’s see the ideas first with a concrete example.</p>
<p>Suppose that you work as an animal wrangler on a movie with a tempestuous director. Today you’re filming a sequence where the human heroes are chased by some animals. The director asks you for a list of animals you can provide in sufficient numbers to produce a scary chase. You call your office, they prepare the list, and they even arrange those animals into a chart, where the horizontal axis represents each adult animal’s average top speed and the vertical axis represents its average weight, as in <a href="#figure20-1" id="figureanchor20-1">Figure 20-1</a>.</p>
<figure>
<img src="Images/F20001.png" alt="F20001" width="694" height="572"/>
<figcaption><p><a id="figure20-1">Figure 20-1</a>: A collection of animals, organized roughly by land speed horizontally and adult weight vertically, though those axis labels aren’t shown (data from Reisner 2020)</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="567" id="Page_567"/>But due to a printer error, the chart your office sent you is missing the labels on the axes, so you have the chart with the animals laid out in 2D, but you don’t know what the axes mean.</p>
<p>The director doesn’t even look at the chart. “Horses,” she says, “I want horses. They’re exactly what I want and will be perfect and nothing else will do.” So you bring in the horses, and they rehearse the scene.</p>
<p>Unfortunately, the director is unhappy. “No, no, no!” she says. “The horses are too twitchy and quick. They’re like foxes. Give me horses that are less fox-like.”</p>
<p>How on Earth can you satisfy this request? What does it even mean? Happily, you can do just as she asks with the chart, just by combining arrows.</p>
<p>You only need to do two things with arrows: add them and subtract them. To add arrow B to arrow A, place the tail of B onto the head of A. The new arrow A + B starts at the tail of A, and ends at the head of B, as in the middle of <a href="#figure20-2" id="figureanchor20-2">Figure 20-2</a>.</p>
<figure>
<img src="Images/F20002.png" alt="F20002" width="693" height="210"/>
<figcaption><p><a id="figure20-2">Figure 20-2</a>: Arrow arithmetic. Left: Two arrows. Middle: The sum A + B. Right: The difference A – B.</p></figcaption>
</figure>
<p>To subtract B from A, just flip B around by 180 degrees to make –B, and add together A and –B. The result, A – B, starts at the tail of A and ends at the head of –B, as in the right of <a href="#figure20-2">Figure 20-2</a>.</p>
<p>Now you can satisfy the director’s desire to remove the fox qualities from the horses. Start by drawing an arrow from the bottom left of the chart to the horse, and another to the fox, as in the left of <a href="#figure20-3" id="figureanchor20-3">Figure 20-3</a>.</p>
<figure>
<img src="Images/F20003.png" alt="F20003" width="843" height="301"/>
<figcaption><p><a id="figure20-3">Figure 20-3</a>: Left: Arrows from the bottom left to the horse and fox. Right: Subtracting fox from horse gives us a giant sloth.</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="568" id="Page_568"/>Now subtract foxes from horses, as requested, by subtracting the fox arrow from the horse arrow. Following the rules of <a href="#figure20-2">Figure 20-2</a>, that means flipping the fox arrow around and placing its tail at the head of the horse arrow. We get the right side of <a href="#figure20-3">Figure 20-3</a>.</p>
<p>A giant sloth. Well, okay, it’s what the director wanted. We can even write this like a little bit of arithmetic: <span class="CustomCharStyle">horse – fox = giant sloth </span>(at least, according to our diagram).</p>
<p>The director throws her latte on the ground. “No no no! Sure, sloths would look great, but they hardly move! Make them fast! Give me sloths that are like roadrunners!”</p>
<p>Now we know just how to satisfy this ridiculous demand: find the arrow from the bottom left to the roadrunner, as shown in the left of <a href="#figure20-4" id="figureanchor20-4">Figure 20-4</a>, and add that to the head of the arrow pointing to the sloth, giving us a brown bear. That is, <span class="CustomCharStyle">horse – fox + roadrunner = brown bear</span>, as in the right of <a href="#figure20-4">Figure 20-4</a>.</p>
<figure>
<img src="Images/f20004.png" alt="f20004" width="844" height="302"/>
<figcaption><p><a id="figure20-4">Figure 20-4</a>: Left: We can draw an arrow to the roadrunner. Right: Giant sloth + roadrunner = brown bear.</p></figcaption>
</figure>
<p>You offer the director a group of brown bears (called a <em>sleuth</em> of bears). The director rolls her eyes dramatically. “Finally. Something that’s fast like horses, but not twitchy like foxes, and quick like roadrunners. It’s only what I asked for in the first place.” They shoot the chase scene with bears, and the movie later comes out to great acclaim.</p>
<p>There are two key elements to this story. The first is that the animals in our chart were laid out in a useful way, even though we didn’t know what that way was, or what the axes represented about the data. </p>
<p>The second key point is that we didn’t need the axis labels after all. We were able to navigate the chart just by adding and subtracting arrows pointing to elements on the chart itself. That is, we didn’t try to find a “slower horse.” Rather, we worked strictly with the animals themselves, and their various attributes came along implicitly. Removing the speediness of a fox from a big animal like a horse gave us a big, slow animal.</p>
<p>What does this have to do with processing language?</p>
<h3 id="h2-500723c20-0001"><span epub:type="pagebreak" title="569" id="Page_569"/>Embedding Words</h3>
<p class="BodyFirst">To apply what we’ve just seen to words, we’ll replace the animals with words. And instead of using only two axes, we’ll place our words in a space of hundreds of dimensions.</p>
<p>We do this with an algorithm that works out what each axis in this space should mean as it places every word at the appropriate point. Instead of assigning each word a single number, the algorithm assigns the word a whole list of numbers, representing its coordinates in a huge space.</p>
<p>This algorithm is called an <em>embedder</em>, and we say that this process is one of <em>embedding </em>the words in the <em>embedding space</em>, thereby creating <em>word embeddings</em>.</p>
<p>The embedder works out for itself how to construct the space and find the coordinates of each word so that it’s near similar words. For example, if it sees a lot of sentences that begin with <span class="CustomCharStyle">I just drank some</span>, then whatever noun comes next is interpreted as some kind of drink, and it is placed near other kinds of drinks. If it sees <span class="CustomCharStyle">I just ate a red</span>, then whatever comes next is interpreted as something that’s red and edible, and it is placed near other things that are red and near other things that are edible. The same thing is true of dozens or even hundreds of other relationships, both obvious and subtle. Because the space has so many dimensions and the axes can have arbitrarily complex meanings, words can belong simultaneously to many clusters based on seemingly unrelated characteristics.</p>
<p>This idea is both abstract and powerful, so let’s illustrate it with some actual examples. We tried a few “word arithmetic” expressions using a pretrained embedding of 684,754 words saved in a space of 300 dimensions (spaCy authors 2020). Our first test was a famous one: <span class="CustomCharStyle">king – man + woman</span> (El Boukkouri 2018). The system returned <span class="CustomCharStyle">queen</span> as the most likely result, which makes sense: we can imagine that the embedder worked out some sense of nobility on one axis and gender on another. Other tests were close but not perfect. For example, <span class="CustomCharStyle">lemon – yellow + green</span> came back with <span class="CustomCharStyle">ginger</span> as the best match, but the expected <span class="CustomCharStyle">lime</span> wasn’t far back as the fifth-closest word. Similarly, <span class="CustomCharStyle">trumpet – valves + slide</span> returned <span class="CustomCharStyle">saxophone</span> as the most likely result, but the expected <span class="CustomCharStyle">trombone</span> was the first runner-up. </p>
<p>The beauty of training an embedder in a space with hundreds (or even thousands) of dimensions is that it can use the space much more efficiently than any person probably would, enabling it to simultaneously represent an enormous number of relationships.</p>
<p>The word arithmetic we just saw is a fun demonstration of embedding spaces, but it also enables us to meaningfully perform operations on words like comparing them, scaling them, and adding them, all of which are important to the algorithms in this chapter. </p>
<p>Once we have word embeddings, it’s easy to incorporate them into almost any network. Instead of assigning a single integer to each word, we assign the word embedding, which is a list of numbers. So instead of processing zero-dimensional tensors (single numbers), the system processes one-dimensional tensors (lists of numbers).</p>
<p><span epub:type="pagebreak" title="570" id="Page_570"/>This neatly addresses the problem we saw in Chapter 19 where predictions that were close to the target but not exactly right gave us nonsense. Now we can tolerate a bit of imprecision, because similar words are embedded near one another. For example, we might give our language model the phrase <span class="CustomCharStyle">The dragon approached and let out a mighty,</span> expecting the next word to be <span class="CustomCharStyle">roar</span>. The algorithm might predict a tensor that’s near <span class="CustomCharStyle">roar</span> but not exactly on it, giving us <span class="CustomCharStyle">bellow</span> or <span class="CustomCharStyle">blast</span> instead. We probably wouldn’t get back something unrelated, like <span class="CustomCharStyle">daffodil</span>.</p>
<p><a href="#figure20-5" id="figureanchor20-5">Figure 20-5</a> shows six sets of four related words that we gave to a standard word embedder. The more the embeddings of any two words are like one another, the higher that pair of words scored, so the darker their intersection appears. The graph is symmetric around the diagonal from the upper left to the lower right, since the order in which we compare the words doesn’t matter.</p>
<figure>
<img src="Images/F20005.png" alt="F20005" width="675" height="592"/>
<figcaption><p><a id="figure20-5">Figure 20-5</a>: Comparing pairs of words by comparing the similarity of their embeddings</p></figcaption>
</figure>
<p>We can see from <a href="#figure20-5">Figure 20-5</a> that each word matches itself most strongly and also matches related words more strongly than unrelated words. Because we placed related words side by side, the graph shows their similarities as small blocks. There are a few curiosities, however. For example, why does <span class="CustomCharStyle">fish</span> match better than average with <span class="CustomCharStyle">chocolate</span> and <span class="CustomCharStyle">coffee</span>, and why does <span class="CustomCharStyle">blue</span> score well with <span class="CustomCharStyle">caramel</span>? These might be artifacts from the particular training data used for this embedder.</p>
<p><span epub:type="pagebreak" title="571" id="Page_571"/>The coffee drinks and the flavors score well with one another, perhaps because people order coffee drinks with those flavored syrups. There’s also a hint of a relationship between the colors and the flavors.</p>
<p>Many pretrained word embedders are widely available for free, and easily downloaded into almost any library. We can simply import them and immediately get the vector for any word. The GLoVe (Mikolov et al. 2013a; Mikolov et al. 2013b) and word2vec (Pennington, Socher, and Manning 2014) embeddings have been used in many projects. The more recent fastText (Facebook Open Source 2020) project offers embeddings in 157 languages.</p>
<p>We can also embed entire sentences, so that we can compare them as a whole, rather than word by word (Cer et al. 2018). <a href="#figure20-6" id="figureanchor20-6">Figure 20-6</a> shows comparisons between embeddings for a dozen sentences (TensorFlow 2018). In this book, we focus on word embeddings rather than sentences.</p>
<figure>
<img src="Images/F20006.png" alt="F20006" width="675" height="548"/>
<figcaption><p><a id="figure20-6">Figure 20-6</a>: Comparing sentence embeddings. The larger the score, the more the sentences are considered like one another.</p></figcaption>
</figure>
<h3 id="h2-500723c20-0002">ELMo</h3>
<p class="BodyFirst">Word embeddings are a huge advance over assigning single integers to words. But even though word embeddings are powerful, the approach we described earlier to create them has a problem: nuance.</p>
<p>As we saw in Chapter 19, many languages have words with different meanings but are written and pronounced the same way. If we want to make sense of words, we need to distinguish these meanings. One way to do that is to give every meaning of a word its own embedding. So <span class="CustomCharStyle">cupcake</span>, which has one <span epub:type="pagebreak" title="572" id="Page_572"/>meaning, has one embedding. But <span class="CustomCharStyle">train</span> has two embeddings, one for when it’s a noun (as in, “I rode on a train”), and one for when it’s a verb (as in, “I like to train dogs”). These two meanings of <span class="CustomCharStyle">train</span> really are entirely different ideas that just happen to use the same sequence of letters.</p>
<p>Such words present two challenges. First, we have to create unique embeddings for each meaning. Second, we have to select the correct embedding when such words are used as input. Solving these challenges requires that we take into account the context of every word. The first algorithm to do this in a big way was called <em>Embedding from Language Models</em>, but it’s better known by its friendly acronym <em>ELMo </em>(Peters et al. 2018), which is the name of a Muppet on the children’s television show <em>Sesame Street</em>. We say that ELMo produces <em>contextualized word embeddings</em>.</p>
<p>ELMo’s architecture is similar to that of a pair of bi-RNNs, which we saw in <a href="c19.xhtml#figure19-20" id="figureanchor19-20">Figure 19-20</a>, but the pieces are organized differently. In a standard bi-RNN, we couple two RNNs running in opposite directions.</p>
<p>ELMo changes this around. Although it uses two RNN networks that run forward and two that run backward, they are grouped by direction. Each of these groups is a two-layer-deep RNN, like the one we saw in <a href="c19.xhtml#figure19-21" id="figureanchor19-21">Figure 19-21</a>. ELMo’s architecture is shown in <a href="#figure20-7" id="figureanchor20-7">Figure 20-7</a>. It’s traditional to draw ELMo diagrams with a red color scheme, since Elmo on <em>Sesame Street</em> is a bright red character.</p>
<figure>
<img src="Images/F20007.png" alt="F20007" width="838" height="460"/>
<figcaption><p><a id="figure20-7">Figure 20-7</a>: The structure of ELMo in unrolled form. The input text is at the bottom. The embedding of each input element is at the top.</p></figcaption>
</figure>
<p>This architecture means each input word is turned into two new tensors, one from the forward networks (labeled F1 and F2), that take into account the preceding words, and one from the backward networks (labeled B1 and B2) that consider the following words. By concatenating <span epub:type="pagebreak" title="573" id="Page_573"/>these results together, we get contextualized word embeddings informed by all the other words in the sentence.</p>
<p>Trained versions of ELMo are widely available for free downloads in a variety of sizes (Gluon 2020). Once we have a pretrained ELMo, it’s easy to use in any language model. We give our entire sentence to ELMo, and we get back a contextualized word embedding for each word, given its context. </p>
<p><a href="#figure20-8" id="figureanchor20-8">Figure 20-8</a> shows four sentences that use the homonym <span class="CustomCharStyle">train</span> as a verb, and four that use <span class="CustomCharStyle">train</span> as a noun. We gave these to a standard ELMo model trained on a database of 1 billion words that places each word into a space of 1,024 dimensions (TensorFlow 2020a). We extracted ELMo’s embedding of the word <span class="CustomCharStyle">train</span> in each sentence, and compared its embedding to that of the word <span class="CustomCharStyle">train</span> in all the other sentences. Although the word is written in the identical way in each sentence, ELMo is able to identify the correct embedding based on the word’s context. </p>
<figure>
<img src="Images/F20008.png" alt="F20008" width="675" height="613"/>
<figcaption><p><a id="figure20-8">Figure 20-8</a>: Comparing ELMo’s embeddings of <em>train</em> resulting from its use in different sentences. Darker colors mean more similar embeddings.</p></figcaption>
</figure>
<p>We usually place embedding algorithms like ELMo on their own layer in a deep learning system. This is often the very first layer in a language processing network. Our icon for an embedding algorithm, shown in <a href="#figure20-9" id="figureanchor20-9">Figure 20-9</a>, is meant to suggest taking the space of words and placing it inside the larger embedding space.</p>
<span epub:type="pagebreak" title="574" id="Page_574"/><figure>
<img src="Images/F20009.png" alt="F20009" width="147" height="53"/>
<figcaption><p><a id="figure20-9">Figure 20-9</a>: Our icon for an embedding layer</p></figcaption>
</figure>
<p>ELMo and other algorithms like it, such as the <em>Universal Language Model Fine-Tuning</em>, or <em>ULMFiT</em> (Howard and Ruder 2018), are typically trained on general-purpose databases, such as books and documents from the web. When we need them for some specific downstream task, such as medical or legal applications, we usually fine-tune them with additional examples from those domains. The result is a set of embeddings that include the specialized language of those fields, clustered by their special meanings in that jargon.</p>
<p>We’ll use embeddings in the systems we will build later in this chapter. Those networks will rely on the mechanism of attention, so let’s look at that now.</p>
<h2 id="h1-500723c20-0002">Attention</h2>
<p class="BodyFirst">In Chapter 19 we saw how to improve translation by taking into account all of the words in a sentence. But when we’re translating a particular word, not every word in the sentence is equally important, or even relevant.</p>
<p>For example, suppose we’re translating the sentence <span class="CustomCharStyle">I saw a big dog eat his dinner</span>. When we’re translating <span class="CustomCharStyle">dog</span>, we probably don’t care about the word <span class="CustomCharStyle">saw</span>, but to translate the pronoun <span class="CustomCharStyle">his</span> correctly may require us to connect that to the two words <span class="CustomCharStyle">big dog</span>.</p>
<p>If we can work out, for each word in the input, which other words can influence our translation, then we can focus just on those words and ignore the others. This would be a big savings in both memory and computation time. And if we can work this out in a way that doesn’t depend on processing the words serially, we can even do it in parallel.</p>
<p>The algorithm that does this job is called <em>attention</em>, or <em>self-attention </em>(Bahdanau, Cho, and Bengio 2016; Sutskever, Vinyals, and Le 2014; Cho et al. 2014). Attention lets us focus our resources on only the parts of the input that matter.</p>
<p>Modern versions of attention are often based on a technique called <em>query, key, value</em>, or simply <em>QKV</em>. These terms come from the field of databases and can seem somewhat obscure in this context. So we’ll describe the concepts using a different set of terms and then connect them back to query, key, and value at the end.</p>
<h3 id="h2-500723c20-0003">A Motivating Analogy</h3>
<p class="BodyFirst">Let’s begin with an analogy. Suppose that you need to buy some paint, but all you’ve been told is that the color should be “light yellow with a bit of dark orange.”</p>
<p>At the only paint store in town, the only clerk on duty is new to the paint department and isn’t personally familiar with the colors. You both <span epub:type="pagebreak" title="575" id="Page_575"/>presume you’ll need to mix together a few of their standard paints to get the color you want, but you don’t know which paints to choose or how much of each to use.</p>
<p>The clerk suggests that you compare your desired color description with the color names on each can of paint they carry. Some names will probably match better than others. The clerk puts a funnel on top of an empty can and suggests that you pour in some of each can of paint on the shelves, guided by how well that can’s name matches your description. That is, you’ll compare your desired description “light yellow with a bit of dark orange” with what’s printed on the label of each can, and the better the match, the more of that paint you’ll pour into the funnel.</p>
<p><a href="#figure20-10" id="figureanchor20-10">Figure 20-10</a> shows the idea visually for six cans of paint. It shows their names and the quality of each name’s match with your desired color’s description. We got good matches on “Sunny Yellow” and “Orange Crush,” though a little bit of “Lunch with Teal” snuck in thanks to the match with the word “with.” </p>
<figure>
<img src="Images/F20010.png" alt="F20010" width="839" height="152"/>
<figcaption><p><a id="figure20-10">Figure 20-10</a>: Given a color description (left), we combine some of each can based on how well its name matches the description (middle), to get a final result (right).</p></figcaption>
</figure>
<p>There are three things to focus on in this story. First, there’s your <em>request</em>: “light yellow with a bit of dark orange.” Second, there’s the <em>description </em>on each can of paint, like “Sunny Yellow” or “Mellow Blue.” Third, there’s the <em>content </em>of the paint that’s actually inside each can. In the story, you compared your request with each can’s description to find out how well they match. The better the match, the more of that can’s content you used in the final mixture.</p>
<p>That’s attention in a nutshell. Given a request, compare it to the description of each possible item and include some of the content of each item based on how well its description matches the request.</p>
<p>The authors of the first paper on attention compared this process to a common type of transaction used with a database. In database language, we look something up by sending a <em>query </em>to a database. In such a process, every object in the database has a descriptive <em>key</em>, which can be different than the actual <em>value </em>of the object. Note that here the word <em>value</em> refers to the contents of the object, whether it’s a single number or something more complicated, such as a string or tensor. </p>
<p>The database system compares the query (or request) with each key (or description) and uses that score to determine how much of the object’s value (or content) to include in the final result. So our terms of request, description, and content correspond to query, key, and value, or, more commonly, QKV.</p>
<h3 id="h2-500723c20-0004"><span epub:type="pagebreak" title="576" id="Page_576"/>Self-Attention</h3>
<p class="BodyFirst"><a href="#figure20-11" id="figureanchor20-11">Figure 20-11</a> shows the fundamental operation of attention in abstracted form. Here we have five words of input. Each of the three colored boxes represents a small neural network that takes the numerical representation of a word and transforms it into something new (often, these networks are each just a single fully connected layer). In this example, the word <span class="CustomCharStyle">dog</span> is the one we want to translate. So a neural network (in red) transforms the tensor for <span class="CustomCharStyle">dog</span> and turns it into a new tensor representing the query, Q. As the figure shows, two more small neural networks translate the tensor for <span class="CustomCharStyle">dinner</span> into new tensors, corresponding to its key, K (from the blue network) and its value, V (from the green network).</p>
<figure>
<img src="Images/f20011.png" alt="f20011" width="454" height="315"/>
<figcaption><p><a id="figure20-11">Figure 20-11</a>: The core step of attention using <em>dog</em> for the query to determine the relevance of the word <em>dinner</em>. Each box represents a small neural network that transforms its input into a query, key, or value.</p></figcaption>
</figure>
<p>In practice, we compare the query for <span class="CustomCharStyle">dog</span> against the key of every word in the sentence, including <span class="CustomCharStyle">dog</span> itself. For this illustration, we limit our focus to the comparison with the word <span class="CustomCharStyle">dinner</span>. </p>
<p>We compare the query and the key to determine how alike they are. We do this with a little scoring function that we’re indicating with the letter <em>S</em> in a circle. Without getting into the math, this function compares two tensors and produces a single number. The more that the two tensors are like one another, the larger that number. The scoring function is usually designed to produce a number between 0 and 1, with larger values indicating a better match.</p>
<p>We use the output from the scoring function to scale the tensor representing the value for <span class="CustomCharStyle">dinner</span>. The more the query and the key match, the larger the output of the scaling step, and the more the value of <span class="CustomCharStyle">dinner</span> will make it into the output.</p>
<p>Let’s see what it looks like when we apply this fundamental step to all the words in the input simultaneously. We’ll continue to look at translating the word <span class="CustomCharStyle">dog</span>. The overall result is the sum of the individual scaled values of all the input words. <a href="#figure20-12" id="figureanchor20-12">Figure 20-12</a> shows how this looks.</p>
<span epub:type="pagebreak" title="577" id="Page_577"/><figure>
<img src="Images/F20012.png" alt="F20012" width="484" height="397"/>
<figcaption><p><a id="figure20-12">Figure 20-12</a>: Using attention to simultaneously determine the contribution of all five words in the sentence to the word <em>dog</em>. The QKV spatial and color coding matches <a href="#figure20-11">Figure 20-11</a>. All data flows upward in the figure.</p></figcaption>
</figure>
<p>There are a few things to note in <a href="#figure20-12">Figure 20-12</a>. First, only three neural networks are involved—one each to compute the query, key, and value tensors. We use the same “input to query” network (in red in the figure) to turn each input into its query, the same “input to key” network (in blue in the figure) to turn each input into its key, and the same “input to value” network (in green in the figure) to turn each input into its value. We only need to apply these transformations once to each word.</p>
<p>Second, there’s a dashed line after the scores and before the scaling of the values. This represents a softmax step applied to the scores, followed by a division. These two operations keep the numbers coming out of the scores from getting too big or small. The softmax also exaggerates the influence of close matches.</p>
<p>Third, we sum up all the scaled values to get a new tensor for <span class="CustomCharStyle">dog</span>, including that from the value of <span class="CustomCharStyle">dog</span> itself. We often find that each word scores most highly with itself. This isn’t a bad thing, as in this case, the most important word for translating <span class="CustomCharStyle">dog</span> is indeed <span class="CustomCharStyle">dog</span> itself. But there are times when other words will matter more. Some examples include when word order changes, when a word has no direct translation and must rely on other words, or when we’re trying to resolve a pronoun.</p>
<p>The fourth important point is that we apply the processing of <a href="#figure20-12">Figure 20-12</a> to all the words in the input sentence simultaneously. That is, each word is considered the query, and the whole process executes independently for that word, as shown in <a href="#figure20-13" id="figureanchor20-13">Figure 20-13</a>. </p>
<span epub:type="pagebreak" title="578" id="Page_578"/><figure>
<img src="Images/F20013.png" alt="F20013" width="844" height="686"/>
<figcaption><p><a id="figure20-13">Figure 20-13</a>: Applying attention to the other four words in our sentence</p></figcaption>
</figure>
<p>Our fifth and last point is just an explicit recap of something we’ve been noting all along: all of this processing in <a href="#figure20-12">Figure 20-12</a> and <a href="#figure20-13">Figure 20-13</a> together can be done in parallel in just four steps, regardless of the length of the sentence. Step 1 transforms the inputs into query, key, and value tensors. Step 2 scores all the queries and keys against one another. Step 3 uses the scores to scale the values, and step 4 adds up the scaled values to produce a new output for each input.</p>
<p>None of these steps depend on how long the input is, so we can process long sentences in the same amount of time required by short ones, as long as we have the memory and computing power needed.</p>
<p>We call the process of <a href="#figure20-12">Figure 20-12</a> and <a href="#figure20-13">Figure 20-13</a> <em>self-attention</em>, because the attention mechanism is using the same set of inputs for computing everything: the queries, keys, and values. That is, we’re finding how much the input should be paying attention to itself.</p>
<p>When we place self-attention in a deep network, we put it onto its own <em>self-attention layer</em>, often simply called an <em>attention layer</em>. The input is a list of words in numerical form, and the output is the same. </p>
<p><span epub:type="pagebreak" title="579" id="Page_579"/>The engines that power attention are the scoring function and the neural networks that transform the inputs into queries, keys, and values. Let’s consider them briefly.</p>
<p>The scoring function compares a query to a key, returning a value from 0 to 1, where the more the two values are similar, the higher their score. So somehow, the inputs that we think of as being similar need to have similar values going into the scoring function. Now we can see the practical value of embeddings. Recall our discussion of <em>A Tale of Two Cities</em>, in Chapter 19 where we assigned each word a number given by its order in the text. That gave the words <span class="CustomCharStyle">keep </span>and <span class="CustomCharStyle">flint </span>numbers 1,003 and 1,004 respectively. If we just compared these numbers, they would get a high similarity score. For most sentences, this is not what we want. If we’re using the query value for the verb <span class="CustomCharStyle">keep</span>, we usually want it to be similar to the keys for synonyms like <span class="CustomCharStyle">retain</span>, <span class="CustomCharStyle">hold</span>,<span class="CustomCharStyle"> </span>and <span class="CustomCharStyle">reserve</span>, and not at all like the keys for unrelated words like <span class="CustomCharStyle">flint</span>, <span class="CustomCharStyle">preposterous</span>, or <span class="CustomCharStyle">dinosaur</span>. Embeddings are the means by which similar words (or words used in similar ways) are given similar representations.</p>
<p>Doing any necessary fine-tuning to the embeddings is the job of the neural networks, which transform the input words into representations where they can be meaningfully compared in the context of the sentence they’re used in. The only reason we have any chance of that is that the words are already embedded in a space where similar words are near one another.</p>
<p>Similarly, it’s the job of the network that turns inputs into values to represent those values in a way that allow them to be usefully scaled and combined. Mixing two embedded words gives us a word that’s somewhere between them.</p>
<h3 id="h2-500723c20-0005">Q/KV Attention</h3>
<p class="BodyFirst">In the self-attention network of <a href="#figure20-12">Figure 20-12</a>, the queries, keys, and values are all derived from the same inputs, which led to the name self-attention. </p>
<p>A popular variation uses one source for the queries and another for the keys and values. This more closely matches our paint store analogy, where we came in with the query, and the store had the keys and values. We call this variation a <em>Q/KV</em> network, where the slash indicates that the queries come from one source, and the keys and values from another. This version is sometimes used when we add attention to a network like seq2seq, where the queries come from the encoder, and the keys and values from the decoder, so it’s sometimes also called an <em>encoder-decoder attention </em>layer. The structure is shown in <a href="#figure20-14" id="figureanchor20-14">Figure 20-14</a>.</p>
<span epub:type="pagebreak" title="580" id="Page_580"/><figure>
<img src="Images/F20014.png" alt="F20014" width="498" height="379"/>
<figcaption><p><a id="figure20-14">Figure 20-14</a>: A Q/KV layer is like self-attention as shown in <a href="#figure20-12">Figure 20-12</a>, except that the queries don’t come from the inputs.</p></figcaption>
</figure>
<h3 id="h2-500723c20-0006">Multi-Head Attention</h3>
<p class="BodyFirst">The idea of attention is to identify words that are alike and create a useful mix of them. But words can be considered alike based on many different metrics. We might consider nouns to be alike, or colors, or spatial ideas like <span class="CustomCharStyle">up</span> and <span class="CustomCharStyle">down</span>, or temporal ideas like <span class="CustomCharStyle">yesterday</span> and <span class="CustomCharStyle">tomorrow</span>. Which of these is the best choice?</p>
<p>Of course, there is no one best answer. In fact, we often want to compare words using multiple criteria at once. For instance, when writing song lyrics, we may want to assign high scores to pairs of words that have similar meanings, similar sounds in their last syllable, the same number of syllables, and the same stress pattern in the syllables. When writing about sports, we might instead want to say that players on the same teams and with the same roles are like one another. </p>
<p>We can score words along multiple criteria by simply running multiple independent attention networks simultaneously. Each network is called a <em>head</em>. By initializing each head independently, we hope that during training, each head will learn to compare the inputs according to criteria that are simultaneously useful and different from those used by the other layers. If we want, we can add additional processing to explicitly encourage different heads to attend to different aspects of the inputs. The idea is called <em>multi-head attention</em>, and we can apply it to both self-attention networks like <a href="#figure20-12">Figure 20-12</a> and Q/KV networks like <a href="#figure20-14">Figure 20-14</a>. </p>
<p>Each head is a distinct attention network. The more heads we have, the more different aspects of the input they can focus on.</p>
<p>A diagram for a multi-head attention layer is shown in <a href="#figure20-15" id="figureanchor20-15">Figure 20-15</a>. As the figure shows, we usually combine the outputs of the heads into a list and <span epub:type="pagebreak" title="581" id="Page_581"/>run that through a single fully connected layer. This allows the entire multi-head network’s output to have the same shape as its input. This approach makes it easy to place multiple multi-head networks one after another.</p>
<figure>
<img src="Images/F20015.png" alt="F20015" width="407" height="287"/>
<figcaption><p><a id="figure20-15">Figure 20-15</a>: A multi-head attention layer. A box with a diamond inside is our icon for an attention layer.</p></figcaption>
</figure>
<p>Attention is a general concept that we can apply in different forms to any kind of deep network. For example, in a CNN we can scale a filter’s outputs to emphasize the values produced in response to the most relevant locations in the input (Liu et al. 2018; H. Zhang et al. 2019).</p>
<h3 id="h2-500723c20-0007">Layer Icons</h3>
<p class="BodyFirst"><a href="#figure20-16" id="figureanchor20-16">Figure 20-16</a> shows our icons for the different types of attention layers. Multi-head attention is drawn as a little 3D box, suggesting a stack of attention networks. For Q/KV attention, we place a short line inside the diamond to identify the Q inputs and bring in the K and V inputs on an adjacent side.</p>
<figure>
<img src="Images/F20016.png" alt="F20016" width="561" height="149"/>
<figcaption><p><a id="figure20-16">Figure 20-16</a>: Attention layer icons. (a) Self-attention. (b) Multi-head self-attention. (c) Q/KV attention. (d) Multi-head Q/KV attention.</p></figcaption>
</figure>
<h2 id="h1-500723c20-0003">Transformers</h2>
<p class="BodyFirst">Now that we have embedding and attention, we’re ready to make good on our earlier promise to improve on RNNs. </p>
<p>Our goal is to build a translator based not on RNNs, but on attention networks. The key idea is that the attention layers will learn how to transform our inputs into their translations, based on the relationships between words.</p>
<p><span epub:type="pagebreak" title="582" id="Page_582"/>This approach first appeared in a paper with the great title, “Attention Is All You Need” (Vaswani et al. 2017). The authors called their attention-based model a <em>transformer </em>(an unfortunately ambiguous name, but it’s now firmly stuck in the language of the field). The transformer model works so well that we now have a new class of language models that not only can be trained in parallel, but also can outperform RNNs in a wide variety of tasks.</p>
<p>Transformers use three more ideas we haven’t discussed yet. Let’s cover them now, so when we get to the actual transformer architecture, it will be smooth sailing.</p>
<h3 id="h2-500723c20-0008">Skip Connections</h3>
<p class="BodyFirst">The first new idea we cover is called a <em>residual connection </em>or <em>skip connection </em>(He et al. 2015). The inspiration is to reduce the amount of work that’s required of a deep network layer.</p>
<p>Let’s start with an analogy. Suppose you’re painting a real, physical portrait using acrylic paints on canvas. After weeks of sittings, the portrait is done, and you send it to your subject for their approval. They say that they like it, but they regret having worn a particular ring on one finger, and wish they’d worn a different one that they like more. Can you change that?</p>
<p>One way to proceed would be to invite your subject back to the studio and paint a whole new portrait from scratch on a blank canvas, only this time with the new ring on their finger. That would require a lot of time and effort. If they’d allow it, a more expeditious approach would be to take the portrait you have, and unobtrusively paint the new ring over the old one.</p>
<p>Now consider a layer in a deep network. A tensor comes in, and the layer does some processing to change that tensor. If the layer only needs to change the input by small amounts, or only in some places, then it would be wasteful to expend resources processing the parts of the tensor that don’t need to change. Just as with the painting, it would be much more efficient for the layer to compute only the changes it wants to make. Then it can combine those changes with the original input to produce its output.</p>
<p>This idea works beautifully in deep learning networks. It lets us make layers that are smaller and faster, and it even improves the flow of gradients in backpropagation, which lets us efficiently train networks of dozens or even hundreds of layers.</p>
<p>The mechanism is shown on the left side of <a href="#figure20-17" id="figureanchor20-17">Figure 20-17</a>. We feed an input tensor to some layer as usual, let it compute the changes, and then we add the layer’s output to its input tensor.</p>
<figure>
<img src="Images/F20017.png" alt="F20017" width="694" height="76"/>
<figcaption><p><a id="figure20-17">Figure 20-17</a>: Left: A skip connection, shown in red. Right: We can place a skip connection around multiple layers.</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="583" id="Page_583"/>The extra line in the drawing that carries the input to the addition node is called a <em>skip connection</em>, or a <em>residual connection</em> because of its mathematical interpretation.</p>
<p>We can place a skip connection around multiple layers in sequence, if we like, as on the right of <a href="#figure20-17">Figure 20-17</a>.</p>
<p>The skip connection works because each layer is trying to reduce its own contribution to the final error, while participating in the network made up of all the other layers. The skip connection is part of the network, so the layer learns it doesn’t need to process the parts of the tensor that don’t change. This makes the layer’s job simpler, enabling it to be smaller and faster. </p>
<p>We’ll see later that transformers use skip connections not just for efficiency and speed, but also because they allow the transformer to cleverly keep track of the location of each element in its input.</p>
<h3 id="h2-500723c20-0009">Norm-Add</h3>
<p class="BodyFirst">The second idea on our road to transformers is really more of a conceptual and notational shorthand. In transformers, we usually apply a regularization step called <em>layer normalization</em>, or <em>layer norm</em>, to the outputs of a layer, as shown on the left in <a href="#figure20-18" id="figureanchor20-18">Figure 20-18</a> (Vaswani et al. 2017). Layer norm belongs to the class of regularization techniques that we saw in Chapter 15, such as dropout and batchnorm, which help control overfitting by keeping the values flowing through the network from getting too big or too small. The layer norm step learns to adjust the values coming out of a layer so that they approximate the shape of a Gaussian bump with a mean of 0 and standard deviation of 1.</p>
<figure>
<img src="Images/F20018.png" alt="F20018" width="437" height="70"/>
<figcaption><p><a id="figure20-18">Figure 20-18</a>: Left: A layer normalization followed by the addition step of a skip connection. Right: A combined icon for norm-add. This is just a visual and conceptual shorthand for the network on the left.</p></figcaption>
</figure>
<p>Performing layer norms is important in getting a transformer to work well, but there’s some flexibility about exactly where this step can be located. A popular approach places the layer norm just before the addition step of a skip connection, as in the left side of <a href="#figure20-18">Figure 20-18</a>. Since these two operations always come in pairs, it’s convenient to combine them into a single operation that we call <em>norm-add</em>. Our icon for norm-add is a combination of the layer norm and summation icons, and is shown on the right in <a href="#figure20-18">Figure 20-18</a>. This is just a visual shorthand for the two separate steps of layer norm followed by skip connection addition.</p>
<p>People have experimented with other locations for the layer norm operation, such as before the layer (Vaswani et al. 2017), or after the addition node (TensorFlow 2020b). These approaches differ in their details, but in <span epub:type="pagebreak" title="584" id="Page_584"/>practice, it seems that all of these choices are comparable. We’ll stick with the version in <a href="#figure20-18">Figure 20-18</a> here.</p>
<h3 id="h2-500723c20-0010">Positional Encoding</h3>
<p class="BodyFirst">The third idea to cover before we get to transformers was designed to solve a problem that comes up as soon as we take RNNs out of our system: we lose track of where each word is located in the input sentence. This important information is inherent in the RNN structure, because the words come in one at a time, allowing the hidden state inside a recurrent cell to remember the order in which the words arrived.</p>
<p>But as we’ve seen, attention mixes together the representations of multiple words. How can later stages know where each word belongs in the sentence?</p>
<p>The answer is to insert each word’s position, or index, into the representation for the word itself. That way, as the word’s representations get processed, the position information naturally comes along for the ride. The generic name for this process is <em>positional encoding</em>.</p>
<p>A simple approach to positional encoding is to append a few bits to the end of each word to hold its location, as shown on the left of <a href="#figure20-19" id="figureanchor20-19">Figure 20-19</a>. But at some point, we might get a sentence that requires more bits than we’ve made available, and then we’d be in trouble because we wouldn’t be able to assign each word a unique number for its location. And if we make the storage too big, it’s just wasted and slows everything down. This approach is also awkward to implement, since we then need to introduce some special mechanism for handling those bits (Thiruvengadam 2018).</p>
<figure>
<img src="Images/F20019.png" alt="F20019" width="694" height="381"/>
<figcaption><p><a id="figure20-19">Figure 20-19</a>: Tracking the location of each word in a sentence. Left: Appending an index to each word. Middle: Using a function F to turn each index into a vector, then adding it to the word’s representation. Right: Our icon for a positional embedding layer.</p></figcaption>
</figure>
<p>A better answer is to use a mathematical function that creates a unique vector for each position in a sequence. Suppose that our word embeddings are 128 elements long. Then we give this function the index of each word <span epub:type="pagebreak" title="585" id="Page_585"/>(which can be as large as it needs to be), and the function gives us back a new 128-element vector that somehow describes that location. Basically it turns the index into a unique list of values. Our expectation is that the network will learn to associate each of these lists with the word’s position in the input.</p>
<p>Rather than appending this vector to the word’s representation, we add the two vectors together, as in the middle of <a href="#figure20-19">Figure 20-19</a>. Here we literally add the number in each element in the encoding to the corresponding number in the word’s embedding. The appeal of this approach is that we don’t need any extra bits or special processing. This form of positional encoding is called <em>positional embedding</em>, because of its similarity to the <em>word embedding </em>we saw earlier in algorithms like ELMo. The right side of the figure shows our icon for this process, which is drawn with a little sine wave because a popular choice for the embedding function is based on sine waves (derived from Vaswani et al. 2017).</p>
<p>It may seem a bit weird to add position information to each word, rather than append it, as it changes the word’s representation. It also seems that the position information is liable to get lost as the attention network processes the values.</p>
<p>It turns out that the specific function that is frequently used to compute the positional embedding vector usually affects only a few bits at one end of the word’s vector (Vaswani et al. 2017; Kazemnejad 2019). Furthermore, it appears that transformers learn how to distinguish each word’s representation and position information during processing so they’re interpreted separately (TensorFlow 2019a).</p>
<p>But why doesn’t the position embedding get lost altogether during processing? After all, attention changes its inputs using neural networks by turning them into QKV values and then mixing those values. Surely the positional information would be hopelessly scrambled and lost.</p>
<p>The clever solution to this problem is built into the architecture of the transformer itself. As we’ll see, the transformer network wraps up each operation (except the very last) in a skip connection. The embedding information never gets lost, because it gets added back in after every stage of processing. <a href="#figure20-20" id="figureanchor20-20">Figure 20-20</a> illustrates how positional embedding and norm-add skip connections are structurally similar. In short, each layer can change its input vector in any way it wants, and then the positional embedding gets added back in so that it’s available to the next layer.</p>
<figure>
<img src="Images/F20020.png" alt="F20020" width="844" height="157"/>
<figcaption><p><a id="figure20-20">Figure 20-20</a>: Left: Creating a position embedding and adding it to a word. Right: A norm-add operation implicitly adds a word’s embedding information back in after processing.</p></figcaption>
</figure>
<h3 id="h2-500723c20-0011"><span epub:type="pagebreak" title="586" id="Page_586"/>Assembling a Transformer</h3>
<p class="BodyFirst">We now have all the pieces in place to build a transformer. We’ll continue to use word-level translation as our running example.</p>
<p>It’s important to note that the name <em>transformer</em> refers to a wide variety of networks inspired by the architecture in the original transformer paper (Vaswani et al. 2017). In this discussion, we’ll stick to a generic version.</p>
<p>Our block diagram of a transformer is shown in <a href="#figure20-21" id="figureanchor20-21">Figure 20-21</a>. The blocks marked <em>E</em> and <em>D</em> are repeated sequences of layers, or <em>blocks</em>, built around attention layers. We’ll look at both types of block in detail in a moment. The big picture is that an encoder stage (built from <em>encoder blocks</em>, marked with an <em>E</em>) accepts a sentence, and a decoder (build from <em>decoder blocks</em>, marked with a <em>D</em>) accepts information from the encoder and produces new output (the structure of this diagram is reminiscent in some ways of an unrolled seq2seq diagram, but there are no recurrent cells here).</p>
<figure>
<img src="Images/F20021.png" alt="F20021" width="436" height="226"/>
<figcaption><p><a id="figure20-21">Figure 20-21</a>: A block diagram of a transformer. An input is encoded and then decoded. The decoder’s output is fed back to its input autoregressively. The dashed lines stand for repeated elements.</p></figcaption>
</figure>
<p>Both the encoder and decoder begin with word embedding followed by positional embedding. The decoder has the usual fully connected layer and softmax at the end for predicting the next word. The decoder is autoregressive, so it appends each output word to the list of its outputs (shown by the box at the bottom of the figure), and that list becomes the decoder’s input for generating the next word. The decoder contains multi-head Q/KV attention networks, as in <a href="#figure20-14">Figure 20-14</a>, which receive their keys and values from the outputs of the encoder blocks, shown in the middle of <a href="#figure20-21">Figure 20-21</a>, where the encoder outputs are delivered to the decoder blocks. This illustrates why Q/KV attention is also called encoder-decoder attention.</p>
<p>Let’s look more closely at the blocks in <a href="#figure20-21">Figure 20-21</a> starting with the encoder block, shown in <a href="#figure20-22" id="figureanchor20-22">Figure 20-22</a>.</p>
<span epub:type="pagebreak" title="587" id="Page_587"/><figure>
<img src="Images/F20022.png" alt="F20022" width="690" height="211"/>
<figcaption><p><a id="figure20-22">Figure 20-22</a>: The transformer’s encoder block. The first layer is self-attention.</p></figcaption>
</figure>
<p>The encoder block begins with a layer of multi-head self-attention, shown here with eight heads. Because this layer applies self-attention, the queries, keys, and values are all derived from the single set of inputs that arrive at the block. This multi-head attention is surrounded by a norm-add skip connection to help keep the numbers looking like a Gaussian and to retain the positional embeddings.</p>
<p>This is followed by two layers that are usually referred to collectively as a <em>pointwise feed-forward layer</em> (another unfortunately vague name). Though the original transformers paper described these as a pair of modified fully connected layers (Vaswani et al. 2017), we can more conveniently think of them as two layers of 1×1 convolution (Chromiak 2017; Singhal 2020; A. Zhang et al. 2020). They learn how to adjust the output of the multi-head attention layer to remove redundancy and focus on just the information that will be of the most value to whatever processing comes next. The first convolution uses a ReLU activation function, while the second has no activation function. As usual, these two steps are wrapped in a norm-add skip connection.</p>
<p>Now let’s look at the decoder block, shown in <a href="#figure20-23" id="figureanchor20-23">Figure 20-23</a>.</p>
<figure>
<img src="Images/F20023.png" alt="F20023" width="694" height="250"/>
<figcaption><p><a id="figure20-23">Figure 20-23</a>: The transformer’s decoder block. Note that the first attention layer is self-attention, whereas the second is Q/KV attention. The triangle on the left of the self-attention layer indicates that the layer uses masking.</p></figcaption>
</figure>
<p>At a high level, it looks a lot like the encoder block, with an extra step of attention. Let’s walk through the layers.</p>
<p><span epub:type="pagebreak" title="588" id="Page_588"/>We begin with a multi-head self-attention layer, just like the encoder block. The input to the layer is the words output so far by the transformer. If we’re just beginning, this sentence contains only the <code>[START]</code> token. Like any self-attention layer, the purpose here is to look at all of the input words and work out which ones are most strongly related to which others. As usual, this is wrapped in a skip connection with a norm-add node at the end. During training, we add an extra detail called <em>masking</em> to this self-attention step (indicated with a small triangle in <a href="#figure20-23">Figure 20-23</a>), which we’ll come back to shortly.</p>
<p>The self-attention layer is followed by a multi-head Q/KV attention layer. The query, or Q, vectors come from the output of the previous self-attention layer. The keys and values come from the concatenated outputs of all the encoder blocks. This layer also is wrapped in a skip connection with a norm-add node at the end. This stage uses the outputs of the previous attention network to choose among the keys coming from the encoder and then mix the values corresponding to those keys. Finally, we have a pair of 1×1 convolutions, following the same pattern as in the encoder block.</p>
<p>We can now put the pieces together. <a href="#figure20-24" id="figureanchor20-24">Figure 20-24</a> shows the structure of a transformer model.</p>
<figure>
<img src="Images/F20024.png" alt="F20024" width="844" height="254"/>
<figcaption><p><a id="figure20-24">Figure 20-24</a>: A complete transformer. The icons showing two stacked boxes represent two consecutive 1×1 convolutions. The dashed lines stand for repeated elements that are not drawn.</p></figcaption>
</figure>
<p>We promised to return to a detail regarding the first attention layer in each decoder block. As we mentioned, one of the great values of the attention mechanism at the heart of the transformer is that it allows for a lot of parallelism. Whether an attention block is given five words or five hundred, it runs in the same amount of time.</p>
<p>Suppose we’re training the system to predict the next word in a sentence. We can provide it with the entire sentence and ask it to predict the first word, the second word, the third word, and so on, all in parallel.</p>
<p>But there’s a problem here. Suppose the sentence is <span class="CustomCharStyle">My dog loves taking long walks</span>. We could give the system <span class="CustomCharStyle">My dog loves taking long</span>, and ask it to predict the sixth word, <span class="CustomCharStyle">walks</span>. But because we’re training in parallel, we want it to use this same input to predict each of the previous words, at the same time. That is, we also want it to predict the fifth word, <span class="CustomCharStyle">long</span>, from the input <span class="CustomCharStyle">My dog loves taking long</span>.</p>
<p><span epub:type="pagebreak" title="589" id="Page_589"/>That’s too easy: the word <span class="CustomCharStyle">long</span> is right there! The system would find that all it has to do is return the fifth word, which is definitely not the same as learning how to predict it. We want to give the system <span class="CustomCharStyle">My dog loves taking long</span> as input, but for predicting the fifth word, it should only see <span class="CustomCharStyle">My dog loves taking</span>. We want to hide, or <span class="CustomCharStyle">mask</span>, the word <span class="CustomCharStyle">long</span> when we’re trying to predict it. Similarly, to predict the fourth word, it should only see <span class="CustomCharStyle">My dog loves</span>, to predict the third word it should only see <span class="CustomCharStyle">My dog</span>, and so on.</p>
<p>In short, our transformer will run five parallel computations, each predicting a different word, but each computation should only be given the words that came before the one it’s supposed to predict.</p>
<p>The mechanism to pull this off is called <em>masking</em>. We add an extra step to the first self-attention layer in the decoder block that masks, or hides, the words that each prediction step isn’t supposed to see. Thus the computation predicting the first word sees no input words, the computation predicting the second word only sees <span class="CustomCharStyle">My</span>, the one predicting the third word only sees <span class="CustomCharStyle">My dog</span>, and so on. Because of this extra step, the first attention layer in the decoder block is sometimes called a <em>masked multi-head self-attention </em>layer, which is a mouthful, so we often just refer to it as a <em>masked attention </em>layer.</p>
<h3 id="h2-500723c20-0012">Transformers in Action</h3>
<p class="BodyFirst">Let’s see a transformer in action performing a translation. We trained a transformer following roughly the architecture of <a href="#figure20-24">Figure 20-24</a> to translate from Portuguese to English (TensorFlow 2019b). We used a dataset of 50,000 training examples, which is small by today’s standards but good enough to demonstrate the ideas while also of a practical size to train from on a home computer (Kelly 2020).</p>
<p>We gave our trained transformer the Portuguese question, <span class="CustomCharStyle">você se sente da mesma maneira que eu?</span> which Google Translate renders into English as <span class="CustomCharStyle">do you feel the same that way I do?</span> Our system produced the translation, <span class="CustomCharStyle">do you see , do you get the same way i do ?</span> This isn’t perfect, but given the small training database, it does a great job of capturing the spirit of the question. As always, more training data and training time would surely improve the results.</p>
<p>Heatmaps showing the attention paid to each input word by each output word, for each of the eight heads in the final Q/KV attention layer of the decoder, are shown in <a href="#figure20-25" id="figureanchor20-25">Figure 20-25</a>. The brighter the cell, the more attention was paid. Note that some input words were broken up into multiple tokens by a preprocessor.</p>
<p>Transformers trained on larger datasets than this example, and for longer periods, can produce results that are as good or better than RNNs, and they can be trained in parallel. They don’t need recurrent cells with finite internal states that can run out of memory, nor do they need multiple neural networks to learn how to control those states. These are big advantages and explain why transformers have replaced RNNs in many applications.</p>
<span epub:type="pagebreak" title="590" id="Page_590"/><figure>
<img src="Images/F20025.png" alt="F20025" width="844" height="418"/>
<figcaption><p><a id="figure20-25">Figure 20-25</a>: Heatmaps for each of the eight heads in the final Q/KV attention layer of the decoder during a translation of “<em>Você se sente da mesma maneira que eu?</em>” from Portuguese to English</p></figcaption>
</figure>
<p>One downside of transformers is that the memory required by the attention layers grows dramatically with the size of the input. There are ways to adjust the attention mechanism, and the transformer in general, to reduce these costs in different situations (Tay et al. 2020).</p>
<h2 id="h1-500723c20-0004">BERT and GPT-2</h2>
<p class="BodyFirst">The full transformer model of <a href="#figure20-24">Figure 20-24</a> consists of an encoder, which is designed to analyze the input text and create a series of context vectors that describe it, and a decoder, which uses that information to autoregressively generate a translation of the input.</p>
<p>The blocks making up the encoder and decoder are not specific to translation. Each is just one or more attention layers, followed by a pair of 1×1 convolutions. These blocks can be used as general-purpose processors for working out the relationship between elements of a sequence, and language in particular. Let’s look at two recent architectures that have used transformer blocks in ways that go way beyond translation.</p>
<h3 id="h2-500723c20-0013">BERT</h3>
<p class="BodyFirst">Let’s use transformer blocks to create a general-purpose language model. It can be used for any of the tasks we listed at the start of Chapter 19.</p>
<p>The system is called <em>Bidirectional Encoder Representations from Transformers</em>, but it’s more commonly known by its acronym, <em>BERT </em>(Devlin et al. 2019) (another Muppet from <em>Sesame Street</em>, and a nodding reference to the ELMo system we saw earlier). The structure of BERT begins with a word embedder and a position embedder, followed by multiple transformer encoder blocks. <span epub:type="pagebreak" title="591" id="Page_591"/>The basic architecture is shown in in <a href="#figure20-26" id="figureanchor20-26">Figure 20-26</a> (in practice, other details help with training and performance, such as dropout layers). In this diagram, we’re showing the many inputs and outputs so that it’s clear that BERT is processing an entire sentence. For consistency and clarity, we’re only using a single line inside the blocks, but the parallel operations are still being carried out. It’s traditional to draw BERT diagrams with a yellow color scheme, since Bert on <em>Sesame Street</em> is a yellow character.</p>
<figure>
<img src="Images/F20026.png" alt="F20026" width="694" height="144"/>
<figcaption><p><a id="figure20-26">Figure 20-26</a>: The basic structure of BERT. The dashed lines stand for more encoder blocks that are not drawn.</p></figcaption>
</figure>
<p>The original “large” version of BERT deserved its name, with 340 million weights, or parameters. The system was trained on Wikipedia and over 10,000 books (Zhu et al. 2015). Currently, 24 trained versions of the original BERT system are available freely online (Devlin et al. 2020), as well as a growing number of variations and improvements on the basic approach (Rajasekharan 2019).</p>
<p>BERT was trained on two tasks. The first is called <em>next sentence prediction</em>, or <em>NSP</em>. In this technique, we give BERT two sentences at once (with a special token to separate them), and we ask it to determine if the second sentence reasonably follows the first. The second task presents the system with sentences where some of the words have been removed, and we ask it to fill in the blanks (language educators call this the <em>cloze task; </em>Taylor 1953). It’s the linguistic analog of the visual process called <em>closure</em>, describing the human tendency to fill in the blanks in images. Closure is illustrated in <a href="#figure20-27" id="figureanchor20-27">Figure 20-27</a>.</p>
<figure>
<img src="Images/F20027.png" alt="F20027" width="414" height="177"/>
<figcaption><p><a id="figure20-27">Figure 20-27</a>: Demonstrating the principle of closure. Incomplete shapes like these are usually filled in by the human visual system to create objects.</p></figcaption>
</figure>
<p>BERT is able to do well on these tasks because, compared to the RNN-based methods we saw before, BERT’s attention layers extract much more information from their inputs. Our first RNN models were <em>unidirectional</em>, reading inputs left to right. Then they became <em>bidirectional</em>, culminating in ELMo, which can be said to be <em>shallowly bidirectional</em>, where <em>shallow</em> refers to <span epub:type="pagebreak" title="592" id="Page_592"/>the architecture’s use of only two layers in each direction. Thanks to attention, BERT is able to determine the influence of every word on every other word, and by repeating the encoder block, it can do this many times in a row. BERT is sometimes called <em>deeply bidirectional</em>, but it might be more useful to think of it as <em>deeply dense</em>, since it considers every word simultaneously. The notion of direction really doesn’t apply when we’re using attention.</p>
<p>Let’s take BERT out for a spin. We’ll start with a pretrained model of 12 encoder blocks (McCormick and Ryan 2020). We’ll fine-tune it to determine if an input sentence is grammatical or not (Warstadt, Singh, and Bowman 2018; Warstadt, Singh, and Bowman 2019). This is basically a classification problem, producing a yes/no answer. Therefore, our downstream model should be a classifier of some kind. Let’s use a simple classifier consisting of a single fully connected layer. Our combined pair of models is shown in <a href="#figure20-28" id="figureanchor20-28">Figure 20-28</a>. </p>
<figure>
<img src="Images/F20028.png" alt="F20028" width="844" height="151"/>
<figcaption><p><a id="figure20-28">Figure 20-28</a>: BERT with a small downstream classifier at the end. The dashed lines stand for the 10 additional, identical encoder blocks that are present, but not drawn.</p></figcaption>
</figure>
<p>After four epochs of training, here are six results from the testing data. The first three are grammatical, and the second three are not. BERT produced the correct answer on all six.</p>
<ul>
<li><span class="CustomCharStyle">Chris walks, Pat eats broccoli, and Sandy plays squash.</span></li>
<li><span class="CustomCharStyle">There was some particular dog who saved every family.</span></li>
<li><span class="CustomCharStyle">Susan frightens her.</span></li>
<li><span class="CustomCharStyle">The person confessed responsible.</span></li>
<li><span class="CustomCharStyle">The cat slept soundly and furry.</span></li>
<li><span class="CustomCharStyle">The soundly and furry cat slept.</span></li>
</ul>
<p>On the test set of about 1,000 sentences, this little version of BERT got about 82 percent of the examples correct. Some BERT variants have achieved more than 88 percent right on this task (Wang et al. 2019; Wang et al. 2020).</p>
<p>Let’s try BERT out on another task, called <em>sentiment analysis</em>. We’ll classify short movie reviews as being either positive or negative in tone. The data comes from a database of almost 7,000 movie reviews called <em>SST2</em>, where each review has been labeled as positive or negative (Socher et al. 2013a; Socher et al. 2013b). </p>
<p><span epub:type="pagebreak" title="593" id="Page_593"/>For this run, we used a pretrained BERT model called DistillBERT (Sanh et al. 2020; Alammar 2019) (the term <em>distilling </em>is often used when we carefully trim a trained neural network to make it smaller and faster without losing much performance). We’re again doing a classification task, so we can reuse the model of <a href="#figure20-28">Figure 20-28</a>.</p>
<p>Here are six examples verbatim from the test data (there’s no indication of what movies they each refer to). DistillBERT properly classified the first three reviews as positive and the second three as negative (the reviews are all lowercase, and commas are treated as their own tokens).</p>
<ul>
<li><span class="CustomCharStyle">a beautiful , entertaining two hours</span></li>
<li><span class="CustomCharStyle">this is a shrewd and effective film from a director who understands how to create and sustain a mood</span></li>
<li><span class="CustomCharStyle">a thoroughly engaging , surprisingly touching british comedy</span></li>
<li><span class="CustomCharStyle">the movie slides downhill as soon as macho action conventions assert themselves</span></li>
<li><span class="CustomCharStyle">a zombie movie in every sense of the word mindless , lifeless , meandering , loud , painful , obnoxious</span></li>
<li><span class="CustomCharStyle">it is that rare combination of bad writing , bad direction and bad acting the trifecta of badness</span></li>
</ul>
<p>Of the 1,730 reviews in the test set, DistillBERT correctly predicted the sentiment of about 82 percent of them.</p>
<p>To recap, models based on the BERT architecture are united by their use of a sequence of encoder blocks. They create an embedding of a sentence that captures enough information that downstream applications can perform a wide range of operations upon it. With an appropriate downstream model, BERT can be used to perform many of the NLP tasks we mentioned at the start of Chapter 19.</p>
<p>If we’re willing to get clever, we can make BERT generate language, but it’s not easy (Mishra 2020; Mansimov et al. 2020). A better solution is to use decoder blocks, as we’ll see next.</p>
<h3 id="h2-500723c20-0014">GPT-2</h3>
<p class="BodyFirst">We’ve seen how transformers use a series of decoder blocks to generate words for a translation. We can also use a sequence of decoder blocks to generate new text. </p>
<p>Since we don’t have an encoder stage to receive KV values from, as in the full transformer of <a href="#figure20-24">Figure 20-24</a>, let’s remove the Q/KV multi-head attention layer from each decoder block, leaving us with just masked self-attention and a pair of 1×1 convolutions. The first system to do this in a big way was called the<em> Generative Pre-Training model 2</em>, or simply <em>GPT-2</em> (Radford et al. 2019). Its architecture is shown in <a href="#figure20-29" id="figureanchor20-29">Figure 20-29</a>.</p>
<span epub:type="pagebreak" title="594" id="Page_594"/><figure>
<img src="Images/F20029.png" alt="F20029" width="691" height="201"/>
<figcaption><p><a id="figure20-29">Figure 20-29</a>: A block diagram of GPT-2, made out of transformer decoder blocks without the Q/KV layer. The dashed lines stand for more repeated, identical decoder blocks. Note that because these are versions of decoder blocks, the first multi-head attention layer in each block is a masked attention layer.</p></figcaption>
</figure>
<p>Like BERT, we start with token embedding followed by positional embedding for each input word. The self-attention layer in each decoder block uses masking as before so that as we compute attention for any given word, we can only use information from that word and those that precede it.</p>
<p>The original GPT-2 model was released in several different sizes, the largest of which processed 512 tokens at a time through 48 decoder blocks with 12 heads in each, for a total of 1,542 million parameters. That’s 1.5 <em>billion </em>parameters. GPT-2 was trained on a dataset called <em>WebText</em>, which contained about eight million documents for a total of about 40GB of text (Radford et al. 2019).</p>
<p>We typically use GPT-2 by starting with the pretrained model, and then we fine-tune it by providing an additional dataset to learn from, adjusting all of the weights in the process (Alammar 2018).</p>
<p>We started each of our text generators in Chapter 19 with a seed, but that’s only one way to get them started. A simpler approach starts the system with general guidance and a prompt. This is called a <em>zero-shot </em>scenario, since the system has been given zero “shots,” or examples, for it to use as a model for new text. </p>
<p>For instance, suppose we built a system to advise us on what to wear each day. A zero-shot scenario might start with the instruction, <span class="CustomCharStyle">Describe today’s outfit</span>, followed by the prompt, <span class="CustomCharStyle">Today I should wear:</span> The generator takes it from there. It has no examples or context to work from, so it might suggest a suit of armor, a spacesuit, or a bear skin.</p>
<p>Alternatively, we can provide one or more examples, or shots. In a one-shot scenario, we might give the instruction <span class="CustomCharStyle">Describe today’s outfit</span>, followed by the example, <span class="CustomCharStyle">Yesterday I wore a blue shirt and black pants</span>, and conclude with the prompt, <span class="CustomCharStyle">Today I should wear:</span> The thinking is that the text that’s provided before the prompt can help guide the system into the kind of output we want. In this case, the bear skin would be less likely.</p>
<p>If we give the system two or three shots, but not many more, we usually call it a <em>few-shot </em>scenario (these terms don’t have sharp cutoffs). People usually prefer generators that require as few shots as possible in order to provide the output we want.</p>
<p><span epub:type="pagebreak" title="595" id="Page_595"/>Let’s see GPT-2 in action, using a medium-sized, pretrained GPT-2 model (von Platen 2020). We won’t do any fine tuning, so the system will generate text based only on its core training data. Let’s take a zero-shot approach, and give it no information except the starting prompt, <span class="CustomCharStyle">I woke up this morning to the roar of a hippopotamus</span>. Here’s a typical output, verbatim:</p>
<blockquote class="Quote">
<p class="QuotePara"><span class="Red">I woke up this morning to the roar of a hippopotamus.</span> I was in the middle of a long walk, and I saw a huge hippopotamus. I was so excited. I was so excited. I was so excited. I was so excited.</p></blockquote>

<p>At this point the system kept repeating <span class="CustomCharStyle">I was so excited</span> endlessly. That’s not the generator’s fault, it’s ours. The system is producing grammatical output, which is what we trained it to do. The problem with the output is that, despite its emphasis on excitement, it’s boring. The end of one sentence happened to lead back to the start of that same sentence, and we got locked in a loop. The system as it is now has no idea that such output is boring or undesirable.</p>
<p>To make output more interesting, we can chip away at the problem, removing characteristics of the output we see as undesirable. Let’s look at just two such changes (Vijayakumar et al. 2018; Shao et al. 2017).</p>
<p>First, let’s do away with that repetition. We can penalize the system if it generates the same group of words repeatedly. This is called an <em>n-gram penalty</em> because a sequence of <em>n</em> words is called an <em>n-gram</em> (Paulus, Xiong, and Socher 2017; Klein et al. 2017). Let’s take it easy on the system and only punish repeated pairs of words, or 2-grams.</p>
<p>Second, rather than always picking the word with the highest probability, let’s pick several of them and see where they lead. That is, for each word, we will generate more words following on from that. When we’ve followed several paths for a while, we will score each of them to determine which one has the highest probability of being generated. We use the first word of that branch as our predicted word, and throw away the rest of that branch and all of the other branches. Then we repeat the process for the next word, and the next, and so on. This is called a <em>beam search </em>(Russell and Norvig 2009). This should give us longer sequences that hang together. And instead of always choosing the very highest probability word, we’ll pick among several of the most probable words, according to their probabilities, which will help make the output less predictable. This is called increasing the <em>temperature </em>of the word selection process (a temperature of zero corresponds to always picking the most probable choice).</p>
<p>With those two changes in place, let’s try generating text again using the same starting sentence. Here is one verbatim result:</p>
<blockquote class="Quote">
<p class="QuotePara"><span class="Red">I woke up this morning to the roar of a hippopotamus.</span><span class="CustomCharStyle"> “Oh, my God,” I said. “I’m so sorry. I didn’t mean to hurt you. It’s just that I don’t know how to deal with it. You know what I mean? I’m not going to let you hurt me, okay? You’re my friend, and I love you so much. And I want you to know that you’re not alone in this. We’ve been together for a long time now, so it’s not like there’s anything we can do about it, but I can’t help but feel sorry for you, because I know you’ll never be alone with me again.</span></p></blockquote>

<p><span epub:type="pagebreak" title="596" id="Page_596"/>Wow. That’s remarkable. We could keep improving the output by using more processing techniques (Welleck et al. 2020), but this is already pretty great, particularly considering that we’re using a stripped-down GPT-2 model.</p>
<p>For comparison, let’s revisit the same Sherlock Holmes stories from Chapter 19. We started with a small, pre-trained GPT-2, fine-tuned it on these stories (Woolf 2019), and then gave it the prompt, <span class="CustomCharStyle">I asked Holmes to explain</span>.</p>
<p>We used a slightly warm temperature, so each time we ran the algorithm, we got a different result. Here’s just one, which we arbitrarily cut off after a few paragraphs.</p>
<blockquote class="Quote">
<p class="QuotePara"><span class="CustomCharStyle">I asked Holmes to explain.</span></p>
<p class="QuotePara"><span class="CustomCharStyle">“It is very simple really,” he replied. “The short answer is, of course, that the law does not apply to these matters, and that the case should be left entirely in the discretion of the Crown Prosecution and the police. But the long answer is that the case clearly does not rest with us, and that it is very much in the power of the police to prevent it.</span></p>
<p class="QuotePara"><span class="CustomCharStyle">“You remember, Watson, that it was upon this very day that the young man, Mr. Latimer, picked up that long revolver which he used to brand me. The revolver which he used to brand me, is probably the best known weapon in the world. You remember, Watson, that I said at the time that it was better not to know than to hate the fear of it. Now, we have heard, and we must not let ourselves be drawn into a dilemma. The matter has been sufficiently complicated, and it is not necessary that the facts should be remarked upon in this fashion. The man who does not know is not himself a danger to himself. Let us take the man who does.</span></p></blockquote>

<p>These results are grammatical and even refer to themselves. Compare this output to what we got from character-based autoregression with RNNs.</p>
<p>GPT-2 can do lots of other tasks well, such as running a version of the cloze test, predicting the next word of a phrase where essential information appears at least 50 tokens before, answering questions about text, summarizing documents, and translating from one language to another.</p>
<h3 id="h2-500723c20-0015">Generators Discussion</h3>
<p class="BodyFirst">GPT-2 shows that if we process 512 tokens at a time through 48 decoder layers with 12 attention heads in each, for a total of 1.5 billion parameters, we can produce some pretty good text. What if we scaled everything up? That is, we won’t modify the basic architecture at all, but just use a lot more of everything.</p>
<p>This was the plan of the successor to GPT-2, which was called (surprise) <em>GPT-3</em>. The block diagram for GPT-3 looks generally like that of GPT-2 in <a href="#figure20-29">Figure 20-29</a> (aside from some efficiency improvements). There’s just more of everything. A lot more. GPT-3 processes 2,048 tokens at a time, on 96 decoder layers, with 96 attention heads in each layer, for a total of 175 billion parameters (Brown et al. 2020). 175 billion. Training this behemoth required an estimated 355 GPU years at an estimated cost of US$4.6 million (Alammar 2018).</p>
<p><span epub:type="pagebreak" title="597" id="Page_597"/>GPT-3 was trained using a database called the <em>Common Crawl </em>dataset (Common Crawl 2020). It started with about a trillion words from books and the web. After removing duplications and cleaning the database, the database still had about 420 billion words (Raffel et al. 2020).</p>
<p>GPT-3 is capable of creating lots of different kinds of data. It was made available to the public for a period as a kind of beta test, but it’s now a commercial product (Scott 2020). During the beta test, people used GPT-3 for many applications, such as writing code for web layouts, writing actual computer programs, taking imaginary employment interviews, rewriting legal text in plain language, writing new text that looks like legal language, and, of course, writing in creative genres like fiction and poetry (Huston 2020). </p>
<p>All this power is a mixed bag. Fine-tuning such a system requires enormous resources, and it becomes harder and harder to fine tune, as that requires finding task-specific data that wasn’t in the original data.</p>
<p>If bigger is better, would even bigger still be even better still? The researchers behind GPT-3 have estimated that we can extract everything we need to know about any text (at least from the point of view of NLP-type tasks) with a model that uses 1 trillion parameters trained on 1 trillion tokens (Kaplan et al. 2020). These numbers are rough predictions and could be far off, but it’s interesting to think that there could be a point at which a stack of decoder blocks (and some support mechanisms) could extract almost all the information we need from a piece of text. We’ll probably know the answer soon, as other huge firms with enormous resources are sure to produce their own gargantuan NLP systems trained on their own colossal databases. Training these vast systems is a game only the big and rich can play.</p>
<p>On a light-hearted note, we can play an interactive text-based fantasy game online, driven by a GPT-3 implementation (Walton 2020). The system was trained on a variety of genres, ranging from fantasy and cyberpunk to spy stories. Perhaps the most fun way to play with this system is to treat the AI as an improv partner, agreeing with and expanding on whatever the system throws at us. Let the AI set the flow and go with it.</p>
<p>Generated text can often hold up well in short doses, but how well does it do when we look closer? A recent study asked many language generators, including GPT-3, to perform 57 tasks, based on topics from humanities like law and history, to social sciences like economics and psychology, and STEM subjects like physics and mathematics (Hendrycks et al. 2020). Most output never came near human performance. The systems fared especially poorly on important social issues like morality and law.</p>
<p>This shouldn’t be a surprise. These systems are simply producing words based on their probabilities of belonging together. In a real and fundamental sense, they have no idea what they’re talking about. </p>
<p>For all their power, text generators like those we’ve seen here have no common sense. Worse, they blindly reiterate the stereotypes and prejudices inherited wholesale from the gender, racial, social, political, age, and other biases in their training data. Text generators have no idea of accuracy, fairness, kindness, or honesty. They don’t know when they’re stating facts or <span epub:type="pagebreak" title="598" id="Page_598"/>making things up. They just generate words that follow the statistics of the training data, and perpetuate every prejudice and limitation to be found there.</p>
<h3 id="h2-500723c20-0016">Data Poisoning</h3>
<p class="BodyFirst">We saw in Chapter 17 that adversarial attacks can trick convolutional neural networks into generating incorrect results. Natural language processing algorithms are also susceptible to intentional attacks, called <em>data poisoning</em>.</p>
<p>The idea behind data poisoning is to manipulate the training data for an NLP system in such a way that the system produces a desired type of inaccurate result, perhaps consistently, or perhaps only in the presence of a triggering word or phrase. For example, one can insert sentences or phrases into the training data that suggest that strawberries are made of cement. If these new entries are not discovered, then if the system is later used to generate stocking orders for a supermarket or a building contractor, they may find that their inventories end up being consistently and mysteriously wrong.</p>
<p>This is particularly concerning because, as we’ve seen, NLP systems are typically trained on massive databases of millions or billions of words, so nobody is carefully reviewing the database for misleading phrases. Even if one or more people carefully read the entire training set, the poisoning texts can be designed so that they never explicitly refer to their targets, making them essentially indetectable and their effects unpredictable.</p>
<p>Returning to our previous example, such phrases can convince a system that strawberries are made of cement, while never referring to fruit or building materials at all. This is called <em>concealed data poisoning</em>, and it can be fiendishly hard to detect and prevent (Wallace et al. 2020).</p>
<p>Another kind of attack is based on changing the training data in a seemingly benign way. Suppose we’re working with a system that classifies news headlines into different categories. Any given headline can be subtly rewritten so that the obvious meaning is not changed, but the story is incorrectly classified. For instance, the original headline, <span class="CustomCharStyle">Turkey is put on track for EU membership</span>, would be correctly classified under “World.” But if an editor rephrases this into the active voice—<span class="CustomCharStyle">EU puts Turkey on track for full membership</span>—this would now be misclassified as “Business” (Xu, Ramirez, and Veeramachaneni 2020).</p>
<p>Data poisoning is particularly nefarious for several reasons. First, it can be done by people who have no connection to the organizations building or training the NLP models. Since significant amounts of training data are usually drawn from public sources, such as the web, a poisoner only needs to publish the manipulative phrases in a public blog or other location where they’re likely to be scooped up and used. Second, data poisoning can be done well ahead of any specific system’s use, or indeed, even before it’s conceived of.</p>
<p>There’s no knowing how much training data has already been poisoned and is simply awaiting activation, like the sleeper agents in <em>The Manchurian Candidate</em> (Frankenheimer 1962). Finally, unlike adversarial attacks on CNNs, poisoned data compromises the NLP system from within, making its influence an inherent part of the trained model.</p>
<p><span epub:type="pagebreak" title="599" id="Page_599"/>When a compromised system is used to make important decisions, such as evaluating school admission essays, interpreting medical notes, monitoring social media for fraud and manipulation, or searching legal records, then data poisoning can produce errors that change the course of people’s lives. Before any NLP system is used in such sensitive applications, in addition to examining it for signs of bias and historical prejudice, we must also analyze it for data poisoning, and certify it as safe only if it is demonstrably not biased or poisoned. Unfortunately, no methods for robust detection or certification of any of these problems currently exist.</p>
<h2 id="h1-500723c20-0005">Summary</h2>
<p class="BodyFirst">We started this chapter with word embedding, which assigns each word a vector in a high-dimensional space representing its use. We saw how ELMo lets us capture multiple meanings based on content.</p>
<p>We discussed the mechanism of attention, which lets us simultaneously find words in the input that seem related, and build combinations of versions of the vectors describing those words. </p>
<p>Then we looked at transformers, which do away with recurrent cells entirely and replace them with multiple attention networks. This change allows us to train in parallel, which is of huge practical value.</p>
<p>Finally, we saw how to use multiple transformer encoder blocks to build BERT, a system for high-quality encoding, and how to use multiple decoder blocks to build GPT-2, a high-quality text generator.</p>
<p>In the next chapter we’ll turn our attention to reinforcement learning, which offers a way to train neural networks by evaluating their guesses, rather than expecting them to predict a single correct answer.</p>
</section>
</div></body></html>