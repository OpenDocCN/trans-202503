- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Essential Statistics
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 基础统计学
- en: '![](Images/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/chapterart.png)'
- en: The better we understand our data, the better we can design a deep learning
    system that can make the most of that data.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对数据的理解越深入，就能设计出更好的深度学习系统，从而最大化地利用这些数据。
- en: By studying and analyzing the data we’re starting with, we can pick the best
    algorithms for learning from it. The ideas and tools that let us do this analysis
    are generally bundled together under the heading of *statistics*. Statistical
    ideas and language appear everywhere in machine learning, from papers and source
    code comments to library documentation.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 通过研究和分析我们所开始的数据，我们可以选择最佳的算法来从中学习。让我们能够进行这种分析的思想和工具通常被归纳为*统计学*。统计学的概念和语言在机器学习中随处可见，从论文和源代码注释到库文档。
- en: In this chapter, we’ll cover the key statistical ideas essential to doing deep
    learning without delving into the math or details. The ideas fall roughly into
    two categories. The first category involves random numbers and how to describe
    them in ways that are of the most value in machine learning. The second category
    involves the ways in which we can choose objects (like numbers) from a collection
    and how to measure how well such selections represent the collection as a whole.
    Our goal here is to develop enough understanding and intuition to help us make
    good decisions when we do machine learning.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖进行深度学习时所需的关键统计学概念，而不深入探讨数学或细节。这些概念大致分为两类。第一类涉及随机数，以及如何以对机器学习最有价值的方式描述它们。第二类涉及如何从一个集合中选择对象（如数字），以及如何衡量这些选择如何代表整个集合。我们在这里的目标是发展足够的理解和直觉，以帮助我们在进行机器学习时做出正确的决策。
- en: If you’re already familiar with statistics and random values, at least skim
    the chapter. That way you’ll know the language we use in this book, and you’ll
    know where to come back to later if you want to brush up.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经熟悉统计学和随机值，至少快速浏览一下本章。这样你将了解我们在本书中使用的语言，并且如果你想复习时可以知道哪里需要回头查看。
- en: Describing Randomness
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 描述随机性
- en: 'Random numbers play an important role in many machine learning algorithms.
    We use them to initialize our systems, to control steps during the learning process,
    and sometimes even to influence output. Picking random numbers properly is important:
    doing so can mean the difference between a system that learns from our data and
    produces useful results and a system that stubbornly refuses to learn anything.
    Rather than simply picking some arbitrary numbers out of thin air, we use a variety
    of tools to control what kinds of numbers we want to use and how we select them.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 随机数在许多机器学习算法中起着重要作用。我们使用它们来初始化系统、控制学习过程中的步骤，有时甚至影响输出。正确选择随机数非常重要：这样做可能意味着一个系统能够从我们的数据中学习并产生有用的结果，或者一个系统顽固地拒绝学习任何东西。我们不会仅仅凭空挑选一些任意的数字，而是使用各种工具来控制我们想要使用哪些类型的数字，以及如何选择它们。
- en: We typically select a random number between a given minimum and maximum, such
    as when someone has us “pick a number from 1 to 10.” In this example, it’s implied
    that our choice is limited to a finite number of options (the integers from 1
    to 10). We’ll frequently work with *real* numbers, which may lie between the integers.
    There are 10 integers from 1 to 10 (including the endpoints), but there is an
    infinite quantity of real numbers in that range.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常在给定的最小值和最大值之间选择一个随机数，比如当有人让我们“从1到10中选择一个数字”时。在这个例子中，意味着我们的选择限制在有限的选项范围内（从1到10的整数）。我们还经常使用*实数*，这些数字可能位于整数之间。从1到10之间有10个整数（包括端点），但是在这个范围内有无限多个实数。
- en: 'When we talk about collections of numbers, random or otherwise, we often also
    talk about their *average*. This is a useful way to quickly characterize the collection
    of values. There are three different common ways to compute an average, and they
    come up frequently, so we’ll identify them here. As a running example, let’s work
    with a list of five numbers: 1, 3, 4, 4, 13.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们讨论数字集合时，无论是随机的还是其他类型的，我们通常还会谈到它们的*平均值*。这是一个快速描述数值集合的有用方法。有三种常见的计算平均值的方法，它们经常出现，因此我们将在这里标出它们。作为一个示例，我们使用五个数字的列表：1,
    3, 4, 4, 13。
- en: The *mean* is the value usually meant in everyday language when we say *average*.
    It’s the sum of all the entries divided by the number of entries in the list.
    In our example, adding together all the list elements gives us 1 + 3 + 4 + 4 +
    13 = 25\. There are five elements, so the mean is 25 / 5, or 5.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*均值*是我们在日常语言中所说的*平均数*。它是所有条目之和除以列表中的条目数量。在我们的例子中，将所有列表元素加起来得到1 + 3 + 4 + 4
    + 13 = 25。共有五个元素，所以均值是25 / 5，或者5。'
- en: The *mode*is the value that occurs the most often in the list. In our example,
    4 appears twice, and the other three values each appear only once, so 4 is the
    mode. If no value occurs more often than any other, we say the list has no mode.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*众数*是列表中出现次数最多的值。在我们的例子中，4出现了两次，而其他三个值各出现一次，所以4是众数。如果没有任何值比其他值出现得更多，我们就说列表没有众数。'
- en: Finally, the *median* is the number in the middle of the list when we write
    the values sorted from the smallest to the largest. In our list, which is already
    sorted, 1 and 3 make up the left side, 4 and 13 make up the right side, and another
    4 is in the middle. So 4 is the median. If a list has an even number of entries,
    then the median is the mean of the two middle entries. For the list 1, 3, 4, 8,
    the median would be the average of 3 and 4, which is 3.5\.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，*中位数*是当我们将列表中的值从小到大排序后，位于中间的那个数。在我们的列表中，已排序的列表为1、3、4、4、13，1和3构成左边部分，4和13构成右边部分，另一个4位于中间。因此，4是中位数。如果列表包含偶数个条目，那么中位数是两个中间条目的平均值。例如，对于列表1、3、4、8，中位数是3和4的平均值，即3.5。
- en: Averages are useful, but they don’t tell us much about how the numbers in a
    collection are distributed. For example, they might be spread out equally across
    their range, or grouped into one or more clusters. We’ll now look at the techniques
    that let us describe how numbers are distributed.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 平均值很有用，但它并不能告诉我们集合中数字的分布情况。例如，数字可能在整个范围内均匀分布，也可能分成一个或多个聚类。接下来，我们将探讨描述数字分布情况的技术。
- en: Random Variables and Probability Distributions
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机变量和概率分布
- en: Before getting into details, let’s build up our intuition with a running analogy.
    Suppose we’re photographers who have been assigned to support an article on auto
    junkyards by taking lots of pictures of broken-down trucks and cars. Feeling adventurous,
    we go to a junkyard that contains many broken-down vehicles. We talk to the owner,
    and we agree that the best way to get great photos is for us to pay her to bring
    us vehicles to photograph, one by one. She makes it fun by using an old carnival
    wheel she has in her office. It has one equally sized slot for each car on the
    lot, as in [Figure 2-1](#figure2-1). Both the slots and the cars are numbered
    starting at 1.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入细节之前，让我们通过一个类比来培养直觉。假设我们是摄影师，被指派拍摄关于汽车废品场的文章，任务是拍摄大量的报废卡车和汽车。出于冒险的心态，我们去了一个有很多报废车辆的废品场。我们与老板交谈，达成一致，最好的拍摄方式是由我们付钱，让她把车一辆一辆地送来拍照。她通过在办公室里使用一个老式的嘉年华轮盘让过程更加有趣。轮盘上每个车位大小相等，正如[图2-1](#figure2-1)所示。车位和车辆都从1开始编号。
- en: '![F02001](Images/F02001.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![F02001](Images/F02001.png)'
- en: 'Figure 2-1: The junkyard owner’s carnival wheel. Each equally sized sliver
    represents one car in her lot.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图2-1：废品场老板的嘉年华轮盘。每个大小相等的扇区代表她场地中的一辆车。
- en: Once we pay her, she spins the wheel. When the wheel stops, she notes the number
    at the top, goes out with her tow truck, and drags the vehicle with the corresponding
    number back to us. We take some pictures, and she returns the vehicle to the lot.
    If we want to photograph another vehicle, we pay again, she spins the wheel again,
    and the process repeats.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们付钱，她就会旋转轮盘。当轮盘停止时，她记录下顶部的数字，然后带着拖车出去，把对应编号的车辆拖回来。我们拍照后，她将车辆送回废品场。如果我们想拍摄另一辆车，就再次付费，她再旋转轮盘，过程就这样重复。
- en: 'Suppose that our assignment calls for us to get pictures of five different
    types of cars: a sedan, a pickup, a minivan, an SUV, and a wagon. For each type
    of car, we would like to know the chance that if she spins the wheel, we’ll get
    that type. To work that out, let’s suppose that we go into the lot to examine
    every vehicle, and we assign each one to one of these five categories. Our results
    are shown in [Figure 2-2](#figure2-2).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的任务是拍摄五种不同类型的车：轿车、皮卡、厢式车、SUV和旅行车。对于每种车，我们希望知道她旋转轮盘时，我们得到该类型车辆的概率。为了计算这一点，假设我们进入车场，检查每辆车，并将其分配到这五个类别中的一个。我们的结果如[图2-2](#figure2-2)所示。
- en: '![F02002](Images/F02002.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![F02002](Images/F02002.png)'
- en: 'Figure 2-2: Our junkyard has five different types of cars in it. Each bar tells
    us how many cars we have of that type.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图2-2：我们的废品场中有五种不同类型的车。每个条形显示了我们拥有的该类型车的数量。
- en: Of the almost 950 cars on her lot, the largest population is minivans, followed
    by pickups, wagons, sedans, and SUVs, in that order. Since every vehicle on her
    lot has an equal chance of being selected, on each spin of the wheel, we’re most
    likely to get a minivan.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在她的车场上，几乎有950辆车，其中最大数量的是迷你面包车，其次是皮卡车、旅行车、轿车和SUV，按此顺序排列。由于车场上的每辆车被选中的机会是均等的，因此在每次转动轮盘时，我们最有可能选中迷你面包车。
- en: But specifically how *much* more likely are we to get a minivan? To determine
    how likely it is that we’ll get each kind of vehicle, we can divide the height
    of each bar in [Figure 2-2](#figure2-2) by the total number of vehicles. This
    value gives us the probability that we’ll get that given type of car, as shown
    in [Figure 2-3](#figure2-3).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 但具体来说，获得迷你面包车的可能性*有多大*？为了确定获得每种类型车辆的可能性，我们可以将[图2-2](#figure2-2)中每个条形的高度除以车辆的总数。这个数值给出了我们获得某种类型汽车的概率，如[图2-3](#figure2-3)所示。
- en: '![F02003](Images/F02003.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![F02003](Images/F02003.png)'
- en: 'Figure 2-3: The probability of getting each type of car in the junkyard'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图2-3：获得废品场中每种类型汽车的概率
- en: 'To convert the numbers in [Figure 2-3](#figure2-3) into percentages, we multiply
    them by 100\. For example, the height of the minivan bar is about 0.34, so we
    say that there’s a 34 percent chance of getting a minivan. We say that the height
    of each bar is the *probability* that we’ll get that kind of vehicle. If we add
    up the heights of all five bars, we’ll find that the sum is 1.0\. This illustrates
    the rules that turn any list of numbers into probabilities: the values must all
    be between 0 and 1, and add up to 1\.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 要将[图2-3](#figure2-3)中的数值转换为百分比，我们将它们乘以100。例如，迷你面包车条形的高度约为0.34，因此我们说获得迷你面包车的概率是34%。我们说每个条形的高度是我们获得那种类型车辆的*概率*。如果我们将所有五个条形的高度相加，我们会发现总和是1.0。这说明了将任何数字列表转化为概率的规则：数值必须介于0和1之间，且总和为1。
- en: We call [Figure 2-3](#figure2-3) a *probability distribution* because it’s distributing
    the 100 percent probability of getting some vehicle among the available options.
    We also sometimes say that [Figure 2-3](#figure2-3) is a *normalized* version
    of [Figure 2-2](#figure2-2), which means that the values all add up to 1.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将[图2-3](#figure2-3)称为*概率分布*，因为它将获得某种车辆的100%概率分配到所有可选项中。我们有时也说[图2-3](#figure2-3)是[图2-2](#figure2-2)的*归一化*版本，这意味着所有数值相加的结果是1。
- en: We can use our probability distribution to draw a simplified carnival wheel,
    as in [Figure 2-4](#figure2-4).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用我们的概率分布来绘制一个简化版的嘉年华轮盘，如[图2-4](#figure2-4)所示。
- en: '![F02004](Images/F02004.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![F02004](Images/F02004.png)'
- en: 'Figure 2-4: A simplified carnival wheel that tells us what kind of vehicle
    we’ll get if the owner spins the big wheel of [Figure 2-1](#figure2-1)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图2-4：一个简化版的嘉年华轮盘，告诉我们如果车主转动[图2-1](#figure2-1)的大轮盘，我们会获得哪种类型的车辆
- en: The chance that the wheel will end up with the pointer in a given region is
    given by the portion of the wheel’s circumference taken up by that region, which
    we’ve drawn with the same proportions as those shown in [Figure 2-3](#figure2-3).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 轮盘的指针停在某个区域的概率由该区域所占轮盘周长的比例决定，这个比例与[图2-3](#figure2-3)中显示的比例相同。
- en: Most of the time we don’t have carnival wheels around when we generate random
    numbers on the computer. Instead, we rely on software to simulate the process.
    For instance, we might give a library function a list of values, like the heights
    of the bars in [Figure 2-3](#figure2-3), and ask it to return a value. We expect
    that we’ll get back minivan about 34 percent of the time, pickup about 26 percent
    of the time, and so on.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数时候，在我们生成计算机上的随机数时并没有嘉年华轮盘。相反，我们依赖软件来模拟这一过程。例如，我们可能将一个数值列表（如[图2-3](#figure2-3)中条形的高度）传递给一个库函数，并要求它返回一个值。我们期望大约34%的时间会返回迷你面包车，大约26%的时间会返回皮卡车，依此类推。
- en: The job of picking a value at random from a list of options, each with its own
    probability, takes a bit of work. For convenience, we package up this selection
    process into its own conceptual procedure called a *random variable*.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 从一系列具有各自概率的选项中随机选一个值的工作需要一些操作。为了方便，我们将这个选择过程封装成一个概念化的过程，称为*随机变量*。
- en: This term can create confusion for programmers, because programmers think of
    a variable as being a named piece of storage that can hold data. In this context,
    rather than a piece of storage, a random variable is a *function* (Wikipedia 2017b),
    which takes a probability distribution as an input and produces a single value
    as an output. The process of selecting a value from a distribution is called *drawing*
    a value from the random variable.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这个术语可能会让程序员感到困惑，因为程序员通常将变量视为一个有名称的存储单元，可以用来存储数据。在这个上下文中，随机变量不是一个存储单元，而是一个*函数*（维基百科
    2017b），它接受一个概率分布作为输入，并产生一个单一的值作为输出。从分布中选择一个值的过程叫做*从随机变量中抽取*一个值。
- en: We called [Figure 2-3](#figure2-3) a probability distribution, but we can also
    think of it as a function. We call the function, and it returns one of the types
    of vehicles, given these probabilities. This idea leads us to two more formal
    names for a distribution. When there are only a finite number of possible return
    values, like the five values in [Figure 2-3](#figure2-3), we sometimes use the
    oblique name *probability mass function* or *pmf* (this acronym is usually written
    in lowercase).A pmf is also sometimes called a *discrete probability distribution*
    (adding *function* at the end of this term is optional). These terms are meant
    to remind us that there are only a fixed number of possible outputs.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称[图 2-3](#figure2-3)为概率分布，但我们也可以将其视为一个函数。我们调用该函数，它会根据这些概率返回某一种类型的车辆。这个概念引出了分布的两个更正式的名称。当只有有限数量的可能返回值时，比如[图
    2-3](#figure2-3)中的五个值，我们有时会使用斜体名称*概率质量函数*或*pmf*（这个缩写通常写为小写）。pmf 有时也被称为*离散概率分布*（在这个术语末尾加上*函数*是可选的）。这些术语旨在提醒我们，只有固定数量的可能输出。
- en: We can easily create probability distributions that are continuous. We use approximations
    of such functions when we initialize the values in a neural network. As an analogy,
    let’s suppose that we want to know how much oil is left in each car that our junkyard
    dealer brings us. The amount of oil is a continuous variable, because it can take
    on any real number. [Figure 2-5](#figure2-5) shows a continuous graph for our
    oil measurements. This graph shows us the probability of getting back not just
    a few specific values, but any real value at all, here between 0 (empty) and 1
    (full).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以轻松创建连续的概率分布。当我们初始化神经网络中的值时，我们使用这些函数的近似值。做个类比，假设我们想知道废车场商人带给我们的每辆车剩下多少油。油量是一个连续变量，因为它可以取任何实数。[图
    2-5](#figure2-5)展示了我们的油量测量的连续图表。这个图表告诉我们，不仅仅是几个特定值的概率，而是任何实数值的概率，这些值位于 0（空）和 1（满）之间。
- en: '![F02005](Images/F02005.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![F02005](Images/F02005.png)'
- en: 'Figure 2-5: A distribution of probabilities for a continuous range of values'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2-5：连续范围值的概率分布
- en: A distribution like [Figure 2-5](#figure2-5) is called a *continuous probability
    distribution* (or *cpd*), or a *probability density function* (or *pdf*). Sometimes
    people casually use the term probability density function, or pdf, to refer to
    discrete distributions, rather than continuous ones. The context usually makes
    it clear which interpretation is intended.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图 2-5](#figure2-5)所示的分布被称为*连续概率分布*（或*cpd*），或*概率密度函数*（或*pdf*）。有时，人们会随意使用概率密度函数或
    pdf 来指代离散分布，而不是连续分布。通常情况下，上下文会清楚地表明是指哪种解释。
- en: Recall that for the discrete case, all the possible return values need to add
    up to 1\. In the continuous case, as in [Figure 2-5](#figure2-5), that means that
    the area under the curve is 1\.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，对于离散情况，所有可能的返回值需要加起来为 1。在连续情况下，如[图 2-5](#figure2-5)所示，这意味着曲线下方的面积为 1。
- en: Most of the time, we obtain random numbers by selecting a distribution and then
    calling a library function to produce a value from that distribution (that is,
    it draws a random variable from our given distribution). We can make our own distributions
    when we want them, but most libraries provide a handful of distributions that
    have been found to cover most situations. That way we can just use one of these
    prebuilt distributions when we choose our random numbers. Let’s take a look at
    some of these distributions.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数时候，我们通过选择一个分布，然后调用一个库函数来从该分布中生成一个值，从而获得随机数（即，从我们给定的分布中抽取一个随机变量）。当我们需要时，可以自己创建分布，但大多数库提供了一些已经被证明适用于大多数情况的分布。这样，我们只需在选择随机数时使用这些预构建的分布之一。让我们来看一下这些分布。
- en: Some Common Distributions
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一些常见的分布
- en: 'We’ve mentioned that we can draw a random variable from a distribution. Each
    time we draw a random variable, it takes on a number in accordance with the distribution:
    numbers with a large corresponding value in the distribution are more likely than
    those with a smaller value in the distribution. This makes distributions of great
    practical value, since different algorithms will want to use random variables
    that take on different values with particular probabilities. To achieve this,
    we merely need to pick an appropriate distribution.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到过，可以从一个分布中抽取一个随机变量。每次抽取随机变量时，它会根据分布的规律取一个数值：在分布中对应值较大的数字比对应值较小的数字更可能出现。这使得分布具有很大的实际价值，因为不同的算法会希望使用具有特定概率值的随机变量。为此，我们只需要选择一个合适的分布。
- en: Continuous Distributions
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 连续分布
- en: Most of the following distributions are offered as built-in routines by major
    libraries, so they’re easy to specify and use. For simplicity, we’ll demonstrate
    the following two distributions in their continuous forms. Most libraries offer
    us a choice between continuous and discrete versions, or they may offer a general-purpose
    routine to turn any continuous distribution into a discrete one on demand, or
    vice versa. We’ll look at some discrete distributions later in the section.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 以下大多数分布都是由主要库提供的内置例程，因此它们易于指定和使用。为了简便起见，我们将演示以下两个分布的连续形式。大多数库为我们提供了连续版本和离散版本的选择，或者它们可能提供一个通用例程，根据需要将任何连续分布转换为离散分布，反之亦然。我们将在后面的部分讨论一些离散分布。
- en: The Uniform Distribution
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 均匀分布
- en: '[Figure 2-6](#figure2-6) shows the *uniform distribution*. The basic uniform
    distribution is 0 everywhere except between 0 and 1, where it has the value 1.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2-6](#figure2-6) 显示了*均匀分布*。基本的均匀分布在0到1之间的取值为1，其他地方的取值为0。'
- en: In [Figure 2-6](#figure2-6), it may appear that there are two values at 0 and
    two values at 1, but there aren’t. Our convention is that an open circle (as on
    the lower line) means “this point is not part of the line,” and a closed circle
    (as on the upper line) means “this point is part of the line.” So, at the input
    values 0 and 1, our graph has an output of 1\. This is a common way to define
    this function, but some implementations make either or both of those outputs 0\.
    It always pays to check.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图2-6](#figure2-6)中，可能看起来在0和1处各有两个值，但实际上并没有。我们的约定是，开放圆圈（如下线所示）表示“该点不属于该线”，而闭合圆圈（如上线所示）表示“该点属于该线”。因此，在输入值为0和1时，图形的输出值为1。这是一种常见的函数定义方式，但有些实现可能会将其中一个或两个输出值设为0。因此，检查非常重要。
- en: '![F02006](Images/F02006.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![F02006](Images/F02006.png)'
- en: 'Figure 2-6: An example of a uniform distribution'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图2-6：均匀分布的示例
- en: This distribution has two key features. First, we can only get back values between
    0 and 1, because the probability of all other values is 0\. Second, every value
    in the range 0 to 1 is equally probable. It’s just as likely we’d get 0.25 as
    0.33 or 0.793718\. We say that [Figure 2-6](#figure2-6) is *uniform*, or *constant*,
    or *flat*, in the range 0 to 1, all of which tell us that all the values in that
    range are equally probable. We also say that it’s *finite*, meaning that all the
    nonzero values are within some specific range (that is, we can say with certainty
    that 0 and 1 are the smallest and largest values it can return).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分布有两个关键特征。首先，我们只能得到0到1之间的值，因为所有其他值的概率为0。第二，在0到1的范围内，每个值的概率是相同的。我们得到0.25、0.33或0.793718的概率是一样的。我们说[图2-6](#figure2-6)是*均匀*的，或*常数*的，或*平坦*的，在0到1的范围内，这三种说法都意味着该范围内的所有值的概率是相同的。我们还说它是*有限*的，意味着所有非零值都在某个特定的范围内（也就是说，我们可以确定0和1是它能返回的最小值和最大值）。
- en: Library functions that create uniform distributions for us often let us choose
    to start and end the nonzero region where we like. Probably the most popular choice,
    after the default of 0 to 1, is the range −1 to 1\. The library takes care of
    details like adjusting the height of the function so that the area is always 1.0
    (recall that this is required if a graph is to represent a probability distribution).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为我们创建均匀分布的库函数通常允许我们选择非零区域的起始点和结束点。除了默认的0到1范围外，最流行的选择可能是−1到1的范围。库会处理诸如调整函数高度以确保面积始终为1.0等细节（回想一下，如果图形要表示一个概率分布，这是必需的）。
- en: The Normal Distribution
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 正态分布
- en: Another frequently used distribution is the *normal distribution*, also called
    the *Gaussian distribution*, or simply the *bell curve*. Unlike the uniform distribution,
    it’s smooth and has no sharp corners or abrupt jumps.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常用的分布是*正态分布*，也叫做*高斯分布*，或简单地称为*钟形曲线*。与均匀分布不同，它是平滑的，没有尖锐的角或突兀的跳跃。
- en: '[Figure 2-7](#figure2-7) shows a few typical normal distributions.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2-7](#figure2-7)展示了几个典型的正态分布。'
- en: All four curves in [Figure 2-7](#figure2-7) have the same basic shape. The shapes
    only vary because we scaled the curve, moved it horizontally, or both. For these
    illustrations, we didn’t scale the curve so that the area under it sums to 1.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2-7](#figure2-7)中的所有四条曲线具有相同的基本形状。它们的形状之所以不同，是因为我们对曲线进行了缩放、水平移动，或两者同时进行。对于这些插图，我们没有对曲线进行缩放，使得曲线下面的面积之和为1。'
- en: '![F02007](Images/F02007.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![F02007](Images/F02007.png)'
- en: 'Figure 2-7: A few normal distributions'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2-7：一些正态分布
- en: '[Figure 2-8](#figure2-8) shows some representative samples that we’d get by
    drawing values from each distribution. They bunch up where the distribution’s
    value is high (that is, getting a sample at that value has a high probability),
    and they are sparser where the distribution’s value is low (where getting back
    a sample has a low probability). The vertical locations of the red dots representing
    the samples are jittered only to make the samples easier to see, and have no meaning.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2-8](#figure2-8)展示了通过从每个分布中抽取值得到的一些代表性样本。它们在分布值高的地方聚集（即，在该值处抽到样本的概率较高），在分布值低的地方则较为稀疏（即，在该值处抽到样本的概率较低）。红点的垂直位置仅为了使样本更容易观察而进行了抖动，没有实际意义。'
- en: '![F02008](Images/F02008.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![F02008](Images/F02008.png)'
- en: 'Figure 2-8: Each red circle shows the value of a sample resulting from drawing
    a value from its normal distribution.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2-8：每个红色圆圈表示从其正态分布中抽取的一个样本的值。
- en: The normal distribution is nearly 0 almost everywhere, except for the region
    where it rises up in a smooth bump. Though the values drop off ever closer to
    0 to the sides of the bump, they never quite reach 0\. So we say that the width
    of this distribution is *infinite*. In practice, we usually treat any value that’s
    very nearly 0 as actually 0, giving us a finite distribution. A few other terms
    sometimes come up when people discuss normal distributions. The values produced
    by random variables from a normal distribution are sometimes called *normal deviates*,
    and are said to be *normally distributed*. We also say these values *fit*, or
    follow, a normal distribution.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 正态分布在几乎所有地方都接近0，除了它在某个区域平滑地上升形成凸起。尽管值在凸起两侧逐渐趋近于0，但永远不会真正达到0。因此我们说这个分布的宽度是*无限的*。实际上，我们通常将接近0的任何值视为0，从而得到一个有限的分布。在讨论正态分布时，还会出现一些其他术语。由正态分布的随机变量产生的值有时称为*正态偏差*，并且说这些值是*正态分布的*。我们也说这些值*符合*或遵循正态分布。
- en: 'Each normal distribution is defined by two numbers: the *mean* and the *standard
    deviation*. The mean tells us the location of the center of the bump. [Figure
    2-9](#figure2-9) shows our four Gaussians from [Figure 2-7](#figure2-7), with
    their means. Here’s one of the many nice properties of a normal distribution:
    its mean is also its median and its mode.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 每个正态分布由两个数字定义：*均值*和*标准差*。均值告诉我们凸起中心的位置。[图 2-9](#figure2-9)展示了[图 2-7](#figure2-7)中的四个高斯分布及其均值。这是正态分布的许多优良性质之一：其均值也是中位数和众数。
- en: '![F02009](Images/F02009.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![F02009](Images/F02009.png)'
- en: 'Figure 2-9: The mean of a normal distribution is the center of the bump, here
    shown with a red line.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2-9：正态分布的均值是凸起中心的位置，这里用红线表示。
- en: The *standard deviation* is also a number, often represented by the lower-case
    Greek letter σ (sigma), which tells us the width of the bump. Imagine starting
    at the center of the bump and moving symmetrically outward until we’re enclosing
    about 68 percent of the total area under the curve. The distance from the center
    of the bump to either of these ends is called *one standard deviation* for that
    curve. [Figure 2-10](#figure2-10) shows our four Gaussians, with the area inside
    one standard deviation shaded in green.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*标准差*也是一个数字，通常用小写希腊字母σ（西格玛）表示，它告诉我们凸起的宽度。可以想象从凸起的中心开始，向外对称移动，直到包围了曲线下面约68%的总面积。凸起中心到这些端点的距离叫做该曲线的*一个标准差*。[图
    2-10](#figure2-10)展示了我们的四个高斯分布，其中一个标准差范围内的区域用绿色阴影表示。'
- en: 'We can use the standard deviation to characterize a bump: when the standard
    deviation is small, it means the bump is narrow. As the standard deviation increases,
    the bump becomes more spread out horizontally.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用标准差来表征一个凸起：当标准差较小时，意味着凸起较窄；当标准差增大时，凸起在水平方向上变得更加宽广。
- en: '![F02010](Images/F02010.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![F02010](Images/F02010.png)'
- en: 'Figure 2-10: Some normal distributions with the area within one standard deviation
    shaded in green'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图2-10：一些正态分布，其中一个标准差内的区域用绿色阴影标出
- en: If we go out symmetrically from the center by another standard deviation (that
    is, the same distance again), then we’ve enclosed about 95 percent of the area
    under the curve, as shown in [Figure 2-11](#figure2-11). And if we go out one
    more standard deviation, we’ve enclosed about 99.7 percent of the area under the
    curve, also shown in [Figure 2-11](#figure2-11). This property is sometimes called
    the *three-sigma rule*, because of the use of σ for the standard deviation. It
    also sometimes goes by the catchy name of the *68-95-99.7 rule*.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们从中心对称地再往外走一个标准差（也就是相同的距离），那么我们就涵盖了大约95%的曲线下方的区域，如[图2-11](#figure2-11)所示。再往外走一个标准差，我们就涵盖了大约99.7%的曲线下方的区域，这也在[图2-11](#figure2-11)中展示了。这一特性有时被称为*三西格玛规则*，因为标准差使用的是符号σ。它有时也被称为*68-95-99.7规则*，这个名字听起来也很有吸引力。
- en: '![F02011](Images/F02011.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![F02011](Images/F02011.png)'
- en: 'Figure 2-11: The three-sigma, or 68-95-99.7 rule'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图2-11：三西格玛规则，或68-95-99.7规则
- en: Suppose we look at 1,000 samples drawn from any normal distribution. We would
    find that about 680 of them are no more than one standard deviation from that
    distribution’s mean, or in the range −σ to σ; about 950 of them are within two
    standard deviations, or in the range −2σ to 2σ; and about 997 of them are within
    three standard deviations, or in the range −3σ to 3σ.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们从任何正态分布中抽取1000个样本。我们会发现，大约680个样本的值距离该分布的均值不超过一个标准差，即在范围−σ到σ之间；大约950个样本的值在两个标准差范围内，即在−2σ到2σ之间；大约997个样本的值在三个标准差范围内，即在−3σ到3σ之间。
- en: To summarize, the mean tells us where the center of the curve is, and the standard
    deviation tells us how spread out the curve is. The larger the standard deviation,
    the broader the curve, because that 68 percent cutoff is farther away.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，均值告诉我们曲线的中心在哪里，而标准差告诉我们曲线的分布范围有多广。标准差越大，曲线就越宽，因为68%的临界值离中心越远。
- en: 'Sometimes instead of the standard deviation, people use a different but related
    value called the *variance*. The variance is just the standard deviation multiplied
    by itself (that is, the standard deviation squared). This value is sometimes more
    convenient in calculations. The general interpretation is the same, though: curves
    with big variances are more spread out than curves with small variances.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 有时人们会使用一个不同但相关的值，称为*方差*，而不是标准差。方差就是标准差的平方（即标准差乘以自身）。这个值在计算中有时更为方便。尽管如此，它的一般解释是相同的：方差较大的曲线比方差较小的曲线更为分散。
- en: The normal distribution appears frequently in machine learning and in other
    fields, because it naturally describes many real-world observations. If we measure
    the height of adult male horses in some region, or the size of sunflowers, or
    the lifespans of fruit flies, we’ll find that these all tend to take on the shape
    of a normal distribution.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 正态分布在机器学习和其他领域中经常出现，因为它自然地描述了许多现实世界的观察结果。如果我们测量某一地区成年公马的身高，或是向日葵的大小，或者果蝇的寿命，我们会发现这些数据往往呈现出正态分布的形态。
- en: Discrete Distributions
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 离散分布
- en: Now let’s look at two discrete distributions.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看一下两个离散分布。
- en: The Bernoulli Distribution
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 伯努利分布
- en: 'A useful, but special, discrete distribution is the *Bernoulli distribution*
    (pronounced ber-noo′-lee). This distribution returns just two possible values:
    0 and 1\. A common example of a Bernoulli distribution is the probability of getting
    heads and tails from flipping a coin. We usually use the letter *p* to describe
    the probability of getting back a 1 (let’s say that means heads). If we ignore
    weird cases like when the coin lands sideways, the two probabilities of heads
    and tails must add up to 1, which means that the probability of getting back a
    0 (or tails) is 1 − *p*. [Figure 2-12](#figure2-12) shows this graphically for
    a fair coin and a weighted coin. Because we have just two values, we can draw
    the Bernoulli distribution as a bar chart, rather than the lines and curves we
    saw for the continuous distributions.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有用但特殊的离散分布是*伯努利分布*（发音为 ber-noo′-lee）。该分布只返回两个可能的值：0 和 1\。伯努利分布的常见例子是掷硬币时获得正面或反面的概率。我们通常用字母
    *p* 来描述得到 1（假设是正面）的概率。如果忽略硬币侧立等特殊情况，正反面的概率之和必须等于 1，这意味着得到 0（或反面）的概率是 1 − *p*。[图
    2-12](#figure2-12) 以图形方式展示了公平硬币和加权硬币的概率。由于我们只有两个值，所以我们可以将伯努利分布绘制为柱状图，而不是我们在连续分布中看到的线条和曲线。
- en: It may seem like overkill to use the language of distributions for such a simple
    situation, but the payoff is that we can use it with our library routines that
    produce values from distributions. We can hand our routine a uniform distribution,
    or a Gaussian, or a Bernoulli, and it will return a value drawn from that distribution
    according to its probabilities. This makes programming easier.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 使用分布语言来处理这么简单的情况可能看起来有些过度，但它的好处在于我们可以与从分布中生成值的库例程一起使用它。我们可以将一个均匀分布、正态分布或伯努利分布传递给我们的例程，它将根据分布的概率返回一个从该分布中抽取的值。这使得编程变得更加容易。
- en: '![F02012](Images/F02012.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![F02012](Images/F02012.png)'
- en: 'Figure 2-12: The Bernoulli distribution tells us the chance of drawing either
    a 0 or 1\. Left: A fair coin. Right: An unfair coin.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2-12：伯努利分布告诉我们抽取 0 或 1 的概率。左侧：公平硬币。右侧：不公平硬币。
- en: The Multinoulli Distribution
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多项伯努利分布
- en: The Bernoulli distribution only returns one of two possible values. But suppose
    we are running an experiment that can return any one of a larger number or possibilities.
    For instance, instead of flipping a coin that can come up either heads or tails,
    we might roll a 20-sided die that can come up with any of 20 values.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 伯努利分布只返回两种可能的值。但假设我们正在进行一个实验，实验的结果可以是多个可能性中的任何一种。例如，除了抛硬币只会出现正面或反面，我们可能会掷一个
    20 面的骰子，它可以显示 20 个不同的值中的任何一个。
- en: To simulate the result of rolling this die, our random variable could return
    an integer from 1 to 20\. But in other situations, it’s useful to have a list
    of possible values where all the entries have a corresponding probability of 0
    except for the one entry we drew, which is set to 1\. Such a list is useful when
    we build machine learning systems to classify inputs into different categories—for
    example, to describe which of 10 different animals appears in a photograph.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了模拟掷骰子的结果，我们的随机变量可以返回一个从 1 到 20 之间的整数。但在其他情况下，拥有一个可能值列表也很有用，其中除了我们抽取的条目，其余所有条目的概率值都是
    0，而抽中的条目的概率是 1\。这样的列表在我们构建机器学习系统时非常有用，可以用来将输入分类为不同的类别——例如，描述照片中出现的是 10 种不同动物中的哪一种。
- en: Let’s suppose that we have a photo of an alligator, and that’s entry five in
    our list. If our algorithm wasn’t sure what the image was, we might get back something
    like the left of [Figure 2-13](#figure2-13), where three animals are identified
    as possibilities. We’d want the system to produce the output on the right, where
    every entry is 0 except for the alligator, which is 1\.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一张鳄鱼的照片，并且这是我们列表中的第五个条目。如果我们的算法无法确定图像是什么，我们可能会得到类似于[图 2-13](#figure2-13)左侧的结果，其中识别出了三种动物作为可能性。我们希望系统能够输出右侧的结果，其中除了鳄鱼的值为
    1 外，其他每个条目的值都为 0\。
- en: This way of representing a single choice from a list is a key step in training
    classifiers with more than two possible classes. We’ll return to this idea in
    Chapter 6, as a component of an idea called *cross entropy*.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式表示从列表中选择一个单一选项是训练具有两个以上类别的分类器的关键步骤。我们将在第六章中回到这个想法，它是一个名为*交叉熵*的概念的组成部分。
- en: Because each distribution in [Figure 2-13](#figure2-13) is a generalization
    of the two-outcome Bernoulli distribution into multiple outcomes, we could call
    it a “multiple-Bernoulli distribution,” but instead we mush the words together
    into a portmanteau and call it a *multinoulli distribution*(or sometimes the less
    colorful *categorical distribution*).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 因为[图2-13](#figure2-13)中的每个分布都是将二项分布的两个结果推广到多个结果的泛化，我们可以称之为“多项伯努利分布”，但我们将这些词组合在一起，创造了一个新词，称之为*多项伯努利分布*（有时也叫做较为平淡的*分类分布*）。
- en: '![F02013](Images/F02013.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![F02013](Images/F02013.png)'
- en: 'Figure 2-13: Left: Possible predicted probabilities for a picture of an alligator.
    Right: The probabilities we want.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图2-13：左：鳄鱼图片的可能预测概率。右：我们期望的概率。
- en: Collections of Random Values
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机值的集合
- en: We’ve seen how to generate random values from a distribution. We draw a value
    from a random variable using the probabilities of that distribution to tell us
    which return values are more likely to be selected than others.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到如何从分布中生成随机值。我们通过该分布的概率来抽取值，从而确定哪些返回值比其他值更可能被选中。
- en: When we have lots of values drawn from one or more random variables, it’s useful
    to characterize the collection so that we can speak of it as a group. Let’s look
    at three such ideas that show up frequently in machine learning.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们从一个或多个随机变量中抽取了大量值时，通常需要对这些值进行描述，以便我们可以将其作为一个整体来讨论。让我们来看三个在机器学习中常出现的相关概念。
- en: Expected Value
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 期望值
- en: If we pick a value from any probability distribution, and then we pick another,
    and another, over time we build up a long list of values.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们从任何一个概率分布中选取一个值，然后再选取另一个，再选取另一个，随着时间的推移，我们会积累一长串的值。
- en: If these values are numbers, their mean is called the *expected value*. This
    is useful information for many applications. For a simple example, we might have
    a need for random numbers between –1 and 1, with a roughly equal number of positive
    and negative values. If the expected value of the random variable is 0, then we
    know we’re getting a balanced set of values.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这些值是数字，那么它们的均值称为*期望值*。这对于许多应用来说是有用的信息。举个简单的例子，我们可能需要在-1到1之间生成随机数，且正负值大致相等。如果随机变量的期望值为0，那么我们就知道我们得到了一个平衡的值集。
- en: Note that the expected value might not be one of the values ever drawn from
    the distribution! For example, if the values 1, 3, 5, and 7 are the only ones
    available, and they are all equally likely, then the expected value of the random
    variable that we use to draw a value from this list would be (1 + 3 + 5 + 7) /
    4 = 4, a value that we’d never get back from the distribution.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，期望值可能并不是从分布中抽取的值之一！例如，如果1、3、5和7是唯一的值，且它们的概率相等，那么我们用来从该列表中抽取值的随机变量的期望值为（1
    + 3 + 5 + 7）/ 4 = 4，这个值在分布中是无法抽取到的。
- en: Dependence
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 依赖性
- en: The random variables we’ve seen so far have been completely disconnected from
    one another. When we draw a value from a distribution, it doesn’t matter if we’ve
    drawn other values before. Each time we draw a new random variable, it’s a whole
    new world.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们看到的随机变量彼此之间完全没有关联。当我们从某个分布中抽取一个值时，之前是否抽取过其他值并不重要。每次抽取新的随机变量时，都是一个全新的世界。
- en: We call these *independent* variables, because they don’t depend on each other
    in any way. These are the easiest kind of random variable to work with, because
    we don’t have to worry about managing how two or more random variables might influence
    one another.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称这些为*独立*变量，因为它们之间没有任何依赖关系。这是最容易处理的随机变量类型，因为我们不需要担心如何管理两个或更多随机变量可能相互影响。
- en: 'By contrast, there are *dependent* variables, which do depend on each other.
    For example, suppose we have several distributions for the fur length of different
    animals: dogs, cats, hamsters, and so on. We might first pick an animal at random
    from a list of animals, and then use that to select the appropriate fur length
    distribution. We’d then draw a value from that distribution to find a value for
    the animal’s fur. The choice of animal depends on nothing else, so it’s an independent
    variable. But the length of the fur depends on the distribution we use, which
    in turn depends on which animal we chose, so fur length is a dependent variable.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，存在*依赖*变量，它们之间相互依赖。例如，假设我们有几种动物的毛发长度分布：狗、猫、仓鼠等等。我们可能首先从动物列表中随机选择一种动物，然后使用该动物来选择适当的毛发长度分布。接着，我们从该分布中抽取一个值，以找到该动物的毛发长度。动物的选择不依赖于任何其他因素，因此它是一个独立变量。但毛发长度依赖于我们使用的分布，而该分布又取决于我们选择了哪种动物，因此毛发长度是一个依赖变量。
- en: Independent and Identically Distributed Variables
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 独立同分布变量
- en: The math and algorithms of many machine-learning techniques are designed to
    work with multiple values that are drawn from random variables with the same distribution
    and are also independent of each other. That is, we draw values from the same
    distribution over and over, and there is no relationship between successive values.
    In fact, some algorithms requirethat we generate our random values this way, while
    others work best when we do.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习技术的数学和算法都是设计来处理从具有相同分布的随机变量中抽取的多个值，并且这些值彼此独立。也就是说，我们反复从同一个分布中抽取值，并且连续的值之间没有关系。实际上，一些算法要求我们以这种方式生成随机值，而其他算法则在这种方式下效果最好。
- en: 'This requirement is common enough that such variables have a special name:
    *i.i.d.*, which stands for *independent and identically distributed* (the acronym
    is unusual because it’s usually written in lowercase, with periods between the
    letters). We might see, for example, the arguments for a library function described
    this way: “Make sure successive inputs are i.i.d.”'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这一要求非常常见，以至于这些变量有一个专门的名称：*i.i.d.*，表示*独立同分布*（这个缩写不常见，因为通常会以小写字母书写，并且字母之间有点号）。例如，我们可能会看到某个库函数的参数这样描述：“确保连续的输入是i.i.d.”。
- en: The phrase *identically distributed* is just a compact way of saying “selected
    from the same distribution.”
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '*同分布*这个词只是简洁地表达“从相同的分布中选择”的意思。'
- en: Sampling and Replacement
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 抽样与替换
- en: In machine learning it’s often useful to build new datasets from existing ones
    by randomly selecting some of the elements of the existing set. We’ll do just
    this in the next section, when we look for the mean value of a set of samples.
    Let’s look at two ways of creating a list of selections, chosen from a starting
    pool of objects.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，通常会通过随机选择现有数据集中的一些元素来构建新的数据集。我们将在下一节中进行这种操作，寻找一组样本的均值。让我们看两种方法，如何从一个起始对象池中选择并创建一个选择列表。
- en: Selection with Replacement
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 带替换的选择
- en: Let’s first look at an approach where we make a copy of each selected item,
    so the original stays in place, as in [Figure 2-14](#figure2-14). We call this
    approach *selection with replacement*, or *SWR*, because we can think of it as
    removing the object, making a copy for ourselves, and replacing the original.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先来看一种方法，其中我们对每个选中的项目进行复制，这样原始项目保持不变，如[图2-14](#figure2-14)所示。我们称这种方法为*带替换的选择*，或者*SWR*，因为我们可以把它理解为移除物体，制作副本并替换原物。
- en: '![f02014](Images/f02014.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![f02014](Images/f02014.png)'
- en: 'Figure 2-14: Selection with replacement'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图2-14：带替换的选择
- en: One implication of selection with replacement is that we might end up with the
    same object more than once. In an extreme case, our entire new dataset might be
    nothing more than multiple copies of the same object. A second implication is
    that we can make a new dataset that is smaller than the original, or the same
    size, or even much bigger. Since the original dataset is never altered, we can
    continue picking elements as long as we like.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 带替换选择的一个含义是我们可能会多次得到相同的物体。在极端情况下，我们的新数据集可能完全由相同物体的多个副本组成。第二个含义是我们可以创建一个比原始数据集小、新数据集与原始数据集相同大小，甚至更大的数据集。由于原始数据集从未被更改，我们可以继续选择元素，直到我们满意为止。
- en: A statistical implication of this process is that our selections are *independent*
    of one another. There is no history, so our selections are not at all affected
    by previous choices, nor do they influence future choices. To see this, note that
    the pool (or starting dataset) in [Figure 2-14](#figure2-14) always has eight
    objects, so the odds of picking each one are 1 in 8\. In the figure, we first
    picked element C. Now our new dataset has element C inside of it, but we’ve replaced
    that element back into the pool after selecting it. When we look again at the
    pool, all eight items are still there, and if we choose again, each still has
    a 1 in 8 chance of being picked.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这一过程的统计含义是我们的选择是*相互独立*的。没有历史记录，所以我们的选择完全不受之前选择的影响，也不影响未来的选择。为了说明这一点，请注意，[图 2-14](#figure2-14)中的池（或起始数据集）始终包含八个物体，因此每个物体被选中的几率是1/8。在图中，我们首先选择了元素C。现在我们新数据集里包含了C元素，但在选择后我们将该元素重新放回了池中。当我们再次查看池时，所有八个项目依然存在，如果我们再次选择，每个项目依旧有1/8的机会被选中。
- en: An everyday approximation of sampling with replacement is ordering at a well-stocked
    coffee shop. If we order a vanilla latte, it’s not removed from the menu but remains
    available to the next customer.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 一个日常的放回抽样近似是去一个备货充足的咖啡店点单。如果我们点了一杯香草拿铁，它并不会从菜单中被移除，而是继续供下一个顾客选择。
- en: Selection Without Replacement
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 不放回抽样
- en: The other way to randomly choose our new dataset is to remove our choice from
    the original dataset and place it in our new one. We don’t make a copy, so the
    original dataset has just lost one element. This approach is called *selection
    without replacement*, or *SWOR*, and is shown in [Figure 2-15](#figure2-15).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种随机选择新数据集的方法是从原始数据集中移除我们的选择，并将其放入新数据集中。我们不进行复制，因此原始数据集就失去了一个元素。这种方法叫做*不放回抽样*，或*SWOR*，如[图
    2-15](#figure2-15)所示。
- en: An everyday example of sampling without replacement is playing a card game like
    poker. Each time a card is dealt, it’s gone from the pack and cannot be dealt
    again (until the cards are recollected and shuffled).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 一个日常的非放回抽样例子是玩扑克牌等卡牌游戏。每次发牌时，卡片会被从牌堆中移除，不能再被发出（直到牌被收集并洗牌）。
- en: '![F02015](Images/F02015.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![F02015](Images/F02015.png)'
- en: 'Figure 2-15: Selection without replacement'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2-15：不放回抽样
- en: Let’s compare the implications of SWOR with those of SWR. First, in SWOR, no
    object can be selected more than once, because we remove it from the original
    dataset. Second, our new dataset can be smaller than the original, or the same
    size, but it can never be larger. Third, our choices are now dependent. In [Figure
    2-15](#figure2-15), each element originally had the same 1 in 8 chance of being
    picked the first time. When we selected item C, we did notreplace it. When we
    go to make another selection, there are only seven elements available to us, each
    now with a 1 in 7 chance of being selected. The odds of selecting any one of those
    elements have gone up, simply because there are fewer elements competing for selection.
    If we select another item, the remaining elements each have a 1 in 6 chance of
    being picked, and so on. And after we’ve selected seven items, the last one is
    100 percent sure to be selected the next time.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将不放回抽样（SWOR）与放回抽样（SWR）的含义进行比较。首先，在不放回抽样中，任何物品不能被选中超过一次，因为我们将其从原始数据集中移除。其次，我们的新数据集可能比原始数据集小，或者大小相同，但绝不会更大。第三，我们的选择现在是相互依赖的。在[图
    2-15](#figure2-15)中，每个元素最初有相同的1/8的机会被第一次选中。当我们选择了C项时，它并未被放回。当我们进行下一次选择时，只有七个元素可供选择，每个元素现在有1/7的机会被选中。选择其中任何一个元素的几率增加了，因为竞争选择的元素减少了。如果我们再选择一个项目，其余的元素每个有1/6的机会被选中，依此类推。在我们选择了七个项目后，最后一个项目在下一次选择中有100%的概率被选中。
- en: 'Continuing the comparison, suppose that we want to make a new dataset that’s
    smaller than the original pool. We could build it with or without replacement.
    But sampling with replacement can generate many more possiblenew collections than
    sampling without. To see this, suppose we had just three objects in our original
    pool (let’s call them A, B, and C), and we want a new collection of two objects.
    Sampling without replacement gives us only three possible new collections: (A,B),
    (A,C), and (B,C). Sampling with replacement gives us those three, and also (A,A),
    (B,B), and (C,C). Generally speaking, sampling with replacement always gives us
    a larger set of possible new collections.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 继续比较，假设我们想要创建一个比原始数据池更小的数据集。我们可以选择有放回抽样或无放回抽样。但是有放回抽样可以生成比无放回抽样更多的可能新集合。为了说明这一点，假设我们在原始数据池中只有三个物体（我们称它们为A、B和C），并且我们想要一个包含两个物体的新集合。无放回抽样只能给我们三个可能的新集合：(A,B)、(A,C)
    和 (B,C)。有放回抽样则给我们这三个集合，还包括(A,A)、(B,B) 和 (C,C)。一般来说，有放回抽样总是能给我们更多可能的新集合。
- en: Bootstrapping
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自助法
- en: Let’s look at a useful application for the SWR and SWOR algorithms we just covered.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个刚才提到的无放回和有放回抽样算法的实用应用。
- en: Sometimes we want to know some statistics about a dataset that’s much too large
    for us to work with in practice. For example, suppose we want to know the mean
    height of all people alive in the world right now. There’s just no practical way
    to measure everyone. Usually we try to answer this kind of question by extracting
    a representative piece of the dataset, and then measuring that. We might find
    the height of a few thousand people, and hope that the mean of those measurements
    is close to what we’d get if we were able to measure everyone.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候我们想了解一些数据集的统计信息，而这些数据集太大，以至于我们无法在实践中处理。例如，假设我们想知道当前全球所有人的平均身高。实际上，根本没有办法测量每个人。通常，我们会通过提取数据集中的一个具有代表性的部分，然后进行测量来回答这类问题。我们可能会找出几千个人的身高，并希望这些测量值的平均值接近于我们如果能够测量每个人时得到的平均值。
- en: Let’s call every person in the world our *population*. Since that’s too many
    people to work with, we’ll gather a reasonably sized group of people that we hope
    are representative of the population. We call that smaller group a *sample set*.
    We’ll build this sample set without replacement, so each time we select a value
    from the population (that is, a person’s height), it’s removed from the population,
    placed into the sample set, and cannot be chosen again.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们把全世界的每个人都看作我们的*总体*。由于人数太多，无法直接操作，我们将收集一个合理规模的群体，期望它能够代表这个总体。我们称这个较小的群体为*样本集*。我们将在没有放回的情况下构建这个样本集，因此每次从总体中选择一个值（即一个人的身高）时，它会从总体中移除，放入样本集中，并且不能再被选择。
- en: We hope that by building our sample set carefully, we are making it a reasonable
    proxy for the whole population with respect to the properties we want to measure.
    [Figure 2-16](#figure2-16) shows the idea for a population of 21 circled numbers.
    The sample set contains 12 elements from the population.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望通过仔细构建我们的样本集，使其在我们想要衡量的属性上能够合理地代表整个总体。[图 2-16](#figure2-16)展示了一个包含21个圆圈数字的总体的构思。样本集包含了来自该总体的12个元素。
- en: '![F02016](Images/F02016.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![F02016](Images/F02016.png)'
- en: 'Figure 2-16: Creating a sample set from a population by sampling without replacement'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2-16：通过无放回抽样从总体中创建样本集
- en: Now we’ll measure the mean of the sample set, and use that as our estimate of
    the mean of the population. In this little example, we can compute the mean of
    the population, which comes out to about 4.3\. The mean of our sample set is about
    3.8\. This match isn’t great, but it’s not wildly wrong.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将测量样本集的平均值，并将其作为总体平均值的估计。在这个小例子中，我们可以计算出总体的平均值，约为4.3。而我们的样本集的平均值约为3.8。虽然这两者的匹配不是很完美，但也并不算非常偏差。
- en: Most of the time we won’t be able to measure the population (that’s why we’re
    building the sample set in the first place). By finding the mean of the sample
    set, we’ve come up with an approximation, but how good is it? Is this a number
    we ought to rely on as a good estimate for the whole population? It’s hard to
    say. Things would be better if we could express our result in terms of a *confidence
    interval*. This lets us make a statement of the form, “We are 98 percent certain
    that the mean of the population is between 3.1 and 4.5.” To make such a statement,
    we need to know the upper and lower bounds of the range (here, 3.1 and 4.5) and
    have a measure of how confident we are that the value is in that range (here,
    98 percent). Typically, we pick the confidence we need for whatever task we have
    at hand, and from that, we find the lower and upper values of the corresponding
    range.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数时候我们无法测量总体（这也是我们首先要建立样本集的原因）。通过计算样本集的均值，我们得到了一个近似值，但这个值有多准确呢？我们应该依赖这个值作为对整个总体的良好估计吗？很难说。如果我们能将结果表示为**置信区间**，情况就会更好。这让我们能够做出类似“我们有98%的把握，总体均值在3.1到4.5之间”的陈述。为了做出这样的陈述，我们需要知道范围的上下界（在这里是3.1和4.5），并且知道我们有多大的信心认为值位于该范围内（在这里是98%）。通常，我们会根据任务需求选择所需的置信度，然后从中找出相应范围的上下值。
- en: We’d like to be able to make this kind of statement about the mean, or any other
    statistical measure we’re interested in. We can do this with the technique of
    *bootstrapping* (Efron and Tibshirani 1993; Teknomo 2015), which involves two
    basic steps. The first is the step we saw in [Figure 2-16](#figure2-16), where
    we create a sample set from the original population using SWOR. The second step
    involves resampling that sample set to make new sets, this time using SWR. Each
    of these new sets is called a *bootstrap*. The bootstraps are the key to coming
    up with our confidence statement.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望能够对均值或我们感兴趣的任何统计量做出类似的陈述。我们可以通过**自助法**（Efron 和 Tibshirani 1993; Teknomo
    2015）来实现这一点，该方法包括两个基本步骤。第一个步骤是我们在[图 2-16](#figure2-16)中看到的步骤，即使用**不放回抽样**从原始总体中创建一个样本集。第二个步骤是对样本集进行重抽样，生成新的样本集，这次使用**放回抽样**。每个新的样本集都叫做一个**自助抽样**。这些自助抽样是我们构建置信度声明的关键。
- en: To create a bootstrap, we first decide how many elements we want to pick out
    of the starting sample set. We can pick any number up to the number of elements
    in the set, though we often use far fewer. Once we’ve picked that number, we randomly
    extract that many elements from the sample set with replacement, so it’s possible
    that we’ll pick the same element more than once. The process is illustrated in
    [Figure 2-17](#figure2-17).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个自助抽样，我们首先决定从起始样本集中选取多少个元素。我们可以选择任何数量，最多为样本集中的元素数量，尽管我们通常只选取较少的元素。一旦我们确定了选取的数量，就会从样本集中随机抽取这么多元素，且允许放回抽样，因此有可能会多次抽取到相同的元素。这个过程在[图
    2-17](#figure2-17)中有所示。
- en: '![F02017](Images/F02017.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![F02017](Images/F02017.png)'
- en: 'Figure 2-17: Creating three bootstraps using SWR and finding their means'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2-17：使用**放回抽样**创建三个自助抽样并求出它们的均值
- en: To recap, we start with a population. We make a sample set from the population
    using sampling without replacement. Then we make bootstraps from that sample set
    using sampling *with* replacement. We need to select with replacement in this
    last step because we might want to build bootstraps that have the same size as
    the sample set. In our example, we might want our bootstraps to hold 12 values.
    If we didn’t sample with replacement, then every bootstrap would be identical
    to the sample set.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们从一个总体开始。我们通过**不放回抽样**从总体中抽取一个样本集。接着，我们使用**放回抽样**从这个样本集生成自助抽样。最后一步需要使用放回抽样，因为我们可能想要生成与样本集大小相同的自助抽样。在我们的示例中，我们可能希望我们的自助抽样包含12个值。如果不使用放回抽样，那么每个自助抽样将与样本集完全相同。
- en: If we really hope to find the average height of everyone in the world, we need
    a lot more than 21 measurements. Let’s scale up the number of samples and zoom
    way down on their range. For convenience, let’s focus on the size of two-month-old
    babies. They are typically about 500 centimeters long, so we created a simulated
    population of 5,000 measurements with lengths from 0 to 1,000 millimeters (that’s
    1 meter, or about 3.2 feet). From this population, we drew 500 measurements at
    random to make a sample set, and then we created 1,000 bootstraps, each with 20
    elements. [Figure 2-18](#figure2-18) shows the number of bootstraps that had a
    mean value in each of about 100 different bins across the range from 0 to 1,000
    (there were almost no means below 200 or above 800). Graphs like this take the
    form of an approximate bell curve almost every time, because the nature of the
    bootstraps causes more of them to have a mean around the true mean, and fewer
    with mean values farther away.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们真的希望找到全世界每个人的平均身高，我们需要比21个测量值多得多的样本。让我们增加样本量，并大幅缩小它们的范围。为了方便起见，我们将聚焦于两个月大婴儿的身高。它们通常约为500厘米长，因此我们创建了一个模拟的5,000个测量值的种群，长度从0到1,000毫米（即1米，约3.2英尺）。从这个种群中，我们随机抽取了500个测量值作为样本集，然后创建了1,000个自助法样本，每个样本包含20个元素。[图2-18](#figure2-18)展示了每个大约100个不同区间中，每个区间内具有均值的自助法样本的数量，这些区间的范围从0到1,000（几乎没有均值低于200或高于800）。像这样的图形几乎总是呈现近似的钟形曲线，因为自助法的特性使得更多的样本均值靠近真实均值，而远离真实均值的样本较少。
- en: '![F02018](Images/F02018.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![F02018](Images/F02018.png)'
- en: 'Figure 2-18: The histogram shows how many bootstraps have a mean of the given
    value. The blue bar at about 490 is the mean of the sample set. The red bar at
    about 500 is the mean of the population.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图2-18：直方图显示了有多少自助样本的均值为给定值。大约490的蓝色条是样本集的均值。大约500的红色条是种群的均值。
- en: Since we created the data, we know the mean of the population is 500\. The mean
    of our sample set is close to this, at about 490\. The purpose of bootstrapping
    is to help us determine how much we should trust this value of 490\. Without going
    into the math, the approximate bell curve of the mean values of the bootstraps
    tells us everything we need to know. Let’s say we want to find the values that
    we’re 80 percent confident brackets the mean of the population. Then we only need
    to slice off the lowest and highest 10 percent of the bootstrap values, leaving
    the middle 80 percent (Brownlee 2017). [Figure 2-19](#figure2-19) shows a box
    that does just this, enclosing the values that we are 80 percent confident contain
    the real value, which we know is 500\. Reading from the graph, we could now say,
    “We are 80 percent confident that the mean of the original population is between
    about 410 and 560.”
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们是创建数据的，因此我们知道种群的均值是500。我们的样本集的均值接近这个值，大约为490。自助法的目的是帮助我们确定应该多大程度上信任这个490的值。虽然不深入数学计算，自助样本均值的近似钟形曲线已经能告诉我们所有需要知道的信息。假设我们想找到我们有80％信心包含种群均值的区间，那么我们只需要切掉自助法样本值中最低和最高的10％，留下中间的80％（Brownlee
    2017）。[图2-19](#figure2-19)显示了一个框，正是做了这件事，封闭了我们80％信心认为包含真实值的区间，我们知道真实值是500。从图表中读取，我们现在可以说：“我们有80%的信心，原始种群的均值在大约410到560之间。”
- en: '![F02019](Images/F02019.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![F02019](Images/F02019.png)'
- en: 'Figure 2-19: We are 80 percent confident that the box contains the population’s
    mean.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图2-19：我们有80%的信心，这个框包含了种群的均值。
- en: Bootstrapping is appealing because often we can use small bootstraps, perhaps
    only 10 or 20 elements each, even with huge populations of millions of measurements.
    Because each bootstrap is small, it’s typically fast to build and process. To
    compensate for their small size, we often create thousands of bootstraps. The
    more bootstraps we build, the more the results look like a Gaussian bump, and
    the more precise we can be with our confidence intervals.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 自助法具有吸引力，因为我们通常可以使用小型自助样本，可能每个样本只有10个或20个元素，即使在具有数百万测量值的大型种群中也适用。由于每个自助样本都很小，构建和处理通常比较快。为了弥补样本小的不足，我们通常会创建数千个自助样本。我们构建的自助样本越多，结果就越像一个高斯分布的峰值，我们对置信区间的精确度也可以越高。
- en: Covariance and Correlation
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 协方差与相关性
- en: 'Sometimes variables can be related to one another. For example, one variable
    might give us the temperature outside, and the other the chance of snow. When
    the temperature is very high, there’s no chance of snow, so knowledge of one of
    the variables tells us something about the other. In this case, the relationship
    is *negative*: as the temperature goes up, the chance of snow goes down, and vice
    versa. On the other hand, our second variable might tell us the number of people
    we expect to find swimming in the local lake. The connection between the temperature
    and the number of people swimming is *positive*, because on warmer days we see
    more swimmers, and vice versa.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，变量之间可能会相互关联。例如，一个变量可能告诉我们外面的温度，另一个变量则是降雪的几率。当温度非常高时，降雪的几率为零，因此了解其中一个变量可以告诉我们关于另一个变量的信息。在这种情况下，关系是*负相关*的：温度上升时，降雪几率下降，反之亦然。另一方面，我们的第二个变量可能告诉我们预计在本地湖泊中游泳的人数。温度和游泳人数之间的关系是*正相关*的，因为在较暖的日子里我们会看到更多的游泳者，反之亦然。
- en: It’s useful to be able to find these relationships, and measure their strength.
    For instance, suppose we’re planning to teach an algorithm to extract information
    from a dataset. If we find that two of the values in the data are strongly related
    (like temperature and chance of snow), we might be able to remove one of them
    from the data, since it’s redundant. This can improve our training speed and can
    even improve our results.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 能够发现这些关系并衡量它们的强度是很有用的。例如，假设我们计划教授一个算法来从数据集中提取信息。如果我们发现数据中的两个值密切相关（例如温度和降雪几率），我们可能能够从数据中删除其中一个变量，因为它是冗余的。这可以提高我们的训练速度，甚至改善我们的结果。
- en: In this section, we’ll look at a measurement called *covariance,* developed
    by mathematicians to let us determine the strengths of these relationships. We’ll
    also see a variation called *correlation,* which is often more useful because
    it doesn’t depend on the sizes of the number involved.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将介绍一个叫做*协方差*的度量，它由数学家们开发，用来让我们确定这些关系的强度。我们还将看到一种变体，叫做*相关性*，它通常更有用，因为它不依赖于涉及的数字大小。
- en: Covariance
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 协方差
- en: Suppose that we have two variables and we notice a specific numerical pattern
    involving them. When either variable’s value increases, the other increases by
    a fixed multiple of that amount, and the same thing happens when either variable
    decreases. For example, suppose variable A goes up by 3, and variable B goes up
    by 6\. Then later, B goes up by 4, and A goes up by 2\. Then A decreases by 4,
    and B decreases by 8\. In every example, B goes up or down by twice the amount
    A went up or down, so our *fixed multiple* is 2.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有两个变量，并且注意到它们之间存在特定的数值模式。当任何一个变量的值增加时，另一个变量增加该量的固定倍数，反之亦然。举个例子，假设变量A增加了3，变量B增加了6。然后，B增加了4，A增加了2。接着A减少了4，B减少了8。在每个例子中，B的增减都等于A增减的两倍，因此我们的*固定倍数*是2。
- en: If we see such a relationship (for any multiple, not just 2), we say that the
    two variables *covary*. We measure the strength of the connection between the
    two variables, or the consistency with which they covary, with a number called
    the *covariance*. If we find that when one value increases or decreases the other
    does the same by a predictable amount, then the covariance is a positive number,
    and we say that the two variables are demonstrating *positive covariance*.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们看到这样的关系（对于任何倍数，而不仅仅是2），我们说这两个变量*协变*。我们用一个叫做*协方差*的数字来衡量两个变量之间的关系强度，或者它们协变的一致性。如果我们发现当一个值增加或减少时，另一个值按可预测的量做出相同的变化，那么协方差是一个正数，我们说这两个变量表现出*正协方差*。
- en: The classic way to talk about covariance is to draw points in 2D, as in [Figure
    2-20](#figure2-20). Here we see two different sets of covariant points. Each point
    has coordinates x and y, but those are just stand-ins for whatever two variables
    we are interested in comparing. The more consistently the change in y tracks the
    change in x, the stronger the covariance.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，讲解协方差的方式是绘制二维图点，如[图2-20](#figure2-20)所示。在这里我们看到两组不同的协变点。每个点都有x和y坐标，但这些坐标仅仅是我们想要比较的两个变量的代表。y的变化越一致地跟随x的变化，协方差就越强。
- en: '![F02020](Images/F02020.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![F02020](Images/F02020.png)'
- en: 'Figure 2-20: Each diagram shows a different set of points with positive covariance.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图2-20：每个图示展示了具有正协方差的不同点集。
- en: In the left-hand side of [Figure 2-20](#figure2-20), the change in y between
    each pair of horizontally neighboring points is roughly the same. This is positive
    covariance. On the right side, the change in y is a little more variable between
    each pair of points, indicating weaker positive covariance. A very strong positive
    covariance tells us that the two variables move together, so every time one of
    them changes by a given amount, the other changes by a consistent, predictable
    amount.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 2-20](#figure2-20)的左侧，水平相邻的每一对点之间的y值变化大致相同。这是正协方差。在右侧，y值的变化在每对点之间稍有不同，表示较弱的正协方差。非常强的正协方差告诉我们，这两个变量是一起变化的，因此每当其中一个变量按给定量变化时，另一个变量会以一致且可预测的量变化。
- en: If one value decreaseswhenever the other increases, we say the variables have
    *negative covariance*. [Figure 2-21](#figure2-21) shows two different sets of
    negatively covariant points.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个值在另一个值增加时减少，我们称这两个变量具有*负协方差*。[图 2-21](#figure2-21)展示了两组具有负协方差的点。
- en: '![F02021](Images/F02021.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![F02021](Images/F02021.png)'
- en: 'Figure 2-21: Each diagram shows a different set of points with negative covariance.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2-21：每个图示展示了一组具有负协方差的点。
- en: If the two variables have no such consistently matched motion, then the covariance
    is zero. [Figure 2-22](#figure2-22) shows some examples.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两个变量没有这种一致匹配的变化，那么协方差为零。[图 2-22](#figure2-22)展示了一些例子。
- en: '![F02022](Images/F02022.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![F02022](Images/F02022.png)'
- en: 'Figure 2-22: Each of these sets of data points has zero covariance.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2-22：这些数据点的每一组都具有零协方差。
- en: Our idea of covariance only captures relationships between variables when their
    changes are multiples of each other. The graph on the right of [Figure 2-22](#figure2-22)
    shows that there can be a clear pattern among the data (here the dots form part
    of a circle), but the covariance is still zero because the relationships are so
    inconsistent.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对协方差的理解只捕捉到变量之间在它们的变化是彼此倍数关系时的关系。[图 2-22](#figure2-22)右侧的图表显示数据中可以有明显的模式（这里点的形状构成了一个圆的一部分），但协方差仍然为零，因为它们的关系如此不一致。
- en: Correlation
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相关性
- en: 'Covariance is a useful concept, but it has a problem. Because of the way it’s
    defined mathematically, it doesn’t take into account relationships between the
    units of the two variables, which makes it hard for us to compare the strengths
    of different covariances. For example, suppose we measured a dozen variables describing
    a guitar: the thickness of the wood, the length of the neck, the time that a note
    resonates, the tension on the strings, and so on. We might find the covariance
    between various pairs of these measurements, but we are not able to meaningfully
    compare the amount of covariance to find which pairs have the strongest and weakest
    relationships. Even the scale matters: if we find the covariance for a pair of
    measurements in centimeters and the covariance for another pair of measurements
    in inches, we can’t compare those values to say which pair is more strongly covariant.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 协方差是一个有用的概念，但它有一个问题。由于它的数学定义方式，它没有考虑到两个变量单位之间的关系，这使得我们很难比较不同协方差的强度。例如，假设我们测量了一打描述吉他的变量：木材的厚度、琴颈的长度、音符的共鸣时间、弦的张力等等。我们可能会找到这些测量值之间的协方差，但无法有意义地比较协方差的大小，以找出哪些对变量之间的关系最强或最弱。甚至尺度也很重要：如果我们找到一对测量值的协方差单位是厘米，而另一对测量值的协方差单位是英寸，我们无法比较这两个值，以确定哪一对的协方差更强。
- en: 'The *sign* of the covariance is all we learn: a positive value means a positive
    relationship, a negative value means a negative relationship, and zero means no
    relationship. Having only the sign is a problem, because we really want to compare
    different sets of variables. Then we can find out useful information such as which
    variables are the most and the least strongly positively and negatively correlated.
    We can use that information to then prune the size of our dataset, for example,
    by removing one of the measurements in one or more strongly related pairs.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 协方差的*符号*是我们唯一能够了解的信息：正值表示正相关，负值表示负相关，零表示无关。仅仅有符号是不够的，因为我们实际上想要比较不同变量集之间的关系。这样我们可以找出哪些变量之间正相关最强或负相关最强。然后，我们可以根据这些信息来缩小数据集的规模，例如，通过删除一个或多个强相关对中的一个测量值。
- en: To get a measure that lets us make these comparisons, we can compute a slightly
    different number called the *correlation coefficient*, or just the *correlation*.
    This value starts with the covariance but includes one extra step of computation.
    The result is a number that does not depend on the units that were chosen for
    the variables. We can think of the correlation as a scaled version of the covariance,
    always giving us back a value between −1 and 1\. A value of +1 tells us we have
    a *perfect positive correlation*, while a value of −1 tells us we have a *perfect
    negative correlation*.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得一个可以进行比较的度量，我们可以计算一个稍微不同的数值，叫做*相关系数*，或者简称*相关性*。这个值从协方差开始，但包括了一步额外的计算。结果是一个不依赖于所选变量单位的数字。我们可以把相关性看作协方差的缩放版本，总是给我们一个介于
    −1 和 1 之间的值。+1 的值告诉我们存在*完全正相关*，而 −1 的值则告诉我们存在*完全负相关*。
- en: 'Perfect positive correlation is easy to spot: all the dots fall along a straight
    line that moves northeast-southwest, as in [Figure 2-23](#figure2-23).'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 完全正相关很容易辨认：所有的点都落在沿东北-西南方向移动的直线上一，如[图 2-23](#figure2-23)所示。
- en: '![F02023](Images/F02023.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![F02023](Images/F02023.png)'
- en: 'Figure 2-23: Plots showing perfect positive correlation, or a correlation of
    +1'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2-23：显示完全正相关，或相关系数为 +1 的图形
- en: What kind of relationship between points gives us a positive correlation, but
    somewhere in the range between 0 and 1? It’s one where the y value continues to
    increase with x, but the proportion won’t be constant. We might not be able to
    predict how much it changes, but we know that increases in x cause increases in
    y, and decreases in x cause decreases in y. [Figure 2-24](#figure2-24) shows dot
    diagrams for some of positive values of correlation between 0 and 1\. The closer
    the dots are to falling on a straight line, the closer the correlation value is
    to 1\. We say that if the value is near zero the correlation is *weak* (or *low*),
    if it’s around 0.5 it’s *moderate*, and if it’s near 1 it’s *strong* (or *high*).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 什么样的点之间的关系会给我们一个正相关，但在 0 和 1 之间的某个范围内？这是一个 y 值随着 x 值持续增加，但比例不会保持恒定的情况。我们可能无法预测它变化的程度，但我们知道
    x 增加会导致 y 增加，x 减少会导致 y 减少。[图 2-24](#figure2-24)显示了相关系数在 0 和 1 之间的正值的一些点图。点越接近落在一条直线上，相关值就越接近
    1。我们说，如果值接近零，则相关性是*弱*（或*低*），如果接近 0.5，则是*中等*，如果接近 1，则是*强*（或*高*）。
- en: '![F02024](Images/F02024.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![F02024](Images/F02024.png)'
- en: 'Figure 2-24: Examples of decreasing positive correlation from left to right'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2-24：从左到右显示减少的正相关例子
- en: Let’s now look at a correlation value of zero. A zero correlation means that
    there is no relationship between a change in one variable and a change in the
    other. We can’t predict what’s going to happen. Recall that the correlation is
    just a scaled version of the covariance, so when the covariance is zero, so is
    the correlation. [Figure 2-25](#figure2-25) shows some data with zero correlation.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下零相关值。零相关意味着一个变量的变化与另一个变量的变化之间没有关系。我们无法预测会发生什么。回想一下，相关性只是协方差的缩放版本，因此当协方差为零时，相关性也为零。[图
    2-25](#figure2-25)展示了一些具有零相关的数据。
- en: '![F02025](Images/F02025.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![F02025](Images/F02025.png)'
- en: 'Figure 2-25: These patterns have zero correlation.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2-25：这些模式具有零相关性。
- en: 'Negative correlations are just like positive ones, only the variables move
    in opposite directions: as x increases, y decreases. Some examples of negative
    correlations are shown in [Figure 2-26](#figure2-26). Just like with positive
    correlation, if the value is near zero the correlation is *weak* (or *low*), if
    it’s around −0.5 it’s *moderate*, and if it’s near −1 it’s strong (or *high*).'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 负相关就像正相关一样，只是变量朝相反的方向移动：当 x 增加时，y 减少。[图 2-26](#figure2-26)展示了一些负相关的例子。就像正相关一样，如果值接近零，则相关性是*弱*（或*低*），如果接近
    −0.5，则是*中等*，如果接近 −1，则是强（或*高*）。
- en: '![F02026](Images/F02026.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![F02026](Images/F02026.png)'
- en: 'Figure 2-26: Examples of decreasing negative correlation, left to right'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2-26：从左到右显示减少的负相关例子
- en: Finally, [Figure 2-27](#figure2-27) shows perfect negative correlation, or a
    correlation of −1.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，[图 2-27](#figure2-27)展示了完全负相关，或相关系数为 −1 的情况。
- en: '![F02027](Images/F02027.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![F02027](Images/F02027.png)'
- en: 'Figure 2-27: These patterns have a perfect negative correlation, or a correlation
    of −1.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2-27：这些模式具有完全负相关，或相关系数为 −1。
- en: A few other terms are worth mentioning because they pop up in documentation
    and literature from time to time. Our preceding discussion of two variables is
    usually called *simple correlation*. We can find the relationship between more
    variables, however, and this is called *multiple correlation*. If we have a bunch
    of variables but we’re only studying how two of them affect each other, that’s
    called *partial correlation*.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: When two variables have a perfect positive or negative correlation (that is,
    a value of +1 and −1), we say that the variables are *linearly correlated*, because
    (as we’ve seen) the points lie on a line. Variables described by any other values
    of the correlation are said to be *nonlinearly correlated*.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 2-28](#figure2-28) summarizes the meanings of different values of linear
    correlation.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '![F02028](Images/F02028.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-28: Summarizing the meanings of different values of linear correlation'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Statistics Don’t Tell Us Everything
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The statistics we’ve seen in this chapter tell us a lot about a set of data.
    But we shouldn’t assume that the statistics tell us everything. A famous example
    of how we can be fooled by statistics is composed of four different sets of 2D
    points. These sets look nothing like one another, yet they all have the same mean,
    variance, correlation, and straight-line fit. The data is known as *Anscombe’s
    quartet*, after the mathematician who invented these values (Anscombe 1973). The
    values of the four datasets are widely available online (Wikipedia 2017a).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 2-29](#figure2-29) shows the four datasets in this quartet, along with
    the straight line that best fits each set.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: The amazing thing about these four different sets of data is that they share
    many of the same statistics. The mean of the x values in each dataset is 9.0\.
    The mean of the y values in each dataset is 7.5\. The standard deviation of each
    set of x values is 3.16, and the standard deviation of each set of y values is
    1.94\. The correlation between x and y in each dataset is 0.82\. And the best
    straight line through each dataset has a Y axis intercept of 3 and a slope of
    0.5.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: In other words, all seven of these statistical measures have almost the same
    values for all four sets of points (some of the statistics differ from one another
    when we look farther out into more digits). If we just went by the statistics,
    we’d assume that these four datasets were identical.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '![F02029](Images/F02029.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-29: The four datasets in Anscombe’s quartet and the straight lines
    that fit them best'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 2-30](#figure2-30) superimposes all four sets of points, and their
    best straight-line approximations. All four lines are the same, so we only see
    one line in the plot.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '![F02030](Images/F02030.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-30: The four datasets of Anscombe’s quartet and their best straight-line
    fits, superimposed'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: These four sets of data, while famous, are not special. If we want to make more
    sets of different data that have identical (or near-identical) statistics, we
    can make as many as we want (Matejka and Fitzmaurice 2017). The moral is that
    we shouldn’t assume that statistics tell us the whole story about any set of data.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这四组数据，虽然很著名，但并不特殊。如果我们想要制作更多具有相同（或近似相同）统计特征的不同数据集，我们可以随意制作多少组都可以（Matejka 和 Fitzmaurice
    2017）。这个道理是，我们不应假设统计数据能告诉我们任何数据集的完整故事。
- en: Any time we work with a new set of data, it’s always worth spending time to
    get to know it. This can include computing statistics, but the investigative process
    usually also includes drawing plots and other visualizations. Generally speaking,
    the better we understand our data, the better we can design and train algorithms
    to learn from that data.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们处理一组新的数据时，花时间了解它总是值得的。这可以包括计算统计数据，但调查过程通常还包括绘制图表和其他可视化内容。一般来说，理解数据越深，我们设计和训练算法从数据中学习的效果就会越好。
- en: High-Dimensional Spaces
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高维空间
- en: Let’s visit one more topic dealing with numbers. It’s more of a concept than
    a statistical tool, but it influences how we think about our data when we do statistics,
    or machine learning, or almost anything else with large datasets.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再探讨一个与数字相关的话题。它更多的是一个概念，而不是一个统计工具，但它影响我们在进行统计分析、机器学习，或是处理大数据集时对数据的思考方式。
- en: In machine learning, we often bundle up many numbers into a single *sample*,
    or piece of data. For example, we might describe a piece of fruit by its weight,
    color, and size. We call each number a *feature* of the sample. A photograph would
    be described as a sample whose features are the numbers that describe the color
    of each pixel.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，我们通常将多个数字组合成一个单一的*样本*，或者数据片段。例如，我们可能会通过水果的重量、颜色和大小来描述一个水果。我们称每个数字为样本的*特征*。一张照片则可以描述为一个样本，其特征是描述每个像素颜色的数字。
- en: We often talk about how each sample is a point in some enormous *space*. If
    a sample has two features, we can plot the sample as a point, or dot, on a page
    by associating one feature with the X axis, and the other with the Y axis. If
    the sample has three features, we can place a dot in 3D space. But we often have
    samples with far more features. For example, a grayscale photograph that is 1,000
    pixels wide by 1,000 pixels high is described by 1,000 × 1,000 pixel values. That’s
    a million numbers. We can’t draw a picture of a dot in a space with a million
    dimensions, and we can’t even imagine what such a space might look like, but we
    can reason about it by analogy with the 2D and 3D spaces we’re familiar with.
    This is an important mental tool for working with real data, so let’s get a feeling
    for the spaces occupied by samples with huge numbers of features.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常谈论每个样本是某个巨大*空间*中的一个点。如果一个样本有两个特征，我们可以通过将一个特征与X轴关联，另一个特征与Y轴关联，把样本绘制成平面上的一个点或圆点。如果样本有三个特征，我们可以在三维空间中放置一个点。但我们通常有更多特征的样本。例如，一张宽1000像素、高1000像素的灰度照片由1000
    × 1000个像素值描述。这是一百万个数字。我们无法在一个拥有一百万维的空间中画出一个点的图像，甚至无法想象这样的空间可能是什么样子，但我们可以通过类比我们熟悉的二维和三维空间来推理它。这是处理实际数据时一个重要的思维工具，所以让我们感受一下由大量特征占据的样本空间。
- en: The general idea is that each dimension, or axis, of a space corresponds to
    a single feature in our sample. It’s useful to think of all the features (that
    is, all the numbers) in our sample as making up a list. If we have a piece of
    data that has just one feature (say, a temperature), then we can represent that
    feature with a list that is only one number long. Visually, we need only show
    the length of a line to show the size of that measurement, as in [Figure 2-31](#figure2-31).
    We call this line a *one-dimensional space*, because from any point on the line,
    we can only move in one dimension, or direction. In [Figure 2-31](#figure2-31),
    that one choice is horizontally.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，空间的每个维度或轴对应样本中的一个特征。将所有特征（即所有数字）看作是一个列表是很有帮助的。如果我们有一个只有一个特征的数据（比如温度），那么我们可以用一个仅包含一个数字的列表来表示该特征。在视觉上，我们只需要显示一条线的长度来表示这个测量值的大小，如[图
    2-31](#figure2-31)所示。我们称这条线为*一维空间*，因为从线上的任何一点开始，我们只能在一个维度或方向上移动。在[图 2-31](#figure2-31)中，这个方向是水平方向的。
- en: '![F02031](Images/F02031.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![F02031](Images/F02031.png)'
- en: 'Figure 2-31: A piece of data with a single value requires only one axis, or
    dimension, to plot its value. Left: The X axis. Right: Some pieces of data represented
    by either dots on the X axis, or line segments of different lengths.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2-31：一个只有单个值的数据只需要一个坐标轴或维度来绘制它的值。左：X轴。右：一些数据点，用X轴上的点或者不同长度的线段表示。
- en: If we have two pieces of information in a sample, say the temperature and wind
    speed, then we need a list that is two items long. To draw it, we need two dimensions,
    one for each measurement. Graphically, we usually use two perpendicular axes,
    as in [Figure 2-32](#figure2-32). The location of a point is given by moving along
    the X axis by an amount given by the first measurement, and then along the Y axis
    by an amount given by the second measurement. We say that this is a *two-dimensional
    space*.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在样本中有两条信息，比如温度和风速，那么我们需要一个长度为两个项的列表。为了绘制它，我们需要两个维度，每个维度对应一个测量值。在图形上，我们通常使用两个垂直的坐标轴，如[图
    2-32](#figure2-32)所示。一个点的位置是先沿X轴按第一个测量值的量移动，然后沿Y轴按第二个测量值的量移动。我们说这是一个*二维空间*。
- en: '![F02032](Images/F02032.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![F02032](Images/F02032.png)'
- en: 'Figure 2-32: If our data has two values, we need two dimensions, or axes, to
    plot that data.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2-32：如果我们的数据有两个值，我们需要两个维度或坐标轴来绘制这些数据。
- en: If we have three values in our sample, then we use a list of three values. As
    before, each value has a corresponding dimension in the space we’re going to plot
    it. These three dimensions can be represented using three axes, as in [Figure
    2-33](#figure2-33). We call this a *three-dimensional space*.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的样本中有三个值，那么我们使用一个包含三个值的列表。如同之前，每个值在我们将其绘制的空间中都有一个对应的维度。这三个维度可以通过三个坐标轴表示，如[图
    2-33](#figure2-33)所示。我们称之为*三维空间*。
- en: '![F02033](Images/F02033.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![F02033](Images/F02033.png)'
- en: 'Figure 2-33: When each piece of data has three values, we need three dimensions,
    or axes, to draw it.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2-33：当每个数据点有三个值时，我们需要三个维度或坐标轴来绘制它。
- en: What if we have four measurements? Despite some valiant efforts, there’s no
    generally accepted way to draw a four-dimensional space, particularly on a two-dimensional
    page (Banchoff 1990; Norton 2014; ten Bosch 2020). And once we start getting up
    to five, ten, or a million dimensions, drawing a picture of the space is pretty
    much a lost cause.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有四个测量值呢？尽管有一些勇敢的尝试，但目前没有一种公认的方式可以绘制四维空间，特别是在二维的页面上（Banchoff 1990; Norton
    2014; ten Bosch 2020）。而一旦我们涉及到五维、十维或百万维，绘制空间图像几乎就成了不可能完成的任务。
- en: It might seem that these high-dimensional spaces are esoteric and rare, but
    in fact they’re common and we see them every day. As we saw, a grayscale picture
    that’s 1,000 pixels on a side has a million values, corresponding to 1,000,000
    dimensions. A color picture of the same size has 3,000,000 values, so it’s a point
    (or a dot) in a space of three million dimensions. There’s no way we can draw
    a picture with that many dimensions. There’s no way we can even picture one in
    our minds. Yet our machine learning algorithms can handle such a space as easily
    as if it had two or three dimensions. The mathematics and algorithms don’t care
    how many dimensions there are.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这些高维空间可能看起来很深奥且罕见，但实际上它们是很常见的，我们每天都能见到。正如我们所看到的，一个边长为1,000像素的灰度图像有一百万个值，等同于一百万维的空间。一个相同大小的彩色图像有三百万个值，所以它是一个位于三百万维空间中的点（或点）。我们根本无法绘制出如此多维的图像。我们甚至无法在脑海中构想出这样一个图像。然而，我们的机器学习算法可以像处理二维或三维空间一样轻松地处理这样的空间。数学和算法并不关心空间的维度有多少。
- en: The key thing to keep in mind is that each piece of data can be interpreted
    as a single point in some vast space. Just as a two-dimensional (2D) point uses
    two numbers to tell us where it is on the plane, a 750,000-dimensional point uses
    750,000 numbers to tell us where it’s located in that enormous space. We often
    name spaces so that we can keep track of what they describe, so we might say that
    our image is represented by a single point in a *picture space*.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的关键点是，每一条数据都可以解释为某个广阔空间中的一个单独的点。就像一个二维（2D）点用两个数字来告诉我们它在平面上的位置一样，一个750,000维的点用750,000个数字来告诉我们它在那个巨大的空间中的位置。我们通常会为空间命名，以便跟踪它们描述的内容，所以我们可能会说我们的图像由*图像空间*中的一个单一点表示。
- en: We call spaces that have lots of dimensions *high-dimensional spaces*. There’s
    no formal agreement on just when “high” begins, but the phrase is often used for
    spaces that have more than the three dimensions we can reasonably draw. Certainly,
    dozens or hundreds of dimensions would qualify as high for most people.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称具有许多维度的空间为*高维空间*。关于“高”从何时开始，并没有正式的共识，但这个短语通常用于描述那些具有超过我们可以合理绘制的三维空间的空间。当然，对于大多数人来说，几十个或几百个维度就足以被认为是高维空间。
- en: One of the great strengths of the algorithms we’ll be using in this book is
    that they can handle data with any number of dimensions. Computations take more
    time when more data is involved, but in theory, we can handle data with 2,000
    dimensions the same way as data with 2 dimensions (in practice, we usually tune
    our algorithms and data structures to be most efficient with the dimensionality
    of the dataset they’ll be working with).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中我们将使用的算法的一个重要优势是，它们能够处理具有任意维度的数据。当涉及到更多数据时，计算需要更多时间，但理论上，我们可以以与处理二维数据相同的方式处理具有2000个维度的数据（实际上，我们通常会根据数据集的维度对算法和数据结构进行调优，以确保它们在处理数据时最为高效）。
- en: We’ll frequently work with data that can be thought of as points in abstract,
    high-dimensional spaces. Rather than dive into the math, we’ll rely on an intuitive
    generalization of the ideas we’ve just seen, thinking of our spaces as giant (and
    unvisualizable) analogies of our line, square, and cube, where each piece of data
    is represented by a point in some vast, abstract space where each direction, or
    dimension, corresponds to a single value in the sample. We need to be careful
    about relying on our intuition too much, though. In Chapter 7, we’ll see that
    high-dimensional spaces don’t always behave like the 2D and 3D spaces we’re used
    to.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将经常处理可以看作是抽象高维空间中点的数据。我们不会深入探讨数学内容，而是依赖于我们刚才看到的概念的直观推广，将我们的空间看作是巨大的（且无法视觉化的）类比，类似于我们的直线、正方形和立方体，每个数据点都由某个庞大抽象空间中的一个点表示，在这个空间中，每个方向或维度都对应样本中的一个单一值。然而，我们需要小心不要过度依赖我们的直觉。在第七章中，我们将看到，高维空间并不总是像我们习惯的二维或三维空间那样表现。
- en: Summary
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: We often need to characterize collections of numbers. The field of statistics
    is devoted to finding useful ways to describe such collections. In this chapter,
    we looked at basic statistical measurements that will be useful to us throughout
    the book. We saw that a convenient way to control the kinds of numbers we need
    in machine learning is to use a distribution, and we saw some useful distributions.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常需要描述一组数字。统计学领域致力于找到描述这些集合的有用方法。在本章中，我们了解了基本的统计度量，它们将在整本书中对我们有帮助。我们看到，控制我们在机器学习中需要的数字类型的一个方便方法是使用分布，并且我们了解了一些有用的分布。
- en: We saw that we can choose elements from a population with or without replacement,
    giving us different kinds of collections. We can use the statistics of many such
    collections, or bootstraps, to estimate the statistics of the starting population.
    We looked at the ideas of covariance and correlation, which give us a way to measure
    the amount by which a change in one variable predicts the change in another. And
    we saw that we can think of lists of numbers as points in spaces of any number
    of dimensions.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，我们可以选择有放回或无放回地从总体中抽取元素，这样会得到不同种类的集合。我们可以使用许多这样的集合或自助法（bootstraps）的统计数据，来估计起始总体的统计数据。我们了解了协方差和相关性的概念，它们为我们提供了一种衡量一个变量变化如何预测另一个变量变化的方式。我们还看到，我们可以将数字列表看作是任何维度空间中的点。
- en: In the next chapter, we’ll turn to the ideas of probability, in which we take
    random events and try to describe how likely they are to happen, and how likely
    one event is to be followed by another, or occur at the same time as another.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将转向概率的概念，在这一概念中，我们考虑随机事件，并试图描述它们发生的可能性，以及一个事件发生后另一个事件发生的可能性，或者与另一个事件同时发生的可能性。
