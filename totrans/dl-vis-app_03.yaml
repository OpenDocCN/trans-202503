- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Essential Statistics
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/chapterart.png)'
  prefs: []
  type: TYPE_IMG
- en: The better we understand our data, the better we can design a deep learning
    system that can make the most of that data.
  prefs: []
  type: TYPE_NORMAL
- en: By studying and analyzing the data we’re starting with, we can pick the best
    algorithms for learning from it. The ideas and tools that let us do this analysis
    are generally bundled together under the heading of *statistics*. Statistical
    ideas and language appear everywhere in machine learning, from papers and source
    code comments to library documentation.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll cover the key statistical ideas essential to doing deep
    learning without delving into the math or details. The ideas fall roughly into
    two categories. The first category involves random numbers and how to describe
    them in ways that are of the most value in machine learning. The second category
    involves the ways in which we can choose objects (like numbers) from a collection
    and how to measure how well such selections represent the collection as a whole.
    Our goal here is to develop enough understanding and intuition to help us make
    good decisions when we do machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re already familiar with statistics and random values, at least skim
    the chapter. That way you’ll know the language we use in this book, and you’ll
    know where to come back to later if you want to brush up.
  prefs: []
  type: TYPE_NORMAL
- en: Describing Randomness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Random numbers play an important role in many machine learning algorithms.
    We use them to initialize our systems, to control steps during the learning process,
    and sometimes even to influence output. Picking random numbers properly is important:
    doing so can mean the difference between a system that learns from our data and
    produces useful results and a system that stubbornly refuses to learn anything.
    Rather than simply picking some arbitrary numbers out of thin air, we use a variety
    of tools to control what kinds of numbers we want to use and how we select them.'
  prefs: []
  type: TYPE_NORMAL
- en: We typically select a random number between a given minimum and maximum, such
    as when someone has us “pick a number from 1 to 10.” In this example, it’s implied
    that our choice is limited to a finite number of options (the integers from 1
    to 10). We’ll frequently work with *real* numbers, which may lie between the integers.
    There are 10 integers from 1 to 10 (including the endpoints), but there is an
    infinite quantity of real numbers in that range.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we talk about collections of numbers, random or otherwise, we often also
    talk about their *average*. This is a useful way to quickly characterize the collection
    of values. There are three different common ways to compute an average, and they
    come up frequently, so we’ll identify them here. As a running example, let’s work
    with a list of five numbers: 1, 3, 4, 4, 13.'
  prefs: []
  type: TYPE_NORMAL
- en: The *mean* is the value usually meant in everyday language when we say *average*.
    It’s the sum of all the entries divided by the number of entries in the list.
    In our example, adding together all the list elements gives us 1 + 3 + 4 + 4 +
    13 = 25\. There are five elements, so the mean is 25 / 5, or 5.
  prefs: []
  type: TYPE_NORMAL
- en: The *mode*is the value that occurs the most often in the list. In our example,
    4 appears twice, and the other three values each appear only once, so 4 is the
    mode. If no value occurs more often than any other, we say the list has no mode.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the *median* is the number in the middle of the list when we write
    the values sorted from the smallest to the largest. In our list, which is already
    sorted, 1 and 3 make up the left side, 4 and 13 make up the right side, and another
    4 is in the middle. So 4 is the median. If a list has an even number of entries,
    then the median is the mean of the two middle entries. For the list 1, 3, 4, 8,
    the median would be the average of 3 and 4, which is 3.5\.
  prefs: []
  type: TYPE_NORMAL
- en: Averages are useful, but they don’t tell us much about how the numbers in a
    collection are distributed. For example, they might be spread out equally across
    their range, or grouped into one or more clusters. We’ll now look at the techniques
    that let us describe how numbers are distributed.
  prefs: []
  type: TYPE_NORMAL
- en: Random Variables and Probability Distributions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before getting into details, let’s build up our intuition with a running analogy.
    Suppose we’re photographers who have been assigned to support an article on auto
    junkyards by taking lots of pictures of broken-down trucks and cars. Feeling adventurous,
    we go to a junkyard that contains many broken-down vehicles. We talk to the owner,
    and we agree that the best way to get great photos is for us to pay her to bring
    us vehicles to photograph, one by one. She makes it fun by using an old carnival
    wheel she has in her office. It has one equally sized slot for each car on the
    lot, as in [Figure 2-1](#figure2-1). Both the slots and the cars are numbered
    starting at 1.
  prefs: []
  type: TYPE_NORMAL
- en: '![F02001](Images/F02001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-1: The junkyard owner’s carnival wheel. Each equally sized sliver
    represents one car in her lot.'
  prefs: []
  type: TYPE_NORMAL
- en: Once we pay her, she spins the wheel. When the wheel stops, she notes the number
    at the top, goes out with her tow truck, and drags the vehicle with the corresponding
    number back to us. We take some pictures, and she returns the vehicle to the lot.
    If we want to photograph another vehicle, we pay again, she spins the wheel again,
    and the process repeats.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose that our assignment calls for us to get pictures of five different
    types of cars: a sedan, a pickup, a minivan, an SUV, and a wagon. For each type
    of car, we would like to know the chance that if she spins the wheel, we’ll get
    that type. To work that out, let’s suppose that we go into the lot to examine
    every vehicle, and we assign each one to one of these five categories. Our results
    are shown in [Figure 2-2](#figure2-2).'
  prefs: []
  type: TYPE_NORMAL
- en: '![F02002](Images/F02002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-2: Our junkyard has five different types of cars in it. Each bar tells
    us how many cars we have of that type.'
  prefs: []
  type: TYPE_NORMAL
- en: Of the almost 950 cars on her lot, the largest population is minivans, followed
    by pickups, wagons, sedans, and SUVs, in that order. Since every vehicle on her
    lot has an equal chance of being selected, on each spin of the wheel, we’re most
    likely to get a minivan.
  prefs: []
  type: TYPE_NORMAL
- en: But specifically how *much* more likely are we to get a minivan? To determine
    how likely it is that we’ll get each kind of vehicle, we can divide the height
    of each bar in [Figure 2-2](#figure2-2) by the total number of vehicles. This
    value gives us the probability that we’ll get that given type of car, as shown
    in [Figure 2-3](#figure2-3).
  prefs: []
  type: TYPE_NORMAL
- en: '![F02003](Images/F02003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-3: The probability of getting each type of car in the junkyard'
  prefs: []
  type: TYPE_NORMAL
- en: 'To convert the numbers in [Figure 2-3](#figure2-3) into percentages, we multiply
    them by 100\. For example, the height of the minivan bar is about 0.34, so we
    say that there’s a 34 percent chance of getting a minivan. We say that the height
    of each bar is the *probability* that we’ll get that kind of vehicle. If we add
    up the heights of all five bars, we’ll find that the sum is 1.0\. This illustrates
    the rules that turn any list of numbers into probabilities: the values must all
    be between 0 and 1, and add up to 1\.'
  prefs: []
  type: TYPE_NORMAL
- en: We call [Figure 2-3](#figure2-3) a *probability distribution* because it’s distributing
    the 100 percent probability of getting some vehicle among the available options.
    We also sometimes say that [Figure 2-3](#figure2-3) is a *normalized* version
    of [Figure 2-2](#figure2-2), which means that the values all add up to 1.
  prefs: []
  type: TYPE_NORMAL
- en: We can use our probability distribution to draw a simplified carnival wheel,
    as in [Figure 2-4](#figure2-4).
  prefs: []
  type: TYPE_NORMAL
- en: '![F02004](Images/F02004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-4: A simplified carnival wheel that tells us what kind of vehicle
    we’ll get if the owner spins the big wheel of [Figure 2-1](#figure2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The chance that the wheel will end up with the pointer in a given region is
    given by the portion of the wheel’s circumference taken up by that region, which
    we’ve drawn with the same proportions as those shown in [Figure 2-3](#figure2-3).
  prefs: []
  type: TYPE_NORMAL
- en: Most of the time we don’t have carnival wheels around when we generate random
    numbers on the computer. Instead, we rely on software to simulate the process.
    For instance, we might give a library function a list of values, like the heights
    of the bars in [Figure 2-3](#figure2-3), and ask it to return a value. We expect
    that we’ll get back minivan about 34 percent of the time, pickup about 26 percent
    of the time, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: The job of picking a value at random from a list of options, each with its own
    probability, takes a bit of work. For convenience, we package up this selection
    process into its own conceptual procedure called a *random variable*.
  prefs: []
  type: TYPE_NORMAL
- en: This term can create confusion for programmers, because programmers think of
    a variable as being a named piece of storage that can hold data. In this context,
    rather than a piece of storage, a random variable is a *function* (Wikipedia 2017b),
    which takes a probability distribution as an input and produces a single value
    as an output. The process of selecting a value from a distribution is called *drawing*
    a value from the random variable.
  prefs: []
  type: TYPE_NORMAL
- en: We called [Figure 2-3](#figure2-3) a probability distribution, but we can also
    think of it as a function. We call the function, and it returns one of the types
    of vehicles, given these probabilities. This idea leads us to two more formal
    names for a distribution. When there are only a finite number of possible return
    values, like the five values in [Figure 2-3](#figure2-3), we sometimes use the
    oblique name *probability mass function* or *pmf* (this acronym is usually written
    in lowercase).A pmf is also sometimes called a *discrete probability distribution*
    (adding *function* at the end of this term is optional). These terms are meant
    to remind us that there are only a fixed number of possible outputs.
  prefs: []
  type: TYPE_NORMAL
- en: We can easily create probability distributions that are continuous. We use approximations
    of such functions when we initialize the values in a neural network. As an analogy,
    let’s suppose that we want to know how much oil is left in each car that our junkyard
    dealer brings us. The amount of oil is a continuous variable, because it can take
    on any real number. [Figure 2-5](#figure2-5) shows a continuous graph for our
    oil measurements. This graph shows us the probability of getting back not just
    a few specific values, but any real value at all, here between 0 (empty) and 1
    (full).
  prefs: []
  type: TYPE_NORMAL
- en: '![F02005](Images/F02005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-5: A distribution of probabilities for a continuous range of values'
  prefs: []
  type: TYPE_NORMAL
- en: A distribution like [Figure 2-5](#figure2-5) is called a *continuous probability
    distribution* (or *cpd*), or a *probability density function* (or *pdf*). Sometimes
    people casually use the term probability density function, or pdf, to refer to
    discrete distributions, rather than continuous ones. The context usually makes
    it clear which interpretation is intended.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that for the discrete case, all the possible return values need to add
    up to 1\. In the continuous case, as in [Figure 2-5](#figure2-5), that means that
    the area under the curve is 1\.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the time, we obtain random numbers by selecting a distribution and then
    calling a library function to produce a value from that distribution (that is,
    it draws a random variable from our given distribution). We can make our own distributions
    when we want them, but most libraries provide a handful of distributions that
    have been found to cover most situations. That way we can just use one of these
    prebuilt distributions when we choose our random numbers. Let’s take a look at
    some of these distributions.
  prefs: []
  type: TYPE_NORMAL
- en: Some Common Distributions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ve mentioned that we can draw a random variable from a distribution. Each
    time we draw a random variable, it takes on a number in accordance with the distribution:
    numbers with a large corresponding value in the distribution are more likely than
    those with a smaller value in the distribution. This makes distributions of great
    practical value, since different algorithms will want to use random variables
    that take on different values with particular probabilities. To achieve this,
    we merely need to pick an appropriate distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: Continuous Distributions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most of the following distributions are offered as built-in routines by major
    libraries, so they’re easy to specify and use. For simplicity, we’ll demonstrate
    the following two distributions in their continuous forms. Most libraries offer
    us a choice between continuous and discrete versions, or they may offer a general-purpose
    routine to turn any continuous distribution into a discrete one on demand, or
    vice versa. We’ll look at some discrete distributions later in the section.
  prefs: []
  type: TYPE_NORMAL
- en: The Uniform Distribution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[Figure 2-6](#figure2-6) shows the *uniform distribution*. The basic uniform
    distribution is 0 everywhere except between 0 and 1, where it has the value 1.'
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 2-6](#figure2-6), it may appear that there are two values at 0 and
    two values at 1, but there aren’t. Our convention is that an open circle (as on
    the lower line) means “this point is not part of the line,” and a closed circle
    (as on the upper line) means “this point is part of the line.” So, at the input
    values 0 and 1, our graph has an output of 1\. This is a common way to define
    this function, but some implementations make either or both of those outputs 0\.
    It always pays to check.
  prefs: []
  type: TYPE_NORMAL
- en: '![F02006](Images/F02006.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-6: An example of a uniform distribution'
  prefs: []
  type: TYPE_NORMAL
- en: This distribution has two key features. First, we can only get back values between
    0 and 1, because the probability of all other values is 0\. Second, every value
    in the range 0 to 1 is equally probable. It’s just as likely we’d get 0.25 as
    0.33 or 0.793718\. We say that [Figure 2-6](#figure2-6) is *uniform*, or *constant*,
    or *flat*, in the range 0 to 1, all of which tell us that all the values in that
    range are equally probable. We also say that it’s *finite*, meaning that all the
    nonzero values are within some specific range (that is, we can say with certainty
    that 0 and 1 are the smallest and largest values it can return).
  prefs: []
  type: TYPE_NORMAL
- en: Library functions that create uniform distributions for us often let us choose
    to start and end the nonzero region where we like. Probably the most popular choice,
    after the default of 0 to 1, is the range −1 to 1\. The library takes care of
    details like adjusting the height of the function so that the area is always 1.0
    (recall that this is required if a graph is to represent a probability distribution).
  prefs: []
  type: TYPE_NORMAL
- en: The Normal Distribution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Another frequently used distribution is the *normal distribution*, also called
    the *Gaussian distribution*, or simply the *bell curve*. Unlike the uniform distribution,
    it’s smooth and has no sharp corners or abrupt jumps.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 2-7](#figure2-7) shows a few typical normal distributions.'
  prefs: []
  type: TYPE_NORMAL
- en: All four curves in [Figure 2-7](#figure2-7) have the same basic shape. The shapes
    only vary because we scaled the curve, moved it horizontally, or both. For these
    illustrations, we didn’t scale the curve so that the area under it sums to 1.
  prefs: []
  type: TYPE_NORMAL
- en: '![F02007](Images/F02007.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-7: A few normal distributions'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 2-8](#figure2-8) shows some representative samples that we’d get by
    drawing values from each distribution. They bunch up where the distribution’s
    value is high (that is, getting a sample at that value has a high probability),
    and they are sparser where the distribution’s value is low (where getting back
    a sample has a low probability). The vertical locations of the red dots representing
    the samples are jittered only to make the samples easier to see, and have no meaning.'
  prefs: []
  type: TYPE_NORMAL
- en: '![F02008](Images/F02008.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-8: Each red circle shows the value of a sample resulting from drawing
    a value from its normal distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: The normal distribution is nearly 0 almost everywhere, except for the region
    where it rises up in a smooth bump. Though the values drop off ever closer to
    0 to the sides of the bump, they never quite reach 0\. So we say that the width
    of this distribution is *infinite*. In practice, we usually treat any value that’s
    very nearly 0 as actually 0, giving us a finite distribution. A few other terms
    sometimes come up when people discuss normal distributions. The values produced
    by random variables from a normal distribution are sometimes called *normal deviates*,
    and are said to be *normally distributed*. We also say these values *fit*, or
    follow, a normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each normal distribution is defined by two numbers: the *mean* and the *standard
    deviation*. The mean tells us the location of the center of the bump. [Figure
    2-9](#figure2-9) shows our four Gaussians from [Figure 2-7](#figure2-7), with
    their means. Here’s one of the many nice properties of a normal distribution:
    its mean is also its median and its mode.'
  prefs: []
  type: TYPE_NORMAL
- en: '![F02009](Images/F02009.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-9: The mean of a normal distribution is the center of the bump, here
    shown with a red line.'
  prefs: []
  type: TYPE_NORMAL
- en: The *standard deviation* is also a number, often represented by the lower-case
    Greek letter σ (sigma), which tells us the width of the bump. Imagine starting
    at the center of the bump and moving symmetrically outward until we’re enclosing
    about 68 percent of the total area under the curve. The distance from the center
    of the bump to either of these ends is called *one standard deviation* for that
    curve. [Figure 2-10](#figure2-10) shows our four Gaussians, with the area inside
    one standard deviation shaded in green.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the standard deviation to characterize a bump: when the standard
    deviation is small, it means the bump is narrow. As the standard deviation increases,
    the bump becomes more spread out horizontally.'
  prefs: []
  type: TYPE_NORMAL
- en: '![F02010](Images/F02010.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-10: Some normal distributions with the area within one standard deviation
    shaded in green'
  prefs: []
  type: TYPE_NORMAL
- en: If we go out symmetrically from the center by another standard deviation (that
    is, the same distance again), then we’ve enclosed about 95 percent of the area
    under the curve, as shown in [Figure 2-11](#figure2-11). And if we go out one
    more standard deviation, we’ve enclosed about 99.7 percent of the area under the
    curve, also shown in [Figure 2-11](#figure2-11). This property is sometimes called
    the *three-sigma rule*, because of the use of σ for the standard deviation. It
    also sometimes goes by the catchy name of the *68-95-99.7 rule*.
  prefs: []
  type: TYPE_NORMAL
- en: '![F02011](Images/F02011.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-11: The three-sigma, or 68-95-99.7 rule'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we look at 1,000 samples drawn from any normal distribution. We would
    find that about 680 of them are no more than one standard deviation from that
    distribution’s mean, or in the range −σ to σ; about 950 of them are within two
    standard deviations, or in the range −2σ to 2σ; and about 997 of them are within
    three standard deviations, or in the range −3σ to 3σ.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, the mean tells us where the center of the curve is, and the standard
    deviation tells us how spread out the curve is. The larger the standard deviation,
    the broader the curve, because that 68 percent cutoff is farther away.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes instead of the standard deviation, people use a different but related
    value called the *variance*. The variance is just the standard deviation multiplied
    by itself (that is, the standard deviation squared). This value is sometimes more
    convenient in calculations. The general interpretation is the same, though: curves
    with big variances are more spread out than curves with small variances.'
  prefs: []
  type: TYPE_NORMAL
- en: The normal distribution appears frequently in machine learning and in other
    fields, because it naturally describes many real-world observations. If we measure
    the height of adult male horses in some region, or the size of sunflowers, or
    the lifespans of fruit flies, we’ll find that these all tend to take on the shape
    of a normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Discrete Distributions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now let’s look at two discrete distributions.
  prefs: []
  type: TYPE_NORMAL
- en: The Bernoulli Distribution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A useful, but special, discrete distribution is the *Bernoulli distribution*
    (pronounced ber-noo′-lee). This distribution returns just two possible values:
    0 and 1\. A common example of a Bernoulli distribution is the probability of getting
    heads and tails from flipping a coin. We usually use the letter *p* to describe
    the probability of getting back a 1 (let’s say that means heads). If we ignore
    weird cases like when the coin lands sideways, the two probabilities of heads
    and tails must add up to 1, which means that the probability of getting back a
    0 (or tails) is 1 − *p*. [Figure 2-12](#figure2-12) shows this graphically for
    a fair coin and a weighted coin. Because we have just two values, we can draw
    the Bernoulli distribution as a bar chart, rather than the lines and curves we
    saw for the continuous distributions.'
  prefs: []
  type: TYPE_NORMAL
- en: It may seem like overkill to use the language of distributions for such a simple
    situation, but the payoff is that we can use it with our library routines that
    produce values from distributions. We can hand our routine a uniform distribution,
    or a Gaussian, or a Bernoulli, and it will return a value drawn from that distribution
    according to its probabilities. This makes programming easier.
  prefs: []
  type: TYPE_NORMAL
- en: '![F02012](Images/F02012.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-12: The Bernoulli distribution tells us the chance of drawing either
    a 0 or 1\. Left: A fair coin. Right: An unfair coin.'
  prefs: []
  type: TYPE_NORMAL
- en: The Multinoulli Distribution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Bernoulli distribution only returns one of two possible values. But suppose
    we are running an experiment that can return any one of a larger number or possibilities.
    For instance, instead of flipping a coin that can come up either heads or tails,
    we might roll a 20-sided die that can come up with any of 20 values.
  prefs: []
  type: TYPE_NORMAL
- en: To simulate the result of rolling this die, our random variable could return
    an integer from 1 to 20\. But in other situations, it’s useful to have a list
    of possible values where all the entries have a corresponding probability of 0
    except for the one entry we drew, which is set to 1\. Such a list is useful when
    we build machine learning systems to classify inputs into different categories—for
    example, to describe which of 10 different animals appears in a photograph.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s suppose that we have a photo of an alligator, and that’s entry five in
    our list. If our algorithm wasn’t sure what the image was, we might get back something
    like the left of [Figure 2-13](#figure2-13), where three animals are identified
    as possibilities. We’d want the system to produce the output on the right, where
    every entry is 0 except for the alligator, which is 1\.
  prefs: []
  type: TYPE_NORMAL
- en: This way of representing a single choice from a list is a key step in training
    classifiers with more than two possible classes. We’ll return to this idea in
    Chapter 6, as a component of an idea called *cross entropy*.
  prefs: []
  type: TYPE_NORMAL
- en: Because each distribution in [Figure 2-13](#figure2-13) is a generalization
    of the two-outcome Bernoulli distribution into multiple outcomes, we could call
    it a “multiple-Bernoulli distribution,” but instead we mush the words together
    into a portmanteau and call it a *multinoulli distribution*(or sometimes the less
    colorful *categorical distribution*).
  prefs: []
  type: TYPE_NORMAL
- en: '![F02013](Images/F02013.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-13: Left: Possible predicted probabilities for a picture of an alligator.
    Right: The probabilities we want.'
  prefs: []
  type: TYPE_NORMAL
- en: Collections of Random Values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve seen how to generate random values from a distribution. We draw a value
    from a random variable using the probabilities of that distribution to tell us
    which return values are more likely to be selected than others.
  prefs: []
  type: TYPE_NORMAL
- en: When we have lots of values drawn from one or more random variables, it’s useful
    to characterize the collection so that we can speak of it as a group. Let’s look
    at three such ideas that show up frequently in machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Expected Value
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If we pick a value from any probability distribution, and then we pick another,
    and another, over time we build up a long list of values.
  prefs: []
  type: TYPE_NORMAL
- en: If these values are numbers, their mean is called the *expected value*. This
    is useful information for many applications. For a simple example, we might have
    a need for random numbers between –1 and 1, with a roughly equal number of positive
    and negative values. If the expected value of the random variable is 0, then we
    know we’re getting a balanced set of values.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the expected value might not be one of the values ever drawn from
    the distribution! For example, if the values 1, 3, 5, and 7 are the only ones
    available, and they are all equally likely, then the expected value of the random
    variable that we use to draw a value from this list would be (1 + 3 + 5 + 7) /
    4 = 4, a value that we’d never get back from the distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Dependence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The random variables we’ve seen so far have been completely disconnected from
    one another. When we draw a value from a distribution, it doesn’t matter if we’ve
    drawn other values before. Each time we draw a new random variable, it’s a whole
    new world.
  prefs: []
  type: TYPE_NORMAL
- en: We call these *independent* variables, because they don’t depend on each other
    in any way. These are the easiest kind of random variable to work with, because
    we don’t have to worry about managing how two or more random variables might influence
    one another.
  prefs: []
  type: TYPE_NORMAL
- en: 'By contrast, there are *dependent* variables, which do depend on each other.
    For example, suppose we have several distributions for the fur length of different
    animals: dogs, cats, hamsters, and so on. We might first pick an animal at random
    from a list of animals, and then use that to select the appropriate fur length
    distribution. We’d then draw a value from that distribution to find a value for
    the animal’s fur. The choice of animal depends on nothing else, so it’s an independent
    variable. But the length of the fur depends on the distribution we use, which
    in turn depends on which animal we chose, so fur length is a dependent variable.'
  prefs: []
  type: TYPE_NORMAL
- en: Independent and Identically Distributed Variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The math and algorithms of many machine-learning techniques are designed to
    work with multiple values that are drawn from random variables with the same distribution
    and are also independent of each other. That is, we draw values from the same
    distribution over and over, and there is no relationship between successive values.
    In fact, some algorithms requirethat we generate our random values this way, while
    others work best when we do.
  prefs: []
  type: TYPE_NORMAL
- en: 'This requirement is common enough that such variables have a special name:
    *i.i.d.*, which stands for *independent and identically distributed* (the acronym
    is unusual because it’s usually written in lowercase, with periods between the
    letters). We might see, for example, the arguments for a library function described
    this way: “Make sure successive inputs are i.i.d.”'
  prefs: []
  type: TYPE_NORMAL
- en: The phrase *identically distributed* is just a compact way of saying “selected
    from the same distribution.”
  prefs: []
  type: TYPE_NORMAL
- en: Sampling and Replacement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In machine learning it’s often useful to build new datasets from existing ones
    by randomly selecting some of the elements of the existing set. We’ll do just
    this in the next section, when we look for the mean value of a set of samples.
    Let’s look at two ways of creating a list of selections, chosen from a starting
    pool of objects.
  prefs: []
  type: TYPE_NORMAL
- en: Selection with Replacement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s first look at an approach where we make a copy of each selected item,
    so the original stays in place, as in [Figure 2-14](#figure2-14). We call this
    approach *selection with replacement*, or *SWR*, because we can think of it as
    removing the object, making a copy for ourselves, and replacing the original.
  prefs: []
  type: TYPE_NORMAL
- en: '![f02014](Images/f02014.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-14: Selection with replacement'
  prefs: []
  type: TYPE_NORMAL
- en: One implication of selection with replacement is that we might end up with the
    same object more than once. In an extreme case, our entire new dataset might be
    nothing more than multiple copies of the same object. A second implication is
    that we can make a new dataset that is smaller than the original, or the same
    size, or even much bigger. Since the original dataset is never altered, we can
    continue picking elements as long as we like.
  prefs: []
  type: TYPE_NORMAL
- en: A statistical implication of this process is that our selections are *independent*
    of one another. There is no history, so our selections are not at all affected
    by previous choices, nor do they influence future choices. To see this, note that
    the pool (or starting dataset) in [Figure 2-14](#figure2-14) always has eight
    objects, so the odds of picking each one are 1 in 8\. In the figure, we first
    picked element C. Now our new dataset has element C inside of it, but we’ve replaced
    that element back into the pool after selecting it. When we look again at the
    pool, all eight items are still there, and if we choose again, each still has
    a 1 in 8 chance of being picked.
  prefs: []
  type: TYPE_NORMAL
- en: An everyday approximation of sampling with replacement is ordering at a well-stocked
    coffee shop. If we order a vanilla latte, it’s not removed from the menu but remains
    available to the next customer.
  prefs: []
  type: TYPE_NORMAL
- en: Selection Without Replacement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The other way to randomly choose our new dataset is to remove our choice from
    the original dataset and place it in our new one. We don’t make a copy, so the
    original dataset has just lost one element. This approach is called *selection
    without replacement*, or *SWOR*, and is shown in [Figure 2-15](#figure2-15).
  prefs: []
  type: TYPE_NORMAL
- en: An everyday example of sampling without replacement is playing a card game like
    poker. Each time a card is dealt, it’s gone from the pack and cannot be dealt
    again (until the cards are recollected and shuffled).
  prefs: []
  type: TYPE_NORMAL
- en: '![F02015](Images/F02015.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-15: Selection without replacement'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s compare the implications of SWOR with those of SWR. First, in SWOR, no
    object can be selected more than once, because we remove it from the original
    dataset. Second, our new dataset can be smaller than the original, or the same
    size, but it can never be larger. Third, our choices are now dependent. In [Figure
    2-15](#figure2-15), each element originally had the same 1 in 8 chance of being
    picked the first time. When we selected item C, we did notreplace it. When we
    go to make another selection, there are only seven elements available to us, each
    now with a 1 in 7 chance of being selected. The odds of selecting any one of those
    elements have gone up, simply because there are fewer elements competing for selection.
    If we select another item, the remaining elements each have a 1 in 6 chance of
    being picked, and so on. And after we’ve selected seven items, the last one is
    100 percent sure to be selected the next time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing the comparison, suppose that we want to make a new dataset that’s
    smaller than the original pool. We could build it with or without replacement.
    But sampling with replacement can generate many more possiblenew collections than
    sampling without. To see this, suppose we had just three objects in our original
    pool (let’s call them A, B, and C), and we want a new collection of two objects.
    Sampling without replacement gives us only three possible new collections: (A,B),
    (A,C), and (B,C). Sampling with replacement gives us those three, and also (A,A),
    (B,B), and (C,C). Generally speaking, sampling with replacement always gives us
    a larger set of possible new collections.'
  prefs: []
  type: TYPE_NORMAL
- en: Bootstrapping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s look at a useful application for the SWR and SWOR algorithms we just covered.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes we want to know some statistics about a dataset that’s much too large
    for us to work with in practice. For example, suppose we want to know the mean
    height of all people alive in the world right now. There’s just no practical way
    to measure everyone. Usually we try to answer this kind of question by extracting
    a representative piece of the dataset, and then measuring that. We might find
    the height of a few thousand people, and hope that the mean of those measurements
    is close to what we’d get if we were able to measure everyone.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s call every person in the world our *population*. Since that’s too many
    people to work with, we’ll gather a reasonably sized group of people that we hope
    are representative of the population. We call that smaller group a *sample set*.
    We’ll build this sample set without replacement, so each time we select a value
    from the population (that is, a person’s height), it’s removed from the population,
    placed into the sample set, and cannot be chosen again.
  prefs: []
  type: TYPE_NORMAL
- en: We hope that by building our sample set carefully, we are making it a reasonable
    proxy for the whole population with respect to the properties we want to measure.
    [Figure 2-16](#figure2-16) shows the idea for a population of 21 circled numbers.
    The sample set contains 12 elements from the population.
  prefs: []
  type: TYPE_NORMAL
- en: '![F02016](Images/F02016.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-16: Creating a sample set from a population by sampling without replacement'
  prefs: []
  type: TYPE_NORMAL
- en: Now we’ll measure the mean of the sample set, and use that as our estimate of
    the mean of the population. In this little example, we can compute the mean of
    the population, which comes out to about 4.3\. The mean of our sample set is about
    3.8\. This match isn’t great, but it’s not wildly wrong.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the time we won’t be able to measure the population (that’s why we’re
    building the sample set in the first place). By finding the mean of the sample
    set, we’ve come up with an approximation, but how good is it? Is this a number
    we ought to rely on as a good estimate for the whole population? It’s hard to
    say. Things would be better if we could express our result in terms of a *confidence
    interval*. This lets us make a statement of the form, “We are 98 percent certain
    that the mean of the population is between 3.1 and 4.5.” To make such a statement,
    we need to know the upper and lower bounds of the range (here, 3.1 and 4.5) and
    have a measure of how confident we are that the value is in that range (here,
    98 percent). Typically, we pick the confidence we need for whatever task we have
    at hand, and from that, we find the lower and upper values of the corresponding
    range.
  prefs: []
  type: TYPE_NORMAL
- en: We’d like to be able to make this kind of statement about the mean, or any other
    statistical measure we’re interested in. We can do this with the technique of
    *bootstrapping* (Efron and Tibshirani 1993; Teknomo 2015), which involves two
    basic steps. The first is the step we saw in [Figure 2-16](#figure2-16), where
    we create a sample set from the original population using SWOR. The second step
    involves resampling that sample set to make new sets, this time using SWR. Each
    of these new sets is called a *bootstrap*. The bootstraps are the key to coming
    up with our confidence statement.
  prefs: []
  type: TYPE_NORMAL
- en: To create a bootstrap, we first decide how many elements we want to pick out
    of the starting sample set. We can pick any number up to the number of elements
    in the set, though we often use far fewer. Once we’ve picked that number, we randomly
    extract that many elements from the sample set with replacement, so it’s possible
    that we’ll pick the same element more than once. The process is illustrated in
    [Figure 2-17](#figure2-17).
  prefs: []
  type: TYPE_NORMAL
- en: '![F02017](Images/F02017.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-17: Creating three bootstraps using SWR and finding their means'
  prefs: []
  type: TYPE_NORMAL
- en: To recap, we start with a population. We make a sample set from the population
    using sampling without replacement. Then we make bootstraps from that sample set
    using sampling *with* replacement. We need to select with replacement in this
    last step because we might want to build bootstraps that have the same size as
    the sample set. In our example, we might want our bootstraps to hold 12 values.
    If we didn’t sample with replacement, then every bootstrap would be identical
    to the sample set.
  prefs: []
  type: TYPE_NORMAL
- en: If we really hope to find the average height of everyone in the world, we need
    a lot more than 21 measurements. Let’s scale up the number of samples and zoom
    way down on their range. For convenience, let’s focus on the size of two-month-old
    babies. They are typically about 500 centimeters long, so we created a simulated
    population of 5,000 measurements with lengths from 0 to 1,000 millimeters (that’s
    1 meter, or about 3.2 feet). From this population, we drew 500 measurements at
    random to make a sample set, and then we created 1,000 bootstraps, each with 20
    elements. [Figure 2-18](#figure2-18) shows the number of bootstraps that had a
    mean value in each of about 100 different bins across the range from 0 to 1,000
    (there were almost no means below 200 or above 800). Graphs like this take the
    form of an approximate bell curve almost every time, because the nature of the
    bootstraps causes more of them to have a mean around the true mean, and fewer
    with mean values farther away.
  prefs: []
  type: TYPE_NORMAL
- en: '![F02018](Images/F02018.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-18: The histogram shows how many bootstraps have a mean of the given
    value. The blue bar at about 490 is the mean of the sample set. The red bar at
    about 500 is the mean of the population.'
  prefs: []
  type: TYPE_NORMAL
- en: Since we created the data, we know the mean of the population is 500\. The mean
    of our sample set is close to this, at about 490\. The purpose of bootstrapping
    is to help us determine how much we should trust this value of 490\. Without going
    into the math, the approximate bell curve of the mean values of the bootstraps
    tells us everything we need to know. Let’s say we want to find the values that
    we’re 80 percent confident brackets the mean of the population. Then we only need
    to slice off the lowest and highest 10 percent of the bootstrap values, leaving
    the middle 80 percent (Brownlee 2017). [Figure 2-19](#figure2-19) shows a box
    that does just this, enclosing the values that we are 80 percent confident contain
    the real value, which we know is 500\. Reading from the graph, we could now say,
    “We are 80 percent confident that the mean of the original population is between
    about 410 and 560.”
  prefs: []
  type: TYPE_NORMAL
- en: '![F02019](Images/F02019.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-19: We are 80 percent confident that the box contains the population’s
    mean.'
  prefs: []
  type: TYPE_NORMAL
- en: Bootstrapping is appealing because often we can use small bootstraps, perhaps
    only 10 or 20 elements each, even with huge populations of millions of measurements.
    Because each bootstrap is small, it’s typically fast to build and process. To
    compensate for their small size, we often create thousands of bootstraps. The
    more bootstraps we build, the more the results look like a Gaussian bump, and
    the more precise we can be with our confidence intervals.
  prefs: []
  type: TYPE_NORMAL
- en: Covariance and Correlation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Sometimes variables can be related to one another. For example, one variable
    might give us the temperature outside, and the other the chance of snow. When
    the temperature is very high, there’s no chance of snow, so knowledge of one of
    the variables tells us something about the other. In this case, the relationship
    is *negative*: as the temperature goes up, the chance of snow goes down, and vice
    versa. On the other hand, our second variable might tell us the number of people
    we expect to find swimming in the local lake. The connection between the temperature
    and the number of people swimming is *positive*, because on warmer days we see
    more swimmers, and vice versa.'
  prefs: []
  type: TYPE_NORMAL
- en: It’s useful to be able to find these relationships, and measure their strength.
    For instance, suppose we’re planning to teach an algorithm to extract information
    from a dataset. If we find that two of the values in the data are strongly related
    (like temperature and chance of snow), we might be able to remove one of them
    from the data, since it’s redundant. This can improve our training speed and can
    even improve our results.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ll look at a measurement called *covariance,* developed
    by mathematicians to let us determine the strengths of these relationships. We’ll
    also see a variation called *correlation,* which is often more useful because
    it doesn’t depend on the sizes of the number involved.
  prefs: []
  type: TYPE_NORMAL
- en: Covariance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Suppose that we have two variables and we notice a specific numerical pattern
    involving them. When either variable’s value increases, the other increases by
    a fixed multiple of that amount, and the same thing happens when either variable
    decreases. For example, suppose variable A goes up by 3, and variable B goes up
    by 6\. Then later, B goes up by 4, and A goes up by 2\. Then A decreases by 4,
    and B decreases by 8\. In every example, B goes up or down by twice the amount
    A went up or down, so our *fixed multiple* is 2.
  prefs: []
  type: TYPE_NORMAL
- en: If we see such a relationship (for any multiple, not just 2), we say that the
    two variables *covary*. We measure the strength of the connection between the
    two variables, or the consistency with which they covary, with a number called
    the *covariance*. If we find that when one value increases or decreases the other
    does the same by a predictable amount, then the covariance is a positive number,
    and we say that the two variables are demonstrating *positive covariance*.
  prefs: []
  type: TYPE_NORMAL
- en: The classic way to talk about covariance is to draw points in 2D, as in [Figure
    2-20](#figure2-20). Here we see two different sets of covariant points. Each point
    has coordinates x and y, but those are just stand-ins for whatever two variables
    we are interested in comparing. The more consistently the change in y tracks the
    change in x, the stronger the covariance.
  prefs: []
  type: TYPE_NORMAL
- en: '![F02020](Images/F02020.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-20: Each diagram shows a different set of points with positive covariance.'
  prefs: []
  type: TYPE_NORMAL
- en: In the left-hand side of [Figure 2-20](#figure2-20), the change in y between
    each pair of horizontally neighboring points is roughly the same. This is positive
    covariance. On the right side, the change in y is a little more variable between
    each pair of points, indicating weaker positive covariance. A very strong positive
    covariance tells us that the two variables move together, so every time one of
    them changes by a given amount, the other changes by a consistent, predictable
    amount.
  prefs: []
  type: TYPE_NORMAL
- en: If one value decreaseswhenever the other increases, we say the variables have
    *negative covariance*. [Figure 2-21](#figure2-21) shows two different sets of
    negatively covariant points.
  prefs: []
  type: TYPE_NORMAL
- en: '![F02021](Images/F02021.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-21: Each diagram shows a different set of points with negative covariance.'
  prefs: []
  type: TYPE_NORMAL
- en: If the two variables have no such consistently matched motion, then the covariance
    is zero. [Figure 2-22](#figure2-22) shows some examples.
  prefs: []
  type: TYPE_NORMAL
- en: '![F02022](Images/F02022.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-22: Each of these sets of data points has zero covariance.'
  prefs: []
  type: TYPE_NORMAL
- en: Our idea of covariance only captures relationships between variables when their
    changes are multiples of each other. The graph on the right of [Figure 2-22](#figure2-22)
    shows that there can be a clear pattern among the data (here the dots form part
    of a circle), but the covariance is still zero because the relationships are so
    inconsistent.
  prefs: []
  type: TYPE_NORMAL
- en: Correlation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Covariance is a useful concept, but it has a problem. Because of the way it’s
    defined mathematically, it doesn’t take into account relationships between the
    units of the two variables, which makes it hard for us to compare the strengths
    of different covariances. For example, suppose we measured a dozen variables describing
    a guitar: the thickness of the wood, the length of the neck, the time that a note
    resonates, the tension on the strings, and so on. We might find the covariance
    between various pairs of these measurements, but we are not able to meaningfully
    compare the amount of covariance to find which pairs have the strongest and weakest
    relationships. Even the scale matters: if we find the covariance for a pair of
    measurements in centimeters and the covariance for another pair of measurements
    in inches, we can’t compare those values to say which pair is more strongly covariant.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The *sign* of the covariance is all we learn: a positive value means a positive
    relationship, a negative value means a negative relationship, and zero means no
    relationship. Having only the sign is a problem, because we really want to compare
    different sets of variables. Then we can find out useful information such as which
    variables are the most and the least strongly positively and negatively correlated.
    We can use that information to then prune the size of our dataset, for example,
    by removing one of the measurements in one or more strongly related pairs.'
  prefs: []
  type: TYPE_NORMAL
- en: To get a measure that lets us make these comparisons, we can compute a slightly
    different number called the *correlation coefficient*, or just the *correlation*.
    This value starts with the covariance but includes one extra step of computation.
    The result is a number that does not depend on the units that were chosen for
    the variables. We can think of the correlation as a scaled version of the covariance,
    always giving us back a value between −1 and 1\. A value of +1 tells us we have
    a *perfect positive correlation*, while a value of −1 tells us we have a *perfect
    negative correlation*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perfect positive correlation is easy to spot: all the dots fall along a straight
    line that moves northeast-southwest, as in [Figure 2-23](#figure2-23).'
  prefs: []
  type: TYPE_NORMAL
- en: '![F02023](Images/F02023.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-23: Plots showing perfect positive correlation, or a correlation of
    +1'
  prefs: []
  type: TYPE_NORMAL
- en: What kind of relationship between points gives us a positive correlation, but
    somewhere in the range between 0 and 1? It’s one where the y value continues to
    increase with x, but the proportion won’t be constant. We might not be able to
    predict how much it changes, but we know that increases in x cause increases in
    y, and decreases in x cause decreases in y. [Figure 2-24](#figure2-24) shows dot
    diagrams for some of positive values of correlation between 0 and 1\. The closer
    the dots are to falling on a straight line, the closer the correlation value is
    to 1\. We say that if the value is near zero the correlation is *weak* (or *low*),
    if it’s around 0.5 it’s *moderate*, and if it’s near 1 it’s *strong* (or *high*).
  prefs: []
  type: TYPE_NORMAL
- en: '![F02024](Images/F02024.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-24: Examples of decreasing positive correlation from left to right'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now look at a correlation value of zero. A zero correlation means that
    there is no relationship between a change in one variable and a change in the
    other. We can’t predict what’s going to happen. Recall that the correlation is
    just a scaled version of the covariance, so when the covariance is zero, so is
    the correlation. [Figure 2-25](#figure2-25) shows some data with zero correlation.
  prefs: []
  type: TYPE_NORMAL
- en: '![F02025](Images/F02025.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-25: These patterns have zero correlation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Negative correlations are just like positive ones, only the variables move
    in opposite directions: as x increases, y decreases. Some examples of negative
    correlations are shown in [Figure 2-26](#figure2-26). Just like with positive
    correlation, if the value is near zero the correlation is *weak* (or *low*), if
    it’s around −0.5 it’s *moderate*, and if it’s near −1 it’s strong (or *high*).'
  prefs: []
  type: TYPE_NORMAL
- en: '![F02026](Images/F02026.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-26: Examples of decreasing negative correlation, left to right'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, [Figure 2-27](#figure2-27) shows perfect negative correlation, or a
    correlation of −1.
  prefs: []
  type: TYPE_NORMAL
- en: '![F02027](Images/F02027.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-27: These patterns have a perfect negative correlation, or a correlation
    of −1.'
  prefs: []
  type: TYPE_NORMAL
- en: A few other terms are worth mentioning because they pop up in documentation
    and literature from time to time. Our preceding discussion of two variables is
    usually called *simple correlation*. We can find the relationship between more
    variables, however, and this is called *multiple correlation*. If we have a bunch
    of variables but we’re only studying how two of them affect each other, that’s
    called *partial correlation*.
  prefs: []
  type: TYPE_NORMAL
- en: When two variables have a perfect positive or negative correlation (that is,
    a value of +1 and −1), we say that the variables are *linearly correlated*, because
    (as we’ve seen) the points lie on a line. Variables described by any other values
    of the correlation are said to be *nonlinearly correlated*.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 2-28](#figure2-28) summarizes the meanings of different values of linear
    correlation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![F02028](Images/F02028.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-28: Summarizing the meanings of different values of linear correlation'
  prefs: []
  type: TYPE_NORMAL
- en: Statistics Don’t Tell Us Everything
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The statistics we’ve seen in this chapter tell us a lot about a set of data.
    But we shouldn’t assume that the statistics tell us everything. A famous example
    of how we can be fooled by statistics is composed of four different sets of 2D
    points. These sets look nothing like one another, yet they all have the same mean,
    variance, correlation, and straight-line fit. The data is known as *Anscombe’s
    quartet*, after the mathematician who invented these values (Anscombe 1973). The
    values of the four datasets are widely available online (Wikipedia 2017a).
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 2-29](#figure2-29) shows the four datasets in this quartet, along with
    the straight line that best fits each set.'
  prefs: []
  type: TYPE_NORMAL
- en: The amazing thing about these four different sets of data is that they share
    many of the same statistics. The mean of the x values in each dataset is 9.0\.
    The mean of the y values in each dataset is 7.5\. The standard deviation of each
    set of x values is 3.16, and the standard deviation of each set of y values is
    1.94\. The correlation between x and y in each dataset is 0.82\. And the best
    straight line through each dataset has a Y axis intercept of 3 and a slope of
    0.5.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, all seven of these statistical measures have almost the same
    values for all four sets of points (some of the statistics differ from one another
    when we look farther out into more digits). If we just went by the statistics,
    we’d assume that these four datasets were identical.
  prefs: []
  type: TYPE_NORMAL
- en: '![F02029](Images/F02029.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-29: The four datasets in Anscombe’s quartet and the straight lines
    that fit them best'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 2-30](#figure2-30) superimposes all four sets of points, and their
    best straight-line approximations. All four lines are the same, so we only see
    one line in the plot.'
  prefs: []
  type: TYPE_NORMAL
- en: '![F02030](Images/F02030.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-30: The four datasets of Anscombe’s quartet and their best straight-line
    fits, superimposed'
  prefs: []
  type: TYPE_NORMAL
- en: These four sets of data, while famous, are not special. If we want to make more
    sets of different data that have identical (or near-identical) statistics, we
    can make as many as we want (Matejka and Fitzmaurice 2017). The moral is that
    we shouldn’t assume that statistics tell us the whole story about any set of data.
  prefs: []
  type: TYPE_NORMAL
- en: Any time we work with a new set of data, it’s always worth spending time to
    get to know it. This can include computing statistics, but the investigative process
    usually also includes drawing plots and other visualizations. Generally speaking,
    the better we understand our data, the better we can design and train algorithms
    to learn from that data.
  prefs: []
  type: TYPE_NORMAL
- en: High-Dimensional Spaces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s visit one more topic dealing with numbers. It’s more of a concept than
    a statistical tool, but it influences how we think about our data when we do statistics,
    or machine learning, or almost anything else with large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: In machine learning, we often bundle up many numbers into a single *sample*,
    or piece of data. For example, we might describe a piece of fruit by its weight,
    color, and size. We call each number a *feature* of the sample. A photograph would
    be described as a sample whose features are the numbers that describe the color
    of each pixel.
  prefs: []
  type: TYPE_NORMAL
- en: We often talk about how each sample is a point in some enormous *space*. If
    a sample has two features, we can plot the sample as a point, or dot, on a page
    by associating one feature with the X axis, and the other with the Y axis. If
    the sample has three features, we can place a dot in 3D space. But we often have
    samples with far more features. For example, a grayscale photograph that is 1,000
    pixels wide by 1,000 pixels high is described by 1,000 × 1,000 pixel values. That’s
    a million numbers. We can’t draw a picture of a dot in a space with a million
    dimensions, and we can’t even imagine what such a space might look like, but we
    can reason about it by analogy with the 2D and 3D spaces we’re familiar with.
    This is an important mental tool for working with real data, so let’s get a feeling
    for the spaces occupied by samples with huge numbers of features.
  prefs: []
  type: TYPE_NORMAL
- en: The general idea is that each dimension, or axis, of a space corresponds to
    a single feature in our sample. It’s useful to think of all the features (that
    is, all the numbers) in our sample as making up a list. If we have a piece of
    data that has just one feature (say, a temperature), then we can represent that
    feature with a list that is only one number long. Visually, we need only show
    the length of a line to show the size of that measurement, as in [Figure 2-31](#figure2-31).
    We call this line a *one-dimensional space*, because from any point on the line,
    we can only move in one dimension, or direction. In [Figure 2-31](#figure2-31),
    that one choice is horizontally.
  prefs: []
  type: TYPE_NORMAL
- en: '![F02031](Images/F02031.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-31: A piece of data with a single value requires only one axis, or
    dimension, to plot its value. Left: The X axis. Right: Some pieces of data represented
    by either dots on the X axis, or line segments of different lengths.'
  prefs: []
  type: TYPE_NORMAL
- en: If we have two pieces of information in a sample, say the temperature and wind
    speed, then we need a list that is two items long. To draw it, we need two dimensions,
    one for each measurement. Graphically, we usually use two perpendicular axes,
    as in [Figure 2-32](#figure2-32). The location of a point is given by moving along
    the X axis by an amount given by the first measurement, and then along the Y axis
    by an amount given by the second measurement. We say that this is a *two-dimensional
    space*.
  prefs: []
  type: TYPE_NORMAL
- en: '![F02032](Images/F02032.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-32: If our data has two values, we need two dimensions, or axes, to
    plot that data.'
  prefs: []
  type: TYPE_NORMAL
- en: If we have three values in our sample, then we use a list of three values. As
    before, each value has a corresponding dimension in the space we’re going to plot
    it. These three dimensions can be represented using three axes, as in [Figure
    2-33](#figure2-33). We call this a *three-dimensional space*.
  prefs: []
  type: TYPE_NORMAL
- en: '![F02033](Images/F02033.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-33: When each piece of data has three values, we need three dimensions,
    or axes, to draw it.'
  prefs: []
  type: TYPE_NORMAL
- en: What if we have four measurements? Despite some valiant efforts, there’s no
    generally accepted way to draw a four-dimensional space, particularly on a two-dimensional
    page (Banchoff 1990; Norton 2014; ten Bosch 2020). And once we start getting up
    to five, ten, or a million dimensions, drawing a picture of the space is pretty
    much a lost cause.
  prefs: []
  type: TYPE_NORMAL
- en: It might seem that these high-dimensional spaces are esoteric and rare, but
    in fact they’re common and we see them every day. As we saw, a grayscale picture
    that’s 1,000 pixels on a side has a million values, corresponding to 1,000,000
    dimensions. A color picture of the same size has 3,000,000 values, so it’s a point
    (or a dot) in a space of three million dimensions. There’s no way we can draw
    a picture with that many dimensions. There’s no way we can even picture one in
    our minds. Yet our machine learning algorithms can handle such a space as easily
    as if it had two or three dimensions. The mathematics and algorithms don’t care
    how many dimensions there are.
  prefs: []
  type: TYPE_NORMAL
- en: The key thing to keep in mind is that each piece of data can be interpreted
    as a single point in some vast space. Just as a two-dimensional (2D) point uses
    two numbers to tell us where it is on the plane, a 750,000-dimensional point uses
    750,000 numbers to tell us where it’s located in that enormous space. We often
    name spaces so that we can keep track of what they describe, so we might say that
    our image is represented by a single point in a *picture space*.
  prefs: []
  type: TYPE_NORMAL
- en: We call spaces that have lots of dimensions *high-dimensional spaces*. There’s
    no formal agreement on just when “high” begins, but the phrase is often used for
    spaces that have more than the three dimensions we can reasonably draw. Certainly,
    dozens or hundreds of dimensions would qualify as high for most people.
  prefs: []
  type: TYPE_NORMAL
- en: One of the great strengths of the algorithms we’ll be using in this book is
    that they can handle data with any number of dimensions. Computations take more
    time when more data is involved, but in theory, we can handle data with 2,000
    dimensions the same way as data with 2 dimensions (in practice, we usually tune
    our algorithms and data structures to be most efficient with the dimensionality
    of the dataset they’ll be working with).
  prefs: []
  type: TYPE_NORMAL
- en: We’ll frequently work with data that can be thought of as points in abstract,
    high-dimensional spaces. Rather than dive into the math, we’ll rely on an intuitive
    generalization of the ideas we’ve just seen, thinking of our spaces as giant (and
    unvisualizable) analogies of our line, square, and cube, where each piece of data
    is represented by a point in some vast, abstract space where each direction, or
    dimension, corresponds to a single value in the sample. We need to be careful
    about relying on our intuition too much, though. In Chapter 7, we’ll see that
    high-dimensional spaces don’t always behave like the 2D and 3D spaces we’re used
    to.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We often need to characterize collections of numbers. The field of statistics
    is devoted to finding useful ways to describe such collections. In this chapter,
    we looked at basic statistical measurements that will be useful to us throughout
    the book. We saw that a convenient way to control the kinds of numbers we need
    in machine learning is to use a distribution, and we saw some useful distributions.
  prefs: []
  type: TYPE_NORMAL
- en: We saw that we can choose elements from a population with or without replacement,
    giving us different kinds of collections. We can use the statistics of many such
    collections, or bootstraps, to estimate the statistics of the starting population.
    We looked at the ideas of covariance and correlation, which give us a way to measure
    the amount by which a change in one variable predicts the change in another. And
    we saw that we can think of lists of numbers as points in spaces of any number
    of dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll turn to the ideas of probability, in which we take
    random events and try to describe how likely they are to happen, and how likely
    one event is to be followed by another, or occur at the same time as another.
  prefs: []
  type: TYPE_NORMAL
