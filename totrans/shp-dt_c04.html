<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" epub:prefix="index: http://www.index.com/" lang="en" xml:lang="en">
<head>
<title>Chapter 4: Network Filtration</title>
<link href="NSTemplate_v1.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:ea5eeba6-9dea-4463-bfe4-b91d6c6b5861" name="Adept.expected.resource"/>
</head>
<body epub:type="bodymatter chapter">
<section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" id="Page_75" title="75"/><a class="XrefDestination" id="4"/><span class="XrefDestination" id="xref-503083c04-001"/>4</span><br/>
<span class="ChapterTitle"><a class="XrefDestination" id="BeyondNetworks"/><span class="XrefDestination" id="xref-503083c04-002"/>Network Filtration</span></h1>
</header>
<figure class="opener">
<img alt="" src="image_fi/book_art/chapterart.png"/>
</figure>
<p class="ChapterIntro">We’ve explored many ways to analyze network data by measuring geometric properties. In this chapter, we’ll introduce network filtration for weighted networks, which tracks geometric properties and network metrics over threshold values imposed on the network. Then we’ll examine how network data can be transformed into a higher-dimensional topological object called a <em>simplicial complex</em>, and we’ll explore higher-dimensional versions of the network metrics we’ve previously considered. From there, we’ll return to graph comparisons using a tool from topology related to filtrations.</p>
<h2 id="h1-503083c04-0001"><span epub:type="pagebreak" id="Page_76" title="76"/><a class="XrefDestination" id="GraphFiltration"/><span class="XrefDestination" id="xref-503083c04-003"/>Graph Filtration</h2>
<p class="BodyFirst">In the previous chapters, we reviewed different network metrics, including different measures of centrality, entropy, spectral radius, diameter, and many others. There’s an interesting way to understand topological properties of weighted networks: <em>graph filtration</em>, a method of creating a series of weighted networks by iteratively removing edges below a certain threshold (for instance, all edges with weights lower than 0.2, 0.4, or 0.6). By creating a series of thresholded graphs, it’s possible to identify persistent network metrics, or local and global network metrics that persist across a wide range of filtration values. This gives us features that can be plotted or tracked across filtrations. This is one of the core ideas of topological data analysis (TDA).</p>
<p>To explore this further, let’s say we’re examining longitudinal educational or risk behavior outcomes of adolescents based on adolescent friendship or informal social ties within a community. Imagine we have weighted social networks with high degree metrics for each vertex, where edges are weighted by hours spent with friends over a normal week. The first group of friends might spend a couple of hours together playing soccer on the weekend. The second group might study together once or twice a week and see each other in classes. The third group might play sports often, do homework together after dinner or in the mornings before school, and stay over at each other’s homes often. As we filter hours spent together, the degree metrics will drop for the first two groups of friends in a network. The last group will retain a high degree metric over the filtration, as they spend more time together. This persistence of degree will likely shed light on the strength of whatever social ties we’re examining in our study.</p>
<p>Let’s examine how we can implement graph filtrations by decomposing and exploring two small example social networks, Graph 1 and Graph 2. First, we’ll load the two networks into R and explore the structures of the full networks with the script in <a href="#listing4-1" id="listinganchor4-1">Listing 4-1</a>.</p>
<pre><code>#load both networks in R
mydata1&lt;-as.matrix(read.csv("Graph1w.csv",header=F))
mydata2&lt;-as.matrix(read.csv("Graph2w.csv",header=F))

#load igraph and convert to graph objects
library(igraph)
g1&lt;-graph_from_adjacency_matrix(mydata1,mode="undirected",weighted=T)
g2&lt;-graph_from_adjacency_matrix(mydata2,mode="undirected",weighted=T)

#plot the two graphs
plot(g1,edge.label=E(g1)$weight,main="Graph 1")
plot(g2,edge.label=E(g2)$weight,main="Graph 2")</code></pre>
<p class="CodeListingCaption"><a id="listing4-1">Listing 4-1</a>: A script that loads two different network structures for filtration</p>
<p>The script in <a href="#listing4-1">Listing 4-1</a> should load two different networks, Graph 1 and Graph 2, which have different connectivity patterns but the same number of vertices. It should also plot both networks with edge weights given in the plots. Let’s compare the networks, shown in <a href="#figure4-1" id="figureanchor4-1">Figure 4-1</a>.</p>
<span epub:type="pagebreak" id="Page_77" title="77"/><figure>
<img alt="" class="" src="image_fi/503083c04/f04001_m.png"/>
<figcaption><p><a id="figure4-1">Figure 4-1</a>: Plots of the two example networks</p></figcaption>
</figure>
<p><a href="#figure4-1">Figure 4-1</a> suggests that Graph 1 is a sparsely connected network with mostly large edge weights (perhaps a sample of students in the same class showing up for a service activity over the course of a weekend), whereas Graph 2 is a densely connected network with a mixture of different edge weights (perhaps a friendship network within a sports team). We’d expect higher hub scores and other centrality measures in Graph 2, but a filtration might change those metrics more quickly than we’d expect them to change in Graph 1.</p>
<p>Let’s create filtrations of the networks; this will allow us to explore a few centrality metrics on these networks. We can do this by adding the following code to the script in <a href="#listing4-1">Listing 4-1</a>:</p>
<pre><code>#filter Graph 1
mydata1[mydata1&lt;0.2]&lt;-0
g12&lt;-graph_from_adjacency_matrix(mydata1,mode="undirected",weighted=T)
mydata1[mydata1&lt;0.4]&lt;-0
g14&lt;-graph_from_adjacency_matrix(mydata1,mode="undirected",weighted=T)
mydata1[mydata1&lt;0.6]&lt;-0
g16&lt;-graph_from_adjacency_matrix(mydata1,mode="undirected",weighted=T)
mydata1[mydata1&lt;0.8]&lt;-0
g18&lt;-graph_from_adjacency_matrix(mydata1,mode="undirected",weighted=T)

#filter Graph 2
mydata2[mydata2&lt;0.2]&lt;-0
g22&lt;-graph_from_adjacency_matrix(mydata2,mode="undirected",weighted=T)
mydata2[mydata2&lt;0.4]&lt;-0
g24&lt;-graph_from_adjacency_matrix(mydata2,mode="undirected",weighted=T)
mydata2[mydata2&lt;0.6]&lt;-0
g26&lt;-graph_from_adjacency_matrix(mydata2,mode="undirected",weighted=T)
mydata2[mydata2&lt;0.8]&lt;-0
g28&lt;-graph_from_adjacency_matrix(mydata2,mode="undirected",weighted=T)</code></pre>
<p><span epub:type="pagebreak" id="Page_78" title="78"/>The previous code filters Graph 1 and Graph 2 by edge weight, using increasing intervals of 0.2. This yields a series of five networks in each graph filtration, which can be further examined by applying network metrics to each sequence of filtered graphs.</p>
<p>Let’s examine the degree centrality of each vertex across the filtration of Graph 1 by adding the following to our script:</p>
<pre><code>#calculate degree centrality for Graph 1's filtration sequence
d1&lt;-degree(g1)
d12&lt;-degree(g12)
d14&lt;-degree(g14)
d16&lt;-degree(g16)
d18&lt;-degree(g18)

#create a dataset tracking degree centrality across the filtration
g1deg&lt;-cbind(d1,d12,d14,d16,d18)</code></pre>
<p>This code calculates degree centrality across filtrations of Graph 1, which should yield a dataset containing the information in <a href="#table4-1" id="tableanchor4-1">Table 4-1</a>.</p>
<figure>
<figcaption class="TableTitle"><p><a id="table4-1">Table 4-1</a>: Degree Centrality Across Graph 1 Filtrations</p></figcaption>
<table border="1" id="table-503083c04-0001">
<thead>
<tr>
<td><b>Column1</b></td>
<td><b>d1</b></td>
<td><b>d12</b></td>
<td><b>d14</b></td>
<td><b>d16</b></td>
<td><b>d18</b></td>
</tr>
</thead>
<tbody>
<tr>
<td><b>V1</b></td>
<td>3</td>
<td>2</td>
<td>2</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td><b>V2</b></td>
<td>2</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td><b>V3</b></td>
<td>3</td>
<td>3</td>
<td>3</td>
<td>2</td>
<td>1</td>
</tr>
<tr>
<td><b>V4</b></td>
<td>3</td>
<td>3</td>
<td>3</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td><b>V5</b></td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>1</td>
</tr>
<tr>
<td><b>V6</b></td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
</tbody>
</table>
</figure>
<p><a href="#table4-1">Table 4-1</a> shows that vertices 1, 3, and 4 have high degree centralities; however, vertices 3 and 4 retain these high degree centrality values across much more of the filtration than vertex 1, suggesting they are more important to the network, despite having the same centrality metric on the unfiltered network (column 1).</p>
<p>Now, let’s add some code to calculate degree centrality across Graph 2’s filtration:</p>
<pre><code>#calculate degree centrality for Graph 2's filtration sequence
d2&lt;-degree(g2)
d22&lt;-degree(g22)
d24&lt;-degree(g24)
d26&lt;-degree(g26)
d28&lt;-degree(g28)

#create a dataset tracking degree centrality across the filtration
g2deg&lt;-cbind(d2,d22,d24,d26,d28)</code></pre>
<p>This code calculates degree centrality across the filtration of Graph 2, yielding a table similar to that obtained by Graph 1’s filtration and <span epub:type="pagebreak" id="Page_79" title="79"/>centrality calculation. <a href="#table4-2" id="tableanchor4-2">Table 4-2</a> summarizes the findings from the Graph 2 filtration and centrality calculation.</p>
<figure>
<figcaption class="TableTitle"><p><a id="table4-2">Table 4-2</a>: Degree Centrality Across Graph 2 Filtrations</p></figcaption>
<table border="1" id="table-503083c04-0002">
<thead>
<tr>
<td><b>Column1</b></td>
<td><b>d2</b></td>
<td><b>d22</b></td>
<td><b>d24</b></td>
<td><b>d26</b></td>
<td><b>d28</b></td>
</tr>
</thead>
<tbody>
<tr>
<td><b>V1</b></td>
<td>4</td>
<td>3</td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td><b>V2</b></td>
<td>4</td>
<td>4</td>
<td>3</td>
<td>2</td>
<td>1</td>
</tr>
<tr>
<td><b>V3</b></td>
<td>4</td>
<td>4</td>
<td>3</td>
<td>2</td>
<td>1</td>
</tr>
<tr>
<td><b>V4</b></td>
<td>5</td>
<td>4</td>
<td>3</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td><b>V5</b></td>
<td>3</td>
<td>3</td>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td><b>V6</b></td>
<td>4</td>
<td>4</td>
<td>3</td>
<td>2</td>
<td>1</td>
</tr>
</tbody>
</table>
</figure>
<p>As <a href="#table4-2">Table 4-2</a> shows, there are relatively high degree centrality measures in the unfiltered Graph 2; however, the pattern changes by vertex after the filtration begins. Some vertices, like vertex 1, retain a high degree centrality throughout the filtration. Others, such as vertex 4, retain a high degree centrality and then drop to 0. Others still, like vertex 6, show a slow degradation of degree centrality over the full filtration. This may be informative in a study of social ties within a subgroup of interest. A high degree of informal social ties, represented by a high centrality degree, has been linked to positive educational attainment, career achievement, and resilience to life adversity in young adults.</p>
<p>Degree centrality is only one example of metrics that we can calculate across a filtration; we can also calculate other local metrics such as betweenness centrality or triadic closure. In addition, we can calculate global metrics, such as the spectral radius or the Euler characteristic, across a filtration. Let’s add the following to <a href="#listing4-1">Listing 4-1</a> to calculate the diameter of each filtration of Graph 1:</p>
<pre><code>#calculate graph diameter of Graph 1's filtration
di1&lt;-diameter(g1)
di12&lt;-diameter(g12)
di14&lt;-diameter(g14)
di16&lt;-diameter(g16)
di18&lt;-diameter(g18)</code></pre>
<p>The sequence of diameters calculated across the filtration of Graph 1 by this code is 2.1, 2.9, 2.9, 1.6, and 0.9. Let’s calculate the diameters for Graph 2’s filtration:</p>
<pre><code>#calculate graph diameter of Graph 2's filtration
di2&lt;-diameter(g2)
di22&lt;-diameter(g22)
di24&lt;-diameter(g24)
di26&lt;-diameter(g26)
di28&lt;-diameter(g28)</code></pre>
<p><span epub:type="pagebreak" id="Page_80" title="80"/>The sequence of diameters calculated across the filtration of Graph 2 by this code is 0.9, 1.2, 1.6, 2.4, and 1.7. This is different than Graph 1’s diameter sequence, suggesting that the diameter is generally smaller until later in the filtration sequence. This metric’s filtration might be useful in assessing a community’s overall level and depth of informal social ties, a measure of community resources available to residents in need. <a href="#figure4-2" id="figureanchor4-2">Figure 4-2</a> shows the diameter plots across both filtrations to compare the two networks.</p>
<figure>
<img alt="" class="" src="image_fi/503083c04/f04002.png"/>
<figcaption><p><a id="figure4-2">Figure 4-2</a>: A plot of graph diameter metrics across filtrations of Graph 1 and Graph 2</p></figcaption>
</figure>
<p>As we can see in <a href="#figure4-2">Figure 4-2</a>, Graph 1 has a larger graph diameter than Graph 2 early in the filtration, but this relationship switches after a filtration value of 0.4. This suggests that there is greater eccentricity in Graph 1 early in the filtrations but greater eccentricity in Graph 2 later in the filtration. Remember that eccentricity is the maximum distance from one point to another in the network.</p>
<p>Graph filtration tracking as we’ve plotted in <a href="#figure4-2">Figure 4-2</a> can be helpful in distinguishing similar graphs with different connectivity patterns or weights. Dynamic networks, in which weights can change over time, could be a use case of graph filtrations. In addition, they are quite useful in comparison among networks with the same vertices but potentially different weights (such as patient groups in brain imaging studies); in fact, brain imaging studies are one of the applications for which graph filtration was developed. Higher eccentricity values suggest longer pathways to relay neural signals; stronger edge weights represent stronger connections between two areas of the brain. Strong edges with low eccentricity suggest a functional module activated in a particular task given to the patient groups on which imaging was performed.</p>
<p>Although graph filtration is a relatively new concept, it has mainly been confined to biological network data, including networks based on brain imaging studies. However, the graph filtration method is widely applicable to weighted network data, and its tool set lends itself to further development <span epub:type="pagebreak" id="Page_81" title="81"/>in other fields. If you want to explore this topic in more depth, look through the references at the end of this book and play around with graph filtrations on their own data. For now, let’s turn our attention to a topological view of graphs, which allows us to extend the relationships captured in graphs to other types of interactions between people or things.</p>
<h2 id="h1-503083c04-0002"><a class="XrefDestination" id="SimplicialComplexes"/><span class="XrefDestination" id="xref-503083c04-004"/>From Graphs to Simplicial Complexes</h2>
<p class="BodyFirst">Graphs can be considered topological objects that have defined global properties we can leverage in our analyses, and it’s possible to turn a graph into a higher-dimensional version of a graph, called a <em>simplicial complex</em>, by considering three-way, four-way, and <em>n</em>-way interactions by individuals and vertices in the graph. Let’s consider three colleagues who often collaborate on academic papers but have never published with all three names on a paper. We’ll create a simple graph for the three colleagues, shown in <a href="#figure4-3" id="figureanchor4-3">Figure 4-3</a>.</p>
<figure>
<img alt="" class="" src="image_fi/503083c04/f04003.png"/>
<figcaption><p><a id="figure4-3">Figure 4-3</a>: A simplicial complex showing two-way interactions among three colleagues</p></figcaption>
</figure>
<p>Now let’s imagine a paper where all three colleagues participate and have their names on the paper. This is a three-way interaction, rather than three two-way interactions, and we’d end up with a filled-in triangle rather than three sets of two-way arrows, as shown in <a href="#figure4-4" id="figureanchor4-4">Figure 4-4</a>.</p>
<figure>
<img alt="" class="" src="image_fi/503083c04/f04004.png"/>
<figcaption><p><a id="figure4-4">Figure 4-4</a>: A simplicial complex showing three-way interactions among three colleagues</p></figcaption>
</figure>
<p><span epub:type="pagebreak" id="Page_82" title="82"/><a href="#figure4-4">Figure 4-4</a> uses a triangle to represent a three-way connection among colleagues, similar to how the arrows between two colleagues represented two-way connections. This can be generalized to tetrahedra for four-way interactions and more exotic shapes to represent higher <em>n</em>-way interactions. There’s no limit as to how high of a number <em>n</em> can be, but computational issues will come into play at some point as we work our way up to <em>n</em>-way interactions in a simplicial complex. Analyses involving email chains, co-authors on papers, or conference calls are common applications that extend social network analysis and graphs into the analysis of simplicial complexes. Depending on the size of the network and the size of the <em>n</em>-way interactions, simplicial complex representations of individuals and mutual interactions can become very complicated across values of <em>n</em>. Analyzing these structures can involve a lot of computing power and tools that extend network metrics. However, because graphs are topological objects, many theorems and tools of topology can be successfully applied to them without transformations or other hassles. This, in turn, allows for other areas of math, including partial differential equations and probability theory, to be applied and developed on graphs.</p>
<p>Just as we could filter a weighted graph, we also can filter simplicial complexes. The filtration process for simplicial complexes varies depending on how the simplicial complex is built. In most topological data analysis algorithms, we start with a point cloud of data within a space where a distance metric can be defined. Points are included in a simplicial complex if they share either mutual <em>n</em>-way overlapping sets with each other (<em>Čech complex</em>) or pairwise overlapping sets (<em>Vietoris–Rips complex</em>). By sequentially increasing or decreasing the value of the distance metric, we obtain a filtration of simplicial complexes. In practice, the Vietoris–Rips complex is easier to compute and underlies many common topological data analysis packages. This leads us to a very new and emerging part of network analytics: extensions of network tools to simplicial complexes.</p>
<p>Many of the tools introduced in the previous chapters have simplicial complex analogs, including eccentricity, shortest path algorithms, centrality metrics (Katz centrality, eigenvector centrality, closeness centrality, and so on), triadic closure, and many more. Typically, simplicial complexes of network data are built by computing maximal cliques within the network (though it’s possible to define a distance metric and apply the process defined in the prior paragraph to build simplicial complexes from network data as well). <em>Maximal cliques</em> of a network include the highest <em>n</em>-way mutual edges among groups of vertices. These maximal cliques correspond to an (<em>n</em> – 1)-simplicial complex. The <em>flag complex</em> of the graph involves building the graph’s simplicial complex by computing the graph’s maximal cliques. From this complex, it’s possible to define quantities at each simplicial complex level, which can be combined into a total metric across levels. This means we can glean more information about the overall structure of the network and its components at various levels of a simplicial complex.</p>
<p>Let’s return to Farrelly’s social network introduced in prior chapters and look at an extension of degree centrality, dubbed <em>topological dimension</em>. We can define topological dimension as a weighted degree centrality, <span epub:type="pagebreak" id="Page_83" title="83"/>weighting each vertex by the dimension of the cliques in which it resides, which involves summing across a vertex’s cliques of different dimensions. For instance, a vertex in a maximal two-clique and a maximal three-clique within the network would have a topological dimension of 5. A vertex in a maximal five-clique and no other cliques would also have a topological dimension of 5. However, the former vertex might have a degree of 3, connecting to one other vertex in the two-clique and two other vertices in the three-clique; the latter would have a degree of 4, connecting to the four other vertices in the five-clique.</p>
<p>In <a href="#listing4-2" id="listinganchor4-2">Listing 4-2</a>, we have a script that calculates the maximal cliques and the topological dimension of vertices within Farrelly’s social network.</p>
<pre><code>#load the author's network
g_social&lt;-read.csv("SocialNetwork.csv")

#create the graph
library(igraph)
g1&lt;-graph_from_adjacency_matrix(g_social,mode="undirected",weighted=F)

#compute the maximal cliques in the author's network data
cl&lt;-maximal.cliques(g1)

#create array
cl&lt;-as.array(cl)

#get clique size from maximal clique array
d&lt;-dim(cl)
l&lt;-rep(NA,d)
for (i in 1:d){
  l[i]&lt;-length(as.vector(cl[[i]]))
}

#create matrix of vertices in maximal cliques
av&lt;-matrix(rep(NA,d*20),20)
for (i in 1:20){
  for (j in 1:d){
    av[i,j]&lt;-i%in%cl[[j]]
  }
}

#convert to binary indicators
avind&lt;-ifelse(av==TRUE,1,0)

#multiply out to calculate each vertex's topological dimension
topmat&lt;-t(avind)*l
topdim&lt;-colSums(topmat)</code></pre>
<p class="CodeListingCaption"><a id="listing4-2">Listing 4-2</a>: A script that calculates topological dimension across vertices in Farrelly’s social network</p>
<p>This script results in a topological dimension calculation based on the flag complex of the graph. It first calculates the flag complex from the maximal cliques; it then stores the information of each clique, such that we can cycle through each clique to see which vertices belong to each <span epub:type="pagebreak" id="Page_84" title="84"/>clique. Converting this information to a binary indicator matrix allows us to multiply the dimension of the clique and the indicator matrix, resulting in a vector containing the topological dimension of each vertex. <a href="#table4-3" id="tableanchor4-3">Table 4-3</a> shows the topological dimension and degree of each vertex in the author’s network dataset.</p>
<figure>
<figcaption class="TableTitle"><p><a id="table4-3">Table 4-3</a>: Topological Dimension and Degree Summary for Vertices in Farrelly’s Social Network</p></figcaption>
<table border="1" id="table-503083c04-0003">
<thead>
<tr>
<td><b>Vertex</b></td>
<td><b>Degree</b></td>
<td><b>Topological dimension</b></td>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>2</td>
<td>3</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>3</td>
<td>5</td>
<td>11</td>
</tr>
<tr>
<td>4</td>
<td>2</td>
<td>3</td>
</tr>
<tr>
<td>5</td>
<td>4</td>
<td>7</td>
</tr>
<tr>
<td>6</td>
<td>3</td>
<td>6</td>
</tr>
<tr>
<td>7</td>
<td>8</td>
<td>18</td>
</tr>
<tr>
<td>8</td>
<td>3</td>
<td>4</td>
</tr>
<tr>
<td>9</td>
<td>3</td>
<td>4</td>
</tr>
<tr>
<td>10</td>
<td>3</td>
<td>6</td>
</tr>
<tr>
<td>11</td>
<td>3</td>
<td>5</td>
</tr>
<tr>
<td>12</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>13</td>
<td>4</td>
<td>8</td>
</tr>
<tr>
<td>14</td>
<td>4</td>
<td>7</td>
</tr>
<tr>
<td>15</td>
<td>4</td>
<td>8</td>
</tr>
<tr>
<td>16</td>
<td>2</td>
<td>4</td>
</tr>
<tr>
<td>17</td>
<td>2</td>
<td>4</td>
</tr>
<tr>
<td>18</td>
<td>3</td>
<td>6</td>
</tr>
<tr>
<td>19</td>
<td>2</td>
<td>4</td>
</tr>
<tr>
<td>20</td>
<td>1</td>
<td>2</td>
</tr>
</tbody>
</table>
</figure>
<p><a href="#table4-3">Table 4-3</a> shows a distinct difference between degree, which includes only the vertices and edges of the author’s network in its calculation, and the topological dimension, which includes higher-order interactions. For instance, vertices 9 and 10 both have a degree of 3; however, their topological dimensions differ, with vertex 9 having a score of 4 and vertex 10 having a score of 6. The importance of vertex 10 to the overall network structure is larger than the importance of vertex 9 to the overall network structure. Without considering higher-order interactions within the network, we would not be able to distinguish between the two vertices with respect to this metric.</p>
<p>For weighted networks, it’s possible to combine these simplicial-complex-based metrics with graph filtration, yielding a sequence of metrics over the <span epub:type="pagebreak" id="Page_85" title="85"/>filtration based on the simplicial complex of the network. You’ll see this when we discuss a tool called <em>persistent homology</em> in the next section of this chapter. You could do the same with the Euler characteristic or the topological dimension or a yet-to-be-developed simplicial complex extension of network metrics.</p>
<p>Simplicial complex extensions of network metrics are a very new area of study within network science, and few packages or open source functions exist to calculate the simplicial analogs of network metrics. However, it is hoped that this example and some of the papers on this topic will spark the addition of simplex-based metric within network science packages. Perhaps you will take up the challenge and contribute functions to the igraph package or other open source network science tools.</p>
<p>The next tools we look at will involve a bit more topology than we’ve encountered so far, so first let’s explore another topological concept that’s useful in graph analytics and in understanding simplicial complexes.</p>
<h2 id="h1-503083c04-0003"><a class="XrefDestination" id="IntroductiontoHomology"/><span class="XrefDestination" id="xref-503083c04-005"/>Introduction to Homology</h2>
<p class="BodyFirst">The basic topological premise of our next set of tools involves counting different dimensions of holes in an object or dataset. Consider a piece of paper with a hole in the middle of it or a basketball with a sphere of air inside it. These are holes of different dimensions, and each hole separates connected pieces of an object from other pieces of itself. When these holes exist in manifolds or functions, we can systematically study them and classify objects or spaces based on the number and dimension of these holes.</p>
<p><em>Homology</em> is the counting of varying-dimensional holes (connected components, circles, spheres, voids, and so on) within a given object or space, usually to classify that object or space. For low-dimensional spaces, this is fairly straightforward; you can actually build a physical model of the space and count the holes. However, there are also variants of homology that allow topologists to distinguish between different types of objects and spaces that may be higher dimensional or strangely shaped without requiring a physical model.</p>
<p>Numbers corresponding to holes in each dimension create a handy collection of values, called <em>Betti numbers</em>, that organize the number and type of hole within a given object or space such that each object can be classified and studied alongside other objects whose numbers match. If you’re familiar with algebraic topology, this is a standard procedure for the classification of abstract mathematical structures. Commonly, these numbers are stored in a vector. It’s a bit abstract, but we’ll go through some simple examples.</p>
<h3 id="h2-503083c04-0001"><a class="XrefDestination" id="ExamplesofBettiNumbers"/><span class="XrefDestination" id="xref-503083c04-006"/>Examples of Betti Numbers</h3>
<p class="BodyFirst">Many sports involve using a ball, but not all balls are the same, topologically speaking. Basketballs and baseballs are both round balls in <span epub:type="pagebreak" id="Page_86" title="86"/>three-dimensional space. Basketballs are usually bigger than baseballs, but if there were a child’s toy basketball of the same size as a baseball, one might look at them and think they are quite similar.</p>
<figure>
<img alt="" class="" src="image_fi/503083c04/f04005.png"/>
<figcaption><p><a id="figure4-5">Figure 4-5</a>: An example baseball and basketball, which look similar but are topologically distinct</p></figcaption>
</figure>
<p>Topologically, though, they are quite distinct. These two balls differ in second Betti numbers, which count three-dimensional voids in an object. A vector of Betti numbers is an infinite sequence of numbers representing the number of holes in each dimension, starting with connected components on the zeroth number position and moving to circles (first number position), voids (second number position), and higher-dimensional voids (starting from the third position and going to infinite position). In practice, most datasets don’t have many holes past the first Betti number, so we can fill the rest of the vector with zeros. The hollow basketball has a hole past the first Betti number because it contains a void, giving a vector of Betti numbers (1, 0, 1, 0, . . .), while the solid baseball has no holes of any dimension, corresponding to a Betti number vector of (1, 0, 0, 0, . . .).</p>
<p>Some objects have more than one hole in a given dimension. For instance, imagine gluing a second basketball to the outer surface of the basketball in <a href="#figure4-5" id="figureanchor4-5">Figure 4-5</a>. This object would obviously have another void, yielding a Betti number vector of (1, 0, 2, 0, . . .). A donut, or <em>torus</em>, has a vector of (1, 2, 1, 0, . . .), as it has two open circles defining the ends of the tube, which form a void when connected at the ends. <a href="#figure4-6" id="figureanchor4-6">Figure 4-6</a> shows the classical construction of a torus from a sheet of paper.</p>
<figure>
<img alt="" class="" src="image_fi/503083c04/f04006.png"/>
<figcaption><p><a id="figure4-6">Figure 4-6</a>: The construction of a torus from a sheet of paper connected at the edges</p></figcaption>
</figure>
<p>It’s fairly easy to classify objects and spaces that can be easily visualized in three dimensions. However, many datasets used in the industry involve more than three dimensions, and comparisons and classifications of these objects require algorithms that can discern the Betti numbers associated with those objects; among these are genomics datasets (which can involve million-dimensional spaces), video sequences, and multivariate time series.</p>
<h3 id="h2-503083c04-0002"><span epub:type="pagebreak" id="Page_87" title="87"/><a class="XrefDestination" id="TheEulerCharacteristic"/><span class="XrefDestination" id="xref-503083c04-007"/>The Euler Characteristic</h3>
<p class="BodyFirst">One of the topology-based metrics shows up both in the analysis of networks and in their higher-dimensional simplicial complex cousins, and it ties back to the notion of curvature introduced in prior chapters. The <em>Euler characteristic</em>, often given the notation of <span class="CyrillicChar">χ</span>, provides a single number to summarize a topological space and is a topological invariant, meaning that the topological quantity being calculated does not change as the space is continuously deformed (stretched, twisted, or otherwise manipulated without tearing the space). The Euler characteristic can be defined using Betti numbers; technically, computing the Euler characteristic this way involves an alternating sum of Betti numbers (zeroth Betti number – first Betti number + second Betti number – third Betti number + fourth Betti number . . . up until the highest Betti number that exists).</p>
<p>The Euler characteristic can also be defined through the dimensions of the simplicial complex (number of vertices – number of edges + number of triangles – number of mutual 4-way interactions + . . .). However, vertices included in an edge aren’t counted in the number of vertices. A triangle that makes up part of a mutual four-way interaction won’t be counted either.</p>
<p>However, there is an easy way to obtain the largest pieces of a network or its higher-dimensional simplicial complex using an igraph function related to maximal cliques (as mentioned earlier). Maximal <em>k </em>– cliques denote and count the <em>k </em>– 1 simplices of the full simplicial complex derived from the network. They’re a convenient way to build the full simplicial complex and keep track of the pieces involved at each <em>n</em>-way interaction. Let’s add to the script in <a href="#listing4-2">Listing 4-2</a> to count the maximal cliques in the author’s network:</p>
<pre><code>#create a table counting the number of k+1 simplices in the simplicial complex
summ&lt;-as.numeric(summary(cl)[,1])
jjj&lt;-table(summ)</code></pre>
<p>This code creates a table summarizing the maximal cliques in the network that we previously computed. The result should yield 11 two-cliques (one-simplices, or edges), 6 three-cliques (two-simplices, or triangles), and 1 four-clique (three-simplices, or a mutual four-way interaction). We can plug these values into the Euler characteristic formula:</p>
<p class="Equation"><span class="CyrillicChar">χ</span> = 0 vertices – 11 edges + 6 triangles – 1 tetrahedron</p>
<p>This gives a <span class="CyrillicChar">χ</span> of –6. Recent studies have shown that most real-world networks have negative Euler characteristics. There’s a very interesting reason that network data tends toward negative Euler characteristics related to the curvature of the network. Negative curvature in graphs is associated with the robustness of the network; biological networks with highly negative curvature can often withstand loss of function within <span epub:type="pagebreak" id="Page_88" title="88"/>parts of the network without adverse effects on the organism. The <em>Gauss–Bonnet theorem</em> relates the Euler characteristic, defined through homology, and the curvature of the object, including the manifold’s curvature and the curvature of the manifold’s boundary. There have been some recent attempts to link network analytics tools such as homology and Forman–Ricci curvature for a deeper study into network properties. This is a deep result in a branch of mathematics called <em>differential geometry</em> that connects an object’s local geometry to its global topology, and it’s a newer area of study in network science. Now that we know network topology and geometry are related to each other, let’s look at a topological tool called <em>persistent homology</em>.</p>
<h3 id="h2-503083c04-0003"><a class="XrefDestination" id="PersistentHomology"/><span class="XrefDestination" id="xref-503083c04-008"/>Persistent Homology</h3>
<p class="BodyFirst">One of the most common topology-based algorithms used in data analysis today is persistent homology, which has been applied in genomics, healthcare, economics, energy, psychometrics, and many other fields. In essence, the idea of the persistent homology algorithm is to build a point cloud from the data, filter it into a series of simplicial complexes based on different thresholds of the data (akin to an MRI), and track topological features, such as holes or voids, appearing and disappearing in each slice. For instance, consider the three slices of cheese in <a href="#figure4-7" id="figureanchor4-7">Figure 4-7</a>, each containing holes in the shape of circles; these circles affect the first Betti numbers of the datasets.</p>
<figure>
<img alt="" class="" src="image_fi/503083c04/f04007.png"/>
<figcaption><p><a id="figure4-7">Figure 4-7</a>: Three slices of a cheese block containing holes in different places</p></figcaption>
</figure>
<p>In <a href="#figure4-7">Figure 4-7</a> one hole appears in all three slices, another appears in only the middle slice, and one appears in two slices. Holes and voids can be of different sizes in real data, and as we move across slices, holes might grow or shrink in diameter. Persistent homology algorithms have thresholds for both the lifetime of a feature and the minimum size considered for measuring a hole. In our example, we have features that are likely noise (either too small a radius or only appearing in one slice of our cheese) and features that are likely real features in the dataset (such as the void appearing in all three slices). Let’s unpack this intuition.</p>
<p>Say we want to compare two datasets to see whether they are collected from the same distribution or shape. This is common when matching image <span epub:type="pagebreak" id="Page_89" title="89"/>data. While image data rarely comes with cheese holes, circles come up in image data quite frequently in the form of eyes.</p>
<p>Technically speaking, by varying the distances used to build the simplicial complex from the point cloud data (or filtering), you can track various Betti numbers through the filtration and assign each hole in the data an importance score, with important features lasting over longer filtration distances (longer <em>persistence</em>, in the parlance of persistent homology). In <a href="#figure4-7">Figure 4-7</a>, the hole that appears in all three slices would be considered the most important feature, and the hole that appears in only the second slice might be a result of noise in the data. These features can then be plotted on a <em>barcode</em> or <em>persistence diagram</em> that tracks these features’ lifetimes (distance scale over which they exist in the filtration). We’ll explore barcodes and persistence diagrams in the following example analysis.</p>
<p>In practice, datasets are usually examined only for low-dimensional holes and features due to computational issues, and the zeroth (connected components) and first (circles) Betti numbers are used most commonly unless you are explicitly computing high-dimensional shape data. The example in <a href="#figure4-7">Figure 4-7</a> is connected in all three slices, so it has a zeroth Betti number of 1 across all slices. However, circles appear and disappear through the filtration, giving a barcode that looks like <a href="#figure4-8" id="figureanchor4-8">Figure 4-8</a>.</p>
<figure>
<img alt="" class="" src="image_fi/503083c04/f04008.png"/>
<figcaption><p><a id="figure4-8">Figure 4-8</a>: A diagram plotting the persistence of features (holes) captured in the box of <a href="#figure4-7">Figure 4-7</a></p></figcaption>
</figure>
<p>The barcode shows the time at which features appear and disappear. For instance, in <a href="#figure4-8">Figure 4-8</a>, we can see a feature that appears at time 2 and disappears at time 3 (our bottom cheese hole in <a href="#figure4-7">Figure 4-7</a>). The sequence of connected components across the data slices has a curious relationship with another machine learning method, single-linkage hierarchical clustering, in which clusters at each height level correspond to the connected components at that particular slice. When both techniques use the same distance metric, the results are actually identical; however, the persistent homology approach will give more information than single-linkage hierarchical clustering’s dendrogram regarding the structure of the data. This means that machine learning practitioners can choose the technique that fits the problem best, as these two options come with their own plots and statistical tests. For instance, with a nontechnical audience, single-linkage hierarchical clustering might be preferable, as dendrograms and heatmaps are more familiar to biologists or social scientists.</p>
<h2 id="h1-503083c04-0004"><span epub:type="pagebreak" id="Page_90" title="90"/><a class="XrefDestination" id="ComparisonofNetworkswithPersistentHomology"/><span class="XrefDestination" id="xref-503083c04-009"/>Comparison of Networks with Persistent Homology</h2>
<p class="BodyFirst">Within the realm of network analytics, persistent homology can be a useful way to compare network structures to see if different networks have the same underlying geometry. Let’s explore this further with an application to simulated networks. In neuroscience, it’s common to translate fMRI or PET data into a network structure, where different regions of the brain are translated to vertices and connected to other regions of the brain based on activity patterns (sequential activation of an area, for instance, or co-activation of multiple regions during one task). Often, outcomes of interest involve comparing groups of patients, either healthy patients against a group of patients with a particular neurological or psychological disorder or two disease groups, to understand differences in the brain activation patterns across disorders.</p>
<p>We’ll explore the use of persistent homology in the comparison of two such networks. Because fMRI data isn’t readily available as open source, we’ll simulate networks in igraph that are approximately the size of brain imaging networks; this will demonstrate how this methodology would be applied to imaging data that has been transformed to network data.</p>
<p>The igraph package allows you to simulate many types of network data, including Erdös–Renyi graphs, scale-free graphs, and Watts–Strogatz graphs. We’ll create each of these types of graphs using the script in <a href="#listing4-3" id="listinganchor4-3">Listing 4-3</a>.</p>
<pre><code>#simulate three graphs using the igraph package for further comparison
library(igraph)

#create an Erdos-Renyi graph
g1&lt;-erdos.renyi.game(30,0.3)

#create a scale-free graph
g2&lt;-sample_pa(30,power=2.5,directed=F)

#create a Watts-Strogatz graph
g3&lt;-sample_smallworld(2,5,3,0.3)

#plot the three graphs created
plot(g1,main="Erdos-Renyi Graph")
plot(g2,main="Scale-Free Graph")
plot(g3,main="Watts-Strogatz Graph")</code></pre>
<p class="CodeListingCaption"><a id="listing4-3">Listing 4-3</a>: A script that simulates three different types of network structures for statistical comparison</p>
<p><a href="#listing4-3">Listing 4-3</a> creates three different types of networks that can later be compared via persistent homology; it also visualizes the networks, which should yield something similar (but probably not identical) to <a href="#figure4-9" id="figureanchor4-9">Figure 4-9</a>.</p>
<span epub:type="pagebreak" id="Page_91" title="91"/><figure>
<img alt="" class="" src="image_fi/503083c04/f04009_m.png"/>
<figcaption><p><a id="figure4-9">Figure 4-9</a>: Plots of the three simulated network types</p></figcaption>
</figure>
<p><a href="#figure4-9">Figure 4-9</a> shows very different types of graphs. The scale-free graph in the middle includes a hub with many vertices connected to the hub but not to other vertices. The Erdös–Renyi graph on the left and the Watts–Strogatz graph on the right have many more interconnections, but the Watts–Strogatz model seems to have more structure connecting vertices into cliques, rather than randomly connecting vertices.</p>
<p>Let’s apply persistent homology to these networks and compare the distance between persistence diagrams among these networks by adding the following to <a href="#listing4-3">Listing 4-3</a>; again, your results may vary given the simulation of each network type:</p>
<pre><code>#load TDA package
library(TDAstats)

#get adjacency matrices
m1&lt;-as.matrix(get.adjacency(g1))
m2&lt;-as.matrix(get.adjacency(g2))
m3&lt;-as.matrix(get.adjacency(g3))

#compute persistent homology
d1&lt;-calculate_homology(m1,dim=2,format="cloud")
d2&lt;-calculate_homology(m2,dim=2,format="cloud")
d3&lt;-calculate_homology(m3,dim=2,format="cloud")

#plot persistence diagrams
plot_persist(d1)
plot_persist(d2)
plot_persist(d3)

#compute distances among graphs
w1&lt;-phom.dist(d1,d2,limit.num=0)
w2&lt;-phom.dist(d1,d3,limit.num=0)
w3&lt;-phom.dist(d2,d3,limit.num=0)</code></pre>
<p><span epub:type="pagebreak" id="Page_92" title="92"/>This addition derives an adjacency matrix from each of the simulated graphs and computes a persistence diagram from this adjacency matrix, which is then compared through the distances between the zeroth homology groups. This script should produce three persistence diagrams that look like <a href="#figure4-10" id="figureanchor4-10">Figure 4-10</a> (note they won’t be identical, as each run will produce something slightly different).</p>
<figure>
<img alt="" class="" src="image_fi/503083c04/f04010_m.png"/>
<figcaption><p><a id="figure4-10">Figure 4-10</a>: Persistence diagram plot for the three simulated network types (from top to bottom: Erdös–Renyi, scale-free, and Watts–Strogatz)</p></figcaption>
</figure>
<p><a href="#figure4-10">Figure 4-10</a> shows varying topological features found in each of the network types. The Watts–Strogatz network and Erdös–Renyi graphs both produce many large zeroth homology features (the dots), while the scale-free <span epub:type="pagebreak" id="Page_93" title="93"/>graph has a variety of zeroth homology feature sizes. The scale-free graph does not have higher-order homology features, while the other two graphs have first homology features (the triangles), albeit very near the diagonal line (suggesting that they may be noise). A point directly on the diagonal line is a feature that is in only one slice of the data; the farther from the diagonal line a point lies, the longer it has existed in the data. With respect to our three simulated networks, it’s hard to tell if the scale-free and Watts–Strogatz graphs differ significantly from the Erdös–Renyi graph just by looking at the persistence diagrams.</p>
<p>We can add to our script to derive a null distribution for the Erdös–Renyi persistence diagram and use a special distance metric, Wasserstein distance, to statistically test the structural differences between the Erdös–Renyi persistence diagram and the scale-free and Watts–Strogatz persistence diagrams:</p>
<pre><code>#get Wasserstein distance between random graphs with the same structure
ww&lt;-rep(NA,100)

for (i in 1:100){
  g1&lt;-erdos.renyi.game(30,0.3)
  g2&lt;-erdos.renyi.game(30,0.3)
  m1&lt;-as.matrix(get.adjacency(g1))
  m2&lt;-as.matrix(get.adjacency(g2))
  d1&lt;-calculate_homology(m1,dim=2,format="cloud")
  d2&lt;-calculate_homology(m2,dim=2,format="cloud")
  ww[i]&lt;-phom.dist(d1,d2,limit.num=0)
}

#compute 95% confidence intervals from the simulated null distribution
quantile(ww,c(0.025,0.975))</code></pre>
<p>This script creates a null distribution of Erdös–Renyi persistence diagrams from the same distribution that the original persistence diagram was constructed from; your results may vary, given the random component to the simulation piece. Quantiles of our null distribution give a confidence interval of (0.91, 8.36), which includes quite a bit smaller distances than the distances computed between the persistence diagrams of the Erdös–Renyi graph and the Watts–Strogatz graph (23.59) and between the persistence diagrams of the Erdös–Renyi graph and the scale-free graph (39.78). Thus, we can conclude that the structures of the Watts–Strogatz graph and the scale-free graph are not random. There is a significant structural component to each of these graphs.</p>
<p>This type of simulation can be very useful in testing differences between persistence diagrams of brain networks derived from fMRI and PET imaging studies, and it’s easy to implement in R. This methodology can also be applied to other networks with a hypothesized underlying structure, such as social networks or power grids. Many other types of network analysis tools can also be used to compare graph structures, such as local and global metrics (including graph radius and diameter, degree distributions, clustering <span epub:type="pagebreak" id="Page_94" title="94"/>graph coefficients, and so on), and many of these comparisons haven’t been explored much yet.</p>
<h2 id="h1-503083c04-0005"><a class="XrefDestination" id="Summary"/><span class="XrefDestination" id="xref-503083c04-010"/>Summary</h2>
<p class="BodyFirst">In this chapter, we filtered weighted networks to understand how network metrics change as edges are removed based on their weights. Then, we built simplicial complexes from network data to leverage several topological tools, including an extension of the degree metric, the Euler characteristic, and a filtration-based algorithm called persistent homology that can be used to compare networks. In the next chapter, we’ll transition from network science to distance geometry as we explore how different measurement choices impact supervised and unsupervised learning algorithms.</p>
</section>
</body>
</html>