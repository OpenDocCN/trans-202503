["```\n➊ from keras import layers\n➋ from keras.models import Model\n\n   input = layers.Input(➌shape=(1024,), ➍dtype='float32')\n➎ middle = layers.Dense(units=512, activation='relu')(input)\n➏ output = layers.Dense(units=1, activation='sigmoid')(middle)\n➐ model = Model(inputs=input, outputs=output)\n   model.compile(➑optimizer='adam', \n                 ➒loss='binary_crossentropy', \n                 ➓metrics=['accuracy'])\n```", "```\nimport numpy as np\nimport murmur\nimport re\nimport os\n\ndef read_file(sha, dir):\n    with open(os.path.join(dir, sha), 'r') as fp:\n       file = fp.read()\n    return file\n\ndef extract_features(sha, path_to_files_dir,\n                     hash_dim=1024, ➊split_regex=r\"\\s+\"):\n  ➋ file = read_file(sha=sha, dir=path_to_files_dir)\n  ➌ tokens = re.split(pattern=split_regex, string=file)\n    # now take the modulo(hash of each token) so that each token is replaced\n    # by bucket (category) from 1:hash_dim.\n    token_hash_buckets = [\n      ➍ (murmur.string_hash(w) % (hash_dim - 1) + 1) for w in tokens\n    ]\n    # Finally, we'll count how many hits each bucket got, so that our features\n    # always have length hash_dim, regardless of the size of the HTML file:\n    token_bucket_counts = np.zeros(hash_dim)\n    # this returns the frequency counts for each unique value in\n    # token_hash_buckets:\n    buckets, counts = np.unique(token_hash_buckets, return_counts=True)\n    # and now we insert these counts into our token_bucket_counts object:\n    for bucket, count in zip(buckets, counts):\n      ➎ token_bucket_counts[bucket] = count\n    return np.array(token_bucket_counts)\n```", "```\n# first you would load in my_data and my_labels via some means, and then:\nmodel.fit(my_data, my_labels, epochs=10, batch_size=32)\n```", "```\ndef my_generator(benign_files, malicious_files,\n                 path_to_benign_files, path_to_malicious_files,\n                 batch_size, features_length=1024):\n    n_samples_per_class = batch_size / 2\n  ➊ assert len(benign_files) >= n_samples_per_class\n    assert len(malicious_files) >= n_samples_per_class\n  ➋ while True:\n        ben_features = [\n            extract_features(sha, path_to_files_dir=path_to_benign_files,\n                             hash_dim=features_length)\n            for sha in np.random.choice(benign_files, n_samples_per_class,\n                                        replace=False)\n        ]\n        mal_features = [\n          ➌ extract_features(sha, path_to_files_dir=path_to_malicious_files,\n                             hash_dim=features_length)\n          ➍ for sha in np.random.choice(malicious_files, n_samples_per_class,\n                                        replace=False)\n        ]\n      ➎ all_features = ben_features + mal_features\n        labels = [0 for i in range(n_samples_per_class)] + [1 for i in range(\n                  n_samples_per_class)]\n\n        idx = np.random.choice(range(batch_size), batch_size)\n      ➏ all_features = np.array([np.array(all_features[i]) for i in idx]) \n        labels = np.array([labels[i] for i in idx])\n      ➐ yield all_features, labels\n```", "```\n   import os\n\n   batch_size = 128\n   features_length = 1024\n   path_to_training_benign_files = 'data/html/benign_files/training/'\n   path_to_training_malicious_files = 'data/html/malicious_files/training/'\n   steps_per_epoch = 1000 # artificially small for example-code speed!\n\n➊ train_benign_files = os.listdir(path_to_training_benign_files)\n➋ train_malicious_files = os.listdir(path_to_training_malicious_files)\n\n   # make our training data generator!\n➌ training_generator = my_generator(\n       benign_files=train_benign_files,\n       malicious_files=train_malicious_files,\n       path_to_benign_files=path_to_training_benign_files,\n       path_to_malicious_files=path_to_training_malicious_files,\n       batch_size=batch_size,\n       features_length=features_length\n   )\n\n➍ model.fit_generator(\n    ➎ generator=training_generator,\n    ➏ steps_per_epoch=steps_per_epoch,\n    ➐ epochs=10\n   )\n```", "```\n   import os\n   path_to_validation_benign_files = 'data/html/benign_files/validation/'\n   path_to_validation_malicious_files = 'data/html/malicious_files/validation/'\n   # get the validation keys:\n   val_benign_file_keys = os.listdir(path_to_validation_benign_files)\n   val_malicious_file_keys = os.listdir(path_to_validation_malicious_files)\n   # grab the validation data and extract the features:\n➊ validation_data = my_generator(\n       benign_files=val_benign_files,\n       malicious_files=val_malicious_files,\n       path_to_benign_files=path_to_validation_benign_files,\n       path_to_malicious_files=path_to_validation_malicious_files,\n     ➋ batch_size=10000,\n       features_length=features_length\n➌ ).next()\n```", "```\nmodel.fit_generator(\n  ➊ validation_data=validation_data,\n    generator=training_generator,\n    steps_per_epoch=steps_per_epoch,\n    epochs=10\n)\n```", "```\n   from keras.models import load_model\n   # save the model\n➊ model.save('my_model.h5')\n   # load the model back into memory from the file:\n➋ same_model = load_model('my_model.h5')\n```", "```\n   from sklearn import metrics\n\n➊ validation_labels = validation_data[1]\n➋ validation_scores = [el[0] for el in model.predict(validation_data[0])]\n➌ fpr, tpr, thres = metrics.roc_curve(y_true=validation_labels,\n                                       y_score=validation_scores)\n➍ auc = metrics.auc(fpr, tpr)\n   print('Validation AUC = {}'.format(auc))\n```", "```\nfrom ch11.model_evaluation import roc_plot\nroc_plot(fpr=fpr, tpr=tpr, path_to_file='roc_curve.png')\n```", "```\nfrom keras import callbacks\n\nmodel.fit_generator(\n    generator=training_generator,\n    # lowering steps_per_epoch so the example code runs fast:\n    steps_per_epoch=50,\n    epochs=5,\n    validation_data=validation_data,\n    callbacks=[\n        callbacks.ModelCheckpoint(save_best_only=True,➊\n                                  ➋ filepath='results/best_model.h5',\n                                  ➌ monitor='val_loss')\n   ],\n)\n```", "```\ncallbacks.ModelCheckpoint(save_best_only=False,➍\n                        ➎ filepath='results/model_epoch_{epoch}.h5',\n                          monitor='val_loss')\n```", "```\n   import numpy as np\n   from keras import callbacks\n   from sklearn import metrics\n\n➊ class MyCallback(callbacks.Callback):\n\n    ➋ def on_epoch_end(self, epoch, logs={}):\n        ➌ validation_labels = self.validation_data[1]\n           validation_scores = self.model.predict(self.validation_data[0])\n           # flatten the scores:\n           validation_scores = [el[0] for el in validation_scores]\n           fpr, tpr, thres = metrics.roc_curve(y_true=validation_labels,\n                                               y_score=validation_scores)\n        ➍ auc = metrics.auc(fpr, tpr)\n           print('\\n\\tEpoch {}, Validation AUC = {}'.format(epoch,\n                                                            np.round(auc, 6)))\n   model.fit_generator(\n       generator=training_generator,\n       # lowering steps_per_epoch so the example code runs fast:\n       steps_per_epoch=50,\n       epochs=5,\n    ➎ validation_data=validation_data,\n    ➏ callbacks=[\n           callbacks.ModelCheckpoint('results/model_epoch_{epoch}.h5',\n                                     monitor='val_loss',\n                                     save_best_only=False,\n                                     save_weights_only=False)\n       ]\n   )\n```"]