- en: '**20'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**20'
- en: STATELESS AND STATEFUL TRAINING**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**无状态与有状态训练**'
- en: '![Image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/common.jpg)'
- en: What is the difference between stateless and stateful training workflows in
    the context of production and deployment systems?
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产和部署系统的背景下，无状态和有状态训练工作流之间有什么区别？
- en: Stateless training and stateful training refer to different ways of training
    a production model.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 无状态训练和有状态训练是训练生产模型的两种不同方式。
- en: '**Stateless (Re)training**'
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**无状态（重新）训练**'
- en: In stateless training, the more conventional approach, we first train an initial
    model on the original training set and then retrain it as new data arrives. Hence,
    stateless training is also commonly referred to as stateless *retraining*.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在无状态训练中，这是一种更为传统的方法，我们首先在原始训练集上训练初始模型，然后随着新数据的到来重新训练模型。因此，无状态训练通常也被称为无状态*重新训练*。
- en: As [Figure 20-1](ch20.xhtml#ch20fig1) shows, we can think of stateless retraining
    as a sliding window approach in which we retrain the initial model on different
    parts of the data from a given data stream.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图 20-1](ch20.xhtml#ch20fig1)所示，我们可以将无状态重新训练视为一种滑动窗口方法，在这种方法中，我们会基于给定数据流的不同部分对初始模型进行重新训练。
- en: '![Image](../images/20fig01.jpg)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/20fig01.jpg)'
- en: '*Figure 20-1: Stateless training replaces the model periodically.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 20-1：无状态训练定期替换模型。*'
- en: For example, to update the initial model in [Figure 20-1](ch20.xhtml#ch20fig1)
    (Model 1) to a newer model (Model 2), we train the model on 30 percent of the
    initial data and 70 percent of the most recent data at a given point in time.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，为了将[图 20-1](ch20.xhtml#ch20fig1)中的初始模型（模型 1）更新为更新模型（模型 2），我们会使用初始数据的 30%
    和最新数据的 70% 来训练模型。
- en: Stateless retraining is a straightforward approach that allows us to adapt the
    model to the most recent changes in the data and feature-target relationships
    via retraining the model from scratch in user-defined checkpoint intervals. This
    approach is prevalent with conventional machine learning systems that cannot be
    fine-tuned as part of a transfer or self-supervised learning workflow (see [Chapter
    2](ch02.xhtml)). For example, standard implementations of tree-based models, such
    as random forests and gradient boosting (XGBoost, CatBoost, and LightGBM), fall
    into this category.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 无状态重新训练是一种直接的方法，它通过在用户定义的检查点间隔内从头开始重新训练模型，使我们能够将模型适应数据和特征-目标关系中的最新变化。这种方法在传统的机器学习系统中很常见，尤其是那些不能作为迁移学习或自监督学习工作流的一部分进行微调的系统（参见[第2章](ch02.xhtml)）。例如，基于树的标准模型实现，如随机森林和梯度提升（XGBoost、CatBoost
    和 LightGBM），就属于这一类。
- en: '**Stateful Training**'
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**有状态训练**'
- en: In stateful training, we train the model on an initial batch of data and then
    update it periodically (as opposed to retraining it) when new data arrives.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在有状态训练中，我们在初始数据批次上训练模型，然后在新数据到达时定期更新它（而不是重新训练）。
- en: As illustrated in [Figure 20-2](ch20.xhtml#ch20fig2), we do not retrain the
    initial model (Model 1.0) from scratch; instead, we update or fine-tune it as
    new data arrives. This approach is particularly attractive for models compatible
    with transfer learning or self-supervised learning.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图 20-2](ch20.xhtml#ch20fig2)所示，我们不会从头开始重新训练初始模型（模型 1.0）；相反，我们会随着新数据的到来更新或微调它。这种方法对于与迁移学习或自监督学习兼容的模型特别有吸引力。
- en: '![Image](../images/20fig02.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/20fig02.jpg)'
- en: '*Figure 20-2: Stateful training updates models periodically.*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 20-2：有状态训练定期更新模型。*'
- en: The stateful approach mimics a transfer or self-supervised learning work-flow
    where we adopt a pretrained model for fine-tuning. However, stateful training
    differs fundamentally from transfer and self-supervised learning because it updates
    the model to accommodate concept, feature, and label drifts. In contrast, transfer
    and self-supervised learning aim to adopt the model for a different classification
    task. For instance, in transfer learning, the target labels often differ. In self-supervised
    learning, we obtain the target labels from the dataset features.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 有状态方法模仿了迁移学习或自监督学习的工作流，在这种方法中，我们采用预训练模型进行微调。然而，有状态训练与迁移学习和自监督学习有根本性的不同，因为它会更新模型，以适应概念、特征和标签的漂移。相比之下，迁移学习和自监督学习的目标是将模型应用于不同的分类任务。例如，在迁移学习中，目标标签通常不同；在自监督学习中，我们从数据集特征中获取目标标签。
- en: One significant advantage of stateful training is that we do not need to store
    data for retraining; instead, we can use it to update the model as soon as it
    arrives. This is particularly attractive when data storage is a concern due to
    privacy or resource limitations.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 有状态训练的一个显著优点是我们不需要存储数据进行重新训练；相反，我们可以在数据到达时立即用它来更新模型。当数据存储因隐私或资源限制而成为问题时，这一点尤为吸引人。
- en: '**Exercises**'
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**练习**'
- en: '**20-1.** Suppose we train a classifier for stock trading recommendations using
    a random forest model, including the moving average of the stock price as a feature.
    Since new stock market data arrives daily, we are considering how to update the
    classifier daily to keep it up to date. Should we take a stateless training or
    stateless retraining approach to update the classifier?'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**20-1.** 假设我们使用随机森林模型训练一个用于股票交易推荐的分类器，将股票价格的移动平均作为特征。由于新的股市数据每天都会到达，我们正在考虑如何每天更新分类器以保持其最新状态。我们是应该采用无状态训练还是有状态重新训练的方法来更新分类器？'
- en: '**20-2.** Suppose we deploy a large language model (transformer) such as Chat-GPT
    that can answer user queries. The dialogue interface includes thumbs-up and thumbs-down
    buttons so that users can give direct feedback based on the generated queries.
    While collecting the user feedback, we don’t update the model immediately as new
    feedback arrives. However, we are planning to release a new or updated model at
    least once per month. Should we use stateless or stateful retraining for this
    model?'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**20-2.** 假设我们部署一个大型语言模型（例如Chat-GPT），该模型可以回答用户查询。对话界面包括点赞和点踩按钮，用户可以根据生成的查询直接反馈。在收集用户反馈时，我们不会在新反馈到达时立即更新模型。然而，我们计划每月至少发布一次新的或更新的模型。我们应该为这个模型使用无状态还是有状态重新训练？'
