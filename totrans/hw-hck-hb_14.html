<html><head></head><body>
<section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" title="401" id="Page_401"/>14</span><br/>
<span class="ChapterTitle">Think of the Children: Countermeasures, Certifications, and Goodbytes</span></h1>
</header>
<figure class="opener">
<img src="image_fi/book_art/chapterart.png" alt=""/>
</figure>
<p class="ChapterIntro">We’ve written much about various attacks, but the ultimate goal of a defensive hacker is to improve security. With that in mind, we dedicate this chapter to countermeasures that mitigate fault attacks and side-channel analysis, various certifications that exist, and how you can get better. This is also the concluding chapter to our book, which we see as the bridge to the next step in our journey, which is to fix the problems you will expose.</p>
<p>Countermeasures are as old as the field of side-channel power analysis itself, and an area of active research. We’ll cover several of the classic countermeasures that are good first steps, along with their limitations. When you first hear about side-channel analysis, some obvious countermeasures come to mind, but it’s always important to evaluate them. For example, just adding noise to the system might sound like a good countermeasure, but in practice this makes the attack only slightly harder. The countermeasures in this chapter are publicly known (no NDAs were violated in the making of this book) <span epub:type="pagebreak" title="402" id="Page_402"/>and are typically ones that have some usage in the industry and represent a “reasonable effort.” Countermeasure development in highly secure products requires significant investment and collaboration between hardware design and software design teams. However, even with some software-only changes, we can make SCA and FI attacks much more difficult to execute.</p>
<p>It’s critically important that you evaluate the effectiveness of your countermeasures. For both power analysis and fault injection countermeasures, this must be a continuous evaluation. If you are writing C code, for example, your C compiler can simply optimize countermeasures away. A very common story in embedded security is that a “secure” product with a highly effective countermeasure was evaluated only at certain stages of the design. The compiler, synthesis tool, or implementation destroyed the effectiveness of the countermeasure. If you don’t test early and often, you’ll end up shipping products that you think are protected but simply aren’t.</p>
<p>The tools we have taught you in this book are a great starting point for this evaluation. You can even start to set up a fully automated analysis, for example, so your product is being continuously evaluated with the actual toolchain in use.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">NOTE</span></h2>
<p>	Many of the examples in this chapter will refer to the companion notebook. As with other chapters, we have decided to put more substantial code samples into a Jupyter notebook as part of the website for this book. This allows you to easily run and interact with the examples, which makes understanding how the various countermeasures work easier than staring at code in a printed book. Some of the more basic examples include code in this book, but even for those we encourage you to experiment with our examples to see how the countermeasures work.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<h2 id="h1-278748c14-0001">Countermeasures</h2>
<p class="BodyFirst">Ideal countermeasures don’t exist, but adding several together can make an attacker’s job hard enough for them to give up. In this section, we’ll provide several countermeasure constructions that you can apply in software or hardware. We’ll also discuss countermeasure verification, which is effectively the application of the techniques you learned in different chapters to see how much harder the attack becomes. The examples that follow are simplified to demonstrate each principle; therefore, we “ignore” some recommendations from the other principles. Many of these countermeasures are covered in the whitepaper “Secure Application Programming in the Presence of Side Channel Attacks” by Marc Witteman et al.</p>
<h3 id="h2-278748c14-0001">Implementing Countermeasures</h3>
<p class="BodyFirst">Implementing countermeasures in a commercial product is very difficult and therefore hard to get “right” the first time; in this context, “right” means the right balance of cost, power, performance, security, debuggability, development complexity, and whatever else you care about. Most successful manufacturers reach a good balance of these considerations after several product iterations. Once you start exploring conflicts between <span epub:type="pagebreak" title="403" id="Page_403"/>security and other aspects, at least you know you’re doing something right. You’ve hopefully already implemented the low-hanging fruit countermeasures and are now reaching the point where real tradeoffs need to be made. This means you are actively doing cost/benefit analyses, and you realize there is no absolute security; this is life and this is good.</p>
<p>You do want to avoid some common pitfalls. What we typically see is that the law of leaky abstractions (“all nontrivial abstractions, to some degree, are leaky,” by Joel Spolsky) applies to security vulnerabilities; side channels and faults are clearly cases of it, but it also applies to countermeasures. Electrical engineers will come up with a new circuit, computer scientists with improved code, and crypto people with a new cipher. The problem is that they commonly use the same abstraction when designing a countermeasure as when designing the object that contains the vulnerability, and that leads to ineffective countermeasures. You’ll see a basic example of how a secure countermeasure from one implementation (software) can fail on another implementation (hardware) in the “Noncorrelating/Constant Power Consumption Everywhere” section later in this chapter.</p>
<p>Breaking through the abstractions requires a fundamental understanding of every level of your stack, good-enough simulators, and/or plain-old testing of your final product. In other words, this is hard and iterative work; you won’t get it right the first time, but if you do it right, you’ll get gradually better.</p>
<p>One of the key insights about countermeasures is that they operate by breaking an attack’s assumptions. Every attack makes some assumptions that are required to be true in order for the attack to succeed. For instance, in differential power analysis (DPA), the assumption is that your operations are aligned in time, so a countermeasure introducing misalignment breaks this assumption and reduces DPA’s effectiveness. Having an attack tree ready with known attacks and choosing countermeasures that break those attacks’ assumptions is a good strategy.</p>
<p>This reasoning also works in the opposite direction: countermeasures rely on assumptions on the attacks, and it’s up to attackers to break them. The previous example of introducing misalignment as a countermeasure to DPA operates under the assumption that an attacker isn’t able to recognize features in a trace and perform alignment. This is where cat-and-mouse games start.</p>
<p>With these cat-and-mouse games, countermeasures are broken and upgraded, and attacks are thwarted and improved. In software, the main game plan is patching. With hardware, that strategy is not possible. In some cases, you can patch hardware vulnerabilities using software countermeasures, which means you can keep a product secure for a bit longer. In other cases, you’ll rely on the security of a product as it was shipped. Ideally, products are shipped with a hardware security margin that makes them resistant against attackers <em>X</em>  years in the future (although determining <em>X</em>  is impossible due to the nonlinear nature of attacks), kind of like medicine products need to have some expiry date for their safe usage. In reality, this is impossible, and the common strategy is one of “best effort” combined with allowing patching through firmware updates and configuration changes.</p>
<p><span epub:type="pagebreak" title="404" id="Page_404"/>None of the countermeasures presented here are perfect, but they don’t need to be. With some extra effort or more clever attack, an attacker will be able to bypass them. The point is not to create an unbreakable system, but one where the cost of a successful attack is lower than the cost of the countermeasures or where the cost of attacking is higher than the attacker’s budget.</p>
<h4 id="h3-278748c14-0001">Noncorrelating/Constant Time Everywhere</h4>
<p class="BodyFirst">If an operation’s duration depends on some secret, either simple power analysis (SPA) or timing analysis may be able to recover that secret. The classical example of correlating time is using <code>strcmp()</code> or <code>memcmp()</code> to verify a password or PIN. (Storing the plaintext password or PIN instead of a hashed form is not secure in the first place, but let’s take it as an example.) Both of these C functions have an early termination condition, as they return after the first differing byte, giving an attacker who can measure timing the information of which character of an entered PIN differs from a stored PIN. For examples, see Chapter 8 on timing attacks and the <code>memcmp()</code> example in this chapter’s companion notebook (available at <a href="https://nostarch.com/hardwarehacking/" class="LinkURL">https://nostarch.com/hardwarehacking/</a>).</p>
<p>The trick is to implement a countermeasure that <em>decorrelates</em> the timing between the operation and the secret, which means making the operating <em>time constant</em> (and possibly adding <em>timing randomization</em> on top), as shown in <a href="#listing14-7" id="listinganchor14-7">Listing 14-7</a>. One solution is to implement a time constant memory comparison, like in <code>memcmp_consttime()</code> in this chapter’s notebook. We have the core of that function shown in <a href="#listing14-1" id="listinganchor14-1">Listing 14-1</a>.</p>
<pre><code>def memcmp_consttime(c1, c2, num):
    # Accumulate differing bits in diff
    diff = 0
    for i in range(num):
        # If bits differ, the xor is nonzero, therefore diff will be nonzero
        diff = diff | (c1[i] ^ c2[i])
    return diff</code></pre>
<p class="CodeListingCaption"><a id="listing14-1">Listing 14-1</a>: A constant time <code>memcmp</code><code>()</code> function</p>
<p>Instead of terminating on the first differing byte, for each set of bytes in the two buffers, we calculate the XOR, which is zero if the bytes are the same and nonzero otherwise. We then accumulate all XORs by OR-ing them into <code>diff</code>, which means that once a single bit differs, this bit will remain set in <code>diff</code>. This code has no branches that depend on the contents of either buffer. Even better from a leakage perspective is to compare hashes of values instead, but doing so will be slower. Note that this example doesn’t include overflow checks for simplicity.</p>
<p>Timing attacks on hash-based message authentication code (HMAC) comparisons are common in cryptographic implementations. If you have a data blob that’s signed using HMAC, the target system computes the HMAC over the blob and compares it to the signature. If that comparison leaks timing information, it allows brute-forcing the HMAC value, just like <span epub:type="pagebreak" title="405" id="Page_405"/>the preceding password example, without the HMAC key ever being known. This attack was used to bypass Xbox 360 code verification, called the <em>Xbox 360 timing attack</em> (unlike the FI attack in Chapter 13). To fix this, the <em>constant time comparison</em> can be used. </p>
<p>Another important aspect is the timing of branches that are conditional on a sensitive value. A simple example would be the code shown in <a href="#listing14-2" id="listinganchor14-2">Listing 14-2</a>. If the secret value passed is <code>0xCA</code>, the execution of <code>leakSecret()</code> takes much longer than if the value is different.</p>
<pre><code>if secret == 0xCA:
    res = takesLong()
else:
    res = muchShorter()</code></pre>
<p class="CodeListingCaption"><a id="listing14-2">Listing 14-2</a>: We can identify whether or not <code>secret</code> is <code>0xCA</code> by measuring the execution time of this code.</p>
<p>Now, just by measuring the duration of the process, or by looking at SPA signals, an attacker can derive whether the secret value equals <code>0xca</code>. An attacker can also use knowledge of the timing of the <code>if()</code> statement in order to try to fault it.</p>
<p>One solution is to make the relevant code <em>branchless</em>, like in <code>dontLeakSecret()</code> in <a href="#listing14-3" id="listinganchor14-3">Listing 14-3</a>.</p>
<pre><code>def dontLeakSecret(secret):
    # Run both sides of the if() condition
    res1 = takesLong()
    res2 = muchShorter()
    # Mask is either all bits 0 or all bits 1, depending on if() condition
    mask = int(secret == 0xCA) - 1 
    res = (res1 &amp; ~mask) | (res2 &amp; mask) # Use mask to select one value return res
    return res</code></pre>
<p class="CodeListingCaption"><a id="listing14-3">Listing 14-3</a>: We avoid the obvious power analysis by always executing both operations.</p>
<p>The idea is to execute both sides of the branch and store the results separately. Then we calculate a <code>mask</code>, which is either all zeros or all ones in binary, depending on the outcome of the <code>if()</code> condition. We can use this mask to combine the results logically: if the mask is all zeros, we take the result from one side of the branch; if it’s all ones, we take the result from the other. We’ve also tried to use operations to do the mask generation and assignment without a conditional code flow, but as we mention later, the risk here is that a clever compiler may yet detect what we are doing and replace our code with conditional code. The example from <a href="#listing14-3">Listing 14-3</a> (along with all the examples) may be easier to understand when running the code yourself, so be sure to see the companion notebook for this chapter to better understand the program flow. There are some obvious limitations here: <code>takesLong()</code> and <code>muchShorter()</code> should not have any side effects, and the performance of this code will be poorer.</p>
<p>Finally, <em>timing randomization</em> is the insertion of nonconstant time operations that don’t depend on a secret. The simplest is just a loop that iterates <span epub:type="pagebreak" title="406" id="Page_406"/>some random number of times, which should be tuned such that it introduces sufficient uncertainty in timing for the processed secret. If a secret would normally leak during a particular clock cycle, you want to spread that out over at least dozens or hundreds of clock cycles. Realignment is nontrivial for an attacker if timing randomization is combined with sufficient noise addition (see the “Noncorrelating/Constant Power Consumption Everywhere” section, next).</p>
<p>Timing randomization also helps against fault injection, as an attacker now either has to be lucky that the timing of the fault coincides with the randomized timing or needs to spend extra time on a setup that synchronizes with the target operation.</p>
<p>Device clocks that are driven by a PLL and not directly by an external crystal are usually not perfectly stable. Therefore, some timing randomization already comes “naturally”. Similarly, interrupts can add instability to the timing. These effects may add sufficient randomization for some use cases.</p>
<p>If not, it is recommended to add timing randomization explicitly before sensitive operations. Timing randomization may be easily seen in side-channel traces, so it points a big arrow to the sensitive operations. Noise addition may be able to help here, as it makes attack techniques such as alignment and Fourier transforms that discard timing information more difficult. If you can afford the performance hit, you should sprinkle timing randomization throughout your hardware design or software code.</p>
<h4 id="h3-278748c14-0002">Noncorrelating/Constant Power Consumption Everywhere</h4>
<p class="BodyFirst">You can observe leakage in the power consumption signal’s amplitude. The less correlation there is between sensitive data/operations and power consumption, the better, but that’s nontrivial to achieve. The most basic way to do it is to add <em>noise</em> to the power consumption by running any piece of hardware or software in parallel. This strategy doesn’t fully decorrelate the signal, but it increases the noise and therefore increases the attack cost. In hardware, generating this noise can mean running a random number generator, a special noise generator, or a video decoder on dummy data. In software, you could run a parallel thread on another CPU core that performs decoy or dummy operations.</p>
<p>In hardware, it’s possible to design a circuit that’s <em>balanced</em>—that is, for every clock, the same number of bitflips occurs irrespective of the data being processed. This balancing is called <em>dual-rail logic</em>, and the idea behind it is that each gate and line has an inverted version as well, such that a zero-to-one transition co-occurs with a one-to-zero transition. Adding this balancing is very expensive in terms of chip area and requires extremely careful and low-level balancing to make sure each transition happens at the same time. Imbalances still lead to leakage, though much less than without this technique. Additionally, electromagnetic signals must also be taken into account: two inverted signals may amplify or cancel each other out, depending on the spatial arrangement of the signals.</p>
<p><span epub:type="pagebreak" title="407" id="Page_407"/>For crypto, we can go beyond adding random noise and play some nice tricks using <em>masking</em>. Ideally, for every encryption or decryption, a random mask value is generated and is mixed in with the data at the start of the cipher. We then modify the cipher implementation such that the intermediate values stay masked, and at the end of the cipher, we “unmask” the result. Theoretically, nowhere during the cipher’s execution should any intermediate value be present without a mask. This means DPA should fail, as DPA critically depends on being able to predict an (unmasked) intermediate value. Masking thereby should not have <em>first-order</em> <em>leakage</em>, which is leakage that can be exploited by only looking at a single point in time.</p>
<p>One example of masking is rotating S-box masking of AES (see “RSM: A Small and Fast Countermeasure for AES, Secure Against 1st and 2nd-Order Zero-Offset SCAs,” by Maxime Nassar, Youssef Souissi, Sylvain Guilley, and Jean-Luc Danger). In <em>Rotating</em> <em>S-boxes Masking (RSM)</em>, we modify each of the 16 S-boxes such that they take in a mask value <em>M</em><sub><em>i</em></sub>, and they produce an output value masked with <em>M</em><sub>(</sub><sub><em>i+1</em></sub><sub>)</sub><sub><em> mod </em></sub><sub>16</sub>, where <em>M</em><sub><em>i</em></sub> is a randomly chosen 8-bit value for 0 ≤ <em>i</em> &lt; 16. Masking is simply done using XOR. The S-box tables are recalculated only once before executing the cipher. For the cipher invocation, we XOR the initial mask onto the key, which in turn XOR masks the data during <code>AddRoundKey</code>. The XOR masks are preserved by the modified S-box in <code>SubBytes</code> and <code>ShiftRows</code> operations. The <code>MixColumns</code> operation is executed as is, but afterward is “fixed” by XORing in a state that effectively remasks the state vector. The result is a masked AES state vector after the first round and masked intermediate values all throughout the computation. These steps are repeated for all rounds, and then the data is unmasked by a final XOR. </p>
<p>The problem with masking is usually that the “perfect” model doesn’t always apply in reality. As in the case of RSM, masks are reused, and therefore “perfect” has been traded for a performance gain. The paper “Lowering the Bar: Deep Learning for Side-Channel Analysis,” by Guilherme Perin, Baris Ege, and Jasper van Woudenberg, shows that first-order leakage is still present for one implementation of RSM.</p>
<p>Even if masking is “perfect,” so-called <em>second-order attacks</em> on masking exist, which work on the principle that we look at two intermediate values, <em>X</em> and <em>Y</em>. For example, <em>X</em> could be a byte of state after <code>AddRoundKey</code>, and <em>Y</em> a byte after <code>SubBytes</code>. If they are both masked with the same mask <em>M</em> during execution—that is, <em>X</em> ⊕ <em>M</em> and <em>Y</em> ⊕ <em>M</em>—we can do the following. We measure a side-channel signal of <em>X</em> ⊕ <em>M</em> and <em>Y</em> ⊕ <em>M</em>. Assume for a moment we know the points in time <em>x</em> and <em>y</em> where the signal of <em>X</em> ⊕ <em>M</em> and <em>Y</em> ⊕ <em>M</em> are leaking, which means we can obtain their corresponding sample values <em>t</em><sub><em>x</em></sub> and <em>t</em><sub><em>y</em></sub>. We can combine these two measurements points (for example, by calculating their absolute difference as |<em>t</em><sub><em>x</em></sub> − <em>t</em><sub><em>y</em></sub>|). We also know (<em>X</em> ⊕ <em>M</em>) ⊕ (<em>Y</em> ⊕  <em>M</em>) = <em>X</em> ⊕ <em>Y</em>. As it turns out, there is actually a correlation between |<em>t</em><sub><em>x</em></sub>  − <em>t</em><sub><em>y</em></sub>| and <em>X</em> ⊕ <em>Y</em>, and on that correlation, we can perform DPA. This is called a second-order<em> </em>attack because we combine two points on the trace, but the idea extends up to any <em>higher-order attacks</em>: first-order masking applies one mask to a value (that is, <em>X</em> ⊕ <em>M</em>) and can be attacked with <span epub:type="pagebreak" title="408" id="Page_408"/>second-order DPA. Second-order masking applies two masks to a value (that is, <em>X</em> ⊕ <em>M</em><sub><em>1</em></sub> ⊕ <em>M</em><sub><em>2</em></sub>) and can be attacked with third-order DPA, and so on. In general, <em>n</em>th-order masking can be attacked with (<em>n</em> + 1)th-order DPA.</p>
<p>The problem with a second-order attack is finding the points in time <em>x</em> and <em>y</em> where the signals of <em>X</em> ⊕ <em>M</em> and <em>Y</em> ⊕ <em>M</em> are leaking. In normal DPA, we just correlate all samples at a single point in time in a trace to find leakage. If we don’t know time <em>x</em> and <em>y</em>, we have to “brute-force” them by combining all possible samples in a trace and perform DPA on all these combinations. This is a problem of quadratic complexity in the number of samples in a trace. Also, the correlation isn’t perfect, so proper masking forces an attacker to perform more measurements and more computation. In other words, masking, though expensive and error-prone to implement, also puts a significant burden on an attacker.</p>
<p><em>Blinding</em> is similar to masking, except that the origins of those techniques are in (non-side-channel) cryptography. Various blinding techniques for RSA and ECC exist, and they rely on math. One example is RSA message blinding. For ciphertext <em>C</em>, message <em>M</em>, modulus <em>N</em>, public exponent <em>e</em> and private exponent <em>d</em>, and a random blind <em>1 </em>&lt; <em>r </em>&lt; <em>N</em>, we first calculate the blinded message <em>R </em>= <em>M</em> × <em>r</em><sup><em>e</em></sup> mod <em>N</em>. Next, we perform the RSA signing on the blinded message, <em>R</em><sup><em>d </em></sup>= (<em>M </em>× <em>r</em><sup><em>e</em></sup>)<sup><em>d </em></sup>= <em>M</em><sup><em>d</em></sup> × <em>r</em><sup><em>ed</em></sup> = <em>C</em> × <em>r</em>, and we unblind by calculating (<em>C</em> × <em>r</em>) × <em>r</em><sup><em>–1</em></sup> = <em>C</em>. This results in the same value as textbook RSA without blinding, which would directly calculate <em>M</em><sup><em>d</em></sup> <sup><em> </em></sup>= <em>C</em>. However, because <em>R</em> in <em>R</em><sup><em>d</em></sup> is unpredictable for an attacker, timing attacks that require the message <em>M</em> to be raised to <em>d</em> fail. This is called <em>message blinding</em>.</p>
<p>Since RSA uses one or a few bits of exponent <em>d</em> at a time, the exponent is also prone to timing or other side-channel attacks. To mitigate the side-channel leakage of the exponent value, exponent blinding is needed, which ensures that the exponent used in every RSA calculation is different by creating a random number 1 ≤ <em>r </em>&lt; 2<sup>64</sup> and creating a new exponent <em>d′</em> = <em>d</em> + ϕ(<em>N</em>) × <em>r</em>, where ϕ(<em>N</em>) = (<em>p</em> – 1) × (<em>q</em> – 1) is the group order. The new exponent is “automatically” unblinded by the modular reduction (that is, <em>M</em><sup><em>d</em></sup>=<em>M</em><sup><em>d′</em></sup> mod <em>N</em>) but is unpredictable from the point of view of a side-channel attacker. The blinded exponent <em>d′</em> can be random for each invocation of the cipher, so an attacker isn’t able to learn more and more about <em>d</em> or a single <em>d′</em> by taking more traces. This raises the bar for an attacker. Instead of being able to acquire more information by acquiring more traces, an attacker is forced to break a single trace. However, if the implementation is very leaky, SPA attacks may be effective: completely extracting <em>d</em>′ from a single trace is equivalent to finding the unblinded private key <em>d</em>.</p>
<p>Many more blinding and masking techniques exist, as well as <em>time-constant</em> or <em>randomized exponentiation algorithms</em> for RSA and <em>scalar multiplication algorithms</em> for ECC: <em>modulus blinding</em>, <em>Montgomery ladders</em>, <em>randomized additions chains</em>, <em>randomized projective coordinates</em>, and <em>higher-order masking</em>. It’s an active field of study, and we recommend researching the latest in attacks and countermeasures.</p>
<p>When working with these countermeasures, be aware of their underlying assumptions. The example of masking earlier in this section implied <span epub:type="pagebreak" title="409" id="Page_409"/>an assumed Hamming weight leakage. But what if we implemented this in hardware, and a register leaked the Hamming distance between consecutive values? It’s possible then that the masking would be annihilated. The unmasking happens when a register consecutively contains the two masked values, <em>X</em> ⊕ <em>M</em> and then <em>Y</em> ⊕ <em>M</em>, which would leak HD(<em>X</em> ⊕ <em>M</em>, <em>Y </em>⊕ <em>M</em>). The issue can be seen if we rewrite this as follows: HD(<em>X</em> ⊕ <em>M</em>, <em>Y</em> ⊕ <em>M</em>) = HW(<em>X</em> ⊕ <em>M</em> ⊕ <em>Y</em> ⊕ <em>M</em>) = HW(<em>X</em> ⊕ <em>Y</em>) = HD(<em>X</em>, <em>Y</em>). Effectively, the hardware has unmasked the value for you and just leaks the same Hamming distance. Therefore, at the algorithm level, this countermeasure seems like a good one, but the implementation can bite you back.</p>
<h4 id="h3-278748c14-0003">Randomize Access to Confidential Array Values</h4>
<p class="BodyFirst">This countermeasure is an easy one. If you’re looping over some secret that’s stored in an array, do it in a <em>random order</em>, or at least pick a <em>random starting point</em> and then loop over the array in order. This method disallows an attacker with side-channel possibilities from learning about a specific entry in the array. Examples where this is useful include verifying HMACs (or plaintext passwords) or zeroing/wiping keys from memory, as you don’t want to leak some of this information accidentally at a predictable point in time. See the companion notebook for an example in the <code>memcmp_randorder()</code> function that starts at an arbitrary point in the two arrays and does not branch, depending on buffer data. Alternatively, you can refer to <a href="#listing14-4" id="listinganchor14-4">Listing 14-4</a>.</p>
<h4 id="h3-278748c14-0004">Perform Decoy Operations or Infective Computing</h4>
<p class="BodyFirst"><em>Decoy operations</em> are designed to mimic an actual sensitive operation (from a side-channel perspective), but they have no actual effect on the output of the operation. They fool an attacker into analyzing the wrong part of a side-channel trace and can double as a way to decorrelate timing. One example is the <em>square-and-multiply-always countermeasure</em> for modular exponentiation in RSA. In textbook RSA, for every bit of the exponent, you perform a square operation if the exponent bit is 0, and you perform a multiplication-and-square operation if the bit is 1. This difference in operation for a 0-versus-1 bit has very obvious (SPA) side-channel leakage. To even it out, you can perform a decoy multiplication and discard the result if the bit is 0. Now, the number of squares and multiplications are balanced. Another example is adding extra rounds to AES that discard their results.</p>
<p>To stick with our running memory compare example in the notebook, we add some random decoy rounds in <code>memcmp_decoys()</code>. It works by randomly executing a decoy XOR and making sure the result doesn’t get accumulated. This is also used in <a href="#listing14-4">Listing 14-4</a>.</p>
<p><em>Infective computing</em> goes one step further: it uses the decoy operations as a way to “infect” the output. If any error occurs in the decoy operation, it corrupts the output. This is particularly handy in crypto operations; see “Infective Computation and Dummy Rounds: Fault Protection for Block Ciphers Without Check Before-Output” by Benedikt Gierlichs, Jörn-Marc Schmidt, and Michael Tunstall.</p>
<p><span epub:type="pagebreak" title="410" id="Page_410"/>Another good use of decoy operations is detecting faults (detect and respond to faults). If the decoy operation has a known output, you can verify that output is correct; if not, a fault must have occurred.</p>
<h4 id="h3-278748c14-0005">Side-Channel-Resistant Crypto Libraries, Primitives, and Protocols</h4>
<p class="BodyFirst">Saying “use vetted <em>crypto libraries</em>” is along the lines of the Crypto 101 rule “don’t roll your own crypto.” The caveat here is that most open source crypto libraries do not provide any power analysis side-channel resistance or fault-resistance guarantees. Common libraries (such as OpenSSL and NaCl) and primitives (such as Ed25519) do protect against timing side-channel attacks, mainly because timing attacks can be exploited remotely. If you’re building on top of a microcontroller or secure element, the crypto cores and/or library that comes with the chip may claim to have some resistance. Check the datasheet for the word <em>countermeasure</em>, <em>side channel</em>, or <em>fault</em>, or check any certifications. Even better, test the chip!</p>
<p>If you’re stuck with a crypto library or primitive that is not power side-channel resistant, you may be able to use a <em>leakage-resistant protocol</em>. These protocols basically ensure that keys are used only once or a few times, thus making DPA significantly harder. For instance, you can hash a key in order to create a new key for a next message. This type of operation is used, for example, in the AES mode implemented by NXP with the LPC55S69, which is called <em>Indexed Code Block</em> mode.</p>
<p>Finally, you can <em>wrap</em> the library to perform some safety checks against faults. For instance, after signing with ECC or RSA, you can verify the signature to check whether it passes. If not, some fault must have happened. Similarly, you can decrypt after encrypting to check that you obtained the plaintext again. Performing these checks pushes an attacker into double faults: one to target the algorithm and another to bypass the fault check.</p>
<h4 id="h3-278748c14-0006">Don’t Handle Keys When You Can Avoid It</h4>
<p class="BodyFirst">Pretend that you are Superman and that keys are kryptonite; handle them with care and only when absolutely needed. Don’t copy (or integrity-check) them, and do pass them by <em>reference</em> in your application rather than by value. When using a crypto engine, avoid loading the key to the engine more than necessary to avoid <em>key-loading attacks</em>. This practice obviously reduces the possibilities for side-channel leakage, but also for fault attacks on the key. Differential fault analysis is a class of fancy crypto fault attacks, but there are more fault attacks on crypto. </p>
<p>Say an attacker can just zero out (part of) a key (for instance, during a key copy operation). Doing so can break challenge-response protocols. Challenge-response is basically used by one party to establish whether the other party has knowledge of a key: Alice sends Bob a nonce <em>c</em> (the challenge), and Bob encrypts <em>c</em> with shared key <em>k</em> and sends the response <em>r</em>. Alice performs the same encryption and verifies that Bob sent the correct <em>r</em>. Now Alice knows that Bob has knowledge of key <em>k</em>.</p>
<p>That’s all fine and dandy, except the Fault Fairy now has physical access to Alice’s crypto device. The key Alice uses for verification is now corrupted <span epub:type="pagebreak" title="411" id="Page_411"/>by a fault such that it is all zeros. Because the Fault Fairy knows this, she can spoof Bob by encrypting <em>r</em> with a zero key. Alternatively, if the Fault Fairy has access to Bob’s crypto device and can partially zero a key (for example, all except one byte), she can use one pair of <em>c</em> and <em>r</em> to brute-force the one nonzero key byte. Iterating over the other key bytes can expose the entire key. If the device is reloading the key frequently, the Fault Fairy has many attempts to zero out different parts of the key.</p>
<h4 id="h3-278748c14-0007">Use Nontrivial Constants</h4>
<p class="BodyFirst">A Boolean in software is stored in 32 or 64 bits on a modern CPU. You can make use of all those other bits to build in fault mitigation and detection. In Chapter 7, you saw in the demonstration of the Trezor One glitch that a simple comparison could be skipped. Likewise, imagine you are using the following code to verify a signature operation:</p>
<pre><code>if verify_signature(new_code_array):
    erase_and_flash(new_code_array)</code></pre>
<p>The only return value of <code>verify_signature()</code> that <em>won’t</em> result in the code in question being flashed in is <code>0</code>. Every other possible return value will evaluate to “true” by the code! This is an example of using trivial constants that result in a particularly easily fault-injectable code.</p>
<p>A typical fault model is that an attacker can zero out or “<code>0xffffffff</code> out” a word. In this model, it’s unlikely the attacker can set a specific 32-bit value. So, instead of using zero and one for a Boolean, we can use <em>nontrivial constants</em> with a large Hamming distance (for instance <code>0xA5C3B4D2</code> and <code>0x5A3C4B2D</code>). These require a large number of bitflips (through a fault) to get from one to the other. Simultaneously, we could define <code>0x0</code> and <code>0xffffffff</code> to be invalid values to catch faults.</p>
<p>This idea can be extended to states in an enum, and similarly can be done in hardware state machines. Note that the application of this construct for states in an enum is typically trivial, but for Booleans, it can be infeasible to implement consistently, specifically when standard functions are used.</p>
<p>In the example <code>memcmp_nontrivial()</code> in the notebook, we extend our memory compare function with nontrivial values for important state. This version is also shown in <a href="#listing14-4">Listing 14-4</a>, which includes decoys, starting at a random index and constant time.</p>
<pre><code>def memcmp_nontrivial(c1, c2, num):
    # Prep decoy values, initialize to 0
    decoy1 = bytes(len(c1))
    decoy2 = bytes(len(c2))

    # Init diff accumulator and random starting point
    diff = 0
    rnd = random.randint(0, num-1)

    i = 0 
<span epub:type="pagebreak" title="412" id="Page_412"/>    while i &lt; num:
        # Get index, wrap around if needed
        idx = (i + rnd) % num

        # Flip coin to check we have a decoy round
        do_decoy = random.random() &lt; DECOY_PROBABILITY
        if do_decoy:
            decoy = (CONST1 | decoy1[idx]) ^ (CONST2 | decoy2[idx]) # Do similar operation
            tmpdiff = CONST1 | CONST2 # Set tmpdiff so we still have nontrivial consts
        else: 
            tmpdiff = (CONST1 | c1[idx]) ^ (CONST2 | c2[idx]) # Real operation, put in tmpdiff
            decoy = CONST1 | CONST2 # Just to mimic other branch 
        
        # Accumulate diff 
        diff = diff | tmpdiff 

        # Adjust index if not a decoy 
        i = i + int(not do_decoy) 

return diff</code></pre>
<p class="CodeListingCaption"><a id="listing14-4">Listing 14-4</a>: A complicated <code>memcmp</code> function with decoy functions and nontrivial constants</p>
<p>The trick is to encode the values for <code>diff</code> and <code>tmpdiff</code> such that they are never just all 1 or all 0. For that, we use two special values: <code>CONST_1==</code> <code>0xC0A0B000</code> and <code>CONST_2==0x03050400</code>. They’ve been designed to have the lower byte set to 0. This lower byte will be used to store the XOR of 2 bytes in memory, and we accumulate this in the <code>diff</code> variable. In addition, we’ll use the upper 24 bits of <code>diff</code> as a nontrivial constant. As you can see in the code, we also accumulate the values of <code>CONST_1</code> and <code>CONST_2</code> into <code>diff</code>. The way this is done is such that under normal circumstances, the top 24 bits of <code>diff</code> will have a fixed, known value—namely, the same as the top 24 bits of <code>CONST_1</code> | <code>CONST_2</code>. If there is a data fault that flips a bit in the top 24 bits of <code>tmpdiff</code>, it can be detected; you’ll see what to do later in the “Detect and Respond to Faults” section.</p>
<p>The examples of the different memory compare functions show how hard it is to write something that mitigates faults. When you’re using optimizing (JIT) compilers, it’s even harder to write the code such that the countermeasures don’t get compiled out. The obvious answer is to do this in assembly (with the downside of having to code in assembly) or to make a compiler that injects these kinds of countermeasures. There have been some academic publications on the topic, but the problem seems to be acceptance—either for performance reasons or for concerns around potentially introducing issues into otherwise well-tested compiler behavior.</p>
<p>In hardware, <em>error correcting codes (ECCs)</em> can be considered “nontrivial constants” used to mitigate faults. They typically have limited error correction and detection capabilities, and for an attacker who can flip many bits (for example, an entire word), this may reduce fault effectiveness less than an order of magnitude. Care should also be taken that, for example, an all-zero word (including ECC bits) is not a correct encoding.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<span epub:type="pagebreak" title="413" id="Page_413"/><h2><span class="NoteHead">WARNING</span></h2>
<p>	Watch out for acronym reuse, as ECC is used both for <em>error correcting code</em> and for <em>Elliptic Curve Cryptography</em>.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<h4 id="h3-278748c14-0008">Status Variable Reuse</h4>
<p class="BodyFirst">Using nontrivial constants is great, but consider the code flow of <code>check_fw()</code> in the companion notebook, also shown in <a href="#listing14-5" id="listinganchor14-5">Listing 14-5</a>. It sets <code>rv = validate_address(a)</code>, which returns a nontrivial constant. If the constant is <code>SECURE_OK</code>, it does <code>rv = validate_signature(a)</code>.</p>
<pre><code>SECURE_OK = 0xc001bead
def check_fw(a, s, fault_skip):
  <span class="CodeAnnotationCode" aria-label="annotation1">1</span> rv = validate_address(a) 
    if rv == SECURE_OK: 
      <span class="CodeAnnotationCode" aria-label="annotation2">2</span> rv = validate_signature(s) 

        if rv == SECURE_OK: 
            print("Firmware ok. Flashing!")</code></pre>
<p class="CodeListingCaption"><a id="listing14-5">Listing 14-5</a>: Using nontrivial constants isn’t an immediate fix for everything.</p>
<p>An attacker can do something easily here; they could use FI to skip the call at <span class="CodeAnnotation" aria-label="annotation2">2</span> to <code>validate_signature()</code>. The variable <code>rv</code> already has the <code>SECURE_OK</code> value present from the previous call to <code>validate_address()</code> at <span class="CodeAnnotation" aria-label="annotation1">1</span>. Instead, we should be clearing the value after usage. In languages that support macros, we can do this relatively easily with a macro that wraps some of these calls. Alternatively, we can use a different variable (for example, by introducing an <code>rv2</code> for the second call) or verify control flow (see the next section). Note that all these methods are prone to compiler optimization (see the section “Fighting Compilers” later in the chapter).</p>
<h4 id="h3-278748c14-0009">Verify Control Flow</h4>
<p class="BodyFirst">Fault injection can alter control flow, so any critical control flow should be <em>verified</em> to decrease the probability of a successful fault. A simple example is a “default fail” statement in a <code>switch</code> statement in C; the case statements should enumerate all valid cases, and the default case should therefore never be reached. If the default case is reached, we know a fault has occurred. Similarly, you can do this for <code>if</code> statements where the final <code>else</code> is a failure mode. You can see an example of this in <code>default_fail()</code> in the notebook.</p>
<p>When implementing any <em>conditional branch</em> (including one using the fancy “nontrivial constants”), also be aware of how the compiler’s implementation of your conditional may drastically affect the ability of an attacker to bypass a given code check. The high-level <code>if</code> statement will likely be implemented as a “branch if equal” or “branch of not equal” type of instruction. Like in Chapter 4, we’re going to go back to assembly code to see how this is implemented. The assembly code resulting from a typical <code>if</code>…<code>else</code> statement is given in <a href="#listing14-6" id="listinganchor14-6">Listing 14-6</a>.</p>
<pre><code><span epub:type="pagebreak" title="414" id="Page_414"/>      <span class="CodeAnnotationCode" aria-label="annotation1">1</span> bl      signature_ok(IMG_PTR)
        mov     r3, r0
        cmp     r3, #0
        movne   r3, #1
        moveq   r3, #0
        and     r3, r3, #255
        cmp     r3, #0
      <span class="CodeAnnotationCode" aria-label="annotation2">2</span> beq     .L2
        ldr     r0, [fp, #-8]
      <span class="CodeAnnotationCode" aria-label="annotation3">3</span> bl      boot_image(IMG_PTR)
        b       .L3
.L2:
      <span class="CodeAnnotationCode" aria-label="annotation4">4</span> bl      panic()
.L3:
        nop</code></pre>
<p class="CodeListingCaption"><a id="listing14-6">Listing 14-6</a>: Arm assembly code showing an <code>if</code> statement as implemented by the compiler</p>
<p>This <code>if</code> statement was designed to check whether or not an image (pointed to with <code>IMG_PTR</code>) should be booted. The function <code>signature_ok()</code> is called at <span class="CodeAnnotation" aria-label="annotation1">1</span>, which has some special return value in <code>r0</code> to indicate if the signature should allow the image to boot. This comparison ultimately boils down to a branch if equal (<code>beq</code>) at <span class="CodeAnnotation" aria-label="annotation2">2</span>, where if the branch to <code>.L2</code> is taken, the <code>panic()</code> function is called at <span class="CodeAnnotation" aria-label="annotation4">4</span>. The problem is if an attacker skips the <code>beq</code> at <span class="CodeAnnotation" aria-label="annotation2">2</span>, it will fall through to the <code>boot_image()</code> function at <span class="CodeAnnotation" aria-label="annotation3">3</span>. Switching the order of the comparison such that skipping the <code>beq</code> at <span class="CodeAnnotation" aria-label="annotation2">2</span> would fall through to the <code>panic()</code> function would be good practice in this example. You may need to work with your compiler to get this effect (check <code>__builtin_expect</code> in gcc and clang compilers), and it’s a good reminder why investigating the actual assembly output is important. See the section “Simulation and Emulation” later in the chapter for links to tools to help you automated these tests.</p>
<p>Double- or multi-checking sensitive decisions is also a means to verify control flow. Specifically, you implement multiple <code>if</code> statements that are logically equivalent but contain different operations. In the <code>double_check()</code> example in the notebook, the memory compare is executed twice and checked twice with slightly different logic. If the results of the second comparison disagree with the first, we’ve detected a fault.</p>
<p>The <code>double_check()</code> example is already hardened against single faults, but multiple faults timed at exactly the number of cycles between the <code>memcmp()</code> invocations can skip both checks. Therefore, it’s best to add some <em>random wait state</em> in between and ideally perform some <em>non-sensitive operations</em>, as shown in the <code>double_check_wait()</code> example in the notebook and also shown in <a href="#listing14-7">Listing 14-7</a>. The non-sensitive operations help because, first, a long glitch may corrupt consecutive conditional branches, and, second, the side-channel signal of the random wait gives away information to the attacker about when sensitive operations are happening. Compared to the previous examples, faults that were 100 percent successful before are now less likely.</p>
<pre><code><span epub:type="pagebreak" title="415" id="Page_415"/>def double_check_wait(input, secret):
    # Check result
    result = memcmp(input, secret, len(input))

    if result == 0:
        # Random wait
        wait = random.randint(0,3)
        for i in range(wait):
            None 
    
        # This is also a good point to insert some not-so-sensitive other operations
        # Just to decouple the random wait loop from the sensitive operation

        # Do memcmp again
        result2 = memcmp(input, secret, len(input))

        # Double check with some different logic
        if not result2 ^ 0xff != 0xff:
            print("Access granted, my liege")
        else: 
            print("Fault2 detected!") <span class="CodeAnnotationCode" aria-label="annotation1">1</span></code></pre>
<p class="CodeListingCaption"><a id="listing14-7">Listing 14-7</a>: Double-checking <code>memcmp</code> operations with random delays</p>
<p>Another simple control flow check is to see whether a sensitive loop operation terminates with the correct loop count. The <code>check_loop_end()</code> example in the companion notebook illustrates this; after the loop ends, the iterator value is checked against a “known-good” value.</p>
<p>A more convoluted but broader countermeasure is that of <em>control flow integrity</em>. There are many ways of implementing this, but we give one example with a <em>cyclic redundancy check (CRC)</em>. CRCs are very fast. The idea is to represent a sequence of operations as a sequence of bytes, over which we calculate the CRC. At the end, we check whether the CRC matches what we expect, which should always be the case, unless a fault changed the sequence of operations. You’ll have to add some code to aid in your control flow integrity work.</p>
<p>The companion notebook shows this in <code>crc_check()</code>, where several function calls update a running CRC. First, we enable a <code>DEBUG</code> mode, which causes the final CRC to be printed. Next, this CRC is embedded in the code as a check, and debug mode is turned off. Now, control flow checking is active. If a function call is skipped, the final CRC value will differ. You can verify that it works by setting the <code>FAULT</code> variable to 0 and 1.</p>
<p>You can perform this type of simple control flow checking wherever there are no conditional branches. If you have a few conditional branches, you can still hard-code a few valid CRC values for each of the paths through the program. Alternatively, you also can have local control flow that operates only within one function.</p>
<p>CRCs are, of course, not cryptographically secure. Cryptographic security isn’t very important here, because all we need is a verification code that <span epub:type="pagebreak" title="416" id="Page_416"/>is hard to forge. In this case, forging would mean fault injections to set the CRC to a specific value, which we assume is outside of the capabilities of an attacker.</p>
<h4 id="h3-278748c14-0010">Detect and Respond to Faults</h4>
<p class="BodyFirst">By using nontrivial constants, double-checks, or decoy operations, we can start building <em>fault detection</em>. If we encounter an invalid state, we know it’s caused by a fault. This means in <code>if</code> statements, we check <code>condition==TRUE</code>, then <code>condition==FALSE</code>, and if we reach the final <code>else</code>, we know a fault has occurred. Similarly for “switch” statements, the “default” case should always be a fault option. See <code>memcmp_fault_detect()</code> in the notebook for an example of using nontrivial constants to detect faults; it simply checks whether the bits in the nontrivial bits in <code>diff</code> and <code>tmpdiff</code> are correctly set and returns <code>None</code> otherwise. Another example is <span class="CodeAnnotation" aria-label="annotation1">1</span> in <a href="#listing14-7">Listing 14-7</a>, where the first check succeeded but the second one failed.</p>
<p>Similar to decoy operations, we can use any parallel process in software or hardware to build generic <em>fault canaries</em>. Under normal circumstances, they should have some fixed, verifiable output, but under attack, their output changes.</p>
<p>In hardware, we can build similar constructs. Additionally, hardware can include specific <em>fault sensors</em> that detect anomalies in the supply voltage or external clock, or even on-die <em>optical sensors</em>. These can be effective against specific fault types, but a different type of attack can bypass them. For instance, an optical sensor will detect a laser pulse, but will not detect a voltage perturbation.</p>
<p>A <em>fault response</em> is what to do when a fault is detected. The goal here is to reduce the chances of a successful attack to the point where an attacker will give up. On the one end of the spectrum, you can implement a program exit, OS reboot, or chip reset. These actions will delay an attacker but in principle still allow them infinite tries. Somewhere in the middle of the spectrum is signaling a backend system to flag this device as suspicious and perhaps disable the account. On the other end of the spectrum, you can implement permanent measures like wiping keys, accounts, or even burning fuses that disallow the chip from booting up.</p>
<p>How to respond to faults can be difficult to decide, as it depends strongly on how tolerant you are to false positives, whether the system is safety critical, and how bad the impact of a compromise really is. In a credit card application, it’s perfectly acceptable to wipe keys and disable all functionality when under attack. At the same time, it’s not acceptable if this happens at a large scale due to false positives. Some balance needs to be struck on how many false positives (and faults!) can be had within a certain time frame or lifetime.</p>
<p>To balance false positives and actual faults, a <em>fault counter</em> can be used. Initial counter increments are considered false positives, until the counter increments to a certain <em>counter threshold</em>. At the threshold, we conclude we are under (fault) attack. This counter must be nonvolatile, as you don’t want <span epub:type="pagebreak" title="417" id="Page_417"/>a power-down to reset the counter. An attacker would easily abuse this by just resetting between each fault attempt.</p>
<p>Even a nonvolatile counter must be implemented with care. We’ve done attacks where we detect the detection mechanism through a side-channel measurement and then power off the target before the counter can be updated in nonvolatile storage. That attack can be thwarted by incrementing the counter <em>before</em> a sensitive operation, storing it, performing the sensitive operation, and, only if no faults are detected, decrementing the counter again. A power-off will now simply mean the counter was increased.</p>
<p>The counter threshold depends on your application’s exposure and tolerance for false positives; in automotive and aerospace/space applications, faults caused by nature are much more common because of the exposure to radiation and strong electromagnetic fields. The tolerance depends on the application. In the credit card case, wiping keys and effectively disabling functionality is acceptable. However, that wouldn’t be acceptable behavior for devices that have a safety function, such as medical or automotive devices. It may even not be acceptable from a field failure rate perspective for other applications. In that case, a response could be to inform a backend covertly that the device may be under attack. At this point, what to do is a product design decision, but it often involves trading off security for safety, cost, performance, and so on.</p>
<h3 id="h2-278748c14-0002">Verifying Countermeasures</h3>
<p class="BodyFirst">The countermeasures in this section will potentially make attacks harder. That’s an intentionally weak statement. Unfortunately, we’re not in a clean cryptographic world where elegant proofs exist that can reduce to existing and well-studied hard mathematical problems. We don’t even have the same kind of heuristic security as in cryptography, as countermeasure effectiveness varies from chip type to chip type, and sometimes from individual chip to chip. At best, literature analyzes countermeasures in a noiseless setting and validates them on (often) simple microcontrollers or FPGAs that behave relatively “cleanly.” That’s why—until we get better theoretical means to predict countermeasure effectiveness—testing effectiveness on real systems is critical.</p>
<h4 id="h3-278748c14-0011">Strength and Bypassability</h4>
<p class="BodyFirst">Two main angles need to be analyzed when verifying a countermeasure: strength and bypassability. In real-world analogies, <em>strength</em> is about how hard it is to pry open a door lock, and <em>bypassability</em> is about whether you can avoid the lock by entering through the window.</p>
<p>Strength can be measured by turning the countermeasure on and off and then verifying the difference in attack resistance. For fault injection, you can represent this difference as the decrease in fault probability. For side-channel analysis, you can express this difference as the increase in the number of traces until key exposure.</p>
<p>See the notebook for an example of testing the strength of the nontrivial constants countermeasure of the <code>memcmp_fault_detect()</code> function. This function <span epub:type="pagebreak" title="418" id="Page_418"/>uses the top 24 nontrivial constant bits (see also <a href="#listing14-4">Listing 14-4</a>) as a fault detection mechanism. We simulate single-byte faults in the <code>diff</code> and <code>tmpdiff</code> values. We can observe that in roughly 81.2 percent of the cases, the fault is successfully detected, and in about 18.8 percent of the cases, there is no fault, or it has no observable effect. However, our countermeasure is not perfect: in about 0.0065 percent of the cases, the fault manages to flip the bits of <code>diff</code> or <code>tmpdiff</code> such that <code>memcmp_fault_detect()</code> concludes that the inputs are equal. Though that sounds like a low success rate, if this were a password check, we’d expect a successful login after 15,385 fault injections (1/0.000065). If you can do one fault per second, you’d be in within five hours.</p>
<p>The second (and more tricky) angle is bypassability: what is the effort in going around the countermeasure? To determine that, consider building an attack tree (see Chapter 1), which allows you to enumerate other attacks. You may mitigate voltage glitches, but an attacker can still do electromagnetic fault injection.</p>
<h4 id="h3-278748c14-0012">Fighting Compilers</h4>
<p class="BodyFirst">Once you verify your countermeasures a few times, you’ll find they sometimes are completely ineffective, which can be due to bad coverage (for example, you plugged one leak where there were many). What also can happen is that your toolchain optimizes out your countermeasures because they don’t have any side effects. For instance, double-checking a value is the logical equivalent of checking a value once, so an optimizing compiler cleverly removes your double-check. Similar situations can happen during synthesizing hardware, where duplicated logic may be optimized out.</p>
<p>If you use the <code>volatile</code> keyword on variables in C or C++, this can help avoid optimizing away countermeasures. With <code>volatile</code>, the compiler may not assume that two reads of the same variable yield the same value. Therefore, if you check a variable twice in a double-check, it will not be compiled out. Note that this generates more memory accesses, so if a chip is particularly sensitive to memory access glitches, it’s a double-edged sword. You can also use <code>__attribute__((optnone))</code> to turn off optimizations for particular functions.</p>
<p>The code in <a href="#listing14-6">Listing 14-6</a> is another example where compiler optimizations will result in changes in your fault countermeasure. The compiler may choose to reorder the assembly code generated, which will lead to a fall-through condition if an attacker skips the single branch instruction.</p>
<p>There is some research on making compilers output code that is more resistant to faults, which is an obvious solution direction; see Hillebold Christoph’s thesis “Compiler-Assisted Integrity Against Fault Injection Attacks.” Blanket application of such techniques are not be desirable for performance reasons.</p>
<h4 id="h3-278748c14-0013">Simulation and Emulation</h4>
<p class="BodyFirst">Use of simulators is also important during verification. With hardware design, the cycle from initial design to first silicon may take years. Ideally, <span epub:type="pagebreak" title="419" id="Page_419"/>we want to be able to “measure” leakage well before silicon, when there is still time to fix things. See “Design Time Engineering of Side Channel Resistant Cipher Implementations” by Alessandro Barenghi et al.</p>
<p>Similar research is ongoing on fault injection: by simulating various instruction corruptions, we can test whether single fault injection points exist. For more information, see “Secure Boot Under Attack: Simulation to Enhance Fault Injection and Defenses” by Martijn Bogaard and Niek Timmers. Riscure has an open source CPU emulator that implements instruction skipping and corruption at <a href="https://github.com/Riscure/FiSim/" class="LinkURL">https://github.com/Riscure/FiSim/</a> that you can try to test your software countermeasures in. We recommend you try out this emulator—you can quickly learn which countermeasures work well and which won’t. More importantly, you’ll learn which countermeasure combinations are required to get down to a low fault count. Getting it down to zero faults is not easy!</p>
<h4 id="h3-278748c14-0014">Verification and Enlightenment</h4>
<p class="BodyFirst">Countermeasure strength is something you can measure yourself; for countermeasure bypassability, it’s best to engage someone who wasn’t involved in the design. Countermeasures can be regarded as a security system, and as Schneier’s law states, “Any person can invent a security system so clever that he or she can’t imagine a way of breaking it.” </p>
<p>On this topic, allow us a small excursion into what we’ll call the <em>four stages of security enlightenment</em>. It is our completely unscientific observation and subjective experience of how people generally respond to the notion of hardware attacks and how to solve them.</p>
<p>The <em>first stage</em> is basic denial of the possibility or practicality of side-channel or fault attacks. The issue here is that basic software engineering assumptions—assumptions you’ve experienced and heard about all the time—can be broken: the hardware actually isn’t executing instructions that it’s fed, and it’s telling the world all about the data it’s processing. It’s like finding out the world isn’t flat.</p>
<p>Once the first stage is passed, the <em>second stage</em> is that countermeasures are easy or unbreakable. It’s the natural response to not yet grasp the full depth of the security issues, the cost of the countermeasures, or that attackers are adaptive creatures. It usually takes some countermeasures being broken (or some “yeah, but if you do that then…” conversations with a security expert) before moving on to the next stage, which is <em>security nihilism</em>.</p>
<p>Security nihilism is the idea that everything is broken, so there’s nothing we can do anyway to prevent attacks. It’s true that everything can be broken, given a motivated and well-resourced attacker—and that is the crux. There are a limited number of attackers, and they have varying motivation and resources. As it stands, it’s still much easier to clone a magstripe credit card than to perform a side-channel attack on a credit card. As James Mickens said, “If your threat model includes the Mossad, you’re still gonna be Mossad’ed upon.” But, if you’re not a target for the Mossad, you probably will not be Mossad’ed upon. They also need to prioritize.</p>
<p><span epub:type="pagebreak" title="420" id="Page_420"/>The fourth and final stage is <em>enlightenment</em>: understanding that security is about risk; risk will never be zero, but risk isn’t about the worst case happening all the time. In other words, it’s about making an attack as uninteresting for an attacker as feasible. Ideally, countermeasures raise the bar to the point where the cost of an attack isn’t worth the payoff. Or often more realistically, countermeasures make another product more interesting to attack than yours. Enlightenment is about realizing the limitations of countermeasures, and making risk-based tradeoffs as to which countermeasures to include. It’s also about being able to sleep again at night.</p>
<h2 id="h1-278748c14-0002">Industry Certifications</h2>
<p class="BodyFirst">Certification for side-channel analysis and fault injection resistance has been available through various organizations, which we’ll list in this section. We know from Chapter 1 that security isn’t binary, so what do industry certifications mean if an unbreakable product doesn’t exist?</p>
<p>The goal of these certifications is for vendors to demonstrate to third parties that they have some level of <em>assurance</em> of some level of <em>attack resistance</em>. It also means that only for a limited time; a certificate that’s a few years old obviously does not include attacks most recently discovered.</p>
<p>Let’s briefly consider attack resistance first. A product passes <em>Common Criteria PP-0084 (CC)/EMVCo</em> certification if it demonstrably has all the security functionality required, and the certifying lab cannot show an attack path exists that has fewer than 31 points in the <em>JIL score</em> (see “Scoring Hardware Attack Paths” in <span class="xref" itemid="xref_target_Chapter 1">Chapter 1</span>). An attack path is only an attack path if it ends with the compromise of a well-defined asset, such as a key. That means both positive and negative testing is used, establishing “does it do what it should do” as well as “does it not do what it shouldn’t do.” The latter is very important when the adversary is intelligent and adaptive.</p>
<p>Effectively, the JIL scoring limits the time, equipment, knowledge, personnel, and number of (open) samples that can be used for the attack. Whatever attacks a lab knows about or can develop are relevant for CC/EMVCo, as long as the scoring is within 31 points. See the latest version of the JIL document titled “Application of Attack Potential to Smartcards and Similar Devices” (which is available publicly online) for a good reference on how this scoring is done. A certificate tells you that the lab was unable to identify any attack that scored less than 31 points. Labs won’t even test whether attacks of 31 points and higher work. Going back to our earlier point about unbreakable products, the point system means you may still be able to find attacks at the high ratings. A great example is “Deconstructing a ‘Secure’ Processor,” by Christopher Tarnovsky, presented at Black Hat 2010, where he impressively goes beyond the effort a lab would put into a certification.</p>
<p>Now, let’s consider levels of assurance, which is the aspect of “how <em>sure</em> are we it resists the relevant attacks.” On the one hand, you can read the product datasheet and see “side channel countermeasures,” and you can conclude that’s true based on the sheet, for a <em>low</em> level of assurance. Or, <span epub:type="pagebreak" title="421" id="Page_421"/>you can spend a year testing everything and mathematically proving lower bounds on the amount of leakage on your special protocol, and then you have a <em>high</em> level of assurance.</p>
<p>For CC, the level of assurance is defined as the <em>evaluation assurance level (EAL)</em> ; for smart cards, you’ll often see EAL5, EAL5+, EAL6, or EAL6+. We won’t go in details here, but just make sure you outsmart your friends by knowing EAL doesn’t mean “how secure it is.” Instead, it means “how sure am I of the security?” (And if you want to be supersmart, know that + means a few extra assurance requirements.)</p>
<p>Speaking of <em>labs</em>, the labs must prove they are capable of state-of-the-art attacks, which is verified by the standards bodies. Additionally, for CC, labs must participate and share new attacks in the <em>Joint Hardware Attack Subgroup (JHAS)</em>. The JHAS maintains the JIL document referred to earlier and updates it with new attacks and scores. This way, the standard does not have to prescribe what attacks must be performed, which is good, because hardware security is a constantly moving field. Because the attacks are in the JIL, it’s mainly up to labs to pick the relevant attacks for a product. This comes at the “cost” of variability in the labs’ approach. The issue with the latter is that vendors can pick labs with a track record of finding fewer issues, so labs essentially have competitive pressure to find less. It’s up to the standards body to make sure labs still meet the bar.</p>
<p>A similar approach to CC was adopted by <em>GlobalPlatform</em> for its <em>Trusted Execution Environment (TEE) </em>certification. The number of points needed is 21, lower than that of smart cards, which means that most hardware attacks are considered relevant only if they are trivially scalable, such as through software means. For example, if we use a fault injection or side-channel attack to dump a master key that allows us to hack any similar device, it is considered a relevant attack. If we have to do a side-channel attack for every device we want to break, and it takes a month for each device to get the key out, it is considered out of certification scope, simply because the attack rating will be more than 21.</p>
<p>Arm has a certification program called <em>Platform Security Architecture (PSA)</em>. The PSA has several levels of certification. Level 3 includes physical attacks such as side-channel and fault injection resistance. PSA in general is designed to target IoT and embedded platforms. As such, it may be more suited to general-purpose platforms, but if you are building products with general-purpose microcontrollers, the PSA level is the most likely one you will see such devices certified to. At lower levels, PSA also helps fix some of the basic problems we still see today, such as a debug interface that’s left open.</p>
<p>Another approach is that of <em>ISO 19790</em>, which is aligned with the US/Canadian standard <em>FIPS 140-3</em> that focuses on cryptographic algorithms and modules. The <em>Cryptographic Module Verification Program (CMVP)</em> validates whether modules satisfy the FIPS 140-3 requirements. The approach here is heavily biased toward <em>verification</em>—that is, making sure the product conforms to the security functional requirements. In our earlier words, it’s biased toward testing strength rather than bypassability. The standard prescribes the types of tests that are to be performed on products, which aides reproducibility among labs. The issue is that attacks evolve quickly, and <span epub:type="pagebreak" title="422" id="Page_422"/>“standard sets of tests defined by a government body” do not. FIPS 140-2 (the predecessor of FIPS 140-3) was published in 2001 and didn’t include a way to validate side-channel attacks. In other words, a product can be FIPS 140-2 certified, meaning that the AES engine performs proper AES encryptions, the keys are accessible only by authorized parties, and so on, but also that the keys could leak in 100 side-channel traces, because SCA is not in the testing scope for FIPS 140-2. It took 18 years for its successor FIPS 140-3 to become effective, which does include side-channel testing in the form of the <em>test vector leakage assessment (TVLA)</em>. With TVLA the testing is precisely specified, but too much cleverness in filtering, and so on, on the side of the attacker is excluded. This means “passing” the testing doesn’t mean there is no side-channel leakage, only that the most straightforward of leakage was not detected.</p>
<p>Yet another approach to side-channel leakage certification is explored in <em>ISO 17825</em>, which again takes some of the TVLA testing we described in Chapter 11 and standardizes it. The eventual goal may be to achieve a “datasheet figure” for leakage. Like ISO 19790, the ISO 17825 testing isn’t designed to perform the same work as Common Criteria. With Common Criteria, the question is more broadly looking at attack resistance, while ISO 17825 attempts to provide a method of comparing specific side-channel leakage with automated methods. This means that ISO 17825 isn’t supposed to provide a general security metric across a range of attacks, but it’s useful when you are trying to understand the impact of enabling certain side-channel countermeasures. In other words, it measures countermeasure strength, not bypassability.</p>
<p>ISO/SAE 21434 is an automotive cybersecurity standard that is mandated in the EU per July 2022 for new vehicle types. It specifies <em>security engineering</em> requirements, and requires hardware attacks to be considered. This brings all of the attacks we learned about in this book into scope for the automotive space! When certifications hit marketing departments, you’ll find that “it’s secure!” is conflated with “it’s certified up to a certain assurance level against this limited set of threats.” This is understandable because the latter is a mouthful. However, it means it’s up to you to understand what the certification on a product actually means and how that fits your threat model. For example, if you’re trying to validate that a given system is generally resistant to various advanced attacks, someone offering ISO 17825 testing won’t have anywhere near the scope you require. But if you go only by the standard title (“Testing methods for the mitigation of noninvasive attack classes against cryptographic modules”) and a bit of marketing material the test provider gives you, you may easily be seduced into believing the value. Of course, there is a significant cost and effort difference as well between different certifications.</p>
<p>Certification has helped (at least) the smart card industry reach high levels of side-channel attack and fault injection resistance. No one will have an easy time breaking a modern, certified card. At the same time, it’s imperative to look at what’s behind a certification, as there are always limits to what the certification means.</p>
<h2 id="h1-278748c14-0003"><span epub:type="pagebreak" title="423" id="Page_423"/>Getting Better</h2>
<p class="BodyFirst">A number of different training courses are available on learning side-channel analysis and fault injection. When selecting a course, we recommend investigating the syllabus up front. This book covers the basics and theory, and if you sufficiently grasp them, it’d be better to select a course that focuses on the practical matters. The entire area of hardware hacking has people coming from all sorts of backgrounds. Some will be coming at it having done ten years of low-level chip design but never having dealt with finite field arithmetic. Others may have a PhD in theoretical mathematics but have never touched an oscilloscope before. So when you approach a topic, be sure to figure out the most valuable background for you. Whether you want more information on cryptography, signal processing, or the math behind DPA, find a course that focuses on those topics. Similarly, some training courses are more offense than defense focused, so find the one that matches your needs best. (Full disclosure: both authors’ companies run training courses.)</p>
<p>You can also visit talks at conferences and learn from and discuss with people already in the field. You’ll find them at academic conferences, such as CHES, FDTC, COSADE, but also more (hardware) hacker-oriented conferences like Black Hat, Hardwear.io, DEF CON, CCC, and REcon. Definitely consider this an invite to say “Hi!” when you run into us at one of these events.</p>
<p>Attending a training course and attending events are also a great ways to learn new things outside of your background experience while sharing your unique background with others. You might have spent years working on the design of analog ICs, and we bet you will have some insight about how voltage spikes might be propagating inside a die that someone who has only worked with FPGAs won’t have.</p>
<h2 id="h1-278748c14-0004">Summary</h2>
<p class="BodyFirst">In this chapter, we described a number of countermeasure strategies. Each countermeasure can be a building blocks of a “secure enough” system, and none of them will be individually sufficient. There’s also a number of caveats in building countermeasures, so make sure to verify they work as intended at each stage during development. We touched upon the professional side of verification through various certification strategies. </p>
<p>Finally, we talked a bit about how to keep improving in this field. The best teacher is still practice. Start with simple microcontrollers. For example, try something clocked under 100 MHz that you fully control, so no OS throws interrupts and multitasking at you. Next, start building countermeasures and see how they hold up to your attacks, or better yet, get a friend to build their own countermeasure, and try to break each other’s. You’ll find that testing strength is easier than bypassability. Once you’re pretty comfortable attacking and defending, start complicating things: faster clocks, more complex CPUs, less control over the target application, less knowledge of the target application, and so on. Realize you are still learning; a new target may make you feel like a beginner again. Keep going at it; ultimately patience leads to luck, and luck leads to skill. Good luck on your journey! </p>
</section>
</body></html>