<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><h2 class="h2" id="index"><span epub:type="pagebreak" id="page_223"/><strong>INDEX</strong></h2>&#13;
<h3 class="h3i"><strong>A</strong></h3>&#13;
<p class="indexmain">active learning, <a href="ch30.xhtml#page_195">195</a>, <a href="ch30.xhtml#page_203">203</a>–<a href="ch30.xhtml#page_204">204</a>, <a href="appendix.xhtml#page_222">222</a></p>&#13;
<p class="indexmain">Adam optimizer, <a href="ch07.xhtml#page_42">42</a>, <a href="ch11.xhtml#page_73">73</a>, <a href="appendix.xhtml#page_211">211</a>, <a href="appendix.xhtml#page_212">212</a>–<a href="appendix.xhtml#page_213">213</a></p>&#13;
<p class="indexmain">adapter methods, <a href="ch18.xhtml#page_119">119</a>, <a href="ch18.xhtml#page_121">121</a>–<a href="ch18.xhtml#page_123">123</a>, <a href="ch18.xhtml#page_125">125</a>–<a href="ch18.xhtml#page_126">126</a>, <a href="appendix.xhtml#page_216">216</a></p>&#13;
<p class="indexmain">Add &amp; Norm step, <a href="ch17.xhtml#page_107">107</a></p>&#13;
<p class="indexmain">adversarial examples, <a href="ch05.xhtml#page_27">27</a></p>&#13;
<p class="indexmain">adversarial validation, <a href="ch23.xhtml#page_154">154</a>, <a href="ch29.xhtml#page_190">190</a>–<a href="ch29.xhtml#page_191">191</a>, <a href="appendix.xhtml#page_218">218</a>, <a href="appendix.xhtml#page_221">221</a></p>&#13;
<p class="indexmain">AI (artificial intelligence)</p>&#13;
<p class="indexsub">data-centric, <a href="ch21.xhtml#page_143">143</a>–<a href="ch21.xhtml#page_146">146</a>, <a href="appendix.xhtml#page_217">217</a></p>&#13;
<p class="indexsub">model-centric, <a href="ch21.xhtml#page_143">143</a>–<a href="ch21.xhtml#page_144">144</a>, <a href="ch21.xhtml#page_145">145</a></p>&#13;
<p class="indexmain">AlexNet, <a href="ch01.xhtml#page_6">6</a>, <a href="ch01.xhtml#page_7">7</a></p>&#13;
<p class="indexmain">asymptotic coverage guarantees, confidence intervals, <a href="ch26.xhtml#page_177">177</a></p>&#13;
<p class="indexmain">attention mechanism. <em>See also</em> self-attention mechanism</p>&#13;
<p class="indexsub">Bahdanau, <a href="ch16.xhtml#page_99">99</a>–<a href="ch16.xhtml#page_101">101</a>, <a href="ch16.xhtml#page_103">103</a>, <a href="ch17.xhtml#page_112">112</a></p>&#13;
<p class="indexsub">transformers, <a href="ch07.xhtml#page_40">40</a>, <a href="ch08.xhtml#page_43">43</a>–<a href="ch08.xhtml#page_45">45</a>, <a href="ch08.xhtml#page_46">46</a>, <a href="ch08.xhtml#page_47">47</a></p>&#13;
<p class="indexmain">augmented data</p>&#13;
<p class="indexsub">reducing overfitting with, <a href="ch05.xhtml#page_24">24</a>–<a href="ch05.xhtml#page_25">25</a>, <a href="ch05.xhtml#page_26">26</a>, <a href="appendix.xhtml#page_210">210</a></p>&#13;
<p class="indent">for text, <a href="ch15.xhtml#page_93">93</a>–<a href="ch15.xhtml#page_97">97</a>, <a href="appendix.xhtml#page_214">214</a>–<a href="appendix.xhtml#page_215">215</a></p>&#13;
<p class="indexmain">autoencoders</p>&#13;
<p class="indexsub">defined, <a href="ch09.xhtml#page_51">51</a></p>&#13;
<p class="indexsub">latent space, <a href="ch01.xhtml#page_5">5</a>–<a href="ch01.xhtml#page_6">6</a></p>&#13;
<p class="indexsub">variational, <a href="ch09.xhtml#page_51">51</a>–<a href="ch09.xhtml#page_52">52</a></p>&#13;
<p class="indexmain">automatic prompt engineering method, <a href="ch18.xhtml#page_125">125</a></p>&#13;
<p class="indexmain">autoregressive decoding, <a href="ch17.xhtml#page_107">107</a>–<a href="ch17.xhtml#page_110">110</a></p>&#13;
<p class="indexmain">autoregressive models, <a href="ch09.xhtml#page_54">54</a>–<a href="ch09.xhtml#page_55">55</a>, <a href="ch09.xhtml#page_57">57</a>, <a href="ch10.xhtml#page_63">63</a>–<a href="ch10.xhtml#page_64">64</a></p>&#13;
<p class="indexmain">auxiliary tasks, <a href="ch30.xhtml#page_199">199</a></p>&#13;
<h3 class="h3i"><strong>B</strong></h3>&#13;
<p class="indexmain">backpropagation, <a href="ch18.xhtml#page_115">115</a>–<a href="ch18.xhtml#page_116">116</a></p>&#13;
<p class="indexmain">back translation, <a href="ch15.xhtml#page_96">96</a></p>&#13;
<p class="indexmain">bag-of-words model, <a href="appendix.xhtml#page_207">207</a></p>&#13;
<p class="indexsub">continuous bag-of-words (CBOW) approach, <a href="ch14.xhtml#page_90">90</a></p>&#13;
<p class="indexmain">Bahdanau attention mechanism, <a href="ch16.xhtml#page_99">99</a>–<a href="ch16.xhtml#page_101">101</a>, <a href="ch16.xhtml#page_103">103</a>, <a href="ch17.xhtml#page_112">112</a></p>&#13;
<p class="indexmain">BART encoder-decoder architecture, <a href="ch17.xhtml#page_112">112</a></p>&#13;
<p class="indexmain">base classes in few-shot learning, <a href="ch03.xhtml#page_16">16</a></p>&#13;
<p class="indexmain">Basic Linear Algebra Subprograms (BLAS), <a href="ch22.xhtml#page_148">148</a>, <a href="ch22.xhtml#page_152">152</a></p>&#13;
<p class="indexmain">batched inference, <a href="ch22.xhtml#page_147">147</a>–<a href="ch22.xhtml#page_148">148</a></p>&#13;
<p class="indexmain">batch normalization (BatchNorm), <a href="ch11.xhtml#page_73">73</a>, <a href="appendix.xhtml#page_213">213</a></p>&#13;
<p class="indexmain">Berry–Esseen theorem, <a href="ch25.xhtml#page_167">167</a>, <a href="ch25.xhtml#page_171">171</a></p>&#13;
<p class="indexmain">BERT model</p>&#13;
<p class="indexsub">adopting for classification tasks, <a href="ch17.xhtml#page_112">112</a>, <a href="appendix.xhtml#page_215">215</a>–<a href="appendix.xhtml#page_216">216</a></p>&#13;
<p class="indexsub">distributional hypothesis, <a href="ch14.xhtml#page_91">91</a>, <a href="ch14.xhtml#page_92">92</a></p>&#13;
<p class="indexsub">encoder-only architectures, <a href="ch17.xhtml#page_107">107</a>–<a href="ch17.xhtml#page_108">108</a></p>&#13;
<p class="indexmain">BERTScore, <a href="ch19.xhtml#page_132">132</a>–<a href="ch19.xhtml#page_133">133</a>, <a href="ch19.xhtml#page_134">134</a>, <a href="appendix.xhtml#page_216">216</a>–<a href="appendix.xhtml#page_217">217</a></p>&#13;
<p class="indexmain">bias units</p>&#13;
<p class="indexsub">in convolutional layers, <a href="ch11.xhtml#page_70">70</a>–<a href="ch11.xhtml#page_71">71</a></p>&#13;
<p class="indexsub">in fully connected layers, <a href="ch11.xhtml#page_72">72</a>, <a href="ch12.xhtml#page_76">76</a></p>&#13;
<p class="indexmain">binomial proportion confidence interval, <a href="ch25.xhtml#page_171">171</a></p>&#13;
<p class="indexmain">BLAS (Basic Linear Algebra Subprograms), <a href="ch22.xhtml#page_148">148</a>, <a href="ch22.xhtml#page_152">152</a></p>&#13;
<p class="indexmain">BLEU (bilingual evaluation understudy) score, <a href="ch19.xhtml#page_128">128</a>, <a href="ch19.xhtml#page_129">129</a>–<a href="ch19.xhtml#page_131">131</a>, <a href="ch19.xhtml#page_133">133</a>, <a href="ch19.xhtml#page_134">134</a></p>&#13;
<p class="indexmain">bootstrapping</p>&#13;
<p class="indexsub">improving performance with limited data, <a href="ch30.xhtml#page_194">194</a></p>&#13;
<p class="indexsub">out-of-bag, <a href="ch25.xhtml#page_167">167</a>–<a href="ch25.xhtml#page_169">169</a>, <a href="ch25.xhtml#page_170">170</a>, <a href="ch25.xhtml#page_171">171</a></p>&#13;
<p class="indexsub">test set predictions, <a href="ch25.xhtml#page_169">169</a>, <a href="ch25.xhtml#page_170">170</a>, <a href="ch25.xhtml#page_171">171</a>, <a href="appendix.xhtml#page_219">219</a></p>&#13;
<h3 class="h3i"><strong>C</strong></h3>&#13;
<p class="indexmain">calibration set, <a href="ch26.xhtml#page_176">176</a></p>&#13;
<p class="indexmain">CBOW (continuous bag-of-words) approach, <a href="ch14.xhtml#page_90">90</a></p>&#13;
<p class="indexmain">CE (cross-entropy) loss, <a href="ch19.xhtml#page_128">128</a>, <a href="ch27.xhtml#page_182">182</a></p>&#13;
<p class="indexmain">central limit theorem, <a href="ch25.xhtml#page_167">167</a>, <a href="ch25.xhtml#page_171">171</a></p>&#13;
<p class="indexmain"><span epub:type="pagebreak" id="page_224"/>ChatGPT model</p>&#13;
<p class="indexsub">autoregressive models, <a href="ch09.xhtml#page_54">54</a></p>&#13;
<p class="indexsub">randomness by design, <a href="ch10.xhtml#page_63">63</a></p>&#13;
<p class="indexsub">reinforcement learning with human feedback (RLHF), <a href="ch18.xhtml#page_124">124</a></p>&#13;
<p class="indexsub">stateless vs. stateful training, <a href="ch20.xhtml#page_141">141</a>, <a href="appendix.xhtml#page_217">217</a></p>&#13;
<p class="indexsub">zero-shot learning, <a href="ch30.xhtml#page_196">196</a></p>&#13;
<p class="indexmain">classic bias-variance theory, <a href="ch06.xhtml#page_31">31</a>, <a href="ch06.xhtml#page_35">35</a></p>&#13;
<p class="indexmain">classification head, <a href="appendix.xhtml#page_215">215</a>–<a href="appendix.xhtml#page_216">216</a></p>&#13;
<p class="indexmain">classification tasks</p>&#13;
<p class="indexsub">adopting encoder-style transformers for, <a href="ch17.xhtml#page_112">112</a>, <a href="appendix.xhtml#page_215">215</a>–<a href="appendix.xhtml#page_216">216</a></p>&#13;
<p class="indexsub">cross entropy and, <a href="ch19.xhtml#page_128">128</a></p>&#13;
<p class="indexsub">fine-tuning decoder-style transformers for, <a href="ch17.xhtml#page_112">112</a>, <a href="appendix.xhtml#page_216">216</a></p>&#13;
<p class="indexsub">using pretrained transformers for, <a href="ch18.xhtml#page_113">113</a>–<a href="ch18.xhtml#page_116">116</a></p>&#13;
<p class="indexmain">Cleanlab open source library, <a href="ch21.xhtml#page_146">146</a></p>&#13;
<p class="indexmain">[CLS] token, <a href="ch17.xhtml#page_108">108</a>, <a href="appendix.xhtml#page_215">215</a>–<a href="appendix.xhtml#page_216">216</a></p>&#13;
<p class="indexmain">CNNs. <em>See</em> convolutional neural networks; neural networks</p>&#13;
<p class="indexmain">coloring video data, <a href="appendix.xhtml#page_208">208</a></p>&#13;
<p class="indexmain">Colossal AI, <a href="ch07.xhtml#page_37">37</a>, <a href="ch07.xhtml#page_42">42</a></p>&#13;
<p class="indexmain">COMET neural framework, <a href="ch19.xhtml#page_131">131</a>, <a href="ch19.xhtml#page_135">135</a></p>&#13;
<p class="indexmain">computer vision</p>&#13;
<p class="indexsub">calculating number of parameters, <a href="ch11.xhtml#page_69">69</a>–<a href="ch11.xhtml#page_73">73</a>, <a href="appendix.xhtml#page_212">212</a>–<a href="appendix.xhtml#page_213">213</a></p>&#13;
<p class="indexsub">distributional hypothesis, <a href="appendix.xhtml#page_214">214</a></p>&#13;
<p class="indexsub">fully connected and convolutional layers, <a href="ch12.xhtml#page_75">75</a>–<a href="ch12.xhtml#page_78">78</a>, <a href="appendix.xhtml#page_213">213</a></p>&#13;
<p class="indexsub">large training sets for vision transformers, <a href="ch13.xhtml#page_79">79</a>–<a href="ch13.xhtml#page_85">85</a>, <a href="appendix.xhtml#page_213">213</a>–<a href="appendix.xhtml#page_214">214</a></p>&#13;
<p class="indexsub">self-attention mechanism, <a href="ch16.xhtml#page_103">103</a>, <a href="appendix.xhtml#page_215">215</a></p>&#13;
<p class="indexmain">concept drift, <a href="ch23.xhtml#page_155">155</a>–<a href="ch23.xhtml#page_156">156</a></p>&#13;
<p class="indexmain">confidence intervals</p>&#13;
<p class="indexsub">asymptotic coverage guarantees, <a href="ch26.xhtml#page_177">177</a></p>&#13;
<p class="indexsub">bootstrapping test set predictions, <a href="ch25.xhtml#page_169">169</a></p>&#13;
<p class="indexsub">bootstrapping training sets, <a href="ch25.xhtml#page_167">167</a>–<a href="ch25.xhtml#page_169">169</a></p>&#13;
<p class="indexsub">vs. conformal predictions, <a href="ch26.xhtml#page_173">173</a>–<a href="ch26.xhtml#page_178">178</a></p>&#13;
<p class="indexsub">defined, <a href="ch25.xhtml#page_164">164</a>–<a href="ch25.xhtml#page_165">165</a></p>&#13;
<p class="indexsub">normal approximation intervals, <a href="ch25.xhtml#page_166">166</a>–<a href="ch25.xhtml#page_167">167</a></p>&#13;
<p class="indexsub">overview, <a href="ch25.xhtml#page_163">163</a>, <a href="ch26.xhtml#page_173">173</a></p>&#13;
<p class="indexsub">and prediction intervals, <a href="ch26.xhtml#page_174">174</a></p>&#13;
<p class="indexsub">recommendations for, <a href="ch25.xhtml#page_170">170</a>, <a href="ch26.xhtml#page_178">178</a></p>&#13;
<p class="indexsub">retraining models with different random seeds, <a href="ch25.xhtml#page_169">169</a>–<a href="ch25.xhtml#page_170">170</a></p>&#13;
<p class="indexmain">confidence scores in active learning, <a href="ch30.xhtml#page_204">204</a>, <a href="appendix.xhtml#page_222">222</a></p>&#13;
<p class="indexmain">conformal predictions</p>&#13;
<p class="indexsub">benefits of, <a href="ch26.xhtml#page_177">177</a>–<a href="ch26.xhtml#page_178">178</a></p>&#13;
<p class="indexsub">computing, <a href="ch26.xhtml#page_175">175</a>–<a href="ch26.xhtml#page_176">176</a></p>&#13;
<p class="indexsub">example of, <a href="ch26.xhtml#page_176">176</a>–<a href="ch26.xhtml#page_177">177</a></p>&#13;
<p class="indexsub">overview, <a href="ch26.xhtml#page_173">173</a></p>&#13;
<p class="indexsub">and prediction intervals, <a href="ch26.xhtml#page_174">174</a></p>&#13;
<p class="indexsub">recommendations for, <a href="ch26.xhtml#page_178">178</a></p>&#13;
<p class="indexmain">connectivity, <a href="ch13.xhtml#page_80">80</a>, <a href="ch13.xhtml#page_81">81</a></p>&#13;
<p class="indexmain">consistency models, <a href="ch09.xhtml#page_56">56</a>–<a href="ch09.xhtml#page_57">57</a>, <a href="ch09.xhtml#page_58">58</a>, <a href="appendix.xhtml#page_212">212</a></p>&#13;
<p class="indexmain">continuous bag-of-words (CBOW) approach, <a href="ch14.xhtml#page_90">90</a></p>&#13;
<p class="indexmain">contrastive learning, <a href="appendix.xhtml#page_208">208</a></p>&#13;
<p class="indexmain">contrastive self-supervised learning, <a href="ch02.xhtml#page_12">12</a>–<a href="ch02.xhtml#page_14">14</a></p>&#13;
<p class="indexmain">convolutional layers</p>&#13;
<p class="indexsub">calculating number of parameters in, <a href="ch11.xhtml#page_70">70</a>–<a href="ch11.xhtml#page_71">71</a></p>&#13;
<p class="indexsub">as high-pass and low-pass filters, <a href="ch13.xhtml#page_84">84</a></p>&#13;
<p class="indexsub">recommendations for, <a href="ch12.xhtml#page_78">78</a></p>&#13;
<p class="indexsub">replacing fully connected layers with, <a href="ch12.xhtml#page_75">75</a>–<a href="ch12.xhtml#page_78">78</a></p>&#13;
<p class="indexmain">convolutional neural networks (CNNs). <em>See also</em> neural networks</p>&#13;
<p class="indexsub">calculating number of parameters in, <a href="ch11.xhtml#page_69">69</a>–<a href="ch11.xhtml#page_73">73</a>, <a href="appendix.xhtml#page_212">212</a>–<a href="appendix.xhtml#page_213">213</a></p>&#13;
<p class="indexsub">embeddings from, <a href="ch01.xhtml#page_4">4</a>, <a href="ch01.xhtml#page_6">6</a>, <a href="appendix.xhtml#page_207">207</a></p>&#13;
<p class="indexsub">high-pass and low-pass filters in, <a href="ch13.xhtml#page_84">84</a></p>&#13;
<p class="indexsub">inductive biases in, <a href="ch13.xhtml#page_80">80</a>–<a href="ch13.xhtml#page_82">82</a></p>&#13;
<p class="indexsub">recommendations for, <a href="ch13.xhtml#page_84">84</a></p>&#13;
<p class="indexsub">with vision transformers, <a href="ch13.xhtml#page_79">79</a>, <a href="ch13.xhtml#page_82">82</a>–<a href="ch13.xhtml#page_83">83</a>, <a href="ch13.xhtml#page_84">84</a></p>&#13;
<p class="indexmain">convolution operation, <a href="ch10.xhtml#page_61">61</a>–<a href="ch10.xhtml#page_62">62</a></p>&#13;
<p class="indexmain">cosine similarity, <a href="ch19.xhtml#page_132">132</a>, <a href="ch19.xhtml#page_134">134</a>, <a href="appendix.xhtml#page_216">216</a></p>&#13;
<p class="indexmain">count data, <a href="ch24.xhtml#page_161">161</a></p>&#13;
<p class="indexmain">covariate shift, <a href="ch23.xhtml#page_153">153</a>–<a href="ch23.xhtml#page_154">154</a>, <a href="ch23.xhtml#page_156">156</a>, <a href="ch23.xhtml#page_157">157</a></p>&#13;
<p class="indexmain">CPUs, data parallelism on, <a href="ch07.xhtml#page_42">42</a>, <a href="appendix.xhtml#page_211">211</a></p>&#13;
<p class="indexmain">cross-entropy (CE) loss, <a href="ch19.xhtml#page_128">128</a>, <a href="ch27.xhtml#page_182">182</a></p>&#13;
<p class="indexmain">cross-validation</p>&#13;
<p class="indexsub">5-fold cross-validation, <a href="ch28.xhtml#page_187">187</a>, <a href="ch28.xhtml#page_188">188</a>, <a href="appendix.xhtml#page_221">221</a></p>&#13;
<p class="indexsub"><em>k</em>-fold cross-validation, <a href="ch28.xhtml#page_185">185</a>–<a href="ch28.xhtml#page_188">188</a>, <a href="appendix.xhtml#page_221">221</a></p>&#13;
<p class="indexsub"><span epub:type="pagebreak" id="page_225"/>leave-one-out cross-validation (LOOCV), <a href="ch28.xhtml#page_188">188</a>, <a href="appendix.xhtml#page_221">221</a></p>&#13;
<p class="indexsub">10-fold cross-validation, <a href="ch28.xhtml#page_187">187</a>, <a href="ch28.xhtml#page_188">188</a></p>&#13;
<p class="indexmain">CUDA Deep Neural Network library (cuDNN), <a href="ch10.xhtml#page_62">62</a></p>&#13;
<h3 class="h3i"><strong>D</strong></h3>&#13;
<p class="indexmain">data. <em>See also</em> limited labeled data</p>&#13;
<p class="indexsub">applying self-supervised learning to video, <a href="ch02.xhtml#page_14">14</a>, <a href="appendix.xhtml#page_208">208</a></p>&#13;
<p class="indexsub">count, <a href="ch24.xhtml#page_161">161</a></p>&#13;
<p class="indexsub">reducing overfitting with, <a href="ch05.xhtml#page_23">23</a>–<a href="ch05.xhtml#page_27">27</a>, <a href="appendix.xhtml#page_209">209</a>–<a href="appendix.xhtml#page_210">210</a></p>&#13;
<p class="indexsub">self-supervised learning for tabular, <a href="ch02.xhtml#page_14">14</a>, <a href="appendix.xhtml#page_208">208</a></p>&#13;
<p class="indexsub">synthetic, generation of, <a href="ch15.xhtml#page_96">96</a>–<a href="ch15.xhtml#page_97">97</a></p>&#13;
<p class="indexsub">unlabeled, in self-supervised learning, <a href="ch02.xhtml#page_10">10</a>, <a href="ch02.xhtml#page_11">11</a></p>&#13;
<p class="indexmain">data augmentation</p>&#13;
<p class="indexsub">to reduce overfitting, <a href="ch05.xhtml#page_24">24</a>–<a href="ch05.xhtml#page_25">25</a>, <a href="ch05.xhtml#page_26">26</a>, <a href="appendix.xhtml#page_210">210</a></p>&#13;
<p class="indexsub">for text, <a href="ch15.xhtml#page_93">93</a>–<a href="ch15.xhtml#page_97">97</a>, <a href="appendix.xhtml#page_214">214</a>–<a href="appendix.xhtml#page_215">215</a></p>&#13;
<p class="indexmain">data-centric AI, <a href="ch21.xhtml#page_143">143</a>–<a href="ch21.xhtml#page_146">146</a>, <a href="appendix.xhtml#page_217">217</a></p>&#13;
<p class="indexmain">data distribution shifts</p>&#13;
<p class="indexsub">concept drift, <a href="ch23.xhtml#page_155">155</a></p>&#13;
<p class="indexsub">covariate shift, <a href="ch23.xhtml#page_153">153</a>–<a href="ch23.xhtml#page_154">154</a></p>&#13;
<p class="indexsub">domain shift, <a href="ch23.xhtml#page_155">155</a>–<a href="ch23.xhtml#page_156">156</a></p>&#13;
<p class="indexsub">label shift, <a href="ch23.xhtml#page_154">154</a>–<a href="ch23.xhtml#page_155">155</a></p>&#13;
<p class="indexsub">overview, <a href="ch23.xhtml#page_153">153</a></p>&#13;
<p class="indexsub">types of, <a href="ch23.xhtml#page_156">156</a>–<a href="ch23.xhtml#page_157">157</a></p>&#13;
<p class="indexmain">data parallelism, <a href="ch07.xhtml#page_37">37</a>, <a href="ch07.xhtml#page_38">38</a>, <a href="ch07.xhtml#page_39">39</a>–<a href="ch07.xhtml#page_40">40</a>, <a href="ch07.xhtml#page_41">41</a>–<a href="ch07.xhtml#page_42">42</a>, <a href="appendix.xhtml#page_211">211</a></p>&#13;
<p class="indexmain">datasets</p>&#13;
<p class="indexsub">for few-shot learning, <a href="ch03.xhtml#page_15">15</a></p>&#13;
<p class="indexsub">sampling and shuffling as source of randomness, <a href="ch10.xhtml#page_60">60</a></p>&#13;
<p class="indexsub">for transformers, <a href="ch08.xhtml#page_45">45</a></p>&#13;
<p class="indexmain">DBMs (deep Boltzmann machines), <a href="ch09.xhtml#page_50">50</a>–<a href="ch09.xhtml#page_51">51</a>, <a href="ch09.xhtml#page_57">57</a></p>&#13;
<p class="indexmain">dead neurons, <a href="appendix.xhtml#page_209">209</a></p>&#13;
<p class="indexmain">decision trees, <a href="ch30.xhtml#page_204">204</a></p>&#13;
<p class="indexmain">decoder network (VAE model), <a href="ch09.xhtml#page_51">51</a>–<a href="ch09.xhtml#page_52">52</a></p>&#13;
<p class="indexmain">decoders</p>&#13;
<p class="indexsub">in Bahdanau attention mechanism, <a href="ch16.xhtml#page_100">100</a>–<a href="ch16.xhtml#page_101">101</a></p>&#13;
<p class="indexsub">in original transformer architecture, <a href="ch17.xhtml#page_105">105</a>–<a href="ch17.xhtml#page_106">106</a>, <a href="ch17.xhtml#page_107">107</a></p>&#13;
<p class="indexmain">decoder-style transformers. <em>See also</em> encoder-style transformers</p>&#13;
<p class="indexsub">contemporary transformer models, <a href="ch17.xhtml#page_111">111</a>–<a href="ch17.xhtml#page_112">112</a></p>&#13;
<p class="indexsub">distributional hypothesis, <a href="ch14.xhtml#page_91">91</a></p>&#13;
<p class="indexsub">encoder-decoder hybrids, <a href="ch17.xhtml#page_110">110</a></p>&#13;
<p class="indexsub">overview, <a href="ch17.xhtml#page_105">105</a>, <a href="ch17.xhtml#page_108">108</a>–<a href="ch17.xhtml#page_110">110</a></p>&#13;
<p class="indexsub">synthetic data generation, <a href="ch15.xhtml#page_96">96</a>–<a href="ch15.xhtml#page_97">97</a></p>&#13;
<p class="indexsub">terminology related to, <a href="ch17.xhtml#page_110">110</a></p>&#13;
<p class="indexmain">deep Boltzmann machines (DBMs), <a href="ch09.xhtml#page_50">50</a>–<a href="ch09.xhtml#page_51">51</a>, <a href="ch09.xhtml#page_57">57</a></p>&#13;
<p class="indexmain">deep generative models. <em>See</em> generative AI models</p>&#13;
<p class="indexmain">deep learning. <em>See also</em> generative AI models</p>&#13;
<p class="indexsub">embeddings, <a href="ch01.xhtml#page_3">3</a>–<a href="ch01.xhtml#page_7">7</a>, <a href="appendix.xhtml#page_207">207</a></p>&#13;
<p class="indexsub">few-shot learning, <a href="ch03.xhtml#page_15">15</a>–<a href="ch03.xhtml#page_18">18</a>, <a href="appendix.xhtml#page_208">208</a>–<a href="appendix.xhtml#page_209">209</a></p>&#13;
<p class="indexsub">lottery ticket hypothesis, <a href="ch04.xhtml#page_19">19</a>–<a href="ch04.xhtml#page_21">21</a>, <a href="appendix.xhtml#page_209">209</a></p>&#13;
<p class="indexsub">multi-GPU training paradigms, <a href="ch07.xhtml#page_37">37</a>–<a href="ch07.xhtml#page_42">42</a>, <a href="appendix.xhtml#page_211">211</a></p>&#13;
<p class="indexsub">reducing overfitting</p>&#13;
<p class="indexsubsub">with data, <a href="ch05.xhtml#page_23">23</a>–<a href="ch05.xhtml#page_27">27</a>, <a href="appendix.xhtml#page_209">209</a>–<a href="appendix.xhtml#page_210">210</a></p>&#13;
<p class="indexsubsub">with model modifications, <a href="ch06.xhtml#page_29">29</a>–<a href="ch06.xhtml#page_36">36</a>, <a href="appendix.xhtml#page_210">210</a></p>&#13;
<p class="indexsub">self-supervised learning, <a href="ch02.xhtml#page_9">9</a>–<a href="ch02.xhtml#page_14">14</a>, <a href="appendix.xhtml#page_208">208</a></p>&#13;
<p class="indexsub">sources of randomness, <a href="ch10.xhtml#page_59">59</a>–<a href="ch10.xhtml#page_65">65</a>, <a href="appendix.xhtml#page_212">212</a></p>&#13;
<p class="indexsub">transformers, success of, <a href="ch08.xhtml#page_43">43</a>–<a href="ch08.xhtml#page_47">47</a>, <a href="appendix.xhtml#page_211">211</a></p>&#13;
<p class="indexmain">DeepSpeed, <a href="ch07.xhtml#page_37">37</a>, <a href="ch07.xhtml#page_42">42</a></p>&#13;
<p class="indexmain">deletion, word, as data augmentation technique, <a href="ch15.xhtml#page_94">94</a></p>&#13;
<p class="indexmain">deterministic algorithms, <a href="ch10.xhtml#page_62">62</a>, <a href="ch10.xhtml#page_65">65</a></p>&#13;
<p class="indexmain">diffusion models, <a href="ch09.xhtml#page_55">55</a>–<a href="ch09.xhtml#page_56">56</a>, <a href="ch09.xhtml#page_57">57</a>, <a href="ch09.xhtml#page_58">58</a></p>&#13;
<p class="indexmain">dimension contrastive self-supervised learning, <a href="ch02.xhtml#page_14">14</a></p>&#13;
<p class="indexmain">direct convolution, <a href="ch10.xhtml#page_61">61</a></p>&#13;
<p class="indexmain">discriminative models, <a href="ch09.xhtml#page_49">49</a>–<a href="ch09.xhtml#page_50">50</a></p>&#13;
<p class="indexmain">discriminator in GANs, <a href="ch09.xhtml#page_52">52</a>–<a href="ch09.xhtml#page_53">53</a></p>&#13;
<p class="indexmain">distance, embeddings as encoding, <a href="ch01.xhtml#page_5">5</a></p>&#13;
<p class="indexmain">distance functions, <a href="ch27.xhtml#page_179">179</a>–<a href="ch27.xhtml#page_183">183</a></p>&#13;
<p class="indexmain">distributional hypothesis, <a href="ch14.xhtml#page_89">89</a>–<a href="ch14.xhtml#page_92">92</a>, <a href="appendix.xhtml#page_214">214</a></p>&#13;
<p class="indexmain">domain shift (joint distribution shift), <a href="ch23.xhtml#page_155">155</a>–<a href="ch23.xhtml#page_156">156</a>, <a href="ch23.xhtml#page_157">157</a></p>&#13;
<p class="indexmain">double descent, <a href="ch06.xhtml#page_32">32</a>–<a href="ch06.xhtml#page_33">33</a>, <a href="ch06.xhtml#page_36">36</a></p>&#13;
<p class="indexmain">downstream model for pretrained transformers, <a href="ch18.xhtml#page_114">114</a></p>&#13;
<p class="indexmain">downstream task, <a href="ch02.xhtml#page_11">11</a></p>&#13;
<p class="indexmain">drivers as source of randomness, <a href="ch10.xhtml#page_62">62</a></p>&#13;
<p class="indexmain">dropout, <a href="ch06.xhtml#page_30">30</a>, <a href="ch06.xhtml#page_36">36</a>, <a href="ch10.xhtml#page_61">61</a>, <a href="ch10.xhtml#page_64">64</a>–<a href="ch10.xhtml#page_65">65</a>, <a href="appendix.xhtml#page_212">212</a></p>&#13;
<h3 class="h3i"><span epub:type="pagebreak" id="page_226"/><strong>E</strong></h3>&#13;
<p class="indexmain">early stopping, <a href="ch06.xhtml#page_30">30</a>–<a href="ch06.xhtml#page_31">31</a>, <a href="ch06.xhtml#page_35">35</a>, <a href="appendix.xhtml#page_210">210</a></p>&#13;
<p class="indexmain">EBMs (energy-based models), <a href="ch09.xhtml#page_50">50</a>–<a href="ch09.xhtml#page_51">51</a></p>&#13;
<p class="indexmain">EfficientNetV2 CNN architecture, <a href="ch13.xhtml#page_85">85</a></p>&#13;
<p class="indexmain">embeddings</p>&#13;
<p class="indexsub">distributional hypothesis, <a href="ch14.xhtml#page_90">90</a>–<a href="ch14.xhtml#page_91">91</a></p>&#13;
<p class="indexsub">in few-shot learning, <a href="ch03.xhtml#page_17">17</a></p>&#13;
<p class="indexsub">latent space, <a href="ch01.xhtml#page_5">5</a>–<a href="ch01.xhtml#page_6">6</a></p>&#13;
<p class="indexsub">in original transformer architecture, <a href="ch17.xhtml#page_106">106</a></p>&#13;
<p class="indexsub">overview, <a href="ch01.xhtml#page_3">3</a>–<a href="ch01.xhtml#page_5">5</a></p>&#13;
<p class="indexsub">representations, <a href="ch01.xhtml#page_6">6</a></p>&#13;
<p class="indexmain">emergent properties, GPT models, <a href="ch17.xhtml#page_110">110</a></p>&#13;
<p class="indexmain">encoder-decoder models, <a href="ch17.xhtml#page_110">110</a>, <a href="ch17.xhtml#page_111">111</a></p>&#13;
<p class="indexmain">encoder network (VAE model), <a href="ch09.xhtml#page_51">51</a>–<a href="ch09.xhtml#page_52">52</a></p>&#13;
<p class="indexmain">encoders</p>&#13;
<p class="indexsub">in Bahdanau attention mechanism, <a href="ch16.xhtml#page_100">100</a>–<a href="ch16.xhtml#page_101">101</a></p>&#13;
<p class="indexsub">in original transformer architecture, <a href="ch17.xhtml#page_105">105</a>–<a href="ch17.xhtml#page_107">107</a></p>&#13;
<p class="indexmain">encoder-style transformers. <em>See also</em> decoder-style transformers</p>&#13;
<p class="indexsub">contemporary transformer models, <a href="ch17.xhtml#page_111">111</a>–<a href="ch17.xhtml#page_112">112</a></p>&#13;
<p class="indexsub">encoder-decoder hybrids, <a href="ch17.xhtml#page_110">110</a></p>&#13;
<p class="indexsub">overview, <a href="ch17.xhtml#page_105">105</a>, <a href="ch17.xhtml#page_107">107</a>–<a href="ch17.xhtml#page_108">108</a></p>&#13;
<p class="indexsub">terminology related to, <a href="ch17.xhtml#page_110">110</a></p>&#13;
<p class="indexmain">energy-based models (EBMs), <a href="ch09.xhtml#page_50">50</a>–<a href="ch09.xhtml#page_51">51</a></p>&#13;
<p class="indexmain">ensemble methods, <a href="ch06.xhtml#page_33">33</a>–<a href="ch06.xhtml#page_34">34</a>, <a href="ch06.xhtml#page_35">35</a>, <a href="appendix.xhtml#page_210">210</a>, <a href="appendix.xhtml#page_221">221</a>, <a href="appendix.xhtml#page_222">222</a></p>&#13;
<p class="indexmain">episodes in few-shot learning, <a href="ch03.xhtml#page_16">16</a></p>&#13;
<p class="indexmain">Euclidean distance, <a href="ch27.xhtml#page_181">181</a></p>&#13;
<p class="indexmain">evaluation metrics for generative LLMs</p>&#13;
<p class="indexsub">BERTScore, <a href="ch19.xhtml#page_132">132</a>–<a href="ch19.xhtml#page_133">133</a></p>&#13;
<p class="indexsub">BLEU score, <a href="ch19.xhtml#page_129">129</a>–<a href="ch19.xhtml#page_131">131</a></p>&#13;
<p class="indexsub">overview, <a href="ch19.xhtml#page_127">127</a>–<a href="ch19.xhtml#page_128">128</a></p>&#13;
<p class="indexsub">perplexity, <a href="ch19.xhtml#page_128">128</a>–<a href="ch19.xhtml#page_129">129</a></p>&#13;
<p class="indexsub">ROUGE score, <a href="ch19.xhtml#page_131">131</a>–<a href="ch19.xhtml#page_132">132</a></p>&#13;
<p class="indexsub">surrogate metrics, <a href="ch19.xhtml#page_133">133</a></p>&#13;
<p class="indexmain">extrinsic metrics, <a href="ch19.xhtml#page_128">128</a></p>&#13;
<h3 class="h3i"><strong>F</strong></h3>&#13;
<p class="indexmain">fast Fourier transform (FFT)-based convolution, <a href="ch10.xhtml#page_62">62</a>, <a href="ch10.xhtml#page_65">65</a></p>&#13;
<p class="indexmain">FC layers. <em>See</em> fully connected layers</p>&#13;
<p class="indexmain">feature selection, self-attention as form of, <a href="ch08.xhtml#page_46">46</a>, <a href="appendix.xhtml#page_211">211</a></p>&#13;
<p class="indexmain">few-shot learning. <em>See also</em> in-context learning</p>&#13;
<p class="indexsub">datasets and terminology, <a href="ch03.xhtml#page_15">15</a>–<a href="ch03.xhtml#page_17">17</a></p>&#13;
<p class="indexsub">limited labeled data, <a href="ch30.xhtml#page_195">195</a>–<a href="ch30.xhtml#page_196">196</a>, <a href="ch30.xhtml#page_203">203</a></p>&#13;
<p class="indexsub">overview, <a href="ch03.xhtml#page_15">15</a></p>&#13;
<p class="indexsub">reducing overfitting with, <a href="ch05.xhtml#page_25">25</a></p>&#13;
<p class="indexmain">FFT (fast Fourier transform)-based convolution, <a href="ch10.xhtml#page_62">62</a>, <a href="ch10.xhtml#page_65">65</a></p>&#13;
<p class="indexmain">fine-tuning pretrained transformers, <a href="ch18.xhtml#page_113">113</a>–<a href="ch18.xhtml#page_116">116</a>, <a href="ch18.xhtml#page_119">119</a>–<a href="ch18.xhtml#page_124">124</a>, <a href="ch18.xhtml#page_125">125</a>–<a href="ch18.xhtml#page_126">126</a>, <a href="appendix.xhtml#page_216">216</a></p>&#13;
<p class="indexmain">finite-sample guarantees of conformal predictions, <a href="ch26.xhtml#page_177">177</a></p>&#13;
<p class="indexmain">5-fold cross-validation, <a href="ch28.xhtml#page_187">187</a>, <a href="ch28.xhtml#page_188">188</a>, <a href="appendix.xhtml#page_221">221</a></p>&#13;
<p class="indexmain">flow-based models (normalizing flows), <a href="ch09.xhtml#page_53">53</a>–<a href="ch09.xhtml#page_54">54</a>, <a href="ch09.xhtml#page_57">57</a></p>&#13;
<p class="indexmain">Fréchet inception distance approach, <a href="appendix.xhtml#page_212">212</a></p>&#13;
<p class="indexmain">fully connected (FC) layers</p>&#13;
<p class="indexsub">calculating number of parameters in, <a href="ch11.xhtml#page_70">70</a>, <a href="ch11.xhtml#page_72">72</a></p>&#13;
<p class="indexsub">lack of spatial invariance or equivariance, <a href="ch13.xhtml#page_82">82</a></p>&#13;
<p class="indexsub">recommendations for, <a href="ch12.xhtml#page_78">78</a></p>&#13;
<p class="indexsub">replacing with convolutional layers, <a href="ch12.xhtml#page_75">75</a>–<a href="ch12.xhtml#page_78">78</a></p>&#13;
<p class="indexsub">using to create embeddings, <a href="ch01.xhtml#page_6">6</a>, <a href="appendix.xhtml#page_207">207</a></p>&#13;
<h3 class="h3i"><strong>G</strong></h3>&#13;
<p class="indexmain">generalization accuracy, <a href="ch25.xhtml#page_164">164</a></p>&#13;
<p class="indexmain">generalization performance, <a href="ch06.xhtml#page_32">32</a>–<a href="ch06.xhtml#page_33">33</a></p>&#13;
<p class="indexmain">generative adversarial networks (GANs), <a href="ch09.xhtml#page_52">52</a>–<a href="ch09.xhtml#page_53">53</a>, <a href="ch09.xhtml#page_54">54</a>, <a href="ch09.xhtml#page_57">57</a>, <a href="ch09.xhtml#page_58">58</a></p>&#13;
<p class="indexmain">generative AI models</p>&#13;
<p class="indexsub">autoregressive models, <a href="ch09.xhtml#page_54">54</a>–<a href="ch09.xhtml#page_55">55</a></p>&#13;
<p class="indexsub">consistency models, <a href="ch09.xhtml#page_56">56</a>–<a href="ch09.xhtml#page_57">57</a></p>&#13;
<p class="indexsub">diffusion models, <a href="ch09.xhtml#page_55">55</a>–<a href="ch09.xhtml#page_56">56</a></p>&#13;
<p class="indexsub">energy-based models, <a href="ch09.xhtml#page_50">50</a>–<a href="ch09.xhtml#page_51">51</a></p>&#13;
<p class="indexsub">flow-based models, <a href="ch09.xhtml#page_53">53</a>–<a href="ch09.xhtml#page_54">54</a></p>&#13;
<p class="indexsub">generative adversarial networks, <a href="ch09.xhtml#page_52">52</a>–<a href="ch09.xhtml#page_53">53</a></p>&#13;
<p class="indexsub">generative vs. discriminative modeling, <a href="ch09.xhtml#page_49">49</a>–<a href="ch09.xhtml#page_50">50</a></p>&#13;
<p class="indexsub">overview, <a href="ch09.xhtml#page_49">49</a></p>&#13;
<p class="indexsub">randomness and, <a href="ch10.xhtml#page_62">62</a>–<a href="ch10.xhtml#page_64">64</a></p>&#13;
<p class="indexsub">recommendations for, <a href="ch09.xhtml#page_57">57</a></p>&#13;
<p class="indexsub">variational autoencoders, <a href="ch09.xhtml#page_51">51</a>–<a href="ch09.xhtml#page_52">52</a></p>&#13;
<p class="indexmain"><span epub:type="pagebreak" id="page_227"/>generative large language models. <em>See</em> evaluation metrics for generative LLMs; large language models; natural language processing</p>&#13;
<p class="indexmain">generator in GANs, <a href="ch09.xhtml#page_52">52</a>–<a href="ch09.xhtml#page_53">53</a></p>&#13;
<p class="indexmain">Gibbs sampling, <a href="ch09.xhtml#page_51">51</a></p>&#13;
<p class="indexmain">GPT (generative pretrained transformer) models</p>&#13;
<p class="indexsub">decoder-style transformers, <a href="ch14.xhtml#page_91">91</a>, <a href="ch17.xhtml#page_109">109</a>–<a href="ch17.xhtml#page_110">110</a></p>&#13;
<p class="indexsub">fine-tuning for classification, <a href="ch17.xhtml#page_112">112</a>, <a href="appendix.xhtml#page_216">216</a></p>&#13;
<p class="indexsub">randomness by design, <a href="ch10.xhtml#page_63">63</a></p>&#13;
<p class="indexsub">self-prediction, <a href="ch02.xhtml#page_12">12</a></p>&#13;
<p class="indexmain">GPUs. <em>See</em> multi-GPU training paradigms</p>&#13;
<p class="indexmain">grokking, <a href="ch06.xhtml#page_32">32</a>–<a href="ch06.xhtml#page_33">33</a>, <a href="ch06.xhtml#page_36">36</a></p>&#13;
<h3 class="h3i"><strong>H</strong></h3>&#13;
<p class="indexmain">hard attention, <a href="appendix.xhtml#page_211">211</a></p>&#13;
<p class="indexmain">hard parameter sharing, <a href="ch30.xhtml#page_200">200</a></p>&#13;
<p class="indexmain">hard prompt tuning, <a href="ch18.xhtml#page_117">117</a>–<a href="ch18.xhtml#page_118">118</a></p>&#13;
<p class="indexmain">hardware as source of randomness, <a href="ch10.xhtml#page_62">62</a></p>&#13;
<p class="indexmain">hierarchical processing in CNNs, <a href="ch13.xhtml#page_80">80</a></p>&#13;
<p class="indexmain">histograms, <a href="appendix.xhtml#page_207">207</a></p>&#13;
<p class="indexmain">holdout validation as source of randomness, <a href="ch10.xhtml#page_60">60</a></p>&#13;
<p class="indexmain">homophones, <a href="ch14.xhtml#page_92">92</a>, <a href="appendix.xhtml#page_214">214</a></p>&#13;
<p class="indexmain">human feedback, reinforcement learning with, <a href="ch18.xhtml#page_124">124</a></p>&#13;
<p class="indexmain">hyperparameter tuning, <a href="ch28.xhtml#page_188">188</a></p>&#13;
<h3 class="h3i"><strong>I</strong></h3>&#13;
<p class="indexmain">image denoising, <a href="ch09.xhtml#page_56">56</a>–<a href="ch09.xhtml#page_57">57</a></p>&#13;
<p class="indexmain">image generation, <a href="ch09.xhtml#page_51">51</a>, <a href="ch09.xhtml#page_52">52</a>, <a href="ch09.xhtml#page_54">54</a>–<a href="ch09.xhtml#page_57">57</a>, <a href="appendix.xhtml#page_211">211</a>–<a href="appendix.xhtml#page_212">212</a></p>&#13;
<p class="indexmain">image histograms, <a href="appendix.xhtml#page_207">207</a></p>&#13;
<p class="indexmain">“An Image Is Worth 16×16 Words” (Dosovitskiy et al.), <a href="ch13.xhtml#page_83">83</a>, <a href="ch13.xhtml#page_85">85</a></p>&#13;
<p class="indexmain">ImageNet dataset, <a href="ch02.xhtml#page_9">9</a>, <a href="ch02.xhtml#page_14">14</a>, <a href="ch26.xhtml#page_175">175</a></p>&#13;
<p class="indexmain">image processing. <em>See</em> computer vision</p>&#13;
<p class="indexmain">importance weighting, <a href="ch23.xhtml#page_154">154</a>, <a href="ch23.xhtml#page_155">155</a>, <a href="ch23.xhtml#page_157">157</a>, <a href="appendix.xhtml#page_218">218</a></p>&#13;
<p class="indexmain">in-context learning, <a href="ch18.xhtml#page_113">113</a>, <a href="ch18.xhtml#page_116">116</a>–<a href="ch18.xhtml#page_119">119</a>, <a href="ch18.xhtml#page_125">125</a>, <a href="appendix.xhtml#page_216">216</a>. <em>See also</em> few-shot learning</p>&#13;
<p class="indexmain">indexing, <a href="ch18.xhtml#page_118">118</a>–<a href="ch18.xhtml#page_119">119</a>, <a href="ch18.xhtml#page_125">125</a></p>&#13;
<p class="indexmain">inductive biases</p>&#13;
<p class="indexsub">in convolutional neural networks, <a href="ch13.xhtml#page_80">80</a>–<a href="ch13.xhtml#page_82">82</a></p>&#13;
<p class="indexsub">limited labeled data, <a href="ch30.xhtml#page_202">202</a></p>&#13;
<p class="indexsub">overview, <a href="ch13.xhtml#page_79">79</a></p>&#13;
<p class="indexsub">in vision transformers, <a href="ch13.xhtml#page_83">83</a>–<a href="ch13.xhtml#page_84">84</a></p>&#13;
<p class="indexmain">inference, speeding up. <em>See</em> model inference, speeding up</p>&#13;
<p class="indexmain">inpainting, <a href="ch30.xhtml#page_194">194</a>–<a href="ch30.xhtml#page_195">195</a>, <a href="appendix.xhtml#page_208">208</a></p>&#13;
<p class="indexmain">input channels in convolutional layers, <a href="ch11.xhtml#page_70">70</a>–<a href="ch11.xhtml#page_71">71</a>, <a href="ch12.xhtml#page_76">76</a>–<a href="ch12.xhtml#page_77">77</a></p>&#13;
<p class="indexmain">input embedding, <a href="ch01.xhtml#page_4">4</a></p>&#13;
<p class="indexmain">input representations, <a href="ch01.xhtml#page_6">6</a>, <a href="appendix.xhtml#page_207">207</a></p>&#13;
<p class="indexmain">InstructGPT model, <a href="ch18.xhtml#page_124">124</a>, <a href="ch18.xhtml#page_126">126</a>, <a href="ch19.xhtml#page_133">133</a>, <a href="ch19.xhtml#page_135">135</a></p>&#13;
<p class="indexmain">inter-op parallelism (model parallelism), <a href="ch07.xhtml#page_37">37</a>, <a href="ch07.xhtml#page_38">38</a>, <a href="ch07.xhtml#page_39">39</a>–<a href="ch07.xhtml#page_40">40</a>, <a href="ch07.xhtml#page_41">41</a>–<a href="ch07.xhtml#page_42">42</a></p>&#13;
<p class="indexmain">intra-op parallelism (tensor parallelism), <a href="ch07.xhtml#page_37">37</a>, <a href="ch07.xhtml#page_38">38</a>–<a href="ch07.xhtml#page_40">40</a>, <a href="ch07.xhtml#page_41">41</a>–<a href="ch07.xhtml#page_42">42</a>, <a href="appendix.xhtml#page_211">211</a></p>&#13;
<p class="indexmain">intrinsic metrics, <a href="ch19.xhtml#page_128">128</a></p>&#13;
<p class="indexmain">iterative pruning, <a href="ch04.xhtml#page_20">20</a>, <a href="ch06.xhtml#page_31">31</a></p>&#13;
<h3 class="h3i"><strong>J</strong></h3>&#13;
<p class="indexmain">joint distribution shift (domain shift), <a href="ch23.xhtml#page_155">155</a>–<a href="ch23.xhtml#page_156">156</a>, <a href="ch23.xhtml#page_157">157</a></p>&#13;
<h3 class="h3i"><strong>K</strong></h3>&#13;
<p class="indexmain">kernel size in convolutional layers, <a href="ch11.xhtml#page_70">70</a>–<a href="ch11.xhtml#page_71">71</a>, <a href="ch12.xhtml#page_76">76</a>–<a href="ch12.xhtml#page_77">77</a></p>&#13;
<p class="indexmain"><em>k</em>-fold cross-validation</p>&#13;
<p class="indexsub">determining appropriate values for <em>k</em>, <a href="ch28.xhtml#page_187">187</a>–<a href="ch28.xhtml#page_188">188</a></p>&#13;
<p class="indexsub">ensemble approach, <a href="ch06.xhtml#page_33">33</a>–<a href="ch06.xhtml#page_34">34</a></p>&#13;
<p class="indexsub">overview, <a href="ch28.xhtml#page_185">185</a>–<a href="ch28.xhtml#page_186">186</a></p>&#13;
<p class="indexsub">as source of randomness, <a href="ch10.xhtml#page_60">60</a></p>&#13;
<p class="indexsub">trade-offs in selecting values for <em>k</em>, <a href="ch28.xhtml#page_186">186</a>–<a href="ch28.xhtml#page_187">187</a></p>&#13;
<p class="indexmain">knowledge distillation, <a href="ch06.xhtml#page_31">31</a>–<a href="ch06.xhtml#page_33">33</a>, <a href="ch06.xhtml#page_35">35</a>, <a href="ch06.xhtml#page_36">36</a>, <a href="ch22.xhtml#page_151">151</a>, <a href="ch30.xhtml#page_199">199</a></p>&#13;
<p class="indexmain">Kullback–Leibler divergence (KL divergence), <a href="ch06.xhtml#page_32">32</a>, <a href="ch09.xhtml#page_52">52</a>, <a href="appendix.xhtml#page_211">211</a></p>&#13;
<h3 class="h3i"><strong>L</strong></h3>&#13;
<p class="indexmain"><em>L</em>2 distance, <a href="ch27.xhtml#page_181">181</a></p>&#13;
<p class="indexmain"><em>L</em>2 regularization, <a href="ch06.xhtml#page_30">30</a>, <a href="ch06.xhtml#page_35">35</a></p>&#13;
<p class="indexmain"><span epub:type="pagebreak" id="page_228"/>labeled data, limited. <em>See</em> limited labeled data</p>&#13;
<p class="indexmain">label shift (prior probability shift), <a href="ch23.xhtml#page_154">154</a>–<a href="ch23.xhtml#page_155">155</a>, <a href="ch23.xhtml#page_156">156</a></p>&#13;
<p class="indexmain">label smoothing, <a href="ch05.xhtml#page_27">27</a></p>&#13;
<p class="indexmain">language transformers. <em>See</em> transformers</p>&#13;
<p class="indexmain">large language models (LLMs). <em>See also</em> natural language processing; transformers</p>&#13;
<p class="indexsub">distributional hypothesis, <a href="ch14.xhtml#page_91">91</a></p>&#13;
<p class="indexsub">evaluation metrics for, <a href="ch19.xhtml#page_127">127</a>–<a href="ch19.xhtml#page_135">135</a>, <a href="appendix.xhtml#page_216">216</a>–<a href="appendix.xhtml#page_217">217</a></p>&#13;
<p class="indexsub">stateless vs. stateful training, <a href="ch20.xhtml#page_141">141</a>, <a href="appendix.xhtml#page_217">217</a></p>&#13;
<p class="indexsub">synthetic data generation, <a href="ch15.xhtml#page_96">96</a>–<a href="ch15.xhtml#page_97">97</a></p>&#13;
<p class="indexmain">latent space, <a href="ch01.xhtml#page_3">3</a>, <a href="ch01.xhtml#page_5">5</a>–<a href="ch01.xhtml#page_7">7</a></p>&#13;
<p class="indexmain">layer input normalization techniques, <a href="ch06.xhtml#page_34">34</a>–<a href="ch06.xhtml#page_35">35</a></p>&#13;
<p class="indexmain">layers</p>&#13;
<p class="indexsub">convolutional layers</p>&#13;
<p class="indexsubsub">calculating number of parameters in, <a href="ch11.xhtml#page_70">70</a>–<a href="ch11.xhtml#page_71">71</a></p>&#13;
<p class="indexsubsub">as high-pass and low-pass filters, <a href="ch13.xhtml#page_84">84</a></p>&#13;
<p class="indexsubsub">recommendations for, <a href="ch12.xhtml#page_78">78</a></p>&#13;
<p class="indexsubsub">replacing fully connected layers with, <a href="ch12.xhtml#page_75">75</a>–<a href="ch12.xhtml#page_78">78</a></p>&#13;
<p class="indexsub">normalization in original transformer architecture, <a href="ch17.xhtml#page_106">106</a>–<a href="ch17.xhtml#page_107">107</a></p>&#13;
<p class="indexsub">updating when fine-tuning pretrained transformers, <a href="ch18.xhtml#page_115">115</a>–<a href="ch18.xhtml#page_116">116</a></p>&#13;
<p class="indexsub">using to create embeddings, <a href="appendix.xhtml#page_207">207</a></p>&#13;
<p class="indexmain">leave-one-out cross-validation (LOOCV), <a href="ch28.xhtml#page_188">188</a>, <a href="appendix.xhtml#page_221">221</a></p>&#13;
<p class="indexmain">limited labeled data</p>&#13;
<p class="indexsub">active learning, <a href="ch30.xhtml#page_195">195</a></p>&#13;
<p class="indexsub">bootstrapping data, <a href="ch30.xhtml#page_194">194</a></p>&#13;
<p class="indexsub">few-shot learning, <a href="ch30.xhtml#page_195">195</a>–<a href="ch30.xhtml#page_196">196</a></p>&#13;
<p class="indexsub">inductive biases, <a href="ch30.xhtml#page_202">202</a></p>&#13;
<p class="indexsub">labeling more data, <a href="ch30.xhtml#page_193">193</a>–<a href="ch30.xhtml#page_194">194</a></p>&#13;
<p class="indexsub">meta-learning, <a href="ch30.xhtml#page_196">196</a>–<a href="ch30.xhtml#page_197">197</a></p>&#13;
<p class="indexsub">multimodal learning, <a href="ch30.xhtml#page_200">200</a>–<a href="ch30.xhtml#page_202">202</a></p>&#13;
<p class="indexsub">multi-task learning, <a href="ch30.xhtml#page_199">199</a>–<a href="ch30.xhtml#page_200">200</a></p>&#13;
<p class="indexsub">overview, <a href="ch30.xhtml#page_193">193</a></p>&#13;
<p class="indexsub">recommendations for choosing technique, <a href="ch30.xhtml#page_202">202</a>–<a href="ch30.xhtml#page_203">203</a></p>&#13;
<p class="indexsub">self-supervised learning, <a href="ch30.xhtml#page_194">194</a>–<a href="ch30.xhtml#page_195">195</a></p>&#13;
<p class="indexsub">self-training, <a href="ch30.xhtml#page_199">199</a></p>&#13;
<p class="indexsub">semi-supervised learning, <a href="ch30.xhtml#page_198">198</a>–<a href="ch30.xhtml#page_199">199</a></p>&#13;
<p class="indexsub">transfer learning, <a href="ch30.xhtml#page_194">194</a></p>&#13;
<p class="indexsub">weakly supervised learning, <a href="ch30.xhtml#page_197">197</a>–<a href="ch30.xhtml#page_198">198</a></p>&#13;
<p class="indexmain">linear classifiers, <a href="ch18.xhtml#page_114">114</a></p>&#13;
<p class="indexmain">LLMs. <em>See</em> large language models; natural language processing; transformers</p>&#13;
<p class="indexmain">local connectivity in CNNs, <a href="ch13.xhtml#page_80">80</a>, <a href="ch13.xhtml#page_81">81</a></p>&#13;
<p class="indexmain">logistic regression classifier, <a href="ch09.xhtml#page_49">49</a>–<a href="ch09.xhtml#page_50">50</a></p>&#13;
<p class="indexmain">LOOCV (leave-one-out cross-validation), <a href="ch28.xhtml#page_188">188</a>, <a href="appendix.xhtml#page_221">221</a></p>&#13;
<p class="indexmain">loop fusion (operator fusion), <a href="ch22.xhtml#page_150">150</a>–<a href="ch22.xhtml#page_151">151</a></p>&#13;
<p class="indexmain">loop tiling (loop nest optimization), <a href="ch22.xhtml#page_149">149</a>–<a href="ch22.xhtml#page_150">150</a>, <a href="ch22.xhtml#page_151">151</a>, <a href="ch22.xhtml#page_152">152</a>, <a href="appendix.xhtml#page_218">218</a></p>&#13;
<p class="indexmain">LoRA (low-rank adaptation), <a href="ch18.xhtml#page_119">119</a>, <a href="ch18.xhtml#page_123">123</a>–<a href="ch18.xhtml#page_124">124</a>, <a href="ch18.xhtml#page_125">125</a>, <a href="ch18.xhtml#page_126">126</a>, <a href="appendix.xhtml#page_216">216</a></p>&#13;
<p class="indexmain">loss function, VAEs, <a href="ch09.xhtml#page_52">52</a></p>&#13;
<p class="indexmain">lottery ticket hypothesis</p>&#13;
<p class="indexsub">overview, <a href="ch04.xhtml#page_19">19</a></p>&#13;
<p class="indexsub">practical implications and limitations, <a href="ch04.xhtml#page_20">20</a>–<a href="ch04.xhtml#page_21">21</a></p>&#13;
<p class="indexsub">training procedure for, <a href="ch04.xhtml#page_19">19</a>–<a href="ch04.xhtml#page_20">20</a></p>&#13;
<p class="indexmain">low-rank adaptation (LoRA), <a href="ch18.xhtml#page_119">119</a>, <a href="ch18.xhtml#page_123">123</a>–<a href="ch18.xhtml#page_124">124</a>, <a href="ch18.xhtml#page_125">125</a>, <a href="ch18.xhtml#page_126">126</a>, <a href="appendix.xhtml#page_216">216</a></p>&#13;
<p class="indexmain">low-rank transformation, <a href="ch18.xhtml#page_123">123</a></p>&#13;
<h3 class="h3i"><strong>M</strong></h3>&#13;
<p class="indexmain">MAE (mean absolute error), <a href="ch27.xhtml#page_183">183</a>, <a href="appendix.xhtml#page_220">220</a>–<a href="appendix.xhtml#page_221">221</a></p>&#13;
<p class="indexmain">majority voting, <a href="ch06.xhtml#page_33">33</a></p>&#13;
<p class="indexmain">MAPIE library, <a href="ch26.xhtml#page_178">178</a></p>&#13;
<p class="indexmain">masked (missing) input self-prediction methods, <a href="ch02.xhtml#page_12">12</a></p>&#13;
<p class="indexmain">masked frames, predicting, <a href="appendix.xhtml#page_208">208</a></p>&#13;
<p class="indexmain">masked language modeling, <a href="ch14.xhtml#page_91">91</a>, <a href="ch17.xhtml#page_107">107</a>–<a href="ch17.xhtml#page_108">108</a>, <a href="ch30.xhtml#page_194">194</a></p>&#13;
<p class="indexmain">mean absolute error (MAE), <a href="ch27.xhtml#page_183">183</a>, <a href="appendix.xhtml#page_220">220</a>–<a href="appendix.xhtml#page_221">221</a></p>&#13;
<p class="indexmain">mean squared error (MSE) loss, <a href="ch27.xhtml#page_180">180</a>–<a href="ch27.xhtml#page_181">181</a></p>&#13;
<p class="indexmain">memory complexity of self-attention, <a href="ch16.xhtml#page_103">103</a>, <a href="appendix.xhtml#page_215">215</a></p>&#13;
<p class="indexmain">metadata (meta-features) extraction, <a href="ch30.xhtml#page_197">197</a></p>&#13;
<p class="indexmain">meta-learning, <a href="ch03.xhtml#page_17">17</a>, <a href="ch30.xhtml#page_196">196</a>–<a href="ch30.xhtml#page_197">197</a></p>&#13;
<p class="indexmain">METEOR metric, <a href="ch19.xhtml#page_131">131</a>, <a href="ch19.xhtml#page_134">134</a></p>&#13;
<p class="indexmain">metrics, proper. <em>See</em> proper metrics</p>&#13;
<p class="indexmain"><span epub:type="pagebreak" id="page_229"/>missing (masked) input self-prediction methods, <a href="ch02.xhtml#page_12">12</a></p>&#13;
<p class="indexmain">missing frames, predicting, <a href="appendix.xhtml#page_208">208</a></p>&#13;
<p class="indexmain">Mixup, <a href="ch05.xhtml#page_27">27</a></p>&#13;
<p class="indexmain">MLPs (multilayer perceptrons), <a href="ch09.xhtml#page_50">50</a>, <a href="ch13.xhtml#page_82">82</a></p>&#13;
<p class="indexmain">MNIST dataset, <a href="ch03.xhtml#page_15">15</a>, <a href="ch03.xhtml#page_18">18</a>, <a href="ch05.xhtml#page_26">26</a>, <a href="appendix.xhtml#page_208">208</a>, <a href="appendix.xhtml#page_210">210</a></p>&#13;
<p class="indexmain">model-centric AI, <a href="ch21.xhtml#page_143">143</a>–<a href="ch21.xhtml#page_144">144</a>, <a href="ch21.xhtml#page_145">145</a></p>&#13;
<p class="indexmain">model ensembling, <a href="ch06.xhtml#page_33">33</a>–<a href="ch06.xhtml#page_34">34</a>, <a href="ch06.xhtml#page_35">35</a>, <a href="appendix.xhtml#page_210">210</a>, <a href="appendix.xhtml#page_221">221</a>, <a href="appendix.xhtml#page_222">222</a></p>&#13;
<p class="indexmain">model evaluation. <em>See</em> predictive performance and model evaluation</p>&#13;
<p class="indexmain">model inference, speeding up</p>&#13;
<p class="indexsub">loop tiling, <a href="ch22.xhtml#page_149">149</a>–<a href="ch22.xhtml#page_150">150</a></p>&#13;
<p class="indexsub">operator fusion, <a href="ch22.xhtml#page_150">150</a>–<a href="ch22.xhtml#page_151">151</a></p>&#13;
<p class="indexsub">overview, <a href="ch22.xhtml#page_147">147</a></p>&#13;
<p class="indexsub">parallelization, <a href="ch22.xhtml#page_147">147</a>–<a href="ch22.xhtml#page_148">148</a></p>&#13;
<p class="indexsub">quantization, <a href="ch22.xhtml#page_151">151</a></p>&#13;
<p class="indexsub">vectorization, <a href="ch22.xhtml#page_148">148</a>–<a href="ch22.xhtml#page_149">149</a></p>&#13;
<p class="indexmain">model modifications, reducing overfitting with, <a href="ch06.xhtml#page_29">29</a>–<a href="ch06.xhtml#page_36">36</a>, <a href="appendix.xhtml#page_210">210</a></p>&#13;
<p class="indexmain">model parallelism (inter-op parallelism), <a href="ch07.xhtml#page_37">37</a>, <a href="ch07.xhtml#page_38">38</a>, <a href="ch07.xhtml#page_39">39</a>–<a href="ch07.xhtml#page_40">40</a>, <a href="ch07.xhtml#page_41">41</a>–<a href="ch07.xhtml#page_42">42</a></p>&#13;
<p class="indexmain">model weight initialization as source of randomness, <a href="ch10.xhtml#page_59">59</a>–<a href="ch10.xhtml#page_60">60</a></p>&#13;
<p class="indexmain">MSE (mean squared error) loss, <a href="ch27.xhtml#page_180">180</a>–<a href="ch27.xhtml#page_181">181</a></p>&#13;
<p class="indexmain">multi-GPU training paradigms</p>&#13;
<p class="indexsub">data parallelism, <a href="ch07.xhtml#page_38">38</a></p>&#13;
<p class="indexsub">model parallelism, <a href="ch07.xhtml#page_38">38</a> overview, <a href="ch07.xhtml#page_37">37</a></p>&#13;
<p class="indexsub">pipeline parallelism, <a href="ch07.xhtml#page_40">40</a></p>&#13;
<p class="indexsub">recommendations for, <a href="ch07.xhtml#page_41">41</a>–<a href="ch07.xhtml#page_42">42</a></p>&#13;
<p class="indexsub">sequence parallelism, <a href="ch07.xhtml#page_40">40</a>–<a href="ch07.xhtml#page_41">41</a></p>&#13;
<p class="indexsub">speeding up inference, <a href="ch22.xhtml#page_152">152</a>, <a href="appendix.xhtml#page_218">218</a></p>&#13;
<p class="indexsub">tensor parallelism, <a href="ch07.xhtml#page_38">38</a>–<a href="ch07.xhtml#page_40">40</a></p>&#13;
<p class="indexmain">multilayer perceptrons (MLPs), <a href="ch09.xhtml#page_50">50</a>, <a href="ch13.xhtml#page_82">82</a></p>&#13;
<p class="indexmain">multimodal learning, <a href="ch30.xhtml#page_200">200</a>–<a href="ch30.xhtml#page_202">202</a>, <a href="ch30.xhtml#page_204">204</a></p>&#13;
<p class="indexmain">multi-task learning, <a href="ch30.xhtml#page_199">199</a>–<a href="ch30.xhtml#page_200">200</a>, <a href="ch30.xhtml#page_204">204</a></p>&#13;
<h3 class="h3i"><strong>N</strong></h3>&#13;
<p class="indexmain">naive Bayes classifier, <a href="ch09.xhtml#page_49">49</a>–<a href="ch09.xhtml#page_50">50</a></p>&#13;
<p class="indexmain">natural language processing (NLP). <em>See also</em> transformers</p>&#13;
<p class="indexsub">data augmentation for text, <a href="ch15.xhtml#page_93">93</a>–<a href="ch15.xhtml#page_97">97</a>, <a href="appendix.xhtml#page_214">214</a>–<a href="appendix.xhtml#page_215">215</a></p>&#13;
<p class="indexsub">distributional hypothesis, <a href="ch14.xhtml#page_89">89</a>–<a href="ch14.xhtml#page_92">92</a>, <a href="appendix.xhtml#page_214">214</a></p>&#13;
<p class="indexsub">evaluating generative LLMs, <a href="ch19.xhtml#page_127">127</a>–<a href="ch19.xhtml#page_135">135</a>, <a href="appendix.xhtml#page_211">211</a>–<a href="appendix.xhtml#page_212">212</a></p>&#13;
<p class="indexsub">self-attention, <a href="ch16.xhtml#page_99">99</a>–<a href="ch16.xhtml#page_103">103</a>, <a href="appendix.xhtml#page_215">215</a></p>&#13;
<p class="indexmain">neural networks. <em>See also</em> convolutional neural networks; generative AI models; transformers</p>&#13;
<p class="indexsub">attention mechanism for, <a href="ch16.xhtml#page_99">99</a>–<a href="ch16.xhtml#page_101">101</a></p>&#13;
<p class="indexsub">calculating number of parameters in, <a href="ch11.xhtml#page_69">69</a>–<a href="ch11.xhtml#page_73">73</a>, <a href="appendix.xhtml#page_212">212</a>–<a href="appendix.xhtml#page_213">213</a></p>&#13;
<p class="indexsub">embeddings, <a href="ch01.xhtml#page_3">3</a>–<a href="ch01.xhtml#page_7">7</a>, <a href="appendix.xhtml#page_207">207</a></p>&#13;
<p class="indexsub">few-shot learning, <a href="ch03.xhtml#page_15">15</a>–<a href="ch03.xhtml#page_18">18</a>, <a href="appendix.xhtml#page_208">208</a>–<a href="appendix.xhtml#page_209">209</a></p>&#13;
<p class="indexsub">lottery ticket hypothesis, <a href="ch04.xhtml#page_19">19</a>–<a href="ch04.xhtml#page_21">21</a>, <a href="appendix.xhtml#page_209">209</a></p>&#13;
<p class="indexsub">multi-GPU training paradigms, <a href="ch07.xhtml#page_37">37</a>–<a href="ch07.xhtml#page_42">42</a>, <a href="appendix.xhtml#page_211">211</a></p>&#13;
<p class="indexsub">reducing overfitting</p>&#13;
<p class="indexsubsub">with data, <a href="ch05.xhtml#page_23">23</a>–<a href="ch05.xhtml#page_27">27</a>, <a href="appendix.xhtml#page_209">209</a>–<a href="appendix.xhtml#page_210">210</a></p>&#13;
<p class="indexsubsub">with model modifications, <a href="ch06.xhtml#page_29">29</a>–<a href="ch06.xhtml#page_36">36</a>, <a href="appendix.xhtml#page_210">210</a></p>&#13;
<p class="indexsub">self-attention, <a href="ch16.xhtml#page_99">99</a>–<a href="ch16.xhtml#page_103">103</a></p>&#13;
<p class="indexsub">self-supervised learning, <a href="ch02.xhtml#page_9">9</a>–<a href="ch02.xhtml#page_14">14</a>, <a href="appendix.xhtml#page_208">208</a></p>&#13;
<p class="indexsub">sources of randomness, <a href="ch10.xhtml#page_59">59</a>–<a href="ch10.xhtml#page_65">65</a>, <a href="appendix.xhtml#page_212">212</a></p>&#13;
<p class="indexsub">transformers, success of, <a href="ch08.xhtml#page_43">43</a>–<a href="ch08.xhtml#page_47">47</a>, <a href="appendix.xhtml#page_211">211</a></p>&#13;
<p class="indexmain">next-sentence/next-word prediction task, <a href="ch02.xhtml#page_12">12</a>, <a href="ch17.xhtml#page_108">108</a>, <a href="ch17.xhtml#page_109">109</a>, <a href="ch30.xhtml#page_194">194</a></p>&#13;
<p class="indexmain">NICE (non-linear independent components estimation), <a href="ch09.xhtml#page_53">53</a>–<a href="ch09.xhtml#page_54">54</a>, <a href="ch09.xhtml#page_58">58</a></p>&#13;
<p class="indexmain">NLP. <em>See</em> natural language processing; transformers</p>&#13;
<p class="indexmain">noise</p>&#13;
<p class="indexsub">consistency models and, <a href="ch09.xhtml#page_56">56</a>–<a href="ch09.xhtml#page_57">57</a></p>&#13;
<p class="indexsub">diffusion models and, <a href="ch09.xhtml#page_56">56</a></p>&#13;
<p class="indexmain">noise injection, <a href="ch15.xhtml#page_95">95</a>–<a href="ch15.xhtml#page_96">96</a></p>&#13;
<p class="indexmain">nonconformity measure, <a href="ch26.xhtml#page_176">176</a>–<a href="ch26.xhtml#page_177">177</a></p>&#13;
<p class="indexmain">nondeterministic algorithms, <a href="ch10.xhtml#page_61">61</a></p>&#13;
<p class="indexmain">non-linear independent components estimation (NICE), <a href="ch09.xhtml#page_53">53</a>–<a href="ch09.xhtml#page_54">54</a>, <a href="ch09.xhtml#page_58">58</a></p>&#13;
<p class="indexmain">normal approximation intervals, <a href="ch25.xhtml#page_166">166</a>–<a href="ch25.xhtml#page_167">167</a>, <a href="ch25.xhtml#page_170">170</a>, <a href="ch25.xhtml#page_171">171</a></p>&#13;
<p class="indexmain">normalizing flows (flow-based models), <a href="ch09.xhtml#page_53">53</a>–<a href="ch09.xhtml#page_54">54</a>, <a href="ch09.xhtml#page_57">57</a></p>&#13;
<p class="indexmain">nucleus sampling (top-<em>p</em> sampling), <a href="ch10.xhtml#page_63">63</a>–<a href="ch10.xhtml#page_64">64</a>, <a href="appendix.xhtml#page_212">212</a></p>&#13;
<p class="indexmain">NVIDIA graphics cards, <a href="ch10.xhtml#page_62">62</a>, <a href="ch10.xhtml#page_65">65</a></p>&#13;
<p class="indexmain"><em>N</em>-way <em>K</em>-shot (few-shot learning), <a href="ch03.xhtml#page_15">15</a>–<a href="ch03.xhtml#page_16">16</a></p>&#13;
<h3 class="h3i"><strong>O</strong></h3>&#13;
<p class="indexmain">ODE (ordinary differential equation) trajectory, <a href="ch09.xhtml#page_56">56</a>–<a href="ch09.xhtml#page_57">57</a></p>&#13;
<p class="indexmain"><span epub:type="pagebreak" id="page_230"/>one-hot encoding, <a href="ch01.xhtml#page_4">4</a>, <a href="appendix.xhtml#page_207">207</a></p>&#13;
<p class="indexmain">online resources, <a href="ch00.xhtml#page_xxviii">xxviii</a></p>&#13;
<p class="indexmain">operator fusion (loop fusion), <a href="ch22.xhtml#page_150">150</a>–<a href="ch22.xhtml#page_151">151</a></p>&#13;
<p class="indexmain">ordinal regression, <a href="ch24.xhtml#page_161">161</a>–<a href="ch24.xhtml#page_162">162</a>, <a href="appendix.xhtml#page_218">218</a>–<a href="appendix.xhtml#page_219">219</a></p>&#13;
<p class="indexmain">ordinary differential equation (ODE) trajectory, <a href="ch09.xhtml#page_56">56</a>–<a href="ch09.xhtml#page_57">57</a></p>&#13;
<p class="indexmain">outlier detection, <a href="appendix.xhtml#page_218">218</a></p>&#13;
<p class="indexmain">out-of-bag bootstrapping, <a href="ch25.xhtml#page_167">167</a>–<a href="ch25.xhtml#page_169">169</a>, <a href="ch25.xhtml#page_170">170</a>, <a href="ch25.xhtml#page_171">171</a></p>&#13;
<p class="indexmain">output channels in convolutional layers, <a href="ch11.xhtml#page_70">70</a>–<a href="ch11.xhtml#page_71">71</a>, <a href="ch12.xhtml#page_76">76</a>–<a href="ch12.xhtml#page_77">77</a></p>&#13;
<p class="indexmain">output layers, updating, <a href="ch18.xhtml#page_115">115</a>–<a href="ch18.xhtml#page_116">116</a></p>&#13;
<p class="indexmain">overfitting</p>&#13;
<p class="indexsub">overview, <a href="ch05.xhtml#page_23">23</a></p>&#13;
<p class="indexsub">reducing with data, <a href="ch05.xhtml#page_23">23</a>–<a href="ch05.xhtml#page_27">27</a>, <a href="appendix.xhtml#page_209">209</a>–<a href="appendix.xhtml#page_210">210</a></p>&#13;
<p class="indexsub">reducing with model modifications, <a href="ch06.xhtml#page_29">29</a>–<a href="ch06.xhtml#page_36">36</a>, <a href="appendix.xhtml#page_210">210</a></p>&#13;
<h3 class="h3i"><strong>P</strong></h3>&#13;
<p class="indexmain">parallelization</p>&#13;
<p class="indexsub">model inference, speeding up, <a href="ch22.xhtml#page_147">147</a>–<a href="ch22.xhtml#page_148">148</a></p>&#13;
<p class="indexsub">of transformers, <a href="ch08.xhtml#page_45">45</a>–<a href="ch08.xhtml#page_46">46</a></p>&#13;
<p class="indexmain">parameter-efficient fine-tuning, <a href="ch18.xhtml#page_113">113</a>, <a href="ch18.xhtml#page_119">119</a>–<a href="ch18.xhtml#page_124">124</a>, <a href="ch18.xhtml#page_125">125</a>, <a href="ch18.xhtml#page_126">126</a></p>&#13;
<p class="indexmain">parameters</p>&#13;
<p class="indexsub">calculating number of in CNNs, <a href="ch11.xhtml#page_69">69</a>–<a href="ch11.xhtml#page_73">73</a>, <a href="appendix.xhtml#page_212">212</a>–<a href="appendix.xhtml#page_213">213</a></p>&#13;
<p class="indexsub">of transformers, scale and number of, <a href="ch08.xhtml#page_45">45</a>, <a href="ch08.xhtml#page_47">47</a></p>&#13;
<p class="indexmain">patchifying inductive bias, <a href="ch13.xhtml#page_83">83</a>, <a href="ch13.xhtml#page_85">85</a>, <a href="appendix.xhtml#page_213">213</a>–<a href="appendix.xhtml#page_214">214</a></p>&#13;
<p class="indexmain">perplexity metric, <a href="ch19.xhtml#page_127">127</a>–<a href="ch19.xhtml#page_129">129</a></p>&#13;
<p class="indexmain">pipeline parallelism, <a href="ch07.xhtml#page_37">37</a>, <a href="ch07.xhtml#page_40">40</a>, <a href="ch07.xhtml#page_41">41</a>, <a href="ch07.xhtml#page_42">42</a></p>&#13;
<p class="indexmain">PixelCNN model, <a href="ch09.xhtml#page_54">54</a>, <a href="ch09.xhtml#page_58">58</a></p>&#13;
<p class="indexmain">pixel generation, autoregressive, <a href="ch09.xhtml#page_54">54</a>–<a href="ch09.xhtml#page_55">55</a></p>&#13;
<p class="indexmain">Poisson regression, <a href="ch24.xhtml#page_161">161</a>–<a href="ch24.xhtml#page_162">162</a>, <a href="appendix.xhtml#page_218">218</a>–<a href="appendix.xhtml#page_219">219</a></p>&#13;
<p class="indexmain">polysemous words, <a href="ch14.xhtml#page_90">90</a></p>&#13;
<p class="indexmain">population parameters, <a href="ch25.xhtml#page_164">164</a></p>&#13;
<p class="indexmain">positive-unlabeled learning (PU-learning), <a href="ch30.xhtml#page_198">198</a></p>&#13;
<p class="indexmain">post-training quantization, <a href="ch22.xhtml#page_151">151</a></p>&#13;
<p class="indexmain">prediction intervals, <a href="ch26.xhtml#page_173">173</a>–<a href="ch26.xhtml#page_175">175</a>, <a href="ch26.xhtml#page_178">178</a></p>&#13;
<p class="indexmain">prediction regions, <a href="ch26.xhtml#page_174">174</a>–<a href="ch26.xhtml#page_175">175</a></p>&#13;
<p class="indexmain">prediction sets, <a href="ch26.xhtml#page_174">174</a>, <a href="ch26.xhtml#page_178">178</a>, <a href="appendix.xhtml#page_219">219</a>–<a href="appendix.xhtml#page_220">220</a></p>&#13;
<p class="indexmain">predictive analytics in healthcare, <a href="ch21.xhtml#page_146">146</a>, <a href="appendix.xhtml#page_217">217</a></p>&#13;
<p class="indexmain">predictive performance and model evaluation. <em>See also</em> limited labeled data</p>&#13;
<p class="indexsub">confidence intervals vs. conformal predictions, <a href="ch26.xhtml#page_173">173</a>–<a href="ch26.xhtml#page_178">178</a>, <a href="appendix.xhtml#page_219">219</a>–<a href="appendix.xhtml#page_220">220</a></p>&#13;
<p class="indexsub">constructing confidence intervals, <a href="ch25.xhtml#page_163">163</a>–<a href="ch25.xhtml#page_171">171</a>, <a href="appendix.xhtml#page_219">219</a></p>&#13;
<p class="indexsub"><em>k</em>-fold cross-validation, <a href="ch28.xhtml#page_185">185</a>–<a href="ch28.xhtml#page_188">188</a>, <a href="appendix.xhtml#page_221">221</a></p>&#13;
<p class="indexsub">Poisson and ordinal regression, <a href="ch24.xhtml#page_161">161</a>–<a href="ch24.xhtml#page_162">162</a>, <a href="appendix.xhtml#page_218">218</a>–<a href="appendix.xhtml#page_219">219</a></p>&#13;
<p class="indexsub">proper metrics, <a href="ch27.xhtml#page_179">179</a>–<a href="ch27.xhtml#page_183">183</a>, <a href="appendix.xhtml#page_220">220</a>–<a href="appendix.xhtml#page_221">221</a></p>&#13;
<p class="indexsub">training and test set discordance, <a href="ch29.xhtml#page_189">189</a>–<a href="ch29.xhtml#page_191">191</a>, <a href="appendix.xhtml#page_221">221</a></p>&#13;
<p class="indexmain">prefix tuning, <a href="ch18.xhtml#page_119">119</a>, <a href="ch18.xhtml#page_120">120</a>–<a href="ch18.xhtml#page_121">121</a>, <a href="ch18.xhtml#page_125">125</a>, <a href="ch18.xhtml#page_126">126</a>, <a href="appendix.xhtml#page_216">216</a></p>&#13;
<p class="indexmain">pretext tasks, <a href="ch02.xhtml#page_10">10</a></p>&#13;
<p class="indexmain">pretrained transformers</p>&#13;
<p class="indexsub">adapting, <a href="ch18.xhtml#page_124">124</a>–<a href="ch18.xhtml#page_125">125</a></p>&#13;
<p class="indexsub">classification tasks, <a href="ch18.xhtml#page_113">113</a>–<a href="ch18.xhtml#page_116">116</a></p>&#13;
<p class="indexsub">in-context learning, indexing, and prompt tuning, <a href="ch18.xhtml#page_116">116</a>–<a href="ch18.xhtml#page_119">119</a></p>&#13;
<p class="indexsub">overview, <a href="ch18.xhtml#page_113">113</a></p>&#13;
<p class="indexsub">parameter-efficient fine-tuning, <a href="ch18.xhtml#page_119">119</a>–<a href="ch18.xhtml#page_124">124</a></p>&#13;
<p class="indexsub">reinforcement learning with human feedback (RLHF), <a href="ch18.xhtml#page_124">124</a></p>&#13;
<p class="indexmain">pretraining</p>&#13;
<p class="indexsub">encoder-only architectures, <a href="ch17.xhtml#page_107">107</a>–<a href="ch17.xhtml#page_108">108</a></p>&#13;
<p class="indexsub">to reduce overfitting, <a href="ch05.xhtml#page_25">25</a></p>&#13;
<p class="indexsub">with self-supervised learning, <a href="ch02.xhtml#page_10">10</a>–<a href="ch02.xhtml#page_11">11</a></p>&#13;
<p class="indexsub">with transfer learning, <a href="ch02.xhtml#page_9">9</a>–<a href="ch02.xhtml#page_10">10</a></p>&#13;
<p class="indexsub">transformers, via self-supervised learning, <a href="ch08.xhtml#page_45">45</a></p>&#13;
<p class="indexsub">for vision transformers, <a href="ch13.xhtml#page_83">83</a></p>&#13;
<p class="indexmain">prior probability shift (label shift), <a href="ch23.xhtml#page_154">154</a>–<a href="ch23.xhtml#page_155">155</a>, <a href="ch23.xhtml#page_156">156</a></p>&#13;
<p class="indexmain">production and deployment</p>&#13;
<p class="indexsub">data distribution shifts, <a href="ch23.xhtml#page_153">153</a>–<a href="ch23.xhtml#page_157">157</a>, <a href="appendix.xhtml#page_218">218</a></p>&#13;
<p class="indexsub">model inference, speeding up, <a href="ch22.xhtml#page_147">147</a>–<a href="ch22.xhtml#page_152">152</a>, <a href="appendix.xhtml#page_218">218</a></p>&#13;
<p class="indexsub">stateless and stateful training, <a href="ch20.xhtml#page_139">139</a>–<a href="ch20.xhtml#page_141">141</a>, <a href="appendix.xhtml#page_217">217</a></p>&#13;
<p class="indexmain">prompt tuning, <a href="ch18.xhtml#page_117">117</a>–<a href="ch18.xhtml#page_118">118</a></p>&#13;
<p class="indexmain"><span epub:type="pagebreak" id="page_231"/>proper metrics</p>&#13;
<p class="indexsub">criteria for, <a href="ch27.xhtml#page_179">179</a>–<a href="ch27.xhtml#page_180">180</a></p>&#13;
<p class="indexsub">cross-entropy loss, <a href="ch27.xhtml#page_182">182</a></p>&#13;
<p class="indexsub">mean squared error loss, <a href="ch27.xhtml#page_180">180</a>–<a href="ch27.xhtml#page_181">181</a></p>&#13;
<p class="indexsub">overview, <a href="ch27.xhtml#page_179">179</a></p>&#13;
<p class="indexmain">protein modeling, <a href="appendix.xhtml#page_214">214</a></p>&#13;
<p class="indexmain">proximal policy optimization, <a href="ch18.xhtml#page_124">124</a>, <a href="ch18.xhtml#page_126">126</a></p>&#13;
<p class="indexmain">pruning, <a href="ch06.xhtml#page_31">31</a>, <a href="ch06.xhtml#page_32">32</a>–<a href="ch06.xhtml#page_33">33</a>, <a href="ch06.xhtml#page_35">35</a>, <a href="ch06.xhtml#page_36">36</a>, <a href="ch22.xhtml#page_151">151</a></p>&#13;
<p class="indexmain">pseudo-labelers, <a href="ch30.xhtml#page_199">199</a> PU-learning (positive-unlabeled learning), <a href="ch30.xhtml#page_198">198</a></p>&#13;
<p class="indexmain">PyTorch framework, <a href="ch10.xhtml#page_59">59</a>, <a href="ch10.xhtml#page_61">61</a>, <a href="ch10.xhtml#page_62">62</a>, <a href="ch10.xhtml#page_65">65</a>, <a href="ch22.xhtml#page_149">149</a></p>&#13;
<h3 class="h3i"><strong>Q</strong></h3>&#13;
<p class="indexmain">quantization, <a href="ch22.xhtml#page_151">151</a>, <a href="ch22.xhtml#page_152">152</a></p>&#13;
<p class="indexmain">quantization-aware training, <a href="ch22.xhtml#page_151">151</a></p>&#13;
<h3 class="h3i"><strong>R</strong></h3>&#13;
<p class="indexmain">random characters, <a href="ch15.xhtml#page_95">95</a></p>&#13;
<p class="indexmain">random initialization, <a href="appendix.xhtml#page_209">209</a></p>&#13;
<p class="indexmain">randomness, sources of</p>&#13;
<p class="indexsub">dataset sampling and shuffling, <a href="ch10.xhtml#page_60">60</a></p>&#13;
<p class="indexsub">different runtime algorithms, <a href="ch10.xhtml#page_61">61</a>–<a href="ch10.xhtml#page_62">62</a></p>&#13;
<p class="indexsub">and generative AI, <a href="ch10.xhtml#page_62">62</a>–<a href="ch10.xhtml#page_64">64</a></p>&#13;
<p class="indexsub">hardware and drivers, <a href="ch10.xhtml#page_62">62</a></p>&#13;
<p class="indexsub">model weight initialization, <a href="ch10.xhtml#page_59">59</a>–<a href="ch10.xhtml#page_60">60</a></p>&#13;
<p class="indexsub">nondeterministic algorithms, <a href="ch10.xhtml#page_61">61</a></p>&#13;
<p class="indexsub">overview, <a href="ch10.xhtml#page_59">59</a></p>&#13;
<p class="indexmain">random seeds, <a href="ch25.xhtml#page_169">169</a>–<a href="ch25.xhtml#page_170">170</a></p>&#13;
<p class="indexmain">recall-oriented understudy for gisting evaluation (ROUGE) score, <a href="ch19.xhtml#page_128">128</a>, <a href="ch19.xhtml#page_131">131</a>–<a href="ch19.xhtml#page_132">132</a>, <a href="ch19.xhtml#page_133">133</a>, <a href="ch19.xhtml#page_134">134</a></p>&#13;
<p class="indexmain">reconstruction error, measuring, <a href="appendix.xhtml#page_218">218</a></p>&#13;
<p class="indexmain">reconstruction loss, <a href="ch09.xhtml#page_52">52</a></p>&#13;
<p class="indexmain">rectified linear unit (ReLU) activation function, <a href="ch04.xhtml#page_21">21</a>, <a href="appendix.xhtml#page_209">209</a></p>&#13;
<p class="indexmain">recurrent neural networks (RNNs), <a href="ch16.xhtml#page_99">99</a>–<a href="ch16.xhtml#page_101">101</a>, <a href="ch16.xhtml#page_103">103</a>, <a href="ch17.xhtml#page_112">112</a>. <em>See also</em> neural networks</p>&#13;
<p class="indexmain">reducing overfitting</p>&#13;
<p class="indexsub">with data, <a href="ch05.xhtml#page_23">23</a>–<a href="ch05.xhtml#page_27">27</a>, <a href="appendix.xhtml#page_209">209</a>–<a href="appendix.xhtml#page_210">210</a></p>&#13;
<p class="indexsub">with model modifications, <a href="ch06.xhtml#page_29">29</a>–<a href="ch06.xhtml#page_36">36</a>, <a href="appendix.xhtml#page_210">210</a></p>&#13;
<p class="indexmain">regression, conformal prediction and confidence intervals for, <a href="ch26.xhtml#page_178">178</a>, <a href="appendix.xhtml#page_220">220</a></p>&#13;
<p class="indexmain">regularization, reducing overfitting with, <a href="ch06.xhtml#page_30">30</a>–<a href="ch06.xhtml#page_31">31</a>, <a href="ch06.xhtml#page_36">36</a></p>&#13;
<p class="indexmain">reinforcement learning with human feedback (RLHF), <a href="ch18.xhtml#page_124">124</a></p>&#13;
<p class="indexmain">relative positional embeddings (relative positional encodings), <a href="ch13.xhtml#page_82">82</a>, <a href="ch13.xhtml#page_85">85</a></p>&#13;
<p class="indexmain">ReLU (rectified linear unit) activation function, <a href="ch04.xhtml#page_21">21</a>, <a href="appendix.xhtml#page_209">209</a></p>&#13;
<p class="indexmain">reparameterization, <a href="ch22.xhtml#page_151">151</a></p>&#13;
<p class="indexmain">representation learning, <a href="ch02.xhtml#page_11">11</a></p>&#13;
<p class="indexmain">representations, <a href="ch01.xhtml#page_3">3</a>, <a href="ch01.xhtml#page_6">6</a>–<a href="ch01.xhtml#page_7">7</a></p>&#13;
<p class="indexmain">RepVGG CNN architecture, <a href="ch22.xhtml#page_151">151</a>, <a href="ch22.xhtml#page_152">152</a></p>&#13;
<p class="indexmain">residual connection in transformer architecture, <a href="ch17.xhtml#page_107">107</a></p>&#13;
<p class="indexmain">ResNet-34 convolutional neural networks, <a href="ch21.xhtml#page_146">146</a>, <a href="appendix.xhtml#page_217">217</a></p>&#13;
<p class="indexmain">resources, online, <a href="ch00.xhtml#page_xxviii">xxviii</a></p>&#13;
<p class="indexmain">retraining</p>&#13;
<p class="indexsub">with different random seeds, <a href="ch25.xhtml#page_169">169</a>–<a href="ch25.xhtml#page_170">170</a></p>&#13;
<p class="indexsub">stateless, <a href="ch20.xhtml#page_139">139</a>–<a href="ch20.xhtml#page_140">140</a>, <a href="ch20.xhtml#page_141">141</a>, <a href="appendix.xhtml#page_217">217</a></p>&#13;
<p class="indexmain">RLHF (reinforcement learning with human feedback), <a href="ch18.xhtml#page_124">124</a></p>&#13;
<p class="indexmain">RNNs (recurrent neural networks), <a href="ch16.xhtml#page_99">99</a>–<a href="ch16.xhtml#page_101">101</a>, <a href="ch16.xhtml#page_103">103</a>, <a href="ch17.xhtml#page_112">112</a>. <em>See also</em> neural networks</p>&#13;
<p class="indexmain">RoBERTa (robustly optimized BERT approach), <a href="ch17.xhtml#page_108">108</a>, <a href="ch17.xhtml#page_112">112</a></p>&#13;
<p class="indexmain">root mean square error (RMSE), <a href="ch27.xhtml#page_183">183</a>, <a href="appendix.xhtml#page_220">220</a>–<a href="appendix.xhtml#page_221">221</a></p>&#13;
<p class="indexmain">root-squared error, <a href="ch27.xhtml#page_181">181</a></p>&#13;
<p class="indexmain">ROUGE (recall-oriented understudy for gisting evaluation) score, <a href="ch19.xhtml#page_128">128</a>, <a href="ch19.xhtml#page_131">131</a>–<a href="ch19.xhtml#page_132">132</a>, <a href="ch19.xhtml#page_133">133</a>, <a href="ch19.xhtml#page_134">134</a></p>&#13;
<p class="indexmain">runtime algorithms as source of randomness, <a href="ch10.xhtml#page_61">61</a>–<a href="ch10.xhtml#page_62">62</a></p>&#13;
<h3 class="h3i"><strong>S</strong></h3>&#13;
<p class="indexmain">SAINT method, <a href="appendix.xhtml#page_208">208</a></p>&#13;
<p class="indexmain">sample contrastive self-supervised learning, <a href="ch02.xhtml#page_14">14</a></p>&#13;
<p class="indexmain">sampling as source of randomness, <a href="ch10.xhtml#page_60">60</a>, <a href="ch10.xhtml#page_65">65</a></p>&#13;
<p class="indexmain">sanity check, <a href="ch29.xhtml#page_189">189</a></p>&#13;
<p class="indexmain">scaled-dot product attention, <a href="ch07.xhtml#page_40">40</a>, <a href="ch07.xhtml#page_42">42</a>. <em>See also</em> self-attention mechanism</p>&#13;
<p class="indexmain">SCARF method, <a href="appendix.xhtml#page_208">208</a></p>&#13;
<p class="indexmain">score method of conformal prediction, <a href="ch26.xhtml#page_176">176</a>–<a href="ch26.xhtml#page_177">177</a>, <a href="ch26.xhtml#page_178">178</a></p>&#13;
<p class="indexmain">SE (squared error) loss, <a href="ch27.xhtml#page_181">181</a></p>&#13;
<p class="indexmain"><span epub:type="pagebreak" id="page_232"/>seeding random generator, <a href="ch10.xhtml#page_60">60</a>, <a href="ch10.xhtml#page_61">61</a></p>&#13;
<p class="indexmain">self-attention mechanism. <em>See also</em> transformers</p>&#13;
<p class="indexsub">vs. Bahdanau attention mechanism, <a href="ch16.xhtml#page_99">99</a>–<a href="ch16.xhtml#page_101">101</a></p>&#13;
<p class="indexsub">overview, <a href="ch16.xhtml#page_99">99</a>, <a href="ch16.xhtml#page_101">101</a>–<a href="ch16.xhtml#page_102">102</a></p>&#13;
<p class="indexsub">sequence parallelism, <a href="ch07.xhtml#page_40">40</a></p>&#13;
<p class="indexsub">transformers, <a href="ch07.xhtml#page_42">42</a>, <a href="ch08.xhtml#page_43">43</a>–<a href="ch08.xhtml#page_45">45</a>, <a href="ch08.xhtml#page_46">46</a>, <a href="ch08.xhtml#page_47">47</a></p>&#13;
<p class="indexsub">in vision transformers, <a href="ch13.xhtml#page_83">83</a>–<a href="ch13.xhtml#page_84">84</a></p>&#13;
<p class="indexmain">self-prediction, <a href="ch02.xhtml#page_11">11</a>–<a href="ch02.xhtml#page_12">12</a></p>&#13;
<p class="indexmain">self-supervised learning</p>&#13;
<p class="indexsub">contrastive, <a href="ch02.xhtml#page_12">12</a>–<a href="ch02.xhtml#page_14">14</a></p>&#13;
<p class="indexsub">encoder-only architectures, <a href="ch17.xhtml#page_108">108</a></p>&#13;
<p class="indexsub">leveraging unlabeled data, <a href="ch02.xhtml#page_11">11</a></p>&#13;
<p class="indexsub">limited labeled data, <a href="ch30.xhtml#page_194">194</a>–<a href="ch30.xhtml#page_195">195</a>, <a href="ch30.xhtml#page_203">203</a>, <a href="ch30.xhtml#page_204">204</a>, <a href="appendix.xhtml#page_221">221</a>–<a href="appendix.xhtml#page_222">222</a></p>&#13;
<p class="indexsub">overview, <a href="ch02.xhtml#page_9">9</a></p>&#13;
<p class="indexsub">pretraining transformers via, <a href="ch08.xhtml#page_45">45</a></p>&#13;
<p class="indexsub">reducing overfitting with, <a href="ch05.xhtml#page_25">25</a></p>&#13;
<p class="indexsub">self-prediction, <a href="ch02.xhtml#page_11">11</a>–<a href="ch02.xhtml#page_14">14</a></p>&#13;
<p class="indexsub">vs. transfer learning, <a href="ch02.xhtml#page_9">9</a>–<a href="ch02.xhtml#page_11">11</a></p>&#13;
<p class="indexmain">self-training, <a href="ch30.xhtml#page_199">199</a>. <em>See also</em> knowledge distillation</p>&#13;
<p class="indexmain">semi-supervised learning, <a href="ch30.xhtml#page_198">198</a>–<a href="ch30.xhtml#page_199">199</a>, <a href="ch30.xhtml#page_203">203</a></p>&#13;
<p class="indexmain">sentence shuffling, <a href="ch15.xhtml#page_95">95</a></p>&#13;
<p class="indexmain">[SEP] token, <a href="ch17.xhtml#page_108">108</a></p>&#13;
<p class="indexmain">sequence parallelism, <a href="ch07.xhtml#page_40">40</a>–<a href="ch07.xhtml#page_41">41</a>, <a href="ch07.xhtml#page_42">42</a></p>&#13;
<p class="indexmain">sequence-to-sequence (seq2seq) models, <a href="ch17.xhtml#page_107">107</a>–<a href="ch17.xhtml#page_110">110</a></p>&#13;
<p class="indexmain">sequential inference, <a href="ch22.xhtml#page_148">148</a></p>&#13;
<p class="indexmain">SGD (stochastic gradient descent) optimizer, <a href="ch11.xhtml#page_73">73</a>, <a href="appendix.xhtml#page_212">212</a></p>&#13;
<p class="indexmain">shortcut connection, <a href="ch17.xhtml#page_107">107</a></p>&#13;
<p class="indexmain">siamese network setup, <a href="ch02.xhtml#page_13">13</a></p>&#13;
<p class="indexmain">similarity, embeddings as encoding, <a href="ch01.xhtml#page_5">5</a></p>&#13;
<p class="indexmain">.632 bootstrap, <a href="ch25.xhtml#page_171">171</a></p>&#13;
<p class="indexmain">skip connection in transformer architecture, <a href="ch17.xhtml#page_107">107</a></p>&#13;
<p class="indexmain">skip-gram approach, Word2vec, <a href="ch14.xhtml#page_90">90</a></p>&#13;
<p class="indexmain">smaller models, reducing overfitting with, <a href="ch06.xhtml#page_31">31</a>–<a href="ch06.xhtml#page_33">33</a></p>&#13;
<p class="indexmain">soft attention, <a href="appendix.xhtml#page_211">211</a></p>&#13;
<p class="indexmain">soft parameter sharing, <a href="ch30.xhtml#page_200">200</a></p>&#13;
<p class="indexmain">soft prompting, <a href="ch18.xhtml#page_119">119</a>–<a href="ch18.xhtml#page_121">121</a>, <a href="ch18.xhtml#page_125">125</a></p>&#13;
<p class="indexmain">sources of randomness</p>&#13;
<p class="indexsub">dataset sampling and shuffling, <a href="ch10.xhtml#page_60">60</a></p>&#13;
<p class="indexsub">different runtime algorithms, <a href="ch10.xhtml#page_61">61</a>–<a href="ch10.xhtml#page_62">62</a></p>&#13;
<p class="indexsub">and generative AI, <a href="ch10.xhtml#page_62">62</a>–<a href="ch10.xhtml#page_64">64</a></p>&#13;
<p class="indexsub">hardware and drivers, <a href="ch10.xhtml#page_62">62</a></p>&#13;
<p class="indexsub">model weight initialization, <a href="ch10.xhtml#page_59">59</a>–<a href="ch10.xhtml#page_60">60</a></p>&#13;
<p class="indexsub">nondeterministic algorithms, <a href="ch10.xhtml#page_61">61</a></p>&#13;
<p class="indexsub">overview, <a href="ch10.xhtml#page_59">59</a></p>&#13;
<p class="indexmain">spatial attention, <a href="appendix.xhtml#page_215">215</a></p>&#13;
<p class="indexmain">spatial invariance, <a href="ch13.xhtml#page_80">80</a>–<a href="ch13.xhtml#page_82">82</a></p>&#13;
<p class="indexmain">speeding up inference. <em>See</em> model inference, speeding up</p>&#13;
<p class="indexmain">squared error (SE) loss, <a href="ch27.xhtml#page_181">181</a></p>&#13;
<p class="indexmain">Stable Diffusion latent diffusion model, <a href="ch09.xhtml#page_58">58</a></p>&#13;
<p class="indexmain">stacking (stacked generalization), <a href="ch06.xhtml#page_33">33</a></p>&#13;
<p class="indexmain">stateful training, <a href="ch20.xhtml#page_139">139</a>, <a href="ch20.xhtml#page_140">140</a>–<a href="ch20.xhtml#page_141">141</a>, <a href="appendix.xhtml#page_217">217</a></p>&#13;
<p class="indexmain">stateless training (stateless retraining), <a href="ch20.xhtml#page_139">139</a>–<a href="ch20.xhtml#page_140">140</a>, <a href="ch20.xhtml#page_141">141</a>, <a href="appendix.xhtml#page_217">217</a></p>&#13;
<p class="indexmain">statistical population, <a href="ch25.xhtml#page_164">164</a></p>&#13;
<p class="indexmain">statistical two-sample tests, <a href="appendix.xhtml#page_218">218</a></p>&#13;
<p class="indexmain">stochastic diffusion process, <a href="ch09.xhtml#page_56">56</a></p>&#13;
<p class="indexmain">stochastic gradient descent (SGD) optimizer, <a href="ch11.xhtml#page_73">73</a>, <a href="appendix.xhtml#page_212">212</a></p>&#13;
<p class="indexmain">stride, <a href="ch12.xhtml#page_78">78</a>, <a href="appendix.xhtml#page_213">213</a></p>&#13;
<p class="indexmain">structured pruning, <a href="ch04.xhtml#page_20">20</a></p>&#13;
<p class="indexmain">student in knowledge distillation, <a href="ch06.xhtml#page_31">31</a>–<a href="ch06.xhtml#page_32">32</a></p>&#13;
<p class="indexmain">supervised learning, <a href="ch03.xhtml#page_15">15</a>. <em>See also</em> limited labeled data</p>&#13;
<p class="indexmain">support set in few-shot learning, <a href="ch03.xhtml#page_16">16</a></p>&#13;
<p class="indexmain">synonym replacement (text augmentation), <a href="ch15.xhtml#page_93">93</a>–<a href="ch15.xhtml#page_94">94</a></p>&#13;
<p class="indexmain">synthetic data generation, <a href="ch15.xhtml#page_96">96</a>–<a href="ch15.xhtml#page_97">97</a></p>&#13;
<h3 class="h3i"><strong>T</strong></h3>&#13;
<p class="indexmain">TabNet, <a href="appendix.xhtml#page_208">208</a></p>&#13;
<p class="indexmain">tabular data, self-supervised learning for, <a href="ch02.xhtml#page_14">14</a>, <a href="appendix.xhtml#page_208">208</a></p>&#13;
<p class="indexmain">teacher in knowledge distillation, <a href="ch06.xhtml#page_31">31</a>–<a href="ch06.xhtml#page_32">32</a></p>&#13;
<p class="indexmain">10-fold cross-validation, <a href="ch28.xhtml#page_187">187</a>, <a href="ch28.xhtml#page_188">188</a></p>&#13;
<p class="indexmain">TensorFlow framework, <a href="ch10.xhtml#page_59">59</a>, <a href="ch10.xhtml#page_62">62</a>, <a href="ch22.xhtml#page_149">149</a></p>&#13;
<p class="indexmain">tensor parallelism, <a href="ch07.xhtml#page_37">37</a>, <a href="ch07.xhtml#page_38">38</a>–<a href="ch07.xhtml#page_40">40</a>, <a href="ch07.xhtml#page_41">41</a>–<a href="ch07.xhtml#page_42">42</a>, <a href="appendix.xhtml#page_211">211</a></p>&#13;
<p class="indexmain">test sets</p>&#13;
<p class="indexsub">bootstrapping, <a href="ch25.xhtml#page_169">169</a>, <a href="ch25.xhtml#page_170">170</a>, <a href="ch25.xhtml#page_171">171</a></p>&#13;
<p class="indexsub">conformal predictions, <a href="ch26.xhtml#page_176">176</a></p>&#13;
<p class="indexsub">discordance with training sets, <a href="ch29.xhtml#page_189">189</a>–<a href="ch29.xhtml#page_191">191</a>, <a href="appendix.xhtml#page_221">221</a></p>&#13;
<p class="indexmain">text, data augmentation for, <a href="ch15.xhtml#page_93">93</a>–<a href="ch15.xhtml#page_97">97</a>, <a href="appendix.xhtml#page_214">214</a>–<a href="appendix.xhtml#page_215">215</a></p>&#13;
<p class="indexmain">T5 encoder-decoder architecture, <a href="ch17.xhtml#page_112">112</a></p>&#13;
<p class="indexmain"><span epub:type="pagebreak" id="page_233"/>time complexity of self-attention, <a href="ch16.xhtml#page_103">103</a>, <a href="appendix.xhtml#page_215">215</a></p>&#13;
<p class="indexmain">top-<em>k</em> sampling, <a href="ch10.xhtml#page_63">63</a>–<a href="ch10.xhtml#page_64">64</a>, <a href="appendix.xhtml#page_212">212</a></p>&#13;
<p class="indexmain">training. <em>See also</em> multi-GPU training paradigms; pretraining; randomness, sources of; retraining</p>&#13;
<p class="indexsub">epochs, tuning number of, <a href="ch06.xhtml#page_35">35</a>, <a href="appendix.xhtml#page_210">210</a></p>&#13;
<p class="indexsub">post-training quantization, <a href="ch22.xhtml#page_151">151</a></p>&#13;
<p class="indexsub">procedure for lottery ticket hypothesis, <a href="ch04.xhtml#page_19">19</a>–<a href="ch04.xhtml#page_20">20</a></p>&#13;
<p class="indexsub">quantization-aware, <a href="ch22.xhtml#page_151">151</a></p>&#13;
<p class="indexsub">self-training, <a href="ch30.xhtml#page_199">199</a></p>&#13;
<p class="indexsub">stateless and stateful, <a href="ch20.xhtml#page_139">139</a>–<a href="ch20.xhtml#page_141">141</a>, <a href="appendix.xhtml#page_217">217</a></p>&#13;
<p class="indexmain">training sets</p>&#13;
<p class="indexsub">conformal predictions, <a href="ch26.xhtml#page_176">176</a>, <a href="ch26.xhtml#page_177">177</a></p>&#13;
<p class="indexsub">discordance with test sets, <a href="ch29.xhtml#page_189">189</a>–<a href="ch29.xhtml#page_191">191</a>, <a href="appendix.xhtml#page_221">221</a></p>&#13;
<p class="indexsub">for vision transformers, <a href="ch13.xhtml#page_79">79</a>–<a href="ch13.xhtml#page_85">85</a>, <a href="appendix.xhtml#page_213">213</a>–<a href="appendix.xhtml#page_214">214</a></p>&#13;
<p class="indexmain">transfer learning</p>&#13;
<p class="indexsub">limited labeled data, <a href="ch30.xhtml#page_194">194</a>, <a href="ch30.xhtml#page_203">203</a>, <a href="ch30.xhtml#page_204">204</a>, <a href="appendix.xhtml#page_221">221</a>–<a href="appendix.xhtml#page_222">222</a></p>&#13;
<p class="indexsub">reducing overfitting with, <a href="ch05.xhtml#page_25">25</a>, <a href="ch05.xhtml#page_26">26</a>, <a href="appendix.xhtml#page_209">209</a></p>&#13;
<p class="indexsub">vs. self-supervised learning, <a href="ch02.xhtml#page_9">9</a>–<a href="ch02.xhtml#page_11">11</a></p>&#13;
<p class="indexmain">transformers. <em>See also</em> self-attention mechanism</p>&#13;
<p class="indexsub">adapting pretrained language models, <a href="ch18.xhtml#page_124">124</a>–<a href="ch18.xhtml#page_125">125</a></p>&#13;
<p class="indexsub">attention mechanism, <a href="ch07.xhtml#page_40">40</a>, <a href="ch08.xhtml#page_43">43</a>–<a href="ch08.xhtml#page_45">45</a></p>&#13;
<p class="indexsub">classification tasks, <a href="ch18.xhtml#page_113">113</a>–<a href="ch18.xhtml#page_116">116</a></p>&#13;
<p class="indexsub">contemporary models, <a href="ch17.xhtml#page_111">111</a>–<a href="ch17.xhtml#page_112">112</a></p>&#13;
<p class="indexsub">decoders, <a href="ch17.xhtml#page_108">108</a>–<a href="ch17.xhtml#page_110">110</a></p>&#13;
<p class="indexsub">encoder-decoder hybrids, <a href="ch17.xhtml#page_110">110</a></p>&#13;
<p class="indexsub">encoders, <a href="ch17.xhtml#page_107">107</a>–<a href="ch17.xhtml#page_108">108</a></p>&#13;
<p class="indexsub">in-context learning, indexing, and prompt tuning, <a href="ch18.xhtml#page_116">116</a>–<a href="ch18.xhtml#page_119">119</a></p>&#13;
<p class="indexsub">multi-GPU training paradigms, <a href="ch07.xhtml#page_40">40</a>, <a href="ch07.xhtml#page_42">42</a></p>&#13;
<p class="indexsub">number of parameters, <a href="ch08.xhtml#page_45">45</a></p>&#13;
<p class="indexsub">original architecture for, <a href="ch17.xhtml#page_105">105</a>–<a href="ch17.xhtml#page_110">110</a></p>&#13;
<p class="indexsub">overview, <a href="ch17.xhtml#page_105">105</a>, <a href="ch18.xhtml#page_113">113</a></p>&#13;
<p class="indexsub">parallelization, <a href="ch08.xhtml#page_45">45</a>–<a href="ch08.xhtml#page_46">46</a></p>&#13;
<p class="indexsub">parameter-efficient fine-tuning, <a href="ch18.xhtml#page_119">119</a>–<a href="ch18.xhtml#page_124">124</a></p>&#13;
<p class="indexsub">pretraining via self-supervised learning, <a href="ch08.xhtml#page_45">45</a></p>&#13;
<p class="indexsub">reinforcement learning with human feedback (RLHF), <a href="ch18.xhtml#page_124">124</a></p>&#13;
<p class="indexsub">success of, <a href="ch08.xhtml#page_43">43</a>–<a href="ch08.xhtml#page_47">47</a></p>&#13;
<p class="indexsub">terminology, <a href="ch17.xhtml#page_110">110</a></p>&#13;
<p class="indexsub">transfer learning, <a href="ch02.xhtml#page_11">11</a></p>&#13;
<p class="indexmain">translation</p>&#13;
<p class="indexsub">back, <a href="ch15.xhtml#page_96">96</a></p>&#13;
<p class="indexsub">invariance and equivariance, <a href="ch13.xhtml#page_80">80</a>–<a href="ch13.xhtml#page_82">82</a></p>&#13;
<p class="indexsub">tokens, <a href="ch08.xhtml#page_44">44</a>–<a href="ch08.xhtml#page_46">46</a>, <a href="ch10.xhtml#page_63">63</a>–<a href="ch10.xhtml#page_64">64</a>, <a href="ch16.xhtml#page_102">102</a>, <a href="ch17.xhtml#page_106">106</a>–<a href="ch17.xhtml#page_110">110</a>, <a href="ch18.xhtml#page_117">117</a>–<a href="ch18.xhtml#page_118">118</a></p>&#13;
<p class="indexmain">triangle inequality, <a href="ch27.xhtml#page_180">180</a>, <a href="ch27.xhtml#page_181">181</a>, <a href="ch27.xhtml#page_182">182</a></p>&#13;
<p class="indexmain">true generalization accuracy, <a href="ch25.xhtml#page_164">164</a></p>&#13;
<p class="indexmain">two-dimensional embeddings, <a href="ch01.xhtml#page_4">4</a>–<a href="ch01.xhtml#page_5">5</a></p>&#13;
<p class="indexmain">typo introduction, <a href="ch15.xhtml#page_95">95</a></p>&#13;
<h3 class="h3i"><strong>U</strong></h3>&#13;
<p class="indexmain">unlabeled data in self-supervised learning, <a href="ch02.xhtml#page_10">10</a>, <a href="ch02.xhtml#page_11">11</a></p>&#13;
<p class="indexmain">unstructured pruning, <a href="ch04.xhtml#page_20">20</a></p>&#13;
<p class="indexmain">unsupervised pretraining. <em>See</em> self-supervised learning</p>&#13;
<h3 class="h3i"><strong>V</strong></h3>&#13;
<p class="indexmain">variational autoencoders (VAEs), <a href="ch09.xhtml#page_51">51</a>–<a href="ch09.xhtml#page_52">52</a>, <a href="ch09.xhtml#page_53">53</a>, <a href="ch09.xhtml#page_54">54</a>, <a href="ch09.xhtml#page_57">57</a>, <a href="ch09.xhtml#page_58">58</a></p>&#13;
<p class="indexmain">variational inference, <a href="ch09.xhtml#page_51">51</a></p>&#13;
<p class="indexmain">vectorization, <a href="ch22.xhtml#page_148">148</a>–<a href="ch22.xhtml#page_149">149</a>, <a href="ch22.xhtml#page_152">152</a>, <a href="appendix.xhtml#page_218">218</a></p>&#13;
<p class="indexmain">VideoBERT model, <a href="ch30.xhtml#page_201">201</a>, <a href="ch30.xhtml#page_204">204</a></p>&#13;
<p class="indexmain">video data, applying self-supervised learning to, <a href="ch02.xhtml#page_14">14</a>, <a href="appendix.xhtml#page_208">208</a></p>&#13;
<p class="indexmain">vision transformers (ViTs)</p>&#13;
<p class="indexsub">vs. convolutional neural networks, <a href="ch13.xhtml#page_79">79</a>, <a href="ch13.xhtml#page_82">82</a>–<a href="ch13.xhtml#page_83">83</a>, <a href="ch13.xhtml#page_84">84</a></p>&#13;
<p class="indexsub">inductive biases in, <a href="ch13.xhtml#page_83">83</a>–<a href="ch13.xhtml#page_84">84</a></p>&#13;
<p class="indexsub">large training sets for, <a href="ch13.xhtml#page_79">79</a></p>&#13;
<p class="indexsub">positional information in, <a href="ch13.xhtml#page_82">82</a>, <a href="ch13.xhtml#page_85">85</a></p>&#13;
<p class="indexsub">recommendations for, <a href="ch13.xhtml#page_84">84</a></p>&#13;
<h3 class="h3i"><strong>W</strong></h3>&#13;
<p class="indexmain">weakly supervised learning, <a href="ch30.xhtml#page_197">197</a>–<a href="ch30.xhtml#page_199">199</a>, <a href="ch30.xhtml#page_203">203</a></p>&#13;
<p class="indexmain">weight decay, <a href="ch06.xhtml#page_30">30</a>, <a href="ch06.xhtml#page_35">35</a></p>&#13;
<p class="indexmain">weighted loss function, <a href="ch23.xhtml#page_155">155</a></p>&#13;
<p class="indexmain">weight initialization, <a href="ch10.xhtml#page_59">59</a>–<a href="ch10.xhtml#page_60">60</a></p>&#13;
<p class="indexmain">weight normalization, <a href="ch06.xhtml#page_34">34</a>–<a href="ch06.xhtml#page_35">35</a></p>&#13;
<p class="indexmain"><span epub:type="pagebreak" id="page_234"/>weight pruning, <a href="ch04.xhtml#page_19">19</a>, <a href="ch04.xhtml#page_20">20</a></p>&#13;
<p class="indexmain">weights</p>&#13;
<p class="indexsub">in convolutional layers, <a href="ch11.xhtml#page_70">70</a>–<a href="ch11.xhtml#page_71">71</a>, <a href="ch12.xhtml#page_76">76</a>–<a href="ch12.xhtml#page_77">77</a></p>&#13;
<p class="indexsub">in fully connected layers, <a href="ch11.xhtml#page_72">72</a>, <a href="ch12.xhtml#page_76">76</a></p>&#13;
<p class="indexmain">weight sharing, <a href="ch13.xhtml#page_80">80</a>, <a href="ch13.xhtml#page_81">81</a></p>&#13;
<p class="indexmain">winning tickets (lottery ticket hypothesis), <a href="ch04.xhtml#page_20">20</a>, <a href="ch04.xhtml#page_21">21</a></p>&#13;
<p class="indexmain">Winograd-based convolution, <a href="ch10.xhtml#page_62">62</a>, <a href="ch10.xhtml#page_65">65</a></p>&#13;
<p class="indexmain">word deletion (text augmentation), <a href="ch15.xhtml#page_94">94</a></p>&#13;
<p class="indexmain">word embeddings. <em>See</em> embeddings</p>&#13;
<p class="indexmain">WordNet thesaurus, <a href="ch15.xhtml#page_94">94</a>, <a href="ch15.xhtml#page_97">97</a></p>&#13;
<p class="indexmain">word position swapping (word shuffling/permutation), <a href="ch15.xhtml#page_94">94</a>–<a href="ch15.xhtml#page_95">95</a></p>&#13;
<p class="indexmain">Word2vec model, <a href="ch14.xhtml#page_90">90</a>, <a href="ch14.xhtml#page_92">92</a></p>&#13;
<h3 class="h3i"><strong>X</strong></h3>&#13;
<p class="indexmain">XGBoost model, <a href="ch05.xhtml#page_26">26</a>, <a href="appendix.xhtml#page_209">209</a></p>&#13;
<h3 class="h3i"><strong>Z</strong></h3>&#13;
<p class="indexmain">zero-shot learning, <a href="ch30.xhtml#page_195">195</a>–<a href="ch30.xhtml#page_196">196</a>. <em>See also</em> few-shot learning; in-context learning</p>&#13;
<p class="indexmain"><em>z</em>-scores (confidence intervals), <a href="ch25.xhtml#page_166">166</a></p>&#13;
</div>
</div>
</body></html>