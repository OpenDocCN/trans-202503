- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tools for Topological Data Analysis
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/book_art/chapterart.png)'
  prefs: []
  type: TYPE_IMG
- en: In this chapter, we’ll explore algorithms that have a direct basis in topology
    and use them to understand the dataset of self-reported educational data encountered
    in [Chapter 6](c06.xhtml). The branch of machine learning that includes topology-based
    algorithms is called *topological data analysis (TDA)*. You already saw some TDA
    in [Chapter 4](c04.xhtml), where we used persistent homology to explore network
    differences. Persistent homology has gained a lot of attention in the machine
    learning community lately and has been used in psychometric data validation, image
    comparison analyses, pooling steps of convolutional neural networks, and comparisons
    of small samples of data. In this chapter, we’ll reexamine persistent homology
    and look at the Mapper algorithm (now commercialized by Ayasdi).
  prefs: []
  type: TYPE_NORMAL
- en: Finding Distinctive Groups with Unique Behavior
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Previously, we used persistent homology to distinguish different types of graphs.
    Recall from [Chapter 4](c04.xhtml) that persistent homology creates simplicial
    complexes from point cloud data, applies a series of thresholds to those simplicial
    complexes, and calculates a series of numbers related to topological features
    present within each thresholded simplicial complex. To compare objects, we can
    use Wasserstein distance to measure the differences in topological features across
    slices.
  prefs: []
  type: TYPE_NORMAL
- en: Persistent homology has many uses in industry today. *Subgroup mining*, where
    we look for distinctive groups with unique behavior in the data, is one prominent
    use. In particular, we’re often searching for connected components with the zeroth
    homology groups, or groups that are connected to each other geometrically (such
    as clusters in hierarchical clustering). In psychometric survey validation, for
    example, subgroup mining allows us to find distinct groups within the survey,
    such as different subtypes of depression within a survey measuring depression.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s walk through a practical example of subgroup mining with persistent homology
    related to self-reported educational data from a social networking site. We’ll
    simulate data and compare persistent homology results using the TDAstats package
    in R and single-linkage hierarchical clustering using the `hclust()` function
    in R (see [Listing 7-1](#listing7-1)). We’ll return to [Chapter 6](c06.xhtml)’s
    example dataset of gifted Quora users self-reporting their school experiences
    (see the book files for this dataset). In this example, we’ll split the sample
    into sets of 11 individuals so that we can compare the persistent homology results
    statistically to ensure our measurements don’t vary across samples from our population
    of students. This provides a validation that our measurement is consistent across
    the population.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 7-1: A script that loads the educational dataset and splits it into
    two sets to be explored with persistent homology'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have our dataset, we can apply persistent homology to understand
    the clusters. Specifically, we’re looking at the zeroth Betti numbers, which correspond
    to connected groups, and other topological features of the data—see [Chapter 4](c04.xhtml)
    for a refresher.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to compute the Manhattan distances between each student in the
    social network; we’ll use these to define the filtration. Manhattan distances
    are often a go-to distance metric for discrete data. Add the following to your
    script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we want to apply the persistent homology algorithm to the distance-based
    data to reveal the persistent features. Using the TDAstats package, we can then
    add code to compute the zeroth and first Betti numbers of this dataset, using
    a relatively low-filtration setting set as the largest scale for the approximation
    (this will give us larger clusters). Finally, we can plot the results in a persistence
    diagram and a plot of hierarchical clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `calculate_homology()` function converts the point-cloud data from the distance
    dataset to a simplicial complex; we can then apply a filtration to identify topological
    features appearing and disappearing across the filtration. There are other methods
    that can create simplicial complexes from data, but the Rips complex in this package
    is one of the easiest to compute.
  prefs: []
  type: TYPE_NORMAL
- en: Using the previous code, we’ve plotted two figures. The call to `plot_persist()`
    should give something like [Figure 7-1](#figure7-1). You can see there that it
    appears one main group exists, along with possibly a subgroup or two at the lower
    filtration level; however, the subgroup may or may not be a significant feature,
    as it is near the diagonal.
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c07/f07001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7-1: A persistence diagram of the first set of educational experience
    data'
  prefs: []
  type: TYPE_NORMAL
- en: When using the hierarchical clustering results ([Figure 7-2](#figure7-2)), it’s
    easy to see a main group and then several splits at smaller distance thresholds.
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c07/f07002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7-2: A dendrogram of the simulated data'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you cut the clusters at a height of 5, the dendrogram results suggest that
    two main subgroups exist. Let’s split the `set1` data according to the two main
    clusters found in the hierarchical clustering by adding to [Listing 7-1](#listing7-1),
    first examining the smaller cluster and then examining the larger cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This should output the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In this cluster of individuals, no depression or outside learning was reported.
    Some individuals did report bullying, teacher hostility, boredom, remediation,
    or lack of motivation. Let’s contrast that with the larger cluster found in our
    dendrogram:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This should output something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Compared to the first cluster, these individuals mostly report outside learning
    and no bullying. This seems to separate learning experiences while in school.
    Learning outside of school and not dealing with bullying may have relevance to
    learning outcomes and overall school experience for students.
  prefs: []
  type: TYPE_NORMAL
- en: 'One item of interest in this analysis is individual 6, who seems to be an outlier
    in the [Figure 7-2](#figure7-2) dendrogram. This individual did not deal with
    bullying or teacher hostility but did deal with every other issue during their
    schooling. Outliers can be important and influential in analyses. Topology-based
    algorithms like persistent homology are often more robust to outliers than other
    algorithms and statistical models: extreme values or subgroups in the population
    won’t impact the results as dramatically when we use TDA as compared to other
    methods. For instance, in our gifted sample, individual 6 might impact k-means
    results more than the results of persistent homology.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Subgroup mining is one important use of persistent homology—both for identifying
    groups within the dataset and for identifying outliers that might impact optimization
    steps in more traditional clustering methods. Let’s continue by exploring another
    important use: as a measurement validation tool.'
  prefs: []
  type: TYPE_NORMAL
- en: Validating Measurement Tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many methods exist to compare dendrograms or persistence diagrams; this is still
    an active area of research. Persistence diagrams need to be turned into metric
    spaces, which allows us to construct nonparametric tests with a compatible distance
    metric, which in turn lets us compare two diagrams and simulate random samples
    from the null distribution, which finally we can use to compare the test distance.
    All in all, this lets us validate measurement tools. In our example, we want to
    validate our measurement of school problems by comparing samples from the same
    population (our Quora sample). If persistent homology results are the same, our
    measurement tool is consistent, which is a key property of measurement design
    in the field of psychometrics.
  prefs: []
  type: TYPE_NORMAL
- en: For persistence diagrams, we typically use the Wasserstein distance, as it works
    well for comparing distributions and sets of points in finite samples. For dendrograms,
    Hausdorff and Gromov–Hausdorff distance are two good options, both of which measure
    the largest distance within a set of smallest distances between points on a shape.
    However, the Gromov–Hausdorff distance is more complicated and imposes more structural
    information, which makes it less ideal.
  prefs: []
  type: TYPE_NORMAL
- en: 'To compare the distances of another persistence diagram to the current one,
    let’s use the second set of individuals in our self-reported educational dataset,
    adding to [Listing 7-1](#listing7-1):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Note that we’ve changed the dataset being analyzed to the second set of individuals
    from the full sample. This creates a comparison set that should be part of the
    same population; in this example, there are more potential subgroups that come
    out in the analysis. The plot should look something like [Figure 7-3](#figure7-3).
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c07/f07003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7-3: Another persistence diagram of simulated data, this time with different
    parameters used to simulate data'
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 7-3](#figure7-3), we see a few distinct groups similar to [Figure
    7-1](#figure7-1)’s sample. We also see some points corresponding to Betti number
    1; however, given how close they are to the line, these points are likely noise.
    The farther from the diagonal line a point lies, the more likely it is a real
    feature in the dataset. These new Betti number features are different than our
    prior sample but likely not real features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Computing the distance between the diagrams is easy with the TDAstats package.
    Add these lines to your script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This computes the distance between the persistence diagrams for the zeroth
    and first homology groups shown in [Figure 7-1](#figure7-1) and [Figure 7-3](#figure7-3)
    and should yield a distance of approximately 10.73 (zeroth homology) and 0.44
    (first homology), though the values may vary according to your version of R. Now
    it’s possible to compute the distances between random samples drawn from the original
    sample. The TDAstats package has a handy way of computing this within a function
    so that we don’t have to write the entire test ourselves. Let’s add these pieces
    to our script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This script will now compute a permutation test between the two samples’ features,
    yielding a test statistic and p-value for each homology level computed. As expected,
    our zeroth homology differences are not significant at a 95 percent confidence
    level (*p* = 0.08). Given that we don’t have any first homology features in our
    first sample, we do see a significant difference between the samples for our first
    homology differences; however, the statistic itself is 0, suggesting that this
    is an artificial finding.
  prefs: []
  type: TYPE_NORMAL
- en: While this example involves a convenience sample without an actual survey being
    administered, it does relate to how a real psychometric tool administered across
    population samples can be validated through persistent homology. We can also use
    this methodology to compare differences across different populations to explore
    how a measurement’s behavior changes across populations. Perhaps one sample of
    students had been accelerated (skipped one or more grades) and one had not. We
    might end up with very different self-reported experiences in school. In this
    case, the measurement tool might show very different behavior across the proposed
    accelerated and nonaccelerated groups.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Mapper Algorithm for Subgroup Mining
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In data science, we often are faced with clustering problems where data has
    extreme variable scale differences, includes sparse or spread-out data, includes
    outliers, or has substantial group overlap. These scenarios can pose issues to
    common clustering algorithms like k-means (group overlap, in particular) or DBSCAN
    (sparse or spread-out data). The *Mapper algorithm*—which finds clusters through
    a multistage process based on binning, clustering, and pulling back the clusters
    into a graph or simplicial complex—is another useful clustering tool for subgroup
    mining. This algorithm ties together some of the concepts in Morse theory with
    the filtration concept in persistent homology to provide a topologically grounded
    clustering algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Stepping Through the Mapper Algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The basic steps of the Mapper algorithm involve filtering a point cloud using
    a scalar-valued function called a *Morse function*; we then separate data into
    overlapping bins, cluster data within each bin, and connect the clusters into
    a graph or simplicial complex, based on overlap of the clusters across bins. To
    visualize this, let’s consider a simple point cloud with a defined scalar-valued
    function; we’ll shade the object according to the results we get when applying
    the function to the point cloud. Take a look at the results in [Figure 7-4](#figure7-4).
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c07/f07004r.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7-4: A multishaded object with a Morse function defined by a shade gradient'
  prefs: []
  type: TYPE_NORMAL
- en: This shape can now be chunked into four overlapping bins. This allows us to
    see potentially interesting relationships between areas with slightly different
    Morse function values, which will become relevant when we apply a clustering algorithm,
    as in [Figure 7-5](#figure7-5).
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c07/f07005r.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7-5: Binning results that chunk [Figure 7-4](#figure7-4) by shade gradient'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve binned our function ([Figure 7-5](#figure7-5)), we can start
    clustering. The clustering across bins can get a little bit more complicated than
    simply applying a clustering algorithm. This clustering is needed to define the
    complex and the overlapping of clusters across bins. Clustering within each of
    these bins and combining results to understand connectivity of clusters across
    bins would give a final result. An advantage of the Mapper algorithm is that results
    can be easily visualized as a graph or simplex; the final result of our example
    would likely output something like [Figure 7-6](#figure7-6), where two distinct
    groups evolve from a single point connecting them.
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c07/f07006r.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7-6: The clusters defined by binning the results of [Figure 7-4](#figure7-4)'
  prefs: []
  type: TYPE_NORMAL
- en: In practice, a distance metric—correlation, Euclidean distance, Hamming distance,
    and so on—is typically applied to the raw data before filtering as a way to process
    the point cloud data and create better filter functions prior to clustering. Clustering
    of the distance metric dataset can be done with a variety of algorithms, though
    single-linkage hierarchical clustering is usually used in practice. The coordinate
    systems used generally don’t matter for Mapper results or results from other TDA
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: There are a few advantages of the Mapper algorithm over other clustering methods,
    as well as topological data analysis compared to other methods in general. Invariance
    under small perturbations (noise) in the data allows Mapper to be more robust
    than k-means, which is sensitive to different starting seeds and can come up with
    very different results for each run. (Note that Mapper is sensitive to parameter
    changes but fairly robust to repeated runs with added noise.) The compression
    or visualization of results allows for easy visualization of clustering results
    for high-dimensional data. The lack of dependence on coordinate systems allows
    us to compare data on different scales or collected from different platforms.
    In addition, Mapper can deal with cluster overlap, which poses significant challenges
    to k-means algorithms and their derivatives. Lastly, Mapper’s ability to handle
    sparsity and outliers gives it an advantage over DBSCAN. This makes it ideal for
    use on small datasets, datasets where predictors might outnumber observations,
    or messy data that is likely to contain a lot of noise.
  prefs: []
  type: TYPE_NORMAL
- en: Using TDAmapper to Find Cluster Structures in Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The TDAmapper R package provides an implementation of the Mapper algorithm that
    can handle many types of processed data. For this example, we’ll return again
    to the self-reported educational dataset from the sample of gifted Quora users,
    including seven main school issues (bullying, teacher hostility, boredom, depression,
    lack of motivation, outside learning, put in remediation courses) reported across
    22 individuals who provided scores in the gifted range and discussed at least
    one of the issues of interest. The objective is to understand the relation between
    issues within this sample (somewhat like creating subscales within the measurement).
    This is binary data, so we’ll use inverse Hamming distance to obtain a distance
    matrix. Hamming distance measures bit-by-bit differences in binary strings to
    get a dissimilarity measurement. Other distances can be used on binary data, but
    Hamming distance works well to compare overall differences between individuals
    scored on binary variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s load the data and prepare it for analysis in [Listing 7-2](#listing7-2):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 7-2: A script that loads and processes the data to obtain a distance
    matrix'
  prefs: []
  type: TYPE_NORMAL
- en: The code in [Listing 7-2](#listing7-2) loads our dataset and packages needed
    for the analysis and then processes the data to obtain a distance matrix to feed
    into the Mapper algorithm. Other distances can be used on binary data, but Hamming
    distance works well to compare overall differences between individuals scored
    on binary variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s apply the Mapper algorithm. We’ll set Mapper to process the distance
    matrix using three intervals with 70 percent overlap and three bins for clustering.
    A higher overlap parameter on a small dataset will encourage connectivity between
    clusters found across bins; in practice, a setting between 30 to 70 percent usually
    gives good results. In addition, the small number of intervals and bins correspond
    to about half the number of instances to be clustered in this dataset, which usually
    works well in practice. Generally, it’s useful to use different parameter settings,
    as the results will vary depending on starting parameters; a few recent papers
    have suggested that the Mapper algorithm with nonvarying parameters is not wholly
    stable with respect to results. We’ll also set Filter values according to minimum
    and maximum Hamming distances. We can do both by adding these lines to the script
    in [Listing 7-2](#listing7-2):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This code runs the Mapper algorithm on the data with the parameters set earlier.
    The summary gives a list of objects in the Mapper object regarding results. The
    summary of points within a vertex gives us information as to how these variables
    separate into clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exploring the Mapper object yields some insight into which issues cluster together.
    We can gather a lot of information from the Mapper object, but this exploration
    will be limited to understanding which points from the dataset ended up in which
    cluster (vertex) in the Mapper object. Let’s examine the output from our last
    addition to [Listing 7-2](#listing7-2):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: From the previous results, which show which variable shows up in which clusters,
    we can see that variables 1 and 2 (bullying and teacher hostility) tend to occur
    in isolation (points vertices 1 and 2), while other issues tend to occur in clusters
    (points in the remaining vertices). Given that these are authority-social and
    peer-social issues of social etiology rather than curriculum etiology, this makes
    some sense. How teachers interact and how students behave is typically independent
    of the curriculum, while issues stemming from lack of challenge in the classroom
    stem directly from a curriculum cause.
  prefs: []
  type: TYPE_NORMAL
- en: 'Adding to our script, we can plot in igraph to obtain a bit more insight into
    the connectivity of the clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This code turns the Mapper’s overlapping cluster results into a graph object
    that can be plotted and analyzed to see how the clusters overlap with each other.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 7-7](#figure7-7) shows the isolation of the socially stemming issues
    of teacher hostility and bullying by peers. The curriculum-based issues tend to
    overlap to some extent with lack of motivation and outside learning (items 5 and
    6) being the strong ties between these clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c07/f07007.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7-7: A network plot of the clusters found in the Quora sample analysis'
  prefs: []
  type: TYPE_NORMAL
- en: One of the noted issues with Mapper is its instability with respect to overlap
    and binning of the filtration. For instance, changing the bin overlap to 20 percent
    results in the unconnected graph shown in [Figure 7-8](#figure7-8).
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c07/f07008.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7-8: A network plot of the Quora sample results with a different parameter
    defining bin overlap'
  prefs: []
  type: TYPE_NORMAL
- en: Some recent papers suggest using multiple scales to stabilize the output; however,
    most exploration of this notion is purely theoretical at this point. In general,
    using a variety of overlap fractions can yield a general idea of cluster structures
    in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we explored a few tools from topological data analysis. We
    compared data from samples of an educational population using persistent homology
    and explored educational experience groups within a self-selected sample of profoundly
    gifted individuals. TDA has grown quite a bit in recent years, and many problems
    can be solved with one or more tools from TDA. In the next chapter, we’ll explore
    one more popular tool from this growing field.
  prefs: []
  type: TYPE_NORMAL
