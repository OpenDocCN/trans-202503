<html><head></head><body>
<h2 class="h2b" id="ch02"><span epub:type="pagebreak" id="page_9" class="calibre1"/><strong class="calibre2"><span class="big">2</span></strong><br class="calibre9"/><strong class="calibre2">ACCESS METHODS</strong></h2>
<div class="image"><img src="../images/00015.jpeg" alt="image" class="calibre3"/></div>
<p class="noindent">Once you have a signed scope agreement in hand and have notified Microsoft, it’s time to gain privileged access to the target subscriptions. This chapter focuses on how to obtain credentials for an Azure subscription from a legitimate user or service. We start by looking at the different mechanisms Azure uses to control access to subscriptions, and how deployments and permissions are managed. Next, we cover common places where Azure credentials can be found, and how to capture them. Finally, we look at two-factor authentication, which may be in use to provide additional protection for a subscription, and then examine several ways it can be circumvented.</p>
<h3 class="h1" id="lev15"><span epub:type="pagebreak" id="page_10" class="calibre1"/><strong class="calibre2">Azure Deployment Models</strong></h3>
<p class="noindent">Before we begin sniffing out access to a subscription, let’s discuss Azure’s two authentication and permission models. Azure has both a legacy model, <em class="calibre7">Azure Service Management (ASM)</em>, which was used when Azure was first released, and a more recent role-based system, <em class="calibre7">Azure Resource Manager (ARM)</em>. Because both models are still in use, it’s important to understand how each model works and how each can be circumvented.</p>
<p class="indent">Although both models can coexist for any given subscription, each resource in a particular subscription uses only one model. Therefore, if you authenticate to the legacy portal, you’ll only be able to see “classic” Azure services. Likewise, running the newer Azure PowerShell commands will typically give you access only to modern resources.</p>
<p class="indent">The upshot is that hacking one user’s account may provide access to only a fraction of the services running under a subscription. Therefore, it’s crucial to attempt to compromise both models in any target subscription to ensure a complete test.</p>
<h4 class="h2" id="lev16"><strong class="calibre2"><em class="calibre10">Azure Service Management</em></strong></h4>
<p class="noindent">Azure Service Management is the original design for deploying and interacting with Azure resources. Sometimes referred to as “Azure Classic,” ASM is most commonly associated with the older Azure management website, <em class="calibre7"><a href="https://manage.windowsazure.com/" class="calibre6">https://manage.windowsazure.com/</a></em>.</p>
<p class="indent">ASM has many different components, including the following:</p>
<ul class="calibre8">
<li class="noindent1">An application programming interface (API) to programmatically manage resources</li>
<li class="noindent1">A collection of PowerShell cmdlets for interrogating and interacting with services</li>
<li class="noindent1">Username/password authentication support</li>
<li class="noindent1">X.509 certificate-based authentication</li>
<li class="noindent1">A command line interface to control resources</li>
<li class="noindent1">The management website</li>
</ul>
<p class="indent">Each component represents a potential point of entry or an information source for penetration testers.</p>
<h5 class="h3" id="lev17"><strong class="calibre2">Authorization in ASM</strong></h5>
<p class="noindent">The Azure Service Management model uses a simple authorization mechanism with only three possible roles: <em class="calibre7">Service Administrator</em>, <em class="calibre7">Account Administrator</em>, and <em class="calibre7">Co-Administrator</em>. The first two roles are limited to one each per subscription. Both can be assigned to a single user, if desired.</p>
<p class="indent"><span epub:type="pagebreak" id="page_11"/>The Service Administrator is the primary management account. It can make any changes to the subscription’s services and add users as Co-Administrators. The Account Administrator (also known as Account Owner) can change billing details and the account assigned to the Service Administrator role for the subscription but cannot modify services. The Co-Administrator has the same rights as the Service Administrator, except for the ability to change the role of another user to Service Administrator.</p>
<p class="indent">Because Co-Administrators are essentially equivalent to Service Administrators, and both have full control over any ASM-created resource, once you obtain ASM access to an Azure subscription, all ASM resources are entirely under your control.</p>
<p class="indent">A user or service account can authenticate against ASM with a username and password pair or with an X.509 certificate. The owner of a subscription can log in to the management portal and add users to their subscription. The accounts they add must be either a <em class="calibre7">Microsoft Account (MSA)</em>, which is an email address registered with Microsoft (formerly known as a Live ID, and Passport before that), or an account in <em class="calibre7">Azure Active Directory (AAD)</em>. Once added to the subscription, that user simply connects using their email address and the password they set for their MSA or their account in AAD.</p>
<p class="indent">Certificate-based authentication is unique to ASM and is not implemented (directly) in ARM, discussed later in this chapter. Referred to as <em class="calibre7">management certificates</em> in ASM, X.509 authentication was originally intended for services that needed to interact with Azure programmatically. It was also used for deploying code straight to Azure from Visual Studio and could be used in place of username/password credentials when using PowerShell to manage subscriptions.</p>
<p class="indent">These are all reasonable use cases, and, theoretically, certificates should be more secure than passwords for authentication. After all, certificates can’t be easily divulged by users in phishing attacks, aren’t subject to guessing or dictionary attacks like passwords are, and almost certainly have more entropy than a user’s password. Then why would Azure not carry them forward to the more modern model? There are likely a number of reasons, but the issue I most often encounter when penetration testing is certificate manageability.</p>
<h5 class="h3" id="lev18"><strong class="calibre2">Certificate Management in ASM</strong></h5>
<p class="noindent">Manageability is the top issue with Azure management certificates. Some problems with management certificates include determining where a certificate is used, certificate name reuse, lack of revocation lists, improper storage, and nonrepudiation.</p>
<p class="indent"><a href="part0011.html#ch02fig1" class="calibre6">Figure 2-1</a> shows Azure’s management certificate settings page, which includes details about each of the certificates added to the subscription and allows administrators to add new certificates or remove existing ones.</p>
<div class="image1"><span epub:type="pagebreak" id="page_12"/><a id="ch02fig1" class="calibre6"/><img src="../images/00017.jpeg" alt="image" class="calibre3"/></div>
<p class="figcap"><em class="calibre7">Figure 2-1: Azure management certificate settings</em></p>
<p class="indent">Let’s look at some of the difficulties involved in managing these certificates, which can lead to security issues.</p>
<p class="noindentt"><strong class="calibre4">Tracking Certificates Across Subscriptions</strong></p>
<p class="indenta">When a certificate is added to a subscription, the Azure portal doesn’t tell you who created the certificate or who uploaded it. (Note the lack of an owner or creator column in <a href="part0011.html#ch02fig1" class="calibre6">Figure 2-1</a>.) To further complicate things, there is no way to look up all the subscriptions where a given certificate is authorized. This means that if a cyber defense team is alerted to a particular certificate having been compromised, they won’t necessarily know which subscriptions are affected.</p>
<p class="noindentt"><strong class="calibre4">Name Reuse</strong></p>
<p class="indenta">Poorly named certificates are another problem for administrators trying to maintain a subscription. Because certificates are automatically generated by various tools (Visual Studio, PowerShell, and even the Azure portal itself), different certificates frequently have the same names. For example, <a href="part0011.html#ch02fig1" class="calibre6">Figure 2-1</a> shows multiple Visual Studio–generated certificates that use the same name—“Visual Studio Ultimate” <span class="ent">➊</span>—distinguished only by their thumbprints <span class="ent">➋</span>.</p>
<p class="indentaa">Because each Azure subscription can have up to 100 management certificates, name reuse can quickly make it difficult to determine who <span epub:type="pagebreak" id="page_13"/>owns which certificate. If an administrator is fired, how are the remaining administrators to know which certificate(s) must be deleted?</p>
<p class="noindentt"><strong class="calibre4">Revocation</strong></p>
<p class="indenta">Unlike most systems that use X.509 certificates, Azure doesn’t implement <em class="calibre7">Certificate Revocation Lists (CRLs)</em> for management certificates. CRLs document when a certificate is no longer trusted in a central location that services can check. For example, if CRLs were implemented, an administrator could publish an update stating “No longer trust certificate X,” and all services permitting that certificate would block it automatically. Without CRLs, a compromised certificate must be deleted from each subscription manually. However, because there’s no way to determine which subscriptions can be accessed with a particular certificate, it’s common to find bad certificates inadvertently left in some subscriptions.</p>
<p class="noindentt"><strong class="calibre4">Storage</strong></p>
<p class="indenta">Another critical issue with management certificates has to do with proper, secure storage. Because certificates are frequently generated by tools such as Visual Studio, the location of these files is often predictable. In fact, they can routinely be found in source code repositories and users’ <em class="calibre7">Downloads</em> folders. They may even be exported directly from the certificate store on an administrator’s computer.</p>
<p class="noindentt"><strong class="calibre4">Nonrepudiation</strong></p>
<p class="indenta"><em class="calibre7">Nonrepudiation</em> describes the ability of a system to definitively state that an action was performed by a given user, such that the user cannot claim that someone else performed the action. Nonrepudiation is most straightforward with usernames and passwords, and it’s well established that passwords should not be shared. Unfortunately, users often don’t respect certificates the way they do passwords, and it’s common for the members of a team to all use one shared certificate to access numerous subscriptions.</p>
<p class="indentt">These concerns make consistent, thorough auditing and cleanup of management certificates difficult. Orphaned management certificates can leave a subscription vulnerable, and use of a forgotten certificate may well go unnoticed for an extended period.</p>
<h4 class="h2" id="lev19"><strong class="calibre2"><em class="calibre10">Azure Resource Manager</em></strong></h4>
<p class="noindent">Several years following the initial release of Azure, Microsoft realized it needed to improve several aspects of Azure management. Rather than integrate the changes into the existing ASM management portal and APIs, it launched Azure Resource Manager as a replacement.</p>
<p class="indent">ARM’s most obvious change is the portal available at <em class="calibre7"><a href="https://portal.azure.com/" class="calibre6">https://portal.azure.com/</a></em>, but that’s only the most visible part of the model. By order of significance, notable changes introduced in ARM include the following:</p>
<ul class="calibre8">
<li class="noindent1">Role-based access control</li>
<li class="noindent1">Removal of management certificates</li>
<li class="noindent1"><span epub:type="pagebreak" id="page_14"/>Addition of service principals</li>
<li class="noindent1">Ability to manage a group of resources as one unit</li>
<li class="noindent1">New PowerShell cmdlets</li>
<li class="noindent1">Templates to quickly deploy complex services</li>
</ul>
<p class="indent"><em class="calibre7">Role-based access control (RBAC)</em> brought the biggest change for penetration testers. Unlike ASM, with its limited set of roles, ARM offers numerous roles that can be assigned to users both at a subscription level and on a per-resource basis.</p>
<p class="indent">The most common roles are Owner (full control), Contributor (all rights except the ability to change permissions), Reader (read-only control), and User Access Administrator (ability to edit permissions only). Other service-specific roles such as SQL DB Contributor and Website Contributor permit the Owner to limit database administrators to only SQL server access while allowing web developers to modify websites only. When compromising a subscription, you’ll ideally want to target users who are Owners for the entire subscription.</p>
<p class="indent">Another important change was the addition of <em class="calibre7">service principals</em>. These accounts are similar to service accounts in an on-premises server—like the Apache daemon and Internet Information Services (IIS) accounts that are used to run web servers. Service principals allow an application to run under an account not associated with a regular user and still access other cloud resources. For example, a company’s Azure website may need to access Azure Active Directory (AAD) to look up employee information. The site needs some account to log in to AAD, but the developer certainly doesn’t want the site to use their user credentials to perform those lookups. This is where a service principal is needed.</p>
<p class="indent">Because service principals are used for software, scripts, and automation, these accounts can use either passwords (automatically generated and referred to as a “Client Secret”) or certificates to authenticate, though their configuration and use differ from ASM management certificates. Following the principle of least privilege, service principals are often assigned only enough access through RBAC to perform specific tasks so that compromising one will only provide access to a small subset of resources within a subscription.</p>
<div class="sidebar">
<p class="sidebart"><strong class="calibre4">DEFENDER’S TIP</strong></p>
<p class="spara">Because ARM offers several security advantages over ASM, you should migrate any existing ASM-based services to ARM. To do so, download the tools MigAz and ASM2ARM from GitHub. Microsoft also has several articles on ARM migration posted at <em class="calibre10"><a href="https://docs.microsoft.com/en-us/azure/virtual-machines/windows/migration-classic-resource-manager-overview/" class="calibre11">https://docs.microsoft.com/en-us/azure/virtual-machines/windows/migration-classic-resource-manager-overview/</a></em>.</p>
</div>
<h3 class="h1" id="lev20"><span epub:type="pagebreak" id="page_15" class="calibre1"/><strong class="calibre2">Obtaining Credentials</strong></h3>
<p class="noindent">As penetration testers, we must gather credentials to demonstrate what a real attacker might do with access to a client’s resources. Our target account would be one that provides administrator access to a target’s ASM resources, has Owner permissions for all ARM resources in the subscription, and has two-factor authentication (2FA) disabled. Such an account would be able to create, examine, change, or delete any service within the subscription and log in without responding to a phone prompt. Finding such an account on Azure would be equivalent to finding a root account in Linux that uses a default password and that can log in remotely.</p>
<p class="indent">The first step in finding our target account would be to locate a service account that uses a username and password to log in and that is a Co-Administrator of the target subscription in ASM. Service accounts are ideal because they rarely have 2FA enabled, infrequently change their password, and often have passwords left in source code. Failing that, the account of a human administrative user (such as a manager or lead developer) would do well, especially because they are likely to have full control over all resources, even if they have 2FA enabled. As a last resort, consider management certificates. Although they won’t provide access to ARM resources, they are usually easy to come by and are infrequently changed or removed.</p>
<p class="indent">By investigating credentials, you will be able to determine if your customer is properly protecting these crucial secrets and, if not, provide guidance for how they can secure them. Let’s look at how to try to obtain these credentials.</p>
<h3 class="h1" id="lev21"><strong class="calibre2">Mimikatz</strong></h3>
<p class="noindent">Obtaining credentials directly from a user’s operating system has to be one of my favorite pentest methods. The concept is simple enough: even when the system is unplugged from the network, an operating system needs to keep track of a user’s password for tasks such as validating the password and forwarding the password on to other systems so the user doesn’t have to retype it, such as when connecting to a file server.</p>
<p class="indent">Tools to grab passwords or password hashes from various places in the operating system have been available for years. Early examples like Cain &amp; Abel could extract them from the Windows Security Account Manager (SAM) file, and PwDump has had numerous iterations with different methods. However, the release of Benjamin Delpy’s Mimikatz changed the game by allowing password theft straight from a system’s memory.</p>
<h4 class="h2" id="lev22"><strong class="calibre2"><em class="calibre10">Using Mimikatz</em></strong></h4>
<p class="noindent">The primary feature of Mimikatz works by identifying the running Local Security Authority Subsystem Service (LSASS) on a Windows system, attaching to it, and siphoning secrets out of its memory. Although Mimikatz can grab numerous kinds of secrets, we’ll look only at user passwords.</p>
<p class="indent"><span epub:type="pagebreak" id="page_16"/>When using Mimikatz, you first need to obtain administrative access to a system used by the target administrator. In a domain environment, this usually isn’t difficult. For example, you might phish an administrator of a terminal server that is also used by the target user and run Mimikatz there, or you could social engineer a helpdesk employee in a security group with administrative rights to all workstations on the domain. All you need is an administrator account on any system that has recently been serviced by the helpdesk, and you can execute Mimikatz on that system to get the helpdesk password.</p>
<p class="indent">Once you have administrative access to a system, it’s time to download Mimikatz from <em class="calibre7"><a href="https://github.com/gentilkiwi/mimikatz/" class="calibre6">https://github.com/gentilkiwi/mimikatz/</a></em>. If the download is flagged by antivirus, it’s easy enough to run a version that has been converted to a PowerShell script available as part of the PowerSploit framework from <em class="calibre7"><a href="https://github.com/PowerShellMafia/PowerSploit/" class="calibre6">https://github.com/PowerShellMafia/PowerSploit/</a></em>. You could also retrieve the Mimikatz source code, make some small modifications, and recompile it (and rename the binary) in order to bypass any signature-based antivirus detections. (The Mimikatz GitHub page has detailed directions for how to do this.)</p>
<p class="indent">Now launch an elevated command prompt on the target system and execute the 32- or 64-bit version of <em class="calibre7">mimikatz.exe</em>, depending on the operating system architecture. (If you’re unsure of the architecture, run <span class="codestrong">wmic OS get OSArchitecture</span>.)</p>
<h4 class="h2" id="lev23"><strong class="calibre2"><em class="calibre10">Capturing Credentials</em></strong></h4>
<p class="noindent">To capture credentials, Mimikatz needs debugging rights. It uses this privilege to be able to read memory in LSASS. To give it this access, enter <span class="codestrong">privilege::debug</span> at the Mimikatz prompt, as shown here:</p>
<pre>mimikatz # <span class="codestrong1">privilege::debug</span><br class="calibre5"/>Privilege '20' OK</pre>
<p class="indent">Next, issue the <span class="codestrong">sekurlsa::logonpasswords</span> command to dump all the passwords and hashes Mimikatz can find, as shown in <a href="part0011.html#ch02list1" class="calibre6">Listing 2-1</a>.</p>
<pre>mimikatz # <span class="codestrong1">sekurlsa::logonpasswords</span><br class="calibre5"/>Authentication Id : 0 ; 249835 (00000000:0003cfeb)<br class="calibre5"/>Session           : Interactive from 1<br class="calibre5"/>User Name         : <span class="codeitalic">Administrator</span><br class="calibre5"/>Domain            : <span class="codeitalic">Corporation</span><br class="calibre5"/>Logon Server      : <span class="codeitalic">Workstation</span><br class="calibre5"/>Logon Time        : 11/1/2016 11:09:59 PM<br class="calibre5"/>SID               : S-1-5-21-2220999950-2000000220-1111191198-1001<br class="calibre5"/>        msv :<br class="calibre5"/>         [00000003] Primary<br class="calibre5"/>         * Username : <span class="codeitalic">TargetUser</span><br class="calibre5"/>         * Domain   : <span class="codeitalic">Corporation</span><br class="calibre5"/>       <span class="ent">➊</span> * NTLM     : 92937945b518814341de3f726500d4ff<br class="calibre5"/>         * SHA1     : 02726d40f378e716981c4321d60ba3a325ed6a4c<br class="calibre5"/><span epub:type="pagebreak" id="page_17"/>         [00010000] CredentialKeys<br class="calibre5"/>         * NTLM     : 92937945b518814341de3f726500d4ff<br class="calibre5"/>         * SHA1     : 02726d40f378e716981c4321d60ba3a325ed6a4c<br class="calibre5"/>       <span class="ent">➋</span> tspkg :<br class="calibre5"/>         * Username : <span class="codeitalic">TargetUser</span><br class="calibre5"/>         * Domain   : <span class="codeitalic">Corporation</span><br class="calibre5"/>         * Password : <span class="codeitalic">Pa$$w0rd</span><br class="calibre5"/>        wdigest :<br class="calibre5"/>         * Username : <span class="codeitalic">TargetUser</span><br class="calibre5"/>         * Domain   : <span class="codeitalic">Corporation</span><br class="calibre5"/>         * Password : <span class="codeitalic">Pa$$w0rd</span><br class="calibre5"/>        kerberos :<br class="calibre5"/>         * Username : <span class="codeitalic">TargetUser</span><br class="calibre5"/>         * Domain   : <span class="codeitalic">Corporation</span><br class="calibre5"/>         * Password : (null)</pre>
<p class="listing" id="ch02list1"><em class="calibre7">Listing 2-1: Retrieving passwords with Mimikatz</em></p>
<p class="indent">As you can see in the output, Mimikatz was able to find the NTLM and SHA1 hashes for TargetUser’s password <span class="ent">➊</span>. It was also able to find the plaintext, non-hashed version of the password in both the <em class="calibre7">tspkg</em> and <em class="calibre7">wdigest</em> extensions present in LSASS <span class="ent">➋</span>.</p>
<h4 class="h2" id="lev24"><strong class="calibre2"><em class="calibre10">Factors Affecting Success</em></strong></h4>
<p class="noindent">Several factors impact Mimikatz’s ability to retrieve passwords. Most important is what operating system the user is running. Although Mimikatz supports everything from Windows 2000 through Windows 10, newer versions of Windows have improved credential storage. For example, it was common to get plaintext passwords from Windows Vista and Windows Server 2008, even after a user had logged off (as long as the system hadn’t been rebooted). Although it’s still possible to get hashes from Windows 10, plaintext passwords are hit-or-miss and are only possible to retrieve while the user’s session is active. Additionally, the Credential Guard feature in Windows 10 Enterprise, when enabled, moves these secrets into an isolated container that is better protected from hacking tools.</p>
<p class="indent">Mimikatz’s ability to capture credentials is also contingent on how the target system is configured and on what applications are installed. Certain applications and Windows features rely on having a copy of users’ credentials so that users won’t be prompted to re-enter their password each time a remote connection is established. With each new revision, Windows eliminates some of these dependencies for plaintext passwords, but Microsoft can’t control what third-party software does, so it may be a while before all credentials are cleaned from memory.</p>
<p class="indent">Mimikatz relies on the fact that certain locations in Windows are known to hold credentials, and the program evolves as Windows evolves. With that in mind, if your target is running some unusual build of Windows (such as a technical preview copy), Mimikatz probably won’t be able to determine where credentials are held in memory.</p>
<div class="sidebar">
<p class="sidebart"><span epub:type="pagebreak" id="page_18"/><strong class="calibre4">DEFENDER’S TIP</strong></p>
<p class="spara">Using Credential Guard is one of the best ways to protect user credentials from hacking tools such as Mimikatz, though it isn’t available on operating systems before Windows 10 or Windows Server 2016. For an attacker, it is one of the most frustrating security features to encounter. You can learn more about this Windows feature at <em class="calibre10"><a href="https://technet.microsoft.com/en-us/itpro/windows/keep-secure/credential-guard/" class="calibre11">https://technet.microsoft.com/en-us/itpro/windows/keep-secure/credential-guard/</a></em>.</p>
</div>
<h3 class="h1" id="lev25"><strong class="calibre2">Best Practices: Usernames and Passwords</strong></h3>
<p class="noindent">In spite of passwords being in use for decades, weak password selection is still a major factor in security breaches. Although it can be difficult to get an entire user population to all choose good passwords, administrators and corporate policy creators can help support their users in making good password choices by eliminating rules that lead to poor password construction.</p>
<p class="indent">For example, conventional wisdom stated that companies should enforce short password lifetimes, so users had to choose new passwords every few months. Although this does help prevent password hash cracking for lengthy passwords, it also means users are expected to come up with a novel, complex password that they can remember, one that isn’t based on a past password, multiple times a year. In practice, this often leads to users selecting passwords that just barely meet corporate standards for length and that contain predictable elements such as dictionary words or dates.</p>
<p class="indent">Instead, the 2017 Digital Identity Guidelines from the U.S. National Institute of Standards and Technology (NIST) now suggest not enforcing frequent password changes, in order to allow users to create a very strong password and keep it for an extended period. The guidance suggests only forcing a change if the credential is determined to have been compromised.</p>
<p class="indent">Companies can also encourage users to use a suitable password manager to generate and store credentials. These utilities help ensure that users select a strong, random password for each system, service, or website they use. This greatly improves security, because password reuse across multiple sites means that if any one site is breached, the security of any other service where a user has chosen the same password is now also at risk.</p>
<p class="indent">Additionally, even strong passwords can still be obtained if a user is susceptible to phishing (see “<a href="part0011.html#lev28" class="calibre6">Phishing</a>” on <a href="part0011.html#page_19" class="calibre6">page 19</a> for more on this topic). One of the most effective ways to stop phishing attacks is to enable multi-factor authentication on your services, such as requiring the user to enter a code received on their mobile device in addition to their password. This greatly increases the complexity of an attack for an adversary.</p>
<p class="indent">Finally, we know that web-facing services that use password-based authentication are frequently the target of password-guessing attacks, as described in “<a href="part0011.html#lev30" class="calibre6">Guessing Passwords</a>” on <a href="part0011.html#page_21" class="calibre6">page 21</a>. To help reduce this risk, <span epub:type="pagebreak" id="page_19"/>make sure that any administrative accounts for these services use unique usernames, as attackers will often try just a few usernames, such as <em class="calibre7">administrator</em>, <em class="calibre7">admin</em>, and <em class="calibre7">root</em>.</p>
<h3 class="h1" id="lev26"><strong class="calibre2">Usernames and Passwords</strong></h3>
<p class="noindent">When Mimikatz is not an option, you’ll need another way to grab usernames and passwords. This can be accomplished by searching unencrypted documents, phishing, finding saved authentication tokens, or using educated guesses. Each method has its advantages and disadvantages.</p>
<h4 class="h2" id="lev27"><strong class="calibre2"><em class="calibre10">Searching Unencrypted Documents</em></strong></h4>
<p class="noindent">Corporate penetration testers often find a surprising number of passwords just lying around, readily available for a sleuthing attacker. Although the cliché password on a sticky note attached to a monitor is sadly still an issue in some companies, most penetration testers can’t go office-to-office looking for credentials. Fortunately for the penetration tester, many passwords are kept in unencrypted files that are easily accessed remotely.</p>
<p class="indent">If your target is a service account, you will often find the account’s password in source code and configuration (<em class="calibre7">.config</em>) files used by that service. Passwords may also appear in design documents on a team portal or file share.</p>
<p class="indent">When targeting a human in search of a username and password, look for passwords in a text file or spreadsheet, often on the user’s desktop or in their <em class="calibre7">Documents</em> directory. (You will of course need access to that user’s PC or network.) As you surely know, browsers offer to save passwords on the user’s behalf, and these are usually trivial to recover once on the system.</p>
<h4 class="h2" id="lev28"><strong class="calibre2"><em class="calibre10">Phishing</em></strong></h4>
<p class="noindent">One surprisingly successful way to collect passwords is by <em class="calibre7">phishing</em>—or more accurately, <em class="calibre7">spear phishing</em>—for them. When phishing, you email a wide range of users to try to trick them into taking some action, such as divulging their username and password by convincing them to visit a malicious site or getting them to install malware.</p>
<p class="indent">Spear phishing is simply a more targeted version of phishing: you email a very specific group using language that looks familiar to the target, and make it appear as though the email came from a legitimate or expected address. For example, whereas a typical phishing email might contain a link to a supposed greeting card and is sent to thousands of users, a spear-phishing email might look like it comes from the HR department and is sent to only a dozen people with a request to update their contact information.</p>
<p class="indent">In my experience as a security professional, I find many spear-phishing attacks mimic the type of email a user generally expects, including the style and language of some leaked corporate emails. Often the emails come from a legitimate-sounding address and contain a link to a plausible <span epub:type="pagebreak" id="page_20"/>URL. For example, one might register a domain name that’s very close to that of the target corporation’s real address—perhaps using <em class="calibre7">.net</em> instead of <em class="calibre7">.com</em> or a character replacement, such as swapping an uppercase <em class="calibre7">I</em> with a lowercase <em class="calibre7">l</em>.</p>
<p class="indent">The most successful phishing attacks play on people’s hopes and fears. Emails offering some reward, such as free event tickets or gift cards, or threatening to take away some employee perk or suspend the user’s account almost always get a quick response.</p>
<p class="indent">Phishing emails contain a link designed to entice the user into clicking it, directing the user to a web page where they’re prompted to sign in. Successful destination pages look just like the real one used by the target user’s company. The phishing page will save the password to a secure log or database that the attacker controls and then redirect the user somewhere plausible so as not to arouse suspicion, such as to a real logon page, a page that says the promotion mentioned in the email has expired, or a page that says that the company has reconsidered and will not be charging employees for use of the photocopier.</p>
<div class="note">
<p class="notet"><strong class="calibre2"><span class="notes">WARNING</span></strong></p>
<p class="notep"><em class="calibre7">Be extremely careful if setting up credential-capturing systems. You should follow all security best practices for your phishing site and database, including using encryption in transit, encryption at rest, and strong, multi-factor authentication to access the secrets. Your site should be code-reviewed for flaws, and the underlying services/system should be fully patched. Failing to take these precautions could put employee credentials at a much greater risk, violate your target company’s policies, and lead to a real compromise.</em></p>
</div>
<p class="indent">However, phishing isn’t without its downsides. For one thing, it can only be used to target users, not service accounts. Also, it only takes one user to recognize the email as a phishing attempt and report it before the target organization’s security team swoops in and quarantines the email, blacklists the phishing website, and resets the passwords for any accounts you’ve already obtained.</p>
<h4 class="h2" id="lev29"><strong class="calibre2"><em class="calibre10">Looking for Saved ARM Profile Tokens</em></strong></h4>
<p class="noindent">JavaScript Object Notation ( JSON) files are another place that is capable of storing credentials. Because developers often need to use different accounts when accessing ARM resources (perhaps for automation or testing purposes), Azure provides an ARM PowerShell cmdlet to save an Azure credential as a <em class="calibre7">profile</em>: <code>Save-AzureRmProfile</code>. These profiles are just JSON files, and the developer can choose to store them wherever they like. Inside these JSON files is a token, which is a stored representation of the saved credential. To use it, simply run the <code>Select-AzureRmProfile</code> cmdlet and specify the JSON file using the <code>-Path</code> parameter.</p>
<p class="indent">Finding these stored profiles can be a little tricky because they don’t have a unique extension (in fact, they could have any extension, though <span epub:type="pagebreak" id="page_21"/>most users choose <em class="calibre7">.json</em> because it is used in the documentation). However, you should be able to locate these profiles by performing a search for files containing keywords used in the profiles. Search for a term like <em class="calibre7">TokenCache</em>, which is the variable in the file that stores the actual credential. If that turns up too many false positives on your target user’s system, try <em class="calibre7">Tenant</em>, <em class="calibre7">PublishSettingsFileUrl</em>, and <em class="calibre7">ManagementPortalUrl</em>. These keywords should be sufficient to locate any saved profiles with minimal false positives.</p>
<h4 class="h2" id="lev30"><strong class="calibre2"><em class="calibre10">Guessing Passwords</em></strong></h4>
<p class="noindent">One final way to obtain an account password is simply to guess. Uneducated guessing is not likely to be fruitful, but combined with a bit of reasoning and research, guessing can bear fruit.</p>
<p class="indent">When trying to guess a password, first try to find the organization’s password policy. If all passwords must be at least nine characters long and include letters and numbers, simply trying someone’s birthday is sure to fail. Additionally, knowing if there is an account lockout policy is crucial because it determines how many guesses can be made against a single account before it is locked, thus alerting the user to the attempts.</p>
<p class="indent">Next, try to collect information about the target user. The names of a spouse, children, and pets can be very useful, as can birth dates, anniversaries, and graduations. Even knowing how often an organization mandates a password change can be useful. Users who must come up with a new password every 30 days use the names of the month (or its numeric equivalent) in their passwords with disturbing frequency.</p>
<p class="indent">When guessing, try to find some public endpoint that will validate the user’s credentials and report the result quickly. Corporate webmail sites and virtual private network (VPN) endpoints might be good options. A site that does not rate-limit logon attempts and does not lock out user accounts is useful to attackers.</p>
<div class="sidebar">
<p class="sidebart"><strong class="calibre4">DEFENDER’S TIP</strong></p>
<p class="spara">Implementing automatic account lockouts after a certain number of failed logon attempts is a popular way to address password guessing attempts; however, they can have the unintended consequence of preventing the legitimate account holder from accessing network resources until their account is unblocked. For this reason, rate limiting logon attempts may be a better option, either based on the IP address of the source machine attempting the logon or based on the account being tested. Regardless of the approach, defending against this type of attack should be a priority for system administrators. Defense teams should also set up monitoring on applicable endpoints to improve their awareness of attacks taking place.</p>
</div>
<p class="indent"><span epub:type="pagebreak" id="page_22"/>In response to account lockout policies, <em class="calibre7">password spraying</em> has become a common technique used by attackers. Whereas traditional <em class="calibre7">brute-force</em> attempts try many different passwords against only a handful of accounts, password spraying tries just a handful of common passwords against many different accounts: this identifies all the accounts that share the same weak passwords. Even if the resulting accounts don’t have access to the target resources, they may serve as a springboard into the environment to target other systems. This is a good method to employ as a pentester, so you can demonstrate an increasingly common real-world attack as well as measure the target organization’s ability to detect and respond to it.</p>
<p class="indent">Hydra by The Hacker’s Choice (THC) is a particularly useful tool for password guessing. You can find it at <em class="calibre7"><a href="https://github.com/vanhauser-thc/thc-hydra/" class="calibre6">https://github.com/vanhauser-thc/thc-hydra/</a></em> or <em class="calibre7"><a href="https://www.thc.org/thc-hydra/" class="calibre6">https://www.thc.org/thc-hydra/</a>.</em></p>
<h3 class="h1" id="lev31"><strong class="calibre2">Best Practices: Management Certificates</strong></h3>
<p class="noindent">Management certificates are intended to programmatically manage classic, ASM-based resources. In ARM, which is the new and recommended way to deploy Azure resources, service principals have replaced management certificates. Service principals offer a number of benefits over management certificates—most notably the ability to specify granular permissions, reducing the damage that can be caused by a compromised account. Wherever possible, it makes sense to move away from management certificates and to use service principals.</p>
<p class="indent">However, if you must maintain management certificates for existing services, there are several steps you can take to protect them. These include tracking where management certificates are used and who owns them, storing them securely, using the certificates exclusively for Azure management, and, when possible, moving away from management certificates.</p>
<p class="indent">As I mentioned earlier, the difficulty of managing management certificates is one of their biggest drawbacks. I’d suggest performing a detailed inventory of any certificates that exist in all of your subscriptions, including their name, thumbprint, which subscription(s) they are present in, and, if you can, who created them or uses them and their purpose. Then make it a policy that any new management certificates must be logged before being added, and failure to do so will result in their removal. Once this inventory is in place, perform periodic audits to look for changes to the certificate list in all of your subscriptions and remove any that are no longer used.</p>
<p class="indent">Additionally, to help track certificate usage, I suggest using unique names for all certificates that are not automatically generated. You might even consider removing all automatically generated certificates during each audit—just be sure developers know that this is policy, so they don’t expect them to persist.</p>
<p class="indent">Another concern is properly securing management certificates. Never check certificates into source control, as that makes it too easy for them to be overshared. Instead, treat them like other credentials and place them in <span epub:type="pagebreak" id="page_23"/>a secure location. Don’t even temporarily store private keys on improperly secured workstations or drives. Also, be sure to use strong passwords on the <em class="calibre7">.pfx</em> files containing the management certificates’ private keys.</p>
<p class="indent">One other common mistake is the use of certificates for multiple purposes, such as using the same SSL/TLS certificate both to secure website traffic and for managing the subscription hosting the site. Don’t do this! Reuse of certificates in this way is not only confusing but also means that if a certificate is compromised in one place, every system using it is vulnerable. Azure management certificates don’t need to be fancy, expensive, publicly trusted certificates; a free, self-signed certificate works just fine.</p>
<p class="indent">If possible, private keys or key pairs should be generated on the system that will ultimately use the private key. If an administrator routinely generates key pairs for production systems on their own workstation, those private keys are unnecessarily exposed on a single system, which will thereby become a high-value target.</p>
<h3 class="h1" id="lev32"><strong class="calibre2">Finding Management Certificates</strong></h3>
<p class="noindent">Recall from earlier in this chapter that in addition to authenticating users by username and password, ASM also accepts certificates. In this section, we look at how to use certificates to gain access to management certificates in Publish Settings files, the certificate store, configuration files, and Cloud Service Package files.</p>
<p class="indent">Keep in mind that Azure uses asymmetric X.509 certificates, which means that each certificate has a public and private key. It is important to obtain the private key portion of the certificate, as this is the component required for authentication.</p>
<p class="indent">Although certificates can have a number of file extensions (when not embedded in some other file, as discussed in the next section), the two most common extensions on Windows are <em class="calibre7">.pfx</em> and <em class="calibre7">.cer</em>. Typically, <em class="calibre7">.cer</em> files will only contain the public key, whereas <em class="calibre7">.pfx</em> files will also contain the private key. For this reason, attackers often search a target machine’s file system for <em class="calibre7">*.pfx</em> files.</p>
<p class="indent">If you find a <em class="calibre7">.pfx</em> file that is password protected, look for text files in the same directory. Users often save the password in a plain-text file in the same directory as the certificate itself!</p>
<h4 class="h2" id="lev33"><strong class="calibre2"><em class="calibre10">Publish Settings Files</em></strong></h4>
<p class="noindent"><em class="calibre7">Publish Settings</em> files are XML documents that contain details about an Azure subscription, including the subscription’s name, ID, and, most importantly, a base64-encoded management certificate. These files can easily be identified by their somewhat unwieldy extension, <em class="calibre7">.publishsettings</em>.</p>
<p class="indent">Publish Settings files are designed to make it easy for developers to deploy projects to Azure. For example, after creating an Azure website in Visual Studio, the Publishing Wizard accepts a Publish Settings file to <span epub:type="pagebreak" id="page_24"/>authenticate to Azure and push the solution to the cloud. Because these files are downloaded from the Azure management portal and are often used in Visual Studio, they can usually be found in a user’s <em class="calibre7">Downloads</em> directory or saved with Visual Studio project files.</p>
<p class="indent">Once you have a Publish Settings file, open it in a text editor, copy everything between the quotation marks in the <em class="calibre7">ManagementCertificate</em> section, paste the contents into a new document, and save it with a <em class="calibre7">.pfx</em> extension. Note that there is no password for this <em class="calibre7">.pfx</em> file, so if you are prompted for a password when using it, simply click Next or OK.</p>
<h4 class="h2" id="lev34"><strong class="calibre2"><em class="calibre10">Reused Certificates</em></strong></h4>
<p class="noindent">Reused certificates are another surprising source of management certificates. Some IT professionals think that certificates are costly or difficult to create, so they simply reuse the same certificate everywhere. (Whereas certificates used for public-facing websites should come from a trusted public certificate authority and may be costly, self-signed certificates work just fine for Azure management—and they’re free.) As a result, you may find that the private key for the certificate used for SSL/TLS on a company’s website is also used for the company’s Azure subscription.</p>
<p class="indent">Attackers can’t retrieve the private key portion of a website’s certificate simply by visiting the site; instead, the web server must be compromised and the certificate store raided. Once that is accomplished, the attacker needs to extract the certificate from the server. Sadly for the pentester, most servers mark their certificates as “non-exportable,” which prevents them from being copied directly; however, Mimikatz is able to retrieve protected certificates.</p>
<p class="indent">To extract certificates from a server, run Mimikatz from an administrative command prompt and then issue these commands:</p>
<pre>mimikatz # <span class="codestrong1">crypto::capi</span><br class="calibre5"/>mimikatz # <span class="codestrong1">privilege::debug</span><br class="calibre5"/>mimikatz # <span class="codestrong1">crypto::cng</span><br class="calibre5"/>mimikatz # <span class="codestrong1">crypto::certificates /systemstore:</span><span class="codestrongitalic">local_machine</span> <span class="codestrong1">/store:</span><span class="codestrongitalic">my</span> <span class="codestrong1">/export</span></pre>
<p class="indent">The first three commands give Mimikatz access to the certificates. The final command exports all certificates from the local machine store’s personal certificate folder and saves them to the current working directory as both <em class="calibre7">.pfx</em> and <em class="calibre7">.cer</em> files. (For the names of other possible <code>store</code> and <code>systemstore</code> values, see <em class="calibre7"><a href="https://github.com/gentilkiwi/mimikatz/wiki/module-~-crypto/" class="calibre6">https://github.com/gentilkiwi/mimikatz/wiki/module-~-crypto/</a></em>.)</p>
<h4 class="h2" id="lev35"><strong class="calibre2"><em class="calibre10">Configuration Files</em></strong></h4>
<p class="noindent">Management certificates are typically used either to deploy a service or for an application to interact with a resource once it is running in Azure. Although Publish Settings files take care of service deployments, configuration files <span epub:type="pagebreak" id="page_25"/>can be used by applications connecting to Azure services. <em class="calibre7">Configuration files</em> typically have a <em class="calibre7">.config</em> extension and are most often named <em class="calibre7">app.config</em> (for applications) or <em class="calibre7">web.config</em> (for web services). The purpose of a configuration file is to move the details of a service outside of an application’s code and keep it in a user-editable XML file. This way, if the service moves or is renamed, the application doesn’t have to be recompiled. For example, instead of hard-coding the name and connection details of a SQL server into an application, you can save that information in XML format. The flaw in this approach from a security standpoint occurs when developers include both server addresses and unencrypted credentials in these configuration files.</p>
<p class="indent">The most commonly found credentials are connection strings for Azure SQL databases, including usernames and passwords in plaintext. The next most common are access keys used to interact with Azure Storage accounts because applications often need to read/write data to storage. (We’ll cover Azure Storage more in <a href="part0013.html#ch04" class="calibre6">Chapter 4</a>.)</p>
<p class="indent">Less commonly found is the type of credential we’re looking for: a base64-encoded management certificate. Because developers can use any name for variables in a configuration file, management certificates won’t always be obvious, but they’re easy enough to spot because they have certain characteristics. They’re usually the longest string in a configuration file (a little over 3,000 characters), they begin with a capital <em class="calibre7">M</em>, often end with one or two equals signs, and contain only base64 characters (A–Z, a–z, 0–9, +, /, and =).</p>
<p class="indent">Once you’ve found a certificate, copy it out of the file and save it with a <em class="calibre7">.pfx</em> extension. Because certificates can be used for non-Azure-related purposes, look through the configuration file for a subscription ID. If you find a subscription ID, the certificate is almost certainly used for Azure management, and you know at least one subscription where the certificate should be valid.</p>
<h4 class="h2" id="lev36"><strong class="calibre2"><em class="calibre10">Cloud Service Packages</em></strong></h4>
<p class="noindent">When a developer creates an application to deploy to Azure, Visual Studio packages up the entire deployment into a <em class="calibre7">Cloud Service Package</em> (<em class="calibre7">.cspkg</em>) file. These files are simply ZIP files with specific elements, including compiled code, configuration files, manifests, and dependencies. Although some of these files will have unusual extensions, almost every file in the package will be a ZIP file, an XML file, a plaintext file, or a compiled binary.</p>
<p class="indent">Whenever you encounter a Cloud Service Package, review its contents and try opening nested files in your favorite text editor and file compression tool. Because services in Azure often invoke other services in Azure (for example, an Azure website that gets content from Azure Storage and Azure SQL), you will sometimes find management certificates or other credentials embedded within the .<em class="calibre7">cspkg</em> file.</p>
<h3 class="h1" id="lev37"><span epub:type="pagebreak" id="page_26" class="calibre1"/><strong class="calibre2">Best Practices: Protecting Privileged Accounts</strong></h3>
<p class="noindent">Privileged accounts need to be tightly protected to prevent an attacker from taking control of the systems they administer. Some very effective ways to do this include the use of separate credentials, credential vaulting, Privileged Access Workstations, and just-in-time administration.</p>
<p class="indent">The most important step in protecting these credentials is to separate them from normal business tasks like checking email and browsing the web. Instead of granting a user’s standard account administrative rights to sensitive systems (or high-powered roles in Azure like Owner), create a separate account for the user that they use only for service administration. Additionally, ensure this account requires strong authentication, meaning a strong password with multi-factor authentication enabled—or even better, smartcard-based authentication. If the account does use a password, consider requiring the use of a secure password manager or vault to ensure that the password is long, frequently changed, and auditable.</p>
<p class="indent">Even with these protections in place, such an account can still be compromised if it is used from the same system where a user is browsing the web or opening documents from their standard account. Instead, the use of a Privileged Access Workstation (PAW) is a great way to reduce the sensitive account’s exposure by focusing on protecting the client used by an administrator. A PAW is a dedicated, hardened workstation that an administrator uses for accessing high-value systems, using an account they don’t use on other systems.</p>
<p class="indent">The PAW should be accessible only from the privileged account; the user should not be a local administrator. Additionally, the PAW should enforce predefined software and website whitelists, so only approved apps and sites can be accessed on the device (for example, the Azure portal). You can learn more about PAWs at <em class="calibre7"><a href="https://docs.microsoft.com/en-us/windows-server/identity/securing-privileged-access/privileged-access-workstations/" class="calibre6">https://docs.microsoft.com/en-us/windows-server/identity/securing-privileged-access/privileged-access-workstations/</a></em>.</p>
<p class="indent">To further limit the risk of one of these accounts being breached, consider using <em class="calibre7">just-in-time (JIT) administration</em> or <em class="calibre7">just enough admin (JEA)</em>. With JIT, accounts are present in highly privileged roles only when the user needs to perform an administrative task. Similarly, with JEA, the exact rights and responsibilities of each administrator are closely examined, and only the smallest set of permissions needed for a user to perform their work is granted. Azure supports JIT by using the Privileged Identity Management (PIM) feature. For more information about how to configure it, see <em class="calibre7"><a href="https://docs.microsoft.com/en-us/azure/active-directory/active-directory-privileged-identity-management-configure/" class="calibre6">https://docs.microsoft.com/en-us/azure/active-directory/active-directory-privileged-identity-management-configure/</a></em>.</p>
<h3 class="h1" id="lev38"><strong class="calibre2">Encountering Two-Factor Authentication</strong></h3>
<p class="noindent">For increased security against credential theft, some companies turn to <em class="calibre7">two-factor authentication (2FA)</em>, sometimes referred to as <em class="calibre7">multi-factor authentication (MFA)</em>. When signing in, the user must submit not only something they know (a password) but also proof of something they have in <span epub:type="pagebreak" id="page_27"/>their possession (such as a phone or smartcard) or something they are (biometric validation).</p>
<p class="indent">Two-factor authentication is natively supported by Azure and can be enabled by an administrator using the settings shown in <a href="part0011.html#ch02fig2" class="calibre6">Figure 2-2</a>, which can be found in the classic portal by selecting the <strong class="calibre4">Active Directory</strong> service, clicking <strong class="calibre4">Multi-Factor Auth Providers</strong>, and then clicking <strong class="calibre4">Manage</strong>.</p>
<div class="image1"><a id="ch02fig2" class="calibre6"/><img src="../images/00018.jpeg" alt="image" class="calibre3"/></div>
<p class="figcap"><em class="calibre7">Figure 2-2: Azure multi-factor authentication settings</em></p>
<p class="indent">If MFA is enabled, you’ll likely encounter a prompt for a second factor when authenticating with a username and password—typically one of the following:</p>
<ul class="calibre8">
<li class="noindent1">A code from an SMS text message sent to that user’s registered mobile phone</li>
<li class="noindent1">A code from a one-time-code-generating app such as Microsoft Authenticator</li>
<li class="noindent1">The user’s smartcard and its associated personal identification number (PIN)</li>
<li class="noindent1">An acknowledgment to a notification on the user’s smartphone from an enrolled mobile app</li>
<li class="noindent1">A phone call, which may provide a code or request a confirmation or PIN</li>
</ul>
<p class="indent">Assuming you don’t have the user’s mobile device, this can be a significant hurdle to overcome. Luckily, there are several ways to get around this obstacle.</p>
<h4 class="h2" id="lev39"><span epub:type="pagebreak" id="page_28" class="calibre1"/><strong class="calibre2"><em class="calibre10">Using Certificate Authentication</em></strong></h4>
<p class="noindent">One straightforward way to avoid 2FA is to authenticate to Azure using a management certificate instead of a username and password. Because certificate authentication is often used in automation, without a user present to enter a token, certificates are typically exempt from 2FA requirements. Although this may be a great option, certificates are limited to ASM access, so you may need a different bypass method to get to ARM resources.</p>
<h4 class="h2" id="lev40"><strong class="calibre2"><em class="calibre10">Using a Service Principal or a Service Account</em></strong></h4>
<p class="noindent">Another way to try to bypass MFA would be to obtain the credentials for a service account that has access to the target subscription. Service accounts are typically used either by a service to complete actions programmatically in Azure or with an account shared by a group of people at a company. In either case, 2FA is unlikely because services don’t have phones and groups can’t easily share 2FA tokens. This means service accounts are usually exempt from using a second factor.</p>
<h4 class="h2" id="lev41"><strong class="calibre2"><em class="calibre10">Accessing Cookies</em></strong></h4>
<p class="noindent">Notice in Azure’s multi-factor authentication settings page at the bottom of <a href="part0011.html#ch02fig2" class="calibre6">Figure 2-2</a> the option for users to flag devices as trusted for a period of time. This option is there to quell a common complaint of two-factor authentication: that entering a code or inserting a smartcard is tedious, especially on a system that a user logs in from frequently. With this setting enabled, a user may check a box during authentication to stop the system from re-prompting for credentials or 2FA tokens for a certain amount of time. This feature works by saving a cookie with a token in the user’s web browser after the user was successfully authenticated with 2FA. The token is a long, encrypted string that gives the bearer of the cookie immediate access to Azure. Note that this approach isn’t unique to Azure, but is common across many sites.</p>
<p class="indent">Because cookie storage is usually not particularly secure, all a pentester needs to do to grab that cookie is to gain access to the user’s workstation, copy the cookie, and then place it in the browser on their own system. Typically, these tokens are not prevented from working on a different host, so they can be used anywhere once retrieved.</p>
<p class="indent">The method to obtain a cookie varies based on the target user’s choice of web browser and the type of access the pentester has to the workstation. If the pentester can run code in the security context of the user, exporting cookies can be as simple as using a suitable post-exploitation framework. Don’t forget to check if the user has installed a cookie manager—like a real attacker, you might find that all the tools you need are already installed. Some browsers also store cookies without encryption on the file system, making them even easier to retrieve.</p>
<div class="sidebar">
<p class="sidebart"><span epub:type="pagebreak" id="page_29"/><strong class="calibre4">DEFENDER’S TIP</strong></p>
<p class="spara">Many sites rely on cookies containing encrypted tokens to validate a user’s requests after they’ve authenticated (and completed 2FA where applicable). Without these, a user would be re-prompted for credentials far too frequently. Since these cookies contain everything needed to make requests as the user to whom they were issued, they shouldn’t be left lying around. To prevent cookies from being stolen for critical sites like the Azure Portal, users should sign out as soon as they are finished with their administrative work, and also clear their cookies. (In this case, I’d suggest clearing cookies for at least the <em class="calibre10"><a href="http://microsoftonline.com" class="calibre11">microsoftonline.com</a></em> and <em class="calibre10"><a href="http://azure.com" class="calibre11">azure.com</a></em> domains.) Alternatively, “private” modes in most web browsers can be used, as they ensure these cookies don’t persist after the browser is closed.</p>
</div>
<h4 class="h2" id="lev42"><strong class="calibre2"><em class="calibre10">Proxying Traffic Through the User’s Browser</em></strong></h4>
<p class="noindent">An alternative to using cookies is to route web requests through a target user’s web browser so that these requests use the user’s session tokens and appear to come from their PC. The logistics of this method can be difficult: on the user’s system, you need to get a stealthy, malicious application running that can listen to requests from your system, route them through the user’s browser, and then obtain the responses and pass them back to you. Fortunately, this particular scenario is built into Cobalt Strike, a hacking command-and-control tool.</p>
<p class="indent">To create the proxy, you’ll need to have a Cobalt Strike server running and a Cobalt Strike payload package, known as a Beacon, deployed to the user’s system. From there, use the Browser Pivot command to create a proxy.</p>
<p class="indent">Now, with the proxy running, set your own browser to use the target system as a proxy server. At that point, web requests from your system will be routed through the target user’s web browser (completely invisible to the user). Your traffic will inherit the user’s sessions and credentials, bypassing any prompts. Using this method helps demonstrate to organizations that security issues on their workstations can lead to the compromise of cloud resources.</p>
<div class="note">
<p class="notet"><strong class="calibre2"><span class="notes">NOTE</span></strong></p>
<p class="notep"><em class="calibre7">You’ll find additional details on this scenario at</em> <a href="http://blog.cobaltstrike.com/2013/09/26/browser-pivoting-get-past-two-factor-auth/" class="calibre6">http://blog.cobaltstrike.com/2013/09/26/browser-pivoting-get-past-two-factor-auth/</a><em class="calibre7">. For Cobalt Strike–specific instructions, see</em> <a href="https://cobaltstrike.com/help-browser-pivoting" class="calibre6">https://cobaltstrike.com/help-browser-pivoting</a><em class="calibre7">.</em></p>
</div>
<div class="sidebar">
<p class="sidebart"><span epub:type="pagebreak" id="page_30"/><strong class="calibre4">DEFENDER’S TIP</strong></p>
<p class="spara">The browser proxy attack demonstrates that the need to secure important services isn’t limited to just the systems on which they run but expands to the entire environment, including engineers’ credentials and workstations. Once an attacker is on a user’s workstation, it can be hard to detect their activity because the web traffic appears to be coming from a legitimate user on their usual computer. However, you may be able to detect the Command and Control (C2) back-channel traffic that is forwarding the requests and responses from the workstation to the attacker’s system. For web traffic proxy attacks, this traffic will typically be larger and much more frequent than normal C2 network activity.</p>
</div>
<h4 class="h2" id="lev43"><strong class="calibre2"><em class="calibre10">Utilizing Smartcards</em></strong></h4>
<p class="noindent">The whole idea behind 2FA is that the user presents two items during authentication to prove who they are. The first factor is usually a password—something the user knows. The second factor either validates “something the user has” (such as a phone) or “something the user is” (such as fingerprints). Although the most common second factor involves validating that the person signing in has the correct phone through an authenticator app or text messaging, this isn’t the only option. Some organizations use <em class="calibre7">smartcards</em> (physical cards with an embedded cryptographic chip) to confirm the users are who they claim to be. Therefore, if smartcards are being used, then obtaining one is a possible way to bypass 2FA. There are two ways to get a user’s smartcard. The first is to gain control of a system where the smartcard is currently inserted and use it from there, and the second is to physically obtain the user’s card. Each method has its challenges.</p>
<p class="indent">Leveraging a smartcard inserted in a different system can be accomplished if you already have control of that system. Simply pass requests through that host using the method discussed in the previous section. The difficulty comes from the fact that you not only need access to the target user’s system but you must make the requests while the user has their smartcard inserted and after they’ve already entered their PIN (so it is cached).</p>
<p class="indent">When you’re stealing a user’s physical smartcard, the main challenges are actually obtaining the card, avoiding detection, and determining the user’s PIN. To overcome the first challenge, you have to find a way to get close to the user and take their smartcard without them noticing. This leads to the second impediment: most users will notice if their card is missing, especially if they rely on it to log in to their computer. Some companies’ smartcards also double as their employee badges and control access to their buildings, in which case the user is even more likely to realize what has happened and report it.</p>
<p class="indent"><span epub:type="pagebreak" id="page_31"/>Another challenge is that smartcards typically have PINs associated with them, which are required to unlock the cards and use them for authentication. You could try to guess the PIN (perhaps going with common number patterns or the user’s birthday), but the smartcard could be configured to lock after a specified number of incorrect PIN attempts. A better way is to obtain the user’s PIN directly—for instance, by installing a keylogger (either a physical device or a surreptitious application) on the user’s system to try to catch the PIN as they type it. However, an often more effective method is to grab the PIN out of the memory of the user’s computer while the card is in use.</p>
<p class="indent">Mimikatz can retrieve that smartcard’s PIN from memory as long as the user is logged in, their smartcard is inserted into the system, and they have used their smartcard to log in. If all these conditions are met, the PIN will appear in the Mimikatz output.</p>
<div class="sidebar">
<p class="sidebart"><strong class="calibre4">DEFENDER’S TIP</strong></p>
<p class="spara">To ensure that smartcards remain secure, it is important to isolate the process of issuing smartcard certificates from the rest of your infrastructure. Also, because there are often many different templates available, with a variety of sensitivity levels (virtual smartcards, VPN certificates, and so on), be sure to properly restrict which of those templates can be used to satisfy your 2FA requirements. Have thorough auditing, monitoring, and alerting in place for certificate operations.</p>
<p class="spara1">Additionally, you must ensure the security of the systems used to connect to sensitive servers, such as those that issue smartcards. Using a PAW, as discussed in “<a href="part0011.html#lev37" class="calibre11">Best Practices: Protecting Privileged Accounts</a>” on <a href="part0011.html#page_26" class="calibre11">page 26</a>, is a great way to achieve this. Because PAWs aren’t used for email or web browsing, they are much less likely to be compromised than an administrator’s primary system.</p>
</div>
<h4 class="h2" id="lev44"><strong class="calibre2"><em class="calibre10">Stealing a Phone or Phone Number</em></strong></h4>
<p class="noindent">This is probably the most difficult of the 2FA bypass options (and also the least likely to be allowed under standard rules of engagement), but if you pull it off, it has a high degree of success. As in the smartcard bypass, we are once again obtaining something that provides a second factor for authentication, only this time it is the user’s phone or control of their phone number.</p>
<p class="indent">The most obvious approach is simply to steal the target user’s phone. If the Azure subscription supports using text messages for authentication, that is ideal. Because many phone operating systems display the first line of a text message as a notification, on top of the lock screen, you can probably obtain a texted 2FA code without even unlocking the phone. When <span epub:type="pagebreak" id="page_32"/>authenticator app–generated codes are used, you will somehow need to guess or obtain the phone’s unlock code, if one is set. (This is beyond the scope of this book.)</p>
<p class="indent">Another option is to obtain the user’s phone number and authenticate with a text message option. Although most people consider a phone and its number to be a unit, mobile phones and their numbers are actually loosely coupled. In a number of recent reports, criminals were able to enter a local mobile phone store pretending to be a customer and convince the store to sell them a phone upgrade (billing the new phone to the real customer’s account). Because an Azure penetration tester’s goal isn’t to steal the latest smartphone, another tactic would be to tell the store clerk that you replaced your phone and need a new subscriber identification module (SIM) card. After leaving the store, simply insert the card into your phone and authenticate.</p>
<p class="indent">This option requires using text message or phone call authentication, because even when using a SIM card with the user’s phone number installed, the authentication apps wouldn’t be registered with the 2FA backend. This typically requires an out-of-band setup process that, hopefully, requires additional validation to confirm that the user performing the enrollment is who they claim to be.</p>
<div class="note">
<p class="notet"><strong class="calibre2"><span class="notes">NOTE</span></strong></p>
<p class="notep"><em class="calibre7">Aside from possibly being considered theft and potentially violating the phone provider’s terms of service, this is very risky. As soon as a new phone or SIM is issued on that user’s account, their existing number will be transferred to it and the user’s existing phone will be disabled. Most users will notice very quickly when their phone no longer has service, so know that once the theft is perpetrated, the time until the incident is reported is extremely limited. In other words, you are likely to be caught and removed from the target subscription very quickly. Save this option for a last resort and always consult your client and an attorney before attempting it!</em></p>
</div>
<h4 class="h2" id="lev45"><strong class="calibre2"><em class="calibre10">Prompting the User for 2FA</em></strong></h4>
<p class="noindent">Finally, it may be possible to trick the user into giving up their 2FA token through <em class="calibre7">social engineering</em>, which is the process of convincing a user to do something they wouldn’t normally do. This method is probably the least likely to succeed because it relies on the user not noticing something is amiss, so only use it if you are desperate. If the user is set up on their phone to receive a pop-up alert that they need to acknowledge, this could be as simple as triggering the authentication request and seeing if the user accepts it. It is unlikely, but some users are so conditioned to acknowledge prompts that they will do so even when they are not expecting one. Of course, a savvy user may report such an event to their security team.</p>
<p class="indent">A slightly more advanced variation on this approach is to try to watch the user’s activity and send a message when they are expecting this prompt. Perhaps you suspect this user always logs in to the Azure Portal when they arrive at work and you can time the prompt to coincide with this. Or maybe you notice they work from a coffee shop and can see when they log in and <span epub:type="pagebreak" id="page_33"/>send the request then. Many users would think that their initial authorization did not go through and that the system must simply be prompting them again.</p>
<p class="indent">If the user relies on entering codes from text messages or an authenticator application, it still may be possible to obtain the code. Two common ways to do this are through phishing websites and phone calls.</p>
<p class="indent">To demonstrate how an attacker could use phishing to obtain 2FA codes, you would first set up a page as we did in “Phishing” on <a href="part0011.html#page_19" class="calibre6">page 19</a>. Next, you would modify the web page so that after prompting for the username and password, the page asks for the user’s 2FA code. Because time is of the essence, you need to design the page so that as soon as this information is submitted, the site invokes a script on your machine to authenticate to Azure, thus providing you access. As in the earlier example, the page should then redirect the user to the real logon page so that they believe something went wrong with their authentication. Once the site is functional, you would email the user a link, as before.</p>
<p class="indent">Another way to obtain a code from the user would be to call them and ask for it. For this to work, you would need to use <em class="calibre7">pretexting</em>, or making up some legitimate-sounding reason for the call. For example, you could claim to be from their IT department and that, due to a data corruption issue in the user database, you need their current code to re-enable their access. This method is probably as likely to get you reported as it is to get you a valid code, but it can be used as a last resort.</p>
<div class="sidebar">
<p class="sidebart"><strong class="calibre4">DEFENDER’S TIP</strong></p>
<p class="spara">Despite some of the weaknesses in multi-factor authentication described in this section, it is still one of the best ways to slow or prevent an attacker from gaining access to a subscription. It increases an attacker’s time to compromise considerably, especially if the target subscription has a minimal number of management certificates and service accounts. Given that multi-factor support is built in to Azure, it is relatively easy to enable. To get started, visit <em class="calibre10"><a href="https://azure.microsoft.com/en-us/documentation/articles/multi-factor-authentication/" class="calibre11">https://azure.microsoft.com/en-us/documentation/articles/multi-factor-authentication/</a></em>.</p>
</div>
<h3 class="h1" id="lev46"><strong class="calibre2">Summary</strong></h3>
<p class="noindent">In this chapter, we discussed the two different Azure models—Azure Service Management and Azure Resource Manager—and how each may impact a penetration test. I demonstrated various ways to obtain credentials for Azure, including recovering passwords from plaintext documents, phishing, using memory, and even guessing. Next, we looked at using certificates for authentication and places they might be found, such as Publish Settings files, recycled certificates in the certificate store, configuration <span epub:type="pagebreak" id="page_34"/>files, and Cloud Service Packages. Finally, we examined two-factor authentication bypasses via certificates, service accounts, stolen cookies, stolen phone numbers, and social engineering.</p>
<p class="indent">Studying these access methods, we identified areas where users may have left behind old credentials that are no longer in use. Cleaning up these items reduces the attack surface of a client’s subscription. Additionally, testing accounts for weak passwords can help find vulnerable credentials before an attacker discovers them, as well as help teach users about proper password construction, in case the client is not already using <em class="calibre7">high-entropy</em> (highly random, unpredictable) computer-generated passwords for everything other than primary user accounts. Finally, we saw how much more difficult it is to gain illegitimate access to a subscription when multi-factor authentication is used consistently across all accounts.</p>
<p class="indent">In the next chapter, you’ll explore the subscriptions you’ve compromised in your engagement and get a high-level view of the services running inside them.</p>
</body></html>