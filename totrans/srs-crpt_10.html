<html><head></head><body>
<h2 class="h2" id="ch9"><span epub:type="pagebreak" id="page_163"/><span class="big">9</span><br/>HARD PROBLEMS</h2>
<div class="image9"><img src="../images/common01.jpg" alt="image"/></div>
<p class="noindent">Hard computational problems are the cornerstone of modern cryptography. They’re problems that are simple to describe yet practically impossible to solve. These are problems for which even the best algorithm wouldn’t find a solution before the sun burns out.</p>
<p class="indent">In the 1970s, the rigorous study of hard problems gave rise to a new field of science called computational complexity theory, which would dramatically impact cryptography and many other fields, including economics, physics, and biology. In this chapter, I’ll give you the conceptual tools from complexity theory necessary to understand the foundations of cryptographic security, and I’ll introduce the hard problems behind public-key schemes such as RSA encryption and Diffie–Hellman key agreement. We’ll touch on some deep concepts, but I’ll minimize the technical details and only scratch the surface. Still, I hope you’ll see the beauty in the way cryptography leverages computational complexity theory to maximize security.</p>
<h3 class="h3" id="lev1sec60"><span epub:type="pagebreak" id="page_164"/>Computational Hardness</h3>
<p class="noindent">A computational problem is a question that can be answered by doing enough computation, for example, “Is 2017 a prime number?” or “How many <em>i</em> letters are there in <em>incomprehensibilities</em>?” <em>Computational hardness</em> is the property of computational problems for which there is no algorithm that will run in a reasonable amount of time. Such problems are also called <em>intractable problems</em> and are often practically impossible to solve.</p>
<p class="indent">Surprisingly, computational hardness is independent of the type of computing device used, be it a general-purpose CPU, an integrated circuit, or a mechanical Turing machine. Indeed, one of the first findings of computational complexity theory is that all computing models are equivalent. If a problem can be solved efficiently with one computing device, it can be solved efficiently on any other device by porting the algorithm to the other device’s language—an exception is quantum computers, but these do not exist (yet). The upshot is that we won’t need to specify the underlying computing device or hardware when discussing computational hardness; instead, we’ll just discuss algorithms.</p>
<p class="indent">To evaluate hardness, we’ll first find a way to measure the complexity of an algorithm, or its running time. We’ll then categorize running times as hard or easy.</p>
<h4 class="h4" id="lev2sec114"><em>Measuring Running Time</em></h4>
<p class="noindent">Most developers are familiar with <em>computational complexity</em>, or the approximate number of operations done by an algorithm as a function of its input size. The size is counted in bits or in the number of elements taken as input. For example, take the algorithm shown in <a href="ch09.xhtml#ch9list1">Listing 9-1</a>, written in pseudocode. It searches for a value, <em>x</em>, within an array of <em>n</em> elements and then returns its index position.</p>
<p class="programs">search(x, array, n):<br/>    for i from 1 to n {<br/>        if (array[i] == x) {<br/>            return i;<br/>        }<br/>    }<br/>    return 0;<br/>}</p>
<p class="figcap"><a id="ch9list1"/><em>Listing 9-1: A simple search algorithm, written in pseudocode, of complexity linear with respect to the array length</em> n. <em>The algorithm returns the index where the value</em> x <em>is found in [1,</em> n<em>], or 0 if</em> x <em>isn’t found in the array.</em></p>
<p class="indent">In this algorithm, we use a <span class="literal">for</span> loop to find a specific value, <em>x</em>, by iterating through an array. On each iteration, we assign the variable <em>i</em> a number starting with 1. Then we check whether the value of position <em>i</em> in <span class="literal">array</span> is equal to the value of <em>x</em>. If it is, we return the position <em>i</em>. Otherwise, we increment <em>i</em> and try the next position until we reach <em>n</em>, the length of the array, at which point we return 0.</p>
<p class="indent"><span epub:type="pagebreak" id="page_165"/>For this kind of algorithm, we count complexity as the number of iterations of the <span class="literal">for</span> loop: 1 in the best case (if <em>x</em> is equal to <span class="literal">array[1]</span>), <em>n</em> in the worst case (if <em>x</em> is equal to <span class="literal">array[n]</span> or if <em>x</em> is not in found in <span class="literal">array</span>), and <em>n</em>/2 on average if <em>x</em> is randomly distributed in one of the <em>n</em> cells of the array. With an array 10 times as large, the algorithm will be 10 times as slow. Complexity is therefore proportional to <em>n</em>, or “linear” in <em>n</em>. A complexity linear in <em>n</em> is considered fast, as opposed to complexities exponential in <em>n</em>. Although processing larger input values will be slower, it will make a difference of at most just seconds for most practical uses.</p>
<p class="indent">But many useful algorithms are slower than that and have a complexity higher than linear. The textbook example is sorting algorithms: given a list of <em>n</em> values in a random order, you’ll need on average <em>n</em> × log <em>n</em> basic operations to sort the list, which is sometimes called <em>linearithmic complexity</em>. Since <em>n</em> × log <em>n</em> grows faster than <em>n</em>, sorting speed will slow down faster than proportionally to <em>n</em>. Yet such sorting algorithms will remain in the realm of <em>practical</em> computation, or computation that can be carried out in a reasonable amount of time.</p>
<p class="indent">At some point, we’ll hit the ceiling of what’s feasible even for relatively small input lengths. Take the simplest example from cryptanalysis: the brute-force search for a secret key. Recall from <a href="ch01.xhtml#ch1">Chapter 1</a> that given a plaintext <em>P</em> and a ciphertext <em>C</em> = <strong>E</strong>(<em>K</em>, <em>P</em>), it takes at most 2<sup><em>n</em></sup> attempts to recover an <em>n</em>-bit symmetric key because there are 2<sup><em>n</em></sup> possible keys—an example of a complexity that grows exponentially. For complexity theorists, <em>exponential complexity</em> means a problem that is practically impossible to solve, because as <em>n</em> grows, the effort very rapidly becomes infeasible.</p>
<p class="indent">You may object that we’re comparing oranges and apples here: in the <span class="literal">search()</span> function in <a href="ch09.xhtml#ch9list1">Listing 9-1</a>, we counted the number of <span class="literal">if (array[i] == x)</span> operations, whereas key recovery counts the number of encryptions, each thousands of times slower than a single <span class="literal">==</span> comparison. This inconsistency can make a difference if you compare two algorithms with very similar complexities, but most of the time it won’t matter because the number of operations will have a greater impact than the cost of an individual operation. Also, complexity estimates ignore <em>constant factors</em>: when we say that an algorithm takes time in the order of <em>n</em><sup>3</sup> operations (which is <em>quadratic com</em><em>plexity</em>), it may actually take 41 × <em>n</em><sup>3</sup> operations, or 12345 × <em>n</em><sup>3</sup> operations—but again, as <em>n</em> grows, the constant factors lose significance to the point that we can ignore them. Complexity analysis is about theoretical hardness as a function of the input size; it doesn’t care about the exact number of CPU cycles it will take on your computer.</p>
<p class="indent">You’ll often find the <em>O</em>() notation (“big O”) used to express complexities. For example, <em>O</em>(<em>n</em><sup>3</sup>) means that complexity grows no faster than <em>n</em><sup>3</sup>, ignoring potential constant factors. <em>O</em>() denotes the <em>upper bound</em> of an algorithm’s complexity. The notation <em>O</em>(1) means that an algorithm runs in <em>constant time</em>—that is, the running time doesn’t depend on the input length! For example, the algorithm that determines an integer’s parity by looking at its least significant bit (LSB) and returning “even” if it’s zero and “odd” otherwise will do the same thing at the same cost whatever the integer’s length.</p>
<p class="indent"><span epub:type="pagebreak" id="page_166"/>To see the difference between linear, quadratic, and exponential time complexities, look at how complexity grows for <em>O</em>(<em>n</em>) (linear) versus <em>O</em>(<em>n</em><sup>2</sup>) (quadratic) versus <em>O</em>(2<sup><em>n</em></sup>) (exponential) in <a href="ch09.xhtml#ch9fig1">Figure 9-1</a>.</p>
<div class="image"><img src="../images/f09-01.jpg" alt="image"/></div>
<p class="figcap"><a id="ch9fig1"/><em>Figure 9-1: Growth of exponential, quadratic, and linear complexities, from the fastest to the slowest growing</em></p>
<p class="indent">Exponential complexity means the problem is practically impossible to solve, and linear complexity means the solution is feasible, whereas quadratic complexity is somewhere between the two.</p>
<h4 class="h4" id="lev2sec115"><em>Polynomial vs. Superpolynomial Time</em></h4>
<p class="noindent">The <em>O</em>(<em>n</em><sup>2</sup>) complexity discussed in the last section (the middle curve in <a href="ch09.xhtml#ch9fig1">Figure 9-1</a>) is a special case of the broader class of polynomial complexities, or <em>O</em>(<em>n</em><sup><em>k</em></sup>), where <em>k</em> is some fixed number such as 3, 2.373, 7/10, or the square root of 17. Polynomial-time algorithms are eminently important in complexity theory and in crypto because they’re the very definition of practically feasible. When an algorithm runs in <em>polynomial time</em>, or <em>poly</em><em>time</em> for short, it will complete in a decent amount of time even if the input is large. That’s why polynomial time is synonymous with “efficient” for complexity theorists and cryptographers.</p>
<p class="indent">In contrast, algorithms running in <em>superpolynomial</em> <em>time</em>—that is, in <em>O</em>(<em>f</em>(<em>n</em>)), where <em>f</em>(<em>n</em>) is any function that grows faster than any poly­nomial—are viewed as impractical. I’m saying superpolynomial, and not just exponential, because there are complexities in between polynomial and the well-known exponential complexity <em>O</em>(2<sup><em>n</em></sup>), such as <em>O</em>(<em>n</em><sup>log(<em>n</em>)</sup>), as <a href="ch09.xhtml#ch9fig2">Figure 9-2</a> shows.</p>
<div class="image"><span epub:type="pagebreak" id="page_167"/><img src="../images/f09-02.jpg" alt="image"/></div>
<p class="figcap"><a id="ch9fig2"/><em>Figure 9-2: Growth of the</em> 2<sup>n</sup>, n<sup>log(n)</sup>, <em>and</em> n<sup>2</sup> <em>functions, from the fastest to the slowest growing</em></p>
<div class="note">
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>
<p class="notep"><em>Exponential complexity</em> O(2<sup>n</sup>) <em>is not the worst you can get. Some complexities grow even faster and thus characterize algorithms even slower to compute—for example, the complexity</em> O(n<sup>n</sup>) <em>or the</em> exponential factorial O(n<sup>f(n – 1)</sup>), <em>where for any</em> x, <em>the function</em> f <em>is here recursively defined as</em> f(x) = x<sup>f(x – 1)</sup>. <em>In practice, you’ll never encounter algorithms with such preposterous complexities.</em></p>
</div>
<p class="indent"><em>O</em>(<em>n</em><sup>2</sup>) or <em>O</em>(<em>n</em><sup>3</sup>) may be efficient, but <em>O</em>(<em>n</em><sup>99999999999</sup>) obviously isn’t. In other words, polytime is fast as long as the exponent isn’t too large. Fortunately, all polynomial-time algorithms found to solve actual problems do have small exponents. For example, <em>O</em>(<em>n</em><sup>1.465</sup>) is the time for multiplying two <em>n</em>-bit integers, or <em>O</em>(<em>n</em><sup>2.373</sup>) for multiplying two <em>n</em> × <em>n</em> matrices. The 2002 breakthrough polytime algorithm for identifying prime numbers initially had a complexity <em>O</em>(<em>n</em><sup>12</sup>), but it was later improved to <em>O</em>(<em>n</em><sup>6</sup>). Polynomial time thus may not be the perfect definition of a practical time for an algorithm, but it’s the best we have.</p>
<p class="indent">By extension, a problem that can’t be solved by a polynomial-time algorithm is considered impractical, or <em>hard</em>. For example, for a straightforward key search, there’s no way to beat the <em>O</em>(2<sup><em>n</em></sup>) complexity unless the cipher is somehow broken.</p>
<p class="indent">We know for sure that there’s no way to beat the <em>O</em>(2<sup><em>n</em></sup>) complexity of a brute-force key search (as long as the cipher is secure), but we don’t always know what the fastest way to solve a problem is. A large portion of the research in complexity theory is about proving complexity <em>bounds</em> on the <span epub:type="pagebreak" id="page_168"/>running time of algorithms solving a given problem. To make their job easier, complexity theorists have categorized computational problems in different groups, or <em>classes</em>, according to the effort needed to solve them.</p>
<h3 class="h3" id="lev1sec61">Complexity Classes</h3>
<p class="noindent">In mathematics, a <em>class</em> is a group of objects with some similar attribute. For example, all computational problems solvable in time <em>O</em>(<em>n</em><sup>2</sup>), which complexity theorists simply denote <strong>TIME</strong>(<em>n</em><sup>2</sup>), are one class. Likewise, <strong>TIME</strong>(<em>n</em><sup>3</sup>) is the class of problems solvable in time <em>O</em>(<em>n</em><sup>3</sup>), <strong>TIME</strong>(2<sup><em>n</em></sup>) is the class of problems solvable in time <em>O</em>(2<sup><em>n</em></sup>), and so on. For the same reason that a supercomputer can compute whatever a laptop can compute, any problem solvable in <em>O</em>(<em>n</em><sup>2</sup>) is also solvable in <em>O</em>(<em>n</em><sup>3</sup>). Hence, any problem in the class <strong>TIME</strong>(<em>n</em><sup>2</sup>) also belongs to the class <strong>TIME</strong>(<em>n</em><sup>3</sup>), which both also belong to the class <strong>TIME</strong>(<em>n</em><sup>4</sup>), and so on. The union of all these classes of problems, <strong>TIME</strong>(<em>n</em><sup><em>k</em></sup>), where <em>k</em> is a constant, is called <strong>P</strong>, which stands for polynomial time.</p>
<p class="indent">If you’ve ever programmed a computer, you’ll know that seemingly fast algorithms may still crash your system by eating all its memory resources. When selecting an algorithm, you should not only consider its time complexity but also how much memory it uses, or its <em>space complexity</em>. This is especially important because a single memory access is usually orders of magnitudes slower than a basic arithmetic operation in a CPU.</p>
<p class="indent">Formally, you can define an algorithm’s memory consumption as a function of its input length, <em>n</em>, in the same way we defined time complexity. The class of problems solvable using <em>f</em>(<em>n</em>) bits of memory is <strong>SPACE</strong>(<em>f</em>(<em>n</em>)). For example, <strong>SPACE</strong>(<em>n</em><sup>3</sup>) is the class of problems solvable using of the order of <em>n</em><sup>3</sup> bits of memory. Just as we had <strong>P</strong> as the union of all <strong>TIME</strong>(<em>n</em><sup><em>k</em></sup>), the union of all <strong>SPACE</strong>(<em>n</em><sup><em>k</em></sup>) problems is called <strong>PSPACE</strong>.</p>
<p class="indent">Obviously, the lower the memory the better, but a polynomial amount of memory doesn’t necessarily imply that an algorithm is practical. Why? Well, take for example a brute-force key search: again, it takes only negligible memory but is slow as hell. More generally, an algorithm can take forever, even if it uses just a few bytes of memory.</p>
<p class="indent">Any problem solvable in time <em>f</em>(<em>n</em>) needs at most <em>f</em>(<em>n</em>) memory, so <strong>TIME</strong>(<em>f</em>(<em>n</em>)) is included in <strong>SPACE</strong>(<em>f</em>(<em>n</em>)). In time <em>f</em>(<em>n</em>), you can only write up to <em>f</em>(<em>n</em>) bits, and no more, because writing (or reading) 1 bit is assumed to take one unit of time; therefore, any problem in <strong>TIME</strong>(<em>f</em>(<em>n</em>)) can’t use more than <em>f</em>(<em>n</em>) space. As a consequence, <strong>P</strong> is a subset of <strong>PSPACE</strong>.</p>
<h4 class="h4" id="lev2sec116"><em>Nondeterministic Polynomial Time</em></h4>
<p class="noindent"><strong>NP</strong> is the second most important complexity class, after the class <strong>P</strong> of all polynomial-time algorithms. No, <strong>NP</strong> doesn’t stand for non-polynomial time, but for <em>nondeterministic</em> polynomial time. What does that mean?</p>
<p class="indent"><strong>NP</strong> is the class of problems for which a solution can be verified in polynomial time—that is, efficiently—even though the solution may be hard to find. By <em>verified</em>, I mean that given a potential solution, you can run some polynomial-time algorithm that will verify whether you’ve found <span epub:type="pagebreak" id="page_169"/>an actual solution. For example, the problem of recovering a secret key with a known plaintext is in <strong>NP</strong>, because given <em>P</em>, <em>C</em> = <strong>E</strong>(<em>K</em>, <em>P</em>), and some candidate key <em>K</em><sub>0</sub>, you can check that <em>K</em><sub>0</sub> is the correct key by verifying that <strong>E</strong>(<em>K</em><sub>0</sub>, <em>P</em>) equals <em>C</em>. The process of finding a potential key (the solution) can’t be done in polynomial time, but checking whether the key is correct is done using a polynomial-time algorithm.</p>
<p class="indent">Now for a counterexample: what about known-ciphertext attacks? This time, you only get some <strong>E</strong>(<em>K</em>, <em>P</em>) values for random unknown plaintext <em>P</em>s. If you don’t know what the <em>P</em>s are, then there’s no way to verify whether a potential key, <em>K</em><sub>0</sub>, is the right one. In other words, the key-recovery problem under known-ciphertext attacks is not in <strong>NP</strong> (let alone in <strong>P</strong>).</p>
<p class="indent">Another example of a problem not in <strong>NP</strong> is that of verifying the <em>absence</em> of a solution to a problem. Verifying that a solution is correct boils down to computing some algorithm with the candidate solution as an input and then checking the return value. However, to verify that <em>no</em> solution exists, you may need to go through all possible inputs. And if there’s an exponential number of inputs, you won’t be able to efficiently prove that no solution exists. The absence of a solution is hard to show for the hardest problems in the class <strong>NP</strong>—the so-called <strong>NP</strong>-complete problems, which we’ll discuss next.</p>
<h4 class="h4" id="lev2sec117"><em>NP-Complete Problems</em></h4>
<p class="noindent">The hardest problems in the class <strong>NP</strong> are called <strong>NP</strong>-complete; we don’t know how to solve these problems in polynomial time. And as complexity theorists discovered in the 1970s when they developed the theory of <strong>NP</strong>-completeness, <strong>NP</strong>’s hardest problems are all equally hard. This was proven by showing that any efficient solution to any of the <strong>NP</strong>-complete problems can be turned into an efficient solution for any of the other <strong>NP</strong>-complete problems. In other words, if you can solve any <strong>NP</strong>-complete problem efficiently, you can solve all of them, as well as all problems in <strong>NP</strong>. How can this be?</p>
<p class="indent"><strong>NP</strong>-complete problems come in different disguises, but they’re fundamentally similar from a mathematical perspective. In fact, you can reduce any <strong>NP</strong>-complete problem to any other <strong>NP</strong>-complete problem such that solving the first one depends on solving the second.</p>
<p class="indentb">Here are some examples of <strong>NP</strong>-complete problems:</p>
<p class="hang"><strong>The traveling salesman problem</strong> Given a set of points on a map (cities, addresses, or other geographic locations) and the distances between each point from each other point, find a path that visits every point such that the total distance is smaller than a given distance of <em>x</em>.</p>
<p class="hang"><strong>The clique problem</strong> Given a number, <em>x</em>, and a graph (a set of nodes connected by edges, as in <a href="ch09.xhtml#ch9fig3">Figure 9-3</a>), determine if there’s a set of <em>x</em> points or less such that all points are connected to each other.</p>
<p class="hang"><strong>The knapsack problem</strong> Given two numbers, <em>x</em> and <em>y</em>, and a set of items, each of a known value and weight, can we pick a group of items such that the total value is at least <em>x</em> and the total weight at most <em>y</em>?</p>
<div class="image"><span epub:type="pagebreak" id="page_170"/><img src="../images/f09-03.jpg" alt="image"/></div>
<p class="figcap"><a id="ch9fig3"/><em>Figure 9-3: A graph containing a clique of four points. The general problem of finding a clique (set of nodes all connected to each other) of given size in a graph is <strong>NP</strong>-complete.</em></p>
<p class="indent">Such <strong>NP</strong>-complete problems are found everywhere, from scheduling problems (given jobs of some priority and duration, and one or more processors, assign jobs to the processors by respecting the priority while minimizing total execution time) to constraint-satisfaction problems (determine values that satisfy a set of mathematical constraints, such as logical equations). Even the task of winning in certain video games can sometimes be proven to be <strong>NP</strong>-complete (for famous games including <em>Tetris</em>, <em>Super Mario Bros.</em>, <em>Pokémon</em>, and <em>Candy Crush Saga</em>). For example, the article “Classic Nintendo Games Are (Computationally) Hard” (<em><a href="https://arxiv.org/abs/1203.1895">https://arxiv.org/abs/1203.1895</a></em>) considers “the decision problem of reachability” to determine the possibility of reaching the goal point from a particular starting point.</p>
<p class="indent">Some of these video game problems are actually even harder than <strong>NP</strong>-complete and are called <strong>NP</strong>-<em>hard</em>. We say that a problem is <strong>NP</strong>-hard when it’s at least as hard as <strong>NP</strong>-complete problems. More formally, a problem is <strong>NP</strong>-hard if what it takes to solve it can be proven to also solve <strong>NP</strong>-complete problems.</p>
<p class="indent">I have to mention an important caveat. Not all <em>instances</em> of <strong>NP</strong>-complete problems are actually hard to solve. Some specific instances, because they’re small or because they have a specific structure, may be solved efficiently. Take, for example, the graph in <a href="ch09.xhtml#ch9fig3">Figure 9-3</a>. By just looking at it for a few seconds you’ll spot the clique, which is the top four connected nodes—even though the aforementioned clique problem is <strong>NP</strong>-hard, there’s nothing hard here. So being <strong>NP</strong>-complete doesn’t mean that all instances of a given problem are hard, but that as the problem size grows, many of them are.</p>
<h4 class="h4" id="lev2sec118"><em>The P vs. NP Problem</em></h4>
<p class="noindent">If you could solve the hardest <strong>NP</strong> problems in polynomial time, then you could solve <em>all</em> <strong>NP</strong> problems in polynomial time, and therefore <strong>NP</strong> would <span epub:type="pagebreak" id="page_171"/>equal <strong>P</strong>. That sounds preposterous; isn’t it obvious that there are problems for which a solution is easy to verify but hard to find? For example, isn’t it obvious that exponential-time brute force is the fastest way to recover the key of a symmetric cipher, and therefore that the problem can’t be in <strong>P</strong>? It turns out that, as crazy as it sounds, no one has proved that <strong>P</strong> is different from <strong>NP</strong>, despite a bounty of literally one million dollars.</p>
<p class="indent">The Clay Mathematics Institute will award this bounty to anyone who proves that either <strong>P</strong> ≠ <strong>NP</strong> or <strong>P</strong> = <strong>NP</strong>. This problem, known as <strong>P</strong> vs. <strong>NP</strong>, was called “one of the deepest questions that human beings have ever asked” by renowned complexity theorist Scott Aaronson. Think about it: if <strong>P</strong> were equal to <strong>NP</strong>, then any easily checked solution would also be easy to find. All cryptography used in practice would be insecure, because you could recover symmetric keys and invert hash functions efficiently.</p>
<div class="image"><img src="../images/f09-04.jpg" alt="image"/></div>
<p class="figcap"><a id="ch9fig4"/><em>Figure 9-4: The classes <strong>NP</strong>, <strong>P</strong>, and the set of <strong>NP</strong>-complete problems</em></p>
<p class="indent">But don’t panic: most complexity theorists believe <strong>P</strong> isn’t equal to <strong>NP</strong>, and therefore that <strong>P</strong> is instead a strict subset of <strong>NP</strong>, as <a href="ch09.xhtml#ch9fig4">Figure 9-4</a> shows, where <strong>NP</strong>-complete problems are another subset of <strong>NP</strong> not overlapping with <strong>P</strong>. In other words, problems that look hard actually are hard. It’s just difficult to prove this mathematically. While proving that <strong>P</strong> = <strong>NP</strong> would only need a polynomial-time algorithm for an <strong>NP</strong>-complete problem, proving the nonexistence of such an algorithm is fundamentally harder. But this didn’t stop wacky mathematicians from coming up with simple proofs that, while usually obviously wrong, often make for funny reads; for an example, see “The P-versus-NP page” (<em><a href="https://www.win.tue.nl/~gwoegi/P-versus-NP.htm">https://www.win.tue.nl/~gwoegi/P-versus-NP.htm</a></em>).</p>
<p class="indent">Now if we’re almost sure that hard problems do exist, what about leveraging them to build strong, provably secure crypto? Imagine a proof that breaking some cipher is <strong>NP</strong>-complete, and therefore that the cipher is unbreakable as long as <strong>P</strong> isn’t equal to <strong>NP</strong>. But reality is disappointing: <strong>NP</strong>-complete problems have proved difficult to use for crypto purposes because the very structure that makes them hard in general can make them easy in specific cases—cases that sometimes occur in crypto. Instead, cryptography often relies on problems that are <em>probably not</em> <strong>NP</strong>-hard.</p>
<h3 class="h3" id="lev1sec62">The Factoring Problem</h3>
<p class="noindent">The factoring problem consists of finding the prime numbers <em>p</em> and <em>q</em> given a large number, <em>N</em> = <em>p</em> × <em>q</em>. The widely used RSA algorithms are based on the fact that factoring a number is difficult. In fact, the hardness of the factoring problem is what makes RSA encryption and signature schemes secure. But before we see how RSA leverages the factoring problem in <a href="ch10.xhtml#ch10">Chapter 10</a>, I’d like to convince you that this problem is indeed hard, yet probably not <strong>NP</strong>-complete.</p>
<p class="indent"><span epub:type="pagebreak" id="page_172"/>First, some kindergarten math. A <em>prime number</em> is a number that isn’t divisible by any other number but itself and 1. For example, the numbers 3, 7, and 11 are prime; the numbers 4 = 2 × 2, 6 = 2 × 3, and 12 = 2 × 2 × 3 are not prime. A fundamental theorem of number theory says that any integer number can be uniquely written as a product of primes, a representation called the <em>factorization</em> of that number. For example, the factorization of 123456 is 2<sup>6</sup> × 3 × 643; the factorization of 1234567 is = 127 × 9721; and so on. Any integer has a unique factorization, or a unique way to write it as a product of prime numbers. But how do we know that a given factorization contains only prime numbers or that a given number is prime? The answer is found through polynomial-time primality testing algorithms, which allow us to efficiently test whether a given number is prime. Getting from a number to its prime factors, however, is another matter.</p>
<h4 class="h4" id="lev2sec119"><em>Factoring Large Numbers in Practice</em></h4>
<p class="noindent">So how do we go from a number to its factorization—namely, its decomposition as a product of prime numbers? The most basic way to factor a number, <em>N</em>, is to try dividing it by all the numbers lower than it until you find a number, <em>x</em>, that divides <em>N</em>. Then attempt to divide <em>N</em> with the next number, <em>x</em> + 1, and so on. You’ll end up with a list of factors of <em>N</em>. What’s the time complexity of this? First, remember that we express complexities as a function of the input’s <em>length</em>. The bit length of the number <em>N</em> is <em>n</em> = log<sub>2</sub> <em>N</em>. By the basic definition of logarithm, this means that <em>N</em> = 2<sup><em>n</em></sup>. Because all the numbers less than <em>N</em>/2 are reasonable guesses for possible factors of <em>N</em>, there are about <em>N/</em>2 = 2<sup><em>n</em></sup>/2 values to try. The complexity of our naive factoring algorithm is therefore <em>O</em>(2<sup><em>n</em></sup>), ignoring the 1/2 coefficient in the <em>O</em>() notation.</p>
<p class="indent">Of course, many numbers are easy to factor by first finding any small factors (2, 3, 5, and so on) and then iteratively factoring any other nonprime factors. But here we’re interested in numbers of the form <em>N</em> = <em>p</em> × <em>q</em>, where <em>p</em> and <em>q</em> are large, as found in cryptography.</p>
<p class="indent">Let’s be a bit smarter. We don’t need to test all numbers lower than <em>N</em>/2, but rather only the prime numbers, and we can start by trying only those smaller than the square root of <em>N</em>. Indeed, if <em>N</em> is not a prime number, then it has to have at least one factor lower than its square root √<em>N.</em> This is because if both of <em>N</em>’s factors <em>p</em> and <em>q</em> are greater than √<em>N</em>, then their product would be greater than √<em>N</em> × √<em>N</em> = <em>N</em>, which is impossible. For example, if we say <em>N</em> = 100, then its factors <em>p</em> and <em>q</em> can’t both be greater than 10 because that would result in a product greater than 100. Either <em>p</em> or <em>q</em> has to be smaller than √<em>N</em>.</p>
<p class="indent">So what’s the complexity of testing only the primes less than √<em>N</em>? The <em>prime number theorem</em> states that there are approximately <em>N</em>/log <em>N</em> primes less than <em>N</em>. Hence, there are approximately √<em>N</em>/log √<em>N</em> primes less than √<em>N</em>. Expressing this value, we get approximately 2<sup><em>n</em>/2</sup>/<em>n</em> possible prime factors and therefore a complexity of <em>O</em>(2<sup><em>n</em>/2</sup>/<em>n</em>), since √<em>N</em> = 2<sup><em>n</em>/2</sup> and 1/log√<em>N</em> = 1/(<em>n</em>/2) = 2<em>n</em>. This is faster than testing all prime numbers, but it’s still painfully slow—on the order of 2<sup>120</sup> operations for a 256-bit number. That’s quite an impractical computational effort.</p>
<p class="indent"><span epub:type="pagebreak" id="page_173"/>The fastest factoring algorithm is the <em>general number field sieve (GNFS)</em>, which I won’t describe here because it requires the introduction of several advanced mathematical concepts. A rough estimate of GNFS’s complexity is exp(1.91 × <em>n</em><sup>1/3</sup> (log <em>n</em>)<sup>2/3</sup>), where exp(…) is just a different notation for the exponential function <em>e<sup>x</sup></em>, with <em>e</em> the exponential constant approximately equal to 2.718. However, it’s difficult to get an accurate estimate of GNFS’s actual complexity for a given number size. Therefore, we have to rely on heuristical complexity estimates, which show how security increases with a longer <em>n</em>. For example:</p>
<ul>
<li class="noindent">Factoring a <strong>1024-bit</strong> number, which would have two prime factors of approximately 500 bits each, will take on the order of 2<sup>70</sup> basic operations.</li>
<li class="noindent">Factoring a <strong>2048-bit</strong> number, which would have two prime factors of approximately 1000 bits each, will take on the order of 2<sup>90</sup> basic operations, which is about a million times slower than for a 1024-bit number.</li>
</ul>
<p class="indent1">And we estimate that at least 4096 bits are needed to reach 128-bit security. Note that these values should be taken with a grain of salt, and researchers don’t always agree on these estimates. Take a look at these experimental results to see the actual cost of factoring:</p>
<ul>
<li class="noindent">In 2005, after about 18 months of computation—and thanks to the power of a cluster of 80 processors, with a total effort equivalent to 75 years of computation on a single processor—a group of researchers factored a <strong>663-bit</strong> (200-decimal digit) number.</li>
<li class="noindent">In 2009, after about two years and using several hundred processors, with a total effort equivalent to about 2,000 years of computation on a single processor, another group of researchers factored a <strong>768-bit</strong> (232-decimal digit) number.</li>
</ul>
<p class="indentt">As you can see, the numbers actually factored by academic researchers are shorter than those in real applications, which are at least 1024-bit and often more than 2048-bit. As I write this, no one has reported the factoring of a 1024-bit number, but many speculate that well-funded organizations such as the NSA can do it.</p>
<p class="indent">In sum, 1024-bit RSA should be viewed as insecure, and RSA should be used with at least a 2048-bit value—and preferably a 4096-bit one to ensure higher security.</p>
<h4 class="h4" id="lev2sec120"><em>Is Factoring NP-Complete?</em></h4>
<p class="noindent">We don’t know how to factor large numbers efficiently, which suggests that the factoring problem doesn’t belong to <strong>P</strong>. However, factoring is clearly in <strong>NP</strong>, because given a factorization, we can verify the solution by checking that all factors are prime numbers, thanks to the aforementioned primality testing algorithm, and that when multiplied together, the factors do give the expected number. For example, to check that 3 × 5 is the factorization of 15, you’ll check that both 3 and 5 are prime and that 3 times 5 equals 15.</p>
<p class="indent"><span epub:type="pagebreak" id="page_174"/>So we have a problem that is in <strong>NP</strong> and that looks hard, but is it as hard as the hardest <strong>NP</strong> problems? In other words, is factoring <strong>NP</strong>-complete? Spoiler alert: probably not.</p>
<p class="indent">There’s no mathematical proof that factoring isn’t <strong>NP</strong>-complete, but we have a few pieces of soft evidence. First, all known <strong>NP</strong>-complete problems can have one solution, but can also have more than one solution, or no solution at all. In contrast, factoring always has exactly one solution. Also, the factoring problem has a mathematical structure that allows for the GNFS algorithm to significantly outperform a naive algorithm, a structure that <strong>NP</strong>-complete problems don’t have. Factoring would be easy if we had a <em>quantum computer</em>, a computing model that exploits quantum mechanical phenomena to run different kinds of algorithms and that would have the capability to factor large numbers efficiently (not because it’d run the algorithm faster, but because it could run a quantum algorithm dedicated to factoring large numbers). A quantum computer doesn’t exist yet, though—and might never exist. Regardless, a quantum computer would be useless in tackling <strong>NP</strong>-complete problems because it’d be no faster than a classical one (see <a href="ch14.xhtml#ch14">Chapter 14</a>).</p>
<p class="indent">Factoring may then be slightly easier than <strong>NP</strong>-complete in theory, but as far as cryptography is concerned, it’s hard enough, and even more reliable than <strong>NP</strong>-complete problems. Indeed, it’s easier to build cryptosystems on top of the factoring problem than <strong>NP</strong>-complete problems, because it’s hard to know exactly how hard it is to break a cryptosystem based on some <strong>NP</strong>-complete problems—in other words, how many bits of security you’d get.</p>
<p class="indent">The factoring problem is just one of several problems used in cryptography as a <em>hardness assumption</em>, which is an assumption that some problem is computationally hard. This assumption is used when proving that breaking a cryptosystem’s security is at least as hard as solving said problem. Another problem used as a hardness assumption, the <em>discrete logarithm problem (DLP)</em>, is actually a family of problems, which we’ll discuss next.</p>
<h3 class="h3" id="lev1sec63">The Discrete Logarithm Problem</h3>
<p class="noindent">The DLP predates the factoring problem in the official history of cryptography. Whereas RSA appeared in 1977, a second cryptographic breakthrough, the Diffie–Hellman key agreement (covered in <a href="ch11.xhtml#ch11">Chapter 11</a>), came about a year earlier, grounding its security on the hardness of the DLP. Like the factoring problem, the DLP deals with large numbers, but it’s a bit less straightforward—it will take you a few minutes rather than a few seconds to get it and requires a bit more math than factoring. So let me introduce the mathematical notion of a <em>group</em> in the context of discrete logarithms.</p>
<h4 class="h4" id="lev2sec121"><em>What Is a Group?</em></h4>
<p class="noindent">In mathematical context, a <em>group</em> is a set of elements (typically, numbers) that are related to each other according to certain well-defined rules. An example of a group is the set of nonzero integers (between 1 and <em>p</em> – 1) modulo some prime number <em>p</em>, which we write <strong>Z</strong><sub><em>p</em></sub><sup>*</sup>. For <em>p</em> = 5, we get the group <strong>Z</strong><sub>5</sub><sup>*</sup> = {1,2,3,4}. <span epub:type="pagebreak" id="page_175"/>In the group <strong>Z</strong><sub>5</sub><sup>*</sup>, operations are carried out modulo 5; hence, we don’t have 3 × 4 = 12 but instead have 3 × 4 = 2, because 12 mod 5 = 2. We nonetheless use the same sign (×) that we use for normal integer multiplication. Likewise, we also use the exponent notation to denote a group element’s multiplication with itself mod <em>p</em>, a common operation in cryptography. For example, in the context of <strong>Z</strong><sub>5</sub><sup>*</sup>, 2<sup>3</sup> = 2 × 2 × 2 = 3 rather than 8, because 8 mod 5 is equal to 3.</p>
<p class="indentb">To be a group, a mathematical set should have the following characteristics, called <em>group axioms</em>:</p>
<p class="hang"><strong>Closure</strong> For any two <em>x</em> and <em>y</em> in the group, <em>x</em> × <em>y</em> is in the group too. In <strong>Z</strong><sub>5</sub><sup>*</sup>, 2 × 3 = 1 (because 6 = 1 mod 5), 2 × 4 = 3, and so on.</p>
<p class="hang"><strong>Associativity</strong> For any <em>x</em>, <em>y</em>, <em>z</em> in the group, (<em>x</em> × <em>y</em>) × <em>z</em> = <em>x</em> × (<em>y</em> × <em>z</em>). In <strong>Z</strong><sub>5</sub><sup>*</sup>, (2 × 3) × 4 = 1 × 4 = 2 × (3 × 4) = 2 × 2 = 4.</p>
<p class="hang"><strong>Identity existence</strong> There’s an element <em>e</em> such that <em>e</em> × <em>x</em> = <em>x</em> × <em>e</em> = <em>x</em>. In any <strong>Z</strong><sub><em>p</em></sub><sup>*</sup>, the identity element is 1.</p>
<p class="hang"><strong>Inverse existence</strong> For any <em>x</em> in the group, there’s a <em>y</em> such that <em>x</em> × <em>y</em> = <em>y</em> × <em>x</em> = <em>e</em>. In <strong>Z</strong><sub>5</sub><sup>*</sup>, the inverse of 2 is 3, and the inverse of 3 is 2, while 4 is its own inverse because 4 × 4 = 16 = 1 mod 5.</p>
<p class="indentt">In addition, a group is called <em>commutative</em> if <em>x</em> × <em>y</em> = <em>y</em> × <em>x</em> for any group elements <em>x</em> and <em>y</em>. That’s also true for any multiplicative group of integers <strong>Z</strong><sub><em>p</em></sub><sup>*</sup>. In particular, <strong>Z</strong><sub>5</sub><sup>*</sup> is commutative: 3 × 4 = 4 × 3, 2 × 3 = 3 × 2, and so on.</p>
<p class="indent">A group is called <em>cyclic</em> if there’s at least one element <em>g</em> such that its powers (<em>g</em><sup>1</sup>, <em>g</em><sup>2</sup>, <em>g</em><sup>3</sup>, and so on) mod <em>p</em> span all distinct group elements. The element <em>g</em> is then called a <em>generator</em> of the group. <strong>Z</strong><sub>5</sub><sup>*</sup> is cyclic and has two generators, 2 and 3, because 2<sup>1</sup> = 2, 2<sup>2</sup> = 4, 2<sup>3</sup> = 3, 2<sup>4</sup> = 1, and 3<sup>1</sup> = 3, 3<sup>2</sup> = 4, 3<sup>3</sup> = 2, 3<sup>4</sup> = 1.</p>
<p class="indent">Note that I’m using multiplication as a group operator, but you can also get groups from other operators. For example, the most straightforward group is the set of all integers, positive and negative, with addition as a group operation. Let’s check that the group axioms hold with addition, in the preceding order: clearly, the number <em>x</em> + <em>y</em> is an integer if <em>x</em> and <em>y</em> are integers (closure); (<em>x</em> + <em>y) + z</em> = <em>x</em> + <em>(y + z)</em> for any <em>x</em>, <em>y</em>, and <em>z</em> (associativity); zero is the identity element; and the inverse of any number <em>x</em> in the group is –<em>x</em> because <em>x</em> + (–<em>x</em>) = 0 for any integer <em>x</em>. A big difference, though, is that this group of integers is of infinite size, whereas in crypto we’ll only deal with <em>finite groups</em>, or groups with a finite number of elements. Typically, we’ll use groups <strong>Z</strong><sub><em>p</em></sub><sup>*</sup>, where <em>p</em> is <em>thousands</em> of bits long (that is, groups that contain on the order of 2<sup><em>p</em></sup> numbers).</p>
<h4 class="h4" id="lev2sec122"><em>The Hard Thing</em></h4>
<p class="noindent">The DLP consists of finding the <em>y</em> for which <em>g</em><sup><em>y</em></sup> = <em>x</em>, given a base number <em>g</em> within some group <strong>Z</strong><sub><em>p</em></sub><sup>*</sup>, where <em>p</em> is a prime number, and given a group element <em>x</em>. The DLP is called <em>discrete</em> because we’re dealing with integers as opposed to real numbers (continuous), and it’s called a <em>logarithm</em> because we’re looking for the logarithm of <em>x</em> in base <em>g</em>. (For example, the logarithm of 256 in base 2 is 8 because 2<sup>8</sup> = 256.)</p>
<p class="indent"><span epub:type="pagebreak" id="page_176"/>People often ask me whether factoring or a discrete logarithm is more secure—or in other words, which problem is the hardest? My answer is that they’re about equally hard. In fact, algorithms to solve DLP bear similarities with those factoring integers, and you get about the same security level with <em>n</em>-bit hard-to-factor numbers as with discrete logarithms in an <em>n</em>-bit group. And for the same reason as factoring, DLP isn’t <strong>NP</strong>-complete. (Note that there are certain groups where the DLP is easier to solve, but here I’m only referring to the case of DLP groups consisting of a number modulo a prime.)</p>
<h3 class="h3" id="lev1sec64">How Things Can Go Wrong</h3>
<p class="noindent">More than 40 years later, we still don’t know how to efficiently factor large numbers or solve discrete logarithms. Amateurs may argue that someone may eventually break factoring—and we have no proof that it’ll never be broken—but we also don’t have proof that <strong>P</strong> ≠ <strong>NP</strong>. Likewise, you can speculate that <strong>P</strong> may be equal to <strong>NP</strong>; however, according to experts, that surprise is unlikely. So there’s no need to worry. And indeed all the public-key crypto deployed today relies on either factoring (RSA) or DLP (Diffie–Hellman, ElGamal, elliptic curve cryptography). However, although math may not fail us, real-world concerns and human error can sneak in.</p>
<h4 class="h4" id="lev2sec123"><em>When Factoring Is Easy</em></h4>
<p class="noindent">Factoring large numbers isn’t always hard. For example, take the 1024-bit number <em>N</em>, which is equal to the following:</p>
<div class="image1"><img src="../images/f0176-01.jpg" alt="image"/></div>
<p class="indent">For 1024-bit numbers used in RSA encryption or signature schemes where <em>N</em> = <em>pq</em>, we expect the best factoring algorithms to need around 2<sup>70</sup> operations, as we discussed earlier. But you can factor this sample number in seconds using SageMath, a piece of Python-based mathematical software. Using SageMath’s <span class="literal">factor()</span> function on my 2015 MacBook, it took less than five seconds to find the following factorization:</p>
<div class="image1"><img src="../images/f0176-02.jpg" alt="image"/></div>
<p class="indent">Right, I cheated. This number isn’t of the form <em>N</em> = <em>pq</em> because it doesn’t have just two large prime factors but rather five, including very small ones, which makes it easy to factor. First, you’ll identify the 2<sup>800</sup> × 641 × 6700417 part by trying small primes from a precomputed list of prime numbers, which leaves you with a 192-bit number that’s much easier to factor than a 1024-bit number with two large factors.</p>
<p class="indent"><span epub:type="pagebreak" id="page_177"/>But factoring can be easy not only when <em>n</em> has small prime factors, but also when <em>N</em> or its factors <em>p</em> and <em>q</em> have particular forms—for example, when <em>N</em> = <em>pq</em> with <em>p</em> and <em>q</em> both close to some 2<sup><em>b</em></sup>, when <em>N</em> = <em>pq</em> and some bits of <em>p</em> or <em>q</em> are known, or when <em>N</em> is of the form <em>N</em> = <em>p</em><sup><em>r</em></sup><em>q</em><sup><em>s</em></sup> and <em>r</em> is greater than log <em>p</em>. However, detailing the reasons for these weaknesses is way too technical for this book.</p>
<p class="indent">The upshot here is that the RSA encryption and signature algorithms (covered in <a href="ch10.xhtml#ch10">Chapter 10</a>) will need to work with a value of <em>N</em> = <em>pq</em>, where <em>p</em> and <em>q</em> are carefully chosen, to avoid easy factorization of <em>N</em>, which can result in security disaster.</p>
<h4 class="h4" id="lev2sec124"><em>Small Hard Problems Aren’t Hard</em></h4>
<p class="noindent">Computationally hard problems become easy when they’re small enough, and even exponential-time algorithms become practical as the problem size shrinks. A symmetric cipher may be secure in the sense that there’s no faster attack than the 2<sup><em>n</em></sup>-time brute force, but if the key length is <em>n</em> = 32, you’ll break the cipher in minutes. This sounds obvious, and you’d think that no one would be naive enough to use small keys, but in reality there are plenty of reasons why this could happen. The following are two true stories.</p>
<p class="indent">Say you’re a developer who knows nothing about crypto but has some API to encrypt with RSA and has been told to encrypt with 128-bit security. What RSA key size would you pick? I’ve seen real cases of 128-bit RSA, or RSA based on a 128-bit number <em>N</em> = <em>pq</em>. However, although factoring is impractically hard for an <em>N</em> thousands of bits long, factoring a 128-bit number is easy. Using the SageMath software, the commands shown in <a href="ch09.xhtml#ch9list2">Listing 9-2</a> complete instantaneously.</p>
<p class="programs">sage: <span class="codestrong">p = random_prime(2**64)</span><br/>sage: <span class="codestrong">q = random_prime(2**64)</span><br/>sage: <span class="codestrong">factor(p*q)</span><br/>6822485253121677229 * 17596998848870549923</p>
<p class="figcap"><a id="ch9list2"/><em>Listing 9-2: Generating an RSA modulus by picking two random prime numbers and factoring it instantaneously</em></p>
<p class="indentt"><a href="ch09.xhtml#ch9list2">Listing 9-2</a> shows that a 128-bit number taken randomly as the product of two 64-bit prime numbers can be easily factored on a typical laptop. However, if I chose 1024-bit prime numbers instead by using <span class="literal">p = random_prime(2**1024)</span>, the command <span class="literal">factor(p*q)</span> would never complete, at least not in my lifetime.</p>
<p class="indent">To be fair, the tools available don’t help prevent the naive use of insecurely short parameters. For example, the OpenSSL toolkit lets you generate RSA keys as short as 31 bits without any warning; obviously, such short keys are totally insecure, as shown in <a href="ch09.xhtml#ch9list3">Listing 9-3</a>.</p>
<p class="programs">$ <span class="codestrong">openssl genrsa 31</span><br/>Generating RSA private key, 31 bit long modulus<br/>.+++++++++++++++++++++++++++<br/>.+++++++++++++++++++++++++++<br/>e is 65537 (0x10001)<br/>-----BEGIN RSA PRIVATE KEY-----<br/>MCsCAQACBHHqFuUCAwEAAQIEP6zEJQIDANATAgMAjCcCAwCSBwICTGsCAhpp<br/>-----END RSA PRIVATE KEY-----</p>
<p class="figcap"><span epub:type="pagebreak" id="page_178"/><a id="ch9list3"/><em>Listing 9-3: Generating an insecure RSA private key using the OpenSSL toolkit</em></p>
<p class="indent">When reviewing cryptography, you should not only check the type of algorithms used, but also their parameters and the length of their secret values. However, as you’ll see in the following story, what’s secure enough today may be insecure tomorrow.</p>
<p class="indent">In 2015, researchers discovered that many HTTPS servers and email servers still supported an older, insecure version of the Diffie–Hellman key agreement protocol. Namely, the underlying TLS implementation supported Diffie–Hellman within a group, <strong>Z</strong><sub><em>p</em></sub><sup>*</sup>, defined by a prime number, <em>p</em>, of only 512 bits, where the discrete logarithm problem was no longer practically impossible to compute.</p>
<p class="indent">Not only did servers support a weak algorithm, but attackers could force a benign client to use that algorithm by injecting malicious traffic within the client’s session. Even better for attackers, the largest part of the attack could be carried out once and recycled to attack multiple clients. After about a week of computations to attack a specific group, <strong>Z</strong><sub><em>p</em></sub><sup>*</sup>, it took only 70 seconds to break individual sessions of different users.</p>
<p class="indent">A secure protocol is worthless if it’s undermined by a weakened algorithm, and a reliable algorithm is useless if sabotaged by weak parameters. In cryptography, you should always read the fine print.</p>
<p class="indent">For more details about this story, check the research article “Imperfect Forward Secrecy: How Diffie–Hellman Fails in Practice” (<em><a href="https://weakdh.org/imperfect-forward-secrecy-ccs15.pdf">https://weakdh.org/imperfect-forward-secrecy-ccs15.pdf</a></em>).</p>
<h3 class="h3" id="lev1sec65">Further Reading</h3>
<p class="noindent">I encourage you to look deeper into the foundational aspects of computation in the context of computability (what functions can be computed?) and complexity (at what cost?), and how they relate to cryptography. I’ve mostly talked about the classes <strong>P</strong> and <strong>NP</strong>, but there are many more classes and points of interest for cryptographers. I highly recommend the book <em>Quantum Computing Since Democritus</em> by Scott Aaronson (Cambridge University Press, 2013). It’s in large part about quantum computing, but its first chapters brilliantly introduce complexity theory and cryptography.</p>
<p class="indent">In the cryptography research literature you’ll also find other hard computational problems. I’ll mention them in later chapters, but here are some examples that illustrate the diversity of problems leveraged by cryptographers:</p>
<ul>
<li class="noindent">The Diffie–Hellman problem (given <em>g</em><sup><em>x</em></sup> and <em>g</em><sup><em>y</em></sup>, find <em>g</em><sup>xy</sup>) is a variant of the discrete logarithm problem, and is widely used in key agreement protocols.</li>
<li class="noindent"><span epub:type="pagebreak" id="page_179"/>Lattice problems, such as the shortest vector problem (SVP) and the learning with errors (LWE) problem, are the only examples of <strong>NP</strong>-hard problems successfully used in cryptography.</li>
<li class="noindent">Coding problems rely on the hardness of decoding error-correcting codes with insufficient information, and have been studied since the late 1970s.</li>
<li class="noindent">Multivariate problems are about solving nonlinear systems of equations and are potentially <strong>NP</strong>-hard, but they’ve failed to provide reliable cryptosystems because hard versions are too big and slow, and practical versions were found to be insecure.</li>
</ul>
<p class="indentt">In <a href="ch10.xhtml#ch10">Chapter 10</a>, we’ll keep talking about hard problems, especially factoring and its main variant, the RSA problem.<span epub:type="pagebreak" id="page_180"/></p>
</body></html>