- en: '**20**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**SIMPLE LINEAR REGRESSION**'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/common-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Though straightforward comparative tests of individual statistics are useful
    in their own right, you’ll often want to learn more from your data. In this chapter,
    you’ll look at *linear regression* models: a suite of methods used to evaluate
    precisely *how* variables relate to each other.'
  prefs: []
  type: TYPE_NORMAL
- en: Simple linear regression models describe the effect that a particular variable,
    called the *explanatory variable*, might have on the value of a continuous outcome
    variable, called the *response variable*. The explanatory variable may be continuous,
    discrete, or categorical, but to introduce the key concepts, I’ll concentrate
    on continuous explanatory variables for the first several sections in this chapter.
    Then, I’ll cover how the representation of the model changes if the explanatory
    variable is categorical.
  prefs: []
  type: TYPE_NORMAL
- en: '**20.1 An Example of a Linear Relationship**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As an example to start with, let’s continue with the data used in [Section 19.3](ch19.xhtml#ch19lev1sec61)
    and look at the student survey data (the `survey` data frame in the package `MASS`)
    a little more closely. If you haven’t already done so, with the required package
    loaded (call `library("MASS")`), you can read the help file `?survey` for details
    on the variables present.
  prefs: []
  type: TYPE_NORMAL
- en: Plot the student heights on the *y*-axis and their handspans (of their writing
    hand) on the *x*-axis.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 20-1](ch20.xhtml#ch20fig1) shows the result.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f20-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 20-1: A scatterplot of height against writing handspan for a sample
    of first-year statistics students*'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the call to `plot` uses formula notation (also referred to as *symbolic
    notation*) to specify “height *on* handspan.” You can produce the same scatterplot
    by using the coordinate vector form of (*x*, *y*), that is, `plot(survey$Wr.Hnd,survey$Height,...)`,
    but I’m using the symbolic notation here because it nicely reflects how you’ll
    fit the linear model in a moment.
  prefs: []
  type: TYPE_NORMAL
- en: As you might expect, there’s a positive association between a student’s handspan
    and their height. That relationship appears to be linear in nature. To assess
    the strength of the linear relationship (refer to [Section 13.2.5](ch13.xhtml#ch13lev2sec120)),
    you can find the estimated correlation coefficient.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Though there are 237 records in the data frame, the plot doesn’t actually show
    237 points. This is because there are missing observations (coded `NA`; see [Section
    6.1.3](ch06.xhtml#ch06lev2sec57)). By default, R removes any “incomplete” pairs
    when producing a plot like this. To find out how many offending observations have
    been deleted, you can use the short-form logical operator `|` ([Section 4.1.3](ch04.xhtml#ch04lev2sec39))
    in conjunction with `is.na` ([Section 6.1.3](ch06.xhtml#ch06lev2sec57)) and `which`
    ([Section 4.1.5](ch04.xhtml#ch04lev2sec41)). You then use `length` to discover
    there are 29 missing observation pairs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Because there are* `NA`*s in the vectors supplied to the correlation coefficient
    function* `cor`*, you must also specify the optional argument* `use="complete.obs"`*.
    This means that the calculated statistic takes into account only those observation
    pairs in the* `Wr.Hnd` *and* `Height` *vectors where* neither *element is* `NA`*.
    You can think of this argument as doing much the same thing as* `na.rm=TRUE` *in
    univariate summary statistic functions such as* `mean` *and* `sd`.'
  prefs: []
  type: TYPE_NORMAL
- en: '**20.2 General Concepts**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The purpose of a linear regression model is to come up with a function that
    estimates the *mean* of one variable given a particular value of another variable.
    These variables are known as the *response variable* (the “outcome” variable whose
    mean you are attempting to find) and the *explanatory variable* (the “predictor”
    variable whose value you already have).
  prefs: []
  type: TYPE_NORMAL
- en: In terms of the student survey example, you might ask something like “What’s
    the expected height of a student if their handspan is 14.5 cm?” Here the response
    variable is the height, and the explanatory variable is the handspan.
  prefs: []
  type: TYPE_NORMAL
- en: '***20.2.1 Definition of the Model***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Assume you’re looking to determine the value of response variable *Y* given
    the value of an explanatory variable *X*. The *simple linear regression model*
    states that the value of a response is expressed as the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e20-1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: On the left side of [Equation (20.1)](ch20.xhtml#ch20eq1), the notation *Y*|*X*
    reads as “the value of *Y* conditional upon the value of *X*.”
  prefs: []
  type: TYPE_NORMAL
- en: '**Residual Assumptions**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The validity of the conclusions you can draw based on the model in (20.1) is
    critically dependent on the assumptions made about *∊*, which are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: • The value of *∊* is assumed to be normally distributed such that *∊* ~ N(0,*σ*).
  prefs: []
  type: TYPE_NORMAL
- en: • That *∊* is centered (that is, has a mean of) zero.
  prefs: []
  type: TYPE_NORMAL
- en: • The variance of *∊*, *σ*², is constant.
  prefs: []
  type: TYPE_NORMAL
- en: The *∊* term represents random error. In other words, you assume that any raw
    value of the response is owed to a linear change in a given value of *X*, plus
    or minus some random, *residual* variation or normally distributed *noise*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Parameters**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The value denoted by *β*[0] is called the *intercept*, and that of *β*[1] is
    called the *slope*. Together, they are also referred to as the *regression coefficients*
    and are interpreted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: • The intercept, *β*[0], is interpreted as the expected value of the response
    variable when the predictor is zero.
  prefs: []
  type: TYPE_NORMAL
- en: • Generally, the slope, *β*[1], is the focus of interest. This is interpreted
    as the change in the mean response for each one-unit increase in the predictor.
    When the slope is positive, the regression line increases from left to right (the
    mean response is higher when the predictor is higher); when the slope is negative,
    the line decreases from left to right (the mean response is lower when the predictor
    is higher). When the slope is zero, this implies that the predictor has no effect
    on the value of the response. The more extreme the value of *β*[1] (that is, away
    from zero), the steeper the increasing or decreasing line becomes.
  prefs: []
  type: TYPE_NORMAL
- en: '***20.2.2 Estimating the Intercept and Slope Parameters***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The goal is to use your data to estimate the regression parameters, yielding
    the estimates ![image](../images/b0.jpg) and ![image](../images/b1.jpg); this
    is referred to as *fitting* the linear model. In this case, the data comprise
    *n* pairs of observations for each individual. The fitted model of interest concerns
    the mean response value, denoted ŷ, for a specific value of the predictor, *x*,
    and is written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e20-2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Sometimes, alternative notation such as ![image](../images/e.jpg)[*Y*] or ![image](../images/e.jpg)[*Y*|*X*
    = *x*] is used on the left side of (20.2) to emphasize the fact that the model
    gives the mean (that is, the expected value) of the response. For compactness,
    many simply use something like ŷ, as shown here.
  prefs: []
  type: TYPE_NORMAL
- en: Let your *n* observed data pairs be denoted *x[i]* and *y[i]* for the predictor
    and response variables, respectively; *i* = 1, . . . , *n*. Then, the parameter
    estimates for the simple linear regression function are
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e20-3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: • *x̄* and *ȳ* are the sample means of the *x[i]*s and *y[i]*s.
  prefs: []
  type: TYPE_NORMAL
- en: • *s[x]* and *s[y]* are the sample standard deviations of the *x[i]*s and *y[i]*s.
  prefs: []
  type: TYPE_NORMAL
- en: • *ρ[xy]* is the estimate of correlation between *X* and *Y* based on the data
    (see [Section 13.2.5](ch13.xhtml#ch13lev2sec120)).
  prefs: []
  type: TYPE_NORMAL
- en: Estimating the model parameters in this way is referred to as *least-squares
    regression*; the reason for this will become clear in a moment.
  prefs: []
  type: TYPE_NORMAL
- en: '***20.2.3 Fitting Linear Models with lm***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In R, the command `lm` performs the estimation for you. For example, the following
    line creates a fitted linear model object of the mean student height by handspan
    and stores it in your global environment as `survfit`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The first argument is the now-familiar `response` ~ predictor formula, which
    specifies the desired model. You don’t have to use the `survey$` prefix to extract
    the vectors from the data frame because you specifically instruct `lm` to look
    in the object supplied to the `data` argument.
  prefs: []
  type: TYPE_NORMAL
- en: The fitted linear model object itself, `survfit`, has a special class in R—one
    of `"lm"`. An object of class `"lm"` can essentially be thought of as a list containing
    several components that describe the model. You’ll look at these in a moment.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you simply enter the name of the `"lm"` object at the prompt, it will provide
    the most basic output: a repeat of your call and the estimates of the intercept
    (![image](../images/b0.jpg)) and slope (![image](../images/b1.jpg)).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This reveals that the linear model for this example is estimated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e20-4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If you evaluate the mathematical function for ŷ—[Equation (20.2)](ch20.xhtml#ch20eq2)—at
    a range of different values for *x*, you end up with a straight line when you
    plot the results. Considering the definition of intercept given earlier as the
    expected value of the response variable when the predictor is zero, in the current
    example, this would imply that the mean height of a student with a handspan of
    0 cm is 113.954 cm (an arguably less-than-useful statement since a value of zero
    for the explanatory variable doesn’t make sense; you’ll consider these and related
    issues in [Section 20.4](ch20.xhtml#ch20lev1sec65)). The slope, the change in
    the mean response for each one-unit increase in the predictor, is 3.117\. This
    states that, on average, for every 1 cm increase in handspan, a student’s height
    is estimated to increase by 3.117 cm.
  prefs: []
  type: TYPE_NORMAL
- en: With all this in mind, once more run the line to plot the raw data as given
    in [Section 20.1](ch20.xhtml#ch20lev1sec62) and shown in [Figure 20-1](ch20.xhtml#ch20fig1),
    but now add the fitted regression line using `abline`. So far, you’ve only used
    the `abline` command to add perfectly horizontal and vertical lines to an existing
    plot, but when passed an object of class `"lm"` that represents a simple linear
    model, like `survfit`, the fitted regression line will be added instead.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This adds the slightly thickened diagonally increasing line shown in [Figure
    20-2](ch20.xhtml#ch20fig2).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f20-02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 20-2: The simple linear regression line (solid, bold) fitted to the
    observed data. Two dashed vertical line segments provide examples of a positive
    (leftmost) and negative (rightmost)* residual.'
  prefs: []
  type: TYPE_NORMAL
- en: '***20.2.4 Illustrating Residuals***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When the parameters are estimated as shown here, using (20.3), the fitted line
    is referred to as an implementation of *least-squares regression* because it’s
    the line that minimizes the average squared difference between the observed data
    and itself. This concept is easier to understand by drawing the distances between
    the observations and the fitted line, formally called *residuals*, for a couple
    of individual observations within [Figure 20-2](ch20.xhtml#ch20fig2).
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s extract two specific records from the `Wr.Hnd` and `Height` data
    vectors and call the resulting vectors `obsA` and `obsB`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Next, briefly inspect the names of the members of the `survfit` object.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: These members are the components that automatically make up a fitted model object
    of class `"lm"`, mentioned briefly earlier. Note that there’s a component called
    `"coefficients"`. This contains a numeric vector of the estimates of the intercept
    and slope.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can extract this component (and indeed any of the other ones listed here)
    in the same way you would perform a member reference on a named list: by entering
    `survfit$coefficients` at the prompt. Where possible, though, it’s technically
    preferable for programming purposes to extract such components using a “direct-access”
    function. For the `coefficients` component of an `"lm"` object, the function you
    use is `coef`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Here, the regression coefficients are extracted from the object and then separately
    assigned to the objects `beta0.hat` and `beta1.hat`. Other common direct-access
    functions include `resid` and `fitted`; these two pertain to the `"residuals"`
    and `"fitted.values"` components, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, I use `segments` to draw the vertical dashed lines present in [Figure
    20-2](ch20.xhtml#ch20fig2).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Note that the dashed lines meet the fitted line at the vertical axis locations
    passed to `y0`, which, with the use of the regression coefficients `beta0.hat`
    and `beta1.hat`, reflects [Equation (20.4)](ch20.xhtml#ch20eq4).
  prefs: []
  type: TYPE_NORMAL
- en: Now, imagine a collection of alternative regression lines drawn through the
    data (achieved by altering the value of the intercept and slope). Then, for each
    of the alternative regression lines, imagine you calculate the residuals (vertical
    distances) between the response value of every observation and the *fitted value*
    of that line. The simple linear regression line estimated as per (20.3) is the
    line that lies “closest to all observations.” By this, it is meant that the fitted
    regression model is represented by the estimated line that passes through the
    coordinate provided by the variable means (*x̄*, *ȳ*), and it’s the line that
    yields the smallest overall measure of the squared residual distances. For this
    reason, another name for a least-squares-estimated regression equation like this
    is the *line of best fit*.
  prefs: []
  type: TYPE_NORMAL
- en: '**20.3 Statistical Inference**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The estimation of a regression equation is relatively straightforward, but
    this is merely the beginning. You should now think about what can be inferred
    from your result. In simple linear regression, there’s a natural question that
    should always be asked: Is there statistical evidence to support the presence
    of a relationship between the predictor and the response? To put it another way,
    is there evidence that a change in the explanatory variable affects the mean outcome?
    You investigate this following the same ideas that were introduced in [Chapter
    17](ch17.xhtml#ch17) when you began thinking about the variability present in
    estimated statistics and then continued to infer from your results using confidence
    intervals and, in [Chapter 18](ch18.xhtml#ch18), hypothesis testing.'
  prefs: []
  type: TYPE_NORMAL
- en: '***20.3.1 Summarizing the Fitted Model***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This kind of *model-based inference* is automatically carried out by R when
    `lm` objects are processed. Using the `summary` function on an object created
    by `lm` provides you with output far more detailed than simply printing the object
    to the console. For the moment, you’ll focus on just two aspects of the information
    presented in `summary`: the significance tests associated with the regression
    coefficients and the interpretation of the so-called *coefficient of determination*
    (labeled `R-squared` in the output), which I’ll explain shortly.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use `summary` on the current model object `survfit`, and you’ll see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '***20.3.2 Regression Coefficient Significance Tests***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s begin by focusing on the way the estimated regression coefficients are
    reported. The first column of the coefficients table contains the point estimates
    of the intercept and slope (the intercept is labeled as such, and the slope is
    labeled after the name of the predictor variable in the data frame); the table
    also includes estimates of the standard errors of these statistics. It can be
    shown that simple linear regression coefficients, when estimated using least-squares,
    follow a *t*-distribution with *n* − 2 degrees of freedom (when given the number
    of observations, *n*, used in the model fit). The standardized *t* value and a
    *p*-value are reported for each parameter. These represent the results of a two-tailed
    hypothesis test formally defined as
  prefs: []
  type: TYPE_NORMAL
- en: 'H[0] : *β[j]* = 0'
  prefs: []
  type: TYPE_NORMAL
- en: 'H[A] : *β[j]* ≠ 0'
  prefs: []
  type: TYPE_NORMAL
- en: where *j* = 0 for the intercept and *j* = 1 for the slope, using the notation
    in [Equation (20.1)](ch20.xhtml#ch20eq1).
  prefs: []
  type: TYPE_NORMAL
- en: 'Focus on the row of results for the predictor. With a null value of zero, truth
    of H[0] implies that the predictor has no effect on the response. The claim here
    is interested in whether there is *any effect* of the covariate, not the direction
    of this effect, so H[A] is two-sided (via ≠). As with any hypothesis test, the
    smaller the *p*-value, the stronger the evidence against H[0]. With a small *p*-value
    (< 2 × 10^−^(16)) attached to this particular test statistic (which you can confirm
    using the formula in [Chapter 18](ch18.xhtml#ch18): *T* = (3.116 − 0)/0.2888 =
    10.79), you’d therefore conclude there is strong evidence *against* the claim
    that the predictor has no effect on the mean level of the response.'
  prefs: []
  type: TYPE_NORMAL
- en: The same test is carried out for the intercept, but the test for the slope parameter
    *β*[1] is typically more interesting (since rejection of the null hypothesis for
    *β*[0] simply indicates evidence that the regression line does not strike the
    vertical axis at zero), especially when the observed data don’t include *x* =
    0, as is the case here.
  prefs: []
  type: TYPE_NORMAL
- en: From this, you can conclude that the fitted model suggests there is evidence
    that an increase in handspan is associated with an increase in height among the
    population being studied. For each additional centimeter of handspan, the average
    increase in height is approximately 3.12 cm.
  prefs: []
  type: TYPE_NORMAL
- en: You could also produce confidence intervals for your estimates using [Equation
    (17.2)](ch17.xhtml#ch17eq2) on [page 378](ch17.xhtml#page_378) and knowledge of
    the sampling distributions of the regression parameters; however, yet again, R
    provides a convenient function for an object of class `"lm"` to do this for you.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: To the `confint` function you pass your model object as the first argument and
    your desired level of confidence as `level`. This indicates that you should be
    95 percent confident the true value of *β*[1] lies somewhere between 2.55 and
    3.69 (to 2 d.p.). As usual, the exclusion of the null value of zero reflects the
    statistically significant result from earlier.
  prefs: []
  type: TYPE_NORMAL
- en: '***20.3.3 Coefficient of Determination***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The output of `summary` also provides you with the values of `Multiple R-squared`
    and `Adjusted R-squared`, which are particularly interesting. Both of these are
    referred to as the *coefficient of determination*; they describe the proportion
    of the variation in the response that can be attributed to the predictor.
  prefs: []
  type: TYPE_NORMAL
- en: 'For simple linear regression, the first (unadjusted) measure is simply obtained
    as the square of the estimated correlation coefficient (refer to [Section 13.2.5](ch13.xhtml#ch13lev2sec120)).
    For the student height example, first store the estimated correlation between
    `Wr.Hnd` and `Height` as `rho.xy`, and then square it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: You get the same result as the `Multiple R-squared` value (usually written mathematically
    as *R*²). This tells you that about 36.1 percent of the variation in the student
    heights can be attributed to handspan.
  prefs: []
  type: TYPE_NORMAL
- en: The adjusted measure is an alternative estimate that takes into account the
    number of parameters that require estimation. The adjusted measure is generally
    important only if you’re using the coefficient of determination to assess the
    overall “quality” of the fitted model in terms of a balance between goodness of
    fit and complexity. I’ll cover this in [Chapter 22](ch22.xhtml#ch22), so I won’t
    go into any more detail just yet.
  prefs: []
  type: TYPE_NORMAL
- en: '***20.3.4 Other summary Output***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `summary` of the model object provides you with even more useful information.
    The “residual standard error” is the estimated standard error of the *∊* term
    (in other words, the square root of the estimated variance of *∊*, namely, *σ*²);
    below that it also reports any missing values. (The 29 observation pairs “deleted
    due to missingness” here matches the number of incomplete observations determined
    in [Section 20.1](ch20.xhtml#ch20lev1sec62).)
  prefs: []
  type: TYPE_NORMAL
- en: The output also provides a five-number summary ([Section 13.2.3](ch13.xhtml#ch13lev2sec118))
    for the residual distances—I’ll cover this further in [Section 22.3](ch22.xhtml#ch22lev1sec74).
    As the final result, you’re provided with a certain hypothesis test performed
    using the *F*-distribution. This is a global test of the impact of your predictor(s)
    on the response; this will be explored alongside multiple linear regression in
    [Section 21.3.5](ch21.xhtml#ch21lev2sec197).
  prefs: []
  type: TYPE_NORMAL
- en: You can access all the output provided by `summary` directly, as individual
    R objects, rather than having to read them off the screen from the entire printed
    summary. Just as `names(survfit)` provides you with an indication of the contents
    of the stand-alone `survfit` object, the following code gives you the names of
    all the components accessible *after* `summary` is used to process `survfit`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'It’s fairly easy to match most of the components with the printed `summary`
    output, and they can be extracted using the dollar operator as usual. The residual
    standard error, for example, can be retrieved directly with this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: There are further details on this in the `?summary.lm` help file.
  prefs: []
  type: TYPE_NORMAL
- en: '**20.4 Prediction**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To wrap up these preliminary details of linear regression, you’ll now look at
    using your fitted model for predictive purposes. The ability to fit a statistical
    model means that you not only can understand and quantify the nature of relationships
    in your data (like the estimated 3.1166 cm increase in mean height per 1 cm increase
    in handspan for the student example) but can also *predict* values of the outcome
    of interest, even where you haven’t actually observed the values of any explanatory
    variables in the original data set. As with any statistic, though, there is always
    a need to accompany any point estimates or predictions with a measure of spread.
  prefs: []
  type: TYPE_NORMAL
- en: '***20.4.1 Confidence Interval or Prediction Interval?***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: With a fitted simple linear model you’re able to calculate a point estimate
    of the *mean response* value, conditional upon the value of an explanatory variable.
    To do this, you simply plug in (to the fitted model equation) the value of *x*
    you’re interested in. A statistic like this is always subject to variation, so
    just as with sample statistics explored in earlier chapters, you use a *confidence
    interval for the mean response (CI)* to gauge this uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: Assume a simple linear regression line has been fitted to *n* observations such
    that ![image](../images/f0462-01.jpg) percent confidence interval for the mean
    response given a value of *x* is calculated with
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e20-5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where you obtain the lower limit by subtraction, the upper limit by addition.
  prefs: []
  type: TYPE_NORMAL
- en: Here, ŷ is the fitted value (from the regression line) at *x*; *t*[(][1][−]
    [/][2][,][n][−][2][)] is the appropriate critical value from a *t*-distribution
    with *n* − 2 degrees of freedom (in other words, resulting in an upper-tail area
    of exactly /2); *s[ɛ]* is the estimated residual standard error; and *x̄* and
    ![image](../images/common-05.jpg) represent the sample mean and the variance of
    the observations of the predictor, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'A *prediction interval (PI)* for an observed response is different from the
    confidence interval in terms of context. Where CIs are used to describe the variability
    of the *mean* response, a PI is used to provide the possible range of values that
    an *individual realization* of the response variable might take, given *x*. This
    distinction is subtle but important: the CI corresponds to a mean, and the PI
    corresponds to an individual observation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s remain with the previous notation. It can be shown that 100(1 − *α*)
    percent prediction interval for an individual response given a value of *x* is
    calculated with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e20-6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: It turns out that the only difference from (20.5) is the 1+ that appears in
    the square root. As such, a PI at *x* is wider than a CI at *x*.
  prefs: []
  type: TYPE_NORMAL
- en: '***20.4.2 Interpreting Intervals***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Continuing with our example, let’s say you want to determine the mean height
    for students with a handspan of 14.5 cm and for students with a handspan of 24
    cm. The point estimates themselves are easy—just plug the desired *x* values into
    the regression [equation (20.4)](ch20.xhtml#ch20eq4).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: According to the model, you can expect mean heights to be around 159.14 and
    188.75 cm for handspans of 14.5 and 24 cm, respectively. The `as.numeric` coercion
    function (first encountered in [Section 6.2.4](ch06.xhtml#ch06lev2sec62)) is used
    simply to strip the result of the annotative names that are otherwise present
    from the `beta0.hat` and `beta1.hat` objects.
  prefs: []
  type: TYPE_NORMAL
- en: '**Confidence Intervals for Mean Heights**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'To find confidence intervals for these estimates, you could calculate them
    manually using (20.5), but of course R has a built-in `predict` command to do
    it for you. To use `predict`, you first need to store your *x* values in a particular
    way: as a column in a new data frame. The name of the column must match the predictor
    used in the original call to create the fitted model object. In this example,
    I’ll create a new data frame, `xvals`, with the column named `Wr.Hnd`, which contains
    only two values of interest—the handspans of 14.5 and 24 cm.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Now, when `predict` is called, the first argument must be the fitted model object
    of interest, `survfit` for this example. Next, in the argument `newdata`, you
    pass the specially constructed data frame containing the specified predictor values.
    To the `interval` argument you must specify `"confidence"` as a character string
    value. The confidence level, here set for 95 percent, is passed (on the scale
    of a probability) to `level`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This call will return a matrix with three columns, whose number (and order)
    of rows correspond to the predictor values you supplied in the `newdata` data
    frame. The first column, with a heading of `fit`, is the point estimate on the
    regression line; you can see that these numbers match the values you worked out
    earlier. The other columns provide the lower and upper CI limits as the `lwr`
    and `upr` columns, respectively. In this case, you’d interpret this as 95 percent
    confidence that the mean height of a student with a handspan of 14.5 cm lies somewhere
    between 156.5 cm and 161.8 cm and lies between 185.6 cm and 191.9 cm for a handspan
    of 24 cm (when rounded to 1 d.p.). Remember, these CIs, calculated as per (20.5)
    through `predict`, are for the *mean* response value.
  prefs: []
  type: TYPE_NORMAL
- en: '**Prediction Intervals for Individual Observations**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The `predict` function will also provide your prediction intervals. To find
    the prediction interval for possible individual observations with a certain probability,
    you simply need to change the `interval` argument to `"prediction"`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the fitted values remain the same, as Equations (20.5) and (20.6)
    indicate. The widths of the PIs, however, are significantly larger than those
    of the corresponding CIs—this is because raw observations themselves, at a specific
    *x* value, will naturally be more variable than their mean.
  prefs: []
  type: TYPE_NORMAL
- en: Interpretation changes accordingly. The intervals describe where raw student
    heights are predicted to lie “95 percent of the time.” For a handspan of 14.5
    cm, the model predicts individual observations to lie somewhere between 143.3
    cm and 175.0 cm with a probability of 0.95; for a handspan of 24 cm, the same
    PI is estimated at 172.8 cm and 204.7 cm (when rounded to 1 d.p.).
  prefs: []
  type: TYPE_NORMAL
- en: '***20.4.3 Plotting Intervals***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Both CIs and PIs are well suited to visualization for simple linear regression
    models. With the following code, you can start off [Figure 20-3](ch20.xhtml#ch20fig3)
    by plotting the data and estimated regression line just as for [Figure 20-2](ch20.xhtml#ch20fig2),
    but this time using `xlim` and `ylim` in `plot` to widen the *x*- and *y*-limits
    a little in order to accommodate the full length and breadth of the CI and PI.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: To this you add the locations of the fitted values for *x* = 14.5 and *x* =
    24, as well as two sets of vertical lines showing the CIs and PIs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The call to `points` marks the fitted values for these two particular values
    of *x*. The first call to `segments` lays down the PIs as thickened vertical gray
    lines, and the second lays down the CIs as the shorter vertical black lines. The
    coordinates for these plotted line segments are taken directly from the `mypred.pi`
    and `mypred.ci` objects, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: You can also produce “bands” around the fitted regression line that mark one
    or both of these intervals over *all* values of the predictor. From a programming
    standpoint, this isn’t technically possible for a continuous variable, but you
    can achieve it practically by defining a fine sequence of values along the *x*-axis
    (using `seq` with a high `length` value) and evaluating the CI and PI at every
    point in this fine sequence. Then you just join resulting points as lines when
    plotting.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f20-03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 20-3: The student height regression example, with a fitted regression
    line and point estimates at* x *= 14.5 and* x *= 24 and with corresponding 95
    percent CIs (black vertical lines) and PIs (gray vertical lines). The dashed black
    and dashed gray lines provide 95 percent confidence and prediction bands for the
    response variable over the visible range of* x *values.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'In R, this requires you to rerun the `predict` command as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The first line in this code creates the fine sequence of predictor values and
    stores it in the format required by the `newdata` argument. The *y*-axis coordinates
    for CI and PI bands are stored as the second and third columns of the matrix objects
    `ci.band` and `pi.band`. Finally, `lines` is used to add each of the four dashed
    lines corresponding to the upper and lower limits of the two intervals, and a
    legend adds a final touch.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Note that the black dashed CI bands meet the vertical black lines and the gray
    dashed PI bands meet the vertical gray lines for the two individual *x* values
    from earlier, just as you’d expect.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 20-3](ch20.xhtml#ch20fig3) shows the end result of all these additions
    to the plot. The “bowing inwards” curvature of the intervals is characteristic
    of this kind of plot and is especially visible in the CI. This curve occurs because
    there is naturally less variation if you’re predicting where there are more data.
    For more information on `predict` for linear model objects, take a look at the
    `?predict.lm` help file.'
  prefs: []
  type: TYPE_NORMAL
- en: '***20.4.4 Interpolation vs. Extrapolation***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Before finishing this introduction to prediction, it’s important to clarify
    the definitions of two key terms: *interpolation* and *extrapolation*. These terms
    describe the nature of a given prediction. A prediction is referred to as interpolation
    if the *x* value you specify falls within the range of your observed data; extrapolation
    is when the *x* value of interest lies outside this range. From the point-predictions
    you just made, you can see that the location *x* = 14.5 is an example of interpolation,
    and *x* = 24 is an example of extrapolation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, interpolation is preferable to extrapolation—it makes more sense
    to use a fitted model for prediction in the vicinity of data that have already
    been observed. Extrapolation that isn’t too far out of that vicinity may still
    be considered reliable, though. The extrapolation for the student height example
    at *x* = 24 is a case in point. This is outside the range of the observed data,
    but not by much in terms of scale, and the estimated intervals for the expected
    value of ŷ = 188.75 cm appear, at least visually, not unreasonable given the distribution
    of the other observations. In contrast, it would make less sense to use the fitted
    model to predict student height at a handspan of, say, 50 cm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Such an extreme extrapolation suggests that the mean height of an individual
    with a handspan of 50 cm is almost 270 cm, both being fairly unrealistic measurements.
    The same is true in the other direction; the intercept ![image](../images/b0.jpg)
    doesn’t have a particularly useful practical interpretation, indicating that the
    mean height of a student with a handspan of 0 cm is around 114 cm.
  prefs: []
  type: TYPE_NORMAL
- en: The main message here is to use common sense when making any prediction from
    a linear model fit. In terms of the reliability of the results, predictions made
    at values within an appropriate proximity of the observed data are preferable.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 20.1**'
  prefs: []
  type: TYPE_NORMAL
- en: Continue to use the `survey` data frame from the package `MASS` for the next
    few exercises.
  prefs: []
  type: TYPE_NORMAL
- en: Using your fitted model of student height on writing handspan, `survfit`, provide
    point estimates and 99 percent confidence intervals for the mean student height
    for handspans of 12, 15.2, 17, and 19.9 cm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In [Section 20.1](ch20.xhtml#ch20lev1sec62), you defined the object `incomplete.obs`,
    a numeric vector that provides the records of `survey` that were automatically
    removed from consideration when estimating the model parameters. Now, use the
    `incomplete.obs` vector along with `survey` and [Equation (20.3)](ch20.xhtml#ch20eq3)
    to calculate ![image](../images/b0.jpg) and ![image](../images/b1.jpg) in R. (Remember
    the functions `mean`, `sd`, and `cor`. Ensure your answers match the output from
    `survfit`.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `survey` data frame has a number of other variables present aside from `Height`
    and `Wr.Hnd`. For this exercise, the end aim is to fit a simple linear model to
    predict the mean student height, but this time from their pulse rate, given in
    `Pulse` (continue to assume the conditions listed in [Section 20.2](ch20.xhtml#ch20lev1sec63)
    are satisfied).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the regression model and produce a scatterplot with the fitted line superimposed
    upon the data. Make sure you can write down the fitted model equation and keep
    the plot open.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Identify and interpret the point estimate of the slope, as well as the outcome
    of the test associated with the hypotheses H[0] : *β*[1] = 0; H[A] : *β*[1] ≠
    0\. Also find a 90 percent CI for the slope parameter.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Using your model, add lines for 90 percent confidence and prediction interval
    bands on the plot from (i) and add a legend to differentiate between the lines.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Create an `incomplete.obs` vector for the current “height on pulse” data. Use
    that vector to calculate the sample mean of the height observations that were
    used for the model fitted in (i). Then add a perfectly horizontal line to the
    plot at this mean (use color or line type options to avoid confusion with the
    other lines present). What do you notice? Does the plot support your conclusions
    from (ii)?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, examine the help file for the `mtcars` data set, which you first saw in
    [Exercise 13.4](ch13.xhtml#ch13exc4) on [page 287](ch13.xhtml#page_287). For this
    exercise, the goal is to model fuel efficiency, measured in miles per gallon (MPG),
    in terms of the overall weight of the vehicle (in thousands of pounds).
  prefs: []
  type: TYPE_NORMAL
- en: Plot the data—`mpg` on the *y*-axis and `wt` on the *x*-axis.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the simple linear regression model. Add the fitted line to the plot from
    (d).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write down the regression equation and interpret the point estimate of the slope.
    Is the effect of `wt` on mean `mpg` estimated to be statistically significant?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Produce a point estimate and associated 95 percent PI for a car that weighs
    6,000 lbs. Do you trust the model to predict observations accurately for this
    value of the explanatory variable? Why or why not?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**20.5 Understanding Categorical Predictors**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'So far, you’ve looked at simple linear regression models that rely on continuous
    explanatory variables, but it’s also possible to use a discrete or categorical
    explanatory variable, made up of *k* distinct groups or levels, to model the mean
    response. You must be able to make the same assumptions noted in [Section 20.2](ch20.xhtml#ch20lev1sec63):
    that observations are all independent of one another and residuals are normally
    distributed with an equal variance. To begin with, you’ll look at the simplest
    case in which *k* = 2 (a binary-valued predictor), which forms the basis of the
    slightly more complicated situation in which the categorical predictor has more
    than two levels (a multilevel predictor: *k* > 2).'
  prefs: []
  type: TYPE_NORMAL
- en: '***20.5.1 Binary Variables: k = 2***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Turn your attention back to [Equation (20.1)](ch20.xhtml#ch20eq1), where the
    regression model is specified as *Y*|*X* = *β*[0] + *β*[1]*X* + *∊* for a response
    variable *Y* and predictor *X*, and *∊* ~ N(0,*σ*²). Now, suppose your predictor
    variable is categorical, with only two possible levels (binary; *k* = 2) and observations
    coded either 0 or 1\. For this case, (20.1) still holds, but the interpretation
    of the model parameters, *β*[0] and *β*[1], isn’t really one of an “intercept”
    and a “slope” anymore. Instead, it’s better to think of them as being something
    like two intercepts, where *β*[0] provides the *baseline* or *reference* value
    of the response when *X* = 0 and *β*[1] represents the *additive effect* on the
    mean response if *X* = 1\. In other words, if *X* = 0, then *Y* = *β*[0] + *∊*;
    if *X* = 1, then *Y* = *β*[0] + *β*[1] + *∊*. As usual, estimation is in terms
    of finding the *mean* response ŷ ![image](../images/e.jpg)[*Y* |*X* = *x*] as
    per [Equation (20.2)](ch20.xhtml#ch20eq2), so the equation becomes ![image](../images/f0468-01.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: 'Go back to the `survey` data frame and note that you have a `Sex` variable,
    where the students recorded their gender. Look at the documentation on the help
    page `?survey` or enter something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: You’ll see that the sex data column is a factor vector with two levels, `Female`
    and `Male`, and that there happens to be an equal number of the two (one of the
    237 records has a missing value for this variable).
  prefs: []
  type: TYPE_NORMAL
- en: You’re going to determine whether there is statistical evidence that the height
    of a student is affected by sex. This means that you’re again interested in modeling
    height as the response variable, but this time, it’s with the categorical sex
    variable as the predictor.
  prefs: []
  type: TYPE_NORMAL
- en: To visualize the data, if you make a call to `plot` as follows, you’ll get a
    pair of boxplots.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This is because the response variable specified to the left of the `~` is numeric
    and the explanatory variable to the right is a factor, and the default behavior
    of R in that situation is to produce side-by-side boxplots.
  prefs: []
  type: TYPE_NORMAL
- en: To further emphasize the categorical nature of the explanatory variable, you
    can superimpose the raw height and sex observations on top of the boxplots. To
    do this, just convert the factor vector to numeric with a call to `as.numeric`;
    this can be done directly in a call to `points`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Remember that boxplots mark off the median as the central bold line but that
    least-squares linear regressions are defined by the mean outcome, so it’s useful
    to also display the mean heights according to sex.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: You were introduced to `tapply` in [Section 10.2.3](ch10.xhtml#ch10lev2sec94);
    in this call, the argument `na.rm=TRUE` is matched to the ellipsis in the definition
    of `tapply` and is passed to `mean` (you need it to ensure the missing values
    present in the data do not end up producing `NA`s as the results). A further call
    to `points` adds those coordinates (as × symbols) to the image; [Figure 20-4](ch20.xhtml#ch20fig4)
    gives the final result.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f20-04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 20-4: Boxplots of the student heights split by sex, with the raw observations
    and sample means (small ○ and large × symbols, respectively) superimposed*'
  prefs: []
  type: TYPE_NORMAL
- en: The plot indicates, overall, that males tend to be taller than females—but is
    there statistical evidence of a difference to back this up?
  prefs: []
  type: TYPE_NORMAL
- en: '**Linear Regression Model of Binary Variables**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To answer this with a simple linear regression model, you can use `lm` to produce
    least-squares estimates just like with every other model you’ve fitted so far.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: However, because the predictor is a factor vector instead of a numeric vector,
    the reporting of the coefficients is slightly different. The estimate of [0] is
    again reported as `(Intercept)`; this is the estimate of the mean height if a
    student is female. The estimate of *β*[1] is reported as `SexMale`. The corresponding
    regression coefficient of 13.139 is the estimated difference that is imparted
    upon the mean height of a student if male. If you look at the corresponding regression
    equation
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e20-7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'you can see that the model has been fitted assuming the variable *x* is defined
    as “the individual is male”—0 for no/false, 1 for yes/true. In other words, the
    level of “female” for the sex variable is assumed as a reference, and it is the
    effect of “being male” on mean height that is explicitly estimated. The hypothesis
    test for *β*[0] and *β*[1] is performed with the same hypotheses defined in [Section
    20.3.2](ch20.xhtml#ch20lev2sec178):'
  prefs: []
  type: TYPE_NORMAL
- en: 'H[0] : *β[j]* = 0'
  prefs: []
  type: TYPE_NORMAL
- en: 'H[A] : *β[j]* ≠ 0'
  prefs: []
  type: TYPE_NORMAL
- en: Again, it’s the test for *β*[1] that’s generally of the most interest since
    it’s this value that tells you whether there is statistical evidence that the
    mean response variable is affected by the explanatory variable, that is, if *β*[1]
    is significantly different from zero.
  prefs: []
  type: TYPE_NORMAL
- en: '**Predictions from a Binary Categorical Variable**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Because there are only two possible values for *x*, prediction is straightforward
    here. When you evaluate the equation, the only decision that needs to be made
    is whether ![image](../images/b1.jpg) needs to be used (in other words, if an
    individual is male) or not (if an individual is female). For example, you can
    enter the following code to create a factor of five extra observations with the
    same level names as the original data and store the new data in `extra.obs`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Then, use `predict` in the now-familiar fashion to find the mean heights at
    those extra values of the predictor. (Remember that when you pass in new data
    to `predict` using the `newdata` argument, the predictors must be in the same
    form as the data that were used to fit the model in the first place.)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: You can see from the output that the predictions are different only between
    the two sets of values—the point estimates of the two instances of `Female` are
    identical, simply ![image](../images/b0.jpg) with 90 percent CIs. The point estimates
    and CIs for the instances of `Male` are also all the same as each other, based
    on a point estimate of ![image](../images/f0472-01.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: On its own, admittedly, this example isn’t too exciting. However, it’s critical
    to understand how R presents regression results when using categorical predictors,
    especially when considering multiple regression in [Chapter 21](ch21.xhtml#ch21).
  prefs: []
  type: TYPE_NORMAL
- en: '***20.5.2 Multilevel Variables: k > 2***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It’s common to work with data where the categorical predictor variables have
    more than two levels so that (*k* > 2). These can also be referred to as *multilevel*
    categorical variables. To deal with this more complicated situation while retaining
    interpretability of your parameters, you must first dummy code your predictor
    into *k* − 1 binary variables.
  prefs: []
  type: TYPE_NORMAL
- en: '**Dummy Coding Multilevel Variables**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To see how this is done, assume that you want to find the value of response
    variable *Y* when given the value of a categorical variable *X*, where *X* has
    *k* > 2 levels (also assume the conditions for validity of the linear regression
    model—[Section 20.2](ch20.xhtml#ch20lev1sec63)—are satisfied).
  prefs: []
  type: TYPE_NORMAL
- en: In regression modeling, *dummy coding* is the procedure used to create several
    binary variables from a categorical variable like *X*. Instead of the single categorical
    variable with possible realizations
  prefs: []
  type: TYPE_NORMAL
- en: '*X* = 1,2,3, . . . , *k*'
  prefs: []
  type: TYPE_NORMAL
- en: 'you recode it into several yes/no variables—one for each level—with possible
    realizations:'
  prefs: []
  type: TYPE_NORMAL
- en: '*X*[(][1][)] = 0,1; *X*[(][2][)] = 0,1; *X*[(][3][)] = 0,1; . . . ; *X*[(][k][)]
    = 0,1'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, *X*[(][i][)] represents a binary variable for the *i*th level
    of the original *X*. For example, if an individual has *X* = 2 in the original
    categorical variable, then *X*[(][2][)] = 1 (yes) and all of the others (*X*[(][1][)],
    *X*[(][3][)], . . . , *X*[(][k][)]) will be zero (no).
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose *X* is a variable that can take any one of the *k* = 4 values 1, 2,
    3, or 4, and you’ve made six observations of this variable: 1, 2, 2, 1, 4, 3\.
    [Table 20-1](ch20.xhtml#ch20tab1) shows these observations and their dummy-coded
    equivalents *X*[(][1][)], *X*[(][2][)], *X*[(][3][)], and *X*[(][4][)].'
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 20-1:** Illustrative Example of Dummy Coding for Six Observations of
    a Categorical Variable with *k* = 4 Groups'
  prefs: []
  type: TYPE_NORMAL
- en: '| *X* | *X*[(][1][)] | *X*[(][2][)] | *X*[(][3][)] | *X*[(][4][)] |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0 | 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0 | 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 0 | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0 | 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: In fitting the subsequent model, you usually only use *k* − 1 of the dummy binary
    variables—one of the variables acts as a *reference* or *baseline* level, and
    it’s incorporated into the overall intercept of the model. In practice, you would
    end up with an estimated model like this,
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e20-8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: assuming 1 is the reference level. As you can see, in addition to the overall
    intercept term ![image](../images/b0.jpg), you have *k* −1 other estimated intercept
    terms that modify the baseline coefficient ![image](../images/b0.jpg), depending
    on which of the original categories an observation takes on. For example, in light
    of the coding imposed in (20.8), if an observation has *X*[(][3][)] = 1 and all
    other binary values are therefore zero (so that observation would’ve had a value
    of *X* = 3 for the original categorical variable), the predicted mean value of
    the response would be ![image](../images/f0473-01.jpg). On the other hand, because
    the reference level is defined as 1, if an observation has values of zero for
    all the binary variables, it implies the observation originally had *X* = 1, and
    the prediction would be simply ![image](../images/f0473-02.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: The reason it’s necessary to dummy code for categorical variables of this nature
    is that, in general, categories cannot be related to each other in the same numeric
    sense as continuous variables. It’s often not appropriate, for example, to think
    that an observation in category 4 is “twice as much” as one in category 2, which
    is what the estimation methods would assume. Binary presence/absence variables
    are valid, however, and can be easily incorporated into the modeling framework.
    Choosing the reference level is generally of secondary importance—the specific
    values of the estimated coefficients will change accordingly, but any overall
    interpretations you make based on the fitted model will be the same regardless.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Implementing this dummy-coding approach is technically a form of multiple
    regression since you’re now including several binary variables in the model. It’s
    important, however, to be aware of the somewhat artificial nature of dummy coding—you
    should still think of the multiple coefficients as representing a single categorical
    variable since the binary variables X*[(][1][)]*, . . . , X*[(][k][)] *are not
    independent of one another. This is why I’ve chosen to define these models in
    this chapter; multiple regression will be formally discussed in [Chapter 21](ch21.xhtml#ch21).*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Linear Regression Model of Multilevel Variables**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: R makes working with categorical predictors in this way quite simple since it
    automatically dummy codes for any such explanatory variable when you call `lm`.
    There are two things you should check before fitting your model, though.
  prefs: []
  type: TYPE_NORMAL
- en: The categorical variable of interest should be stored as a (formally unordered)
    factor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You should check that you’re happy with the category assigned as the reference
    level (for interpretative purposes—see [Section 20.5.3](ch20.xhtml#ch20lev2sec187)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You must also of course be happy with the validity of the familiar assumptions
    of normality and independence of *∊*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate all these definitions and ideas, let’s return to the student
    survey data from the `MASS` package and keep “student height” as the response
    variable of interest. Among the data is the variable `Smoke`. This variable describes
    the kind of smoker each student reports themselves as, defined by frequency and
    split into four categories: “heavy,” “never,” “occasional,” and “regular.”'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Here, the result from `is.factor(survey$Smoke)` indicates that you do indeed
    have a factor vector at hand, the call to `table` yields the number of students
    in each of the four categories, and as per [Chapter 5](ch05.xhtml#ch05), you can
    explicitly request the levels attribute of any R factor via `levels`.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s ask whether there’s statistical evidence to support a difference in mean
    student height according to smoking frequency. You can create a set of boxplots
    of these data with the following two lines; [Figure 20-5](ch20.xhtml#ch20fig5)
    shows the result.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '![image](../images/f20-05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 20-5: Boxplots of the observed student heights split by smoking frequency;
    respective sample means marked with* ×'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note from earlier R output that unless explicitly defined at creation, the
    levels of a factor appear in alphabetical order by default—as is the case for
    `Smoke`—and R will automatically set the first one (as shown in the output of
    a call to `levels`) as the reference level when that factor is used as a predictor
    in subsequent model fitting. Fitting the linear model in mind using `lm`, you
    can see from a subsequent call to `summary` that indeed the first level of `Smoke`,
    for “heavy”, has been used as the reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: As outlined in [Equation (20.8)](ch20.xhtml#ch20eq8), you get estimates of coefficients
    corresponding to the dummy binary variables for three of the four possible categories
    in this example—the three nonreference levels. The observation in the reference
    category `Heavy` is represented solely by ![image](../images/b0.jpg), designated
    first as the overall `(Intercept)`, with the other coefficients providing the
    effects associated with an observation in one of the other categories.
  prefs: []
  type: TYPE_NORMAL
- en: '**Predictions from a Multilevel Categorical Variable**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: You find point estimates through prediction, as usual.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Here, I’ve created the object `one.of.each` for illustrative purposes; it represents
    one observation in each of the four categories, stored as an object matching the
    class (and levels) of the original `Smoke` data. A student in the `Occas` category,
    for example, is predicted to have a mean height of 173.772 − 0.7433 = 173.0287.
  prefs: []
  type: TYPE_NORMAL
- en: The output from the model summary earlier, however, shows that none of the binary
    dummy variable coefficients are considered statistically significant from zero
    (because all the *p*-values are too large). The results indicate, as you might
    have suspected, that there’s no evidence that smoking frequency (or more specifically,
    having a smoking frequency that’s different from the reference level) affects
    mean student heights based on this sample of individuals. As is common, the baseline
    coefficient ![image](../images/b0.jpg) is highly statistically significant—but
    that only suggests that the overall intercept probably isn’t zero. (Because your
    response variable is a measurement of height and will clearly not be centered
    anywhere near 0 cm, that result makes sense.) The confidence intervals supplied
    are calculated in the usual *t*-based fashion.
  prefs: []
  type: TYPE_NORMAL
- en: The small `R-Squared` value reinforces this conclusion, indicating that barely
    any of the variation in the response can be explained by changing the category
    of smoking frequency. Furthermore, the overall *F* -test *p*-value is rather large
    at around 0.215, suggesting an overall nonsignificant effect of the predictor
    on the response; you’ll look at this in more detail in a moment in [Section 20.5.5](ch20.xhtml#ch20lev2sec189)
    and later on in [Section 21.3.5](ch21.xhtml#ch21lev2sec197).
  prefs: []
  type: TYPE_NORMAL
- en: As noted earlier, it’s important that you interpret these results—indeed any
    based on a *k*-level categorical variable in regression—in a *collective* fashion.
    You can claim only that there is *no* discernible effect of smoking on height
    because *all* the *p*-values for the binary dummy coefficients are nonsignificant.
    If one of the levels was in fact highly significant (through a small *p*-value),
    it would imply that the smoking factor as defined here, as a whole, *does* have
    a statistically detectable effect on the response (even if the other two levels
    were still associated with very high *p*-values). This will be discussed further
    in several more examples in [Chapter 21](ch21.xhtml#ch21).
  prefs: []
  type: TYPE_NORMAL
- en: '***20.5.3 Changing the Reference Level***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Sometimes you might decide to change the automatically selected reference level,
    compared to which the effects of taking on any of the other levels are estimated.
    Changing the baseline will result in the estimation of different coefficients,
    meaning that individual *p*-values are subject to change, but the overall result
    (in terms of global significance of the factor) will not be affected. Because
    of this, altering the reference level is only done for interpretative purposes—sometimes
    there’s an intuitively natural baseline of the predictor (for example, “Placebo”
    versus “Drug A” and “Drug B” as a treatment variable in the analysis of some clinical
    trial) from which you want to estimate deviation in the mean response with respect
    to the other possible categories.
  prefs: []
  type: TYPE_NORMAL
- en: Redefining the reference level can be achieved quickly using the built-in `relevel`
    function in R. This function allows you to choose which level comes first in the
    definition of a given factor vector object and will therefore be designated as
    the reference level in subsequent model fitting. In the current example, let’s
    say you’d rather have the nonsmokers as the reference level.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The `relevel` function has moved the `Never` category into the first position
    in the new factor vector. If you go ahead fit the model again using `SmokeReordered`
    instead of the original `Smoke` column of `survey`, it’ll provide estimates of
    coefficients associated with the three different levels of smokers.
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth noting the differences in the treatment of unordered versus ordered
    factor vectors in regression applications. It might seem sensible to formally
    order the smoking variable by, for example, increasing the frequency of smoking
    when creating a new factor vector. However, when an ordered factor vector is supplied
    in a call to `lm`, R reacts in a different way—it doesn’t perform the relatively
    simple dummy coding discussed here, where an effect is associated with each optional
    level to the baseline (technically referred to as *orthogonal contrasts*). Instead,
    the default behavior is to fit the model based on something called *polynomial
    contrasts*, where the effect of the ordered categorical variable on the response
    is defined in a more complicated functional form. That discussion is beyond the
    scope of this text, but it suffices to say that this approach can be beneficial
    when your interest lies in the specific functional nature of “moving up” through
    an ordered set of categories. For more on the technical details, see Kuhn and
    Johnson ([2013](ref.xhtml#ref37)). For all relevant regression examples in this
    book, we’ll work exclusively with unordered factor vectors.
  prefs: []
  type: TYPE_NORMAL
- en: '***20.5.4 Treating Categorical Variables as Numeric***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The way in which `lm` decides to define the parameters of the fitted model depends
    primarily on the kind of data you pass to the function. As discussed, `lm` imposes
    dummy coding only if the explanatory variable is an unordered factor vector.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes the categorical data you want to analyze haven’t been stored as a
    factor in your data object. If the categorical variable is a character vector,
    `lm` will implicitly coerce it into a factor. If, however, the intended categorical
    variable is numeric, then `lm` performs linear regression exactly as if it were
    a continuous numeric predictor; it estimates a single regression coefficient,
    which is interpreted as a “per-one-unit-change” in the mean response.
  prefs: []
  type: TYPE_NORMAL
- en: This may seem inappropriate if the original explanatory variable is supposed
    to be made up of distinct groups. In some settings, however, especially when the
    variable can be naturally treated as numeric-discrete, this treatment is not only
    valid statistically but also helps with interpretation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a break from the `survey` data and go back to the ready-to-use `mtcars`
    data set. Say you’re interested in the variables mileage, `mpg` (continuous),
    and number of cylinders, `cyl` (discrete; the data set contains cars with either
    4, 6, or 8 cylinders). Now, it’s perfectly sensible to automatically think of
    `cyl` as a categorical variable. Taking `mpg` to be the response variable, box-plots
    are well suited to reflect the grouped nature of `cyl` as a predictor; the result
    of the following line is given on the left of [Figure 20-6](ch20.xhtml#ch20fig6):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: When fitting the associated regression model, you must be aware of what you’re
    instructing R to do. Since the `cyl` column of `mtcars` is numeric, and not a
    factor vector per se, `lm` will treat it as continuous if you just directly access
    the data frame.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Just as in earlier sections, you’ve received an intercept and a slope estimate;
    the latter is highly statistically significant, indicating that there is evidence
    against the true value of the slope being zero. Your fitted regression line is
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f0479-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where ŷ is the average mileage and *x* is numeric—the number of cylinders. For
    each single additional cylinder, the model says your mileage will decrease by
    2.88 MPG, on average.
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s important to recognize the fact that you’ve fitted a continuous line to
    what is effectively categorical data. The right panel of [Figure 20-6](ch20.xhtml#ch20fig6),
    created with the following lines, highlights this fact:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '![image](../images/f20-06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 20-6: Left: Boxplots of mileage split by cylinders for the* `mtcars`
    *data set. Right: Scatterplot of the same data with fitted regression line (treating*
    `cyl` *as numeric-continuous) superimposed.*'
  prefs: []
  type: TYPE_NORMAL
- en: Some researchers fit categorical or discrete predictors as continuous variables
    purposefully. First, it allows interpolation; for example, you could use this
    model to evaluate the average MPG for a 5-cylinder car. Second, it means there
    are fewer parameters that require estimation; in other words, instead of *k* −
    1 intercepts for a categorical variable with *k* groups, you need only one parameter
    for the slope. Finally, it can be a convenient way to control for so-called nuisance
    variables; this will become clearer in [Chapter 21](ch21.xhtml#ch21). On the other
    hand, it means that you no longer get group-specific information. It can be misleading
    to proceed in this way if any differences in the mean response according to the
    predictor category of an observation are not well represented linearly—detection
    of significant effects can be lost altogether.
  prefs: []
  type: TYPE_NORMAL
- en: At the very least, it’s important to recognize this distinction when fitting
    models. If you had only just now recognized that R had fitted the `cyl` variable
    as continuous and wanted to actually fit the model with `cyl` as categorical,
    you’d have to explicitly convert it into a factor vector beforehand or in the
    actual call to `lm`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Here, by wrapping `cyl` in a call to `factor` when specifying the formula for
    `lm`, you can see you’ve obtained regression coefficient estimates for the levels
    of `cyl` corresponding to 6- and 8-cylinder cars (with the reference level automatically
    set to 4-cylinder cars).
  prefs: []
  type: TYPE_NORMAL
- en: '***20.5.5 Equivalence with One-Way ANOVA***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There’s one final observation to make about regression models with a single
    nominal categorical predictor. Think about the fact that these models describe
    a mean response value for the *k* different groups. Does this remind you of anything?
    In this particular setting, you’re actually doing the same thing as in one-way
    ANOVA ([Section 19.1](ch19.xhtml#ch19lev1sec59)): comparing more than two means
    and determining whether there is statistical evidence that at least one mean is
    different from the others. You need to be able to make the same key assumptions
    of independence and normality for both techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: In fact, simple linear regression with a single categorical predictor, implemented
    using least-squares estimation, is just another way to perform one-way ANOVA.
    Or, perhaps more concisely, ANOVA is a special case of least-squares regression.
    The outcome of a one-way ANOVA test is a single *p*-value quantifying a level
    of statistical evidence against the null hypothesis that states that group means
    are equal. When you have one categorical predictor in a regression, it’s exactly
    that *p*-value that’s reported at the end of the `summary` of an `lm` object—something
    I’ve referred to a couple of times now as the “overall” or “global” significance
    test (for example, in [Section 20.3.3](ch20.xhtml#ch20lev2sec179)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Look back to the final result of that global significance test for the student
    height modeled by smoking status example—you had a *p*-value of 0.2147\. This
    came from an *F* test statistic of 1.504 with df[1] = 3 and df[2] = 205\. Now,
    suppose you were just handed the data and asked to perform a one-way ANOVA of
    height on smoking. Using the `aov` function as introduced in [Section 19.1](ch19.xhtml#ch19lev1sec59),
    you’d call something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Those same values are returned here; you can also find the square root of the
    MSE:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: This is in fact the “residual standard error” given in the `lm` summary. The
    two conclusions you’d draw about the impact of smoking status on height (one for
    the `lm` output, the other for the ANOVA test) are of course also the same.
  prefs: []
  type: TYPE_NORMAL
- en: The global test that `lm` provides isn’t just there for the benefit of confirming
    ANOVA results. As a generalization of ANOVA, least-squares regression models provide
    more than just coefficient-specific tests. That global test is formally referred
    to as the *omnibus* F-*test*, and while it is indeed equivalent to one-way ANOVA
    in the “single categorical predictor” setting, it’s also a useful overall, stand-alone
    test of the statistical contribution of several predictors to the outcome value.
    You’ll explore this further in [Section 21.3.5](ch21.xhtml#ch21lev2sec197) after
    you’ve begun modeling your response variable using multiple explanatory variables.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 20.2**'
  prefs: []
  type: TYPE_NORMAL
- en: Continue using the `survey` data frame from the package `MASS` for the next
    few exercises.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `survey` data set has a variable named `Exer`, a factor with *k* = 3 levels
    describing the amount of physical exercise time each student gets: none, some,
    or frequent. Obtain a count of the number of students in each category and produce
    side-by-side boxplots of student height split by exercise.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assuming independence of the observations and normality as usual, fit a linear
    regression model with height as the response variable and exercise as the explanatory
    variable (dummy coding). What’s the default reference level of the predictor?
    Produce a model summary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Draw a conclusion based on the fitted model from (b)—does it appear that exercise
    frequency has any impact on mean height? What is the nature of the estimated effect?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Predict the mean heights of one individual in each of the three exercise categories,
    accompanied by 95 percent prediction intervals.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do you arrive at the same result and interpretation for the height-by-exercise
    model if you construct an ANOVA table using `aov`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is there any change to the outcome of (e) if you alter the model so that the
    reference level of the exercise variable is “none”? Would you expect there to
    be?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, turn back to the ready-to-use `mtcars` data set. One of the variables in
    this data frame is `qsec`, described as the time in seconds it takes to race a
    quarter mile; another is `gear`, the number of forward gears (cars in this data
    set have either 3, 4, or 5 gears).
  prefs: []
  type: TYPE_NORMAL
- en: Using the vectors straight from the data frame, fit a simple linear regression
    model with `qsec` as the response variable and `gear` as the explanatory variable
    and interpret the model summary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explicitly convert `gear` to a factor vector and refit the model. Compare the
    model summary with that from (g). What do you find?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain, with the aid of a relevant plot in the same style as the right image
    of [Figure 20-6](ch20.xhtml#ch20fig6), why you think there is a difference between
    the two models (g) and (h).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Important Code in This Chapter**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| **Function/operator** | **Brief description** | **First occurrence** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `lm` | Fit linear model | [Section 20.2.3](ch20.xhtml#ch20lev2sec175), [p.
    455](ch20.xhtml#page_455) |'
  prefs: []
  type: TYPE_TB
- en: '| `coef` | Get estimated coefficients | [Section 20.2.4](ch20.xhtml#ch20lev2sec176),
    [p. 457](ch20.xhtml#page_457) |'
  prefs: []
  type: TYPE_TB
- en: '| `summary` | Summarize linear model | [Section 20.3.1](ch20.xhtml#ch20lev2sec177),
    [p. 458](ch20.xhtml#page_458) |'
  prefs: []
  type: TYPE_TB
- en: '| `confint` | Get CIs for estimated coefficients | [Section 20.3.2](ch20.xhtml#ch20lev2sec178),
    [p. 460](ch20.xhtml#page_460) |'
  prefs: []
  type: TYPE_TB
- en: '| `predict` | Predict from linear model | [Section 20.4.2](ch20.xhtml#ch20lev2sec182),
    [p. 463](ch20.xhtml#page_463) |'
  prefs: []
  type: TYPE_TB
- en: '| `relevel` | Change factor reference level | [Section 20.5.3](ch20.xhtml#ch20lev2sec187),
    [p. 477](ch20.xhtml#page_477) |'
  prefs: []
  type: TYPE_TB
