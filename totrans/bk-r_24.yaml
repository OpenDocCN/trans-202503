- en: '**20**'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**SIMPLE LINEAR REGRESSION**'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/common-01.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: 'Though straightforward comparative tests of individual statistics are useful
    in their own right, you’ll often want to learn more from your data. In this chapter,
    you’ll look at *linear regression* models: a suite of methods used to evaluate
    precisely *how* variables relate to each other.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Simple linear regression models describe the effect that a particular variable,
    called the *explanatory variable*, might have on the value of a continuous outcome
    variable, called the *response variable*. The explanatory variable may be continuous,
    discrete, or categorical, but to introduce the key concepts, I’ll concentrate
    on continuous explanatory variables for the first several sections in this chapter.
    Then, I’ll cover how the representation of the model changes if the explanatory
    variable is categorical.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '**20.1 An Example of a Linear Relationship**'
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As an example to start with, let’s continue with the data used in [Section 19.3](ch19.xhtml#ch19lev1sec61)
    and look at the student survey data (the `survey` data frame in the package `MASS`)
    a little more closely. If you haven’t already done so, with the required package
    loaded (call `library("MASS")`), you can read the help file `?survey` for details
    on the variables present.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Plot the student heights on the *y*-axis and their handspans (of their writing
    hand) on the *x*-axis.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[Figure 20-1](ch20.xhtml#ch20fig1) shows the result.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f20-01.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
- en: '*Figure 20-1: A scatterplot of height against writing handspan for a sample
    of first-year statistics students*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Note that the call to `plot` uses formula notation (also referred to as *symbolic
    notation*) to specify “height *on* handspan.” You can produce the same scatterplot
    by using the coordinate vector form of (*x*, *y*), that is, `plot(survey$Wr.Hnd,survey$Height,...)`,
    but I’m using the symbolic notation here because it nicely reflects how you’ll
    fit the linear model in a moment.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: As you might expect, there’s a positive association between a student’s handspan
    and their height. That relationship appears to be linear in nature. To assess
    the strength of the linear relationship (refer to [Section 13.2.5](ch13.xhtml#ch13lev2sec120)),
    you can find the estimated correlation coefficient.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Though there are 237 records in the data frame, the plot doesn’t actually show
    237 points. This is because there are missing observations (coded `NA`; see [Section
    6.1.3](ch06.xhtml#ch06lev2sec57)). By default, R removes any “incomplete” pairs
    when producing a plot like this. To find out how many offending observations have
    been deleted, you can use the short-form logical operator `|` ([Section 4.1.3](ch04.xhtml#ch04lev2sec39))
    in conjunction with `is.na` ([Section 6.1.3](ch06.xhtml#ch06lev2sec57)) and `which`
    ([Section 4.1.5](ch04.xhtml#ch04lev2sec41)). You then use `length` to discover
    there are 29 missing observation pairs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**NOTE**'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '*Because there are* `NA`*s in the vectors supplied to the correlation coefficient
    function* `cor`*, you must also specify the optional argument* `use="complete.obs"`*.
    This means that the calculated statistic takes into account only those observation
    pairs in the* `Wr.Hnd` *and* `Height` *vectors where* neither *element is* `NA`*.
    You can think of this argument as doing much the same thing as* `na.rm=TRUE` *in
    univariate summary statistic functions such as* `mean` *and* `sd`.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '**20.2 General Concepts**'
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The purpose of a linear regression model is to come up with a function that
    estimates the *mean* of one variable given a particular value of another variable.
    These variables are known as the *response variable* (the “outcome” variable whose
    mean you are attempting to find) and the *explanatory variable* (the “predictor”
    variable whose value you already have).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: In terms of the student survey example, you might ask something like “What’s
    the expected height of a student if their handspan is 14.5 cm?” Here the response
    variable is the height, and the explanatory variable is the handspan.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '***20.2.1 Definition of the Model***'
  id: totrans-22
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Assume you’re looking to determine the value of response variable *Y* given
    the value of an explanatory variable *X*. The *simple linear regression model*
    states that the value of a response is expressed as the following equation:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e20-1.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
- en: On the left side of [Equation (20.1)](ch20.xhtml#ch20eq1), the notation *Y*|*X*
    reads as “the value of *Y* conditional upon the value of *X*.”
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '**Residual Assumptions**'
  id: totrans-26
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The validity of the conclusions you can draw based on the model in (20.1) is
    critically dependent on the assumptions made about *∊*, which are defined as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: • The value of *∊* is assumed to be normally distributed such that *∊* ~ N(0,*σ*).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: • That *∊* is centered (that is, has a mean of) zero.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: • The variance of *∊*, *σ*², is constant.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: The *∊* term represents random error. In other words, you assume that any raw
    value of the response is owed to a linear change in a given value of *X*, plus
    or minus some random, *residual* variation or normally distributed *noise*.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '**Parameters**'
  id: totrans-32
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The value denoted by *β*[0] is called the *intercept*, and that of *β*[1] is
    called the *slope*. Together, they are also referred to as the *regression coefficients*
    and are interpreted as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: • The intercept, *β*[0], is interpreted as the expected value of the response
    variable when the predictor is zero.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: • Generally, the slope, *β*[1], is the focus of interest. This is interpreted
    as the change in the mean response for each one-unit increase in the predictor.
    When the slope is positive, the regression line increases from left to right (the
    mean response is higher when the predictor is higher); when the slope is negative,
    the line decreases from left to right (the mean response is lower when the predictor
    is higher). When the slope is zero, this implies that the predictor has no effect
    on the value of the response. The more extreme the value of *β*[1] (that is, away
    from zero), the steeper the increasing or decreasing line becomes.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: • 通常，斜率*β*[1]是关注的重点。它表示每增加一个单位的预测变量，平均响应的变化。当斜率为正时，回归线从左到右上升（当预测变量增大时，平均响应也增大）；当斜率为负时，回归线从左到右下降（当预测变量增大时，平均响应减小）。当斜率为零时，表示预测变量对响应值没有影响。*β*[1]的绝对值越大（即远离零），回归线的升降越陡峭。
- en: '***20.2.2 Estimating the Intercept and Slope Parameters***'
  id: totrans-36
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***20.2.2 估计截距和斜率参数***'
- en: 'The goal is to use your data to estimate the regression parameters, yielding
    the estimates ![image](../images/b0.jpg) and ![image](../images/b1.jpg); this
    is referred to as *fitting* the linear model. In this case, the data comprise
    *n* pairs of observations for each individual. The fitted model of interest concerns
    the mean response value, denoted ŷ, for a specific value of the predictor, *x*,
    and is written as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是使用你的数据来估计回归参数，从而得到估计值![image](../images/b0.jpg)和![image](../images/b1.jpg)；这被称为*拟合*线性模型。在这种情况下，数据包括每个个体的*n*对观察值。所关注的拟合模型涉及对特定预测值*x*的平均响应值，记作ŷ，公式如下：
- en: '![image](../images/e20-2.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/e20-2.jpg)'
- en: Sometimes, alternative notation such as ![image](../images/e.jpg)[*Y*] or ![image](../images/e.jpg)[*Y*|*X*
    = *x*] is used on the left side of (20.2) to emphasize the fact that the model
    gives the mean (that is, the expected value) of the response. For compactness,
    many simply use something like ŷ, as shown here.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，会使用替代符号，如![image](../images/e.jpg)[*Y*]或![image](../images/e.jpg)[*Y*|*X*
    = *x*]，在公式（20.2）的左侧，强调模型给出的响应的均值（即期望值）。为了简洁起见，许多人直接使用类似ŷ的符号，如这里所示。
- en: Let your *n* observed data pairs be denoted *x[i]* and *y[i]* for the predictor
    and response variables, respectively; *i* = 1, . . . , *n*. Then, the parameter
    estimates for the simple linear regression function are
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 令你观察到的*n*对数据分别记作*X[i]*和*Y[i]*，其中*i* = 1, . . . , *n*。然后，简单线性回归函数的参数估计值为
- en: '![image](../images/e20-3.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/e20-3.jpg)'
- en: where
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: • *x̄* and *ȳ* are the sample means of the *x[i]*s and *y[i]*s.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: • *x̄*和*ȳ*分别是*x[i]*和*y[i]*的样本均值。
- en: • *s[x]* and *s[y]* are the sample standard deviations of the *x[i]*s and *y[i]*s.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: • *s[x]*和*s[y]*分别是*x[i]*和*y[i]*的样本标准差。
- en: • *ρ[xy]* is the estimate of correlation between *X* and *Y* based on the data
    (see [Section 13.2.5](ch13.xhtml#ch13lev2sec120)).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: • *ρ[xy]*是基于数据估计的*X*和*Y*之间的相关性（见[第13.2.5节](ch13.xhtml#ch13lev2sec120)）。
- en: Estimating the model parameters in this way is referred to as *least-squares
    regression*; the reason for this will become clear in a moment.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式估计模型参数被称为*最小二乘回归*；稍后你会明白其原因。
- en: '***20.2.3 Fitting Linear Models with lm***'
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***20.2.3 使用lm拟合线性模型***'
- en: 'In R, the command `lm` performs the estimation for you. For example, the following
    line creates a fitted linear model object of the mean student height by handspan
    and stores it in your global environment as `survfit`:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在R语言中，命令`lm`会为你执行估计。例如，下面这一行手动创建了一个基于手掌跨度的平均学生身高的拟合线性模型对象，并将其存储在你的全局环境中，命名为`survfit`：
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The first argument is the now-familiar `response` ~ predictor formula, which
    specifies the desired model. You don’t have to use the `survey$` prefix to extract
    the vectors from the data frame because you specifically instruct `lm` to look
    in the object supplied to the `data` argument.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个参数是现在熟悉的`response` ~ predictor公式，它指定了所需的模型。你不需要使用`survey$`前缀来从数据框中提取向量，因为你已经明确指示`lm`去查找提供给`data`参数的对象。
- en: The fitted linear model object itself, `survfit`, has a special class in R—one
    of `"lm"`. An object of class `"lm"` can essentially be thought of as a list containing
    several components that describe the model. You’ll look at these in a moment.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 拟合的线性模型对象本身`survfit`在R中有一个特殊的类——即`"lm"`类。`"lm"`类的对象本质上可以看作是一个包含多个组件的列表，这些组件描述了该模型。稍后你将查看这些组件。
- en: 'If you simply enter the name of the `"lm"` object at the prompt, it will provide
    the most basic output: a repeat of your call and the estimates of the intercept
    (![image](../images/b0.jpg)) and slope (![image](../images/b1.jpg)).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This reveals that the linear model for this example is estimated as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e20-4.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
- en: If you evaluate the mathematical function for ŷ—[Equation (20.2)](ch20.xhtml#ch20eq2)—at
    a range of different values for *x*, you end up with a straight line when you
    plot the results. Considering the definition of intercept given earlier as the
    expected value of the response variable when the predictor is zero, in the current
    example, this would imply that the mean height of a student with a handspan of
    0 cm is 113.954 cm (an arguably less-than-useful statement since a value of zero
    for the explanatory variable doesn’t make sense; you’ll consider these and related
    issues in [Section 20.4](ch20.xhtml#ch20lev1sec65)). The slope, the change in
    the mean response for each one-unit increase in the predictor, is 3.117\. This
    states that, on average, for every 1 cm increase in handspan, a student’s height
    is estimated to increase by 3.117 cm.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: With all this in mind, once more run the line to plot the raw data as given
    in [Section 20.1](ch20.xhtml#ch20lev1sec62) and shown in [Figure 20-1](ch20.xhtml#ch20fig1),
    but now add the fitted regression line using `abline`. So far, you’ve only used
    the `abline` command to add perfectly horizontal and vertical lines to an existing
    plot, but when passed an object of class `"lm"` that represents a simple linear
    model, like `survfit`, the fitted regression line will be added instead.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This adds the slightly thickened diagonally increasing line shown in [Figure
    20-2](ch20.xhtml#ch20fig2).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f20-02.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
- en: '*Figure 20-2: The simple linear regression line (solid, bold) fitted to the
    observed data. Two dashed vertical line segments provide examples of a positive
    (leftmost) and negative (rightmost)* residual.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '***20.2.4 Illustrating Residuals***'
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When the parameters are estimated as shown here, using (20.3), the fitted line
    is referred to as an implementation of *least-squares regression* because it’s
    the line that minimizes the average squared difference between the observed data
    and itself. This concept is easier to understand by drawing the distances between
    the observations and the fitted line, formally called *residuals*, for a couple
    of individual observations within [Figure 20-2](ch20.xhtml#ch20fig2).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s extract two specific records from the `Wr.Hnd` and `Height` data
    vectors and call the resulting vectors `obsA` and `obsB`.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Next, briefly inspect the names of the members of the `survfit` object.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: These members are the components that automatically make up a fitted model object
    of class `"lm"`, mentioned briefly earlier. Note that there’s a component called
    `"coefficients"`. This contains a numeric vector of the estimates of the intercept
    and slope.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 'You can extract this component (and indeed any of the other ones listed here)
    in the same way you would perform a member reference on a named list: by entering
    `survfit$coefficients` at the prompt. Where possible, though, it’s technically
    preferable for programming purposes to extract such components using a “direct-access”
    function. For the `coefficients` component of an `"lm"` object, the function you
    use is `coef`.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以像在命名列表上进行成员引用那样提取这个组件（以及这里列出的任何其他组件）：通过在提示符处输入`survfit$coefficients`。不过，在可能的情况下，从编程角度来说，使用“直接访问”函数提取这些组件是更为推荐的做法。对于`"lm"`对象的`coefficients`组件，你需要使用的函数是`coef`。
- en: '[PRE8]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Here, the regression coefficients are extracted from the object and then separately
    assigned to the objects `beta0.hat` and `beta1.hat`. Other common direct-access
    functions include `resid` and `fitted`; these two pertain to the `"residuals"`
    and `"fitted.values"` components, respectively.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，回归系数从对象中提取出来，然后分别赋值给对象`beta0.hat`和`beta1.hat`。其他常见的直接访问函数包括`resid`和`fitted`；这两个函数分别涉及到“残差”和“拟合值”组件。
- en: Finally, I use `segments` to draw the vertical dashed lines present in [Figure
    20-2](ch20.xhtml#ch20fig2).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我使用`segments`绘制[图20-2](ch20.xhtml#ch20fig2)中存在的垂直虚线。
- en: '[PRE9]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note that the dashed lines meet the fitted line at the vertical axis locations
    passed to `y0`, which, with the use of the regression coefficients `beta0.hat`
    and `beta1.hat`, reflects [Equation (20.4)](ch20.xhtml#ch20eq4).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这些虚线与拟合线在垂直轴位置相交，这些位置通过传递给`y0`的值反映了使用回归系数`beta0.hat`和`beta1.hat`后，[方程(20.4)](ch20.xhtml#ch20eq4)。
- en: Now, imagine a collection of alternative regression lines drawn through the
    data (achieved by altering the value of the intercept and slope). Then, for each
    of the alternative regression lines, imagine you calculate the residuals (vertical
    distances) between the response value of every observation and the *fitted value*
    of that line. The simple linear regression line estimated as per (20.3) is the
    line that lies “closest to all observations.” By this, it is meant that the fitted
    regression model is represented by the estimated line that passes through the
    coordinate provided by the variable means (*x̄*, *ȳ*), and it’s the line that
    yields the smallest overall measure of the squared residual distances. For this
    reason, another name for a least-squares-estimated regression equation like this
    is the *line of best fit*.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设有一组通过数据绘制的不同回归线（通过改变截距和斜率的值实现）。然后，对于每一条替代的回归线，假设你计算每个观察值的响应值与该回归线的*拟合值*之间的残差（垂直距离）。根据(20.3)估算的简单线性回归线是“最接近所有观察值”的那条线。这里的意思是，拟合的回归模型由通过由变量均值（*x̄*，*ȳ*）给出的坐标的估计线表示，它是那条使得残差的平方和最小的线。因此，像这样最小二乘估计的回归方程的另一个名字是*最佳拟合线*。
- en: '**20.3 Statistical Inference**'
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**20.3 统计推断**'
- en: 'The estimation of a regression equation is relatively straightforward, but
    this is merely the beginning. You should now think about what can be inferred
    from your result. In simple linear regression, there’s a natural question that
    should always be asked: Is there statistical evidence to support the presence
    of a relationship between the predictor and the response? To put it another way,
    is there evidence that a change in the explanatory variable affects the mean outcome?
    You investigate this following the same ideas that were introduced in [Chapter
    17](ch17.xhtml#ch17) when you began thinking about the variability present in
    estimated statistics and then continued to infer from your results using confidence
    intervals and, in [Chapter 18](ch18.xhtml#ch18), hypothesis testing.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 回归方程的估计相对简单，但这仅仅是开始。现在你应该考虑从结果中可以推断出什么。在简单线性回归中，始终应该提出一个自然的问题：是否有统计证据支持预测变量与响应变量之间存在关系？换句话说，是否有证据表明解释变量的变化会影响平均结果？你可以通过类似于在[第17章](ch17.xhtml#ch17)中引入的思想来研究这个问题，那里你开始思考估计统计量中的变异性，并随后使用置信区间进行推断，在[第18章](ch18.xhtml#ch18)中进行了假设检验。
- en: '***20.3.1 Summarizing the Fitted Model***'
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***20.3.1 总结拟合模型***'
- en: 'This kind of *model-based inference* is automatically carried out by R when
    `lm` objects are processed. Using the `summary` function on an object created
    by `lm` provides you with output far more detailed than simply printing the object
    to the console. For the moment, you’ll focus on just two aspects of the information
    presented in `summary`: the significance tests associated with the regression
    coefficients and the interpretation of the so-called *coefficient of determination*
    (labeled `R-squared` in the output), which I’ll explain shortly.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这种*基于模型的推断*在处理`lm`对象时会自动由R执行。对`lm`创建的对象使用`summary`函数，将为你提供比仅仅打印对象到控制台更详细的输出。此时，你将重点关注`summary`提供的两个方面的信息：与回归系数相关的显著性检验，以及所谓的*决定系数*（在输出中标记为`R-squared`）的解释，我将在稍后进行说明。
- en: 'Use `summary` on the current model object `survfit`, and you’ll see the following:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 对当前模型对象`survfit`使用`summary`，你将看到以下内容：
- en: '[PRE10]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '***20.3.2 Regression Coefficient Significance Tests***'
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***20.3.2 回归系数显著性检验***'
- en: Let’s begin by focusing on the way the estimated regression coefficients are
    reported. The first column of the coefficients table contains the point estimates
    of the intercept and slope (the intercept is labeled as such, and the slope is
    labeled after the name of the predictor variable in the data frame); the table
    also includes estimates of the standard errors of these statistics. It can be
    shown that simple linear regression coefficients, when estimated using least-squares,
    follow a *t*-distribution with *n* − 2 degrees of freedom (when given the number
    of observations, *n*, used in the model fit). The standardized *t* value and a
    *p*-value are reported for each parameter. These represent the results of a two-tailed
    hypothesis test formally defined as
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先关注回归系数报告的方式。系数表的第一列包含截距和斜率的点估计（截距标记为“截距”，斜率则标记为数据框中预测变量的名称）；表格还包括这些统计量的标准误差估计。可以证明，简单线性回归系数在使用最小二乘法估计时，遵循具有*n*
    − 2自由度的*t*分布（其中*n*为模型拟合中使用的观测值数量）。每个参数都报告了标准化的*t*值和*p*-值。这些值表示一个双尾假设检验的结果，定义为
- en: 'H[0] : *β[j]* = 0'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 'H[0] : *β[j]* = 0'
- en: 'H[A] : *β[j]* ≠ 0'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 'H[A] : *β[j]* ≠ 0'
- en: where *j* = 0 for the intercept and *j* = 1 for the slope, using the notation
    in [Equation (20.1)](ch20.xhtml#ch20eq1).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*j* = 0表示截距，*j* = 1表示斜率，使用[公式(20.1)](ch20.xhtml#ch20eq1)中的符号。
- en: 'Focus on the row of results for the predictor. With a null value of zero, truth
    of H[0] implies that the predictor has no effect on the response. The claim here
    is interested in whether there is *any effect* of the covariate, not the direction
    of this effect, so H[A] is two-sided (via ≠). As with any hypothesis test, the
    smaller the *p*-value, the stronger the evidence against H[0]. With a small *p*-value
    (< 2 × 10^−^(16)) attached to this particular test statistic (which you can confirm
    using the formula in [Chapter 18](ch18.xhtml#ch18): *T* = (3.116 − 0)/0.2888 =
    10.79), you’d therefore conclude there is strong evidence *against* the claim
    that the predictor has no effect on the mean level of the response.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 关注预测变量的结果行。零的假设值下，H[0]的成立意味着预测变量对响应没有影响。这里关注的是协变量是否有*任何影响*，而不是这种影响的方向，因此H[A]是双尾的（通过≠）。与任何假设检验一样，*p*-值越小，反对H[0]的证据就越强。在此特定检验统计量下，*p*-值非常小（<
    2 × 10^−^(16)）（你可以使用[第18章](ch18.xhtml#ch18)中的公式确认：*T* = (3.116 − 0)/0.2888 = 10.79），因此你可以得出结论，存在强有力的证据*反对*预测变量对响应均值没有影响的说法。
- en: The same test is carried out for the intercept, but the test for the slope parameter
    *β*[1] is typically more interesting (since rejection of the null hypothesis for
    *β*[0] simply indicates evidence that the regression line does not strike the
    vertical axis at zero), especially when the observed data don’t include *x* =
    0, as is the case here.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 对截距进行相同的检验，但对斜率参数*β*[1]的检验通常更为有趣（因为拒绝零假设的*β*[0]仅表示回归线与纵轴交点不为零），尤其是在观察数据不包含*x*
    = 0的情况下，正如这里的情况。
- en: From this, you can conclude that the fitted model suggests there is evidence
    that an increase in handspan is associated with an increase in height among the
    population being studied. For each additional centimeter of handspan, the average
    increase in height is approximately 3.12 cm.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 从中可以得出结论，拟合模型表明，手掌跨度的增加与所研究人群的身高增加之间存在关联。每增加一厘米的手掌跨度，身高的平均增加约为3.12厘米。
- en: You could also produce confidence intervals for your estimates using [Equation
    (17.2)](ch17.xhtml#ch17eq2) on [page 378](ch17.xhtml#page_378) and knowledge of
    the sampling distributions of the regression parameters; however, yet again, R
    provides a convenient function for an object of class `"lm"` to do this for you.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用[方程式(17.2)](ch17.xhtml#ch17eq2)和回归参数的抽样分布知识，计算估计值的置信区间；然而，R再次为`"lm"`类对象提供了一个方便的函数，可以为你自动完成此操作。
- en: '[PRE11]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: To the `confint` function you pass your model object as the first argument and
    your desired level of confidence as `level`. This indicates that you should be
    95 percent confident the true value of *β*[1] lies somewhere between 2.55 and
    3.69 (to 2 d.p.). As usual, the exclusion of the null value of zero reflects the
    statistically significant result from earlier.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`confint`函数，你需要将模型对象作为第一个参数，并将所需的置信水平作为`level`传递。这表明你应该有95%的信心，*β*[1]的真实值位于2.55和3.69之间（保留两位小数）。和之前一样，排除零的原假设值反映了早先的统计显著结果。
- en: '***20.3.3 Coefficient of Determination***'
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***20.3.3 决定系数***'
- en: The output of `summary` also provides you with the values of `Multiple R-squared`
    and `Adjusted R-squared`, which are particularly interesting. Both of these are
    referred to as the *coefficient of determination*; they describe the proportion
    of the variation in the response that can be attributed to the predictor.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`summary`的输出还为你提供了`Multiple R-squared`和`Adjusted R-squared`的值，这些值尤其有趣。这两者都被称为*决定系数*；它们描述了响应中变异的比例，这部分变异可以归因于预测变量。'
- en: 'For simple linear regression, the first (unadjusted) measure is simply obtained
    as the square of the estimated correlation coefficient (refer to [Section 13.2.5](ch13.xhtml#ch13lev2sec120)).
    For the student height example, first store the estimated correlation between
    `Wr.Hnd` and `Height` as `rho.xy`, and then square it:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 对于简单线性回归，第一项（未经调整的）度量是通过估计的相关系数的平方获得的（请参考[第13.2.5节](ch13.xhtml#ch13lev2sec120)）。对于学生身高的例子，首先将`Wr.Hnd`和`Height`之间的估计相关性存储为`rho.xy`，然后对其平方：
- en: '[PRE12]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: You get the same result as the `Multiple R-squared` value (usually written mathematically
    as *R*²). This tells you that about 36.1 percent of the variation in the student
    heights can be attributed to handspan.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 你将得到与`Multiple R-squared`值相同的结果（通常用数学符号表示为*R*²）。这告诉你，大约36.1%的学生身高变异可以归因于手跨度。
- en: The adjusted measure is an alternative estimate that takes into account the
    number of parameters that require estimation. The adjusted measure is generally
    important only if you’re using the coefficient of determination to assess the
    overall “quality” of the fitted model in terms of a balance between goodness of
    fit and complexity. I’ll cover this in [Chapter 22](ch22.xhtml#ch22), so I won’t
    go into any more detail just yet.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 调整后的度量是一种替代估计，考虑了需要估计的参数数量。调整后的度量通常只有在你使用决定系数来评估拟合模型的整体“质量”，即平衡拟合优度与复杂性时，才显得重要。我将在[第22章](ch22.xhtml#ch22)中讲解这一点，所以目前不会再深入探讨。
- en: '***20.3.4 Other summary Output***'
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***20.3.4 其他摘要输出***'
- en: The `summary` of the model object provides you with even more useful information.
    The “residual standard error” is the estimated standard error of the *∊* term
    (in other words, the square root of the estimated variance of *∊*, namely, *σ*²);
    below that it also reports any missing values. (The 29 observation pairs “deleted
    due to missingness” here matches the number of incomplete observations determined
    in [Section 20.1](ch20.xhtml#ch20lev1sec62).)
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 模型对象的`summary`会为你提供更有用的信息。“残差标准误”是*∊*项的估计标准误（换句话说，就是*∊*的估计方差的平方根，即*σ*²）；下面还报告了任何缺失值。（这里“因缺失而删除的29对观测值”与[第20.1节](ch20.xhtml#ch20lev1sec62)中确定的不完整观测值的数量一致。）
- en: The output also provides a five-number summary ([Section 13.2.3](ch13.xhtml#ch13lev2sec118))
    for the residual distances—I’ll cover this further in [Section 22.3](ch22.xhtml#ch22lev1sec74).
    As the final result, you’re provided with a certain hypothesis test performed
    using the *F*-distribution. This is a global test of the impact of your predictor(s)
    on the response; this will be explored alongside multiple linear regression in
    [Section 21.3.5](ch21.xhtml#ch21lev2sec197).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 输出还提供了残差距离的五数总结（[第13.2.3节](ch13.xhtml#ch13lev2sec118)），我将在[第22.3节](ch22.xhtml#ch22lev1sec74)中进一步讲解。最终结果中，你将得到使用*F*分布进行的某个假设检验。这是一个关于预测变量对响应变量影响的全局检验；这一点将在[第21.3.5节](ch21.xhtml#ch21lev2sec197)中与多元线性回归一起探讨。
- en: You can access all the output provided by `summary` directly, as individual
    R objects, rather than having to read them off the screen from the entire printed
    summary. Just as `names(survfit)` provides you with an indication of the contents
    of the stand-alone `survfit` object, the following code gives you the names of
    all the components accessible *after* `summary` is used to process `survfit`.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'It’s fairly easy to match most of the components with the printed `summary`
    output, and they can be extracted using the dollar operator as usual. The residual
    standard error, for example, can be retrieved directly with this:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: There are further details on this in the `?summary.lm` help file.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '**20.4 Prediction**'
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To wrap up these preliminary details of linear regression, you’ll now look at
    using your fitted model for predictive purposes. The ability to fit a statistical
    model means that you not only can understand and quantify the nature of relationships
    in your data (like the estimated 3.1166 cm increase in mean height per 1 cm increase
    in handspan for the student example) but can also *predict* values of the outcome
    of interest, even where you haven’t actually observed the values of any explanatory
    variables in the original data set. As with any statistic, though, there is always
    a need to accompany any point estimates or predictions with a measure of spread.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '***20.4.1 Confidence Interval or Prediction Interval?***'
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: With a fitted simple linear model you’re able to calculate a point estimate
    of the *mean response* value, conditional upon the value of an explanatory variable.
    To do this, you simply plug in (to the fitted model equation) the value of *x*
    you’re interested in. A statistic like this is always subject to variation, so
    just as with sample statistics explored in earlier chapters, you use a *confidence
    interval for the mean response (CI)* to gauge this uncertainty.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Assume a simple linear regression line has been fitted to *n* observations such
    that ![image](../images/f0462-01.jpg) percent confidence interval for the mean
    response given a value of *x* is calculated with
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e20-5.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
- en: where you obtain the lower limit by subtraction, the upper limit by addition.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Here, ŷ is the fitted value (from the regression line) at *x*; *t*[(][1][−]
    [/][2][,][n][−][2][)] is the appropriate critical value from a *t*-distribution
    with *n* − 2 degrees of freedom (in other words, resulting in an upper-tail area
    of exactly /2); *s[ɛ]* is the estimated residual standard error; and *x̄* and
    ![image](../images/common-05.jpg) represent the sample mean and the variance of
    the observations of the predictor, respectively.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 'A *prediction interval (PI)* for an observed response is different from the
    confidence interval in terms of context. Where CIs are used to describe the variability
    of the *mean* response, a PI is used to provide the possible range of values that
    an *individual realization* of the response variable might take, given *x*. This
    distinction is subtle but important: the CI corresponds to a mean, and the PI
    corresponds to an individual observation.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s remain with the previous notation. It can be shown that 100(1 − *α*)
    percent prediction interval for an individual response given a value of *x* is
    calculated with the following:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e20-6.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
- en: It turns out that the only difference from (20.5) is the 1+ that appears in
    the square root. As such, a PI at *x* is wider than a CI at *x*.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '***20.4.2 Interpreting Intervals***'
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Continuing with our example, let’s say you want to determine the mean height
    for students with a handspan of 14.5 cm and for students with a handspan of 24
    cm. The point estimates themselves are easy—just plug the desired *x* values into
    the regression [equation (20.4)](ch20.xhtml#ch20eq4).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: According to the model, you can expect mean heights to be around 159.14 and
    188.75 cm for handspans of 14.5 and 24 cm, respectively. The `as.numeric` coercion
    function (first encountered in [Section 6.2.4](ch06.xhtml#ch06lev2sec62)) is used
    simply to strip the result of the annotative names that are otherwise present
    from the `beta0.hat` and `beta1.hat` objects.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '**Confidence Intervals for Mean Heights**'
  id: totrans-123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'To find confidence intervals for these estimates, you could calculate them
    manually using (20.5), but of course R has a built-in `predict` command to do
    it for you. To use `predict`, you first need to store your *x* values in a particular
    way: as a column in a new data frame. The name of the column must match the predictor
    used in the original call to create the fitted model object. In this example,
    I’ll create a new data frame, `xvals`, with the column named `Wr.Hnd`, which contains
    only two values of interest—the handspans of 14.5 and 24 cm.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now, when `predict` is called, the first argument must be the fitted model object
    of interest, `survfit` for this example. Next, in the argument `newdata`, you
    pass the specially constructed data frame containing the specified predictor values.
    To the `interval` argument you must specify `"confidence"` as a character string
    value. The confidence level, here set for 95 percent, is passed (on the scale
    of a probability) to `level`.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This call will return a matrix with three columns, whose number (and order)
    of rows correspond to the predictor values you supplied in the `newdata` data
    frame. The first column, with a heading of `fit`, is the point estimate on the
    regression line; you can see that these numbers match the values you worked out
    earlier. The other columns provide the lower and upper CI limits as the `lwr`
    and `upr` columns, respectively. In this case, you’d interpret this as 95 percent
    confidence that the mean height of a student with a handspan of 14.5 cm lies somewhere
    between 156.5 cm and 161.8 cm and lies between 185.6 cm and 191.9 cm for a handspan
    of 24 cm (when rounded to 1 d.p.). Remember, these CIs, calculated as per (20.5)
    through `predict`, are for the *mean* response value.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '**Prediction Intervals for Individual Observations**'
  id: totrans-129
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The `predict` function will also provide your prediction intervals. To find
    the prediction interval for possible individual observations with a certain probability,
    you simply need to change the `interval` argument to `"prediction"`.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Notice that the fitted values remain the same, as Equations (20.5) and (20.6)
    indicate. The widths of the PIs, however, are significantly larger than those
    of the corresponding CIs—this is because raw observations themselves, at a specific
    *x* value, will naturally be more variable than their mean.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Interpretation changes accordingly. The intervals describe where raw student
    heights are predicted to lie “95 percent of the time.” For a handspan of 14.5
    cm, the model predicts individual observations to lie somewhere between 143.3
    cm and 175.0 cm with a probability of 0.95; for a handspan of 24 cm, the same
    PI is estimated at 172.8 cm and 204.7 cm (when rounded to 1 d.p.).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '***20.4.3 Plotting Intervals***'
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Both CIs and PIs are well suited to visualization for simple linear regression
    models. With the following code, you can start off [Figure 20-3](ch20.xhtml#ch20fig3)
    by plotting the data and estimated regression line just as for [Figure 20-2](ch20.xhtml#ch20fig2),
    but this time using `xlim` and `ylim` in `plot` to widen the *x*- and *y*-limits
    a little in order to accommodate the full length and breadth of the CI and PI.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: To this you add the locations of the fitted values for *x* = 14.5 and *x* =
    24, as well as two sets of vertical lines showing the CIs and PIs.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The call to `points` marks the fitted values for these two particular values
    of *x*. The first call to `segments` lays down the PIs as thickened vertical gray
    lines, and the second lays down the CIs as the shorter vertical black lines. The
    coordinates for these plotted line segments are taken directly from the `mypred.pi`
    and `mypred.ci` objects, respectively.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: You can also produce “bands” around the fitted regression line that mark one
    or both of these intervals over *all* values of the predictor. From a programming
    standpoint, this isn’t technically possible for a continuous variable, but you
    can achieve it practically by defining a fine sequence of values along the *x*-axis
    (using `seq` with a high `length` value) and evaluating the CI and PI at every
    point in this fine sequence. Then you just join resulting points as lines when
    plotting.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f20-03.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
- en: '*Figure 20-3: The student height regression example, with a fitted regression
    line and point estimates at* x *= 14.5 and* x *= 24 and with corresponding 95
    percent CIs (black vertical lines) and PIs (gray vertical lines). The dashed black
    and dashed gray lines provide 95 percent confidence and prediction bands for the
    response variable over the visible range of* x *values.*'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 'In R, this requires you to rerun the `predict` command as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The first line in this code creates the fine sequence of predictor values and
    stores it in the format required by the `newdata` argument. The *y*-axis coordinates
    for CI and PI bands are stored as the second and third columns of the matrix objects
    `ci.band` and `pi.band`. Finally, `lines` is used to add each of the four dashed
    lines corresponding to the upper and lower limits of the two intervals, and a
    legend adds a final touch.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Note that the black dashed CI bands meet the vertical black lines and the gray
    dashed PI bands meet the vertical gray lines for the two individual *x* values
    from earlier, just as you’d expect.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 20-3](ch20.xhtml#ch20fig3) shows the end result of all these additions
    to the plot. The “bowing inwards” curvature of the intervals is characteristic
    of this kind of plot and is especially visible in the CI. This curve occurs because
    there is naturally less variation if you’re predicting where there are more data.
    For more information on `predict` for linear model objects, take a look at the
    `?predict.lm` help file.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '***20.4.4 Interpolation vs. Extrapolation***'
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Before finishing this introduction to prediction, it’s important to clarify
    the definitions of two key terms: *interpolation* and *extrapolation*. These terms
    describe the nature of a given prediction. A prediction is referred to as interpolation
    if the *x* value you specify falls within the range of your observed data; extrapolation
    is when the *x* value of interest lies outside this range. From the point-predictions
    you just made, you can see that the location *x* = 14.5 is an example of interpolation,
    and *x* = 24 is an example of extrapolation.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, interpolation is preferable to extrapolation—it makes more sense
    to use a fitted model for prediction in the vicinity of data that have already
    been observed. Extrapolation that isn’t too far out of that vicinity may still
    be considered reliable, though. The extrapolation for the student height example
    at *x* = 24 is a case in point. This is outside the range of the observed data,
    but not by much in terms of scale, and the estimated intervals for the expected
    value of ŷ = 188.75 cm appear, at least visually, not unreasonable given the distribution
    of the other observations. In contrast, it would make less sense to use the fitted
    model to predict student height at a handspan of, say, 50 cm:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Such an extreme extrapolation suggests that the mean height of an individual
    with a handspan of 50 cm is almost 270 cm, both being fairly unrealistic measurements.
    The same is true in the other direction; the intercept ![image](../images/b0.jpg)
    doesn’t have a particularly useful practical interpretation, indicating that the
    mean height of a student with a handspan of 0 cm is around 114 cm.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: The main message here is to use common sense when making any prediction from
    a linear model fit. In terms of the reliability of the results, predictions made
    at values within an appropriate proximity of the observed data are preferable.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 20.1**'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Continue to use the `survey` data frame from the package `MASS` for the next
    few exercises.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Using your fitted model of student height on writing handspan, `survfit`, provide
    point estimates and 99 percent confidence intervals for the mean student height
    for handspans of 12, 15.2, 17, and 19.9 cm.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In [Section 20.1](ch20.xhtml#ch20lev1sec62), you defined the object `incomplete.obs`,
    a numeric vector that provides the records of `survey` that were automatically
    removed from consideration when estimating the model parameters. Now, use the
    `incomplete.obs` vector along with `survey` and [Equation (20.3)](ch20.xhtml#ch20eq3)
    to calculate ![image](../images/b0.jpg) and ![image](../images/b1.jpg) in R. (Remember
    the functions `mean`, `sd`, and `cor`. Ensure your answers match the output from
    `survfit`.)
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `survey` data frame has a number of other variables present aside from `Height`
    and `Wr.Hnd`. For this exercise, the end aim is to fit a simple linear model to
    predict the mean student height, but this time from their pulse rate, given in
    `Pulse` (continue to assume the conditions listed in [Section 20.2](ch20.xhtml#ch20lev1sec63)
    are satisfied).
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the regression model and produce a scatterplot with the fitted line superimposed
    upon the data. Make sure you can write down the fitted model equation and keep
    the plot open.
  id: totrans-160
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Identify and interpret the point estimate of the slope, as well as the outcome
    of the test associated with the hypotheses H[0] : *β*[1] = 0; H[A] : *β*[1] ≠
    0\. Also find a 90 percent CI for the slope parameter.'
  id: totrans-161
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Using your model, add lines for 90 percent confidence and prediction interval
    bands on the plot from (i) and add a legend to differentiate between the lines.
  id: totrans-162
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Create an `incomplete.obs` vector for the current “height on pulse” data. Use
    that vector to calculate the sample mean of the height observations that were
    used for the model fitted in (i). Then add a perfectly horizontal line to the
    plot at this mean (use color or line type options to avoid confusion with the
    other lines present). What do you notice? Does the plot support your conclusions
    from (ii)?
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, examine the help file for the `mtcars` data set, which you first saw in
    [Exercise 13.4](ch13.xhtml#ch13exc4) on [page 287](ch13.xhtml#page_287). For this
    exercise, the goal is to model fuel efficiency, measured in miles per gallon (MPG),
    in terms of the overall weight of the vehicle (in thousands of pounds).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Plot the data—`mpg` on the *y*-axis and `wt` on the *x*-axis.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the simple linear regression model. Add the fitted line to the plot from
    (d).
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write down the regression equation and interpret the point estimate of the slope.
    Is the effect of `wt` on mean `mpg` estimated to be statistically significant?
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Produce a point estimate and associated 95 percent PI for a car that weighs
    6,000 lbs. Do you trust the model to predict observations accurately for this
    value of the explanatory variable? Why or why not?
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**20.5 Understanding Categorical Predictors**'
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'So far, you’ve looked at simple linear regression models that rely on continuous
    explanatory variables, but it’s also possible to use a discrete or categorical
    explanatory variable, made up of *k* distinct groups or levels, to model the mean
    response. You must be able to make the same assumptions noted in [Section 20.2](ch20.xhtml#ch20lev1sec63):
    that observations are all independent of one another and residuals are normally
    distributed with an equal variance. To begin with, you’ll look at the simplest
    case in which *k* = 2 (a binary-valued predictor), which forms the basis of the
    slightly more complicated situation in which the categorical predictor has more
    than two levels (a multilevel predictor: *k* > 2).'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '***20.5.1 Binary Variables: k = 2***'
  id: totrans-171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Turn your attention back to [Equation (20.1)](ch20.xhtml#ch20eq1), where the
    regression model is specified as *Y*|*X* = *β*[0] + *β*[1]*X* + *∊* for a response
    variable *Y* and predictor *X*, and *∊* ~ N(0,*σ*²). Now, suppose your predictor
    variable is categorical, with only two possible levels (binary; *k* = 2) and observations
    coded either 0 or 1\. For this case, (20.1) still holds, but the interpretation
    of the model parameters, *β*[0] and *β*[1], isn’t really one of an “intercept”
    and a “slope” anymore. Instead, it’s better to think of them as being something
    like two intercepts, where *β*[0] provides the *baseline* or *reference* value
    of the response when *X* = 0 and *β*[1] represents the *additive effect* on the
    mean response if *X* = 1\. In other words, if *X* = 0, then *Y* = *β*[0] + *∊*;
    if *X* = 1, then *Y* = *β*[0] + *β*[1] + *∊*. As usual, estimation is in terms
    of finding the *mean* response ŷ ![image](../images/e.jpg)[*Y* |*X* = *x*] as
    per [Equation (20.2)](ch20.xhtml#ch20eq2), so the equation becomes ![image](../images/f0468-01.jpg).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'Go back to the `survey` data frame and note that you have a `Sex` variable,
    where the students recorded their gender. Look at the documentation on the help
    page `?survey` or enter something like this:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: You’ll see that the sex data column is a factor vector with two levels, `Female`
    and `Male`, and that there happens to be an equal number of the two (one of the
    237 records has a missing value for this variable).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: You’re going to determine whether there is statistical evidence that the height
    of a student is affected by sex. This means that you’re again interested in modeling
    height as the response variable, but this time, it’s with the categorical sex
    variable as the predictor.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: To visualize the data, if you make a call to `plot` as follows, you’ll get a
    pair of boxplots.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This is because the response variable specified to the left of the `~` is numeric
    and the explanatory variable to the right is a factor, and the default behavior
    of R in that situation is to produce side-by-side boxplots.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: To further emphasize the categorical nature of the explanatory variable, you
    can superimpose the raw height and sex observations on top of the boxplots. To
    do this, just convert the factor vector to numeric with a call to `as.numeric`;
    this can be done directly in a call to `points`.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Remember that boxplots mark off the median as the central bold line but that
    least-squares linear regressions are defined by the mean outcome, so it’s useful
    to also display the mean heights according to sex.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: You were introduced to `tapply` in [Section 10.2.3](ch10.xhtml#ch10lev2sec94);
    in this call, the argument `na.rm=TRUE` is matched to the ellipsis in the definition
    of `tapply` and is passed to `mean` (you need it to ensure the missing values
    present in the data do not end up producing `NA`s as the results). A further call
    to `points` adds those coordinates (as × symbols) to the image; [Figure 20-4](ch20.xhtml#ch20fig4)
    gives the final result.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f20-04.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
- en: '*Figure 20-4: Boxplots of the student heights split by sex, with the raw observations
    and sample means (small ○ and large × symbols, respectively) superimposed*'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: The plot indicates, overall, that males tend to be taller than females—but is
    there statistical evidence of a difference to back this up?
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '**Linear Regression Model of Binary Variables**'
  id: totrans-188
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To answer this with a simple linear regression model, you can use `lm` to produce
    least-squares estimates just like with every other model you’ve fitted so far.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: However, because the predictor is a factor vector instead of a numeric vector,
    the reporting of the coefficients is slightly different. The estimate of [0] is
    again reported as `(Intercept)`; this is the estimate of the mean height if a
    student is female. The estimate of *β*[1] is reported as `SexMale`. The corresponding
    regression coefficient of 13.139 is the estimated difference that is imparted
    upon the mean height of a student if male. If you look at the corresponding regression
    equation
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e20-7.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
- en: 'you can see that the model has been fitted assuming the variable *x* is defined
    as “the individual is male”—0 for no/false, 1 for yes/true. In other words, the
    level of “female” for the sex variable is assumed as a reference, and it is the
    effect of “being male” on mean height that is explicitly estimated. The hypothesis
    test for *β*[0] and *β*[1] is performed with the same hypotheses defined in [Section
    20.3.2](ch20.xhtml#ch20lev2sec178):'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 'H[0] : *β[j]* = 0'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'H[A] : *β[j]* ≠ 0'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Again, it’s the test for *β*[1] that’s generally of the most interest since
    it’s this value that tells you whether there is statistical evidence that the
    mean response variable is affected by the explanatory variable, that is, if *β*[1]
    is significantly different from zero.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '**Predictions from a Binary Categorical Variable**'
  id: totrans-197
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Because there are only two possible values for *x*, prediction is straightforward
    here. When you evaluate the equation, the only decision that needs to be made
    is whether ![image](../images/b1.jpg) needs to be used (in other words, if an
    individual is male) or not (if an individual is female). For example, you can
    enter the following code to create a factor of five extra observations with the
    same level names as the original data and store the new data in `extra.obs`:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Then, use `predict` in the now-familiar fashion to find the mean heights at
    those extra values of the predictor. (Remember that when you pass in new data
    to `predict` using the `newdata` argument, the predictors must be in the same
    form as the data that were used to fit the model in the first place.)
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: You can see from the output that the predictions are different only between
    the two sets of values—the point estimates of the two instances of `Female` are
    identical, simply ![image](../images/b0.jpg) with 90 percent CIs. The point estimates
    and CIs for the instances of `Male` are also all the same as each other, based
    on a point estimate of ![image](../images/f0472-01.jpg).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: On its own, admittedly, this example isn’t too exciting. However, it’s critical
    to understand how R presents regression results when using categorical predictors,
    especially when considering multiple regression in [Chapter 21](ch21.xhtml#ch21).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '***20.5.2 Multilevel Variables: k > 2***'
  id: totrans-204
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It’s common to work with data where the categorical predictor variables have
    more than two levels so that (*k* > 2). These can also be referred to as *multilevel*
    categorical variables. To deal with this more complicated situation while retaining
    interpretability of your parameters, you must first dummy code your predictor
    into *k* − 1 binary variables.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '**Dummy Coding Multilevel Variables**'
  id: totrans-206
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To see how this is done, assume that you want to find the value of response
    variable *Y* when given the value of a categorical variable *X*, where *X* has
    *k* > 2 levels (also assume the conditions for validity of the linear regression
    model—[Section 20.2](ch20.xhtml#ch20lev1sec63)—are satisfied).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: In regression modeling, *dummy coding* is the procedure used to create several
    binary variables from a categorical variable like *X*. Instead of the single categorical
    variable with possible realizations
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '*X* = 1,2,3, . . . , *k*'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'you recode it into several yes/no variables—one for each level—with possible
    realizations:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '*X*[(][1][)] = 0,1; *X*[(][2][)] = 0,1; *X*[(][3][)] = 0,1; . . . ; *X*[(][k][)]
    = 0,1'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, *X*[(][i][)] represents a binary variable for the *i*th level
    of the original *X*. For example, if an individual has *X* = 2 in the original
    categorical variable, then *X*[(][2][)] = 1 (yes) and all of the others (*X*[(][1][)],
    *X*[(][3][)], . . . , *X*[(][k][)]) will be zero (no).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose *X* is a variable that can take any one of the *k* = 4 values 1, 2,
    3, or 4, and you’ve made six observations of this variable: 1, 2, 2, 1, 4, 3\.
    [Table 20-1](ch20.xhtml#ch20tab1) shows these observations and their dummy-coded
    equivalents *X*[(][1][)], *X*[(][2][)], *X*[(][3][)], and *X*[(][4][)].'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 20-1:** Illustrative Example of Dummy Coding for Six Observations of
    a Categorical Variable with *k* = 4 Groups'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '| *X* | *X*[(][1][)] | *X*[(][2][)] | *X*[(][3][)] | *X*[(][4][)] |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 0 | 0 | 0 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0 | 1 | 0 | 0 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0 | 1 | 0 | 0 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 0 | 0 | 0 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
- en: '| 4 | 0 | 0 | 0 | 1 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0 | 0 | 1 | 0 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
- en: In fitting the subsequent model, you usually only use *k* − 1 of the dummy binary
    variables—one of the variables acts as a *reference* or *baseline* level, and
    it’s incorporated into the overall intercept of the model. In practice, you would
    end up with an estimated model like this,
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e20-8.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
- en: assuming 1 is the reference level. As you can see, in addition to the overall
    intercept term ![image](../images/b0.jpg), you have *k* −1 other estimated intercept
    terms that modify the baseline coefficient ![image](../images/b0.jpg), depending
    on which of the original categories an observation takes on. For example, in light
    of the coding imposed in (20.8), if an observation has *X*[(][3][)] = 1 and all
    other binary values are therefore zero (so that observation would’ve had a value
    of *X* = 3 for the original categorical variable), the predicted mean value of
    the response would be ![image](../images/f0473-01.jpg). On the other hand, because
    the reference level is defined as 1, if an observation has values of zero for
    all the binary variables, it implies the observation originally had *X* = 1, and
    the prediction would be simply ![image](../images/f0473-02.jpg).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: The reason it’s necessary to dummy code for categorical variables of this nature
    is that, in general, categories cannot be related to each other in the same numeric
    sense as continuous variables. It’s often not appropriate, for example, to think
    that an observation in category 4 is “twice as much” as one in category 2, which
    is what the estimation methods would assume. Binary presence/absence variables
    are valid, however, and can be easily incorporated into the modeling framework.
    Choosing the reference level is generally of secondary importance—the specific
    values of the estimated coefficients will change accordingly, but any overall
    interpretations you make based on the fitted model will be the same regardless.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '*Implementing this dummy-coding approach is technically a form of multiple
    regression since you’re now including several binary variables in the model. It’s
    important, however, to be aware of the somewhat artificial nature of dummy coding—you
    should still think of the multiple coefficients as representing a single categorical
    variable since the binary variables X*[(][1][)]*, . . . , X*[(][k][)] *are not
    independent of one another. This is why I’ve chosen to define these models in
    this chapter; multiple regression will be formally discussed in [Chapter 21](ch21.xhtml#ch21).*'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '**Linear Regression Model of Multilevel Variables**'
  id: totrans-229
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: R makes working with categorical predictors in this way quite simple since it
    automatically dummy codes for any such explanatory variable when you call `lm`.
    There are two things you should check before fitting your model, though.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: The categorical variable of interest should be stored as a (formally unordered)
    factor.
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You should check that you’re happy with the category assigned as the reference
    level (for interpretative purposes—see [Section 20.5.3](ch20.xhtml#ch20lev2sec187)).
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You must also of course be happy with the validity of the familiar assumptions
    of normality and independence of *∊*.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate all these definitions and ideas, let’s return to the student
    survey data from the `MASS` package and keep “student height” as the response
    variable of interest. Among the data is the variable `Smoke`. This variable describes
    the kind of smoker each student reports themselves as, defined by frequency and
    split into four categories: “heavy,” “never,” “occasional,” and “regular.”'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Here, the result from `is.factor(survey$Smoke)` indicates that you do indeed
    have a factor vector at hand, the call to `table` yields the number of students
    in each of the four categories, and as per [Chapter 5](ch05.xhtml#ch05), you can
    explicitly request the levels attribute of any R factor via `levels`.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Let’s ask whether there’s statistical evidence to support a difference in mean
    student height according to smoking frequency. You can create a set of boxplots
    of these data with the following two lines; [Figure 20-5](ch20.xhtml#ch20fig5)
    shows the result.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '![image](../images/f20-05.jpg)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
- en: '*Figure 20-5: Boxplots of the observed student heights split by smoking frequency;
    respective sample means marked with* ×'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: 'Note from earlier R output that unless explicitly defined at creation, the
    levels of a factor appear in alphabetical order by default—as is the case for
    `Smoke`—and R will automatically set the first one (as shown in the output of
    a call to `levels`) as the reference level when that factor is used as a predictor
    in subsequent model fitting. Fitting the linear model in mind using `lm`, you
    can see from a subsequent call to `summary` that indeed the first level of `Smoke`,
    for “heavy”, has been used as the reference:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: As outlined in [Equation (20.8)](ch20.xhtml#ch20eq8), you get estimates of coefficients
    corresponding to the dummy binary variables for three of the four possible categories
    in this example—the three nonreference levels. The observation in the reference
    category `Heavy` is represented solely by ![image](../images/b0.jpg), designated
    first as the overall `(Intercept)`, with the other coefficients providing the
    effects associated with an observation in one of the other categories.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '**Predictions from a Multilevel Categorical Variable**'
  id: totrans-244
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: You find point estimates through prediction, as usual.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Here, I’ve created the object `one.of.each` for illustrative purposes; it represents
    one observation in each of the four categories, stored as an object matching the
    class (and levels) of the original `Smoke` data. A student in the `Occas` category,
    for example, is predicted to have a mean height of 173.772 − 0.7433 = 173.0287.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: The output from the model summary earlier, however, shows that none of the binary
    dummy variable coefficients are considered statistically significant from zero
    (because all the *p*-values are too large). The results indicate, as you might
    have suspected, that there’s no evidence that smoking frequency (or more specifically,
    having a smoking frequency that’s different from the reference level) affects
    mean student heights based on this sample of individuals. As is common, the baseline
    coefficient ![image](../images/b0.jpg) is highly statistically significant—but
    that only suggests that the overall intercept probably isn’t zero. (Because your
    response variable is a measurement of height and will clearly not be centered
    anywhere near 0 cm, that result makes sense.) The confidence intervals supplied
    are calculated in the usual *t*-based fashion.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: The small `R-Squared` value reinforces this conclusion, indicating that barely
    any of the variation in the response can be explained by changing the category
    of smoking frequency. Furthermore, the overall *F* -test *p*-value is rather large
    at around 0.215, suggesting an overall nonsignificant effect of the predictor
    on the response; you’ll look at this in more detail in a moment in [Section 20.5.5](ch20.xhtml#ch20lev2sec189)
    and later on in [Section 21.3.5](ch21.xhtml#ch21lev2sec197).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: As noted earlier, it’s important that you interpret these results—indeed any
    based on a *k*-level categorical variable in regression—in a *collective* fashion.
    You can claim only that there is *no* discernible effect of smoking on height
    because *all* the *p*-values for the binary dummy coefficients are nonsignificant.
    If one of the levels was in fact highly significant (through a small *p*-value),
    it would imply that the smoking factor as defined here, as a whole, *does* have
    a statistically detectable effect on the response (even if the other two levels
    were still associated with very high *p*-values). This will be discussed further
    in several more examples in [Chapter 21](ch21.xhtml#ch21).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '***20.5.3 Changing the Reference Level***'
  id: totrans-251
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Sometimes you might decide to change the automatically selected reference level,
    compared to which the effects of taking on any of the other levels are estimated.
    Changing the baseline will result in the estimation of different coefficients,
    meaning that individual *p*-values are subject to change, but the overall result
    (in terms of global significance of the factor) will not be affected. Because
    of this, altering the reference level is only done for interpretative purposes—sometimes
    there’s an intuitively natural baseline of the predictor (for example, “Placebo”
    versus “Drug A” and “Drug B” as a treatment variable in the analysis of some clinical
    trial) from which you want to estimate deviation in the mean response with respect
    to the other possible categories.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Redefining the reference level can be achieved quickly using the built-in `relevel`
    function in R. This function allows you to choose which level comes first in the
    definition of a given factor vector object and will therefore be designated as
    the reference level in subsequent model fitting. In the current example, let’s
    say you’d rather have the nonsmokers as the reference level.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The `relevel` function has moved the `Never` category into the first position
    in the new factor vector. If you go ahead fit the model again using `SmokeReordered`
    instead of the original `Smoke` column of `survey`, it’ll provide estimates of
    coefficients associated with the three different levels of smokers.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth noting the differences in the treatment of unordered versus ordered
    factor vectors in regression applications. It might seem sensible to formally
    order the smoking variable by, for example, increasing the frequency of smoking
    when creating a new factor vector. However, when an ordered factor vector is supplied
    in a call to `lm`, R reacts in a different way—it doesn’t perform the relatively
    simple dummy coding discussed here, where an effect is associated with each optional
    level to the baseline (technically referred to as *orthogonal contrasts*). Instead,
    the default behavior is to fit the model based on something called *polynomial
    contrasts*, where the effect of the ordered categorical variable on the response
    is defined in a more complicated functional form. That discussion is beyond the
    scope of this text, but it suffices to say that this approach can be beneficial
    when your interest lies in the specific functional nature of “moving up” through
    an ordered set of categories. For more on the technical details, see Kuhn and
    Johnson ([2013](ref.xhtml#ref37)). For all relevant regression examples in this
    book, we’ll work exclusively with unordered factor vectors.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '***20.5.4 Treating Categorical Variables as Numeric***'
  id: totrans-257
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The way in which `lm` decides to define the parameters of the fitted model depends
    primarily on the kind of data you pass to the function. As discussed, `lm` imposes
    dummy coding only if the explanatory variable is an unordered factor vector.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes the categorical data you want to analyze haven’t been stored as a
    factor in your data object. If the categorical variable is a character vector,
    `lm` will implicitly coerce it into a factor. If, however, the intended categorical
    variable is numeric, then `lm` performs linear regression exactly as if it were
    a continuous numeric predictor; it estimates a single regression coefficient,
    which is interpreted as a “per-one-unit-change” in the mean response.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: This may seem inappropriate if the original explanatory variable is supposed
    to be made up of distinct groups. In some settings, however, especially when the
    variable can be naturally treated as numeric-discrete, this treatment is not only
    valid statistically but also helps with interpretation.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a break from the `survey` data and go back to the ready-to-use `mtcars`
    data set. Say you’re interested in the variables mileage, `mpg` (continuous),
    and number of cylinders, `cyl` (discrete; the data set contains cars with either
    4, 6, or 8 cylinders). Now, it’s perfectly sensible to automatically think of
    `cyl` as a categorical variable. Taking `mpg` to be the response variable, box-plots
    are well suited to reflect the grouped nature of `cyl` as a predictor; the result
    of the following line is given on the left of [Figure 20-6](ch20.xhtml#ch20fig6):'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: When fitting the associated regression model, you must be aware of what you’re
    instructing R to do. Since the `cyl` column of `mtcars` is numeric, and not a
    factor vector per se, `lm` will treat it as continuous if you just directly access
    the data frame.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Just as in earlier sections, you’ve received an intercept and a slope estimate;
    the latter is highly statistically significant, indicating that there is evidence
    against the true value of the slope being zero. Your fitted regression line is
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f0479-01.jpg)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
- en: where ŷ is the average mileage and *x* is numeric—the number of cylinders. For
    each single additional cylinder, the model says your mileage will decrease by
    2.88 MPG, on average.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s important to recognize the fact that you’ve fitted a continuous line to
    what is effectively categorical data. The right panel of [Figure 20-6](ch20.xhtml#ch20fig6),
    created with the following lines, highlights this fact:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '![image](../images/f20-06.jpg)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
- en: '*Figure 20-6: Left: Boxplots of mileage split by cylinders for the* `mtcars`
    *data set. Right: Scatterplot of the same data with fitted regression line (treating*
    `cyl` *as numeric-continuous) superimposed.*'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Some researchers fit categorical or discrete predictors as continuous variables
    purposefully. First, it allows interpolation; for example, you could use this
    model to evaluate the average MPG for a 5-cylinder car. Second, it means there
    are fewer parameters that require estimation; in other words, instead of *k* −
    1 intercepts for a categorical variable with *k* groups, you need only one parameter
    for the slope. Finally, it can be a convenient way to control for so-called nuisance
    variables; this will become clearer in [Chapter 21](ch21.xhtml#ch21). On the other
    hand, it means that you no longer get group-specific information. It can be misleading
    to proceed in this way if any differences in the mean response according to the
    predictor category of an observation are not well represented linearly—detection
    of significant effects can be lost altogether.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: At the very least, it’s important to recognize this distinction when fitting
    models. If you had only just now recognized that R had fitted the `cyl` variable
    as continuous and wanted to actually fit the model with `cyl` as categorical,
    you’d have to explicitly convert it into a factor vector beforehand or in the
    actual call to `lm`.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Here, by wrapping `cyl` in a call to `factor` when specifying the formula for
    `lm`, you can see you’ve obtained regression coefficient estimates for the levels
    of `cyl` corresponding to 6- and 8-cylinder cars (with the reference level automatically
    set to 4-cylinder cars).
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '***20.5.5 Equivalence with One-Way ANOVA***'
  id: totrans-276
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There’s one final observation to make about regression models with a single
    nominal categorical predictor. Think about the fact that these models describe
    a mean response value for the *k* different groups. Does this remind you of anything?
    In this particular setting, you’re actually doing the same thing as in one-way
    ANOVA ([Section 19.1](ch19.xhtml#ch19lev1sec59)): comparing more than two means
    and determining whether there is statistical evidence that at least one mean is
    different from the others. You need to be able to make the same key assumptions
    of independence and normality for both techniques.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: In fact, simple linear regression with a single categorical predictor, implemented
    using least-squares estimation, is just another way to perform one-way ANOVA.
    Or, perhaps more concisely, ANOVA is a special case of least-squares regression.
    The outcome of a one-way ANOVA test is a single *p*-value quantifying a level
    of statistical evidence against the null hypothesis that states that group means
    are equal. When you have one categorical predictor in a regression, it’s exactly
    that *p*-value that’s reported at the end of the `summary` of an `lm` object—something
    I’ve referred to a couple of times now as the “overall” or “global” significance
    test (for example, in [Section 20.3.3](ch20.xhtml#ch20lev2sec179)).
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 'Look back to the final result of that global significance test for the student
    height modeled by smoking status example—you had a *p*-value of 0.2147\. This
    came from an *F* test statistic of 1.504 with df[1] = 3 and df[2] = 205\. Now,
    suppose you were just handed the data and asked to perform a one-way ANOVA of
    height on smoking. Using the `aov` function as introduced in [Section 19.1](ch19.xhtml#ch19lev1sec59),
    you’d call something like this:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Those same values are returned here; you can also find the square root of the
    MSE:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: This is in fact the “residual standard error” given in the `lm` summary. The
    two conclusions you’d draw about the impact of smoking status on height (one for
    the `lm` output, the other for the ANOVA test) are of course also the same.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: The global test that `lm` provides isn’t just there for the benefit of confirming
    ANOVA results. As a generalization of ANOVA, least-squares regression models provide
    more than just coefficient-specific tests. That global test is formally referred
    to as the *omnibus* F-*test*, and while it is indeed equivalent to one-way ANOVA
    in the “single categorical predictor” setting, it’s also a useful overall, stand-alone
    test of the statistical contribution of several predictors to the outcome value.
    You’ll explore this further in [Section 21.3.5](ch21.xhtml#ch21lev2sec197) after
    you’ve begun modeling your response variable using multiple explanatory variables.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 20.2**'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: Continue using the `survey` data frame from the package `MASS` for the next
    few exercises.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: 'The `survey` data set has a variable named `Exer`, a factor with *k* = 3 levels
    describing the amount of physical exercise time each student gets: none, some,
    or frequent. Obtain a count of the number of students in each category and produce
    side-by-side boxplots of student height split by exercise.'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assuming independence of the observations and normality as usual, fit a linear
    regression model with height as the response variable and exercise as the explanatory
    variable (dummy coding). What’s the default reference level of the predictor?
    Produce a model summary.
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Draw a conclusion based on the fitted model from (b)—does it appear that exercise
    frequency has any impact on mean height? What is the nature of the estimated effect?
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Predict the mean heights of one individual in each of the three exercise categories,
    accompanied by 95 percent prediction intervals.
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do you arrive at the same result and interpretation for the height-by-exercise
    model if you construct an ANOVA table using `aov`?
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is there any change to the outcome of (e) if you alter the model so that the
    reference level of the exercise variable is “none”? Would you expect there to
    be?
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, turn back to the ready-to-use `mtcars` data set. One of the variables in
    this data frame is `qsec`, described as the time in seconds it takes to race a
    quarter mile; another is `gear`, the number of forward gears (cars in this data
    set have either 3, 4, or 5 gears).
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: Using the vectors straight from the data frame, fit a simple linear regression
    model with `qsec` as the response variable and `gear` as the explanatory variable
    and interpret the model summary.
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explicitly convert `gear` to a factor vector and refit the model. Compare the
    model summary with that from (g). What do you find?
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain, with the aid of a relevant plot in the same style as the right image
    of [Figure 20-6](ch20.xhtml#ch20fig6), why you think there is a difference between
    the two models (g) and (h).
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Important Code in This Chapter**'
  id: totrans-297
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| **Function/operator** | **Brief description** | **First occurrence** |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
- en: '| `lm` | Fit linear model | [Section 20.2.3](ch20.xhtml#ch20lev2sec175), [p.
    455](ch20.xhtml#page_455) |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
- en: '| `coef` | Get estimated coefficients | [Section 20.2.4](ch20.xhtml#ch20lev2sec176),
    [p. 457](ch20.xhtml#page_457) |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
- en: '| `summary` | Summarize linear model | [Section 20.3.1](ch20.xhtml#ch20lev2sec177),
    [p. 458](ch20.xhtml#page_458) |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
- en: '| `confint` | Get CIs for estimated coefficients | [Section 20.3.2](ch20.xhtml#ch20lev2sec178),
    [p. 460](ch20.xhtml#page_460) |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
- en: '| `predict` | Predict from linear model | [Section 20.4.2](ch20.xhtml#ch20lev2sec182),
    [p. 463](ch20.xhtml#page_463) |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
- en: '| `relevel` | Change factor reference level | [Section 20.5.3](ch20.xhtml#ch20lev2sec187),
    [p. 477](ch20.xhtml#page_477) |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
