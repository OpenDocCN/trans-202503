<html><head></head><body>
<h2 class="h2" id="ch15"><span epub:type="pagebreak" id="page_249"/><span class="big">15</span><br/>PERSISTENT STORAGE</h2>&#13;
<div class="image1"><img alt="image" src="../images/common01.jpg"/></div>&#13;
<p class="noindent">Scalability and rapid failover are big advantages of containerized applications, and it’s a lot easier to scale, update, and replace stateless containers that don’t have any persistent storage. As a result, we’ve mostly used Deployments to create one or more instances of Pods with only temporary storage.</p>&#13;
<p class="indent">However, even if we have an application architecture in which most of the components are stateless, we still need some amount of persistent storage for our application. At the same time, we don’t want to lose the ability to deploy a Pod to any node in the cluster, and we don’t want to lose the contents of our persistent storage if a container or a node fails.</p>&#13;
<p class="indent">In this chapter, we’ll see how Kubernetes offers persistent storage on demand to Pods by using a plug-in architecture that allows any supported distributed storage engine to act as the backing store.</p>&#13;
<h3 class="h3" id="ch00lev1sec63">Storage Classes</h3>&#13;
<p class="noindent">The Kubernetes storage plug-in architecture is highly flexible; it recognizes that some clusters may not need storage at all, whereas others need multiple storage plug-ins to handle large amounts of data or low-latency storage. <span epub:type="pagebreak" id="page_250"/>For this reason, <code>kubeadm</code> doesn’t set up storage immediately during cluster installation; it’s configured afterward by adding <em>StorageClass</em> resources to the cluster.</p>&#13;
<p class="indent">Each StorageClass identifies a particular storage plug-in that will provide the actual storage along with any additional required parameters. We can use multiple storage classes to define different plug-ins or parameters, or even multiple storage classes with the same plug-in but different parameters, allowing for separate classes of service for different purposes. For example, a cluster may provide in-memory, solid-state, and traditional spinning-disk media to give applications the opportunity to select the storage type that is most applicable for a given purpose. The cluster may offer smaller quotas for more expensive and lower-latency storage, while offering large quotas for slower storage that is more suitable for infrequently accessed data.</p>&#13;
<p class="indent">Kubernetes has a set of internal storage provisioners built in. This includes storage drivers for popular cloud providers such as Amazon Web Services, Microsoft Azure, and Google Container Engine. However, using any storage plug-in is easy as long as it has support for the Container Storage Interface (CSI), a published standard for interfacing with a storage provider.</p>&#13;
<p class="indent">Of course, to be compatible with CSI, the storage provider must include a minimum set of features that are essential for storage in a Kubernetes cluster. The most important of these are dynamic storage management (provisioning and deprovisioning) and dynamic storage attachment (mounting storage on any node in the cluster). Together, these two key features allow the cluster to allocate storage for any Pod that requests it, schedule that Pod on any node, and start a new Pod with the same storage on any node if the existing node fails or the Pod is replaced.</p>&#13;
<h4 class="h4" id="ch00lev2sec93">Storage Class Definition</h4>&#13;
<p class="noindent">Our Kubernetes cluster deployment in <a href="ch06.xhtml#ch06">Chapter 6</a> included the Longhorn storage plug-in (see “Installing Storage” on <a href="ch06.xhtml#ch00lev2sec46">page 102</a>). The automation scripts have installed it in the cluster for each following chapter. Part of this installation created a DaemonSet so that Longhorn components exist on every node. That DaemonSet kicked off a number of Longhorn components and then created a StorageClass resource to tell Kubernetes how to use Longhorn to provision storage for a Pod.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>The example repository for this book is at</em> <a href="https://github.com/book-of-kubernetes/examples">https://github.com/book-of-kubernetes/examples</a>. <em>See “Running Examples” on <a href="ch00.xhtml#ch00lev1sec2">page xx</a> for details on getting set up.</em></p>&#13;
</div>&#13;
<p class="indent"><a href="ch15.xhtml#ch15list1">Listing 15-1</a> shows the StorageClass that Longhorn created.</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get storageclass</span>&#13;
NAME      PROVISIONER         RECLAIMPOLICY  VOLUMEBINDINGMODE  ALLOWVOLUMEEXPANSION ...&#13;
longhorn  driver.longhorn.io  Delete         Immediate          true                 ...</pre>&#13;
<p class="caption" id="ch15list1"><em>Listing 15-1: Longhorn StorageClass</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_251"/>The two most important fields show the name of the StorageClass and the provisioner. The name is used in resource specifications to identify that the Longhorn StorageClass should be used to provision the requested volume, whereas the provisioner is used internally by <code>kubelet</code> to communicate with the Longhorn CSI plug-in.</p>&#13;
<h4 class="h4" id="ch00lev2sec94">CSI Plug-in Internals</h4>&#13;
<p class="noindent">Let’s look quickly at how <code>kubelet</code> finds and communicates with the Longhorn CSI plug-in before moving on to provisioning volumes and attaching them to Pods. Note that <code>kubelet</code> runs as a service directly on the cluster nodes; on the other hand, all of the Longhorn components are containerized. This means that the two need a little help to communicate in the form of a Unix socket that is created on the host filesystem and then mounted into the filesystem of the Longhorn containers. A Unix socket allows two processes to communicate by streaming data, similar to a network connection but without the network overhead.</p>&#13;
<p class="indent">To explore how this communication works, first we’ll list the Longhorn containers that are running on <code>host01</code>:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">crictl ps --name 'longhorn.*|csi.*'</span>&#13;
CONTAINER     ... STATE    NAME ...&#13;
c8347a513f71e ... Running  csi-provisioner ...&#13;
47f950a3e8dbf ... Running  csi-provisioner ...&#13;
3aad0fef7454e ... Running  longhorn-csi-plugin ...&#13;
9bfb61f786afa ... Running  csi-snapshotter ...&#13;
24a2994a264a1 ... Running  csi-snapshotter ...&#13;
7ee4c748b4c02 ... Running  csi-snapshotter ...&#13;
8d92886fdacda ... Running  csi-resizer ...&#13;
9868014407fe0 ... Running  csi-resizer ...&#13;
408d16181af51 ... Running  csi-attacher ...&#13;
0c6c341debb0c ... Running  longhorn-driver-deployer ...&#13;
ba328a9d0aaf2 ... Running  longhorn-manager ...&#13;
c39e5c4fee3bb ... Running  longhorn-ui ...</pre>&#13;
<p class="indent">Longhorn creates containers with names that start with either <code>longhorn</code> or <code>csi</code>, so we use a regular expression with <code>crictl</code> to show only those containers.</p>&#13;
<p class="indent">Let’s capture the container ID of the <code>csi-attacher</code> container and then inspect it to see what volume mounts it has:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">CID=$(crictl ps -q --name csi-attacher)</span>&#13;
root@host01:~# <span class="codestrong1">crictl inspect $CID</span>&#13;
{&#13;
...&#13;
    "mounts": [&#13;
      {&#13;
        "containerPath": "/csi/",&#13;
<span epub:type="pagebreak" id="page_252"/>     <span class="ent">➊</span> "hostPath": "/var/lib/kubelet/plugins/driver.longhorn.io",&#13;
        "propagation": "PROPAGATION_PRIVATE",&#13;
        "readonly": false,&#13;
        "selinuxRelabel": false&#13;
      }&#13;
...&#13;
      "envs": [&#13;
        {&#13;
          "key": "ADDRESS",&#13;
       <span class="ent">➋</span> "value": "/csi/csi.sock"&#13;
        },&#13;
...&#13;
}</pre>&#13;
<p class="indent">The <code>crictl inspect</code> command returns a lot of data from the container, but we show only the relevant data in this example. We can see that this Longhorn component is instructed to connect to <em>/csi/csi.sock</em> <span class="ent">➋</span>, which is the mount point inside the container for the Unix socket that <code>kubelet</code> uses to communicate with the storage driver. We can also see that <em>/csi</em> inside the container is <em>/var/lib/kubelet/plugins/driver.longhorn.io</em> <span class="ent">➊</span>. The location <em>/var/lib/kubelet/plugins</em> is a standard location for <code>kubelet</code> to look for storage plug-ins, and of course, <em>driver.longhorn.io</em> is the value of the <code>provisioner</code> field, as defined in the Longhorn StorageClass in <a href="ch15.xhtml#ch15list1">Listing 15-1</a>.</p>&#13;
<p class="indent">If we look on the host, we can confirm that this Unix socket exists:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">ls -l /var/lib/kubelet/plugins/driver.longhorn.io</span>&#13;
total 0&#13;
srwxr-xr-x 1 root root 0 Feb 18 20:17 csi.sock</pre>&#13;
<p class="indent">The <code>s</code> as the first character indicates that this is a Unix socket.</p>&#13;
<h3 class="h3" id="ch00lev1sec64">Persistent Volumes</h3>&#13;
<p class="noindent">Now that we’ve seen how <code>kubelet</code> communicates with an external storage driver, let’s look at how to request allocation of storage and then attach that storage to a Pod.</p>&#13;
<h4 class="h4" id="ch00lev2sec95">Stateful Sets</h4>&#13;
<p class="noindent">The easiest way to get storage in a Pod is to use a StatefulSet (a resource described in <a href="ch07.xhtml#ch07">Chapter 7</a>). Like a Deployment, a StatefulSet creates multiple Pods, which can be allocated to any node. However, a StatefulSet also creates persistent storage as well as a mapping between each Pod and its storage. If a Pod needs to be replaced, it is replaced with a new Pod with the same identifier and the same persistent storage.</p>&#13;
<p class="indent"><a href="ch15.xhtml#ch15list2">Listing 15-2</a> presents an example StatefulSet that creates two PostgreSQL Pods with persistent storage.</p>&#13;
<p class="noindent6"><span epub:type="pagebreak" id="page_253"/><em>pgsql-set.yaml</em></p>&#13;
<pre>---&#13;
apiVersion: apps/v1&#13;
kind: StatefulSet&#13;
metadata:&#13;
  name: postgres&#13;
spec:&#13;
  serviceName: postgres&#13;
  replicas: 2&#13;
  selector:&#13;
    matchLabels:&#13;
      app: postgres&#13;
  template:&#13;
    metadata:&#13;
      labels:&#13;
        app: postgres&#13;
    spec:&#13;
      containers:&#13;
      - name: postgres&#13;
        image: postgres&#13;
        env:&#13;
        - name: POSTGRES_PASSWORD&#13;
       <span class="ent">➊</span> value: "supersecret"&#13;
        - name: PGDATA&#13;
       <span class="ent">➋</span> value: /data/pgdata&#13;
        volumeMounts:&#13;
        - name: postgres-volume&#13;
       <span class="ent">➌</span> mountPath: /data&#13;
  volumeClaimTemplates:&#13;
  - metadata:&#13;
      name: postgres-volume&#13;
    spec:&#13;
      storageClassName: longhorn&#13;
      accessModes:&#13;
        - ReadWriteOnce&#13;
      resources:&#13;
        requests:&#13;
          storage: 1Gi</pre>&#13;
<p class="caption" id="ch15list2"><em>Listing 15-2: PostgreSQL StatefulSet</em></p>&#13;
<p class="indent">In addition to setting the password using an environment variable <span class="ent">➊</span>, we also set <code>PGDATA</code> to <em>/data/pgdata</em> <span class="ent">➋</span>, which tells PostgreSQL where to store the files for the database. It aligns with the volume mount we also declare as part of the StatefulSet, as that persistent volume will be mounted at <em>/data</em> <span class="ent">➌</span>. The PostgreSQL container image documentation recommends configuring the database files to reside in a subdirectory beneath the mount point to avoid a potential issue with ownership of the data directory.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_254"/>Separate from the configuration for the PostgreSQL Pods, we supply the StatefulSet with the <code>volumeClaimTemplates</code> field. This field tells the StatefulSet how we want the persistent storage to be configured. It includes the name of the StorageClass and the requested size, and it also includes an <code>accessMode</code> of <code>ReadWriteOnce</code>, which we’ll explore later. The StatefulSet will use this specification to allocate independent storage for each Pod.</p>&#13;
<p class="indent">As mentioned in <a href="ch07.xhtml#ch07">Chapter 7</a>, this StatefulSet references a Service using the <code>serviceName</code> field, and this Service is used to create the domain name for the Pods. The Service is defined in the same file as follows:</p>&#13;
<p class="noindent6"><em>pgsql-set.yaml</em></p>&#13;
<pre>---&#13;
apiVersion: v1&#13;
kind: Service&#13;
metadata:&#13;
  name: postgres&#13;
spec:&#13;
  clusterIP: None&#13;
  selector:&#13;
    app: postgres</pre>&#13;
<p class="indent">Setting the <code>clusterIP</code> field to <code>None</code> makes this a <em>Headless Service</em>, which means that no IP address is allocated from the service IP range and none of the load balancing described in <a href="ch09.xhtml#ch09">Chapter 9</a> is configured for this Service. This approach is typical for a StatefulSet. With a StatefulSet, each Pod has its own unique identity and unique storage. Because service load balancing just randomly chooses a destination, it is typically not useful with a StatefulSet. Instead, clients explicitly select a Pod instance as a destination.</p>&#13;
<p class="indent">Let’s create the Service and StatefulSet:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/pgsql-set.yaml</span> &#13;
service/postgres created&#13;
statefulset.apps/postgres created</pre>&#13;
<p class="indent">It will take some time to get the Pods up and running because they are created sequentially, one at a time. After they are running, we can see how they’ve been named:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get pods</span>&#13;
NAME         READY   STATUS    RESTARTS   AGE&#13;
postgres-0   1/1     Running   0          97s&#13;
postgres-1   1/1     Running   0          51s</pre>&#13;
<p class="indent">Let’s examine the persistent storage from within the container:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl exec -ti postgres-0 -- /bin/sh</span>&#13;
# <span class="codestrong1">findmnt /data</span>&#13;
TARGET SOURCE                         FSTYPE OPTIONS&#13;
/data  /dev/longhorn/pvc-83becdac-... ext4   rw,relatime&#13;
# <span class="codestrong1">exit</span></pre>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_255"/>As requested, we see a Longhorn device that has been mounted at <em>/data</em>. Kubernetes will keep this persistent storage even if the node fails or the Pod is upgraded.</p>&#13;
<p class="indent">This StatefulSet has two more important resources to explore. First is the headless Service that we created:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get svc</span>&#13;
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE&#13;
kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP   54m&#13;
postgres     ClusterIP   None         &lt;none&gt;        &lt;none&gt;    19m</pre>&#13;
<p class="indent">The <code>postgres</code> Service exists, but no cluster IP address is shown because we created it as a headless Service. However, it has created DNS entries for the associated Pods, so we can use it to connect to specific PostgreSQL Pods without knowing the Pod IP address.</p>&#13;
<p class="indent">We need to use the cluster DNS to do the lookup. The easiest way to do that is from within a container:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl run -ti --image=alpine --restart=Never alpine</span>&#13;
If you don't see a command prompt, try pressing enter.&#13;
/ #</pre>&#13;
<p class="indent">This form of the <code>run</code> command stays in the foreground and gives us an interactive terminal. It also tells Kubernetes not to try to restart the container when we exit the shell.</p>&#13;
<p class="indent">From inside this container, we can refer to either of our PostgreSQL Pods by a well-known name:</p>&#13;
<pre>/ # <span class="codestrong1">ping -c 1 postgres-0.postgres.default.svc</span>&#13;
PING postgres-0.postgres.default.svc (172.31.239.198): 56 data bytes&#13;
64 bytes from 172.31.239.198: seq=0 ttl=63 time=0.093 ms&#13;
...&#13;
/# <span class="codestrong1">ping -c 1 postgres-1.postgres.default.svc</span>&#13;
PING postgres-1.postgres.default.svc (172.31.239.199): 56 data bytes&#13;
64 bytes from 172.31.239.199: seq=0 ttl=63 time=0.300 ms&#13;
...&#13;
# <span class="codestrong1">exit</span></pre>&#13;
<p class="indent">The naming convention is identical to what we saw for Services in <a href="ch09.xhtml#ch09">Chapter 9</a>, but with an extra hostname prefix for the name of the Pod; in this case, either <code>postgres-0</code> or <code>postgres-1</code>.</p>&#13;
<p class="indent">The other important resource is the <em>PersistentVolumeClaim</em> that the StatefulSet created automatically. The PersistentVolumeClaim is what actually allocates storage using the Longhorn StorageClass:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get pvc</span>&#13;
NAME                         STATUS   VOLUME      ...   CAPACITY   ...&#13;
postgres-volume-postgres-0   Bound    pvc-83becdac...   1Gi        ...&#13;
postgres-volume-postgres-1   Bound    pvc-0d850889...   1Gi        ...</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_256"/>We use the abbreviation <code>pvc</code> in lieu of its full name, <code>persistentvolumeclaim</code>.</p>&#13;
<p class="indent">The StatefulSet used the data in the <code>volumeClaimTemplates</code> field in <a href="ch15.xhtml#ch15list2">Listing 15-2</a> to create these two PersistentVolumeClaims. However, if we delete the StatefulSet, the PersistentVolumeClaims continue to exist:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl delete -f /opt/pgsql-set.yaml</span> &#13;
service "postgres" deleted&#13;
statefulset.apps "postgres" deleted&#13;
root@host01:~# <span class="codestrong1">kubectl get pvc</span>&#13;
NAME                         STATUS   VOLUME      ...   CAPACITY   ...&#13;
postgres-volume-postgres-0   Bound    pvc-83becdac...   1Gi        ...&#13;
postgres-volume-postgres-1   Bound    pvc-0d850889...   1Gi        ...</pre>&#13;
<p class="indent">This protects us from accidentally deleting our persistent storage. If we create the StatefulSet again and keep the same name in the volume claim template, our new Pods will get the same storage back.</p>&#13;
<div class="box5">&#13;
<p class="boxtitle-d"><strong>HIGHLY AVAILABLE POSTGRESQL</strong></p>&#13;
<p class="noindents">We’ve deployed two separate instances of PostgreSQL, each with its own independent persistent storage. However, that’s only the first step in deploying a highly available database. We would also need to configure one instance as primary and the other as backup, configure replication from the primary to the backup, and configure failover. We would also need to configure clients to talk to the primary and switch to a new primary when there’s a failure. Fortunately, we don’t need to do this configuration ourselves. In <a href="ch17.xhtml#ch17">Chapter 17</a>, we’ll see how to take advantage of the power of custom resources to deploy a Kubernetes Operator for PostgreSQL that automatically will handle all of this.</p>&#13;
</div>&#13;
<p class="indent">The StatefulSet is the best way to handle the case in which we need multiple instances of a container, each with its own independent storage. However, we can also use persistent volumes more directly, which gives us more control over how they’re mounted into our Pods.</p>&#13;
<h4 class="h4" id="ch00lev2sec96">Volumes and Claims</h4>&#13;
<p class="noindent">Kubernetes has both a <em>PersistentVolume</em> and a PersistentVolumeClaim resource type. The PersistentVolumeClaim represents a request for allocated storage, whereas the PersistentVolume holds information on the allocated storage. For the most part, this distinction doesn’t matter, and we can just focus on the PersistentVolumeClaim. However, the difference is important in two cases:</p>&#13;
<ul>&#13;
<li><p class="noindent">Administrators can create a PersistentVolume manually, and this PersistentVolume can be directly mounted into a Pod.</p></li>&#13;
<li><p class="noindent">If there is an issue allocating storage as specified in the PersistentVolumeClaim, the PersistentVolume will not be created.</p></li>&#13;
</ul>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_257"/>To illustrate, first we’ll start with a PersistentVolumeClaim that automatically allocates storage:</p>&#13;
<p class="noindent6"><em>pvc.yaml</em></p>&#13;
<pre>---&#13;
apiVersion: v1&#13;
kind: PersistentVolumeClaim&#13;
metadata:&#13;
  name: nginx-storage&#13;
spec:&#13;
  storageClassName: longhorn&#13;
  accessModes:&#13;
    - ReadWriteOnce&#13;
  resources:&#13;
    requests:&#13;
      storage: 100Mi</pre>&#13;
<p class="indent">We named this PersistentVolumeClaim <code>nginx-storage</code> because that’s how we’ll use it in a moment. The PersistentVolumeClaim requests 100MiB of storage from the <code>longhorn</code> StorageClass. When we apply this PersistentVolumeClaim to the cluster, Kubernetes invokes the Longhorn storage driver and allocates the storage, creating a PersistentVolume in the process:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/pvc.yaml</span> &#13;
persistentvolumeclaim/nginx-storage created&#13;
root@host01:~# <span class="codestrong1">kubectl get pv</span>&#13;
NAME         ...  CAPACITY ... STATUS  CLAIM                               STORAGECLASS ...&#13;
pvc-0b50e5b4-...  1Gi      ... Bound   default/postgres-volume-postgres-1  longhorn     ...&#13;
pvc-ad092ba9-...  1Gi      ... Bound   default/postgres-volume-postgres-0  longhorn     ...&#13;
pvc-cb671684-...  100Mi    ... Bound   default/nginx-storage               longhorn     ...</pre>&#13;
<p class="indent">The abbreviation <code>pv</code> is short for <code>persistentvolumes</code>.</p>&#13;
<p class="indent">Even though no Pod is using the storage, it still shows a status of <code>Bound</code> because there is an active PersistentVolumeClaim for the storage.</p>&#13;
<p class="indent">If we try to create a PersistentVolumeClaim without a matching storage class, the cluster won’t be able to create the corresponding PersistentVolume:</p>&#13;
<p class="noindent6"><em>pvc-man.yaml</em></p>&#13;
<pre>---&#13;
apiVersion: v1&#13;
kind: PersistentVolumeClaim&#13;
metadata:&#13;
  name: manual&#13;
spec:&#13;
  storageClassName: manual&#13;
  accessModes:&#13;
    - ReadWriteOnce&#13;
  resources:&#13;
    requests:&#13;
      storage: 100Mi</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_258"/>Because there is no StorageClass called <code>manual</code>, Kubernetes can’t create this storage automatically:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/pvc-man.yaml</span> &#13;
persistentvolumeclaim/manual created&#13;
root@host01:~# <span class="codestrong1">kubectl get pvc</span>&#13;
NAME                         STATUS    ... STORAGECLASS   AGE&#13;
manual                       Pending   ... manual         6s&#13;
...&#13;
root@host01:~# <span class="codestrong1">kubectl get pv</span>&#13;
NAME                                       ...&#13;
pvc-0b50e5b4-9889-4c8d-a651-df78fa2bc764   ...&#13;
pvc-ad092ba9-cf30-4b7d-af01-ff02a5924db7   ...&#13;
pvc-cb671684-1719-4c33-9dd8-bcbbf24523b4   ...</pre>&#13;
<p class="indent">Our PersistentVolumeClaim has a status of <code>Pending</code> and there is no corresponding PersistentVolume. However, as a cluster administrator, we can create this PersistentVolume manually:</p>&#13;
<p class="noindent6"><em>pv.yaml</em></p>&#13;
<pre>---&#13;
apiVersion: v1&#13;
kind: PersistentVolume&#13;
metadata:&#13;
  name: manual&#13;
spec:&#13;
  claimRef:&#13;
    name: manual&#13;
    namespace: default&#13;
  accessModes:&#13;
    - ReadWriteOnce&#13;
  capacity:&#13;
    storage: 100Mi&#13;
  csi:&#13;
    driver: driver.longhorn.io&#13;
    volumeHandle: manual</pre>&#13;
<p class="indent">When creating a PersistentVolume in this way, we need to specify the type of volume we want. In this case, by including the <code>csi</code> field, we identify this as a volume created by a CSI plug-in. We then specify the <code>driver</code> to use and provide a unique value for <code>volumeHandle</code>. After the PersistentVolume is created, Kubernetes directly invokes the Longhorn storage driver to allocate storage.</p>&#13;
<p class="indent">We create the PersistentVolume with the following:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/pv.yaml</span> &#13;
persistentvolume/manual created</pre>&#13;
<p class="indent">Because we specified a <code>claimRef</code> for this PersistentVolume, it will automatically move into the <code>Bound</code> state:</p>&#13;
<pre><span epub:type="pagebreak" id="page_259"/>root@host01:~# <span class="codestrong1">kubectl get pv manual</span>&#13;
NAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   ...&#13;
manual   100Mi      RWO            Retain           Bound    ...</pre>&#13;
<p class="indent">It will take a few seconds, so the PersistentVolume may show up as <code>Available</code> briefly.</p>&#13;
<p class="indent">The PersistentVolumeClaim also moves into the <code>Bound</code> state:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get pvc manual</span>&#13;
NAME     STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE&#13;
manual   Bound    manual   100Mi      RWO            manual         2m20s</pre>&#13;
<p class="indent">It is useful for an administrator to create a PersistentVolume manually for those rare cases when specialized storage is needed for an application. However, for most persistent storage, it is much better to automate storage allocation through a StorageClass and either a PersistentVolumeClaim or a StatefulSet.</p>&#13;
<h4 class="h4" id="ch00lev2sec97">Deployments</h4>&#13;
<p class="noindent">Now that we’ve directly created a PersistentVolumeClaim and we have the associated volume, we can use it in a Deployment. To demonstrate this, we’ll show how we can use persistent storage to hold HTML files served by an NGINX web server:</p>&#13;
<p class="noindent6"><em>nginx.yaml</em></p>&#13;
<pre>---&#13;
apiVersion: apps/v1&#13;
kind: Deployment&#13;
metadata:&#13;
  name: nginx&#13;
spec:&#13;
  replicas: 1&#13;
  selector:&#13;
    matchLabels:&#13;
      app: nginx&#13;
  template:&#13;
    metadata:&#13;
      labels:&#13;
        app: nginx&#13;
    spec:&#13;
      containers:&#13;
      - name: nginx&#13;
        image: nginx&#13;
        volumeMounts:&#13;
       <span class="ent">➊</span> - name: html&#13;
            mountPath: /usr/share/nginx/html&#13;
      volumes:&#13;
     <span class="ent">➋</span> - name: html&#13;
<span epub:type="pagebreak" id="page_260"/>          persistentVolumeClaim:&#13;
            claimName: nginx-storage</pre>&#13;
<p class="indent">It takes two steps to get the persistent storage mounted into our container. First, we declare a <code>volume</code> named <code>html</code> <span class="ent">➋</span> that references the PersistentVolumeClaim we created. This makes the storage available in the Pod. Next, we declare a <code>volumeMount</code> <span class="ent">➊</span> to specify where in the container’s filesystem this particular volume should appear. The advantage of having these two separate steps is that we can mount the same volume in multiple containers within the same Pod, which enables us to share data between processes using files even for cases in which the processes come from separate container images.</p>&#13;
<p class="indent">This capability allows for some interesting use cases. For example, suppose that we’re building a web application that includes some static content. We might deploy an NGINX web server to serve that content, as we’re doing here. At the same time, we also need a way to update the content. We might do that by having an additional container in the Pod that periodically checks for new content and updates a persistent volume that is shared with the NGINX container.</p>&#13;
<p class="indent">Let’s create the NGINX Deployment so that we can demonstrate that HTML files can be served from the persistent storage. The persistent storage will start empty, so at first there won’t be any web content to serve. Let’s see how NGINX behaves in that case:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/nginx.yaml</span> &#13;
deployment.apps/nginx created</pre>&#13;
<p class="indent">As soon as the NGINX server is up and running, we need to grab its IP address so that we can make an HTTP request using <code>curl</code>:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">IP=$(kubectl get po -l app=nginx -o jsonpath='{..podIP}')</span>&#13;
root@host01:~# <span class="codestrong1">curl -v http://$IP</span>&#13;
...&#13;
* Connected to 172.31.25.200 (172.31.25.200) port 80 (#0)&#13;
&gt; GET / HTTP/1.1&#13;
...&#13;
&lt; HTTP/1.1 403 Forbidden</pre>&#13;
<p class="indent">To grab the IP address in this case, we use the <code>jsonpath</code> output format for <code>kubectl</code> rather than use <code>jq</code> to filter JSON output; <code>jsonpath</code> has a very useful syntax for searching into a JSON object and pulling out a single uniquely named field (in this example, <code>podIP</code>). We could use a <code>jq</code> filter similar to what we did in <a href="ch08.xhtml#ch08">Chapter 8</a>, but the <code>jq</code> syntax for recursion is more complex.</p>&#13;
<p class="indent">After we have the IP, we use <code>curl</code> to contact NGINX. As expected, we don’t see an HTML response, because our persistent storage is empty. However, we know that our volume mounted correctly because in this case we don’t even see the default NGINX welcome page.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_261"/>Let’s copy in an <em>index.html</em> file to give our NGINX server something to serve:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">POD=$(kubectl get po -l app=nginx -o jsonpath='{..metadata.name}')</span>&#13;
root@host01:~# <span class="codestrong1">kubectl cp /opt/index.html $POD:/usr/share/nginx/html</span></pre>&#13;
<p class="indent">First, we capture the name of the Pod as randomly generated by the Deployment and then we use <code>kubectl cp</code> to copy in an HTML file. If we try running <code>curl</code> again, we’ll see a much better response:</p>&#13;
<pre>root@host01:~# curl -v http://$IP&#13;
...&#13;
* Connected to 172.31.239.210 (172.31.239.210) port 80 (#0)&#13;
&gt; GET / HTTP/1.1&#13;
...&#13;
&lt; HTTP/1.1 200 OK&#13;
...&#13;
&lt;html&gt;&#13;
  &lt;head&gt;&#13;
    &lt;title&gt;Hello, World&lt;/title&gt;&#13;
  &lt;/head&gt;&#13;
  &lt;body&gt;&#13;
    &lt;h1&gt;Hello, World!&lt;/h1&gt;&#13;
  &lt;/body&gt;&#13;
&lt;/html&gt;&#13;
...</pre>&#13;
<p class="indent">Because this is persistent storage, this HTML content will remain available even if we delete the Deployment and create it again.</p>&#13;
<p class="indent">However, we still have one significant problem to overcome. One of the primary reasons to have a Deployment is to be able to scale to multiple Pod instances. Scaling this Deployment makes a lot of sense, as we could have multiple Pod instances serving the same HTML content. Unfortunately, scaling won’t currently work:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl scale --replicas=3 deployment/nginx</span>&#13;
deployment.apps/nginx scaled</pre>&#13;
<p class="indent">The Deployment appears to scale, but if we look at the Pods, we will see that we don’t really have multiple running instances:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get pods</span>&#13;
NAME                    READY   STATUS              RESTARTS   AGE&#13;
...&#13;
nginx-db4f4d5d9-7q7rd   0/1     ContainerCreating   0          46s&#13;
nginx-db4f4d5d9-gbqxm   0/1     ContainerCreating   0          46s&#13;
nginx-db4f4d5d9-vrzr4   1/1     Running             0          10m</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_262"/>The two new instances are stuck in <code>ContainerCreating</code>. Let’s examine one of those two Pods to see why:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl describe</span> <span class="codestrong1"><span class="codeitalic1">pod/nginx-db4f4d5d9-7q7rd</span></span>&#13;
Name:           nginx-db4f4d5d9-7q7rd&#13;
...&#13;
Status:         Pending&#13;
Events:&#13;
  Type     Reason              Age   From                     Message&#13;
  ----     ------              ----  ----                     -------&#13;
...&#13;
  Warning  FailedAttachVolume  110s  attachdetach-controller  Multi-Attach &#13;
    error for volume "pvc-cb671684-1719-4c33-9dd8-bcbbf24523b4" Volume is &#13;
    already used by pod(s) nginx-db4f4d5d9-vrzr4</pre>&#13;
<p class="indent">The first Pod we created has claimed the volume, and no other Pods can attach to it, so they are stuck in a <code>Pending</code> state. Even worse, this doesn’t just prevent scaling, it also prevents upgrading or making other configuration changes to the Deployment. If we update the Deployment configuration, Kubernetes will try to start a Pod using the new configuration before shutting down any old Pods. The new Pods can’t attach to the volume and therefore can’t start, so the old Pod will never be cleaned up and the configuration change will never take place.</p>&#13;
<p class="indent">We could force a Pod update in a couple ways. First, we could manually delete and re-create the Deployment anytime we made changes. Second, we could configure Kubernetes to delete the old Pod first by using a <code>Recreate</code> update strategy. We explore update strategy options in greater detail in <a href="ch20.xhtml#ch20">Chapter 20</a>. For now, it’s worth noting that this still would not allow us to scale the Deployment.</p>&#13;
<p class="indent">If we want to fix this so that we can scale the Deployment, we’ll need to allow multiple Pods to attach to the volume at the same time. We can do this by changing the access mode for the persistent volume.</p>&#13;
<h4 class="h4" id="ch00lev2sec98">Access Modes</h4>&#13;
<p class="noindent">Kubernetes is refusing to attach multiple Pods to the same persistent volume because we configured the PersistentVolumeClaim with an access mode of <code>ReadWriteOnce</code>. An alternate access mode, <code>ReadWriteMany</code>, will allow all of the NGINX server Pods to mount the storage simultaneously. Only some storage drivers support the <code>ReadWriteMany</code> access mode, because it requires the ability to manage simultaneous changes to files, including communicating changes dynamically to all of the nodes in the cluster.</p>&#13;
<p class="indent">Longhorn does support <code>ReadWriteMany</code>, so creating a PersistentVolumeClaim with <code>ReadWriteMany</code> access mode is an easy change:</p>&#13;
<p class="noindent6"><em>pvc-rwx.yaml</em></p>&#13;
<pre>---&#13;
apiVersion: v1&#13;
kind: PersistentVolumeClaim&#13;
<span epub:type="pagebreak" id="page_263"/>metadata:&#13;
  name: storage&#13;
spec:&#13;
  storageClassName: longhorn&#13;
  accessModes:&#13;
    - ReadWriteMany&#13;
  resources:&#13;
    requests:&#13;
      storage: 100Mi</pre>&#13;
<p class="indent">Unfortunately, we can’t modify our existing PersistentVolumeClaim to change the access mode. And we can’t delete the PersistentVolumeClaim while the storage is in use by our Deployment. So we need to clean up everything and then deploy again:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl delete deploy/nginx pvc/storage</span>&#13;
deployment.apps "nginx" deleted&#13;
persistentvolumeclaim "storage" deleted&#13;
root@host01:~# <span class="codestrong1">kubectl apply -f /opt/pvc-rwx.yaml</span> &#13;
persistentvolumeclaim/storage created&#13;
root@host01:~# <span class="codestrong1">kubectl apply -f /opt/nginx.yaml</span> &#13;
deployment.apps/nginx created</pre>&#13;
<p class="indent">We specify <code>deploy/nginx</code> and <code>pvc/storage</code> as the resources to delete. This style of identifying the resources allows us to operate on two resources in the same command.</p>&#13;
<p class="indent">After a minute or so, the new NGINX Pod will be running:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get pods</span>&#13;
NAME                    READY   STATUS      RESTARTS   AGE&#13;
...&#13;
nginx-db4f4d5d9-6thzs   1/1     Running     0          44s</pre>&#13;
<p class="indent">At this point, we need to copy our HTML content over again because deleting the PersistentVolumeClaim deleted the previous storage:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">POD=$(kubectl get po -l app=nginx -o jsonpath='{..metadata.name}')</span>&#13;
root@host01:~# <span class="codestrong1">kubectl cp /opt/index.html $POD:/usr/share/nginx/html</span>&#13;
... no output ...</pre>&#13;
<p class="indent">This time, when we scale our NGINX Deployment, the additional two Pods are able to mount the storage and start running:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl scale --replicas=3 deploy nginx</span>&#13;
deployment.apps/nginx scaled&#13;
root@host01:~# <span class="codestrong1">kubectl get po</span>&#13;
NAME                    READY   STATUS      RESTARTS   AGE&#13;
...&#13;
nginx-db4f4d5d9-2j629   1/1     Running     0          23s&#13;
<span epub:type="pagebreak" id="page_264"/>nginx-db4f4d5d9-6thzs   1/1     Running     0          5m19s&#13;
nginx-db4f4d5d9-7r5qj   1/1     Running     0          23s</pre>&#13;
<p class="indent">All three NGINX Pods are serving the same content, as we can see if we fetch the IP address for one of the new Pods and connect to it:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">IP=$(kubectl get po <span class="codeitalic1">nginx-db4f4d5d9-2j629</span> -o jsonpath='{..podIP}')</span>&#13;
root@host01:~# <span class="codestrong1">curl http://$IP</span>&#13;
&lt;html&gt;&#13;
  &lt;head&gt;&#13;
    &lt;title&gt;Hello, World&lt;/title&gt;&#13;
  &lt;/head&gt;&#13;
  &lt;body&gt;&#13;
    &lt;h1&gt;Hello, World!&lt;/h1&gt;&#13;
  &lt;/body&gt;&#13;
&lt;/html&gt;</pre>&#13;
<p class="indent">At this point, we could use any NGINX Pod to update the HTML content and all Pods would serve the new content. We could even use a separate CronJob with an application component that updates the content dynamically, and NGINX would happily serve whatever files are in place.</p>&#13;
<h3 class="h3" id="ch00lev1sec65">Final Thoughts</h3>&#13;
<p class="noindent">Persistent storage is an essential requirement for building a fully functioning application. After a cluster administrator has configured one or more storage classes, it’s easy for application developers to dynamically request persistent storage as part of their application deployment. In most cases, the best way to do this is with a StatefulSet, as Kubernetes will automatically handle allocating independent storage for each Pod and will maintain a one-to-one relationship between Pod and storage during failover and upgrades.</p>&#13;
<p class="indent">At the same time, there are other storage use cases, such as having multiple Pods access the same storage. We can easily handle those use cases by directly creating a PersistentVolumeClaim resource and then declaring it as a volume in a controller such as a Deployment or Job.</p>&#13;
<p class="indent">Although persistent storage is an effective way to make file content available to containers, Kubernetes has other powerful resource types that can store configuration data and pass it to containers as either environment variables or file content. In the next chapter, we’ll explore how to manage application configuration and secrets.</p>&#13;
</body></html>