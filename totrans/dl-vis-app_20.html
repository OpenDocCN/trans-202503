<html><head></head><body><div id="sbo-rt-content"><section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" title="429" id="Page_429"/>16</span><br/>
<span class="ChapterTitle">Convolutional Neural Networks</span></h1>
</header>
<figure class="opener">
<img src="Images/chapterart.png" alt="" width="206" height="206"/>
</figure>
<p class="ChapterIntro">This chapter is all about a deep learning technique called <em>convolution</em>. Among its uses, convolution has become the standard method for classifying, manipulating, and generating images. Convolution is easy to use in deep learning because it can be easily encapsulated in a <em>convolution layer</em> (also called a <em>convolutional layer</em>). In this chapter, we look at the key ideas behind convolution and the related techniques we use to make convolution work in practice. We will see how to arrange a series of these operations to create a hierarchy of operations, which turns a series of simple operations into a powerful tool.</p>
<p><span epub:type="pagebreak" title="430" id="Page_430"/>In order to stay specific, in this chapter we focus our discussion of convolution on working with images. Models that use convolution have been spectacularly successful in this domain. For example, they excel at basic classification tasks like determining if an image is a leopard or a cheetah, or a planet or a marble. We can recognize the people in a photograph (Sun, Wang, and Tang 2014); detect and classify different types of skin cancers (Esteva et al. 2017); repair image damage like dust, scratches, and blur (Mao, Shen, and Yang 2016); and classify people’s age and gender from their photos (Levi and Hassner 2015). Convolution-based networks are also useful in many other applications, such as natural language processing (Britz 2015), where we can work out the structure of sentences (Kalchbrenner, Grefenstette, and Blunsom 2014) or classify sentences into different categories (Kim 2014).</p>
<h2 id="h1-500723c16-0001">Introducing Convolution</h2>
<p class="BodyFirst">In deep learning, images are 3D tensors, with a height, width, and number of <em>channels</em>, or values per pixel. A grayscale image has only one value per pixel, and thus only one channel. A color image stored as RGB has three channels (with values for red, green, and blue). Sometimes people use the terms<em> depth</em> or <em>fiber size</em> to refer to the number of channels in a tensor. Unfortunately, <em>depth</em> is also used to refer to the number of layers in a deep network, and <em>fiber size</em> has not caught on widely. To avoid confusion, we always refer to the three dimensions of an image (and related 3D tensors) as height, width, and channels. Using our deep learning terminology, each image we provide to the network for processing is a sample. Each pixel in an image is a feature. </p>
<p>When a tensor moves through a series of convolution layers, it often changes in width, height, and number of channels. If a tensor happens to have 1 or 3 channels, we can think of it as an image. But if a tensor has, say, 14 or 512 channels, it’s probably best not to think of it as an image any more. This suggests that we shouldn’t refer to individual elements of the tensor as <em>pixels</em>, which is an image-centric term. Instead, we call them <em>elements</em>. <a href="#figure16-1" id="figureanchor16-1">Figure 16-1</a> shows these terms visually.</p>
<figure>
<img src="Images/F16001.png" alt="F16001" width="694" height="184"/>
<figcaption><p><a id="figure16-1">Figure 16-1</a>: Left: When our tensor has one or three channels, we can say that it’s made up of pixels. Right: For tensors with any number of channels, we call each slice through the channels an element.   </p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="431" id="Page_431"/>A network in which the convolution layers play a central role is usually called a <em>convolutional neural network</em>, <em>convnet</em>, or <em>CNN</em>. Sometimes people also say <em>CNN network</em> (an example of “redundant acronym syndrome syndrome” [Memmott 2015]). </p>
<h3 id="h2-500723c16-0001">Detecting Yellow</h3>
<p class="BodyFirst">To kick off our discussion of convolution, let’s consider processing a color image. Each pixel contains three numbers: one each for red, green, and blue. Suppose we want to create a grayscale output that has the same height and width as our color image, but where the amount of white in each pixel corresponds to the amount of yellow in its input pixel. </p>
<p>For simplicity, let’s assume our RGB values are numbers from 0 to 1. Then a pixel that’s pure yellow has red and green values of 1, and a blue value of 0. As the red and green values decrease, or the blue value increases, the pixel’s color shifts away from yellow. </p>
<p>We want to combine each input pixel’s RGB values into a single number from 0 to 1 that represents “yellowness,” which is the output pixel’s value. <a href="#figure16-2" id="figureanchor16-2">Figure 16-2</a> shows one way to do this.</p>
<figure>
<img src="Images/F16002.png" alt="F16002" width="694" height="551"/>
<figcaption><p><a id="figure16-2">Figure 16-2</a>: Representing our yellow detector as a simple neuron</p></figcaption>
</figure>
<p>This sure looks familiar. It has the same structure as an artificial neuron. When we interpret <a href="#figure16-2">Figure 16-2</a> as a neuron, +1, +1, and −1 are the three weights, and the numbers associated with the color values are the three inputs. <a href="#figure16-3" id="figureanchor16-3">Figure 16-3</a> shows how to apply this neuron to any pixel in an image. </p>
<span epub:type="pagebreak" title="432" id="Page_432"/><figure>
<img src="Images/F16003.png" alt="F16003" width="693" height="579"/>
<figcaption><p><a id="figure16-3">Figure 16-3</a>: Applying our neuron in <a href="#figure16-2">Figure 16-2</a> to a pixel in an image </p></figcaption>
</figure>
<p>We can apply this operation to every pixel in the input, creating a single output value for every pixel. The result is a new tensor with the same width and height as the input, but only one channel, as shown in <a href="#figure16-4" id="figureanchor16-4">Figure 16-4</a>.</p>
<figure>
<img src="Images/F16004.png" alt="F16004" width="689" height="369"/>
<figcaption><p><a id="figure16-4">Figure 16-4</a>: Applying our neuron in <a href="#figure16-3">Figure 16-3</a> to each pixel in the input produces an output tensor with the same width and height, but only one channel.</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="433" id="Page_433"/>We often imagine applying the neuron to the upper-left pixel, then moving it one step at a time to the right until we reach the end of the row, then repeating this for the next row, and the next, until we reach the bottom right pixel. We say that we’re <em>sweeping</em> the neuron over the input, or <em>scanning</em> the input. </p>
<p><a href="#figure16-5" id="figureanchor16-5">Figure 16-5</a> shows the result of this process on a picture of a yellow frog. As we intended, the more yellow that’s present in each input pixel, the more white we see in its corresponding output. We say that the neuron is <em>identifying</em> or <em>detecting</em> yellow in the input.</p>
<figure>
<img src="Images/F16005.png" alt="F16005" width="694" height="227"/>
<figcaption><p><a id="figure16-5">Figure 16-5</a>: An application of our yellow-finding operation. The image on the right runs from black to white, depending on the yellowness of the corresponding source pixel in the left image.</p></figcaption>
</figure>
<p>Of course there’s nothing special about yellow. We can build a little neuron to detect any color. When we use a neuron in this way, we often say that it is <em>filtering</em> the input. In this context, the weights are sometimes collectively called the <em>filter values</em> or just the <em>filter</em>. Inheriting language from their mathematical roots, the weights are also called the <em>filter kernel</em> or just the <em>kernel</em>. It’s also common to refer to the entire neuron as a filter. Whether the word <em>filter</em> refers to a neuron, or specifically to its weights, is usually clear from context.</p>
<p>This operation of sweeping the filter over the input corresponds to a mathematical operation called <em>convolution</em> (Oppenheim and Nawab 1996). We say that the right side of <a href="#figure16-5">Figure 16-5</a> is the result of convolution of the color image with the yellow-detecting filter. We also say that we <em>convolve</em> the image with the filter. Sometimes we combine these terms and refer to a filter (whether an entire neuron, or just its weights) as a <em>convolution filter</em>.</p>
<h3 id="h2-500723c16-0002">Weight Sharing</h3>
<p class="BodyFirst">In the last section, we imagined sweeping our neuron over the input image, performing exactly the same operation at every pixel. If we want to go faster, we can create a huge grid of identical neurons and apply them to all the pixels simultaneously. In other words, we process the pixels in parallel. </p>
<p>In this approach, every neuron has identical weights. Rather than repeating the same weights in a separate piece of memory for every neuron, we can imagine that the weights are stored in some shared piece of memory, as in <a href="#figure16-6" id="figureanchor16-6">Figure 16-6</a>. We say that the neurons are <em>weight sharing</em>. </p>
<span epub:type="pagebreak" title="434" id="Page_434"/><figure>
<img src="Images/F16006.png" alt="F16006" width="839" height="511"/>
<figcaption><p><a id="figure16-6">Figure 16-6</a>: We can apply our neuron to every pixel in the input simultaneously. Each neuron uses the same weights, found in a piece of shared memory.</p></figcaption>
</figure>
<p>This lets us save on memory. In our yellow detector example, weight sharing also makes it easy to change the color we’re detecting. Rather than change the weights in thousands of neurons (or more), we just change the one set in the shared memory.</p>
<p>We can actually implement this scheme on a GPU, which is capable of performing many identical sequences of operations at once. Weight sharing lets us save on precious GPU memory, freeing it up for other uses.</p>
<h3 id="h2-500723c16-0003">Larger Filters</h3>
<p class="BodyFirst">So far, we’ve been sweeping our neuron over the image (or applying it in parallel using weight sharing), processing one pixel at a time, using only that pixel’s values for input. In many situations, it’s also useful to look at the pixels near the one we’re processing. Usually we consider a pixel’s eight immediate <em>neighbors</em>. That is, we use the values in a little three by three box that’s centered on the pixel. </p>
<p><a href="#figure16-7" id="figureanchor16-7">Figure 16-7</a> shows three different operations we can apply using a three by three block of numbers in this way: blurring, detecting horizontal edges, and detecting vertical edges. </p>
<p>To compute each image, we center the block of weights over each pixel in turn and multiply each of the nine values under it by the corresponding weight. We add up the results and use their sum as the output value for that pixel. Let’s see how to implement this process with a neuron.</p>
<span epub:type="pagebreak" title="435" id="Page_435"/><figure>
<img src="Images/F16007.png" alt="F16007" width="600" height="204"/>
<figcaption><p><a id="figure16-7">Figure 16-7</a>: Processing a grayscale image of the frog in <a href="#figure16-5">Figure 16-5</a> by moving a three by three template of numbers over the image. From left to right, we blur the image, find horizontal edges, and find vertical edges.</p></figcaption>
</figure>
<p>For simplicity, we’ll stick with a grayscale input for now. We can think of the blocks of numbers in <a href="#figure16-7">Figure 16-7</a> as weights, or filter kernels. In this scenario, we have a grid of nine weights that we place over a grid of nine pixel values. Each pixel value is multiplied by its corresponding weight, the results are summed up and run through an activation function, and we have our output. <a href="#figure16-8" id="figureanchor16-8">Figure 16-8</a> shows the idea.</p>
<figure>
<img src="Images/F16008.png" alt="F16008" width="520" height="631"/>
<figcaption><p><a id="figure16-8">Figure 16-8</a>: Processing a grayscale input (red) with a three by three filter (blue) </p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="436" id="Page_436"/>This figure shows how to process a single pixel (shown in dark red). We center the filter over the intended pixel and multiply each of the nine values in the input with its corresponding filter value. We add up all nine results and pass that sum through an activation function.</p>
<p>The shape of the pixels that form a neuron’s input in this scheme is called that neuron’s <em>local receptive field</em>, or more simply its <em>footprint</em>. In <a href="#figure16-8">Figure 16-8</a>, the neuron’s footprint is a square, three pixels on a side. In our yellow detector, the footprint was a single pixel. When a filter’s footprint is larger than a single pixel, we sometimes emphasize that quality by calling a <em>spatial filter</em>.</p>
<p>Note that the neuron in <a href="#figure16-8">Figure 16-8</a> is just like any other neuron. It receives nine numbers as inputs, multiplies each one by its corresponding weight, adds the results together, and passes that number through an activation function. It doesn’t know or care that these nine numbers are coming from a square region of the input, or even that they’re coming from an image.</p>
<p>We apply this three by three filter to an image by convolving it with the image, just as before, by sweeping it over each pixel in turn. For each input pixel, we imagine centering the three by three grid of weights over that pixel, applying the neuron, and creating a single output value, as in <a href="#figure16-9" id="figureanchor16-9">Figure 16-9</a>. We say that the pixel we’re centering the filter over is the <em>anchor</em> (or the <em>reference point</em> or <em>zero point</em>).</p>
<figure>
<img src="Images/F16009.png" alt="F16009" width="446" height="297"/>
<figcaption><p><a id="figure16-9">Figure 16-9</a>: Applying a three by three filter (center) to a grayscale image (left), creating a new single-channel image (right)</p></figcaption>
</figure>
<p>We can design our filters to have footprints of any size and shape we like. In practice, small sizes are most common, since they are faster to evaluate than larger footprints. We usually use small squares with an odd number of pixels on each side (often between one and nine). Such squares let us place the anchor in the center of the footprint. This keeps everything symmetrical and easier to understand.</p>
<p>Let’s put this into practice. <a href="#figure16-10" id="figureanchor16-10">Figure 16-10</a> shows the result of convolving a seven by seven input with a three by three filter. Note that if we were to center the filter over the input’s corners or edges, the filter’s footprint would extend beyond the input, and the neuron would require input values that aren’t present. We address this a little later. For now, let’s just limit ourselves <span epub:type="pagebreak" title="437" id="Page_437"/>to those locations where the filter sits entirely on top of the image. That means that the output image is only five by five.</p>
<p>We motivated our discussion by looking at spatial filters that can do things like blur an image or detect edges. But why are such things useful for deep learning? To answer this, let’s look at filters more closely.</p>
<figure>
<img src="Images/F16010.png" alt="F16010" width="684" height="604"/>
<figcaption><p><a id="figure16-10">Figure 16-10</a>: To convolve an image with a filter, we move the filter across the image and apply it at each position. We’re skipping corners and edges for this figure.</p></figcaption>
</figure>
<h3 id="h2-500723c16-0004">Filters and Features</h3>
<p class="BodyFirst">Some biologists who study toads think that certain cells in the animal’s visual system are sensitive to specific types of visual patterns (Ewert et al. 1985). The theory is that a toad is looking for particular shapes corresponding to the creatures it likes to eat and to certain motions that those animals make. People used to think that a toad’s eyes absorbed all the light that struck them, sent that mass of information to the brain, and it was the brain’s job to sift among the results looking for food. The new hypothesis is that the cells in the eye are doing some early steps in this detection process (such as finding edges) all by themselves, and they only fire and pass on information to the brain if they “think” they’re looking at prey.</p>
<p>The theory has been extended to the human visual system, where it has led to the surprising hypothesis that some individual neurons are so precisely fine-tuned that they only fire in response to pictures of specific <span epub:type="pagebreak" title="438" id="Page_438"/>people. The original study that led to this suggestion showed people 87 different images, including people, animals, and landmarks. In one volunteer they found a specific neuron that only fired when the volunteer was shown a photo of the actress Jennifer Aniston (Quiroga 2005). Even more curiously, that neuron only fired when Aniston was alone, and not when she was pictured together with other people, including famous actors.</p>
<p>The idea that our neurons are precision pattern-matching devices is not universally accepted, but we’re not doing real neuroscience and biology here. We’re just looking for inspiration. And this idea of letting neurons perform detection work seems like some pretty great inspiration. </p>
<p>The connection to convolution is that we can use filters to simulate the cells in the toad’s eyes. Our filters also pick out specific patterns and then pass on their discoveries to later filters that look for even bigger patterns. Some of the terminology we use for this process echoes terms that we’ve seen before. Specifically, we’ve been using the word <em>feature</em> to refer to one of the values contained in a sample. But in this context, the word <em>feature</em> also refers to a particular structure in an input that the filter is trying to detect, like an edge, a feather, or scaly skin. We say that a filter is <em>looking for</em> a stripe feature, or eyeglasses, or a sports car. Continuing this usage, the filters themselves are sometimes called <em>feature detectors</em>. When a feature detector has been swept over an entire input, we say that its output is a <em>feature map</em> (the word <em>map</em> in this context comes from mathematical language). The feature map tells us, pixel by pixel, how well the image around that pixel matched what the filter was looking for.</p>
<p>Let’s see how feature detection works. In <a href="#figure16-11" id="figureanchor16-11">Figure 16-11</a> we show the process of using a filter to find short, isolated vertical white stripes in a binary image. </p>
<figure>
<img src="Images/F16011.png" alt="F16011" width="694" height="144"/>
<figcaption><p><a id="figure16-11">Figure 16-11</a>: 2D pattern matching with convolution. (a) The filter. (b) The input. (c) The feature map, scaled to [0, 1] for display. (d) Feature map entries with value 3. (e) Neighborhoods of (b) around the white pixels in (d).</p></figcaption>
</figure>
<p><a href="#figure16-11">Figure 16-11</a>(a) shows a three by three filter with values −1 (black) and 1 (white). <a href="#figure16-11">Figure 16-11</a>(b) shows a noisy input image, consisting only of black and white pixels. <a href="#figure16-11">Figure 16-11</a>(c) shows the result of applying the filter to each pixel in the input image (except for the outermost border). Here the values range from −6 to +3, which we scaled to [0, 1] for display. The larger the value in this image, the better the match between the filter and the pixel (and its neighborhood). A value of +3 means the filter matched the image perfectly at that pixel.</p>
<p><a href="#figure16-11">Figure 16-11</a>(d) shows a thresholded version of <a href="#figure16-11">Figure 16-11</a>(c), where pixels with a value of +3 are shown in white, and all others are black. Finally, <a href="#figure16-11">Figure 16-11</a>(e) shows the noisy image of <a href="#figure16-11">Figure 16-11</a>(b) with the three by <span epub:type="pagebreak" title="439" id="Page_439"/>three grid of pixels around the white pixels in <a href="#figure16-11">Figure 16-11</a>(d) highlighted. We can see that the filter found those places in the image where the pixels matched the filter’s pattern.</p>
<p>Let’s see why this worked. In the top row of <a href="#figure16-12" id="figureanchor16-12">Figure 16-12</a> we show our filter and a three by three patch of the image, along with the pixel-by-pixel results. </p>
<figure>
<img src="Images/F16012.png" alt="F16012" width="694" height="314"/>
<figcaption><p><a id="figure16-12">Figure 16-12</a>: Applying a filter to two image fragments. From left to right, each row shows the filter, an input, and the result. The final number is the sum of the rightmost three by three grid.</p></figcaption>
</figure>
<p>Consider the pixels shown in the middle of the top row. The black pixels (shown in gray here), with a value of 0, don’t contribute to the output. The white pixels (shown in light yellow here), with a value of 1, get multiplied by either 1 or –1, depending on the filter value. In the top row of pixels, only one of the white pixels (the top center) is matched by a 1 in the filter. This gives a result of 1 × 1 = 1. The other three white pixels are matched up with −1, giving three results of −1 × 1 = −1. Adding these gives us −3 + 1 = −2.</p>
<p>In the lower row, our image matches the filter. All three weights of 1 on the filter are sitting on white pixels, and there are no other white pixels in the input. The result is a score of 3, indicating a perfect match.</p>
<p><a href="#figure16-13" id="figureanchor16-13">Figure 16-13</a> shows another filter, this time looking for diagonals. Let’s run it over the same image. This diagonal of three white pixels surrounded by black is present in two places.</p>
<figure>
<img src="Images/F16013.png" alt="F16013" width="694" height="144"/>
<figcaption><p><a id="figure16-13">Figure 16-13</a>: Another filter and its result on our random image. (a) The filter. (b) The input. (c) The feature map. (d) Feature map entries with value 3. (e) Neighborhoods of (b) around the white pixels in (d).</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="440" id="Page_440"/>By sweeping a filter over the image and computing the output value at each pixel, we can hunt for lots of different simple patterns. In practice, our filter and pixel values are all real numbers (not just 0 and 1), so we can make much more complex patterns that find more complex features (Snavely 2013).</p>
<p>If we take the output of a set of filters and feed them to another set of filters, we can look for patterns of patterns. If we feed that second set of outputs to a third set of filters, we can look for patterns of patterns of patterns. This process lets us build up from, say, a collection of edges, to a set of shapes, such as ovals and rectangles, to ultimately matching a pattern corresponding to some specific object, such as a guitar or bicycle. </p>
<p>Applying successive groups of filters in this way, in concert with another technique we will soon discuss called <em>pooling</em>, enormously expands the sorts of patterns that we can detect. The reason is that the filters operate <em>hierarchically</em>, where each filter’s patterns are combinations of the patterns found by earlier filters. Such a hierarchy allows us to look for features of great complexity, such as the face of a friend, the grain of a basketball, or the eye on the end of a peacock’s feather.</p>
<p>If we had to work out these filters by hand, classifying images would be impractical. What are the proper weights in a chain of eight filters that tell us if a picture shows a kitten or an airplane? How could we even go about working out that problem? And how would we know when we found the best filters? In Chapter 1 we discussed expert systems, in which people tried to do this kind of feature engineering by hand. It’s a formidable task for simple problems, and it grows in complexity so quickly that really interesting problems, such as distinguishing cats from airplanes, seem entirely out of reach. </p>
<p>The beauty of CNNs is that they carry out the goals of expert systems, but we don’t have to figure out the values of the filters by hand. The learning process that we’ve seen in previous chapters, involving measuring error, backpropagating the gradients, and then improving the weights, teaches a CNN to find the filters it needs. The learning process modifies the kernel of each filter (that is, the weights in each neuron), until the network is producing results that match our targets. In other words, training tunes the values in the filters until they find the features that enable it to come up with the right class for the object in the image. And this can happen for hundreds or even thousands of filters, all at once.</p>
<p>This can seem like magic. Starting with random numbers, the system learns what patterns it needs to look for in order to distinguish a piano from an apricot from an elephant, and then it learns what numbers to put into the filter kernels in order to find those patterns. </p>
<p>That this process can even come close in one situation is remarkable. The fact that it often produces highly accurate results in a vast range of applications is one of the great discoveries in deep learning.</p>
<h3 id="h2-500723c16-0005">Padding</h3>
<p class="BodyFirst">Earlier, we promised to return to the issue of what happens when a convolution filter is centered over an element in a corner or on an edge of an input tensor. Let’s look at that now.</p>
<p><span epub:type="pagebreak" title="441" id="Page_441"/>Suppose that we want to apply a 5 by 5 filter to a 10 by 10 input. If we’re somewhere in the middle of the tensor, as in <a href="#figure16-14" id="figureanchor16-14">Figure 16-14</a>, then our job is easy. We pull out the 25 values from the input, and apply them to the convolution filter.</p>
<figure>
<img src="Images/F16014.png" alt="F16014" width="285" height="269"/>
<figcaption><p><a id="figure16-14">Figure 16-14</a>: A five by five filter located somewhere in the middle of a tensor. The bright red pixel is the anchor, while the lighter ones make up the receptive field.</p></figcaption>
</figure>
<p>But what if we’re on, or near, an edge, as in <a href="#figure16-15" id="figureanchor16-15">Figure 16-15</a>?</p>
<figure>
<img src="Images/F16015.png" alt="F16015" width="309" height="282"/>
<figcaption><p><a id="figure16-15">Figure 16-15</a>: Near the edge, the filter’s receptive field can fall off the side of the input. What values do we use for these missing elements?</p></figcaption>
</figure>
<p>The footprint of the filter is hanging off the edge of the input. There aren’t any input elements there. How do we compute an output value for the filter when it’s missing some of its inputs?</p>
<p>We have a few choices. One is to disallow this case so we can only place the footprint where it is entirely within the input image. The result is an output that’s smaller in height and width. <a href="#figure16-16" id="figureanchor16-16">Figure 16-16</a> shows this idea.</p>
<p>While simple, this is a lousy solution. We said that we often apply many filters in sequence. If we sacrificed one or more rings of elements each time, we would lose information with every step we take through the network.</p>
<span epub:type="pagebreak" title="442" id="Page_442"/><figure>
<img src="Images/F16016.png" alt="F16016" width="684" height="299"/>
<figcaption><p><a id="figure16-16">Figure 16-16</a>: We can avoid the “falling off the edge” problem by never letting our filter get that far. With a 5 by 5 filter, we can only center the filter over the elements marked here in blue, reducing our 10 by 10 input to a 6 by 6 output. </p></figcaption>
</figure>
<p>A popular alternative is to use a technique called <em>padding, </em>which lets us create an output image of the same width and height as the input. The idea is that we add a border of extra elements around the outside of the input, as in <a href="#figure16-17" id="figureanchor16-17">Figure 16-17</a>. All of these elements have the same value. If we place zeros in all the new elements, we call the technique <em>zero-padding</em>. In practice, we almost always use zeros, so people often refer to zero-padding as merely padding, with the understanding that if they mean to use any value other than zero, they say so explicitly.</p>
<figure>
<img src="Images/F16017.png" alt="F16017" width="370" height="365"/>
<figcaption><p><a id="figure16-17">Figure 16-17</a>: A better way to solve the “falling off the edge” problem is to add padding, or extra elements (in light blue), around the border of the input. </p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="443" id="Page_443"/>The thickness of the border depends on the size of the filter. We usually use just enough padding so that the filter can be centered on every element of the input. Every filter needs to have its input padded if we don’t want to lose information from the sides.</p>
<p>Most deep learning libraries automatically calculate the necessary amount of padding so that our output has the same width and height as our input, and apply it for us as a default. </p>
<h2 id="h1-500723c16-0002">Multidimensional Convolution</h2>
<p class="BodyFirst">So far in this chapter, we’ve mostly been considering grayscale images with only one channel of color information. We know that most color images have three channels, representing the red, green, and blue components of each pixel. Let’s see how to handle those. Once we can work with images with three channels, we can work with tensors of any number of channels.</p>
<p>To process an input with multiple channels, our filters (which can have any footprint) need to have an identical number of channels. That’s because each value in the input needs to have a corresponding value in the filter. For an RGB image, a filter needs three channels. So, a filter with a footprint of three by three needs to have three channels, for a total of 27 numbers, as shown in <a href="#figure16-18" id="figureanchor16-18">Figure 16-18</a>.</p>
<figure>
<img src="Images/F16018.png" alt="F16018" width="146" height="167"/>
<figcaption><p><a id="figure16-18">Figure 16-18</a>: A three-channel filter with a three by three footprint. We’ve colored the values to show which input channel’s values they will multiply.</p></figcaption>
</figure>
<p>To apply this kernel to a three-channel color image, we proceed much as before, but now we think in terms of blocks (or tensors of three dimensions).</p>
<p>Let’s take the filter of <a href="#figure16-18">Figure 16-18</a>, with a three by three footprint and three channels, and use it to process an RGB image with three color channels. For each input pixel, we center the filter’s footprint over that pixel as before, and match up each of the 27 numbers in the image with the 27 numbers in the filter, as in <a href="#figure16-19" id="figureanchor16-19">Figure 16-19</a>.</p>
<span epub:type="pagebreak" title="444" id="Page_444"/><figure>
<img src="Images/F16019.png" alt="F16019" width="656" height="527"/>
<figcaption><p><a id="figure16-19">Figure 16-19</a>: Convolving an RGB image with a three by three by three kernel. We can imagine that each channel is filtered by its own channel in the filter.</p></figcaption>
</figure>
<p>In <a href="#figure16-19">Figure 16-19</a>, our input has three channels, so our filter has three channels as well. It may be helpful to think of the red, green, and blue channels as each getting filtered by their corresponding channel in the filter, as shown in <a href="#figure16-19">Figure 16-19</a>. In practice, we treat the input and the filter as three by three by three blocks, and each of the 27 input values get multiplied with its corresponding filter value.</p>
<p>This idea generalizes to any number of channels. In order to make sure that every input value has a corresponding filter value, we can state the necessary property as a rule: every filter must have the same number of channels as the tensor it’s filtering. </p>
<h2 id="h1-500723c16-0003">Multiple Filters</h2>
<p class="BodyFirst">We’ve been applying a single filter at a time, but that’s rare in practice. Usually we bundle up tens or hundreds of filters into one <em>convolution layer</em> and apply them all simultaneously (and independently) to that layer’s input.</p>
<p>To see the general picture, imagine that we’ve been given a black-and-white image, and we want to look for several low-level features in the pixels, such as vertical stripes, horizontal stripes, isolated dots, and plus signs. We can create one filter for each of these features and run each one over the input independently. Each filter produces an output image with one channel. Combining the four outputs gives us one tensor with four channels. <a href="#figure16-20" id="figureanchor16-20">Figure 16-20</a> shows the idea.</p>
<span epub:type="pagebreak" title="445" id="Page_445"/><figure>
<img src="Images/F16020.png" alt="F16020" width="694" height="325"/>
<figcaption><p><a id="figure16-20">Figure 16-20</a>: We can run multiple filters (in color) over the same input (in gray). Each filter creates its own channel in the output. They are then combined to create a single element in the output tensor with four channels. </p></figcaption>
</figure>
<p>Instead of a grayscale image with one channel, or a color image with three channels, we now have an output tensor with four channels. If we used seven filters, then the output is a new image with seven channels. The key thing to note here is that the output tensor has one channel for each filter that’s applied.</p>
<p>Generally speaking, our filters can have any footprint, and we can apply as many of them as we like to any input image. <a href="#figure16-21" id="figureanchor16-21">Figure 16-21</a> shows this idea. </p>
<figure>
<img src="Images/F16021.png" alt="F16021" width="844" height="391"/>
<figcaption><p><a id="figure16-21">Figure 16-21</a>: When we convolve filters with an input, each filter must have as many channels as the input. The output tensor has one channel for each filter.</p></figcaption>
</figure>
<p>The input tensor at the far left has seven channels. We’re applying four different filters, each with a three by three footprint, so each filter is a <span epub:type="pagebreak" title="446" id="Page_446"/>tensor of size three by three by seven. The output of each filter is a feature map of a single channel. The output tensor is what we get from stacking these four feature maps, so it has four channels.</p>
<p>Although in principle each filter we apply can have a different footprint, in practice we almost always use the same footprint for every filter in any given convolution layer. For example, in <a href="#figure16-21">Figure 16-21</a> all the filters have a footprint of three by three. </p>
<p>Let’s gather together the two numerical rules from the previous section and this one. First, every filter in a convolution layer must have the same number of channels as that layer’s input tensor. Second, a convolution layer’s output tensor will have as many channels as there are filters in the layer. </p>
<h2 id="h1-500723c16-0004">Convolution Layers</h2>
<p class="BodyFirst">Let’s take a closer look at the mechanics of convolution layers. A convolution layer is simply a bunch of filters gathered together. They’re applied independently to the input tensor, as in <a href="#figure16-21">Figure 16-21</a>, and their outputs are combined to create a new output tensor. The input is not changed by this process.</p>
<p>When we create a convolution layer in code, we typically tell our library how many filters we want, what their footprint should be, and other optional details like whether we want to use padding and what activation function we want to use—the library takes care of all the rest. Most importantly, training improves the kernel values in each filter, so that the filters learn the values that enable them to produce the best results.</p>
<p>When we draw a diagram of a deep learner, we usually label our convolution layers with how many filters are used, their footprints, and their activation function. Since it’s common to use the same padding all around the input, we often just provide a single value rather than two, with the understanding that it applies to both width and height. </p>
<p>Like the weights in fully connected layers, the values in a convolution layer’s filters start out with random values and are improved with training. Also like fully connected layers, if we’re careful about choosing these random initial values, training usually goes faster. Most libraries offer a variety of initialization methods. Generally speaking, the built-in defaults normally work fine, and we rarely need to explicitly choose an initialization algorithm.</p>
<p>If we do want to pick a method, the He algorithm is a good first choice (He et al. 2015; Karpathy 2016). If that’s not available, or doesn’t work well in a given situation, Glorot is a good second choice (Glorot and Bengio 2010).</p>
<p>Let’s look at a couple of special types of convolution that have their own names.</p>
<h3 id="h2-500723c16-0006">1D Convolution</h3>
<p class="BodyFirst">An interesting special case of sweeping a filter over an input is called <em>1D convolution</em>. Here we sweep over the input as usual in either height or width, but not the other (Snavely 2013). This is a popular technique when <span epub:type="pagebreak" title="447" id="Page_447"/>working with text, which can be represented as a grid where each element holds a single letter, and rows contain complete words (or a fixed number of letters) (Britz 2015).</p>
<p>The basic idea is shown in <a href="#figure16-22" id="figureanchor16-22">Figure 16-22</a>. </p>
<figure>
<img src="Images/F16022.png" alt="F16022" width="844" height="147"/>
<figcaption><p><a id="figure16-22">Figure 16-22</a>: An example of 1D convolution. The filter only moves downward.</p></figcaption>
</figure>
<p>Here, we’ve created a filter that is the entire width of the input and two rows high. The first application of the filter processes everything in the first two rows. Then we move the filter down and process the next two rows. We don’t move the filter horizontally. The name <em>1D convolution</em> comes from this single direction, or dimension, of movement.</p>
<p>As always, we can have multiple filters sliding down the grid. We can perform 1D convolution on an input tensor of any number of dimensions, as long as the filter itself moves in just one dimension. There’s nothing otherwise special about 1D convolution: it’s just a filter that only moves in one direction. The technique has its own name to emphasize the filter’s limited mobility. </p>
<p>The name 1D convolution is almost the same as the name of another, quite different, technique. Let’s look at that now.</p>
<h3 id="h2-500723c16-0007">1×1 Convolutions </h3>
<p class="BodyFirst">Sometimes we want to reduce the number of channels in a tensor as it flows through a network. Often this is because we think that some of the channels contain redundant information. This isn’t uncommon. For example, suppose we have a classifier that identifies the dominant object in a photograph. The classifier might have a dozen or more filters that look for eyes of different sorts: human eyes, cat eyes, fish eyes, and so on. If our classifier is going to ultimately lump all living things together into one class called “living things,” then there’s no need to care about which kind of eye we find. It’s enough just to know that a particular region in the input image has an eye.</p>
<p>Suppose that we have a layer containing filters that detect 12 different kinds of eyes. Then the output tensor from that layer will have at least 12 channels, one from each filter. If we only care about whether or not an eye is found, then it would be useful to modify that tensor by combining, or compressing, those 12 channels into just 1 channel representing whether or not an eye is found at each location.</p>
<p>This doesn’t require anything new. We want to process one input element at a time, so we create a filter with a footprint of one by one, like we saw in <a href="#figure16-6">Figure 16-6</a>. We make sure that we have at least 11 fewer filters than <span epub:type="pagebreak" title="448" id="Page_448"/>there are input channels. The result is a tensor of the same width and height as the input, but the multiple eye channels get crunched together into just one channel.</p>
<p>We don’t have to do anything explicit to make this happen. The network learns weights for the filters such that the network produces the correct output for each input. If that means combining all the channels for eyes, then the network learns to do that.</p>
<p><a href="#figure16-23" id="figureanchor16-23">Figure 16-23</a> shows how to use these filters to compress a tensor with 300 channels into a new tensor of the same width and height, but with only 175 channels.</p>
<figure>
<img src="Images/F16023.png" alt="F16023" width="838" height="367"/>
<figcaption><p><a id="figure16-23">Figure 16-23</a>: Applying 1×1 convolution to perform feature reduction</p></figcaption>
</figure>
<p>The technique of using one by one filters has been given its own name. We say that we apply a <em>one by one filter</em>, often written as a <em>1×1 filter</em>, and use that to perform <em>1×1 convolution</em> (Lin, Chen, and Yan 2014).</p>
<p>In Chapter 10 we talked about the value of preprocessing our input data in order to save processing time and memory. Rather than perform this processing once, before the data has entered our system, 1×1 convolution lets us apply this compression and restructuring of the data on the fly, inside of the network. If our network produces information that can be compressed or removed entirely, then 1×1 convolutions can find and then compress or remove that data. We can do this anywhere, even in the middle of a network.</p>
<p>When the channels are correlated, 1×1 convolution is particularly effective (Canziani, Paszke, and Culurciello 2016; Culurciello 2017). This means that the filters on the previous layers have created results that are in sync with one another, so that when one goes up, we can predict by how much the others will go up or down. The better this correlation, the more likely it is that we can remove some of the channels and suffer little to no loss of information. The 1×1 filters are perfect for this job.</p>
<p><span epub:type="pagebreak" title="449" id="Page_449"/>The term <em>1×1 convolution</em> is uncomfortably close to <em>1D convolution</em>, which we discussed in the last section. But these names refer to quite distinct techniques. When encountering either of these terms, it is worth taking a moment to make sure we have the correct idea in mind.</p>
<h2 id="h1-500723c16-0005">Changing Output Size </h2>
<p class="BodyFirst">We’ve just seen how to change the number of channels in a tensor by using 1×1 convolution. We can also change the width and height, which is useful for at least two reasons. The first is that if we can make the data flowing through our network smaller, we can use a simpler network and save time, computing resources, and energy. The second is that reducing the width and height can make some operations, like classification, more efficient and even more accurate. Let’s see why this is so. </p>
<h3 id="h2-500723c16-0008">Pooling</h3>
<p class="BodyFirst">In previous sections, we applied each filter to one pixel, or one region of pixels. The filter matches the feature it’s looking for if the underlying pixels match the filter’s values. But what if some of the elements of the feature are in slightly wrong places? Then the filter won’t match. There’s no way for the filter to look around and report a match if one or more pieces of the pattern it’s looking for are present but slightly out of position. This would be a real problem if we didn’t address it. For example, suppose we’re looking for a capital T on a page of text. Due to a minor mechanical error during printing, a column of pixels was displaced downward by one pixel. </p>
<p>We still want to find the T. The situation is illustrated in <a href="#figure16-24" id="figureanchor16-24">Figure 16-24</a>. </p>
<figure>
<img src="Images/F16024.png" alt="F16024" width="691" height="190"/>
<figcaption><p><a id="figure16-24">Figure 16-24</a>: From left to right: A five by five filter looking for a letter T, a misprinted T, the filter on top of the image, and the filter’s resulting values. The filter would not report a match to the letter T.</p></figcaption>
</figure>
<p>We begin with a five by five filter that is looking for a T in the center. We illustrate this using blue for 1 and yellow for 0. </p>
<p>We’ve labeled this the “perfect filter,” a name that will make sense in a moment. To its right is the misprinted text we’re going to examine, labeled “perfect image.” To the right of that, we overlay the filter on the image. At the far right is the result. Only when the filter and the input are both blue <span epub:type="pagebreak" title="450" id="Page_450"/>will the output be blue. Since the filter’s upper-right element did not find the blue pixel it was expecting, the filter as a whole reports either no match, or a weak one.</p>
<p>If the upper-right element in the filter could look around and notice the blue pixel just below it, it could match the input. One way to make this happen is to let each filter element “see” more of the input. The most convenient way to do that mathematically is to make the filter a bit blurry.</p>
<p>On the top row of <a href="#figure16-25" id="figureanchor16-25">Figure 16-25</a> we picked out one element of the filter and blurred it. If the filter finds a blue pixel anywhere in this larger, blurry region, it reports finding blue. If we do this for all the entries in the filter, we create a “blurry filter.” Thanks to this extended reach, the upper-right blue filter element now overlaps two blue pixels, and since the other blue elements also overlap blue pixels, the filter now reports a match. </p>
<figure>
<img src="Images/F16025.png" alt="F16025" width="727" height="444"/>
<figcaption><p><a id="figure16-25">Figure 16-25</a>: Top row: Replacing a filter element with a bigger, blurrier version. Bottom row: Applying the blur to every filter element gives us a blurry filter. Applying this to the image matches the misprinted T.</p></figcaption>
</figure>
<p>Unfortunately, we can’t blur filters like this. If we modified our filter values by blurring them, our training process would go haywire, since we would be altering the very values we’re trying to learn. But there’s nothing stopping us from blurring the input! This is particularly easy to see if the input is a picture, but we can blur any tensor. So rather than applying a blurry filter to a perfect input, let’s flip that around and apply a perfect filter to a blurry input.</p>
<p>The top row of <a href="#figure16-26" id="figureanchor16-26">Figure 16-26</a> shows a single pixel from the misprinted T, and the version of that pixel after it’s been blurred. After we apply this blurring to all the pixels, we can apply the perfect filter to this blurry image. Now every blue dot in the filter sees blue under it. Success!</p>
<span epub:type="pagebreak" title="451" id="Page_451"/><figure>
<img src="Images/F16026.png" alt="F16026" width="670" height="467"/>
<figcaption><p><a id="figure16-26">Figure 16-26</a>: Top row: The effect of blurring one pixel in the input. Bottom row: We apply the perfect filter to a blurred version of the image. This matches the misprinted T. </p></figcaption>
</figure>
<p>Taking this as our inspiration, we can come up with a technique to blur a tensor. We call the method <em>pooling</em>, or<em> downsampling.</em> Let’s see how pooling works numerically with a small tensor with a single channel. Suppose we start with a tensor that has a width and height of four, as shown in <a href="#figure16-27" id="figureanchor16-27">Figure 16-27</a>(a).</p>
<figure>
<img src="Images/F16027.png" alt="F16027" width="687" height="186"/>
<figcaption><p><a id="figure16-27">Figure 16-27</a>: Pooling, or downsampling, a tensor. (a) Our input tensor. (b) Subdividing (a) into two by two blocks. (c) The result of average pooling. (d) The result of max pooling. (e) Our icon for a pooling layer. </p></figcaption>
</figure>
<p>Let’s subdivide the width and height of this tensor into two by two blocks, as in <a href="#figure16-27">Figure 16-27</a>(b). To blur the input tensor, recall <a href="#figure16-7">Figure 16-7</a>. We saw that by convolving with a filter whose contents are all 1’s, the image got blurry. Such a filter is called a <em>low-pass filter</em>, or more specifically, a <em>box filter.</em> </p>
<p>To apply a box filter to a tensor, we can use a two by two filter where every weight is a 1. Applying this filter merely means adding up the four <span epub:type="pagebreak" title="452" id="Page_452"/>numbers in each two by two block. Because we don’t want our numbers to grow without bound, we divide the result by four to get the average value in that block. Since this average now stands in for the entire block, we save it just once. We do the same thing for the other three blocks. The result is a new tensor of size two by two, shown in <a href="#figure16-27">Figure 16-27</a>(c). This technique is called <em>average pooling.</em></p>
<p>There’s a variation on this method: instead of computing the average value, we just use the largest value in each block. This is called <em>maximum pooling </em>(or more often, just<em> max pooling)</em>, and is shown in <a href="#figure16-27">Figure 16-27</a>(d). It’s common to think of these pooling operations as being carried out by a little utility layer. In <a href="#figure16-27">Figure 16-27</a>(e) we show our icon for such a <em>pooling layer</em>. Experience has shown that networks that use max pooling learn more quickly than those using average pooling, so when people speak of pooling with no other qualifiers, they usually mean max pooling. </p>
<p>The power of pooling appears when we apply multiple convolution layers in succession. Just as with a filter and a blurred input, if the first filter’s values aren’t in quite the expected locations, pooling helps the second layer’s filter still find them. For example, suppose that we have two layers in succession, and Layer 2 has a filter that is looking for a strong match from Layer 1, directly above a match of about half that value (maybe this is characteristic of a particular animal’s coloration). Nothing in the original 4 by 4 tensor in <a href="#figure16-27">Figure 16-27</a>(a) fits that pattern. There’s a 20 over a 2, but the 2 isn’t close to being half of 20. And there’s a 6 over 3, but 6 isn’t a very strong output. So Layer 2’s filter would fail to find what it was looking for. That’s too bad, because there is a 20 that’s close to being over a 9, which is what the filter wants to find. The problem is that the 20 and the 9 are not exactly vertical neighbors.</p>
<p>But the max pooling version has the 20 over the 9. The pooling operation is communicating to Layer 2 that there is a strong match of 20 somewhere in the upper right two by two block, and a match of 9 somewhere in the block directly below the 20. That’s the pattern we’re looking for, and the filter will tell us that it found a match.</p>
<p>We’ve discussed pooling for just one channel. When our tensors have multiple channels, we apply the same process to each channel. <a href="#figure16-28" id="figureanchor16-28">Figure 16-28</a> shows the idea.</p>
<p>We start with an input tensor of height and width 6 and one channel, padded with a ring of zeros. The convolution layer applies three filters, each producing a feature map of six by six. The output of the convolution layer is a tensor of size six by six by three. The pooling layer then conceptually considers each channel of this tensor, and applies max pooling to it, reducing each feature map to three by three. Those feature maps are then combined as before to produce an output tensor of width and height 3, with three channels.</p>
<span epub:type="pagebreak" title="453" id="Page_453"/><figure>
<img src="Images/F16028.png" alt="F16028" width="845" height="470"/>
<figcaption><p><a id="figure16-28">Figure 16-28</a>: Pooling, or downsampling, with multiple filters</p></figcaption>
</figure>
<p>We’ve been using binary images and filters as examples. This means that a feature that straddles cell boundaries could be missed, or wind up in the wrong element in the pooled tensor. When we use real valued inputs and filter kernels, this problem is greatly reduced.</p>
<p>Pooling is a powerful operation that frees filters from requiring their inputs to be in precisely the right place. Mathematicians refer to a change in location as <em>translation</em> or <em>shift</em>, and if some operation is insensitive to a certain kind of change it’s called <em>invariant</em> with respect to that operation. Combining these, we sometimes say that pooling allows our convolutions to be <em>translationally invariant</em>, or <em>shift invariant</em> (Zhang 2019).</p>
<p>Pooling also has the bonus benefit of reducing the size of the tensors flowing through our network, which reduces both memory needs and execution time. </p>
<h3 id="h2-500723c16-0009">Striding</h3>
<p class="BodyFirst">We’ve seen how useful pooling is in a convolutional network. Though pooling layers are common, we can save time by bundling the pooling step right into the convolution process. This combined operation is much faster than two distinct layers. The tensors resulting from the two procedures usually contain different values, but experience has shown that the faster, combined operation usually produces results that are just as useful as the slower, sequential operations.</p>
<p>As we saw, during convolution we can imagine starting the filter in the upper-left pixel of the input image (let’s assume we have padding). The filter produces an output, then takes one step right, produces another output, <span epub:type="pagebreak" title="454" id="Page_454"/>moves another step right, and so on until it reaches the right edge of that row. Then it moves down one row and back to the left side, and the process repeats.</p>
<p>But we don’t have to move in single steps. Suppose we move, or <em>stride</em>, more than one pixel to the right, or more than one pixel down, as we sweep our filter. Then our output will end up being smaller than the input. We usually use the word <em>stride</em> (and the related <em>striding</em>) only when we use steps greater than one in any dimension. </p>
<p>To visualize striding, let’s see how the filter moves starting from the upper left. As the filter moves left to right, it produces a sequence of outputs, and those get placed one after the other, also left to right, in the output. When the filter moves down, the new outputs go on a new line of cells in the output.</p>
<p>Now suppose that instead of moving the filter to the right by one element on each horizontal step, we moved to the right by three elements. And perhaps on each vertical step we move down by two rows, rather than one. We still grow the output by one element for each output. The idea is shown in <a href="#figure16-29" id="figureanchor16-29">Figure 16-29</a>.</p>
<figure>
<img src="Images/F16029.png" alt="F16029" width="681" height="542"/>
<figcaption><p><a id="figure16-29">Figure 16-29</a>: Our input scanning can skip over input elements as it moves. Here we move three elements to the right on each horizontal step, and two elements down on each vertical step.</p></figcaption>
</figure>
<p>In <a href="#figure16-29">Figure 16-29</a> we used a stride of three horizontally, and a stride of two vertically. More often we specify a single stride value for both axes. A stride of two on both axes can be thought of as evaluating every other pixel <span epub:type="pagebreak" title="455" id="Page_455"/>both horizontally and vertically. This results in an output that has half the input dimensions as the input, which means the output has the same dimensions as striding by one and then pooling with two by two blocks. <a href="#figure16-30" id="figureanchor16-30">Figure 16-30</a> shows where the filter lands in the input for a couple of different pairs of strides. </p>
<figure>
<img src="Images/F16030.png" alt="F16030" width="512" height="313"/>
<figcaption><p><a id="figure16-30">Figure 16-30</a>: Examples of striding. (a) A stride of two in both directions means centering the filter over every other pixel, both horizontally and vertically. (b) A stride of three in both directions means centering over every third pixel.</p></figcaption>
</figure>
<p>When we move by one element on every step, a filter with a three by three footprint processes the same input elements multiple times. When we stride by larger amounts, our filter can still process some elements multiple times, as shown in <a href="#figure16-31" id="figureanchor16-31">Figure 16-31</a>.</p>
<figure>
<img src="Images/F16031.png" alt="F16031" width="600" height="383"/>
<figcaption><p><a id="figure16-31">Figure 16-31</a>: This three by three filter is moving with a stride of two in each dimension, reading left to right, top to bottom. The gray elements show what’s been processed so far. The green elements are those that have already been used by the filter on previous evaluations but are being used again.</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="456" id="Page_456"/>There’s nothing wrong with reusing an input value repeatedly, but if we’re trying to save time, we might want to do as little computation as possible. Then we can use striding to prevent any input element from being used more than once. For instance, if we’re moving a three by three filter over an image, we might use a stride of three in both directions, so that no pixel gets used more than once, as in <a href="#figure16-32" id="figureanchor16-32">Figure 16-32</a>.</p>
<figure>
<img src="Images/F16032.png" alt="F16032" width="597" height="364"/>
<figcaption><p><a id="figure16-32">Figure 16-32</a>: Like <a href="#figure16-31">Figure 16-31</a>, only now we’re striding by three in each dimension. Every input element is processed exactly one time.</p></figcaption>
</figure>
<p>The striding in <a href="#figure16-32">Figure 16-32</a> produces an output tensor with a height and width that are each one-third of the input tensor’s height and width. Consider that in <a href="#figure16-32">Figure 16-32</a> we processed a nine by six block of input elements with just six filter evaluations. By doing this, we created a three by two block of outputs with no explicit pooling. If we don’t stride, and then pool, we need many more filter evaluations to cover the same region, and then we need to run the pooling operation on the filter outputs. </p>
<p>Strided convolutions are faster than convolution without striding followed by pooling for two reasons. First, we evaluate the filter fewer times, and second, we don’t have an explicit pooling step to compute. Like padding, striding can (and often is) carried out on any convolutional layer, not just the first.</p>
<p>The filters learned from striding are usually different than those learned from convolution without striding followed by pooling. This means we can’t take a trained network and replace pairs of convolution and pooling with strided convolution (or vice versa) and expect things to still work properly. If we want to change our network’s architecture, we have to retrain it.</p>
<p>Most of the time, training with strided convolution gives us final results that are roughly the same as those we get from convolution followed by pooling, delivered in less time. But sometimes the slower combination works better for a given dataset and architecture.</p>
<h3 id="h2-500723c16-0010"><span epub:type="pagebreak" title="457" id="Page_457"/>Transposed Convolution</h3>
<p class="BodyFirst">We’ve seen how to reduce the size of the input, or <em>downsize</em> it, using either pooling or striding. We can also increase the size of the input, or <em>upsize</em> it. As with downsizing, when we upsize a tensor, we increase its width and height, but we don’t change the number of channels. </p>
<p>Just as with downsampling, we can upsize with a separate layer or build it into the convolution layer. A distinct upsampling layer usually just repeats the input tensor values as many times as we request. For example, if we upsample a tensor by two in both the width and height, each input element turns into a little two by two square. <a href="#figure16-33" id="figureanchor16-33">Figure 16-33</a> shows the idea.</p>
<figure>
<img src="Images/F16033.png" alt="F16033" width="694" height="273"/>
<figcaption><p><a id="figure16-33">Figure 16-33</a>: Upsampling a tensor by two in each direction. Left: The input tensor. Each element of this tensor is repeated twice vertically and horizontally. Right: The output tensor. The number of channels is unchanged.</p></figcaption>
</figure>
<p>We have seen that we can combine downsampling with convolution by using striding. We can also combine upsampling with convolution. This combined step is called <em>transposed convolution</em>, <em>fractional striding</em>, <em>dilated convolution</em>, or <em>atrous convolution. </em>The word <em>transposed</em> comes from the mathematical operation of transposition, which we can use to write the equation for this operation. The word <em>atrous</em> is French for “with holes.” We’ll see where that term, and the others, come from in a moment. Note that some authors refer to the combination of upsampling and convolution as <em>deconvolution</em>, but it’s best to avoid that term, since it’s already in use and refers to a different idea (Zeiler et al. 2010). Following current practice, we’ll use the term <em>transposed convolution</em>.</p>
<p>Let’s see how transposed convolution works to enlarge a tensor (Dumoulin and Visin 2016). Suppose that we have a starting image of width and height three by three (remember, the number of channels won’t be changing), and we’d like to process it with a three by three filter, but we’d like to end up with a five by five image. One approach is to pad the input with two rings of zeros, as in <a href="#figure16-34" id="figureanchor16-34">Figure 16-34</a>.</p>
<span epub:type="pagebreak" title="458" id="Page_458"/><figure>
<img src="Images/F16034.png" alt="F16034" width="844" height="705"/>
<figcaption><p><a id="figure16-34">Figure 16-34</a>: Our original three by three input is shown in white in the outer grids, padded with two elements of zeros all around. The three by three filter now produces a five by five result, shown in the center.</p></figcaption>
</figure>
<p>If we add more rings of zeros to the input, we get larger outputs, but they will produce rings of zeros around the central five by five core. That’s not very useful.</p>
<p>An alternative way to enlarge the input is to spread it out before convolving by inserting padding both around and <em>between </em>the input elements. Let’s try this out. Let’s insert a single row and column of zeros between each element of our starting three by three image, and pad all of that with two rings of zeros around the outside, like before. The result is that our three by three input now has dimensions nine by nine, though a lot of those entries are zero. When we sweep our three by three filter over this grid, we get a seven by seven output, as shown in <a href="#figure16-35" id="figureanchor16-35">Figure 16-35</a>.</p>
<p>Our original three by three image is shown in the outer grids with white pixels. We’ve inserted a row and column of zeros (blue) between each <span epub:type="pagebreak" title="459" id="Page_459"/>pixel, and then surrounded the whole thing with two rings of zeros. When we convolve our three by three filter (red) with this grid, we get a seven by seven result, shown in the center.</p>
<figure>
<img src="Images/F16035.png" alt="F16035" width="844" height="806"/>
<figcaption><p><a id="figure16-35">Figure 16-35</a>: Transposed convolution, convolving a three by three filter into a seven by seven result</p></figcaption>
</figure>
<p><a href="#figure16-35">Figure 16-35</a> suggests where the names <em>atrous</em> (French for “with holes”) <em>convolution</em> and<em> dilated convolution</em> come from. We can make our output even bigger by inserting another row and column between each original input element, as in <a href="#figure16-36" id="figureanchor16-36">Figure 16-36</a>. Now our 3 by 3 input has become an 11 by 11 input, and the output is 9 by 9.</p>
<span epub:type="pagebreak" title="460" id="Page_460"/><figure>
<img src="Images/F16036.png" alt="F16036" width="844" height="811"/>
<figcaption><p><a id="figure16-36">Figure 16-36</a>: The same setup as <a href="#figure16-35">Figure 16-35</a>, only now we have two rows and columns between our original input pixels, producing the nine by nine result in the center</p></figcaption>
</figure>
<p>We can’t push this technique any further without producing rows and columns of zeros in the output. The limit of two rows or columns of zeros is due to our filter having a footprint of three by three. If the filter was, say, five by five, we could use up to four rows and columns of zeros. This technique of inserting zeros can create little checkerboard-like artifacts in the output tensors. But library routines can usually avoid these if they take steps to handle the convolution and upsampling carefully (Odena, Dumoulin, and Olah 2018; Aitken et al. 2017).</p>
<p>There is a connection between transposed convolution and striding. With some imagination, we can describe a transposed convolution process like that of <a href="#figure16-36">Figure 16-36</a> as using a stride of one-third in each dimension. We don’t mean that we literally move one-third of an element, but rather <span epub:type="pagebreak" title="461" id="Page_461"/>that we need to take three steps in the 11 by 11 grid to move the equivalent of one step in the original 3 by 3 input. This point of view explains why the method is sometimes called <em>fractional striding</em>.</p>
<p>Just as striding combines convolution with a downsampling (or pooling) step, transposed convolution (or fractional striding) combines convolution with an upsampling step. This results in faster execution time, which is always nice. A problem is that there is a limit to how much we can increase the input size. In practice, we commonly double the input dimensions, and use filters with a footprint of three by three, and transposed convolution supports that combination without introducing extraneous zeros in the output.</p>
<p>As with striding, the output of transposed convolution is different than the output of upsampling followed by standard convolution, so if we’re given a trained network using upsampling followed by convolution, we can’t just replace those two layers with one transposed convolution layer and use the same filters. </p>
<p>Transposed convolution is becoming more common than upsampling followed by convolution because of the increased efficiency, and similarity of the results (Springenberg et al. 2015).</p>
<p>We’ve covered a lot of basic tools, from different types of convolution to padding and changing the output size. In the next section, we put these all together to create a complete, but simple, convolutional network. </p>
<h2 id="h1-500723c16-0006">Hierarchies of Filters</h2>
<p class="BodyFirst">Many real visual systems seem to be arranged <em>hierarchically</em> (Serre 2014). In broad terms, many biologists think of the processing in the visual system as taking place in a series of layers, with each successive layer working at a higher level of abstraction than the one before. </p>
<p>We’ve taken inspiration from biology already in this chapter, and we can do it again now. </p>
<p>Let’s return to our discussion of the visual system of a toad. The first layer of cells to receive light may be looking for “bug-colored blobs,” the next may be looking for “combinations of blobs from the previous layer that form bug-like shapes,” the next may be looking for “combinations of bug-like shapes from the previous layer that look like a thorax with wings,” and so on, up to the top layer, which looks for “flies” (these features are completely imaginary, and only meant to illustrate the idea).</p>
<p>This approach is nice conceptually because it lets us structure our analysis of an image in terms of a hierarchy of image features and the filters that look for them. It’s also nice for implementations because it’s a flexible and efficient way to analyze an image.</p>
<h3 id="h2-500723c16-0011">Simplifying Assumptions</h3>
<p class="BodyFirst">To illustrate the use of hierarchies, let’s solve a recognition problem with a convolutional network. To focus this discussion just on the concepts, we’ll <span epub:type="pagebreak" title="462" id="Page_462"/>make use of some simplifications. These simplifications in no way change the principles we’re demonstrating; they just make the pictures easier to draw and interpret.</p>
<p>First, we restrict ourselves to binary images: just black and white, with no shades of gray (though for clarity, we draw them with beige and green for 0 and 1, respectively). In real applications, each channel in our input images is usually either an integer in the range [0, 255], or more commonly a real number in the range [0, 1].</p>
<p>Second, our filters are also binary and look for exact matches in their inputs. In real networks, our filters use real numbers, and they match their inputs to different degrees, represented by different real numbers at their output.</p>
<p>Third, we hand-create all of our filters. In other words, we do our own feature engineering. When we looked at expert systems, we said that their biggest problem was that they required people to manually build features, and here we are, doing just that! We’re doing so just for this discussion, however. In practice, our filter values are learned by training. Since we’re not interested in the training step right now, we’ll use handmade filters (we can think of them as filters that resulted from training).</p>
<p>Fourth, we won’t use padding. This also is just to keep things simple.</p>
<p>Finally, our example uses tiny input images that are just 12 pixels on a side. This is large enough to demonstrate the ideas but small enough that we can draw everything clearly on the page.</p>
<p>With these simplifications in place, we’re ready to get started. </p>
<h3 id="h2-500723c16-0012">Finding Face Masks</h3>
<p class="BodyFirst">Let’s suppose that we work at a museum that has received a big collection of art, and it’s our job to organize it all. One of our tasks is to find all of the drawings of grid-based face masks that are close matches to the simple mask in <a href="#figure16-37" id="figureanchor16-37">Figure 16-37</a>.</p>
<figure>
<img src="Images/F16037.png" alt="F16037" width="205" height="205"/>
<figcaption><p><a id="figure16-37">Figure 16-37</a>: A simple binary mask on a 12 by 12 mesh</p></figcaption>
</figure>
<p>Suppose we’re given the new mask in the middle of <a href="#figure16-37">Figure 16-37</a>. Let’s call this the <em>candidate</em>. We want to determine whether it’s roughly the “same” as the original mask, which we call the <em>reference</em>. We can just overlay the two masks and see if they match up, as in the right of <a href="#figure16-38" id="figureanchor16-38">Figure 16-38</a>.</p>
<span epub:type="pagebreak" title="463" id="Page_463"/><figure>
<img src="Images/F16038.png" alt="F16038" width="694" height="206"/>
<figcaption><p><a id="figure16-38">Figure 16-38</a>: Testing for similarity. On the left is our original mask, or reference. In the middle is a new mask, or candidate. To see if they’re close to one another, we can overlay them, at the right. </p></figcaption>
</figure>
<p>In this case, it’s a perfect match, which is easy to detect. But what if a candidate is slightly different than the reference, as in <a href="#figure16-39" id="figureanchor16-39">Figure 16-39</a>? Here one eye has moved down by one pixel.</p>
<figure>
<img src="Images/F16039.png" alt="F16039" width="694" height="206"/>
<figcaption><p><a id="figure16-39">Figure 16-39</a>: Like <a href="#figure16-38">Figure 16-38</a>, only the candidate’s left eye has moved down by one pixel. The overlay is now imperfect. </p></figcaption>
</figure>
<p>Let’s say that we still want to accept this candidate, since it has all the same features as the reference, and they’re mostly in the right places. But the overlay shows that they’re not identical, so a simple pixel-by-pixel comparison won’t do the job.</p>
<p>In this simple example, we could come up with lots of ways to detect close matches, but let’s use convolution to determine that a candidate like the one in <a href="#figure16-39">Figure 16-39</a> is “like” the reference. As mentioned earlier, we’re going to hand-engineer our filters. To describe our hierarchy, it’s easiest to work backward, from the final step of convolution to the first. </p>
<p>Let’s begin by describing the reference mask. Then we can determine if a candidate shares its qualities. Let’s say that our reference is characterized by having one eye in each of the upper corners, a nose in the middle, and a mouth under the nose. That description applies to all of the masks we saw in Figures 16-38 and 16-39.</p>
<p>We can formalize this description with a three by three filter, as in the top-left grid of <a href="#figure16-40" id="figureanchor16-40">Figure 16-40</a>. This will be one of our last filters: if we run a candidate through a series of convolutions, ultimately producing a three by three tensor (we’ll see how that happens shortly), then if that tensor matches this filter, we’ve found a successful match, and an acceptable candidate. The cells with an × in them mean “don’t care.” For instance, suppose <span epub:type="pagebreak" title="464" id="Page_464"/>a candidate has a tattoo on one cheek that falls into the × to the right of the nose. This doesn’t affect our decision, so we explicitly don’t care about what’s in that cell.</p>
<figure>
<img src="Images/F16040.png" alt="F16040" width="597" height="455"/>
<figcaption><p><a id="figure16-40">Figure 16-40</a>: Filters for mask recognition. Top and bottom rows: Finding a mask facing forward, or in profile. Left column: Characterizing the reference. Middle: An exploded version of the tensor described by the grid at the left. Right: An X-ray view of the filter (see text).</p></figcaption>
</figure>
<p>Since our filters only contain the values 1 (green) and 0 (beige), we can’t make a filter like the upper left diagram of <a href="#figure16-40">Figure 16-40</a> directly. Instead, since it’s looking for three different kinds of features, we need to redraw it as a filter with three channels, which we’ll apply to an input tensor with three channels. One input channel tells us all the locations where an eye was located in the input, the next tells us all the locations of a nose, and the last tells us all the locations of a mouth. Our upper-left diagram corresponds, then, to a three by three by three tensor, shown in the upper middle diagram, where we’ve staggered the channels so we can read each one. </p>
<p>We drew the staggered version because if we drew that tensor as a solid block, we wouldn’t be able to see most of the values on the N (nose) and M (mouth) channels. The staggered version is useful, but it will get too complicated when we start comparing tensors in the following discussion. Instead, let’s draw an “X-ray view” of the tensor, as in the upper right. We imagine we’re looking through the channels of the tensor, and we mark each cell with the names of all the channels that have a 1 in that cell.</p>
<p>Since this filter is looking for a mask facing forward, we label it F. For fun, we can make another mask that’s looking for a face in profile, which we can call P. We won’t look at any candidates that would be matched by P, but we’re including it here to show the generality of this process. The layers to come, which operate before the filters of <a href="#figure16-40">Figure 16-40</a>, will tell <span epub:type="pagebreak" title="465" id="Page_465"/>us where they found an eye, nose, and mouth. We use that information in <a href="#figure16-40">Figure 16-40</a> to recognize different arrangements of these facial features just by using different filters.</p>
<h3 id="h2-500723c16-0013">Finding Eyes, Noses, and Mouths</h3>
<p class="BodyFirst">Let’s see how to turn a 12 by 12 candidate picture into the 3 by 3 grid required by the filters of <a href="#figure16-40">Figure 16-40</a>. We can do that with a series of convolutions, each followed by a pooling step. Since the filters of <a href="#figure16-40">Figure 16-40</a> are trying to match eyes, a nose, and a mouth, we know that the convolution layer before these filters has to produce those features. So, let’s design filters that search for them.</p>
<p>In <a href="#figure16-41" id="figureanchor16-41">Figure 16-41</a>, we show three filters, each with a four by four footprint. They’re labeled E4, N4, and M4. They look for an eye, a nose, and a mouth, respectively. The reason for placing the “4” at the end of each name will be clear in a moment.</p>
<figure>
<img src="Images/F16041.png" alt="F16041" width="694" height="287"/>
<figcaption><p><a id="figure16-41">Figure 16-41</a>: Three filters that detect an eye, nose, and mouth </p></figcaption>
</figure>
<p>We can jump right in and apply these three filters to any candidate image. Since the images are 12 by 12, and we’re not padding, the outputs will be 10 by 10. If we pool those down to 3 by 3, we can then apply the filters of <a href="#figure16-40">Figure 16-40</a> to the output of the filters in <a href="#figure16-41">Figure 16-41</a> to determine if the candidate is a mask looking forward, or in profile, or neither.</p>
<p>But applying four by four filters requires a lot of computation. Worse, if we want to look for another feature (like a winking eye), we have to build another four by four filter and also apply that to the whole image. We can make our system more flexible, and also faster, by introducing another layer of convolution before this one.</p>
<p>What features can make up our E4, N4, and M4 filters of <a href="#figure16-41">Figure 16-41</a>? If we think of each four by four filter as a grid of two by two blocks, then we need only four types of two by two blocks to make up all three filters. The top row of <a href="#figure16-42" id="figureanchor16-42">Figure 16-42</a> shows those four little blocks, and the rows below that show how they can be combined to make our eye, nose, and mouth filters. We’ve called these T, Q, L, and R for top, quartet, lower-left corner, and lower-right corner, respectively.</p>
<span epub:type="pagebreak" title="466" id="Page_466"/><figure>
<img src="Images/F16042.png" alt="F16042" width="674" height="721"/>
<figcaption><p><a id="figure16-42">Figure 16-42</a>: Top row: The two by two filters T, Q, L, and R. Second row, left to right: Filter E4, breaking it into four smaller blocks and the tensor form of those blocks. The far right shows the X-ray view of the two by two by four filter E. Third and fourth rows: Filters N4 and M4.</p></figcaption>
</figure>
<p>Starting with the eye filter E4, we break the four by four filter into four two by two blocks. The third drawing in the E4 row shows the four channels that we expect as input, one each for T, Q, L, and R, drawn as a single tensor where we staggered the channels. To draw that tensor more conveniently, we use the X-ray convention we saw in <a href="#figure16-40">Figure 16-40</a>. This gives us a new filter, of size two by two by four. This is the filter we really want to use to detect eyes, so we drop the “4” and just call this E. </p>
<p>The N and M filters are created by the same process of subdivision and assembly from T, Q, L, and R.</p>
<p>Now imagine running the little T, Q, L, and R filters over a candidate image. They’re looking for patterns of pixels. Then the E, N, and M filters look for specific arrangements of T, Q, L, and R patterns. And then the F and P filters look for specific arrangements of E, N, and M patterns. Thus, <span epub:type="pagebreak" title="467" id="Page_467"/>we have a series of convolution layers, with each output serving as the next layer’s input. <a href="#figure16-43" id="figureanchor16-43">Figure 16-43</a> shows this graphically.</p>
<figure>
<img src="Images/F16043.png" alt="F16043" width="570" height="696"/>
<figcaption><p><a id="figure16-43">Figure 16-43</a>: Using three layers of convolution to analyze an input candidate</p></figcaption>
</figure>
<p>Now that we have our filters, we can start at the bottom and process an input. Along the way, we’ll see where to put the pooling layers.</p>
<h3 id="h2-500723c16-0014">Applying Our Filters</h3>
<p class="BodyFirst">Let’s start at the bottom of <a href="#figure16-43">Figure 16-43</a> and apply the filters of Layer 1. <a href="#figure16-44" id="figureanchor16-44">Figure 16-44</a> shows the result of sweeping the T filter over the 12 by 12 candidate image. Because T is 2 by 2, it doesn’t have a center, so we arbitrarily place its anchor in its upper-left corner. Because we’re not padding, and the filter is 2 by 2, the output will be 11 by 11. In <a href="#figure16-44">Figure 16-44</a>, each location where T finds an exact match is marked in light green; otherwise, it’s marked in pink. We’ll call this output the T-map.</p>
<p>Now we want to make sure that the E, N, and M filters that are looking for T matches still succeed even if the T matches aren’t exactly where our reference mask had them. As we saw in the previous section, the way to make <span epub:type="pagebreak" title="468" id="Page_468"/>filters robust to small displacements in their input is to use pooling. Let’s use the most common form of pooling: max pooling with two by two blocks. </p>
<figure>
<img src="Images/F16044.png" alt="F16044" width="517" height="266"/>
<figcaption><p><a id="figure16-44">Figure 16-44</a>: Convolving the 12 by 12 input image with the 2 by 2 filter T produces the 11 by 11 output, or feature map, which we call the T-map. </p></figcaption>
</figure>
<p><a href="#figure16-45" id="figureanchor16-45">Figure 16-45</a> shows max pooling applied to the T-map. For each two by two block, if there’s at least one green value in the block, the output is green (recall that green elements have a value of 1, and the red are 0). When the pooling blocks fall off the right and bottom sides of the input, we just ignore the missing entries and apply pooling to the values we actually have. We call the result of pooling the T-pool tensor.</p>
<figure>
<img src="Images/F16045.png" alt="F16045" width="597" height="250"/>
<figcaption><p><a id="figure16-45">Figure 16-45</a>: Applying two by two max pooling to the T-map to produce the T-pool tensor. Green means 1, and pink means 0. </p></figcaption>
</figure>
<p>The upper-left element of T-pool tells us if the T filter matched when placed on top of <em>any</em> of the four pixels in the upper left of the input. In this case, it did, so that element is turned green (that is, it’s assigned a value of 1).</p>
<p>Let’s repeat this process for the other three first-layer filters (Q, L, and R). The results are shown in the left part of <a href="#figure16-46" id="figureanchor16-46">Figure 16-46</a>.</p>
<p>The four T, Q, L, and R filters together produce a result with four feature maps, each six by six after pooling. Recall from <a href="#figure16-40">Figure 16-40</a> that the E, N, and M filters are expecting a tensor with four channels. To combine these individual outputs into one tensor, we can just stack them up, as in <span epub:type="pagebreak" title="469" id="Page_469"/>the center of <a href="#figure16-46">Figure 16-46</a>. As usual, we then draw this as a 2D grid using our X-ray view convention. This gives us a tensor of four channels, which is just what Layer 2 is expecting as input.</p>
<figure>
<img src="Images/F16046.png" alt="F16046" width="841" height="288"/>
<figcaption><p><a id="figure16-46">Figure 16-46</a>: Left: The result of applying all four first-level filters to our candidate and then pooling. Center: Stacking up the outputs into a single tensor. Right: Drawing the six by six by four tensor in X-ray view. </p></figcaption>
</figure>
<p>Now we can move up to the filters in Layer 2. Let’s start with E, in <a href="#figure16-47" id="figureanchor16-47">Figure 16-47</a>. </p>
<figure>
<img src="Images/F16047.png" alt="F16047" width="840" height="232"/>
<figcaption><p><a id="figure16-47">Figure 16-47</a>: Applying the E filter. As before, from left to right, we have the input tensor, the E filter (both in our X-ray view), the result of applying that filter, the pooling grid, and the result of pooling. </p></figcaption>
</figure>
<p><a href="#figure16-47">Figure 16-47</a> shows our input tensor (the output of Layer 1) and the E filter, both in X-ray view. To their right, we see the E-map resulting from applying the E filter, the process of applying two by two pooling to the E-map, and finally the E-pool feature map. We can see already how the pooling process allows the next filter to match the locations of the eyes, even though one eye is not located where it was in the reference mask. </p>
<p>We can follow the same process for the N and M filters, producing a new output tensor for the second layer, as shown in <a href="#figure16-48" id="figureanchor16-48">Figure 16-48</a>.</p>
<p>Now we have a three by three tensor with three channels, just right for the filters we created for F and P back in <a href="#figure16-40">Figure 16-40</a>. We’re ready to move up another level to Layer 3.</p>
<span epub:type="pagebreak" title="470" id="Page_470"/><figure>
<img src="Images/F16048.png" alt="F16048" width="694" height="294"/>
<figcaption><p><a id="figure16-48">Figure 16-48</a>: Computing outputs for the E, N, and M filters, then stacking them up into a tensor with three channels </p></figcaption>
</figure>
<p>This final step is easy: we just apply the F and P filters to their entire input, since their sizes are the same (that is, there’s no need to scan the filter over the image). The result is a tensor with shape one by one by two. If the element in the first channel in this tensor is green, then F matches, and the candidate should be accepted as a match to our reference. If it’s beige, the candidate’s not a match. </p>
<figure>
<img src="Images/F16049.png" alt="F16049" width="449" height="435"/>
<figcaption><p><a id="figure16-49">Figure 16-49</a>: Applying the F and P filters to the output tensor of the second layer. In this layer, each filter is the same size as the input, so the layer produces an output tensor of size one by one by two.  </p></figcaption>
</figure>
<p>And we’re done! We used three layers of convolution to characterize a candidate image as being either like, or unlike, a reference image. We found that our candidate with one eye dropped down by one pixel was still close enough to our reference that we should accept it.</p>
<p><span epub:type="pagebreak" title="471" id="Page_471"/>We solved this problem by creating not just a sequence of convolutions, but a hierarchy. Each convolution used the results of the previous one. The first layer looked for patterns in the pixels, the second looked for patterns of those patterns, and the third looked for larger patterns still, corresponding to a face looking forward or in profile. Pooling enabled the network to recognize a candidate even though one important block of pixels was shifted a little.</p>
<p><a href="#figure16-50" id="figureanchor16-50">Figure 16-50</a> shows our whole network at a glance. Since the only layers with neurons are convolution layers, we call this an <em>all-convolutional network.</em></p>
<figure>
<img src="Images/F16050.png" alt="F16050" width="694" height="205"/>
<figcaption><p><a id="figure16-50">Figure 16-50</a>: Our all-convolutional network for evaluating masks. We’re also showing the input, output, and intermediate tensors. The icons with nested boxes are convolution layers, the trapezoids are pooling layers.</p></figcaption>
</figure>
<p>In <a href="#figure16-50">Figure 16-50</a>, the icons with a box in a box represent convolution, and the trapezoids represent pooling layers.</p>
<p>If we want to match even more types of faces, we can just add more filters to the final layer. This lets us match any pattern of eyes, noses, and mouths that we want, with little additional cost. By reducing the size of the tensors in our network, pooling reduces the amount of computation we have to do. This means that not only is the network with pooling more robust than a version without pooling, it also consumes less memory and runs faster. </p>
<p>There’s a sense in which our filters are getting more powerful as we work our way up the levels. For example, our eye filter E is processing a four by four region, though it’s only two by two itself, because each of its tensor elements is the result of a two by two convolution. In this way, the filters at higher levels in a hierarchy are able to look for large and complex features, even though they use only small (and therefore fast) filters.</p>
<p>Higher levels are able to combine the results of lower levels in multiple ways. Suppose we want to classify a variety of different birds in a photo. Low-level filters may look for feathers or beaks, while higher filters are able to combine different types of feathers or beaks to recognize different species of birds, all in a single pass through a photo. We sometimes say that using this technique of convolution and pooling to analyze an input is applying a <em>hierarchy of scales</em>.</p>
<h2 id="h1-500723c16-0007"><span epub:type="pagebreak" title="472" id="Page_472"/>Summary</h2>
<p class="BodyFirst">This chapter was all about convolution: the method of taking a filter or kernel (that is, a neuron with a set of weights) and moving it over an input. Each time we apply the filter to the input, we produce a single value of output. The filter may use just a single input element in its calculation, or it may have a larger footprint and use the values of multiple input elements. If a filter has a footprint that is larger than one by one, there will be places in the input where the filter “spills” over the edge, requiring input data that isn’t there. If we don’t place the filter in such places, the output has a smaller width or height (or both) than the input. To avoid this, we commonly pad the input by surrounding it with enough rings of zeroes so that the filter can be placed over every input element.</p>
<p>We can bundle up many filters into a single convolution layer. In such a layer, typically every filter has the same footprint and the same activation function. Every filter produces one channel per filter. The output of the layer has one channel per filter. </p>
<p>If we want to change the width and height of a tensor, we can perform downsampling (to reduce either or both dimensions) or upsampling (to increase either or both dimensions). To downsample, we can use a pooling layer, which finds the average or maximum value in blocks from the input. To upsample, we can use an upsampling layer, which duplicates input elements. Either of these techniques may be combined with the convolution step itself. To downsample, we use striding, in which the filter is moved by more than one step horizontally, vertically, or both. To upsample, we use fractional striding, or transposed convolution, in which we insert rows and/or columns of zeroes between the input elements.</p>
<p>We saw that by applying convolutions in a series of layers with downsampling, we are able to create a hierarchy of filters that work at different scales. This also means that the system enjoys the property of shift invariance, meaning that it’s able to find the patterns it seeks even if they’re not exactly where they’re expected to be. </p>
<p>In the next chapter, we’ll examine real convnets and look at their filters to see how they do their jobs. </p>
</section>
</div></body></html>