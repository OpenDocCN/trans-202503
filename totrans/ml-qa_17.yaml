- en: '**14'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**14**'
- en: THE DISTRIBUTIONAL HYPOTHESIS**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**分布假设**'
- en: '![Image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/common.jpg)'
- en: What is the distributional hypothesis in natural language processing (NLP)?
    Where is it used, and how far does it hold true?
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是自然语言处理（NLP）中的分布假设？它在哪里使用，它的适用范围有多广？
- en: The distributional hypothesis is a linguistic theory suggesting that words occurring
    in the same contexts tend to have similar meanings, according to the original
    source, “Distributional Structure” by Zellig S. Harris. Succinctly, the more similar
    the meanings of two words are, the more often they appear in similar contexts.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 分布假设是一种语言学理论，建议在相同语境中出现的单词往往具有相似的含义，这一观点来源于 Zellig S. Harris 的原始著作《Distributional
    Structure》。简而言之，两个单词的含义越相似，它们出现在相似语境中的频率就越高。
- en: Consider the sentence in [Figure 14-1](ch14.xhtml#ch14fig1), for example. The
    words *cats* and *dogs* often occur in similar contexts, and we could replace
    *cats* with *dogs* without making the sentence sound awkward. We could also replace
    *cats* with *hamsters*, since both are mammals and pets, and the sentence would
    still sound plausible. However, replacing *cats* with an unrelated word such as
    *sandwiches* would render the sentence clearly wrong, and replacing *cats* with
    the unrelated word *driving* would also make the sentence grammatically incorrect.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 以[图 14-1](ch14.xhtml#ch14fig1)中的句子为例。*cats* 和 *dogs* 经常出现在相似的语境中，我们可以将 *cats*
    替换为 *dogs*，而不至于使句子听起来别扭。我们也可以将 *cats* 替换为 *hamsters*，因为它们都是哺乳动物和宠物，句子仍然听起来合理。然而，如果将
    *cats* 替换为一个不相关的词如 *sandwiches*，句子就会明显错误，而将 *cats* 替换为不相关的词 *driving* 也会使句子语法上不正确。
- en: '![Image](../images/14fig01.jpg)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/14fig01.jpg)'
- en: '*Figure 14-1: Common and uncommon words in a given context*'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 14-1：在给定语境中的常见词和不常见词*'
- en: 'It is easy to construct counterexamples using polysemous words, that is, words
    that have multiple meanings that are related but not identical. For example, consider
    the word *bank*. As a noun, it can refer to a financial institution, the “rising
    ground bordering a river,” the “steep incline of a hill,” or a “protective cushioning
    rim” (according to the Merriam-Webster dictionary). It can even be a verb: to
    bank on something means to rely or depend on it. These different meanings have
    different distributional properties and may not always occur in similar contexts.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多义词很容易构造反例，即具有多重含义的词，这些含义相关但不完全相同。例如，考虑单词 *bank*。作为名词，它可以指代金融机构、"河岸的升高地带"、"山坡的陡峭倾斜"
    或 "保护性衬垫边缘"（根据 Merriam-Webster 字典）。它甚至可以作为动词使用：to bank on something 意味着依赖或依靠某事。这些不同的含义有不同的分布特征，并不总是在相似的语境中出现。
- en: Nonetheless, the distributional hypothesis is quite useful. Word embeddings
    (introduced in [Chapter 1](ch01.xhtml)) such as Word2vec, as well as many large
    language transformer models, rely on this idea. This includes the masked language
    model in BERT and the next-word pretraining task used in GPT.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，分布假设仍然非常有用。像 Word2vec 这样的词嵌入（在[第一章](ch01.xhtml)中介绍）以及许多大型语言变换模型都依赖于这一思想。这包括
    BERT 中的掩码语言模型和 GPT 中用于预训练的下一个词任务。
- en: '**Word2vec, BERT, and GPT**'
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**Word2vec、BERT 和 GPT**'
- en: 'The Word2vec approach uses a simple, two-layer neural network to encode words
    into embedding vectors such that the embedding vectors of similar words are both
    semantically and syntactically close. There are two ways to train a Word2vec model:
    the continuous bag-of-words (CBOW) approach and the skip-gram approach. When using
    CBOW, the Word2vec model learns to predict the current words by using the surrounding
    context words. Conversely, in the skip-gram model, Word2vec predicts the context
    words from a selected word. While skip-gram is more effective for infrequent words,
    CBOW is usually faster to train.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Word2vec 方法使用一个简单的、两层的神经网络将单词编码成嵌入向量，使得相似单词的嵌入向量在语义和句法上都接近。训练 Word2vec 模型有两种方式：连续词袋模型（CBOW）和跳字模型（skip-gram）。在使用
    CBOW 时，Word2vec 模型通过使用周围的上下文单词来预测当前单词。相反，在跳字模型中，Word2vec 根据一个选定的单词预测上下文单词。虽然跳字模型对不常见单词更有效，但
    CBOW 通常训练速度较快。
- en: After training, word embeddings are placed within the vector space so that words
    with common contexts in the corpus—that is, words with semantic and syntactic
    similarities—are positioned close to each other, as illustrated in [Figure 14-2](ch14.xhtml#ch14fig2).
    Conversely, dissimilar words are located farther apart in the embedding space.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 经过训练后，词嵌入被置于向量空间中，使得在语料库中具有共同上下文的词——即在语义和句法上相似的词——被定位得彼此接近，如[图 14-2](ch14.xhtml#ch14fig2)所示。相反，不相似的词则被定位得相距较远。
- en: '![Image](../images/14fig02.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/14fig02.jpg)'
- en: '*Figure 14-2: Word2vec embeddings in a two-dimensional vector space*'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 14-2：Word2vec 在二维向量空间中的嵌入*'
- en: BERT is an LLM based on the transformer architecture (see [Chapter 8](ch08.xhtml))
    that uses a masked language modeling approach that involves masking (hiding) some
    of the words in a sentence. Its task is to predict these masked words based on
    the other words in the sequence, as illustrated in [Figure 14-3](ch14.xhtml#ch14fig3).
    This is a form of the self-supervised learning used to pretrain LLMs (see [Chapter
    2](ch02.xhtml) for more on self-supervised learning). The pretrained model produces
    embeddings in which similar words (or tokens) are close in the embedding space.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 是一种基于 Transformer 架构的 LLM（详见[第 8 章](ch08.xhtml)），它采用掩蔽语言模型方法，涉及掩蔽（隐藏）句子中的一些词。它的任务是根据序列中的其他词预测这些被掩蔽的词，如[图
    14-3](ch14.xhtml#ch14fig3)所示。这是一种自监督学习的形式，用于预训练 LLM（有关自监督学习的更多内容，请参见[第 2 章](ch02.xhtml)）。预训练模型生成的词嵌入中，相似的词（或标记）在嵌入空间中较为接近。
- en: '![Image](../images/14fig03.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/14fig03.jpg)'
- en: '*Figure 14-3: BERT’s pretraining task involves predicting randomly masked words.*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 14-3：BERT 的预训练任务涉及预测随机掩蔽的词。*'
- en: GPT, which like BERT is also an LLM based on the transformer architecture, functions
    as a decoder. Decoder-style models like GPT learn to predict subsequent words
    in a sequence based on the preceding ones, as illustrated in [Figure 14-4](ch14.xhtml#ch14fig4).
    GPT contrasts with BERT, an encoder model, as it emphasizes predicting what follows
    rather than encoding the entire sequence simultaneously.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: GPT 和 BERT 一样，也是基于 Transformer 架构的 LLM，它作为一个解码器工作。像 GPT 这样的解码器模型学习根据前面的词预测序列中的后续词，如[图
    14-4](ch14.xhtml#ch14fig4)所示。与 BERT 这种编码器模型不同，GPT 更侧重于预测后续内容，而不是同时编码整个序列。
- en: '![Image](../images/14fig04.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/14fig04.jpg)'
- en: '*Figure 14-4: GPT is pretrained by predicting the next word.*'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 14-4：GPT 通过预测下一个词进行预训练。*'
- en: Where BERT is a bidirectional language model that considers the whole input
    sequence, GPT only strictly parses previous sequence elements. This means BERT
    is usually better suited for classification tasks, whereas GPT is more suited
    for text generation tasks. Similar to BERT, GPT produces high-quality contextualized
    word embeddings that capture semantic similarity.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 是一个双向语言模型，考虑整个输入序列，而 GPT 只严格解析前面的序列元素。这意味着 BERT 通常更适合分类任务，而 GPT 更适合文本生成任务。与
    BERT 类似，GPT 生成的高质量上下文化词嵌入能够捕捉语义相似性。
- en: '**Does the Hypothesis Hold?**'
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**假设是否成立？**'
- en: For large datasets, the distributional hypothesis more or less holds true, making
    it quite useful for understanding and modeling language patterns, word relationships,
    and semantic meanings. For example, this concept enables techniques like word
    embedding and semantic analysis, which, in turn, facilitate natural language processing
    tasks such as text classification, sentiment analysis, and machine translation.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大型数据集，分布假设或多或少成立，这使得它在理解和建模语言模式、词关系和语义意义方面非常有用。例如，这一概念使得词嵌入和语义分析等技术成为可能，从而促进了自然语言处理任务，如文本分类、情感分析和机器翻译。
- en: In conclusion, while there are counterexamples in which the distributional hypothesis
    does not hold, it is a very useful concept that forms the cornerstone of modern
    language transformer models.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，尽管在某些反例中分布假设并不成立，但它仍是一个非常有用的概念，构成了现代语言 Transformer 模型的基石。
- en: '**Exercises**'
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**练习**'
- en: '**14-1.** Does the distributional hypothesis hold true in the case of homophones,
    or words that sound the same but have different meanings, such as *there* and
    *their*?'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**14-1.** 在同音异义词的情况下，分布假设是否成立？比如像*there*和*their*这样的词，它们发音相同但含义不同。'
- en: '**14-2.** Can you think of another domain where a concept similar to the distributional
    hypothesis applies? (Hint: think of other input modalities for neural networks.)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**14-2.** 你能想到另一个类似于分布假设的领域吗？（提示：考虑神经网络的其他输入模式。）'
- en: '**References**'
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: 'The original source describing the distributional hypothesis: Zellig S. Harris,
    “Distributional Structure” (1954), *[https://doi.org/10.1080/00437956.1954.11659520](https://doi.org/10.1080/00437956.1954.11659520)*.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述分布假设的原始文献：Zellig S. Harris，“分布结构”（1954），*[https://doi.org/10.1080/00437956.1954.11659520](https://doi.org/10.1080/00437956.1954.11659520)*。
- en: 'The paper introducing the Word2vec model: Tomas Mikolov et al., “Efficient
    Estimation of Word Representations in Vector Space” (2013), *[https://arxiv.org/abs/1301.3781](https://arxiv.org/abs/1301.3781)*.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍Word2vec模型的论文：Tomas Mikolov 等人，“在向量空间中高效估计词表示”（2013），*[https://arxiv.org/abs/1301.3781](https://arxiv.org/abs/1301.3781)*。
- en: 'The paper introducing the BERT model: Jacob Devlin et al., “BERT: Pre-training
    of Deep Bidirectional Transformers for Language Understanding” (2018), *[https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)*.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍BERT模型的论文：Jacob Devlin 等人，“BERT：用于语言理解的深度双向变换器预训练”（2018），*[https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)*。
- en: 'The paper introducing the GPT model: Alec Radford and Karthik Narasimhan, “Improving
    Language Understanding by Generative Pre-Training” (2018), *[https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035](https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035)*.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍GPT模型的论文：Alec Radford 和 Karthik Narasimhan，“通过生成预训练提高语言理解”（2018），*[https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035](https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035)*。
- en: 'BERT produces embeddings in which similar words (or tokens) are close in the
    embedding space: Nelson F. Liu et al., “Linguistic Knowledge and Transferability
    of Contextual Representations” (2019), *[https://arxiv.org/abs/1903.08855](https://arxiv.org/abs/1903.08855)*.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT生成的嵌入表示中，相似的词（或符号）在嵌入空间中靠得很近：Nelson F. Liu 等人，“上下文表示的语言知识与可迁移性”（2019），*[https://arxiv.org/abs/1903.08855](https://arxiv.org/abs/1903.08855)*。
- en: 'The paper showing that GPT produces high-quality contextualized word embeddings
    that capture semantic similarity: Fabio Petroni et al., “Language Models as Knowledge
    Bases?” (2019), *[https://arxiv.org/abs/1909.01066](https://arxiv.org/abs/1909.01066)*.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 证明GPT生成高质量上下文化词嵌入，能够捕捉语义相似性的论文：Fabio Petroni 等人，“语言模型作为知识库？”（2019），*[https://arxiv.org/abs/1909.01066](https://arxiv.org/abs/1909.01066)*。
