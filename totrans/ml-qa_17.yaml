- en: '**14'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: THE DISTRIBUTIONAL HYPOTHESIS**
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: What is the distributional hypothesis in natural language processing (NLP)?
    Where is it used, and how far does it hold true?
  prefs: []
  type: TYPE_NORMAL
- en: The distributional hypothesis is a linguistic theory suggesting that words occurring
    in the same contexts tend to have similar meanings, according to the original
    source, “Distributional Structure” by Zellig S. Harris. Succinctly, the more similar
    the meanings of two words are, the more often they appear in similar contexts.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the sentence in [Figure 14-1](ch14.xhtml#ch14fig1), for example. The
    words *cats* and *dogs* often occur in similar contexts, and we could replace
    *cats* with *dogs* without making the sentence sound awkward. We could also replace
    *cats* with *hamsters*, since both are mammals and pets, and the sentence would
    still sound plausible. However, replacing *cats* with an unrelated word such as
    *sandwiches* would render the sentence clearly wrong, and replacing *cats* with
    the unrelated word *driving* would also make the sentence grammatically incorrect.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/14fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 14-1: Common and uncommon words in a given context*'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is easy to construct counterexamples using polysemous words, that is, words
    that have multiple meanings that are related but not identical. For example, consider
    the word *bank*. As a noun, it can refer to a financial institution, the “rising
    ground bordering a river,” the “steep incline of a hill,” or a “protective cushioning
    rim” (according to the Merriam-Webster dictionary). It can even be a verb: to
    bank on something means to rely or depend on it. These different meanings have
    different distributional properties and may not always occur in similar contexts.'
  prefs: []
  type: TYPE_NORMAL
- en: Nonetheless, the distributional hypothesis is quite useful. Word embeddings
    (introduced in [Chapter 1](ch01.xhtml)) such as Word2vec, as well as many large
    language transformer models, rely on this idea. This includes the masked language
    model in BERT and the next-word pretraining task used in GPT.
  prefs: []
  type: TYPE_NORMAL
- en: '**Word2vec, BERT, and GPT**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Word2vec approach uses a simple, two-layer neural network to encode words
    into embedding vectors such that the embedding vectors of similar words are both
    semantically and syntactically close. There are two ways to train a Word2vec model:
    the continuous bag-of-words (CBOW) approach and the skip-gram approach. When using
    CBOW, the Word2vec model learns to predict the current words by using the surrounding
    context words. Conversely, in the skip-gram model, Word2vec predicts the context
    words from a selected word. While skip-gram is more effective for infrequent words,
    CBOW is usually faster to train.'
  prefs: []
  type: TYPE_NORMAL
- en: After training, word embeddings are placed within the vector space so that words
    with common contexts in the corpus—that is, words with semantic and syntactic
    similarities—are positioned close to each other, as illustrated in [Figure 14-2](ch14.xhtml#ch14fig2).
    Conversely, dissimilar words are located farther apart in the embedding space.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/14fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 14-2: Word2vec embeddings in a two-dimensional vector space*'
  prefs: []
  type: TYPE_NORMAL
- en: BERT is an LLM based on the transformer architecture (see [Chapter 8](ch08.xhtml))
    that uses a masked language modeling approach that involves masking (hiding) some
    of the words in a sentence. Its task is to predict these masked words based on
    the other words in the sequence, as illustrated in [Figure 14-3](ch14.xhtml#ch14fig3).
    This is a form of the self-supervised learning used to pretrain LLMs (see [Chapter
    2](ch02.xhtml) for more on self-supervised learning). The pretrained model produces
    embeddings in which similar words (or tokens) are close in the embedding space.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/14fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 14-3: BERT’s pretraining task involves predicting randomly masked words.*'
  prefs: []
  type: TYPE_NORMAL
- en: GPT, which like BERT is also an LLM based on the transformer architecture, functions
    as a decoder. Decoder-style models like GPT learn to predict subsequent words
    in a sequence based on the preceding ones, as illustrated in [Figure 14-4](ch14.xhtml#ch14fig4).
    GPT contrasts with BERT, an encoder model, as it emphasizes predicting what follows
    rather than encoding the entire sequence simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/14fig04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 14-4: GPT is pretrained by predicting the next word.*'
  prefs: []
  type: TYPE_NORMAL
- en: Where BERT is a bidirectional language model that considers the whole input
    sequence, GPT only strictly parses previous sequence elements. This means BERT
    is usually better suited for classification tasks, whereas GPT is more suited
    for text generation tasks. Similar to BERT, GPT produces high-quality contextualized
    word embeddings that capture semantic similarity.
  prefs: []
  type: TYPE_NORMAL
- en: '**Does the Hypothesis Hold?**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For large datasets, the distributional hypothesis more or less holds true, making
    it quite useful for understanding and modeling language patterns, word relationships,
    and semantic meanings. For example, this concept enables techniques like word
    embedding and semantic analysis, which, in turn, facilitate natural language processing
    tasks such as text classification, sentiment analysis, and machine translation.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, while there are counterexamples in which the distributional hypothesis
    does not hold, it is a very useful concept that forms the cornerstone of modern
    language transformer models.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercises**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**14-1.** Does the distributional hypothesis hold true in the case of homophones,
    or words that sound the same but have different meanings, such as *there* and
    *their*?'
  prefs: []
  type: TYPE_NORMAL
- en: '**14-2.** Can you think of another domain where a concept similar to the distributional
    hypothesis applies? (Hint: think of other input modalities for neural networks.)'
  prefs: []
  type: TYPE_NORMAL
- en: '**References**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The original source describing the distributional hypothesis: Zellig S. Harris,
    “Distributional Structure” (1954), *[https://doi.org/10.1080/00437956.1954.11659520](https://doi.org/10.1080/00437956.1954.11659520)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The paper introducing the Word2vec model: Tomas Mikolov et al., “Efficient
    Estimation of Word Representations in Vector Space” (2013), *[https://arxiv.org/abs/1301.3781](https://arxiv.org/abs/1301.3781)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The paper introducing the BERT model: Jacob Devlin et al., “BERT: Pre-training
    of Deep Bidirectional Transformers for Language Understanding” (2018), *[https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The paper introducing the GPT model: Alec Radford and Karthik Narasimhan, “Improving
    Language Understanding by Generative Pre-Training” (2018), *[https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035](https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BERT produces embeddings in which similar words (or tokens) are close in the
    embedding space: Nelson F. Liu et al., “Linguistic Knowledge and Transferability
    of Contextual Representations” (2019), *[https://arxiv.org/abs/1903.08855](https://arxiv.org/abs/1903.08855)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The paper showing that GPT produces high-quality contextualized word embeddings
    that capture semantic similarity: Fabio Petroni et al., “Language Models as Knowledge
    Bases?” (2019), *[https://arxiv.org/abs/1909.01066](https://arxiv.org/abs/1909.01066)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
