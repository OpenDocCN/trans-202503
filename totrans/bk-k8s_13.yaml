- en: '11'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CONTROL PLANE AND ACCESS CONTROL
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/common01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The control plane manages the Kubernetes cluster, storing the desired state
    of applications, monitoring the current state to detect and recover from any issues,
    scheduling new containers, and configuring network routing. In this chapter, we’ll
    look closely at the API server, the primary interface for the control plane and
    the entry point for any status retrieval and changes made to the entire cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although we will focus on the API server, the control plane includes multiple
    other services, each with a role to play. The other control plane services act
    as clients to the API server, watching cluster changes and taking appropriate
    action to update the state of the cluster. The following list describes the other
    control plane components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scheduler** Assigns each new Pod to a node.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Controller manager** Has multiple responsibilities, including creating Pods
    for Deployments, monitoring nodes, and reacting to outages.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cloud controller manager** This optional component interfaces with an underlying
    cloud provider to check on nodes and configure network traffic routing.'
  prefs: []
  type: TYPE_NORMAL
- en: As we demonstrate the workings of the API server, we’ll also see how Kubernetes
    manages security to ensure that only authorized users and services can query the
    cluster and make changes. The purpose of a container orchestration environment
    like Kubernetes is to provide a platform for any kind of containerized application
    we might need to run, so this security is critically important to ensure that
    the cluster is used only as intended.
  prefs: []
  type: TYPE_NORMAL
- en: API Server
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Despite its centrality to the Kubernetes architecture, the API server’s purpose
    is simple. It exposes an interface using HTTP and representational state transfer
    (REST) to perform basic creation, retrieval, update, and deletion of resources
    in the cluster. It performs authentication to identify clients, authorization
    to ensure that clients have permission for the specific request, and validation
    to ensure that any created or updated resources match the corresponding specification.
    It also reads from and writes to a data store based on the commands it receives
    from clients.
  prefs: []
  type: TYPE_NORMAL
- en: However, the API server is not responsible for actually updating the current
    state of the cluster to match the desired state. That is the responsibility of
    other control plane and node components. For example, if a client creates a new
    Kubernetes Deployment, the API server’s job is solely to update the data store
    with the resource information. It is then the job of the scheduler to decide where
    the Pods will run, and the job of the `kubelet` service on the assigned nodes
    to create and monitor the containers and to configure networking to route traffic
    to the containers.
  prefs: []
  type: TYPE_NORMAL
- en: For this chapter, we have a three-node Kubernetes cluster configured by our
    automation scripts. Each of the three nodes acts as a control plane node, so three
    copies of the API server are running. We can communicate with any of these three
    because they all share the same backend database. The API server is listening
    for secure HTTP connections on port 6443, the default port.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*The example repository for this book is at* [https://github.com/book-of-kubernetes/examples](https://github.com/book-of-kubernetes/examples).
    *See “Running Examples” on [page xx](ch00.xhtml#ch00lev1sec2) for details on getting
    set up.*'
  prefs: []
  type: TYPE_NORMAL
- en: We’ve been using `kubectl` to communicate with the API server to create and
    delete resources and retrieve status, and `kubectl` has been using secure HTTP
    on port 6443 to talk to the cluster. It knows to do this because of a Kubernetes
    configuration file that was installed into */etc/kubernetes* by `kubeadm` when
    the cluster was initialized. This configuration file also contains authentication
    information that gives us permission to read cluster status and make changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because the API server is expecting secure HTTP, we can use `curl` to communicate
    directly with the Kubernetes API. This will give us a better feel for how the
    communication actually works. Let’s begin with a simple `curl` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This error message shows that `curl` does not trust the certificate that the
    API server is offering. We can use `curl` to see this certificate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `-k` option tells `curl` to ignore any certificate issues, whereas `-v`
    tells `curl` to provide us with extra logging information about the connection.
  prefs: []
  type: TYPE_NORMAL
- en: 'For `curl` to trust this certificate, it will need to trust the `issuer`, as
    the issuer is the signer of the certificate. Let’s fetch the certificate from
    our Kubernetes installation so that we can point `curl` to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Be sure to add the `.` at the end to copy this file to the current directory.
    We’re doing this solely to make the following commands easier to type.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s examine this certificate before we use it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The `Issuer` and the `Subject` are the same, so this is a *self-signed* certificate.
    It was created by `kubeadm` when we initialized this cluster. Using a generated
    certificate allows `kubeadm` to adapt to our particular cluster networking configuration
    and allows our cluster to have a unique certificate and key without requiring
    an external certificate authority (CA). However, it does mean that we need to
    configure `kubectl` to trust this certificate on any system for which we need
    to communicate with this API server.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now tell `curl` to use this certificate to verify the API server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Now that we’re providing `curl` with the correct root certificate, `curl` can
    validate the API server certificate and we can successfully connect to the API
    server. However, the API server responds with a 403 error, indicating that we
    are not authorized. This is because at the moment we are not providing any authentication
    information for `curl` to pass to the API server, so the API server sees us as
    an anonymous user.
  prefs: []
  type: TYPE_NORMAL
- en: 'One final note: for this `curl` command to work, we need to be selective in
    the hostname or IP address we use. The API server is listening on all network
    interfaces, so we could connect to it using `localhost` or `127.0.0.1`. However,
    those are not listed in the `kube-apiserver` certificate and cannot be used for
    secure HTTP because `curl` will not trust the connection.'
  prefs: []
  type: TYPE_NORMAL
- en: API Server Authentication
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We need to provide authentication information before the API server will accept
    our requests, so let’s understand the API server’s process for authentication.
    Authentication is handled through a set of plug-ins, each of which looks at the
    request to determine whether it can identify the client. The first plug-in that
    successfully identifies the client provides identity information to the API server.
    This identity is then used with authorization to determine what the client is
    allowed to do.
  prefs: []
  type: TYPE_NORMAL
- en: Because authentication is based on plug-ins, it’s possible to have as many different
    ways of authenticating clients as needed. It’s even possible to add a proxy in
    front of the API server that performs custom authentication logic and passes the
    user’s identity to the API server in an HTTP header.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our purposes, we’ll focus on three authentication primary plug-ins that
    are used within the cluster itself or as part of the cluster setup process: *client
    certificates*, *bootstrap tokens*, and *service accounts*.'
  prefs: []
  type: TYPE_NORMAL
- en: Client Certificates
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As mentioned previously, an HTTP client like `curl` validates the server’s identity
    by comparing the server’s hostname to its certificate and also by checking the
    certificate’s signature against a list of trusted CAs. In addition to checking
    the server identity, secure HTTP also allows a client to submit a certificate
    to the server. The server checks the signature against its list of trusted authorities
    and then uses the subject of the certificate as the client’s identity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes uses HTTP client certificate authentication extensively to enable
    cluster services to authenticate with the API server. This includes control plane
    components as well as the `kubelet` service running on each node. We can use `kubeadm`
    to list the certificates used by the control plane:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The `RESIDUAL TIME` column shows how much time is left before these certificates
    expire; by default, they expire after one year. Use `kubeadm certs renew` to renew
    them, passing the name of the certificate as a parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first item in the list, `admin.conf`, is how we’ve been authenticating
    ourselves to the cluster in the past few chapters. During initialization, `kubeadm`
    created this certificate and stored its information in the */etc/kubernetes/admin.conf*
    file. Every `kubectl` command we’ve run has been using this file because our automation
    scripts are setting the `KUBECONFIG` environment variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: If we had not set `KUBECONFIG`, `kubectl` would be using the default, which
    is a file called *.kube/config* in the user’s home directory.
  prefs: []
  type: TYPE_NORMAL
- en: The *admin.conf* credentials are designed to provide emergency access to the
    cluster, bypassing authorization. In a production cluster, we would avoid using
    these credentials directly for everyday operations. Instead, the best practice
    for a production cluster is to integrate a separate identity manager for administrators
    and normal users. For our example, because we don’t have a separate identity manager,
    we’ll instead create an additional certificate for a regular user. This kind of
    certificate may be useful for an automated process that runs outside the cluster,
    but it can’t integrate with the identity manager.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create a new client certificate using `kubeadm`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The `kubeadm kubeconfig user` command asks the API server to generate a new
    client certificate. Because this certificate is signed by the cluster’s CA, it
    is valid for authentication. The certificate is saved into the *kubeconfig* file
    along with the necessary configuration to connect to the API server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `clusters` section defines the information needed to connect to the API
    server, including the load-balanced address shared by all three API servers in
    our highly available configuration. The `users` section defines the new user we
    created along with its client certificate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus far, we’ve successfully created a new user, but we haven’t given that
    user any permissions yet, so we won’t be very successful using these credentials:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Later in the chapter, we’ll see how to give permissions to this user.
  prefs: []
  type: TYPE_NORMAL
- en: Bootstrap Tokens
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Initializing a distributed system like a Kubernetes cluster is challenging.
    The `kubelet` service running on each node must be added to the cluster. To do
    this, `kubelet` must connect to the API server and obtain a client certificate
    signed by the cluster’s CA. The `kubelet` service then uses this client certificate
    to authenticate to the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: This certificate generation must be done securely so that we eliminate the possibility
    of adding rogue nodes to the cluster and eliminate the possibility of a rogue
    process being able to impersonate a real node. For this reason, the API server
    cannot provide a certificate for just any node that asks to be added to the cluster.
    Instead, the node must generate its own private key, submit a certificate signing
    request (CSR) to the API server, and receive a signed certificate.
  prefs: []
  type: TYPE_NORMAL
- en: To keep this process secure, we need to ensure that a node is authorized to
    submit a certificate signing request. But this submission must happen before the
    node has the client certificate that it uses for more permanent authentication—we
    have a chicken-or-egg problem! Kubernetes solves this via time-limited tokens,
    known as *Bootstrap Tokens*. The bootstrap token becomes a preshared secret that
    is known to the API server and the new nodes. Making this token time limited reduces
    the risk to the cluster if it is exposed. The Kubernetes controller manager has
    the task of automatically cleaning up bootstrap tokens when they expire.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we initialized our cluster, `kubeadm` created a bootstrap token, but it
    was configured to expire after two hours. If we need to join additional nodes
    to the cluster after that, we can use `kubeadm` to generate a new bootstrap token:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This token is added as a Kubernetes *Secret* in the `kube-system` Namespace.
    We look at secrets in more detail in [Chapter 16](ch16.xhtml#ch16). For now, let’s
    just verify that it exists:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We can use this token to make requests of the API server by using HTTP Bearer
    authentication. This means that we provide the token in an HTTP header called
    `Authorization`, prefaced with the word `Bearer`. When the bootstrap token authentication
    plug-in sees that header and matches the provided token against the corresponding
    secret, it authenticates us to the API server and allows us access to the API.
  prefs: []
  type: TYPE_NORMAL
- en: For security reasons, bootstrap tokens have access only to the certificate signing
    request functionality of the API server, so that’s all our token will be allowed
    to do.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use our bootstrap token to list all of the certificate signing requests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: It’s important to know how bootstrap tokens work, given that they’re essential
    to adding nodes to the cluster. However, as the name implies, that’s really the
    only purpose for a bootstrap token; it’s not typical to use them for normal API
    server access. For normal API server access, especially from inside the cluster,
    we need a *ServiceAccount*.
  prefs: []
  type: TYPE_NORMAL
- en: Service Accounts
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Containers running in the Kubernetes cluster often need to communicate with
    the API server. For example, all of the various components we deployed on top
    of our cluster in [Chapter 6](ch06.xhtml#ch06), including the Calico network plug-in,
    the Longhorn storage driver, and the metrics server, communicate with the API
    server to watch and modify the cluster state. To support this, Kubernetes automatically
    injects credentials into every running container.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, for security reasons, giving each container only the API server permissions
    it requires is important, so we should create a separate ServiceAccount for each
    application or cluster component to do that. The information for these ServiceAccounts
    is then added to the Deployment or other controller so that Kubernetes will inject
    the correct credentials. In some cases, we may use multiple ServiceAccount with
    a single application, restricting each application component to only the access
    it needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to using a separate ServiceAccount per application or component,
    it’s also good practice to use a separate Namespace per application. As we’ll
    see in a moment, permissions can be limited to a single Namespace. Let’s start
    by creating the Namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'A ServiceAccount uses a bearer token, which is stored in a secret automatically
    generated by Kubernetes when the ServiceAccount is created. Let’s make a ServiceAccount
    for a Deployment that we’ll create in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '*read-pods-sa.yaml*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we use the metadata to place this ServiceAccount in the `sample`
    Namespace we just created. We could also use the `-n` flag with `kubectl` to specify
    the Namespace. We’ll use the usual `kubectl apply` to create this ServiceAccount:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'When the ServiceAccount is created, the controller manager detects this and
    automatically creates a Secret with the credentials:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Note that in addition to the `read-pods` ServiceAccount we just created, there
    is already a `default` ServiceAccount. This account was created automatically
    when the Namespace was created; it will be used if we don’t specify to Kubernetes
    which ServiceAccount to use for a Pod.
  prefs: []
  type: TYPE_NORMAL
- en: The newly created ServiceAccount does not have any permissions yet. To start
    adding permissions, we need to take a look at *role-based access control* (RBAC).
  prefs: []
  type: TYPE_NORMAL
- en: Role-Based Access Controls
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After the API server has found an authentication plug-in that can identify the
    client, it uses the identity to determine whether the client has permissions to
    perform the desired action, which is done by assembling a list of roles that belong
    to the user. Roles can be associated directly with a user or with a group in which
    the user is a member. Group membership is part of the identity. For example, client
    certificates can specify a user’s groups by including organization fields as part
    of the certificate’s subject.
  prefs: []
  type: TYPE_NORMAL
- en: Roles and Cluster Roles
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Each role has a set of permissions. A permission allows a client to perform
    one or more actions on one or more types of resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, let’s define a role that will give a client permission to read
    Pod status. We have two choices: we can create a *Role* or a *ClusterRole*. A
    Role is visible and usable within a single Namespace, whereas a ClusterRole is
    visible and usable across all Namespaces. This difference allows administrators
    to define common roles across the cluster that are immediately available when
    new Namespaces are created, while also allowing the delegation of access control
    for a specific Namespace.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example definition of a ClusterRole. This role only has the ability
    to read data about Pods; it cannot change Pods or access any other cluster information:'
  prefs: []
  type: TYPE_NORMAL
- en: '*pod-reader.yaml*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Because this is a cluster-wide role, it doesn’t make sense to assign it to a
    Namespace, so we don’t specify one.
  prefs: []
  type: TYPE_NORMAL
- en: The critical part of this definition is the list of rules. Each ClusterRole
    or Role can have as many rules as necessary. Each rule has a list of `verbs` that
    define what actions are allowed. In this case, we identified `get`, `watch`, and
    `list` as the verbs, with the effect that the role allows reading Pods but not
    any actions that would modify them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each rule applies to one or more resource types, based on the combination of
    `apiGroups` and `resources` identified. Each rule gives permissions for the actions
    listed as `verbs`. In this case, the empty string `""` is used to refer to the
    default API group, which is where Pods are located. If we wanted to also include
    Deployments and StatefulSets, we would need to define our rule as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We need to add `"apps"` to the `apiGroups` field because Deployment and StatefulSet
    are part of that group (as identified in the `apiVersion` when we declare the
    resource). When we declare a Role or ClusterRole, the API server will accept any
    strings in the `apiGroups` and `resources` fields, regardless of whether the combination
    actually identifies any resource types, so it’s important to pay attention to
    which group a resource is in.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s define our `pod-reader` ClusterRole:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Now that the ClusterRole exists, we can apply it. To do that, we need to create
    a role binding.
  prefs: []
  type: TYPE_NORMAL
- en: Role Bindings and Cluster Role Bindings
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let’s apply this `pod-reader` ClusterRole to the `read-pods` ServiceAccount
    we created earlier. We have two options: we can create a *RoleBinding*, which
    will assign the permissions in a specific Namespace, or a *ClusterRoleBinding*,
    which will assign the permissions across all Namespaces. This feature is beneficial
    because it means we can create a ClusterRole such as `pod-reader` once and have
    it visible across the cluster, but create the binding in an individual Namespace
    so that users and ServiceAccount are restricted to only the Namespaces they should
    be allowed to access. This helps us apply the pattern we saw earlier of having
    a Namespace per application, while at the same time it keeps non-administrators
    away from key infrastructure components such as the components running in the
    `kube-system` Namespace.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In keeping with this practice, we’ll create a RoleBinding so that our ServiceAccount
    has permissions to read Pods only in the `sample` Namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '*read-pods-bind.yaml*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Not surprisingly, a RoleBinding ties together a Role or a ClusterRole and a
    subject. The RoleBinding can contain multiple subjects, so we can bind the same
    role to multiple users or groups with a single binding.
  prefs: []
  type: TYPE_NORMAL
- en: We define a Namespace in both the metadata and where we identify the subject.
    In this case, these are both `sample`, as we want to grant the ServiceAccount
    the ability to read Pod status in its own Namespace. However, these could be different
    to allow a ServiceAccount in one Namespace to have specific permissions in another
    Namespace. And of course we could also use a ClusterRoleBinding to give out permissions
    across all Namespaces.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now create the RoleBinding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We’ve now given permission for the `read-pods` ServiceAccount to read Pods in
    the `sample` Namespace. To demonstrate how it works, we need to create a Pod that
    is assigned to the `read-pods` ServiceAccount.
  prefs: []
  type: TYPE_NORMAL
- en: Assigning a Service Account to Pods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To assign a ServiceAccount to a Pod, just add the `serviceAccountName` field
    to the Pod spec:'
  prefs: []
  type: TYPE_NORMAL
- en: '*read-pods-deploy.yaml*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The ServiceAccount identified must exist in the Namespace that the Pod is created
    in. Kubernetes will inject the Pod’s containers with the Service-Account token
    so that the containers can authenticate to the API server.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s walk through an example to show how this works and how the authorization
    is applied. Start by creating this Deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: This creates an Alpine container running `sleep` that we can use as a base for
    shell commands.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get to a shell prompt, we’ll first get the generated name of the Pod and
    then use `kubectl exec` to create the shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The ServiceAccount token is mounted in the directory */run/secrets/kubernetes.io/serviceaccount*,
    so change to that directory and list its contents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: These files show up as odd looking symbolic links, but the contents are there
    as expected. The *ca.crt* file is the root certificate for the cluster, which
    is needed to trust the connection to the API server.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s save the token in a variable so that we can use it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now use this token with `curl` to connect to the API server. First,
    though, we need to install `curl` into our Alpine container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Our ServiceAccount is allowed to perform `get`, `list`, and `watch` operations
    on Pods. Let’s list all Pods in the `sample` Namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: As with the bootstrap token, we use HTTP Bearer authentication to pass the ServiceAccount
    token to the API server. Because we’re operating from inside a container, we can
    use the standard address `kubernetes.default.svc` to find the API server. This
    works because a Kubernetes cluster always has a service in the `default` Namespace
    that routes traffic to API server instances using the Service networking we saw
    in [Chapter 9](ch09.xhtml#ch09).
  prefs: []
  type: TYPE_NORMAL
- en: 'The `curl` command is successful because our ServiceAccount is bound to the
    `pod-reader` Role we created. However, the RoleBinding is limited to the `sample`
    Namespace, and as a result, we aren’t allowed to list Pods in a different Namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: We can use the error message to be certain that our ServiceAccount assignment
    and authentication worked as expected because the API server recognizes us as
    the `read-pods` ServiceAccount. However, we don’t have a RoleBinding with the
    right permissions to read Pods in the `kube-system` Namespace, so the request
    is rejected.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, because we have permission only for Pods, we can’t list our Deployment,
    even though it is also in the `sample` Namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The slightly different path scheme for the URL, starting with */apis/apps/v1*
    instead of */api/v1*, is needed because Deployments are in the `apps` API group
    rather than the default group. This command fails in a similar way because we
    don’t have the necessary permissions to list Deployments.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’re finished with this shell session, so let’s exit it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Before we leave the RBAC topic, though, let’s illustrate an easy way to grant
    normal user permissions for a Namespace without allowing any administrator functions.
  prefs: []
  type: TYPE_NORMAL
- en: Binding Roles to Users
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To grant normal user permissions, we’ll leverage an existing ClusterRole called
    `edit` that’s already set up to grant view and edit permissions for most of the
    resource types users need.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a quick look at the `edit` ClusterRole to see what permissions it
    has:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The full list has a large number of different rules, each with its own set of
    permissions. The subset in this example shows just one rule, used to provide edit
    permission for Pods.
  prefs: []
  type: TYPE_NORMAL
- en: Some commands related to Pods, such as `exec`, are listed separately to allow
    for more granular control. For example, for a production system, it can be useful
    to allow some individuals the ability to create and delete Pods and see logs,
    but not provide the ability to use `exec`, because that might be used to access
    sensitive production data.
  prefs: []
  type: TYPE_NORMAL
- en: Previously, we created a user called *me* and saved the client certificate to
    a file called *kubeconfig*. However, we didn’t bind any roles to that user yet,
    so the user has only the very limited permissions that come with automatic membership
    in the *system:authenticated* group.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, as we saw earlier, our normal user can’t even list Pods in the
    `default` Namespace. Let’s bind this user to the edit role. As before, we’ll use
    a regular RoleBinding, scoped to the `sample` Namespace, so this user won’t be
    able to access our cluster infrastructure components in the `kube-system` Namespace.
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 11-1](ch11.xhtml#ch11list1) presents the RoleBinding we need.'
  prefs: []
  type: TYPE_NORMAL
- en: '*edit-bind.yaml*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 11-1: Bind the edit role to a user*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we apply this RoleBinding to add permissions to our user:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We’re now able to use this user to view and modify Pods, Deployments, and many
    other resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'However, because we used a RoleBinding and not a ClusterRoleBinding, this user
    has no visibility into other Namespaces:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The error message displayed by `kubectl` is identical in form to the `message`
    field that is part of the API server’s JSON response. This is not a coincidence;
    `kubectl` is a friendly command line interface in front of the API server’s REST
    API.
  prefs: []
  type: TYPE_NORMAL
- en: Final Thoughts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The API server is an essential component in the Kubernetes control plane. Every
    other service in the cluster is continuously connected to the API server, watching
    the cluster for changes, so it can take appropriate action. Users also use the
    API server to deploy and configure applications and to monitor state. In this
    chapter, we saw the underlying REST API that the API server provides to create,
    retrieve, update, and delete resources. We also saw the extensive authentication
    and authorization capabilities built in to the API server to ensure that only
    authorized users and services can access and modify the cluster state.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we’ll examine the other side of our cluster’s infrastructure:
    the node components. We’ll see how the `kubelet` Service hides any differences
    between container engines and how it uses the container capabilities we saw in
    [Part I](part01.xhtml#part01) to create, start, and configure containers in the
    cluster.'
  prefs: []
  type: TYPE_NORMAL
