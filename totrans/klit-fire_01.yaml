- en: '1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Time Is a Flat Circle
  prefs: []
  type: TYPE_NORMAL
- en: In the summer of 2016, I found myself sitting in front of the weirdest system
    I had ever encountered as a software engineer. There was a fairly banal web application
    written in Java that was connecting to what I would eventually figure out was
    a mainframe. The mainframe itself wasn’t the weird part. When you venture into
    the world of legacy modernization, you quickly realize that mainframes are still
    *everywhere*—in banks, in government, buried deep in the foundation of civil society.
    Having a web application send requests to a mainframe wasn’t so weird. I had a
    hard time accepting that a technology designed for bulk transactions would respond
    quickly enough to meet the demands of a website at a reasonable scale, but despite
    my concerns, it did appear to be doing okay.
  prefs: []
  type: TYPE_NORMAL
- en: No, what was weird was that the mainframe in question was from the 1960s and
    storing data on magnetic tape. There was no way *that* mainframe could respond
    quickly enough, so when I saw this on the architecture diagrams, I focused on
    a group of mysterious machines that were sitting in the middle; a modern web application
    was on one side and an ancient mainframe on the other.
  prefs: []
  type: TYPE_NORMAL
- en: 'The only information I had about this cluster of machines was the acronym the
    organization used for it. Nobody on the engineering teams I was working with seemed
    to know what the machines did. It took a lot of digging through several decades
    of documentation before I figured out what they were: *Unisys ClearPath Dorados*.
    In other words, they were more mainframes, newer ones, that were effectively configured
    like a cache in front of the old mainframe. That was how 60-year-old code was
    responding fast enough to serve requests from the modern internet. The organization
    had a new machine sitting in between that was storing a temporary copy of the
    relevant data. About once a week, the new mainframes would request an update from
    the older mainframe.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When I asked an engineer who worked on this system what he thought about this
    arrangement, he said something that has stuck with me ever since and ultimately
    changed my understanding of modernizing legacy computer systems: “Well, how is
    the cloud any different from old time-sharing schemes on mainframes?”'
  prefs: []
  type: TYPE_NORMAL
- en: 'The answer is that it isn’t really. Both of these approaches charge you for
    time spent on shared resources maintained by a much larger institution. You are
    connecting over the same lines of communication, sometimes with the same protocols.
    The client/server model is virtually the same; only the interfaces and programming
    languages are different. On this point, the engineer added another interesting
    observation: “We started with thin-client mainframe green-screen terminal applications,
    then they wanted us to migrate to fat clients on PCs, now they want APIs with
    thin clients again.”'
  prefs: []
  type: TYPE_NORMAL
- en: The first mistake software engineers make with legacy modernization is assuming
    technical advancement is linear. With that frame of mind, anything built in older
    design patterns or with an older architectural philosophy is inferior to newer
    options. Improving the performance of an old system is just a matter of rearranging
    it to a new pattern.
  prefs: []
  type: TYPE_NORMAL
- en: My experiences dealing with Frankenstein systems like the one described taught
    me that progress in technology is not linear. It’s cyclical. We advance, but we
    advance slowly, while moving tangentially. We abandon patterns only to reinvent
    them later and sell them as completely new.
  prefs: []
  type: TYPE_NORMAL
- en: Technology advances not by building on what came before, but by *pivoting* from
    it. We take core concepts from what exists already and modify them to address
    a gap in the market; then we optimize around filling in that gap until that optimization
    has aggregated all the people and use cases not covered by the new tech into its
    own distinct market that another “advancement” will capture.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, the arms race around data centers left smaller organizations
    behind and created a demand for the commercial cloud. Optimizing the cloud for
    customization and control created the market for managed platforms and eventually
    serverless computing. The serverless model will feed its consumers more and more
    development along its most appealing features until the edge cases where serverless
    approaches don’t quite fit start to find common ground among each other. Then
    a new product will come out that will address those needs.
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging What Came Before
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most people realize that technology can be invented and not become popular until
    much later, but they typically attribute this effect to the lack of vision of
    the inventor or deficiencies of skills within the marketing department or the
    maturity level of the technology itself.
  prefs: []
  type: TYPE_NORMAL
- en: Economists have a different explanation for adoption rates of new technology.
    They typically describe it as the contrast between alignable and nonalignable
    differences. *Alignable differences* are those for which the consumer has a reference
    point. For example, *this* car is faster than *that* car, or *this* phone has
    a better camera than *that* phone.
  prefs: []
  type: TYPE_NORMAL
- en: '*Nonalignable differences* are characteristics that are wholly unique and innovative;
    there are no reference points with which to compare. You might assume that nonalignable
    differences are more appealing to potential consumers. After all, there’s no competition!
    You’re doing things differently from everyone else. But when it comes time to
    make a purchasing decision, if there is no comparison, there is no clear sense
    of value. How does one judge the worth of something—and therefore estimate the
    trade-offs of buying it at a particular price—that has no equivalent? For a nonalignable
    difference to make an impact, the estimated value it produces has to be greater
    than all the alignable differences and all the other nonalignable differences
    put together.^([1](#c01-footnote-1))'
  prefs: []
  type: TYPE_NORMAL
- en: Consumers just aren’t confident about having to do such guesswork.^([2](#c01-footnote-2))
    It increases the risk of buyer’s remorse, and reasoning about needs and utility
    makes consumers uncomfortable. Therefore, products of all kinds differentiate
    themselves on the market by finding specific characteristics that can be labeled
    as different from characteristics of existing solutions. This pushes technology
    into cycles. People do not get exactly the same experience from the same products.
    As a company iterates to improve a certain characteristic of the product, it ultimately
    makes the product less desirable for the group of existing customers. Companies
    do this with the hope that a larger group of new customers will make that loss
    irrelevant.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the time this gradual optimization only creates annoyances that play
    themselves out over social media and eventually die down. Occasionally, there
    are enough people who have experienced a loss in utility from the optimization
    that they themselves become a potential market to be captured. That includes consumers
    who never bought the product in the first place but would have if it had been
    optimized in some other way. Leveraging alignable differences is pushing the product
    further away from what those consumers want to buy, but creating an opportunity
    for another company to figure out.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following: Is it better to have a small cellphone or a large cellphone?'
  prefs: []
  type: TYPE_NORMAL
- en: The world’s first commercially available cellphone was Motorola’s DynaTAC 800\.
    It was the big brick phone now used to signal the 1980s in satirical pieces. More
    than 10 inches tall, it wasn’t the sort of thing one could easily carry around
    in a pocket. Obviously, market demands would push cellphones to grow smaller.
    By 1994, IBM’s Simon had gotten them down to 8 inches tall and added the industry’s
    first attempt at smart features such as sending faxes and emails, maintaining
    a calendar, keeping notes, reading the news, and viewing stock prices. Despite
    those impressive advancements, the Simon was quickly made irrelevant by the flip
    phone, which took the 8-inch size and literally folded it in half, making it similar
    in size to the average width and the depth of a pants pocket.^([3](#c01-footnote-3))
    Companies went from selling tens of thousands to millions of devices.
  prefs: []
  type: TYPE_NORMAL
- en: 'I was in high school during this time, and despite the ubiquity of them now,
    a cellphone was not something a kid in the 1990s would have considered a worthwhile
    purchase. I had only one friend who had a cellphone, and it served two primary
    functions for him: something for when his car broke down driving back and forth
    to his part-time job at Taco Bell, and playing a grayscale version of *Snake*
    during class. For me and my peers, though, we were just as likely to run into
    the people we might want to call in the hallway between classes, or we could communicate
    with them through other means that our parents paid for. Texting was not a medium
    we considered for the cellphone. Pagers worked just as well.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, a Pew Research study revealed that *landlines* were still the communication
    medium of choice for American teenagers almost a decade later in 2009.^([4](#c01-footnote-4))
    More than half of teenagers had never even sent a text message. By this point,
    the iPhone had been on the market for two years, and it was in its third iteration.
    A year later, a follow-up report from Pew painted a completely different picture:
    cellphone use in the United States was rapidly growing among teenagers, overtaking
    all other communication options.^([5](#c01-footnote-5))'
  prefs: []
  type: TYPE_NORMAL
- en: What happened?
  prefs: []
  type: TYPE_NORMAL
- en: An internet meme goes around every now and again that pegs the pivot point on
    cellphone size at around 2005 with the words “Here we realize we can see porn
    in the mobile.” In reality, screen sizes varied up until 2010 with plenty of options
    from the all-encompassing touch screen look to more modest interfaces with physical
    keyboards that were optimized for business use cases. Teenagers were lingering
    as an underserved market until screen size started to increase and cameras became
    more of a first-class feature of phones. What teenagers wanted to use cellphones
    for was sharing pictures and videos with each other.^([6](#c01-footnote-6)) Once
    it became clear there was a market to capture by selling phones as entertainment
    devices, cellphones abruptly stopped shrinking and started growing. Innovations
    around resolution, display, and camera quality accelerated.
  prefs: []
  type: TYPE_NORMAL
- en: It’s tempting to look at these trends and assume the technology simply matured
    to the point where it was able to find and seize its market. But the data doesn’t
    actually bear that out. Nokia’s N95 offered a 5-megapixel (MP) camera in 2007\.
    Shortly thereafter, the first-generation iPhone came out with a 2MP camera, and
    the HTC Dream was released in 2008 with 3.15MP camera. In 2010, the iPhone and
    HTC would debut front-facing cameras with 0.3MP and 1.3MP, respectively. The technology
    on the market wasn’t getting better; it was briefly getting worse.^([7](#c01-footnote-7))
  prefs: []
  type: TYPE_NORMAL
- en: Simply producing a cellphone that was geared toward teenagers in the 1990s or
    the early 2000s would not have led to an explosion of growth. Teenagers had no
    strong reference point for cellphones. From their viewpoint, there were no alignable
    differences attractive enough to justify the expense. It was only once their parents’
    devices became prevalent in cultural references and everyday life that the market
    potential of teenagers and the large screens necessary to capture their interest
    were unlocked.
  prefs: []
  type: TYPE_NORMAL
- en: Every feature that took off with younger American users had existed before 2010\.
    Cameras have been on phones since 2000, and they sold well. Phones that streamed
    live broadcasts were already available in 2004 when I was living in Japan. It
    was not some impressive technical advancement that shifted the market. The growing
    ubiquity of cellphones in daily life had primed a new, more lucrative market to
    force the design of cellphones to do a complete 180.
  prefs: []
  type: TYPE_NORMAL
- en: The history of technology is filled with about-faces like this. A certain approach
    or technique becomes popular but doesn’t fit everyone’s use case. Companies start
    experimenting and applying that hot new approach to more and more things until
    the number of situations where that approach does not work or is not ideal grows
    into a force that reverses momentum. The industry rediscovers a different way
    of doing things and swings back.
  prefs: []
  type: TYPE_NORMAL
- en: Engineers praised the publish/subscribe model of Kafka as superior to the hub-and-spoke
    model of Enterprise Service Buses (ESBs). ESBs were a single point of failure
    and an anti-pattern for service-oriented architecture. Then Kafka added its Connect
    framework (version 0.9) and its Streams API (0.10), which reintroduced many of
    the core concepts of ESBs. Google developed Accelerated Mobile Pages to advance
    asynchronous loading through JavaScript and then added server-side rendering to
    them—breaking its own spec to move back to a pattern already established by HTML.
  prefs: []
  type: TYPE_NORMAL
- en: Market shifts are complex events. We can see the pattern of technology cycling
    through the same approaches and structures over and over, but these shifts are
    less about the superiority of a given characteristic and more about how potential
    consumers organize themselves.
  prefs: []
  type: TYPE_NORMAL
- en: The User as the Market Co-Creator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Broadly, these kinds of complex compounding shifts are referred to as *Service
    Dominate Logic**(S-D Logic*). S-D Logic says that consumer value is not created
    by companies producing products but by an active collaboration between many actors.
    According to S-D Logic, consumers are not passive, thoughtless sheep whose wants
    and desires are engineered for them by industry. Instead, consumers actively participate
    in creating the markets that are leveraged to sell them things.
  prefs: []
  type: TYPE_NORMAL
- en: Consumers and companies create value largely by playing off one another. Anyone
    who has ever tried to start their own business will tell you the existence of
    a problem does not mean there’s a market for solving it. In 2004, the inability
    to stream TV and movies from a handheld device easily was not a problem consumers
    had much interest in having solved. The technology to do so existed by then, but
    no one was willing to invest hard-earned dollars in a solution to a problem that
    everyone else had to deal with too. Once cellphones solved the practical problem
    of keeping in touch with the office when on the move, they appeared in the field
    of vision of huge numbers of other consumers. This caused other needs to begin
    to consolidate into marketable problems. You are unlikely to put much thought
    into the problem of not being able to watch the newest episode of your favorite
    TV show while flying across the country. But, if you know that other people have
    such an option and you are missing out, the solution suddenly becomes much more
    marketable.
  prefs: []
  type: TYPE_NORMAL
- en: The Mainframe and the Cloud
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Sometimes, however, the centrifugal forces that govern progress are a lot more
    basic and fundamental. Let’s go back to my story at the beginning of this chapter:
    Why did we migrate from time-sharing on mainframes to bulky applications on personal
    computers to time-sharing on the commercial cloud? We could have, for example,
    continued to develop mainframes until they became clouds. Why didn’t we? Why did
    we spend millions of dollars migrating thousands of applications to a new paradigm
    only to have to migrate them all back to thinner clients a decade or two later?'
  prefs: []
  type: TYPE_NORMAL
- en: Technology is, and probably always will be, an expensive element of any organization’s
    operational model. A great deal of advancement and market co-creation in technology
    can be understood as the interplay between hardware costs and network costs. Computers
    are data processors. They move data around and rearrange it into different formats
    and displays for us. That’s about all they do, regardless of whether we use them
    to play video games or crunch spreadsheets.
  prefs: []
  type: TYPE_NORMAL
- en: 'All advancements with data processors come down to one of two things: either
    you make the machine faster or you make the pipes delivering data to the machine
    faster. These forces cannot grow independently of each other. If the pipes pumping
    data in get too far ahead of the chips processing data, the machine crashes. If
    the machine gets too far ahead of the network, the user experiences no actual
    value add from the increases in speed.'
  prefs: []
  type: TYPE_NORMAL
- en: When it becomes possible to create alignable differences by unlocking available
    improvements in either network speeds or hardware speeds, the whole industry tends
    to change paradigms to optimize for that improvement. In doing so, it creates
    a market for the next shift by leaving some potential customers and use cases
    behind.
  prefs: []
  type: TYPE_NORMAL
- en: Mainframes in their heyday existed in a world where processor power was limited.
    Having enough of it to make offloading calculations to a machine required investing
    in a whole room full of equipment and specialized operators, which were expensive.
    The market invested heavily in making the hardware faster. The lowest-hanging
    fruit in that endeavor was actually just making the chips smaller so you could
    pack more of them into the same machine. Doing this did not immediately result
    in smaller mainframes, but rather it expanded the market to capture different
    price points so that large organizations would still pay millions of dollars,
    while small organizations could be persuaded to pay tens of thousands to have
    mainframes of their own. This exposed computers to a larger audience and stimulated
    the market for what eventually became the PC. Even still, there wasn’t any need
    to make improvements to network speeds, because the computers were slow and couldn’t
    store much data. A supercomputer in 1985, for example, had about as much processing
    power as an early-generation iPhone. A more typical computer from that era might
    have a few hundred kilobytes of RAM and storage. The National Science Foundation
    Network, which would eventually become the backbone of the early internet, offered
    56kbps in the 1980s. At that speed, it would have taken only about an hour to
    transfer an entire computer’s worth of data across the network.
  prefs: []
  type: TYPE_NORMAL
- en: Eventually, decades of engineering work changed that relationship. Faster, more
    powerful computers were now waiting for their data to come across the network.
    More and more of these machines were smaller and cheaper with endlessly growing
    storage capacity. As the number of machines increased, the load on their networks
    also increased. The more computers connected to a network at the same time, the
    slower the network becomes. It was going to take time for the demands for more
    speed to produce a market response, so the industry optimized by shifting toward
    applications that stored more data locally on the machine itself. We walked away
    from applications that run on a centralized computer that we communicated with
    across a network. If we don’t need to move data across the network, network speed
    doesn’t hold us back.
  prefs: []
  type: TYPE_NORMAL
- en: The Flat-Rate Internet
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At this point, we don’t have a cycle; we have a transition. The industry preferences
    shifted from processing on big centralized machines to smaller, cheaper, local
    workstations. It’s not a spoiler to say that it shifted back because the internet
    got faster and cheaper, but was that inevitable?
  prefs: []
  type: TYPE_NORMAL
- en: It’s unlikely that the private sector ever would have built the internet once
    it had unlocked the personal computing market. Computer manufacturers were benefiting
    much more financially from their proprietary standards at the time. The core innovation
    of the internet was the networking of many different types of networks into one
    inter-network (hence the name *internet*), which required common standards open
    to all manufacturers. Building a network that would scale to cross a single country
    was itself a significant engineering challenge. In fact, many national computer
    network projects were attempted during the same period as the internet. The United
    Kingdom had one; France had two; the Soviet Union had three failed attempts. The
    United States ultimately prevailed because it was not trying to build a national
    network; it was simply trying to solve compatibility issues caused by all the
    proprietary standards computer manufacturers were pushing. The US military had
    funded a number of expensive computers, and it wanted research institutions hundreds
    of miles away from one another to be able to share those resources. Had it been
    left up to the computer manufacturers, they obviously would have preferred that
    all the research institutions bought their own machines.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, the internet was built. Slow and cumbersome at first, without
    a specific business implementation, it filled up with scholars, hobbyists, futurists,
    and weirdos. Whereas saturation of the business market unlocked consumers’ desire
    for cellphones as personal entertainment devices, and saturation of the mainframe
    computer market got smaller organizations with fewer resources looking for smaller
    machines, the internet penetrated the business market through a slightly anarchist
    creative community. In 2000, 76 percent of online users were connecting from homes,
    while 41 percent were connecting from businesses. By 2014, home internet use had
    exploded to 90 percent, and work internet use remained stagnant at 44 percent.^([8](#c01-footnote-8))
  prefs: []
  type: TYPE_NORMAL
- en: What’s interesting about the internet is that it is the only modern-day communication
    medium that has been historically flat-rate priced.^([9](#c01-footnote-9)) All
    packets on the internet are billed basically the same way, regardless of what
    they are or where they are going.^([10](#c01-footnote-10)) By contrast, you pay
    more when you call long-distance versus placing a local call, or you pay more
    when connecting to a cell network in a foreign country versus your own. On the
    internet, consumers pay more to get faster speeds. That put the pressure on telecommunication
    companies to compete by making connections faster. The faster the internet became,
    the more people put on it. The more content that was on the internet, the more
    consumers started logging on. The more people trying to access a given resource
    on the internet, the more expensive hosting those resources on your own machines
    became. Eventually, this flipped the value proposition of the computer industry
    by making it cheaper to process data “in the cloud” than it was to process it
    locally. We returned to the notion of renting time on expensive computers someone
    else owns versus assuming the costs of buying, maintaining, and upgrading those
    expensive computers ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: One can track how architectural paradigms fall in and out of favor roughly by
    whether processing power and storage capacity are growing faster than network
    speeds; however, faster processors are often a component of what telecoms use
    to boost their network speeds. This kind of interdependency is true for basically
    any market. Product development shifts consumer behavior, which shifts product
    development. Technology doesn’t advance in a straight line, because a straight
    line is not actually efficient. The economy is not a flat plane, but a rich topography
    with ridges and valleys to navigate around.
  prefs: []
  type: TYPE_NORMAL
- en: 'The factors that influence shifts are also fractal and interdisciplinary in
    nature. The reason American internet service providers (ISPs) settled on a flat-rate
    pricing structure early on is that the landscape offered them two options for
    building the network of communication lines necessary to be in business in the
    first place: either they built their own links or they rented other people’s links.
    In the latter category, a wide variety of options were available, not just existing
    telecom networks built for the telephone, like AT&T, but also private lines maintained
    by large institutions to connect their data centers. A company like AOL in the
    1990s was both in competition with telecoms to sell internet access and a customer
    of those same telecoms. This made ISPs much more sensitive to customer feedback
    and the psychological draw of simple, flat pricing a necessity.^([11](#c01-footnote-11))
    Among other things, charging for usage levels means services grow more expensive
    as they get worse. More activity by more users ultimately leads to congestion
    and degrades network performance. The fact that users have trouble correctly estimating
    their usage levels and that the majority of them pay more with flat prices further
    incentivized the industry. If you want a quick demonstration of that, look up
    your real cellular data usage and compare it to what limits you are paying for
    monthly. Most users never come close to exhausting it.'
  prefs: []
  type: TYPE_NORMAL
- en: In Europe, it was far more common for telecoms to be government-run, which meant
    less competition forcing simpler pricing models. The European Union would eventually
    liberalize the internet market in the late 1990s, while the United States allowed
    broadband to consolidate. As a result, a lot more competition in Europe has subsequently
    pushed speeds up and prices down. Today, the internet is faster in many places
    than it is in the United States. What this means for future shifts in the tech
    industry is unclear. Any number of factors can change which paradigms are being
    pushed as best practices. Exposure to technology can create a new market, and
    that market could run parallel as mainframes and PCs still do, or it could completely
    overtake another market, just as entertainment-optimized cellphones wiped out
    BlackBerries and other business-focused phones. Prices could drop. Resources could
    become scarce. Rarely if ever are these changes fueled by pure technical superiority.
  prefs: []
  type: TYPE_NORMAL
- en: Migrating for Value, Not for Trends
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What does any of this have to do with legacy modernization? When people assume
    that technology advances in a linear fashion, they also assume that anything new
    is naturally more advanced and better than whatever they are currently running.
    Adopting new practices doesn’t necessarily make technology better, but doing so
    almost always makes technology more complicated, and more complicated technology
    is hard to maintain and ultimately more prone to failure.
  prefs: []
  type: TYPE_NORMAL
- en: And yet, information technology that never changes is doomed. It’s important
    to understand that we advance in cycles, because that’s the only way we learn
    how to avoid unnecessary rewrites and partial migrations. Changing technology
    should be about real value and trade-offs, not faulty assumptions that newer is
    by default more advanced.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes it is difficult to compare your use case to the use cases of other
    seemingly similar organizations. The biggest offender on this front is the commercial
    cloud, precisely because it adds value to such a broad set of use cases. People
    tend to assume that means it is a superior technology for all use cases, which
    is not true. I have a friend who runs a Hadoop cluster to process financial data
    for the Department of Treasury. Her chief information officer (CIO) insists that
    they need to shut down the servers they maintain to move this process to the cloud.
    What the CIO doesn’t appreciate is that moving data, while cheaper and easier
    than it was in the 1980s, is still expensive. There’s no question that speed and
    performance are better if you’re processing data in the same place that you’re
    storing it—in this case, on site. Whether Big Data as a Service saves you any
    money depends on how big your big data actually is, where it is centralized, and
    how long it takes it to get that big in the first place. Having petabytes of data
    collected over a five-year period is a different situation from having petabytes
    generated over the course of a few hours.
  prefs: []
  type: TYPE_NORMAL
- en: Value propositions are often complicated questions for this reason. It’s hard
    enough for a purely technical organization to get it right; it’s even harder at
    organizations where the only people with enough knowledge to advise on these issues
    are vendors.
  prefs: []
  type: TYPE_NORMAL
