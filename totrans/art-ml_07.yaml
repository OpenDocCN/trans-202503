- en: '**5'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A STEP BEYOND K-NN: DECISION TREES**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In k-NN, we looked at the neighborhood of the data point to be predicted. Here
    again we will look at neighborhoods, but in a more sophisticated way. This approach
    will be easy to implement and explain, lends itself to nice pictures, and has
    more available hyperparameters with which to fine-tune it.
  prefs: []
  type: TYPE_NORMAL
- en: Here we will introduce *decision trees (DTs)*, one of the mainstays in the ML
    field. Besides being used directly, DTs are also the basis for *random forests*
    and *gradient boosting*, which we will cover in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Basics of Decision Trees
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Though some ideas had been proposed earlier, the DT approach became widely used
    due to the work of statisticians Leo Breiman, Jerry Friedman, Richard Olshen,
    and Chuck Stone. They called their method *classification and regression trees
    (CART)* and described it in their book *Classification and Regression Trees* (Wadsworth,
    1984).
  prefs: []
  type: TYPE_NORMAL
- en: A DT method basically sets up the prediction process as a flow chart, hence
    the name *decision tree*. For instance, look at [Figure 5-1](ch05.xhtml#ch05fig01)
    in [Section 5.2.1](ch05.xhtml#ch05lev2sec1). There we are predicting ozone level
    from features such as temperature and wind speed. In predicting a new case, we
    start at the top of the tree and follow some path to the bottom, making decisions
    along the way as to whether to turn left or right. At the bottom of the tree,
    we make our prediction.
  prefs: []
  type: TYPE_NORMAL
- en: We produce a tree using our training set data. The top of the tree (the *root
    node*) contains all of that data. We then split the data into two parts according
    to whether some feature is smaller or larger than a given value. This creates
    two new nodes, below and to the left or right of the root node. Then we split
    each of *those* parts into two further parts and so on. Thus an alternative name
    for the process is *recursive partitioning*.
  prefs: []
  type: TYPE_NORMAL
- en: At each step, we have the option of stopping—that is, making no further splits
    along that particular path or branch within the tree. In that case, the non-split
    node is called a *leaf* or *terminal node* of the tree. Any given branch of the
    tree will end at some leaf node.
  prefs: []
  type: TYPE_NORMAL
- en: In the end, to predict a new case, we start at the root of the tree and work
    our way down to a leaf. Our predicted *Y* value then depends on the type of application.
    In numeric- *Y* cases, our predicted *Y* is then the average of all the *Y* values
    in that node. For classification applications, our predicted *Y* value is the
    class that is most numerous in the given leaf node. Or equivalently, express *Y*
    as dummy variables and take the average of each dummy. This gives us the probabilities
    of the various classes, and we set the predicted class to be the one of largest
    probability.
  prefs: []
  type: TYPE_NORMAL
- en: It is in this sense that DTs are analogous to k-NN. A leaf node serves as analogous
    to the neighborhood concept of k-NN.
  prefs: []
  type: TYPE_NORMAL
- en: Various schemes have been devised to decide (a) *whether* to split a given node
    in the tree, and (b) if so, *how* to do the split. More on this shortly.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 The qeDT() Function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: R’s CRAN repository has several DT packages, but two I like especially are `partykit`
    and its earlier version, `party`. (These names are a pun on the term *recursive
    partitioning*.) Our `qe*`-series function `qeDT()` wraps `party::ctree()`. To
    illustrate, let’s run an example from the package.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset here, `airquality`, is built into R and looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Our goal is to predict ozone level from the other features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Since this is such a small dataset, we decided against having a holdout set.
  prefs: []
  type: TYPE_NORMAL
- en: We predict new data points as usual (after all, the `qe*`-series is supposed
    to give a uniform interface to the various functions they wrap). Say we have a
    new day to predict, with values the same as in `airq[1,]` but with wind at 8.8
    miles per hour. What value would we predict for the ozone level?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We would predict ozone at about 18.5 parts per million.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you know, `qe*`-series functions are wrappers, and their return objects
    usually include a component containing the return object from the wrapped function.
    This is the case here for `qeDT()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Here `ctout` is the object returned by `ctree()` when the latter is invoked
    from `qeDT()`. By the way, `ctout` is of class `'party'`.
  prefs: []
  type: TYPE_NORMAL
- en: We are using default hyperparameters here and might get better predictions with
    a better set of them. More on this in [Section 5.6](ch05.xhtml#ch05lev6), but
    let’s focus now on how the tree process works by plotting the flow chart.
  prefs: []
  type: TYPE_NORMAL
- en: '***5.2.1 Looking at the Plot***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Most DT packages allow you to plot the tree, which sometimes can provide useful
    insights for the analyst. In our setting here, though, we will use the plot to
    gain a better understanding of how the DT process works.
  prefs: []
  type: TYPE_NORMAL
- en: 'The call is simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As mentioned before, `plot()` is an R *generic function* (that is, a placeholder).
    The above call is dispatched to `plot.qeDT(dtout)`. And since the latter has been
    written to call `plot()` on the `ctout` component, in the end, that `plot()` call
    above will eventually be dispatched to `plot.party()`.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 5-1](ch05.xhtml#ch05fig01) shows the plot. As we are just getting an
    overview now, don’t try to grasp the entire picture in a single glance.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch05fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-1: Sample plot from*'
  prefs: []
  type: TYPE_NORMAL
- en: A DT indeed takes the form of a flow chart. For a day with given levels of `Solar.R`,
    `Wind`, and so on, what value should we predict for `Ozone`? The graph shows our
    prediction procedure.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s see what happens when we predict a new point, say, `w` from above.
    We start at the root, Node 1, and look at `Temp`. Since the value of the latter
    for `w` is 67, which is smaller than 82 degrees, we go left, to Node 2\. There
    we ask whether `Wind` is less than or equal to 6.9 miles per hour. It’s 8.8, so
    we go right, to Node 4, where we are told to compare `Temp` to 77\. Again, the
    value in `w` is 67, so we go left, to Node 5.
  prefs: []
  type: TYPE_NORMAL
- en: We saw earlier that our predicted value was 18.47917\. How did the tree produce
    this from Node 5?
  prefs: []
  type: TYPE_NORMAL
- en: 'Our predicted value will be the mean *Y* value for all training set data points
    in Node 5\. There is information in `dtout` as to which data points are in that
    node. Specifically, the `termNodeMembers` component of `qeDT()` output is an R
    list, with one element for each tree node. To gain an understanding of the workings
    of that function, let’s check Node 5 “by hand”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We see that 48 data points of `airq` ended up in Node 5, specifically the points
    `airq[1,]`, `airq[2,]`, and so on. DT then computes the mean *Y* for these points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This matches the value we obtained from `predict()`.
  prefs: []
  type: TYPE_NORMAL
- en: '5.3 Example: New York City Taxi Data'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s try all this on a larger dataset. Fortunately for us data analysts, the
    New York City Taxi and Limousine Commmission makes available voluminous data on
    taxi trips in the city.^([1](footnote.xhtml#ch5fn1b)) A small portion of that
    data is available as `yell10k` in the `regtools` package.
  prefs: []
  type: TYPE_NORMAL
- en: That dataset consists of 10,000 random records from the January 2019 dataset.
    It retains only 7 of the original 18 features, and some date conversion has been
    done.
  prefs: []
  type: TYPE_NORMAL
- en: It would be nice if taxi operators had an app to predict travel time, which
    many passengers may wish to know. This will be our goal here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Here `PU` and `DO` mean “pickup” and “dropoff.” Trip distance is in miles, and
    trip time is in seconds.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, trip distance is not enough; the pickup and dropoff locations
    are important, as some parts of the city may be slower to navigate than others.
    The original data also had time of day, which is important but not used here for
    simplicity.
  prefs: []
  type: TYPE_NORMAL
- en: '***5.3.1 Pitfall: Too Many Combinations of Factor Levels***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now, note the location IDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: There are potentially 29,315 pickup and dropoff combinations! Since we have
    only *n* = 10000 data points, we risk serious overfitting problems. And at the
    very least, having so many potential tree nodes will affect run time on the training
    set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, when I tried this with the `partykit` package rather than `party`,
    I encountered an error message: “Too many levels.” The documentation recommends
    using `party` in such cases, but even then we would likely run into trouble with
    larger datasets of this type.'
  prefs: []
  type: TYPE_NORMAL
- en: This suggests possible use of consolidation or embedding (see [Section 4.3.1](ch04.xhtml#ch04lev3sec1)).
    We may, for instance, wish to form groups of contiguous locations. Or we could
    try an embedding—that is, replacing location IDs by latitude and longitude. But
    let’s see what happens without taking such measures.
  prefs: []
  type: TYPE_NORMAL
- en: '***5.3.2 Tree-Based Analysis***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As noted, this dataset may present challenges, especially regarding possible
    overfitting issues. Let’s give it a try:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Not bad; we cut MAPE in half by using the features here. Again, we might do
    considerably better with nondefault hyperparameter combinations, as well as by
    adding some of the features in the original dataset that are not in `yell10k`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset, even in the scaled-down form we are using here, is far too complex
    for plotting its tree. We can still display it in printed form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Though the display is quite complex even in printed form, forcing only a partial
    listing here, and though it contains some quantities we have not yet described,
    one may still glean some interesting information. First we see that there were
    40 terminal nodes, as opposed to 5 in our previous example, reflecting the greater
    complexity of this dataset. (There are 79 nodes in the entire tree, as can be
    seen by typing `dtout$nNodes`.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, we see in part how that reduction was accomplished: DT was able to
    form its own groups of pickup and dropoff locations, such as in Node 9:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We go left if `DOLocationID` is one of 13, 68, and so on, and otherwise go right.
    This addresses our concerns in [Section 5.3.1](ch05.xhtml#ch05lev3sec1). The DT
    grouped the locations for us! No wonder DTs are so popular!
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*If we type an expression when we are in R interactive mode, R prints that
    expression. Here we typed* dtout*, so it’s equivalent to typing* print(dtout)*.
    But* print() *is yet another R generic function, and we will thus have a similar
    chain of calls as for* plot() *above, ending with* print.party(dtout$ctout)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: One thing worth checking in DT analysis is the numbers of data points in the
    various leaf nodes. Say some node has rather few data points. That’s analogous
    to having too few points in a k-NN neighborhood. Just as we can try different
    values of *k* in the latter case, here we may wish to tweak some DT hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll look at hyperparameters in [Section 5.6](ch05.xhtml#ch05lev6), but for
    now, let’s see how to check for leaf nodes with rather few data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: There are a few small nodes, notably Node 78 with only 10 data points. This
    is a possible reason to tweak the hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: '5.4 Example: Forest Cover Data'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another UCI dataset, Covertype, aims to “[predict] forest cover type from cartographic
    variables only.”^([2](footnote.xhtml#ch5fn2)) The idea is that one might use remote
    sensing to determine what kinds of grasses there are in difficult-to-access regions.
    There are 581,012 data points, with 54 features, such as elevation, hillside shade
    at noon, and distance to the nearest surface water. There are seven different
    cover types, which are stored in column 55.
  prefs: []
  type: TYPE_NORMAL
- en: This example is useful for a number of reasons. Here we’ll see DT in action
    in a classification problem, with multiple classes, and of a size larger than
    what we’ve seen so far. And besides, what could be better in a chapter on trees
    than data on forests!
  prefs: []
  type: TYPE_NORMAL
- en: 'Input the data, say, with `data.table::fread()` for speed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The class, in column `V55`, was read in as an integer, whereas `qe*`-series
    functions need *Y* to be R factors in classification problems. We could have used
    `fread()`’s `colClasses` argument, but let’s just fix it directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'There are seven classes, but some are much more common than others:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Cover types 1 and 2 are the most numerous.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since both *n* and *p* are large, let’s run on a random subset of 50,000 records
    to more conveniently illustrate the ideas. This approach is also common in data
    analysis: do a preliminary analysis on a subset of the data, again for convenience,
    but then do a more thorough analysis on the full data.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Again, we are doing much better with the features (25 percent error rate) than
    without them (51 percent).
  prefs: []
  type: TYPE_NORMAL
- en: 'We might also look at the confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Class 1 is often mispredicted as Class 2, and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the larger sample size *n* and number of features *p* here, a really large
    tree might be generated. In fact, it is much larger than in our previous examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The tree has 1,000 nodes, and about half of those are terminal nodes!
  prefs: []
  type: TYPE_NORMAL
- en: '5.5 Decision Tree Hyperparameters: How to Split?'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: DT packages differ from one another in terms of the details of their node-splitting
    actions. In most cases, the process is quite complex and thus beyond the scope
    of this book. The splitting process in `party` is no exception, but we need to
    have at least a rough overview of the process. We will focus on a major splitting
    criterion in `party` known as the *p-value*.
  prefs: []
  type: TYPE_NORMAL
- en: Look again at [Figure 5-1](ch05.xhtml#ch05fig01). The oval contents show that
    the feature used to split is `Wind`, with a p-value of 0.002 and with a split
    point of 6.9\. But originally, as the tree was being built, that oval was empty,
    with no lines emanating out of the bottom. How, then, was this node built to what
    we see in the figure?
  prefs: []
  type: TYPE_NORMAL
- en: 'Node 2 inherited data points from the left branch out of Node 1\. Then the
    following algorithm was run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We do the following on the output of the above pseudocode:'
  prefs: []
  type: TYPE_NORMAL
- en: If the smallest p-value is below a user-specified criterion, split the node
    using whichever feature and split point yielded the smallest p-value (in this
    case, `Wind` and 6.9).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If, on the other hand, the smallest p-value was not smaller than the user-specified
    criterion, do not split the node.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We see, for instance, that for Node 2 and the potential (and later, actual)
    splitting feature `Wind`, there are many candidates for a potential split point:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Any of the values 2.8, 3.4, . . . , 20.1 could be used. The algorithm takes
    each one into consideration.
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, we would like the split to produce two approximately balanced subsets,
    say, with a split at 9.7\. But a more urgent requirement is that the two subsets
    differ a lot in their mean values of *Y*. If mean *Y* is fairly similar in the
    two candidate subsets, the node is deemed homogeneous and not split—at least for
    that feature.
  prefs: []
  type: TYPE_NORMAL
- en: Well, then, what constitutes differing by “a lot”? This is decided by a formal
    statistical significance test. This book does not assume a background in statistics,
    and, for our purposes here, we just state that a test is summarized by a number
    known as a p-value.
  prefs: []
  type: TYPE_NORMAL
- en: Testing has come under much criticism in recent years, and for good reason,
    in my opinion (see the file *NoPVals.md* in `regtools`). However, for the node-splitting
    purpose here, the p-value threshold is just another hyperparameter, named `alpha`
    in `qeDT()`. This default value is 0.05.
  prefs: []
  type: TYPE_NORMAL
- en: If the p-value is less than `alpha` for some candidate feature and candidate
    split point pair, then the node is deemed worth splitting. The feature and split
    point chosen are the pair with the smallest p-value. We see in [Figure 5-1](ch05.xhtml#ch05fig01)
    that the minimum p-value happened to be 0.002, which was associated with the `Wind`
    feature and a split point of 6.9\. Since 0.002 < 0.05, the node was split accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: If no split point meets the above criterion, the node is not split. That happened
    in Node 3, so it became a terminal node.
  prefs: []
  type: TYPE_NORMAL
- en: 5.6 Hyperparameters in the qeDT() Function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As noted, DTs may be viewed as an extension of the k-NN idea. Each leaf node
    forms a kind of neighborhood whose data points have similar values of certain
    features. Recall from [Chapter 3](ch03.xhtml) that small neighborhoods lead to
    larger variance in a predicted *Y* value—just too small a sample to work from—
    while large neighborhoods may have bias problems (that is, points in the same
    neighborhood may be quite different from each other and thus not representative).
  prefs: []
  type: TYPE_NORMAL
- en: In a DT context, then, we should look at the leaf nodes to consider the Bias-Variance
    Trade-off. If there are too many small terminal nodes, we risk a variance problem,
    while too many large terminal nodes may mean a bias issue.
  prefs: []
  type: TYPE_NORMAL
- en: Here is where hyperparameters come into play. They control the tree configuration
    in various ways, and we can use cross-validation to choose the tree configuration
    with the best predictive ability.
  prefs: []
  type: TYPE_NORMAL
- en: 'The general call form is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The `data`, `yName`, and `holdout` arguments are common to all the `qe*`-series
    functions. The remainder, `alpha`, `minsplit`, `minbucket`, `maxdepth`, and `mtry`,
    all deal with splitting criteria. Here are their roles:'
  prefs: []
  type: TYPE_NORMAL
- en: alpha   As explained above.
  prefs: []
  type: TYPE_NORMAL
- en: minsplit   Here we can specify the minimum size for any node. The default of
    20 means that we will not allow any node splitting to result in a node with fewer
    than 20 data points.
  prefs: []
  type: TYPE_NORMAL
- en: minbucket   Like `minsplit`, but specifically for terminal nodes.
  prefs: []
  type: TYPE_NORMAL
- en: maxdepth   Maximum number of levels or rows of the tree. In [Figure 5-1](ch05.xhtml#ch05fig01),
    we have 4 levels, with the root in Level 1 and the leaf nodes in Level 4.
  prefs: []
  type: TYPE_NORMAL
- en: mtry   If this is nonzero, it is the number of features to try at each node;
    see below.
  prefs: []
  type: TYPE_NORMAL
- en: 'If `mtry` is nonzero, our splitting algorithm changes a bit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This adds some randomness to the tree construction process, a step toward the
    ML method of *random forests*. We will see in the next chapter why this may be
    useful, but for the strict DT method, it is usually not used.
  prefs: []
  type: TYPE_NORMAL
- en: Consider each of the above hyperparameters in terms of the Bias-Variance Trade-off.
    Say we wish to make the leaf nodes smaller. All else being equal, we could accomplish
    this by making `alpha` larger, `minsplit` smaller, `minbucket` smaller, `maxdepth`
    larger, and `mtry` larger (or 0).
  prefs: []
  type: TYPE_NORMAL
- en: For instance, with a larger `alpha`, more p-values will be below this high threshold,
    so it is more likely that a node will split. As we go further down a tree, fewer
    data points remain, so if we encourage splits, when we reach a node that can’t
    be split, it won’t have many points left in it.
  prefs: []
  type: TYPE_NORMAL
- en: These hyperparameters don’t work independently of each other, so setting too
    many of them probably becomes redundant.
  prefs: []
  type: TYPE_NORMAL
- en: 5.7 Conclusions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Decision trees play a fundamental role in ML, and we will see them again in
    our material on bagging and boosting. As with any ML algorithm, we must deal with
    various hyperparameters, another topic to be viewed in depth later in the book.
  prefs: []
  type: TYPE_NORMAL
