<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><h2 class="h2" id="ch00"><span epub:type="pagebreak" id="page_xxiii"/><strong>INTRODUCTION</strong></h2>&#13;
<div class="image1"><img src="../images/common.jpg" alt="Image" width="252" height="252"/></div>&#13;
<p class="noindent">Thanks to rapid advancements in deep learning, we have seen a significant expansion of machine learning and AI in recent years.</p>&#13;
<p class="indent">This progress is exciting if we expect these advancements to create new industries, transform existing ones, and improve the quality of life for people around the world. On the other hand, the constant emergence of new techniques can make it challenging and time-consuming to keep abreast of the latest developments. Nonetheless, staying current is essential for professionals and organizations that use these technologies.</p>&#13;
<p class="indent">I wrote this book as a resource for readers and machine learning practitioners who want to advance their expertise in the field and learn about techniques that I consider useful and significant but that are often overlooked in traditional and introductory textbooks and classes. I hope you’ll find this book a valuable resource for obtaining new insights and discovering new techniques you can implement in your work.</p>&#13;
<h3 class="h3" id="ch00lev1"><strong>Who Is This Book For?</strong></h3>&#13;
<p class="noindent">Navigating the world of AI and machine learning literature can often feel like walking a tightrope, with most books positioned at either end: broad beginner’s introductions or deeply mathematical treatises. This book <span epub:type="pagebreak" id="page_xxiv"/>illustrates and discusses important developments in these fields while staying approachable and not requiring an advanced math or coding background.</p>&#13;
<p class="indent">This book is for people with some experience with machine learning who want to learn new concepts and techniques. It’s ideal for those who have taken a beginner course in machine learning or deep learning or have read an equivalent introductory book on the topic. (Throughout this book, I will use <em>machine learning</em> as an umbrella term for machine learning, deep learning, and AI.)</p>&#13;
<h3 class="h3" id="ch00lev2"><strong>What Will You Get Out of This Book?</strong></h3>&#13;
<p class="noindent">This book adopts a unique Q&amp;A style, where each brief chapter is structured around a central question related to fundamental concepts in machine learning, deep learning, and AI. Every question is followed by an explanation, with several illustrations and figures, as well as exercises to test your understanding. Many chapters also include references for further reading. These bite-sized nuggets of information provide an enjoyable jumping-off point on your journey from machine learning beginner to expert.</p>&#13;
<p class="indent">The book covers a wide range of topics. It includes new insights about established architectures, such as convolutional networks, that allow you to utilize these technologies more effectively. It also discusses more advanced techniques, such as the inner workings of large language models (LLMs) and vision transformers. Even experienced machine learning researchers and practitioners will encounter something new to add to their arsenal of techniques.</p>&#13;
<p class="indent">While this book will expose you to new concepts and ideas, it’s not a math or coding book. You won’t need to solve any proofs or run any code while reading. In other words, this book is a perfect travel companion or something you can read on your favorite reading chair with your morning coffee or tea.</p>&#13;
<h3 class="h3" id="ch00lev3"><strong>How to Read This Book</strong></h3>&#13;
<p class="noindent">Each chapter of this book is designed to be self-contained, offering you the freedom to jump between topics as you wish. When a concept from one chapter is explained in more detail in another, I’ve included chapter references you can follow to fill in gaps in your understanding.</p>&#13;
<p class="indent">However, there’s a strategic sequence to the chapters. For example, the early chapter on embeddings sets the stage for later discussions on self-supervised learning and few-shot learning. For the easiest reading experience and the most comprehensive grasp of the content, my recommendation is to approach the book from start to finish.</p>&#13;
<p class="indent">Each chapter is accompanied by optional exercises for readers who want to test their understanding, with an answer key located at the end of the book. In addition, for any papers referenced in a chapter or further reading on that chapter’s topic, you can find the complete citation information in that chapter’s “References” section.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_xxv"/>The book is structured into five main parts centered on the most important topics in machine learning and AI today.</p>&#13;
<p class="indent"><strong><a href="part01.xhtml">Part I: Neural Networks and Deep Learning</a></strong> covers questions about deep neural networks and deep learning that are not specific to a particular subdomain. For example, we discuss alternatives to supervised learning and techniques for reducing overfitting, which is a common problem when using machine learning models for real-world problems where data is limited.</p>&#13;
<p class="noindentin"><strong><a href="ch01.xhtml">Chapter 1: Embeddings, Latent Space, and Representations</a></strong>   Delves into the distinctions and similarities between embedding vectors, latent vectors, and representations. Elucidates how these concepts help encode information in the context of machine learning.</p>&#13;
<p class="noindentin"><strong><a href="ch02.xhtml">Chapter 2: Self-Supervised Learning</a></strong>    Focuses on self-supervised learning, a method that allows neural networks to utilize large, unlabeled datasets in a supervised manner.</p>&#13;
<p class="noindentin"><strong><a href="ch03.xhtml">Chapter 3: Few-Shot Learning</a></strong>    Introduces few-shot learning, a specialized supervised learning technique tailored for small training datasets.</p>&#13;
<p class="noindentin"><strong><a href="ch04.xhtml">Chapter 4: The Lottery Ticket Hypothesis</a></strong>    Explores the idea that randomly initialized neural networks contain smaller, efficient subnetworks.</p>&#13;
<p class="noindentin"><strong><a href="ch05.xhtml">Chapter 5: Reducing Overfitting with Data</a></strong>    Addresses the challenge of overfitting in machine learning, discussing strategies centered on data augmentation and the use of unlabeled data to reduce overfitting.</p>&#13;
<p class="noindentin"><strong><a href="ch06.xhtml">Chapter 6: Reducing Overfitting with Model Modifications</a></strong>    Extends the conversation on overfitting, focusing on model-related solutions like regularization, opting for simpler models, and ensemble techniques.</p>&#13;
<p class="noindentin"><strong><a href="ch07.xhtml">Chapter 7: Multi-GPU Training Paradigms</a></strong>    Explains various training paradigms for multi-GPU setups to accelerate model training, including data and model parallelism.</p>&#13;
<p class="noindentin"><strong><a href="ch08.xhtml">Chapter 8: The Success of Transformers</a></strong>    Explores the popular transformer architecture, highlighting features like attention mechanisms, parallelization ease, and high parameter counts.</p>&#13;
<p class="noindentin"><strong><a href="ch09.xhtml">Chapter 9: Generative AI Models</a></strong>    Provides a comprehensive overview of deep generative models, which are used to produce various media forms, including images, text, and audio. Discusses the strengths and weaknesses of each model type.</p>&#13;
<p class="noindentin"><strong><a href="ch10.xhtml">Chapter 10: Sources of Randomness</a></strong>    Addresses the various sources of randomness in the training of deep neural networks that may lead to inconsistent and non-reproducible results during both training and inference. While randomness can be accidental, it can also be intentionally introduced by design.</p>&#13;
<p class="indentt"><strong><a href="part02.xhtml">Part II: Computer Vision</a></strong> focuses on topics mainly related to deep learning but specific to computer vision, many of which cover convolutional neural networks and vision transformers.<span epub:type="pagebreak" id="page_xxvi"/></p>&#13;
<p class="noindentin"><strong><a href="ch11.xhtml">Chapter 11: Calculating the Number of Parameters</a></strong>    Explains the procedure for determining the parameters in a convolutional neural network, which is useful for gauging a model’s storage and memory requirements.</p>&#13;
<p class="noindentin"><strong><a href="ch12.xhtml">Chapter 12: Fully Connected and Convolutional Layers</a></strong>    Illustrates the circumstances in which convolutional layers can seamlessly replace fully connected layers, which can be useful for hardware optimization or simplifying implementations.</p>&#13;
<p class="noindentin"><strong><a href="ch13.xhtml">Chapter 13: Large Training Sets for Vision Transformers</a></strong>    Probes the rationale behind vision transformers requiring more extensive training sets compared to conventional convolutional neural networks.</p>&#13;
<p class="indentt"><strong><a href="part03.xhtml">Part III: Natural Language Processing</a></strong> covers topics around working with text, many of which are related to transformer architectures and self-attention.</p>&#13;
<p class="noindentin"><strong><a href="ch14.xhtml">Chapter 14: The Distributional Hypothesis</a></strong>    Delves into the distributional hypothesis, a linguistic theory suggesting that words appearing in the same contexts tend to possess similar meanings, which has useful implications for training machine learning models.</p>&#13;
<p class="noindentin"><strong><a href="ch15.xhtml">Chapter 15: Data Augmentation for Text</a></strong>    Highlights the significance of data augmentation for text, a technique used to artificially increase dataset sizes, which can help with improving model performance.</p>&#13;
<p class="noindentin"><strong><a href="ch16.xhtml">Chapter 16: Self-Attention</a></strong>    Introduces self-attention, a mechanism allowing each segment of a neural network’s input to refer to other parts. Self-attention is a key mechanism in modern large language models.</p>&#13;
<p class="noindentin"><strong><a href="ch17.xhtml">Chapter 17: Encoder- and Decoder-Style Transformers</a></strong>    Describes the nuances of encoder and decoder transformer architectures and explains which type of architecture is most useful for each language processing task.</p>&#13;
<p class="noindentin"><strong><a href="ch18.xhtml">Chapter 18: Using and Fine-Tuning Pretrained Transformers</a></strong>    Explains different methods for fine-tuning pretrained large language models and discusses their strengths and weaknesses.</p>&#13;
<p class="noindentin"><strong><a href="ch19.xhtml">Chapter 19: Evaluating Generative Large Language Models</a></strong>    Lists prominent evaluation metrics for language models like Perplexity, BLEU, ROUGE, and BERTScore.</p>&#13;
<p class="indentt"><strong><a href="part04.xhtml">Part IV: Production and Deployment</a></strong> covers questions pertaining to practical scenarios, such as increasing inference speeds and various types of distribution shifts.</p>&#13;
<p class="noindentin"><strong><a href="ch20.xhtml">Chapter 20: Stateless and Stateful Training</a></strong>    Distinguishes between stateless and stateful training methodologies used in deploying models.</p>&#13;
<p class="noindentin"><strong><a href="ch21.xhtml">Chapter 21: Data-Centric AI</a></strong>    Explores data-centric AI, which priori-tizes refining datasets to enhance model performance. This approach <span epub:type="pagebreak" id="page_xxvii"/>contrasts with the conventional model-centric approach, which emphasizes improving model architectures or methods.</p>&#13;
<p class="noindentin"><strong><a href="ch22.xhtml">Chapter 22: Speeding Up Inference</a></strong>    Introduces techniques to enhance the speed of model inference without tweaking the model’s architecture or compromising accuracy.</p>&#13;
<p class="noindentin"><strong><a href="ch23.xhtml">Chapter 23: Data Distribution Shifts</a></strong>    Post-deployment, AI models may face discrepancies between training data and real-world data distributions, known as data distribution shifts. These shifts can deteriorate model performance. This chapter categorizes and elaborates on common shifts like covariate shift, concept drift, label shift, and domain shift.</p>&#13;
<p class="indentt"><strong><a href="part05.xhtml">Part V: Predictive Performance and Model Evaluation</a></strong> dives deeper into various aspects of squeezing out predictive performance, such as changing the loss function, setting up <em>k</em>-fold cross-validation, and dealing with limited labeled data.</p>&#13;
<p class="noindentin"><strong><a href="ch24.xhtml">Chapter 24: Poisson and Ordinal Regression</a></strong>    Highlights the differences between Poisson and ordinal regression. Poisson regression is suitable for count data that follows a Poisson distribution, like the number of colds contracted on an airplane. In contrast, ordinal regression caters to ordered categorical data without assuming equidistant categories, such as disease severity.</p>&#13;
<p class="noindentin"><strong><a href="ch25.xhtml">Chapter 25: Confidence Intervals</a></strong>    Delves into methods for constructing confidence intervals for machine learning classifiers. Reviews the purpose of confidence intervals, discusses how they estimate unknown population parameters, and introduces techniques such as normal approximation intervals, bootstrapping, and retraining with various random seeds.</p>&#13;
<p class="noindentin"><strong><a href="ch26.xhtml">Chapter 26: Confidence Intervals vs. Conformal Predictions</a></strong>    Discusses the distinction between confidence intervals and conformal predictions and describes the latter as a tool for creating prediction intervals that cover actual outcomes with specific probability.</p>&#13;
<p class="noindentin"><strong><a href="ch27.xhtml">Chapter 27: Proper Metrics</a></strong>    Focuses on the essential properties of a proper metric in mathematics and computer science. Examines whether commonly used loss functions in machine learning, such as mean squared error and cross-entropy loss, satisfy these properties.</p>&#13;
<p class="noindentin"><strong><a href="ch28.xhtml">Chapter 28: The <em>k</em> in <em>k</em>-Fold Cross-Validation</a></strong>    Explores the role of the <em>k</em> in <em>k</em>-fold cross-validation and provides insight into the advantages and disadvantages of selecting a large <em>k</em>.</p>&#13;
<p class="noindentin"><strong><a href="ch29.xhtml">Chapter 29: Training and Test Set Discordance</a></strong>    Addresses the scenario where a model performs better on a test dataset than the training dataset. Offers strategies to discover and address discrepancies between training and test datasets, introducing the concept of adversarial validation. <span epub:type="pagebreak" id="page_xxviii"/></p>&#13;
<p class="noindentin"><strong><a href="ch30.xhtml">Chapter 30: Limited Labeled Data</a></strong>    Introduces various techniques to enhance model performance in situations where data is limited. Covers data labeling, bootstrapping, and paradigms such as transfer learning, active learning, and multimodal learning.</p>&#13;
<h3 class="h3" id="ch00lev4"><strong>Online Resources</strong></h3>&#13;
<p class="noindent">I’ve provided optional supplementary materials on GitHub with code examples for certain chapters to enhance your learning experience (see <em><a href="https://github.com/rasbt/MachineLearning-QandAI-book">https://github.com/rasbt/MachineLearning-QandAI-book</a></em>). These materials are designed as practical extensions and deep dives into topics covered in the book. You can use them alongside each chapter or explore them after reading to solidify and expand your knowledge.</p>&#13;
<p class="indent">Without further ado, let’s dive in.</p>&#13;
</div>
</div>
</body></html>