- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Newer Applications of Geometry in Machine Learning
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/book_art/chapterart.png)'
  prefs: []
  type: TYPE_IMG
- en: In [Chapter 5](c05.xhtml), we explored the contributions of metric geometry
    to machine learning and its myriad uses in model measurements and input. However,
    geometry has provided many other contributions to machine learning; in this chapter,
    we’ll explore tangent-space-based approaches to model estimation, exterior calculus,
    tools related to the intersection of curves (which can be used to replace linear
    algebra in algorithms), and rank-based models that involve vector fields acting
    on datasets’ tangent spaces. We’ll see how these tools can help in supervised
    learning on small datasets, help communities plan for disasters, and discern choice
    preferences of customers.
  prefs: []
  type: TYPE_NORMAL
- en: Working with Nonlinear Spaces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our first tool helps mathematicians and machine learning engineers work with
    nonlinear spaces such as manifolds; it’s the definition of a point’s *tangent
    space*. Thinking back to calculus classes, we recall the *tangent lines* of a
    function are lines that touch a point on a curve without crossing the curve—where
    the slope of the curve equals the slope of the tangent line (giving the first
    derivative of the curve). Consider the sine wave example and a point on that sine
    wave, along with its tangent line, shown in [Figure 6-1](#figure6-1).
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c06/f06001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-1: A sine wave example with tangent line drawn at one of the local
    maxima'
  prefs: []
  type: TYPE_NORMAL
- en: This kind of tangent line works well in two dimensions. However, trying to define
    tangent lines to a point on a surface gets trickier, as many (infinitely many)
    lines can be tangent to a given point (see [Figure 6-2](#figure6-2), point A).
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c06/f06002r.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-2: An ellipse with multiple possible tangent lines through point A'
  prefs: []
  type: TYPE_NORMAL
- en: In fact, the lines in [Figure 6-2](#figure6-2) form a two-dimensional plane
    tangent to point A, akin to a sheet of paper that touches the ellipse at point
    A. What one could do is glue this *tangent plane* to point A, as in [Figure 6-3](#figure6-3).
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c06/f06003r.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-3: An ellipse with point A and the tangent plane associated with point
    A that extends tangent lines to tangent planes and spaces'
  prefs: []
  type: TYPE_NORMAL
- en: In the case of higher-dimensional objects, the tangent spaces can grow to involve
    more dimensions (it can be 3-dimensional, 100-dimensional, or even an infinite-dimensional
    box). These tangent spaces have a nice connection to linear algebra. Remember
    that a vector space can be defined by a set of independent vectors collected into
    a matrix, called the *basis* of the space, technically known as *Hamel basis*.
    The basis for the tangent space of an object at a point, in fact, is the set of
    a point’s partial derivatives. As mentioned earlier, in a one-dimensional space,
    this is exactly the slope of the tangent line. This gives a nice Euclidean space
    associated with each point on the manifold, which can be used to derive unit distances
    between points, provide mappings to a Euclidean space from a manifold, and understand
    multicollinearity. Multicollinearity occurs when variables are strongly correlated,
    which results in matrix columns or rows that are identical or nearly identical
    (causing singular matrices). Multicollinearity is a problem for regression-based
    algorithms, as it leads to redundant predictors and singular matrices. Variables
    with perfect overlap of variance (highly collinear predictors) will have the same
    tangent space or at least share some overlapping tangent space.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing dgLARS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One useful machine learning algorithm based on tangent spaces is the *dgLARS*
    algorithm (dgLARS stands for “differential geometry least angle regression”).
    dgLARS extends traditional least angle regression (LARS) to an algorithm that
    fits to a given model’s error tangent space. The LARS algorithm traditionally
    starts with each coefficient in the regression model set to 0, with predictors
    added progressively according to which predictor is most correlated with the outcome;
    coefficients are adjusted through least squares computation until a higher correlation
    enters the model. When multiple predictors have entered the model, coefficients
    are increased in joint least squares directions.
  prefs: []
  type: TYPE_NORMAL
- en: dgLARS considers the model’s tangent space, scaling the score function used
    to optimize the coefficients. Each update to the model is done using the square
    root of a tool called the *conditional Fisher information*. The conditional Fisher
    information roughly measures the amount of information a given variable contains
    relative to a target (such as an outcome variable). For more technical-minded
    readers, the Fisher information of a parameter is the variance of the parameter’s
    score, which is the partial derivative of that parameter with respect to the natural
    logarithm of the likelihood function.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s make this concrete with an example. Say we are creating a model to understand
    factors that impact adolescent risk-taking behaviors, such as drug use or petty
    crime. We may have many known factors measured already (such as family socioeconomic
    status, secondary school grades, and prior school or legal incidents). However,
    we’d like to compare an index from a survey we’ve designed to measure risk propensity
    in adolescents (index 1) to a known index that measures risk-taking in adults
    (index 2). Both surveys likely capture different types of information and different
    levels of relevant information with respect to our risk outcomes ([Figure 6-4](#figure6-4)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c06/f06004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-4: Comparison of number of questions loading onto a risk index'
  prefs: []
  type: TYPE_NORMAL
- en: In the [Figure 6-5](#figure6-5) example, survey 1’s index contains a greater
    volume of information. However, there may be some overlap between the variables
    or irrelevant information in survey 2’s index, and it would be nice to have a
    measurement that can capture such information. Perhaps there is some overlap of
    information between the questions asked and some irrelevant information in each
    survey as well. Let’s consider unique and relevant information contained in each
    risk-propensity survey ([Figure 6-5](#figure6-5)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c06/f06005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-5: Two adjusted measures of relevant information captured in index
    questions'
  prefs: []
  type: TYPE_NORMAL
- en: From [Figure 6-5](#figure6-5), we see that index 1 and index 2 contain some
    irrelevant information and some overlapping information. Index 1 does seem to
    contain a bit more information than index 2, and if we had to choose which survey
    to administer to a larger population of at-risk adolescents, we’d be better off
    starting with index 1.
  prefs: []
  type: TYPE_NORMAL
- en: This is a bit how Fisher information and variable selection in dgLARS happen.
    Technically, a *score* is calculated through partial derivatives of the model’s
    log likelihood function, and the variance of this score is the Fisher information,
    which can be entered into a matrix to capture information across partial derivatives
    of the model. Interestingly, this matrix can also be derived as the Hessian of
    the relative entropy (Kullback–Leiber divergence), and the Fisher information
    gives the curvature of relative entropy in this case. At a less technical level,
    the Fisher information used to select variables has to do with both a statistical
    measure (the log likelihood function) and the information geometry of the independent
    variables being considered.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of generalized linear models, the Fisher information matrix can
    be used to derive a score called the *conditional Rao score*, which can test whether
    the coefficient for a given variable is statistically different from 0\. If the
    score is not statistically different from 0, the variable is dropped from consideration
    in the model. In the dgLARS algorithm, these calculations are done by searching
    coefficient vectors in the model error’s tangent space, starting with the null
    model. This space’s geometry is then iteratively partitioned into three sets:
    *selected predictors*, which have good fit scores in the error tangent space;
    *redundant predictors*, which share an error tangent space with selected predictors;
    and *nonselected predictors*, which have poor fit scores in the error tangent
    space. Thus, dgLARS leverages information about the model’s geometry to find a
    best-fitting model.'
  prefs: []
  type: TYPE_NORMAL
- en: dgLARS has had success on problems involving more predictors than observations
    in datasets, and many of the publications on this algorithm focus on genomics
    applications, where the number of patients might be 300 and the number of genes
    sequences might be in the 1,000,000 range. R provides a package, dglars, that
    implements this algorithm for generalized linear models, including link functions
    for logistic regression, Poisson regression, linear regression, and gamma regression.
    For those unfamiliar with generalized linear regression, a *link function* essentially
    is a special type of transformation of a dependent variable that allows regression
    to work mathematically for count variables, binary variables, and other types
    of not normally distributed dependent variables (with some restrictions on the
    geometry of the distribution).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve covered some of the theory, let’s put it into practice.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting Depression with dgLARS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’d like to predict self-reported depression based on school issues and IQ.
    The data we’ll use is self-reported school issues in a self-selected sample of
    profoundly gifted Quora users (with IQs above 155), including seven main school
    issues (bullying, teacher hostility, boredom, depression, lack of motivation,
    outside learning, put in remediation courses). The data was reported across 22
    individuals who provided scores in the profoundly gifted range and discussed at
    least one of the issues of interest in a school system, with a bias toward users
    in the United States. This dataset was collected from the platform and processed
    manually to obtain a final dataset with categories of school issues for posters
    who met the IQ criterion. You can find the dataset in the book’s files. Let’s
    load the data first with the code in [Listing 6-1](#listing6-1).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 6-1: A script that loads the Quora dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code in [Listing 6-1](#listing6-1) loads a dataset examining educational
    interventions and a psychological metric of self-esteem. Now let’s modify the
    script to run the dgLARS algorithm on the dataset. The R package gives us the
    option of doing cross-validation or running the algorithm without cross-validation.
    Let’s modify [Listing 6-1](#listing6-1) to run both model options:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This runs the cross-validated and non-cross-validated dgLARS algorithms on
    the full set of student data. The cross-validated version does not work well on
    small datasets with sparse predictors, so if you run into an error, keep trying
    to run the cross-validated version, as some partitions may produce an error related
    to splitting and fitting models to the splits. The output from the two dgLARS
    algorithms should agree on many predictors, though the cross-validated split version
    of the algorithm may vary a bit, as the data is randomly partitioned. Let’s add
    our script to look at the summary of the first model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: No factors are selected as important predictors in this model. There are a few
    main reasons why no factors might be selected by the model, including the existence
    of subpopulations with opposite effects that “average out” the angles of the subpopulations,
    the existence of outliers that warp the geometry, or a true null effect for predictors.
    Model fit is reasonable, with an Akaike information criterion (AIC) of 20.86 and
    a residual deviance much smaller than the null deviance with no terms added to
    the model. The dispersion parameter tells us that the data fits reasonably well
    to the binomial distribution (see how `g` is close to 1) without problems in the
    distribution that sometimes occur in real-world data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s add to our script to look at the results from the cross-validated
    trials:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The cross-validated model should give a bit different result than the non-cross-validated
    model, suggesting the models are finding some consistency but not completely overlapping
    across folds. In the cross-validated model, profoundly gifted children put in
    remedial courses tend to have higher rates of depression, and children like the
    ones in this sample who are put in remedial courses and start showing signs of
    depression may benefit from this intervention. As you can see, bullying is also
    a potential issue leading to depression, suggesting something we likely already
    know: that bullying in general shouldn’t be tolerated in a school for optimal
    mental health outcomes.'
  prefs: []
  type: TYPE_NORMAL
- en: Given the small sample size, it’s likely that a generalized linear regression
    model would struggle to estimate the coefficients. The necessary sample size for
    topology- and geometry-based linear models seems to be smaller than linear regression,
    and the consistent results on this problem suggest these models can work on very,
    very small data. However, there is still probably a minimum sample size needed
    for cross-validation, so it’s best to avoid doing even these analyses if the sample
    size is less than 10.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting Credit Default with dgLARS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To understand how dgLARS works on a larger dataset with a binary outcome (logistic
    regression) and more observations and variables, let’s consider another dataset.
    The UCI credit default dataset includes 30,000 credit cases in Taiwan (late 2005)
    and 23 predictors of defaulting, including demographics (age, marriage status,
    education status, and gender), credit limit, and prior usage and payment information.
    The goals of our analysis are to figure out what predictors are related to whether
    an account ends up defaulting and to assess the model fit of our dgLARS model.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started on this with the code in [Listing 6-2](#listing6-2).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 6-2: A script that loads, processes, and analyzes the UCI credit default
    dataset with the dgLARS and cross-validated dgLARS algorithms'
  prefs: []
  type: TYPE_NORMAL
- en: This should yield two models with coefficients for most predictors. The first
    model is the non-cross-validated model version (DG), and the second model is the
    cross-validated version (DG1). [Table 6-1](#table6-1) shows results from our run
    of the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6-1: Coefficients of Terms in the UCI Credit Default dgLARS Model'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Column1** | **DG estimate** | **DG1 estimate** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Int. | –1.47 | –1.45 |'
  prefs: []
  type: TYPE_TB
- en: '| LIMIT_BAL | –0.10 | –0.09 |'
  prefs: []
  type: TYPE_TB
- en: '| SEX | –0.05 | –0.04 |'
  prefs: []
  type: TYPE_TB
- en: '| EDUCATION | –0.08 | –0.07 |'
  prefs: []
  type: TYPE_TB
- en: '| MARRIAGE | –0.08 | –0.07 |'
  prefs: []
  type: TYPE_TB
- en: '| AGE | 0.07 | 0.06 |'
  prefs: []
  type: TYPE_TB
- en: '| PAY_0 | 0.65 | 0.65 |'
  prefs: []
  type: TYPE_TB
- en: '| PAY_2 | 0.10 | 0.09 |'
  prefs: []
  type: TYPE_TB
- en: '| PAY_3 | 0.09 | 0.09 |'
  prefs: []
  type: TYPE_TB
- en: '| PAY_4 | 0.03 | 0.03 |'
  prefs: []
  type: TYPE_TB
- en: '| PAY_5 | 0.04 | 0.04 |'
  prefs: []
  type: TYPE_TB
- en: '| PAY_6 | 0.01 | 0.01 |'
  prefs: []
  type: TYPE_TB
- en: '| BILL_AMT1 | –0.39 | –0.15 |'
  prefs: []
  type: TYPE_TB
- en: '| BILL_AMT2 | 0.16 | 0.02 |'
  prefs: []
  type: TYPE_TB
- en: '| BILL_AMT3 | 0.09 | 0.02 |'
  prefs: []
  type: TYPE_TB
- en: '| BILL_AMT5 | 0.03 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '| BILL_AMT6 | 0.02 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '| PAY_AMT1 | –0.22 | –0.17 |'
  prefs: []
  type: TYPE_TB
- en: '| PAY_AMT2 | –0.22 | –0.19 |'
  prefs: []
  type: TYPE_TB
- en: '| PAY_AMT3 | –0.05 | –0.05 |'
  prefs: []
  type: TYPE_TB
- en: '| PAY_AMT4 | –0.06 | –0.06 |'
  prefs: []
  type: TYPE_TB
- en: '| PAY_AMT5 | –0.05 | –0.05 |'
  prefs: []
  type: TYPE_TB
- en: '| PAY_AMT6 | –0.04 | –0.03 |'
  prefs: []
  type: TYPE_TB
- en: Some of the biggest predictors of default include the prior month’s billing
    and payment history. Those with lower usage (`BILL_AMT1`), lower payments (`PAY_AMT1`),
    and on-time payments (`PAY_0`) are less likely to default on payment in the following
    month. This makes a lot of sense, given that most lending metrics prioritize lending
    at the best rates to those with low loads of debt and a track record of on-time
    payment.
  prefs: []
  type: TYPE_NORMAL
- en: The cross-validated dgLARS model penalizes prior month usage and payment total
    less than the non-cross-validated model, suggesting that prior month on-time payment
    is more important than specific numbers. The AIC score on the first model is fairly
    large (27,924), but it is quite a bit smaller than the null model’s AIC score
    (31,705), indicating a better fit than the null model even with several predictors
    included.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s compare this model with logistic regression and compare the AIC fit
    statistics, adding the following to our script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This snippet of code runs the logistic regression on the dataset and calculates
    the model’s AIC. In this example, the AIC should come out to around 27,925, almost
    exactly that of the dgLARS models. This suggests a convergence of logistic regression
    and the dgLARS algorithm; at this large a sample size, this result is expected.
    Logistic regression is the typical tool for large sample sizes, and it doesn’t
    seem that we get a gain from using dgLARS in this case. However, given the convergence
    on large sample sizes, it’s likely that dgLARS gives quality results at the smaller
    sample sizes that won’t work with logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: Applying Discrete Exterior Derivatives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another useful tool that has come out of differential geometry is *discrete
    exterior derivatives*. Discrete exterior derivatives involve building up discrete
    shapes from lower-dimensional discrete shapes. In prior chapters, we examined
    the concept of homology, which counts the holes in a given object; technically,
    this is done by finding an object’s boundaries at a specific dimension. For instance,
    consider the boundaries of a triangle ([Figure 6-6](#figure6-6)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c06/f06006.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-6: The boundaries of a triangle'
  prefs: []
  type: TYPE_NORMAL
- en: We can take this a step further and break down lines into each point connected
    by the line, as in [Figure 6-7](#figure6-7).
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c06/f06007r.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-7: The boundaries of a line'
  prefs: []
  type: TYPE_NORMAL
- en: Just as we can take apart shapes by identifying and separating out boundaries,
    we can also build shapes up from lower-dimensional boundaries by combining those
    boundaries. Technically, this is called *cohomology*, which is the realm of discrete
    exterior calculus. We might start with two points that are related in some way
    (perhaps within a certain distance of each other or sharing a characteristic)
    and connect them with a line ([Figure 6-8](#figure6-8)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c06/f06008r.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-8: Two points built into a line'
  prefs: []
  type: TYPE_NORMAL
- en: For more technically minded readers, we’re looking at the discrete version of
    differential forms, which are cochains on simplicial complexes. These differential
    forms have vector fields associated with them. We can then define operators that
    change those fields or objects, combine them, or count what exists within a field
    or cochain. This allows us to wrangle certain types of data to understand problems
    like resource capacity in electrical grids or burden of disease within social
    networks (or even rendering graphics across groups of pixels within a computer
    screen).
  prefs: []
  type: TYPE_NORMAL
- en: This can continue up to arbitrarily many dimensions, with lines building up
    triangles and triangles building up tetrahedra and so on. We can also jump levels
    with discrete exterior derivatives, going from points to triangles or tetrahedra
    rather than lines. Thus, discrete data, such as rendering pixel data or engineering
    data, can be grouped and connected for further study.
  prefs: []
  type: TYPE_NORMAL
- en: One of the newer applications of discrete exterior derivatives (and homology)
    is within social network analysis. As we mentioned in prior chapters, graphs are
    discrete objects of zero and one dimension (points and lines); however, connections
    between individuals can be extended from two-way mutual interactions (lines connecting
    points) to cliques of 3-way (triangle) or 4-way (tetrahedron) or 100-way mutual
    interactions (a very large dimensional sort of object), as demonstrated by three
    colleagues mutually connected in [Figure 6-9](#figure6-9).
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c06/f06009.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-9: A graph of three colleagues working on projects together represented
    through a three-way interaction'
  prefs: []
  type: TYPE_NORMAL
- en: On the left of [Figure 6-9](#figure6-9), we see three colleagues (Colleen, Jodelle,
    and Yaé) who collaborate in pairs but have not worked on a project involving all
    three of them. On the right of [Figure 6-9](#figure6-9), we see a representation
    of a project that involves all three colleagues working together. If they collaborate
    on many papers, we can sum up their two-way collaborations and their three-way
    collaborations to get a summary total for each *n*-way collaboration. This might
    be useful for understanding the strength of this collaborative network’s parts.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s explore how these concepts can help in disaster logistics planning. Suppose
    there are four towns in a region with all towns connected to at least one other
    town by a road. Suppose also that each town has its own stock of supplies (perhaps
    liters of water for each resident) in case a cyclone hits the region and limits
    transportation for days or weeks. We can model this by creating a graph in R using
    the code in [Listing 6-3](#listing6-3).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 6-3: A script that generates the example graph of connected towns,
    plots the graph, adds resources to each town, visualizes these resources, and
    analyzes mutually connected town resources'
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 6-3](#listing6-3) creates a matrix of towns connected by roads and
    then converts this into a weighted graph. Once it is in graph form, we can add
    in information about resources available in each town and plot a picture with
    this information included, along with the distances between towns connected by
    a road. We can then calculate mutual resources between towns and minimum travel
    distances from a given town to another. This will help us assess resources available
    in a disaster and the best routes down which to send supplies.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 6-3](#listing6-3)’s first plot should output a diagram showing which
    towns are connected and the number of miles between towns, as shown in [Figure
    6-10](#figure6-10).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c06/f06010.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-10: A plot of town connectivity and miles of road between connected
    towns'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 6-10](#figure6-10) shows that towns 2, 3, and 4 are connected by multiple
    roads, such that if one road is blocked, the town can still be reached by looping
    around through another town. However, town 1 is relatively isolated despite being
    located only 4 miles from the nearest town. The road connecting towns 2 and 3
    is quite long (perhaps this is a back road that meanders through a densely wooded
    area or around several canals).'
  prefs: []
  type: TYPE_NORMAL
- en: The second `plot` in [Listing 6-3](#listing6-3) should output a graph that adds
    total resources for each town, as shown in [Figure 6-11](#figure6-11).
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c06/F06011r.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-11: A plot of town connectivity, miles of road between connected towns,
    and resources within each town'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 6-11](#figure6-11) gives us a richer understanding of connectivity
    and potentially shared resources among the four towns. Notice that town 1 has
    a relatively small water supply stocked for the disaster (10 liters per resident).
    However, if the road between towns 1 and 4 holds up during the disaster, it is
    easy to move some of the water from town 4 (with 200 liters per resident) to town
    1, such that town 1 has sufficient water. It’s also easy to move water between
    towns 4 and 2 (which has 500 liters per resident stocked up) provided the road
    directly connecting these towns holds up in the disaster.'
  prefs: []
  type: TYPE_NORMAL
- en: The maximal clique calculation yields mutually connected towns (towns with mutual
    *n*-way connections). This gives a connection between towns 1 and 4 (with the
    single road) and towns 2, 3, and 4, which are mutually connected. From this information,
    we can calculate resources at each town’s disposal should the roads hold between
    towns. Towns 1 and 4 mutually contain 210 resident-liters; the three-way clique
    (towns 2, 3, and 4) contains 780 resident-liters.
  prefs: []
  type: TYPE_NORMAL
- en: Using a shortest path algorithm, we can calculate shortest routes between any
    two towns to understand how quickly supplies might be routed between towns should
    supplies run low in a given town and roads are not damaged by the disaster. [Table
    6-2](#table6-2) gives the miles that would need to be driven between towns in
    this scenario.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6-2: Shortest Distances Between Pairs of Towns'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Town 1** | **Town 2** | **Town 3** | **Town 4** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Town 1** | 0 | 6 | 10 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| **Town 2** | 6 | 0 | 8 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| **Town 3** | 10 | 8 | 0 | 6 |'
  prefs: []
  type: TYPE_TB
- en: '| **Town 4** | 4 | 2 | 6 | 0 |'
  prefs: []
  type: TYPE_TB
- en: The longest route in this scenario (10 miles) is trying to get supplies from
    town 3 to town 1 when town 1 runs low on water. Notice that it’s shorter to bypass
    the longest road to route supplies between towns 2 and 3 (8 miles versus 12 miles).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can examine a scenario where one or more roads is damaged in the disaster
    by adding to [Listing 6-3](#listing6-3) to remove roads connecting towns and re-examine
    mutual supplies and shortest paths:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This script removes a link between towns 2 and 4, recalculating the metrics
    to help us assess how the situation has changed with the blockage of the road
    between towns 2 and 4\. These modifications should yield a plot of situation 2
    similar to [Figure 6-12](#figure6-12).
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c06/F06012r.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-12: A modified plot of situation 1 with one road destroyed in the
    disaster'
  prefs: []
  type: TYPE_NORMAL
- en: As [Figure 6-12](#figure6-12) shows, this scenario breaks the triangle present
    in situation 1, isolating town 2\. If town 1’s water supply runs low, the water
    must be shared between towns 1 and 2 with the expectation that it may be difficult
    to move water from town 2 to replenish the supplies.
  prefs: []
  type: TYPE_NORMAL
- en: If we look at the cliques, we can see two-way connections among towns, and mutual
    supplies between towns are more spread out than in the three-way connection present
    in situation 1\. Towns 2 and 3 have a decent mutual supply, but the other towns
    may have delays in routing needed water. In addition, the miles needed to travel
    increases, as we can see in the shortest path table ([Table 6-3](#table6-3)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6-3: Shortest Distances Among Pairs of Towns'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Town 1** | **Town 2** | **Town 3** | **Town 4** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Town 1** | 0 | 22 | 10 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| **Town 2** | 22 | 0 | 12 | 18 |'
  prefs: []
  type: TYPE_TB
- en: '| **Town 3** | 10 | 12 | 0 | 6 |'
  prefs: []
  type: TYPE_TB
- en: '| **Town 4** | 4 | 18 | 6 | 0 |'
  prefs: []
  type: TYPE_TB
- en: Routing water from the town with the largest supply (town 2) has become a lot
    more difficult, with greatly increased expected travel times. Should scenario
    2 appear likely, it’s probably best to redistribute the water prior to the disaster
    to avoid delays in routing.
  prefs: []
  type: TYPE_NORMAL
- en: Discrete exterior derivatives have other applications. Graphics rendering and
    engineering problems have been particular areas of interest within discrete exterior
    derivatives (and discrete exterior calculus in general). A few of the more common
    applications in engineering are flux and flow calculations on discrete objects
    or computer modeling of processes. Within graphics rendering, graphs are typically
    replaced with more general meshes.
  prefs: []
  type: TYPE_NORMAL
- en: In some instances, it is easier to use cohomology (and its tool, discrete exterior
    derivatives) to study an object or point cloud than to use persistent homology,
    as the end result will find the same boundaries and objects. However, little has
    been done to make explicit R packages for applying discrete exterior derivatives
    to data, and code must be parsed together as in [Listing 6-3](#listing6-3). Automation
    and object manipulation code would help facilitate the adoption of discrete exterior
    derivatives within data science and other fields.
  prefs: []
  type: TYPE_NORMAL
- en: Nonlinear Algebra in Machine Learning Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another intriguing and recent development with respect to geometry in machine
    learning is the notion of nonlinear algebra in machine learning algorithms. Many
    machine learning algorithms rely heavily on linear algebra to compute things such
    as gradients, least squares estimators, and so on. However, in relationships and
    spaces that aren’t flat or involving straight lines, the linear algebra provides
    only an approximation of quantities calculated. Take a look at [Figure 6-13](#figure6-13),
    which shows a straight line (assumed by linear algebra tools, such as those used
    in regression) and a curved line (which may cause an estimation problem for linear
    algebra tools).
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c06/f06013r.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-13: A straight line and curved line'
  prefs: []
  type: TYPE_NORMAL
- en: Nonlinearity introduces error into the calculations and final result of an algorithm
    when curves and nonlinear spaces are estimated using linear tools. Imagine using
    a ruler to measure the lower line in [Figure 6-13](#figure6-13). It would be hard
    to get an exact length of the line relative to the straight line above. If the
    length of the line were a quantity that needed to be minimized or maximized by
    an algorithm, such a measurement could potentially find a nonglobal solution or
    distort the quantity enough to cause problems in predictive accuracy or model
    fit statistics like sum of square error or Bayesian information criterion.
  prefs: []
  type: TYPE_NORMAL
- en: One proposed alternative to linear algebraic calculations within machine algorithms
    is to use *numerical algebraic geometry*, a branch of nonlinear algebra that deals
    with the intersections of curves. For instance, consider the two-dimensional ellipse
    intersected by a one-dimensional curve, shown in [Figure 6-14](#figure6-14).
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c06/f06014r.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-14: A plot of a curve intersecting an ellipse'
  prefs: []
  type: TYPE_NORMAL
- en: Different types of matrices and matrix operations can be used to solve nonlinear
    systems analogously to how linear algebra solves linear systems; numbers populating
    a matrix are simply replaced with polynomial equations. Some of these intersecting
    curve problems are nonconvex problems, which often pose issues to machine learning
    algorithms and the linear algebra powering them.
  prefs: []
  type: TYPE_NORMAL
- en: '*Convex optimization problems* are those in which the optimization function
    creates a region in which a line passing through the region is in the region continuously
    (rather than passing multiple regions and nonregions within the object), as shown
    in [Figure 6-15](#figure6-15).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c06/f06015r.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-15: A convex object'
  prefs: []
  type: TYPE_NORMAL
- en: If, however, the region contains holes or indents, this is no longer the case,
    and the region is designated as *nonconvex*, such as in [Figure 6-16](#figure6-16),
    where the hole inside the region splits the interior set into the set within the
    region and the set within the hole.
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c06/f06016r.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-16: A nonconvex object'
  prefs: []
  type: TYPE_NORMAL
- en: Optimization algorithms often struggle with nonconvex functions and regions
    within the optimization function, as the linear programming commonly used to solve
    these problems isn’t as amenable to nonconvex optimization problems and as stepwise
    methods can get stuck in the local optima around the hole or divots in the region.
    Unfortunately, many real-world datasets and the optimization functions used on
    them involve nonconvex regions. Numerical algebraic geometry offers an accurate
    solution for nonconvex optimization problems, which come up often in real-world
    data situations. Once the system of polynomials is set up in matrix form, many
    software platforms and programs exist to do the computations, including the Bertini
    and Macauley software, which can connect to both R and Python. This allows for
    solvers that work well in nonconvex optimization problems.
  prefs: []
  type: TYPE_NORMAL
- en: Several recent publications and workshops in numerical algebraic geometry suggest
    that nonlinear algebra is a viable alternative to the current machinery in algorithms
    for other types of problems that may be convex, loosening mathematical assumptions
    and providing improvements in accuracy. One recent paper (Evans, 2018) found that
    the local geometry of many possible solutions overlaps for many types of statistical
    models (including Lasso, ARIMA models for time-series data, and Bayesian networks).
    This means that algorithms can’t distinguish well between potential predictor
    sets or parameter values in the model space, particularly for the sample sizes
    commonly used and suggested for these problems. One can solve this problem by
    either fitting the model in tangent space, as we saw in the previous section,
    or using numerical algebraic geometry instead of linear algebra for optimization.
    This suggests some immediate applications of numerical algebraic geometry and
    other nonlinear approaches to machine learning algorithms for improved algorithm
    performance and model fits.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, packages to implement these algorithms do not exist in R at this
    time, so we won’t explore this further in an example.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing Choice Rankings with HodgeRank
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Discerning choice preference across a population of customers is a common machine
    learning task. For instance, a company might want customers to compare items on
    a list of potential new features in software to prioritize the engineering team’s
    time in the coming year. Comparing choice rankings also helps companies market
    new products and services to existing users and derive new campaigns for items
    that are likely to sell well with existing customers.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at a simple example of ranking activity preference for a day at the
    beach. Perhaps we’re looking at data that a hotel collected from recent guests
    on which activities they preferred during their stay; this would allow them to
    prioritize beach usage and staff hiring to meet future demand for the main activities
    their guests prefer. Here, we have three choices of activity (lying on the beach,
    swimming, or surfing), with one preference as a clear favorite (surfing). [Figure
    6-17](#figure6-17) summarizes this simple situation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c06/f06017.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-17: A diagram ranking three choices relative to the other choices'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can complicate this problem by adding a potentially new fourth activity
    that is preferred to the other three: kiteboarding. The ranking is still relatively
    easy to compute, as all activities are preferred to lying on the beach, one is
    preferable only to lying on the beach (swimming), one is preferred to every activity
    but kiteboarding (surfing), and one is preferred to the other three (kiteboarding),
    as shown in [Figure 6-18](#figure6-18).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c06/f06018.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-18: A diagram with another activity added to the preference data'
  prefs: []
  type: TYPE_NORMAL
- en: All of the information is given in the diagram, with this particular person
    filling out all choices relative to each other. This is rarely the case with real-world
    data, as shown in [Figure 6-19](#figure6-19).
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c06/f06019.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-19: An incomplete preference diagram of the four activities'
  prefs: []
  type: TYPE_NORMAL
- en: However, this information still shows a strong preference toward kiteboarding,
    with all pair-ranks existing for kiteboarding pairs and all pair-ranks pointing
    toward kiteboarding as the most preferred activity. In real-world data, it’s common
    not only to have missing information but to have preference loops in the data,
    such as the one in [Figure 6-20](#figure6-20), where surfing is preferred to swimming,
    and kiteboarding is preferred to surfing but not to swimming.
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c06/f06020.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-20: An example of incomplete and inconsistent ranking preference data'
  prefs: []
  type: TYPE_NORMAL
- en: This gives a local inconsistency of where kiteboarding and swimming might rank
    relative to each other when other options are available. However, surfing and
    kiteboarding are preferred to two other activities, suggesting they rank toward
    the top of possible options, and kiteboarding is preferred to surfing (giving
    a tiebreaker of sorts).
  prefs: []
  type: TYPE_NORMAL
- en: The situation becomes a bit more nebulous when no consistent preferences exist
    globally, with each activity preferred to another activity, as shown in [Figure
    6-21](#figure6-21).
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c06/f06021.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-21: A diagram of incomplete preference data with no consistent preferences'
  prefs: []
  type: TYPE_NORMAL
- en: In the case of [Figure 6-21](#figure6-21), nothing can be said about which activity
    would be preferred to other activities, and results of the analyses would be inconclusive.
    This is common when customers are asked to rank features in financial apps or
    guests are asked to rank preferred activities. Most customers will exist in subgroups
    with their own unique needs, which might be the opposite needs of another important
    customer subgroup. It’s a challenge to prioritize features for development or
    choices to give guests to please the largest number of customers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many algorithms exist to do pairwise-rank comparisons to get a ranked list
    of preferences relative to each other (SVMRank, PageRank, and more); however,
    in general, they do not provide information about local and global inconsistencies
    in rank that might influence where an item or choice is placed relative to others.
    Algebraic geometry recently added a tool to the collection of pairwise-rank algorithms
    that can decompose the ranking results to include information about local and
    global inconsistencies of items; that would be *HodgeRank*, which can derive this
    information by leveraging an algebraic-geometry-based theorem common in engineering
    problems: the Hodge–Helmholtz decomposition.'
  prefs: []
  type: TYPE_NORMAL
- en: '*The Hodge*–*Helmholtz decomposition* partitions a vector flow (or flow on
    a graph) into three distinct components, shown in [Figure 6-22](#figure6-22):
    the *gradient flow*, which is locally and globally consistent; the *curl flow*,
    which is locally consistent but globally inconsistent; and the *harmonic flow*,
    which is locally inconsistent but globally consistent.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c06/f06022.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-22: A diagram showing the flows broken down by the Hodge–Helmholtz
    decomposition'
  prefs: []
  type: TYPE_NORMAL
- en: In the beach examples, [Figure 6-20](#figure6-20) has a curl flow involving
    swimming, surfing, and kiteboarding (also harmonic if another activity is not
    ranked there). [Figure 6-21](#figure6-21) is an example of global harmonic flow
    (as well as local curl flow).
  prefs: []
  type: TYPE_NORMAL
- en: The HodgeRank algorithm essentially extends PageRank for pairwise-ranking problems;
    the math boils down to a least squares problem on the graph data, allowing for
    assessments of global ranking and local ranking consistency. Thus, suspicious
    rankings can be flagged for a human analyst’s review. The algorithm also allows
    for a lot of missing data in the original pairwise ranking sets, making it widely
    applicable to the often-incomplete data on ranking problems in the real world
    (where users won’t click through three million video options to rank each relative
    to all of the others!). While a package does not exist in R and thus we will not
    walk through an example, implementations do exist in Matlab, and readers familiar
    with Matlab who are interested in this algorithm are encouraged to use the resources
    listed for HodgeRank.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we learned about newer algorithms derived from differential
    and algebraic geometry and explored the use of both dgLARS and discrete exterior
    calculus on data analysis problems, including the Quora gifted sample, a credit-lending
    sample, and a disaster-planning scenario. Many more algorithms are being developed,
    and we’ve given an overview of how nonlinear algebra and Hodge theory have contributed
    to machine learning in recent years, impacting important types of industry problems
    (such as preference ranking and parametric model estimation).
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll return to persistent homology and examine another
    tool of algebraic topology called the Mapper algorithm. Both of these will be
    used on our student sample introduced in this chapter.
  prefs: []
  type: TYPE_NORMAL
