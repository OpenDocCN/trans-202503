- en: '**12'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MATHEMATICS**
  prefs: []
  type: TYPE_NORMAL
- en: '*The people of Ulm are mathematicians.*'
  prefs: []
  type: TYPE_NORMAL
- en: —Motto of Ulm, the birthplace of Albert Einstein
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this chapter, we’ll explore several Julia packages for symbolic and numerical
    mathematics. Symbolic mathematical software can replace tedious pencil-and-paper
    calculations or long evenings in the company of tables of integrals with automated
    manipulations of mathematical expressions. Numerical packages include modules
    for linear algebra, equation solving, and related fields. The two classes of packages
    have substantial overlap, and both are a boon to the applied mathematician or,
    potentially, to anyone who uses mathematics in research.
  prefs: []
  type: TYPE_NORMAL
- en: '**Symbolic Mathematics**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This category of software is sometimes called *computer algebra*, but it includes
    all types of automated symbol manipulation, such as algebraic and trigonometric
    simplification; generation of Taylor series; calculation of limits, derivatives,
    and integrals; and more specialized areas such as algebraic number theory.
  prefs: []
  type: TYPE_NORMAL
- en: Symbolic mathematical software is distinguished from the more familiar intersection
    of computers and math by its ability to handle mathematics as mathematics, rather
    than by simply performing arithmetic. We feed it expressions incorporating variables,
    and it returns rewritten expressions, or the solution to a problem, in terms of
    those variables, rather than numbers.
  prefs: []
  type: TYPE_NORMAL
- en: '***Numerical-Symbolic Modeling with Symbolics***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This section introduces `Symbolics`, which is described as a symbolic modeling
    language and as numerical-symbolic software. These descriptions are meant to suggest
    that `Symbolics` emphasizes the synergy between symbolic and numerical calculations,
    and is designed with efficiency in mind. `Symbolics` does not feature all the
    abilities of a full-blown computer algebra system—it can’t calculate indefinite
    integrals, for example. But it has other, unique abilities. For example, it can
    transform a normal Julia function into a symbolic function, and it can create
    a C program from a Julia `Symbolics` program. `Symbolics` is written entirely
    in Julia, which means that we can reach for any part of the language in working
    with its symbolic expressions. `Symbolics` is a key part of the `ModelingToolkit`
    package, a framework for automatically parallelized scientific machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: To establish names as symbolic variables, as shown in [Listing 12-1](ch12.xhtml#ch12lis1),
    it’s most convenient to use a macro supplied by the `Symbolics` package.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 12-1: Declaring* Symbolics *variables*'
  prefs: []
  type: TYPE_NORMAL
- en: After calling this macro, we can use the five mentioned variables similarly
    to how we would use variables in mathematical expressions. They have the type
    `Num` and share much of the behavior of the `Real` type, but they have extra powers,
    which we’ll explore next.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create a rotation matrix as we did in “Matrix Multiplication” on [page
    146](ch05.xhtml#ch05lev1sec18):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Since φ is a `Symbolics` variable, this matrix is a `Symbolics` expression.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see what happens if we try to rotate a vector with it using matrix multiplication,
    as we did with the “normal” rotation matrix in [Chapter 5](ch05.xhtml):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In each case, the matrix multiplication returns an exact result, correct for
    any value of φ. The `*` operator is able to operate on `Symbolics` expressions,
    performing matrix multiplication as it does with matrices of numbers. This is
    another example of the composability of Julia packages. Most array and numerical
    operators and functions will handle `Symbolics` expressions the way we would expect.
  prefs: []
  type: TYPE_NORMAL
- en: 'To compute a numerical result, we can use the `substitute()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The result is identical to the one in “Matrix Multiplication” on [page 146](ch05.xhtml#ch05lev1sec18).
  prefs: []
  type: TYPE_NORMAL
- en: 'The `substitute()` function takes a `Symbolics` expression in its first argument
    and a dictionary of substitutions to make in its second argument. The resulting
    expression is not always simplified as we might expect:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Here we have a polynomial that we attempt to write in a slightly simpler form
    by making a change of variable. Our first attempt is foiled because `Symbolics`
    seems not to know that, for example, `sqrt(b)^2 = b`. We had better luck on our
    second try.
  prefs: []
  type: TYPE_NORMAL
- en: '`Symbolics` is able to automatically simplify expressions involving multiplication
    or division of variables raised to integer powers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: It also comes with a `simplify()` function, but it’s not able to do much—not
    even the limited simplification that appears in the documentation. The emphasis
    of `Symbolics`, as mentioned previously, is on efficient numeric-symbolic modeling.
    We can always turn to `SymPy`, explored in the next section, to perform nontrivial
    simplifications of an expression, the results of which we can use in a `Symbolics`
    program.
  prefs: []
  type: TYPE_NORMAL
- en: '**An Example: Bessel Functions**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As an example of a practical use of `Symbolics`, let’s say we need to compute
    the Bessel function of the first kind, of various orders, and some of its derivatives.
    These functions appear throughout physics and engineering. We used a Bessel function
    in [Listing 7-5](ch07.xhtml#ch7lis5) on [page 206](ch07.xhtml#ch07lev4) to represent
    the shape of a vibrating drumhead, where we gained access to it through the `SpecialFunctions`
    package.
  prefs: []
  type: TYPE_NORMAL
- en: 'To roll our own Bessel function, which we’ll denote *J*[*m*](*x*), where *m*
    is the order, we can turn to its well-known series representation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/384math.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A Julia function implementing this representation, shown in [Listing 12-2](ch12.xhtml#ch12lis2),
    will accept `x`, `m`, and a number of terms (because we can’t compute an infinite
    number of terms) that we’ll call `N`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 12-2: Calculating a Bessel function using its series expansion*'
  prefs: []
  type: TYPE_NORMAL
- en: This function will return the value of *J*[*m*](*x*) computed using `N` terms
    in the series. Because it uses normal integers, rather than `big` integers, we
    can only use it with `N` < 19 (see “‘Big’ and Irrational Types” on [page 216](ch08.xhtml#ch08lev1sec1)).
    Keeping nine terms is more than sufficient for an extremely accurate approximation
    in the interval 0 ≤ *x* ≤ 6.
  prefs: []
  type: TYPE_NORMAL
- en: Our little function `Jm()` is useful if we need to know the numerical value
    of *J*[*m*](*x*) at various values of *x*, especially if we don’t know about the
    `Special` `Functions` package. If we happen to need the value of various derivatives
    of *J*[*m*](*x*), we could calculate them using some finite difference scheme,
    calling `Jm(x, m, N)` at two or more closely spaced values of `x` to compute the
    derivative at *x*. However, the numerical error intrinsic to these methods accumulates
    as the order of the derivative increases, and the repeated evaluations of `Jm(x,
    m, N)` are an additional computational cost. Let’s see how an approach using `Symbolics`
    neatly dispenses with both of those issues.
  prefs: []
  type: TYPE_NORMAL
- en: If we call `Jm(x, m, N)` with numerical values for `x`, `m`, and `N`, we get
    a number back, the approximation for the *m*th Bessel function at *x*. [Listing
    12-3](ch12.xhtml#ch12lis3) shows what we get if, instead of a number for `x`,
    we supply the name of a `Symbolics` variable.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 12-3: A* Symbolics *expression approximating* J*[1](z)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'In [Listing 12-1](ch12.xhtml#ch12lis1), we created the `Symbolics` variable
    `z`, among others. When we pass `z` to `Jm()`, it returns the nine terms of the
    series expansion generated with `m` = 1 and `N` = 9, in an unfortunate random
    order. We assigned this `Symbolics` expression to the variable `J19`. We can get
    the numerical value of this expression through substitution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The difference in the value in the last place is due to a difference in the
    order of operations. The strategy shown in [Listing 12-2](ch12.xhtml#ch12lis2)
    of adding up the small terms in a series before the larger ones should be somewhat
    more accurate.
  prefs: []
  type: TYPE_NORMAL
- en: 'As another example of the power of composing Julia packages, we can use `Latexify`
    to render a LaTeX version of a `Symbolics` expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Copying and pasting the contents (with some line breaks added) of the resulting
    LaTeX string into the source of this book, which is typeset using LaTeX, shows
    us the rendered expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/386math.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The process illustrated here, of taking a normal Julia function and repurposing
    it to generate a `Symbolics` expression, is sometimes called *tracing*. Only functions
    that are in a sense deterministic can be traced. What this means, in the case
    of our `Jm()` function, is that we can supply a `Symbolics` variable for `x`,
    but not for the number of terms, `N`. For that, we must supply an integer. If
    we try to sneak in a `Symbolics` variable for the third positional argument, we
    get a cryptic error message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The reason we didn’t enforce an integer `N` in the function signature, as we
    did for `m`, was to illustrate this behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'The problem with attempting to trace `Jm()` while using a `Symbolics` variable
    representing the number of terms is that the loop limits are unknown: what expression
    is to be returned? We can trace only functions that generate a completely determined
    expression based on their inputs. The particular error message appearing in this
    listing is a signal that we’ve run into this problem.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Differentiating the Bessel Function**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Since we’re in possession of an *analytic* expression, generated in [Listing
    12-2](ch12.xhtml#ch12lis2), for the approximation to *J*[1](*z*), we can derive
    its analytic derivative at *any order* to get d^(*p*)*J*[1]/d*z*^(*p*), the *p*th
    derivative. Since `J19` is only a polynomial, this is a simple, albeit tedious
    and error-prone, procedure.
  prefs: []
  type: TYPE_NORMAL
- en: '`Symbolics` can relieve us of the burden of differentiating by hand:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Here we use the `Differential()` function. `Differential(t)` returns another
    function that calculates the derivative with respect to `t` of the `Symbolics`
    expression that it receives. To actually see the result of this manipulation,
    we need to pass it to `expand_derivatives()`. The result is the correct differentiation
    of the polynomial `J19`, with its terms in yet another random order.
  prefs: []
  type: TYPE_NORMAL
- en: 'As suggested previously, we can repeatedly apply `Differential()` to generate
    derivatives at any order without worrying about the accumulation of finite differencing
    errors. Let’s take a look at the first 10 derivatives of the Bessel function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We intend to plot the derivatives, so first we import `Plots` and, to get typeset
    math in the axis labels, `LaTeXStrings`. We calculate the derivative of the Bessel
    function, as we did before, and place the result inside a vector. In a loop ➊
    we apply the derivative operator repeatedly to the previous result, generating
    the first 10 derivatives. We set up the plot by graphing *J*[1](*x*), using LaTeX
    strings for the labels, and then loop through ➋ the elements of the vector of
    derivatives, adding each one to the visualization. [Figure 12-1](ch12.xhtml#ch12fig1)
    shows the result.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch12fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-1: The first 10 derivatives of* J*[1](z)*'
  prefs: []
  type: TYPE_NORMAL
- en: The thick solid line shows *J*[1](*x*). The `linestyle=auto` keyword argument
    to `plot!()` creates a series of lines with different dash patterns, which are
    plotted using the default line thickness. These are the 10 derivatives.
  prefs: []
  type: TYPE_NORMAL
- en: That we’re able to plot these `Symbolics` expressions directly, without setting
    up vectors of numerical variables or having to make numerical substitutions by
    hand, is another example of composability. The `Plots` package was written without
    any knowledge of the (future) `Symbolics` package, yet it’s able to deal with
    `Symbolics` expressions in a natural way.
  prefs: []
  type: TYPE_NORMAL
- en: '***Math Manipulation with SymPy and Pluto***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For more general symbolic mathematics, `SymPy` is probably the best package
    available at the moment. This package is a Julia wrapper around the highly capable
    Python library of the same name, so it’s limited to Python performance; however,
    for the kind of work typically done with such packages, raw speed is not usually
    a crucial consideration.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*In order to use* SymPy *from Julia, with some systems and configurations it
    may be sufficient to merely execute* add SymPy *in Julia’s package mode, followed
    by* using SymPy*. On other systems, we need to install the Python* SymPy *library
    (and perhaps Python itself) outside of Julia. For example, on Linux (where Python
    is routinely available with most distributions), we can execute* pip3 install
    sympy *in the shell. However, as there is no official method of installing libraries
    or resolving dependencies in the Python world, it’s impossible to provide a command
    that will work for everyone. The remainder of this section assumes that you’ve
    successfully executed* add SymPy *and* using SymPy *in a Julia environment.*'
  prefs: []
  type: TYPE_NORMAL
- en: '`SymPy` works from any such environment, and does a nice job of rendering mathematical
    notation in the terminal REPL. Its use from Pluto, however, is more delightful,
    and we’ll use examples from that environment. In Pluto, math is automatically
    rendered in LaTeX, so the results are immediately in the form of beautifully typeset
    formulas, embedded within the notebook. Pluto uses MathJax for its math rendering.
    A right-click on any displayed expression brings up a contextual menu providing
    several options, the most important providing one to copy the LaTeX commands that
    create the expression to the clipboard.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another reason Pluto is a natural fit for `SymPy` is that, when using a computer
    algebra library, we’re usually in discovery or exploration mode, or using Julia
    with `SymPy` as a calculator, rather than developing a large program. The reactive
    nature of Pluto lends itself well to this mode of interaction (see “Pluto: A Better
    Notebook” on [page 17](ch01.xhtml#ch01lev1sec9)). Because of Pluto’s dependency
    graph, we can know that all the equations displayed in the notebook at any time
    are consistent with each other, something that is decidedly not true with Jupyter.'
  prefs: []
  type: TYPE_NORMAL
- en: The ability to use Pluto is one reason we might prefer to use `SymPy` from within
    Julia rather than with Python directly. Another is that the wrapping of functions
    and data structures provided by `SymPy` presents a more familiar interface for
    the Julia programmer and eases interoperation with other Julia programs and libraries.
    This wrapping is not complete in a sense, however. The user of `SymPy` will encounter
    remnants of Python’s class-method syntax, as we’ll see in such calls as `sol.rhs()`,
    for the right-hand side of a solution `sol`.
  prefs: []
  type: TYPE_NORMAL
- en: Since Pluto is such a powerful (and fun) environment for using `SymPy`, the
    examples in this section will take the form of screenshots from a Pluto session
    (see [Chapter 1](ch01.xhtml) for a reminder of how to start up a Pluto notebook
    session).
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 12-2](ch12.xhtml#ch12fig2) shows the start of the session.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch12fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-2: Starting a* SymPy *session within Pluto*'
  prefs: []
  type: TYPE_NORMAL
- en: After importing the package, we establish some variables as `SymPy` symbolic
    names using the `@syms` macro. This serves the same purpose as the `@variables`
    macro used with the `Symbolics` package. Entering one of the names as `f()` establishes
    `f` as the symbolic name of a function that we can use as an unknown in, for instance,
    the definition of a differential equation (we’ll look at this shortly).
  prefs: []
  type: TYPE_NORMAL
- en: '**Algebra with SymPy**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '`SymPy` can perform algebraic simplification, expansion, and its inverse, factoring,
    as shown in [Figure 12-3](ch12.xhtml#ch12fig3).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch12fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-3: Simplification, expansion, and factoring*'
  prefs: []
  type: TYPE_NORMAL
- en: The subtle underlines adorning some characters in the input cells in [Figure
    12-3](ch12.xhtml#ch12fig3) indicate which are `SymPy` symbols—a nice refinement
    to the interface.
  prefs: []
  type: TYPE_NORMAL
- en: In order to solve systems of algebraic equations, we can place the equations
    into a vector and call `solve()` with the vector as an argument, as shown in [Figure
    12-4](ch12.xhtml#ch12fig4).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch12fig04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-4: Solving a system of equations*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The vector `p` contains two equations, entered so their right-hand sides equal
    0; therefore, `p` represents the following system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/391math.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The result of the call to `solve()` is the solution *a* = *–*1/7, *b* = 3/7.
  prefs: []
  type: TYPE_NORMAL
- en: '**Numerical Solutions with SymPy**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Our example happens to involve linear equations, but `SymPy` can handle higher-order
    polynomials, rational equations, and more, and it can find complex and multiple
    solutions. We can also turn to its built-in numerical solver, useful in cases
    where no symbolic solution exists.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, let’s say we were interested in values of `a` for which
  prefs: []
  type: TYPE_NORMAL
- en: sin(*a*) + log(*a*) = 1
  prefs: []
  type: TYPE_NORMAL
- en: An attempt to throw this at the symbolic solver only gets us an error message
    lamenting that `SymPy` knows no algorithms for its analytic solution. This is
    a job for an approximate, numerical solver.
  prefs: []
  type: TYPE_NORMAL
- en: Intelligent numerical solution behooves us to understand something about the
    behavior of the equation of interest, at least within and near the neighborhood
    where we seek solutions. A good first step is to look at a graph of the equation,
    as shown in [Figure 12-5](ch12.xhtml#ch12fig5).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch12fig05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-5: The first step in finding a numerical solution*'
  prefs: []
  type: TYPE_NORMAL
- en: Here we’ve plotted the left-hand side of the equation; the curve’s intersections
    with the horizontal line at 1 show us where we can expect the solutions. Inspection
    of the graph shows three solutions near `a =` 1, 3, and 5.
  prefs: []
  type: TYPE_NORMAL
- en: '`SymPy`’s numerical solver is the `nsolve()` function. It expects a symbolic
    expression in its first argument and a guess for a root for the expression in
    its second argument. By calling the function three times with three approximate
    roots, we can get three precise answers, as shown in [Figure 12-6](ch12.xhtml#ch12fig6).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch12fig06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-6: Numerical root finding*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Integration with SymPy**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '`SymPy` knows calculus, and it can largely replace weighty tables of integrals.
    We’ll use the package to evaluate the indefinite and a definite integral of the
    Gaussian distribution (see “The Normal Distribution” on [page 323](ch10.xhtml#ch10lev1sec3)).
    We can evaluate these integrals in one step by using the `integrate()` function,
    but we can also divide the problem into two stages. The first stage will be to
    define expressions for the *unevaluated* integrals, shown in [Figure 12-7](ch12.xhtml#ch12fig7).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch12fig07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-7: Unevaluated integrals*'
  prefs: []
  type: TYPE_NORMAL
- en: 'We create an unevaluated integral using the `sympy.Integral()` function, which
    requires the namespace prefix because it’s not exported by the package. In this
    case, the expression under the integral has only one independent variable, but
    if it had more than one, we would supply the variable of integration as a second
    argument (which we can in any case, with the same result). The second argument
    appears in the definite integral version, where the tuple contains the variable
    of integration and the lower and upper limits. Here *e* is Euler’s number, which
    we can enter by typing \euler followed by TAB or by directly entering the Unicode
    character. We enter symbolic infinity using a double `o`, and symbolic π using
    `PI`—which is not to be confused with the irrational Julia π. The two are not
    interchangeable: if we use π instead of `PI`, the former will be converted into
    an approximation to π, and factors of π will fail to cancel in subsequent manipulations.'
  prefs: []
  type: TYPE_NORMAL
- en: There can be several reasons for creating such intermediate expressions, rather
    than integrating in one step. We may want to use these unevaluated integrals in
    other calculations, or we may simply want to examine their typeset form to ensure
    that we’ve entered them correctly—something that’s easier to accomplish with conventional
    mathematical notation than even the exceptionally legible computerese that Julia
    makes available to us.
  prefs: []
  type: TYPE_NORMAL
- en: To evaluate the integrals, we pass them to the `doit()` function, as shown in
    [Figure 12-8](ch12.xhtml#ch12fig8).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch12fig08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-8: Evaluating the integrals*'
  prefs: []
  type: TYPE_NORMAL
- en: The indefinite integral (antiderivative) of the Gaussian is not expressible
    in closed form in terms of elementary functions. It’s defined as the *error function*,
    abbreviated erf(*z*). This is the type of mathematical knowledge built into most
    capable computer algebra systems, and `SymPy` is no exception. The ![Image](../images/393math.jpg)
    factor in the integral normalizes the result so that the definite integral over
    the whole line yields 1\. With this normalization, the integrand is a probability
    density function, and the definite integral from *a* to *b* is the probability
    of an observation falling within that interval.
  prefs: []
  type: TYPE_NORMAL
- en: '**Differential Equations with SymPy**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '`SymPy` can also solve differential equations. In keeping with our minor theme
    of the Bessel functions, let’s recall that these mainstays of applied mathematics
    arise as the solutions of differential equations. [Figure 12-9](ch12.xhtml#ch12fig9)
    shows a particular example that demonstrates how to define a differential equation
    in `SymPy`.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch12fig09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-9: Bessell’s equation*'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 12-9](ch12.xhtml#ch12fig9) shows the construction of the differential
    equation for the Bessel function of the first kind of order 1\. We define the
    equation using the `Eq()` function, which takes the left-hand and right-hand sides
    as its two arguments. In the definition, we’ve used the symbolic differential
    operator: `diff(f(z), z, n)` is the *n*th derivative of *f*(*z*) with respect
    to *z*. It was with this in mind that we established `f()` as a symbolic function
    in [Figure 12-2](ch12.xhtml#ch12fig2).'
  prefs: []
  type: TYPE_NORMAL
- en: To find the solution to a differential equation, we use `SymPy`’s `dsolve()`
    function, which takes the equation to solve and the function to solve it for in
    its first two arguments. But since boundary conditions are essential for nailing
    down which solutions we’re interested in, `dsolve()` also takes a dictionary of
    boundary conditions as the value of the keyword argument `ics`. We can specify
    values or derivatives at specific points in this dictionary; here we only need
    a simple condition to exclude another Bessel function that’s singular at the origin.
    [Figure 12-10](ch12.xhtml#ch12fig10) shows the call that generates the solution
    of interest.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch12fig10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-10: Solving a differential equation*'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 12-10](ch12.xhtml#ch12fig10) shows that `SymPy` uses the conventional
    notation for the Bessel function (in Pluto; in the REPL it spells out the name).
    The solution with the supplied boundary condition is undetermined up to a multiplicative
    constant, which `SymPy` names *C*[1]. The second cell in [Figure 12-10](ch12.xhtml#ch12fig10)
    shows how to extract the `rhs` (right-hand side) of the solution while specifying
    a value for the constant, in this case 1\. We can use the `rhs` to plot the solution,
    as shown in [Figure 12-11](ch12.xhtml#ch12fig11).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch12fig11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-11: Plotting the solution to Bessel’s equation*'
  prefs: []
  type: TYPE_NORMAL
- en: The curve shown in [Figure 12-11](ch12.xhtml#ch12fig11) agrees with the Bessel
    function calculated by other means in [Figure 12-1](ch12.xhtml#ch12fig1).
  prefs: []
  type: TYPE_NORMAL
- en: '**Linear Algebra**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As Professor L. Fox says in his 1965 textbook *An Introduction to Numerical
    Linear Algebra*, about 75 percent of scientific computing involves, wholly or
    in part, numerical linear algebra. Whatever the current proportion happens to
    be, linear algebra is, and likely always will be, a central part of any enterprise
    where we turn to computers to help us solve problems in science, mathematics,
    or engineering. The fundamental reason for this is because the central problem
    of numerical linear algebra, the solution of simultaneous systems of linear equations,
    arises repeatedly in the modeling of an enormous variety of systems—not only those
    whose behavior is truly linear, but those whose behavior can be linearly modeled
    within some range of parameters. For example, a system of partial differential
    equations can often be approximated by a linear algebraic system close to some
    initial condition or for a small range of a controlling parameter.
  prefs: []
  type: TYPE_NORMAL
- en: '***Views***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In performing calculations using matrices (or arrays of other shapes), we often
    employ views. A *view* in Julia is a reference to a part of an array that we can
    create and manipulate without copying any data; modifications to the view modify
    the original array.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create views using the `@view` or `@views` macros. The first version
    immediately precedes the array expression that we want to turn into a view, while
    the second transforms all the slicing operations within an entire expression or
    code block into views:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: After creating a view of the first row of the random matrix `R`, we set all
    of its elements to 17 ➊. Since modifying a view modifies the original, the first
    row of `R` is transformed ➋. We create the same view using the `@views` macro,
    and verify that the views are indeed the same with the last expression.
  prefs: []
  type: TYPE_NORMAL
- en: The slice syntax used earlier, without the `@view` or `@views` macros, would
    create a new array with a *copy* of the data from the first row of `R`. Modifying
    the copy would do nothing to the original array.
  prefs: []
  type: TYPE_NORMAL
- en: When should we use copies and when should we use views? The answer depends on
    the pattern of computation to which we intend to subject the data structures.
    In this example, since arrays are stored in column-major order, manipulating a
    row uses noncontiguous memory accesses. If, after extracting the row, we use it
    repeatedly, then the time consumed in creating the copy may be a good investment.
    However, if the array is large, the copy will consume significant memory that
    the use of a view would avoid. Copies use more memory, but can lead to faster
    code. There is no universal answer to the question beginning this paragraph. Whether
    it’s better to use views or copies depends on the size of the arrays involved
    and how we use the data.
  prefs: []
  type: TYPE_NORMAL
- en: '***Linear Algebra Examples***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s look at a simple example problem. Consider the 2×2 system shown in [Equation
    12.1](#ch12equ1).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/397math.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this system of equations, *x*[1] and *x*[2] are the unknowns for which we
    ultimately seek a solution; the *a*[*xx*]s are numerical coefficients, whose indices
    indicate their positions in the system. The right-hand side of the system consists
    of the two numbers *b*[1] and *b*[2].
  prefs: []
  type: TYPE_NORMAL
- en: In order to apply the machinery of numerical linear algebra, we’ll follow the
    universal convention and write the system more compactly as
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/397math1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where A is the matrix
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/397math2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*x* is the vector [*x*[1], *x*[2]], and *b* is the vector [*b*[1], *b*[2]].
    The juxtaposition of A and *x* indicates the usual matrix multiplication.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The form of [Equation 12.2](#ch12equ2) suggests that we can somehow divide
    by A to solve for *x*, and that is indeed true. As this is a section on *numerical*
    linear algebra, in [Equation 12.3](#ch12equ3), let’s try some actual numbers in
    place of the symbols in [Equation 12.1](#ch12equ1):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/397math3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This equation may, or may not, have a solution for *x*[1] and *x*[2]. In order
    to try to solve it numerically, we’ll define a Julia matrix and a vector for the
    right-hand side, corresponding to A and *b* in [Equation 12.2](#ch12equ2), as
    shown in [Listing 12-4](ch12.xhtml#ch12lis4).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 12-4: A small linear system*'
  prefs: []
  type: TYPE_NORMAL
- en: At this point, if we could make sense of the idea of dividing by a matrix, then
    we would expect that the solution could be calculated by dividing `b` by `A`.
    This, in fact, will be our first approach to solving the equation system in [Listing
    12-4](ch12.xhtml#ch12lis4).
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course we’re familiar with the `/` operator for division. Julia comes with
    a “reverse” version, called the *left division operator*, that we haven’t had
    occasion to use until now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Julia’s `Base` extends the left division operator to operate on matrices, calculating
    the inverse of a matrix and then performing a matrix multiplication. The result
    should be a column array containing the solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This is indeed the solution, as we can immediately verify:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The result is `b`, as defined in [Listing 12-4](ch12.xhtml#ch12lis4).
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned, the meaning of `A \ b` is the matrix multiplication of the *inverse*
    of `A` with `b`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The second input expression shows another way to spell the inverse of a matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Although this is the formal meaning of the `\` operator, we should never solve
    equation systems using `inv()`, but instead with an expression such as `A \ b`.
    This is because the left division operator solves the system using the most efficient
    algorithm available, which may not involve the calculation of the inverse matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'The inverse of a matrix is defined such that A^(−1) A and AA^(−1) are both
    equal to the *identity matrix*, which has the same shape as A and has 1.0 on the
    diagonal and 0.0 elsewhere:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The identity matrix is conventionally represented as I, and is called thus
    because it is the identity element under matrix multiplication:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: In general, matrix multiplication is not commutative, but multiplication by
    the identity matrix, and multiplication of a matrix by its inverse, are.
  prefs: []
  type: TYPE_NORMAL
- en: '***The LinearAlgebra Package***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The examples in this section so far require no package imports, as `inv()` and
    the extension of `\` to matrices are part of `Base`. To go further, we need to
    import the `LinearAlgebra` package, which is part of the standard library, so
    it imports quickly and nothing needs to be downloaded. The rest of the code examples
    in this section assume that you’ve executed `using LinearAlgebra`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `LinearAlgebra` package can perform all of the standard operations on matrices.
    We’ll demonstrate using our little matrix `A`. First, the trace and the determinant:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, the calculations of eigenvalues and eigenvectors (A*x* = *λx* if *x*
    is an eigenvector of A and *λ* is its eigenvalue):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The *n*th eigenvector/eigenvalue pair is the *n*th column of the matrix returned
    by `eigvecs()` along with the *n*th element of the vector returned by `eigvals()`.
    We can check to see if the `LinearAlgebra` functions return the correct values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Here we’ve assigned names to the first eigenvector and its eigenvalue; we should
    see that `A * evec1` is equal to `eval1 * evec1`. Comparing the two values in
    the final expression, we see that they are the same within floating-point accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '***Specialized Matrix Types***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Linear algebra routines, such as `eigvals()` and others, are written to dispatch
    an algorithm designed to take advantage of the symmetries or other properties
    of the matrices involved. The routines check for relevant properties of the matrix
    arguments passed to them in order to choose the most efficient method of solution.
    For example, the `eigvals()` function checks for symmetry of real matrices using
    the `issymmetric()` function, and hermiticity of complex matrices using `ishermitian()`.
  prefs: []
  type: TYPE_NORMAL
- en: The matrix properties that are important in choosing an efficient routine include,
    among others, whether a matrix is symmetric, banded, triangular, hermitian, sparse
    (see “The Adjacency Matrix” on [page 196](ch07.xhtml#ch07lev1sec1)), or diagonal.
    Each of these matrix classes has an associated Julia type. We can convert a general
    matrix to one of these more specific types by creating a view using the appropriate
    function. For example, `Symmetric(M)` creates a view of the matrix `M` that is
    symmetric. We might want to do this in order to pass the result to a linear algebra
    function ensuring that it selects the optimal algorithm, in case it doesn’t detect
    the character of the matrix.
  prefs: []
  type: TYPE_NORMAL
- en: To get an idea of how all this works, let’s look at the behavior of the `eigvals()`
    function. First, we create a moderately large matrix for our timing study, as
    shown in [Listing 12-5](ch12.xhtml#ch12lis5).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 12-5: Creating a random, symmetric matrix*'
  prefs: []
  type: TYPE_NORMAL
- en: The final assignment creates a symmetric matrix by adding `G`, elementwise,
    to its transpose. Let’s compute the eigenvalues of `G` in several ways, as shown
    in [Listing 12-6](ch12.xhtml#ch12lis6). We don’t care about the results, but we’re
    interested in the timings.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 12-6: Timing the calculation of eigenvalues*'
  prefs: []
  type: TYPE_NORMAL
- en: The first two timings demonstrate that the `eigvals()` function can exploit
    the symmetry of the matrix to drastically reduce the calculation time. We also
    create a `Symmetric` view of `sG` ➊, which contains the same values as the original
    matrix, but is of a different type. In this case, the use of `SsG` doesn’t affect
    the calculation time ➋, as `eigvals()` has already detected that `sG` is symmetric.
    We could also ask `eigvals()` to compute `eigvals(Symmetric(G))`, and it would
    do so as quickly as it computed the eigenvalues of the actually symmetric matrix
    just shown. But in this case, the computed eigenvalues would not be the eigenvalues
    of `G`, as `G` is not symmetric.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `eigvals()` and `eigvecs()` functions check for symmetric or hermitian
    arguments, but not for other properties. We can demonstrate this by calculating
    the eigenvalues of an upper triangular matrix: a matrix with zero elements below
    the diagonal. First we need to construct the matrices for use in the test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'After making, again, a random matrix `G`, we create ➊ an `UpperTriangular`
    view of this matrix and assign it to `UTt`. Then we assign it to `UT` after converting
    it to a basic `Matrix` type. This is a convenient way to make a full matrix that
    happens to be upper triangular. The two objects contain the same elements ➋ but
    are of different types. The type of `UTt` tells `LinearAlgebra` functions that
    it’s upper triangular, so they can take advantage of that in case a specialized
    algorithm is available. `eigvals()` is one of these functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The time to compute the 3,000 eigenvalues is much shorter than for a matrix
    with no structure ([Listing 12-6](ch12.xhtml#ch12lis6)) due to all the zeros in
    `UT`. The time that `eigvals()` needs to work on the `UpperTriangular` view of
    the matrix is drastically reduced (note the units in the timings returned by `@btime`),
    as are the memory requirements. The matrices have identical elements, and the
    computed eigenvalues are the same (but are returned in a different order). However,
    the information carried by the `UpperTriangular` type informs `eigvals()` about
    the matrix’s structure, which is information it can use in dispatching to an algorithm
    more efficient than the general-purpose one.
  prefs: []
  type: TYPE_NORMAL
- en: The moral of this story is that we should pass the most informative view possible
    to any `LinearAlgebra` function.
  prefs: []
  type: TYPE_NORMAL
- en: '***Equation Solving and factorize()***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A *factorization* of a matrix, analogous to the factorization of a number,
    is a series of matrices that, when (matrix) multiplied together, yield the original
    matrix. Matrix factoring is often an early step in the solution of a matrix equation
    (a system of linear equations), and is attempted by the left division operator,
    the standard function for solving such systems. The factorization can be the most
    time-consuming part of the calculation of the solution, which often proceeds rapidly
    after the factorization is complete. As many problems involve the repeated solution
    of equations in the form of [Equation 12.2](#ch12equ2) using different *b* vectors,
    it would save significant time if we could perform the factorization once, separating
    out that part of the calculation. This is what the `LinearAlgebra` function `factorize()`
    enables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Here we see that solving the equation system using the pre-factored matrix is
    about 200 times faster, and uses a small fraction of the memory required, than
    when we use the unfactored matrix. However, the call to `factorize()` itself takes
    about as much time as the calculation `G \ g`. The advantage is that we can use
    `fG` in subsequent problems that vary only in their right-hand sides to get solutions
    cheaply.
  prefs: []
  type: TYPE_NORMAL
- en: 'Telling `\` about the properties of the matrix using views doesn’t help, as
    it did with `eigvals()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Here, also, although the `Symmetric` view doesn’t help, we observe a large speedup
    and decrease in memory consumed when using the factorized matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '**Conclusion**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This chapter covers two large topics that, I believe, are generally useful to
    scientists, engineers, and other technical users of Julia.
  prefs: []
  type: TYPE_NORMAL
- en: The use of symbolic mathematics packages is potentially valuable for everyone,
    and my discussions with various students and researchers convinces me that many
    are unaware that computers can calculate integrals and derivatives, solve equations
    symbolically, and perform other feats of real mathematical manipulation—not merely
    arithmetic. Opening this door leads to many possibilities, especially when symbolic
    and numerical methods are combined, as encouraged by the `Symbolics` package.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, linear algebra is a vast traditional area for computer application,
    and we only scratched the surface here. Julia is particularly convenient for calculations
    in this arena. BLAS (Basic Linear Algebra Subprograms) and LAPACK are the Fortran
    libraries at the heart of numerical linear algebra, and most languages’ linear
    algebra abilities amount to interfaces to these venerable collections of optimized
    routines. Julia is unusual in several regards: BLAS and LAPACK are being rewritten
    in pure Julia, an ongoing project, and, through the `libblastrampoline` package,
    Julia offers the unique ability to switch between BLAS implementations on the
    fly.'
  prefs: []
  type: TYPE_NORMAL
- en: '**FURTHER READING**'
  prefs: []
  type: TYPE_NORMAL
- en: 'See “Symbolic Mathematics on Linux” for more details on symbolic math: [*https://lwn.net/Articles/710537/*](https://lwn.net/Articles/710537/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for `Symbolics.jl` is available at [*https://symbolics.juliasymbolics.org/stable/*](https://symbolics.juliasymbolics.org/stable/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OSCAR is a computer algebra package that covers algebra, geometry, and number
    theory: [*https://oscar.computeralgebra.de*](https://oscar.computeralgebra.de).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a list of matrices with special symmetries and structures, visit [*https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#Special-matrices*](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#Special-matrices).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`libblastrampoline` is available at [*https://github.com/JuliaLinearAlgebra/libblastrampoline*](https://github.com/JuliaLinearAlgebra/libblastrampoline).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The recently developed `LinearSolve` package provides a unified interface for
    a selection of linear equation solvers: [*https://github.com/SciML/LinearSolve.jl*](https://github.com/SciML/LinearSolve.jl).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
