- en: '**10'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: EXPERIMENTAL DESIGN**
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The *scientific method* is the backbone of science. It involves the creation
    of theories from hypotheses that are tested through experiments and supported
    by evidence gleaned from those experiments. In this chapter, we’ll explore the
    design of experiments, or *experimental design*, a foundational part of the scientific
    method. Randomness is critical to successful experimental design for two main
    reasons. First, many measurements, regardless of the field, involve uncertainty
    or other factors outside of the researcher’s control, called *random noise*. We
    use randomness in experimental design to combat noise, like fighting fire with
    fire. Second, randomness makes the results conform to the expectations of statistics.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter uses simulation ([Chapter 3](ch03.xhtml)) to explore three common
    approaches to randomization in experimental design. Our running example mimics
    medical research, but the concepts involved apply everywhere.
  prefs: []
  type: TYPE_NORMAL
- en: '**Randomization in Experiments**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Say a researcher wants to know the general public’s opinion regarding a ballot
    proposal to enact a noise ordinance restricting loud parties after 8 PM. He decides
    to use a phone survey, picking 100 names at random from the phone book and calling
    each on a Wednesday afternoon. He gets 64 answering machines, 36 pickups, and
    17 willing to talk. Of those 17, 15 support the ordinance and 2 oppose it. Are
    these results a fair representation of the general public’s stance on this issue?
  prefs: []
  type: TYPE_NORMAL
- en: I suspect your answer is no because there are many possible sources of bias
    in the results. The researcher used the phone book, which generally lists only
    landlines; relied on those called to be willing to offer their opinion; and called
    on a weekday in the afternoon.
  prefs: []
  type: TYPE_NORMAL
- en: His sample is strongly biased toward retired people who tend to be older and
    (stereotypically) less inclined to have parties, so his results do not necessarily
    reflect the population as a whole. His sample excludes the group most likely to
    be affected by the ordinance—young people who use cell phones and were probably
    at school or work during the time he collected his data.
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding such *sample bias* is an excellent reason to use randomness when conducting
    experiments. However, bias in surveys and polling is tough to correct and often
    leads to contrary results. More subtle, and perhaps more dangerous, is sample
    bias in selecting cohorts for medical studies.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code in *bad_sample.py* generates a population where individuals are a
    combination of four randomly selected characteristics: age, income, smoker or
    not, and the average number of alcoholic drinks per week. The characteristics
    are linked to the person’s age, so an older person is likelier to have a higher
    income, not smoke, and drink less alcohol.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Population` function generates a population of individuals as a NumPy
    array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The code loops append four-element lists of characteristics, each derived from
    the selected `age`. Notice, `age` is in years, `income` is in thousands, `smoker`
    is binary, and `drink` is average number of alcoholic drinks per week.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code selects a random subset of the population to simulate a random sample
    as a cohort for a medical study. For example, here’s one run of *bad_sample.py*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The population size is 1,000, from which we randomly select 10\. The `1` argument
    samples once. As usual, the randomness source and seed follow.
  prefs: []
  type: TYPE_NORMAL
- en: We sort the output by age, income, smoker, and drink. The first column shows
    the mean of these values for the entire population. The second is the mean for
    the 10-person random sample. The values in parentheses are t-test results where
    `t` is the *t* statistic and `p` the p-value. A significant difference between
    the population and sample produces a low p-value. The sign of the *t* statistic
    is such that a positive *t* signifies that the population mean exceeds the sample
    mean.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the 10-person random sample was similar to the population regarding
    drinking and overall age. However, income was different, which might impact the
    results of a study that claims this sample is a suitable stand-in for the general
    population.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s run the same command again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This random sample is significantly older than the population and is consequently
    much less likely to smoke. Try a few runs of *bad_sample.py*. While some samples
    are similar to the population, others deviate significantly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run *bad_sample.py*, but change the number of samples from 1 to 40:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Each person in the population is a 4-tuple (age, income, smoker, drink). As
    such, they become a point in a four-dimensional space. The entire population becomes
    a single point in space if we consider the means of the individual age, income,
    smoker, and drink characters. The *bad_sample.py* file reports the mean (± standard
    error) Euclidean distance between the mean point of the population and the mean
    point of the sample, for each of the 40 samples taken. In this case, with a sample
    size of 10, the mean distance was 6.37 ± 0.50.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s increase the sample size to 100:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The mean distance between the population and the samples decreases. If we have
    a larger sample, it should be a better representation of the population as a whole.
    This effect is the motivation behind working with as much data as possible, both
    for scientific research and machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: The file *bad_sample_test.py* repeats the process for a population of 10,000
    and sample sizes from 10 to 5,000\. For each sample size, we collect 40 samples.
    The results are amenable to plotting, as [Figure 10-1](ch010.xhtml#ch010fig01)
    illustrates.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/10fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10-1: The Euclidean distance from the population mean and sample mean
    as a function of the sample size*'
  prefs: []
  type: TYPE_NORMAL
- en: The *x*-axis in [Figure 10-1](ch010.xhtml#ch010fig01) is the number of samples
    in each set. We plot the average deviation of the mean of those samples from the
    population mean with error bars representing the standard error. I included end
    caps on the error bars to make the range easier to spot. Each point is the mean
    over 40 population-sample pairs.
  prefs: []
  type: TYPE_NORMAL
- en: Larger sample sizes lead to a smaller distance between the population and sample
    means, indicating that larger samples are more likely to be better representations
    of the population than smaller samples. Note that the error bars shrink as the
    sample size increases. The noise in the samples, or the deviation from the population
    mean, drops to nearly zero.
  prefs: []
  type: TYPE_NORMAL
- en: The error bars for a sample size of 10 are large, and the mean deviation is
    about 7, roughly seven times the mean deviation for samples of size 500\. Notice
    also how quickly the curve decreases as the sample size increases. A large sample
    size might not be necessary for many experiments, but the strength of the effect
    being investigated influences sample size choice. More is always better.
  prefs: []
  type: TYPE_NORMAL
- en: Experiments often report mean results along with the standard deviation or standard
    error of the mean. Then they perform a significance test, like a t-test; if the
    p-value is below the (arbitrary) threshold of 0.05, they declare victory and label
    the result as statistically significant. However, this is only part of the story.
    If the sample sizes are substantial, as might be the case in some cohort studies
    based on widely collected health data, then it’s entirely possible that the result
    is *p* < 0.05, but, because of the large cohort, the *effect size* as measured
    by Cohen’s *d* is small. In other words, the effect is real but perhaps relatively
    meaningless in practice because the effect size is small. The moral of the story
    is to report effect sizes whenever possible.
  prefs: []
  type: TYPE_NORMAL
- en: The noise in [Figure 10-1](ch010.xhtml#ch010fig01) for small sample sizes justifies
    my rant in the previous paragraph. If the effect is small, and we expect a slight
    deviation from a population mean, then a small sample size is unlikely to capture
    the effect because it’s swamped by the inherent noise in the sample itself. We’ll
    return to this notion later in the chapter when using experimental design techniques
    to compensate for selection bias.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the very thing that makes small sample sizes in [Figure 10-1](ch010.xhtml#ch010fig01)
    a poor choice for research purposes is what drives evolution via genetic drift.
    In [Chapter 3](ch03.xhtml), we saw that the random mix of characters in a subpopulation
    separated by chance from a larger population led to a drift that ultimately produced
    a new species. In that sense, evolution is a poor researcher but a clever tinkerer
    able to take sample bias and turn it into something useful in the long run.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know sample bias can lead to poor research results, let’s try to
    find ways to compensate for it.
  prefs: []
  type: TYPE_NORMAL
- en: Randomization in experimental design comes in three broad categories. The code
    we’ll use throughout the remainder of the chapter supports all three approaches.
  prefs: []
  type: TYPE_NORMAL
- en: '***Simple***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If the experiment is testing one outcome—such as the effectiveness of a particular
    treatment—we might build the treatment and control groups by recruiting members
    one at a time, flipping a coin to decide group assignment. We can simulate such
    *simple randomization* with a snippet of Python code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We configure `RE` to return coin flips, 0 or 1\. Both calls return 10 flips
    each. Therefore, we have the assignments for 20 participants: 11 for the treatment
    group and 9 for the control group.'
  prefs: []
  type: TYPE_NORMAL
- en: Simple randomization seems like a good approach, but it has an obvious drawback,
    similar to what we observed when working with *bad_sample.py*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try one more assignment for an experiment with 10 participants:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: If we follow simple randomization, we’ll have seven in the treatment group and
    only three in the control group—this seems unwise. Note that this isn’t a contrived
    example; the output is from a single run of that line of code.
  prefs: []
  type: TYPE_NORMAL
- en: 'While simple randomization isn’t the best approach when the study size is small,
    it often works well for larger study sizes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the difference between treatment and control group size is a smaller
    fraction of the total study size, so we might expect each group to be similar.
  prefs: []
  type: TYPE_NORMAL
- en: Simple randomization’s weakness, especially for small study sizes, is that the
    number of subjects in the treatment and control groups might be highly imbalanced.
    We can remedy this with block randomization.
  prefs: []
  type: TYPE_NORMAL
- en: '***Block***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Block randomization* ensures that the number of subjects in each group is
    the same. For a binary experiment with only two groups—treatment and control—we
    first select a block size, usually between 4 and 6\. We’ll use a constant block
    size of 4 for our example. Next, create all possible blocks of four subjects where
    the assignments are balanced. For us, this means all combinations of four-digit
    binary numbers where the number of 1s and 0s is the same:'
  prefs: []
  type: TYPE_NORMAL
- en: 1100, 1010, 1001, 0110, 0101, 0011
  prefs: []
  type: TYPE_NORMAL
- en: Each block has two 1s and two 0s.
  prefs: []
  type: TYPE_NORMAL
- en: We select as many blocks as needed to cover the subjects to generate the final
    group assignments. For completely balanced groups, the number of subjects must
    be a multiple of the block size, in this case, a multiple of four. So, for 32
    subjects total, we need a sequence of eight blocks because 32 / 4 = 8\. We randomly
    select the eight blocks from the set, but no matter which blocks we select, the
    total number of 1s and 0s will be the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, here’s a randomly selected sequence of eight blocks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: There are 16 subjects assigned to both the treatment and the control groups.
    Run the code again and the sequence will be different, but the number assigned
    to each group is still 16\. Apply the sequence to the 32 subjects selected for
    the study, perhaps by matching the assigned subject ID number in order with the
    sequence. See [Table 10-1](ch010.xhtml#ch010tab01).
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 10-1:** Matching Subject ID and Assigned Group'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **0** | **1** | **2** | **3** | **4** | **5** | **6** | **7** | **. .
    .** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Subject** | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | . . . |'
  prefs: []
  type: TYPE_TB
- en: '| **Group** | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 1 | . . . |'
  prefs: []
  type: TYPE_TB
- en: This continues for all 32 subjects.
  prefs: []
  type: TYPE_NORMAL
- en: Block randomization is an improvement over simple randomization in that it balances
    the number of subjects in the treatment and control groups. However, neither approach
    pays attention to other characteristics that may impact or mask treatment effects.
    If the outcome of the treatment is dramatic and broadly applicable, both simple
    and block randomization will show it. But if the treatment effect is weaker or
    relevant only to a specific demographic, then neither approach on its own is desirable.
    Enter stratified randomization.
  prefs: []
  type: TYPE_NORMAL
- en: '***Stratified***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In *stratified randomization*, we assign subjects to treatment and control groups
    such that each group contains a consistent mix of subjects with other characteristics,
    called *covariates*, that we (the experimenters) believe are likely to influence
    the results. For example, if we use a block design for a research project that
    tests running endurance after taking a supplement for three months, and our treatment
    group happens to have an abundance of non-smokers under age 25, there will likely
    be a strong difference in endurance between the treatment and control groups at
    the end of the experiment.
  prefs: []
  type: TYPE_NORMAL
- en: Stratified randomization manages the group composition as much as possible by
    matching the covariates of the controls to the treatment group.
  prefs: []
  type: TYPE_NORMAL
- en: There are different approaches to stratified randomization. In our simulation,
    we’ll pick a treatment group subject at random from a large population of potential
    subjects. Based on the covariates for that subject, we’ll search for a matching
    subject to put in the control group. This way, we have balanced numbers in the
    treatment and control groups, and we’ll know that every person in the treatment
    group has a matched control to make the overall characteristics of both groups
    the same. In other words, we’ll adjust for variation in the covariates.
  prefs: []
  type: TYPE_NORMAL
- en: Our experience with *bad_sample.py* demonstrated that small study sizes are
    susceptible to dramatic variation in factors like age, income, smoking, and drinking.
    These are the covariates we’ll use in our simulation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Defining the Simulation**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The remainder of the chapter works with the code in *design.py*. As always,
    I recommend reading the code before continuing. We’ll do a detailed walkthrough
    in the next section; in this section, we’ll define what we hope to accomplish
    with the simulation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the scenario: we want to evaluate the impact a novel supplement has
    on overall health after one year of use. To do this, we first select a treatment
    group that takes the supplement for a year. Then, we compare their overall health
    score to the score of a control group that did not take the supplement.'
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Modern clinical trials often use* placebos*, or “fake” treatments, with the
    control group. Here the placebo would be a sugar pill that looks like the supplement.
    Neither the participants nor the researchers would be aware of who received what,
    thereby making the study double-blind. Alternatively, the control group might
    not be told anything about the supplement group. They would be queried or tested
    at the beginning of the study and again at the end, with no placebo given. While
    we’ll use the latter approach, either will work in this case.*'
  prefs: []
  type: TYPE_NORMAL
- en: The simulation is rigged; there will always be a positive health benefit for
    the treatment group. The strength of the effect is under our control. Naturally,
    there is no supplement; the point of the simulation is to understand how each
    of the three experimental design choices affects the experiment’s outcome. Is
    it easy to detect the treatment effect? How convinced might we be if we see an
    improvement that the measured effect is real? We’ll seek to answer these questions.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we did with *bad_sample.py*, we’ll generate populations of individuals using
    the same covariates: age, income, smoking, and drinking. Unlike *bad_sample.py*,
    we won’t use age to influence the other covariates, and we’ll immediately place
    covariates into bins. In other words, we won’t say this person is 48 or 11; instead,
    we’ll select from three age bins, meaning age will be 0, 1, or 2\. We’ll do the
    same for income and drinks per week. Smoking is binary, 0 or 1.'
  prefs: []
  type: TYPE_NORMAL
- en: Each run of the code accumulates statistics for a user-selected number of experiments.
    We randomly generate a population, and select treatment and control groups according
    to the desired randomization scheme. We then apply the treatment (supplement),
    and use the health score for both groups to generate metrics. When all experiments
    are complete, we use the collected information to create output that provides
    insight into how well the randomization scheme worked.
  prefs: []
  type: TYPE_NORMAL
- en: 'As with all simulations, we must be careful to convince ourselves that the
    thing being simulated is a fair approximation of reality to the level we require.
    Remember Box’s adage: all models are wrong, but some are useful.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Implementing the Simulation**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s understand the essential parts of *design.py*, beginning with the main
    loop near the bottom of the file. Then, as needed, we’ll discuss functions that
    loop calls. After that comes the code to analyze the results. Algorithmically,
    the main loop repeats the following for the desired number of experiments (`nsimulations`):'
  prefs: []
  type: TYPE_NORMAL
- en: Create a population (a list of `Person` objects).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select `control` and `treatment` cohorts according to the user-supplied randomization
    scheme (`typ`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply the treatment to the `treatment` group by calling the `Treat` method,
    passing the desired treatment effect size (`beta`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Accumulate stats on the `control` and `treatment` cohorts. This includes the
    per-subject health score and the average value of each covariate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Append the collected data to the `results` list, with each element being a dictionary
    that holds the results for that simulated experiment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The main loop is in [Listing 10-1](ch010.xhtml#ch010list01).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 10-1: The main loop*'
  prefs: []
  type: TYPE_NORMAL
- en: There are five code paragraphs. The first creates `pop`, a list of `Person`
    objects from which we select `control` and `treatment` subjects.
  prefs: []
  type: TYPE_NORMAL
- en: The second code paragraph, a single line, selects the proper randomization function
    passing the population and the total number of subjects in the experiment (`nsubj`).
    For `Block` and `Stratified` randomization, the number of subjects in each group
    is always half the desired total. Both `control` and `treatment` are lists of
    `Person` objects selected from `pop`.
  prefs: []
  type: TYPE_NORMAL
- en: The next code paragraph calls the `Treat` method on each subject in the `treatment`
    group. The argument, `beta`, [0, 1), is the user-supplied treatment effect size,
    where a higher `beta` implies a stronger positive treatment effect. In other words,
    the supplement’s effectiveness improves as `beta` increases.
  prefs: []
  type: TYPE_NORMAL
- en: The fourth code paragraph calculates the per-subject health scores and covariate
    averages for the `control` and `treatment` groups.
  prefs: []
  type: TYPE_NORMAL
- en: The last code paragraph appends the results of this experiment to the `results`
    list. The call to `ttest_ind` performs a t-test between the treatment and control
    health scores while `Cohen_d` calculates Cohen’s *d* to measure the effect size.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s explore the supporting cast called by the main loop, followed by each
    randomization function. We want to simulate the latter correctly.
  prefs: []
  type: TYPE_NORMAL
- en: '***Functions and Classes***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Populations are lists of `Person` instances ([Listing 10-2](ch010.xhtml#ch010list02)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 10-2: The* Person *class*'
  prefs: []
  type: TYPE_NORMAL
- en: 'A person is a collection of five characteristics. Three of them are integers
    in [0, 2]: `age`, `income`, and `drink`. Notice the call to the globally defined
    `rng`, an instance of the `RE` class according to the supplied randomness source
    and seed. The `smoker` characteristic is binary, so a person has a 20 percent
    chance of being a smoker. The fifth characteristic, `adj`, is a random float in
    [–1, 1]. We’ll learn why we use this binning scheme in the next section.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Health` method returns a float indicating the person’s overall health:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Health` = 3 × (2 – `age`) + 2 × `income` – 2 × `smoker` – `drink` + `adj`'
  prefs: []
  type: TYPE_NORMAL
- en: Better health (a higher float) is associated with younger age, higher income
    bracket, not being a smoker, and minimized drinking, plus the random adjustment.
  prefs: []
  type: TYPE_NORMAL
- en: The `Treat` method applies the treatment by altering `adj` based on `beta`,
    the desired treatment effect. It does this by increasing `adj` by three times
    a random draw from a binomial distribution using a fixed number of trials (300).
  prefs: []
  type: TYPE_NORMAL
- en: We haven’t worked with binomial distributions before. If there’s an event with
    a probability *p* of happening on any given trial—think of flipping a loaded coin
    with probability *p* of landing heads up—and there are *n* trials, then the number
    of successful events in *n* trials follows a binomial distribution for an expected
    number of outcomes, [0, *n*].
  prefs: []
  type: TYPE_NORMAL
- en: For example, if the probability of an event is *p* = 0.7, or 70 percent, and
    there are *n* = 10 trials, then the expected number of events for repetitions
    follows the distribution in [Figure 10-2](ch010.xhtml#ch010fig02).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/10fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10-2: The binomial distribution for 10 trials and 70 percent probability
    of success per trial*'
  prefs: []
  type: TYPE_NORMAL
- en: The most frequent number of successes in 10 trials at 70 percent probability
    of success is 7, as we might expect. However, about 2.5 percent of the time, every
    trial was successful.
  prefs: []
  type: TYPE_NORMAL
- en: As the probability of success increases, so does the number of possible successful
    events over a given number of trials. In `Treat`, we scale the number of successful
    trials for probability `beta` by 300; thus, the entire expression becomes a number
    in [0, 1], which we multiply by 3 and add to `adj`. When the number of trials
    is large, the binomial distribution looks like a narrow normal distribution, where
    the treatment effect is narrowly centered on the supplied `beta` value.
  prefs: []
  type: TYPE_NORMAL
- en: The `binomial` function simulates a draw from a binomial distribution using
    the uniform distribution supplied by `rng`. This algorithm is inefficient because
    it uses many calls to `rng` to return one sample from the desired binomial distribution.
    Still, it’s good enough for us ([Listing 10-3](ch010.xhtml#ch010list03)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 10-3: Sampling from a binomial distribution*'
  prefs: []
  type: TYPE_NORMAL
- en: The number of successful trials is in `k`. The conditional return and assignment
    to `p` makes use of symmetry in the binomial distribution to return the proper
    sample for any desired probability of success for a given probability (`a`).
  prefs: []
  type: TYPE_NORMAL
- en: The main loop calls two more functions, `Cohen_d` and `Summarize`, as in [Listing
    10-4](ch010.xhtml#ch010list04).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 10-4: Additional helper functions*'
  prefs: []
  type: TYPE_NORMAL
- en: The `Cohen_d` function measures the effect size by calculating the difference
    in the means of two datasets along with the square root of their averaged variances.
  prefs: []
  type: TYPE_NORMAL
- en: The `Summarize` function queries a collection of subjects from either the control
    or treatment group to return a vector of subject health scores along with the
    mean age, income, smoke, and drink values over the subjects.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get to the heart of the matter and discuss the different randomization
    schemes themselves.
  prefs: []
  type: TYPE_NORMAL
- en: '***Schemes***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[Listing 10-5](ch010.xhtml#ch010list05) uses simple randomization to flip a
    coin, assigning selected subjects to either the treatment or control group.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 10-5: Simple random assignment*'
  prefs: []
  type: TYPE_NORMAL
- en: The `Simple` function first generates a random ordering of the population. This
    isn’t strictly necessary, as we randomly generated `pop`, but we shouldn’t assume
    that if we don’t need to.
  prefs: []
  type: TYPE_NORMAL
- en: The `for` loop picks subjects sequentially and, based on the coin flip, assigns
    them to the treatment or the control group. Note that the coin flip means the
    number of subjects in each group might not be the same. When done, we return both
    lists.
  prefs: []
  type: TYPE_NORMAL
- en: Block randomization ensures balanced treatment and control group sizes, as shown
    in [Listing 10-6](ch010.xhtml#ch010list06).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 10-6: Block randomization*'
  prefs: []
  type: TYPE_NORMAL
- en: The first line of `Block` uses integer division to set `ns` to the nearest multiple
    of 4 less than or equal to `nsubj`. Then come the block definitions as strings
    of binary numbers. These blocks are the six ways we can split four items evenly
    among two groups, treatment (1) and control (0).
  prefs: []
  type: TYPE_NORMAL
- en: We create the `seq` string with the number of blocks (`nblocks`) by randomly
    concatenating block definitions until the string is `ns` characters long.
  prefs: []
  type: TYPE_NORMAL
- en: The final `for` loop mimics `Simple` in randomizing the order of `pop` before
    assigning subjects to treatment and control groups based on the current character
    of `seq`.
  prefs: []
  type: TYPE_NORMAL
- en: Our approach to stratified randomization assigns a random subject from the population
    to the treatment group, searches the population for an unassigned person with
    the same characteristics, and assigns them to the control group. This way, both
    groups are balanced in number and in overall characteristics. In code, this becomes
    [Listing 10-7](ch010.xhtml#ch010list07).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 10-7: Stratified randomization*'
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 10-7](ch010.xhtml#ch010list07) creates a flag vector, `selected`,
    to mark members of the population already assigned to a group. The `while` loop
    then runs until the treatment group contains half the requested number of subjects.
    The body of the `while` loop sets `n` to the index of an unselected member of
    the population, who becomes a member of the treatment group. We update `selected`
    once `n` is found, and append the person to the treatment group (`t`).'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we find another unselected member of the population who has characteristics
    matching those of the person we just added to the treatment group; this is why
    the `Person` class uses bins for characteristics. There are only 3 × 3 × 2 × 3
    = 54 possible combinations of characters. With a large population, we’re virtually
    assured of finding a match, which happens when the `match` embedded function returns
    `True`. After we append the match to the control group, the outer `while` loop
    continues until it assigns all subjects.
  prefs: []
  type: TYPE_NORMAL
- en: The remaining code in *design.py* analyzes the results of multiple experiments.
    While I won’t walk through it, I’ll describe the analysis in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exploring the Simulation**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We’re ready to run some experiments. First, let’s orient ourselves with a quick
    recap:'
  prefs: []
  type: TYPE_NORMAL
- en: We are simulating experiments using a known positive treatment effect of a size
    we supply.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We are supplying the randomization scheme: simple, block, or stratified.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are evaluating the results of many experiments to understand the randomization
    scheme’s influence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We aren’t simulating a single experiment, but dozens to hundreds. We’ll evaluate
    the combined results from these experiments to (hopefully) arrive at a picture
    of the subtle influence randomization schemes have on experiment outcomes. The
    *design.py* file is a stand-in for an entire universe of treatment studies, all
    performed using the given randomization scheme.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s dive in. The *design.py* file expects several command line arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We must supply the population size (`npop`), number of subjects in each experiment
    (`nsubj`), treatment effect strength (`beta`), number of experiments to simulate
    (`nsim`), and type of randomization to use (`type`). As always, the randomness
    source and optional seed value are last.
  prefs: []
  type: TYPE_NORMAL
- en: '***Simple***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We can run our first simulation like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: A plot should appear along with the text output; we’ll get to it momentarily.
    We asked for 40 simulations of an experiment where we selected 32 subjects from
    a pool of 10,000 using simple randomization (0). The treatment effect size was
    0.3, meaning the `Treat` method on each `Person` object added a random value centered
    on 0.3 to the overall health of the individual.
  prefs: []
  type: TYPE_NORMAL
- en: These stats are drawn from the results of the simulations. The first two lines
    show the mean p-value for the lowest and highest 10 percent of experiments. Recall,
    the p-value for an experiment is from the health scores for the treatment and
    control groups. In this case, each experiment had approximately 16 subjects per
    group and there were 40 simulated experiments, so the highest and lowest 10 percent
    p-values correspond to 4 experiments. Because we used simple randomization, the
    treatment and control group size for a particular simulated experiment isn’t always
    16.
  prefs: []
  type: TYPE_NORMAL
- en: The next two lines report the mean Cohen’s *d* for the same set of experiments.
    Cohen’s *d* is a scaled version of the difference in the means between the treatment
    and control groups. Therefore, as we’ll see when using stratified randomization,
    we might expect the mean effect size for at least the lowest 10 percent group
    to be about the same as the requested effect size. Here, we asked for a smaller
    effect size of 0.3, but the mean was a substantial 1.0\. For the highest 10 percent
    group, the effect size was insignificant, which matches the p-value of 0.83\.
    For this group of experiments, we found no meaningful difference between the treatment
    and control groups.
  prefs: []
  type: TYPE_NORMAL
- en: The final four lines of output present the means of the per-experiment covariate
    *differences* for each group, low p-value and high p-value. We use a t-test here
    to see if the covariate differences between treatment and control significantly
    differed between the groups. For this run, the age difference was significant.
  prefs: []
  type: TYPE_NORMAL
- en: The *t* statistic for age is negative, meaning the low p-value experiments consisted
    of treatment and control groups with large age differences. In other words, for
    the low p-value experiments, the age covariate for the treatment and control groups
    was more likely to be different than for the high p-value experiments. Age is
    the most prominent factor in the health score, so a significant imbalance between
    treatment and control groups strongly affects measured results.
  prefs: []
  type: TYPE_NORMAL
- en: So, what should we make of these results? We ran many simulations, and, even
    for the best performing of them—those that showed a significant difference after
    treatment—we shouldn’t believe the results, as the covariates of those experiments
    were quite different. If we ran a single experiment, we’d likely see no effect
    and judge the treatment as a failure. For those cases where we did see an effect,
    it was likely artificially inflated because the treatment group was quite different
    from the control group in an important covariate.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that these results are for 40 experiments, meaning 32 subjects and
    simple random sampling into treatment and control groups tend toward unreliable
    results that either exaggerate the effectiveness of the treatment or mask it entirely.
  prefs: []
  type: TYPE_NORMAL
- en: The left side of [Figure 10-3](ch010.xhtml#ch010fig03) shows the *design.py*
    plot, a histogram displaying the t-test p-values between the health of the treatment
    and control groups for the 40 simulated experiments. The results to the left of
    the vertical dashed line at 0.05 indicate results that would, typically, be heralded
    as statistically significant.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/10fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10-3: Example plot output from* design.py *by randomization type*'
  prefs: []
  type: TYPE_NORMAL
- en: From the leftmost plot in [Figure 10-3](ch010.xhtml#ch010fig03), we see that
    about 12.5 percent of the experiments yielded a statistically significant difference.
    These are the experiments we’d likely read about in a scientific journal. The
    remaining 87.5 percent of the experiments didn’t result in a statistically significant
    treatment effect. Proper scientific adherence states that these experiments should
    be published as negative results, but such papers seldom appear—a known issue
    in the scientific literature.
  prefs: []
  type: TYPE_NORMAL
- en: While we don’t know a priori that the treatment effect is present and positive
    in the real world, in our case we do—and that all 40 simulated experiments should
    have found a positive result. So, why wasn’t it found? The size of the cohort
    is a contributing factor that we’ll experiment with shortly, but as shown in the
    simulation, so is the imbalance of covariates due to simple randomization, both
    in masking and exaggerating the effect.
  prefs: []
  type: TYPE_NORMAL
- en: '***Block***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Switching to block randomization requires changing a 0 to a 1 on the command
    line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Not much has changed when compared to the simple randomization results. Selecting
    balanced treatment and control group sizes for each experiment has eliminated
    any possible effect due to bad splits. Still, that effect is unlikely to show
    up at the level of many experiments, which we’re considering here. The middle
    plot in [Figure 10-3](ch010.xhtml#ch010fig03) is much like the simple randomization
    plot; in both, 12.5 percent of experiments were statistically significant at the
    *p* < 0.05 level. However, age is again highly different between the lowest and
    highest p-value experiment groups, though not at the level it was for the simple
    randomization case. We might improve this by increasing the number of subjects,
    but let’s switch to stratified randomization first and see what happens.
  prefs: []
  type: TYPE_NORMAL
- en: '***Stratified***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To switch to stratified randomization, change the 1 on the command line to
    a 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Note that the output lacks any “delta” lines because treatment and control groups
    have the same covariate makeup in stratified randomization, so all the deltas
    are zero; there is nothing to report. Recall, the “delta” lines refer to the means
    of the individual experiment differences between treatment and control groups.
    Those differences are zero because we selected a matched control group subject
    for each treatment group subject.
  prefs: []
  type: TYPE_NORMAL
- en: The plot is on the right in [Figure 10-3](ch010.xhtml#ch010fig03). It doesn’t
    look like the simple or block randomization plots, as the covariate effect has
    been compensated for by the stratified (matching) selection process. What the
    plot shows is due to other factors.
  prefs: []
  type: TYPE_NORMAL
- en: However, no experiment produced a p-value below the 0.05 threshold. After properly
    accounting for the covariates we believe influence health, the treatment effect,
    while present, is too weak to detect with only 32 subjects in the study.
  prefs: []
  type: TYPE_NORMAL
- en: Cohen’s *d* for the lowest p-value group is 0.47—not too far from the 0.3 we
    should get if we detect the treatment effect consistently. To improve matters
    further, we’ll adjust the study size by increasing the number of subjects.
  prefs: []
  type: TYPE_NORMAL
- en: '**Increasing the Number of Subjects**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Let’s move from 32 subjects to 128 subjects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The lowest 10 percent of experiments has a mean p-value of 0.06 and a mean *d*
    of 0.334, so we’re getting closer, but no experiments passed the magic threshold
    of 0.05.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s double the study size again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Now even the highest p-value group is near 0.05, and both *d* values are in
    the 0.3 ballpark. The plot (not shown) illustrates that almost all experiments
    produce *p* < 0.05 results.
  prefs: []
  type: TYPE_NORMAL
- en: With stratified randomization, we’ve accounted for covariates, so we’re seeing
    the actual treatment effect in these means. Still, we needed a sufficiently large
    number of subjects to tease the treatment effect out of the data. Let’s find out
    how many subjects we need for a given desired effect size.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code in *cohen_d_test.py* uses *design.py* to estimate the number of subjects
    necessary to achieve a mean p-value of 0.05 or less in the highest 10 percent
    of experiments. A run of the code produces:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The requested effect size is on the left, followed by the estimated number of
    subjects necessary, the p-value, and *d* for the highest 10 percent of simulated
    experiments. My code run took about five hours, mostly working the *d* = 0.1 case.
    Generally, the printed *d* values are close to the requested values, which is
    a good sign.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code is a loop over the desired effect sizes with an inner `while` loop
    that increments the number of subjects until the mean p-value is 0.05 or less:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The number of subjects starts at 10 (`base`) and is multiplied by a value `k`
    that increments by 1 inside the `while` loop until the p-value is at or below
    0.05\. The `RunTest` function constructs the *design.py* command line, executes
    it, and parses the output file to get *p* and *d*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Calculating the Simulation’s Statistical Power**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Let’s use `TTestIndPower` from the `statsmodels` package to see whether the
    simulation captures the correct number of subjects necessary for a desired *d*
    value.
  prefs: []
  type: TYPE_NORMAL
- en: The `TTestIndPower` class performs a *power* analysis for two independent samples,
    the treatment and control groups. See *power_analysis.py*, which produces output
    for the same set of effect sizes as *cohen_d_test.py*. Comparing the two in terms
    of the number of subjects results in [Table 10-2](ch010.xhtml#ch010tab02).
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 10-2:** Comparing the Calculated and Estimated Number of Subjects'
  prefs: []
  type: TYPE_NORMAL
- en: '| ***d*** | **Calculated** | **Estimated** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0.9 | 26 | 50 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.8 | 33 | 60 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.7 | 43 | 80 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.6 | 59 | 90 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.5 | 85 | 110 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.4 | 132 | 170 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.3 | 234 | 280 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.2 | 526 | 580 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.1 | 2,102 | 2,370 |'
  prefs: []
  type: TYPE_TB
- en: The calculated results are for a threshold of 0.05 (alpha) and a power of 0.9,
    meaning we want a 90 percent chance of achieving at least 0.05 as the p-value
    for the t-test between the treatment and control groups. I selected a power of
    0.9 because the empirical code seeks to make the mean of the top 10 percent of
    the highest p-values 0.05 or less, implying that almost all simulated experiments
    will find a result at a p-value of 0.05 or lower. The agreement between the calculated
    and estimated numbers of subjects is rough but reasonable, which validates the
    simulation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercises**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This chapter has only two exercises, both of which are challenging.
  prefs: []
  type: TYPE_NORMAL
- en: '***Working with Two Treatments***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Apply block randomization to a study with three conditions: control, treatment
    1, and treatment 2\. The block size should be a multiple of the number of conditions;
    fix the block size at 6, meaning each block will use each condition twice. For
    example, `[0,0,1,1,2,2]` is a valid block of six subjects where 0 is control,
    1 receives treatment 1, and 2 receives treatment 2.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We need a way to select at random from the set containing all possible permutations
    of the set of three conditions used twice. In other words, we’d like a NumPy array
    where each row is a valid block. Use code like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The `blocks` array is a 90×6 matrix with 90 unique blocks to select from when
    building a balanced cohort. The `permutations` function returns an iterable that
    `set` uses to return all unique permutations of the basic block, `b`. To make
    a NumPy array, convert the set into a list before passing it to `array`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Your task: alter a copy of *design.py* to select subjects for simple and block
    randomization for the control, treatment 1, and treatment 2 groups. For simple
    randomization, roll a three-sided die. Ignore stratified randomization for now.'
  prefs: []
  type: TYPE_NORMAL
- en: '***Combining Block and Stratified Randomization***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To combine block and stratified randomization, create blocks where all subjects
    are matched according to the desired covariates, and each block includes all conditions
    in a balanced manner. If there are three conditions—as in the previous exercise—and
    the blocks have six subjects each, then the subjects in each block must be matched
    to some level in terms of covariates.
  prefs: []
  type: TYPE_NORMAL
- en: Alter the copy of *design.py* from Possibility 1, but account for the 56 possible
    combinations of the four covariates. The subjects placed into a block must all
    have the same covariate values.
  prefs: []
  type: TYPE_NORMAL
- en: If the block is `[0,1,2,1,2,0]` and the covariate set is `[2,1,0,1]`, for example,
    then search the population for six subjects in age group 2, income group 1, smoker
    0, and drink 1, assigning them to the conditions as dictated by the block. Continue
    for *n* subjects where *n* is a multiple of six, the block size. Add this capability
    to your altered *design.py* from Exercise 1.
  prefs: []
  type: TYPE_NORMAL
- en: '**Summary**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This chapter focused on randomization in experimental design, particularly
    during subject selection. After briefly discussing the need for randomization,
    we explored the most common types: simple, block, and stratified.'
  prefs: []
  type: TYPE_NORMAL
- en: Simple randomization uses coin flips or dice rolls to place subjects in treatment
    or control groups. While this process can introduce bias due to unequal group
    sizes, block randomization removes this possibility by ensuring that treatment
    and control group sizes are balanced.
  prefs: []
  type: TYPE_NORMAL
- en: We learned about covariates, factors outside the study scope that may affect
    the results by enhancing or masking any possible treatment effect. Stratified
    randomization uses covariates to construct treatment and control groups with matching
    characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: We then created a simulation to test randomization schemes on the outcome of
    an experiment where we knew the treatment effect we should see. We learned that
    small cohorts using simple randomization are prone to bias; while block randomization
    removes bias due to imbalanced group sizes, stratified randomization corrects
    for bias in the treatment and control groups. This allows us to measure the effect
    we expect from the simulation, provided the number of subjects is of some minimal
    size.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we matched the number of subjects necessary in a stratified setting
    to achieve the desired effect size at a *p* < 0.05 level for most of our simulated
    experiments to the number of subjects indicated by a t-test-based power analysis.
    The empirical number was in reasonably good agreement with the model, so we claimed
    this as validation of our approach.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s continue our journey with randomized algorithms, an essential class of
    algorithms in computer science.
  prefs: []
  type: TYPE_NORMAL
