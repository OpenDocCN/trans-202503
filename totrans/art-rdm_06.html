<html><head></head><body>
<h2 class="h2" id="ch06"><span epub:type="pagebreak" id="page_173"/><strong><span class="big">6</span><br/>MACHINE LEARNING</strong></h2>
<div class="image1"><img alt="Image" src="../images/common.jpg"/></div>
<p class="noindent">The goal of machine learning is to train models to generate correct outputs when given previously unseen inputs. This is usually done by repeatedly presenting the model with a collection of known inputs and outputs until the model succeeds in properly assigning outputs to inputs, or <em>learns</em>.</p>
<p class="indent">In this chapter, we’ll explore randomness in machine learning by building two datasets for histology slides and images of handwritten digits. As we’ll learn, randomness is essential to building suitable machine learning datasets.</p>
<p class="indent">Next, we’ll explore randomness in neural networks—the driving force behind the AI revolution. We’ll restrict ourselves to traditional neural network architectures; randomness is just as important, if not more so, when working with advanced models.</p>
<p class="indent">After neural networks come <em>extreme learning machines</em>, simple neural networks that fundamentally depend on randomness. Unlike their grown-up cousins, extreme learning machines don’t require extensive training, but instead rely on the power of randomness to do most of the learning for them.</p>
<p class="indent"><em>Random forests</em> close out the chapter, and are also critically dependent on randomness for their success.</p>
<p class="indent"><span epub:type="pagebreak" id="page_174"/>I’ll point out where randomness appears as we move through the chapter. Randomness is central to the success of machine learning, from your favorite smart speaker to the self-driving car you may (someday soon) be riding in.</p>
<h3 class="h3" id="ch00lev1_41"><strong>Datasets</strong></h3>
<p class="noindent">In machine learning, we train models from sample data. Therefore, we must build datasets before beginning our explorations. Randomness plays a critical role in this process.</p>
<p class="indent">We’ll build two datasets. The first consists of measurements of cells from histology slides that hopefully enable the model to learn whether the tissue sample is benign (class 0) or malignant (class 1).</p>
<p class="indent">The second dataset consists of 28×28-pixel grayscale images of handwritten digits: 1, 4, 7, and 9. The images aren’t stored in the usual format; rather, they’re unraveled into vectors where the first row is followed by the second, and so on, to map the 28×28 pixels to 784-element vectors.</p>
<h4 class="h4" id="ch00lev2_55"><em><strong>Histology Slide Data</strong></em></h4>
<p class="noindent">Machine learning etiquette dictates that training a model requires a minimum of two datasets. The first is the <em>training set</em>, a collection of pairs, (<em><strong>x</strong></em>, <em>y</em>), where <em><strong>x</strong></em> are input vectors and <em>y</em> are the corresponding output labels. The second is a <em>test set</em>, of the same kind as the training set, but it’s not used until training is complete. The model’s performance on the test set decides how well it’s learned.</p>
<p class="indent">The <em>raw</em> directory holds the <em>bc_data.npy</em> and <em>bc_labels.npy</em> files. The first is a dataset that contains a two-dimensional NumPy array of 569 rows and 30 columns. Each row is a <em>sample</em>, and each column a <em>feature</em>. The 30 elements of a sample represent 10 measurements of three different cells on the histology slide. The second file contains the label, 0 for benign and 1 for malignant. There’s a one-to-one correspondence between the rows of the data and the labels. Therefore, row 0 of <em>bc_data.npy</em> represents features from a benign sample, while row 2 has features from a malignant sample because the first element of the vector in <em>bc_labels.npy</em> is a 0, and the third is a 1.</p>
<p class="indent">We’ll build two datasets from the 569 samples, using a 70/30 split, meaning 70 percent of the samples are for training (398) and the remaining 30 percent for testing (171).</p>
<p class="indent">As machine learning models are notoriously slow to learn, we should be concerned that 398 samples are not enough to condition the model. We want more data, but don’t have any more; what are we to do?</p>
<p class="indent">Randomness comes to our aid. We can <em>augment</em> the data by creating fake samples that, plausibly, come from the same source as our training data. We’ll apply random alterations to the existing data—enough to make it different, but not so much that the labels are no longer accurate. Data augmentation is a powerful part of modern machine learning that helps models learn not to pay too much attention to the particulars of the training <span epub:type="pagebreak" id="page_175"/>set, but instead seek more general characteristics that differentiate between the classes.</p>
<p class="indent">Before we augment the training samples, we need to standardize the data. Many machine learning models have difficulty with features that have different ranges. For example, one feature might sit in the range [0, 2] while another uses [–30,000, 30,000]. To bring both features into the same relative range, we subtract the mean value of each feature and then divide by the standard deviation of the features. After this transformation, each feature has a mean value close to zero and a standard deviation of one.</p>
<p class="indent">We have standardized features that we’ve split into two disjoint groups, one for training and one for testing. We’re now ready to augment the training data by employing <em>principal component analysis (PCA)</em>. If we were able to plot the data in 30 dimensions, we’d see that it’s spread out in some directions more than others. PCA finds these directions and, in effect, rotates the 30-dimensional coordinate system so that the first coordinate aligns with the direction where there’s the most variation in the data, then the second coordinate with the next, and so on. This means that later coordinates are less important in representing the data (though perhaps not in distinguishing between classes). We’ll take advantage of this decreasing importance in directions to randomly alter the coordinate directions, producing training data similar to the original but not identical. By making small changes, we can be (reasonably) confident that the new data represents an instance of the original class.</p>
<p class="indent">The code we need is in <em>build_bc_data.py</em>. Let’s walk through the important bits, beginning with loading the raw data and separating it into train and test (<a href="ch06.xhtml#ch06list01">Listing 6-1</a>).</p>
<pre class="pre">   np.random.seed(8675309)
   x = np.load("raw/bc_data.npy")
   y = np.load("raw/bc_labels.npy")
<span class="ent">➊</span> x = (x - x.mean(axis=0)) / x.std(ddof=1,axis=0)
   i = np.argsort(np.random.random(len(y)))
   x = x[i]
   y = y[i]
   n = int(0.7*len(y))
   xtrn = x[:n]
   ytrn = y[:n]
   xtst = x[n:]
   ytst = y[n:]</pre>
<p class="list" id="ch06list01"><em>Listing 6-1: Splitting the raw histology data</em></p>
<p class="indent">First, we fix NumPy’s pseudorandom number seed so the same dataset is built each time the code is run. Generally, altering NumPy’s seed this way is not a good idea, as it affects <em>all</em> code using NumPy, even inside other modules (like the scikit-learn modules we’ll use later in the chapter). However, in this case, we’ll take the risk.</p>
<p class="indent">Next, we load the raw data and labels before standardizing <span class="ent">➊</span>. We want the mean value of each feature. The columns of <code>x</code> are the features, requiring <span epub:type="pagebreak" id="page_176"/>the use of <code>axis=0</code>. This keyword applies the function, <code>mean</code>, across the rows of <code>x</code>, thereby delivering a 30-element vector where each element is the mean of the corresponding column of <code>x</code>.</p>
<p class="indent">We subtract this mean from each sample, or each row of <code>x</code>. With NumPy’s broadcast rules, we can do this automatically, with no looping required. NumPy is smart enough to see that we’re attempting to subtract a 30-element vector from a 2D array where the second dimension is 30, so it performs the subtraction and repeats it for each row.</p>
<p class="indent">Next, we divide the mean-subtracted data by the standard deviation of each feature. Again, <code>axis=0</code> lets us apply the function across the rows. Because of NumPy’s broadcast rules, the division is applied down the rows of <code>x</code> to produce the final, standardized dataset.</p>
<p class="indent">The next three lines randomize the dataset by assigning <code>i</code> to a random permutation of the numbers 0 through 568. NumPy’s <code>argsort</code> function doesn’t sort a vector, but instead returns the sequence of indices that would sort it. The following two lines apply this permutation to both <code>x</code> and <code>y</code> to scramble the data and the labels in sync.</p>
<p class="indent">The final five lines split the raw dataset into training sets (<code>xtrn</code>, <code>ytrn</code>) and testing sets (<code>xtst</code>, <code>ytst</code>). Note that we split the dataset before augmenting; if we split it after, augmented versions of a sample will likely end up in the test set, thereby making the model seem better than it is.</p>
<p class="indent">We’re now in a position to augment the data. First, we need to learn the principal components of the raw data, then we build a new training set where each original sample is kept, along with nine augmented versions of that sample.</p>
<p class="indent">In <a href="ch06.xhtml#ch06list02">Listing 6-2</a>, scikit-learn supplies <code>PCA</code>.</p>
<pre class="pre">from sklearn import decomposition
pca = decomposition.PCA(n_components=xtrn.shape[1])
pca.fit(x)</pre>
<p class="list" id="ch06list02"><em>Listing 6-2: Using PCA to learn the principal components</em></p>
<p class="noindent">The <code>PCA</code> class follows scikit-learn’s standard approach of defining an instance of a class before calling <code>fit</code> on that instance. We set the number of components to the number of features in the dataset (30).</p>
<p class="indent">We’ll use the trained <code>pca</code> object in a loop to build a new training set of augmented samples, as shown in <a href="ch06.xhtml#ch06list03">Listing 6-3</a>.</p>
<pre class="pre">start = 24
nsets = 10
nsamp = xtrn.shape[0]
newx = np.zeros((nsets*nsamp, xtrn.shape[1]))
newy = np.zeros(nsets*nsamp, dtype="uint8")

for i in range(nsets):
    if (i == 0):
        newx[0:nsamp,:] = xtrn
        newy[0:nsamp] = ytrn<span epub:type="pagebreak" id="page_177"/>
    else:
        newx[(i*nsamp):(i*nsamp+nsamp),:] = generateData(pca, xtrn, start)
        newy[(i*nsamp):(i*nsamp+nsamp)] = ytrn</pre>
<p class="list" id="ch06list03"><em>Listing 6-3: Augmenting the samples</em></p>
<p class="indent">The new training data is in <code>newx</code> and <code>newy</code>. Each existing training sample will be accompanied by nine augmented versions of it, so the new training set will have 3,980 samples instead of a mere 398.</p>
<p class="indent">The loop constructs the dataset in blocks of 398 samples. The first pass stores the original data, and subsequent passes call <code>generateData</code> to return a new, augmented version of the samples in the original dataset. The order of the new samples is identical to the original, meaning the labels are in the same order as well.</p>
<p class="indent">In <a href="ch06.xhtml#ch06list04">Listing 6-4</a>, the function <code>generateData</code> applies the PCA transformation and alters the least important coordinate directions beginning with <code>start</code> (24).</p>
<pre class="pre">def generateData(pca, x, start):
    original = pca.components_.copy()
    ncomp = pca.components_.shape[0]
    a = pca.transform(x)
    for i in range(start, ncomp):
        pca.components_[i,:] += np.random.normal(scale=0.1, size=ncomp)
    b = pca.inverse_transform(a)
    pca.components_ = original.copy()
    return b</pre>
<p class="list" id="ch06list04"><em>Listing 6-4: Applying PCA</em></p>
<p class="indent">PCA is a reversible transformation. The <code>generateData</code> function alters the PCA components by adding a small, normally distributed value to each one beginning with component 24 (out of 30). When the inverse transform uses the altered components, the resulting values (<code>b</code>), a block of 398 samples, are no longer identical to the original. These augmented versions build the next block of the new dataset.</p>
<p class="indent">The new training dataset is in <code>newx</code> and <code>newy</code>, but the order of the samples is not random because it was built in blocks. Therefore, we perform a final randomization before writing the train and test datasets to disk (<a href="ch06.xhtml#ch06list05">Listing 6-5</a>).</p>
<pre class="pre">i = np.argsort(np.random.random(nsets*nsamp))
newx = newx[i]
newy = newy[i]
np.save("datasets/bc_train_data.npy", newx)
np.save("datasets/bc_train_labels.npy", newy)
np.save("datasets/bc_test_data.npy", xtst)
np.save("datasets/bc_test_labels.npy", ytst)</pre>
<p class="list" id="ch06list05"><em>Listing 6-5: Storing the augmented datasets</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_178"/>The histology training and test datasets are now ready for use. We applied randomness multiple times to scramble the order of the data and to alter the principal components to build augmented versions of the training data.</p>
<h4 class="h4" id="ch00lev2_56"><em><strong>Handwritten Digits</strong></em></h4>
<p class="noindent">One of the first great successes of the deep learning revolution involved correctly identifying objects in images. While the dataset we’ll build isn’t very advanced comparably, it is part of MNIST, a larger, workhorse dataset commonly used in machine learning. We’ll build a dataset consisting of handwritten 1s, 4s, 7s, and 9s. I selected these four digits because even humans often confuse one for another, so we might expect a machine learning model to do the same (time will tell).</p>
<p class="indent"><a href="ch06.xhtml#ch06fig01">Figure 6-1</a> shows samples of each digit type.</p>
<div class="image"><img alt="Image" id="ch06fig01" src="../images/06fig01.jpg"/></div>
<p class="figcap"><em>Figure 6-1: Sample digits, where 1s and 7s are often confused, as are 4s and 9s</em></p>
<p class="indent">The images are 28×28-pixel grayscale, with each pixel an integer in [0, 255]. We’ll work with the digits as 784-element vectors (28 × 28 = 784). I collected the digits and their labels in the files <em>mnist_data.npy</em> and <em>mnist_labels.npy</em>, respectively.</p>
<p class="indent">The raw digits dataset is relatively small, with 100 samples of each digit; we split the raw data to have 50 train and 50 test samples. We’ll augment each image multiple times to expand the size of the training set.</p>
<p class="indent">We used PCA to augment the histology data, but because we’re working with images here, we’ll apply basic image processing transformations to randomly produce slightly altered versions of each training image. In particular, we’ll rotate each image in the range [–3, 3] degrees; about 10 percent of the time, we’ll zoom the image to [0.8, 1.2] times its original size while maintaining a final size of 28×28 pixels by cropping or embedding a smaller image inside a blank 28×28 image.</p>
<p class="indent">The code we need is in <em>build_mnist_dataset.py</em>. While it mirrors the code that built the histology dataset, differences include splitting the raw data 50/50 between train and test, storing the unaugmented training data, and augmenting the training data 20 times instead of 10 times, resulting in an augmented training set of 4,200 samples (200 original plus 20 more for each original sample); see <a href="ch06.xhtml#ch06list06">Listing 6-6</a>.</p>
<pre class="pre">   newx = []
   newy = []
   for i in range(len(ytrn)):
       newx.append(xtrn[i])<span epub:type="pagebreak" id="page_179"/>
       newy.append(ytrn[i])
    <span class="ent">➊</span> for j in range(20):
           newx.append(augment(xtrn[i]))
           newy.append(ytrn[i])
   xtrn = np.array(newx)
   ytrn = np.array(newy)
<span class="ent">➋</span> i = np.argsort(np.random.random(len(ytrn)))
   xtrn = xtrn[i]
   ytrn = ytrn[i]</pre>
<p class="list" id="ch06list06"><em>Listing 6-6: Augmenting the digit images</em></p>
<p class="indent">The original 200 training samples (<code>xtrn</code>, <code>ytrn</code>) are examined, one by one. First, we add the original image to the augmented output (<code>newx</code>, <code>newy</code>). Then, we add 20 augmented versions of the same image by making repeated calls to <code>augment</code> <span class="ent">➊</span>. After adding the augmented images, we scramble the entire training set again to mix the order of the images <span class="ent">➋</span>.</p>
<p class="indent"><a href="ch06.xhtml#ch06list07">Listing 6-7</a> shows the code to augment images.</p>
<pre class="pre">from scipy.ndimage import rotate, zoom
def augment(x):
    im = x.reshape((28,28))
    if (np.random.random() &lt; 0.5):
        angle = -3 + 6*np.random.random()
        im = rotate(im, angle, reshape=False)
    if (np.random.random() &lt; 0.1):
        f = 0.8 + 0.4*np.random.random()
        t = zoom(im, f)
        if (t.shape[0] &lt; 28):
            im = np.zeros((28,28), dtype="uint8")
            c = (28-t.shape[0])//2
            im[c:(c+t.shape[0]),c:(c+t.shape[0])] = t
        if (t.shape[0] &gt; 28):
            c = (t.shape[0]-28)//2
            im = t[c:(c+28),c:(c+28)]
    return im.ravel()</pre>
<p class="list" id="ch06list07"><em>Listing 6-7: Augmenting an image</em></p>
<p class="indent">The input image (<code>x</code>), a NumPy vector, is first reshaped into a 28×28-element two-dimensional array, <code>im</code>. Next, two <code>if</code> statements ask whether a random value is less than 0.5 or 0.1. The first, executed 50 percent of the time, applies a random rotation of the image by some value in the range [–3, 3] degrees. Notice the use of <code>rotate</code> from <code>scipy.ndimage</code>. The <code>reshape=False</code> keyword forces <code>rotate</code> to return an output array that’s the same size as the input array.</p>
<p class="indent">The second <code>if</code> statement, executed 10 percent of the time, uses <code>zoom</code> to magnify the image by a random scale factor in [0.8, 1.2], meaning the zoomed image is anywhere from 80 to 120 percent the size of the original. <span epub:type="pagebreak" id="page_180"/>The code after the call to <code>zoom</code> ensures the output image is still 28×28 pixels by either embedding the smaller image in a blank 28×28 image or selecting the central 28×28 pixels if magnifying beyond 100 percent. The newly augmented image is returned after unraveling it and transformed back into a 784-element vector.</p>
<p class="indent">The code in <em>build_mnist_dataset.py</em> stores the smaller, unaugmented training set and the augmented training set. The file <em>mnist_test.py</em> uses <code>sklearn</code>’s <code>MLPCLassifier</code>—which we’ll learn about soon—to train 40 models using the training sets, keeping the overall accuracy of each. The models use default values and a single hidden layer of 100 nodes. The mean accuracy for the unaugmented dataset was 87.3 percent, while augmented training data led to a mean overall accuracy of 90.3 percent, a statistically significant difference, thereby delivering evidence that the random augmentation process aids training models. It may seem tedious to spend this section detailing how the datasets are constructed, but it’s hard to overemphasize the importance randomness plays in the process. Dataset construction is so crucial to modern machine learning that competitions are held where the models are fixed and good dataset construction is required to produce winning results (search for “Data-Centric AI Competition”).</p>
<p class="indent">Now that we have our datasets, let’s put them to the test.</p>
<h3 class="h3" id="ch00lev1_42"><strong>Neural Networks</strong></h3>
<p class="noindent">A <em>neural network</em> is a set of nodes that, layer by layer, successively transforms an input to an output. Network nodes accept multiple inputs and produce a single output, an operation sufficiently similar to the operation of a biological neuron that the name “neural network” has persisted. Neural networks are not artificial brains; rather, they are feed-forward, directed, acyclic <em>graphs</em>, a data structure commonly used in computer science.</p>
<p class="indent">The operation of the network transforms an input to an output; in other words, neural networks are a type of function, <em><strong>y</strong></em> = <em>f</em>(<em><strong>x</strong></em>; <em><strong>θ</strong></em>). Training the neural network means locating <em><strong>θ</strong></em>, the set of parameters causing the network to perform as desired.</p>
<h4 class="h4" id="ch00lev2_57"><em><strong>Anatomy Analysis</strong></em></h4>
<p class="noindent">If a neural network is a set of nodes working in layers, then it makes sense to begin with a node. See <a href="ch06.xhtml#ch06fig02">Figure 6-2</a>.</p>
<div class="image"><img alt="Image" id="ch06fig02" src="../images/06fig02.jpg"/></div>
<p class="figcap"><em>Figure 6-2: A neural network node</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_181"/>Data flows from left to right. The inputs (<em>x</em>), either the input to the network or the output of a previous network layer, are multiplied by <em>weights</em> (<em>w</em>) and summed along with a <em>bias</em> (<em>b</em>) before passing the total to an <em>activation</em> function (<em>h</em>) to produce the output, <em>a</em>. In symbols:</p>
<p class="center"><em>a</em> = <em>h</em>(<em>w</em><sub>0</sub><em>x</em><sub>0</sub> + <em>w</em><sub>1</sub><em>x</em><sub>1</sub> + <em>w</em><sub>2</sub><em>x</em><sub>2</sub> + <em>b</em>)</p>
<p class="indent">The activation function is nonlinear, or some function beyond <em>x</em> to the first power. Modern networks often use <em>rectified linear unit (ReLU)</em> activation functions, which output the input unless the input is less than zero, in which case the output is zero, <em>h</em> = max(0, <em>x</em>).</p>
<p class="indent">A network layer consists of a collection of nodes that, collectively, receive the output of the previous layer as their inputs and produce new output, one per node, passed to the next layer. Each node in a layer receives each input; in graph terms, the network is <em>fully connected</em>.</p>
<p class="indent">Data flows through the network, layer by layer, to the output. In this way, the network maps input <em><strong>x</strong></em> to output <em><strong>y</strong></em>. The output is often a vector, but can be a scalar, <em>y</em>.</p>
<p class="indent">Moving data through a neural network is most easily accomplished by representing the weights between layers as a matrix and the biases as a vector. This representation automatically applies each input to each node and activation function, thereby reducing the entire layer operation to a matrix multiplied by a vector plus another vector passed to a vector version of the activation function. Training means learning a set of matrices for the weights, one matrix per layer, and a set of bias vectors, one per layer.</p>
<p class="indent"><span epub:type="pagebreak" id="page_182"/>The weights and biases, which live on the connections between the nodes of the layers, are the parameters of <em><strong>θ</strong></em>; these are the things the network learns during training. This is analogous to fitting a curve, but in this case, the function is determined by the architecture of the network. Unlike curve fitting, training a neural network usually doesn’t involve reducing the error on the training set to zero. Instead, the goal is to coax the network to learn weights and biases that make the network generally applicable to new inputs. After all, the entire point of training the model is to use it with new, unknown inputs.</p>
<p class="indent">A detailed discussion of neural network training is beyond our present scope, as our goal is to understand randomness’s role in the process. If you’re interested in learning more about neural networks, I recommend my book <em>Practical Deep Learning: A Python-Based Introduction</em> (2021), also available from No Starch Press. Remember that the training process produces a specific set of weights and biases that tailor the network to the problem at hand.</p>
<h4 class="h4" id="ch00lev2_58"><em><strong>Randomness</strong></em></h4>
<p class="noindent">Randomness is critically important at the beginning of the training process, in the selection of the initial weights and biases.</p>
<p class="indent">Training a neural network follows this general algorithm:</p>
<ol>
<li class="noindent">Randomly initialize the weights and biases of the network.</li>
<li class="noindent">Pass a randomly selected subset of the training data through the network.</li>
<li class="noindent">Use a measure of the error between the network’s output and the desired output to update the weights and biases.</li>
<li class="noindent">Repeat from step 2 until training is done (however decided).</li>
</ol>
<p class="indent">In this section, we’re concerned with step 1. Steps 2 and 3 involve a <em>loss function</em>, a measure of the mistakes the network has made, and a two-step process to update the weights and biases: <em>backpropagation</em> and <em>gradient descent</em>. The former uses the chain rule from differential calculus to determine how each weight and bias value contributes to the error, and the latter uses the gradient formed from those measures to alter the weights and biases to minimize the loss function.</p>
<h4 class="h4" id="ch00lev2_59"><em><strong>Initialization</strong></em></h4>
<p class="noindent">It was only in 2010–2012 that deep neural networks (with many layers) exploded on the scene. One of the factors contributing to this was the realization that previous algorithms for initializing the weights and biases of models were relatively poor; better options exist.</p>
<p class="indent">We will explore these options by employing the <code>MLPCLassifier</code> class from <code>sklearn</code> to implement a traditional neural network. <em>MLP</em> stands for <span epub:type="pagebreak" id="page_183"/><em>multilayer perceptron</em>, with “perceptron” being an old name for a neural network (I recommend searching for Frank Rosenblatt and his Perceptron machine—fascinating research that was not sufficiently appreciated in its time).</p>
<h5 class="h5"><strong>Initializing the Network</strong></h5>
<p class="noindent">The <code>MLPClassifier</code> class includes an internal method, <code>_init_coef</code>, which is responsible for assigning the initial weights and biases. We will subclass <code>MLPClassifier</code> and override this method, allowing us to alter the initialization approach while still taking advantage of everything else the <code>MLPClassifier</code> has to offer. Take a look at <a href="ch06.xhtml#ch06list08">Listing 6-8</a>.</p>
<pre class="pre">from sklearn.neural_network import MLPClassifier

def normal(rng, mu=0, sigma=1):
    if (normal.state):
        normal.state = False
        return sigma*normal.z2 + mu
    else:
        u1,u2 = rng.random(2)
        m = np.sqrt(-2.0*np.log(u1))
        z1 = m*np.cos(2*np.pi*u2)
        normal.z2 = m*np.sin(2*np.pi*u2)
        normal.state = True
        return sigma*z1 + mu
normal.state = False

class Classifier(MLPClassifier):
    def _init_coef(self, fan_in, fan_out, dtype):
     <span class="ent">➊</span> def normvec(fan_in, fan_out):
            vec = np.zeros(fan_in*fan_out)
            for i in range(fan_in*fan_out):
                vec[i] = normal(self.rng)
            return vec.reshape((fan_in,fan_out))

        if (self.init_scheme == 0):
         <span class="ent">➋</span> return super(Classifier, self)._init_coef(fan_in, fan_out, dtype)
        elif (self.init_scheme == 1):
         <span class="ent">➌</span> vec = self.rng.random(fan_in*fan_out).reshape((fan_in,fan_out))
            weights = 0.01*(vec-0.5)
            biases = np.zeros(fan_out)
        elif (self.init_scheme == 2):
         <span class="ent">➍</span> weights = 0.005*normvec(fan_in, fan_out)
            biases = np.zeros(fan_out)<span epub:type="pagebreak" id="page_184"/>
        elif (self.init_scheme == 3):
         <span class="ent">➎</span> weights = normvec(fan_in, fan_out)*np.sqrt(2.0/fan_in)
            biases = np.zeros(fan_out)

        return weights.astype(dtype, copy=False), biases.astype(dtype,
            copy=False)</pre>
<p class="list" id="ch06list08"><em>Listing 6-8: Overriding the initialization method</em></p>
<p class="indent">The subclass overrides scikit-learn’s approach <span class="ent">➋</span> with three other approaches. The method returns a weight matrix and a bias vector for a layer that has <code>fan_in</code> inputs and <code>fan_out</code> outputs. The <code>dtype</code> parameter specifies the data type, typically 32- or 64-bit floating point.</p>
<p class="indent">By default, scikit-learn uses <em>Glorot initialization</em>. We get it by calling the superclass version of the method <span class="ent">➋</span>. Glorot initialization depends on the number of inputs and outputs and is one of the initialization approaches leading to improved model performance. At least, that’s the claim. We’ll put it to the test.</p>
<p class="indent">Another modern initialization approach is <em>He initialization</em> <span class="ent">➎</span>, which is suitable for networks using ReLU activation functions. He initialization depends on a matrix of samples from a normal distribution with a mean of 0 and a standard deviation of 1. We get that here via the embedded <code>normvec</code> function <span class="ent">➊</span>, which permits us to use our <code>RE</code> class. We’ll see precisely how momentarily.</p>
<p class="indent">We initialize classical neural networks through the use of small uniformly distributed <span class="ent">➌</span> or normally distributed <span class="ent">➍</span> random values. The first draws random samples from a uniform distribution, [–0.005, 0.005]. The second uses normally distributed samples scaled by 0.005. Both approaches are intuitively reasonable, but, as we’ll see, they are not ideal.</p>
<p class="indent">The <code>normal</code> function returns a normally distributed sample with a mean value of <code>mu</code> and a standard deviation of 1. A normal distribution selects samples symmetrically around the mean; return to <a href="ch01.xhtml#ch01fig01">Figure 1-1</a> on <a href="ch01.xhtml#ch01fig01">page 4</a> as an example. NumPy provides a function to select samples from a normal distribution, but we want to use our <code>RE</code> class, so we need to define a function that creates normally distributed samples from the uniformly distributed samples <code>RE</code> returns. We can use the Box-Muller transformation introduced in <a href="ch01.xhtml">Chapter 1</a>:</p>
<div class="image1"><img alt="Image" src="../images/f0184-01.jpg"/></div>
<p class="noindent">Here <em>u</em><sub>1</sub> and <em>u</em><sub>2</sub> are uniformly distributed samples in [0, 1), precisely what <code>RE</code> returns (how convenient!).</p>
<p class="indent">The code implementing <code>normal</code> requires explanation. Note the indentation on the final line: it’s not part of <code>normal</code>, but refers to it immediately after its definition.</p>
<p class="indent">The Box-Muller transformation equations use a pair of uniform samples to produce a pair of normally distributed samples. While we can ask <code>RE</code> objects to return a pair of uniform samples, we want <code>normal</code> to return only <span epub:type="pagebreak" id="page_185"/>one normally distributed sample when called. We can generate two samples and throw one away, or carefully try to preserve both. This way, each call to <code>normal</code> returns a single sample and generates new samples only when required. Accomplishing this requires <code>normal</code> to preserve state between calls. The Python way to do this is to use a class, but that seems a bit overkill. Instead, we’ll make use of the fact that Python functions are objects; we can add new attributes (member variables) to objects at will. We’ll define <code>normal</code> as a Python function and then immediately add a <code>state</code> variable to it initialized to <code>False</code>—the final line in <a href="ch06.xhtml#ch06list08">Listing 6-8</a>.</p>
<p class="indent">When we call <code>normal</code>, the <code>state</code> variable has a value, <code>False</code>. This value persists between calls because it’s an attribute of the <code>normal</code> function, which uses the value of <code>state</code> to decide whether to generate two new samples and return the first caching the second, or to return the second. If <code>state</code> is <code>True</code>, return the second sample, <code>z2</code>, after multiplying it by the desired standard deviation (<code>sigma</code>) and adding the mean value (<code>mu</code>). Then set <code>state</code> to <code>False</code>. If <code>state</code> is <code>False</code>, get two uniform samples from <code>rng</code> and use them to calculate two normal samples, <code>z1</code> and <code>z2</code>. Cache <code>z2</code> and return <code>z1</code>, properly scaled and offset.</p>
<p class="indent">To use <code>Classifier</code> we create a neural network using scikit-learn and add two new attributes: <code>init_scheme</code> to specify the desired initialization scheme, and <code>rng</code>, an instance of <code>RE</code>, to allow access to the different randomness sources we’ve developed throughout the book.</p>
<h5 class="h5"><strong>Experimenting with Initialization</strong></h5>
<p class="noindent">Let’s experiment with <code>Classifier</code> and neural network initialization. Along the way, we’ll set up a scikit-learn training session.</p>
<p class="indent">The code we need is in <em>init_test.py</em>. It loads the digits dataset created previously and trains 12 models using each initialization scheme. Training multiple models for each is essential because initialization is random; for example, we might get a bad dice roll for one initialization scheme when it’s actually a good approach. Averaging the results from multiple models lets us apply statistical tests.</p>
<p class="indent"><a href="ch06.xhtml#ch06list09">Listing 6-9</a> shows the beginning of the main part of <em>init_test.py</em>.</p>
<pre class="pre">import numpy as np
from Classifier import *
from RE import *
from scipy.stats import ttest_ind, mannwhitneyu

xtrn = np.load("../../data/datasets/mnist_train_data.npy")/256.0
ytrn = np.load("../../data/datasets/mnist_train_labels.npy")
xtst = np.load("../../data/datasets/mnist_test_data.npy")/256.0
ytst = np.load("../../data/datasets/mnist_test_labels.npy")

N = 12
init0 = []

<span epub:type="pagebreak" id="page_186"/>for i in range(N):
    init0.append(Run(0, xtrn,ytrn,xtst,ytst))
init0 = np.array(init0)</pre>
<p class="list" id="ch06list09"><em>Listing 6-9: Setting up the initialization experiment</em></p>
<p class="indent">The imports make the <code>Classifier</code> class available, along with <code>RE</code> and two statistical tests: the t-test and the <em>Mann-Whitney U</em> test, a nonparametric version of the t-test. <em>Nonparametric tests</em> make no assumptions about the data and are harder to satisfy. I often consider both together to account for the possibility that the t-test results are invalid because the data isn’t normally distributed—a fundamental assumption of the t-test.</p>
<p class="indent">Next, we load the digits dataset, first the training (<code>xtrn</code>) followed by the test (<code>xtst</code>). We divide the data by 256 to map it from [0, 255] to [0, 1).</p>
<p class="indent">The following code paragraph accumulates the output of <code>Run</code> for initialization scheme 0. The return value is the overall accuracy of a model using scikit-learn’s Glorot initialization. Similar code captures the accuracy for the other four initialization schemes.</p>
<p class="indent">We then transform the results for all schemes into means and standard errors before printing, as in <a href="ch06.xhtml#ch06list010">Listing 6-10</a>.</p>
<pre class="pre">m0,s0 = init0.mean(), init0.std(ddof=1)/np.sqrt(N)
m1,s1 = init1.mean(), init1.std(ddof=1)/np.sqrt(N)
m2,s2 = init2.mean(), init2.std(ddof=1)/np.sqrt(N)
m3,s3 = init3.mean(), init3.std(ddof=1)/np.sqrt(N)</pre>
<p class="list" id="ch06list010"><em>Listing 6-10: Reporting the results</em></p>
<p class="indent">Finally, we run the statistical tests to compare one initialization scheme against another. Theoretically, we might expect initialization scheme 3 (He) to be the best performing, so we compare it (<code>init3</code>) with each of the others and report the corresponding p-values (the “u” is the p-value for the Mann-Whitney U test); see <a href="ch06.xhtml#ch06list011">Listing 6-11</a>.</p>
<pre class="pre">_,p = ttest_ind(init3,init0)
_,u = mannwhitneyu(init3,init0)
print("init3 vs init0: p=%0.8f, u=%0.8f" % (p,u))
_,p = ttest_ind(init3,init2)
_,u = mannwhitneyu(init3,init2)
print("init3 vs init2: p=%0.8f, u=%0.8f" % (p,u))
_,p = ttest_ind(init3,init1)
_,u = mannwhitneyu(init3,init1)
print("init3 vs init1: p=%0.8f, u=%0.8f" % (p,u))</pre>
<p class="list" id="ch06list011"><em>Listing 6-11: Running the statistical tests</em></p>
<p class="indent">The statistical tests, <code>ttest_ind</code> and <code>mannwhitneyu</code>, first return their respective test statistics and then their associated p-value. The p-value tells us the probability that we’ll measure the difference in the means between the two sets of accuracies, given the two sets are from the same distribution. The <span epub:type="pagebreak" id="page_187"/>smaller the p-value, the less likely the two sets are from the same distribution, thereby giving us confidence that the difference observed is real.</p>
<h5 class="h5"><strong>Training the Network</strong></h5>
<p class="noindent">The code to configure and train a neural network is in the <code>Run</code> function, as in <a href="ch06.xhtml#ch06list012">Listing 6-12</a>.</p>
<pre class="pre">def Run(init_scheme, xtrn,ytrn,xtst,ytst):
    clf = Classifier(hidden_layer_sizes=(100,50), max_iter=4000)
    clf.init_scheme = init_scheme
    clf.rng = RE()
    clf.fit(xtrn,ytrn)
    pred = clf.predict(xtst)
    _,acc = Confusion(pred,ytst)
    return acc</pre>
<p class="list" id="ch06list012"><em>Listing 6-12: Training a neural network</em></p>
<p class="indent">Here’s where we begin to appreciate the power of scikit-learn. The arguments to <code>Run</code> include which initialization scheme to use, [0, 3], followed by the train and test datasets. First, we create the neural network by creating an instance of <code>Classifier</code>, our subclass of scikit-learn’s <code>MLPClassifier</code>. By default, the neural network uses a ReLU activation function, which is what we want. It also trains until training no longer improves things or <code>max_iter</code> passes through the entire training set. Each pass through is known as an <em>epoch</em>.</p>
<p class="indent">The <code>hidden_layer_sizes</code> keyword defines the architecture of the model. We know the input has 784 elements and the output 4 because there are four classes, the digits 1, 4, 7, and 9. The layers between the input and output are hidden; we’re specifying two: the first with 100 nodes and the second with 50.</p>
<p class="indent">Before training, we need to set the initialization scheme (<code>init_scheme</code>) and define <code>rng</code>, an instance of <code>RE</code>, using the default of PCG64 and floating-point values in [0, 1).</p>
<p class="indent">Train the neural network with <code>clf.fit(xtrn,ytrn)</code>, in which the <code>fit</code> method accepts the input vectors (<code>xtrn</code>) and the associated labels (<code>ytrn</code>). When the method returns, the model is trained and has found values for all the weights and biases (<em><strong>θ</strong></em>).</p>
<h5 class="h5"><strong>Evaluating the Network</strong></h5>
<p class="noindent">When given the test data (<code>xtst</code>), <code>predict</code> generates a set of predicted class labels, <code>pred</code>—a vector of the same length as <code>ytst</code>, the known class labels. We compare the two to evaluate the model by building a <em>confusion matrix</em>. The rows of the confusion matrix represent the known, true class labels. The columns are the model’s assigned labels. The matrix elements are counts of the number of times each possible pairing of the true label and assigned <span epub:type="pagebreak" id="page_188"/>label happened. A perfect classifier will always assign correct labels, meaning the confusion matrix will be diagonal. Any counts off the main diagonal represent errors.</p>
<p class="indent">The <code>Confusion</code> function generates a confusion matrix, and overall accuracy, from a set of known and predicted labels; see <a href="ch06.xhtml#ch06list013">Listing 6-13</a>.</p>
<pre class="pre">def Confusion(y,p):
    cm = np.zeros((4,4), dtype="uint16")
    for i in range(len(p)):
        cm[y[i],p[i]] += 1
    acc = np.diag(cm).sum() / cm.sum()
    return cm, acc</pre>
<p class="list" id="ch06list013"><em>Listing 6-13: Creating a confusion matrix</em></p>
<p class="indent">The confusion matrix (<code>cm</code>) is a 4×4 matrix because there are four classes. The most important line updates <code>cm</code> by indexing first by the known class label (<code>y</code>) and then by the assigned class label (<code>p</code>). Adding one counts each time that particular pairing of the true class label and assigned label occurs.</p>
<p class="indent">The overall accuracy is either the number of times the known and assigned labels matched or the sum along the main diagonal divided by the sum of all matrix elements.</p>
<p class="indent">Running <em>init_test.py</em> takes a few minutes. My run produced:</p>
<pre class="pre">init0: 0.92667 +/- 0.00167
init1: 0.89958 +/- 0.00311
init2: 0.90083 +/- 0.00183
init3: 0.92500 +/- 0.00213

init3 vs init0: p=0.54429253, u=0.55049935
init3 vs init2: p=0.00000002, u=0.00004054
init3 vs init1: p=0.00000088, u=0.00006765</pre>
<p class="indent">The first block displays the mean accuracy over the 12 models for each initialization scheme (mean ± SE). Glorot and He initialization averaged 92.7 and 92.5 percent, respectively. The older uniform and normal initialization strategies achieved 90.0 and 90.1 percent, respectively. These are large differences, but are they statistically significant? For that, look at the second block of results.</p>
<p class="indent">Comparing He initialization (<code>init3</code>) with Glorot (<code>init0</code>) returns p-values of about 0.5 for both the t-test and Mann-Whitney U. Such p-values strongly indicate that there is no difference between either approach.</p>
<p class="indent">Now, look at the p-values comparing He initialization with the older methods. They are virtually zero, implying the difference in mean accuracy is very likely real—He and Glorot initialization both lead to significantly better performing models. Modern deep learning is vindicated. While there was never any doubt, it’s beneficial to confirm directly rather than take it purely on faith.</p>
<h3 class="h3" id="ch00lev1_43"><span epub:type="pagebreak" id="page_189"/><strong>Extreme Learning Machines</strong></h3>
<p class="noindent">An <em>extreme learning machine</em> is a simple, single, hidden-layer neural network. What distinguishes an extreme learning machine from a traditional neural network is the source of the weights and biases.</p>
<p class="indent">To map an input vector through the first hidden layer of a neural network involves a weight matrix, <em><strong>W</strong></em>, and a bias vector, <em><strong>b</strong></em></p>
<p class="center"><em><strong>z</strong></em> = <em>h</em>(<em><strong>Wx</strong></em> + <em><strong>b</strong></em>)</p>
<p class="noindent">where <em><strong>x</strong></em> and <em><strong>z</strong></em> are vectors, and <em>h</em> is the activation function that accepts a vector input and produces a vector output.</p>
<p class="indent">In a traditional neural network, <em><strong>W</strong></em> and <em><strong>b</strong></em> are learned in the training process. In an extreme learning machine, <em><strong>W</strong></em> and <em><strong>b</strong></em> are generated randomly, with no regard for the training data. Random matrices are (sometimes) capable of mapping inputs, like the digit images as vectors, to a new space where it’s easier to separate the classes.</p>
<p class="indent">If we use the equation to map all training data, the output of the hidden layer (<em><strong>z</strong></em>) becomes a matrix (<em><strong>Z</strong></em>) with as many rows as there are training samples and as many columns as there are nodes in the hidden layer. In other words, the random <em><strong>W</strong></em> matrix and <em><strong>b</strong></em> bias vector have produced a new version of the training data, <em><strong>Z</strong></em>.</p>
<p class="indent">The random weight matrix and bias vector link the input and the hidden layer. To finish building the neural network, we need a weight matrix between the hidden layer’s output and the model’s output; there is no bias vector in this case. Learning happens here, but we need to take a detour to see it.</p>
<p class="indent">The training dataset consists of input samples and associated class labels, [0, 3]. Many machine learning models don’t use integer class labels, but transform the labels into <em>one-hot vectors</em>. For example, if there are four classes, the one-hot vector has four elements, all 0 except for the element corresponding to the class label, which is 1. If the class label is 2, then the one-hot vector is {0, 0, 1, 0}. Likewise, if the class label is 0, the one-hot vector is {1, 0, 0, 0}. To finish building the extreme learning machine, we need the training set class labels in this form. I assume that the labels are so transformed for the remainder of this section.</p>
<p class="indent">As one-hot vectors, the class labels become a matrix, <em><strong>Y</strong></em>, with as many rows as there are training samples and as many columns as there are classes. A linear mapping from the output of the hidden layer, <em><strong>Z</strong></em>, to the known labels (as one-hot vectors), <em><strong>Y</strong></em>, uses a matrix, <em><strong>B</strong></em>:</p>
<p class="center"><em><strong>Y</strong></em> = <em><strong>BZ</strong></em></p>
<p class="noindent">We want to find <em><strong>B</strong></em> as the second weight matrix.</p>
<p class="indent">Because we know <em><strong>Z</strong></em> by pushing the training data through the hidden layer and <em><strong>Y</strong></em>, the known labels, we generate <em><strong>B</strong></em> via</p>
<p class="center"><em><strong>YZ</strong></em><sup>–1</sup> = <em><strong>B</strong></em></p>
<p class="noindent">where <em><strong>Z</strong></em><sup>–1</sup> is the inverse of the matrix <em><strong>Z</strong></em>. Multiplying by the inverse of a matrix is akin to multiplying by the reciprocal of a scalar value; they cancel each <span epub:type="pagebreak" id="page_190"/>other. To find the inverse of <em><strong>Z</strong></em>, we need the Moore-Penrose pseudoinverse, which NumPy provides in its linear algebra module, <code>linalg</code>.</p>
<p class="indent">We now have all we need to build the extreme learning machine:</p>
<ol>
<li class="noindent">Select a weight matrix, <em><strong>W</strong></em>, and bias vector, <em><strong>b</strong></em>, at random.</li>
<li class="noindent">Pass the training data through the first hidden layer, <em><strong>Z</strong></em> = <em>h</em>(<em><strong>WX</strong></em> + <em><strong>b</strong></em>), where <em><strong>X</strong></em> and <em><strong>Z</strong></em> are now matrices, one row for each training sample.</li>
<li class="noindent">Calculate the output weight matrix, <em><strong>B</strong></em> = <em><strong>YZ</strong></em><sup>–1</sup>, using the pseudoinverse of <em><strong>Z</strong></em>, the output of the hidden layer.</li>
<li class="noindent">Use <em><strong>W</strong></em>, <em><strong>b</strong></em>, and <em><strong>B</strong></em> as the weights and biases of the extreme learning machine.</li>
</ol>
<p class="indent">This is rather abstract. Let’s make it concrete in code.</p>
<h4 class="h4" id="ch00lev2_60"><em><strong>Implementation</strong></em></h4>
<p class="noindent">The file <em>elm.py</em> implements an extreme learning machine and applies it to the digits dataset. <a href="ch06.xhtml#ch06list014">Listing 6-14</a> shows the <code>train</code> function.</p>
<pre class="pre">def train(xtrn, ytrn, hidden=100):
    inp = xtrn.shape[1]
    m = xtrn.min()
    d = xtrn.max() - m
    w0 = d*rng.random(inp*hidden).reshape((inp,hidden)) + m
    b0 = d*rng.random(hidden) + m
    z = activation(np.dot(xtrn,w0) + b0)
    zinv = np.linalg.pinv(z)
    w1 = np.dot(zinv, ytrn)
    return (w0,b0,w1)</pre>
<p class="list" id="ch06list014"><em>Listing 6-14: Defining an extreme learning machine</em></p>
<p class="indent">The first line sets <code>inp</code> to the number of features in the training data, here 784. The next two lines define the smallest training feature value (<code>m</code>) and the difference between that and the largest (<code>d</code>).</p>
<p class="indent">The following two lines generate the random weight matrix and bias vector, <em><strong>W</strong></em>i (<code>w0</code>) and <em><strong>b</strong></em> (<code>b0</code>), by randomly sampling in the range of the training data, [<em>m</em>, <em>m</em> + <em>d</em>]. Again, <em><strong>W</strong></em> and <em><strong>b</strong></em> are completely random, but fixed once selected.</p>
<p class="indent">The final piece of the extreme learning machine is <em><strong>B</strong></em> (<code>w1</code>), which is found by passing the training data through the hidden layer</p>
<pre class="pre">z = activation(np.dot(xtrn,w0) + b0)</pre>
<p class="noindent">where <code>activation</code> is the selected activation function (<em>h</em>()).</p>
<p class="indent">Finally, to define <em><strong>B</strong></em>, we multiply <em><strong>YZ</strong></em><sup>–1</sup>:</p>
<pre class="pre">zinv = np.linalg.pinv(z)
w1 = np.dot(zinv, ytrn)</pre>
<p class="noindent"><span epub:type="pagebreak" id="page_191"/>The return value for the function is the defined and trained extreme learning machine: <code>(w0,b0,w1)</code>. Let’s take it for a test drive.</p>
<h4 class="h4" id="ch00lev2_61"><em><strong>Testing</strong></em></h4>
<p class="noindent">The file <em>elm.py</em>, from which we extracted <code>train</code>, trains an extreme learning machine to classify the digits dataset using a user-supplied number of nodes in the hidden layer. For example:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 elm.py 200 mt19937</span>
[[49  0  1  0]
 [ 1 42  0  9]
 [ 2  4 39  4]
 [ 1  5  6 37]]

accuracy = 0.835000</pre>
<p class="noindent">We use the Mersenne Twister pseudorandom generator to build a machine with 200 hidden layer nodes. The output shows the confusion matrix and the overall accuracy, 83.5 percent. The confusion matrix is almost diagonal, which is a good sign.</p>
<p class="indent">Let’s scrutinize the confusion matrix. The rows represent the true class labels from top to bottom, ones, fours, sevens, and nines. The columns, from left to right, are the model’s assigned class label (we’ll see how soon). Looking at the top row, of the 50 ones in the test dataset, the model called a one a one 49 times, but once called it a seven.</p>
<p class="indent">The model had the most difficulty with nines, the final row of the confusion matrix. The model got it right 37 out of 49 times, but it called it a seven 6 times and a four 5 times. Only once did the model confuse a nine for a one.</p>
<p class="indent">The <code>train</code> function creates the extreme learning machine. To use it, we need <code>predict</code>, as shown in <a href="ch06.xhtml#ch06list015">Listing 6-15</a>.</p>
<pre class="pre">def predict(xtst, model):
    w0,bias,w1 = model
    z = activation(np.dot(xtst,w0) + bias)
    return np.dot(z,w1)</pre>
<p class="list" id="ch06list015"><em>Listing 6-15: Prediction with an extreme learning machine</em></p>
<p class="indent">We pull the weight matrices and bias vector from the supplied model, and then pass the test data through the hidden layer and, finally, the output layer. The <code>activation</code> variable is set to the specific activation function currently in use—by default, the ReLU. We’ll experiment with different activation functions in the next section.</p>
<p class="indent">The output of <code>predict</code> is a two-dimensional matrix with as many rows as are in <code>xtst</code> (200) and as many columns as there are classes (4). For example, the output of predict for the first test sample (<code>xtst[0]</code>) is</p>
<p class="center">0.19551782, 0.90971894, 0.05398019, –0.06542743</p>
<p class="noindent"><span epub:type="pagebreak" id="page_192"/>and the first known test label, as a one-hot vector, is:</p>
<p class="center">0, 1, 0, 0</p>
<p class="noindent">Notice the largest value in the model’s output vector is at index 1, as is the largest for the one-hot label. In other words, the first test sample belongs to class 1 (four), and the model successfully predicted class 1 as the most likely class. The last output value is negative: the model is not outputting a probability, but a decision function value where the largest is the most likely class label, even if we don’t have a true probability associated with that value.</p>
<p class="indent">Therefore, to construct a confusion matrix, we will need code like <a href="ch06.xhtml#ch06list016">Listing 6-16</a>.</p>
<pre class="pre">def confusion(prob,ytst):
    nc = ytst.shape[1]
    cm = np.zeros((nc,nc), dtype="uint16")
    for i in range(len(prob)):
        n = np.argmax(ytst[i])
        m = np.argmax(prob[i])
        cm[n,m] += 1
    acc = np.diag(cm).sum() / cm.sum()
    return cm,acc</pre>
<p class="list" id="ch06list016"><em>Listing 6-16: Building a confusion matrix for an extreme learning machine</em></p>
<p class="indent">The <code>confusion</code> function returns the confusion matrix and overall accuracy, but instead of directly using the values in <code>ytst</code> and <code>prob</code>, we apply NumPy’s <code>argmax</code> function to return the index of the largest value in the four-element vectors.</p>
<p class="indent"><a href="ch06.xhtml#ch06list017">Listing 6-17</a> shows the remainder of <em>elm.py</em>, which loads the digit data-sets, scaling them by 256, and then trains and tests the extreme learning machine with three lines of code.</p>
<pre class="pre">model = train(xtrn, ytrn, nodes)
prob = predict(xtst,model)
cm,acc = confusion(prob,ytst)</pre>
<p class="list" id="ch06list017"><em>Listing 6-17: Training and testing an extreme learning machine</em></p>
<p class="noindent">The <code>nodes</code> parameter is the number of nodes in the hidden layer as read from the command line.</p>
<p class="indent">How sensitive is the extreme learning machine to the number of nodes in the hidden layer? That’s a good question.</p>
<p class="indent">By default, the code in <em>elm.py</em> uses a ReLU activation function. However, the file defines several other activation functions. In this section, we’ll explore each in combination with differing numbers of hidden layer nodes to find whether there’s a best combination.</p>
<p class="indent">The ReLU activation function uses NumPy’s <code>maximize</code> function, which returns the largest of the two arguments, element by element:<span epub:type="pagebreak" id="page_193"/></p>
<pre class="pre">def relu(x):
    return np.maximum(x,0)</pre>
<p class="indent">We’re not constrained to use only the ReLU. Classically, neural networks made heavy use of the sigmoid and hyperbolic tangent functions:</p>
<pre class="pre">def sigmoid(x):
    return 1 / (1 + np.exp(-0.01*x))
def tanh(x):
    return np.tanh(0.01*x)</pre>
<p class="noindent">Both of these functions return S-shaped curves. I added a factor of 0.01 to scale <em>x</em> in the sigmoid and hyperbolic tangent. This is not usually done, but it was necessary here to prevent overflow errors.</p>
<p class="indent">For fun, I defined several other activation functions:</p>
<pre class="pre">def cube(x):
    return x**3
def absolute(x):
    return np.abs(x)
def recip(x):
    return 1/x
def identity(x):
    return x</pre>
<p class="indent">Let’s test the activation functions as the number of nodes in the hidden layer is varied from 10 to 400. The code is in <em>elm_test.py</em>. It makes use of the <code>train</code>, <code>predict</code>, and <code>confusion</code> functions. The main loop looks like <a href="ch06.xhtml#ch06list018">Listing 6-18</a>.</p>
<pre class="pre">acts = [relu, sigmoid, tanh, cube, absolute, recip, identity]
nodes = [10,20,30,40,50,100,150,200,250,300,350,400]
N = 50
acc = np.zeros((len(acts),len(nodes),N))

for i,act in enumerate(acts):
    for j,n in enumerate(nodes):
        for k in range(N):
            activation = act
            model = train(xtrn, ytrn, n)
            prob = predict(xtst, model)
            _,a = confusion(prob, ytst)
            acc[i,j,k] = a

np.save("elm_test_results.npy", acc)</pre>
<p class="list" id="ch06list018"><em>Listing 6-18: Testing activation functions and hidden layer sizes</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_194"/>Fifty extreme learning machines are trained for each combination of activation function and hidden layer size, tracking the overall accuracy (<code>acc</code>). Notice the assignment of <code>act</code> to <code>activation</code>. Functions may be freely assigned to variables in Python and then used when the variable is referenced (as in <code>train</code>).</p>
<p class="indent">Run <em>elm_test.py</em> by passing it a randomness source (I used MT19937). When it finishes, run <em>elm_test_results.py</em> to parse the output and generate a plot like that of <a href="ch06.xhtml#ch06fig03">Figure 6-3</a> showing the mean accuracy by hidden layer size and activation function. Error bars are present but small.</p>
<div class="image"><img alt="Image" id="ch06fig03" src="../images/06fig03.jpg"/></div>
<p class="figcap"><em>Figure 6-3: Extreme learning machine performance as a function of the hidden layer size and activation function</em></p>
<p class="indent"><a href="ch06.xhtml#ch06fig03">Figure 6-3</a>’s most obvious statement is that <em>y</em> = <em>x</em><sup>3</sup> is a lousy activation function, as it always results in inferior models compared to the others. Another interesting observation is that the activation functions follow the same shape: a rapid increase in model accuracy as the number of nodes in the hidden layer increases, followed by a maximum and a slower decline. The difference between activation functions is slight, especially near the maximum of 100 hidden nodes; however, the hyperbolic tangent came out on top in this run. In fact, <code>tanh</code> wins for most runs, so it’s fair to say that for this particular dataset, an extreme learning machine using <code>tanh</code> and 100 nodes in the hidden layer is the way to go.</p>
<p class="indent">The identity activation function, <em>f</em>(<em><strong>x</strong></em>) = <em><strong>x</strong></em>, eschews nonlinearity; all the performance comes from the linear top layer mapping the hidden layer output to the predictions per class.</p>
<h4 class="h4" id="ch00lev2_62"><em><strong>Reckless Swarm Optimizations</strong></em></h4>
<p class="noindent">An extreme learning machine’s attraction is the incredible speed with which a model is trained compared to all the calculations necessary to train a traditional neural network of the same size. Sure, the first model might not be that good, but trying a few times in a row and keeping the best-performing <span epub:type="pagebreak" id="page_195"/>model seems a reasonable thing to do. I can imagine a scenario where an autonomous system might want to train a model quickly in response to rapidly changing input data.</p>
<p class="indent">However, the random part of the extreme learning machine—that is, the weight matrix and bias vector from the input to the hidden layer—made me wonder: Might we be able to learn the weight matrix and bias vector via a swarm optimization instead? Would this work, and if so, might it be any better than the random versions? My thought is clearly missing the point that extreme learning machines are meant to use random weights and biases, but I want to know if swarm optimization might prove helpful, even if vastly more computationally intensive than traditional neural network training.</p>
<p class="indent">The true challenge is the dimensionality of the problem. Our inputs are 784-element vectors. We learned that 100 nodes in the hidden layer seems a good thing to have, so the total dimensionality of the weight matrix and bias vector is:</p>
<p class="center">784 × 100 + 100 = 78,500</p>
<p class="indent">We’ll be asking the swarms to search a space of 78,500 dimensions to come up with a good position, one that leads to a model with the highest accuracy. That’s a tall order.</p>
<p class="indent">The code I experimented with is in <em>elm_swarm.py</em>. I won’t walk through it, but you’ll see it follows similar optimization code from earlier chapters. The objective function uses each particle position as a weight matrix and bias vector, and then learns the output weight matrix and evaluates the model on the test set to produce an overall accuracy. Therefore, each call to the <code>Evaluate</code> method results in a trained and tested model.</p>
<p class="indent">To run the code, use a command line like this:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 elm_swarm.py 100 tanh 20 60 de de.pkl</span></pre>
<p class="noindent">Here, differential evolution (<code>de</code>) searches for a model with <code>tanh</code> activation functions and 100 hidden layer nodes. The swarm has 20 particles and runs for 60 iterations before reporting the final confusion matrix and accuracy. The best model is stored in the file <em>de.pkl</em>. The current best accuracy is shown on each iteration, so you can watch the swarm learn. Run <em>elm_swarm.py</em> without arguments to see all options.</p>
<p class="indent">For example, the previous command produced the following output:</p>
<pre class="pre">  0: 0.90000 (mean swarm distance 111.844796164)
  1: 0.90000 (mean swarm distance 111.253958636)
  2: 0.90000 (mean swarm distance 110.932668482)
  3: 0.90000 (mean swarm distance 110.743740374)
  4: 0.90000 (mean swarm distance 110.701071153)
--<span class="codeitalic1">snip</span>--
 57: 0.93000 (mean swarm distance 106.803938775)
 58: 0.93000 (mean swarm distance 106.803938775)
 59: 0.93000 (mean swarm distance 106.803938775)
[[50  0  0  0]
 [ 0 52  0  0]<span epub:type="pagebreak" id="page_196"/>
 [ 1  2 41  5]
 [ 1  3  2 43]]
final accuracy = 0.930000 (DE-tanh-100, 20:60, 1220 models 
  evaluated, 4 best updates)</pre>
<p class="indent">The per-iteration best accuracy is shown along with the mean distance between the particles in the swarm. If the swarm is converging, this distance shrinks during the search, as it does here. The distance between two swarm particles uses the formula to find the distance between two points in two-dimensional or three-dimensional space</p>
<div class="image1"><img alt="Image" src="../images/f0196-01.jpg"/></div>
<p class="noindent">but extended to 78,500 dimensions. The result is still a scalar.</p>
<p class="indent">After 60 iterations, the swarm located a weight and bias vector leading to a 93 percent overall accuracy on the held-out test set. There were four swarm best updates. A second run, using GWO for 600 iterations, found a model with 94 percent overall accuracy. In that run, the swarm collapsed from an initial inter-particle distance of around 90 to less than 1 by the time it reached iteration 600. There were eight swarm best updates.</p>
<p class="indent">I conducted runs of <em>elm_swarm.py</em> for all algorithms, five runs for each. <a href="ch06.xhtml#ch06tab01">Table 6-1</a> displays the resulting average accuracies.</p>
<p class="tabcap" id="ch06tab01"><strong>Table 6-1:</strong> Average Model Accuracies for Each Optimization Algorithm</p>
<table class="table-h">
<colgroup>
<col style="width:50%"/>
<col style="width:50%"/>
</colgroup>
<thead>
<tr>
<th class="tab_th">Algorithm</th>
<th class="tab_th">Accuracy (mean ± SE)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="bg1">GWO</td>
<td class="bg1">0.9260 ± 0.0087</td>
</tr>
<tr>
<td class="bg">Differential evolution</td>
<td class="bg">0.9200 ± 0.0016</td>
</tr>
<tr>
<td class="bg1">Bare-bones PSO</td>
<td class="bg1">0.9190 ± 0.0019</td>
</tr>
<tr>
<td class="bg">Jaya</td>
<td class="bg">0.9180 ± 0.0034</td>
</tr>
<tr>
<td class="bg1">GA</td>
<td class="bg1">0.9170 ± 0.0058</td>
</tr>
<tr>
<td class="bg">RO</td>
<td class="bg">0.9170 ± 0.0025</td>
</tr>
<tr>
<td class="bg1">PSO</td>
<td class="bg1">0.9160 ± 0.0024</td>
</tr>
</tbody>
</table>
<p class="indent">The results are not statistically significantly different, but the ranking from best to worst is typical (except PSO). The GWO standard error of the mean is larger because one search found a model with an accuracy of 95.5 percent, the highest I encountered.</p>
<p class="indent">How many runs of <em>elm.py</em> do we need, on average, to find a model that meets or exceeds a given accuracy? The file <em>elm_brute.py</em> generates extreme learning machine after extreme learning machine, for up to a given maximum number of iterations, attempting to find a model that meets or exceeds the specified test set accuracy.</p>
<p class="indent">Structurally, <em>elm_brute.py</em> is a tweak to <em>elm.py</em> to do the model creation and testing in a loop and then report the performance if successful or note that it wasn’t able to meet the accuracy threshold. Running <em>elm_brute.py</em> for <span epub:type="pagebreak" id="page_197"/>different thresholds, 10 runs for each with a maximum of 2,000 iterations, produced <a href="ch06.xhtml#ch06tab02">Table 6-2</a>.</p>
<p class="tabcap" id="ch06tab02"><strong>Table 6-2:</strong> Number of Models Tried to Achieve a Given Accuracy</p>
<table class="table-h">
<colgroup>
<col style="width:32%"/>
<col style="width:17%"/>
<col style="width:17%"/>
<col style="width:17%"/>
<col style="width:17%"/>
</colgroup>
<thead>
<tr>
<th class="tab_th"><strong>Target accuracy</strong></th>
<th class="tab_th"><strong>Mean</strong></th>
<th class="tab_th"><strong>Min</strong></th>
<th class="tab_th"><strong>Max</strong></th>
<th class="tab_th"><strong>Successes</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="bg1">0.70</td>
<td class="bg1">1</td>
<td class="bg1">1</td>
<td class="bg1">1</td>
<td class="bg1">10</td>
</tr>
<tr>
<td class="bg">0.75</td>
<td class="bg">1</td>
<td class="bg">1</td>
<td class="bg">1</td>
<td class="bg">10</td>
</tr>
<tr>
<td class="bg1">0.80</td>
<td class="bg1">1</td>
<td class="bg1">1</td>
<td class="bg1">1</td>
<td class="bg1">10</td>
</tr>
<tr>
<td class="bg">0.85</td>
<td class="bg">2.2</td>
<td class="bg">1</td>
<td class="bg">6</td>
<td class="bg">10</td>
</tr>
<tr>
<td class="bg1">0.90</td>
<td class="bg1">147.2</td>
<td class="bg1">15</td>
<td class="bg1">270</td>
<td class="bg1">10</td>
</tr>
<tr>
<td class="bg">0.91</td>
<td class="bg">504.2</td>
<td class="bg">87</td>
<td class="bg">1,174</td>
<td class="bg">9</td>
</tr>
<tr>
<td class="bg1">0.915</td>
<td class="bg1">858.4</td>
<td class="bg1">69</td>
<td class="bg1">1,710</td>
<td class="bg1">9</td>
</tr>
<tr>
<td class="bg">0.92</td>
<td class="bg">788.3</td>
<td class="bg">74</td>
<td class="bg">1,325</td>
<td class="bg">4</td>
</tr>
</tbody>
</table>
<p class="indent">The first column shows the target accuracy. Any model meeting or exceeding this accuracy is considered a success. The mean number of models needed to find one at or above the threshold comes next, followed by the minimum and maximum numbers. Finally, the number of successful runs at that threshold, out of 10, is shown.</p>
<p class="indent">Targets at or below 85 percent are easy to find, averaging a little more than two searches. At the 90 percent threshold, however, there is a sudden jump requiring the creation of about 150 models on average. The minimum of 15 and the maximum of 270 implies a long tail to the distribution of the number of models tested.</p>
<p class="indent">Above 90 percent, the mean number of models increases again, rather dramatically, including long tails of up to 1,710 for 91.5 percent. The number of successful searches also goes down, implying that 2,000 iterations were an insufficient maximum in most cases to find a model with 92 percent or greater accuracy.</p>
<p class="indent">We now have two different approaches. The first blindly tries to find a suitable model by randomly assigning weights and biases and then testing over and over—a brute force approach. The second uses a principled swarm search to locate the weights and biases, and is more successful. For example, the previous swarm approach used 1,220 candidate models to find one with an accuracy of 93 percent, and a run of <em>elm_brute.py</em> needed 21,680 candidates to find a model with an accuracy of 93.5 percent.</p>
<p class="indent">It’s impressive that the swarm techniques can find good models while searching such a high-dimensional space. The approach isn’t practical without seriously reengineering the code to be orders of magnitude faster, but that we achieved any level of success is fascinating.</p>
<p class="indent">Extreme learning machines are a prime example of randomness in action. Their structure invites experimentation, so please experiment. If you discover something interesting, let me know. In the meantime, let’s move on to our last example of randomness in machine learning.</p>
<h3 class="h3" id="ch00lev1_44"><span epub:type="pagebreak" id="page_198"/><strong>Random Forests</strong></h3>
<p class="noindent">A <em>random forest</em> is a collection (or ensemble) of <em>decision trees</em>. We’ll define these terms shortly. The ideas behind random forests were developed in the 1990s and brought together by Breiman in his appropriately named 2001 paper, “Random Forests.” Hence they have some history behind them. Decision trees themselves are even older, dating from the early 1960s. Let’s begin there.</p>
<h4 class="h4" id="ch00lev2_63"><em><strong>Decision Trees</strong></em></h4>
<p class="noindent">A decision tree is a machine learning model consisting of a series of yes or no questions asked not of a person, but of a feature vector. The sequence of answers for that feature vector moves through the tree from the root node to a leaf, a terminal node. We then assign the class label associated with the leaf to the input feature vector.</p>
<p class="indent">Decision trees have the benefit of interpretability—by their very operation, they explain themselves. Neural networks can’t easily explain themselves, an issue that’s given birth to <em>Explainable AI (XAI)</em>, a subfield of modern deep learning.</p>
<p class="indent">Decision trees are best understood with an example, for which we’ll use a tiny dataset of two measurements of three species of iris flowers. The dataset is two-dimensional; there are two features with 150 samples total. We’ll use the first 100 for training the decision tree and the remaining 50 for testing. As the dataset has only two dimensions, we can plot its features by class; see <a href="ch06.xhtml#ch06fig04">Figure 6-4</a>.</p>
<div class="image"><img alt="Image" id="ch06fig04" src="../images/06fig04.jpg"/></div>
<p class="figcap"><em>Figure 6-4: The iris features</em></p>
<p class="indent">In <a href="ch06.xhtml#ch06fig04">Figure 6-4</a>, circles represent class 0, squares class 1, and diamonds class 2. Class 0 is well separated from the other two classes, which overlap considerably. Therefore, we might expect the decision tree classifier to do well with class 0 and be most often confused by class 1 and class 2.</p>
<p class="indent"><span epub:type="pagebreak" id="page_199"/>Let’s build a decision tree for this dataset. The code I’m using, which also generates <a href="ch06.xhtml#ch06fig04">Figure 6-4</a>, is in the file <em>iris_tree.py</em>. It uses scikit-learn’s <code>DecisionTreeClassifier</code> class and limits the depth of the tree to three. As with most scikit-learn classes, the <code>fit</code> method uses the training data and the <code>predict</code> method uses the test data. The output of <em>iris_tree.py</em> is:</p>
<pre class="pre">[[18 0 0]
 [ 1 4 11]
 [ 0 1 15]]
0.74</pre>
<p class="noindent">This is a confusion matrix and has an overall accuracy of 74 percent. Class 0 was perfectly classified, 18 out of 18, while the decision tree labeled most of class 1 as class 2.</p>
<p class="indent"><a href="ch06.xhtml#ch06fig05">Figure 6-5</a> shows what the tree looks like.</p>
<div class="image"><img alt="Image" id="ch06fig05" src="../images/06fig05.jpg"/></div>
<p class="figcap"><em>Figure 6-5: The iris decision tree</em></p>
<p class="indent">The tree’s root is at the top. Each box is a node, and the first line of each box contains a question—in this case, “Is <em>x</em><sub>0</sub> ≤ 5.45?” or is feature 0 less than or equal to 5.45? If the answer is yes, move to the left; otherwise, move to the right. Then consider the question in that node. Continue this process until you reach a <em>leaf</em>, or a node with no children.</p>
<p class="indent">The value part of a node indicates the number of training samples of each class present at that node. For example, the leftmost leaf node has <code>value=[1,0,0]</code>, meaning only one member of class 0 is present at this node. Therefore, any path leading to this node assigns class 0 to the input feature <span epub:type="pagebreak" id="page_200"/>vector. Likewise, the leaf immediately to the right is labeled <code>[0,5,0]</code>, so feature vectors leading to this node are assigned to class 1. Finally, the leaf second from the right is labeled <code>[1,18,24]</code>, meaning one class 0 training sample landed in this node, as did 24 samples of class 2. The majority rules when more than one class is represented, so this node assigns feature vectors to class 2.</p>
<p class="indent">There are two other lines in each node: samples and the Gini index. The former is the sum of the values vector, the number of training samples present at that node. The decision tree algorithm uses the Gini index to split nodes; we won’t go into details here, but you can check out the scikit-learn documentation to learn more.</p>
<p class="indent">Classically, decision trees are deterministic; the same dataset generates the same decision tree. Scikit-learn modifies this behavior somewhat, but by fixing the pseudorandom seed, which I do recklessly in <em>iris_tree.py</em>, we can generate a repeatable tree. We’ll assume going forward that decision trees are entirely deterministic.</p>
<p class="indent">Decision trees explain themselves. For example, an input feature vector of {5.6, 3.3} will traverse the path in <a href="ch06.xhtml#ch06fig06">Figure 6-6</a>.</p>
<div class="image"><img alt="Image" id="ch06fig06" src="../images/06fig06.jpg"/></div>
<p class="figcap"><em>Figure 6-6: A path through the decision tree</em></p>
<p class="noindent">This is an example of class 0 because feature 0 is between 5.45 and 5.75, and feature 1 is greater than 3.25.</p>
<p class="indent">The deterministic nature of decision trees led to the development of random forests. The decision tree for a particular dataset either works well or doesn’t; they tend to <em>overfit</em> the data, being too sensitive to the particulars of the training set and not sensitive enough to the general characteristics of <span epub:type="pagebreak" id="page_201"/>the type of data the model might encounter—at least not as sensitive as we’d like. In other words, they might do well on the training set and less well on everything else.</p>
<p class="indent">Let’s learn how we can curtail the decision tree’s penchant for overfitting by introducing randomness.</p>
<h4 class="h4" id="ch00lev2_64"><em><strong>Additional Randomness</strong></em></h4>
<p class="noindent">Three techniques turn a solitary decision tree into a forest of decision trees, or a random forest. The first is <em>bagging</em>, which generates many new datasets by sampling the original dataset with replacement (called <em>bootstrapping</em>). The second uses random subsets of the available features for each new, bootstrapped dataset, and the third is <em>ensembling</em>, the creation of multiple models that somehow vote or otherwise combine their output. Let’s review these techniques before putting them together to grow random forests.</p>
<h5 class="h5"><strong>Creating Datasets with Bagging</strong></h5>
<p class="noindent">I have a dataset with 100 samples, feature 0 from the iris dataset. The values are measurements from a parent distribution, a population from which this particular collection of values is a sample. The sample has a mean value of 5.85, which we can take as an <em>estimate</em> of the population mean, but not necessarily the actual value. Over what range of values do we expect, with 95 percent confidence, to find the actual population mean?</p>
<p class="indent">We have only one set of 100 values, but we’ll use bootstrapping to generate another set, taken randomly from the first and allowing for the possibility of selecting the same sample more than once. This is known as “sampling with replacement.” We now have two collections of samples, both plausibly from the parent population. We can repeat this process many times to generate many sets, each of which has a mean value. Then, with the collection of mean values, we use NumPy’s <code>quantile</code> function to estimate the 95 percent confidence range.</p>
<p class="indent">The file <em>bootstrap.py</em> implements this process. It loads the iris training set, keeping only feature 0, and then generates 10,000 bootstrap datasets, keeping the mean value of each. To get the 95 percent confidence interval, we need to know the 2.5 percent and 97.5 percent quantile values. A quantile provides the percentiles for a dataset. The 50th percentile is the median, the middle value once the data has been sorted. The 25th percentile means that 25 percent of the data is below this value, while the 75th percentile means that 75 percent of the data is under this value.</p>
<p class="indent">To get the 95 percent confidence range, we want the values where 95 percent of the data is between them, implying we need to exclude 5 percent of the data: the bottom and top 2.5 percents. For that, we need the 2.5th and 97.5th percentiles, both of which we find courtesy of NumPy. Let’s review the code, shown in <a href="ch06.xhtml#ch06list019">Listing 6-19</a>.</p>
<pre class="pre">import numpy as np
from RE import *<span epub:type="pagebreak" id="page_202"/>
def bootstrap(x):
    n = RE(mode="int", low=0, high=len(x)).random(len(x))
    return x[n]

x = np.load("iris_train_data.npy")[:,0]

means = [x.mean()]
for i in range(10000):
    y = bootstrap(x)
    means.append(y.mean())
means = np.array(means)

L = np.quantile(means, 0.025)
U = np.quantile(means, 0.975)

print("mean from single measurement %0.4f" % x.mean())
print("population mean 95%% confidence interval [%0.4f, %0.4f]" % (L,U))</pre>
<p class="list" id="ch06list019"><em>Listing 6-19: Bootstrapping confidence intervals</em></p>
<p class="indent">First, load the iris training data, keeping feature 0 (<code>x</code>). Then generate 10,000 bootstrap versions of <code>x</code> tracking the mean of each. Use <code>quantile</code> to find the lower (<code>L</code>) and upper (<code>U</code>) confidence interval bounds before reporting them.</p>
<p class="indent">The <code>bootstrap</code> function creates <code>n</code>, a vector of the same length as <code>x</code> where each value is an integer in the range 0 through <code>len(x)-1</code>. These are indices into <code>x</code> where it’s possible the same index appears more than once—a sample with replacement.</p>
<p class="indent">Each run of <em>bootstrap.py</em> produces a slightly different range:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 bootstrap.py</span>
mean from single measurement 5.8500
population mean 95% confidence interval [5.6940, 6.0090]</pre>
<p class="indent">This output indicates that the true population mean is, with 95 percent confidence, between 5.694 and 6.009. Without the bootstrap process, we couldn’t know this range from a single set of measurements. If we made assumptions about the shape of the distribution, that it’s normal or follows a t-distribution, then we could make an estimate; by bootstrapping, we don’t need to assume normally distributed data.</p>
<p class="indent">Bootstrapping is a helpful technique to get confidence intervals when building random forests because each bootstrapped dataset is a plausible collection of measurements. In that sense, the bootstrapped datasets are newly acquired datasets for training a model. Bagging is the process of using bootstrapped datasets to train multiple decision trees.</p>
<h5 class="h5"><span epub:type="pagebreak" id="page_203"/><strong>Combining Models with Ensembling</strong></h5>
<p class="noindent">Bagging helps because each decision tree is trained on a slightly different dataset; anything causing overfitting in one training set is hopefully compensated for in another.</p>
<p class="indent">We’ll make an ensemble of decision trees with bagging, in which we train multiple models using bootstrapped datasets and average their predictions to see if it improves results over ordinary decision trees. We’ll use the histology dataset from the beginning of this chapter. The file <em>bagging.py</em> trains a user-supplied number of decision trees, each with a different bootstrapped version of the histology dataset. We’ll then apply each model to the histology test data and average the resulting predictions to produce an ensemble output. Averaging model output is one approach to ensembling; we’ll use another, voting, later in this chapter.</p>
<p class="indent">Consider the relevant code in <a href="ch06.xhtml#ch06list020">Listing 6-20</a>.</p>
<pre class="pre">def Bootstrap(xtrn, ytrn):
    n = RE(mode="int", low=0, high=len(xtrn)).random(len(xtrn))
    return xtrn[n], ytrn[n]

xtrn = np.load("../data/datasets/bc_train_data.npy")
ytrn = np.load("../data/datasets/bc_train_labels.npy")
xtst = np.load("../data/datasets/bc_test_data.npy")
ytst = np.load("../data/datasets/bc_test_labels.npy")

trees = []
for i in range(N):
    tr = DecisionTreeClassifier()
    if (bag):
        x,y = Bootstrap(xtrn,ytrn)
        tr.fit(x,y)
    else:
        tr.fit(xtrn,ytrn)
    trees.append(tr)

preds = []
for i in range(N):
    preds.append(trees[i].predict(xtst))
preds = np.array(preds)
pred = np.floor(preds.mean(axis=0) + 0.5).astype("uint8")
cm, acc = Confusion(pred, ytst)</pre>
<p class="list" id="ch06list020"><em>Listing 6-20: Using bagging to build an ensemble of decision trees</em></p>
<p class="indent">The code defines a <code>Bootstrap</code> function, loads the histology train and test datasets, creates and trains multiple decision trees using bootstrapped training sets, and makes predictions on the test data before averaging the results to create a final, aggregate set of predictions.</p>
<p class="indent"><span epub:type="pagebreak" id="page_204"/>The first <code>for</code> loop creates a decision tree object (that is, an instance of <code>DecisionTreeClassifier</code>). If <code>bag</code> is true, it trains the tree using a bootstrapped version of the training data. If <code>bag</code> is false, it uses the entire dataset each time (no bagging). The <code>Bootstrap</code> function needs to select both the feature vectors and the proper label.</p>
<p class="indent">The next loop creates <code>preds</code>, a list of predictions for each decision tree. Each tree was trained on a different bootstrapped dataset, so if <code>bag</code> is true, the predictions will have slightly different errors. Turning <code>preds</code> into a NumPy array lets us average down the rows to get a single vector that’s the average of all tree outputs for each test sample (the columns). We want to assign a class label, 0 or 1, to the average, so we add 0.5 and then use <code>floor</code> to round to the nearest integer (0 or 1).</p>
<p class="indent">Finally, a call to <code>Confusion</code> builds the confusion matrix and overall accuracy, which later code (not shown) displays.</p>
<p class="indent">One run of <em>bagging.py</em>, using a collection of 60 trees and bagging, returned:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 bagging.py 60 1</span>
Bagging with 60 decision trees:
[[101   3]
 [  9  58]]
overall accuracy 0.9298
first six accuracies: 0.9357 0.9064 0.9357 0.8830 0.9123 0.9123</pre>
<p class="noindent">The overall ensemble accuracy was 92.98 percent. The accuracies of the first six decision trees are also shown. Your runs will produce different output.</p>
<p class="indent">Running the code again with no bagging returns:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 bagging.py 60 0</span>
Bagging with 60 decision trees:
[[99  5]
 [13 54]]
overall accuracy 0.8947
first six accuracies: 0.9006 0.9006 0.8947 0.8947 0.8713 0.8947</pre>
<p class="noindent">This is a significantly worse outcome.</p>
<p class="indent">I ran <em>bagging.py</em> 30 times, first with bagging and again without. The respective mean accuracies were 92.73 percent and 89.77 percent, with a p-value of less than 10<sup>–10</sup> using the Mann-Whitney U test. Bagging has a dramatic effect on the quality of the models.</p>
<p class="indent">Bootstrapped training sets and ensembling make up two-thirds of a random forest. Now let’s add the remaining third: random feature sets.</p>
<h5 class="h5"><strong>Using Random Feature Sets</strong></h5>
<p class="noindent">The final ingredient in a random forest is to use random subsets of the available features. Traditionally, the number of features we’ll use is the square root of the available features. For example, the histology dataset has 30 features, so we’ll use 5, selected randomly, whenever we want a bootstrapped dataset.</p>
<p class="indent"><span epub:type="pagebreak" id="page_205"/>The formula is as follows:</p>
<ol>
<li class="noindent">Select a bootstrapped dataset using five randomly selected features.</li>
<li class="noindent">Train a decision tree using this dataset. Keep it and the specific set of features selected (for testing).</li>
<li class="noindent">Repeat step 1 and step 2 for each tree in the forest.</li>
<li class="noindent">Apply each tree to the test data using only the features that the tree expects.</li>
<li class="noindent">Average the results across the test set to get the final predictions from the random forest.</li>
</ol>
<p class="indent">In code, adding random feature selection is only a minor tweak to the bagging example; take a look at <em>forest.py</em>. <a href="ch06.xhtml#ch06list021">Listing 6-21</a> shows the relevant changes from <em>bagging.py</em>.</p>
<pre class="pre">def Bootstrap(xtrn, ytrn):
    n = RE(mode="int", low=0, high=len(xtrn)).random(len(xtrn))
    nf = xtrn.shape[1]
    m = np.argsort(RE().random(nf))[:int(np.sqrt(nf))]
    return xtrn[n][:,m], ytrn[n], m

trees = []
for i in range(N):
    tr = DecisionTreeClassifier()
    x,y,m = Bootstrap(xtrn,ytrn)
    tr.fit(x,y)
    trees.append((tr,m))

preds = []
for i in range(N):
    tr,m = trees[i]
    preds.append(tr.predict(xtst[:,m]))
preds = np.array(preds)</pre>
<p class="list" id="ch06list021"><em>Listing 6-21: Implementing a random forest</em></p>
<p class="indent">First, we must modify <code>Bootstrap</code> to select not only a random sampling of the training set (<code>n</code>) but also a random set of features (<code>nf</code> leading to <code>m</code>). The particular features extracted must be returned so we can use them at test time.</p>
<p class="indent">The second paragraph trains the trees as before, but drags <code>m</code> along for the ride. Finally, at test time, we apply each tree to <code>xtst</code> after keeping only the proper subset of features. The output of <em>forest.py</em> is identical to that of <em>bagging.py</em>, minus the first six individual tree accuracies.</p>
<p class="indent">I ran the code 30 times using 60 trees, as before, to get the mean accuracies in <a href="ch06.xhtml#ch06tab03">Table 6-3</a>. I’ve included previous mean accuracies to show the improvement as each phase of the random forest is added.<span epub:type="pagebreak" id="page_206"/></p>
<p class="tabcap" id="ch06tab03"><strong>Table 6-3:</strong> Mean Accuracy by Model Type</p>
<table class="table-h">
<colgroup>
<col style="width:70%"/>
<col style="width:30%"/>
</colgroup>
<thead>
<tr>
<th class="tab_th"><strong>Option</strong></th>
<th class="tab_th"><strong>Mean accuracy</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="bg1">No bagging</td>
<td class="bg1">89.47%</td>
</tr>
<tr>
<td class="bg">Bagging, ensembling</td>
<td class="bg">92.98%</td>
</tr>
<tr>
<td class="bg1">Bagging, ensembling, random features</td>
<td class="bg1">95.25%</td>
</tr>
</tbody>
</table>
<p class="noindent">All random forest steps combined led to a significant improvement over a simple decision tree.</p>
<p class="indent">As you might expect, scikit-learn also supports random forests via the <code>RandomForestClassifier</code> class:</p>
<pre class="pre">from sklearn.ensemble import RandomForestClassifier</pre>
<p class="indent">The <code>RandomForestClassifier</code> class supports all three tricks plus additional tricks; see the scikit-learn documentation. Training 30 instances of <code>RandomForestClassifier</code> using 60 trees and all defaults results in a mean accuracy of 96.55 percent, which is even better than <em>forest.py</em>.</p>
<h4 class="h4" id="ch00lev2_65"><em><strong>Models Combined with Voting</strong></em></h4>
<p class="noindent">Before we leave this section, let’s investigate how the size of the forest affects performance. For this experiment, we’ll switch to the MNIST digits dataset. Also, instead of averaging each model’s output, we’ll vote to assign class labels.</p>
<p class="indent">The code we want is in <em>forest_mnist.py</em>. Built on <em>forest.py</em>, it loops over different sized forests to determine the mean accuracy for 20 models with that many trees. The output is a plot, but before we examine it, let’s review how to implement voting:</p>
<pre class="pre">preds = []
for i in range(N):
    tr,m = trees[i]
    preds.append(tr.predict(xtst[:,m]))
preds = np.array(preds)

pred = []
for i in range(preds.shape[1]):
    pred.append(np.argmax(np.bincount(preds[:,i])))
pred = np.array(pred)

cm, a = Confusion(pred, ytst)
acc.append(a)</pre>
<p class="indent">The first code paragraph captures the predictions for each tree in the forest. The second creates <code>pred</code>, a vector that holds the most commonly selected class label across each model for each test sample. To get the winner for test sample <code>i</code>, we first use <code>bincount</code> to count how often each label <span epub:type="pagebreak" id="page_207"/>appears for all models (the rows of <code>preds</code>) and then use <code>argmax</code> to return the index of the class label with the highest count. Ties go to the first occurrence of the highest value, that is, the lower index. Breaking ties this way might introduce a slight bias toward lower class labels, but we can live with that—consider it a systematic error as all forest sizes are likewise affected.</p>
<p class="indent"><a href="ch06.xhtml#ch06fig07">Figure 6-7</a> illustrates how the accuracy changes with forest size.</p>
<div class="image"><img alt="Image" id="ch06fig07" src="../images/06fig07.jpg"/></div>
<p class="figcap"><em>Figure 6-7: Mean accuracy on the digits dataset as a function of forest size</em></p>
<p class="indent">At first, increasing the number of trees helps, but eventually, saturation sets in, producing diminishing returns as the forest grows.</p>
<h3 class="h3" id="ch00lev1_45"><strong>Exercises</strong></h3>
<p class="noindent">Machine learning is a vast and critically important field. Here are some exercises related to the topics of this chapter to help you enhance your machine learning expertise and intuition:</p>
<ul>
<li class="noindent">We augmented the digits dataset rather conservatively, with small rotations and zooming. Other image processing options might help improve the performance of the models in this chapter. Experiment with them by adding other options to the <code>augment</code> function in <em>build_mnist_dataset.py</em>. The <code>Image</code> and <code>ImageFilter</code> classes in Python’s <code>PIL</code> module might be helpful. Use <code>Image.fromarray</code> to convert a NumPy array of <code>dtype</code> uint8 to a PIL image. To go the other way, pass the <code>Image</code> object to <code>np.array</code>.</li>
<li class="noindent">The <code>Classifier</code> subclass of <code>MLPClassifier</code>, used by <em>init_test.py</em>, defines multiple approaches to initializing a neural network. Add new ones to see how they affect results. What happens if all weight matrices are initially zero or some constant value? What about when the bias vectors are zero? Consider experimenting with the beta distribution <span epub:type="pagebreak" id="page_208"/>(<code>np.random.beta</code>), as adjusting its two parameters can generate samples with a wide range of shapes.</li>
<li class="noindent">The extreme learning machine uses a randomly generated weight matrix and bias vector to map the input to the first hidden layer. The selection method we used in this chapter is commonly found in the literature. What happens if you alter this method and select random values according to a nonuniform distribution? Consider <code>np.random.normal</code> (or the <code>RE</code>-based version) along with the beta distribution. Is there much of an effect?</li>
<li class="noindent">Create a two-layer extreme learning machine where <code>w0</code>, <code>w1</code>, <code>b0</code>, and <code>b1</code> are randomly selected. The final weight matrix, now <code>w2</code>, will be learned as before from the output of the second hidden layer. Compare the performance to the single-layer version.</li>
<li class="noindent">You’ll find files in the <em>datasets</em> directory beginning with <em>mnist_14x14</em>. They contain 14×14-pixel versions of all MNIST digits, [0, 9]. Try them in place of the four-digit version used throughout the chapter. How well do the various models perform?</li>
<li class="noindent">Modify <em>elm_brute.py</em> to track the number of all tested models over many runs for the same accuracy. Then use <code>np.histogram</code> and Matplotlib to plot histograms for a fixed accuracy (like 0.92). What shape do they have? Does the shape make sense to you?</li>
<li class="noindent">We largely ignored scikit-learn’s <code>RandomForestClassifier</code> in favor of our homegrown version. Explore scikit-learn’s approach in more detail by reading through the documentation page for the class and experimenting with the different options. Consider using both the histology and digits datasets.</li>
<li class="noindent">Run <em>rf_vs_mlp.py</em> followed by <em>rf_vs_mlp_results.py</em>. (Some patience is required.) Consider the output, which demonstrates how scaling the input feature vectors affects model performance. Which type of model is sensitive to the relative ranges of the features, the neural network or the random forest? Why might that be? Think about what a neural network is trying to do and compare that to the individual decision trees of the random forest. Why might one type of model care about the feature ranges while the other might not?</li>
</ul>
<h3 class="h3" id="ch00lev1_46"><strong>Summary</strong></h3>
<p class="noindent">This chapter explored the importance of randomness in machine learning when building datasets, both in terms of ordering the samples during training and in augmenting existing samples with plausible new ones to enlarge the type of data from which models learn.</p>
<p class="indent">Next, we considered the initialization of neural networks. We subclassed scikit-learn’s <code>MLPClassifier</code> to override its initialization method, allowing us to add alternate initialization approaches. We then examined their effect on model performance.</p>
<p class="indent"><span epub:type="pagebreak" id="page_209"/>Following this, we explored extreme learning machines, a subtype of neural networks that employs randomness as one of its essential components. We learned how these machines perform on our datasets, then considered the effect of hidden layer size and activation function. We concluded the section by replacing the random weight matrix and bias vector with ones learned by a swarm optimization exercise. We discovered that swarm algorithms could generate models with performance beyond what most extreme learning machines provide (for the same architecture).</p>
<p class="indent">Lastly, we experimented with random forests, a collection of decision trees. We learned what decision trees are and how to build a random forest from collections of decision trees employing bagging, ensemble voting, and random feature selection.</p>
<p class="indent">The next chapter takes a break from the practical to enhance our lives through generative art.<span epub:type="pagebreak" id="page_210"/></p>
</body></html>