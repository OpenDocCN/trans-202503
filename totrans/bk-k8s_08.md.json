["```\nroot@host01:~# k8s-all modprobe br_netfilter\n...\nroot@host01:~# k8s-all \"echo 'br_netfilter' > /etc/modules-load.d/k8s.conf\"\n```", "```\nroot@host01:~# k8s-all sysctl -w net.ipv4.ip_forward=1 \\\n  net.bridge.bridge-nf-call-ip6tables=1 \\\n  net.bridge.bridge-nf-call-iptables=1\n```", "```\nroot@host01:~# k8s-all apt install -y apt-transport-https \\\n  open-iscsi nfs-common\n```", "```\nroot@host01:~# k8s-all \"curl -fsSL \\\n  https://packages.cloud.google.com/apt/doc/apt-key.gpg | \\\n  gpg --dearmor -o /usr/share/keyrings/google-cloud-keyring.gpg\"\n```", "```\nroot@host01:~# k8s-all \"echo 'deb [arch=amd64' \\\n  'signed-by=/usr/share/keyrings/google-cloud-keyring.gpg]' \\\n  'https://apt.kubernetes.io/ kubernetes-xenial main' > \\\n  /etc/apt/sources.list.d/kubernetes.list\"\n```", "```\nroot@host01:~# k8s-all apt update\n...\n```", "```\nroot@host01:~# source /opt/k8sver\nroot@host01:~# k8s-all apt install -y kubelet=$K8SV kubeadm=$K8SV kubectl=$K8SV\n```", "```\nroot@host01:~# systemctl status kubelet\n  kubelet.service - kubelet: The Kubernetes Node Agent\n...\n   Main PID: 75368 (code=exited, status=1/FAILURE)\n```", "```\nroot@host01:~# k8s-all apt-mark hold kubelet kubeadm kubectl\n```", "```\n---\napiVersion: kubeadm.k8s.io/v1beta3\nkind: InitConfiguration\nbootstrapTokens:\n- groups:\n  - system:bootstrappers:kubeadm:default-node-token\n  token: 1d8fb1.2875d52d62a3282d\n  ttl: 2h0m0s\n  usages:\n  - signing\n  - authentication\nnodeRegistration:\n  kubeletExtraArgs:\n    node-ip: 192.168.61.11\n taints: []\nlocalAPIEndpoint:\n  advertiseAddress: 192.168.61.11\ncertificateKey: \"5a7e07816958efb97635e9a66256adb1\"\n---\napiVersion: kubeadm.k8s.io/v1beta3\nkind: ClusterConfiguration\nkubernetesVersion: 1.21.4\napiServer:\n  extraArgs:\n    service-node-port-range: 80-32767\nnetworking:\n  podSubnet: \"172.31.0.0/16\"\ncontrolPlaneEndpoint: \"192.168.61.10:6443\"\n---\napiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\nserverTLSBootstrap: true\n```", "```\nroot@host01:~# /usr/bin/kubeadm init \\\n  --config /etc/kubernetes/kubeadm-init.yaml --upload-certs\n```", "```\n---\napiVersion: kubeadm.k8s.io/v1beta3\nkind: JoinConfiguration\ndiscovery:\n  bootstrapToken:\n    apiServerEndpoint: 192.168.61.10:6443\n    token: 1d8fb1.2875d52d62a3282d\n    unsafeSkipCAVerification: true\n  timeout: 5m0s\nnodeRegistration:\n  kubeletExtraArgs:\n    cgroup-driver: containerd\n    node-ip: 192.168.61.12\n  taints: []\n  ignorePreflightErrors:\n    - DirAvailable--etc-kubernetes-manifests\ncontrolPlane:\n  localAPIEndpoint:\n    advertiseAddress: 192.168.61.12\n  certificateKey: \"5a7e07816958efb97635e9a66256adb1\"\n```", "```\nroot@host02:~# /usr/bin/kubeadm join --config /etc/kubernetes/kubeadm-join.yaml\n```", "```\nroot@host03:~# /usr/bin/kubeadm join --config /etc/kubernetes/kubeadm-join.yaml\n```", "```\n---\napiVersion: kubeadm.k8s.io/v1beta3\nkind: JoinConfiguration\ndiscovery:\n  bootstrapToken:\n    apiServerEndpoint: 192.168.61.10:6443\n    token: 1d8fb1.2875d52d62a3282d\n    unsafeSkipCAVerification: true\n  timeout: 5m0s\nnodeRegistration:\n  kubeletExtraArgs:\n    cgroup-driver: containerd\n    node-ip: 192.168.61.14\n  taints: []\n```", "```\nroot@host04:~# /usr/bin/kubeadm join --config /etc/kubernetes/kubeadm-join.yaml\n```", "```\nroot@host01:~# export KUBECONFIG=/etc/kubernetes/admin.conf\nroot@host01:~# kubectl get nodes\nNAME     STATUS     ROLES        ...\nhost01   NotReady   control-plane...\nhost02   NotReady   control-plane...\nhost03   NotReady   control-plane...\nhost04   NotReady   <none>       ...\n```", "```\nroot@host01:~# kubectl describe node host04\nName:               host04\n...\nConditions:\n  Type   Status ... Message\n  ----   ------ ... -------\n  Ready  False  ... container runtime network not ready...\n...\n```", "```\nroot@host01:~# kubectl get node -o json | \\\n  jq '.items[]|.metadata.name,.spec.taints[]'\n\"host01\"\n{\n  \"effect\": \"NoSchedule\",\n  \"key\": \"node.kubernetes.io/not-ready\"\n}\n\"host02\"\n{\n  \"effect\": \"NoSchedule\",\n  \"key\": \"node.kubernetes.io/not-ready\"\n}\n\"host03\"\n{\n  \"effect\": \"NoSchedule\",\n  \"key\": \"node.kubernetes.io/not-ready\"\n}\n\"host04\"\n{\n  \"effect\": \"NoSchedule\",\n  \"key\": \"node.kubernetes.io/not-ready\"\n}\n```", "```\nroot@host01:~# cd /etc/kubernetes/components\nroot@host01:/etc/kubernetes/components# curl -L -O $calico_url\n...\n```", "```\nroot@host01:/etc/kubernetes/components# kubectl apply -f tigera-operator.yaml\n...\nroot@host01:/etc/kubernetes/components# kubectl apply -f custom-resources.yaml\n```", "```\nroot@host01:/etc/kubernetes/components# kubectl get nodes\nNAME     STATUS   ROLES                ...\nhost01   Ready    control-plane,master ...\nhost02   Ready    control-plane,master ...\nhost03   Ready    control-plane,master ...\nhost04   Ready    <none>               ...\n```", "```\nroot@host01:/etc/kubernetes/components# kubectl -n calico-system \\\n  get daemonsets -o json | \\\n  jq '.items[].spec.template.spec.tolerations[]'\n{\n  \"key\": \"CriticalAddonsOnly\",\n  \"operator\": \"Exists\"\n}\n{\n  \"effect\": \"NoSchedule\",\n  \"operator\": \"Exists\"\n}\n{\n  \"effect\": \"NoExecute\",\n  \"operator\": \"Exists\"\n}\n```", "```\nroot@host01:/etc/kubernetes/components# k8s-all systemctl enable --now iscsid\n```", "```\nroot@host01:/etc/kubernetes/components# curl -LO $longhorn_url\n```", "```\nroot@host01:/etc/kubernetes/components# kubectl apply -f longhorn.yaml\n```", "```\nroot@host01:/etc/kubernetes/components# curl -Lo ingress-controller.yaml\n  $ingress_url\n```", "```\nroot@host01:/etc/kubernetes/components# kubectl apply -f ingress-controller.yaml\n```", "```\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: ingress-nginx-controller\n  namespace: ingress-nginx\nspec:\n  ports:\n    - port: 80\n      nodePort: 80\n    - port: 443\n      nodePort: 443\n```", "```\nroot@host01:/etc/kubernetes/components# kubectl patch -n ingress-nginx \\\n  service/ingress-nginx-controller --patch-file ingress-patch.yaml\nservice/ingress-nginx-controller patched\n```", "```\nroot@host01:/etc/kubernetes/components# kubectl annotate -n ingress-nginx \\\n  ingressclass/nginx ingressclass.kubernetes.io/is-default-class=\"true\"\ningressclass.networking.k8s.io/nginx annotated\n```", "```\nroot@host01:/etc/kubernetes/components# kubectl get csr\nNAME      ... SIGNERNAME                                  ... CONDITION\ncsr-sgrwz ... kubernetes.io/kubelet-serving               ... Pending\ncsr-agwb6 ... kubernetes.io/kube-apiserver-client-kubelet ... Approved,Issued\ncsr-2kwwk ... kubernetes.io/kubelet-serving               ... Pending\ncsr-5496d ... kubernetes.io/kube-apiserver-client-kubelet ... Approved,Issued\ncsr-hm6lj ... kubernetes.io/kube-apiserver-client-kubelet ... Approved,Issued\ncsr-jbfmx ... kubernetes.io/kubelet-serving               ... Pending\ncsr-njjr7 ... kubernetes.io/kube-apiserver-client-kubelet ... Approved,Issued\ncsr-v7tcs ... kubernetes.io/kubelet-serving               ... Pending\ncsr-vr27n ... kubernetes.io/kubelet-serving               ... Pending\n```", "```\nroot@host01:/etc/kubernetes/components# kubectl certificate approve \\$(kubectl\n  get csr --field-selector spec.signerName=kubernetes.io/kubelet-serving -o name)\ncertificatesigningrequest.certificates.k8s.io/csr-sgrwz approved\n...\n```", "```\nroot@host01:/etc/kubernetes/components# curl -Lo metrics-server.yaml \\$metrics_url\nroot@host01:/etc/kubernetes/components# kubectl apply -f metrics-server.yaml\n...\nroot@host01:/etc/kubernetes/components# cd\nroot@host01:~#\n```", "```\nroot@host01:~# crictl ps\nCONTAINER       ... STATE    NAME                       ...\n25c63f29c1442   ... Running  longhorn-csi-plugin        ...\n2ffdd044a81d8   ... Running  node-driver-registrar      ...\n94468050de89c   ... Running  csi-provisioner            ...\n119fbf417f1db   ... Running  csi-attacher               ...\ne74c1a2a0c422   ... Running  kube-scheduler             ...\nd1ad93cdbc686   ... Running  kube-controller-manager    ...\n76266a522cc3d   ... Running  engine-image-ei-611d1496   ...\nfc3cd1679e33e   ... Running  replica-manager            ...\n48e792a973105   ... Running  engine-manager             ...\ne658baebbc295   ... Running  longhorn-manager           ...\neb51d9ec0f2fc   ... Running  calico-kube-controllers    ...\n53e7e3e4a3148   ... Running  calico-node                ...\n772ac45ceb94e   ... Running  calico-typha               ...\n4005370021f5f   ... Running  kube-proxy                 ...\n26929cde3a264   ... Running  kube-apiserver             ...\n9ea4c2f5af794   ... Running  etcd                       ...\n```", "```\nroot@host01:~# kubectl get namespaces\nNAME              STATUS   AGE\ncalico-system     Active   50m\ndefault           Active   150m\nkube-node-lease   Active   150m\nkube-public       Active   150m\nkube-system       Active   150m\nlonghorn-system   Active   16m\ntigera-operator   Active   50m\n```", "```\nroot@host01:~# kubectl get pods\nNo resources found in default namespace.\n```", "```\nroot@host01:~# kubectl -n kube-system get pods\nNAME                             READY   STATUS    ...\ncoredns-558bd4d5db-7krwr         1/1     Running   ...\n...\nkube-apiserver-host01            1/1     Running   ...\n...\n```", "```\nroot@host01:~# kubectl -n kube-system describe pod kube-apiserver-host01\nName:                 kube-apiserver-host01\nNamespace:            kube-system\n...\nNode:                 host01/192.168.61.11\n...\nStatus:               Running\nContainers:\n  kube-apiserver:\n    Container ID:  containerd://26929cde3a264e...\n...\n```", "```\nroot@host01:~# kubectl -n calico-system get pods\nNAME                                       READY   STATUS    ...\ncalico-kube-controllers-7f58dbcbbd-ch7zt   1/1     Running   ...\ncalico-node-cp88k                          1/1     Running   ...\ncalico-node-dn4rj                          1/1     Running   ...\ncalico-node-xnkmg                          1/1     Running   ...\ncalico-node-zfscp                          1/1     Running   ...\ncalico-typha-68b99cd4bf-7lwss              1/1     Running   ...\ncalico-typha-68b99cd4bf-jjdts              1/1     Running   ...\ncalico-typha-68b99cd4bf-pjr6q              1/1     Running   ...\n```", "```\nroot@host01:~# kubectl -n longhorn-system get pods\nNAME                                       READY   STATUS    RESTARTS   AGE\nengine-image-ei-611d1496-8q58f             1/1     Running   0          31m\n...\nlonghorn-csi-plugin-8vkr6                  2/2     Running   0          31m\n...\nlonghorn-manager-dl9sb                     1/1     Running   1          32m\n...\n```", "```\nroot@host01:~# kubectl run nginx --image=nginx\npod/nginx created\n```", "```\nroot@host01:~# kubectl get pods -o wide\nNAME    READY   STATUS    ... IP               NODE  ...\nnginx   1/1     Running   ... 172.31.89.203   host02 ...\n```", "```\nroot@host01:~# ping -c 1 172.31.89.203\nPING 172.31.89.203 (172.31.89.203) 56(84) bytes of data.\n64 bytes from 172.31.89.203: icmp_seq=1 ttl=63 time=0.848 ms\n\n--- 172.31.89.203 ping statistics ---\n1 packets transmitted, 1 received, 0% packet loss, time 0ms\nrtt min/avg/max/mdev = 0.848/0.848/0.848/0.000 ms\n```", "```\nroot@host01:~# curl http://172.31.89.203\n...\n<title>Welcome to nginx!</title>\n...\n```"]