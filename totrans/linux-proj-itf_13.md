## 第十三章 文件 I/O 缓存

为了提高速度和效率，I/O 系统调用（即内核）和标准 C 库的 I/O 函数（即 *stdio* 函数）在操作磁盘文件时会缓存数据。在本章中，我们将描述这两种缓存方式，并考虑它们如何影响应用程序的性能。我们还将探讨如何影响和禁用这两种缓存的各种技术，并介绍一种称为直接 I/O 的技术，在某些情况下，它对绕过内核缓存非常有用。

## 内核文件 I/O 缓存：缓冲区缓存

在处理磁盘文件时，*read()* 和 *write()* 系统调用不会直接发起磁盘访问。相反，它们只是将数据从用户空间的缓冲区复制到内核的 *缓冲区缓存* 中。例如，下面的调用将 3 个字节的数据从用户空间内存中的缓冲区传输到内核空间中的缓冲区：

```
write(fd, "abc", 3);
```

在此时，*write()* 返回。稍后，内核将其缓冲区（刷新）写入磁盘。（因此，我们说系统调用与磁盘操作并不 *同步*。）如果在此期间，另一个进程尝试读取这些字节的文件，那么内核将自动从缓冲区缓存中提供数据，而不是从（过时的）文件内容中提供。

相应地，对于输入，内核从磁盘读取数据并将其存储在内核缓冲区中。对 *read()* 的调用从该缓冲区获取数据，直到缓冲区被耗尽，此时内核会将文件的下一部分读取到缓冲区缓存中。（这是简化的描述；对于顺序文件访问，内核通常会执行预读操作，试图确保在读取过程需要这些文件的下一块时，下一块已经被读取到缓冲区缓存中。我们将在第 13.5 节进一步讨论预读操作。）

该设计的目的是让 *read()* 和 *write()* 操作快速进行，因为它们不需要等待（慢速的）磁盘操作。这个设计也是高效的，因为它减少了内核必须执行的磁盘传输次数。

Linux 内核对缓冲区缓存的大小没有固定的上限。内核将根据需要分配足够的缓冲区缓存页面，仅受可用物理内存量以及其他用途（例如，运行中的进程所需的文本和数据页面）对物理内存的需求限制。如果可用内存紧张，内核将把一些已修改的缓冲区缓存页面刷新到磁盘，以便释放这些页面供再次使用。

### 注意

更准确地说，从内核 2.4 版本开始，Linux 不再维护一个独立的缓冲区缓存。相反，文件 I/O 缓冲区被包含在页面缓存中，例如，页面缓存还包含来自内存映射文件的页面。然而，在主文本中，我们使用术语 *缓冲区缓存*，因为这个术语在 UNIX 实现中历史悠久。

#### 缓冲区大小对 I/O 系统调用性能的影响

内核执行的磁盘访问次数是相同的，无论我们是进行 1000 次单字节写入，还是一次 1000 字节的写入。然而，后者更可取，因为它只需要一次系统调用，而前者需要 1000 次。尽管比磁盘操作快得多，系统调用仍然需要一定时间，因为内核必须捕获调用，检查系统调用参数的有效性，并在用户空间和内核空间之间传输数据（有关详细信息，请参见系统调用）。

使用不同缓冲区大小进行文件 I/O 操作的影响可以通过运行示例 4-1（在输入输出的普遍性）来观察，使用不同的`BUF_SIZE`值。（`BUF_SIZE`常量指定每次调用*read()*和*write()*时传输的字节数。）表 13-1 显示了该程序在 Linux *ext2*文件系统上使用不同`BUF_SIZE`值复制 100 百万字节文件所需的时间。请注意，关于此表格中的信息，有以下几点：

+   *经过的时间*和*总 CPU*时间列有显而易见的含义。*用户 CPU*和*系统 CPU*列将*总 CPU*时间分别细分为执行用户模式代码的时间和执行内核代码（即系统调用）的时间。

+   表格中显示的测试是在使用*ext2*文件系统、块大小为 4096 字节的原生 2.6.30 内核上进行的。

### 注意

当我们谈到*原生内核*时，我们指的是未打补丁的主线内核。这与大多数发行版提供的内核不同，后者通常会包含各种补丁以修复 bug 或添加功能。

+   每一行显示了给定缓冲区大小的 20 次运行的平均值。在这些测试中，和本章后面展示的其他测试一样，文件系统在每次程序执行之间都被卸载并重新挂载，以确保文件系统的缓冲区缓存为空。计时是使用 shell 的*time*命令完成的。

表 13-1：复制 100 百万字节文件所需的时间

| `BUF_SIZE` | 时间（秒） |
| --- | --- |
| 经过时间 | 总 CPU | 用户 CPU | 系统 CPU |
| --- | --- | --- | --- |
| `1` | `107.43` | `107.32` | `8.20` | `99.12` |
| `2` | `54.16` | `53.89` | `4.13` | `49.76` |
| `4` | `31.72` | `30.96` | `2.30` | `28.66` |
| `8` | `15.59` | `14.34` | `1.08` | `13.26` |
| `16` | `7.50` | `7.14` | `0.51` | `6.63` |
| `32` | `3.76` | `3.68` | `0.26` | `3.41` |
| `64` | `2.19` | `2.04` | `0.13` | `1.91` |
| `128` | `2.16` | `1.59` | `0.11` | `1.48` |
| `256` | `2.06` | `1.75` | `0.10` | `1.65` |
| `512` | `2.06` | `1.03` | `0.05` | `0.98` |
| `1024` | `2.05` | `0.65` | `0.02` | `0.63` |
| `4096` | `2.05` | `0.38` | `0.01` | `0.38` |
| `16384` | `2.05` | `0.34` | `0.00` | `0.33` |
| `65536` | `2.06` | `0.32` | `0.00` | `0.32` |

由于不同缓冲区大小的情况下传输的数据总量（因此磁盘操作的次数）是相同的，表 13-1 展示的是进行 *read()* 和 *write()* 调用的开销。使用 1 字节的缓冲区时，会进行 1 亿次 *read()* 和 *write()* 调用。使用 4096 字节的缓冲区时，每个系统调用的调用次数降到大约 24,000 次，并且接近最优性能。超过这一点后，性能不再显著提高，因为与在用户空间和内核空间之间复制数据以及执行实际磁盘 I/O 所需的时间相比，进行 *read()* 和 *write()* 系统调用的成本变得微不足道。

### 注意

表 13-1 的最后几行使我们能够大致估算用户空间和内核空间之间的数据传输时间，以及文件 I/O 所需的时间。由于这些情况中的系统调用数量相对较少，它们对经过时间和 CPU 时间的贡献可以忽略不计。因此，我们可以说，*系统 CPU* 时间实际上是在测量用户空间和内核空间之间的数据传输时间。*经过* 时间值则给出了数据传输进出磁盘所需时间的估算。（正如我们稍后看到的，这主要是磁盘读取所需的时间。）

总结来说，如果我们正在向文件中传输大量数据，或者从文件中传输大量数据，那么通过将数据缓冲在大块中，从而减少系统调用次数，我们可以显著提高 I/O 性能。

表 13-1 中的数据衡量了一系列因素：执行 *read()* 和 *write()* 系统调用的时间、在内核空间和用户空间之间传输数据的时间，以及在内核缓冲区和磁盘之间传输数据的时间。我们进一步考虑最后一个因素。显然，将输入文件的内容传输到缓冲区缓存是不可避免的。然而，我们已经看到，*write()* 在将数据从用户空间传输到内核缓冲区缓存后立即返回。由于测试系统上的 RAM 大小（4 GB）远大于正在复制的文件的大小（100 MB），我们可以假设在程序完成时，输出文件实际上并未写入磁盘。因此，作为进一步实验，我们运行了一个程序，简单地使用不同的 *write()* 缓冲区大小将任意数据写入文件。结果如 表 13-2 所示。

再次，数据显示在 表 13-2 中是从内核 2.6.30 中获得的，在一个 *ext2* 文件系统上，块大小为 4096 字节，每行显示的是 20 次运行的平均值。我们没有展示测试程序（`filebuff/write_bytes.c`），但它可以在本书的源代码发行版中找到。

表 13-2. 写入 1 亿字节文件所需时间

| `BUF_SIZE` | 时间（秒） |
| --- | --- |
| 经过时间 | 总 CPU | 用户 CPU | 系统 CPU |
| --- | --- | --- | --- |
| `1` | `72.13` | `72.11` | `5.00` | `67.11` |
| `2` | `36.19` | `36.17` | `2.47` | `33.70` |
| `4` | `20.01` | `19.99` | `1.26` | `18.73` |
| `8` | `9.35` | `9.32` | `0.62` | `8.70` |
| `16` | `4.70` | `4.68` | `0.31` | `4.37` |
| `32` | `2.39` | `2.39` | `0.16` | `2.23` |
| `64` | `1.24` | `1.24` | `0.07` | `1.16` |
| `128` | `0.67` | `0.67` | `0.04` | `0.63` |
| `256` | `0.38` | `0.38` | `0.02` | `0.36` |
| `512` | `0.24` | `0.24` | `0.01` | `0.23` |
| `1024` | `0.17` | `0.17` | `0.01` | `0.16` |
| `4096` | `0.11` | `0.11` | `0.00` | `0.11` |
| `16384` | `0.10` | `0.10` | `0.00` | `0.10` |
| `65536` | `0.09` | `0.09` | `0.00` | `0.09` |

表 13-2 展示了仅进行*write()*系统调用并使用不同的*write()*缓冲区大小将数据从用户空间传输到内核缓冲区缓存的成本。对于较大的缓冲区大小，我们看到与表 13-1 中显示的数据存在显著差异。例如，对于 65,536 字节的缓冲区大小，表 13-1 中的耗时为 2.06 秒，而在表 13-2 中为 0.09 秒。这是因为在后者的情况下没有实际的磁盘 I/O 操作。换句话说，表 13-1 中的大多数时间都是由于磁盘读取造成的。

正如我们在控制内核缓存文件 I/O 中看到的，当我们强制输出操作阻塞直到数据被传输到磁盘时，*write()* 调用的时间会显著增加。

最后，值得注意的是，表 13-2 中的信息（以及后面的表 13-3）仅代表文件系统的一个（天真的）基准测试形式。此外，结果可能会在不同的文件系统之间有所差异。文件系统还可以通过其他各种标准进行衡量，例如在重负载多用户情况下的性能、文件创建和删除的速度、大目录中查找文件所需的时间、存储小文件所需的空间，或者在系统崩溃时维护文件完整性。当 I/O 或其他文件系统操作的性能至关重要时，没有什么能替代针对目标平台进行的特定应用程序基准测试。

## *stdio*库中的缓冲

将数据缓冲到大块以减少系统调用，正是 C 库 I/O 函数（例如，*fprintf()*, *fscanf()*, *fgets()*, *fputs()*, *fputc()*, *fgetc()*）在操作磁盘文件时所做的。因此，使用*stdio*库让我们免去了通过*write()*输出数据或通过*read()*输入数据时手动缓冲数据的任务。

#### 设置*stdio*流的缓冲模式

*setvbuf()* 函数控制*stdio*库所采用的缓冲形式。

```
#include <stdio.h>

int `setvbuf`(FILE **stream*, char **buf*, int *mode*, size_t *size*);
```

### 注意

成功时返回 0，出错时返回非零值

*stream*参数标识要修改缓冲区的文件流。在流打开之后，必须在调用任何其他*stdio*函数之前调用*setvbuf()*。*setvbuf()*调用会影响对指定流的所有后续*stdio*操作的行为。

### 注意

*stdio*库使用的流不应与 System V 的 STREAMS 功能混淆。System V 的 STREAMS 功能并未在主线 Linux 内核中实现。

*buf*和*size*参数指定用于*stream*的缓冲区。这些参数可以通过两种方式指定：

+   如果*buf*非`NULL`，则它指向一个大小为*size*字节的内存块，作为*stream*的缓冲区使用。由于*buf*指向的缓冲区将由*stdio*库使用，因此它应该是静态分配的或通过堆动态分配的（使用*malloc()*或类似的函数）。不应该将其作为局部函数变量分配在栈上，因为当该函数返回并且栈帧被释放时会导致混乱。

+   如果*buf*为`NULL`，则*stdio*库会自动为*stream*分配一个缓冲区（除非我们选择无缓冲 I/O，如下所述）。SUSv3 允许，但不要求，实现使用*size*来确定此缓冲区的大小。在*glibc*实现中，在这种情况下，*size*会被忽略。

*mode*参数指定缓冲类型，并具有以下值之一：

`_IONBF`

不使用缓冲 I/O。每个*stdio*库调用都会立即触发*write()*或*read()*系统调用。*buf*和*size*参数会被忽略，并且可以分别指定为`NULL`和 0。这是*stderr*的默认行为，因此错误输出保证会立即显示。

`_IOLBF`

使用行缓冲 I/O。这个标志是指向终端设备的流的默认模式。对于输出流，数据会缓冲，直到输出一个换行符（除非缓冲区先满）。对于输入流，数据会一行一行地读取。

`_IOFBF`

使用全缓冲 I/O。数据通过*read()*或*write()*函数以等于缓冲区大小的单位读取或写入。这种模式是指向磁盘文件的流的默认模式。

以下代码演示了*setvbuf()*的使用：

```
#define BUF_SIZE 1024
static char buf[BUF_SIZE];

if (setvbuf(stdout, buf, _IOFBF, BUF_SIZE) != 0)
    errExit("setvbuf");
```

请注意，*setvbuf()*在发生错误时返回一个非零值（不一定是-1）。

*setbuf()*函数是*setvbuf()*的封装，执行类似的任务。

```
#include <stdio.h>

void `setbuf`(FILE **stream*, char **buf*);
```

除了没有返回函数结果之外，调用*setbuf(fp, buf)*等同于：

```
setvbuf(fp, buf, (buf != NULL) ? _IOFBF: _IONBF, BUFSIZ);
```

*buf*参数可以指定为`NULL`，表示不使用缓冲，或者指定为指向调用者分配的`BUFSIZ`字节的缓冲区指针。(`BUFSIZ`在`<stdio.h>`中定义。在*glibc*实现中，常量的值为 8192，这是典型的值。)

*setbuffer()*函数类似于*setbuf()*，但允许调用者指定*buf*的大小。

```
#define _BSD_SOURCE
#include <stdio.h>

void `setbuffer`(FILE **stream*, char **buf*, size_t *size*);
```

调用 *setbuffer(fp, buf, size)* 相当于以下操作：

```
setvbuf(fp, buf, (buf != NULL) ? _IOFBF : _IONBF, size);
```

*setbuffer()* 函数在 SUSv3 中没有规定，但在大多数 UNIX 实现中是可用的。

#### 刷新 *stdio* 缓冲区

无论当前的缓冲模式如何，随时我们都可以强制将 *stdio* 输出流中的数据写入（即通过 *write()* 刷新到内核缓冲区），这可以通过 *fflush()* 库函数实现。此函数会刷新指定 *stream* 的输出缓冲区。

```
#include <stdio.h>

int `fflush`(FILE **stream*);
```

### 注释

成功时返回 0，出错时返回 `EOF`。

如果 *stream* 为 `NULL`，则 *fflush()* 会刷新所有 *stdio* 缓冲区。

*fflush()* 函数也可以应用于输入流。这会导致任何缓存的输入被丢弃。（当程序下一次尝试从流中读取时，缓冲区将被重新填充。）

当相应的流被关闭时，*stdio* 缓冲区会自动刷新。

在许多 C 库实现中，包括 *glibc*，如果 *stdin* 和 *stdout* 引用的是终端，那么每次从 *stdin* 读取输入时，会隐式执行 *fflush(stdout)*。这样会刷新任何写入 *stdout* 且未包含终止换行符的提示信息（例如，*printf("Date: ")*）。然而，这种行为并未在 SUSv3 或 C99 中规定，也不是所有 C 库都实现了此行为。便携式程序应该显式调用 *fflush(stdout)*，以确保这些提示信息能够显示出来。

### 注释

如果一个流既用于输入又用于输出，C99 标准规定了两个要求。首先，输出操作不能直接跟在输入操作后面，除非中间调用了*fflush()* 或者某些文件定位函数（*fseek()*、*fsetpos()* 或 *rewind()*）。其次，输入操作不能直接跟在输出操作后面，除非中间调用了某些文件定位函数，除非输入操作遇到了文件末尾（end-of-file）。

## 控制文件 I/O 的内核缓冲

可以强制刷新输出文件的内核缓冲区。有时，当应用程序（例如，数据库日志处理程序）必须确保输出确实已写入磁盘（或至少写入磁盘的硬件缓存）后再继续时，这是必要的。

在描述用于控制内核缓冲的系统调用之前，了解一些来自 SUSv3 的相关定义是很有用的。

#### 同步 I/O 数据完整性和同步 I/O 文件完整性

SUSv3 定义了 *synchronized I/O completion*（同步 I/O 完成）为“一个 I/O 操作，要么已经成功地转移到[磁盘]，要么被诊断为不成功”。

SUSv3 定义了两种不同类型的同步 I/O 完成。这些类型的区别在于 *元数据*（即“关于数据的数据”），描述文件的元数据，内核将其与文件数据一起存储。我们在查看文件 i-nodes 时会详细讨论文件元数据，参见 I-nodes，但现在仅需注意，文件元数据包括文件所有者和组、文件权限、文件大小、文件的（硬）链接数、最后访问时间、最后修改时间和最后元数据更改时间等时间戳，以及文件数据块指针等信息。

SUSv3 定义的第一种同步 I/O 完成类型是 *同步 I/O 数据完整性完成*。这涉及确保文件数据更新传输足够的信息，以便以后能够成功检索该数据。

+   对于读操作，这意味着请求的文件数据已经从磁盘传输到进程。如果有任何待处理的写操作影响了请求的数据，这些数据会在执行读取操作之前传输到磁盘。

+   对于写操作，这意味着写请求中指定的数据已经传输（到磁盘），并且检索该数据所需的所有文件元数据也已传输。需要注意的关键点是，并非所有修改过的文件元数据属性都需要被传输才能允许文件数据被检索。一个需要传输的修改文件元数据属性的例子是文件大小（如果写操作扩展了文件）。相比之下，修改的文件时间戳在后续的数据检索之前并不需要传输到磁盘。

SUSv3 定义的另一种同步 I/O 完成类型是 *同步 I/O 文件完整性完成*，这是同步 I/O 数据完整性完成的超集。与此模式的 I/O 完成的区别在于，在文件更新期间，*所有* 更新的文件元数据都将被传输到磁盘，即使这些数据在后续读取文件数据时并不需要。

#### 控制内核文件 I/O 缓冲的系统调用

*fsync()* 系统调用会将与打开的文件描述符 *fd* 关联的缓冲数据及所有元数据刷新到磁盘。调用 *fsync()* 会强制文件达到同步 I/O 文件完整性完成状态。

```
#include <unistd.h>

int `fsync`(int *fd*);
```

### 注意

成功时返回 0，错误时返回 -1

*fsync()* 调用只有在数据传输到磁盘设备（或至少是其缓存）完成后才会返回。

*fdatasync()* 系统调用的操作与 *fsync()* 类似，但仅将文件强制同步到 I/O 数据完整性完成状态。

```
#include <unistd.h>

int `fdatasync`(int *fd*);
```

### 注意

成功时返回 0，错误时返回 -1

使用 *fdatasync()* 可以将磁盘操作的数量从 *fsync()* 所需的两个操作减少为一个。例如，如果文件数据发生了变化，但文件大小没有变化，那么调用 *fdatasync()* 只会强制更新数据。（我们之前提到过，文件元数据属性（如最后修改时间戳）的更改不需要传输，以完成同步 I/O 数据。）相比之下，调用 *fsync()* 会强制将元数据也传输到磁盘。

以这种方式减少磁盘 I/O 操作的数量对于某些性能至关重要且不需要精确维护某些元数据（如时间戳）的应用程序非常有用。这对于进行多次文件更新的应用程序可以带来显著的性能提升：因为文件数据和元数据通常存储在磁盘的不同区域，更新它们都需要在磁盘上反复进行寻道操作。

在 Linux 2.2 及更早版本中，*fdatasync()* 被实现为调用 *fsync()*，因此没有性能上的提升。

### 注意

从内核 2.6.17 开始，Linux 提供了非标准的 *sync_file_range()* 系统调用，它在刷新文件数据时提供比 *fdatasync()* 更精确的控制。调用者可以指定需要刷新的文件区域，并指定控制系统调用是否在磁盘写入时阻塞的标志。有关更多详细信息，请参阅 *sync_file_range(2)* 手册页。

*sync()* 系统调用会使所有包含更新文件信息的内核缓冲区（即数据块、指针块、元数据等）刷新到磁盘。

```
#include <unistd.h>

void `sync`(void);
```

在 Linux 实现中，*sync()* 只有在所有数据都已传输到磁盘设备（或至少传输到其缓存）后才会返回。然而，SUSv3 允许 *sync()* 的实现仅仅调度 I/O 传输，并在完成之前返回。

### 注意

一个永久运行的内核线程确保如果修改后的内核缓冲区在 30 秒内未被显式同步，它们会被刷新到磁盘。这样做是为了确保缓冲区不会长时间与相应的磁盘文件不同步（从而在系统崩溃时容易丢失）。在 Linux 2.6 中，这项任务由 *pdflush* 内核线程执行。（在 Linux 2.4 中，由 *kupdated* 内核线程执行。）

文件 `/proc/sys/vm/dirty_expire_centisecs` 指定脏缓冲区在被 *pdflush* 刷新之前必须达到的时间（以百分之一秒为单位）。同一目录中的其他文件控制 *pdflush* 操作的其他方面。

#### 使所有写操作同步：`O_SYNC`

在调用 *open()* 时指定 `O_SYNC` 标志会使所有后续的输出变为 *同步*：

```
fd = open(pathname, O_WRONLY | O_SYNC);
```

在此 *open()* 调用之后，每次对文件的 *write()* 操作都会自动将文件数据和元数据刷新到磁盘（即，按照同步 I/O 文件完整性完成的要求执行写入）。

### 注意

旧版 BSD 系统使用`O_FSYNC`标志提供`O_SYNC`功能。在*glibc*中，`O_FSYNC`被定义为`O_SYNC`的同义词。

#### `O_SYNC`的性能影响

使用`O_SYNC`标志（或频繁调用*fsync()*、*fdatasync()*或*sync()*）会显著影响性能。表 13-3 展示了在不同缓冲区大小下，使用和不使用`O_SYNC`时，将 100 万字节写入新创建的文件（在*ext2*文件系统上）所需的时间。结果是通过（使用本书源码分发包中提供的`filebuff/write_bytes.c`程序）在 2.6.30 版本的内核和*ext2*文件系统（块大小为 4096 字节）下获得的。每行显示的是给定缓冲区大小下，进行 20 次运行的平均时间。

从表中可以看出，`O_SYNC`显著增加了经过的时间——在 1 字节缓冲区的情况下，增加了超过 1000 倍的时间。此外，注意到使用`O_SYNC`时，写入操作的经过时间和 CPU 时间之间的巨大差异。这是因为程序在每个缓冲区实际传输到磁盘时被阻塞。

表 13-3 中显示的结果省略了使用`O_SYNC`时影响性能的另一个因素。现代磁盘驱动器拥有较大的内部缓存，并且默认情况下，`O_SYNC`只是将数据传输到缓存。如果我们禁用磁盘缓存（使用命令*hdparm -W0*），则`O_SYNC`的性能影响将变得更加极端。在 1 字节的情况下，经过时间从 1030 秒增加到大约 16,000 秒。在 4096 字节的情况下，经过时间从 0.34 秒增加到 4 秒。

总结来说，如果我们需要强制刷新内核缓冲区，应考虑是否能够设计我们的应用程序，使用较大的*write()*缓冲区大小，或合理利用偶尔调用*fsync()*或*fdatasync()*，而不是在打开文件时使用`O_SYNC`标志。

表 13-3. `O_SYNC`标志对写入 100 万字节速度的影响

| `BUF_SIZE` | 所需时间（秒） |
| --- | --- |
| 不使用`O_SYNC` | 使用`O_SYNC` |
| --- | --- |
| 经过时间 | 总 CPU 时间 | 经过时间 | 总 CPU 时间 |
| --- | --- | --- | --- |
| `1` | `0.73` | `0.73` | `1030` | `98.8` |
| `16` | `0.05` | `0.05` | `65.0` | `0.40` |
| `256` | `0.02` | `0.02` | `4.07` | `0.03` |
| `4096` | `0.01` | `0.01` | `0.34` | `0.03` |

#### `O_DSYNC`和`O_RSYNC`标志

SUSv3 规范规定了与同步 I/O 相关的另外两个文件打开状态标志：`O_DSYNC`和`O_RSYNC`。

`O_DSYNC`标志使得写操作按照同步 I/O 数据完整性完成的要求执行（类似于*fdatasync()*）。这与`O_SYNC`形成对比，后者使得写操作按照同步 I/O 文件完整性完成的要求执行（类似于*fsync()*）。

`O_RSYNC`标志与`O_SYNC`或`O_DSYNC`一同指定，扩展了这些标志的写操作行为到读取操作。当打开文件时同时指定`O_RSYNC`和`O_DSYNC`，意味着所有后续的读取操作都会按照同步 I/O 数据完整性（即在执行读取之前，所有挂起的文件写入将像使用`O_DSYNC`进行的那样完成）的要求来完成。当同时指定`O_RSYNC`和`O_SYNC`打开文件时，意味着所有后续的读取操作都会按照同步 I/O 文件完整性（即在执行读取之前，所有挂起的文件写入将像使用`O_SYNC`进行的那样完成）的要求来完成。

在内核 2.6.33 之前，Linux 并没有实现`O_DSYNC`和`O_RSYNC`标志，*glibc*头文件将这些常量定义为与`O_SYNC`相同。（在`O_RSYNC`的情况下，这实际上是不正确的，因为`O_SYNC`并未为读取操作提供任何功能。）

从内核 2.6.33 开始，Linux 实现了`O_DSYNC`，并且未来的内核版本可能会加入`O_RSYNC`的实现。

### 注意

在内核 2.6.33 之前，Linux 并未完全实现`O_SYNC`语义。相反，`O_SYNC`被实现为`O_DSYNC`。为了保持针对旧内核构建的应用程序的行为一致，针对旧版本 GNU C 库链接的应用程序，继续为`O_SYNC`提供`O_DSYNC`语义，即便是在 Linux 2.6.33 及以后的版本中。

## I/O 缓冲概述

图 13-1 提供了*stdio*库和内核所采用的缓冲概述（用于输出文件），以及控制每种缓冲机制的方式。沿着图表中间向下，我们可以看到用户数据通过*stdio*库函数传输到*stdio*缓冲区，该缓冲区保存在用户内存空间中。当该缓冲区被填满时，*stdio*库会调用*write()*系统调用，将数据传输到内核缓冲区缓存中（保存在内核内存中）。最终，内核会启动磁盘操作，将数据传输到磁盘。

图 13-1 的左侧展示了可以在任何时候显式强制刷新缓冲区的调用。右侧则展示了可以通过禁用 *stdio* 库的缓冲，或通过使文件输出系统调用同步，来使刷新变为自动的调用，从而确保每个 *write()* 操作立即刷新到磁盘。

![I/O 缓冲区总结](img/13-1_FILEBUFF-buffering.png.jpg)图 13-1：I/O 缓冲区总结

## 向内核提供 I/O 模式建议

*posix_fadvise()* 系统调用允许进程向内核报告其访问文件数据的可能模式。

```
#include <fcntl.h>

int `posix_fadvise`(int *fd*, off_t *offset*, off_t *len*, int *advice*);
```

### 注意

成功时返回 0，出错时返回一个正的错误号。

内核可能（但不强制）使用 *posix_fadvise()* 提供的信息来优化其缓冲区缓存的使用，从而改善进程和系统整体的 I/O 性能。调用 *posix_fadvise()* 对程序的语义没有影响。

*fd* 参数是一个文件描述符，用于标识我们希望向内核报告其访问模式的文件。*offset* 和 *len* 参数标识文件中给出建议的区域：*offset* 指定该区域的起始偏移量，*len* 指定该区域的大小（以字节为单位）。*len* 的值为 0 意味着从 *offset* 开始直到文件末尾的所有字节。（在 2.6.6 之前的内核中，*len* 为 0 会被字面理解为零字节。）

*advice* 参数指示进程对文件的预期访问模式。它被指定为以下选项之一：

`POSIX_FADV_NORMAL`

该进程没有特殊的访问模式建议。如果没有为文件提供任何建议，这是默认行为。在 Linux 上，此操作将文件预读窗口设置为默认大小（128 kB）。

`POSIX_FADV_SEQUENTIAL`

该进程预计会顺序地从较低偏移量读取数据到较高偏移量。在 Linux 上，此操作会将文件的预读窗口设置为默认大小的两倍。

`POSIX_FADV_RANDOM`

该进程预计会随机顺序访问数据。在 Linux 上，此选项会禁用文件预读。

`POSIX_FADV_WILLNEED`

进程预计在不久的将来会访问指定的文件区域。内核会执行预读操作，将文件数据填充到缓冲区缓存中，缓存的文件范围由*offset*和*len*指定。随后的*read()*调用不会阻塞磁盘 I/O；相反，它们会直接从缓冲区缓存中获取数据。内核无法保证从文件获取的数据在缓冲区缓存中保持的时间。如果其他进程或内核活动对内存的需求足够强烈，那么这些页面最终将被重新使用。换句话说，如果内存压力很大，我们应该确保*posix_fadvise()*调用和随后的*read()*调用之间的时间间隔尽量短。（Linux 特有的*readahead()*系统调用提供了等效于`POSIX_FADV_WILLNEED`操作的功能。）

`POSIX_FADV_DONTNEED`

进程预计在不久的将来不会访问指定的文件区域。这向内核发出提示，可以释放相应的缓存页面（如果有）。在 Linux 上，此操作分为两步。首先，如果底层设备当前没有被一系列排队的写操作阻塞，内核会刷新指定区域中所有修改过的页面。其次，内核尝试释放该区域的缓存页面。对于该区域中的修改页面，第二步只有在第一步中页面已经写入底层设备时才能成功——即如果设备的写队列没有被阻塞。由于应用程序无法控制设备上的拥堵，确保缓存页面可以被释放的另一种方式是，在执行`POSIX_FADV_DONTNEED`操作之前，调用*sync()*或*fdatasync()*并指定*fd*。

`POSIX_FADV_NOREUSE`

进程预计只会访问指定文件区域一次，然后不再重复使用该数据。这个提示告诉内核，它可以在访问过该数据之后释放相关页面。在 Linux 上，此操作当前没有效果。

*posix_fadvise()*的规范是 SUSv3 中新增的，并非所有 UNIX 实现都支持此接口。自 2.6 内核起，Linux 提供了*posix_fadvise()*。

## 绕过缓冲区缓存：直接 I/O

从 2.4 内核开始，Linux 允许应用程序在执行磁盘 I/O 时绕过缓冲区缓存，从用户空间直接将数据传输到文件或磁盘设备。这有时被称为*直接 I/O*或*原始 I/O*。

### 注意

这里描述的细节是特定于 Linux 的，并未由 SUSv3 标准化。然而，大多数 UNIX 实现都提供某种形式的直接 I/O 访问设备和文件。

直接 I/O 有时被误解为一种获得高速 I/O 性能的手段。然而，对于大多数应用程序来说，使用直接 I/O 可能会显著降低性能。这是因为内核对通过缓冲区缓存执行的 I/O 进行了许多优化，包括执行顺序预读、以磁盘块集群方式执行 I/O，以及允许访问同一文件的进程共享缓存中的缓冲区。所有这些优化在使用直接 I/O 时都会丧失。直接 I/O 仅适用于具有特殊 I/O 需求的应用程序。例如，执行自己缓存和 I/O 优化的数据库系统不需要内核浪费 CPU 时间和内存来执行相同的任务。

我们可以在单个文件或块设备（例如磁盘）上执行直接 I/O。为此，我们在使用*open()*打开文件或设备时指定`O_DIRECT`标志。

`O_DIRECT`标志自内核 2.4.10 版本起生效。并非所有 Linux 文件系统和内核版本都支持使用此标志。大多数本地文件系统支持`O_DIRECT`，但许多非 UNIX 文件系统（例如 VFAT）不支持。可能需要测试相关的文件系统（如果文件系统不支持`O_DIRECT`，则*open()*将因错误`EINVAL`失败），或阅读内核源代码以检查是否支持此功能。

### 注意

如果一个文件被一个进程以`O_DIRECT`方式打开，而另一个进程以正常方式打开（即使用缓冲区缓存），则缓冲区缓存中的内容与通过直接 I/O 读取或写入的数据之间没有一致性。这种情况应避免。

*raw(8)*手册页描述了一种较旧（现已弃用）的方法，用于获得对磁盘设备的原始访问权限。

#### 直接 I/O 的对齐限制

由于直接 I/O（无论是磁盘设备还是文件）涉及对磁盘的直接访问，因此在执行 I/O 时必须遵守一些限制：

+   正在传输的数据缓冲区必须与内存边界对齐，且该边界必须是块大小的倍数。

+   数据传输开始时的文件或设备偏移量必须是块大小的倍数。

+   要传输的数据长度必须是块大小的倍数。

未遵守任何这些限制将导致错误`EINVAL`。在上述列表中，*块大小*指的是设备的物理块大小（通常为 512 字节）。

### 注意

在执行直接 I/O 时，Linux 2.4 比 Linux 2.6 更为严格：对齐、长度和偏移量必须是底层文件系统的*逻辑*块大小的倍数。（典型的文件系统逻辑块大小为 1024、2048 或 4096 字节。）

#### 示例程序

示例 13-1 提供了一个简单的示例，展示了在打开文件进行读取时使用 `O_DIRECT`。此程序最多接受四个命令行参数，依次指定：要读取的文件、要从文件中读取的字节数、程序在读取文件之前应定位的偏移量以及传递给 *read()* 的数据缓冲区对齐方式。最后两个参数是可选的，默认为偏移量 0 和 4096 字节。以下是运行该程序时可能看到的几个示例：

```
$ `./direct_read /test/x 512`                *Read 512 bytes at offset 0*
Read 512 bytes                              *Succeeds*
$ `./direct_read /test/x 256`
ERROR [EINVAL Invalid argument] read        *Length is not a multiple of 512*
$ `./direct_read /test/x 512 1`
ERROR [EINVAL Invalid argument] read        *Offset is not a multiple of 512*
$ `./direct_read /test/x 4096 8192 512`
Read 4096 bytes                             *Succeeds*
$ `./direct_read /test/x 4096 512 256`
ERROR [EINVAL Invalid argument] read        *Alignment is not a multiple of 512*
```

### 注意

示例 13-1 中的程序使用 *memalign()* 函数分配一个与第一个参数倍数对齐的内存块。我们在 堆上分配内存的其他方法 中描述了 *memalign()*。

示例 13-1：使用 `O_DIRECT` 绕过缓冲区缓存

```
`filebuff/direct_read.c`
#define _GNU_SOURCE     /* Obtain O_DIRECT definition from <fcntl.h> */
#include <fcntl.h>
#include <malloc.h>
#include "tlpi_hdr.h"

int
main(int argc, char *argv[])
{
    int fd;
    ssize_t numRead;
    size_t length, alignment;
    off_t offset;
    char *buf;

    if (argc < 3 || strcmp(argv[1], "--help") == 0)
        usageErr("%s file length [offset [alignment]]\n", argv[0]);

    length = getLong(argv[2], GN_ANY_BASE, "length");
    offset = (argc > 3) ? getLong(argv[3], GN_ANY_BASE, "offset") : 0;
    alignment = (argc > 4) ? getLong(argv[4], GN_ANY_BASE, "alignment") : 4096;

    fd = open(argv[1], O_RDONLY | O_DIRECT);
    if (fd == -1)
        errExit("open");

    /* memalign() allocates a block of memory aligned on an address that
      is a multiple of its first argument. By specifying this argument as
      2 * 'alignment' and then adding 'alignment' to the returned pointer,
      we ensure that 'buf' is aligned on a non-power-of-two multiple of
      'alignment'. We do this to ensure that if, for example, we ask
      for a 256-byte aligned buffer, we don't accidentally get a
      buffer that is also aligned on a 512-byte boundary. */

    buf = memalign(alignment * 2, length + alignment);
    if (buf == NULL)
        errExit("memalign");

    buf += alignment;

    if (lseek(fd, offset, SEEK_SET) == -1)
        errExit("lseek");

    numRead = read(fd, buf, length);
    if (numRead == -1)
        errExit("read");
    printf("Read %ld bytes\n", (long) numRead);

    exit(EXIT_SUCCESS);
}
     `filebuff/direct_read.c`
```

## 混合使用库函数和系统调用进行文件 I/O

可以混合使用系统调用和标准 C 库函数对同一文件执行 I/O 操作。*fileno()* 和 *fdopen()* 函数帮助我们完成这项任务。

```
#include <stdio.h>

int `fileno`(FILE **stream*);
```

### 注意

成功时返回文件描述符，出错时返回 -1

```
FILE *`fdopen`(int *fd*, const char **mode*);
```

### 注意

成功时返回（新的）文件指针，出错时返回 `NULL`

给定一个流，*fileno()* 会返回相应的文件描述符（即 *stdio* 库为此流打开的文件描述符）。然后，可以像通常那样使用该文件描述符与 I/O 系统调用进行交互，例如 *read()*、*write()*、*dup()* 和 *fcntl()*。

*fdopen()* 函数是 *fileno()* 的逆操作。给定一个文件描述符，它会创建一个相应的流，使用该描述符进行 I/O 操作。*mode* 参数与 *fopen()* 相同；例如，*r* 表示读取，*w* 表示写入，*a* 表示追加。如果此参数与文件描述符 *fd* 的访问模式不一致，则 *fdopen()* 会失败。

*fdopen()* 函数特别适用于指向非常规文件的描述符。正如我们在后面的章节中将看到的，创建套接字和管道的系统调用总是返回文件描述符。为了在这些文件类型上使用 *stdio* 库，我们必须使用 *fdopen()* 来创建一个相应的文件流。

在结合使用 *stdio* 库函数与 I/O 系统调用对磁盘文件进行 I/O 操作时，我们必须注意缓冲问题。I/O 系统调用将数据直接传输到内核缓冲区缓存，而 *stdio* 库则在流的用户空间缓冲区满时，才调用 *write()* 将缓冲区数据传送到内核缓冲区缓存。考虑以下用于写入标准输出的代码：

```
printf("To man the world is twofold, ");
write(STDOUT_FILENO, "in accordance with his twofold attitude.\n", 41);
```

在常见情况下，*printf()*的输出通常会在*write()*的输出之后出现，因此这段代码会产生如下输出：

```
in accordance with his twofold attitude.
To man the world is twofold,
```

在混合使用 I/O 系统调用和*stdio*函数时，可能需要谨慎使用*fflush()*以避免此问题。我们也可以使用*setvbuf()*或*setbuf()*来禁用缓冲，但这样做可能会影响应用程序的 I/O 性能，因为每次输出操作都会导致执行*write()*系统调用。

### 注意

SUSv3 对应用程序能够混合使用 I/O 系统调用和*stdio*函数做了详细规定。有关详细信息，请参见*系统接口*（XSH）卷中*文件描述符与标准 I/O 流的交互*部分，位于*常规信息*章节下。

## 摘要

输入和输出数据的缓冲由内核和*stdio*库共同执行。在某些情况下，我们可能希望禁用缓冲，但需要注意这样做对应用程序性能的影响。可以使用各种系统调用和库函数来控制内核和*stdio*缓冲，并执行一次性缓冲区刷新。

进程可以使用*posix_fadvise()*向内核建议其访问指定文件数据的模式。内核可能会利用这些信息来优化缓冲区缓存的使用，从而提高 I/O 性能。

Linux 特有的*open()* `O_DIRECT`标志允许专用应用程序绕过缓冲区缓存。

*fileno()*和*fdopen()*函数帮助我们将系统调用和标准 C 库函数混合使用，以在同一文件上执行 I/O。给定一个流，*fileno()*返回相应的文件描述符；*fdopen()*执行反向操作，创建一个使用指定打开的文件描述符的新流。

#### 进一步的信息

[Bach, 1986]描述了 System V 中缓冲区缓存的实现和优点。[Goodheart & Cox, 1994]和[Vahalia, 1996]也描述了 System V 缓冲区缓存的原理和实现。有关 Linux 的进一步相关信息，可以参考[Bovet & Cesati, 2005]和[Love, 2010]。

## 练习

1.  使用 Shell 的*time*内置命令，尝试计时在系统中运行示例 4-1（`copy.c`）程序的操作。

    1.  尝试不同的文件和缓冲区大小。你可以在编译程序时使用*-DBUF_SIZE=nbytes*选项来设置缓冲区大小。

    1.  修改*open()*系统调用，添加`O_SYNC`标志。对于不同的缓冲区大小，这对速度的影响有多大？

    1.  尝试在不同的文件系统上执行这些计时测试（例如，*ext3*，*XFS*，*Btrfs*和*JFS*）。结果是否相似？从小缓冲区到大缓冲区时，趋势是否一致？

1.  对`filebuff/write_bytes.c`程序（本书源代码中提供）在不同缓冲区大小和文件系统下的操作进行计时。

1.  以下语句的效果是什么？

    ```
    fflush(fp);
    fsync(fileno(fp));
    ```

1.  解释为什么以下代码的输出会因标准输出是重定向到终端还是磁盘文件而有所不同。

    ```
    printf("If I had more time, \n");
    write(STDOUT_FILENO, "I would have written you a shorter letter.\n", 43);
    ```

1.  命令*tail [ -n num ] file*打印指定文件的最后*num*行（默认是十行）。使用 I/O 系统调用（*lseek()*, *read()*, *write()*等）来实现此命令。请记住本章描述的缓冲问题，以便使实现更加高效。
