["```\n   np.random.seed(8675309)\n   x = np.load(\"raw/bc_data.npy\")\n   y = np.load(\"raw/bc_labels.npy\")\n➊ x = (x - x.mean(axis=0)) / x.std(ddof=1,axis=0)\n   i = np.argsort(np.random.random(len(y)))\n   x = x[i]\n   y = y[i]\n   n = int(0.7*len(y))\n   xtrn = x[:n]\n   ytrn = y[:n]\n   xtst = x[n:]\n   ytst = y[n:]\n```", "```\nfrom sklearn import decomposition\npca = decomposition.PCA(n_components=xtrn.shape[1])\npca.fit(x)\n```", "```\nstart = 24\nnsets = 10\nnsamp = xtrn.shape[0]\nnewx = np.zeros((nsets*nsamp, xtrn.shape[1]))\nnewy = np.zeros(nsets*nsamp, dtype=\"uint8\")\n\nfor i in range(nsets):\n    if (i == 0):\n        newx[0:nsamp,:] = xtrn\n        newy[0:nsamp] = ytrn else:\n        newx[(i*nsamp):(i*nsamp+nsamp),:] = generateData(pca, xtrn, start)\n        newy[(i*nsamp):(i*nsamp+nsamp)] = ytrn\n```", "```\ndef generateData(pca, x, start):\n    original = pca.components_.copy()\n    ncomp = pca.components_.shape[0]\n    a = pca.transform(x)\n    for i in range(start, ncomp):\n        pca.components_[i,:] += np.random.normal(scale=0.1, size=ncomp)\n    b = pca.inverse_transform(a)\n    pca.components_ = original.copy()\n    return b\n```", "```\ni = np.argsort(np.random.random(nsets*nsamp))\nnewx = newx[i]\nnewy = newy[i]\nnp.save(\"datasets/bc_train_data.npy\", newx)\nnp.save(\"datasets/bc_train_labels.npy\", newy)\nnp.save(\"datasets/bc_test_data.npy\", xtst)\nnp.save(\"datasets/bc_test_labels.npy\", ytst)\n```", "```\n   newx = []\n   newy = []\n   for i in range(len(ytrn)):\n       newx.append(xtrn[i]) newy.append(ytrn[i])\n    ➊ for j in range(20):\n           newx.append(augment(xtrn[i]))\n           newy.append(ytrn[i])\n   xtrn = np.array(newx)\n   ytrn = np.array(newy)\n➋ i = np.argsort(np.random.random(len(ytrn)))\n   xtrn = xtrn[i]\n   ytrn = ytrn[i]\n```", "```\nfrom scipy.ndimage import rotate, zoom\ndef augment(x):\n    im = x.reshape((28,28))\n    if (np.random.random() < 0.5):\n        angle = -3 + 6*np.random.random()\n        im = rotate(im, angle, reshape=False)\n    if (np.random.random() < 0.1):\n        f = 0.8 + 0.4*np.random.random()\n        t = zoom(im, f)\n        if (t.shape[0] < 28):\n            im = np.zeros((28,28), dtype=\"uint8\")\n            c = (28-t.shape[0])//2\n            im[c:(c+t.shape[0]),c:(c+t.shape[0])] = t\n        if (t.shape[0] > 28):\n            c = (t.shape[0]-28)//2\n            im = t[c:(c+28),c:(c+28)]\n    return im.ravel()\n```", "```\nfrom sklearn.neural_network import MLPClassifier\n\ndef normal(rng, mu=0, sigma=1):\n    if (normal.state):\n        normal.state = False\n        return sigma*normal.z2 + mu\n    else:\n        u1,u2 = rng.random(2)\n        m = np.sqrt(-2.0*np.log(u1))\n        z1 = m*np.cos(2*np.pi*u2)\n        normal.z2 = m*np.sin(2*np.pi*u2)\n        normal.state = True\n        return sigma*z1 + mu\nnormal.state = False\n\nclass Classifier(MLPClassifier):\n    def _init_coef(self, fan_in, fan_out, dtype):\n     ➊ def normvec(fan_in, fan_out):\n            vec = np.zeros(fan_in*fan_out)\n            for i in range(fan_in*fan_out):\n                vec[i] = normal(self.rng)\n            return vec.reshape((fan_in,fan_out))\n\n        if (self.init_scheme == 0):\n         ➋ return super(Classifier, self)._init_coef(fan_in, fan_out, dtype)\n        elif (self.init_scheme == 1):\n         ➌ vec = self.rng.random(fan_in*fan_out).reshape((fan_in,fan_out))\n            weights = 0.01*(vec-0.5)\n            biases = np.zeros(fan_out)\n        elif (self.init_scheme == 2):\n         ➍ weights = 0.005*normvec(fan_in, fan_out)\n            biases = np.zeros(fan_out) elif (self.init_scheme == 3):\n         ➎ weights = normvec(fan_in, fan_out)*np.sqrt(2.0/fan_in)\n            biases = np.zeros(fan_out)\n\n        return weights.astype(dtype, copy=False), biases.astype(dtype,\n            copy=False)\n```", "```\nimport numpy as np\nfrom Classifier import *\nfrom RE import *\nfrom scipy.stats import ttest_ind, mannwhitneyu\n\nxtrn = np.load(\"../../data/datasets/mnist_train_data.npy\")/256.0\nytrn = np.load(\"../../data/datasets/mnist_train_labels.npy\")\nxtst = np.load(\"../../data/datasets/mnist_test_data.npy\")/256.0\nytst = np.load(\"../../data/datasets/mnist_test_labels.npy\")\n\nN = 12\ninit0 = []\n\nfor i in range(N):\n    init0.append(Run(0, xtrn,ytrn,xtst,ytst))\ninit0 = np.array(init0)\n```", "```\nm0,s0 = init0.mean(), init0.std(ddof=1)/np.sqrt(N)\nm1,s1 = init1.mean(), init1.std(ddof=1)/np.sqrt(N)\nm2,s2 = init2.mean(), init2.std(ddof=1)/np.sqrt(N)\nm3,s3 = init3.mean(), init3.std(ddof=1)/np.sqrt(N)\n```", "```\n_,p = ttest_ind(init3,init0)\n_,u = mannwhitneyu(init3,init0)\nprint(\"init3 vs init0: p=%0.8f, u=%0.8f\" % (p,u))\n_,p = ttest_ind(init3,init2)\n_,u = mannwhitneyu(init3,init2)\nprint(\"init3 vs init2: p=%0.8f, u=%0.8f\" % (p,u))\n_,p = ttest_ind(init3,init1)\n_,u = mannwhitneyu(init3,init1)\nprint(\"init3 vs init1: p=%0.8f, u=%0.8f\" % (p,u))\n```", "```\ndef Run(init_scheme, xtrn,ytrn,xtst,ytst):\n    clf = Classifier(hidden_layer_sizes=(100,50), max_iter=4000)\n    clf.init_scheme = init_scheme\n    clf.rng = RE()\n    clf.fit(xtrn,ytrn)\n    pred = clf.predict(xtst)\n    _,acc = Confusion(pred,ytst)\n    return acc\n```", "```\ndef Confusion(y,p):\n    cm = np.zeros((4,4), dtype=\"uint16\")\n    for i in range(len(p)):\n        cm[y[i],p[i]] += 1\n    acc = np.diag(cm).sum() / cm.sum()\n    return cm, acc\n```", "```\ninit0: 0.92667 +/- 0.00167\ninit1: 0.89958 +/- 0.00311\ninit2: 0.90083 +/- 0.00183\ninit3: 0.92500 +/- 0.00213\n\ninit3 vs init0: p=0.54429253, u=0.55049935\ninit3 vs init2: p=0.00000002, u=0.00004054\ninit3 vs init1: p=0.00000088, u=0.00006765\n```", "```\ndef train(xtrn, ytrn, hidden=100):\n    inp = xtrn.shape[1]\n    m = xtrn.min()\n    d = xtrn.max() - m\n    w0 = d*rng.random(inp*hidden).reshape((inp,hidden)) + m\n    b0 = d*rng.random(hidden) + m\n    z = activation(np.dot(xtrn,w0) + b0)\n    zinv = np.linalg.pinv(z)\n    w1 = np.dot(zinv, ytrn)\n    return (w0,b0,w1)\n```", "```\nz = activation(np.dot(xtrn,w0) + b0)\n```", "```\nzinv = np.linalg.pinv(z)\nw1 = np.dot(zinv, ytrn)\n```", "```\n> python3 elm.py 200 mt19937\n[[49  0  1  0]\n [ 1 42  0  9]\n [ 2  4 39  4]\n [ 1  5  6 37]]\n\naccuracy = 0.835000\n```", "```\ndef predict(xtst, model):\n    w0,bias,w1 = model\n    z = activation(np.dot(xtst,w0) + bias)\n    return np.dot(z,w1)\n```", "```\ndef confusion(prob,ytst):\n    nc = ytst.shape[1]\n    cm = np.zeros((nc,nc), dtype=\"uint16\")\n    for i in range(len(prob)):\n        n = np.argmax(ytst[i])\n        m = np.argmax(prob[i])\n        cm[n,m] += 1\n    acc = np.diag(cm).sum() / cm.sum()\n    return cm,acc\n```", "```\nmodel = train(xtrn, ytrn, nodes)\nprob = predict(xtst,model)\ncm,acc = confusion(prob,ytst)\n```", "```\ndef relu(x):\n    return np.maximum(x,0)\n```", "```\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-0.01*x))\ndef tanh(x):\n    return np.tanh(0.01*x)\n```", "```\ndef cube(x):\n    return x**3\ndef absolute(x):\n    return np.abs(x)\ndef recip(x):\n    return 1/x\ndef identity(x):\n    return x\n```", "```\nacts = [relu, sigmoid, tanh, cube, absolute, recip, identity]\nnodes = [10,20,30,40,50,100,150,200,250,300,350,400]\nN = 50\nacc = np.zeros((len(acts),len(nodes),N))\n\nfor i,act in enumerate(acts):\n    for j,n in enumerate(nodes):\n        for k in range(N):\n            activation = act\n            model = train(xtrn, ytrn, n)\n            prob = predict(xtst, model)\n            _,a = confusion(prob, ytst)\n            acc[i,j,k] = a\n\nnp.save(\"elm_test_results.npy\", acc)\n```", "```\n> python3 elm_swarm.py 100 tanh 20 60 de de.pkl\n```", "```\n  0: 0.90000 (mean swarm distance 111.844796164)\n  1: 0.90000 (mean swarm distance 111.253958636)\n  2: 0.90000 (mean swarm distance 110.932668482)\n  3: 0.90000 (mean swarm distance 110.743740374)\n  4: 0.90000 (mean swarm distance 110.701071153)\n--snip--\n 57: 0.93000 (mean swarm distance 106.803938775)\n 58: 0.93000 (mean swarm distance 106.803938775)\n 59: 0.93000 (mean swarm distance 106.803938775)\n[[50  0  0  0]\n [ 0 52  0  0] [ 1  2 41  5]\n [ 1  3  2 43]]\nfinal accuracy = 0.930000 (DE-tanh-100, 20:60, 1220 models \n  evaluated, 4 best updates)\n```", "```\n[[18 0 0]\n [ 1 4 11]\n [ 0 1 15]]\n0.74\n```", "```\nimport numpy as np\nfrom RE import * def bootstrap(x):\n    n = RE(mode=\"int\", low=0, high=len(x)).random(len(x))\n    return x[n]\n\nx = np.load(\"iris_train_data.npy\")[:,0]\n\nmeans = [x.mean()]\nfor i in range(10000):\n    y = bootstrap(x)\n    means.append(y.mean())\nmeans = np.array(means)\n\nL = np.quantile(means, 0.025)\nU = np.quantile(means, 0.975)\n\nprint(\"mean from single measurement %0.4f\" % x.mean())\nprint(\"population mean 95%% confidence interval [%0.4f, %0.4f]\" % (L,U))\n```", "```\n> python3 bootstrap.py\nmean from single measurement 5.8500\npopulation mean 95% confidence interval [5.6940, 6.0090]\n```", "```\ndef Bootstrap(xtrn, ytrn):\n    n = RE(mode=\"int\", low=0, high=len(xtrn)).random(len(xtrn))\n    return xtrn[n], ytrn[n]\n\nxtrn = np.load(\"../data/datasets/bc_train_data.npy\")\nytrn = np.load(\"../data/datasets/bc_train_labels.npy\")\nxtst = np.load(\"../data/datasets/bc_test_data.npy\")\nytst = np.load(\"../data/datasets/bc_test_labels.npy\")\n\ntrees = []\nfor i in range(N):\n    tr = DecisionTreeClassifier()\n    if (bag):\n        x,y = Bootstrap(xtrn,ytrn)\n        tr.fit(x,y)\n    else:\n        tr.fit(xtrn,ytrn)\n    trees.append(tr)\n\npreds = []\nfor i in range(N):\n    preds.append(trees[i].predict(xtst))\npreds = np.array(preds)\npred = np.floor(preds.mean(axis=0) + 0.5).astype(\"uint8\")\ncm, acc = Confusion(pred, ytst)\n```", "```\n> python3 bagging.py 60 1\nBagging with 60 decision trees:\n[[101   3]\n [  9  58]]\noverall accuracy 0.9298\nfirst six accuracies: 0.9357 0.9064 0.9357 0.8830 0.9123 0.9123\n```", "```\n> python3 bagging.py 60 0\nBagging with 60 decision trees:\n[[99  5]\n [13 54]]\noverall accuracy 0.8947\nfirst six accuracies: 0.9006 0.9006 0.8947 0.8947 0.8713 0.8947\n```", "```\ndef Bootstrap(xtrn, ytrn):\n    n = RE(mode=\"int\", low=0, high=len(xtrn)).random(len(xtrn))\n    nf = xtrn.shape[1]\n    m = np.argsort(RE().random(nf))[:int(np.sqrt(nf))]\n    return xtrn[n][:,m], ytrn[n], m\n\ntrees = []\nfor i in range(N):\n    tr = DecisionTreeClassifier()\n    x,y,m = Bootstrap(xtrn,ytrn)\n    tr.fit(x,y)\n    trees.append((tr,m))\n\npreds = []\nfor i in range(N):\n    tr,m = trees[i]\n    preds.append(tr.predict(xtst[:,m]))\npreds = np.array(preds)\n```", "```\nfrom sklearn.ensemble import RandomForestClassifier\n```", "```\npreds = []\nfor i in range(N):\n    tr,m = trees[i]\n    preds.append(tr.predict(xtst[:,m]))\npreds = np.array(preds)\n\npred = []\nfor i in range(preds.shape[1]):\n    pred.append(np.argmax(np.bincount(preds[:,i])))\npred = np.array(pred)\n\ncm, a = Confusion(pred, ytst)\nacc.append(a)\n```"]