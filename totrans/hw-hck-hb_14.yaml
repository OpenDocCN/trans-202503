- en: '14'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Think of the Children: Countermeasures, Certifications, and Goodbytes'
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/book_art/chapterart.png)'
  prefs: []
  type: TYPE_IMG
- en: We’ve written much about various attacks, but the ultimate goal of a defensive
    hacker is to improve security. With that in mind, we dedicate this chapter to
    countermeasures that mitigate fault attacks and side-channel analysis, various
    certifications that exist, and how you can get better. This is also the concluding
    chapter to our book, which we see as the bridge to the next step in our journey,
    which is to fix the problems you will expose.
  prefs: []
  type: TYPE_NORMAL
- en: Countermeasures are as old as the field of side-channel power analysis itself,
    and an area of active research. We’ll cover several of the classic countermeasures
    that are good first steps, along with their limitations. When you first hear about
    side-channel analysis, some obvious countermeasures come to mind, but it’s always
    important to evaluate them. For example, just adding noise to the system might
    sound like a good countermeasure, but in practice this makes the attack only slightly
    harder. The countermeasures in this chapter are publicly known (no NDAs were violated
    in the making of this book) and are typically ones that have some usage in the
    industry and represent a “reasonable effort.” Countermeasure development in highly
    secure products requires significant investment and collaboration between hardware
    design and software design teams. However, even with some software-only changes,
    we can make SCA and FI attacks much more difficult to execute.
  prefs: []
  type: TYPE_NORMAL
- en: It’s critically important that you evaluate the effectiveness of your countermeasures.
    For both power analysis and fault injection countermeasures, this must be a continuous
    evaluation. If you are writing C code, for example, your C compiler can simply
    optimize countermeasures away. A very common story in embedded security is that
    a “secure” product with a highly effective countermeasure was evaluated only at
    certain stages of the design. The compiler, synthesis tool, or implementation
    destroyed the effectiveness of the countermeasure. If you don’t test early and
    often, you’ll end up shipping products that you think are protected but simply
    aren’t.
  prefs: []
  type: TYPE_NORMAL
- en: The tools we have taught you in this book are a great starting point for this
    evaluation. You can even start to set up a fully automated analysis, for example,
    so your product is being continuously evaluated with the actual toolchain in use.
  prefs: []
  type: TYPE_NORMAL
- en: Countermeasures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ideal countermeasures don’t exist, but adding several together can make an attacker’s
    job hard enough for them to give up. In this section, we’ll provide several countermeasure
    constructions that you can apply in software or hardware. We’ll also discuss countermeasure
    verification, which is effectively the application of the techniques you learned
    in different chapters to see how much harder the attack becomes. The examples
    that follow are simplified to demonstrate each principle; therefore, we “ignore”
    some recommendations from the other principles. Many of these countermeasures
    are covered in the whitepaper “Secure Application Programming in the Presence
    of Side Channel Attacks” by Marc Witteman et al.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Countermeasures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Implementing countermeasures in a commercial product is very difficult and therefore
    hard to get “right” the first time; in this context, “right” means the right balance
    of cost, power, performance, security, debuggability, development complexity,
    and whatever else you care about. Most successful manufacturers reach a good balance
    of these considerations after several product iterations. Once you start exploring
    conflicts between security and other aspects, at least you know you’re doing something
    right. You’ve hopefully already implemented the low-hanging fruit countermeasures
    and are now reaching the point where real tradeoffs need to be made. This means
    you are actively doing cost/benefit analyses, and you realize there is no absolute
    security; this is life and this is good.
  prefs: []
  type: TYPE_NORMAL
- en: You do want to avoid some common pitfalls. What we typically see is that the
    law of leaky abstractions (“all nontrivial abstractions, to some degree, are leaky,”
    by Joel Spolsky) applies to security vulnerabilities; side channels and faults
    are clearly cases of it, but it also applies to countermeasures. Electrical engineers
    will come up with a new circuit, computer scientists with improved code, and crypto
    people with a new cipher. The problem is that they commonly use the same abstraction
    when designing a countermeasure as when designing the object that contains the
    vulnerability, and that leads to ineffective countermeasures. You’ll see a basic
    example of how a secure countermeasure from one implementation (software) can
    fail on another implementation (hardware) in the “Noncorrelating/Constant Power
    Consumption Everywhere” section later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Breaking through the abstractions requires a fundamental understanding of every
    level of your stack, good-enough simulators, and/or plain-old testing of your
    final product. In other words, this is hard and iterative work; you won’t get
    it right the first time, but if you do it right, you’ll get gradually better.
  prefs: []
  type: TYPE_NORMAL
- en: One of the key insights about countermeasures is that they operate by breaking
    an attack’s assumptions. Every attack makes some assumptions that are required
    to be true in order for the attack to succeed. For instance, in differential power
    analysis (DPA), the assumption is that your operations are aligned in time, so
    a countermeasure introducing misalignment breaks this assumption and reduces DPA’s
    effectiveness. Having an attack tree ready with known attacks and choosing countermeasures
    that break those attacks’ assumptions is a good strategy.
  prefs: []
  type: TYPE_NORMAL
- en: 'This reasoning also works in the opposite direction: countermeasures rely on
    assumptions on the attacks, and it’s up to attackers to break them. The previous
    example of introducing misalignment as a countermeasure to DPA operates under
    the assumption that an attacker isn’t able to recognize features in a trace and
    perform alignment. This is where cat-and-mouse games start.'
  prefs: []
  type: TYPE_NORMAL
- en: With these cat-and-mouse games, countermeasures are broken and upgraded, and
    attacks are thwarted and improved. In software, the main game plan is patching.
    With hardware, that strategy is not possible. In some cases, you can patch hardware
    vulnerabilities using software countermeasures, which means you can keep a product
    secure for a bit longer. In other cases, you’ll rely on the security of a product
    as it was shipped. Ideally, products are shipped with a hardware security margin
    that makes them resistant against attackers *X* years in the future (although
    determining *X* is impossible due to the nonlinear nature of attacks), kind of
    like medicine products need to have some expiry date for their safe usage. In
    reality, this is impossible, and the common strategy is one of “best effort” combined
    with allowing patching through firmware updates and configuration changes.
  prefs: []
  type: TYPE_NORMAL
- en: None of the countermeasures presented here are perfect, but they don’t need
    to be. With some extra effort or more clever attack, an attacker will be able
    to bypass them. The point is not to create an unbreakable system, but one where
    the cost of a successful attack is lower than the cost of the countermeasures
    or where the cost of attacking is higher than the attacker’s budget.
  prefs: []
  type: TYPE_NORMAL
- en: Noncorrelating/Constant Time Everywhere
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If an operation’s duration depends on some secret, either simple power analysis
    (SPA) or timing analysis may be able to recover that secret. The classical example
    of correlating time is using `strcmp()` or `memcmp()` to verify a password or
    PIN. (Storing the plaintext password or PIN instead of a hashed form is not secure
    in the first place, but let’s take it as an example.) Both of these C functions
    have an early termination condition, as they return after the first differing
    byte, giving an attacker who can measure timing the information of which character
    of an entered PIN differs from a stored PIN. For examples, see Chapter 8 on timing
    attacks and the `memcmp()` example in this chapter’s companion notebook (available
    at [https://nostarch.com/hardwarehacking/](https://nostarch.com/hardwarehacking/)).
  prefs: []
  type: TYPE_NORMAL
- en: The trick is to implement a countermeasure that *decorrelates* the timing between
    the operation and the secret, which means making the operating *time constant*
    (and possibly adding *timing randomization* on top), as shown in [Listing 14-7](#listing14-7).
    One solution is to implement a time constant memory comparison, like in `memcmp_consttime()`
    in this chapter’s notebook. We have the core of that function shown in [Listing
    14-1](#listing14-1).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 14-1: A constant time `memcmp``()` function'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of terminating on the first differing byte, for each set of bytes in
    the two buffers, we calculate the XOR, which is zero if the bytes are the same
    and nonzero otherwise. We then accumulate all XORs by OR-ing them into `diff`,
    which means that once a single bit differs, this bit will remain set in `diff`.
    This code has no branches that depend on the contents of either buffer. Even better
    from a leakage perspective is to compare hashes of values instead, but doing so
    will be slower. Note that this example doesn’t include overflow checks for simplicity.
  prefs: []
  type: TYPE_NORMAL
- en: Timing attacks on hash-based message authentication code (HMAC) comparisons
    are common in cryptographic implementations. If you have a data blob that’s signed
    using HMAC, the target system computes the HMAC over the blob and compares it
    to the signature. If that comparison leaks timing information, it allows brute-forcing
    the HMAC value, just like the preceding password example, without the HMAC key
    ever being known. This attack was used to bypass Xbox 360 code verification, called
    the *Xbox 360 timing attack* (unlike the FI attack in Chapter 13). To fix this,
    the *constant time comparison* can be used.
  prefs: []
  type: TYPE_NORMAL
- en: Another important aspect is the timing of branches that are conditional on a
    sensitive value. A simple example would be the code shown in [Listing 14-2](#listing14-2).
    If the secret value passed is `0xCA`, the execution of `leakSecret()` takes much
    longer than if the value is different.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 14-2: We can identify whether or not `secret` is `0xCA` by measuring
    the execution time of this code.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, just by measuring the duration of the process, or by looking at SPA signals,
    an attacker can derive whether the secret value equals `0xca`. An attacker can
    also use knowledge of the timing of the `if()` statement in order to try to fault
    it.
  prefs: []
  type: TYPE_NORMAL
- en: One solution is to make the relevant code *branchless*, like in `dontLeakSecret()`
    in [Listing 14-3](#listing14-3).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 14-3: We avoid the obvious power analysis by always executing both
    operations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea is to execute both sides of the branch and store the results separately.
    Then we calculate a `mask`, which is either all zeros or all ones in binary, depending
    on the outcome of the `if()` condition. We can use this mask to combine the results
    logically: if the mask is all zeros, we take the result from one side of the branch;
    if it’s all ones, we take the result from the other. We’ve also tried to use operations
    to do the mask generation and assignment without a conditional code flow, but
    as we mention later, the risk here is that a clever compiler may yet detect what
    we are doing and replace our code with conditional code. The example from [Listing
    14-3](#listing14-3) (along with all the examples) may be easier to understand
    when running the code yourself, so be sure to see the companion notebook for this
    chapter to better understand the program flow. There are some obvious limitations
    here: `takesLong()` and `muchShorter()` should not have any side effects, and
    the performance of this code will be poorer.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, *timing randomization* is the insertion of nonconstant time operations
    that don’t depend on a secret. The simplest is just a loop that iterates some
    random number of times, which should be tuned such that it introduces sufficient
    uncertainty in timing for the processed secret. If a secret would normally leak
    during a particular clock cycle, you want to spread that out over at least dozens
    or hundreds of clock cycles. Realignment is nontrivial for an attacker if timing
    randomization is combined with sufficient noise addition (see the “Noncorrelating/Constant
    Power Consumption Everywhere” section, next).
  prefs: []
  type: TYPE_NORMAL
- en: Timing randomization also helps against fault injection, as an attacker now
    either has to be lucky that the timing of the fault coincides with the randomized
    timing or needs to spend extra time on a setup that synchronizes with the target
    operation.
  prefs: []
  type: TYPE_NORMAL
- en: Device clocks that are driven by a PLL and not directly by an external crystal
    are usually not perfectly stable. Therefore, some timing randomization already
    comes “naturally”. Similarly, interrupts can add instability to the timing. These
    effects may add sufficient randomization for some use cases.
  prefs: []
  type: TYPE_NORMAL
- en: If not, it is recommended to add timing randomization explicitly before sensitive
    operations. Timing randomization may be easily seen in side-channel traces, so
    it points a big arrow to the sensitive operations. Noise addition may be able
    to help here, as it makes attack techniques such as alignment and Fourier transforms
    that discard timing information more difficult. If you can afford the performance
    hit, you should sprinkle timing randomization throughout your hardware design
    or software code.
  prefs: []
  type: TYPE_NORMAL
- en: Noncorrelating/Constant Power Consumption Everywhere
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You can observe leakage in the power consumption signal’s amplitude. The less
    correlation there is between sensitive data/operations and power consumption,
    the better, but that’s nontrivial to achieve. The most basic way to do it is to
    add *noise* to the power consumption by running any piece of hardware or software
    in parallel. This strategy doesn’t fully decorrelate the signal, but it increases
    the noise and therefore increases the attack cost. In hardware, generating this
    noise can mean running a random number generator, a special noise generator, or
    a video decoder on dummy data. In software, you could run a parallel thread on
    another CPU core that performs decoy or dummy operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In hardware, it’s possible to design a circuit that’s *balanced*—that is, for
    every clock, the same number of bitflips occurs irrespective of the data being
    processed. This balancing is called *dual-rail logic*, and the idea behind it
    is that each gate and line has an inverted version as well, such that a zero-to-one
    transition co-occurs with a one-to-zero transition. Adding this balancing is very
    expensive in terms of chip area and requires extremely careful and low-level balancing
    to make sure each transition happens at the same time. Imbalances still lead to
    leakage, though much less than without this technique. Additionally, electromagnetic
    signals must also be taken into account: two inverted signals may amplify or cancel
    each other out, depending on the spatial arrangement of the signals.'
  prefs: []
  type: TYPE_NORMAL
- en: For crypto, we can go beyond adding random noise and play some nice tricks using
    *masking*. Ideally, for every encryption or decryption, a random mask value is
    generated and is mixed in with the data at the start of the cipher. We then modify
    the cipher implementation such that the intermediate values stay masked, and at
    the end of the cipher, we “unmask” the result. Theoretically, nowhere during the
    cipher’s execution should any intermediate value be present without a mask. This
    means DPA should fail, as DPA critically depends on being able to predict an (unmasked)
    intermediate value. Masking thereby should not have *first-order* *leakage*, which
    is leakage that can be exploited by only looking at a single point in time.
  prefs: []
  type: TYPE_NORMAL
- en: 'One example of masking is rotating S-box masking of AES (see “RSM: A Small
    and Fast Countermeasure for AES, Secure Against 1st and 2nd-Order Zero-Offset
    SCAs,” by Maxime Nassar, Youssef Souissi, Sylvain Guilley, and Jean-Luc Danger).
    In *Rotating* *S-boxes Masking (RSM)*, we modify each of the 16 S-boxes such that
    they take in a mask value *M*[*i*], and they produce an output value masked with
    *M*[(][*i+1*][)] [*mod*] [16], where *M*[*i*] is a randomly chosen 8-bit value
    for 0 ≤ *i* < 16\. Masking is simply done using XOR. The S-box tables are recalculated
    only once before executing the cipher. For the cipher invocation, we XOR the initial
    mask onto the key, which in turn XOR masks the data during `AddRoundKey`. The
    XOR masks are preserved by the modified S-box in `SubBytes` and `ShiftRows` operations.
    The `MixColumns` operation is executed as is, but afterward is “fixed” by XORing
    in a state that effectively remasks the state vector. The result is a masked AES
    state vector after the first round and masked intermediate values all throughout
    the computation. These steps are repeated for all rounds, and then the data is
    unmasked by a final XOR.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The problem with masking is usually that the “perfect” model doesn’t always
    apply in reality. As in the case of RSM, masks are reused, and therefore “perfect”
    has been traded for a performance gain. The paper “Lowering the Bar: Deep Learning
    for Side-Channel Analysis,” by Guilherme Perin, Baris Ege, and Jasper van Woudenberg,
    shows that first-order leakage is still present for one implementation of RSM.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Even if masking is “perfect,” so-called *second-order attacks* on masking exist,
    which work on the principle that we look at two intermediate values, *X* and *Y*.
    For example, *X* could be a byte of state after `AddRoundKey`, and *Y* a byte
    after `SubBytes`. If they are both masked with the same mask *M* during execution—that
    is, *X* ⊕ *M* and *Y* ⊕ *M*—we can do the following. We measure a side-channel
    signal of *X* ⊕ *M* and *Y* ⊕ *M*. Assume for a moment we know the points in time
    *x* and *y* where the signal of *X* ⊕ *M* and *Y* ⊕ *M* are leaking, which means
    we can obtain their corresponding sample values *t*[*x*] and *t*[*y*]. We can
    combine these two measurements points (for example, by calculating their absolute
    difference as |*t*[*x*] − *t*[*y*]|). We also know (*X* ⊕ *M*) ⊕ (*Y* ⊕ *M*) =
    *X* ⊕ *Y*. As it turns out, there is actually a correlation between |*t*[*x*]
    − *t*[*y*]| and *X* ⊕ *Y*, and on that correlation, we can perform DPA. This is
    called a second-orderattack because we combine two points on the trace, but the
    idea extends up to any *higher-order attacks*: first-order masking applies one
    mask to a value (that is, *X* ⊕ *M*) and can be attacked with second-order DPA.
    Second-order masking applies two masks to a value (that is, *X* ⊕ *M*[*1*] ⊕ *M*[*2*])
    and can be attacked with third-order DPA, and so on. In general, *n*th-order masking
    can be attacked with (*n* + 1)th-order DPA.'
  prefs: []
  type: TYPE_NORMAL
- en: The problem with a second-order attack is finding the points in time *x* and
    *y* where the signals of *X* ⊕ *M* and *Y* ⊕ *M* are leaking. In normal DPA, we
    just correlate all samples at a single point in time in a trace to find leakage.
    If we don’t know time *x* and *y*, we have to “brute-force” them by combining
    all possible samples in a trace and perform DPA on all these combinations. This
    is a problem of quadratic complexity in the number of samples in a trace. Also,
    the correlation isn’t perfect, so proper masking forces an attacker to perform
    more measurements and more computation. In other words, masking, though expensive
    and error-prone to implement, also puts a significant burden on an attacker.
  prefs: []
  type: TYPE_NORMAL
- en: '*Blinding* is similar to masking, except that the origins of those techniques
    are in (non-side-channel) cryptography. Various blinding techniques for RSA and
    ECC exist, and they rely on math. One example is RSA message blinding. For ciphertext
    *C*, message *M*, modulus *N*, public exponent *e* and private exponent *d*, and
    a random blind *1* < *r* < *N*, we first calculate the blinded message *R* = *M*
    × *r*^(*e*) mod *N*. Next, we perform the RSA signing on the blinded message,
    *R*^(*d*) = (*M* × *r*^(*e*))^(*d*) = *M*^(*d*) × *r*^(*ed*) = *C* × *r*, and
    we unblind by calculating (*C* × *r*) × *r*^(*–1*) = *C*. This results in the
    same value as textbook RSA without blinding, which would directly calculate *M*^(*d*)
    = *C*. However, because *R* in *R*^(*d*) is unpredictable for an attacker, timing
    attacks that require the message *M* to be raised to *d* fail. This is called
    *message blinding*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since RSA uses one or a few bits of exponent *d* at a time, the exponent is
    also prone to timing or other side-channel attacks. To mitigate the side-channel
    leakage of the exponent value, exponent blinding is needed, which ensures that
    the exponent used in every RSA calculation is different by creating a random number
    1 ≤ *r* < 2^(64) and creating a new exponent *d′* = *d* + ϕ(*N*) × *r*, where
    ϕ(*N*) = (*p* – 1) × (*q* – 1) is the group order. The new exponent is “automatically”
    unblinded by the modular reduction (that is, *M*^(*d*)=*M*^(*d′*) mod *N*) but
    is unpredictable from the point of view of a side-channel attacker. The blinded
    exponent *d′* can be random for each invocation of the cipher, so an attacker
    isn’t able to learn more and more about *d* or a single *d′* by taking more traces.
    This raises the bar for an attacker. Instead of being able to acquire more information
    by acquiring more traces, an attacker is forced to break a single trace. However,
    if the implementation is very leaky, SPA attacks may be effective: completely
    extracting *d*′ from a single trace is equivalent to finding the unblinded private
    key *d*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Many more blinding and masking techniques exist, as well as *time-constant*
    or *randomized exponentiation algorithms* for RSA and *scalar multiplication algorithms*
    for ECC: *modulus blinding*, *Montgomery ladders*, *randomized additions chains*,
    *randomized projective coordinates*, and *higher-order masking*. It’s an active
    field of study, and we recommend researching the latest in attacks and countermeasures.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When working with these countermeasures, be aware of their underlying assumptions.
    The example of masking earlier in this section implied an assumed Hamming weight
    leakage. But what if we implemented this in hardware, and a register leaked the
    Hamming distance between consecutive values? It’s possible then that the masking
    would be annihilated. The unmasking happens when a register consecutively contains
    the two masked values, *X* ⊕ *M* and then *Y* ⊕ *M*, which would leak HD(*X* ⊕
    *M*, *Y* ⊕ *M*). The issue can be seen if we rewrite this as follows: HD(*X* ⊕
    *M*, *Y* ⊕ *M*) = HW(*X* ⊕ *M* ⊕ *Y* ⊕ *M*) = HW(*X* ⊕ *Y*) = HD(*X*, *Y*). Effectively,
    the hardware has unmasked the value for you and just leaks the same Hamming distance.
    Therefore, at the algorithm level, this countermeasure seems like a good one,
    but the implementation can bite you back.'
  prefs: []
  type: TYPE_NORMAL
- en: Randomize Access to Confidential Array Values
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This countermeasure is an easy one. If you’re looping over some secret that’s
    stored in an array, do it in a *random order*, or at least pick a *random starting
    point* and then loop over the array in order. This method disallows an attacker
    with side-channel possibilities from learning about a specific entry in the array.
    Examples where this is useful include verifying HMACs (or plaintext passwords)
    or zeroing/wiping keys from memory, as you don’t want to leak some of this information
    accidentally at a predictable point in time. See the companion notebook for an
    example in the `memcmp_randorder()` function that starts at an arbitrary point
    in the two arrays and does not branch, depending on buffer data. Alternatively,
    you can refer to [Listing 14-4](#listing14-4).
  prefs: []
  type: TYPE_NORMAL
- en: Perform Decoy Operations or Infective Computing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Decoy operations* are designed to mimic an actual sensitive operation (from
    a side-channel perspective), but they have no actual effect on the output of the
    operation. They fool an attacker into analyzing the wrong part of a side-channel
    trace and can double as a way to decorrelate timing. One example is the *square-and-multiply-always
    countermeasure* for modular exponentiation in RSA. In textbook RSA, for every
    bit of the exponent, you perform a square operation if the exponent bit is 0,
    and you perform a multiplication-and-square operation if the bit is 1\. This difference
    in operation for a 0-versus-1 bit has very obvious (SPA) side-channel leakage.
    To even it out, you can perform a decoy multiplication and discard the result
    if the bit is 0\. Now, the number of squares and multiplications are balanced.
    Another example is adding extra rounds to AES that discard their results.'
  prefs: []
  type: TYPE_NORMAL
- en: To stick with our running memory compare example in the notebook, we add some
    random decoy rounds in `memcmp_decoys()`. It works by randomly executing a decoy
    XOR and making sure the result doesn’t get accumulated. This is also used in [Listing
    14-4](#listing14-4).
  prefs: []
  type: TYPE_NORMAL
- en: '*Infective computing* goes one step further: it uses the decoy operations as
    a way to “infect” the output. If any error occurs in the decoy operation, it corrupts
    the output. This is particularly handy in crypto operations; see “Infective Computation
    and Dummy Rounds: Fault Protection for Block Ciphers Without Check Before-Output”
    by Benedikt Gierlichs, Jörn-Marc Schmidt, and Michael Tunstall.'
  prefs: []
  type: TYPE_NORMAL
- en: Another good use of decoy operations is detecting faults (detect and respond
    to faults). If the decoy operation has a known output, you can verify that output
    is correct; if not, a fault must have occurred.
  prefs: []
  type: TYPE_NORMAL
- en: Side-Channel-Resistant Crypto Libraries, Primitives, and Protocols
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Saying “use vetted *crypto libraries*” is along the lines of the Crypto 101
    rule “don’t roll your own crypto.” The caveat here is that most open source crypto
    libraries do not provide any power analysis side-channel resistance or fault-resistance
    guarantees. Common libraries (such as OpenSSL and NaCl) and primitives (such as
    Ed25519) do protect against timing side-channel attacks, mainly because timing
    attacks can be exploited remotely. If you’re building on top of a microcontroller
    or secure element, the crypto cores and/or library that comes with the chip may
    claim to have some resistance. Check the datasheet for the word *countermeasure*,
    *side channel*, or *fault*, or check any certifications. Even better, test the
    chip!
  prefs: []
  type: TYPE_NORMAL
- en: If you’re stuck with a crypto library or primitive that is not power side-channel
    resistant, you may be able to use a *leakage-resistant protocol*. These protocols
    basically ensure that keys are used only once or a few times, thus making DPA
    significantly harder. For instance, you can hash a key in order to create a new
    key for a next message. This type of operation is used, for example, in the AES
    mode implemented by NXP with the LPC55S69, which is called *Indexed Code Block*
    mode.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, you can *wrap* the library to perform some safety checks against faults.
    For instance, after signing with ECC or RSA, you can verify the signature to check
    whether it passes. If not, some fault must have happened. Similarly, you can decrypt
    after encrypting to check that you obtained the plaintext again. Performing these
    checks pushes an attacker into double faults: one to target the algorithm and
    another to bypass the fault check.'
  prefs: []
  type: TYPE_NORMAL
- en: Don’t Handle Keys When You Can Avoid It
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Pretend that you are Superman and that keys are kryptonite; handle them with
    care and only when absolutely needed. Don’t copy (or integrity-check) them, and
    do pass them by *reference* in your application rather than by value. When using
    a crypto engine, avoid loading the key to the engine more than necessary to avoid
    *key-loading attacks*. This practice obviously reduces the possibilities for side-channel
    leakage, but also for fault attacks on the key. Differential fault analysis is
    a class of fancy crypto fault attacks, but there are more fault attacks on crypto.
  prefs: []
  type: TYPE_NORMAL
- en: 'Say an attacker can just zero out (part of) a key (for instance, during a key
    copy operation). Doing so can break challenge-response protocols. Challenge-response
    is basically used by one party to establish whether the other party has knowledge
    of a key: Alice sends Bob a nonce *c* (the challenge), and Bob encrypts *c* with
    shared key *k* and sends the response *r*. Alice performs the same encryption
    and verifies that Bob sent the correct *r*. Now Alice knows that Bob has knowledge
    of key *k*.'
  prefs: []
  type: TYPE_NORMAL
- en: That’s all fine and dandy, except the Fault Fairy now has physical access to
    Alice’s crypto device. The key Alice uses for verification is now corrupted by
    a fault such that it is all zeros. Because the Fault Fairy knows this, she can
    spoof Bob by encrypting *r* with a zero key. Alternatively, if the Fault Fairy
    has access to Bob’s crypto device and can partially zero a key (for example, all
    except one byte), she can use one pair of *c* and *r* to brute-force the one nonzero
    key byte. Iterating over the other key bytes can expose the entire key. If the
    device is reloading the key frequently, the Fault Fairy has many attempts to zero
    out different parts of the key.
  prefs: []
  type: TYPE_NORMAL
- en: Use Nontrivial Constants
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A Boolean in software is stored in 32 or 64 bits on a modern CPU. You can make
    use of all those other bits to build in fault mitigation and detection. In Chapter
    7, you saw in the demonstration of the Trezor One glitch that a simple comparison
    could be skipped. Likewise, imagine you are using the following code to verify
    a signature operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The only return value of `verify_signature()` that *won’t* result in the code
    in question being flashed in is `0`. Every other possible return value will evaluate
    to “true” by the code! This is an example of using trivial constants that result
    in a particularly easily fault-injectable code.
  prefs: []
  type: TYPE_NORMAL
- en: A typical fault model is that an attacker can zero out or “`0xffffffff` out”
    a word. In this model, it’s unlikely the attacker can set a specific 32-bit value.
    So, instead of using zero and one for a Boolean, we can use *nontrivial constants*
    with a large Hamming distance (for instance `0xA5C3B4D2` and `0x5A3C4B2D`). These
    require a large number of bitflips (through a fault) to get from one to the other.
    Simultaneously, we could define `0x0` and `0xffffffff` to be invalid values to
    catch faults.
  prefs: []
  type: TYPE_NORMAL
- en: This idea can be extended to states in an enum, and similarly can be done in
    hardware state machines. Note that the application of this construct for states
    in an enum is typically trivial, but for Booleans, it can be infeasible to implement
    consistently, specifically when standard functions are used.
  prefs: []
  type: TYPE_NORMAL
- en: In the example `memcmp_nontrivial()` in the notebook, we extend our memory compare
    function with nontrivial values for important state. This version is also shown
    in [Listing 14-4](#listing14-4), which includes decoys, starting at a random index
    and constant time.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 14-4: A complicated `memcmp` function with decoy functions and nontrivial
    constants'
  prefs: []
  type: TYPE_NORMAL
- en: 'The trick is to encode the values for `diff` and `tmpdiff` such that they are
    never just all 1 or all 0\. For that, we use two special values: `CONST_1==` `0xC0A0B000`
    and `CONST_2==0x03050400`. They’ve been designed to have the lower byte set to
    0\. This lower byte will be used to store the XOR of 2 bytes in memory, and we
    accumulate this in the `diff` variable. In addition, we’ll use the upper 24 bits
    of `diff` as a nontrivial constant. As you can see in the code, we also accumulate
    the values of `CONST_1` and `CONST_2` into `diff`. The way this is done is such
    that under normal circumstances, the top 24 bits of `diff` will have a fixed,
    known value—namely, the same as the top 24 bits of `CONST_1` | `CONST_2`. If there
    is a data fault that flips a bit in the top 24 bits of `tmpdiff`, it can be detected;
    you’ll see what to do later in the “Detect and Respond to Faults” section.'
  prefs: []
  type: TYPE_NORMAL
- en: The examples of the different memory compare functions show how hard it is to
    write something that mitigates faults. When you’re using optimizing (JIT) compilers,
    it’s even harder to write the code such that the countermeasures don’t get compiled
    out. The obvious answer is to do this in assembly (with the downside of having
    to code in assembly) or to make a compiler that injects these kinds of countermeasures.
    There have been some academic publications on the topic, but the problem seems
    to be acceptance—either for performance reasons or for concerns around potentially
    introducing issues into otherwise well-tested compiler behavior.
  prefs: []
  type: TYPE_NORMAL
- en: In hardware, *error correcting codes (ECCs)* can be considered “nontrivial constants”
    used to mitigate faults. They typically have limited error correction and detection
    capabilities, and for an attacker who can flip many bits (for example, an entire
    word), this may reduce fault effectiveness less than an order of magnitude. Care
    should also be taken that, for example, an all-zero word (including ECC bits)
    is not a correct encoding.
  prefs: []
  type: TYPE_NORMAL
- en: Status Variable Reuse
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Using nontrivial constants is great, but consider the code flow of `check_fw()`
    in the companion notebook, also shown in [Listing 14-5](#listing14-5). It sets
    `rv = validate_address(a)`, which returns a nontrivial constant. If the constant
    is `SECURE_OK`, it does `rv = validate_signature(a)`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 14-5: Using nontrivial constants isn’t an immediate fix for everything.'
  prefs: []
  type: TYPE_NORMAL
- en: An attacker can do something easily here; they could use FI to skip the call
    at 2 to `validate_signature()`. The variable `rv` already has the `SECURE_OK`
    value present from the previous call to `validate_address()` at 1. Instead, we
    should be clearing the value after usage. In languages that support macros, we
    can do this relatively easily with a macro that wraps some of these calls. Alternatively,
    we can use a different variable (for example, by introducing an `rv2` for the
    second call) or verify control flow (see the next section). Note that all these
    methods are prone to compiler optimization (see the section “Fighting Compilers”
    later in the chapter).
  prefs: []
  type: TYPE_NORMAL
- en: Verify Control Flow
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Fault injection can alter control flow, so any critical control flow should
    be *verified* to decrease the probability of a successful fault. A simple example
    is a “default fail” statement in a `switch` statement in C; the case statements
    should enumerate all valid cases, and the default case should therefore never
    be reached. If the default case is reached, we know a fault has occurred. Similarly,
    you can do this for `if` statements where the final `else` is a failure mode.
    You can see an example of this in `default_fail()` in the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: When implementing any *conditional branch* (including one using the fancy “nontrivial
    constants”), also be aware of how the compiler’s implementation of your conditional
    may drastically affect the ability of an attacker to bypass a given code check.
    The high-level `if` statement will likely be implemented as a “branch if equal”
    or “branch of not equal” type of instruction. Like in Chapter 4, we’re going to
    go back to assembly code to see how this is implemented. The assembly code resulting
    from a typical `if`…`else` statement is given in [Listing 14-6](#listing14-6).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 14-6: Arm assembly code showing an `if` statement as implemented by
    the compiler'
  prefs: []
  type: TYPE_NORMAL
- en: This `if` statement was designed to check whether or not an image (pointed to
    with `IMG_PTR`) should be booted. The function `signature_ok()` is called at 1,
    which has some special return value in `r0` to indicate if the signature should
    allow the image to boot. This comparison ultimately boils down to a branch if
    equal (`beq`) at 2, where if the branch to `.L2` is taken, the `panic()` function
    is called at 4. The problem is if an attacker skips the `beq` at 2, it will fall
    through to the `boot_image()` function at 3. Switching the order of the comparison
    such that skipping the `beq` at 2 would fall through to the `panic()` function
    would be good practice in this example. You may need to work with your compiler
    to get this effect (check `__builtin_expect` in gcc and clang compilers), and
    it’s a good reminder why investigating the actual assembly output is important.
    See the section “Simulation and Emulation” later in the chapter for links to tools
    to help you automated these tests.
  prefs: []
  type: TYPE_NORMAL
- en: Double- or multi-checking sensitive decisions is also a means to verify control
    flow. Specifically, you implement multiple `if` statements that are logically
    equivalent but contain different operations. In the `double_check()` example in
    the notebook, the memory compare is executed twice and checked twice with slightly
    different logic. If the results of the second comparison disagree with the first,
    we’ve detected a fault.
  prefs: []
  type: TYPE_NORMAL
- en: The `double_check()` example is already hardened against single faults, but
    multiple faults timed at exactly the number of cycles between the `memcmp()` invocations
    can skip both checks. Therefore, it’s best to add some *random wait state* in
    between and ideally perform some *non-sensitive operations*, as shown in the `double_check_wait()`
    example in the notebook and also shown in [Listing 14-7](#listing14-7). The non-sensitive
    operations help because, first, a long glitch may corrupt consecutive conditional
    branches, and, second, the side-channel signal of the random wait gives away information
    to the attacker about when sensitive operations are happening. Compared to the
    previous examples, faults that were 100 percent successful before are now less
    likely.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 14-7: Double-checking `memcmp` operations with random delays'
  prefs: []
  type: TYPE_NORMAL
- en: Another simple control flow check is to see whether a sensitive loop operation
    terminates with the correct loop count. The `check_loop_end()` example in the
    companion notebook illustrates this; after the loop ends, the iterator value is
    checked against a “known-good” value.
  prefs: []
  type: TYPE_NORMAL
- en: A more convoluted but broader countermeasure is that of *control flow integrity*.
    There are many ways of implementing this, but we give one example with a *cyclic
    redundancy check (CRC)*. CRCs are very fast. The idea is to represent a sequence
    of operations as a sequence of bytes, over which we calculate the CRC. At the
    end, we check whether the CRC matches what we expect, which should always be the
    case, unless a fault changed the sequence of operations. You’ll have to add some
    code to aid in your control flow integrity work.
  prefs: []
  type: TYPE_NORMAL
- en: The companion notebook shows this in `crc_check()`, where several function calls
    update a running CRC. First, we enable a `DEBUG` mode, which causes the final
    CRC to be printed. Next, this CRC is embedded in the code as a check, and debug
    mode is turned off. Now, control flow checking is active. If a function call is
    skipped, the final CRC value will differ. You can verify that it works by setting
    the `FAULT` variable to 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: You can perform this type of simple control flow checking wherever there are
    no conditional branches. If you have a few conditional branches, you can still
    hard-code a few valid CRC values for each of the paths through the program. Alternatively,
    you also can have local control flow that operates only within one function.
  prefs: []
  type: TYPE_NORMAL
- en: CRCs are, of course, not cryptographically secure. Cryptographic security isn’t
    very important here, because all we need is a verification code that is hard to
    forge. In this case, forging would mean fault injections to set the CRC to a specific
    value, which we assume is outside of the capabilities of an attacker.
  prefs: []
  type: TYPE_NORMAL
- en: Detect and Respond to Faults
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: By using nontrivial constants, double-checks, or decoy operations, we can start
    building *fault detection*. If we encounter an invalid state, we know it’s caused
    by a fault. This means in `if` statements, we check `condition==TRUE`, then `condition==FALSE`,
    and if we reach the final `else`, we know a fault has occurred. Similarly for
    “switch” statements, the “default” case should always be a fault option. See `memcmp_fault_detect()`
    in the notebook for an example of using nontrivial constants to detect faults;
    it simply checks whether the bits in the nontrivial bits in `diff` and `tmpdiff`
    are correctly set and returns `None` otherwise. Another example is 1 in [Listing
    14-7](#listing14-7), where the first check succeeded but the second one failed.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to decoy operations, we can use any parallel process in software or
    hardware to build generic *fault canaries*. Under normal circumstances, they should
    have some fixed, verifiable output, but under attack, their output changes.
  prefs: []
  type: TYPE_NORMAL
- en: In hardware, we can build similar constructs. Additionally, hardware can include
    specific *fault sensors* that detect anomalies in the supply voltage or external
    clock, or even on-die *optical sensors*. These can be effective against specific
    fault types, but a different type of attack can bypass them. For instance, an
    optical sensor will detect a laser pulse, but will not detect a voltage perturbation.
  prefs: []
  type: TYPE_NORMAL
- en: A *fault response* is what to do when a fault is detected. The goal here is
    to reduce the chances of a successful attack to the point where an attacker will
    give up. On the one end of the spectrum, you can implement a program exit, OS
    reboot, or chip reset. These actions will delay an attacker but in principle still
    allow them infinite tries. Somewhere in the middle of the spectrum is signaling
    a backend system to flag this device as suspicious and perhaps disable the account.
    On the other end of the spectrum, you can implement permanent measures like wiping
    keys, accounts, or even burning fuses that disallow the chip from booting up.
  prefs: []
  type: TYPE_NORMAL
- en: How to respond to faults can be difficult to decide, as it depends strongly
    on how tolerant you are to false positives, whether the system is safety critical,
    and how bad the impact of a compromise really is. In a credit card application,
    it’s perfectly acceptable to wipe keys and disable all functionality when under
    attack. At the same time, it’s not acceptable if this happens at a large scale
    due to false positives. Some balance needs to be struck on how many false positives
    (and faults!) can be had within a certain time frame or lifetime.
  prefs: []
  type: TYPE_NORMAL
- en: To balance false positives and actual faults, a *fault counter* can be used.
    Initial counter increments are considered false positives, until the counter increments
    to a certain *counter threshold*. At the threshold, we conclude we are under (fault)
    attack. This counter must be nonvolatile, as you don’t want a power-down to reset
    the counter. An attacker would easily abuse this by just resetting between each
    fault attempt.
  prefs: []
  type: TYPE_NORMAL
- en: Even a nonvolatile counter must be implemented with care. We’ve done attacks
    where we detect the detection mechanism through a side-channel measurement and
    then power off the target before the counter can be updated in nonvolatile storage.
    That attack can be thwarted by incrementing the counter *before* a sensitive operation,
    storing it, performing the sensitive operation, and, only if no faults are detected,
    decrementing the counter again. A power-off will now simply mean the counter was
    increased.
  prefs: []
  type: TYPE_NORMAL
- en: The counter threshold depends on your application’s exposure and tolerance for
    false positives; in automotive and aerospace/space applications, faults caused
    by nature are much more common because of the exposure to radiation and strong
    electromagnetic fields. The tolerance depends on the application. In the credit
    card case, wiping keys and effectively disabling functionality is acceptable.
    However, that wouldn’t be acceptable behavior for devices that have a safety function,
    such as medical or automotive devices. It may even not be acceptable from a field
    failure rate perspective for other applications. In that case, a response could
    be to inform a backend covertly that the device may be under attack. At this point,
    what to do is a product design decision, but it often involves trading off security
    for safety, cost, performance, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Verifying Countermeasures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The countermeasures in this section will potentially make attacks harder. That’s
    an intentionally weak statement. Unfortunately, we’re not in a clean cryptographic
    world where elegant proofs exist that can reduce to existing and well-studied
    hard mathematical problems. We don’t even have the same kind of heuristic security
    as in cryptography, as countermeasure effectiveness varies from chip type to chip
    type, and sometimes from individual chip to chip. At best, literature analyzes
    countermeasures in a noiseless setting and validates them on (often) simple microcontrollers
    or FPGAs that behave relatively “cleanly.” That’s why—until we get better theoretical
    means to predict countermeasure effectiveness—testing effectiveness on real systems
    is critical.
  prefs: []
  type: TYPE_NORMAL
- en: Strength and Bypassability
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Two main angles need to be analyzed when verifying a countermeasure: strength
    and bypassability. In real-world analogies, *strength* is about how hard it is
    to pry open a door lock, and *bypassability* is about whether you can avoid the
    lock by entering through the window.'
  prefs: []
  type: TYPE_NORMAL
- en: Strength can be measured by turning the countermeasure on and off and then verifying
    the difference in attack resistance. For fault injection, you can represent this
    difference as the decrease in fault probability. For side-channel analysis, you
    can express this difference as the increase in the number of traces until key
    exposure.
  prefs: []
  type: TYPE_NORMAL
- en: 'See the notebook for an example of testing the strength of the nontrivial constants
    countermeasure of the `memcmp_fault_detect()` function. This function uses the
    top 24 nontrivial constant bits (see also [Listing 14-4](#listing14-4)) as a fault
    detection mechanism. We simulate single-byte faults in the `diff` and `tmpdiff`
    values. We can observe that in roughly 81.2 percent of the cases, the fault is
    successfully detected, and in about 18.8 percent of the cases, there is no fault,
    or it has no observable effect. However, our countermeasure is not perfect: in
    about 0.0065 percent of the cases, the fault manages to flip the bits of `diff`
    or `tmpdiff` such that `memcmp_fault_detect()` concludes that the inputs are equal.
    Though that sounds like a low success rate, if this were a password check, we’d
    expect a successful login after 15,385 fault injections (1/0.000065). If you can
    do one fault per second, you’d be in within five hours.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second (and more tricky) angle is bypassability: what is the effort in
    going around the countermeasure? To determine that, consider building an attack
    tree (see Chapter 1), which allows you to enumerate other attacks. You may mitigate
    voltage glitches, but an attacker can still do electromagnetic fault injection.'
  prefs: []
  type: TYPE_NORMAL
- en: Fighting Compilers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Once you verify your countermeasures a few times, you’ll find they sometimes
    are completely ineffective, which can be due to bad coverage (for example, you
    plugged one leak where there were many). What also can happen is that your toolchain
    optimizes out your countermeasures because they don’t have any side effects. For
    instance, double-checking a value is the logical equivalent of checking a value
    once, so an optimizing compiler cleverly removes your double-check. Similar situations
    can happen during synthesizing hardware, where duplicated logic may be optimized
    out.
  prefs: []
  type: TYPE_NORMAL
- en: If you use the `volatile` keyword on variables in C or C++, this can help avoid
    optimizing away countermeasures. With `volatile`, the compiler may not assume
    that two reads of the same variable yield the same value. Therefore, if you check
    a variable twice in a double-check, it will not be compiled out. Note that this
    generates more memory accesses, so if a chip is particularly sensitive to memory
    access glitches, it’s a double-edged sword. You can also use `__attribute__((optnone))`
    to turn off optimizations for particular functions.
  prefs: []
  type: TYPE_NORMAL
- en: The code in [Listing 14-6](#listing14-6) is another example where compiler optimizations
    will result in changes in your fault countermeasure. The compiler may choose to
    reorder the assembly code generated, which will lead to a fall-through condition
    if an attacker skips the single branch instruction.
  prefs: []
  type: TYPE_NORMAL
- en: There is some research on making compilers output code that is more resistant
    to faults, which is an obvious solution direction; see Hillebold Christoph’s thesis
    “Compiler-Assisted Integrity Against Fault Injection Attacks.” Blanket application
    of such techniques are not be desirable for performance reasons.
  prefs: []
  type: TYPE_NORMAL
- en: Simulation and Emulation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Use of simulators is also important during verification. With hardware design,
    the cycle from initial design to first silicon may take years. Ideally, we want
    to be able to “measure” leakage well before silicon, when there is still time
    to fix things. See “Design Time Engineering of Side Channel Resistant Cipher Implementations”
    by Alessandro Barenghi et al.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar research is ongoing on fault injection: by simulating various instruction
    corruptions, we can test whether single fault injection points exist. For more
    information, see “Secure Boot Under Attack: Simulation to Enhance Fault Injection
    and Defenses” by Martijn Bogaard and Niek Timmers. Riscure has an open source
    CPU emulator that implements instruction skipping and corruption at [https://github.com/Riscure/FiSim/](https://github.com/Riscure/FiSim/)
    that you can try to test your software countermeasures in. We recommend you try
    out this emulator—you can quickly learn which countermeasures work well and which
    won’t. More importantly, you’ll learn which countermeasure combinations are required
    to get down to a low fault count. Getting it down to zero faults is not easy!'
  prefs: []
  type: TYPE_NORMAL
- en: Verification and Enlightenment
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Countermeasure strength is something you can measure yourself; for countermeasure
    bypassability, it’s best to engage someone who wasn’t involved in the design.
    Countermeasures can be regarded as a security system, and as Schneier’s law states,
    “Any person can invent a security system so clever that he or she can’t imagine
    a way of breaking it.”
  prefs: []
  type: TYPE_NORMAL
- en: On this topic, allow us a small excursion into what we’ll call the *four stages
    of security enlightenment*. It is our completely unscientific observation and
    subjective experience of how people generally respond to the notion of hardware
    attacks and how to solve them.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *first stage* is basic denial of the possibility or practicality of side-channel
    or fault attacks. The issue here is that basic software engineering assumptions—assumptions
    you’ve experienced and heard about all the time—can be broken: the hardware actually
    isn’t executing instructions that it’s fed, and it’s telling the world all about
    the data it’s processing. It’s like finding out the world isn’t flat.'
  prefs: []
  type: TYPE_NORMAL
- en: Once the first stage is passed, the *second stage* is that countermeasures are
    easy or unbreakable. It’s the natural response to not yet grasp the full depth
    of the security issues, the cost of the countermeasures, or that attackers are
    adaptive creatures. It usually takes some countermeasures being broken (or some
    “yeah, but if you do that then…” conversations with a security expert) before
    moving on to the next stage, which is *security nihilism*.
  prefs: []
  type: TYPE_NORMAL
- en: Security nihilism is the idea that everything is broken, so there’s nothing
    we can do anyway to prevent attacks. It’s true that everything can be broken,
    given a motivated and well-resourced attacker—and that is the crux. There are
    a limited number of attackers, and they have varying motivation and resources.
    As it stands, it’s still much easier to clone a magstripe credit card than to
    perform a side-channel attack on a credit card. As James Mickens said, “If your
    threat model includes the Mossad, you’re still gonna be Mossad’ed upon.” But,
    if you’re not a target for the Mossad, you probably will not be Mossad’ed upon.
    They also need to prioritize.
  prefs: []
  type: TYPE_NORMAL
- en: 'The fourth and final stage is *enlightenment*: understanding that security
    is about risk; risk will never be zero, but risk isn’t about the worst case happening
    all the time. In other words, it’s about making an attack as uninteresting for
    an attacker as feasible. Ideally, countermeasures raise the bar to the point where
    the cost of an attack isn’t worth the payoff. Or often more realistically, countermeasures
    make another product more interesting to attack than yours. Enlightenment is about
    realizing the limitations of countermeasures, and making risk-based tradeoffs
    as to which countermeasures to include. It’s also about being able to sleep again
    at night.'
  prefs: []
  type: TYPE_NORMAL
- en: Industry Certifications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Certification for side-channel analysis and fault injection resistance has been
    available through various organizations, which we’ll list in this section. We
    know from Chapter 1 that security isn’t binary, so what do industry certifications
    mean if an unbreakable product doesn’t exist?
  prefs: []
  type: TYPE_NORMAL
- en: The goal of these certifications is for vendors to demonstrate to third parties
    that they have some level of *assurance* of some level of *attack resistance*.
    It also means that only for a limited time; a certificate that’s a few years old
    obviously does not include attacks most recently discovered.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s briefly consider attack resistance first. A product passes *Common Criteria
    PP-0084 (CC)/EMVCo* certification if it demonstrably has all the security functionality
    required, and the certifying lab cannot show an attack path exists that has fewer
    than 31 points in the *JIL score* (see “Scoring Hardware Attack Paths” in Chapter
    1). An attack path is only an attack path if it ends with the compromise of a
    well-defined asset, such as a key. That means both positive and negative testing
    is used, establishing “does it do what it should do” as well as “does it not do
    what it shouldn’t do.” The latter is very important when the adversary is intelligent
    and adaptive.
  prefs: []
  type: TYPE_NORMAL
- en: Effectively, the JIL scoring limits the time, equipment, knowledge, personnel,
    and number of (open) samples that can be used for the attack. Whatever attacks
    a lab knows about or can develop are relevant for CC/EMVCo, as long as the scoring
    is within 31 points. See the latest version of the JIL document titled “Application
    of Attack Potential to Smartcards and Similar Devices” (which is available publicly
    online) for a good reference on how this scoring is done. A certificate tells
    you that the lab was unable to identify any attack that scored less than 31 points.
    Labs won’t even test whether attacks of 31 points and higher work. Going back
    to our earlier point about unbreakable products, the point system means you may
    still be able to find attacks at the high ratings. A great example is “Deconstructing
    a ‘Secure’ Processor,” by Christopher Tarnovsky, presented at Black Hat 2010,
    where he impressively goes beyond the effort a lab would put into a certification.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s consider levels of assurance, which is the aspect of “how *sure*
    are we it resists the relevant attacks.” On the one hand, you can read the product
    datasheet and see “side channel countermeasures,” and you can conclude that’s
    true based on the sheet, for a *low* level of assurance. Or, you can spend a year
    testing everything and mathematically proving lower bounds on the amount of leakage
    on your special protocol, and then you have a *high* level of assurance.
  prefs: []
  type: TYPE_NORMAL
- en: For CC, the level of assurance is defined as the *evaluation assurance level
    (EAL)* ; for smart cards, you’ll often see EAL5, EAL5+, EAL6, or EAL6+. We won’t
    go in details here, but just make sure you outsmart your friends by knowing EAL
    doesn’t mean “how secure it is.” Instead, it means “how sure am I of the security?”
    (And if you want to be supersmart, know that + means a few extra assurance requirements.)
  prefs: []
  type: TYPE_NORMAL
- en: Speaking of *labs*, the labs must prove they are capable of state-of-the-art
    attacks, which is verified by the standards bodies. Additionally, for CC, labs
    must participate and share new attacks in the *Joint Hardware Attack Subgroup
    (JHAS)*. The JHAS maintains the JIL document referred to earlier and updates it
    with new attacks and scores. This way, the standard does not have to prescribe
    what attacks must be performed, which is good, because hardware security is a
    constantly moving field. Because the attacks are in the JIL, it’s mainly up to
    labs to pick the relevant attacks for a product. This comes at the “cost” of variability
    in the labs’ approach. The issue with the latter is that vendors can pick labs
    with a track record of finding fewer issues, so labs essentially have competitive
    pressure to find less. It’s up to the standards body to make sure labs still meet
    the bar.
  prefs: []
  type: TYPE_NORMAL
- en: A similar approach to CC was adopted by *GlobalPlatform* for its *Trusted Execution
    Environment (TEE)* certification. The number of points needed is 21, lower than
    that of smart cards, which means that most hardware attacks are considered relevant
    only if they are trivially scalable, such as through software means. For example,
    if we use a fault injection or side-channel attack to dump a master key that allows
    us to hack any similar device, it is considered a relevant attack. If we have
    to do a side-channel attack for every device we want to break, and it takes a
    month for each device to get the key out, it is considered out of certification
    scope, simply because the attack rating will be more than 21.
  prefs: []
  type: TYPE_NORMAL
- en: Arm has a certification program called *Platform Security Architecture (PSA)*.
    The PSA has several levels of certification. Level 3 includes physical attacks
    such as side-channel and fault injection resistance. PSA in general is designed
    to target IoT and embedded platforms. As such, it may be more suited to general-purpose
    platforms, but if you are building products with general-purpose microcontrollers,
    the PSA level is the most likely one you will see such devices certified to. At
    lower levels, PSA also helps fix some of the basic problems we still see today,
    such as a debug interface that’s left open.
  prefs: []
  type: TYPE_NORMAL
- en: Another approach is that of *ISO 19790*, which is aligned with the US/Canadian
    standard *FIPS 140-3* that focuses on cryptographic algorithms and modules. The
    *Cryptographic Module Verification Program (CMVP)* validates whether modules satisfy
    the FIPS 140-3 requirements. The approach here is heavily biased toward *verification*—that
    is, making sure the product conforms to the security functional requirements.
    In our earlier words, it’s biased toward testing strength rather than bypassability.
    The standard prescribes the types of tests that are to be performed on products,
    which aides reproducibility among labs. The issue is that attacks evolve quickly,
    and “standard sets of tests defined by a government body” do not. FIPS 140-2 (the
    predecessor of FIPS 140-3) was published in 2001 and didn’t include a way to validate
    side-channel attacks. In other words, a product can be FIPS 140-2 certified, meaning
    that the AES engine performs proper AES encryptions, the keys are accessible only
    by authorized parties, and so on, but also that the keys could leak in 100 side-channel
    traces, because SCA is not in the testing scope for FIPS 140-2\. It took 18 years
    for its successor FIPS 140-3 to become effective, which does include side-channel
    testing in the form of the *test vector leakage assessment (TVLA)*. With TVLA
    the testing is precisely specified, but too much cleverness in filtering, and
    so on, on the side of the attacker is excluded. This means “passing” the testing
    doesn’t mean there is no side-channel leakage, only that the most straightforward
    of leakage was not detected.
  prefs: []
  type: TYPE_NORMAL
- en: Yet another approach to side-channel leakage certification is explored in *ISO
    17825*, which again takes some of the TVLA testing we described in Chapter 11
    and standardizes it. The eventual goal may be to achieve a “datasheet figure”
    for leakage. Like ISO 19790, the ISO 17825 testing isn’t designed to perform the
    same work as Common Criteria. With Common Criteria, the question is more broadly
    looking at attack resistance, while ISO 17825 attempts to provide a method of
    comparing specific side-channel leakage with automated methods. This means that
    ISO 17825 isn’t supposed to provide a general security metric across a range of
    attacks, but it’s useful when you are trying to understand the impact of enabling
    certain side-channel countermeasures. In other words, it measures countermeasure
    strength, not bypassability.
  prefs: []
  type: TYPE_NORMAL
- en: ISO/SAE 21434 is an automotive cybersecurity standard that is mandated in the
    EU per July 2022 for new vehicle types. It specifies *security engineering* requirements,
    and requires hardware attacks to be considered. This brings all of the attacks
    we learned about in this book into scope for the automotive space! When certifications
    hit marketing departments, you’ll find that “it’s secure!” is conflated with “it’s
    certified up to a certain assurance level against this limited set of threats.”
    This is understandable because the latter is a mouthful. However, it means it’s
    up to you to understand what the certification on a product actually means and
    how that fits your threat model. For example, if you’re trying to validate that
    a given system is generally resistant to various advanced attacks, someone offering
    ISO 17825 testing won’t have anywhere near the scope you require. But if you go
    only by the standard title (“Testing methods for the mitigation of noninvasive
    attack classes against cryptographic modules”) and a bit of marketing material
    the test provider gives you, you may easily be seduced into believing the value.
    Of course, there is a significant cost and effort difference as well between different
    certifications.
  prefs: []
  type: TYPE_NORMAL
- en: Certification has helped (at least) the smart card industry reach high levels
    of side-channel attack and fault injection resistance. No one will have an easy
    time breaking a modern, certified card. At the same time, it’s imperative to look
    at what’s behind a certification, as there are always limits to what the certification
    means.
  prefs: []
  type: TYPE_NORMAL
- en: Getting Better
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A number of different training courses are available on learning side-channel
    analysis and fault injection. When selecting a course, we recommend investigating
    the syllabus up front. This book covers the basics and theory, and if you sufficiently
    grasp them, it’d be better to select a course that focuses on the practical matters.
    The entire area of hardware hacking has people coming from all sorts of backgrounds.
    Some will be coming at it having done ten years of low-level chip design but never
    having dealt with finite field arithmetic. Others may have a PhD in theoretical
    mathematics but have never touched an oscilloscope before. So when you approach
    a topic, be sure to figure out the most valuable background for you. Whether you
    want more information on cryptography, signal processing, or the math behind DPA,
    find a course that focuses on those topics. Similarly, some training courses are
    more offense than defense focused, so find the one that matches your needs best.
    (Full disclosure: both authors’ companies run training courses.)'
  prefs: []
  type: TYPE_NORMAL
- en: You can also visit talks at conferences and learn from and discuss with people
    already in the field. You’ll find them at academic conferences, such as CHES,
    FDTC, COSADE, but also more (hardware) hacker-oriented conferences like Black
    Hat, Hardwear.io, DEF CON, CCC, and REcon. Definitely consider this an invite
    to say “Hi!” when you run into us at one of these events.
  prefs: []
  type: TYPE_NORMAL
- en: Attending a training course and attending events are also a great ways to learn
    new things outside of your background experience while sharing your unique background
    with others. You might have spent years working on the design of analog ICs, and
    we bet you will have some insight about how voltage spikes might be propagating
    inside a die that someone who has only worked with FPGAs won’t have.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we described a number of countermeasure strategies. Each countermeasure
    can be a building blocks of a “secure enough” system, and none of them will be
    individually sufficient. There’s also a number of caveats in building countermeasures,
    so make sure to verify they work as intended at each stage during development.
    We touched upon the professional side of verification through various certification
    strategies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we talked a bit about how to keep improving in this field. The best
    teacher is still practice. Start with simple microcontrollers. For example, try
    something clocked under 100 MHz that you fully control, so no OS throws interrupts
    and multitasking at you. Next, start building countermeasures and see how they
    hold up to your attacks, or better yet, get a friend to build their own countermeasure,
    and try to break each other’s. You’ll find that testing strength is easier than
    bypassability. Once you’re pretty comfortable attacking and defending, start complicating
    things: faster clocks, more complex CPUs, less control over the target application,
    less knowledge of the target application, and so on. Realize you are still learning;
    a new target may make you feel like a beginner again. Keep going at it; ultimately
    patience leads to luck, and luck leads to skill. Good luck on your journey!'
  prefs: []
  type: TYPE_NORMAL
