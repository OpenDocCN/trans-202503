<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><h2 class="h2" id="ch06"><span epub:type="pagebreak" id="page_181"/><strong><span class="big">6</span><br/>MACHINE LEARNING FEATURES</strong></h2>&#13;
<div class="image1"><img src="../images/common.jpg" alt="Image" width="189" height="189"/></div>&#13;
<p class="noindentsa">We’ve explored the two kinds of analysis required to understand an Android app: static and dynamic. We’ve also seen how a human security analyst can go back and forth between static and dynamic analysis to pinpoint the locations at which dangerous behavior occurs.</p>&#13;
<p class="indent">However, machine learning algorithms can’t perform the “back and forth” behavior of a human analyst. Because they can’t choose to explore one part of the code more than another part, they must associate a feature vector with each app, regardless of whether it is malicious or benign, and then use a previously trained model to make a prediction about it. This means they must determine in advance what to include in the feature vector.</p>&#13;
<p class="indent">In this chapter, we first describe how to turn static and dynamic sources of information into input for machine learning algorithms, enabling us to scale our malware detection efforts to millions of APKs. Then we explore four novel types of features that are harder for attackers to evade or reverse engineer, yet robust enough to detect malware with high accuracy. These detection techniques take into account the fact that malware developers often understand the static and dynamic analysis methods used by security experts and can apply this knowledge to evade detection.</p>&#13;
<h3 class="h3" id="ch06lev1"><span epub:type="pagebreak" id="page_182"/><strong>Static Features</strong></h3>&#13;
<p class="noindent">The first class of features we can associate with Android apps is based on a static analysis of the code. Unlike with data gleaned through human-based static analysis, software can easily extract their values. These features are immutable, in the sense that once we train a predictive model with a given set of features, we must stick with them when using the predictive model (however, new features can be added and old features removed when retraining).</p>&#13;
<p class="indent">There are several files and folders inside an APK whose properties and content we can turn into machine learning features. One source of features is the <em>AndroidManifest.xml</em> file that every APK contains in its root directory. As discussed in <a href="ch03.xhtml">Chapter 3</a>, the manifest file defines the structure and metadata of the Android application, including the package name and app version. It might also include XML nodes that describe the app’s basic behavior, as well as the permissions requested by the application. <a href="ch06.xhtml#ch6lis1">Listing 6-1</a> shows a snippet from the manifest file of a malware app, Fakebank <em>com.a</em> (v152, 0add), that references XML nodes.</p>&#13;
<pre>&lt;receiver android:label="@string/app2" &#13;
android:name="com.p004a.p005a.DeAdminReciver" &#13;
android:permission="android.permission.BIND_DEVICE_ADMIN" &#13;
android:description="@string/app2"&gt; &#13;
&lt;meta-data android:name="android.app.device_admin" &#13;
android:resource="@xml/an"/&gt;</pre>&#13;
<p class="list" id="ch6lis1"><em>Listing 6-1: XML nodes in the Android manifest associated with the Fakebank app</em></p>&#13;
<p class="indent">We might also find features in the Java source code folder. In Java apps, this folder is part of the original code and is not present in the compiled APK file. Other folders of interest are <em>Res</em>, <em>lib</em>, and <em>assets</em>. The <em>Res</em> folder contains all non-code resources used by the application, such as XML layouts and images. The <em>lib</em> folder is tricky, as its purpose changes after compilation: in Android source code, it’s often used to store common files, utility classes, and imported dependencies associated with applications, while in compiled APK files it stores native code files used by the application. The <em>assets</em> folder might include a wide range of files, such as text, XML, fonts, music, and video. Another source of features is the <em>build.gradle</em> file, which includes build-related configurations. It is present only during development and not included in the final APK.</p>&#13;
<p class="indent">We can define two versions of many features. A <em>binary</em> version of a feature is set to 0 or 1 depending upon whether the feature does or doesn’t occur. For instance, we might associate a feature with an API function call. If an app makes at least one call to that API in its code, we’d set the binary version of the feature to 1; otherwise, we’d set it to 0. A <em>statistical</em> version of the same feature, on the other hand, might reflect the number of calls the app makes to the API. Alternatively, it might record the number of times the API function is invoked with certain inputs and return a statistical quantity, such as the mean, median, or standard deviation of the results. The <span epub:type="pagebreak" id="page_183"/>following features commonly appear in the literature on machine learning–based malware detection:</p>&#13;
<p class="noindentt"><strong>Permissions</strong></p>&#13;
<div class="bq1">&#13;
<p class="noindent">We can design a number of features related to the permissions that an application requests. We might, for example, create a binary feature for each permission. We could also define statistical features corresponding to the number of normal, signature, and dangerous permissions requested. According to the official Android developer site, dangerous permissions are those that either involve the private data of users or could possibly affect such private user data. For instance, we’ve already seen that the <em>com.bp.statis.bloodsugar</em> malware discussed in <a href="ch03.xhtml">Chapter 3</a> requests the <span class="literal">READ_CONTACTS</span> permission even though there is little reason to believe that a blood sugar monitoring app needs access to a user’s contacts.</p>&#13;
</div>&#13;
<p class="noindentt"><strong>Activities</strong></p>&#13;
<div class="bq1">&#13;
<p class="noindent">As discussed in <a href="ch03.xhtml">Chapter 3</a>, activities implement the visual interface of an Android app and are declared in the manifest file. We could create a set of binary features to indicate whether each activity is used or not. The total number of activities is also a potential feature.</p>&#13;
</div>&#13;
<p class="noindentt"><strong>Services</strong></p>&#13;
<div class="bq1">&#13;
<p class="noindent">Apps use services to implement long-running background operations that facilitate interactions with the system. As in the case of activities, we can define binary features indicating whether each service is used or not. Moreover, we can define simple counts and statistical features. For instance, in the case of the <em>com.bp.statis.bloodsugar.PE</em> service discussed in <a href="ch03.xhtml">Chapter 3</a>, we might set this value to 1, as there is little reason for a blood sugar app to listen to incoming notifications from all of the apps in the system.</p>&#13;
</div>&#13;
<p class="noindentt"><strong>Content providers</strong></p>&#13;
<div class="bq1">&#13;
<p class="noindent">A content provider encapsulates data and gives it to other applications. For each content provider, we might have a feature set to 0 if it doesn’t exist and 1 if it does exist in the app. We can also create a feature for the number of content providers the app uses.</p>&#13;
</div>&#13;
<p class="noindentt"><strong>Broadcast receivers</strong></p>&#13;
<div class="bq1">&#13;
<p class="noindent">The broadcast receiver component of an application enables it to receive broadcast messages from the system or other applications. As in the preceding cases, we can create binary features, counts, and statistical features for these receivers. However, while it is easy to find broadcast receivers declared in the manifest file, it is not always easy to find those declared at runtime, especially as they may be part of encrypted or obfuscated code. Moreover, some apps might want to register a <span class="literal">RECEIVE_SMS</span> receiver, which enables them to intercept incoming SMS traffic (for example, one-time passwords or alerts of suspicious activity).</p>&#13;
</div>&#13;
<p class="noindentt"><span epub:type="pagebreak" id="page_184"/><strong>Intent filters</strong></p>&#13;
<div class="bq1">&#13;
<p class="noindent">Activities, services, and broadcast receivers can use intent filters to specify the kinds of operations to which they will respond. In the case of broadcast receivers, intents specify the types of broadcasts that they can handle. As in the preceding cases, we can define and extract binary features and statistical features for intents.</p>&#13;
</div>&#13;
<p class="noindentt"><strong>API calls</strong></p>&#13;
<div class="bq1">&#13;
<p class="noindent">The Android platform provides a set of API packages that developers can use to build applications. We can create binary features for each API package (based on whether it is called or not) as well as for each class within those packages (based on whether the class is called or not). In addition, numeric features for an API package might track the number of times the app calls a class within a package or a function within a class. We’ll provide a detailed introduction to API features later in this chapter, as we can use them to generate more advanced features.</p>&#13;
</div>&#13;
<p class="noindentt"><strong>Network elements</strong></p>&#13;
<div class="bq1">&#13;
<p class="noindent">An Android application’s source code may contain numerous network elements, such as IP addresses, URLs, and hostnames. We can collect these elements to generate binary features and statistical features (for example, the number of hostnames listed in the file). We may also want to use the number of external URLs referenced in the code as a feature.</p>&#13;
</div>&#13;
<p class="indent">The malware author might try to make static analysis difficult through a variety of instruments. These could include using unintelligible names for variables, encrypting parts of the code, and using other obfuscation methods such as reflection (see <a href="ch03.xhtml">Chapter 3</a>). We can also define static features to describe whether such phenomena exist in the app code, as well as their frequency of occurrence.</p>&#13;
<h3 class="h3" id="ch06lev2"><strong>Dynamic Features</strong></h3>&#13;
<p class="noindent">We can turn the results of our dynamic analysis into machine learning features, too. As covered in <a href="ch04.xhtml">Chapter 4</a>, dynamic analysis focuses on observed runtime properties and behavior of applications. Consequently, the features derived from it describe events that were actually observed rather than the more speculative features derived from the static analysis of code.</p>&#13;
<p class="indent">To generate many of these features, we must feed some set of inputs to the app, such as interactions that the app has with the user (an example is the <span class="literal">monkey</span> command discussed in <a href="ch04.xhtml">Chapter 4</a>). We might run the app using the first input and generate some results, then run it with the second input and generate more results, continuing the process until we’ve exhausted all inputs in the set. We can also extract features by analyzing the network traffic generated when the app is run through programs such as tcpdump and Wireshark.</p>&#13;
<p class="indent">The following dynamic features for the Android platform have been widely discussed in the literature:</p>&#13;
<p class="noindentt"><span epub:type="pagebreak" id="page_185"/><strong>Services</strong></p>&#13;
<div class="bq1">&#13;
<p class="noindent">We can generate dynamic features to record each started service. These may be binary (based on whether the app starts that service or not) or numeric (for example, the average number of calls to the service across the set of inputs). The total number of services started can also be a feature. Additionally, we could associate a sequence of services with a feature by recording whether the app ever invokes that sequence (a binary feature) or how many times an app invokes it on average across the set of inputs (a numeric feature).</p>&#13;
</div>&#13;
<p class="noindentt"><strong>The <span class="literal">DexClassLoader</span></strong></p>&#13;
<div class="bq1">&#13;
<p class="noindent">This is a standard Android API used to load classes from <em>.jar</em> and <em>.apk</em>  files that contain a <em>classes.dex</em> file. Malicious apps frequently use this API to evade static analysis because it lets them execute code that didn’t come from the application’s source code (one example is the Xenomorph malware family discussed in <a href="ch04.xhtml">Chapter 4</a>). We could create a feature that is set to 1 if the app calls <span class="literal">DexClassLoader</span> and to 0 otherwise. Additional features could be defined based on the count of calls, <em>n</em>-gram sequences, and other statistics.</p>&#13;
</div>&#13;
<p class="noindentt"><strong>Permissions</strong></p>&#13;
<div class="bq1">&#13;
<p class="noindent">We can create binary features that record whether the app invokes an API that requires some permission, even if the permission doesn’t appear in the application code itself. Although Android apps must explicitly declare any permissions that they request within the manifest file, they might try to circumvent this requirement by acquiring permissions in different ways. One strategy is to use a covert channel, such as the communication between multiple APKs, to share information. This behavior poses a challenge to dynamic analysts, as their lab setups must be able to run multiple interacting apps at the same time. As in the previous cases, we can also generate statistical features and <em>n</em>-gram features based on permissions. For instance, in the Xenomorph malware, we would record the fact that it invoked the accessibility API by setting that value to 1.</p>&#13;
</div>&#13;
<p class="noindentt"><strong>Data leaks</strong></p>&#13;
<div class="bq1">&#13;
<p class="noindent">An app might sometimes leak a user’s personal data, be it accidentally, because the app is poorly coded, or intentionally, in an attempt to steal the data. We can generate features that reflect the leaked content.</p>&#13;
</div>&#13;
<p class="noindentt"><strong>Use of cryptography</strong></p>&#13;
<div class="bq1">&#13;
<p class="noindent">We can define a feature that tracks whether an app performs any cryptographic operations. When an APK executes encryption operations (for example, to store encrypted files), the sandbox used to run it can track and record this. We might set a binary feature to 1 if the app generates any encrypted files during execution and set it to 0 otherwise. We see this behavior in the Xenomorph app; see the <span class="literal">encryptMessage</span> function in <a href="ch04.xhtml#ch4lis5">Listing 4-5</a>, which the app could invoke zero or more times during its execution in a sandbox environment.</p>&#13;
</div>&#13;
<p class="noindentt"><span epub:type="pagebreak" id="page_186"/><strong>Network activities</strong></p>&#13;
<div class="bq1">&#13;
<p class="noindent">We can use a set of features to keep track of operations that open or close network sockets by recording the destination host. We might also create features based on the data received from the network, as well as the source of the data and any data that the application sends to others on the network.</p>&#13;
</div>&#13;
<p class="noindentt"><strong>Sending SMS messages</strong></p>&#13;
<div class="bq1">&#13;
<p class="noindent">When an app sends text messages during its execution phase, we can record the identity of the recipient and the message’s content to use as features. We can also count the total number of messages sent during the execution or define a binary feature that we set to 1 if the app sends any messages at all.</p>&#13;
</div>&#13;
<p class="noindentt"><strong>Phone calls</strong></p>&#13;
<div class="bq1">&#13;
<p class="noindent">Malicious Android apps sometimes make phone calls (for example, to premium rate numbers). In such cases, we can define features to store the numbers called or use a binary feature to record the fact that some external numbers were called.</p>&#13;
</div>&#13;
<p class="noindentt"><strong>Answered intents</strong></p>&#13;
<div class="bq1">&#13;
<p class="noindent">We can capture the intents to which the application responds during its execution and record these as dynamic features.</p>&#13;
</div>&#13;
<p class="noindentt"><strong>Files</strong></p>&#13;
<div class="bq1">&#13;
<p class="noindent">We might create features to record the names of any library files that the app uses. Also, when the application reads or writes to specific files, we can capture the filename and the content, then generate features based on these.</p>&#13;
</div>&#13;
<h3 class="h3" id="ch06lev3"><strong>Method Call Features (A Weak Tactic)</strong></h3>&#13;
<p class="noindent">To go beyond basic static and dynamic data, some researchers have turned to API method calls as potential features. The Android platform provides a set of API packages that developers can use to access a host of valuable functionality. For example, the <em>android.accessibilityservice</em> package can help users with disabilities interact with Android devices. However, malware developers can also use it, and they widely abuse it.</p>&#13;
<p class="indent">Each API package contains a number of classes, and each class has its own methods that we can use to define new features for our models. To create features using the 171 API packages in Android API 23, for example, we might build a 171-dimensional feature vector for each Android app to capture the frequency with which that app calls the methods from each package. For instance, if some API package includes 40 methods belonging to different classes and an app calls each of them twice, the corresponding feature value would be 40 × 2, or 80.</p>&#13;
<p class="indent">These API feature values can vary greatly. For instance, consider the 171 API feature values associated with a goodware sample called <em>ESPN 6.0.4</em>. The largest of these feature values is 161,698, while the smallest is 0, producing a standard deviation of 10,488.26. By contrast, another goodware <span epub:type="pagebreak" id="page_187"/>sample, <em>com.hancom.office.editor</em> (v1, 75d1), has 6 as its largest API feature value and 0 as its smallest, with a standard deviation of only 0.61. You might have the instinct to normalize feature values to account for this difference, but normalizing isn’t necessary because good machine learning algorithms will automatically determine which values of a given feature help create good separators between malware and goodware.</p>&#13;
<p class="indent">While you’ll find these API-based features used in the literature, malware developers can evade them easily. Zhengcuan Cai and Roland Yap studied 57 Android antivirus tools in their 2016 paper “Inferring the Detection Logic and Evaluating the Effectiveness of Android Anti-Virus Apps.” They found that malicious hackers can easily uncover the detection logic in antivirus apps that use static analysis alone, enabling them to evade detection. For instance, in this case the developers could include a bunch of dummy calls to API features in order to change their app’s 171-dimensional feature vector. Likewise, obfuscation methods such as reflection and dynamic code loading can lower an app’s feature counts. The feature counts of particularly well-hidden method calls might even drop to zero if static analysis doesn’t find those calls (which would be the case if, for instance, the calls were in an encrypted section of code). For completeness, machine learning models should include both static and dynamic features of API calls.</p>&#13;
<p class="indent">By contrast, advanced features, based on techniques like triadic suspicion graphs, landmarks, feature clustering, and correlation graphs, are highly effective in identifying malicious Android apps. Experiments have shown that such features are harder for malicious hackers to evade, in part because it is hard for them to determine exactly how these features are used in a detection system. The remainder of this chapter introduces these advanced features.</p>&#13;
<h3 class="h3" id="ch06lev4"><strong>Triadic Suspicion Graph Features</strong></h3>&#13;
<p class="noindent">Rather than using API method calls on their own, we can generate a more robust group of features derived from a special class of graphs called <em>triadic suspicion graphs (TSGs)</em>. Essentially, a TSG aims to understand the differences between the use of an API package by goodware on the one hand and different types of malicious apps on the other hand. <a href="ch06.xhtml#ch6fig1">Figure 6-1</a> is an illustration of a TSG that compares goodware to banking trojans. We’ll walk through its elements in the paragraphs that follow.</p>&#13;
<p class="indent">A TSG is made up of vertices connected by edges. In this context, the TSG contains three kinds of vertices: the complete set of API package calls defined in the Android API, the sampled goodware, and the sampled malware, randomly drawn from some larger collections of goodware and malware, respectively. The TSG’s edges are defined as follows:</p>&#13;
<ol>&#13;
<li class="noindent">For each goodware <em>g</em>  and each API package call <em>a</em>, if <em>g</em>  calls a method from <em>a</em>  at least once, there is an edge from <em>g</em>  to <em>a</em>.</li>&#13;
<li class="noindent">For each pair of API package calls <em>a</em><sub>1</sub> and <em>a</em><sub>2</sub>, if <em>a</em><sub>1</sub> calls any method from <em>a</em><sub>2</sub>, there is an edge from <em>a</em><sub>1</sub> to <em>a</em><sub>2</sub>.</li>&#13;
<li class="noindent"><span epub:type="pagebreak" id="page_188"/>For each malware <em>b</em>  and each API package call <em>a</em>, if <em>b</em> calls any method from <em>a</em>  at least once, there is an edge from <em>b</em> to <em>a</em>.</li>&#13;
</ol>&#13;
<div class="image"><img id="ch6fig1" src="../images/ch06fig01.jpg" alt="Image" width="685" height="262"/></div>&#13;
<p class="figcap"><em>Figure 6-1: A partial TSG containing three goodware samples</em></p>&#13;
<p class="indent">The goodware and malware collections don’t need to be fixed. An analyst might use one sampling in one week, switch to another in the next week, and keep doing so regularly in order to present a moving target. Varying the sets changes the attack surface and makes it harder for an adversary to guess the precise nature of the defense.</p>&#13;
<p class="indent">We also suggest keeping the size of the sets relatively small, and varying it as well. For example, if we had access to 1 million goodware samples and 10,000 malware samples, we might select only, say, 1,000 samples for each of the groups in the first week, 1,322 in the next week, 1,127 in the third week, and so forth. Frequently modifying the sample sizes is another way to keep the attacker in the dark about the nature of the defenses being used; however, the number of samples in the two sets should be approximately the same.</p>&#13;
<p class="indent">Once we’ve determined the vertices and edges in a TSG, we weight the edges using a weight function. In this context, the weights reflect the number of times an app calls a corresponding API package’s methods. For any edge from a goodware or malware app <em>v</em> to an API package <em>a</em>, we use <em>f</em>(<em>v</em>, <em>a</em>) to denote the number of times <em>v</em> calls methods from <em>a</em>. The following equations demonstrate five plausible definitions of a weight function <em>w</em>. Functions <em>w</em><sub>1</sub>, <em>w</em><sub>2</sub>, and <em>w</em><sub>3</sub>, respectively, represent linear, quadratic, and cubic relationships between the API package call frequency and the edge weight, while <em>w</em><sub>4</sub> and <em>w</em><sub>5</sub> capture other possible nonlinear relationships:</p>&#13;
<div class="image1"><img src="../images/math188.jpg" alt="image" width="205" height="156"/></div>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_189"/>Having different definitions is useful because most machine learning algorithms are very sensitive to the input features and can’t always correctly infer the most accurate nonlinear relationships between data points using the modeling framework alone.</p>&#13;
<p class="indent">We set the weights of edges between pairs of API package calls to the same default value, 1. This is because we are more interested in whether a specific edge exists than in the frequency with which one API package calls another within the Android API, as attackers can’t control these relationships.</p>&#13;
<p class="indent">You can see the weighted edges in <a href="ch06.xhtml#ch6fig1">Figure 6-1</a>, which uses the function <em>w</em><sub>1</sub>, as well as directional arrows to show the calling relationships among pairs of API packages. Now you can observe that none of the three goodware samples call the API package <span class="literal">android.app.admin</span>, while two of the three banking trojans call it a few times. These sorts of patterns might help us identify malicious apps.</p>&#13;
<h4 class="h4" id="ch06lev1sec1"><strong><em>Suspicion Scores</em></strong></h4>&#13;
<p class="noindent">With the TSG defined, we can now calculate the <em>suspicion score</em> of an API package. In short, we rank an API package that is frequently invoked by malware but not by goodware as more suspicious than one that is frequently invoked by goodware but not by malware. Suspicion scores alone aren’t enough to predict whether an Android app is malicious or not, but they do generate a set of features that might be able to provide good predictive performance. Moreover, as the malware developer won’t know the reference sets and weight functions used to create the TSGs, they can’t easily evade detection frameworks that use them.</p>&#13;
<p class="indent">We define 12 possible suspicion scoring functions, <em>sus</em><sub>1</sub> through <em>sus</em><sub>12</sub>. Having multiple candidate functions ensures that we are less prone to over-fitting a predefined model. When we supply these scores and other features as input, machine learning techniques can tell us which suspicion scoring function is best able to differentiate benign apps from malicious ones. You might notice that the function definitions, shown next, are closely related to the weight functions <em>w</em><sub>1</sub> through <em>w</em><sub>5</sub>:</p>&#13;
<div class="image1"><img src="../images/math189.jpg" alt="image" width="343" height="245"/></div>&#13;
<div class="image1"><span epub:type="pagebreak" id="page_190"/><img src="../images/math190.jpg" alt="image" width="471" height="1064"/></div>&#13;
<div class="image1"><span epub:type="pagebreak" id="page_191"/><img src="../images/math191-01.jpg" alt="image" width="551" height="508"/></div>&#13;
<p class="noindent">These suspicion score functions all make use of an indicator function <em>I</em>(<em>v</em><sub>1</sub>, <em>v</em><sub>2</sub>) to denote the existence of an edge from vertex <em>v</em><sub>1</sub> to <em>v</em><sub>2</sub>, where <em>v</em><sub>1</sub>, <em>v</em><sub>2</sub> ∈ <img class="middle" src="../images/math191-03.jpg" alt="image" width="18" height="19"/> ∪ <img class="middle" src="../images/math191-04.jpg" alt="image" width="22" height="19"/> ∪ <img class="middle" src="../images/math191-05.jpg" alt="image" width="27" height="18"/>. In other words, if it is the case that <em>f</em>(<em>v</em><sub>1</sub>, <em>v</em><sub>2</sub>) is greater than 0, then <em>I</em>(<em>v</em><sub>1</sub>, <em>v</em><sub>2</sub>) equals 1; otherwise, it is 0. (In fact, we could treat the <em>I</em>(<em>v</em>, <em>a</em>) function for edges from apps to API packages as another kind of weight function.) We use <em>n</em> to denote the number of malware samples and <em>m</em> to denote the number of goodware samples.</p>&#13;
<p class="indent">For example, according to the first function, <em>sus</em><sub>1</sub>, if the API package <em>a</em><sub><em>j</em></sub> is called by 100 malicious apps <em>b</em> and 10 goodware apps <em>g</em> from our samples, we reasonably consider apps that invoke this API package to be more suspicious than ones that do not. The definition in <em>sus</em><sub>7</sub> is another way of capturing the same intuition: that an API function that is more extensively called by malicious apps than by benign ones will have a higher suspicion score. Equations <em>sus</em><sub>7</sub> through <em>sus</em><sub>12</sub> make similar assumptions to <em>sus</em><sub>1</sub> through <em>sus</em><sub>6</sub> except that they evaluate the suspicion score of one API package with respect to all API packages rather than by itself.</p>&#13;
<h4 class="h4" id="ch06lev1sec2"><strong><em>The Suspicion Rank</em></strong></h4>&#13;
<p class="noindent">Suspicion scores label a single Android API package call by looking at how malware and goodware each call that package. However, a package might itself make calls to other packages within the Android API. If a package <em>P</em>1 makes lots of calls to another package <em>Q</em> that has a high suspicion score, we should rank the first package as more suspicious than a package <em>P</em>2 that makes no calls to packages that have a high suspicion score.</p>&#13;
<p class="indent">The situation is a bit like an individual making lots of calls to a drug dealer. Even if the individual isn’t deemed suspicious in their own right, the <span epub:type="pagebreak" id="page_192"/>fact that they’re in regular contact with a drug dealer makes them so. This is precisely the intuition behind Google Search’s famous PageRank algorithm, which captures the importance of a web page by considering the importance of the web pages that link to it.</p>&#13;
<p class="indent">In fact, we can combine our suspicion scores with PageRank to define a family of suspicion ranking functions that capture these intuitions. PageRank calculates the importance of web pages using the following formula:</p>&#13;
<div class="image1"><img src="../images/math192-01.jpg" alt="image" width="337" height="60"/></div>&#13;
<p class="noindent">Here, <em>E</em> is the set of edges in the web; <em>N</em> is the total number of nodes, or vertices, in the web; <em>d</em> ∈ [0, 1], called the damping factor, is usually set to 0.85; and <em>out</em>(<em>u</em>) is the <em>out-degree</em> of node <em>u</em>, or the number of edges that leave it. The <img class="middle" src="../images/math192-02.jpg" alt="image" width="27" height="28"/> expression captures the probability that a user will reach web page <em>v</em> by explicitly entering its address into a browser, while the remaining part of the expression is intended to capture the probability of a user reaching page <em>v</em> by following links.</p>&#13;
<p class="indent">In the following, we define the suspicion rank for an Android API package <em>a</em> with respect to a fixed suspicion scoring function, <em>sus</em>. We could use any of the functions described earlier in this chapter, or an entirely new one, as long as it associates a suspicion score with each function in the Android API:</p>&#13;
<div class="image1"><img src="../images/math192-03.jpg" alt="image" width="462" height="62"/></div>&#13;
<p class="indent">The parameter <em>δ</em> ∈ [0, 1] is a damping factor similar to PageRank’s <em>d</em>. In practice, we set it to 0.85, as is usually done with PageRank. The value <em>a</em>′ is any package invoked by the package <em>a</em>, and the <em>out</em>(<em>a</em>′) value is the out-degree of the node in the TSG corresponding to <em>a</em>′. In other words, it represents the number of API packages invoked by <em>a</em>′.</p>&#13;
<p class="indent">Readers might have noticed that the definition of the suspicion rank mainly relies on a small portion of the TSG—namely, the vertices representing API packages and the edges between them. As a result, this structure is independent of the choice of apps in the goodware and malware sets, and adversaries can’t manipulate it, because it (that is, the Android API) is publicly disclosed in the Android code and documentation. This approach differs from the function call graphs described in previous works, which usually depend on the sequence of operations within specific individual apps and so lack randomness, a key element in keeping malware developers guessing. We list some of these alternative approaches in “<a href="ch06.xhtml#ch06lev8">Further Reading</a>” on <a href="ch06.xhtml#ch06lev8">page 202</a>.</p>&#13;
<h4 class="h4" id="ch06lev1sec3"><strong><em>TSG Features</em></strong></h4>&#13;
<p class="noindent">The preceding two sections define ways to calculate suspicion scores and suspicion ranks for API packages in a given TSG. In total, we have 24 kinds <span epub:type="pagebreak" id="page_193"/>of suspicion-based scores associated with each API package. Researchers can add new ones if they wish. Next, we must use these suspicion-based scores to generate what we call <em>TSG features</em> for Android apps. These features capture the package call behavior of all apps, meaning an app doesn’t have to be in either the malware or the goodware sample set to have TSG features.</p>&#13;
<p class="indent">To generate these features, we first rank the API packages in descending order according to their suspicion score and suspicion rank results. Theoretically, the higher the rank of an API package, the more suspicious it is. However, we will have noise, perhaps stemming from the choice of sample applications. Therefore, instead of directly using the ranked package list, we apply a window-based segmentation to it before deriving TSG features.</p>&#13;
<p class="indent">The basic idea of <em>window-based</em> segmentation is to use an integer <em>W</em> that is greater than 1 to segment the list into a number of buckets, starting from the beginning of the list. As shown in <a href="ch06.xhtml#ch6fig2">Figure 6-2</a>, each bucket (except possibly the last one) contains <em>W</em> API packages with similar suspicion-based scores or ranks.</p>&#13;
<div class="image"><img id="ch6fig2" src="../images/ch06fig02.jpg" alt="Image" width="679" height="115"/></div>&#13;
<p class="figcap"><em>Figure 6-2:  A window-based API package ranking by descending suspicion scores and ranks</em></p>&#13;
<p class="indent">Suppose API packages <em>a</em><sub>1</sub> through <em>a</em><sub><em>W</em></sub>&#13;
are in the same bucket, and suppose that the corresponding API feature values of an app are <em>f</em><sub>1</sub> through <em>f</em><sub><em>W</em></sub>. For each bucket, we can calculate a TSG feature via one of the following six methods:</p>&#13;
<div class="bq1">&#13;
<p class="noindent"><strong>Binary value</strong> Does this app call any API packages in this bucket? If so, this binary feature is 1; otherwise, it is 0.</p>&#13;
<p class="noindent"><strong>Number of API packages</strong> How many API packages in this bucket does the app call? The feature value is an integer <img class="middle" src="../images/math193-01.jpg" alt="image" width="80" height="31"/>, where the function <em>I</em>(<em>f</em><sub><em>j</em></sub>) = 1 is <em>f</em><sub><em>j</em></sub> &gt; 0; otherwise, it equals 0.</p>&#13;
<p class="noindent"><strong>Maximum frequency value</strong> Of the call frequencies from the app to all API packages in the bucket, what is the maximum value? The feature value is an integer <img class="middle" src="../images/math193-02.jpg" alt="image" width="75" height="31"/>.</p>&#13;
<p class="noindent"><strong>Median frequency value</strong> Among the call frequencies, what is the median value? The feature value is an integer median <img class="middle" src="../images/math193-03.jpg" alt="image" width="37" height="32"/>.</p>&#13;
<p class="noindent"><strong>Sum of frequencies</strong> How many times in total does this app call API packages in this bucket? The feature value is an integer <img class="middle" src="../images/math193-04.jpg" alt="image" width="58" height="34"/>.</p>&#13;
<p class="noindent"><strong>Weighted sum</strong> Based on the frequency sum, what would the value be if we took the suspicion score given by function <em>ρ</em>  as the corresponding weight? This feature is a real value <img class="middle" src="../images/math193-05.jpg" alt="image" width="74" height="34"/>, where <em>ρ</em><sub><em>j</em></sub> stands for the suspicion score of API package <em>a</em><sub><em>j</em></sub>.</p>&#13;
</div>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_194"/>To illustrate how these features work, consider the small dataset with three banking trojans and three goodware samples we showed earlier in this chapter. Suppose <a href="ch06.xhtml#ch6tab1">Table 6-1</a> shows the frequency with which the malware sample Regon calls the four API packages.</p>&#13;
<p id="ch6tab1" class="tabcap"><strong>Table 6-1:</strong> Frequency of API Package Calls by Regon</p>&#13;
<table class="all">&#13;
<colgroup>&#13;
<col style="width:20%"/>&#13;
<col style="width:15%"/>&#13;
<col style="width:15%"/>&#13;
<col style="width:25%"/>&#13;
<col style="width:25%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr>&#13;
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"> </p></th>&#13;
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"><strong><em>android.view</em></strong></p></th>&#13;
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"><strong><em>java.net</em></strong></p></th>&#13;
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"><strong><em>android.app.admin</em></strong></p></th>&#13;
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"><strong><em>java.util</em></strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="noindent-tab"><strong>Frequency</strong></p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">35</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">0</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">1</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">112</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">If we use <em>sus</em><sub>1</sub> as our suspicion scoring function, we could sort the packages based on their suspicion scores, in descending order, as shown in <a href="ch06.xhtml#ch6tab2">Table 6-2</a>.</p>&#13;
<p id="ch6tab2" class="tabcap"><strong>Table 6-2:</strong> Suspicion Scores of the Packages Called by Regon</p>&#13;
<table class="all">&#13;
<colgroup>&#13;
<col style="width:20%"/>&#13;
<col style="width:25%"/>&#13;
<col style="width:20%"/>&#13;
<col style="width:20%"/>&#13;
<col style="width:15%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr>&#13;
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"> </p></th>&#13;
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"><strong><em>android.app.admin</em></strong></p></th>&#13;
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"><strong><em>android.view</em></strong></p></th>&#13;
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"><strong><em>java.util</em></strong></p></th>&#13;
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"><strong><em>java.net</em></strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="noindent-tab"><strong>Suspicion score</strong></p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">1</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">0.5</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">0.5</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">0.25</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">Suppose we now use <em>W</em> = 2 as the window size. In this case, there are two buckets, the first containing <em>android.app.admin</em> and <em>android.view</em> and the second containing <em>java.util</em> and <em>java.net</em>. We derive the following feature values for Regon from the first bucket: a binary value of 1, an API package number of 2, a sum of frequencies of 36, a maximum frequency value of 35, a median frequency value of 18, and a weighted sum of 1 × 1 + 0.5 × 35, or 18.5. The values of the features generated by the second bucket are 1, 1, 112, 112, 56, and 56, respectively.</p>&#13;
<p class="indent">Now suppose we repeat this process using both the suspicion scoring function <em>sus</em><sub>1</sub> and the suspicion ranking formula. <a href="ch06.xhtml#ch6tab3">Table 6-3</a> shows the resulting suspicion ranks after sorting.</p>&#13;
<p id="ch6tab3" class="tabcap"><strong>Table 6-3:</strong> Suspicion Ranks for the Packages Called by Regon</p>&#13;
<table class="all">&#13;
<colgroup>&#13;
<col style="width:20%"/>&#13;
<col style="width:20%"/>&#13;
<col style="width:20%"/>&#13;
<col style="width:20%"/>&#13;
<col style="width:20%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr>&#13;
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"> </p></th>&#13;
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"><strong><em>java.util</em></strong></p></th>&#13;
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"><strong><em>java.net</em></strong></p></th>&#13;
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"><strong><em>android.view</em></strong></p></th>&#13;
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"><strong><em>android.app.admin</em></strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="noindent-tab"><strong>Suspicion rank</strong></p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">0.1025</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">0.0811</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">0.0375</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">0.0375</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">These suspicion ranks generate the following feature values for Regon from the first bucket: a binary value of 1, an API package number of 1, a sum of frequencies of 112, a maximum frequency of 112, a median frequency of 56, and a weighted sum of 0.1025 × 112 + 0.0811 × 0, or 11.48. For the API calls in the second bucket, Regon has corresponding feature values 1, 2, 36, 35, 18, and 1.35.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_195"/>In this example, we generated TSG features for Regon based on a subset of Android API packages and a part of the complete TSG. In a real implementation, however, we might use all 171 API packages, 24 different suspicion scoring functions, and 6 methods for computing TSG features for each function. As a result, if we use a <em>W</em> of 10, we could generate 2,592 TSG features for each app.</p>&#13;
<p class="indent">In addition, because we control the <em>W</em> parameter, we can vary it in several ways. For instance, if we have four API packages with the suspicion scores 0.9, 0.3, 0.29, and 0.2, we could divide them into two evenly sized buckets, (0.9, 0.3) and (0.29, 0.2). Alternatively, we could group similar scores together by using a variable window size to segment them into two buckets, (0.9) and (0.3, 0.29, 0.2). Using window size in this way has an advantage: it introduces yet another complication for the adversary. If an attacker changed the number of calls made in a piece of malware to classes in one or two Android API packages, it wouldn’t have a huge impact on how features were derived, because packages that have similar features would be merged, reducing the effects of any single feature. This varying window size could have the potential negative effect of lowering the predictive performance of the resulting classifiers, but it turns out, as subsequent chapters will show, that this is not a major problem.</p>&#13;
<p class="indent">To read more about the experiments that demonstrate the difficulty of bypassing these features, see “DBank: Predictive Behavioral Analysis of Recent Android Banking Trojans” by Chongyang Bai et al. and “Android Malware Detection via (Somewhat) Robust Irreversible Feature Transformations” by Qian Han et al.</p>&#13;
<h3 class="h3" id="ch06lev5"><strong>Landmark-Based Features</strong></h3>&#13;
<p class="noindent">Another way to generate features for Android apps that attackers can’t easily evade relies on the concept of landmarks. Suppose you are considering buying a house. Your estimate of a fair price for the house will likely depend upon several factors, one of which might be the sales prices of certain other houses (for example, those of a similar size and age in the same area). We call these reference houses <em>landmarks</em>.</p>&#13;
<p class="indent">We can adopt the idea of using landmarks to define a new feature space for Android apps. Say there is a set of Android apps that includes both benign and malicious apps, and that each app has some feature vector. We can think of that feature vector as a point in the app feature space, just as we could characterize a house as a point in a housing feature space. When considering buying a house, we compare the house with similar houses; we can do the same with apps when trying to determine whether they are malicious or benign.</p>&#13;
<h4 class="h4" id="ch06lev1sec4"><strong><em>Selecting Landmarks</em></strong></h4>&#13;
<p class="noindent">To use the landmark approach, we first select a subset of the app samples and set them as landmarks. Then we define new features for each app in <span epub:type="pagebreak" id="page_196"/>the dataset by comparing them with each landmark. We suggest keeping the size of the landmark set reasonably small. For example, if there are 1 million samples in the total set of apps, we might select 1,000 landmarks. That way, adversaries will have trouble guessing the selected landmarks, making it even harder to guess the landmark-based features.</p>&#13;
<p class="indent">We propose three methods for selecting the set of landmarks from the sample set. The first, a naive approach, is to randomly select them. Another method is <em>clustering-based selection</em>, in which the apps are first clustered into groups. There are many well-studied algorithms for clustering, such as <em>k</em>-means clustering, <em>k</em>-median clustering, mean shift clustering, density-based spatial clustering of applications with noise (DBSCAN), expectation maximization clustering using Gaussian mixture models, and agglomerative hierarchical clustering. Each clustering algorithm has its own advantages and disadvantages. They may also perform differently due to the characteristics of the dataset.</p>&#13;
<p class="indent">With this approach, after clustering the apps into groups, we select one app from each group as a landmark. The basic idea is that when we group all the apps into clusters, similar apps end up in the same cluster; we can then pick one representative app from each of the clusters. Returning to our housing analogy, the houses in a cluster might have similar neighborhoods, local schools, square footages, prices, and numbers of bedrooms. When deciding whether a house is good or not, we might use one representative from each cluster as a landmark. Once we have our clusters, we can select a representative from each group in many ways. For instance, we could randomly select an app from the cluster. Alternatively, we could compute the sum of the distances of each app in the cluster to each of the other apps in the cluster, then use the app that has the smallest sum—the most “central” app in the cluster—as the landmark. (The distance between two apps can be calculated by finding the distance between their feature vectors, using a metric such as Euclidean distance or cosine distance.)</p>&#13;
<p class="indent">Because there are at least 6 clustering algorithms we can use and at least 2 ways of selecting a landmark app from each cluster, there are at least 12 ways of performing clustering-based landmark selection, even when disregarding the variability in hyperparameters that some of the clustering techniques use internally. In fact, there are many more ways of performing clustering-based landmark, e.g. by varying <em>k</em> in the <em>k</em>-means clustering and <em>k</em>-median clustering algorithms.</p>&#13;
<p class="indent">The third method, <em>maximum distance heuristic selection</em>, provides an algorithm for selecting landmarks that are scattered across the basic feature space. As input, it accepts the set of apps <em>D</em> and the number of landmarks <em>N</em><sub><em>L</em></sub> to select, as well as a distance function <em>d</em> used to evaluate the distance between two app samples based on their feature vectors. We might, for example, use well-known distance functions such as Euclidean distance, Manhattan distance, cosine distance, or Hamming distance. The algorithm is as follows:</p>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_197"/><strong>The Max-Distance Heuristic Selection Algorithm</strong></p>&#13;
<ol>&#13;
<li class="noindent">Randomly select an app from <em>D</em> and add it to the landmark set <em>L</em>′.</li>&#13;
<li class="noindent">If <em>|L</em>′<em>|</em> &lt; <em>N</em><sub><em>L</em></sub>, draw a random set of apps, <em>R</em>, from <em>D – L</em>′.</li>&#13;
<li class="noindent">Choose the best landmark from <em>R</em> using one of the following methods:&#13;
<div class="image"><img src="../images/math197-01.jpg" alt="Image" width="226" height="143"/></div></li>&#13;
<li class="noindent">Add the selected landmark to <em>L</em>′.</li>&#13;
<li class="noindent">When <em>|L</em>′<em>|</em> = <em>N</em><sub><em>L</em></sub>, use <em>L</em>′ as the set of landmarks <em>L</em>.</li>&#13;
</ol>&#13;
<p class="indent">It starts by randomly choosing an app from <em>D</em> as a landmark and adding it to the current set of selected landmarks, <em>L</em>′ (step 1). It then iteratively adds more landmarks (steps 2 through 4). In each iteration, it randomly draws a set of apps from <em>D – L</em>′ (step 2), and then selects the app that is farthest away from the current set of landmarks in <em>L</em>′ (step 3).</p>&#13;
<p class="indent">The distance can be calculated in various ways. For instance, suppose in a given iteration of the algorithm we have 3 landmarks, <em>ℓ</em><sub>1</sub>, <em>ℓ</em><sub>2</sub>, <em>ℓ</em><sub>3</sub>, and suppose <em>D – L</em>′ contains 100 landmarks, <img class="middle" src="../images/math197-02.jpg" alt="image" width="126" height="26"/>. In this case, any one of the 100 landmarks may be added into <em>L</em>′ as a fourth landmark. We could choose to add the landmark <img class="middle" src="../images/math197-03.jpg" alt="image" width="14" height="28"/> that maximizes the distance from the candidate fourth landmark in <em>D – L</em>′ to the previously selected landmarks in <em>L</em>′, or in other words maximizes the sum <img class="middle" src="../images/math197-04.jpg" alt="image" width="99" height="32"/>. Alternatively, we could choose the fourth landmark to be the one in <em>D – L</em>′ that maximizes either the mean distance or the median distance to the previously chosen landmarks <em>ℓ</em><sub>1</sub>, <em>ℓ</em><sub>2</sub>, <em>ℓ</em><sub>3</sub>, for example by choosing <img class="middle" src="../images/math197-05.jpg" alt="image" width="429" height="30"/>. <em>d</em> in this algorithm is a distance function. We let <em>d</em>(<em>ℓ</em>, <em>r</em>) denote the distance between the feature vectors of two apps, <em>ℓ</em> and <em>r</em>. This step ensures that the landmark selected is sufficiently far away from the previously selected landmarks to ensure some diversity among the landmark set.</p>&#13;
<p class="indent">The process ends when <em>N</em><sub><em>L</em></sub> landmarks have been picked (step 5). As there are 4 distance functions and 3 possible definitions of farthest distance, we can apply this landmark selection method in at least 12 ways.</p>&#13;
<p class="indent">Between the three landmark selection methods we’ve described, there are numerous ways to select the set <em>L</em> of landmarks from the set <em>D</em> for each <em>N</em><sub><em>L</em></sub> value. However, to further confound potential adversaries, we suggest that security officers periodically use a new set of landmarks, modify the landmark selection method, or both, and then recompute landmark-based features. By doing this once every week or two, you’ll keep any adversaries guessing and mount a moving target defense.</p>&#13;
<h4 class="h4" id="ch06lev1sec5"><span epub:type="pagebreak" id="page_198"/><strong><em>Computing Landmark-Based Features</em></strong></h4>&#13;
<p class="noindent">Once we’ve selected landmarks, we use them to compute landmark-based features for each app sample <em>i</em> in set <em>D</em>. Here is the algorithm for generating landmark-based features:</p>&#13;
<p class="noindentt"><strong>The Landmark-Based Feature Generation Algorithm</strong></p>&#13;
<ol>&#13;
<li class="noindent">Generate the set of landmarks <em>L</em> using S.</li>&#13;
<li class="noindent">For each landmark <em>ℓ</em> ∈ in each sample app <em>i</em> ∈ <em>D</em>, compute <em>d</em>(<em>i</em>, <em>ℓ</em>).</li>&#13;
<li class="noindent">Compute the features as follows:</li>&#13;
</ol>&#13;
<div class="image1"><img src="../images/math198-01.jpg" alt="Image" width="153" height="27"/></div>&#13;
<p class="indent">As input, we use the set <em>D</em> of Android apps with their associated feature vectors <img class="middle" src="../images/math198-02.jpg" alt="image" width="76" height="26"/>, the number <em>N</em><sub><em>L</em></sub> of landmarks to select, the landmark-selection method S (and its parameters, if applicable), and the distance function <em>d</em>(·).</p>&#13;
<p class="indent">We generate the set <em>L</em> of landmarks using S (step 1). Next, we iteratively compute the landmark feature vectors for each sample app <em>i</em> (steps 2 and 3). This process begins by computing the distance <em>d</em>(<em>i</em>, <em>ℓ</em>) of the sample <em>i</em> to each <em>ℓ</em> ∈, then constructing an <em>N</em><sub><em>L</em></sub><em>–</em>dimensional landmark-based feature vector by using those distances. In other words, the first element in this vector is the distance between app <em>i</em> and the first landmark, the second element in this vector is the distance between app <em>i</em> and the second landmark, and so forth.</p>&#13;
<p class="indent"><a href="ch06.xhtml#ch6fig3">Figure 6-3</a> is a simple illustration of landmark features. It assumes that there are six samples in our set <em>D</em> (in practice, this number would be much larger), each with a four-dimensional API feature vector.</p>&#13;
<div class="image"><img id="ch6fig3" src="../images/ch06fig03.jpg" alt="Image" width="627" height="280"/></div>&#13;
<p class="figcap"><em>Figure 6-3:  Landmark-based features with six apps, two landmarks, and the Euclidean distance function</em></p>&#13;
<p class="indent">Suppose we use the random landmark generation method to select two of the six samples, Perfect Girls and Marcher, as landmarks. We then generate landmark features using the Euclidean distance function. Here, you can see the Euclidean distance from each sample app <em>i</em> to each landmark. The landmark-based feature vector for, say, Regon is then (3551.33, 677.93), while that for Perfect Girls is (0,2903.66).</p>&#13;
<h3 class="h3" id="ch06lev6"><span epub:type="pagebreak" id="page_199"/><strong>Feature Clustering</strong></h3>&#13;
<p class="noindent">Some of the features we generate might have similar relationships to the label we’re attempting to predict. When this happens, we can combine those features to create a smaller, but perhaps more representative, set of new features. The approach, called <em>feature clustering</em>, first groups a set of basic features into a number of categories and then derives aggregated features from each category. We call these new features <em>FC features</em>. You can read more about this approach in “Android Malware Detection via (Somewhat) Robust Irreversible Feature Transformations” by Qian Han et al.</p>&#13;
<h4 class="h4" id="ch06lev1sec6"><strong><em>Generating Feature Clusters</em></strong></h4>&#13;
<p class="noindent">We use the following algorithm to get FC features:</p>&#13;
<p class="noindentt"><strong>The FC Feature Generation Algorithm</strong></p>&#13;
<ol>&#13;
<li class="noindent">Take a subset of samples <em>D</em>′ from <em>D</em>.</li>&#13;
<li class="noindent">Get the feature matrix <em>F</em>′ for samples in <em>D</em>′.</li>&#13;
<li class="noindent">Using <em>Clu</em>, cluster the <em>n</em> basic features into <em>G</em> groups according to column vectors {<em>f</em><sub><em>ij</em></sub>}<em>i</em>′ in <em>F</em>′.</li>&#13;
<li class="noindent">For each feature group <em>F</em><sub><em>g</em></sub> in each sample app <em>i</em>, associate a value with the group:&#13;
<div class="image1"><img src="../images/math199-01.jpg" alt="Image" width="170" height="36"/></div></li>&#13;
<li class="noindent">Perform this calculation for each sample app:</li>&#13;
</ol>&#13;
<div class="image1"><img src="../images/math199-02.jpg" alt="Image" width="156" height="31"/></div>&#13;
<p class="indent">As input, it takes the set <em>D</em> of all sample Android apps and each of their <em>n</em>–dimensional basic feature vectors; the number <em>G</em> of clusters in which to divide the <em>n</em> features; the clustering algorithm used, <em>Clu</em>; and ⊕, the algorithm to aggregate features within one group. We can use any subset or all of the basic static and dynamic analysis features we’ve presented, as well as features defined by other researchers.</p>&#13;
<p class="indent">We extract a subset <em>D</em>′ of sample apps from <em>D</em> (step 1) and use their feature values (step 2) to cluster the <em>n</em> features into <em>G</em> groups (step 3). We use a subset of <em>D</em>, not <em>D</em> itself, for three reasons: first, the dataset might be huge, and clustering the whole thing could be very expensive; second, by using a subset of samples for clustering, we make it harder for an adversary to determine how the feature clustering works; and third, when the set <em>D</em> is extended with the addition of more apps, we can compute the FC features of the new apps without having to rerun the algorithm and recluster basic features. Moreover, as in the case of TSGs, we can periodically update the sample used and recompute the feature clusters to keep adversaries guessing about the nature of the defenses used.</p>&#13;
<p class="indent">Once we’ve clustered the features, we take any app and use ⊕ to associate a single value with each cluster of features (step 4). That value could be <span epub:type="pagebreak" id="page_200"/>a sum, a minimum, or a maximum of the values of the features within that cluster, or it could be a statistical quantity derived from the set, such as the median, standard deviation, variance, or entropy. We perform this action for all clusters in every app in <em>D</em> (step 5).</p>&#13;
<h4 class="h4" id="ch06lev1sec7"><strong><em>Choosing Clustering and Feature Aggregation Algorithms</em></strong></h4>&#13;
<p class="noindent">We can invoke the feature clustering algorithm with many possible clustering and feature aggregation methods. For the clustering algorithm, we might use any of the six methods we mentioned in our discussion of landmark-based features or an entirely different algorithm. We can also choose from numerous possibilities for the feature aggregation algorithm, ⊕. Here are some options:</p>&#13;
<div class="bq1">&#13;
<p class="noindentt"><strong>Product</strong> We compute the new feature as the product of elements in the set.</p>&#13;
<p class="noindentt"><strong>Mean</strong> We use the mean value of the set of values as the new feature value.</p>&#13;
<p class="noindentt"><strong>Median</strong> We use the median value of the set of values as the new feature value.</p>&#13;
<p class="noindentt"><strong>Sum</strong> We compute the new feature as the sum of elements in the set.</p>&#13;
<p class="noindentt"><strong>Weighted sum</strong> We compute the new feature value as the weighted sum of elements in the set. The weight of feature <em>j</em>  is inversely proportional to the distance between the feature’s vector and the centroid feature value of the group <em>j</em><sub><em>c</em></sub>’s vector {<em>fij</em><sub><em>c</em></sub>}<em>i</em>′, which we denote as <em>d</em>(<em>j</em>, <em>j</em><sub><em>c</em></sub>). Thus, we compute the feature value as follows, where <em>α</em> is a parameter for normalization:</p>&#13;
<div class="image1"><img src="../images/math200.jpg" alt="image" width="200" height="55"/></div>&#13;
</div>&#13;
<p class="indent">We usually select a cluster size <em>G</em> that is significantly smaller than the total number of features so that this number decreases dramatically. For instance, if the basic feature vector had 100 elements, we might set <em>G</em> to 8. <a href="ch06.xhtml#ch6fig4">Figure 6-4</a> illustrates an example of feature clustering that uses sample apps and four-dimensional API features.</p>&#13;
<div class="image"><img id="ch6fig4" src="../images/ch06fig04.jpg" alt="Image" width="537" height="227"/></div>&#13;
<p class="figcap"><em>Figure 6-4:  A feature clustering example with two groups, four-dimensional basic API features, and averaging feature aggregation</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_201"/>In this example, we cluster the four apps into two groups and use the mean approach for ⊕. We obtain the FC features for each app shown in the table on the right.</p>&#13;
<p class="indent">While highly representative, FC features are hard for adversaries to guess, since generating them requires security analysts to make several choices that inject considerable uncertainty into the process and are difficult to reverse engineer. These choices include the subset of sample apps to use, the number of clusters to generate, the clustering method and its hyper-parameters, and the aggregation operator ⊕ (along with its hyperparameters, when ⊕ calculates a weighted sum).</p>&#13;
<h3 class="h3" id="ch06lev7"><strong>Correlation Graph–Based Feature Transformation</strong></h3>&#13;
<p class="noindent">Another way to reduce the number of features is to use correlation graphs, which generate what we call <em>CG features</em>. This approach involves creating a fully connected graph with features as its vertices, then using concepts from social network analysis to divide these features into communities. As each community consists of similar features, we can associate one CG feature with each.</p>&#13;
<p class="indent">We use the following algorithm to perform correlation graph–based feature transformation:</p>&#13;
<p class="noindentt"><strong>The CG Feature Generation Algorithm</strong></p>&#13;
<ol>&#13;
<li class="noindent">Take a subset <em>D</em>′ of samples from <em>D</em>.</li>&#13;
<li class="noindent">Get the feature matrix <em>F</em>′ for samples in <em>D</em>′.</li>&#13;
<li class="noindent">Compute the <em>n</em> × <em>n</em>  edge weights of the correlation graph according to the column vectors of <em>F</em>′.</li>&#13;
<li class="noindent">Get <em>G</em>  communities with the <em>n</em>  basic features according to the correlation graph and the community detection algorithm.</li>&#13;
<li class="noindent">For each feature community <em>C</em><sub><em>g</em></sub> in each sample app <em>i</em>, apply the aggregation operator:&#13;
<div class="image1"><img src="../images/math201-01.jpg" alt="image" width="153" height="31"/></div></li>&#13;
<li class="noindent">For each sample app <em>i</em>, calculate its CG feature vector:&#13;
<div class="image1"><img src="../images/math201-02.jpg" alt="image" width="156" height="28"/></div></li>&#13;
</ol>&#13;
<p class="indent">As input, it takes the set of apps <em>D</em>, the feature matrix <em>F</em> of those apps, a community detection algorithm <em>C</em>, the desired number of communities <em>G</em>, and an associative and commutative operator ⊕. It outputs a correlation graph with <em>G</em>-dimensional feature vectors for sample apps in <em>D</em>.</p>&#13;
<p class="indent">We begin by selecting a subset <em>D</em>’ of sample apps from <em>D</em> (step 1) and retrieving their feature matrix <em>F</em>’ (step 2), just as we did when calculating FC features. We then compute the <em>correlation</em> between each pair of features using the Pearson correlation coefficient (step 3). This value becomes the weight of the edge between each pair of features in the correlation graph. <span epub:type="pagebreak" id="page_202"/>Next, we apply the community detection algorithm C (step 4) to produce <em>G</em> communities. Finally, we generate the CG features for each app <em>D</em> using the features in each community and the associative and commutative feature aggregation operator ⊕ (steps 5 and 6).</p>&#13;
<p class="indent">We can define ⊕ in the same five ways as for feature clustering. In addition, we can select many possible community detection algorithms C, including the minimum cut method, the Girvan–Newman algorithm, modularity maximization, statistical interference, and clique-based methods. You can read more about these algorithms in the resources listed in the “Further Reading” section.</p>&#13;
<p class="indent"><a href="ch06.xhtml#ch6fig5">Figure 6-5</a> shows an example of generating correlation graph–based features. Suppose we want to group four API features into two communities, as shown on the left side of the figure. On the right side, you can see the CG features for each sample app created using the averaging feature aggregation method.</p>&#13;
<div class="image"><img id="ch6fig5" src="../images/ch06fig05.jpg" alt="Image" width="537" height="226"/></div>&#13;
<p class="figcap"><em>Figure 6-5:  Generating CG features with two communities and the averaging feature aggregation method</em></p>&#13;
<p class="indent">As with feature clustering, the use of CG features injects a great deal of uncertainty for any adversary attempting to reproduce the CG features. The CG feature generation process consists of many different choices that may end up yielding big differences in the final feature values. Adversaries will therefore have considerable difficulty in determining its real-world implementation.</p>&#13;
<h3 class="h3" id="ch06lev8"><strong>Further Reading</strong></h3>&#13;
<p class="noindent">This section lists resources you can use to further explore the topics introduced in this chapter.</p>&#13;
<p class="indent">To learn more about API-based features like the ones introduced in this chapter, see “DroidAPIMiner: Mining API-Level Features for Robust Malware Detection in Android” by Yousra Aafer et al. and “Machine Learning for Android Malware Detection Using Permission and API Calls” by Naser Peiravian and Xingquan Zhu.</p>&#13;
<p class="indent">To read about TSG features, consult the paper that introduced them, “DBank: Predictive Behavioral Analysis of Recent Android Banking Trojans” by Chongyang Bai et al. In addition, we mentioned that TSGs are an <span epub:type="pagebreak" id="page_203"/>alternative to the many kinds of function call graphs used in other malware detection techniques:</p>&#13;
<ul>&#13;
<li class="noindent">Dependency graphs, introduced in “Semantics-Aware Android Malware Classification Using Weighted Contextual API Dependency Graphs” by Mu Zhang et al.</li>&#13;
<li class="noindent">Control-flow graphs, introduced in “FlowDroid: Precise Context, Flow, Field, Object-Sensitive and Lifecycle-Aware Taint Analysis for Android Apps” by Steven Arzt et al. and “MaMaDroid: Detecting Android Malware by Building Markov Chains of Behavioral Models” by Enrico Mariconti et al.</li>&#13;
<li class="noindent">Code-property graphs, introduced in “Modeling and Discovering Vulnerabilities with Code Property Graphs” by Fabian Yamaguchi et al.</li></ul>&#13;
<p class="indent">Generating the CG features introduced in this chapter requires the use of a community detection algorithm. There are many ways of defining such an algorithm:</p>&#13;
<ul>&#13;
<li class="noindent">The minimum cut method, described in “Odd Minimum Cut-Sets and b-Matchings” by Manfred W. Padberg and M. Ram Rao</li>&#13;
<li class="noindent">Hierarchical clustering, described in “Hierarchical Clustering Schemes” by Stephen C. Johnson</li>&#13;
<li class="noindent">The Girvan–Newman algorithm, described in “Community Structure in Networks: Girvan–Newman Algorithm Improvement” by Ljiljana Despalatović et al.</li>&#13;
<li class="noindent">Modularity maximization, described in “Community Detection via Maximization of Modularity and Its Variants” by Mingming Chen et al.</li>&#13;
<li class="noindent">Statistical inference, described in Kate Calder’s <em>Statistical Inference</em>  (Holt, 1953)</li>&#13;
<li class="noindent">Clique-based methods, described in “A Maximal Clique Based Multiobjective Evolutionary Algorithm for Overlapping Community Detection” by Xuyun Wen et al.</li></ul>&#13;
<h3 class="h3" id="ch06lev9"><strong>Up Next</strong></h3>&#13;
<p class="noindent">Whenever antivirus products detect a piece of malware, the malware’s developers modify it in order to evade detection. By now, malware developers understand that antivirus companies are increasingly using machine learning. They’re also well aware of the types of basic features used to detect their malware and have become adept at modifying their code to change these features to escape detection.</p>&#13;
<p class="indent">In this chapter, we described how to use the manual processes of static and dynamic analysis introduced in <a href="ch03.xhtml">Chapters 3</a> and <a href="ch04.xhtml">4</a> to define features that machine learning algorithms can use. We then discussed two broad classes <span epub:type="pagebreak" id="page_204"/>of techniques that can make life harder for malware developers. The first, based on the notion of a triadic suspicion graph, was initially used to detect Android banking trojans but can in fact be used to detect any form of malware. The second transforms the original features of Android apps into a new set of features of a different size. We described three such methods in this chapter: landmark-based transformations, feature clustering, and correlation graph–based feature transformation, all of which are resilient to reverse engineering.</p>&#13;
<p class="indent">However, no method is perfect at confounding hackers. To further frustrate malware developers, the techniques introduced in this chapter include layers of randomization. In addition, we recommend that organizations change their machine learning–based malware detection settings frequently, just as all users should change their passwords frequently. For instance, in the case of TSGs, defenders could update the malware and goodware samples used to generate their features and modify other parameters, such as the window size, every week. In the case of landmark-based features, defenders could periodically modify the number and identities of their landmarks. These modifications impose a relatively small cost on enterprise security officers but can reap substantial benefits.</p>&#13;
<p class="indent">In the next chapter, we’ll apply what you’ve learned so far about machine learning algorithms and features to look at one important class of malware: rooting malware. This type of malware attempts to acquire root privileges on the user’s device, and once it has done so, it can be hard to dislodge. As a consequence, it’s essential to find characteristics of rooting malware that distinguish it from goodware.</p>&#13;
</div>
</div>
<div style="float: none; margin: 10px 0px 10px 0px; text-align: center;"><p><a href="https://oceanofpdf.com"><i>OceanofPDF.com</i></a></p></div></body></html>