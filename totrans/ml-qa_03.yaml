- en: '**2'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**2'
- en: SELF-SUPERVISED LEARNING**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督学习**
- en: '![Image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/common.jpg)'
- en: What is self-supervised learning, when is it useful, and what are the main approaches
    to implementing it?
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是自监督学习，它在何时有用，实施它的主要方法是什么？
- en: '*Self-supervised learning* is a pretraining procedure that lets neural networks
    leverage large, unlabeled datasets in a supervised fashion. This chapter compares
    self-supervised learning to transfer learning, a related method for pretraining
    neural networks, and discusses the practical applications of self-supervised learning.
    Finally, it outlines the main categories of self-supervised learning.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*自监督学习*是一种预训练过程，使神经网络能够以监督的方式利用大量无标签的数据集。本章将自监督学习与迁移学习进行比较，迁移学习是另一种相关的神经网络预训练方法，并讨论了自监督学习的实际应用。最后，本章概述了自监督学习的主要类别。'
- en: '**Self-Supervised Learning vs. Transfer Learning**'
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**自监督学习与迁移学习**'
- en: Self-supervised learning is related to transfer learning, a technique in which
    a model pretrained on one task is reused as the starting point for a model on
    a second task. For example, suppose we are interested in training an image classifier
    to classify bird species. In transfer learning, we would pretrain a convolutional
    neural network on the ImageNet dataset, a large, labeled image dataset with many
    different categories, including various objects and animals. After pretraining
    on the general ImageNet dataset, we would take that pretrained model and train
    it on the smaller, more specific target dataset that contains the bird species
    of interest. (Often, we just have to change the class-specific output layer, but
    we can otherwise adopt the pretrained network as is.)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督学习与迁移学习相关，迁移学习是一种技术，其中在一个任务上预训练的模型被重用作为第二个任务模型的起点。例如，假设我们有兴趣训练一个图像分类器来分类鸟类物种。在迁移学习中，我们会在ImageNet数据集上预训练一个卷积神经网络，ImageNet是一个大型的、有标签的图像数据集，包含许多不同的类别，包括各种物体和动物。在对通用的ImageNet数据集进行预训练之后，我们会将该预训练模型应用于一个较小且更具体的目标数据集，该数据集包含我们感兴趣的鸟类物种。（通常，我们只需要更改类特定的输出层，但可以直接采用预训练的网络。）
- en: '[Figure 2-1](ch02.xhtml#ch2fig1) illustrates the process of transfer learning.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2-1](ch02.xhtml#ch2fig1)展示了迁移学习的过程。'
- en: '![Image](../images/02fig01.jpg)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/02fig01.jpg)'
- en: '*Figure 2-1: Pretraining with conventional transfer learning*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2-1：使用传统迁移学习进行预训练*'
- en: Self-supervised learning is an alternative approach to transfer learning in
    which the model is pretrained not on labeled data but on *unlabeled* data. We
    consider an unlabeled dataset for which we do not have label information, and
    then we find a way to obtain labels from the dataset’s structure to formulate
    a prediction task for the neural network, as illustrated in [Figure 2-2](ch02.xhtml#ch2fig2).
    These self-supervised training tasks are also called *pretext tasks*.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督学习是迁移学习的另一种方法，其中模型不是在有标签的数据上预训练，而是在*无标签*的数据上进行预训练。我们考虑一个没有标签信息的无标签数据集，然后我们通过数据集的结构找到获取标签的方法，进而为神经网络制定预测任务，如[图
    2-2](ch02.xhtml#ch2fig2)所示。这些自监督训练任务也被称为*前置任务*。
- en: '![Image](../images/02fig02.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/02fig02.jpg)'
- en: '*Figure 2-2: Pretraining with self-supervised learning*'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2-2：使用自监督学习进行预训练*'
- en: The main difference between transfer learning and self-supervised learning lies
    in how we obtain the labels during step 1 in [Figures 2-1](ch02.xhtml#ch2fig1)
    and [2-2](ch02.xhtml#ch2fig2). In transfer learning, we assume that the labels
    are provided along with the data-set; they are typically created by human labelers.
    In self-supervised learning, the labels can be directly derived from the training
    examples.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习与自监督学习之间的主要区别在于我们在[图 2-1](ch02.xhtml#ch2fig1)和[图 2-2](ch02.xhtml#ch2fig2)的步骤
    1 中如何获得标签。在迁移学习中，我们假设标签是与数据集一起提供的；它们通常由人工标注者创建。在自监督学习中，标签可以直接从训练样本中推导出来。
- en: A self-supervised learning task could be a missing-word prediction in a natural
    language processing context. For example, given the sentence “It is beautiful
    and sunny outside,” we can mask out the word *sunny*, feed the network the input
    “It is beautiful and [MASK] outside,” and have the network predict the missing
    word in the “[MASK]” location. Similarly, we could remove image patches in a computer
    vision context and have the neural network fill in the blanks. These are just
    two examples of self-supervised learning tasks; many more methods and paradigms
    for this type of learning exist.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一个自监督学习任务可以是在自然语言处理中的缺失词预测。例如，给定句子“It is beautiful and sunny outside”，我们可以将单词*sunny*掩码掉，将输入传入网络：“It
    is beautiful and [MASK] outside”，然后让网络预测[MASK]位置的缺失单词。类似地，我们可以在计算机视觉中去除图像块，让神经网络填补空白。这只是自监督学习任务的两个例子；还有许多其他方法和范式。
- en: In sum, we can think of self-supervised learning on the pretext task as *representation
    learning*. We can take the pretrained model to fine-tune it on the target task
    (also known as the *downstream* task).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们可以将基于预文本任务的自监督学习看作是*表示学习*。我们可以利用预训练模型对目标任务进行微调（也称为*下游*任务）。
- en: '**Leveraging Unlabeled Data**'
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**利用未标注数据**'
- en: Large neural network architectures require large amounts of labeled data to
    perform and generalize well. However, for many problem areas, we don’t have access
    to large labeled datasets. With self-supervised learning, we can leverage unlabeled
    data. Hence, self-supervised learning is likely to be useful when working with
    large neural networks and with a limited quantity of labeled training data.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 大型神经网络架构需要大量标注数据才能有效执行并良好地推广。然而，对于许多问题领域，我们并没有访问大规模标注数据集。通过自监督学习，我们可以利用未标注数据。因此，当处理大型神经网络且标注训练数据有限时，自监督学习可能会非常有用。
- en: Transformer-based architectures that form the basis of LLMs and vision transformers
    are known to require self-supervised learning for pretraining to perform well.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 作为大型语言模型（LLMs）和视觉转换器基础的基于Transformer的架构被认为需要通过自监督学习进行预训练才能表现良好。
- en: For small neural network models such as multilayer perceptrons with two or three
    layers, self-supervised learning is typically considered neither useful nor necessary.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像两层或三层的多层感知器这样的简单神经网络模型，自监督学习通常被认为既不实用也不必要。
- en: Self-supervised learning likewise isn’t useful in traditional machine learning
    with nonparametric models such as tree-based random forests or gradient boosting.
    Conventional tree-based methods do not have a fixed parameter structure (in contrast
    to the weight matrices, for example). Thus, conventional tree-based methods are
    not capable of transfer learning and are incompatible with self-supervised learning.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督学习在传统的非参数模型（如基于树的随机森林或梯度提升法）中同样没有用处。传统的基于树的方法没有固定的参数结构（例如，与权重矩阵不同）。因此，传统的基于树的方法无法进行迁移学习，也与自监督学习不兼容。
- en: '**Self-Prediction and Contrastive Self-Supervised Learning**'
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**自预测与对比自监督学习**'
- en: 'There are two main categories of self-supervised learning: self-prediction
    and contrastive self-supervised learning. In *self-prediction*, illustrated in
    [Figure 2-3](ch02.xhtml#ch2fig3), we typically change or hide parts of the input
    and train the model to reconstruct the original inputs, such as by using a perturbation
    mask that obfuscates certain pixels in an image.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督学习主要有两类：自预测和对比自监督学习。在*自预测*中，如[图 2-3](ch02.xhtml#ch2fig3)所示，我们通常改变或隐藏输入的部分，并训练模型重建原始输入，例如使用扰动掩码遮掩图像中的某些像素。
- en: '![Image](../images/02fig03.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/02fig03.jpg)'
- en: '*Figure 2-3: Self-prediction after applying a perturbation mask*'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2-3：应用扰动掩码后的自预测*'
- en: A classic example is a denoising autoencoder that learns to remove noise from
    an input image. Alternatively, consider a masked autoencoder that reconstructs
    the missing parts of an image, as shown in [Figure 2-4](ch02.xhtml#ch2fig4).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一个经典的例子是去噪自编码器，它学习从输入图像中去除噪声。或者，考虑一个掩码自编码器，它重建图像中缺失的部分，如[图 2-4](ch02.xhtml#ch2fig4)所示。
- en: '![Image](../images/02fig04.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/02fig04.jpg)'
- en: '*Figure 2-4: A masked autoencoder reconstructing a masked image*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2-4：一个掩码自编码器重建掩码图像*'
- en: Missing (masked) input self-prediction methods are also commonly used in natural
    language processing contexts. Many generative LLMs, such as GPT, are trained on
    a next-word prediction pretext task (GPT will be discussed at greater length in
    [Chapters 14](ch14.xhtml) and [17](ch17.xhtml)). Here, we feed the network text
    fragments, where it has to predict the next word in the sequence (as we’ll discuss
    further in [Chapter 17](ch17.xhtml)).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失（掩码）输入自预测方法在自然语言处理领域也经常使用。许多生成性大语言模型（LLM），如 GPT，是通过下一个词预测任务进行训练的（GPT将在[第14章](ch14.xhtml)和[第17章](ch17.xhtml)中详细讨论）。在这里，我们向网络输入文本片段，网络需要预测序列中的下一个词（我们将在[第17章](ch17.xhtml)中进一步讨论）。
- en: In *contrastive self-supervised learning*, we train the neural network to learn
    an embedding space where similar inputs are close to each other and dissimilar
    inputs are far apart. In other words, we train the network to produce embeddings
    that minimize the distance between similar training inputs and maximize the distance
    between dissimilar training examples.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在*对比自监督学习*中，我们训练神经网络学习一个嵌入空间，其中相似的输入彼此接近，而不相似的输入彼此远离。换句话说，我们训练网络生成嵌入，使得相似的训练输入之间的距离最小化，而不相似的训练样本之间的距离最大化。
- en: Let’s discuss contrastive learning using concrete example inputs. Suppose we
    have a dataset consisting of random animal images. First, we draw a random image
    of a cat (the network does not know the label, because we assume that the dataset
    is unlabeled). We then augment, corrupt, or perturb this cat image, such as by
    adding a random noise layer and cropping it differently, as shown in [Figure 2-5](ch02.xhtml#ch2fig5).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过具体的示例输入来讨论对比学习。假设我们有一个由随机动物图像组成的数据集。首先，我们随机绘制一张猫的图像（网络不知道标签，因为我们假设数据集是未标注的）。然后，我们对这张猫的图像进行增强、腐蚀或扰动，例如通过添加随机噪声层并以不同方式裁剪，如[图
    2-5](ch02.xhtml#ch2fig5)所示。
- en: '![Image](../images/02fig05.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/02fig05.jpg)'
- en: '*Figure 2-5: Image pairs encountered in contrastive learning*'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2-5：对比学习中遇到的图像对*'
- en: The perturbed cat image in this figure still shows the same cat, so we want
    the network to produce a similar embedding vector. We also consider a random image
    drawn from the training set (for example, an elephant, but again, the network
    doesn’t know the label).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这张扰动后的猫图像仍然展示了相同的猫，因此我们希望网络生成一个相似的嵌入向量。我们还考虑从训练集随机选取一张图像（例如，一张大象的图像，但同样，网络并不知道标签）。
- en: For the cat-elephant pair, we want the network to produce dissimilar embeddings.
    This way, we implicitly force the network to capture the image’s core content
    while being somewhat agnostic to small differences and noise. For example, the
    simplest form of a contrastive loss is the *L*[2]-norm (Euclidean distance) between
    the embeddings produced by model *M*(*·*). Let’s say we update the model weights
    to decrease the distance ||*M*(cat) – *M*(cat*′*)||[2] and increase the distance
    ||*M*(*cat*) – *M*(*elephant*)||[2].
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于猫和大象的配对，我们希望网络生成不相似的嵌入。这样，我们间接地迫使网络捕捉图像的核心内容，同时对小的差异和噪声保持一定的无关性。例如，最简单的对比损失形式是模型*M*（·）生成的嵌入之间的*L*[2]范数（欧几里得距离）。假设我们更新模型权重，以减少距离||*M*(cat)
    – *M*(cat*′*)||[2]，并增加距离||*M*(*cat*) – *M*(*elephant*)||[2]。
- en: '[Figure 2-6](ch02.xhtml#ch2fig6) summarizes the central concept behind contrastive
    learning for the perturbed image scenario. The model is shown twice, which is
    known as a *siamese network* setup. Essentially, the same model is utilized in
    two instances: first, to generate the embedding for the original training example,
    and second, to produce the embedding for the perturbed version of the sample.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2-6](ch02.xhtml#ch2fig6)总结了在扰动图像情境下，对比学习的核心概念。模型被展示了两次，这被称为*连体网络*设置。基本上，同一个模型在两个实例中被使用：首先，生成原始训练示例的嵌入；其次，生成扰动版本样本的嵌入。'
- en: '![Image](../images/02fig06.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/02fig06.jpg)'
- en: '*Figure 2-6: Contrastive learning*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2-6：对比学习*'
- en: This example outlines the main idea behind contrastive learning, but many subvariants
    exist. Broadly, we can categorize these into *sample* contrastive and *dimension*
    contrastive methods. The elephant-cat example in [Figure 2-6](ch02.xhtml#ch2fig6)
    illustrates a sample contrastive method, where we focus on learning embeddings
    to minimize and maximize distances between training pairs. In *dimension*-contrastive
    approaches, on the other hand, we focus on making only certain variables in the
    embedding representations of similar training pairs appear close to each other
    while maximizing the distance of others.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例概述了对比学习的主要思想，但有许多子变种。大体上，我们可以将其分为*样本*对比方法和*维度*对比方法。[图 2-6](ch02.xhtml#ch2fig6)中的大象-猫示例展示了样本对比方法，在这种方法中，我们专注于学习嵌入，以最小化和最大化训练对之间的距离。另一方面，在*维度*对比方法中，我们专注于使相似训练对的嵌入表示中的某些变量彼此靠近，同时最大化其他变量之间的距离。
- en: '**Exercises**'
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**习题**'
- en: '**2-1.** How could we apply self-supervised learning to video data?'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**2-1.** 我们如何将自监督学习应用于视频数据？'
- en: '**2-2.** Can self-supervised learning be used for tabular data represented
    as rows and columns? If so, how could we approach this?'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**2-2.** 自监督学习能否用于表示为行列形式的表格数据？如果可以，我们该如何着手？'
- en: '**References**'
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: 'For more on the ImageNet dataset: *[https://en.wikipedia.org/wiki/ImageNet](https://en.wikipedia.org/wiki/ImageNet)*.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于 ImageNet 数据集的更多信息：*[https://en.wikipedia.org/wiki/ImageNet](https://en.wikipedia.org/wiki/ImageNet)*。
- en: 'An example of a contrastive self-supervised learning method: Ting Chen et al.,
    “A Simple Framework for Contrastive Learning of Visual Representations” (2020),
    *[https://arxiv.org/abs/2002.05709](https://arxiv.org/abs/2002.05709)*.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个对比自监督学习方法的示例：Ting Chen 等人，《视觉表征对比学习的简单框架》（2020），*[https://arxiv.org/abs/2002.05709](https://arxiv.org/abs/2002.05709)*。
- en: 'An example of a dimension-contrastive method: Adrien Bardes, Jean Ponce, and
    Yann LeCun, “VICRegL: Self-Supervised Learning of Local Visual Features” (2022),
    *[https://arxiv.org/abs/2210.01571](https://arxiv.org/abs/2210.01571)*.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个维度对比方法的示例：Adrien Bardes, Jean Ponce 和 Yann LeCun，《VICRegL：自监督学习局部视觉特征》（2022），*[https://arxiv.org/abs/2210.01571](https://arxiv.org/abs/2210.01571)*。
- en: 'If you plan to employ self-supervised learning in practice: Randall Balestriero
    et al., “A Cookbook of Self-Supervised Learning” (2023), *[https://arxiv.org/abs/2304.12210](https://arxiv.org/abs/2304.12210)*.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你计划在实践中应用自监督学习：Randall Balestriero 等人，《自监督学习食谱》（2023），*[https://arxiv.org/abs/2304.12210](https://arxiv.org/abs/2304.12210)*。
- en: 'A paper proposing a method of transfer learning and self-supervised learning
    for relatively small multilayer perceptrons on tabular datasets: Dara Bahri et
    al., “SCARF: Self-Supervised Contrastive Learning Using Random Feature Corruption”
    (2021), *[https://arxiv.org/abs/2106.15147](https://arxiv.org/abs/2106.15147)*.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提出了一种迁移学习和自监督学习方法，用于在表格数据集上进行相对较小的多层感知器的论文：Dara Bahri 等人，《SCARF：使用随机特征腐蚀进行自监督对比学习》（2021），*[https://arxiv.org/abs/2106.15147](https://arxiv.org/abs/2106.15147)*。
- en: 'A second paper proposing such a method: Roman Levin et al., “Transfer Learning
    with Deep Tabular Models” (2022), *[https://arxiv.org/abs/2206.15306](https://arxiv.org/abs/2206.15306)*.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二篇提出这种方法的论文：Roman Levin 等人，《使用深度表格模型进行迁移学习》（2022），*[https://arxiv.org/abs/2206.15306](https://arxiv.org/abs/2206.15306)*。
