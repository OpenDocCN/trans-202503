- en: <hgroup>
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_Condensed_B_11">17</samp> <samp class="SANS_Dogma_OT_Bold_B_11">CLIQUES,
    INDEPENDENT SETS, AND VERTEX COVERS</samp>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: </hgroup>
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/opener.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the previous chapter, we saw how the seemingly simple problem of assigning
    colors rapidly explodes into costly searches. Here, we consider the similarly
    challenging problems of assembling sets of nodes that satisfy various criteria:
    maximum cliques, maximum independent sets, and minimum vertex covers.'
  prefs: []
  type: TYPE_NORMAL
- en: For each of these problems, we want to find the largest or smallest set of nodes
    that fulfills some criteria based on immediate neighbors or adjacent edges. While
    it is easy to check whether a single proposed solution satisfies various constraints,
    it can be computationally expensive to find the best solution. Like the graph-coloring
    problem, these problems are classified as NP-hard. Again, we can attack these
    problems with either heuristic or exhaustive approaches.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter begins by returning to the exhaustive backtracking search with
    pruning that was introduced in the previous chapter, adapting it to exhaustively
    search for solutions to each of the three problems covered here. In addition,
    we consider a variety of greedy or heuristic approaches. We’ll also discuss real-world
    applications for each problem, from choosing office locations with cliques to
    avoiding grudges with independent sets to building guard towers with vertex covers.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_B_11">Backtracking Search for Sets of Nodes</samp>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For each of the problems in this chapter, we want to find a set of nodes that
    satisfies given constraints. We use a modified version of the backtracking search
    with pruning introduced in [Chapter 16](chapter16.xhtml) to find potential solutions
    by exploring the different assignments for whether each node is included in the
    set. As with their use in graph coloring, these backtracking searches enumerate
    all valid solutions. While they’ll check every possible valid assignment, they
    are rarely efficient.
  prefs: []
  type: TYPE_NORMAL
- en: The basic concept behind this search is to explore every possible set of nodes
    by considering the nodes one at a time and branching the search into two paths
    at each decision point. In the first path, the search explores the possible sets
    constructed without including the current node in the set. In the second path,
    it explores those possible sets constructed with the current node included in
    the set.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 17-1](#fig17-1) shows this approach with each node’s inclusion in the
    set marked as <samp class="SANS_TheSansMonoCd_W5Regular_11">True</samp> (included)
    or <samp class="SANS_TheSansMonoCd_W5Regular_11">False</samp> (excluded). The
    empty entries in the list indicate nodes we have yet to decide whether to include.
    At each level, the search considers the next unassigned node and branches out
    over both potential assignments.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A tree showing the branching for set assignment. At the top level, a five-element
    array has no items filled in. This array branches into two paths. The one on the
    left has F in the first element. The one on the right has T in the first element.](../images/f17001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<samp class="SANS_Futura_Std_Book_Oblique_I_11">Figure 17-1: A backtracking
    search to exhaustively try all set assignments</samp>'
  prefs: []
  type: TYPE_NORMAL
- en: Because we split each branch into two subbranches in [Figure 17-1](#fig17-1),
    the number of possible options doubles at each level. For a tree with *N* decisions,
    we explore 2*^N* full assignments. In the case of subsets of graph nodes, we consider
    each node as a separate decision, so *N* = |*V* | and we have 2^|*^V* ^| options
    to explore. While pruning invalid paths will help remove some obviously infeasible
    results, it will not save us from the full explosion of complexity this search
    can entail.
  prefs: []
  type: TYPE_NORMAL
- en: We can think of this search as a method of solving a puzzle in a magical dungeon.
    Upon entering the cold stone room, we find five massive switches along the wall.
    We know from our previous studies of magical dungeons that only one correct configuration
    of switches will unlock the door to where the treasure is hidden. Unfortunately,
    the dungeon’s designer was not simply trying to create a fun puzzle; they want
    to protect their treasure and therefore provide absolutely no hints. While it
    doesn’t take long to check any single guess, we might need to try every combination
    to find the right one.
  prefs: []
  type: TYPE_NORMAL
- en: Determined to get the treasure, we start with a guess for the leftmost switch
    (Off), then the second from the left (Off), and so forth until all the switches
    are in the Off position. When the vault door inevitably doesn’t open, we backtrack
    to the last decision point (where we had set the rightmost switch to Off) and
    try the On option. When that doesn’t work, we backtrack further (to the second
    rightmost switch), change that to On, and once again explore each possible setting
    for the final switch. We should consider ourselves lucky that the dungeon designer
    only had the budget for five switches, meaning we need to test only 2⁵ = 32 settings.
    But we find it hard to muster such positive thoughts as we backtrack again and
    again.
  prefs: []
  type: TYPE_NORMAL
- en: For all the algorithms in this chapter, we describe the same two algorithmic
    approaches for finding solutions. We start by describing an approximate greedy
    search to establish the fundamentals of the problem and the factors that impact
    the solutions. We then show how to adapt backtracking search for that problem
    and how to add pruning.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_B_11">Cliques</samp>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Cliques* are subsets of nodes within an undirected graph that are fully connected.
    Formally, we say that a clique is a set of nodes *V*′ ⊆ *V* such that:'
  prefs: []
  type: TYPE_NORMAL
- en: (*u*, *v*) ∈ *E* for all *u* ∈ *V*′ and *v* ∈ *V*′
  prefs: []
  type: TYPE_NORMAL
- en: In a social network, a clique would be a set of people who are all friends with
    each other.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 17-2](#fig17-2) shows a graph with two shaded subsets of nodes. The
    shaded nodes {1, 2, 5} in [Figure 17-2(a)](#fig17-2) form a clique because the
    graph contains an edge between each pair of nodes in the subset. In contrast,
    the shaded nodes {0, 1, 4} in [Figure 17-2(b)](#fig17-2) do not form a clique,
    as there is no edge between nodes 0 and 4 nor between nodes 1 and 4.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A graph with six nodes and undirected edges (0, 1), (0, 3), (1, 2), (1, 5),
    (2, 5), (3, 4), and (4, 5). On the left, the shaded nodes 1, 2, and 5 all have
    edges between them. On the right, the shaded nodes 0, 1, 4 do not.>](../images/f17002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<samp class="SANS_Futura_Std_Book_Oblique_I_11">Figure 17-2: A graph with a
    subset of nodes forming a clique (a) and a non-clique subset of nodes (b)</samp>'
  prefs: []
  type: TYPE_NORMAL
- en: We can determine whether a set of nodes forms a clique by checking each pair
    of nodes and confirming that an edge between them exists, as shown in [Listing
    17-1](#list17-1).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '<samp class="SANS_Futura_Std_Book_Oblique_I_11">Listing 17-1: Checking if a
    set of nodes forms a valid clique</samp>'
  prefs: []
  type: TYPE_NORMAL
- en: The code uses a pair of <samp class="SANS_TheSansMonoCd_W5Regular_11">for</samp>
    loops to iterate over each pair of nodes in the list and check whether the corresponding
    edge exists. If the edge is missing, the code immediately returns <samp class="SANS_TheSansMonoCd_W5Regular_11">False</samp>.
    If the code successfully examines all pairs of nodes in the list without finding
    any missing edges, it returns <samp class="SANS_TheSansMonoCd_W5Regular_11">True</samp>.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can visualize this check as a nosy outsider in a social network. Hearing
    tales of the Great Friend Group within a high school, the skeptical outsider proclaims,
    “There’s no way they all actually like each other,” and sets out to expose the
    group’s hidden divisions. Firmly in junior detective mode, they corner each person
    and ask them about their relationship with all the other members of the group:
    “Are you really friends with Jonny? How about Suzy?” It is not until they have
    confirmed every pairing is genuine that they finally abandon their skepticism.'
  prefs: []
  type: TYPE_NORMAL
- en: While determining whether a given set of nodes forms a clique is straightforward,
    it’s significantly more difficult to build the largest possible clique in a graph.
    The problem of finding the *maximum clique* consists of finding the largest subset
    of nodes *V*′ ⊆ *V* in the graph that form a valid clique. This problem is significantly
    more difficult than finding an arbitrary clique in the graph because the validity
    of a node’s membership depends on the other nodes in the clique. If adding nodes
    one by one, early choices can take us in suboptimal directions and exclude later
    nodes.
  prefs: []
  type: TYPE_NORMAL
- en: '#### <samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">Use Cases</samp>'
  prefs: []
  type: TYPE_NORMAL
- en: We can visualize the importance of cliques by considering locations (nodes)
    that need to be directly joined by transportation routes (edges). The kingdom’s
    Guild of Adventurers, Explorers, and Cartographers is looking to set up regional
    headquarters in locations with magical dungeons. After spending hours debating
    the importance of various criteria, including such considerations as dungeon difficulty
    and access to fresh produce, they conclude that the number one priority is easy
    transportation between the offices. After all, the guild offices share a single
    list of open quests for their members. If an adventurer in the city of Old Melbourne
    learns of a promising quest at the Cliffs of Indecision, they will want to travel
    there directly. The guild leaders enlist their senior cartographers to find the
    largest set of cities such that each city is directly connected by a road. The
    cartographers, familiar with the problem of finding maximum cliques, set to work
    enumerating the possibilities.
  prefs: []
  type: TYPE_NORMAL
- en: In a less fantastical world, we might want to use maximum clique detection to
    choose locations for businesses with direct transportation connections or computational
    nodes with direct links. Each of these problems consists of finding fully connected
    subsets within a graph.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">Greedy Search</samp>
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We can define a greedy algorithm for building cliques by starting with an arbitrary
    node as our clique and continually adding compatible nodes. We always choose to
    add new nodes that would keep our clique *valid*, which is any node that shares
    edges with each of the clique members.
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 17-2](#list17-2) shows how to list the options for clique expansion
    by checking each node to see whether we could add it to the set and still have
    a valid clique.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '<samp class="SANS_Futura_Std_Book_Oblique_I_11">Listing 17-2: Checking which
    nodes can be added to a clique</samp>'
  prefs: []
  type: TYPE_NORMAL
- en: The code iterates through each node in the graph and tests whether that node
    could be added to the clique, first checking whether the node is already part
    of the clique ❶. If not, the code checks a node’s validity by checking that it
    shares an edge with every node in the current clique ❷. If those tests pass for
    each node in the clique, the code adds the current node to the list of expansion
    options.
  prefs: []
  type: TYPE_NORMAL
- en: This function could help the Great Friend Group identify prospective members.
    Every student in the school is a prospective candidate. For each student who is
    not already in the group, the friend group’s chosen representative asks every
    member of the group, “Are you two friends?” If the prospective member is already
    friends with all members of the existing group (there is an edge from the new
    node to every node in the group), the current members quickly welcome in their
    mutual friend.
  prefs: []
  type: TYPE_NORMAL
- en: In [Listing 17-3](#list17-3), we construct a greedy algorithm that incrementally
    builds a clique one node at a time.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '<samp class="SANS_Futura_Std_Book_Oblique_I_11">Listing 17-3: A greedy algorithm
    to find cliques</samp>'
  prefs: []
  type: TYPE_NORMAL
- en: The code starts with an empty list to represent the clique being constructed.
    It uses a <samp class="SANS_TheSansMonoCd_W5Regular_11">while</samp> loop to continually
    find a list of potential options with the <samp class="SANS_TheSansMonoCd_W5Regular_11">clique_expansion_options()</samp>
    function from [Listing 17-2](#list17-2) and adds the first option from the returned
    list to the clique ❶. It stops and returns the list when there are no more nodes
    that can be added to the current clique (<samp class="SANS_TheSansMonoCd_W5Regular_11">len(to_add)</samp>
    <samp class="SANS_TheSansMonoCd_W5Regular_11">==</samp> <samp class="SANS_TheSansMonoCd_W5Regular_11">0</samp>).
  prefs: []
  type: TYPE_NORMAL
- en: When adding nodes one at a time, we immediately run into the question, “Which
    node do we add next?” In the code in [Listing 17-3](#list17-3), we added just
    the first option, but this could be a *terrible* choice. Consider what happens
    if we apply this greedy algorithm to the graph in [Figure 17-3](#fig17-3). As
    written, the greedy algorithm would choose node 0 first and ultimately return
    {0, 1} instead of the larger clique {1, 2, 4, 5}.
  prefs: []
  type: TYPE_NORMAL
- en: '![A graph with six nodes and undirected edges (0, 1), (0, 3), (1, 2), (1, 4),
    (1, 5), (2, 4), (2, 5), (3, 4), and (4, 5).](../images/f17003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<samp class="SANS_Futura_Std_Book_Oblique_I_11">Figure 17-3: A graph for which
    the greedy search for a maximum clique fails</samp>'
  prefs: []
  type: TYPE_NORMAL
- en: The greedy search is not guaranteed to find the maximum clique because decisions
    at each iteration of greedy search are not independent. Each time the algorithm
    adds a node *u* to the clique, this prevents it from adding any future nodes that
    do not have an edge to *u*. We can easily get stuck in a local maximum by adding
    the wrong node early on. We could improve our selection heuristics, such as by
    choosing nodes with the most edges, but this only helps so much. To build a maximum
    clique, we need a more comprehensive (and expensive) search.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">Backtracking Search</samp>
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Backtracking search* for a maximum clique recursively tries to set one node
    of the graph as either a member or non-member of the clique, as shown in [Listing
    17-4](#list17-4). At each level of recursion, the search function takes the clique
    built so far (<samp class="SANS_TheSansMonoCd_W5Regular_11">clique</samp>) and
    the next node to test (<samp class="SANS_TheSansMonoCd_W5Regular_11">index</samp>)
    and recursively tests all combinations of unassigned nodes, returning the biggest
    clique found down that branch of the search. This branching effectively tests
    all 2^|*^V* ^| possible subsets of nodes, while using pruning to cut off invalid
    options.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '<samp class="SANS_Futura_Std_Book_Oblique_I_11">Listing 17-4: Recursively exploring
    possible cliques</samp>'
  prefs: []
  type: TYPE_NORMAL
- en: The code for backtracking search starts by checking whether it has reached the
    termination condition (iterated past the last node in the graph) ❶. If so, there
    is nothing left to check, and <samp class="SANS_TheSansMonoCd_W5Regular_11">clique</samp>
    is the largest subset down this branch of the search. The code returns a copy
    of the current clique to effectively snapshot the state and separate it from the
    <samp class="SANS_TheSansMonoCd_W5Regular_11">clique</samp> object that it will
    continue to modify during the rest of the search.
  prefs: []
  type: TYPE_NORMAL
- en: If the search has not reached the end of the recursion, the code tries building
    cliques both with and without <samp class="SANS_TheSansMonoCd_W5Regular_11">index</samp>.
    It tests the subset without <samp class="SANS_TheSansMonoCd_W5Regular_11">index</samp>
    by calling <samp class="SANS_TheSansMonoCd_W5Regular_11">maximum_clique_recursive()</samp>
    with the current <samp class="SANS_TheSansMonoCd_W5Regular_11">clique</samp> and
    the index of the next node ❷, then saves the best result down that branch for
    comparison.
  prefs: []
  type: TYPE_NORMAL
- en: Before testing the subset with the current node, the code avoids exploring invalid
    subtrees by checking whether this node is compatible with the current <samp class="SANS_TheSansMonoCd_W5Regular_11">clique</samp>
    ❸. As with <samp class="SANS_TheSansMonoCd_W5Regular_11">clique_expansion_options()</samp>
    in [Listing 17-2](#list17-2), the <samp class="SANS_TheSansMonoCd_W5Regular_11">maximum_clique_recursive()</samp>
    function checks that the prospective node <samp class="SANS_TheSansMonoCd_W5Regular_11">index</samp>
    has edges to all the nodes in the current clique. If even a single edge is missing,
    adding <samp class="SANS_TheSansMonoCd_W5Regular_11">index</samp> would result
    in an invalid clique. The code skips recursive exploration on such invalid sets.
  prefs: []
  type: TYPE_NORMAL
- en: If the current node is compatible with the current clique, the code tries adding
    it to the clique and recursively testing the remaining options. It then cleans
    up the <samp class="SANS_TheSansMonoCd_W5Regular_11">clique</samp> data by removing
    <samp class="SANS_TheSansMonoCd_W5Regular_11">index</samp> so that the <samp class="SANS_TheSansMonoCd_W5Regular_11">clique</samp>
    list can continue to be used in other branches ❹. The code compares the results
    of the two branches and keeps the larger valid subset of nodes ❺.
  prefs: []
  type: TYPE_NORMAL
- en: 'We call the function in [Listing 17-4](#list17-4) with an initial <samp class="SANS_TheSansMonoCd_W5Regular_11">clique=[]</samp>
    and <samp class="SANS_TheSansMonoCd_W5Regular_11">index=0</samp> or use a wrapper
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 17-4](#fig17-4) shows a visualization of the search. Each level shows
    the algorithm branching on the inclusion (or not) of a single node. The first
    level considers whether to include node 0\. The second level considers including
    node 1\. Nodes assigned to the clique are shaded, excluded nodes are white, and
    unassigned nodes are dashed circles.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A tree where each node corresponds to a four-node graph with undirected edges
    (0, 1), (0, 3), (1, 2), and (1, 3). At the root node all graph nodes are dashed.
    At each level of the tree, another node becomes either solid white or solid gray.](../images/f17004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<samp class="SANS_Futura_Std_Book_Oblique_I_11">Figure 17-4: The steps of a
    backtracking search for the maximum clique</samp>'
  prefs: []
  type: TYPE_NORMAL
- en: The subgraphs in [Figure 17-4](#fig17-4) show the state of the <samp class="SANS_TheSansMonoCd_W5Regular_11">clique</samp>
    list at the start of each function call. As you can see, the function follows
    only branches that contain valid cliques. For example, when evaluating <samp class="SANS_TheSansMonoCd_W5Regular_11">clique=[0]</samp>
    and <samp class="SANS_TheSansMonoCd_W5Regular_11">index=2</samp>, the search cannot
    follow the right-hand branch because {0, 2} is not a valid clique. As a result,
    the search tests only 10 of the 16 possible full combinations, as shown in the
    final row.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_B_11">Independent Sets</samp>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An *independent set* is effectively the opposite of a clique. We define an
    independent set within an undirected graph as a subset of nodes such that no two
    nodes in the set are neighbors. Formally, an independent set is a set of nodes
    *V*′ ⊆ *V* such that:'
  prefs: []
  type: TYPE_NORMAL
- en: (*u*, *v*) ∉ *E* for all *u* ∈ *V*′ and *v* ∈ *V*′
  prefs: []
  type: TYPE_NORMAL
- en: 'We can envision choosing an independent set as planning the world’s most awkward
    party: we invite a group of people from our school or office such that no one
    at the party likes anyone else.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 17-5](#fig17-5) shows a graph with two shaded subsets of nodes. The
    shaded nodes {0, 2, 4} in [Figure 17-5(a)](#fig17-5) form an independent set because
    the graph does not contain any edges connecting these nodes. In contrast, the
    shaded nodes {0, 1, 4} in [Figure 17-5(b)](#fig17-5) do not form an independent
    set, as there is an edge between nodes 0 and 1.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A graph with six nodes and undirected edges (0, 1), (0, 3), (1, 2), (1, 5),
    (2, 5), (3, 4), and (4, 5). On the left, the shaded nodes are 0, 2, and 4\. On
    the right, the shaded nodes are 0, 1, and 4.](../images/f17005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<samp class="SANS_Futura_Std_Book_Oblique_I_11">Figure 17-5: A graph with independent
    (a) and non-independent (b) subsets of nodes</samp>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Determining whether a set of nodes forms an independent set requires us to
    check each pair of nodes and confirm there is no edge between them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This code is almost identical to the clique-checking algorithm in [Listing 17-1](#list17-1).
    It iterates over each pair of nodes in the list and checks whether they violate
    the independent set criteria.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of our awkward party, the <samp class="SANS_TheSansMonoCd_W5Regular_11">is_independent_set()</samp>
    function plays the role of another skeptical outsider. Unable to bear the silence,
    they insist, “Some people here must be friends.” They query every member of the
    party about their relationships with each other attendee, asking, “Are you sure
    you’re not friends with them?” and “How about them?” Only when every single question
    comes back that no pair are friends do they admit that the awkward atmosphere
    is understandable and that the host must be a bit of a jerk.
  prefs: []
  type: TYPE_NORMAL
- en: As with cliques, generating large independent sets can be difficult because
    adding a single node to our independent set may impact the validity of other nodes.
    The problem of finding the *maximum independent set* consists of finding the largest
    subset of nodes *V*′ ⊆ *V* in the graph that form a valid independent set.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">Use Cases</samp>
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We can visualize the importance of independent sets by considering a problem
    where we want to select people (nodes) without negative connections (edges). Imagine
    being tasked with building a functional project team in a highly dysfunctional
    organization. Every employee holds a collection of grudges against their coworkers
    for “misplaced” lunches or forgotten birthdays. In fact, the HR department has
    gone so far as to build a graph indicating pairwise grudges. Each node represents
    an employee and undirected edges indicate mutual ill will. The problem of finding
    a team of employees who do not dislike each other consists of finding a set of
    nodes within this graph such that no two share an edge.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, we can visualize the independent set problem in the context of
    designing a magical dungeon. Aiming to provide adventurers with an appropriate
    but not impossible challenge, an evil wizard resolves not to include boss-level
    monsters in any two adjacent rooms. They model the dungeon level as a graph with
    nodes as rooms and the tunnels between them as edges. They then set about finding
    the largest possible independent set of rooms that will contain their boss-level
    monsters. The rest of the rooms can consist of low-level slimes to give the adventurers
    a break.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">Greedy Search</samp>
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As with the clique algorithm, we can define a greedy algorithm that builds independent
    sets one node at a time by adding compatible nodes. We list the options for independent
    expansion by checking whether each node is valid, as shown in [Listing 17-5](#list17-5).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '<samp class="SANS_Futura_Std_Book_Oblique_I_11">Listing 17-5: Finding nodes
    that can be added to an independent set</samp>'
  prefs: []
  type: TYPE_NORMAL
- en: The code iterates through each node in the graph and tests whether that node
    could be added to the independent set. For this function, the code checks that
    the node under consideration does not share an edge with any node in the current
    set ❶. Only if those checks pass for every node in the set (and <samp class="SANS_TheSansMonoCd_W5Regular_11">valid</samp>
    is still <samp class="SANS_TheSansMonoCd_W5Regular_11">True</samp>) does the code
    add the current node to the list of options.
  prefs: []
  type: TYPE_NORMAL
- en: Rather than choosing just any viable node, we can often extend greedy searches
    by choosing the next node with a heuristic. This heuristic will not guarantee
    correct results 100 percent of the time, but it can help guide the set construction
    in better directions. One reasonable heuristic for the independent set problem
    is to choose nodes with the fewest number of edges, which are likely to have fewer
    conflicts with other nodes and thus be more compatible with our needs. In the
    context of our dysfunctional organization, this corresponds to choosing team members
    who hold the fewest grudges.
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 17-6](#list17-6) shows how we can codify this heuristic by modifying
    [Listing 17-5](#list17-5) to return the feasible node with the fewest number of
    edges.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '<samp class="SANS_Futura_Std_Book_Oblique_I_11">Listing 17-6: Finding the node
    with the fewest edges that is compatible with the independent set</samp>'
  prefs: []
  type: TYPE_NORMAL
- en: The code largely mirrors that of [Listing 17-5](#list17-5) but tracks the best
    node seen (<samp class="SANS_TheSansMonoCd_W5Regular_11">best_option</samp>) and
    its number of edges (<samp class="SANS_TheSansMonoCd_W5Regular_11">best_num_edges</samp>).
    It starts by setting the best node seen to the invalid option <samp class="SANS_TheSansMonoCd_W5Regular_11">-1</samp>
    and the best number of edges to more than could be adjacent to a single node ❶.
    The code then uses a <samp class="SANS_TheSansMonoCd_W5Regular_11">for</samp>
    loop to check each node for feasibility. However, before the feasibility test
    itself, the code checks whether the node under consideration has fewer edges than
    the best found so far ❷. If not, it does not matter whether the node is feasible,
    as the code will not be returning it anyway. It can therefore skip the feasibility
    test and move on to the next node.
  prefs: []
  type: TYPE_NORMAL
- en: The actual feasibility test is identical to that in [Listing 17-5](#list17-5)
    ❸. The code uses a <samp class="SANS_TheSansMonoCd_W5Regular_11">for</samp> loop
    through the existing independent set to test each node against the current candidate
    <samp class="SANS_TheSansMonoCd_W5Regular_11">i</samp>. Only if those checks pass
    for every node in the set does the code save the current node as the new <samp
    class="SANS_TheSansMonoCd_W5Regular_11">best_option</samp>. After exhausting all
    possible nodes, the code returns the best found.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can build the greedy search by continually adding the best candidate to
    the independent set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The code starts with an empty list <samp class="SANS_TheSansMonoCd_W5Regular_11">i_set</samp>
    and uses the <samp class="SANS_TheSansMonoCd_W5Regular_11">independent_set _lowest_expansion()</samp>
    function from [Listing 17-6](#list17-6) to find the best-looking node to add.
    It uses a <samp class="SANS_TheSansMonoCd_W5Regular_11">while</samp> loop to continually
    find and add nodes until no other nodes can be added.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of our dysfunctional organization example, the greedy search
    algorithm for finding independent sets consists of building a team one person
    at a time by always selecting the employee with the fewest grudges who is compatible
    with everyone previously selected. We start by selecting employees (nodes) who
    have no conflicts (no edges). We can always add these employees to an independent
    set. Next, we consider employees with only a single conflict, and so forth, always
    skipping employees who are incompatible with anyone on the current team.
  prefs: []
  type: TYPE_NORMAL
- en: Greedy search will not always find the maximum independent set. Despite the
    use of an informative heuristic, this greedy search can still make suboptimal
    choices that relegate the solution to a local minimum. Consider what happens if
    we apply this greedy algorithm to the graph in [Figure 17-6](#fig17-6).
  prefs: []
  type: TYPE_NORMAL
- en: '![A graph with six nodes and undirected edges (0, 2), (0, 4), (1, 3), (1, 5),
    (2, 3), (2, 4), (2, 5), and (4, 5). (A) has nodes 0 and 1 shaded. (B) has nodes
    0, 3, and 5 shaded.](../images/f17006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<samp class="SANS_Futura_Std_Book_Oblique_I_11">Figure 17-6: The results of
    a greedy search for maximum independent sets (a) and the true maximum independent
    set (b)</samp>'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in [Figure 17-6(a)](#fig17-6), the greedy search will choose node
    0 and then node 1, locking itself into a local minimum. If the search had instead
    selected node 3 as its second choice, as shown in [Figure 17-6(b)](#fig17-6),
    it would have found {0, 3, 5}.  #### <samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">Backtracking
    Search</samp>'
  prefs: []
  type: TYPE_NORMAL
- en: The backtracking search for constructing a maximum independent set again tries
    to label each node a member or non-member of the set. This branching allows the
    function to test all combinations of nodes and return the largest independent
    set found through each branch of the search. At each level of recursion, the function
    in [Listing 17-7](#list17-7) takes the independent set (<samp class="SANS_TheSansMonoCd_W5Regular_11">current</samp>)
    constructed so far and the next node to test (<samp class="SANS_TheSansMonoCd_W5Regular_11">index</samp>).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '<samp class="SANS_Futura_Std_Book_Oblique_I_11">Listing 17-7: Recursively exploring
    possible independent sets</samp>'
  prefs: []
  type: TYPE_NORMAL
- en: Following the same pattern as the <samp class="SANS_TheSansMonoCd_W5Regular_11">maximum_clique_recursive()</samp>
    function from [Listing 17-4](#list17-4), the <samp class="SANS_TheSansMonoCd_W5Regular_11">maximum_independent_set_rec()</samp>
    function starts by testing whether it has reached the end of the recursion and
    there are no nodes left to check ❶. If so, it returns a copy of the current independent
    set as the best found down this branch.
  prefs: []
  type: TYPE_NORMAL
- en: If the search has not reached the end of the recursion, it tries building out
    independent sets both with and without <samp class="SANS_TheSansMonoCd_W5Regular_11">index</samp>.
    It tests the subset without <samp class="SANS_TheSansMonoCd_W5Regular_11">index</samp>
    by calling the function with the current independent set and the index of the
    next node ❷. This effectively skips adding the current index and moves on to considering
    later nodes. The code saves the best result found down that branch as the baseline
    for other branches.
  prefs: []
  type: TYPE_NORMAL
- en: The code then checks whether the current node (<samp class="SANS_TheSansMonoCd_W5Regular_11">index</samp>)
    is compatible with the independent set under construction ❸. If the current node
    shares an edge with any node in <samp class="SANS_TheSansMonoCd_W5Regular_11">current</samp>,
    adding it would result in an invalid independent set. The code explores only paths
    that result in valid independent sets (<samp class="SANS_TheSansMonoCd_W5Regular_11">can_add</samp>
    <samp class="SANS_TheSansMonoCd_W5Regular_11">==</samp> <samp class="SANS_TheSansMonoCd_W5Regular_11">True</samp>).
  prefs: []
  type: TYPE_NORMAL
- en: If the current node is compatible with the current set, the code tries adding
    the node to <samp class="SANS_TheSansMonoCd_W5Regular_11">current</samp> and recursively
    testing the remaining options ❹. Afterward, it cleans up the <samp class="SANS_TheSansMonoCd_W5Regular_11">current</samp>
    list by removing <samp class="SANS_TheSansMonoCd_W5Regular_11">index</samp> so
    that it can continue to use the list in other branches ❺. The code compares the
    results of the two branches and keeps the larger valid subset of nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We call the function in [Listing 17-7](#list17-7) with an initial <samp class="SANS_TheSansMonoCd_W5Regular_11">current=[]</samp>
    and <samp class="SANS_TheSansMonoCd_W5Regular_11">index=0</samp> or use a wrapper
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 17-7](#fig17-7) shows a visualization of the search where each level
    depicts the algorithm branching on the inclusion (or not) of a single node. Nodes
    assigned to the independent set are shaded, nodes excluded are white, and unassigned
    nodes are dashed circles.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A tree where each node corresponds to a four-node graph with undirected edges
    (0, 1), (0, 3), (1, 2), and (1, 3). At the root node, all graph nodes are dashed.
    At each level of the tree, another node become either solid white or solid gray.](../images/f17007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<samp class="SANS_Futura_Std_Book_Oblique_I_11">Figure 17-7: The exploration
    of a backtracking search for maximum independent sets</samp>'
  prefs: []
  type: TYPE_NORMAL
- en: The subgraphs in [Figure 17-7](#fig17-7) show the state of <samp class="SANS_TheSansMonoCd_W5Regular_11">current</samp>
    at the start of each function call. The first level considers whether to include
    node 0; the second considers including node 1\. Since the function explores only
    branches containing valid independent sets, it reaches only 7 of the 16 possible
    full assignments.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_B_11">Vertex Cover</samp>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Whereas the problems of finding cliques and independent sets both focus on
    whether a pair of nodes are neighbors, the problem of *vertex cover* considers
    the edges that each node touches. We define a vertex cover within an undirected
    graph as a subset of nodes such that every edge has at least one endpoint in the
    set. In other words, each edge is covered by at least one vertex (node). Formally,
    the vertex cover is a set of nodes *V*′ ⊆ *V* such that:'
  prefs: []
  type: TYPE_NORMAL
- en: For every edge (*u*, *v*) ∈ *E*, we have *u* ∈ *V*′, *v* ∈ *V*′, or both.
  prefs: []
  type: TYPE_NORMAL
- en: We can envision vertex covers in the context of a kingdom that consists of an
    archipelago of islands (nodes) connected by bridges (edges). To maintain security,
    the kingdom constructs tall watchtowers to survey each bridge. The watchtower
    on each island views every bridge touching the island, allowing the kingdom to
    be strategic about the towers’ locations. However, each bridge (edge) must end
    on at least one island containing a watchtower (selected node).
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 17-8](#fig17-8) shows a graph with two shaded subsets of nodes. The
    shaded nodes {1, 3, 5} in [Figure 17-8(a)](#fig17-8) form a vertex cover because
    each edge touches at least one shaded node. In contrast, the shaded nodes {0,
    1, 4} in [Figure 17-8(b)](#fig17-8) do not form a vertex cover, as the edge (2,
    5) is not covered by any node in the set.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A graph with six nodes and undirected edges (0, 1), (0, 3), (1, 2), (1, 5),
    (2, 5), (3, 4), and (4, 5). In (A), the shaded nodes are 1, 3, and 5\. In (B),
    the shaded nodes are 0, 1, and 4.](../images/f17008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<samp class="SANS_Futura_Std_Book_Oblique_I_11">Figure 17-8: A graph with a
    subset of nodes forming a vertex cover (a) and a subset of nodes that is not a
    vertex cover (b)</samp>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Determining whether a set of nodes forms a vertex cover requires us to check
    whether each edge in the graph is covered by at least one node in the set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This code starts by creating a set of the included nodes (<samp class="SANS_TheSansMonoCd_W5Regular_11">node_set</samp>)
    to enable fast lookups by using the <samp class="SANS_TheSansMonoCd_W5Regular_11">set</samp>
    data structure instead of searching through a <samp class="SANS_TheSansMonoCd_W5Regular_11">list</samp>
    ❶. It then loops through each edge in the graph and checks whether both the origin
    and destination nodes are missing from the set ❷. If neither node is included
    in the set, the edge is not covered, and the function immediately returns <samp
    class="SANS_TheSansMonoCd_W5Regular_11">False</samp>. If the code makes it through
    all edges, it returns <samp class="SANS_TheSansMonoCd_W5Regular_11">True</samp>.
  prefs: []
  type: TYPE_NORMAL
- en: The problem of finding the *minimum vertex cover* consists of finding the smallest
    subset of nodes *V*′ ⊆ *V* in the graph that form a vertex cover. This problem
    has direct analogies in cost savings. In the watchtower example, the kingdom wants
    to build the minimum number of watchtowers that will secure every bridge.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">Use Cases</samp>
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The problem of vertex cover arises naturally in the context of maintenance.
    Imagine that an evil but tidy wizard has constructed a magical dungeon. Knowing
    that they cannot leave passages unswept or risk torches burning out, they need
    to station a crew of emergency-repair minions near each passage. After adventurers
    blunder down tunnels, knocking stones loose in their fights with monsters, the
    minions rush forth to repair the damage. For the sake of expediency, the wizard
    needs to station a crew in at least one of the two rooms at the end of each passage.
    To minimize costs, the wizard meticulously finds the smallest number of crews
    they can employ.
  prefs: []
  type: TYPE_NORMAL
- en: In a non-magical context, we might be interested in employing maintenance crews
    or toll collectors for transportation networks. To keep costs low, we plan a single
    set of tollbooths through which all incoming and outgoing traffic to the island
    flows.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">Greedy Search</samp>
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We can build on the greedy algorithm presented for independent sets to create
    a greedy approach for finding a vertex cover using a subset of the nodes, as shown
    in [Listing 17-8](#list17-8). This time, we use the heuristic of selecting the
    node that covers the largest number of uncovered edges.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '<samp class="SANS_Futura_Std_Book_Oblique_I_11">Listing 17-8: Heuristically
    selecting a node to add to a vertex cover</samp>'
  prefs: []
  type: TYPE_NORMAL
- en: In comparison to the code in [Listing 17-6](#list17-6), this code does additional
    bookkeeping to track edges already covered in the set <samp class="SANS_TheSansMonoCd_W5Regular_11">edges_covered</samp>.
    It starts by creating an empty set of covered edges ❶. Because of our <samp class="SANS_TheSansMonoCd_W5Regular_11">Graph</samp>
    class’s implementation of undirected edges, the code adds each undirected edge
    in both directions to <samp class="SANS_TheSansMonoCd_W5Regular_11">edges_covered</samp>.
  prefs: []
  type: TYPE_NORMAL
- en: The main <samp class="SANS_TheSansMonoCd_W5Regular_11">for</samp> loop is similar
    to the heuristic for independent sets, where the code iterates through each node
    in the graph and checks its heuristic value. In this case, the code counts how
    many of the current node’s edges would be newly covered ❷, keeps the best option
    seen so far, and returns it. If there are no nodes that would increase the number
    of covered edges (that is, <samp class="SANS_TheSansMonoCd_W5Regular_11">nodes</samp>
    already forms a valid vertex covering), the code returns <samp class="SANS_TheSansMonoCd_W5Regular_11">-1</samp>.
  prefs: []
  type: TYPE_NORMAL
- en: 'We create a full greedy algorithm by putting a loop around the selection logic
    within [Listing 17-8](#list17-8):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The code starts with an empty list of nodes (<samp class="SANS_TheSansMonoCd_W5Regular_11">nodes</samp>)
    to represent the current selection and uses <samp class="SANS_TheSansMonoCd_W5Regular_11">vertex_cover_greedy_choice()</samp>
    from [Listing 17-8](#list17-8) to add nodes one by one until it has constructed
    a valid vertex covering and no additions would increase the coverage.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we could improve the efficiency of this greedy algorithm by maintaining
    the <samp class="SANS_TheSansMonoCd_W5Regular_11">edges_covered</samp> set in
    the outer loop and passing it into the <samp class="SANS_TheSansMonoCd_W5Regular_11">vertex_cover_greedy_choice()</samp>
    function. This avoids the cost of recomputing it with each iteration. For the
    context of this description, we intentionally recompute <samp class="SANS_TheSansMonoCd_W5Regular_11">edges_covered</samp>
    so as to keep the selection function stand-alone.
  prefs: []
  type: TYPE_NORMAL
- en: As was the case with all the other greedy algorithms in this chapter, the greedy
    algorithm for minimum vertex cover is not guaranteed to be optimal. A seemingly
    good-looking initial choice might prove to be suboptimal in the context of the
    entire graph.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine the planner in our watchtower example working on the islands shown in
    [Figure 17-9](#fig17-9). Determined to keep costs low, the planner selects the
    island with the highest number of bridges (node 0) for first watchtower. This
    is generally a good strategy, as that node covers the most edges. However, in
    this case, it leads to the suboptimal solution shown in [Figure 17-9(a)](#fig17-9).
    By choosing node 0 first, the planner needs to choose three more islands to cover
    the rightmost edges. Worse, they will repeat this mistake again and again. As
    long as the greedy algorithm is using deterministic choices, it will always produce
    the same results.
  prefs: []
  type: TYPE_NORMAL
- en: '![A graph with seven nodes and edges (0, 1), (0, 2), (0, 3), (1, 4), (2, 5),
    and (3, 6). In (A), nodes 0, 1, 2, and 3 are shaded. In (B), nodes 1, 2, and 3
    are shaded.](../images/f17009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<samp class="SANS_Futura_Std_Book_Oblique_I_11">Figure 17-9: The results of
    a non-optimal greedy search (a) compared to the optimal solution (b) on a minimum
    vertex cover problem</samp>'
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, [Figure 17-9(b)](#fig17-9) shows a vertex cover that uses fewer
    nodes. Once we have included nodes 1, 2, and 3, we no longer need to include node
    0.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">Backtracking Search</samp>
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While the backtracking search for minimum vertex cover follows a similar node-by-node
    approach to both maximum clique and independent set construction, constructing
    a vertex cover by adding nodes does not offer the same pruning opportunities.
    Our general pruning approach requires us to start with a valid solution and skip
    choices that make our candidate set invalid. However, a subset of a valid vertex
    cover may not cover each edge and thus may not be a valid vertex cover itself.
    We therefore cannot start with an empty set and build up from there.
  prefs: []
  type: TYPE_NORMAL
- en: We regain the opportunity to prune if we start with a full set of nodes and
    remove them one by one, instead of adding nodes to a set. Since the set of all
    nodes is itself a valid vertex cover, we regain the constraint that we are following
    only branches that remain valid vertex covers.
  prefs: []
  type: TYPE_NORMAL
- en: At each level of recursion, the backtracking search function takes the current
    vertex cover (<samp class="SANS_TheSansMonoCd_W5Regular_11">current</samp>) and
    the next node to test for removal (<samp class="SANS_TheSansMonoCd_W5Regular_11">index</samp>)
    and explores the possibilities if it both does and does not remove that node,
    as shown in [Listing 17-9](#list17-9).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '<samp class="SANS_Futura_Std_Book_Oblique_I_11">Listing 17-9: Recursively exploring
    possible vertex covers</samp>'
  prefs: []
  type: TYPE_NORMAL
- en: The code starts by testing whether it has reached the end of the recursion and
    there are no nodes left to check ❶. If so, it returns a copy of the current vertex
    cover as the best found down this branch.
  prefs: []
  type: TYPE_NORMAL
- en: If the function has not reached the end of the recursion, the code tries vertex
    covers with and without <samp class="SANS_TheSansMonoCd_W5Regular_11">index</samp>.
    In contrast to the searches in Listings 17-4 and 17-7, however, it is considering
    whether to *remove* <samp class="SANS_TheSansMonoCd_W5Regular_11">index</samp>.
    The default option is to leave <samp class="SANS_TheSansMonoCd_W5Regular_11">index</samp>
    in the set by recursively calling the function with the current set and the index
    of the next node. The code saves the result of this branch as the baseline best
    result.
  prefs: []
  type: TYPE_NORMAL
- en: Before removing the node, the code checks whether this removal would break the
    vertex cover. For the set to remain valid without <samp class="SANS_TheSansMonoCd_W5Regular_11">index</samp>,
    all the edges currently covered by that node must be covered by another node in
    <samp class="SANS_TheSansMonoCd_W5Regular_11">current</samp> ❷. The code checks
    this by iterating over each of the current node’s edges and checking whether the
    corresponding neighbor (<samp class="SANS_TheSansMonoCd_W5Regular_11">edge.to_node</samp>)
    is in <samp class="SANS_TheSansMonoCd_W5Regular_11">current</samp>.
  prefs: []
  type: TYPE_NORMAL
- en: If it is viable to remove the current node, the code tries removing <samp class="SANS_TheSansMonoCd_W5Regular_11">index</samp>
    from <samp class="SANS_TheSansMonoCd_W5Regular_11">current</samp> and recursively
    testing the remaining options. It then cleans up the <samp class="SANS_TheSansMonoCd_W5Regular_11">current</samp>
    data by re-adding <samp class="SANS_TheSansMonoCd_W5Regular_11">index</samp> so
    the set can be used in other branches ❸. The code compares the results of the
    two branches and keeps the smaller valid subset of nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We call the function in [Listing 17-9](#list17-9) with an initial <samp class="SANS_TheSansMonoCd_W5Regular_11">current</samp>
    equal to a set of all node indices and <samp class="SANS_TheSansMonoCd_W5Regular_11">index=0</samp>
    by using a wrapper function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 17-10](#fig17-10) provides a visualization of this search where each
    level shows the algorithm branching on the removal (or not) of a single node.
    Nodes assigned to the vertex cover are shaded, excluded nodes are white, and unassigned
    nodes are dashed circles initially included in the vertex cover.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A tree where each node corresponds to a four-node graph with undirected edges
    (0, 1), (0, 3), (1, 2), and (1, 3). At the root node, all graph nodes are dashed
    and gray. At each level of the tree, another node becomes either solid white or
    solid gray.](../images/f17010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<samp class="SANS_Futura_Std_Book_Oblique_I_11">Figure 17-10: A backtracking
    search for finding vertex covers</samp>'
  prefs: []
  type: TYPE_NORMAL
- en: The subgraphs in [Figure 17-10](#fig17-10) show the state of <samp class="SANS_TheSansMonoCd_W5Regular_11">current</samp>
    at the start of each function call. Since the function explores only branches
    containing valid vertex covers, it reaches only 7 of the possible 16 full assignments.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_B_11">Randomized Algorithms</samp>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another approach to solving the types of assignment problems discussed in this
    chapter is to evaluate solutions using *randomized algorithms*. Such algorithms
    use a random number generator to select which node to add to or remove from the
    set next. At first this might seem unlikely to work. Deterministically minded
    users might exclaim, “Why add a random node when we could add the best node with
    greedy search? Won’t we waste a lot of time on bad choices?” While randomized
    algorithms can and will explore suboptimal choices, they offer a few important
    advantages to consider.
  prefs: []
  type: TYPE_NORMAL
- en: First, randomized algorithms avoid the local minima that can trap greedy algorithms.
    As we saw in [Figure 17-9(a)](#fig17-9), greedy searches can lead to suboptimal
    solutions by making each choice in isolation. In contrast, the randomized algorithm
    will occasionally guess a good solution, like the one in [Figure 17-9(b)](#fig17-9).
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, a randomized algorithm lends itself well to parallelization: we can
    run many randomized searches in parallel (without significant coordination), then
    compare the best results found in each. This is equivalent to having multiple
    watchtower planners perform their own randomized searches and compare the results,
    perhaps as part of a kingdom-wide competition where contestants vie to produce
    the best watchtower plan. Each group can work in isolation without the need for
    kingdom-wide coordination.'
  prefs: []
  type: TYPE_NORMAL
- en: In its simplest form, there is nothing to prevent a randomized search from trying
    the same solution multiple times. While we could use additional tracking to avoid
    or at least discourage evaluating duplicate options, this adds complexity and,
    in the case of parallel searches, the need for coordination. In this section,
    we focus on the basics of how randomization works and thus keep the implementations
    simple.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">Basic Randomized
    Search</samp>
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The simplest randomized search selects viable options completely at random.
    Let’s consider how this works in the context of finding maximum independent sets.
    We can use the <samp class="SANS_TheSansMonoCd_W5Regular_11">independent_set_expansion_options()</samp>
    function from [Listing 17-5](#list17-5) as follows to provide a list of feasible
    options:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The code starts with an empty independent set (<samp class="SANS_TheSansMonoCd_W5Regular_11">i_set</samp>)
    and a list of all nodes as potential options (<samp class="SANS_TheSansMonoCd_W5Regular_11">options</samp>).
    The code operates by continuously choosing one of the feasible nodes at random
    ❶, adding it to the independent set, and rebuilding the set of feasible expansion
    options ❷. The code uses the <samp class="SANS_TheSansMonoCd_W5Regular_11">randint()</samp>
    function from Python’s <samp class="SANS_TheSansMonoCd_W5Regular_11">random</samp>
    library to select a node, requiring the inclusion of <samp class="SANS_TheSansMonoCd_W5Regular_11">import
    random</samp> at the top of the file. The loop continues until there are no more
    options to add, at which point the code returns the current independent set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite being randomized, this function is guaranteed to produce valid independent
    sets. During each iteration, the algorithm considers only expansions from a list
    of feasible options, meaning the independent set remains valid after each addition.
    We can use a loop to keep searching for a better solution until we hit some maximum
    number of iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The code starts with an empty independent set (<samp class="SANS_TheSansMonoCd_W5Regular_11">best_iset</samp>)
    as the best result seen so far. It then uses a <samp class="SANS_TheSansMonoCd_W5Regular_11">for</samp>
    loop to generate and test more options. During each iteration, the code generates
    a random independent set using <samp class="SANS_TheSansMonoCd_W5Regular_11">independent_set_random()</samp>
    and compares its size to the best it has seen so far. It tracks the largest independent
    set seen as <samp class="SANS_TheSansMonoCd_W5Regular_11">best_iset</samp> and
    returns it after testing <samp class="SANS_TheSansMonoCd_W5Regular_11">iterations</samp>
    options.
  prefs: []
  type: TYPE_NORMAL
- en: We can picture this search in the context of the earlier example of building
    teams in a dysfunctional organization. The planner is determined to build the
    biggest team but lacks the time to do an exhaustive search. Panicked by their
    tight deadline, they resolve to build 100 random but valid teams and present the
    best one to their boss. For each of their 100 attempts to create a team, they
    use random selection to make sure they at least have the possibility of trying
    options they have not previously considered. After 100 tries, they write up the
    best team and run to their boss’s office to meet the deadline.
  prefs: []
  type: TYPE_NORMAL
- en: Like the greedy search, the randomized search is not guaranteed to find the
    optimal solution. However, unlike the greedy search, the randomized search can
    avoid making the same mistake over and over.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">Weighted Randomized
    Search</samp>
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One potential downside of completely randomized searches is that we have an
    equal probability of picking a promising node or a terrible node. While there
    must be some probability of selecting each node to fully explore the solution
    space, we are not constrained to selecting nodes with equal probability. There’s
    no reason we need to give the office diplomat (who has no interpersonal conflicts)
    and office troublemaker (who has ongoing feuds with half the company) equal shots
    at being on the team.
  prefs: []
  type: TYPE_NORMAL
- en: 'A *weighted randomized algorithm* uses information about the problem structure
    to define a custom probability distribution for selecting nodes. As a simple example,
    consider selecting the next node in the maximum independent set problem. Given
    a subset of nodes *V*′ ⊆ *V* representing the current independent set, we can
    define a set of viable candidates *C* as the set of nodes that are not already
    in *V*′ and do not share an edge with a node in *V*′. Formally, we say:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For each *u* ∈ *C* and *v* ∈ *V*′: *u* ≠ *v* and (*u*, *v*) ∉ *E*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given this candidate set *C,* we can define a probability distribution *p*(*v*)
    of selecting node *v* ∈ *V* where:'
  prefs: []
  type: TYPE_NORMAL
- en: '*p*(*v*) = 0 if *v* ∉ *C*'
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: ∑v *p*(*v*) = 1
  prefs: []
  type: TYPE_NORMAL
- en: For example, we could assign each node a weight that is inversely proportional
    to the number of adjacent edges, making it more likely that we will select nodes
    with fewer neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_B_11">Why This Matters</samp>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For all the problems covered in this chapter and the previous one, it’s easy
    to evaluate a proposed solution but difficult to find the best solution. We’ve
    examined a variety of approaches for solving NP-hard graph assignment problems,
    including greedy searches, randomized searches, exhaustive searches, and customized
    (heuristic) algorithms. Yet no known approach is efficient for all cases.
  prefs: []
  type: TYPE_NORMAL
- en: These problems represent only a subset of the NP-hard graph problems. While
    they do not have known general-purpose efficient algorithms, they often correspond
    to vital real-world questions. Therefore, it is important to understand not only
    the structure of the problems but also practical techniques for solving them.
  prefs: []
  type: TYPE_NORMAL
- en: 'We presented two approaches for each of the problems in this chapter—an approximate
    greedy solution and an exhaustive solution using backtracking search—to illustrate
    the problems and the factors making them computationally difficult. These approaches
    barely scratch the surface of the range of techniques that have been studied.
    For example, the interested reader can find a bounded approximation algorithm
    for vertex cover in Cormen, Leiserson, Rivest, and Stein’s *Introduction to Algorithms*,
    4th edition (MIT Press, 2022). Russell and Norvig’s *Artificial Intelligence:
    A Modern Approach*, 4th edition (Pearson, 2020), provides a good introduction
    to the powerful world of constraint satisfaction algorithms and applying those
    to problems such as graph coloring.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, rather than selecting nodes for a set, we tackle the problem
    of choosing which edges to traverse as part of a tour through the graph.
  prefs: []
  type: TYPE_NORMAL
