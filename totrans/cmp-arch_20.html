<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><h2 class="chna"><span epub:type="pagebreak" id="page_425"/><strong>APPENDIX</strong></h2>&#13;
<h2 class="cht"><strong>OPERATING SYSTEM SUPPORT</strong></h2>&#13;
<div class="image1"><img src="../images/f0425-01.jpg" alt="Image" width="267" height="252"/></div>&#13;
<p class="chq">We’ve avoided discussing operating systems in this book in order to see “bare metal” architecture more clearly. Operating systems are a distinct area of study with their own books. It’s common to study architecture first, then operating systems. However, demand from operating systems has led to several features being added at the architectural level, and these <em>do</em> belong in an architecture book.</p>&#13;
<p class="indent">This <a href="bm01.xhtml">appendix</a> is designed for you to come back to later, during your study of operating systems, as it covers the areas in which the two fields overlap. We’ll review some basic features of operating systems, then look at how recent architectures have developed in order to support them at the hardware level.</p>&#13;
<h3 class="h3" id="lev345"><span epub:type="pagebreak" id="page_426"/>Concurrency</h3>&#13;
<p class="noindent">The most basic function of an operating system is to create the illusion of multiple user programs running simultaneously on a single CPU. The operating system program that does this is usually called the <em>kernel</em>. The user programs being run by the kernel are called <em>processes</em>.</p>&#13;
<p class="indent">The kernel runs each process in turn for a short period of time, before switching to the next one; this is called a <em>cycle</em>, and this form of execution is called <em>concurrency</em>. This means that processes appear to be running in parallel, but are actually being time-sliced, with the slices run in series. Concurrency is roughly the opposite of parallel computing. Parallelism usually takes many CPUs and uses them to execute a single program at the same time. Concurrency takes a single CPU and uses it to execute multiple processes at the same time.</p>&#13;
<p class="indent">The kernel typically uses architectural timers, IRQ lines, and IRQ callbacks to control switching between processes and kernel code itself. At startup, the kernel sets up a hardware timer that creates a regular IRQ to the CPU. The kernel also has a subroutine, which we’ll call a <em>callback</em>, that’s set up to be called when this IRQ appears. The kernel is given a set of processes to run. It loads all of them into memory, at different locations. It then jumps to the first process’s main subroutine, passing control to it to run as normal.</p>&#13;
<p class="indent">The first process will run for a while, then the timer that was previously set will activate an IRQ. The IRQ hardware detects this, makes a copy of the program counter somewhere (such as in a dedicated internal register), and then sets the program counter to the address of the callback.</p>&#13;
<p class="indent">The callback is usually programmed to first save a copy of each of the registers and the previously copied program counter in an area of RAM reserved for use by the kernel (that is, not used by any of the processes). It then decides (schedules) which process to run next. The simplest way to do this is for the processes to take turns in a fixed order. The saved register and program counter states for the new process are loaded into the registers and program counter. The updated program counter thus transfers control to the new process until the timer triggers the next IRQ and calls the callback again.</p>&#13;
<h3 class="h3" id="lev346">Kernel Mode and User Mode</h3>&#13;
<p class="noindent">The kernel will work well as long as the processes can be trusted to play nicely with one another—that is, as long as they access only separate individual areas of memory. It won’t work well if processes are malicious. The obvious security problem is that any process could read and write to memory intended for use by the other processes, and by the kernel itself. This could include stealing data, overwriting data, or overwriting code, including overwriting kernel code to take full control of the machine.</p>&#13;
<p class="indent">Modern CPUs prevent this at the architectural level by providing two (or more) <em>CPU modes</em>, called <em>kernel mode</em> and <em>user mode</em>. In kernel mode, all of the CPU’s features are available for the kernel to use. This includes full access to <span epub:type="pagebreak" id="page_427"/>RAM. In user mode, restrictions are enforced that prevent access to instructions and memory locations outside the region of memory allocated to the user process.</p>&#13;
<h3 class="h3" id="lev347">Virtual Memory</h3>&#13;
<p class="noindent">A modern operating system doesn’t allow user processes to access each other’s data, or the kernel’s own data. Each user process is presented by the operating system with a <em>virtual memory</em> space, which appears to the process as if it were memory in a bare metal machine, isolated from the other processes. For example, all processes might think they’re using memory locations 0x00000000 to 0xffffffff. The physical addresses of memory are thus unavailable to the user program, and processes are separated from one another and can’t read and write each other’s memory. The load and store instructions in user programs work entirely using virtual memory addresses.</p>&#13;
<p class="indent">Virtual memory can also be made substantially larger than physical RAM by making use of <em>swap space</em> with secondary and primary memory. Here, both primary and secondary memories are divided into standard-sized chunks called <em>pages</em>. Caching is used to move whole pages between primary and secondary memory according to how recently they were used.</p>&#13;
<p class="indent">Unlike the hardware CPU and RAM caches we saw previously, this is a slower process that’s usually managed at least partly in software by the operating system. A hardware <em>memory management unit (MMU)</em> may be added to the architectural level to perform translations between physical and virtual addresses as configured by the operating system.</p>&#13;
<p class="indent">Different CPU and operating system combinations will use virtual memory in different ways. For example, a key architecture design decision is whether to use physical or virtual addresses in the different CPU-RAM caches.</p>&#13;
<p class="indent">A <em>translation lookaside buffer (TLB)</em> cache is a dedicated cache designed at an architectural level for the operating system to use to implement its virtual memory. It can exist as a third specialist L1 cache along with the instruction and data L1 caches seen in <a href="ch10.xhtml#ch10fig12">Figure 10-12</a>. When a user program mentions a virtual address, the TLB cache looks up and converts it to a physical address, invisible to the user. If the virtual address is missing from the TLB cache, the TLB then calls back to the operating system code using an IRQ, asking it what to do. The operating system will either find the required virtual-physical mapping and add it to the TLB cache, or it will give an <em>access violation</em> error—often known as a <em>segmentation fault</em>—if it’s not available or allowed. If you’ve ever run into a segmentation fault in your C code before, it arises here, when you try to access memory that isn’t allocated to you.</p>&#13;
<h3 class="h3" id="lev348">Device Drivers</h3>&#13;
<p class="noindent">A modern operating system also doesn’t allow user processes to access I/O addresses directly. Instead, they must call operating system subroutines called <em>device drivers</em> to politely request I/O functionality, via the operating <span epub:type="pagebreak" id="page_428"/>system’s API. As with other processes’ memory, user mode prevents processes from loading or storing outside their designated address space, and it will raise an exception—such as a segmentation fault—if this is attempted.</p>&#13;
<p class="indent">I/O modules and device drivers are different concepts. I/O modules are hardware connected to the bus. A device driver is a higher-level concept, a piece of software that takes sole responsibility for all communications with the I/O module or with one (of the many) devices connected using it; it also provides higher-level interfaces (such as C or C++ libraries) that wrap the memory-mapped instructions. In the 8-bit era, these were simple programs located in ROM or loaded into RAM that were accessible to user programs. Today, they’re usually implemented as kernel modules that are accessible only to the operating system, and user programs will request their use via the operating system.</p>&#13;
<div class="sidebar">&#13;
<p class="stitle"><strong>ARCHITECTURAL OPERATING SYSTEM SECURITY</strong></p>&#13;
<p class="stext">Studying the architectural level opens up many interesting opportunities for operating system security. The operating system generally tries to restrict user program access to most parts of the computer, but if you have access to the architectural level, you may be able to circumvent this. What could you do, for example, if you could physically control the IRQ line used by the operating system timer callback, by opening up your computer, attaching a wire to the IRQ pin, and applying voltages to it at times of your choosing?</p>&#13;
<p class="stext">An ongoing security question is whether device drivers should run in kernel or user mode. Often they’re made part of the operating system and given full access to the machine, but this may be dangerous, as it enables any of the writers of the drivers to access your entire machine. This was considered okay in the days when there were just a few reputable printer manufacturers asking to install their own drivers from a CD in the printer box, but it’s more worrying now that there are many more international and untrusted hardware manufacturers in operation, not to mention the unbranded websites claiming to host drivers for their products.</p>&#13;
</div>&#13;
<h3 class="h3" id="lev349">Loaders</h3>&#13;
<p class="noindent">On an 8-bit machine with no operating system, running an executable file simply requires copying its contents to some location in memory, then setting the program counter of the CPU to point to its first line. This is done by a simple program known as a <em>loader</em> stored in ROM. On a modern machine with an operating system, loaders are more complicated: the executable will be running alongside other processes in an area of virtual rather than real memory. A loader thus has to do some work to set this up and alter the executable to use virtual addresses rather than the physical ones the program thinks it’s using. On Linux, the loader is invoked with a command such as <code>./myexecutable</code>, where the <code>./</code> is technically required for security reasons but in practice functions as the loader command.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_429"/>Let’s try writing, loading, and running a “Hello, world!” program from inside an operating system. (We previously did this on the BIOS.) In particular, the following program is able to run inside a window system such as the X Window System and arrange for the text to be displayed in a terminal rather than directly lighting up ASCII patterns of screen pixels. It does this by calling a kernel function—rather than a BIOS function—to request the text display. The operating system’s loader assumes there’s an externally visible (<code>global</code>) label called <code>_start</code>, to which it jumps after loading in the code:</p>&#13;
<pre>          global    _start&#13;
&#13;
_start:   mov       rax, 1                  ; system call for write&#13;
          mov       rdi, 1                  ; file handle 1 is stdout&#13;
          mov       rsi, message            ; address of string to output&#13;
          mov       rdx, 13                 ; number of bytes&#13;
          syscall                           ; invoke OS to do the write&#13;
          mov       rax, 60                 ; system call for exit&#13;
          xor       rdi, rdi                ; exit code 0&#13;
          syscall                           ; invoke OS to exit&#13;
&#13;
message: db         "Hello, Kernel!", 10    ; note the newline at the end</pre>&#13;
<p class="indent">This code runs on 64-bit Linux only. To assemble and run, use this command:</p>&#13;
<pre>&gt; <span class="codestrong1">nasm -felf64 hellok.asm &amp;&amp; ld -o hellok hellok.o &amp;&amp; ./hellok</span></pre>&#13;
<p class="indent">This should write <code>Hello, Kernel!</code> to the console using only system calls.</p>&#13;
<h3 class="h3" id="lev350">Linkers</h3>&#13;
<p class="noindent">When an operating system–hosted executable calls to subroutines in other libraries, virtual memory addresses need to be further relocated. This is to ensure the executable machine code for each library is loaded into memory at a suitable location, meaning one that doesn’t conflict with the others. Tweaking these addresses also ensures the libraries can find one another. If one program or library calls a function in another, the address of the target subroutine needs to be changed in its executable machine code to the correct location where the target has actually been loaded. Making these tweaks is called <em>linking</em> and is performed by a <em>linker</em> program, usually called invisibly by the loader.</p>&#13;
<p class="indent">As an example of linking, here’s another way to write to the terminal, this time by calling the standard C library’s <code>printf</code> subroutine:</p>&#13;
<pre>global main&#13;
extern printf&#13;
&#13;
msg: db "Hello libC!", 0   ; 0 = ASCII endofstring&#13;
fmtstr: db "%s", 10, 0     ; ASCII newline and endofstring&#13;
fmtint: db '%10d', 10, 0   ; ASCII newline and endofstring&#13;
&#13;
main:&#13;
    mov rdi,fmtstr&#13;
    mov rsi,msg     ; pointer to msg&#13;
    mov rax,0       ; num of extra stack args used (none)&#13;
    call printf     ; call C function&#13;
&#13;
    mov rdi,fmtint&#13;
    mov rsi,124     ; 124 is an int to print out&#13;
    mov rax,0       ; num of extra stack args used (none)&#13;
    call printf     ; call C function&#13;
    ret</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_430"/>With this style, you can call any C libraries from your assembly programs, as long as you respect their calling conventions. Because it’s part of a C compiler stack, the <code>gcc</code> compiler looks for an externally visible (<code>global</code>) subroutine named <code>main</code>, as in C. It will create its own lower-level <code>_start</code> subroutine and set it to call <code>main</code>; it will also set up any structures needed by the C libraries.</p>&#13;
<p class="indent">Note that because <code>printf</code> can take a variable number of arguments, we have to tell it how many extra arguments are used and should be expected on the stack; we set this number in RAX. This is standard in most x86 calling conventions for variable arguments.</p>&#13;
<p class="indent">To assemble, link, and run on 64-bit Linux, use this command:</p>&#13;
<pre>&gt; <span class="codestrong1">nasm -felf64 helloc.asm ; gcc -no-pie -o helloc helloc.o ; ./helloc</span></pre>&#13;
<p class="indent">You can see what extra code the linker has added by <em>disassembling</em>—that is, converting the machine code back to human-readable assembly. You can do this with a tool like <code>objdump:</code></p>&#13;
<pre>&gt; <span class="codestrong1">objdump -d helloc</span></pre>&#13;
<p class="indent">Some operating systems make use of x86 segments—or, at least, their assembler directives—to enforce a read-only <code>.text</code> section in the code. They typically allow writes in the <code>.data</code> section.</p>&#13;
<h3 class="h3" id="lev351">Extra Boot Sequence Stages</h3>&#13;
<p class="noindent">Most systems can’t boot an operating system directly at power on. Operating systems are responsible for loading and configuring device drivers, which aren’t available when the operating system still needs to be loaded. Instead, they’re gradually brought into being during later stages of the boot process.</p>&#13;
<p class="indent">We met BIOS and UEFI previously in <a href="ch13.xhtml">Chapter 13</a>. Usually only two programs ever get run on your BIOS: an operating system loader and an operating system loader selector program, such as GRUB2 (Grand Unified Bootloader version 2). PCBIOS runs the first such program from a specific <span epub:type="pagebreak" id="page_431"/>hard disk location called the master boot record. UEFI now has a higher-level view of the filesystem than this, and it includes a specific path on the hard disk to look for and run the first of these programs. GRUB2 provides a text-based user interface, displaying a list of operating systems available on a hard disk and allowing the user to input their selection using their cursor and other keys. GRUB2 checks what kind of BIOS is available, then calls the available subroutines from that BIOS to write the characters on the screen and read the keyboard. When the user makes a selection, it loads that operating system loader and passes control to it.</p>&#13;
<p class="indent">The operating system loader is thus the first program that’s part of the operating system. It will initially rely on the BIOS libraries to access the computer, especially the hard disk that contains the code for the rest of the operating system. An operating system may have its own drivers, hopefully better than those of the BIOS, and it will progressively load and switch over to them. For example, BIOS graphics are by design low resolution so that they work on any monitor, but once the operating system loads it can consider the precise make and model of the monitor and load a new custom driver that can make use of all its features.</p>&#13;
<p class="indent">Modern boot processes have been controversial for security reasons. The boot process occurs before the operating system kicks in, meaning it has access to the entirety of the computer. UEFI keeps running in the background once the operating system has started, allowing the operating system to call its subroutines. But this means that any malicious code built into UEFI firmware could potentially retain access to the whole machine during regular operating system operation.</p>&#13;
<p class="indent">UEFI was designed by a committee whose members included proprietary operating system vendors who successfully lobbied for the introduction of a “secure boot” part of its standard. This allows the boot process to be locked down so that buyers of preinstalled machines can’t install GRUB2 and other operating systems. It’s possible to fix this bug in the standard if you’re able to reset the secure boot system itself. This is usually done by soldering two wires to the UEFI chip and applying a voltage to factory-reset it.</p>&#13;
<p class="indent">Since around 2008, rumors have circulated that Intel motherboards have included an entire additional operating system, based on MINIX3, running somewhere in the boot process between UEFI and the main operating system, as the “Intel Management Engine.” If true, these rumors would suggest a major security loophole, as this operating system would have full access to the entire machine, including internet communications and automatic update systems, which would enable Intel or others to push code at any time over the network to run with full read and write access to your computer. These rumors would also suggest that MINIX is now the most widely run operating system in the world—this would be somewhat ironic, as MINIX was created as an educational operating system, with Linux considered to be the more “real world” evolution of it.</p>&#13;
<h3 class="h3" id="lev352"><span epub:type="pagebreak" id="page_432"/>Hypervisor Mode, Virtualization, and Containers</h3>&#13;
<p class="noindent">Kernel mode is sometimes known as <em>supervisor mode</em>, the supervisor being the kernel that controls the switching of the processes being run. <em>Hypervisor mode</em> is a related but higher-level concept in which—rather than switching between multiple processes within an operating system—the CPU switches between multiple operating systems running concurrently. This concept has become especially important in current cloud computing, in which the many machines in a computer center are shared in this way to provide each user with the experience of being on a scalable group of machines as their root user.</p>&#13;
<p class="indent">Similar operating system sharing can also be achieved using software only: there are programs that emulate or simulate virtual machines. However, these incur performance hits, while hypervisors don’t. With a hypervisor, each operating system really is running directly on the hardware. Dedicated hypervisor architecture is used to manage the swapping of state in and out of the hardware, in a similar style to how a software supervisor swaps processes in and out of execution. Some virtual machine programs, such as the VirtualBox program used in <a href="ch13.xhtml">Chapter 13</a>, can make use of the hypervisor to run their virtual machines on hypervised processors.</p>&#13;
<p class="indent"><em>Containerization</em> is an alternative to virtualization. Rather than creating a set of completely isolated virtual machines, it works together with additional software to create the <em>appearance</em> of many such machines, while having them all actually share a single operating system and other components such as software libraries. (This is arguably what operating systems were intended to do in the first place. But unlike operating systems, containers enable different users to experience different installations and versions of the system, libraries, and installed software.) This is a lighter-weight solution than virtual machines, and it can enable thousands of containers to run together, for different users, on a single computer. Containerization is especially useful for cloud computing, in which thousands of users want to run isolated programs and providers want to minimize costs by having them share a single physical machine.</p>&#13;
<h3 class="h3" id="lev353">Real-Time Operating Systems</h3>&#13;
<p class="noindent">Most embedded systems run just a single small, simple program, so they have no need for an operating system. However, as the needs of some embedded systems grow in complexity, it’s becoming easier and more common to program them as multiple processes. At this stage it can make sense to start running a small operating system on the embedded system, to manage these multiple processes.</p>&#13;
<p class="indent">Embedded environments typically have special requirements for an operating system, most commonly the need for what’s called <em>hard real time</em>. Regular operating systems may switch between processes in a way that, from the programs’ point of view, seems random; their device drivers will often use buffering and interrupts to read and write data also at apparently random times. Such behaviors would be catastrophic for, say, a precision <span epub:type="pagebreak" id="page_433"/>industrial robot controller, working in microseconds and micrometers, as they would interfere with its required level of precision motion in the real world. A hard real-time operating system (RTOS)—such as SMX, QNX, FreeRTOS, or Zephyr—is an operating system specifically designed from the ground up to absolutely guarantee the timing of such tasks. This requires different approaches to scheduling and I/O. Typically, an embedded microcontroller is a much lower-power machine than a desktop, so operating system design requirements must also include low computational overheads.</p>&#13;
<p class="indent">To be used in safety-critical environments, an RTOS, like the microcontroller it runs on, will typically go through an expensive and rigorous safety-assurance process based on either extensive testing or, in the most hardcore cases, formal specification and verification, using mathematics and logic to prove it will always work under various assumptions.</p>&#13;
<p class="indent">RTOSes are distinguished from <em>soft</em> real-time operating systems, such as variants of Linux modified for tasks like computer audio production. In these systems, real time is desirable but not strictly necessary—it won’t, say, explode a nuclear power station if it can’t be absolutely guaranteed every time—and so occasional slips are tolerated.</p>&#13;
<h3 class="h3" id="lev354">Speculative Execution Vulnerabilities</h3>&#13;
<p class="noindent">In our study of architecture, we’ve seen that your computer takes your program and converts it to thousands of different instructions, messes with the order these instructions are run in, tries to execute parts of multiple instructions at once, passes incomplete results between instructions, and secretly updates its microcode to execute in new ways.</p>&#13;
<p class="indent">Each of these behaviors, and the interactions between them, creates enormous complexity in chip design and function. The resulting chip designs are thus some of the most complex systems known to humanity, with no individual human able to fully understand everything taking place in a CPU. It’s natural to ask whether we can thus be confident that our CPU designs are safe and secure when there are so many parts that could go wrong.</p>&#13;
<p class="indent">The answer to this question was recently found to be “no”—this is why we now have <em>speculative execution</em> vulnerabilities, architecture bugs that can enable a process to read the data belonging to another process, such as passwords and bank details. In most cases, this includes the ability for hyper-vised systems belonging to different users on physical cloud machines to spy on one another. This has been considered a catastrophic security threat to many manufacturers; some consider it the most serious hardware problem of all time. Software patches for the vulnerabilities cause a 5 to 30 percent slowdown in performance, while architects are currently working to redesign hardware to avoid them in their next-generation processors.</p>&#13;
<p class="indent">Speculative execution vulnerabilities were first discovered in 2018 as bugs called Spectre and Meltdown, and new variants continue to be found at the time of writing. To give a basic understanding of this large class of bugs, we’ll examine the Meltdown variant here.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_434"/>Meltdown is caused by a complex unintended interaction between multiple modern architectural features: speculative execution, virtual memory, CPU kernel mode switching, cache timing effects, and a race condition in indirect addressing. Suppose the target process is running alongside our own process under an operating system. The operating system defines separate areas of memory for the two processes and restricts each process’s access to only its own memory space. The memory spaces look like this:</p>&#13;
<table class="allc">&#13;
<colgroup>&#13;
<col style="width:50%"/>&#13;
<col style="width:50%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr>&#13;
<th style="vertical-align: top" class="borderb"><p class="tabtextc"><strong>Address</strong></p></th>&#13;
<th style="vertical-align: top" class="borderb"><p class="tabtextc"><strong>Data</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td style="vertical-align: top" class="gray"><p class="tabtextc">1</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="tabtextc"> </p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tabtextc">2</p></td>&#13;
<td style="vertical-align: top"><p class="tabtextc"> </p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="gray"><p class="tabtextc">3=BASE</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="tabtextc"> </p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tabtextc">4=TEST1</p></td>&#13;
<td style="vertical-align: top"><p class="tabtextc">FOO</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="gray"><p class="tabtextc">5=TEST2</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="tabtextc">FOO</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tabtextc">6=TEST3</p></td>&#13;
<td style="vertical-align: top"><p class="tabtextc">FOO</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<table class="allc">&#13;
<colgroup>&#13;
<col style="width:50%"/>&#13;
<col style="width:50%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr>&#13;
<th style="vertical-align: top" class="borderb"><p class="tabtextc"><strong>Address</strong></p></th>&#13;
<th style="vertical-align: top" class="borderb"><p class="tabtextc"><strong>Target’s Data</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td style="vertical-align: top" class="gray"><p class="tabtextc">7</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="tabtextc"> </p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tabtextc">8=TARGET</p></td>&#13;
<td style="vertical-align: top"><p class="tabtextc">PASSWORD</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="gray"><p class="tabtextc">9</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="tabtextc"> </p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tabtextc">10</p></td>&#13;
<td style="vertical-align: top"><p class="tabtextc"> </p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="gray"><p class="tabtextc">11</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="tabtextc"> </p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tabtextc">6</p></td>&#13;
<td style="vertical-align: top"><p class="tabtextc"> </p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">Here, we assume we have access to the source code of the target program that tells us where the user password will be stored in its memory, so the contents of address <code>TARGET</code>, written as <code>*TARGET</code>, is <code>PASSWORD</code>, which we’ll assume is known in advance to be an integer from 1 to 3. We want to read this password from our own process. Our own process’s address space contains a series of addresses marked <code>TEST1</code>, <code>TEST2</code>, and <code>TEST3</code>. We can store any dummy data at these locations, marked as <code>FOO</code>. We’ll be reading this data as part of our attack, but we don’t actually care what its values are. Let’s call the address just before these <code>BASE</code>, because it will act as a base address from which we can use offsets to refer to each of the <code>TEST</code> addresses.</p>&#13;
<p class="indent">To attack, we first execute an indirect offset addressing instruction together with a conditional:</p>&#13;
<pre>if (0) LOAD BASE+(*TARGET) else LOAD 1</pre>&#13;
<p class="indent">Although the semantics of <code>if (0)</code> mean that the condition will never be true—meaning the <code>LOAD BASE+(*TARGET)</code> won’t be completely performed in the program—eager execution (as in <a href="ch08.xhtml">Chapter 8</a>) initially begins to run both branches at the same time. When it does this, <code>BASE+(*TARGET)</code> will be evaluated, giving an address that must be one of 4, 5, or 6. The content of the data at this address (one of the three <code>FOO</code> items) will then be loaded into cache. (The <code>FOO</code> from address 1 is also loaded to cache from the other side of the branch.) While this is happening, the condition is tested and found to be false. At this point the <code>LOAD BASE+(*TARGET)</code> instruction is aborted, but its value has already been loaded into cache even though it won’t be used any further.</p>&#13;
<p class="indent">Note that if the condition were true instead of false, the <code>LOAD BASE+(*TARGET)</code> would then attempt to complete and at that point, and only at that point, a security exception would occur as the <code>TARGET</code> address is tested for security and found to lie in another process’s address space. But because the condition is actually false, this test is never performed.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_435"/>Once the value is loaded into the cache, we run a cache timing attack:</p>&#13;
<pre>for (i=1:3) time(LOAD BASE+i)</pre>&#13;
<p class="indent">All three of the instructions in the loop succeed, loading the three <code>FOO</code> values into registers from their three memory locations. But if we time each of these three <code>LOAD</code>s, we’ll find that one of them is faster than the others because it was cached during the speculative execution. If <code>PASSWORD=i</code>, then <code>LOAD BASE+i</code> is fast, because (<code>BASE+i</code>) was cached. Measuring these times and finding the fast one reveals the value of <code>i</code>, which is equal to <code>PASSWORD</code>, as required.</p>&#13;
<p class="indent">The Meltdown vulnerability existed undetected in almost all major commercial CPUs for 20 years! It may have been known and exploited by secret state actors during this time, but it hasn’t yet been exploited by any other malware as far as we know.</p>&#13;
<p class="indent">The public disclosure sequence of Meltdown in 2017 was a model of how ethical security bug disclosure can and should work. Following public discovery by security researchers, the manufacturers were first informed in secret. Researchers, CPU manufacturers, and operating system programmers then worked together to patch the bug at the operating system software level for all major operating systems. These operating systems were updated in the field by pushing automatic updates to users.</p>&#13;
<p class="indent">The operating system level software patch is called KAISER. Here, the operating system randomizes process memory locations to prevent a Meltdown attack from knowing which addresses to search for target data. This is still not completely secure but makes the bug much harder to exploit. After user machines had been patched with KAISER, the discoverers of Meltdown published their findings in 2018, first immediately on the pre-print arXiv server, then submitted for formal academic peer review, which completed and published in 2020.</p>&#13;
<p class="indent">CISC processors are constructed using microcode that enables their hardware to be “rewired” to some extent by CPU firmware updates; this provides a stronger fix for CISC users. Pushing microcode updates is a more difficult and dangerous procedure than patching operating system software, and developing hardware patches also takes longer, in part because of the extensive testing required before allowing a patch to be pushed out. The cost of “bricking” millions of users’ processors is higher than damaging their operating system, which could be more easily reinstalled in the event of a bad update. Microcode patch development thus continued after publication of the Meltdown paper and was later pushed out as firmware updates for CISC users.</p>&#13;
<p class="indent">The new microcode adds logic to clear cache following all speculative executions, removing the vulnerability. However, this has a cost of a significant performance hit, typically producing a 5 to 30 percent slowdown. Pushing such a performance hit onto users—usually automatically, without telling or asking them—led to some lively debate, especially between the operating system programmers whose work on software-level patches was being replaced by the microcode patches.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_436"/>At the time of writing, CPU architects are working to redesign their basic architectures to fix Meltdown properly at the hardware level. In 2022, some of these fixes for Meltdown were reported by researchers to have introduced a new speculative execution bug, which they named Retbleed. This may become an ongoing game of whack-a-mole, providing employment for architects for many years to come.</p>&#13;
<h3 class="h3" id="lev355">Exercises</h3>&#13;
<h4 class="h4a"><strong>A 6502 Kernel</strong></h4>&#13;
<p class="noindent">Read the assembly code for Joachim Deboy’s minimal 6502 kernel at <em><a href="http://6502.org/source/kernels/minikernel.txt">http://6502.org/source/kernels/minikernel.txt</a></em>. Explain where the IRQs, saves, and restores occur. Try to make an x86 or RISC-V version of the same idea.</p>&#13;
<h4 class="h4a"><strong>Speculative Execution Vulnerability Audit</strong></h4>&#13;
<p class="noindent">Find out if and how your own computer has been patched for speculative execution vulnerabilities. For Linux, <code>lscpu</code> may show some relevant information.</p>&#13;
<h3 class="h3" id="lev356">Further Reading</h3>&#13;
<ul class="bullet">&#13;
<li class="tm">The definitive textbook on operating systems is Andrew Tanenbaum and Herbert Bos, <em>Modern Operating Systems</em>, 4th ed. (Hoboken: Pearson, 2014).</li>&#13;
<li class="tm">For a list of all the subroutines Linux provides for you to call from your x86 code, see R.A. Chapman, “Linux System Calls for x86,” <em><a href="https://blog.rchapman.org/posts/Linux_System_Call_Table_for_x86_64/">https://blog.rchapman.org/posts/Linux_System_Call_Table_for_x86_64/</a></em>.</li>&#13;
<li class="tm">For more on the Meltdown vulnerability, see M. Lipp et al., “Meltdown: Reading Kernel Memory from User Space,” <em>Communications of the ACM</em> 63, no. 6 (2020): 46–56.</li></ul>&#13;
</div>
</div>
</body></html>