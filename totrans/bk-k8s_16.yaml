- en: '14'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LIMITS AND QUOTAS
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/common01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For our cluster to provide a predictable environment for applications, we need
    some control over what resources each individual application component uses. If
    an application component can use all of the CPU or memory on a given node, the
    Kubernetes scheduler will not be able to allocate a new Pod to a node confidently,
    as it won’t know how much available space each node has.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll explore how to specify requested resources and limits
    to ensure that containers get the resources they need without impacting other
    containers. We’ll inspect individual containers at the runtime level so that we
    can see how Kubernetes configures the container technology we saw in [Part I](part01.xhtml#part01)
    to adequately meet the resource requirements of a container without allowing the
    container to exceed its limits.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we’ll look at how role-based access control is used to manage quotas,
    limiting the amount of resources a given user or application can demand, which
    will help us understand how to administer a cluster in a manner that allows it
    to reliably support multiple separate applications or development teams.
  prefs: []
  type: TYPE_NORMAL
- en: Requests and Limits
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Kubernetes supports many different types of resources, including processing,
    memory, storage, network bandwidth, and use of special devices such as graphics
    processing units (GPUs). We’ll look at network limits later in this chapter, but
    let’s start with the most commonly specified resource types: processing and memory.'
  prefs: []
  type: TYPE_NORMAL
- en: Processing and Memory Limits
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The specifications for processing and memory resources serve two purposes:
    scheduling and preventing conflicts. Kubernetes provides a different kind of resource
    specification for each purpose. The Pod’s containers consume processing and memory
    resources in Kubernetes, so that’s where resource specifications are applied.'
  prefs: []
  type: TYPE_NORMAL
- en: When scheduling Pods, Kubernetes uses the `requests` field in the container
    specification, summing this field across all containers in the Pod and finding
    a node with sufficient margin in both processing and memory. Generally, the `requests`
    field is set to the expected average resource requirements for each container
    in the Pod.
  prefs: []
  type: TYPE_NORMAL
- en: The second purpose of resource specification is preventing denial-of-service
    issues in which one container takes all of a node’s resources, negatively affecting
    other containers. This requires runtime enforcement of container resources. Kubernetes
    uses the `limits` field of the container specification for this purpose, thus
    we need to be sure to set the `limits` field high enough that a container is able
    to run correctly without reaching the limit.
  prefs: []
  type: TYPE_NORMAL
- en: '**TUNING FOR PERFORMANCE**'
  prefs: []
  type: TYPE_NORMAL
- en: The idea that requests should match the expected average resource requirements
    is based on an assumption that any load spikes in the various containers in the
    cluster are unpredictable and uncorrelated, and load spikes can therefore be assumed
    to happen at different times. Even with that assumption, there is a risk that
    simultaneous load spikes in multiple containers on a node will result in that
    node being overloaded. And if the load spikes between different Pods are correlated,
    this risk of overload increases. At the same time, if we configure `requests`
    for the worst case scenario, we can end up with a very large cluster that is idle
    most of the time. In [Chapter 19](ch19.xhtml#ch19), we explore the different Quality
    of Service (QoS) classes that Kubernetes offers for Pods and discuss how to find
    a balance between performance guarantees and cluster efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 14-1](ch14.xhtml#ch14list1) kicks off our examination with an example
    of using requests and limits with a Deployment.'
  prefs: []
  type: TYPE_NORMAL
- en: '*nginx-limit.yaml*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 14-1: Deployment with limits*'
  prefs: []
  type: TYPE_NORMAL
- en: We’ll use this Deployment to explore how limits are configured at the level
    of the container runtime, so we use the `nodeName` field to make sure the container
    ends up on *host01*. This constrains where the scheduler can place the Pod, but
    the scheduler still uses the `requests` field to ensure that there are sufficient
    resources. If *host01* becomes too busy, the scheduler will just refuse to schedule
    the Pod, similar to what we saw in [Chapter 10](ch10.xhtml#ch10).
  prefs: []
  type: TYPE_NORMAL
- en: The `resources` field is defined at the level of the individual container, allowing
    us to specify separate resource requirements for each container in a Pod. For
    this container, we specify a memory request of `64Mi` and a memory limit of `128Mi`.
    The suffix `Mi` means that we are using the power-of-2 unit *mebibytes*, which
    is 2 to the 20th power, rather than the power-of-10 unit *megabytes*, which would
    be the slightly smaller value of 10 to the 6th power.
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, the processing request and limit specified using the `cpu` fields
    is not based on any absolute unit of processing. Rather, it is based on a synthetic
    *cpu unit* for our cluster. Each cpu unit roughly corresponds to one virtual CPU
    or core. The `m` suffix specifies a *millicpu* so that our `requests` value of
    `250m` equates to one quarter of a core, whereas the `limit` of `500m` equates
    to half of a core.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*The example repository for this book is at* [https://github.com/book-of-kubernetes/examples](https://github.com/book-of-kubernetes/examples).
    *See “Running Examples” on [page xx](ch00.xhtml#ch00lev1sec2) for details on getting
    set up.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create this Deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The Pod will be allocated to `host01` and started:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'And `host01` will show that resources have been allocated for the Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This is true even though our NGINX web server is idle and is not using a lot
    of processing or memory resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Similar to what we saw in [Chapter 12](ch12.xhtml#ch12), this command queries
    the metrics add-on that is collecting data from `kubelet` running on each cluster
    node.
  prefs: []
  type: TYPE_NORMAL
- en: Cgroup Enforcement
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The processing and memory limits we specified are enforced using the Linux
    control group (cgroup) functionality we described in [Chapter 3](ch03.xhtml#ch03).
    Kubernetes manages its own space within each hierarchy inside the */sys/fs/cgroup*
    filesystem. For example, memory limits are configured in the memory cgroup:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Each Pod on a given host has a directory within the *kubepods.slice* tree. However,
    finding the specific directory for a given Pod takes some work because Kubernetes
    divides Pods into different classes of service, and because the name of the cgroup
    directory does not match the ID of the Pod or its containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'To save us from searching around inside */sys/fs/cgroup*, we’ll use a script
    installed by this chapter’s automated scripts: */opt/cgroup-info*. This script
    uses `crictl` to query the container runtime for the cgroup path and then collects
    CPU and memory limit data from that path. The most important part of the script
    is this section that collects the path:'
  prefs: []
  type: TYPE_NORMAL
- en: '*cgroup-info*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `crictl pods` command collects the Pod’s ID, which is then used with `crictl
    inspectp` and `jq` to collect one specific field, called `cgroup_parent`. This
    field is the cgroup subdirectory created for that pod within each resource type.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s run this script with our NGINX web server to see how the CPU and memory
    limits have been configured:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We first collect the name of the Pod and then use it to collect cgroup information.
    Note that this works only because the Pod is running on `host01`; the script will
    work for any Pod, but it must be run from the host on which that Pod is running.
  prefs: []
  type: TYPE_NORMAL
- en: There are two key pieces of data for the CPU configuration. The quota is the
    hard limit; it means that in any given 100,000 microsecond period, this Pod can
    use only 50,000 microseconds of processor time. This value corresponds to the
    `500m` CPU limit specified in [Listing 14-1](ch14.xhtml#ch14list1) (recall that
    the `500m` limit equates to half a core).
  prefs: []
  type: TYPE_NORMAL
- en: In addition to this hard limit, the CPU request field we specified in [Listing
    14-1](ch14.xhtml#ch14list1) has been used to configure the CPU shares. As we saw
    in [Chapter 3](ch03.xhtml#ch03), this field configures the CPU usage on a relative
    basis. Because it is relative to the values in neighboring directories, it is
    unitless, so Kubernetes computes the CPU share on the basis of one core equaling
    1,024\. We specified a CPU request of `250m`, so this equates to 256.
  prefs: []
  type: TYPE_NORMAL
- en: The CPU share does not set any kind of limit on CPU usage, so if the system
    is idle, a Pod can use processing up to its hard limit. However, as the system
    becomes busy, the CPU share determines how much processing each Pod is allotted
    relative to others in the same class of service. This helps to ensure that if
    the system becomes overloaded, all Pods will be degraded fairly based on their
    CPU request.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, for memory, there is a single relevant value. We specified a memory
    limit of `128Mi`, which equates to 128MiB. As we saw in [Chapter 3](ch03.xhtml#ch03),
    if our container tries to exceed this limit, it will be terminated. For this reason,
    it is critical to either configure the application such that it does not exceed
    this value, or to understand how the application acts under load to choose the
    optimum limit.
  prefs: []
  type: TYPE_NORMAL
- en: The amount of memory actually used by a process is ultimately up to that process,
    meaning that the memory request value has no purpose beyond its initial use in
    ensuring sufficient memory to schedule the Pod. For this reason, we don’t see
    the memory request value of `64Mi` being used anywhere in the cgroup configuration.
  prefs: []
  type: TYPE_NORMAL
- en: The way that resource allocations are reflected in cgroups shows us something
    important about cluster performance. Because `requests` is used for scheduling
    and `limits` is used for runtime enforcement, it is possible for a node to overcommit
    processing and memory. For the case in which containers have higher `limit` than
    `requests`, and containers consistently operate above their `requests`, this can
    cause performance issues with the containers on a node. We’ll discuss this in
    more detail in [Chapter 19](ch19.xhtml#ch19).
  prefs: []
  type: TYPE_NORMAL
- en: 'We’re finished with our NGINX Deployment, so let’s delete it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: So far, the container runtime can enforce the limits we’ve seen. However, the
    cluster must enforce other types of limits, such as networking.
  prefs: []
  type: TYPE_NORMAL
- en: Network Limits
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Ideally, our application will be architected so that required bandwidth for
    intercommunication is moderate, and our cluster will have sufficient bandwidth
    to meet the demand of all the containers. However, if we do have a container that
    tries to take more than its share of the network bandwidth, we need a way to limit
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because the network devices are configured by plug-ins, we need a plug-in to
    manage bandwidth. Fortunately, the `bandwidth` plug-in is part of the standard
    set of CNI plug-ins installed with our Kubernetes cluster. Additionally, as we
    saw in [Chapter 8](ch08.xhtml#ch08), the default CNI configuration enables the
    `bandwidth` plug-in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: As a result, `kubelet` is already calling the `bandwidth` plug-in every time
    a new Pod is created. If a Pod is configured with bandwidth limits, the plug-in
    uses the Linux kernel’s traffic control capabilities that we saw in [Chapter 3](ch03.xhtml#ch03)
    to ensure the Pod’s virtual network devices don’t exceed the specified limit.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at an example. First, let’s deploy an `iperf3` server that will
    listen for client connections:'
  prefs: []
  type: TYPE_NORMAL
- en: '*iperf-server.yaml*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In addition to a Deployment, we also create a Service. This way, our `iperf3`
    clients can find the server under its well-known name of `iperf-server`. We specify
    port 5201, which is the default port for `iperf3`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s deploy this server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s run an `iperf3` client without applying any bandwidth limits. This will
    give us a picture of how fast our cluster’s network is without any traffic control.
    Here’s the client definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '*iperf.yaml*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Normally, `iperf3` in client mode would run once and then terminate. This image
    has a script that runs `iperf3` repeatedly, sleeping for one minute between each
    run. Let’s start a client Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'It will take a few seconds for the Pod to start running, after which it will
    take 10 seconds for the initial run. After 30 seconds or so, the Pod log will
    show the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In this case, we see a transfer rate of `1.36 GBits/sec` between our client
    and server. Your results will be different depending on how your cluster is deployed
    and whether the client and server end up on the same host.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before moving on, we’ll shut down the existing client to prevent it from interfering
    with our next test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Obviously, while it’s running, `iperf3` is trying to use as much network bandwidth
    as possible. That’s fine for a test application, but it isn’t polite behavior
    for an application component in a Kubernetes cluster. To limit its bandwidth,
    we’ll add an annotation to the Pod definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '*iperf-limit.yaml*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We’ll want to inspect how the limits are being applied to the network devices,
    which will be easier if this Pod ends up on `host01`, so we set `nodeName` accordingly.
    Otherwise, the only change in this Pod definition is the `annotations` section
    in the Pod metadata ➊. We set a value of `1M` for ingress and egress, corresponding
    to a 1Mb bandwidth limit on the Pod. When this Pod is scheduled, `kubelet` will
    pick up these annotations and send the specified bandwidth limits to the bandwidth
    plug-in so that it can configure Linux traffic shaping accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create this Pod and get a look at this in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'As before, we wait long enough for the client to complete one test with the
    server and then print the logs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The change is significant, as the Pod is limited to a fraction of the speed
    we saw with an unlimited client. However, because the traffic shaping is based
    on a token bucket filter, the traffic control is inexact over shorter intervals,
    so we see a bitrate of around 20Mb rather than 1Mb. To see why, let’s look at
    the actual traffic shaping configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `bandwidth` plug-in is applying this token bucket filter to the host side
    of the virtual Ethernet (veth) pair that was created for the Pod, so we can see
    it by showing traffic control configuration for the host interfaces:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The combination of `rate` and `burst` shows why our Pod was able to achieve
    20Mb over the 10-second test run. Because of the `burst` value, the Pod was able
    to send a large quantity of data immediately, at the cost of spending several
    seconds without any ability to send or receive. Over a much longer interval, we
    would see an average of 1Mbps, but we would still see this bursting behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before moving on, let’s clean up our client and server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Managing the bandwidth of a Pod can be useful, but as we’ve seen, the bandwidth
    limit can behave like an intermittent connection from the Pod’s perspective. For
    that reason, this kind of traffic shaping should be considered a last resort for
    containers that cannot be configured to moderate their own bandwidth usage.
  prefs: []
  type: TYPE_NORMAL
- en: Quotas
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Limits allow our Kubernetes cluster to ensure that each node has sufficient
    resources for its assigned Pods. However, if we want our cluster to host multiple
    applications reliably, we need a way to control the amount of resources that any
    one application can request.
  prefs: []
  type: TYPE_NORMAL
- en: To do this, we’ll use quotas. Quotas are allocated based on Namespaces; they
    specify the maximum amount of resources that can be allocated within that Namespace.
    This includes not only the primary resources of CPU and memory but also specialized
    cluster resources such as GPUs. We can even use quotas to specify the maximum
    number of a specific object type, such as a Deployment, Service, or CronJob, that
    can be created within a given Namespace.
  prefs: []
  type: TYPE_NORMAL
- en: Because quotas are allocated based on Namespaces, they need to be used in conjunction
    with the access controls we described in [Chapter 11](ch11.xhtml#ch11) to ensure
    that a given user is bound by the quotas we create. This means that creating Namespaces
    and applying quotas is typically handled by the cluster administrator.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create a sample Namespace for our Deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s create a *ResourceQuota* resource type to apply a quota to the Namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '*quota.yaml*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This resource defines a quota for CPU and memory for both requests and limits.
    The units are the same as those used for limits in the Deployment specification
    in [Listing 14-1](ch14.xhtml#ch14list1).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s apply this quota to the `sample` Namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that this quota has been applied successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Even though this quota will apply to all users that try to create Pods in the
    Namespace, even cluster administrators, it’s more realistic to use a normal user,
    given that an administrator can always create new Namespaces to get around a quota.
    Thus, we’ll also create a user:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'As we did in [Chapter 11](ch11.xhtml#ch11), we’ll bind the `edit` role to this
    user to provide the right to create and edit resources in the `sample` Namespace.
    We’ll use the same RoleBinding that we saw in [Listing 11-1](ch11.xhtml#ch11list1):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that our user is set up, let’s set the `KUBECONFIG` environment variable
    so that future `kubectl` commands will operate as our normal user:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we can verify that the `edit` role possessed by our normal user does
    not enable making changes to quotas in a Namespace, which makes sense—quotas are
    an administrator function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now create some Pods in the `sample` Namespace to test the quota. First,
    let’s try to create a Pod with no limits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Because our Namespace has a quota, we are no longer allowed to create Pods without
    specifying limits.
  prefs: []
  type: TYPE_NORMAL
- en: In [Listing 14-2](ch14.xhtml#ch14list2), we try it again, this time using a
    Deployment that specifies resource limits for the Pods it creates.
  prefs: []
  type: TYPE_NORMAL
- en: '*sleep.yaml*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 14-2: Deployment with Limit*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can apply this to the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'This is successful because we specified the necessary request and limit fields
    and we didn’t exceed our quota. Additionally, a Pod is started with the limits
    we specified:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'However, we can see that we’re now using resources out of our quota:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This will limit our ability to scale this Deployment. Let’s illustrate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ve asked for 12 replicas, but we see only three running. If we describe
    the Deployment we can see an issue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'And the Namespace now reports that we have used up enough of our quota that
    there is no room to allocate the resources needed for another Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Our Pods are running `sleep`, so we know they’re barely using any CPU or memory.
    However, Kubernetes bases the quota utilization on what we specified, not what
    the Pod is actually using. This is critical because processes may use more CPU
    or allocate more memory as they get busy, and Kubernetes needs to make sure it
    leaves enough resources for the rest of the cluster to operate correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Final Thoughts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For our containerized applications to be reliable, we need to know that one
    application component can’t take too many resources and effectively starve the
    other containers running in a cluster. Kubernetes is able to use the resource
    limit functionality of the underlying container runtime and the Linux kernel to
    limit each container to only the resources it has been allocated. This practice
    ensures more reliable scheduling of containers onto nodes in the cluster and ensures
    that the available cluster resources are shared in a fair way even as the cluster
    becomes heavily loaded.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ve seen how to specify resource requirements for our Deployments
    and how to apply quotas to Namespaces, effectively enabling us to treat all of
    the nodes in our cluster as one large pool of available resources. In the next
    chapter, we’ll examine how that same principle extends to storage as we look at
    dynamically allocating storage to Pods, no matter where they are scheduled.
  prefs: []
  type: TYPE_NORMAL
