- en: '14'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '14'
- en: LIMITS AND QUOTAS
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 限制与配额
- en: '![image](../images/common01.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/common01.jpg)'
- en: For our cluster to provide a predictable environment for applications, we need
    some control over what resources each individual application component uses. If
    an application component can use all of the CPU or memory on a given node, the
    Kubernetes scheduler will not be able to allocate a new Pod to a node confidently,
    as it won’t know how much available space each node has.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让我们的集群为应用程序提供一个可预测的环境，我们需要控制每个独立应用程序组件使用的资源。如果一个应用程序组件可以使用给定节点上所有的 CPU 或内存，Kubernetes
    调度器将无法自信地将新 Pod 分配到节点，因为它无法知道每个节点的可用空间有多少。
- en: In this chapter, we’ll explore how to specify requested resources and limits
    to ensure that containers get the resources they need without impacting other
    containers. We’ll inspect individual containers at the runtime level so that we
    can see how Kubernetes configures the container technology we saw in [Part I](part01.xhtml#part01)
    to adequately meet the resource requirements of a container without allowing the
    container to exceed its limits.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨如何指定请求的资源和限制，确保容器获得所需的资源而不影响其他容器。我们将在运行时级别检查单个容器，以便我们可以看到 Kubernetes
    如何配置我们在[第一部分](part01.xhtml#part01)中看到的容器技术，足以满足容器的资源需求，同时避免容器超出其限制。
- en: Finally, we’ll look at how role-based access control is used to manage quotas,
    limiting the amount of resources a given user or application can demand, which
    will help us understand how to administer a cluster in a manner that allows it
    to reliably support multiple separate applications or development teams.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将探讨如何使用基于角色的访问控制来管理配额，限制特定用户或应用程序可以请求的资源量，这将帮助我们了解如何以一种可靠支持多个独立应用程序或开发团队的方式管理集群。
- en: Requests and Limits
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 请求与限制
- en: 'Kubernetes supports many different types of resources, including processing,
    memory, storage, network bandwidth, and use of special devices such as graphics
    processing units (GPUs). We’ll look at network limits later in this chapter, but
    let’s start with the most commonly specified resource types: processing and memory.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 支持多种不同类型的资源，包括处理、内存、存储、网络带宽和特殊设备的使用，如图形处理单元（GPU）。我们将在本章后面讨论网络限制，但首先让我们从最常见的资源类型开始：处理和内存。
- en: Processing and Memory Limits
  id: totrans-8
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 处理和内存限制
- en: 'The specifications for processing and memory resources serve two purposes:
    scheduling and preventing conflicts. Kubernetes provides a different kind of resource
    specification for each purpose. The Pod’s containers consume processing and memory
    resources in Kubernetes, so that’s where resource specifications are applied.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 处理和内存资源的规范有两个目的：调度和防止冲突。Kubernetes 为每个目的提供不同类型的资源规范。Pod 的容器在 Kubernetes 中消耗处理和内存资源，因此资源规范应用于这些地方。
- en: When scheduling Pods, Kubernetes uses the `requests` field in the container
    specification, summing this field across all containers in the Pod and finding
    a node with sufficient margin in both processing and memory. Generally, the `requests`
    field is set to the expected average resource requirements for each container
    in the Pod.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在调度 Pods 时，Kubernetes 使用容器规范中的 `requests` 字段，将该字段的值在 Pod 中的所有容器中相加，并找到一个在处理和内存上都有足够余量的节点。通常，`requests`
    字段设置为每个容器在 Pod 中的预期平均资源需求。
- en: The second purpose of resource specification is preventing denial-of-service
    issues in which one container takes all of a node’s resources, negatively affecting
    other containers. This requires runtime enforcement of container resources. Kubernetes
    uses the `limits` field of the container specification for this purpose, thus
    we need to be sure to set the `limits` field high enough that a container is able
    to run correctly without reaching the limit.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 资源规范的第二个目的在于防止拒绝服务问题，其中一个容器占用了整个节点的资源，负面影响到其他容器。这要求在运行时执行容器资源的强制限制。Kubernetes
    使用容器规范中的 `limits` 字段来实现这一目的，因此我们需要确保将 `limits` 字段设置得足够高，以便容器能够在不超出限制的情况下正确运行。
- en: '**TUNING FOR PERFORMANCE**'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**性能调优**'
- en: The idea that requests should match the expected average resource requirements
    is based on an assumption that any load spikes in the various containers in the
    cluster are unpredictable and uncorrelated, and load spikes can therefore be assumed
    to happen at different times. Even with that assumption, there is a risk that
    simultaneous load spikes in multiple containers on a node will result in that
    node being overloaded. And if the load spikes between different Pods are correlated,
    this risk of overload increases. At the same time, if we configure `requests`
    for the worst case scenario, we can end up with a very large cluster that is idle
    most of the time. In [Chapter 19](ch19.xhtml#ch19), we explore the different Quality
    of Service (QoS) classes that Kubernetes offers for Pods and discuss how to find
    a balance between performance guarantees and cluster efficiency.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 请求应与预期的平均资源需求相匹配的想法，基于一个假设，即集群中各个容器的负载峰值是不可预测且不相关的，因此可以假设负载峰值会在不同时间发生。即便如此，仍然存在多个容器在同一节点上出现负载峰值时，导致该节点过载的风险。如果不同Pod之间的负载峰值是相关的，这种过载的风险就会增加。同时，如果我们为最坏情况配置`requests`，可能会导致集群过大，大部分时间都处于闲置状态。在[第19章](ch19.xhtml#ch19)中，我们探讨了Kubernetes为Pod提供的不同服务质量（QoS）类，并讨论了如何在性能保证和集群效率之间找到平衡。
- en: '[Listing 14-1](ch14.xhtml#ch14list1) kicks off our examination with an example
    of using requests and limits with a Deployment.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 14-1](ch14.xhtml#ch14list1)通过使用请求和限制的部署示例开始我们的检查。'
- en: '*nginx-limit.yaml*'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*nginx-limit.yaml*'
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*Listing 14-1: Deployment with limits*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 14-1：带有限制的部署*'
- en: We’ll use this Deployment to explore how limits are configured at the level
    of the container runtime, so we use the `nodeName` field to make sure the container
    ends up on *host01*. This constrains where the scheduler can place the Pod, but
    the scheduler still uses the `requests` field to ensure that there are sufficient
    resources. If *host01* becomes too busy, the scheduler will just refuse to schedule
    the Pod, similar to what we saw in [Chapter 10](ch10.xhtml#ch10).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这个部署来探索如何在容器运行时级别配置资源限制，因此我们使用`nodeName`字段确保容器最终运行在*host01*上。这会限制调度器放置Pod的位置，但调度器仍然会使用`requests`字段来确保有足够的资源。如果*host01*变得过于繁忙，调度器将拒绝调度该Pod，这类似于我们在[第10章](ch10.xhtml#ch10)中看到的情况。
- en: The `resources` field is defined at the level of the individual container, allowing
    us to specify separate resource requirements for each container in a Pod. For
    this container, we specify a memory request of `64Mi` and a memory limit of `128Mi`.
    The suffix `Mi` means that we are using the power-of-2 unit *mebibytes*, which
    is 2 to the 20th power, rather than the power-of-10 unit *megabytes*, which would
    be the slightly smaller value of 10 to the 6th power.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '`resources`字段是在单个容器级别定义的，允许我们为Pod中的每个容器指定单独的资源需求。对于这个容器，我们指定了`64Mi`的内存请求和`128Mi`的内存限制。后缀`Mi`表示我们使用的是2的幂次单位*兆二进制字节*（mebibytes），即2的20次方，而不是10的幂次单位*兆字节*（megabytes），后者的值略小，为10的6次方。'
- en: Meanwhile, the processing request and limit specified using the `cpu` fields
    is not based on any absolute unit of processing. Rather, it is based on a synthetic
    *cpu unit* for our cluster. Each cpu unit roughly corresponds to one virtual CPU
    or core. The `m` suffix specifies a *millicpu* so that our `requests` value of
    `250m` equates to one quarter of a core, whereas the `limit` of `500m` equates
    to half of a core.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，使用`cpu`字段指定的处理请求和限制并不是基于任何绝对的处理单位，而是基于我们集群的合成*cpu单位*。每个cpu单位大致对应一个虚拟CPU或核心。`m`后缀指定了*千分之一cpu*，因此我们的`requests`值为`250m`，相当于四分之一核心，而`limit`为`500m`，相当于半个核心。
- en: '**NOTE**'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*The example repository for this book is at* [https://github.com/book-of-kubernetes/examples](https://github.com/book-of-kubernetes/examples).
    *See “Running Examples” on [page xx](ch00.xhtml#ch00lev1sec2) for details on getting
    set up.*'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*本书的示例代码库位于* [https://github.com/book-of-kubernetes/examples](https://github.com/book-of-kubernetes/examples)。*有关如何设置的详细信息，请参见[第xx页](ch00.xhtml#ch00lev1sec2)的“运行示例”。*'
- en: 'Let’s create this Deployment:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建这个部署：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The Pod will be allocated to `host01` and started:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Pod将被分配到`host01`并启动：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'And `host01` will show that resources have been allocated for the Pod:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然后`host01`将显示资源已分配给Pod。
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This is true even though our NGINX web server is idle and is not using a lot
    of processing or memory resources:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们的NGINX web服务器处于空闲状态，没有使用大量的处理或内存资源，这一点仍然成立：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Similar to what we saw in [Chapter 12](ch12.xhtml#ch12), this command queries
    the metrics add-on that is collecting data from `kubelet` running on each cluster
    node.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于我们在[第 12 章](ch12.xhtml#ch12)中看到的，这个命令查询收集来自每个集群节点上运行的 `kubelet` 数据的度量插件。
- en: Cgroup Enforcement
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Cgroup 强制执行
- en: 'The processing and memory limits we specified are enforced using the Linux
    control group (cgroup) functionality we described in [Chapter 3](ch03.xhtml#ch03).
    Kubernetes manages its own space within each hierarchy inside the */sys/fs/cgroup*
    filesystem. For example, memory limits are configured in the memory cgroup:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们指定的处理和内存限制是通过使用 Linux 控制组（cgroup）功能来强制执行的，这在[第 3 章](ch03.xhtml#ch03)中有描述。Kubernetes
    在 */sys/fs/cgroup* 文件系统中的每个层级内管理自己的空间。例如，内存限制是在内存 cgroup 中配置的：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Each Pod on a given host has a directory within the *kubepods.slice* tree. However,
    finding the specific directory for a given Pod takes some work because Kubernetes
    divides Pods into different classes of service, and because the name of the cgroup
    directory does not match the ID of the Pod or its containers.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 给定主机上的每个 Pod 在 *kubepods.slice* 树中都有一个目录。然而，找到特定 Pod 的目录需要一些工作，因为 Kubernetes
    将 Pod 划分为不同的服务类别，并且 cgroup 目录的名称与 Pod 或其容器的 ID 不匹配。
- en: 'To save us from searching around inside */sys/fs/cgroup*, we’ll use a script
    installed by this chapter’s automated scripts: */opt/cgroup-info*. This script
    uses `crictl` to query the container runtime for the cgroup path and then collects
    CPU and memory limit data from that path. The most important part of the script
    is this section that collects the path:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免我们在 */sys/fs/cgroup* 中四处查找，我们将使用本章自动化脚本安装的一个脚本：*/opt/cgroup-info*。这个脚本使用
    `crictl` 查询容器运行时的 cgroup 路径，然后从该路径收集 CPU 和内存限制数据。脚本的最重要部分是这个收集路径的部分：
- en: '*cgroup-info*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*cgroup-info*'
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `crictl pods` command collects the Pod’s ID, which is then used with `crictl
    inspectp` and `jq` to collect one specific field, called `cgroup_parent`. This
    field is the cgroup subdirectory created for that pod within each resource type.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`crictl pods` 命令收集 Pod 的 ID，然后与 `crictl inspectp` 和 `jq` 一起使用，以收集一个特定字段，称为
    `cgroup_parent`。这个字段是为该 Pod 在每种资源类型中创建的 cgroup 子目录。'
- en: 'Let’s run this script with our NGINX web server to see how the CPU and memory
    limits have been configured:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用我们的 NGINX Web 服务器运行这个脚本，看看 CPU 和内存限制是如何配置的：
- en: '[PRE7]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We first collect the name of the Pod and then use it to collect cgroup information.
    Note that this works only because the Pod is running on `host01`; the script will
    work for any Pod, but it must be run from the host on which that Pod is running.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先收集 Pod 的名称，然后用它来收集 cgroup 信息。请注意，这只有在 Pod 运行在 `host01` 上时才有效；该脚本适用于任何 Pod，但必须从该
    Pod 运行所在的主机上执行。
- en: There are two key pieces of data for the CPU configuration. The quota is the
    hard limit; it means that in any given 100,000 microsecond period, this Pod can
    use only 50,000 microseconds of processor time. This value corresponds to the
    `500m` CPU limit specified in [Listing 14-1](ch14.xhtml#ch14list1) (recall that
    the `500m` limit equates to half a core).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 CPU 配置，有两个关键数据。配额是硬限制；它意味着在任何给定的 100,000 微秒期间，这个 Pod 只能使用 50,000 微秒的处理器时间。这个值对应于[清单
    14-1](ch14.xhtml#ch14list1)中指定的 `500m` CPU 限制（回想一下，`500m` 限制相当于半个核心）。
- en: In addition to this hard limit, the CPU request field we specified in [Listing
    14-1](ch14.xhtml#ch14list1) has been used to configure the CPU shares. As we saw
    in [Chapter 3](ch03.xhtml#ch03), this field configures the CPU usage on a relative
    basis. Because it is relative to the values in neighboring directories, it is
    unitless, so Kubernetes computes the CPU share on the basis of one core equaling
    1,024\. We specified a CPU request of `250m`, so this equates to 256.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这个硬限制之外，我们在[清单 14-1](ch14.xhtml#ch14list1)中指定的 CPU 请求字段已经用于配置 CPU 配额。正如我们在[第
    3 章](ch03.xhtml#ch03)中看到的，这个字段按相对方式配置 CPU 使用率。因为它是相对于相邻目录中的值的，所以没有单位，因此 Kubernetes
    以每个核心等于 1,024 为基础计算 CPU 配额。我们指定了 `250m` 的 CPU 请求，因此这相当于 256。
- en: The CPU share does not set any kind of limit on CPU usage, so if the system
    is idle, a Pod can use processing up to its hard limit. However, as the system
    becomes busy, the CPU share determines how much processing each Pod is allotted
    relative to others in the same class of service. This helps to ensure that if
    the system becomes overloaded, all Pods will be degraded fairly based on their
    CPU request.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: CPU 配额并没有对 CPU 使用设定任何限制，因此如果系统空闲，Pod 可以使用其硬性限制范围内的所有处理能力。然而，随着系统变得繁忙，CPU 配额决定了每个
    Pod 相对于同一服务类中的其他 Pod 分配的处理能力。这有助于确保如果系统超载，所有 Pod 将根据其 CPU 请求公平地降级。
- en: Finally, for memory, there is a single relevant value. We specified a memory
    limit of `128Mi`, which equates to 128MiB. As we saw in [Chapter 3](ch03.xhtml#ch03),
    if our container tries to exceed this limit, it will be terminated. For this reason,
    it is critical to either configure the application such that it does not exceed
    this value, or to understand how the application acts under load to choose the
    optimum limit.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，对于内存，只有一个相关的值。我们指定了 `128Mi` 的内存限制，相当于 128MiB。正如我们在[第3章](ch03.xhtml#ch03)中看到的，如果我们的容器尝试超过此限制，它将被终止。因此，至关重要的是要么配置应用程序使其不会超过此值，要么了解应用程序在负载下的表现，以选择最佳限制。
- en: The amount of memory actually used by a process is ultimately up to that process,
    meaning that the memory request value has no purpose beyond its initial use in
    ensuring sufficient memory to schedule the Pod. For this reason, we don’t see
    the memory request value of `64Mi` being used anywhere in the cgroup configuration.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 一个进程实际使用的内存量最终取决于该进程本身，这意味着内存请求值除了在初始使用时确保有足够的内存来调度 Pod 外没有其他作用。因此，我们在 cgroup
    配置中看不到 `64Mi` 的内存请求值被使用。
- en: The way that resource allocations are reflected in cgroups shows us something
    important about cluster performance. Because `requests` is used for scheduling
    and `limits` is used for runtime enforcement, it is possible for a node to overcommit
    processing and memory. For the case in which containers have higher `limit` than
    `requests`, and containers consistently operate above their `requests`, this can
    cause performance issues with the containers on a node. We’ll discuss this in
    more detail in [Chapter 19](ch19.xhtml#ch19).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 资源分配在 cgroup 中的反映方式让我们了解到关于集群性能的重要信息。因为 `requests` 用于调度，而 `limits` 用于运行时强制执行，所以一个节点可能会过度分配处理能力和内存。如果容器的
    `limit` 大于 `requests`，并且容器始终在其 `requests` 之上运行，这可能会导致节点上的容器出现性能问题。我们将在[第19章](ch19.xhtml#ch19)中更详细地讨论这一点。
- en: 'We’re finished with our NGINX Deployment, so let’s delete it:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了 NGINX 部署，现在让我们将其删除：
- en: '[PRE8]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: So far, the container runtime can enforce the limits we’ve seen. However, the
    cluster must enforce other types of limits, such as networking.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，容器运行时可以强制执行我们所看到的限制。然而，集群必须强制执行其他类型的限制，如网络。
- en: Network Limits
  id: totrans-52
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 网络限制
- en: Ideally, our application will be architected so that required bandwidth for
    intercommunication is moderate, and our cluster will have sufficient bandwidth
    to meet the demand of all the containers. However, if we do have a container that
    tries to take more than its share of the network bandwidth, we need a way to limit
    it.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们的应用程序将设计为中等程度地需要用于互相通信的带宽，并且我们的集群将有足够的带宽来满足所有容器的需求。然而，如果确实有一个容器试图占用超过其份额的网络带宽，我们需要一种方法来限制它。
- en: 'Because the network devices are configured by plug-ins, we need a plug-in to
    manage bandwidth. Fortunately, the `bandwidth` plug-in is part of the standard
    set of CNI plug-ins installed with our Kubernetes cluster. Additionally, as we
    saw in [Chapter 8](ch08.xhtml#ch08), the default CNI configuration enables the
    `bandwidth` plug-in:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 因为网络设备是通过插件配置的，我们需要一个插件来管理带宽。幸运的是，`bandwidth` 插件是与我们的 Kubernetes 集群一起安装的标准 CNI
    插件的一部分。此外，正如我们在[第8章](ch08.xhtml#ch08)中看到的，默认的 CNI 配置启用了 `bandwidth` 插件：
- en: '[PRE9]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As a result, `kubelet` is already calling the `bandwidth` plug-in every time
    a new Pod is created. If a Pod is configured with bandwidth limits, the plug-in
    uses the Linux kernel’s traffic control capabilities that we saw in [Chapter 3](ch03.xhtml#ch03)
    to ensure the Pod’s virtual network devices don’t exceed the specified limit.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，`kubelet` 在每次创建新 Pod 时都会调用 `bandwidth` 插件。如果 Pod 配置了带宽限制，插件将利用我们在[第3章](ch03.xhtml#ch03)中看到的
    Linux 内核的流量控制功能，确保 Pod 的虚拟网络设备不会超过指定的限制。
- en: 'Let’s look at an example. First, let’s deploy an `iperf3` server that will
    listen for client connections:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个例子。首先，我们部署一个 `iperf3` 服务器来监听客户端连接：
- en: '*iperf-server.yaml*'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*iperf-server.yaml*'
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In addition to a Deployment, we also create a Service. This way, our `iperf3`
    clients can find the server under its well-known name of `iperf-server`. We specify
    port 5201, which is the default port for `iperf3`.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 Deployment，我们还创建了一个 Service。这样，我们的 `iperf3` 客户端就可以通过其知名名称 `iperf-server`
    找到服务器。我们指定了端口 5201，这是 `iperf3` 的默认端口。
- en: 'Let’s deploy this server:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们部署这个服务器：
- en: '[PRE11]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let’s run an `iperf3` client without applying any bandwidth limits. This will
    give us a picture of how fast our cluster’s network is without any traffic control.
    Here’s the client definition:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行一个不应用任何带宽限制的 `iperf3` 客户端。这将让我们了解在没有任何流量控制的情况下，集群网络的速度。以下是客户端定义：
- en: '*iperf.yaml*'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*iperf.yaml*'
- en: '[PRE12]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Normally, `iperf3` in client mode would run once and then terminate. This image
    has a script that runs `iperf3` repeatedly, sleeping for one minute between each
    run. Let’s start a client Pod:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，`iperf3` 客户端模式下会运行一次然后终止。这个镜像有一个脚本会重复运行 `iperf3`，每次运行之间休眠一分钟。让我们启动一个客户端 Pod：
- en: '[PRE13]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'It will take a few seconds for the Pod to start running, after which it will
    take 10 seconds for the initial run. After 30 seconds or so, the Pod log will
    show the results:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 启动需要几秒钟，之后初次运行将需要 10 秒钟。大约 30 秒后，Pod 日志将显示结果：
- en: '[PRE14]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In this case, we see a transfer rate of `1.36 GBits/sec` between our client
    and server. Your results will be different depending on how your cluster is deployed
    and whether the client and server end up on the same host.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们看到客户端和服务器之间的传输速率为 `1.36 GBits/sec`。根据您的集群部署情况以及客户端和服务器是否位于同一主机上，您的结果可能会有所不同。
- en: 'Before moving on, we’ll shut down the existing client to prevent it from interfering
    with our next test:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，我们将关闭现有的客户端，以防它干扰我们的下一个测试：
- en: '[PRE15]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Obviously, while it’s running, `iperf3` is trying to use as much network bandwidth
    as possible. That’s fine for a test application, but it isn’t polite behavior
    for an application component in a Kubernetes cluster. To limit its bandwidth,
    we’ll add an annotation to the Pod definition:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，在运行时，`iperf3` 尝试尽可能多地使用网络带宽。这对于测试应用程序来说没问题，但对于 Kubernetes 集群中的应用组件来说，这种行为并不太礼貌。为了限制其带宽，我们将在
    Pod 定义中添加一个注解：
- en: '*iperf-limit.yaml*'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*iperf-limit.yaml*'
- en: '[PRE16]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We’ll want to inspect how the limits are being applied to the network devices,
    which will be easier if this Pod ends up on `host01`, so we set `nodeName` accordingly.
    Otherwise, the only change in this Pod definition is the `annotations` section
    in the Pod metadata ➊. We set a value of `1M` for ingress and egress, corresponding
    to a 1Mb bandwidth limit on the Pod. When this Pod is scheduled, `kubelet` will
    pick up these annotations and send the specified bandwidth limits to the bandwidth
    plug-in so that it can configure Linux traffic shaping accordingly.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望检查如何将限制应用到网络设备上，如果这个 Pod 最终在 `host01` 上，检查会更容易，所以我们相应地设置了 `nodeName`。否则，这个
    Pod 定义中唯一的变化是 Pod 元数据中的 `annotations` 部分 ➊。我们为 ingress 和 egress 设置了 `1M` 的值，相当于对
    Pod 设置了 1Mb 的带宽限制。当这个 Pod 被调度时，`kubelet` 会获取这些注解，并将指定的带宽限制发送给带宽插件，以便它可以相应地配置 Linux
    流量整形。
- en: 'Let’s create this Pod and get a look at this in action:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建这个 Pod 并查看它的实际操作：
- en: '[PRE17]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'As before, we wait long enough for the client to complete one test with the
    server and then print the logs:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前一样，我们等待足够的时间让客户端完成一次与服务器的测试，然后打印日志：
- en: '[PRE18]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The change is significant, as the Pod is limited to a fraction of the speed
    we saw with an unlimited client. However, because the traffic shaping is based
    on a token bucket filter, the traffic control is inexact over shorter intervals,
    so we see a bitrate of around 20Mb rather than 1Mb. To see why, let’s look at
    the actual traffic shaping configuration.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 变化是显著的，因为该 Pod 的速度受限于我们在没有限制的客户端上看到的速度的一小部分。然而，由于流量整形基于令牌桶过滤器，短时间内流量控制并不精确，因此我们看到的比特率大约为
    20Mb 而不是 1Mb。要了解原因，让我们看看实际的流量整形配置。
- en: 'The `bandwidth` plug-in is applying this token bucket filter to the host side
    of the virtual Ethernet (veth) pair that was created for the Pod, so we can see
    it by showing traffic control configuration for the host interfaces:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`bandwidth` 插件将令牌桶过滤器应用于为 Pod 创建的虚拟以太网（veth）对的主机端，因此我们可以通过显示主机接口的流量控制配置来查看它：'
- en: '[PRE19]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The combination of `rate` and `burst` shows why our Pod was able to achieve
    20Mb over the 10-second test run. Because of the `burst` value, the Pod was able
    to send a large quantity of data immediately, at the cost of spending several
    seconds without any ability to send or receive. Over a much longer interval, we
    would see an average of 1Mbps, but we would still see this bursting behavior.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`rate`和`burst`的组合展示了为什么我们的Pod能够在10秒的测试运行中达到20Mb。由于`burst`值，Pod能够立即发送大量数据，但代价是花费了几秒钟的时间，无法发送或接收任何数据。在一个更长的时间间隔内，我们会看到平均为1Mbps的带宽，但我们仍然会看到这种爆发式的行为。'
- en: 'Before moving on, let’s clean up our client and server:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们清理客户端和服务器：
- en: '[PRE20]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Managing the bandwidth of a Pod can be useful, but as we’ve seen, the bandwidth
    limit can behave like an intermittent connection from the Pod’s perspective. For
    that reason, this kind of traffic shaping should be considered a last resort for
    containers that cannot be configured to moderate their own bandwidth usage.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 管理Pod的带宽是有用的，但正如我们所见，带宽限制可能表现为Pod视角中的间歇性连接。因此，这种流量整形应该被视为无法配置自身带宽使用的容器的最后手段。
- en: Quotas
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配额
- en: Limits allow our Kubernetes cluster to ensure that each node has sufficient
    resources for its assigned Pods. However, if we want our cluster to host multiple
    applications reliably, we need a way to control the amount of resources that any
    one application can request.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 限制（Limits）允许我们的Kubernetes集群确保每个节点拥有足够的资源来支持其分配的Pod。然而，如果我们希望集群能够可靠地托管多个应用程序，我们需要一种方法来控制任何一个应用程序可以请求的资源数量。
- en: To do this, we’ll use quotas. Quotas are allocated based on Namespaces; they
    specify the maximum amount of resources that can be allocated within that Namespace.
    This includes not only the primary resources of CPU and memory but also specialized
    cluster resources such as GPUs. We can even use quotas to specify the maximum
    number of a specific object type, such as a Deployment, Service, or CronJob, that
    can be created within a given Namespace.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们将使用配额（quotas）。配额是基于命名空间（Namespaces）分配的，它们指定了在该命名空间内可以分配的最大资源量。这不仅包括CPU和内存等基本资源，还包括如GPU等专用集群资源。我们甚至可以使用配额来指定在给定命名空间内可以创建的特定对象类型的最大数量，比如部署（Deployment）、服务（Service）或定时任务（CronJob）。
- en: Because quotas are allocated based on Namespaces, they need to be used in conjunction
    with the access controls we described in [Chapter 11](ch11.xhtml#ch11) to ensure
    that a given user is bound by the quotas we create. This means that creating Namespaces
    and applying quotas is typically handled by the cluster administrator.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 由于配额是基于命名空间分配的，它们需要与我们在[第11章](ch11.xhtml#ch11)中描述的访问控制结合使用，以确保特定用户受我们创建的配额约束。这意味着创建命名空间和应用配额通常由集群管理员处理。
- en: 'Let’s create a sample Namespace for our Deployment:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为我们的部署创建一个示例命名空间：
- en: '[PRE21]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, let’s create a *ResourceQuota* resource type to apply a quota to the Namespace:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个*ResourceQuota*资源类型，以便为命名空间应用配额：
- en: '*quota.yaml*'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*quota.yaml*'
- en: '[PRE22]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This resource defines a quota for CPU and memory for both requests and limits.
    The units are the same as those used for limits in the Deployment specification
    in [Listing 14-1](ch14.xhtml#ch14list1).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这个资源定义了CPU和内存的配额，适用于请求（requests）和限制（limits）。单位与[Listing 14-1](ch14.xhtml#ch14list1)中部署（Deployment）规范中的限制相同。
- en: 'Let’s apply this quota to the `sample` Namespace:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将此配额应用到`sample`命名空间：
- en: '[PRE23]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We can see that this quota has been applied successfully:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到这个配额已经成功应用：
- en: '[PRE24]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Even though this quota will apply to all users that try to create Pods in the
    Namespace, even cluster administrators, it’s more realistic to use a normal user,
    given that an administrator can always create new Namespaces to get around a quota.
    Thus, we’ll also create a user:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 即使这个配额会应用于所有尝试在命名空间中创建Pod的用户，包括集群管理员，考虑到管理员总是可以创建新的命名空间来绕过配额，使用普通用户更为现实。因此，我们还将创建一个用户：
- en: '[PRE25]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'As we did in [Chapter 11](ch11.xhtml#ch11), we’ll bind the `edit` role to this
    user to provide the right to create and edit resources in the `sample` Namespace.
    We’ll use the same RoleBinding that we saw in [Listing 11-1](ch11.xhtml#ch11list1):'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如同我们在[第11章](ch11.xhtml#ch11)中所做的那样，我们将把`edit`角色绑定到该用户，以提供在`sample`命名空间中创建和编辑资源的权限。我们将使用在[Listing
    11-1](ch11.xhtml#ch11list1)中看到的相同RoleBinding：
- en: '[PRE26]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now that our user is set up, let’s set the `KUBECONFIG` environment variable
    so that future `kubectl` commands will operate as our normal user:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的用户已设置完成，让我们设置`KUBECONFIG`环境变量，以便未来的`kubectl`命令将以我们的正常用户身份执行：
- en: '[PRE27]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'First, we can verify that the `edit` role possessed by our normal user does
    not enable making changes to quotas in a Namespace, which makes sense—quotas are
    an administrator function:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以验证普通用户所拥有的 `edit` 角色并不允许对命名空间中的配额进行更改，这很合理——配额是管理员职能：
- en: '[PRE28]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We can now create some Pods in the `sample` Namespace to test the quota. First,
    let’s try to create a Pod with no limits:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以在 `sample` 命名空间中创建一些 Pods 来测试配额。首先，让我们尝试创建一个没有限制的 Pod：
- en: '[PRE29]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Because our Namespace has a quota, we are no longer allowed to create Pods without
    specifying limits.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们的命名空间有配额，我们不再允许创建没有指定限制的 Pods。
- en: In [Listing 14-2](ch14.xhtml#ch14list2), we try it again, this time using a
    Deployment that specifies resource limits for the Pods it creates.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在[清单 14-2](ch14.xhtml#ch14list2)中，我们再次尝试，这次使用了一个指定资源限制的部署，该部署为它创建的 Pods 设置了资源限制。
- en: '*sleep.yaml*'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '*sleep.yaml*'
- en: '[PRE30]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '*Listing 14-2: Deployment with Limit*'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 14-2：带有限制的部署*'
- en: 'Now we can apply this to the cluster:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将其应用到集群中：
- en: '[PRE31]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This is successful because we specified the necessary request and limit fields
    and we didn’t exceed our quota. Additionally, a Pod is started with the limits
    we specified:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这是成功的，因为我们指定了必要的请求和限制字段，并且没有超过配额。此外，Pod 以我们指定的限制启动：
- en: '[PRE32]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'However, we can see that we’re now using resources out of our quota:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们可以看到，我们现在正在使用配额中的资源：
- en: '[PRE33]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This will limit our ability to scale this Deployment. Let’s illustrate:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这将限制我们扩展该部署的能力。让我们来说明一下：
- en: '[PRE34]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We’ve asked for 12 replicas, but we see only three running. If we describe
    the Deployment we can see an issue:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们请求了 12 个副本，但我们只看到有三个在运行。如果我们描述这个部署，就会看到一个问题：
- en: '[PRE35]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'And the Namespace now reports that we have used up enough of our quota that
    there is no room to allocate the resources needed for another Pod:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在命名空间报告说，我们已经消耗了足够的配额，无法为另一个 Pod 分配所需的资源：
- en: '[PRE36]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Our Pods are running `sleep`, so we know they’re barely using any CPU or memory.
    However, Kubernetes bases the quota utilization on what we specified, not what
    the Pod is actually using. This is critical because processes may use more CPU
    or allocate more memory as they get busy, and Kubernetes needs to make sure it
    leaves enough resources for the rest of the cluster to operate correctly.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 Pods 正在运行 `sleep`，因此我们知道它们几乎不使用任何 CPU 或内存。然而，Kubernetes 是基于我们指定的配额来计算配额利用率，而不是
    Pod 实际使用的资源。这一点至关重要，因为进程在变得繁忙时可能会使用更多的 CPU 或分配更多的内存，而 Kubernetes 需要确保为集群的其他部分留出足够的资源，以保证其正常运行。
- en: Final Thoughts
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最终思考
- en: For our containerized applications to be reliable, we need to know that one
    application component can’t take too many resources and effectively starve the
    other containers running in a cluster. Kubernetes is able to use the resource
    limit functionality of the underlying container runtime and the Linux kernel to
    limit each container to only the resources it has been allocated. This practice
    ensures more reliable scheduling of containers onto nodes in the cluster and ensures
    that the available cluster resources are shared in a fair way even as the cluster
    becomes heavily loaded.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让我们的容器化应用程序更可靠，我们需要确保一个应用组件不会占用过多资源，从而有效地使集群中其他容器“饿死”。Kubernetes 能够利用底层容器运行时和
    Linux 内核的资源限制功能，将每个容器限制在其已分配的资源范围内。这一做法确保了容器在集群节点上的调度更加可靠，并确保即使集群负载较重，集群资源的分配也能公平共享。
- en: In this chapter, we’ve seen how to specify resource requirements for our Deployments
    and how to apply quotas to Namespaces, effectively enabling us to treat all of
    the nodes in our cluster as one large pool of available resources. In the next
    chapter, we’ll examine how that same principle extends to storage as we look at
    dynamically allocating storage to Pods, no matter where they are scheduled.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经了解了如何为我们的部署指定资源需求，以及如何为命名空间应用配额，从而有效地将集群中的所有节点视为一个大型可用资源池。在下一章，我们将探讨这一原理如何扩展到存储方面，看看如何动态地为
    Pods 分配存储，无论它们被调度到哪里。
