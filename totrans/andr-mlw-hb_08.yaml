- en: '**5'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MACHINE LEARNING FUNDAMENTALS**
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the early days of the Android eco-system, defenders analyzed apps manually
    to determine whether they were malicious. This technique was feasible at the time
    because the operating system’s market share was small and, initially, few apps
    were developed for it. However, things have changed. Recent official reports show
    that more than 100,000 Android APKs are released each month on Google Play. Our
    own estimates suggest that the actual number is significantly higher.
  prefs: []
  type: TYPE_NORMAL
- en: It is no longer possible for companies to manually assess the security level
    of so many diverse apps. Initially, analysts solved this problem by relying on
    human-identified patterns present exclusively in malware. They wrote detection
    rules, using YARA or other tools, to flag applications containing such patterns.
    This approach failed to scale, however, as it quickly became infeasible for analysts
    to keep track of the features present in millions of apps.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, analysts began using machine learning algorithms, which have the ability
    to perform these tasks on a large number of applications without explicit programming
    by learning through examples. This approach proved vastly more efficient, and
    reduced the burden on human analysts. This chapter introduces the machine learning
    basics you’ll need to be familiar with in order to understand the material presented
    in the book’s remaining chapters, with a focus on the classification algorithms
    popular in malware detection. Readers already familiar with the topic can skip
    ahead.
  prefs: []
  type: TYPE_NORMAL
- en: '**How Machine Learning for Malware Analysis Works**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In malware analysis, we most often use machine learning methods to classify
    apps as benign, malicious, or, in some cases, possibly malicious. At a deeper
    level, more sophisticated methods can provide increasingly fine-grained labels
    that identify apps as a specific type of malware, like spyware, banking trojans,
    and so on. Given the support that automated methods provide, security analysts
    can focus on examining *gray zone* apps, or those that aren’t accurately classified
    as either goodware or malware. Machine learning significantly reduces the number
    of apps that analysts have to manually review.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning algorithms can be either supervised or unsupervised. *Supervised*
    algorithms require labeled datasets, while *unsupervised* algorithms learn patterns
    inherent in the data. Classification algorithms are the most common type of supervised
    algorithms, while clustering and anomaly detection are common examples of unsupervised
    algorithms. Each has its own purpose in security-related machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: '*Classification* algorithms, also called *classifiers*, consider information
    about an entity, such as an app, a picture, or a user account, and place it into
    one or more classes. For example, in the case of an Android app, we might have
    two classes of interest: malware and goodware. But if we want to classify something
    else—for instance, Instagram accounts—we might have many more classes: *child*
    for those younger than 18, *young adult* for those 18–40 years old, *middle-aged*
    for those 41–65 years old, and *senior* for those 66 years old or older. Engineers
    working on the classification problem define the exact number of categories and
    the meaning of each. One challenge is that classification algorithms often require
    a large number of labeled samples (already classified samples that the algorithm
    can learn from) to produce accurate models, which might not always be available.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Clustering* algorithms take information from multiple entities and group similar
    samples into clusters. For instance, malicious developers often create multiple
    versions of their malware over time as they look for ways to avoid detection or
    add new functionality. In such cases, clusters might correspond to different versions
    of the same malware family. Clustering algorithms need a way to measure the similarity
    of or distance between the entities under observation, and domain experts are
    responsible for defining how that similarity will be computed based on the clustering
    goal.'
  prefs: []
  type: TYPE_NORMAL
- en: While clustering algorithms don’t require labeled data, the clusters they produce
    can be hard to interpret if the algorithm isn’t aware of what the analyst is looking
    for. Malware and goodware often share SDKs and libraries, which might confuse
    the clustering system, causing it to group malware and goodware together merely
    because they share an SDK.
  prefs: []
  type: TYPE_NORMAL
- en: '*Anomaly detection* or *outlier detection* algorithms try to identify entities
    that are substantially different from almost all others in a given dataset. For
    instance, efforts have been made to find malicious apps by checking whether their
    behavior differs substantially from the norm. However, a challenge for Android
    malware detection in particular is that most malware operates within the bounds
    of the Android security model, asking unwitting victims for permission to execute
    malicious actions. Is an app that sends all of your texts to a remote server some
    kind of spyware, or is it an SMS backup app? Anomaly detection algorithms may
    have a hard time distinguishing between these two cases.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As it turns out, the vast majority of successful efforts to use machine learning
    in malware detection have relied on classification algorithms. However, some techniques
    use a mix of clustering and classification in an attempt to identify the family
    to which a given malware sample belongs, such as the system proposed in “EC2:
    Ensemble Clustering and Classification for Predicting Android Malware Families”
    by Tanmoy Chakraborty et al.'
  prefs: []
  type: TYPE_NORMAL
- en: '***Identifying App Features***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Most machine learning algorithms assume that each entity of interest has an
    associated *feature vector*, which is an ordered list of values belonging to important
    properties of the entity being studied. In the case of malware analysis, the entities
    of interest are the apps themselves. The feature vector includes attributes derived
    from the analysis of the APK or the app in execution and can be either handcrafted
    or automatically generated.
  prefs: []
  type: TYPE_NORMAL
- en: For the purposes of malware detection and classification, features might relate
    to whether or not the app requests a specific permission (such as permission to
    read incoming texts), whether or not the code contains encrypted portions, whether
    or not the code tries to connect to an external server, and so forth. For each
    of these questions, the feature is set to 1 if the answer is yes and 0 otherwise.
    Other features may have *non-binary* values. For instance, we might have a feature
    corresponding to the number of times an app’s source code calls a given package
    in the Android API. Machine learning methods use these features to identify and
    classify entities. [Chapter 6](ch06.xhtml) will describe various types of features
    that are important to the analysis of Android malware.
  prefs: []
  type: TYPE_NORMAL
- en: The content of an Android app alone provides a seemingly unlimited number of
    potential features, but we don’t have to limit the feature set to data found inside
    the APK. In fact, there is surprising value in connecting features from APK files
    to external information. For example, in the case of an app that connects to a
    certain domain, we could turn the Whois information for that domain into features.
    Similarly, if an app connects to a certain IP address, we can pull in information
    about who owns the IP address, the datacenter that serves it, the country where
    the associated server is located, or even information from the server itself,
    such as the operating system it runs or the other software it hosts.
  prefs: []
  type: TYPE_NORMAL
- en: To give another example, if an app sends messages to a premium SMS number, we
    might be able to determine which mobile carrier owns that number and what commercial
    entity has it registered in partnership with the mobile carrier. The developers
    of the machine learning system can choose to include these pieces of information
    as features to characterize certain malware families and developers.
  prefs: []
  type: TYPE_NORMAL
- en: '***Creating Training Sets***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A *training sample* for a classification algorithm is a computational object
    consisting of a feature vector and a class label. In formal terms, we say that
    it consists of a pair (*f* , *c*) where *f* is a feature vector and *c* is the
    class to which we believe the sample belongs. Note that in Android malware analysis,
    for an app to be a training sample we must already know its class (for instance,
    malware or goodware).
  prefs: []
  type: TYPE_NORMAL
- en: A *training set* is a finite set of training samples. We can usually represent
    it as a table or spreadsheet. In the case of the malware versus goodware classification,
    the rows in the table would correspond to apps and the columns would correspond
    to various features. A special column would represent the label or class; that
    is, whether the app in a given row is malware (set to 1) or goodware (set to 0).
    [Table 5-1](ch05.xhtml#ch5tab1) shows a small sample training set associated with
    Android apps.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this training set, we show only two features, for the sake of simplicity.
    The *telephony* feature captures the number of calls made to the *android.telephony.cdma*
    package by the app, and the *app* feature does the same for the *android.app*
    package. For instance, the app shown in the first row of [Table 5-1](ch05.xhtml#ch5tab1)
    makes 27 calls in its source code to classes in the *telephony* package and 2,655
    calls to classes in the *app* API package. This app is malware: we see that its
    value in the Label column is 1.'
  prefs: []
  type: TYPE_NORMAL
- en: Each app in this training set can be thought of as a point on a scatter plot.
    For instance, we could position the first app in the table at the coordinate (27,2655).
    In [Figure 5-1](ch05.xhtml#ch5fig1), we depict it using a cross because it is
    malware. We denote goodware using dots. As you can see, an app’s feature vector
    determines its location in this feature space.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, in the real world, analysts might use a much larger training set
    (one with thousands of apps). The number of features might also be in the hundreds,
    thousands, or higher.
  prefs: []
  type: TYPE_NORMAL
- en: Creating good training sets is challenging. Training sets should ideally be
    vast and diverse, and their data, particularly their labels, must be as accurate
    as possible. For malware analysis, this poses a problem. How does one put together
    an accurately labeled set of thousands of malware and goodware apps without many
    months of careful and costly manual research and analysis?
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 5-1:** Simple Training Set'
  prefs: []
  type: TYPE_NORMAL
- en: '| **The telephony feature** | **The app feature** | **Sample name** | **Label**
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 27 | 2655 | 14292932679d6930f521a21de4e8bffd.apk | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 1764 | 04276665aaa3725ea34097c4c874873c.apk | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 870 | e8290db04c7004ec8bb53f7cda155eb9.apk | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 2086 | 03f9eff3229e3a4eefc9224f916202b8.apk | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 329 | 1c4e357a8ec5f13de4ffd57cc2711afe.apk | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 1499 | 080b0ed2d9bf87e9f3d061a1ba48da33.apk | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 27 | 2652 | 08026e2b63ec51cb36bc6cff00c28909.apk | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 27 | 2637 | 094f67a3a682a0cd4305d720cc786e00.apk | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 877 | 3a895a2d19f040d7826e68c2f9596c55.apk | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 2163 | 1a7409b8e0f6cc299a4ac0b9ca67856e.apk | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 2016 | Starbucks_2020-10-22_16_06_36.apk | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1823 | Starbucks_2017-09-29_16_05_56.apk | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 6604 | TikTok_2020-12-03_19_11_34.apk | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 6604 | TikTok_2020-12-03_19_17_05.apk | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 11483 | Walgreens_2020-11-21_21_45_17.apk | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1555 | Starbucks_2016-01-19_16_04_34.apk | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1738 | Starbucks_2016-09-08_16_02_23.apk | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 11483 | Walgreens_2020-11-21_21_46_22.apk | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1384 | Starbucks_2015-12-07_16_07_02.apk | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1812 | Starbucks_2017-09-26_16_02_39.apk | 0 |'
  prefs: []
  type: TYPE_TB
- en: '![Image](../images/ch05fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-1: A visualization of the training set as a scatter plot*'
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, academic researchers have released a few training sets for Android
    malware analysis, of which three are well known. The *Drebin* dataset contains
    5,560 applications from 179 different malware families collected between August
    2010 and October 2012\. You can find it at [*https://www.sec.cs.tubs.de/danarp/drebin*](https://www.sec.cs.tu-bs.de/~danarp/drebin).
    The *AndroZoo* dataset is a growing collection of Android applications that currently
    contains over 17 million APKs, each of which has been analyzed by different antivirus
    products. You can find it at [*https://androzoo.uni.lu*](https://androzoo.uni.lu).
    The *CCCS-CIC-AndMal-2020* dataset contains 200,000 benign samples and 200,000
    malicious ones drawn from 191 prominent malware families. The dataset can be found
    at [*https://www.unb.ca/cic/datasets/andmal2020.html*](https://www.unb.ca/cic/datasets/andmal2020.html).
    If one of these websites goes offline in the future, we’ll publish the samples
    at [*https://github.com/android-malware-ml-book*](https://github.com/android-malware-ml-book)
    so long as there is no legal impediment to doing so.
  prefs: []
  type: TYPE_NORMAL
- en: '***Using Classification Algorithms***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Classification algorithms take a training set as input and try to find some
    condition such that, when the condition is true for a given app’s feature vector,
    the probability that it is malicious is very high, whereas when the condition
    is false, the probability that the app is malicious is very low.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if you consider [Figure 5-2](ch05.xhtml#ch5fig2), you’ll see that
    the condition of an app calling the *app* package fewer than 3,000 times does
    the job. All apps that satisfy this condition are malware (crosses), while all
    apps that don’t satisfy it are goodware (dots). In this case, the horizontal line
    at the 3,000 API calls mark splits the feature space into these two parts.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch05fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-2: Two possible separators for the training set shown in [Figure
    5-1](ch05.xhtml#ch5fig1)*'
  prefs: []
  type: TYPE_NORMAL
- en: However, this is not the only possible separator. We could just as easily have
    selected the dashed line shown in the plot. This line is *y* = 40*x* + 2,000,
    where *x* = *APIPackage:android.telephony.cdma* and *y* = *APIPackage:android.app*.
    All points above this line are goodware, and all points below it are malware.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you might have a number of questions. Should separators always
    split the feature space into two parts, as shown in this figure? Should separators
    always be linear, or can they include circles, ellipses, or other, even weirder
    shapes? [Figure 5-3](ch05.xhtml#ch5fig3) shows a situation in which the data is
    grouped into different regions, some containing goodware and some containing malware.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch05fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-3: Rectangular separators for the training set shown in [Figure 5-1](ch05.xhtml#ch5fig1)*'
  prefs: []
  type: TYPE_NORMAL
- en: One potential problem here is that large parts of the feature space aren’t part
    of either a goodware or malware region, which might make sense, as no samples
    from those parts of the feature space have ever been seen before. In the next
    section, you’ll learn that classification algorithms can take various approaches
    to sorting their samples.
  prefs: []
  type: TYPE_NORMAL
- en: '**Classification Algorithms**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we’ll discuss some well-known classification methods. As you
    will see, classifiers work in different ways.
  prefs: []
  type: TYPE_NORMAL
- en: '***Decision Trees***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A decision tree classification algorithm builds a tree, each node of which compares
    a single feature with a single value. Thus, each path of the tree corresponds
    to a complex logical “and” condition, along with a class label showing the class
    that best matches that condition. Suppose we are given a training set *T* whose
    feature vectors are drawn from an *n*-dimensional space. The samples are Android
    apps that we want to classify into two classes, goodware and malware. To accomplish
    this, we would give each app *a* an associated feature vector *f*[*a*] consisting
    of *n* features. [Figure 5-4](ch05.xhtml#ch5fig4) shows a sample decision tree
    for classifying the apps.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch05fig04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-4: A sample decision tree*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each node in a decision tree implicitly represents a subset of the training
    set. For instance, the root of the decision tree shown in [Figure 5-4](ch05.xhtml#ch5fig4)
    represents a training set of 1,000 apps. Each node includes a Boolean condition
    that splits the set into two disjoint sets: one consisting of all members that
    satisfy the condition and one consisting of all members that do not. For example,
    in the root node, the condition checks whether the number of calls to the opcode
    `iget-boolean` is less than or equal to 1989.5\. This opcode reads a Boolean instance
    field from registers and is expressed as bytecode in *Dalvik format*, which is
    the instruction set used by Android runtimes. All apps that satisfy this condition
    are associated with the left child of the root node, while those that do not satisfy
    the condition end up in the set of apps associated with the right child.'
  prefs: []
  type: TYPE_NORMAL
- en: The nodes also contain some other information to help us classify the apps as
    malicious or benign. For instance, if you look at the root node, you’ll see the
    number of samples, 1,000, and that each of the two classes (goodware followed
    by malware) contains 500 apps. Because there is a 50-50 split at the root node,
    this node can be labeled using either class. In this case, we’ve opted to call
    it goodware. Look now at the left child of the root node. We see that it contains
    519 apps. (That is, 519 samples from the training set satisfied the condition
    in the root node.) From the *Value* field, we see that 39 of these 519 apps are
    goodware, while the remaining 480 are malware. This node is therefore marked as
    malware, because that is how we classify the majority of its apps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, two questions naturally arise. First, how does the decision tree algorithm
    decide what condition to choose at each node in the tree? And second, what is
    the *Gini* field shown in the nodes in [Figure 5-4](ch05.xhtml#ch5fig4)? The answers
    to these questions are closely related. Every decision tree considers some family
    of constraints in order to choose the conditions with which to label the nodes.
    In our sample decision tree, this class consists of constraints of the form *feature*
    ≤ *value*. Beyond this, the algorithm relies on the Gini value, an effort to measure
    the heterogeneity of the classes represented within the set of apps in the training
    sample for a node. For the root node in [Figure 5-4](ch05.xhtml#ch5fig4) the heterogeneity
    is maximized, as both classes are equally represented. But for its left child,
    the apps are overwhelming malware. The Gini metric assigns a high value to nodes
    that are heterogeneous and a low value to nodes that are homogeneous. Consequently,
    the Gini value assigned to the root node is higher than that assigned to its left
    child. The Gini value itself is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Gini*(*X*) = 1 – *P*(*malware*|*X*)² – *P*(*goodware*|*X*)²'
  prefs: []
  type: TYPE_NORMAL
- en: Because at the root node the probability of an app being goodware is the same
    as the probability of it being malware, namely 50 percent, the Gini value of the
    root is 1 *–* (0.5)² – (0.5)² = 0.5\. For the left child of the root, the probability
    of an app being goodware is 480/519\. Hence, its Gini value is 1 – (39/519)² –
    (480/519)² = 1 – 0.075² – 0.925² = 1 – 0.0056 – 0.856 = 0.139.
  prefs: []
  type: TYPE_NORMAL
- en: We won’t go into the details of the decision tree algorithm itself. It suffices
    to say that when we build a decision tree, we try to find a condition from the
    set of all permitted splitting conditions such that the resulting Gini value of
    a combination of the two children is minimized. The process of splitting nodes
    continues until we reach nodes that are considered homogeneous enough, meaning
    their Gini scores fall below a given threshold.
  prefs: []
  type: TYPE_NORMAL
- en: There are many variants of decision trees. Some use criteria such as entropy
    rather than Gini scores to assess the quality of possible ways to split a node.
    Other variants change the types of conditions at each node and even set things
    up so that a decision tree makes a ternary or *n*-ary decision at each node, rather
    than a binary one. You can find more details about the construction of decision
    trees and their variants in “Top-Down Induction of Decision Trees Classifiers—A
    Survey” by Lior Rokach and Oded Maimon and “Optimizing Multi-Path Decision Tree
    by Clustering and K-Nearest Neighbor Methods” by Nasib S. Gill and Reena Hooda.
  prefs: []
  type: TYPE_NORMAL
- en: '***Bagging and Random Forest***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Bagging and random forest (RF) are quintessential examples of *ensemble* classifiers,
    algorithms that combine the predictions of multiple other classifiers. Ensemble
    classifiers typically start with a known classifier (sometimes referred to as
    a *weak learner*). In the case of bagging and RF classifiers, this is often a
    decision tree.
  prefs: []
  type: TYPE_NORMAL
- en: Bagging and RF algorithms use two instruments to provide a level of robustness
    to the classification result based on the training set. The first instrument is
    to randomly select some number of subsets from the training set. The second is
    to randomly select a subset of the features. There is typically no requirement
    for constructing these subsets other than that the size of each be some percentage
    of the size of the original training set–for example, 65 or 80 percent. A weak
    learner is then separately and independently trained on each subset to yield a
    class label. The class of a new app is declared to be the class predicted by the
    majority of the weak learners.
  prefs: []
  type: TYPE_NORMAL
- en: Bagging does not require the weak learner to be a decision tree; it could be
    any type of classifier. On the other hand, random forest classifiers assume that
    the weak learner is a decision tree. For each of the subsets discussed earlier,
    a decision tree is constructed, and at every node in any of the decision trees
    involved, a given set of attributes is deemed *active*. These active attributes
    are those that haven’t been used in the path from the decision tree’s root to
    that node. A random subset of active attributes is then selected at each node,
    and the best splitting condition is selected from the conditions definable by
    the active attributes only; inactive attributes are not considered, even if they
    provide a better Gini result. Each decision tree then generates a label, as before,
    and the class that gets more “votes” ends up being the class assigned to a given
    app.
  prefs: []
  type: TYPE_NORMAL
- en: '***Support Vector Machines***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A support vector machine (SVM) algorithm tries to find a *hyperplane* that splits
    the feature space into two in such a way that the feature vectors associated with
    one class (in our case, malware) primarily lie on one side of the hyperplane and
    the feature vectors associated with the other (goodware) lie on the other side.
    We showed two such hyperplanes in [Figure 5-2](ch05.xhtml#ch5fig2). In a two-dimensional
    feature space, a hyperplane is just a straight line. However, a hyperplane could
    also be a quadratic line or even a sine curve.
  prefs: []
  type: TYPE_NORMAL
- en: 'A *linear SVM* uses only straight lines as separators. In higher dimensions,
    a linear hyperplane has the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '*a*[1]*x*[1] + *a*[2]*x*[2] + ... + *a[n]x[n]* = *b*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, *x*[1], *…*, *x*[*n*] represent the *n* features and *a*[1], *…*, *a*[*n*]
    and *b* are constants. Such a hyperplane divides the feature space into two parts:
    one part that satisfies *a*[1]*x*[1] + *a*[2]*x*[2] + *…* + *a*[*n*]*x*[*n*] ≥
    *b* and another part that satisfies *a*[1]*x*[1] + *a*[2]*x*[2] + … + *a[n]x[n]*
    ≤ *b*. The idea is that most malware will lie in one of these two parts and most
    goodware in the other. Implementers must decide what to do with apps whose feature
    vectors lie directly on the separating line.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To find a good separating hyperplane in a linear SVM, we usually consider two
    major factors: homogeneity of the feature vectors on either side of the hyperplane,
    and avoidance of feature vectors that lie close to the hyperplane. In terms of
    homogeneity, we want most of the app feature vectors on one side of the separator
    to be malware and most of the feature vectors on the other side to be goodware.
    For example, recall the horizontal separation in [Figure 5-2](ch05.xhtml#ch5fig2),
    where we classified a new app by merely counting the number of calls it made to
    classes in a certain package: if that number was greater than 3,000, we classified
    the app as goodware, and otherwise we classified it as malware. Of course, this
    is a highly simplified example. In the real world, the implementation would consider
    many more features, and the separator line’s equation would likely be far more
    complex.'
  prefs: []
  type: TYPE_NORMAL
- en: The second major factor in SVM design concerns feature vectors that lie very
    close to the separator line, like the malware specimen represented as a cross
    on the right side of [Figure 5-2](ch05.xhtml#ch5fig2), just below both separator
    lines. How certain can we be that such samples are correctly classified? To increase
    the distance between feature vectors and the separator line, we make use of *support
    vectors*, which are feature vectors in the training set that are as close to the
    separator line as possible. The distance between a separator line and its support
    vectors is called the *margin*. SVMs try to find the nearest separator line that
    maximizes the margin, reflecting the intuition that we do not want training points
    that are too close to the edge.
  prefs: []
  type: TYPE_NORMAL
- en: The goals of maximizing the margin and minimizing classification errors often
    conflict. As a consequence, we usually formulate the problem of finding the best
    separator line as an optimization problem. We won’t go into the mathematical details
    of SVMs in this chapter, but the interested reader can find more information in
    “Support-Vector Networks” by Corinna Cortes and Vladimir Vapnik and “Improving
    the Accuracy and Speed of Support Vector Machines” by Christopher J. Burges and
    Bernhard Schölkopf.
  prefs: []
  type: TYPE_NORMAL
- en: There are also many nonlinear versions of SVMs. For instance, quadratic SVMs
    allow the separator hyperplane to take the form of a quadratic curve and tackle
    a more complex distribution of feature vectors. Other kinds of SVMs use *kernel*
    tricks, which map the original feature vector to a new feature vector. The mapping
    usually involves a nonlinear method. When we apply a linear SVM to this nonlinear
    transformation, it yields a nonlinear separator for the original data. As a consequence,
    the resulting separators can have unusual shapes. For example, [Figure 5-5](ch05.xhtml#ch5fig5)
    shows a modified training dataset in part (a) that is similar to the training
    set visualizations shown earlier in this chapter. Parts (b), (c), and (d) show
    the separators generated by SVMs using different kinds of kernels.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch05fig05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-5: Sample nonlinear SVM separators generated with kernels for a training
    set (a) with malware feature vectors (crosses) and goodware feature vectors (dots),
    (b) SVM separator using a polynomial kernel, (c) SVM separator using a quadratic
    kernel, and (d) SVM separator using a radial basis kernel*'
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the generated regions are not as easy to describe as those using
    linear separators.
  prefs: []
  type: TYPE_NORMAL
- en: '***k-Nearest Neighbors***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A *k*-nearest neighbor classifier is very simple. It doesn’t really “learn”
    a model. It takes the feature vector of an app that it has never seen before,
    identifies the *k* feature vectors in the training data that are closest to the
    app’s feature vector using some distance metric (for example, Euclidean distance
    or cosine distance), and then finds the classes of those *k* apps. If more than
    half of the *k* apps are malware, it declares the app to be malware, too; otherwise
    it declares it to be goodware. For instance, consider the two apps, *A*1 and *A*2,
    shown in [Figure 5-6](ch05.xhtml#ch5fig6).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch05fig06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-6: A sample k-nearest neighbor classifier with* k = 3'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we consider the three nearest neighbors (in other words, *k* = 3). In
    the case of *A*1, two of the three nearest neighbors are goodware, so app *A*1
    would be considered goodware. However, in the case of *A*2, two of the three nearest
    neighbors are classified as malware; hence, this app would also be classified
    as malware.
  prefs: []
  type: TYPE_NORMAL
- en: '***Naive Bayes***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Naive Bayes classifiers use a very different kind of intuition than the preceding
    classifier types. They learn a set of simple probabilities from the training data,
    then use these probabilities later to classify new feature vectors.
  prefs: []
  type: TYPE_NORMAL
- en: To classify apps as goodware or malware, a naive Bayes classifier may compute
    what we call *class-conditional* probabilities. For a given class (in our case,
    either goodware or malware), we could use the training set to derive the class-conditional
    probability that a feature vector’s *i*th feature has a certain value given that
    the app belongs to a specific class. Consider the small training set shown in
    [Table 5-2](ch05.xhtml#ch5tab2).
  prefs: []
  type: TYPE_NORMAL
- en: Features *A* and *B* represent calls to *APIPackage:android.app* and *Opcode:if-eq*,
    respectively. The probabilities *P*(*A* = 10*|*0) and *P*(*A* = 10*|*1) are the
    class-conditional probabilities of the attribute *A* having the value 10 when
    the classes are 0 and 1, respectively. Using the training set, we can see that
    *P*(*A* = 10*|*0) is 0.2, because 2 of the 10 goodware apps in the training set
    have a value of 10 for *A*. *P*(*A* = 10*|*1) is also 0.2\. In contrast, *P*(*A*
    = 3*|*0) equals 0, while *P*(*A* = 3*|*1) equals 0.3.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 5-2:** Sample Training Set'
  prefs: []
  type: TYPE_NORMAL
- en: '| **A** | **B** | **App ID** | **Class** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0 | app1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0 | app2 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 0 | app3 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 0 | app4 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0 | app5 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 2 | app6 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 1 | 1pp7 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 72 | 82 | app8 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 72 | 24 | app9 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 30 | 10 | app10 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | app11 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | app12 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | app13 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | app14 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | app15 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 1 | app16 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 1 | app17 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 72 | 190 | app18 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 72 | 190 | app19 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 30 | 144 | app20 | 0 |'
  prefs: []
  type: TYPE_TB
- en: 'A naive Bayes classifier might also calculate the *prior probability* of each
    class, which is simply the probability of a random app in the training set belonging
    to that class. In our small training set, these prior probabilities are 0.5 for
    each of the two classes, as the data has 10 goodware samples and 10 malware samples
    in it. Given a new app *a* with an associated feature vector consisting of values
    *f*[*a*] = (*v*[1], *…*, *v*[*n*]), naive Bayes computes the probability of this
    app belonging to class *c* via the Bayes rule, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/math173.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In plain English, this says that the probability of the app *a* belonging to
    the class *c* is the probability of *a*’s feature vector being generated by class
    *c* times the prior probability of class *c* divided by the prior probability
    of the feature vector of app *a*.
  prefs: []
  type: TYPE_NORMAL
- en: To determine the class of a new app *a* with the feature vector *f*[*a*], naive
    Bayes would find the class *c* for which *P*(*c*|*f*[*a*]) is maximal across all
    possible classes. The result is the same as finding the class *c* such that *P*(*f*[*a*]|*c*)
    ×*P*(*c*) is maximal, as the denominator of the probability formula remains the
    same regardless of the class considered. Let us call this product a *pseudo-probability*.
    We want to find the class *c* that maximizes this pseudo-probability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now consider a new app *a* whose feature vector is (3, 1), meaning feature
    *A* equals 3 and *B* equals 1\. Notice that there is no app in the training set
    with this feature vector. Naive Bayes computes the probability of seeing the feature
    vector (3, 1) by making an independence assumption; it assumes that the probability
    of seeing the feature vector is the product of the probability of seeing each
    component of the feature vector. In formal terms, we can write this as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/math174-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![image](../images/math174-02.jpg) is the conditional probability that
    ![image](../images/math174-03.jpg), given that an app is in class *c*, and ![image](../images/math174-04.jpg)
    represents the *i*th component of app *a*’s feature vector *f*[*a*].
  prefs: []
  type: TYPE_NORMAL
- en: 'Returning to the example in [Table 5-2](ch05.xhtml#ch5tab2), we see that the
    pseudo-probabilities for the feature vector (3, 1) are given by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/math174-05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As the latter is larger than the former, this particular app with feature vector
    (3, 1) is classified as malware.
  prefs: []
  type: TYPE_NORMAL
- en: 'Naive Bayes classifiers have several problems. Often, especially when the feature
    vector is long, the numerator in the product calculation ends up being zero, which
    results in a probability of zero. A number of variants of naive Bayes fix such
    problems by making different types of assumptions about the way the values in
    each feature are distributed. For example, Gaussian naive Bayes assumes they are
    distributed in accordance with a normal distribution whose mean and standard deviation
    are computed from the observed values of the feature in the training data. You
    can find more information about naive Bayes classifiers and their different variants
    in “Discrete Bayesian Network Classifiers: A Survey” by Concha Bielza and Pedro
    Larranaga.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Evaluating Machine Learning Models**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once we’re done training a model, we want to know how well it performs. Researchers
    have developed multiple metrics to evaluate machine learning models. We’ll discuss
    a few important ones in this section, focusing on binary classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the evaluation results to be useful, we should compute these metrics using
    samples that aren’t present in the training data. Having a large, randomly sampled
    evaluation set is key to understanding a classifier’s strengths and deficiencies.
    This evaluation set, like the training set, should contain individual samples,
    along with labels for each sample. Generally, our evaluation should take into
    account the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**True positives (TPs)** Apps that are predicted by the classifier to be malware
    and that are in fact labeled as malware'
  prefs: []
  type: TYPE_NORMAL
- en: '**False positives (FPs)** Apps that are predicted by the classifier to be malware
    but are in fact labeled as goodware'
  prefs: []
  type: TYPE_NORMAL
- en: '**True negatives (TNs)** Apps that are predicted to be goodware and are labeled
    as goodware'
  prefs: []
  type: TYPE_NORMAL
- en: '**False negatives (FNs)** Apps that are predicted to be goodware but are labeled
    as malware'
  prefs: []
  type: TYPE_NORMAL
- en: Too many false positives or false negatives indicates poor performance. Other
    important statistical metrics to consider are shown in [Table 5-3](ch05.xhtml#ch5tab3),
    which presents the results of a random forest classifier on a “Goodware vs. Android
    Banking Trojans” dataset we’ve collected from a host of online websites. The following
    discussion describes those metrics in detail.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 5-3:** Example Metrics for Evaluating Machine Learning Models'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Dataset** | **Classifier** | **Accuracy** | **Precision** | **Recall**
    | **F1 score** | **AUC** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Goodware vs. Banking Trojans | RF | 0.9908 | 0.9909 | 0.9910 | 0.9910 | 0.9931
    |'
  prefs: []
  type: TYPE_TB
- en: '*Accuracy* measures how many predictions a classifier got right in the evaluation
    set (in other words, the proportion of TPs and TNs with respect to the total number
    of predictions). We calculate it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*A* = (*TP* + *TN*)/(*TP* + *FP* + *TN* + *FN*)'
  prefs: []
  type: TYPE_NORMAL
- en: While accuracy is an intuitive measurement, it has several issues when applied
    to the detection of malicious apps. Chief among these is that malware occurs very
    rarely, so most of the evaluation data is likely to be labeled as goodware if
    it is representative of real-world conditions. This means that a classifier can
    obtain a very good accuracy rating simply by predicting that every app is goodware.
    It isn’t uncommon for less than 1 percent of samples to be malware; in that case,
    such a classifier would have over 99 percent accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '*Precision* measures how accurate our classifier is when it correctly predicts
    an app to be malware. We calculate it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P* = *TP*/(*TP* + *FP*)'
  prefs: []
  type: TYPE_NORMAL
- en: This metric captures the percentage of items predicted to belong to a class
    that were actually in the class. However, it does not capture the full picture.
    Let’s consider a set that contains 100 samples, of which 50 are malware. If our
    classifier predicts that only 1 sample from the set is malware and that 99 are
    goodware, it will have 100 percent precision, but it isn’t doing a very good job
    at finding malware.
  prefs: []
  type: TYPE_NORMAL
- en: '*Recall* is a complementary measurement to precision that computes how many
    positive samples a classifier misses. We calculate it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*R* = *TP*/(*TP* + *FN*)'
  prefs: []
  type: TYPE_NORMAL
- en: Recall by itself might not be a good performance indicator, as a classifier
    that predicts everything to be malware will achieve 100 percent recall. In general,
    we want a classifier to have both good precision *and* good recall.
  prefs: []
  type: TYPE_NORMAL
- en: One solution is to combine the two. For example, we sometimes calculate the
    *F1 score* of a classifier, or the harmonic mean of precision and recall. This
    value is an attempt to balance the two metrics to identify strong classifiers.
    Most malware classifiers produce an F1 score between 0 and 1, where 0 represents
    higher confidence that the prediction is goodware and 1 means that the app is
    malware.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *receiver operating characteristic (ROC) curve* plots the performance of
    a classifier at various thresholds to help us pick a good threshold and compare
    different classifiers. A ROC graph has two axes. The *true positive rate* is the
    same as recall, and the *false positive rate* is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*FPR* = *FP*/(*FP* + *TN*)'
  prefs: []
  type: TYPE_NORMAL
- en: The *area under the ROC curve (AUC)* gives an overall measurement of the classifier
    across all thresholds. To understand AUC, imagine sorting all the apps in an evaluation
    by the classifier’s score. AUC measures the probability of a randomly selected
    malware app having a higher score than a randomly selected goodware app. An ideal
    classifier would always provide lower scores to goodware than to malware; such
    a classifier would have an AUC of 1\. A really bad classifier that does the opposite
    would have an AUC of 0, and a random classifier would have an AUC of 0.5\. A sample
    ROC curve for the decision tree algorithm is shown in [Figure 5-7](ch05.xhtml#ch5fig7).
  prefs: []
  type: TYPE_NORMAL
- en: 'AUC has several advantages: notably, it is invariant to class skew (which occurs
    when the number of samples in one class far outnumbers that of the other class)
    and independent of specific thresholds. However, it treats both FPs and FNs equally.
    This might not be desirable if you want to make sure that no malware slips through.
    When you’re trying to protect a store like Google Play, for example, it’s better
    to err on the side of caution. That is, it’s preferable to manually review too
    many apps, even if they turn out to be goodware for the most part, than too few.
    So, you might instead want to pick a model that treats FPs as more desirable.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch05fig07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-7: The ROC curve for the decision tree classifier*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Struggles of Machine Learning Classifiers**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we describe some common pitfalls that can adversely affect
    the performance of machine learning classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: '***Identical Feature Vectors***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Some malware identification datasets include apps with identical feature vectors.
    This can happen in two broad cases. In the first case, different apps in the dataset
    are variants of one another. We call these *isomorphic* apps. It’s important to
    make sure that no app in the training data has corresponding isomorphic apps in
    the test data. Otherwise, they will artificially inflate the performance of the
    classifier. The second case occurs when the feature set is impoverished. This
    is also very serious, because it suggests that the selected features aren’t adequate
    enough to distinguish between apps that are truly different.
  prefs: []
  type: TYPE_NORMAL
- en: '***Balance vs. Imbalance***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Machine learning algorithms generally produce good models when the data is *balanced*,
    meaning it has reasonably comparable percentages of samples in the different classes.
    Conversely, some algorithms may struggle to perform when the data is massively
    imbalanced.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, suppose we are trying to distinguish between Android spyware and
    goodware. In most general malware datasets available today, the number of spyware
    samples will be much smaller than the number of goodware samples. This may be
    due to the fact that the dataset was collected to distinguish all forms of malware
    (not just spyware) from goodware. If a classifier is trained to separate spyware
    from goodware, the number of spyware samples would be relatively small compared
    to the number of goodware samples.
  prefs: []
  type: TYPE_NORMAL
- en: Such imbalances in the class sizes can severely affect the performance of classification
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: '***Interpretability***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When detecting malware, security analysts must identify compelling evidence
    of an app’s malicious nature. Machine learning algorithms that return a verdict
    without providing information regarding why a particular app was flagged as malicious
    or benign may be useful for automated protection efforts but not for human-supported
    analysis. For confirmation, an analyst needs the algorithm to lead them toward
    the source of malicious behavior. Without any such guidance, verifying the algorithm’s
    verdict by analyzing the training set, the model, and its output becomes the equivalent
    of a complete app review and is like finding a needle in a haystack.
  prefs: []
  type: TYPE_NORMAL
- en: For that reason, many malware detection methods use handcrafted features that
    enable the analyst to find malicious parts of the code or demonstrate malicious
    behavior when the code is run. This is also one major reason why deep learning
    methods aren’t always the best option for malware detection in industry. It is
    difficult to understand how these machine learning algorithms produce their output.
  prefs: []
  type: TYPE_NORMAL
- en: '***Cross-Validation vs. Rolling Window Prediction***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Many machine learning–based malware detection algorithms have been evaluated
    in the literature using *k*-fold cross-validation, a technique that randomly splits
    the training data into *k* disjointed pieces, called *folds*, then performs *k*
    iterations over the folds. In each iteration, a corresponding fold (for example,
    the third fold on the third iteration) is removed and some classifier of a given
    type (such as an SVM) learns from all the remaining folds. The model then makes
    predictions about the removed fold, and its performance is computed on that iteration
    alone using a metric such as the AUC or F1 score, discussed earlier. The technique
    then makes a final assessment of a model type (for instance, SVM) by taking an
    aggregate value of the performance metric across all *k* folds.
  prefs: []
  type: TYPE_NORMAL
- en: However, the use of cross-validation may not always be appropriate, because
    malware evolves over time and *k*-fold cross validation ignores the time at which
    a given app in the training data was first released into the wild. As a consequence,
    the folds used during any iteration might include apps that were released into
    the wild *after* some of the apps in the removed fold. Intuitively, what this
    means is that we are likely predicting the status of some apps using information
    from the future, which can artificially boost the performance of the classifier.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, *rolling window prediction* sorts the apps *a*[1], *…*, *a*[*n*]
    in the dataset based on the times at which each app entered the wild. We then
    assume that we need at least *j* apps for decent training. For each *i*, such
    that *j* < *i* ≤ *n*, we train on the dataset {*a*[1], *…*, *a*[*j*]} and assess
    the performance ![image](../images/math179-01.jpg) of a given classifier. We consider
    the overall performance of the classifier to be the average of the ![image](../images/math179-01.jpg)s
    for *i* < *j* ≤ *n.* This methodology avoids the possibility of using information
    from the future to predict the past.
  prefs: []
  type: TYPE_NORMAL
- en: '**Up Next**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This chapter presented an overview of machine learning algorithms that are widely
    used in malware analysis and detection. In the next chapter, we explore the features
    we can use as input to these algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Though publicly available machine learning libraries are constantly evolving,
    you may find it worthwhile to explore the possibilities offered by the R, scikit-learn,
    and TensorFlow libraries. You can also find a list of app hashes, as well as the
    static and dynamic features described in this and the next chapter, at [*https://github.com/android-malware-ml-book*](https://github.com/android-malware-ml-book).
    Use these libraries to learn the different types of predictive models capable
    of separating malware from goodware.
  prefs: []
  type: TYPE_NORMAL
- en: '[*OceanofPDF.com*](https://oceanofpdf.com)'
  prefs: []
  type: TYPE_NORMAL
