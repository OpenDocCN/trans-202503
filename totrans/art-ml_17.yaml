- en: '**12'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: IMAGE CLASSIFICATION**
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Probably the most widely celebrated application of NNs is image classification.
    Indeed, the popularity of NNs today is largely due to some spectacular successes
    of NNs in image classification contests in the early 2000s. Earlier, the NN field
    had been treated largely as a curiosity and not a mainstream tool.
  prefs: []
  type: TYPE_NORMAL
- en: 'And the surge in popularity of NNs for image classification then had a feedback
    effect: the more NNs did well with images, the more image classification researchers
    used NNs as their tool, thus the more they refined use of NNs for imaging, which
    in turn led to more NN success in the contests.'
  prefs: []
  type: TYPE_NORMAL
- en: In principle, *any* of the methods in this book could be used on images. The
    features are the pixel intensities, and the outcome is the class of image. Consider,
    for instance, the famous MNIST data. Here we have 70,000 images of handwritten
    digits, with each image having 28 rows and 28 columns of pixels. Each pixel has
    an intensity (brightness) number between 0 and 255, and we have 28² = 784 pixels,
    so we have 784 features and 10 classes.
  prefs: []
  type: TYPE_NORMAL
- en: The “secret sauce” for NNs in the image field has been *convolutional* operations,
    leading to the term *convolutional neural networks (CNNs)*. Actually, those operations
    were not really new; they borrow from classical image processing techniques. And
    most important, convolutional operations are not inherent to NNs. They could be
    used with other ML methods, and in fact, some researchers have developed *convolutional
    SVMs*. But again, the momentum in the image field is solidly with CNNs.
  prefs: []
  type: TYPE_NORMAL
- en: Thus the focus of this chapter on images will be on NNs. We’ll start with a
    non-NN example to make the point that any ML method might be used and then get
    into CNNs.
  prefs: []
  type: TYPE_NORMAL
- en: '12.1 Example: The Fashion MNIST Data'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It’s standard to use the MNIST data as one’s introductory example, but let’s
    be a little different here. The Fashion MNIST data is of the same size as MNIST
    (28 × 28 pixel structure, 10 classes, 70,000 images) but consists of pictures
    of clothing (10 types) rather than digits. (The dataset is available at [*https://github.com/zalandoresearch/fashion-mnist*](https://github.com/zalandoresearch/fashion-mnist).)
  prefs: []
  type: TYPE_NORMAL
- en: One important difference is that while MNIST can be considered basically black
    and white, Fashion MNIST truly has “shades of gray.” An example is shown in [Figure
    12-1](ch12.xhtml#ch12fig01). The blurriness is due to the low 28 × 28 resolution
    of the image set.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch12fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-1: A Fashion MNIST image*'
  prefs: []
  type: TYPE_NORMAL
- en: This makes the dataset more challenging, and accuracy rates are generally somewhat
    lower for Fashion MNIST than for MNIST.
  prefs: []
  type: TYPE_NORMAL
- en: '***12.1.1 A First Try Using a Logit Model***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The dataset actually comes already partitioned into training and test sets (60,000
    and 10,000 rows, respectively), but for convenience, let’s just stick to the training
    set, which I have named `ftrn`, with columns V1, V2, . . . , V785\. That last
    column is the clothing type, with values 0 through 9.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try a logistic model on this data (which, by the way, took about 2 hours
    to run):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'So, we were attaining about 80 percent accuracy. The base was only about 10
    percent accuracy, which makes sense: there are roughly equal numbers of the 10
    clothing types, so random guessing would give us about 10 percent. Thus 80 percent
    is not bad. But since the world’s record best accuracy on this dataset is in the
    high 90s, we would like to do better.'
  prefs: []
  type: TYPE_NORMAL
- en: '***12.1.2 Refinement via PCA***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We might surmise, as we did above, that *p* = 784 is too large and is in need
    of dimension reduction. One possible remedy would be to use PCA:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Ah, now we are up to about 83 percent accuracy. (And it took only about a minute
    to run.)
  prefs: []
  type: TYPE_NORMAL
- en: We could try different values of the number of principal components, but a better
    approach would likely be to take advantage of what we know about images, as we
    will now see.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2 Convolutional Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Though the CNN structure may seem complex at first, it actually is based on
    simple ideas. Let’s get started.
  prefs: []
  type: TYPE_NORMAL
- en: '***12.2.1 Need for Recognition of Locality***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It’s no coincidence that the picture in [Figure 12-1](ch12.xhtml#ch12fig01)
    looks blurry. Remember, these are very low-resolution images (28 × 28 pixels).
    Yet even though this is tiny as images go, it gives us 784 features. With *n*
    = 70, 000, our “![Image](../images/prootn.jpg)” rule of thumb ([Equation 3.2](ch03.xhtml#ch03equ02))
    would suggest a maximum of about 260 features, which is well short of 784\. And
    while that rule is conservative—CNNs do well in spite of ending up with *p* much
    greater than *n*—it’s clear that using the 784 pixels as if they were unrelated
    features is going to impede our ability to predict new cases well.
  prefs: []
  type: TYPE_NORMAL
- en: We need to exploit the *locality* of our images. An image pixel tends to be
    correlated with its neighboring pixels, and the nature of this relationship should
    help us classify the image. Is that pixel part of a short straight line, say,
    or maybe a small circle? The *convolutional model* is designed with this in mind,
    consisting of various operations that are applied to patches within an image.
    These patches are often called *tiles*, which we will work with here.
  prefs: []
  type: TYPE_NORMAL
- en: '***12.2.2 Overview of Convolutional Methods***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let’s first sneak a look at the code and run it on the Fashion MNIST data.
    After that, we’ll explain the operations in the code. We’ll use code adapted from
    an RStudio example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Ah, now we are up to more than 92 percent correct classification. And we almost
    certainly could do even better by tuning the hyperparameters (including changing
    the number and structure of the image operation layers).
  prefs: []
  type: TYPE_NORMAL
- en: What do we see here?
  prefs: []
  type: TYPE_NORMAL
- en: Using `qeNeural()`’s `conv` argument, we have set up five image-operation layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The image-operation layers are followed by “ordinary” layers (not specified
    here), thus taking `qeNeural()`’s default value of two layers of 100 neurons each.
    The “ordinary” layers are termed *dense* layers or *fully connected* layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first image-operation layer performs a convolutional operation on the input
    image, which involves extracting tiles and forming linear combinations of the
    image intensities within each tile. These linear combinations become outputs of
    the layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recall from earlier chapters that in the linear combination, say, 3*a* − 1.5*b*
    + 16.2*c*, the numbers 3, −1.5, and 16.2 are called *coefficients*. In the CNN
    context, they are called *weights*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Usually we will use many different sets of weights. The `conv2d` parameter `filters`
    specifies the number of sets of weights that we want for a layer. It acts analogously
    with the number of neurons in a dense layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `conv2d` parameter `kern` value specifies the tile size, with the value
    3 in the first layer meaning 3 × 3 tiles.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another `conv2d` parameter `stride` controls the number of tiles in an image
    by specifying the amount of overlap a tile has with its neighbors, which will
    be explained below.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The argument `xShape` specifies the size of an image, such as 28 × 28 in the
    current example.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For color data of that size, we would denote the situation as 28 × 28 × 3, with
    the 3 referring to the number of primary colors, red, yellow, and blue. We would
    have a 28 × 28 array of red intensities, then another for yellow, and finally
    one for blue. Then we would set *xShape* to (28,28,3).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That third coordinate, 3, is called a *channel*. We don’t call it “color,” because
    it may not be a color, as will be seen below.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output of one layer is input to the next, and its dimensions may be, say,
    13 × 13 × 64\. That would be treated as a 13 × 13 “image” with 64 “primary colors,”
    both of which are artificial. The point is that, mathematically, any three-dimensional
    array can be treated this way, and it makes the code simpler to do so. (An array
    of three or more dimensions is called a *tensor*, hence the name `tensorflow`
    for the Python package underlying what we do here.)
  prefs: []
  type: TYPE_NORMAL
- en: '***12.2.3 Image Tiling***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The first and third layers in the above code example perform a *convolution*
    operation. (Readers with a background in probability theory, Fourier analysis,
    and so on will find that the term is used somewhat differently in ML.) To explain,
    we first need to discuss breaking an image into tiles.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider this toy example of a 6 × 6 grayscale image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch12equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For example, the intensity of the pixel in row 2, column 4 of the image is 11\.
    In R, we would store this in a matrix of 6 rows and 6 columns. (An R matrix is
    like a data frame in which the elements are all numeric or all character strings
    and so on.)
  prefs: []
  type: TYPE_NORMAL
- en: 'We could break this into non-overlapping tiles of size 3 × 3:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch12equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: So, our original matrix has been partitioned into four submatrices or tiles.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also have overlapping tiles, using a number called the *stride*. In
    the above example, the stride is 3: The first column of the upper-right tile'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch12equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: is 3 columns to the right of the first column of the upper-left tile located
    in [Equation 12.2](ch12.xhtml#ch12equ02)
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch12equ04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: and so on. Similar statements hold for rows. For example, the first row of the
    lower-right tile is 3 rows below the first row of the upper-right tile.
  prefs: []
  type: TYPE_NORMAL
- en: With a stride of 1, say, our first 3 × 3 tile would still be
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch12equ05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: But the second would be just 1 column to the right of the first tile
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch12equ06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: and so on.
  prefs: []
  type: TYPE_NORMAL
- en: The default value of `stride` in `'conv2d'` operations is 1.
  prefs: []
  type: TYPE_NORMAL
- en: '***12.2.4 The Convolution Operation***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Say we are using 3 × 3 tiles. It is convenient to express the coefficients
    of a linear combination in matrix form too, say, in the weights matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch12equ07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For a given tile:'
  prefs: []
  type: TYPE_NORMAL
- en: The tile’s element in row 1, column 1 will be multiplied by *w*[11].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The tile’s element in row 1, column 2 will be multiplied by *w*[12].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: . . .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The tile’s element in row 3, column 3 will be multiplied by *w*[33].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of those products will be summed to produce a single number.
  prefs: []
  type: TYPE_NORMAL
- en: 'This set of weights is then applied to each tile. Applying to the upperleft
    tile in [Equation 12.2](ch12.xhtml#ch12equ02), we have the single number:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch12equ08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We apply the same set of weights to each tile. For instance, applying the weights
    to the upper-right tile, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch12equ09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We do the same for the lower-left and lower-right tiles, yielding four numbers
    altogether, which we arrange in a 2 × 2 matrix. That is output to the next layer.
  prefs: []
  type: TYPE_NORMAL
- en: Now suppose we have, say, 12 filters. That means 12 different sets of weights—that
    is, 12 different versions of the matrix in [Equation 12.7](ch12.xhtml#ch12equ07).
    That means 12 different 2 × 2 matrices coming out of this layer. Thus the output
    of this layer is described as 2 × 2 × 12\. To be sure, yes, the total output of
    this layer will be 48 numbers, but we think of them as consisting of 12 sets of
    2 × 2 matrices, hence the 2 × 2 × 12 notation.
  prefs: []
  type: TYPE_NORMAL
- en: Note we are not choosing these weights ourselves. They are chosen by the NN
    algorithm, which will minimize the overall prediction sum of squares. We choose
    the *number* of sets, 12 here, but not the sets themselves. The algorithm will
    try many different collections of 12 sets of weights, in hope of finding a collection
    that minimizes the prediction sum of squares.
  prefs: []
  type: TYPE_NORMAL
- en: So . . . there is really nothing new. We are taking linear combinations of the
    inputs and feeding them to the next layer, just as in the last chapter. In the
    end, the algorithm minimizes the sum of squared prediction errors, just as before.
  prefs: []
  type: TYPE_NORMAL
- en: The difference, though, is the structuring of the data into tiles, exploiting
    locality. The role of the weights is to determine the relative importance of various
    pixels, especially in how they work together.
  prefs: []
  type: TYPE_NORMAL
- en: It will be helpful to visualize this as a “building” with 12 “floors.” Each
    “floor” consists of four “rooms,” arranged in two rows of two rooms each. We will
    take this approach below.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we still have the usual Bias-Variance Trade-off as with dense layers:
    the more filters, the more opportunities to reduce bias, but the more variance
    we incur in the weights.'
  prefs: []
  type: TYPE_NORMAL
- en: '***12.2.5 The Pooling Operation***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Recall the second layer in the example in [Section 12.2](ch12.xhtml#ch12lev2):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This is not a convolutional layer; it’s a *pooling* layer.
  prefs: []
  type: TYPE_NORMAL
- en: Pooling involves replacing the elements in a tile by some representative value,
    say, the mean or the median, or even the maximum value in the tile. The latter
    is quite common, in fact, and is the one used in the `regtools` and `qeML` packages.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reader may wonder, “Isn’t pooling a special case of convolutional operations?
    For example, isn’t taking the mean in a 2 × 2 tile the same as a convolutional
    operation with all the weights being 0.25?” The answer is yes, but with one big
    difference: here the weights are fixed at 0.25; they are not chosen by the algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the `conv2d` operation, where the default stride is 1, for pooling, the
    default stride is the tile size, specified above as 2.
  prefs: []
  type: TYPE_NORMAL
- en: '***12.2.6 Shape Evolution Across Layers***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now, what will be the structure of the output from this second layer? Let’s
    reason this out. Here again are the specifications of the first two layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The input to the first layer was 28 × 28, or 28 × 28 × 1\. The first breaks
    things into 3 × 3 tiles with a stride of 1\. Just as there is a 2 × 2 array of
    tiles in [Equation 12.2](ch12.xhtml#ch12equ02), here we will have a 26 × 26 array
    of tiles, again taking into account that the stride is 1.
  prefs: []
  type: TYPE_NORMAL
- en: So in that first layer, each filter will output a total of 26² numbers, in 26
    × 26 form. With 32 filters, the total output of that first layer will be 26 ×
    26 × 32\. In the “building” metaphor, this means 32 floors, with each floor having
    26 rows of rooms with 26 rooms per row. Note again here that each “room” holds
    one number.
  prefs: []
  type: TYPE_NORMAL
- en: Now, what then will happen at the second layer? It will receive 32 tiles sized
    26 × 26\. What will it do with them?
  prefs: []
  type: TYPE_NORMAL
- en: The tile size used by this layer, as discussed above, is 2 × 2, with a stride
    of 2\. Applying this to an inputted 26 × 26 tile, 13 rows of 13 2 × 2 tiles in
    each row will be formed. In each 2 × 2 tile, the maximum value among the 4 numbers
    will be extracted.
  prefs: []
  type: TYPE_NORMAL
- en: Again using the “building” metaphor, each “floor” will produce 13² = 169 numbers,
    arranged in 13 × 13 form. Since we have 32 “floors,” the total output of this
    layer will be in the form 13 × 13 × 32\. (The `regtools` and `qeML` packages use
    the two-dimensional form of the pooling operation, so the pooling is done within
    floors and not across floors.)
  prefs: []
  type: TYPE_NORMAL
- en: '***12.2.7 Dropout***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As with the dense layers, the danger of overfitting—too many neurons per convolutional
    layer or too many convolutional layers—is high. The antidote is dropout, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This specifies randomly deleting 50 percent of the nodes in this layer.
  prefs: []
  type: TYPE_NORMAL
- en: '***12.2.8 Summary of Shape Evolution***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `keras` package gives us a summary of our CNN on request:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Recall that `qeNeural()` calls `regtools::krsFit()`, which in turn makes calls
    to the R `keras` package, so this output actually comes from the latter.
  prefs: []
  type: TYPE_NORMAL
- en: 'That last column shows the number of weights at each layer. For instance, here
    is where that 320 figure came from: each filter—that is, each set of numbers *w**[ij]*—is
    a 3 × 3 matrix, thus consisting of 9 numbers. There is also an intercept term
    *w*[0] (like *β*[0] in a linear regression model), for a total of 10 weights.
    Since there were 32 filters, we have 320 weights, as shown in the output table
    above.'
  prefs: []
  type: TYPE_NORMAL
- en: The `flatten` layer merely converts from our *a* × *c* form to ordinary data.
    The output of our second pooling layer had form 5 × 5 × 64, which amounts to 1,600
    numbers. In order to be used by a dense layer, the data is converted to a single
    vector of length 1,600.
  prefs: []
  type: TYPE_NORMAL
- en: Altogether, we have *p* = 179926 but only *n* = 65000\. So we are definitely
    overfitting. The fact that many such models have been found to work well is quite
    a controversy in the ML community!
  prefs: []
  type: TYPE_NORMAL
- en: '***12.2.9 Translation Invariance***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The weight structure lends *translation invariance*—a fancy term that actually
    has a simple meaning—to our analysis. Say we are using 3 × 3 for our tile size.
    That’s 9 pixels. For any tile, consider the pixel in the upper-left corner of
    the tile. Then we have the same weight *w*[11] for that pixel, regardless of whether
    the tile is near the top of the picture, say, or the bottom.
  prefs: []
  type: TYPE_NORMAL
- en: For facial recognition, for instance, this means that, to a large extent, we
    don’t have to worry whether the face is near the top of the picture, near the
    bottom, or near the middle. (Problems do occur near the edges of the picture,
    so the property holds only approximately.) The same statement would hold for left-right
    positioning.
  prefs: []
  type: TYPE_NORMAL
- en: 12.3 Tricks of the Trade
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Well, then, how in the world is one supposed to come up with the models? How
    many layers? What kinds of layers? What parameter values?
  prefs: []
  type: TYPE_NORMAL
- en: 'One might set some of the model on a hunch informed by the nature of the dataset,
    such as the size of various parts of the image, the image texture, and so on.
    But at the end of the day, the answer tends to be rather prosaic: after years
    of experimenting with various architectures (configurations), this one seems to
    work with certain kinds of images. Some architectures have been successful in
    wide-enough application that they have acquired names and become standards, such
    as AlexNet.'
  prefs: []
  type: TYPE_NORMAL
- en: '***12.3.1 Data Augmentation***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'One approach to dealing with smaller image sets is *data augmentation*. The
    idea here is simple: form new images from existing ones. One might shift a given
    image horizontally or vertically, shrink or enlarge the image, flip it horizontally
    or vertically, and so on. The motivation for this is that, later, we might be
    asked to classify a new image that is very similar to one in our training set
    but is, say, much higher or lower within the image frame. We want our algorithm
    to recognize the new image as being similar to the one in the training set.'
  prefs: []
  type: TYPE_NORMAL
- en: This is especially important for medical tissue images, say, from a biopsy,
    as there is no sense of orientation—no top or bottom, left or right, or back or
    front. This is in contrast to MNIST, for instance, where a ‘6’ is an upsidedown
    ‘9’ and the two are quite different.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can perform data augmentation using the `OpenImageR` package, with its `Augmentation()`
    function. In the latter, for instance, we can do a vertical flip operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The `keras` package also offers data augmentation services, including a *shear*
    (twist) operation.
  prefs: []
  type: TYPE_NORMAL
- en: '***12.3.2 Pretrained Networks***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A big issue in the image classification community is *transfer learning*. Here
    the issue is that, instead of starting from scratch in designing a neural network—dense
    layers, convolutional layers, and details of each—one builds on some network that
    others have found useful. One then either uses that network as is or takes it
    as a starting point and does some tweaking.
  prefs: []
  type: TYPE_NORMAL
- en: 12.4 So, What About the Overfitting Issue?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As noted in [Section 12.2.8](ch12.xhtml#ch12lev2sec8), the success of heavily
    overparameterized networks in image classification seems to contradict the conventional
    wisdom regarding overfitting. This has been the subject of much speculation in
    the ML community.
  prefs: []
  type: TYPE_NORMAL
- en: A key point may be that misclassification rates in image contexts tend to be
    very low, near 0 for highly tuned networks. In that sense, we are essentially
    in the settings that were termed *separable* in [Chapter 10](ch10.xhtml). Some
    insight into this issue may then be gained by revisiting [Figure 10-4](ch10.xhtml#ch10fig04)
    in that chapter.
  prefs: []
  type: TYPE_NORMAL
- en: As was pointed out, there are many lines, infinitely many, in fact, that could
    be used to distinguish the two classes, and thus be used to predict a new case.
    SVM chooses a particular line for this—the one halfway between the two closest
    points in the two classes—but again, one might use many other lines instead.
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, the separator need not be a straight line. It could be a “curvy” line,
    say, one obtained by using a polynomial kernel with SVM. Because of the clean
    separation of the two classes, there is plenty of wiggle room in which we could
    fit a very wiggly curve, say, a polynomial, of very high degree. And the higher
    the degree, the more coefficients in the equation of the curve—that is, the larger
    the value of *p*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The result: we could fit a curve that has a value of *p* much greater than
    *n* yet still get perfect prediction accuracy. Noting the connection of NNs to
    polynomial regression (see [Section 11.9](ch11.xhtml#ch11lev9)), we have a plausible
    explanation for the success of overparameterization in image classification.'
  prefs: []
  type: TYPE_NORMAL
- en: 12.5 Conclusions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In spite of this book’s aim to avoid writing many equations, the topic here
    is easily the most mathematical of all the chapters. Viewed from a high level,
    CNNs work from a very simple idea: break an image into tiles and then apply an
    NN to the tiled data. But the old saying “The devil is in the details” is quite
    apt here. It can be challenging, for instance, to keep clear in one’s mind the
    dimensionality of chunks of data as we move from layer to layer. Readers who wish
    to pursue further study beyond the introduction here will find a background in
    linear algebra and calculus to be quite useful.'
  prefs: []
  type: TYPE_NORMAL
