- en: '**12'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: IMAGE CLASSIFICATION**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: Probably the most widely celebrated application of NNs is image classification.
    Indeed, the popularity of NNs today is largely due to some spectacular successes
    of NNs in image classification contests in the early 2000s. Earlier, the NN field
    had been treated largely as a curiosity and not a mainstream tool.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'And the surge in popularity of NNs for image classification then had a feedback
    effect: the more NNs did well with images, the more image classification researchers
    used NNs as their tool, thus the more they refined use of NNs for imaging, which
    in turn led to more NN success in the contests.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: In principle, *any* of the methods in this book could be used on images. The
    features are the pixel intensities, and the outcome is the class of image. Consider,
    for instance, the famous MNIST data. Here we have 70,000 images of handwritten
    digits, with each image having 28 rows and 28 columns of pixels. Each pixel has
    an intensity (brightness) number between 0 and 255, and we have 28² = 784 pixels,
    so we have 784 features and 10 classes.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: The “secret sauce” for NNs in the image field has been *convolutional* operations,
    leading to the term *convolutional neural networks (CNNs)*. Actually, those operations
    were not really new; they borrow from classical image processing techniques. And
    most important, convolutional operations are not inherent to NNs. They could be
    used with other ML methods, and in fact, some researchers have developed *convolutional
    SVMs*. But again, the momentum in the image field is solidly with CNNs.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Thus the focus of this chapter on images will be on NNs. We’ll start with a
    non-NN example to make the point that any ML method might be used and then get
    into CNNs.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '12.1 Example: The Fashion MNIST Data'
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It’s standard to use the MNIST data as one’s introductory example, but let’s
    be a little different here. The Fashion MNIST data is of the same size as MNIST
    (28 × 28 pixel structure, 10 classes, 70,000 images) but consists of pictures
    of clothing (10 types) rather than digits. (The dataset is available at [*https://github.com/zalandoresearch/fashion-mnist*](https://github.com/zalandoresearch/fashion-mnist).)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: One important difference is that while MNIST can be considered basically black
    and white, Fashion MNIST truly has “shades of gray.” An example is shown in [Figure
    12-1](ch12.xhtml#ch12fig01). The blurriness is due to the low 28 × 28 resolution
    of the image set.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch12fig01.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-1: A Fashion MNIST image*'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: This makes the dataset more challenging, and accuracy rates are generally somewhat
    lower for Fashion MNIST than for MNIST.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '***12.1.1 A First Try Using a Logit Model***'
  id: totrans-14
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The dataset actually comes already partitioned into training and test sets (60,000
    and 10,000 rows, respectively), but for convenience, let’s just stick to the training
    set, which I have named `ftrn`, with columns V1, V2, . . . , V785\. That last
    column is the clothing type, with values 0 through 9.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集实际上已经分为训练集和测试集（分别为60,000和10,000行），但为了方便起见，我们只使用训练集，我将其命名为`ftrn`，其中包含列V1、V2、...、V785。最后一列是衣物类型，值为0至9。
- en: 'Let’s try a logistic model on this data (which, by the way, took about 2 hours
    to run):'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在这些数据上尝试一个逻辑回归模型（顺便说一下，运行这个模型大约花了2个小时）：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'So, we were attaining about 80 percent accuracy. The base was only about 10
    percent accuracy, which makes sense: there are roughly equal numbers of the 10
    clothing types, so random guessing would give us about 10 percent. Thus 80 percent
    is not bad. But since the world’s record best accuracy on this dataset is in the
    high 90s, we would like to do better.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们的准确率大约是80%。基线准确率只有大约10%，这很有道理：10种衣物类型大致数量相等，所以随机猜测会得到大约10%的准确率。因此80%的准确率并不算差。但由于这个数据集在世界纪录中的最佳准确率已经接近90%以上，我们希望做得更好。
- en: '***12.1.2 Refinement via PCA***'
  id: totrans-19
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***12.1.2 通过PCA进行改进***'
- en: 'We might surmise, as we did above, that *p* = 784 is too large and is in need
    of dimension reduction. One possible remedy would be to use PCA:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以推测，正如上面所说，*p* = 784太大，需要进行降维。一个可能的解决办法是使用PCA：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Ah, now we are up to about 83 percent accuracy. (And it took only about a minute
    to run.)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 啊，现在我们的准确率已经达到了大约83%。（而且运行只花了大约一分钟。）
- en: We could try different values of the number of principal components, but a better
    approach would likely be to take advantage of what we know about images, as we
    will now see.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以尝试不同的主成分数值，但更好的方法可能是利用我们对图像的了解，正如接下来我们将看到的那样。
- en: 12.2 Convolutional Models
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2 卷积模型
- en: Though the CNN structure may seem complex at first, it actually is based on
    simple ideas. Let’s get started.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管CNN结构一开始看起来可能很复杂，但它实际上基于一些简单的思想。让我们开始吧。
- en: '***12.2.1 Need for Recognition of Locality***'
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***12.2.1 需要识别局部性***'
- en: It’s no coincidence that the picture in [Figure 12-1](ch12.xhtml#ch12fig01)
    looks blurry. Remember, these are very low-resolution images (28 × 28 pixels).
    Yet even though this is tiny as images go, it gives us 784 features. With *n*
    = 70, 000, our “![Image](../images/prootn.jpg)” rule of thumb ([Equation 3.2](ch03.xhtml#ch03equ02))
    would suggest a maximum of about 260 features, which is well short of 784\. And
    while that rule is conservative—CNNs do well in spite of ending up with *p* much
    greater than *n*—it’s clear that using the 784 pixels as if they were unrelated
    features is going to impede our ability to predict new cases well.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[图12-1](ch12.xhtml#ch12fig01)中的图片模糊不清，这并非巧合。记住，这些是非常低分辨率的图像（28 × 28像素）。尽管这些图像很小，但它们给我们提供了784个特征。假设*n*
    = 70,000，我们的“![Image](../images/prootn.jpg)”经验法则（[公式3.2](ch03.xhtml#ch03equ02)）建议最大特征数应为大约260个，这明显低于784个特征。虽然这个经验法则比较保守——卷积神经网络（CNN）在特征数量*p*远大于*n*的情况下也能表现不错——但显然将784个像素当作独立特征来使用，会妨碍我们有效预测新数据。'
- en: We need to exploit the *locality* of our images. An image pixel tends to be
    correlated with its neighboring pixels, and the nature of this relationship should
    help us classify the image. Is that pixel part of a short straight line, say,
    or maybe a small circle? The *convolutional model* is designed with this in mind,
    consisting of various operations that are applied to patches within an image.
    These patches are often called *tiles*, which we will work with here.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要利用图像的*局部性*。一个图像像素往往与其相邻的像素存在相关性，这种关系的性质应该有助于我们对图像进行分类。这个像素是短直线的一部分吗？或者是一个小圆圈？*卷积模型*就是考虑到这一点设计的，包含了对图像内小块区域应用的各种操作。这些小块区域通常被称为*瓦片*，我们在这里也将使用这个术语。
- en: '***12.2.2 Overview of Convolutional Methods***'
  id: totrans-29
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***12.2.2 卷积方法概述***'
- en: 'Let’s first sneak a look at the code and run it on the Fashion MNIST data.
    After that, we’ll explain the operations in the code. We’ll use code adapted from
    an RStudio example:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们来看一下代码，并在Fashion MNIST数据集上运行它。之后，我们将解释代码中的操作。我们将使用从RStudio示例中改编的代码：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Ah, now we are up to more than 92 percent correct classification. And we almost
    certainly could do even better by tuning the hyperparameters (including changing
    the number and structure of the image operation layers).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 啊，现在我们的正确分类率已经超过了92%。我们几乎肯定可以通过调整超参数（包括更改图像操作层的数量和结构）进一步提高准确率。
- en: What do we see here?
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里看到了什么？
- en: Using `qeNeural()`’s `conv` argument, we have set up five image-operation layers.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`qeNeural()`的`conv`参数，我们设置了五个图像操作层。
- en: The image-operation layers are followed by “ordinary” layers (not specified
    here), thus taking `qeNeural()`’s default value of two layers of 100 neurons each.
    The “ordinary” layers are termed *dense* layers or *fully connected* layers.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像操作层后面是“普通”层（此处未指定），即`qeNeural()`的默认值为两层，每层100个神经元。 “普通”层被称为*密集*层或*全连接*层。
- en: The first image-operation layer performs a convolutional operation on the input
    image, which involves extracting tiles and forming linear combinations of the
    image intensities within each tile. These linear combinations become outputs of
    the layer.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个图像操作层对输入图像执行卷积操作，涉及提取瓦片并形成每个瓦片内图像强度的线性组合。这些线性组合成为该层的输出。
- en: Recall from earlier chapters that in the linear combination, say, 3*a* − 1.5*b*
    + 16.2*c*, the numbers 3, −1.5, and 16.2 are called *coefficients*. In the CNN
    context, they are called *weights*.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回顾之前的章节，在线性组合中，例如3*a* − 1.5*b* + 16.2*c*，数字3、−1.5和16.2被称为*系数*。在卷积神经网络（CNN）中，它们被称为*权重*。
- en: Usually we will use many different sets of weights. The `conv2d` parameter `filters`
    specifies the number of sets of weights that we want for a layer. It acts analogously
    with the number of neurons in a dense layer.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常我们会使用多组不同的权重。`conv2d`参数`filters`指定了我们希望为某一层设置的权重组数。它与密集层中的神经元数量类似。
- en: The `conv2d` parameter `kern` value specifies the tile size, with the value
    3 in the first layer meaning 3 × 3 tiles.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conv2d`参数`kern`值指定瓦片的大小，第一层中的值3意味着3 × 3的瓦片。'
- en: Another `conv2d` parameter `stride` controls the number of tiles in an image
    by specifying the amount of overlap a tile has with its neighbors, which will
    be explained below.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个`conv2d`参数`stride`通过指定瓦片与邻域的重叠量来控制图像中瓦片的数量，具体内容将在下文解释。
- en: The argument `xShape` specifies the size of an image, such as 28 × 28 in the
    current example.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数`xShape`指定图像的大小，例如当前示例中的28 × 28。
- en: For color data of that size, we would denote the situation as 28 × 28 × 3, with
    the 3 referring to the number of primary colors, red, yellow, and blue. We would
    have a 28 × 28 array of red intensities, then another for yellow, and finally
    one for blue. Then we would set *xShape* to (28,28,3).
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于此大小的彩色数据，我们会将其表示为28 × 28 × 3，其中3表示基本颜色的数量：红色、黄色和蓝色。我们会有一个28 × 28的红色强度数组，然后是黄色和蓝色的数组。然后我们将`xShape`设置为(28,28,3)。
- en: That third coordinate, 3, is called a *channel*. We don’t call it “color,” because
    it may not be a color, as will be seen below.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三个坐标3被称为*通道*。我们不称其为“颜色”，因为它可能并不是颜色，如下所示。
- en: The output of one layer is input to the next, and its dimensions may be, say,
    13 × 13 × 64\. That would be treated as a 13 × 13 “image” with 64 “primary colors,”
    both of which are artificial. The point is that, mathematically, any three-dimensional
    array can be treated this way, and it makes the code simpler to do so. (An array
    of three or more dimensions is called a *tensor*, hence the name `tensorflow`
    for the Python package underlying what we do here.)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一层的输出作为下一层的输入，输出的维度可能是13 × 13 × 64。例如，这将被视为一个13 × 13的“图像”，具有64种“基本颜色”，这两者都是人为的。关键是，数学上，任何三维数组都可以这样处理，并且这样做使得代码更简洁。（三维或更高维的数组被称为*张量*，因此Python包`tensorflow`的名称来源于此。）
- en: '***12.2.3 Image Tiling***'
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***12.2.3 图像切割***'
- en: The first and third layers in the above code example perform a *convolution*
    operation. (Readers with a background in probability theory, Fourier analysis,
    and so on will find that the term is used somewhat differently in ML.) To explain,
    we first need to discuss breaking an image into tiles.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码示例中的第一层和第三层执行了*卷积*操作。（具有概率论、傅里叶分析等背景的读者会发现，该术语在机器学习中的用法与其他领域有所不同。）为了说明这一点，我们首先需要讨论如何将图像分割成瓦片。
- en: 'Consider this toy example of a 6 × 6 grayscale image:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个6 × 6的灰度图像示例：
- en: '![Image](../images/ch12equ01.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch12equ01.jpg)'
- en: For example, the intensity of the pixel in row 2, column 4 of the image is 11\.
    In R, we would store this in a matrix of 6 rows and 6 columns. (An R matrix is
    like a data frame in which the elements are all numeric or all character strings
    and so on.)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，图像中第二行第四列的像素强度为11。在R中，我们会将其存储在一个6行6列的矩阵中。（R矩阵类似于数据框，其中元素可以是全数字或全字符等。）
- en: 'We could break this into non-overlapping tiles of size 3 × 3:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将其分割为不重叠的3 × 3大小的瓦片：
- en: '![Image](../images/ch12equ02.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
- en: So, our original matrix has been partitioned into four submatrices or tiles.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also have overlapping tiles, using a number called the *stride*. In
    the above example, the stride is 3: The first column of the upper-right tile'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch12equ03.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
- en: is 3 columns to the right of the first column of the upper-left tile located
    in [Equation 12.2](ch12.xhtml#ch12equ02)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch12equ04.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
- en: and so on. Similar statements hold for rows. For example, the first row of the
    lower-right tile is 3 rows below the first row of the upper-right tile.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: With a stride of 1, say, our first 3 × 3 tile would still be
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch12equ05.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
- en: But the second would be just 1 column to the right of the first tile
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch12equ06.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
- en: and so on.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: The default value of `stride` in `'conv2d'` operations is 1.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '***12.2.4 The Convolution Operation***'
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Say we are using 3 × 3 tiles. It is convenient to express the coefficients
    of a linear combination in matrix form too, say, in the weights matrix:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch12equ07.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
- en: 'For a given tile:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: The tile’s element in row 1, column 1 will be multiplied by *w*[11].
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The tile’s element in row 1, column 2 will be multiplied by *w*[12].
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: . . .
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The tile’s element in row 3, column 3 will be multiplied by *w*[33].
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of those products will be summed to produce a single number.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: 'This set of weights is then applied to each tile. Applying to the upperleft
    tile in [Equation 12.2](ch12.xhtml#ch12equ02), we have the single number:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch12equ08.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
- en: 'We apply the same set of weights to each tile. For instance, applying the weights
    to the upper-right tile, we have:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch12equ09.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
- en: We do the same for the lower-left and lower-right tiles, yielding four numbers
    altogether, which we arrange in a 2 × 2 matrix. That is output to the next layer.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Now suppose we have, say, 12 filters. That means 12 different sets of weights—that
    is, 12 different versions of the matrix in [Equation 12.7](ch12.xhtml#ch12equ07).
    That means 12 different 2 × 2 matrices coming out of this layer. Thus the output
    of this layer is described as 2 × 2 × 12\. To be sure, yes, the total output of
    this layer will be 48 numbers, but we think of them as consisting of 12 sets of
    2 × 2 matrices, hence the 2 × 2 × 12 notation.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Note we are not choosing these weights ourselves. They are chosen by the NN
    algorithm, which will minimize the overall prediction sum of squares. We choose
    the *number* of sets, 12 here, but not the sets themselves. The algorithm will
    try many different collections of 12 sets of weights, in hope of finding a collection
    that minimizes the prediction sum of squares.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: So . . . there is really nothing new. We are taking linear combinations of the
    inputs and feeding them to the next layer, just as in the last chapter. In the
    end, the algorithm minimizes the sum of squared prediction errors, just as before.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: The difference, though, is the structuring of the data into tiles, exploiting
    locality. The role of the weights is to determine the relative importance of various
    pixels, especially in how they work together.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: It will be helpful to visualize this as a “building” with 12 “floors.” Each
    “floor” consists of four “rooms,” arranged in two rows of two rooms each. We will
    take this approach below.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we still have the usual Bias-Variance Trade-off as with dense layers:
    the more filters, the more opportunities to reduce bias, but the more variance
    we incur in the weights.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '***12.2.5 The Pooling Operation***'
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Recall the second layer in the example in [Section 12.2](ch12.xhtml#ch12lev2):'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This is not a convolutional layer; it’s a *pooling* layer.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Pooling involves replacing the elements in a tile by some representative value,
    say, the mean or the median, or even the maximum value in the tile. The latter
    is quite common, in fact, and is the one used in the `regtools` and `qeML` packages.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 'The reader may wonder, “Isn’t pooling a special case of convolutional operations?
    For example, isn’t taking the mean in a 2 × 2 tile the same as a convolutional
    operation with all the weights being 0.25?” The answer is yes, but with one big
    difference: here the weights are fixed at 0.25; they are not chosen by the algorithm.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the `conv2d` operation, where the default stride is 1, for pooling, the
    default stride is the tile size, specified above as 2.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '***12.2.6 Shape Evolution Across Layers***'
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now, what will be the structure of the output from this second layer? Let’s
    reason this out. Here again are the specifications of the first two layers:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The input to the first layer was 28 × 28, or 28 × 28 × 1\. The first breaks
    things into 3 × 3 tiles with a stride of 1\. Just as there is a 2 × 2 array of
    tiles in [Equation 12.2](ch12.xhtml#ch12equ02), here we will have a 26 × 26 array
    of tiles, again taking into account that the stride is 1.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: So in that first layer, each filter will output a total of 26² numbers, in 26
    × 26 form. With 32 filters, the total output of that first layer will be 26 ×
    26 × 32\. In the “building” metaphor, this means 32 floors, with each floor having
    26 rows of rooms with 26 rooms per row. Note again here that each “room” holds
    one number.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Now, what then will happen at the second layer? It will receive 32 tiles sized
    26 × 26\. What will it do with them?
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: The tile size used by this layer, as discussed above, is 2 × 2, with a stride
    of 2\. Applying this to an inputted 26 × 26 tile, 13 rows of 13 2 × 2 tiles in
    each row will be formed. In each 2 × 2 tile, the maximum value among the 4 numbers
    will be extracted.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Again using the “building” metaphor, each “floor” will produce 13² = 169 numbers,
    arranged in 13 × 13 form. Since we have 32 “floors,” the total output of this
    layer will be in the form 13 × 13 × 32\. (The `regtools` and `qeML` packages use
    the two-dimensional form of the pooling operation, so the pooling is done within
    floors and not across floors.)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 再次使用“建筑”比喻，每一“层”将产生13² = 169个数字，排列成13 × 13的形式。由于我们有32层，“楼层”的总输出将呈现13 × 13 ×
    32的形式。（`regtools`和`qeML`包使用二维池化操作，因此池化操作在每一层内进行，而不是跨层进行。）
- en: '***12.2.7 Dropout***'
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***12.2.7 Dropout***'
- en: 'As with the dense layers, the danger of overfitting—too many neurons per convolutional
    layer or too many convolutional layers—is high. The antidote is dropout, for example:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 与全连接层一样，过拟合的风险——每个卷积层的神经元太多或卷积层太多——也很高。解决方法是dropout（丢弃法），例如：
- en: '[PRE5]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This specifies randomly deleting 50 percent of the nodes in this layer.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着随机删除这一层中50%的节点。
- en: '***12.2.8 Summary of Shape Evolution***'
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***12.2.8 形状演化总结***'
- en: 'The `keras` package gives us a summary of our CNN on request:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`keras`包可以根据请求给出我们的CNN的总结：'
- en: '[PRE6]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Recall that `qeNeural()` calls `regtools::krsFit()`, which in turn makes calls
    to the R `keras` package, so this output actually comes from the latter.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，`qeNeural()`调用了`regtools::krsFit()`，后者又调用了R语言中的`keras`包，因此该输出实际上来自后者。
- en: 'That last column shows the number of weights at each layer. For instance, here
    is where that 320 figure came from: each filter—that is, each set of numbers *w**[ij]*—is
    a 3 × 3 matrix, thus consisting of 9 numbers. There is also an intercept term
    *w*[0] (like *β*[0] in a linear regression model), for a total of 10 weights.
    Since there were 32 filters, we have 320 weights, as shown in the output table
    above.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一列显示了每一层的权重数量。例如，这里就是320个数字的来源：每个滤波器——即每组数字*w**[ij]*——是一个3 × 3的矩阵，因此包含9个数字。还有一个截距项*w*[0]（类似于线性回归模型中的*β*[0]），所以总共有10个权重。由于有32个滤波器，因此总共有320个权重，正如上面的输出表所示。
- en: The `flatten` layer merely converts from our *a* × *c* form to ordinary data.
    The output of our second pooling layer had form 5 × 5 × 64, which amounts to 1,600
    numbers. In order to be used by a dense layer, the data is converted to a single
    vector of length 1,600.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`flatten`层仅仅是将我们的*a* × *c*形式转换为普通数据。我们第二个池化层的输出形式是5 × 5 × 64，总共是1,600个数字。为了能被全连接层使用，这些数据会转换成一个长度为1,600的单一向量。'
- en: Altogether, we have *p* = 179926 but only *n* = 65000\. So we are definitely
    overfitting. The fact that many such models have been found to work well is quite
    a controversy in the ML community!
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，我们有*p* = 179926，但只有*n* = 65000。所以我们肯定出现了过拟合。事实上，许多类似的模型被发现能有效工作，这在机器学习领域引起了不小的争议！
- en: '***12.2.9 Translation Invariance***'
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***12.2.9 平移不变性***'
- en: The weight structure lends *translation invariance*—a fancy term that actually
    has a simple meaning—to our analysis. Say we are using 3 × 3 for our tile size.
    That’s 9 pixels. For any tile, consider the pixel in the upper-left corner of
    the tile. Then we have the same weight *w*[11] for that pixel, regardless of whether
    the tile is near the top of the picture, say, or the bottom.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 权重结构赋予了*平移不变性*——这是一个看似复杂但实际意义简单的术语——我们分析的工具。假设我们使用3 × 3作为我们的瓦片大小，那就是9个像素。对于任何瓦片，考虑瓦片左上角的像素。那么无论瓦片位于图像的顶部、底部，还是其他位置，我们都会为该像素使用相同的权重*w*[11]。
- en: For facial recognition, for instance, this means that, to a large extent, we
    don’t have to worry whether the face is near the top of the picture, near the
    bottom, or near the middle. (Problems do occur near the edges of the picture,
    so the property holds only approximately.) The same statement would hold for left-right
    positioning.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 以人脸识别为例，这意味着在很大程度上，我们不必担心人脸是在图像的顶部、底部还是中间。（不过，图像边缘的确会出现一些问题，所以这一特性仅近似成立。）对于左右位置也是同样的道理。
- en: 12.3 Tricks of the Trade
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.3 行业技巧
- en: Well, then, how in the world is one supposed to come up with the models? How
    many layers? What kinds of layers? What parameter values?
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，究竟该如何构建这些模型呢？多少层？什么样的层？哪些参数值？
- en: 'One might set some of the model on a hunch informed by the nature of the dataset,
    such as the size of various parts of the image, the image texture, and so on.
    But at the end of the day, the answer tends to be rather prosaic: after years
    of experimenting with various architectures (configurations), this one seems to
    work with certain kinds of images. Some architectures have been successful in
    wide-enough application that they have acquired names and become standards, such
    as AlexNet.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '***12.3.1 Data Augmentation***'
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'One approach to dealing with smaller image sets is *data augmentation*. The
    idea here is simple: form new images from existing ones. One might shift a given
    image horizontally or vertically, shrink or enlarge the image, flip it horizontally
    or vertically, and so on. The motivation for this is that, later, we might be
    asked to classify a new image that is very similar to one in our training set
    but is, say, much higher or lower within the image frame. We want our algorithm
    to recognize the new image as being similar to the one in the training set.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: This is especially important for medical tissue images, say, from a biopsy,
    as there is no sense of orientation—no top or bottom, left or right, or back or
    front. This is in contrast to MNIST, for instance, where a ‘6’ is an upsidedown
    ‘9’ and the two are quite different.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'We can perform data augmentation using the `OpenImageR` package, with its `Augmentation()`
    function. In the latter, for instance, we can do a vertical flip operation:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The `keras` package also offers data augmentation services, including a *shear*
    (twist) operation.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '***12.3.2 Pretrained Networks***'
  id: totrans-122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A big issue in the image classification community is *transfer learning*. Here
    the issue is that, instead of starting from scratch in designing a neural network—dense
    layers, convolutional layers, and details of each—one builds on some network that
    others have found useful. One then either uses that network as is or takes it
    as a starting point and does some tweaking.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: 12.4 So, What About the Overfitting Issue?
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As noted in [Section 12.2.8](ch12.xhtml#ch12lev2sec8), the success of heavily
    overparameterized networks in image classification seems to contradict the conventional
    wisdom regarding overfitting. This has been the subject of much speculation in
    the ML community.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: A key point may be that misclassification rates in image contexts tend to be
    very low, near 0 for highly tuned networks. In that sense, we are essentially
    in the settings that were termed *separable* in [Chapter 10](ch10.xhtml). Some
    insight into this issue may then be gained by revisiting [Figure 10-4](ch10.xhtml#ch10fig04)
    in that chapter.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: As was pointed out, there are many lines, infinitely many, in fact, that could
    be used to distinguish the two classes, and thus be used to predict a new case.
    SVM chooses a particular line for this—the one halfway between the two closest
    points in the two classes—but again, one might use many other lines instead.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, the separator need not be a straight line. It could be a “curvy” line,
    say, one obtained by using a polynomial kernel with SVM. Because of the clean
    separation of the two classes, there is plenty of wiggle room in which we could
    fit a very wiggly curve, say, a polynomial, of very high degree. And the higher
    the degree, the more coefficients in the equation of the curve—that is, the larger
    the value of *p*.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，分隔符不一定非得是直线。它可以是一个“弯曲”的线，例如，通过使用多项式核与支持向量机（SVM）获得的线。由于两类之间有很好的分隔，我们有足够的自由度可以拟合一条非常弯曲的曲线，比如高次多项式。而且次数越高，曲线方程中的系数就越多，也就是说，*p*
    的值越大。
- en: 'The result: we could fit a curve that has a value of *p* much greater than
    *n* yet still get perfect prediction accuracy. Noting the connection of NNs to
    polynomial regression (see [Section 11.9](ch11.xhtml#ch11lev9)), we have a plausible
    explanation for the success of overparameterization in image classification.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：我们可以拟合一条曲线，使得 *p* 的值远大于 *n*，但仍然能得到完美的预测准确性。注意到神经网络与多项式回归的关联（参见[第11.9节](ch11.xhtml#ch11lev9)），我们对过度参数化在图像分类中成功的原因有了一个合理的解释。
- en: 12.5 Conclusions
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.5 结论
- en: 'In spite of this book’s aim to avoid writing many equations, the topic here
    is easily the most mathematical of all the chapters. Viewed from a high level,
    CNNs work from a very simple idea: break an image into tiles and then apply an
    NN to the tiled data. But the old saying “The devil is in the details” is quite
    apt here. It can be challenging, for instance, to keep clear in one’s mind the
    dimensionality of chunks of data as we move from layer to layer. Readers who wish
    to pursue further study beyond the introduction here will find a background in
    linear algebra and calculus to be quite useful.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本书的目的是避免写太多方程式，但这里的内容无疑是所有章节中最具数学性的。从一个高层次来看，卷积神经网络（CNN）基于一个非常简单的想法：将图像分割成小块，然后对分块的数据应用神经网络（NN）。但是那句老话“魔鬼藏在细节中”在这里非常贴切。例如，在从一层到另一层的过程中，保持数据块维度的清晰可能是一个挑战。希望进一步深入学习的读者，线性代数和微积分的背景将非常有用。
