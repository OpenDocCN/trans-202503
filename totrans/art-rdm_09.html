<html><head></head><body>
<h2 class="h2" id="ch09"><span epub:type="pagebreak" id="page_255"/><strong><span class="big">9</span><br/>AUDIO SIGNALS</strong></h2>
<div class="image1"><img alt="Image" src="../images/common.jpg"/></div>
<p class="noindent">In this chapter, we continue exploring the relationship between continuous and discrete signals. The <em>Nyquist–Shannon sampling theorem</em> relates continuous signals and discrete signals. To properly discretize a continuous signal, we must sample it at a rate at least twice that of the highest frequency present in the signal. The theorem is why compact discs sample audio signals at 44.1 kHz, or 44,100 times per second. At that rate, any frequency up to 22,050 Hz will be captured. Note that 22 kHz is the theoretical upper limit on the highest frequency a human can hear, though most adults have a much lower upper limit; mine is about 13.5 kHz.</p>
<p class="indent">This chapter explores <em>compressed sensing</em> (or compressive sensing), a technique for beating Nyquist and Shannon at their own game. With compressed sensing, it becomes possible to acquire less data when digitizing a signal than the Nyquist-Shannon theorem says is required. This is an exciting real-world inverse problem involving randomness.</p>
<p class="indent"><span epub:type="pagebreak" id="page_256"/>We’ll begin by walking through the main points of compressed sensing; we’ll cover some of the math, but I encourage you to explore the rest on your own. Then we’ll explore compressed sensing in one dimension, audio, to see how it lets us break the Nyquist limit. Finally, as unraveled images are, to compressed sensing, no different from signals in time, we’ll apply compressed sensing to reconstruct images from what seems like too little data.</p>
<p class="indent">There’s some matrix-vector math in the first section, but it doesn’t go much beyond what we encountered in <a href="ch07.xhtml">Chapter 7</a> with iterated function systems.</p>
<h3 class="h3" id="ch00lev1_55"><strong>Compressed Sensing</strong></h3>
<p class="noindent">Digitizing a signal usually means reading the output of an analog-to-digital converter at a specified but constant time interval. The number of readings per second is the sampling rate, which the Nyquist-Shannon theorem is concerned with. If we acquire the samples according to the Nyquist-Shannon theorem, we can accurately reconstruct the signal from the samples.</p>
<p class="indent">When we sample at a fixed time interval, we are <em>uniform sampling</em>. However, there are times when uniform sampling isn’t desired, possibly because it’s too expensive or there’s too much risk associated with it (for example, in X-ray tomography). In such situations, it would be nice to acquire less data but still reconstruct the entire signal. For example, if the signal we want is denoted as <em><strong>x</strong></em>, we’ll measure some subset of the signal, <em><strong>y</strong></em>, and from <em><strong>y</strong></em> reconstruct <em><strong>x</strong></em>. Mathematically, we can cast this process as a matrix equation</p>
<div class="image1"><img alt="Image" id="ch09equ1" src="../images/f0256-01.jpg"/></div>
<p class="noindent">where we know vector <em><strong>y</strong></em> because we measured it and matrix <em><strong>C</strong></em> because it dictates the parts of <em><strong>x</strong></em> we sampled. We want <em><strong>x</strong></em>, the vector we would’ve measured following standard sampling theory. Keep <a href="ch09.xhtml#ch09equ1">Equation 9.1</a> in the back of your mind for the time being.</p>
<p class="indent">Let’s return to algebra class, which asks us to solve systems of equations, usually two equations and two unknowns:</p>
<div class="image1"><img alt="Image" src="../images/f0256-02.jpg"/></div>
<p class="noindent">Here, <em>a</em> through <em>f</em> are constants. Because there are two equations and two unknowns, we can find <em>x</em> and <em>y</em> values that satisfy both, assuming one equation isn’t a multiple of the other. In matrix form, we write the system of equations as:</p>
<div class="image1"><img alt="Image" src="../images/f0256-03.jpg"/></div>
<p class="noindent">The rules of matrix-vector multiplication tell us to multiply each row of the matrix, <em><strong>A</strong></em>, by the corresponding elements of the vector, <em><strong>x</strong></em>, then sum. This transforms the matrix equation into the system. Regardless of the number of elements in the vectors, this rule applies.</p>
<p class="indent"><span epub:type="pagebreak" id="page_257"/>The system of equations works because there are as many unknowns as there are equations, meaning the number of elements in the vectors, here <em><strong>b</strong></em> and <em><strong>x</strong></em>, matches the number of rows in the matrix, <em><strong>A</strong></em>. For such an equation, the solution (if there is one), or the <em><strong>x</strong></em> vector that makes the equation true, is <em><strong>A</strong></em><sup>–1</sup><em><strong>b</strong></em> = <em><strong>x</strong></em> for <em><strong>A</strong></em><sup>–1</sup>, the inverse matrix of <em><strong>A</strong></em>. For example, this system</p>
<div class="image1"><img alt="Image" src="../images/f0257-01.jpg"/></div>
<p class="noindent">becomes</p>
<div class="image1"><img alt="Image" src="../images/f0257-02.jpg"/></div>
<p class="noindent">with solution</p>
<div class="image1"><img alt="Image" src="../images/f0257-03.jpg"/></div>
<p class="noindent">where the inverse of a matrix <em><strong>A</strong></em> is <em><strong>A</strong></em><sup>–1</sup> such that <em><strong>AA</strong></em><sup>–1</sup> = <em><strong>A</strong></em><sup>–1</sup><em><strong>A</strong></em> = <em><strong>I</strong></em>.</p>
<p class="indent">Here, <em><strong>I</strong></em> is the <em>identity matrix</em>—the matrix of all zeros with ones along the diagonal. In the world of matrices, <em><strong>I</strong></em> is akin to the number 1. Use NumPy’s <span class="literal">linalg.inv</span> function to find <em><strong>A</strong></em><sup>–1</sup>.</p>
<p class="indent"><em><strong>A</strong></em> is a <em>square matrix</em>, meaning it has as many rows as columns. If <em><strong>A</strong></em> is square, and the number of rows matches the number of elements in <em><strong>b</strong></em> and <em><strong>x</strong></em>, then we can use <em><strong>A</strong></em><sup>–1</sup> to find <em><strong>x</strong></em>.</p>
<p class="indent">Now comes the fun part. Return to <a href="ch09.xhtml#ch09equ1">Equation 9.1</a>. By design, there are <em>fewer</em> elements in <em><strong>y</strong></em>, the values we measure, than in <em><strong>x</strong></em>, the full signal. If there are <em>N</em> elements in <em><strong>y</strong></em> and <em>M</em> elements in <em><strong>x</strong></em>, then <em><strong>C</strong></em> is an <em>N</em>×<em>M</em> matrix with <em>N</em> rows and <em>M</em> columns. There are more unknowns in <a href="ch09.xhtml#ch09equ1">Equation 9.1</a> than there are equations. Such a system is called <em>underdetermined</em>. Underdetermined systems have an infinite number of solutions; there are an infinite number of vectors, <em><strong>x</strong></em>, that, when multiplied by <em><strong>C</strong></em>, give <em><strong>y</strong></em>.</p>
<p class="indent">We want to get <em><strong>x</strong></em> by measuring <em><strong>y</strong></em>, but <em><strong>y</strong></em> alone doesn’t have enough information to tell us <em>which</em> of the infinite set of <em><strong>x</strong></em> vectors we want. Compressed sensing comes to the rescue—at least in some cases.</p>
<p class="indent">According to compressed sensing theory, if <em><strong>x</strong></em> is <em>sparse</em>, meaning most of its elements are essentially zero, then we can recover <em><strong>x</strong></em> from <em><strong>y</strong></em> by solving the <em>inverse problem</em>, which searches for the <em><strong>x</strong></em> that minimizes some measure of the difference <em><strong>Ax</strong></em> – <em><strong>b</strong></em> while strongly encouraging <em><strong>x</strong></em> to be sparse. As we’ll see, algorithms capable of this kind of optimization exist.</p>
<p class="indent">Great! We’re in business. We measure <em><strong>y</strong></em> containing some subset of the elements that would be in <em><strong>x</strong></em>, and we get <em><strong>x</strong></em> from <em><strong>y</strong></em> by solving a minimization problem. But it isn’t that simple; the optimization trick only works if <em><strong>x</strong></em> is sparse. Most signals are not sparse; an audio signal isn’t likely to be. Recall working with the waveforms in <a href="ch08.xhtml">Chapter 8</a>. Are we doomed? Not necessarily.</p>
<p class="indent">While an audio signal isn’t sparse, there are Fourier-like transformations that map from a signal changing in time to one changing in frequency, and it is often the case that the frequency domain signal <em>is</em> sparse. Therefore, if we <span epub:type="pagebreak" id="page_258"/>can write <em><strong>x</strong></em> = <strong>Ψ</strong><em><strong>s</strong></em> for some transformation matrix <strong>Ψ</strong> (psi) and sparse vector <em><strong>s</strong></em>, <a href="ch09.xhtml#ch09equ1">Equation 9.1</a> becomes</p>
<div class="image1"><img alt="Image" id="ch09equ2" src="../images/f0258-03.jpg"/></div>
<p class="noindent">where <strong>Θ</strong> = <em><strong>C</strong></em><strong>Ψ</strong>.</p>
<p class="indent">While <em><strong>x</strong></em> isn’t sparse and is therefore unrecoverable, <em><strong>s</strong></em> is, meaning the optimization trick might have a chance of working. The measurements in <em><strong>y</strong></em> combined with the external knowledge that <em><strong>s</strong></em> is sparse will let us find <em><strong>s</strong></em>. Once we have <em><strong>s</strong></em>, we get <em><strong>x</strong></em>.</p>
<p class="indent">But, what are all these matrices floating around? The <em>measurement matrix</em>, <em><strong>C</strong></em>, can mathematically be any matrix of values such that the values in <em><strong>C</strong></em> satisfy some notion of <em>incoherence</em> in relation to the elements of <strong>Ψ</strong>. For us, the elements of <em><strong>C</strong></em> are binary, zero or one, and serve to select specific elements of <em><strong>x</strong></em> that are actually measured. This requirement supplies the necessary mathematical incoherence between <em><strong>C</strong></em> and <strong>Ψ</strong>. The most important point is that <em><strong>C</strong></em> is somehow <em>random</em>. In practice, we don’t explicitly define <em><strong>C</strong></em>, but our measurement process uses it implicitly. The random bit is essential to the entire operation, however.</p>
<p class="indent">The <strong>Ψ</strong> matrix is a transformation matrix that transforms the sparse vector, <em><strong>s</strong></em>, into a new representation, <em><strong>x</strong></em>, which we ultimately want to measure. For us, <strong>Ψ</strong> is a Fourier-like <em>discrete cosine transformation (DCT)</em>. Signals are often sparse in this domain, thereby making it very useful for compressed sensing.</p>
<p class="indent">Finally, <strong>Θ</strong> represents the combination of the measurement process working on <strong>Ψ</strong>.</p>
<p class="indent">We are now in a position to try solving <em><strong>y</strong></em> = <strong>Θ</strong><em><strong>s</strong></em> for some <em><strong>s</strong></em> that both is sparse and leads to <em><strong>y</strong></em>, the set of measurements we have. There are multiple algorithms available, but we’ll use the Lasso algorithm, courtesy of scikit-learn. Likewise, we need the DCT and its inverse, which SciPy dutifully supplies:</p>
<pre class="pre">from sklearn.linear_model import Lasso
from scipy.fftpack import dct, idct</pre>
<p class="indent">Lasso minimizes the following</p>
<div class="image1"><img alt="Image" id="ch09equ3" src="../images/f0258-01.jpg"/></div>
<p class="noindent">where <em>n</em> is the number of samples, or the number of elements in <em><strong>y</strong></em>.</p>
<p class="indent">The double vertical bar notation refers to a <em>norm</em>, which is a metric measuring distance of some kind. The first term uses the square of the <em>ℓ</em><sup>2</sup> norm, while the second term multiplies the <em>ℓ</em><sup>1</sup> norm by <em>α</em>. The <em>ℓ<sup>p</sup></em> norm of a vector, <em><strong>x</strong></em>, is defined as:</p>
<div class="image1"><img alt="Image" src="../images/f0258-02.jpg"/></div>
<p class="noindent">The <em>ℓ</em><sup>2</sup> norm is the Euclidean distance. The <em>ℓ</em><sup>1</sup> norm, sometimes called the Manhattan or taxicab distance, is the sum of the absolute values of the elements of <em><strong>x</strong></em>. Lasso uses this term, scaled by <em>α</em>, to find an <em><strong>s</strong></em> vector that minimizes the Euclidean distance between the measurements, <em><strong>y</strong></em>, and <strong>Θ</strong><em><strong>s</strong></em> while <span epub:type="pagebreak" id="page_259"/>simultaneously minimizing the sum of the absolute values of the elements of <em><strong>s</strong></em>. This latter constraint forces many elements of <em><strong>s</strong></em> toward zero, thereby ensuring sparsity.</p>
<p class="indent">To understand why the <em>ℓ</em><sup>1</sup> norm term is present in the Lasso objective function, consider a simple case where we have a two-element vector and a single-element output. This is akin to finding a solution to <em>x</em> + <em>y</em> = <em>c</em> that is as sparse as possible, where <em>x</em> = 0 or <em>y</em> = 0. Geometrically, minimizing the <em>ℓ</em><sup>1</sup> norm leads to a situation as on the left of <a href="ch09.xhtml#ch09fig01">Figure 9-1</a>, while minimizing the <em>ℓ</em><sup>2</sup> norm is shown on the right. Minimizing the <em>ℓ</em><sup>2</sup> norm is standard least-squares regression.</p>
<div class="image"><img alt="Image" id="ch09fig01" src="../images/09fig01.jpg"/></div>
<p class="figcap"><em>Figure 9-1: Minimizing the ℓ<sup>1</sup> norm (left) and the ℓ<sup>2</sup> norm (right)</em></p>
<p class="indent">The line in <a href="ch09.xhtml#ch09fig01">Figure 9-1</a> represents the infinite set of solutions to <em>ax</em> + <em>by</em> = <em>c</em> for some <em>c</em>. The diamond on the left and the circle on the right correspond to constant <em>ℓ</em><sup>1</sup> and <em>ℓ</em><sup>2</sup> norms, respectively. Minimizing the <em>ℓ</em><sup>1</sup> norm intersects the line at a point where <em>y</em> is zero, while minimizing <em>ℓ</em><sup>2</sup> intersects the line at a point where neither <em>x</em> nor <em>y</em> is zero. This trend continues as the dimensionality increases. In each case, minimizing the <em>ℓ</em><sup>1</sup> norm implies sparsity in the solution while minimizing the <em>ℓ</em><sup>2</sup> norm distributes the “energy” throughout each dimension, the opposite of enforcing sparsity.</p>
<p class="indent">Let’s sum up. We want to acquire <em><strong>x</strong></em> by measuring a subset of it, <em><strong>y</strong></em>. To solve for random-ish measurement matrix <em><strong>A</strong></em>, we want <em><strong>y</strong></em> = <em><strong>Ax</strong></em>. This expression is underdetermined, meaning there are an infinite number of <em><strong>x</strong></em> that work as solutions, so we need extra information to find the one we (likely) want. We get this information from expressing <em><strong>x</strong></em> in some other form (basis) where it becomes sparse. If sparse, the probability of finding a meaningful and parsimonious solution presents itself. A commonly used basis for this comes from the Fourier family of transformations, like the DCT, <em><strong>x</strong></em> = <strong>Ψ</strong><em><strong>s</strong></em>, where <strong>Ψ</strong> encapsulates the DCT and <em><strong>s</strong></em> is a sparse vector we want to find. If we find <em><strong>s</strong></em>, we find <em><strong>x</strong></em>.</p>
<p class="indent">Combining the measurement matrix and the DCT gives us a new equation, <em><strong>y</strong></em> = <strong>Θ</strong><em><strong>s</strong></em>, where we know <em><strong>y</strong></em> and <strong>Θ</strong>. It’s still underdetermined, but we know <em><strong>s</strong></em> is sparse. To find <em><strong>s</strong></em>, we use an optimization algorithm that knows how to minimize the <em>ℓ</em><sup>1</sup> norm of <em><strong>s</strong></em> in the process. This enforces sparsity and gives us some confidence that we might find a suitable <em><strong>s</strong></em>.</p>
<p class="indent">Let’s give the recipe a go and see what happens.</p>
<h3 class="h3" id="ch00lev1_56"><span epub:type="pagebreak" id="page_260"/><strong>Signal Generation</strong></h3>
<p class="noindent">We’ll walk through <em>cs_signal.py</em>, which illustrates the compressed sensing process and why we need to use random measurements. First, let’s run it, then I’ll explain the various plots it generates. Here’s the command line:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 cs_signal.py 0.2 minstd 65536</span></pre>
<p class="noindent">Several plots should appear in succession; close each to move to the next. Output files are created as well.</p>
<p class="indent">The code first generates a one-second signal, the sum of three sine waves forming a C major chord. Standard Nyquist sampling gives this signal for a sample rate of 4,096 Hz, that is, <em><strong>x</strong></em>. This is a demonstration, so we start with <em><strong>x</strong></em> and then throw much of it away to create a <em><strong>y</strong></em> that we might have plausibly measured in the first place. The command line includes an argument of <span class="literal">0.2</span>, the fraction of <em><strong>x</strong></em> to retain, meaning that <em><strong>y</strong></em> has 20 percent of the samples; we throw the remaining 80 percent away. The remainder of the command line specifies the randomness source (<span class="literal">minstd</span>) and a seed value (<span class="literal">65536</span>).</p>
<p class="indent">The signal comes from:</p>
<pre class="pre">rate = 4096
dur = 1.0
f0,f1,f2 = 261.63, 329.63, 392.0
samples  = np.sin(2*np.pi*np.arange(rate*dur)*f0/rate)
samples += np.sin(2*np.pi*np.arange(rate*dur)*f1/rate)
samples += np.sin(2*np.pi*np.arange(rate*dur)*f2/rate)</pre>
<p class="noindent">We used similar code in <a href="ch08.xhtml">Chapter 8</a>. The three frequencies (<span class="literal">f0</span>, <span class="literal">f1</span>, <span class="literal">f2</span>) are the C major chord. The <span class="literal">samples</span> vector is the final signal, <em><strong>x</strong></em>. It’s a vector of 4,096 elements because the sampling rate is 4,096 Hz and the duration is one second.</p>
<p class="indent">Let’s build <em><strong>y</strong></em> from <em><strong>x</strong></em>. This process makes implicit use of the measurement matrix. We’ll keep 20 percent of the samples in <em><strong>x</strong></em>, first by selecting samples at a uniform interval, and then randomly. The uniform samples correspond to measuring the signal at some rate below the Nyquist limit:</p>
<pre class="pre">nsamp = int(frac*len(samples))
u = np.arange(0, len(samples), int(len(samples)/nsamp))
bu = samples[u]
r = np.argsort(rng.random(len(samples)))[:nsamp]
br = samples[r]</pre>
<p class="indent">There are <span class="literal">nsamp</span> samples in <em><strong>y</strong></em>. The first <em><strong>y</strong></em> vector is <span class="literal">bu</span>, uniformly sampled, and the second is <span class="literal">br</span>, randomly sampled. <a href="ch09.xhtml#ch09fig02">Figure 9-2</a> shows the original signal with the uniform and random samples marked (<em>cs_signal_samples.png</em>).</p>
<div class="image"><img alt="Image" id="ch09fig02" src="../images/09fig02.jpg"/></div>
<p class="figcap"><span epub:type="pagebreak" id="page_261"/><em>Figure 9-2: Random (top) and uniform samples (bottom)</em></p>
<p class="indent">We have the measurements. Now we need <strong>Θ</strong>, the combination of <strong>Ψ</strong> and the measurement matrix. Once we have that, we’re ready to use Lasso. We have two <em><strong>y</strong></em> vectors, so we need two <strong>Θ</strong> matrices:</p>
<pre class="pre">D = dct(np.eye(len(samples)))
U = D[u,:]
R = D[r,:]</pre>
<p class="noindent">The first, <span class="literal">U</span>, keeps only the uniformly selected measurements. The second, <span class="literal">R</span>, uses the randomly selected measurements. Here, <span class="literal">D</span> is the discrete Fourier transform matrix, <strong>Ψ</strong>, and <span class="literal">U</span> and <span class="literal">R</span> are <strong>Θ</strong><em><sub>u</sub></em> and <strong>Θ</strong><em><sub>r</sub></em>, respectively.</p>
<p class="indent">We optimize twice, first <em><strong>y</strong><sub>u</sub></em> = <strong>Θ</strong><em><sub>u</sub><strong>s</strong><sub>u</sub></em> and then <em><strong>y</strong><sub>r</sub></em> = <strong>Θ</strong><em><sub>r</sub><strong>s</strong><sub>r</sub></em>, where the subscripts refer now to the uniformly and randomly sampled measurements:</p>
<pre class="pre">lu = Lasso(alpha=0.01, max_iter=6000)
lu.fit(U, bu)
su = lu.coef_
lr = Lasso(alpha=0.01, max_iter=6000)
lr.fit(R, br)
sr = lr.coef_</pre>
<p class="indent"><span class="literal">Lasso</span> follows the scikit-learn convention of creating an instance of a class and then calling <span class="literal">fit</span> to do the optimization. For <span class="literal">Lasso</span>, the solution vector is buried in the <span class="literal">coef_</span> member variable, which we extract to get <span class="literal">su</span> and <span class="literal">sr</span>, the uniform and random <em><strong>s</strong></em> vectors, respectively. <a href="ch09.xhtml#ch09fig03">Figure 9-3</a> shows the two <em><strong>s</strong></em> vectors (<em>cs_signal_sparse.png</em>).</p>
<div class="image"><img alt="Image" id="ch09fig03" src="../images/09fig03.jpg"/></div>
<p class="figcap"><span epub:type="pagebreak" id="page_262"/><em>Figure 9-3: Random (top) and uniform (bottom) solution vectors,</em> s</p>
<p class="noindent">The top plot shows <em><strong>s</strong><sub>u</sub></em> and the bottom shows <em><strong>s</strong><sub>r</sub></em>. The spikes correspond to DCT components. Both <em><strong>s</strong></em> vectors are sparse, with most of the 4,096 elements near zero, but the bottom vector has more than 10 nonzero elements while the top has only 3 (the shape is sometimes both positive and negative). Recall, <em><strong>x</strong></em> is the sum of three sine waves, so the three sine waves and three spikes in the <em><strong>s</strong><sub>r</sub></em> vector seems promising.</p>
<p class="indent">Lasso has solved <em><strong>y</strong></em> = <strong>Θ</strong><em><strong>s</strong></em> for us. Now we need <em><strong>x</strong></em> = <strong>Ψ</strong><em><strong>s</strong></em>, which we find by calling the inverse DCT:</p>
<pre class="pre">ru = idct(su.reshape((len(samples),1)), axis=0)
rr = idct(sr.reshape((len(samples),1)), axis=0)</pre>
<p class="noindent"><a href="ch09.xhtml#ch09fig04">Figure 9-4</a> shows us <em><strong>x</strong><sub>u</sub></em> (<span class="literal">ru</span>) and <em><strong>x</strong><sub>r</sub></em> (<span class="literal">rr</span>); see <em>cs_signal_recon.png</em>.</p>
<div class="image"><img alt="Image" id="ch09fig04" src="../images/09fig04.jpg"/></div>
<p class="figcap"><em>Figure 9-4: From top: the original, reconstructed randomly sampled, and reconstructed uniformly sampled signals</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_263"/>The topmost plot shows the original signal. The middle plot shows the signal reconstructed from 20 percent of the original signal using randomly selected measurements. Finally, the bottom plot shows the signal reconstructed from the uniformly selected measurements, likewise 20 percent of the original number. Which do you think more faithfully captured the original signal?</p>
<p class="indent">The final step is to output the signal as a WAV file:</p>
<pre class="pre">WriteOutputWav(samples, "original.wav")
WriteOutputWav(rr, "recon_random.wav")
WriteOutputWav(ru, "recon_uniform.wav")</pre>
<p class="noindent">See <a href="ch08.xhtml#ch08list01">Listing 8-1</a> to review how <span class="literal">WriteOutputWav</span> works. Play the output files. I think you’ll agree that random sampling produced the better result.</p>
<p class="indent">Uniform sampling failed for deep mathematical reasons related to coherence between the measurement matrix and the DCT transform basis. However, we can intuitively understand the failure due to uniform sampling at less than the Nyquist rate, which means <em>aliasing</em> where higher frequency signals look like lower frequency signals, and there’s no way to disentangle the two. On the other hand, with random sampling, the likelihood of aliasing decreases, making Lasso more likely to find a suitable <em><strong>s</strong></em> vector.</p>
<p class="indent">Rerun <em>cs_signal.py</em>, but alter the fraction from 20 percent to smaller and higher values. Is there a place where everything falls apart? See if you can re-create the signal from only 10 percent, 5 percent, or even 1 percent of the original, and then try the opposite direction. Sampling even slightly above 50 percent appears to have a dramatic effect on the quality of the uniform sample. Why might that be? Consider the Nyquist-Shannon sampling theorem requirements.</p>
<h3 class="h3" id="ch00lev1_57"><strong>Unraveled Images</strong></h3>
<p class="noindent">The file <em>cs_image.py</em> applies compressed sensing to images. It’s similar to <em>cs_signal.py</em>, but it unravels the image before selecting the measured components (pixels). The image is <em><strong>x</strong></em>, with the selected mask pixels forming <em><strong>y</strong></em>. The code expects these command line arguments:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 cs_image.py</span>

cs_image &lt;image&gt; &lt;output&gt; &lt;fraction&gt; &lt;alpha&gt; [ &lt;kind&gt; | &lt;kind&gt; &lt;seed&gt; ]

  &lt;image&gt;    - source image (RGB or grayscale)
  &lt;output&gt;   - output directory (overwrittten)
  &lt;fraction&gt; - fraction of image to sample
  &lt;alpha&gt;    - L1 lambda coefficient
  &lt;kind&gt;     - randomness source
  &lt;seed&gt;     - seed value</pre>
<p class="noindent"><span epub:type="pagebreak" id="page_264"/>The input image may be grayscale or RGB. If RGB, each channel is processed individually using the same random mask. The output directory contains the original image, the reconstructed image, and a parameter file.</p>
<p class="indent">The code tries to import from scikit-image. It will run if scikit-image isn’t installed, but you can install it with:</p>
<pre class="pre">&gt; <span class="codestrong1">pip3 install scikit-image</span></pre>
<p class="noindent">If scikit-image is present, the code imports <span class="literal">structural_similarity</span>, which measures the mean structural similarity between two images—here the original image and the reconstructed image. Higher similarity is better, with 1.0 indicating an exact match.</p>
<p class="indent">The code loads the input image, converts it to RGB, and tests to see if it’s really grayscale:</p>
<pre class="pre">simg = np.array(Image.open(sname).convert("RGB"))
grayscale = False
if (np.array_equal(simg[:,:,0],simg[:,:,1])):
    grayscale = True</pre>
<p class="noindent">A grayscale image converted to RGB ends up with every channel the same, hence the call to <span class="literal">array_equal</span>. The test is not entirely foolproof, but it’s good enough for us.</p>
<p class="indent">The next step generates the random mask, the subset of actual image pixels that construct <em><strong>y</strong></em>:</p>
<pre class="pre">row, col, _ = simg.shape
mask = np.zeros(row*col, dtype="uint8")
M = int(fraction*row*col)
k = np.argsort(rng.random(row*col))[:M]
mask[k] = 1</pre>
<p class="noindent">The <span class="literal">mask</span> vector is 1 for selected pixels.</p>
<p class="indent">The remainder of the code calls <span class="literal">CS</span> for each image channel, if RGB, or the first channel, if grayscale, before dumping the original image, reconstructed image, and parameters to the output directory. All the action is in <span class="literal">CS</span>:</p>
<pre class="pre">def CS(simg, mask, fraction, alpha, rng):
    row, col = simg.shape
    f = simg.ravel()
    N = len(f)
    k = np.where(mask != 0)[0]
    y = f[k]
    D = dct(np.eye(N))
    A = D[k, :]
    seed = int(10000000*rng.random())
    lasso = Lasso(alpha=alpha, max_iter=6000, tol=1e-4, random_state=seed)
    lasso.fit(A, y.reshape((len(k),)))
    r = idct(lasso.coef_.reshape((N, 1)), axis=0)<span epub:type="pagebreak" id="page_265"/>
    r = (r - r.min()) / (r.max() - r.min())
    oimg = (255*r).astype("uint8").reshape((row,col))
    return oimg</pre>
<p class="noindent">The <span class="literal">CS</span> function is a compact version of the essential code in <em>cs_signal.py</em>. It forms the unraveled image (<span class="literal">f</span>) and then selects the masked regions to form <span class="literal">y</span>.</p>
<p class="indent">To make the code reproducible from a given seed value, we define the local variable, <span class="literal">seed</span>, and pass it to the <span class="literal">Lasso</span> constructor before calling <span class="literal">fit</span>.</p>
<p class="indent">When <span class="literal">fit</span> exits, the inverse DCT uses the sparse vector (<em><strong>s</strong></em>) to recover the image. The image isn’t scaled to [0, 255], so we first scale it to [0, 1] and then multiply by 255 and reshape (<span class="literal">oimg</span>).</p>
<p class="indent">Let’s find out whether <em>cs_image.py</em> works. This command line</p>
<pre class="pre">&gt; <span class="codestrong1">python3 cs_image.py images/peppers.png peppers 0.1 0.001 mt19937 66</span></pre>
<p class="noindent">attempts to reconstruct the peppers image. It will take several minutes to run before producing <a href="ch09.xhtml#ch09fig05">Figure 9-5</a>.</p>
<div class="image"><img alt="Image" id="ch09fig05" src="../images/09fig05.jpg"/></div>
<p class="figcap"><em>Figure 9-5: The original image (left), mask (middle), and reconstructed image (right)</em></p>
<p class="noindent">The original image is on the left, the 10 percent mask in the middle, and the reconstructed image on the right. This is best viewed in color; look at the files in the <em>peppers</em> directory. I inverted the mask image to show the selected pixels in black.</p>
<p class="indent">The reconstructed image isn’t particularly impressive until you remember that 90 percent of the original image information was discarded or, in practice, never measured in the first place.</p>
<p class="indent">I claimed that Lasso finds sparse <em><strong>s</strong></em> vectors. The signal example was sparse, but what about images? The test images are 128×128 = 16, 384 pixels, meaning <em><strong>s</strong></em> has that many elements. A quick test with the <em>barbara.png</em> image, keeping 20 percent of the pixels, returned an <em><strong>s</strong></em> that’s 70 percent zeros. Dropping down to 10 percent jumps to 81 percent zeros, while moving up to 80 percent drops to only 15 percent zeros. Fewer measurements imply a sparser <em><strong>s</strong></em>, which seems reasonable. Recall that <em><strong>s</strong></em> is the representation of the image in the discrete cosine transform space. If we can find only a few presumably low frequency components when attempting to best fit the few measurements in <em><strong>y</strong></em>, we might expect most of <em><strong>s</strong></em> to be zero after imposing <em>ℓ</em><sup>1</sup> regularization.</p>
<p class="indent"><span epub:type="pagebreak" id="page_266"/>The <em>cs_image_test</em> script runs <em>cs_image.py</em> repeatedly on the same test image while varying the measured fraction of pixels from 1 percent up to 80 percent. <a href="ch09.xhtml#ch09fig06">Figure 9-6</a> shows the resulting reconstructed images.</p>
<div class="image"><img alt="Image" id="ch09fig06" src="../images/09fig06.jpg"/></div>
<p class="figcap"><em>Figure 9-6: Reconstructions of the</em> zelda.png <em>image by varying the fraction of the original pixels</em></p>
<p class="indent">At 10 percent, we can start to recognize the image, but it isn’t clear that it’s a person’s face until 20 percent. Note that I altered the intensity of the original <em>zelda.png</em> image to use the entire range [0, 255]; this makes it as bright as the reconstructions.</p>
<p class="indent"><a href="ch09.xhtml#ch09fig07">Figure 9-7</a> shows a plot of the mean structural similarity index (SSIM) between the reconstructions and the original image.</p>
<div class="image"><img alt="Image" id="ch09fig07" src="../images/09fig07.jpg"/></div>
<p class="figcap"><span epub:type="pagebreak" id="page_267"/><em>Figure 9-7: The mean structural similarity index as a function of measurements</em></p>
<p class="indent">As we might anticipate, the index increases rapidly as the number of pixels measured increases. The results are encouraging, because there’s little perceptual difference between the original image and the one made from 20 percent fewer measurements.</p>
<h3 class="h3" id="ch00lev1_58"><strong>Compressed Sensing Applications</strong></h3>
<p class="noindent">Compressed sensing is used in many places, including medical imaging, where its use has improved acquisition times in magnetic resonance imaging and various forms of tomography. Applying compressed sensing to tomography implies collecting fewer projections, leading to a substantial reduction in the amount of X-ray energy used (ionizing radiation).</p>
<p class="indent">Magnetic resonance imaging is a natural target for compressed sensing. The image acquisition process literally measures in <em>k-space</em>, or Fourier space, equivalent to measuring <em><strong>s</strong></em> directly. The desired image is recovered by a two-dimensional inverse Fourier transform, just as we recovered <em><strong>x</strong></em> from <em><strong>s</strong></em> via the inverse discrete cosine transform. Many k-space sampling strategies have been developed to speed image acquisition while still producing clinically valuable images. How magnetic resonance image acquisition works makes the simple random sampling in this chapter impractical, but alternative approaches for sampling k-space in a mathematically incoherent manner exist and lead to reduced acquisition times. For example, GE’s <em>HyperSense</em>, an advanced compressed sensing method, can reduce scan times by up to 50 percent. Faster scan times mean less scanner time for the patient.</p>
<p class="indent"><span epub:type="pagebreak" id="page_268"/>The future of compressed sensing is, however, a bit unclear. Deep neural networks are also quite good at solving inverse linear problems—in fact, likely better than traditional compressed sensing. Using deep neural networks in place of traditional CS, or in combination with it, is an active research area.</p>
<h3 class="h3" id="ch00lev1_59"><strong>Exercises</strong></h3>
<p class="noindent">This brief chapter included two experiments, first with one-dimensional signals, then with images expressed as one-dimensional vectors. Here are some possible avenues for further exploration:</p>
<ul>
<li class="noindent">The code in <em>cs_signal.py</em> worked with the entire one-second sound sample. How might you modify this basic approach to compress an arbitrary WAV file? Hint: try keeping only a random subset of each few hundred milliseconds of sound and reconstructing each.</li>
<li class="noindent">Assuming you build an arbitrary WAV filesystem, can you get away with using the same measurement matrix (the same random sampling) for each subset, or is it better to alter that in some way—maybe by using a fixed pseudorandom seed and selecting measurements in blocks as needed?</li>
<li class="noindent">All of our image experiments used <em>α</em> = 0.001. Try varying <em>α</em> from near 0 up to, or even beyond, 1. If <em>α</em> = 0, the <em>ℓ</em><sup>1</sup> regularization term in Lasso vanishes, and the optimization becomes standard least-squares using only the <em>ℓ</em><sup>2</sup> norm. Does compressed sensing work well when <em>α</em> is very small? Note that the scikit-learn documentation for Lasso warns not to use <em>α</em> = 0, so, for that case, replace <span class="literal">Lasso</span> with <span class="literal">LinearRegression</span>.</li>
<li class="noindent">The <em>cs_image.py</em> file includes checks to see if the supplied randomness source is <span class="literal">quasi</span> and, if so, to interpret the seed value as the quasi-random generator base. What happens if you use <span class="literal">quasi</span> for different prime bases like 2, 3, or 13? Can you explain the results you see?</li>
<li class="noindent">We process RGB images color channel by color channel. As an alternative, we can unravel the full RGB image into a vector three times larger and then perform the optimization (remember to re-form the RGB image on output). Alter <em>cs_image.py</em> to do this. Does it matter? Does it help or hurt?</li>
<li class="noindent">Are all random measurement matrices created equal?</li>
</ul>
<h3 class="h3" id="ch00lev1_60"><span epub:type="pagebreak" id="page_269"/><strong>Summary</strong></h3>
<p class="noindent">Compressed sensing breaks the Nyquist-Shannon sampling theorem limit and allows signals to be reconstructed from fewer samples than initially thought possible. In this chapter, we experimented with a basic form of compressed sensing and applied it to audio signals and images.</p>
<p class="indent">First, we discussed the core concepts in compressed sensing, including sparsity and <em>ℓ</em><sup>1</sup> regularization. We then expressed the compressed sensing problem as an inverse linear problem of the form <em><strong>y</strong></em> = <em><strong>Cx</strong></em> for measured vector <em><strong>y</strong></em> and desired output vector <em><strong>x</strong></em>. In practice, sparsity constraints means using an alternate form of <em><strong>x</strong></em> = <strong>Ψ</strong><em><strong>s</strong></em> for sparse vector <em><strong>s</strong></em> and basis <strong>Ψ</strong>. For us, <strong>Ψ</strong> came from the discrete cosine transform in which signals are known to be sparse.</p>
<p class="indent">The compressed sensing problem then became one of finding solution vector <em><strong>s</strong></em> such that the <em>ℓ</em><sup>2</sup> distance between <em><strong>y</strong></em> and <em><strong>C</strong></em><strong>Ψ</strong><em><strong>s</strong></em> = <strong>Θ</strong><em><strong>s</strong></em> was as small as possible, subject to the constraint that ∥<strong><em>s</em></strong>∥<sub>1</sub> was also as small as possible. We discovered that Lasso regression accomplishes this goal quite nicely.</p>
<p class="indent">Theory in hand, we performed two sets of experiments. The first sought to reconstruct a one-second audio signal, a C major chord, using uniform sampling below the Nyquist limit and random sampling. Uniform sampling couldn’t recover the signal until the sampling rate exceeded half the playback rate (in which more than 50 percent of samples were kept). On the other hand, random sampling with compressed sensing returned good results even after discarding up to 90 percent of the original data.</p>
<p class="indent">In the second experiment, we worked with both grayscale and RGB images. As with signals, we successfully used compressed sensing and the discrete cosine transform to recover images from as little as 10 percent of the original pixels, often with considerable noise. The DCT isn’t necessarily the best basis for images, but better ones, like wavelets, are beyond the scope of this book.</p>
<p class="indent">We closed the chapter by pointing out that compressed sensing has been a boon to medical imaging to improve patient comfort and reduce exposure to ionizing radiation. Finally, we noted that recent advances in deep neural networks will likely substantially impact the future of compressed sensing.</p>
<p class="indent">We’ll take a break from experimenting in the next chapter to explore how we use randomness in experiments themselves. Modern science critically depends on properly designed experiments, and randomness is a powerful player in that process.<span epub:type="pagebreak" id="page_270"/></p>
</body></html>