- en: '**19**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**ANALYSIS OF VARIANCE**'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/common-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Analysis of variance (ANOVA)*, in its simplest form, is used to compare multiple
    means in a test for equivalence. In that sense, it’s a straightforward extension
    of the hypothesis test comparing two means. There’s a continuous variable from
    which the means of interest are calculated, and there’s at least one categorical
    variable that tells you how to define the groups for those means. In this chapter,
    you’ll explore the ideas surrounding ANOVA and look at comparing means first split
    by one categorical variable (one-way ANOVA) and then split by multiple categorical
    variables (multiple-factor ANOVA).'
  prefs: []
  type: TYPE_NORMAL
- en: '**19.1 One-Way ANOVA**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The simplest version of ANOVA is referred to as *one-way* or *one-factor* analysis.
    Simply put, the one-way ANOVA is used to test two or more means for equality.
    Those means are split by a categorical *group* or *factor* variable. ANOVA is
    often used to analyze experimental data to assess the impact of an intervention.
    You might, for example, be interested in comparing the mean weights of the chicks
    in the built-in `chickwts` data set, split according to the different food types
    they were fed.
  prefs: []
  type: TYPE_NORMAL
- en: '***19.1.1 Hypotheses and Diagnostic Checking***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Say you have a categorical-nominal variable that splits a total of *N* numeric
    observations into *k* distinct groups, where *k* ≥ 2\. You’re looking to statistically
    compare the *k* groups’ means, *μ[1]*,...,*μ[k]*, to see whether they can be claimed
    to be equal. The standard hypotheses are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'H[0] : *μ[1]* = *μ[2]* = ... = *μ[k]*'
  prefs: []
  type: TYPE_NORMAL
- en: 'H[A] : *μ[1]*, *μ[2]*, ... , *μ[k]* are not all equal'
  prefs: []
  type: TYPE_NORMAL
- en: (alternatively, at least one mean differs).
  prefs: []
  type: TYPE_NORMAL
- en: In fact, when *k* = 2, the two-sample *t*-test is equivalent to ANOVA; for that
    reason, ANOVA is most frequently employed when *k* > 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following assumptions need to be satisfied in order for the results of
    the basic one-way ANOVA test to be considered reliable:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Independence** The samples making up the *k* groups must be independent of
    one another, and the observations in each group must be independent and identically
    distributed (iid).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Normality** The observations in each group should be normally distributed,
    or at least approximately so.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Equality of variances** The variance of the observations in each group should
    be equal, or at least approximately so.'
  prefs: []
  type: TYPE_NORMAL
- en: If the assumptions of equality of variances or normality are violated, it doesn’t
    necessarily mean your results will be completely worthless, but it will impact
    the overall effectiveness of detecting a true difference in the means (refer to
    the discussion on statistical power in [Section 18.5.4](ch18.xhtml#ch18lev2sec167)).
    It’s always a good idea to assess the validity of these assumptions before using
    ANOVA; I’ll do this informally for the upcoming example.
  prefs: []
  type: TYPE_NORMAL
- en: It’s also worth noting that you don’t need to have an equal number of observations
    in each group to perform the test (in which case it is referred to as *unbalanced*).
    However, having unbalanced groups does render the test more sensitive to potentially
    detrimental effects if your assumptions of equality of variances and normality
    are not sound.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s return to the `chickwts` data for the example—the weights of chicks based
    on *k* = 6 different feeds. You’re interested in comparing the mean weights according
    to feed type to see whether they’re all equal. Use `table` to summarize the six
    sample sizes and use `tapply` (see, for example, [Section 13.2.1](ch13.xhtml#ch13lev2sec116))
    to get each group mean, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Your skills from [Section 14.3.2](ch14.xhtml#ch14lev2sec125) allow you to produce
    side-by-side box-plots of the distributions of weights. The next two lines give
    you the plot on the left of [Figure 19-1](ch19.xhtml#ch19fig1):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![image](../images/f19-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 19-1: Exploring the* `chickwts` *data. Left: Side-by-side boxplots
    of chick weight split by feed type, with the mean marked by* ×*. Right: Normal
    QQ plot of the mean-centered data of each feed group.*'
  prefs: []
  type: TYPE_NORMAL
- en: Because boxplots display the median, not the mean, the second line of code adds
    the feed-specific means (stored in the `chick.means` object you just created)
    to each box using `points`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inspecting the left plot of [Figure 19-1](ch19.xhtml#ch19fig1), it certainly
    looks as though there’s a difference in the mean weights. Is any apparent difference
    statistically significant, though? To find out, the ANOVA test for this example
    concerns the following hypotheses:'
  prefs: []
  type: TYPE_NORMAL
- en: 'H[0] : *μ*[casein] = *μ*[horsebean] = *μ*[linseed] = *μ*[meatmeal] = *μ*[soybean]
    = *μ*[sunflower]'
  prefs: []
  type: TYPE_NORMAL
- en: 'H[A] : The means are not all equal.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assuming independence of the data, before implementing the test, you must first
    check that the other assumptions are valid. To examine equality of variances,
    you can use the same informal rule of thumb as used in the two-sample *t*-test.
    That is, you can assume equality of variances if the ratio of the largest sample
    standard deviation to the smallest is less than 2\. For the chick weights data,
    the following code will determine this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This informal result indicates that it’s reasonable to make the assumption.
  prefs: []
  type: TYPE_NORMAL
- en: Next, consider the assumption of normality of the raw observations. This can
    be difficult to determine in many real-data examples. At the least, though, it’s
    worthwhile to inspect histograms and QQ plots for signs of non-normality. You
    already inspected histograms and QQ plots for all 71 weights in [Section 16.2.2](ch16.xhtml#ch16lev2sec142),
    but for an ANOVA, you need to do this with respect to the grouping of the observations
    (that is, not just “overall” for the whole set of weights regardless of groups).
  prefs: []
  type: TYPE_NORMAL
- en: 'To achieve this for the `chickwts` data, you need to first *mean-center* each
    weight by its respective sample mean. You can do this by taking the original vector
    of weights and subtracting from it the `chick.means` vector, but first you must
    rearrange and replicate the latter elements to correspond to the elements in the
    former. This is done by using `as.numeric` on the factor vector that represents
    feed type, giving the numeric value of the levels of `chickwts$feed` for each
    record in the original data frame. When that numeric vector is passed via the
    square brackets to `chick.means`, you get the correct group mean matched to each
    observation. As an exercise, you can inspect all the ingredients that go into
    creating the following `chick.meancen` object to satisfy yourself of what’s going
    on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In the context of the current analysis, these group-wise, mean-centered values
    are also referred to as *residuals*, a term you’ll come across frequently when
    you study regression methods in the next few chapters.
  prefs: []
  type: TYPE_NORMAL
- en: You can now assess normality of the observations as a whole using the residuals.
    To inspect a normal QQ plot, the relevant functions are `qqnorm` and `qqline`,
    which you first met in [Section 16.2.2](ch16.xhtml#ch16lev2sec142). The following
    two lines produce the image on the right of [Figure 19-1](ch19.xhtml#ch19fig1).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Based on this plot (the proximity of the plotted points to the perfect straight
    line), it doesn’t seem unreasonable to assume normality for these data, particularly
    when compared to QQ plots of generated normal data of the same sample size (an
    example was given on the left of [Figure 16-9](ch16.xhtml#ch16fig9) on [page 355](ch16.xhtml#page_355)).
  prefs: []
  type: TYPE_NORMAL
- en: Investigating the validity of any required assumptions is referred to as *diagnostic
    checking*. If you wanted to perform a more rigorous diagnostic check for an ANOVA,
    other visual diagnostics could involve inspecting QQ plots split by group (you’ll
    do this in an example in [Section 19.3](ch19.xhtml#ch19lev1sec61)) or plotting
    the sample standard deviation for each group against the corresponding sample
    means. Indeed, there are also general hypothesis tests for normality (such as
    the Shapiro-Wilk test or Anderson-Darling test—you’ll see the former used in [Section
    22.3.2](ch22.xhtml#ch22lev2sec215)), as well as tests for equality of variances
    (such as Levene’s test), but I’ll stick with the basic rule of thumb and visual
    checks in this example.
  prefs: []
  type: TYPE_NORMAL
- en: '***19.1.2 One-Way ANOVA Table Construction***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Turning your attention back to the left of [Figure 19-1](ch19.xhtml#ch19fig1),
    remember that the goal is to statistically evaluate the equality of the means
    marked by ×. This task will therefore require you to consider not only the variability
    *within* each of the *k* samples but the variability *between* the samples; this
    is why the test is referred to as an analysis of variance.
  prefs: []
  type: TYPE_NORMAL
- en: The test proceeds by first calculating various metrics associated with the overall
    variability and then calculating the within- and between-group variability. These
    figures involve sums of squared quantities and associated degrees of freedom values.
    All this culminates in a single test statistic and *p*-value targeting the aforementioned
    hypotheses. These ingredients are typically presented in a table, which is defined
    as follows.
  prefs: []
  type: TYPE_NORMAL
- en: Let *x*[1], ... , *x[N]* represent all *N* observations, regardless of group;
    let *x*[1][(][*j*][)], ... , *x[nj](j)* denote the specific group observations
    in group *j* = 1, ... , *k* such that *n[1]* + ... + *n[k]* = *N*. Let the “grand
    mean” of all observations be defined as ![image](../images/f0439-01.jpg). The
    ANOVA table is then constructed, where SS stands for sum-of-squares, df stands
    for degrees of freedom, MS stands for mean square, *F* refers to the *F* test
    statistic, and *p* refers to the *p*-value.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | df | SS | MS | *F* | *p* |'
  prefs: []
  type: TYPE_TB
- en: '| Overall | 1 | **(1)** |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Group (or “[Factor](ch04.xhtml#ch04lev1sec18)”) | *k* − 1 | **(2)** | **(5)**
    | **(5)÷(6)** | *p*-value |'
  prefs: []
  type: TYPE_TB
- en: '| Error (or “Residual”) | *N* − *k* | **(3)** | **(6)** |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| TOTAL | *N* | **(4)** |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: 'You calculate the values with these formulas:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Nx̄*²'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![image](../images/f0439-02.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_IMG
- en: '**(4)–(2)–(1)**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![image](../images/f0439-03.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_IMG
- en: '**(2)**÷(*k* – 1)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**(3)**÷(*N* – *k*)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'There are three input sources that are assumed to make up the observed data,
    which, when added together, result in the TOTAL row. Let’s think about these in
    a little more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Overall row** This relates to the scale on which the data as a whole sit.
    It doesn’t affect the outcome of the hypothesis test (since you’re interested
    only in the relative differences between means) and is sometimes removed from
    the table, affecting the TOTAL values accordingly.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Group row/Factor row** This relates to the data in the individual groups
    of interest, thereby accounting for the *between-group variability*.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Error row/Residual row** This accounts for the random deviation from the
    estimated means of each group, thereby accounting for the *within-group variability*.'
  prefs: []
  type: TYPE_NORMAL
- en: '**TOTAL row** This represents the raw data, based on the previous three ingredients.
    It is used to find the Error SS by differencing.'
  prefs: []
  type: TYPE_NORMAL
- en: The three input sources each have a corresponding degrees of freedom (df) value
    in the first column and a sum-of-squares (SS) value attached to the df in the
    second column. Between- and within-group variability is averaged by dividing the
    SS by the df, giving the mean squared (MS) component for these two items. The
    test statistic, *F*, is found by dividing the mean squared group (MSG) effect
    by the mean squared error (MSE) effect. This test statistic follows the *F*-distribution
    (refer to [Section 16.2.5](ch16.xhtml#ch16lev2sec145)), which itself requires
    a pair of degrees of freedom values ordered as df[1] (which represents the Group
    df, *k*−1) and df[2] (which represents the Error df, *N*−*k*). Like the chi-squared
    distribution, the *F*-distribution is unidirectional in nature, and the *p*-value
    is obtained as the upper-tail area from the test statistic *F*.
  prefs: []
  type: TYPE_NORMAL
- en: '***19.1.3 Building ANOVA Tables with the aov Function***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As you might expect, R allows you to easily construct an ANOVA table for the
    chick weight test using the built-in `aov` function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Then, the table is printed to the console screen using `summary`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: There are several comments to make here. Note that you employ formula notation
    `weight~feed` to specify the measurement variable of interest, weight, as modeled
    by the categorical-nominal variable of interest, feed type. In this case, the
    variable names `weight` and `feed` are not required to be prefaced by `chickwts$`
    since the optional `data` argument has been passed the data frame of interest.
  prefs: []
  type: TYPE_NORMAL
- en: Remember from [Section 14.3.2](ch14.xhtml#ch14lev2sec125) that for the notation
    in the expression `weight~feed`, the “outcome” of interest must always appear
    on the left of the `~` (this notation will become particularly relevant in [Chapters
    20](ch20.xhtml#ch20) to [22](ch22.xhtml#ch22)).
  prefs: []
  type: TYPE_NORMAL
- en: To actually view the table, you must apply the `summary` command to the object
    resulting from the call to `aov`. R omits the first and last rows (Overall and
    TOTAL) since these are not directly involved in calculating the *p*-value. Other
    than that, it’s easy to identify that the `feed` row refers to the Group row and
    the `Residuals` row refers to the Error row.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*By default, R annotates model-based* `summary` *output like this with* significance
    stars*. These show intervals of significance, and the number of stars increases
    as the p-value decreases beyond a cutoff mark of 0.1\. This can be useful when
    you’re examining more complicated analyses where multiple p-values are summarized,
    though not everyone likes this feature. If you want, you can turn off this feature
    in a given R session by entering* `options(show.signif.stars=FALSE)` *at the prompt.
    Alternatively, you can turn off the feature directly in the call to* `summary`
    *by setting the additional argument* `signif.stars=FALSE`*. In this book, I’ll
    leave them be.*'
  prefs: []
  type: TYPE_NORMAL
- en: From the contents of the ANOVA for this example, you can quickly confirm the
    calculations. Note that the MSE, 3009, was defined as the Error SS divided by
    the Error df. Indeed, in R, the same result is achieved manually (the table output
    has been rounded to the nearest integer).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: You can confirm all the other results in the table output using the relevant
    equations from earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting a hypothesis test based on ANOVA follows the same rules as any
    other test. With the understanding of a *p*-value as “the probability that you
    observe the sample statistics at hand or something more extreme, if H[0] is true,”
    a small *p*-value indicates evidence against the null hypothesis. In the current
    example, a tiny *p*-value provides strong evidence against the null that the mean
    chick weights are the same for the different diets. In other words, you reject
    H[0] in favor of H[A]; the latter states that there is a difference.
  prefs: []
  type: TYPE_NORMAL
- en: In a similar fashion as in the chi-squared tests, rejection of the null in one-way
    ANOVA doesn’t tell you exactly where a difference lies, merely that there’s evidence
    one exists. Further scrutiny of the data in the individual groups is necessary
    to identify the offending means. At the simplest level, you could turn back to
    pairwise two-sample *t*-tests, in which case you could also use the MSE from the
    ANOVA table as an estimate of the pooled variance. The substitution is valid if
    the assumption of equal variance holds, and such a step is beneficial because
    the corresponding *t*-based sampling distribution will utilize the Error df (this
    is naturally higher than would otherwise be the case if the df was based on just
    the sample sizes of the two groups of specific interest).
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 19.1**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following data:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Site I** | **Site II** | **Site III** | **Site IV** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 93 | 85 | 100 | 96 |'
  prefs: []
  type: TYPE_TB
- en: '| 120 | 45 | 75 | 58 |'
  prefs: []
  type: TYPE_TB
- en: '| 65 | 80 | 65 | 95 |'
  prefs: []
  type: TYPE_TB
- en: '| 105 | 28 | 40 | 90 |'
  prefs: []
  type: TYPE_TB
- en: '| 115 | 75 | 73 | 65 |'
  prefs: []
  type: TYPE_TB
- en: '| 82 | 70 | 65 | 80 |'
  prefs: []
  type: TYPE_TB
- en: '| 99 | 65 | 50 | 85 |'
  prefs: []
  type: TYPE_TB
- en: '| 87 | 55 | 30 | 95 |'
  prefs: []
  type: TYPE_TB
- en: '| 100 | 50 | 45 | 82 |'
  prefs: []
  type: TYPE_TB
- en: '| 90 | 40 | 50 |  |'
  prefs: []
  type: TYPE_TB
- en: '| 78 |  | 45 |  |'
  prefs: []
  type: TYPE_TB
- en: '| 95 |  | 55 |  |'
  prefs: []
  type: TYPE_TB
- en: '| 93 |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 88 |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 110 |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: These figures provide the depths (in centimeters) at which important archaeological
    finds were made at four sites in New Mexico (see [Woosley and Mcintyre, 1996](ref.xhtml#ref77)).
    Store these data in your R workspace, with one vector containing depth and the
    other vector containing the site of each observation.
  prefs: []
  type: TYPE_NORMAL
- en: Produce side-by-side boxplots of the depths split by group, and use additional
    points to mark the locations of the sample means.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assuming independence, execute diagnostic checks for normality and equality
    of variances.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform and conclude a one-way ANOVA test for evidence of a difference between
    the means.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In [Section 14.4](ch14.xhtml#ch14lev1sec47), you looked at the data set providing
    measurements on petal and sepal sizes for three species of iris flowers. This
    is available in R as `iris`.
  prefs: []
  type: TYPE_NORMAL
- en: Based on diagnostic checks for normality and equality of variances, decide which
    of the four outcome measurements (sepal length/width and petal length/width) would
    be suitable for ANOVA (using the species as the group variable).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Carry out one-way ANOVA for any suitable measurement variables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**19.2 Two-Way ANOVA**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In many studies, the numeric outcome variable you’re interested in will be categorized
    by more than just one grouping variable. In these cases, you would use the *multiple-factor*
    ANOVA rather than the one-way ANOVA. This technique is directly referred to by
    the number of grouping variables used, with two- and three-way ANOVA being the
    next and most common extensions.
  prefs: []
  type: TYPE_NORMAL
- en: Increasing the number of grouping variables complicates matters somewhat—performing
    just a one-way ANOVA for each variable separately is inadequate. In dealing with
    more than one categorical grouping factor, you must consider the *main effects*
    of each factor on the numeric outcome, while simultaneously accounting for the
    presence of the other grouping factor(s). That’s not all, though. It’s just as
    important to additionally investigate the idea of an *interactive effect*; if
    an interactive effect exists, then it suggests that the impact one of the grouping
    variables has on the outcome of interest, specified by its main effect, varies
    according to the levels of the other grouping variable(s).
  prefs: []
  type: TYPE_NORMAL
- en: '***19.2.1 A Suite of Hypotheses***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For this explanation, denote your numeric outcome variable with *O* and your
    two grouping variables as *G[1]* and *G*[2]. In two-way ANOVA, the hypotheses
    should be set along the following lines:'
  prefs: []
  type: TYPE_NORMAL
- en: 'H[0] : *G[1]* has no main (marginal) effect on the mean of *O*.'
  prefs: []
  type: TYPE_NORMAL
- en: '*G*[2] has no main (marginal) effect on the mean of *O*.'
  prefs: []
  type: TYPE_NORMAL
- en: There is no interactive effect of *G[1]* with *G[2]* on the mean of *O*.
  prefs: []
  type: TYPE_NORMAL
- en: 'H[A] : Separately, each statement in H[0] is incorrect.'
  prefs: []
  type: TYPE_NORMAL
- en: You can see from these general hypotheses that you now have to obtain a *p*-value
    for each of the three components.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the example, let’s use the built-in `warpbreaks` data frame ([Tippett,
    1950](ref.xhtml#ref65)), which provides the number of “warp break” imperfections
    (column `breaks`) observed in 54 pieces of yarn of equal length. Each piece of
    yarn is classified according to two categorical variables: `wool` (the type of
    yarn, with levels `A` and `B`) and `tension` (the level of tension applied to
    that piece—`L`, `M`, or `H` for low, medium, or high). Using `tapply`, you can
    inspect the mean number of warp breaks for each classification.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: You can supply more than one grouping variable to the `INDEX` argument as separate
    members of a list (any factor vectors given to this argument should be the same
    length as the first argument that specifies the data of interest). The results
    are returned as a matrix for two grouping variables, a 3D array for three grouping
    variables, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: For some analyses, however, you might need the same information provided earlier
    in a different format. The `aggregate` function is similar to `tapply`, but it
    returns a data frame, the results in *stacked* format according to the specified
    grouping variables (as opposed to an array as returned by `tapply`). It’s called
    in much the same way. The first argument takes the data vector of interest. The
    second argument, `by`, should be a list of the desired grouping variables, and
    in `FUN`, you specify the function to operate on each subset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Here I’ve stored the result of the call to `aggregate` as the object `wb.means`
    for later use.
  prefs: []
  type: TYPE_NORMAL
- en: '***19.2.2 Main Effects and Interactions***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'I mentioned earlier that you could perform just a one-way ANOVA on each grouping
    variable separately, but this, in general, isn’t a good idea. I’ll demonstrate
    this now with the `warpbreaks` data (a quick inspection of the relevant diagnostics
    shows no obvious cause for concern):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This output tells you that if you ignore `tension`, there is no evidence to
    suggest that there is any difference in the mean number of imperfections based
    on the type of `wool` alone (*p*-value 0.108). If you ignore `wool`, however,
    there *is* evidence to suggest a difference in warp breaks according to `tension`
    only.
  prefs: []
  type: TYPE_NORMAL
- en: The problem here is that by ignoring one of the variables, you lose the ability
    to detect differences (or, more generally, statistical relationships) that may
    occur at a finer level. For example, though the `wool` type alone seems to have
    no remarkable impact on the mean number of warp breaks, you cannot tell whether
    this would be the case if you just looked at `wool` types at one particular level
    of `tension`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead, you investigate this kind of question using two-way ANOVA. The following
    executes a two-way ANOVA for the warp breaks data based only on the main effects
    of the two grouping variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Take a look at the formula. Specifying `wool+tension` to the right of the outcome
    variable and the `~` allows you to take both grouping variables into account at
    the same time. The results reveal a small drop in the size of the *p*-values now
    attached to each grouping variable; indeed, the *p*-value for `wool` is around
    0.073, approaching the conventional cutoff significance level of *α* = 0.05\.
    To interpret the results, you hold one grouping variable constant—if you focus
    on just one type of wool, there is still statistically significant evidence to
    suggest a difference in the mean number of warp breaks between the different `tension`
    levels. If you focus on just one level of `tension`, the evidence of a difference
    considering the two `wool` types has increased a little but is still not statistically
    significant (assuming the aforementioned *α* = 0.05).
  prefs: []
  type: TYPE_NORMAL
- en: There’s still a limitation with considering only main effects. While the previous
    analysis shows that there’s variation in the outcome between the different levels
    of the two categorical variables, it doesn’t address the possibility that a difference
    in the mean number of warp breaks might vary further according to precisely *which*
    level of either `tension` or `wool` is being used when holding the other variable
    constant. This relatively subtle yet important consideration is known as an *interaction*.
    Specifically, if there is an interactive effect present between `tension` and
    `wool` with respect to warp breaks, then this would imply that the magnitude and/or
    direction of the difference in the mean number of warp breaks *is not the same*
    at different levels of the two grouping factors.
  prefs: []
  type: TYPE_NORMAL
- en: To account for interactions, you make a slight adjustment to the two-way ANOVA
    model code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: You can explicitly specify the interaction as the main effects model formula
    *plus* the notation `wool:tension`, where the two grouping variables are separated
    by a `:`. (Note, in this setting, the `:` operator has nothing to do with the
    shortcut for creating an integer sequence first discussed in [Section 2.3.2](ch02.xhtml#ch02lev2sec21).)
  prefs: []
  type: TYPE_NORMAL
- en: You can see from the ANOVA table output that, statistically, there is evidence
    of an interactive effect; that is, the very nature of the difference in the means
    is dependent upon the factor levels themselves, even though that evidence is relatively
    weak. Of course, the *p*-value of around 0.021 tells you only that, overall, there
    might be an interaction but not the precise features of the interaction.
  prefs: []
  type: TYPE_NORMAL
- en: To help with this, you can interpret such a two-way interaction effect in more
    detail with an *interaction plot*, provided in R with `interaction.plot`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: When `interaction.plot` is called, the outcome means should be supplied to the
    argument `response`, and the vectors providing the corresponding levels of each
    of the two factors should be supplied to the arguments `x.factor` (for the variable
    on the horizontal axis that refers to moving between levels from the left to the
    right) and `trace.factor` (each level of which will produce a different line,
    referenced in an automatically produced legend; the title of this legend is passed
    to `trace.label`). It doesn’t matter which grouping variable is which; the appearance
    of the plot will change accordingly, but your interpretations will (should!) be
    the same. The result is shown in [Figure 19-2](ch19.xhtml#ch19fig2).
  prefs: []
  type: TYPE_NORMAL
- en: The two-way interaction plot displays the outcome variable on the vertical axis
    and splits the recorded means by the levels of the two grouping variables. This
    allows you to inspect the potential effect that varying the levels of the grouping
    variables has on the outcome. In general, when the lines (or segments thereof)
    are not parallel, it suggests an interaction could be present. Vertical separations
    between the plotted locations indicate the individual main effects of the grouping
    variables.
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that the columns returned by a call to `aggregate` are actually
    perfectly suited to `interaction.plot`. As usual, you can specify the common graphical
    parameters, like those you initially encountered in [Section 7.2](ch07.xhtml#ch07lev1sec24),
    to control specific features of the plot and axis annotation. For [Figure 19-2](ch19.xhtml#ch19fig2),
    you’ve specified that `x.factor` should be the second column of the `wb.means`
    matrix, meaning that the `tension` levels vary horizontally. The `trace.factor`
    here is the type of `wool`, so there are only two distinct lines corresponding
    to the two levels `A` and `B`. The `response` is that third column of `wb.means`,
    extracted using `$x` (take a look at the `wb.means` object; you’ll see the column
    containing the results of interest is labeled `x` by default after a call to `aggregate`).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f19-02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 19-2: An interaction plot for the full two-way ANOVA model of the*
    `warpbreaks` *data set*'
  prefs: []
  type: TYPE_NORMAL
- en: Considering the actual appearance of the plot in [Figure 19-2](ch19.xhtml#ch19fig2),
    it does indeed appear that the mean number of warp breaks for wool type `A` is
    higher if `tension` is low, but the nature of the difference changes if you move
    to a medium tension, where `B` has a higher point estimate than `A`. Moving to
    a high tension, type `A` again has a higher estimate of the mean number of breaks
    than `B`, though here the difference between `A` and `B` is nowhere near as big
    as it is at a low tension. (Note, however, that the interaction plot does not
    display any kind of standard error measurements, so you must remember that all
    point estimates of the means are subject to variability.)
  prefs: []
  type: TYPE_NORMAL
- en: Interactions are certainly not a concept unique to multiple-factor ANOVA; they
    form an important consideration in many different types of statistical models.
    For the moment, it’s good just to gain a basic appreciation of interactions.
  prefs: []
  type: TYPE_NORMAL
- en: '**19.3 Kruskal-Wallis Test**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When comparing multiple means, there may be situations when you’re unwilling
    to assume normality or have even found the assumption of normality invalid in
    diagnostic checks. In this case, you can use the *Kruskal-Wallis test*, an alternative
    to the one-way ANOVA that relaxes the dependence on the necessity for normality.
    This method tests for “equality of distributions” of the measurements in each
    level of the grouping factor. If you make the usual assumption of equal variances
    across these groups, you can therefore think of this test as one that compares
    multiple medians rather than means.
  prefs: []
  type: TYPE_NORMAL
- en: The hypotheses governing the test alter accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'H[0] : Group medians are all equal.'
  prefs: []
  type: TYPE_NORMAL
- en: 'H[A] : Group medians are not all equal'
  prefs: []
  type: TYPE_NORMAL
- en: (alternatively, at least one group median differs).
  prefs: []
  type: TYPE_NORMAL
- en: The Kruskal-Wallis test is a *nonparametric* approach since it does not rely
    on quantiles of a standardized parametric distribution (in other words, the normal
    distribution) or any of its functions. In the same way that the ANOVA is a generalization
    of the two-sample *t*-test, the Kruskal-Wallis ANOVA is a generalization of the
    Mann-Whitney test for two medians. It’s also referred to as the Kruskal-Wallis
    *rank sum* test, and you use the chi-squared distribution to calculate the *p*-value.
  prefs: []
  type: TYPE_NORMAL
- en: Turn your attention to the data frame `survey`, located in the `MASS` package.
    These data record particular characteristics of 237 first-year undergraduate statistics
    students collected from a class at the University of Adelaide, South Australia.
    Load the required package first with a call to `library("MASS")` and then enter
    `?survey` at the prompt. You can read the help file to understand which variables
    are present in the data frame.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose you’re interested to see whether the age of the students, `Age`, tends
    to differ with respect to four smoking categories reported in `Smoke`. An inspection
    of the relevant side-by-side boxplots and a normal QQ plot of the residuals (mean-centered
    observations with respect to each group) suggests a straightforward one-way ANOVA
    isn’t necessarily a good idea. The following code (which mimics the steps you
    saw in [Section 19.1.1](ch19.xhtml#ch19lev2sec168)) produces the two images in
    [Figure 19-3](ch19.xhtml#ch19fig3), which show normality is questionable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: With this possible violation of normality, you could therefore apply the Kruskal-Wallis
    test instead of the parametric ANOVA. A quick check for equality of variances
    further supports this, with the ratio of the largest to the smallest group standard
    deviations clearly being less than 2.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![image](../images/f19-03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 19-3: Side-by-side boxplots (left) and a normal QQ plot of the residuals
    (right) for the student age observations split by smoking status*'
  prefs: []
  type: TYPE_NORMAL
- en: In R, a Kruskal-Wallis test is performed using `kruskal.test`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The syntax for this test is the same as for `aov`. As you might suspect from
    [Figure 19-3](ch19.xhtml#ch19fig3), the large *p*-value suggests there’s no evidence
    against the null hypothesis that states that the medians are all equal. In other
    words, there doesn’t seem to be an overall age difference between the students
    in the four smoking categories.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 19.2**'
  prefs: []
  type: TYPE_NORMAL
- en: Bring up the `quakes` data frame again, which describes the locations, magnitudes,
    depths, and number of observation stations that detected 1,000 seismic events
    off the coast of Fiji.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use `cut` (see [Section 4.3.3](ch04.xhtml#ch04lev2sec48)) to create a new factor
    vector defining the depths of each event according to the following three categories:
    (0,200], (200,400], and (400,680].'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decide whether a one-way ANOVA or a Kruskal-Wallis test is more appropriate
    to use to compare the distributions of the number of detecting stations, split
    according to the three categories in (a).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform your choice of test in (b) (assume a *α* = 0.01 level of significance)
    and conclude.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the `MASS` package with a call to `library("MASS")` if you haven’t already
    done so in the current R session. This package includes the ready-to-use `Cars93`
    data frame, which contains detailed data on 93 cars for sale in the United States
    in 1993 ([Lock, 1993](ref.xhtml#ref43); [Venables and Ripley, 2002](ref.xhtml#ref69)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Use `aggregate` to compute the mean length of the 93 cars, split by two categorical
    variables: `AirBags` (type of airbags available—levels are `Driver & Passenger`,
    `Driver only`, and `None`), and `Man.trans.avail` (whether the car comes in a
    manual transmission—levels are `Yes` and `No`).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Produce an interaction plot using the results in (d). Does there appear to be
    an interactive effect of `AirBags` with `Man.trans.avail` on the mean length of
    these cars (if you consider only these variables)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit a full two-way ANOVA model for the mean lengths according to the two grouping
    variables (assume satisfaction of all relevant assumptions). Is the interactive
    effect statistically significant? Is there evidence of any main effects?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Important Code in This Chapter**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| **Function/operator** | **Brief description** | **First occurrence** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `aov` | Produce ANOVA table | [Section 19.1.3](ch19.xhtml#ch19lev2sec170),
    [p. 440](ch19.xhtml#page_440) |'
  prefs: []
  type: TYPE_TB
- en: '| `aggregate` | Stacked statistics by factor | [Section 19.2.1](ch19.xhtml#ch19lev2sec171),
    [p. 444](ch19.xhtml#page_444) |'
  prefs: []
  type: TYPE_TB
- en: '| `interaction.plot` | Two-factor interaction plot | [Section 19.2.2](ch19.xhtml#ch19lev2sec172),
    [p. 446](ch19.xhtml#page_446) |'
  prefs: []
  type: TYPE_TB
- en: '| `kruskal.test` | Kruskal-Wallis test | [Section 19.3](ch19.xhtml#ch19lev1sec61),
    [p. 449](ch19.xhtml#page_449) |'
  prefs: []
  type: TYPE_TB
