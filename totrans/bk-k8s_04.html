<html><head></head><body>
<h2 class="h2" id="ch03"><span epub:type="pagebreak" id="page_33"/><span class="big">3</span><br/>RESOURCE LIMITING</h2>&#13;
<div class="image1"><img alt="image" src="../images/common01.jpg"/></div>&#13;
<p class="noindent">The process isolation work we did in <a href="ch02.xhtml#ch02">Chapter 2</a> was very important, as a process cannot generally affect what it cannot “see.” However, our process can see the host’s CPU, memory, and networking, so it is possible for a process to prevent other processes from running correctly by using too much of these resources, not leaving enough room for others. In this chapter, we will see how to guarantee that a process uses only its allocated CPU, memory, and network resources, ensuring that we can divide up our resources accurately. This will help when we move on to container orchestration because it will provide Kubernetes with certainty about the resources available on each host when it schedules a container.</p>&#13;
<p class="indent">CPU, memory, and network are important, but there’s one more really important shared resource: storage. However, in a container orchestration environment like Kubernetes, storage is distributed, and limits need to be applied at the level of the whole cluster. For this reason, our discussion of storage must wait until we introduce distributed storage in <a href="ch15.xhtml#ch15">Chapter 15</a>.</p>&#13;
<h3 class="h3" id="ch00lev1sec12"><span epub:type="pagebreak" id="page_34"/>CPU Priorities</h3>&#13;
<p class="noindent">We’ll need to look at CPU, memory, and network separately, as the effect of applying limits is different in each case. Let’s begin by looking at how to control CPU usage. To understand CPU limits, we first need to look at how the Linux kernel decides which process to run and for how long. In the Linux kernel, the <em>scheduler</em> keeps a list of all of the processes. It also tracks which processes are ready to run and how much time each process has received lately. This allows it to create a prioritized list so that it can choose the process that will run next. The scheduler is designed to be as fair as possible (it’s even known as the Completely Fair Scheduler); thus, it tries to give all processes a chance to run. However, it does accept outside input on which of these processes are more important than others. This prioritization is made up of two parts: the scheduling policy, and the priority of each process within that policy.</p>&#13;
<h4 class="h4" id="ch00lev2sec24">Real-Time and Non-Real-Time Policies</h4>&#13;
<p class="noindent">The scheduler supports several different policies, but for our purposes we can group them into real-time policies and non-real-time policies. The term <em>real-time</em> means that some real-world event is critical to the process that creates a deadline. The process needs to complete its processing before this deadline expires, or something bad will happen. For example, the process might be collecting data from an embedded hardware device. In that case, the process must read the data before the hardware buffer overflows. A real-time process is typically not extremely CPU intensive, but when it needs the CPU, it cannot wait, so all processes under a real-time policy are higher priority than any process under a non-real-time policy. Let’s explore this on an example Linux system.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>The example repository for this book is at</em> <a href="https://github.com/book-of-kubernetes/examples">https://github.com/book-of-kubernetes/examples</a>. <em>See “Running Examples” on <a href="ch00.xhtml#ch00lev1sec2">page xx</a> for details on getting set up.</em></p>&#13;
</div>&#13;
<p class="indent">The Linux <span class="literal">ps</span> command tells us the specific policy that applies to each process. Run this command on <em>host01</em> from this chapter’s examples:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">ps -e -o pid,class,rtprio,ni,comm</span>&#13;
 PID CLS RTPRIO  NI COMMAND&#13;
   1 TS       -   0 systemd&#13;
...&#13;
   6 TS       - -20 kworker/0:0H-kblockd&#13;
...&#13;
  11 FF      99   - migration/0&#13;
  12 FF      50   - idle_inject/0&#13;
...&#13;
  85 FF      99   - watchdogd&#13;
...&#13;
 <span epub:type="pagebreak" id="page_35"/>484 RR      99   - multipathd&#13;
...&#13;
7967 TS       -   0 ps</pre>&#13;
<p class="indent">The <span class="literal">-o</span> flag provides <span class="literal">ps</span> with a custom list of output fields, including the scheduling policy <em>class</em> (<span class="literal">CLS</span>) and two numeric priority fields: <span class="literal">RTPRIO</span> and <span class="literal">NI</span>.</p>&#13;
<p class="indent">Looking at the <span class="literal">CLS</span> field first, lots of processes are listed as <span class="literal">TS</span>, which stands for “time-sharing” and is the default non-real-time policy. This includes commands we run ourselves (like the <span class="literal">ps</span> command we ran) as well as important Linux system processes like <span class="literal">systemd</span>. However, we also see processes with policy <span class="literal">FF</span> for first in–first out (FIFO) and policy <span class="literal">RR</span> for round-robin. These are real-time processes, and as such, they have priority over all non-real-time policies in the system. Real-time processes in the list include <span class="literal">watchdog</span>, which detects system lockups and thus might need to preempt other processes, and <span class="literal">multipathd</span>, which watches for device changes and must be able to configure those devices before other processes get a chance to talk to them.</p>&#13;
<p class="indent">In addition to the class, the two numeric priority fields tell us how processes are prioritized within the policy. Not surprisingly, the <span class="literal">RTPRIO</span> field means “real-time priority” and applies only to real-time processes. The <span class="literal">NI</span> field is the “nice” level of the process and applies only to non-real-time processes. For historical reasons, the nice level runs from –20 (least nice, or highest priority) to 19 (nicest, lowest priority).</p>&#13;
<h4 class="h4" id="ch00lev2sec25">Setting Process Priorities</h4>&#13;
<p class="noindent">Linux allows us to set the priority for processes we start. Let’s try to use priorities to control CPU usage. We’ll run a program called <span class="literal">stress</span> that is designed to exercise our system. Let’s use a containerized version of <span class="literal">stress</span> using CRI-O.</p>&#13;
<p class="indent">As before, we need to define YAML files for the Pod and container to tell <span class="literal">crictl</span> what to run. The Pod YAML shown in <a href="ch03.xhtml#ch03list1">Listing 3-1</a> is almost the same as the BusyBox example in <a href="ch02.xhtml#ch02">Chapter 2</a>; only the name is different:</p>&#13;
<p class="noindent6"><em>po-nolim.yaml</em></p>&#13;
<pre>---&#13;
metadata:&#13;
  name: stress&#13;
  namespace: crio&#13;
linux:&#13;
  security_context:&#13;
    namespace_options:&#13;
      network: 2</pre>&#13;
<p class="caption" id="ch03list1"><em>Listing 3-1: BusyBox Pod</em></p>&#13;
<p class="indent">The container YAML has more changes compared to the BusyBox example. In addition to using a different container image, one that already has <span class="literal">stress</span> installed, we also need to provide arguments to <span class="literal">stress</span> to tell it to exercise a single CPU:</p>&#13;
<p class="noindent6"><span epub:type="pagebreak" id="page_36"/><em>co-nolim.yaml</em></p>&#13;
<pre>---&#13;
metadata:&#13;
  name: stress&#13;
image:&#13;
  image: docker.io/bookofkubernetes/stress:stable&#13;
args:&#13;
  - "--cpu"&#13;
  - "1"&#13;
  - "-v"</pre>&#13;
<p class="indent">CRI-O is already installed on <span class="literal">host01</span>, so it just takes a few commands to start this container. First, we’ll pull the image:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">crictl pull docker.io/bookofkubernetes/stress:stable</span>&#13;
Image is up to date for docker.io/bookofkubernetes/stress...</pre>&#13;
<p class="indent">Then, we can run a container from the image:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">cd /opt</span>&#13;
root@host01:/opt# <span class="codestrong1">PUL_ID=$(crictl runp po-nolim.yaml)</span>&#13;
root@host01:/opt# <span class="codestrong1">CUL_ID=$(crictl create $PUL_ID co-nolim.yaml po-nolim.yaml)</span>&#13;
root@host01:/opt# <span class="codestrong1">crictl start $CUL_ID</span>&#13;
...&#13;
root@host01:/opt# <span class="codestrong1">crictl ps</span>&#13;
CONTAINER      IMAGE                                    ...&#13;
971e83927329e  docker.io/bookofkubernetes/stress:stable ...</pre>&#13;
<p class="indent">The <span class="literal">crictl ps</span> command is just to check that our container is running as expected.</p>&#13;
<p class="indent">The <span class="literal">stress</span> program is now running on our system, and we can see the current priority and CPU usage. We want the current CPU usage, so we’ll use <span class="literal">top</span>:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">top -b -n 1 -p $(pgrep -d , stress)</span>&#13;
top - 18:01:58 up  1:39,  1 user,  load average: 1.01, 0.40, 0.16&#13;
Tasks:   2 total,   1 running,   1 sleeping,   0 stopped,   0 zombie&#13;
%Cpu(s): 34.8 us, 0.0 sy, 0.0 ni, 65.2 id, 0.0 wa,  0.0 hi,  0.0 si,  0.0 st&#13;
MiB Mem :   1987.5 total,   1024.5 free,    195.8 used,    767.3 buff/cache&#13;
MiB Swap:      0.0 total,      0.0 free,      0.0 used.   1643.7 avail Mem &#13;
&#13;
  PID   USER  PR  NI  ...  %CPU  %MEM    TIME+ COMMAND&#13;
  13459 root  20   0  ... 100.0   0.2  0:29.78 stress-ng&#13;
  13435 root  20   0  ...   0.0   0.2  0:00.01 stress-ng</pre>&#13;
<p class="indent">The <span class="literal">pgrep</span> command looks up the process IDs (PIDs) for <span class="literal">stress</span>; there are two because <span class="literal">stress</span> forked a separate process for the CPU exercise we requested. This CPU worker is using up 100 percent of one CPU; fortunately, our VM has two CPUs, so it’s not overloaded.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_37"/>We started this process with default priority, so it has a nice value of <span class="literal">0</span>, as shown in the <span class="literal">NI</span> column. What happens if we change that priority? Let’s find out using <span class="literal">renice</span>:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">renice -n 19 -p $(pgrep -d ' ' stress)</span>&#13;
13435 (process ID) old priority 0, new priority 19&#13;
13459 (process ID) old priority 0, new priority 19</pre>&#13;
<p class="indent">The <span class="literal">ps</span> command used previously expected the PIDs to be separated with a comma, whereas the <span class="literal">renice</span> command expects the PIDs to be separated with a space; fortunately, <span class="literal">pgrep</span> can handle both.</p>&#13;
<p class="indent">We have successfully changed the priority of the process:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">top -b -n 1 -p $(pgrep -d , stress)</span>&#13;
top - 18:11:04 up  1:48,  1 user,  load average: 1.07, 0.95, 0.57&#13;
Tasks:   2 total,   1 running,   1 sleeping,   0 stopped,   0 zombie&#13;
%Cpu(s): 0.0 us, 0.0 sy, 28.6 ni, 71.4 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st&#13;
MiB Mem :   1987.5 total,   1035.6 free,    182.2 used,    769.7 buff/cache&#13;
MiB Swap:      0.0 total,      0.0 free,      0.0 used.   1657.2 avail Mem &#13;
&#13;
  PID   USER  PR  NI  ...  %CPU  %MEM     TIME+ COMMAND&#13;
  13459 root  39  19  ... 100.0   0.2   9:35.50 stress-ng&#13;
  13435 root  39  19  ...   0.0   0.2   0:00.01 stress-ng</pre>&#13;
<p class="indent">The new nice value is <span class="literal">19</span>, meaning that our process is lower priority than before. However, the <span class="literal">stress</span> program is still using 100 percent of one CPU! What’s going on here? The problem is that priority is only a relative measurement. If nothing else needs the CPU, as is true in this case, even a lower-priority process can use as much as it wants.</p>&#13;
<p class="indent">This arrangement may seem to be what we want. After all, if the CPU is available, shouldn’t we want our application components to be able to use it? Unfortunately, even though that sounds reasonable, it’s not suitable for our containerized applications for two main reasons. First, a container orchestration environment like Kubernetes works best when a container can be allocated to any host with enough resources to run it. It’s not reasonable for us to know the relative priority of every single container in our Kubernetes cluster, especially when we consider that a single Kubernetes cluster can be <em>multitenant</em>, meaning multiple separate applications or teams might be using a single cluster. Second, without some idea of how much CPU a particular container will use, Kubernetes cannot know which hosts are full and which ones have more room available. We don’t want to get into a situation in which multiple containers on the same host all become busy at the same time, because they will fight for the available CPU cores, and the whole host will slow down.</p>&#13;
<h3 class="h3" id="ch00lev1sec13">Linux Control Groups</h3>&#13;
<p class="noindent">As we saw in the last section, process prioritization will not help a container orchestration environment like Kubernetes know what host to use when <span epub:type="pagebreak" id="page_38"/>scheduling a new container, because even low-priority processes can get a lot of CPU time when the CPU is idle. And because our Kubernetes cluster might be multitenant, the cluster can’t just trust each container to promise to use only a certain amount of CPU. First, that would allow one process to affect another negatively, either maliciously or accidentally. Second, processes don’t really control their own scheduling; they get CPU time when the Linux kernel decides to give them CPU time. We need a different solution for controlling CPU utilization.</p>&#13;
<p class="indent">To find the answer, we can take an approach used by real-time processing. As we mentioned in the previous section, a real-time process is typically not compute intensive, but when it needs the CPU, it needs it immediately. To ensure that all real-time processes get the CPU they need, it is common to reserve a slice of the CPU time for each process. Even though our container processes are non-real-time, we can use the same strategy. If we can configure our containers so that they can use no more than their allocated slice of the CPU time, Kubernetes will be able to calculate how much space is available on each host and will be able to schedule containers onto hosts with sufficient space.</p>&#13;
<p class="indent">To manage container use of CPU cores, we will use <em>control groups</em>. Control groups (cgroups) are a feature of the Linux kernel that manage process resource utilization. Each resource type, such as CPU, memory, or a block device, can have an entire hierarchy of cgroups associated with it. After a process is in a cgroup, the kernel automatically applies the controls from that group.</p>&#13;
<p class="indent">The creation and configuration of cgroups is handled through a specific kind of filesystem, similar to the way that Linux reports information on the system through the <em>/proc</em> filesystem. By default, the filesystem for cgroups is located at <em>/sys/fs/cgroup</em>:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">ls /sys/fs/cgroup</span>&#13;
blkio        cpuacct  freezer  net_cls           perf_event  systemd&#13;
cpu          cpuset   hugetlb  net_cls,net_prio  pids        unified&#13;
cpu,cpuacct  devices  memory   net_prio          rdma</pre>&#13;
<p class="indent">Each of the entries in <em>/sys/fs/cgroup</em> is a different resource that can be limited. If we look in one of those directories, we can begin to see what controls can be applied. For example, for <em>cpu</em>:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">cd /sys/fs/cgroup/cpu</span>&#13;
root@host01:/sys/fs/cgroup/cpu# <span class="codestrong1">ls -F</span>&#13;
cgroup.clone_children  cpuacct.stat               cpuacct.usage_user&#13;
cgroup.procs           cpuacct.usage              init.scope/&#13;
cgroup.sane_behavior   cpuacct.usage_all          notify_on_release&#13;
cpu.cfs_period_us      cpuacct.usage_percpu       release_agent&#13;
<span epub:type="pagebreak" id="page_39"/>cpu.cfs_quota_us       cpuacct.usage_percpu_sys   system.slice/&#13;
cpu.shares             cpuacct.usage_percpu_user  tasks&#13;
cpu.stat               cpuacct.usage_sys          user.slice/</pre>&#13;
<p class="indent">The <span class="literal">-F</span> flag on <span class="literal">ls</span> adds a slash character to directories, which enables us to begin to see the hierarchy. Each of those subdirectories (<em>init.scope</em>, <em>system.slice</em>, and <em>user.slice</em>) is a separate CPU cgroup, and each has its own set of configuration files that apply to processes in that cgroup.</p>&#13;
<h4 class="h4" id="ch00lev2sec26">CPU Quotas with cgroups</h4>&#13;
<p class="noindent">To understand the contents of this directory, let’s see how we can use cgroups to limit the CPU usage of our <span class="literal">stress</span> container. We’ll begin by checking its CPU usage again:</p>&#13;
<pre>root@host01:/sys/fs/cgroup/cpu# <span class="codestrong1">top -b -n 1 -p $(pgrep -d , stress)</span>&#13;
top - 22:40:12 up 12 min,  1 user,  load average: 0.81, 0.35, 0.21&#13;
Tasks:   2 total,   1 running,   1 sleeping,   0 stopped,   0 zombie&#13;
%Cpu(s): 37.0 us, 0.0 sy, 0.0 ni, 63.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st&#13;
MiB Mem :   1987.5 total,   1075.1 free,    179.4 used,    733.0 buff/cache&#13;
MiB Swap:      0.0 total,      0.0 free,      0.0 used.   1646.3 avail Mem &#13;
&#13;
  PID USER   PR  NI ...  %CPU  %MEM     TIME+ COMMAND&#13;
  5964 root  20  19 ...  100.0  0.2   1:19.72 stress-ng&#13;
  5932 root  20  19 ...  0.0    0.2   0:00.02 stress-ng</pre>&#13;
<p class="indent">If you don’t still see <span class="literal">stress</span> running, start it up again using the commands from earlier in this chapter. Next, let’s explore what CPU cgroup our <span class="literal">stress</span> CPU process is in. We can do this by finding its PID inside a file within the <em>/sys/fs/cgroup/cpu</em> hierarchy:</p>&#13;
<pre>root@host01:/sys/fs/cgroup/cpu# <span class="codestrong1">grep -R $(pgrep stress-ng-cpu)</span>&#13;
system.slice/runc-050c.../cgroup.procs:5964&#13;
system.slice/runc-050c.../tasks:5964</pre>&#13;
<p class="indent">The <span class="literal">stress</span> process is part of the <em>system.slice</em> hierarchy, and is in a subdirectory created by <span class="literal">runc</span>, which is one of the internal components of CRI-O. This is really convenient, as it means we don’t need to create our own cgroup and move this process into it. It is also no accident; as we’ll see in a moment, CRI-O supports CPU limits on containers, so it naturally needs to create a cgroup for each container it runs. In fact, the cgroup is named after the container ID.</p>&#13;
<p class="indent">Let’s move into the directory for our container’s cgroup:</p>&#13;
<pre>root@host01:/sys/fs/cgroup/cpu# <span class="codestrong1">cd system.slice/runc-${CUL_ID}.scope</span></pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_40"/>We use the container ID variable we saved earlier to change into the appropriate directory. As soon as we’re in this directory, we can see that it has the same configuration files as the root of the hierarchy <em>/sys/fs/cgroup/cpu</em>:</p>&#13;
<pre>root@host01:/sys/fs/...07.scope# <span class="codestrong1">ls</span>&#13;
cgroup.clone_children  cpu.uclamp.max        cpuacct.usage_percpu_sys&#13;
cgroup.procs           cpu.uclamp.min        cpuacct.usage_percpu_user&#13;
cpu.cfs_period_us      cpuacct.stat          cpuacct.usage_sys&#13;
cpu.cfs_quota_us       cpuacct.usage         cpuacct.usage_user&#13;
cpu.shares             cpuacct.usage_all     notify_on_release&#13;
cpu.stat               cpuacct.usage_percpu  tasks</pre>&#13;
<p class="indent">The <em>cgroup.procs</em> file lists the processes in this control group:</p>&#13;
<pre>root@host01:/sys/fs/...07.scope# <span class="codestrong1">cat cgroup.procs</span>&#13;
5932&#13;
5964</pre>&#13;
<p class="indent">This directory has many other files, but we are mostly interested in three:</p>&#13;
<div class="bqparan">&#13;
<p class="noindent5"><strong><em>cpu.shares</em></strong> Slice of the CPU relative to this cgroup’s peers</p>&#13;
<p class="noindent5"><strong><em>cpu.cfs_period_us</em></strong> Length of a period, in microseconds</p>&#13;
<p class="noindent5"><strong><em>cpu.cfs_quota_us</em></strong> CPU time during a period, in microseconds</p>&#13;
</div>&#13;
<p class="indent">We’ll look at how Kubernetes uses <em>cpu.shares</em> in <a href="ch14.xhtml#ch14">Chapter 14</a>. For now, we need a way to get our instance under control so that it doesn’t overwhelm our system. To do that, we’ll set an absolute quota on this container. First, let’s see the value of <em>cpu.cfs_period_us</em>:</p>&#13;
<pre>root@host01:/sys/fs/...07.scope# <span class="codestrong1">cat cpu.cfs_period_us</span>&#13;
100000</pre>&#13;
<p class="indent">The period is set to 100,000 μs, or 0.1 seconds. We can use this number to figure out what quota to set in order to limit the amount of CPU the <span class="literal">stress</span> container can use. At the moment, there is no quota:</p>&#13;
<pre>root@host01:/sys/fs/...07.scope# <span class="codestrong1">cat cpu.cfs_quota_us</span>&#13;
-1</pre>&#13;
<p class="indent">We can set a quota by just updating the <em>cpu.cfs_quota_us</em> file:</p>&#13;
<pre>root@host01:/sys/fs/...07.scope# <span class="codestrong1">echo "50000" &gt; cpu.cfs_quota_us</span></pre>&#13;
<p class="indent">This provides the processes in this cgroup with 50,000 μs of CPU time per 100,000 μs, which averages out to 50 percent of a CPU. The processes are immediately affected, as we can confirm:</p>&#13;
<pre>root@host01:/sys/fs/...07.scope# <span class="codestrong1">top -b -n 1 -p $(pgrep -d , stress)</span>&#13;
top - 23:53:05 up  1:24,  1 user,  load average: 0.71, 0.93, 0.98&#13;
<span epub:type="pagebreak" id="page_41"/>Tasks:   2 total,   1 running,   1 sleeping,   0 stopped,   0 zombie&#13;
%Cpu(s):  0.0 us, 3.6 sy, 7.1 ni, 89.3 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st&#13;
MiB Mem :   1987.5 total,   1064.9 free,    174.6 used,    748.0 buff/cache&#13;
MiB Swap:      0.0 total,      0.0 free,      0.0 used.   1663.9 avail Mem &#13;
&#13;
  PID USER   PR  NI  ...  %CPU  %MEM     TIME+ COMMAND&#13;
  5964 root  39  19  ...  50.0   0.2  73:45.68 stress-ng-cpu&#13;
  5932 root  39  19  ...   0.0   0.2   0:00.02 stress-ng</pre>&#13;
<p class="indent">Your listing might not show exactly 50 percent CPU usage, because the period during which the <span class="literal">top</span> command measures CPU usage might not align perfectly with the kernel’s scheduling period. But on average, our <span class="literal">stress</span> container now cannot use more than 50 percent of one CPU.</p>&#13;
<p class="indent">Before we move on, let’s stop the <span class="literal">stress</span> container:</p>&#13;
<pre>root@host01:/sys/fs/...07.scope# <span class="codestrong1">cd</span>&#13;
root@host01:/opt# <span class="codestrong1">crictl stop $CUL_ID</span>&#13;
...&#13;
root@host01:/opt# <span class="codestrong1">crictl rm $CUL_ID</span>&#13;
...&#13;
root@host01:/opt# <span class="codestrong1">crictl stopp $PUL_ID</span>&#13;
Stopped sandbox ...&#13;
root@host01:/opt# <span class="codestrong1">crictl rmp $PUL_ID</span>&#13;
Removed sandbox ...</pre>&#13;
<h4 class="h4" id="ch00lev2sec27">CPU Quota with CRI-O and crictl</h4>&#13;
<p class="noindent">It would be tiresome to have to go through the process of finding the cgroup location in the filesystem and updating the CPU quota for every container in order to control CPU usage. Fortunately, we can specify the quota in our <span class="literal">crictl</span> YAML files, and CRI-O will enforce it for us. Let’s look at an example that was installed into <em>/opt</em> when we set up this example virtual machine.</p>&#13;
<p class="indent">The Pod configuration is only slightly different from <a href="ch03.xhtml#ch03list1">Listing 3-1</a>. We add a <span class="literal">cgroup_parent</span> setting so that we can control where CRI-O creates the cgroup, which will make it easier to find the cgroup to see the configuration:</p>&#13;
<p class="noindent6"><em>po-clim.yaml</em></p>&#13;
<pre>---&#13;
metadata:&#13;
  name: stress-clim&#13;
  namespace: crio&#13;
linux:&#13;
  cgroup_parent: pod.slice&#13;
  security_context:&#13;
    namespace_options:&#13;
      network: 2</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_42"/>The container configuration is where we include the CPU limits. Our <span class="literal">stress1</span> container will be allotted only 10 percent of a CPU:</p>&#13;
<p class="noindent6"><em>co-clim.yaml</em></p>&#13;
<pre>---&#13;
---&#13;
metadata:&#13;
  name: stress-clim&#13;
image:&#13;
  image: docker.io/bookofkubernetes/stress:stable&#13;
args:&#13;
  - "--cpu"&#13;
  - "1"&#13;
  - "-v"&#13;
linux:&#13;
  resources:&#13;
    cpu_period: 100000&#13;
    cpu_quota: 10000</pre>&#13;
<p class="indent">The value for <span class="literal">cpu_period</span> corresponds with the file <em>cpu.cfs_period_us</em> and provides the length of the period during which the quota applies. The value for <span class="literal">cpu_quota</span> corresponds with the file <em>cpu.cfs_quota_us</em>. Dividing the quota by the period, we can determine that this will set a CPU limit of 10 percent. Let’s go ahead and launch this <span class="literal">stress</span> container with its CPU limit:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">cd /opt</span>&#13;
root@host01:/opt# <span class="codestrong1">PCL_ID=$(crictl runp po-clim.yaml)</span>&#13;
root@host01:/opt# <span class="codestrong1">CCL_ID=$(crictl create $PCL_ID co-clim.yaml po-clim.yaml)</span>&#13;
root@host01:/opt# <span class="codestrong1">crictl start $CCL_ID</span>&#13;
...&#13;
root@host01:/opt# <span class="codestrong1">crictl ps</span>&#13;
CONTAINER      IMAGE                                    ...&#13;
ea8bccd711b86  docker.io/bookofkubernetes/stress:stable ...</pre>&#13;
<p class="indent">Our container is immediately restricted to 10 percent of a CPU:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">top -b -n 1 -p $(pgrep -d , stress)</span>&#13;
top - 17:26:55 up 19 min,  1 user,  load average: 0.27, 0.16, 0.13&#13;
Tasks:   4 total,   2 running,   2 sleeping,   0 stopped,   0 zombie&#13;
%Cpu(s): 10.3 us, 0.0 sy, 0.0 ni, 89.7 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st&#13;
MiB Mem :   1987.5 total,   1053.4 free,    189.3 used,    744.9 buff/cache&#13;
MiB Swap:      0.0 total,      0.0 free,      0.0 used.   1640.4 avail Mem &#13;
&#13;
  PID USER   PR  NI ... %CPU  %MEM     TIME+ COMMAND&#13;
  8349 root  20   0 ... 10.0   0.2   0:22.67 stress-ng&#13;
  8202 root  20   0 ...  0.0   0.2   0:00.02 stress-ng</pre>&#13;
<p class="indent">As in our earlier example, the CPU usage shown is a snapshot during the time that <span class="literal">top</span> was running, so it might not match the limit exactly, but over the long term, this process will use no more than its allocated CPU.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_43"/>We can inspect the cgroup to confirm that CRI-O put it in the place we specified and automatically configured the CPU quota:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">cd /sys/fs/cgroup/cpu/pod.slice</span>&#13;
root@host01:...pod.slice# <span class="codestrong1">cat crio-$CCL_ID.scope/cpu.cfs_quota_us</span>&#13;
10000</pre>&#13;
<p class="indent">CRI-O created a new cgroup parent <em>pod.slice</em> for our container, created a cgroup within it specific to the container, and configured its CPU quota without us having to lift a finger.</p>&#13;
<p class="indent">We don’t need this container any longer, so let’s remove it:</p>&#13;
<pre>root@host01:/sys/fs/cgroupcpu/pod.slice# <span class="codestrong1">cd</span>&#13;
root@host01:~# <span class="codestrong1">crictl stop $CCL_ID</span>&#13;
...&#13;
root@host01:~# <span class="codestrong1">crictl rm $CCL_ID</span>&#13;
...&#13;
root@host01:~# <span class="codestrong1">crictl stopp $PCL_ID</span>&#13;
Stopped sandbox ...&#13;
root@host01:~# <span class="codestrong1">crictl rmp $PCL_ID</span>&#13;
Removed sandbox ...</pre>&#13;
<p class="indent">With these commands we stop and then delete first the container, then the Pod.</p>&#13;
<h3 class="h3" id="ch00lev1sec14">Memory Limits</h3>&#13;
<p class="noindent">Memory is another important resource for a process. If a system doesn’t have sufficient memory to meet a request, the allocation of memory will fail. This usually causes the process to behave badly or to fail entirely. Of course, most Linux systems use <em>swap space</em> to write memory contents to disk temporarily, which allows the system memory to appear larger than it is but also reduces system performance. It’s a big enough concern that the Kubernetes team discourages having swap enabled in a cluster.</p>&#13;
<p class="indent">Also, even if we could use swap, we don’t want one process grabbing all the resident memory and making other processes very slow. As a result, we need to limit the memory usage of our processes so that they cooperate with one another. We also need to have a clear maximum for memory usage so that Kubernetes can reliably ensure that a host has enough available memory before scheduling a new container onto a host.</p>&#13;
<p class="indent">Linux systems, like other variants of Unix, have traditionally had to deal with multiple users who are sharing scarce resources. For this reason, the kernel supports limits on system resources, including CPU, memory, number of child processes, and number of open files. We can set these limits from the command line using the <span class="literal">ulimit</span> command. For example, one type of limit is a limit on “virtual memory.” This includes not only the amount of <span epub:type="pagebreak" id="page_44"/>RAM a process has in resident memory but also any swap space it is using. Here’s an example of a <span class="literal">ulimit</span> command limiting virtual memory:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">ulimit -v 262144</span></pre>&#13;
<p class="indent">The <span class="literal">-v</span> switch specifies a limit on virtual memory. The parameter is in bytes, so 262144 places a virtual memory limit of 256MiB on each additional process we start from this shell session. Setting a virtual memory limit is a total limit; it allows us to ensure that a process can’t use swap to get around the limit. We can verify the limit was applied by pulling some data into memory:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">cat /dev/zero | head -c 500m | tail</span>&#13;
tail: memory exhausted</pre>&#13;
<p class="indent">This command reads from <em>/dev/zero</em> and tries to keep the first 500MiB of zeros it finds in memory. However, at some point, when the <span class="literal">tail</span> command tries to allocate more space to hold the zeros it is getting from <span class="literal">head</span>, it fails because of the limit.</p>&#13;
<p class="indent">Thus, Unix limits give us the ability to control memory usage for our processes, but they won’t provide everything we need for containers, for a couple of reasons. First, Unix limits can be applied only to individual processes or to an entire user. Neither of those provide what we need, as a container is really a <em>group</em> of processes. A container’s initial process might create many child processes, and all processes in a container need to live within the same limit. At the same time, applying limits to an entire user doesn’t really help us in a container orchestration environment like Kubernetes, because from the perspective of the operating system, all of the containers belong to the same user. Second, when it comes to CPU limits, the only thing that regular Unix limits can do is limit the maximum CPU time our process gets before it is terminated. That isn’t the kind of limit we need for sharing the CPU between long-running processes.</p>&#13;
<p class="indent">Instead of using traditional Unix limits, we’ll use cgroups again, this time to limit the memory available to a process. We’ll use the same <span class="literal">stress</span> container image, this time with a child process that tries to allocate lots of memory.</p>&#13;
<p class="indent">If we were to try to apply a memory limit to this <span class="literal">stress</span> container after starting it, we would find that the kernel won’t let us, because it will have already grabbed too much memory. So instead we’ll apply it immediately in the YAML configuration. As before, we need a Pod:</p>&#13;
<p class="noindent6"><em>po-mlim.yaml</em></p>&#13;
<pre>---&#13;
metadata:&#13;
  name: stress2&#13;
  namespace: crio&#13;
linux:&#13;
  cgroup_parent: pod.slice&#13;
  security_context:&#13;
    namespace_options:&#13;
      network: 2</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_45"/>This is identical to the Pod we used for CPU limit, but the name is different to avoid a collision. As we did earlier, we are asking CRI-O to put the cgroup into <em>pod.slice</em> so that we can find it easily.</p>&#13;
<p class="indent">We also need a container definition:</p>&#13;
<p class="noindent6"><em>co-mlim.yaml</em></p>&#13;
<pre> ---&#13;
 ---&#13;
 metadata:&#13;
   name: stress2&#13;
 image:&#13;
   image: docker.io/bookofkubernetes/stress:stable&#13;
 args:&#13;
   - "--vm"&#13;
   - "1"&#13;
   - "--vm-bytes"&#13;
<span class="ent">➊</span> - "512M"&#13;
   - "-v"&#13;
 linux:&#13;
   resources:&#13;
  <span class="ent">➋</span> memory_limit_in_bytes: 268435456&#13;
     cpu_period: 100000 &#13;
  <span class="ent">➌</span> cpu_quota: 10000</pre>&#13;
<p class="indent">The new resource limit is <span class="literal">memory_limit_in_bytes</span>, which we set to 256MiB <span class="ent">➋</span>. We keep the CPU quota in there <span class="ent">➌</span> because continuously trying to allocate memory is going to use a lot of CPU. Finally, in the <span class="literal">args</span> section, we tell <span class="literal">stress</span> to try to allocate 512MB of memory <span class="ent">➊</span>.</p>&#13;
<p class="indent">We can run this using similar <span class="literal">crictl</span> commands to what we’ve previously used:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">cd /opt</span> &#13;
root@host01:/opt# <span class="codestrong1">PML_ID=$(crictl runp po-mlim.yaml)</span>&#13;
root@host01:/opt# <span class="codestrong1">CML_ID=$(crictl create $PML_ID co-mlim.yaml po-mlim.yaml)</span>&#13;
root@host01:/opt# <span class="codestrong1">crictl start $CML_ID</span>&#13;
...</pre>&#13;
<p class="indent">If we tell <span class="literal">crictl</span> to list containers, everything seems okay:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">crictl ps</span>&#13;
CONTAINER     IMAGE                                    ... STATE   ...&#13;
31025f098a6c9 docker.io/bookofkubernetes/stress:stable ... Running ...</pre>&#13;
<p class="indent">This reports that the container is in a <span class="literal">Running</span> state. However, behind the scenes, <span class="literal">stress</span> is struggling to allocate memory. We can see this if we print out the log messages coming from the <span class="literal">stress</span> container:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">crictl logs $CML_ID</span>&#13;
...&#13;
stress-ng: info:  [6] dispatching hogs: 1 vm&#13;
...&#13;
stress-ng: debug: [11] stress-ng-vm: started [11] (instance 0)&#13;
stress-ng: debug: [11] stress-ng-vm using method 'all'&#13;
stress-ng: debug: [11] stress-ng-vm: child died: signal 9 'SIGKILL' (instance 0)&#13;
stress-ng: debug: [11] stress-ng-vm: assuming killed by OOM killer, restarting again...&#13;
stress-ng: debug: [11] stress-ng-vm: child died: signal 9 'SIGKILL' (instance 0)&#13;
stress-ng: debug: [11] stress-ng-vm: assuming killed by OOM killer, restarting again...</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_46"/>Stress is reporting that its memory allocation process is being continuously killed by the “out of memory.”</p>&#13;
<p class="indent">And we can see the kernel reporting that the <span class="literal">oom_reaper</span> is indeed the reason that the processes are being killed:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">dmesg | grep -i oom_reaper | tail -n 1</span>&#13;
[  696.651056] oom_reaper: reaped process 8756 (stress-ng-vm)...</pre>&#13;
<p class="indent">The <span class="literal">OOM killer</span> is the same feature Linux uses when the whole system is low on memory and it needs to kill one or more processes to protect the system. In this case, it is sending <span class="literal">SIGKILL</span> to the process to keep the cgroup under its memory limit. <span class="literal">SIGKILL</span> is a message to the process that it should immediately terminate without any cleanup.</p>&#13;
<div class="box5">&#13;
<p class="boxtitle-d"><strong>WHY USE THE OOM KILLER?</strong></p>&#13;
<p class="noindents">When we used regular limits to control memory, an attempt to exceed our limits caused the memory allocation to fail, but the kernel didn’t use the OOM killer to kill our process. Why the difference? The answer is that this is the nature of containers. As we look at architecting reliable systems using containerized microservices, we’ll see that a container is supposed to be quick to start and quick to scale. This means that each individual container in our application is intentionally just not very important. This further means that the idea that one of our containers could be killed unexpectedly is not really a concern. Add to that the fact that not checking for memory allocation errors is one of the most common bugs, so it’s considered safer simply to kill the process.</p>&#13;
<p class="noindents">That said, it’s worth noting that it is possible to turn off the OOM killer for a cgroup. However, rather than having the memory allocation fail, the effect is to just pause the process until other processes in the group free up memory. That’s actually worse, as now we have a process that isn’t officially killed but isn’t doing anything useful either.</p>&#13;
</div>&#13;
<p class="indent">Before we move on, let’s put this continuously failing <span class="literal">stress</span> container out of its misery:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">crictl stop $CML_ID</span>&#13;
...&#13;
root@host01:/opt# <span class="codestrong1">crictl rm $CML_ID</span>&#13;
...&#13;
root@host01:/opt# <span class="codestrong1">crictl stopp $PML_ID</span>&#13;
Stopped sandbox ...&#13;
<span epub:type="pagebreak" id="page_47"/>root@host01:/opt# <span class="codestrong1">crictl rmp $PML_ID</span>&#13;
Removed sandbox ...&#13;
root@host01:/opt# <span class="codestrong1">cd</span></pre>&#13;
<p class="indent">Stopping and removing the container and Pod prevents the <span class="literal">stress</span> container from wasting CPU by continually trying to restart the memory allocation process.</p>&#13;
<h3 class="h3" id="ch00lev1sec15">Network Bandwidth Limits</h3>&#13;
<p class="noindent">In this chapter, we’ve moved from resources that are easy to limit to resources that are more difficult to limit. We started with CPU, where the kernel is wholly in charge of which process gets CPU time and how much time it gets before being preempted. Then we looked at memory, where the kernel doesn’t have the ability to force a process to give up memory, but at least the kernel can control whether a memory allocation is successful, or it can kill a process that requests too much memory.</p>&#13;
<p class="indent">Now we’re moving on to network bandwidth, for which control is even more difficult to exert for two important reasons. First, network devices don’t really “sum up” like CPU or memory, so we’ll need to limit usage at the level of each individual network device. Second, our system can’t really control what is sent to it across the network; we can only completely control <em>egress</em> bandwidth, the traffic that is sent on a given network device.</p>&#13;
<div class="box5">&#13;
<p class="boxtitle-d"><strong>PROPER NETWORK MANAGEMENT</strong></p>&#13;
<p class="noindents">To have a completely reliable cluster, merely controlling egress traffic is clearly insufficient. A process that downloads a large file is going to saturate the available bandwidth just as much as one that uploads lots of data. However, we really can’t control what comes into our host via a given network interface, at least not at the host level. If we really want to manage network bandwidth, we need to handle that kind of thing at a switch or a router. For example, it is very common to divide up the physical network into virtual local area networks (VLANs). One VLAN might be an administration network used for auditing, logging, and for administrators to ensure that they can log in. We might also reserve another VLAN for important container traffic, or use traffic shaping to ensure that important packets get through. As long as we perform this kind of configuration at the switch, we can typically allow the remaining bandwidth to be “best effort.”</p>&#13;
</div>&#13;
<p class="indent">Although Linux does provide some cgroup capability for network interfaces, these would only help us prioritize and classify network traffic. For this reason, rather than using cgroups to control egress traffic, we’re going to directly configure the Linux kernel’s <em>traffic control</em> capabilities. We’ll test network performance using <span class="literal">iperf3</span>, apply a limit to outgoing traffic, and then test again. In this chapter’s examples, <em>host02</em> with IP address <span class="literal">192.168.61.12</span> was set up automatically with an <span class="literal">iperf3</span> server running so that we can send data to it from <em>host01</em>.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_48"/>Let’s begin by seeing the egress bandwidth we can get on an unlimited interface:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">iperf3 -c 192.168.61.12</span>&#13;
Connecting to host 192.168.61.12, port 5201&#13;
[  5] local 192.168.61.11 port 49044 connected to 192.168.61.12 port 5201&#13;
...&#13;
[ ID] Interval           Transfer     Bitrate         Retr&#13;
[  5]   0.00-10.00  sec  2.18 GBytes  1.87 Gbits/sec  13184             sender&#13;
[  5]   0.00-10.00  sec  2.18 GBytes  1.87 Gbits/sec                  receiver&#13;
...</pre>&#13;
<p class="indent">This example shows gigabit network speeds. Depending on how you’re running the examples, you might see lower or higher figures. Now that we have a baseline, we can use <span class="literal">tc</span> to set a quota going out. You’ll want to choose a quota that makes sense given your bandwidth; most likely enforcing a 100Mb cap will work:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">IFACE=$(ip -o addr | grep 192.168.61.11 | awk '{print $2}')</span>&#13;
root@host01:~# <span class="codestrong1">tc qdisc add dev $IFACE root tbf rate 100mbit \</span>&#13;
  <span class="codestrong1">burst 256kbit latency 400ms</span></pre>&#13;
<p class="indent">The name of the network interface may be different on different systems, so we use <span class="literal">ip addr</span> to identify which interface we want to control. Then, we use <span class="literal">tc</span> to actually apply the limit. The token <span class="literal">tbf</span> in the command stands for <em>token bucket filter</em>. With a token bucket filter, every packet consumes tokens. The bucket refills with tokens over time, but if at any point the bucket is empty, packets are queued until tokens are available. By controlling the size of the bucket and the rate at which it refills, it is very easy for the kernel to place a bandwidth limit.</p>&#13;
<p class="indent">Now that we’ve applied a limit to this interface, let’s see it in action by running the exact same <span class="literal">iperf3</span> command again:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">iperf3 -c 192.168.61.12</span>&#13;
Connecting to host 192.168.61.12, port 5201&#13;
[  5] local 192.168.61.11 port 49048 connected to 192.168.61.12 port 5201&#13;
...&#13;
[ ID] Interval           Transfer     Bitrate         Retr&#13;
[  5]   0.00-10.00  sec   114 MBytes  95.7 Mbits/sec    0             sender&#13;
[  5]   0.00-10.01  sec   113 MBytes  94.5 Mbits/sec                  receiver&#13;
...</pre>&#13;
<p class="indent">As expected, we are now limited to 100Mbps on this interface.</p>&#13;
<p class="indent">Of course, in this case, we limited the bandwidth available on this network interface for everyone on the system. To use this ability properly to control bandwidth usage, we need to target the limits more precisely. However, in order to do that, we need to isolate a process to its own set of network interfaces, which is the subject of the next chapter.</p>&#13;
<h3 class="h3" id="ch00lev1sec16"><span epub:type="pagebreak" id="page_49"/>Final Thoughts</h3>&#13;
<p class="noindent">Ensuring that a process doesn’t cause problems for other processes on the system includes making sure that it fairly shares system resources such as CPU, memory, and network bandwidth. In this chapter, we looked at how Linux provides control groups (cgroups) that manage CPU and memory limits and traffic control capabilities that manage network interfaces. As we create a Kubernetes cluster and deploy containers to it, we’ll see how Kubernetes uses these underlying Linux kernel features to ensure that containers are scheduled on hosts with sufficient resources and that containers are well behaved on those hosts.</p>&#13;
<p class="indent">We’ve now moved through some of the most important elements of process isolation provided by a container runtime, but there are two types of isolation that we haven’t explored yet: network isolation and storage isolation. In the next chapter, we’ll look at how Linux network namespaces are used to make each container appear to have its own set of network interfaces, complete with separate IP addresses and ports. We’ll also look at how traffic from those separate container interfaces flows through our system so that containers can talk to one another and to the rest of the network.<span epub:type="pagebreak" id="page_50"/></p>&#13;
</body></html>