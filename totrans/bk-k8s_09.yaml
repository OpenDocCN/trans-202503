- en: '7'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DEPLOYING CONTAINERS TO KUBERNETES
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/common01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We’re now ready to begin running containers on our working Kubernetes cluster.
    Because Kubernetes has a declarative API, we’ll create various kinds of resources
    to run them, and we’ll monitor the cluster to see what Kubernetes does for each
    type of resource.
  prefs: []
  type: TYPE_NORMAL
- en: Different containers have different use cases. Some might require multiple identical
    instances with autoscaling to perform well under load. Other containers might
    exist solely to run a one-time command. Still others may require a fixed ordering
    to enable selecting a single primary instance and providing controlled failover
    to a secondary instance. Kubernetes provides different *controller* resource types
    for each of those use cases. We’ll look at each in turn, but we’ll begin with
    the most fundamental of them, the *Pod*, which is utilized by all of those use
    cases.
  prefs: []
  type: TYPE_NORMAL
- en: Pods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A Pod is the most basic resource in Kubernetes and is how we run containers.
    Each Pod can have one or more containers within it. The Pod is used to provide
    the process isolation we saw in [Chapter 2](ch02.xhtml#ch02). Linux kernel namespaces
    are used at the Pod and the container level:'
  prefs: []
  type: TYPE_NORMAL
- en: 'mnt Mount points: each container has its own root filesystem; other mounts
    are available to all containers in the Pod.'
  prefs: []
  type: TYPE_NORMAL
- en: 'uts Unix time sharing: isolated at the Pod level.'
  prefs: []
  type: TYPE_NORMAL
- en: 'ipc Interprocess communication: isolated at the Pod level.'
  prefs: []
  type: TYPE_NORMAL
- en: 'pid Process identifiers: isolated at the container level.'
  prefs: []
  type: TYPE_NORMAL
- en: 'net Network: isolated at the Pod level.'
  prefs: []
  type: TYPE_NORMAL
- en: The biggest advantage of this approach is that multiple containers can act like
    processes on the same virtual host, using the `localhost` address to communicate,
    while still being based on separate container images.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a Pod
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To get started, let’s create a Pod directly. Unlike the previous chapter, in
    which we used `kubectl run` to have the Pod specification created for us, we’ll
    specify it directly using YAML so that we have complete control over the Pod and
    to prepare us for using controllers to create Pods, providing scalability and
    failover.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*The example repository for this book is at* [https://github.com/book-of-kubernetes/examples](https://github.com/book-of-kubernetes/examples).
    *See “Running Examples” on [page xx](ch00.xhtml#ch00lev1sec2) for details on getting
    set up.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The automation script for this chapter does a full cluster install with three
    nodes that run the control plane and regular applications, providing the smallest
    possible highly available cluster for testing. The automation also creates some
    YAML files for Kubernetes resources. Here’s a basic YAML resource to create a
    Pod running NGINX:'
  prefs: []
  type: TYPE_NORMAL
- en: '*nginx-pod.yaml*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Pods are part of the *core* Kubernetes API, so we just specify a version number
    of `v1` for the `apiVersion`. Specifying `Pod` as the `kind` tells Kubernetes
    exactly what resource we’re creating in the API group. We will see these fields
    in all of our Kubernetes resources.
  prefs: []
  type: TYPE_NORMAL
- en: The `metadata` field has many uses. For the Pod, we just need to provide the
    one required field of `name`. We don’t specify the `namespace` in the metadata,
    so by default this Pod will end up in the `default` Namespace.
  prefs: []
  type: TYPE_NORMAL
- en: The remaining field, `spec`, tells Kubernetes everything it needs to know to
    run this Pod. For now we are providing the minimal information, which is a list
    of containers to run, but many other options are available. In this case, we have
    only one container, so we provide just the name and container image Kubernetes
    should use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s add this Pod to the cluster. The automation added files to */opt*, so
    we can do it from `host01` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In [Listing 7-1](ch07.xhtml#ch07list1), we can check the Pod’s status.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 7-1: Status of NGINX*'
  prefs: []
  type: TYPE_NORMAL
- en: It can take some time before the Pod shows `Running`, especially if you just
    set up your Kubernetes cluster and it’s still busy deploying core components.
    Keep trying this `kubectl` command to check the status.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of typing the `kubectl` command multiple times, you can also use `watch`.
    The `watch` command is a great way to observe changes in your cluster over time.
    Just add `watch` in front of your command, and it will be run for you every two
    seconds.
  prefs: []
  type: TYPE_NORMAL
- en: 'We added `-o wide` to the command to see the IP address and node assignment
    for this Pod. Kubernetes manages that for us. In this case, the Pod was scheduled
    on `host03`, so we need to go there to see the running container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Run this command on whatever host your NGINX Pod is on.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we collect the Pod ID, we can see the container as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This output looks very similar to the output from `kubectl get` in [Listing
    7-1](ch07.xhtml#ch07list1), which is not surprising given that our cluster gets
    that information from the `kubelet` service running on this node, which in turn
    uses the same Container Runtime Interface (CRI) API that `crictl` is also using
    to talk to the container engine.
  prefs: []
  type: TYPE_NORMAL
- en: Pod Details and Logging
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The ability to use `crictl` with the underlying container engine to explore
    a container running in the cluster is valuable, but it does require us to connect
    to the specific host running the container. Much of the time, we can avoid that
    by using `kubectl` commands to inspect Pods from anywhere by connecting to our
    cluster’s API server. Let’s move back to `host01` and explore the NGINX Pod further.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [Chapter 6](ch06.xhtml#ch06), we saw how we could use `kubectl describe`
    to see the status and event log for a cluster node. We can use the same command
    to see the status and configuration details of other Kubernetes resources. Here’s
    the event log for our NGINX Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We can use `kubectl describe` with many different Kubernetes resources, so we
    first tell `kubectl` that we are interested in a Pod and provide the name. Because
    we didn’t specify a Namespace, Kubernetes will look for this Pod in the `default`
    Namespace ➊.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*We use the default Namespace for most of the examples in this book to save
    typing, but it’s a good practice to use multiple Namespaces to keep applications
    separate, both to avoid naming conflicts and to manage access control. We look
    at Namespaces in more detail in [Chapter 11](ch11.xhtml#ch11).*'
  prefs: []
  type: TYPE_NORMAL
- en: The `kubectl describe` command output provides an event log ➋, which is the
    first place to look for issues when we have problems starting a container.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes takes a few steps when deploying a container. First, it needs to
    schedule it onto a node, which requires that node to be available with sufficient
    resources. Then, control passes to `kubelet` on that node, which has to interact
    with the container engine to pull the image, create a container, and start it.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the container is started, `kubelet` collects the standard out and standard
    error. We can view this output by using the `kubectl logs` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `kubectl logs` command always refers to a Pod because Pods are the basic
    resource used to run containers, and our Pod has only one container, so we can
    just specify the name of the Pod as a single parameter to `kubectl logs`. As before,
    Kubernetes will look in the `default` Namespace because we didn’t specify the
    Namespace.
  prefs: []
  type: TYPE_NORMAL
- en: The container output is available even if the container has exited, so the `kubectl
    logs` command is the place to look if a container is pulled and started successfully
    but then crashes. Of course, we have to hope that the container printed a log
    message explaining why it crashed. In [Chapter 10](ch10.xhtml#ch10), we look at
    what to do if we can’t get a container going and don’t have any log messages.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’re done with the NGINX Pod, so let’s clean it up:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We can use the same YAML configuration file to delete the Pod, which is convenient
    when we have multiple Kubernetes resources defined in a single file, as a single
    command will delete all of them. The `kubectl` command uses the name of each resource
    defined in the file to perform the delete.
  prefs: []
  type: TYPE_NORMAL
- en: Deployments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To run a container, we need a Pod, but that doesn’t mean we generally want to
    create the Pod directly. When we create a Pod directly, we don’t get all of the
    scalability and failover that Kubernetes offers, because Kubernetes will run only
    one instance of the Pod. This Pod will be allocated to a node only on creation,
    with no re-allocation even if the node fails.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get scalability and failover, we instead need to create a controller to
    manage the Pod for us. We’ll look at multiple controllers that can run Pods, but
    let’s start with the most common: the *Deployment*.'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Deployment
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A Deployment manages one or more *identical* Kubernetes Pods. When we create
    a Deployment, we provide a Pod template. The Deployment then creates Pods matching
    that template with the help of a *ReplicaSet*.
  prefs: []
  type: TYPE_NORMAL
- en: '**DEPLOYMENTS AND REPLICASETS**'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes has evolved its controller resources over time. The first type of
    controller, the *ReplicationController*, provided only basic functionality. It
    was replaced by the ReplicaSet, which has improvements in how it identifies which
    Pods to manage.
  prefs: []
  type: TYPE_NORMAL
- en: Part of the reason to replace ReplicationControllers with ReplicaSets is that
    ReplicationControllers were becoming more and more complicated, making the code
    difficult to maintain. The new approach splits up controller responsibility between
    ReplicaSets and Deployments. ReplicaSets are responsible for basic Pod management,
    including monitoring Pod status and performing failover. Deployments are responsible
    for tracking changes to the Pod template caused by configuration changes or container
    image updates. Deployments and ReplicaSets work together, but the Deployment creates
    its own ReplicaSet, so we usually need to interact only with Deployments. For
    this reason, I use the term *Deployment* generically to refer to features provided
    by the ReplicaSet, such as monitoring Pods to provide the requested number of
    replicas.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the YAML file we’ll use to create an NGINX Deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '*nginx-deploy.yaml*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Deployments are in the `apps` API group, so we specify `apps/v1` for `apiVersion`.
    Like every Kubernetes resource, we need to provide a unique name ➊ to keep this
    Deployment separate from any others we might create.
  prefs: []
  type: TYPE_NORMAL
- en: The Deployment specification has a few important fields, so let’s look at them
    in detail. The `replicas` field tells Kubernetes how many identical instances
    of the Pod we want. Kubernetes will work to keep this many Pods running. The next
    field, `selector`, is used to enable the Deployment to find its Pods. The content
    of `matchLabels` must exactly match the content in the `template.metadata.labels`
    field ➋, or Kubernetes will reject the Deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the content of `template.spec` ➌ will be used as the `spec` for any
    Pods created by this Deployment. The fields here can include any configuration
    we can provide for a Pod. This configuration matches *nginx-pod.yaml* that we
    looked at earlier except that we add a CPU resource request ➍ so that we can configure
    autoscaling later on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create our Deployment from this YAML resource file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We can track the status of the Deployment with `kubectl get`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'When the Deployment is fully up, it will report that it has three replicas
    ready and available, which means that we now have three separate NGINX Pods managed
    by this Deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The name of each Pod begins with the name of the Deployment. Kubernetes adds
    some random characters to build the name of the ReplicaSet, followed by more random
    characters so that each Pod has a unique name. We don’t need to create or manage
    the ReplicaSet directly, but we can use `kubectl get` to see it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Although we generally interact only with Deployments, it is important to know
    about the ReplicaSet, as some specific errors encountered when creating Pods are
    only reported in the ReplicaSet event log.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `nginx` prefix on the ReplicaSet and Pod names are purely for convenience.
    The Deployment does not use names to match itself to Pods. Instead, it uses its
    selector to match the labels on the Pod. We can see these labels if we run `kubectl
    describe` on one of the three Pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This matches the Deployment’s selector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The Deployment queries the API server to identify Pods matching its selector.
    Whereas the Deployment uses the programmatic API, the `kubectl get` command in
    the following example generates a similar API server query, giving us an opportunity
    to see how that works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Using `kubectl get all` in this case allows us to list multiple different kinds
    of resources as long as they match the selector. As a result, we see not only
    the three Pods but also the ReplicaSet that was created by the Deployment to manage
    those Pods.
  prefs: []
  type: TYPE_NORMAL
- en: It may seem strange that the Deployment uses a selector rather than just tracking
    the Pods it created. However, this design makes it easier for Kubernetes to be
    self-healing. At any time, a Kubernetes node might go offline, or we might have
    a network split, during which some control nodes lose their connection to the
    cluster. If a node comes back online, or the cluster needs to recombine after
    a network split, Kubernetes must be able to look at the current state of all of
    the running Pods and figure out what changes are required to achieve the desired
    state. This might mean that a Deployment that started an additional Pod as the
    result of a node disconnection would need to shut down a Pod when that node reconnects
    so that the cluster can maintain the appropriate number of replicas. Using a selector
    avoids the need for the Deployment to remember all the Pods it has ever created,
    even Pods on failed nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring and Scaling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Because the Deployment is monitoring its Pods to make sure we have the correct
    number of replicas, we can delete a Pod, and it will be automatically re-created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'As soon as the old Pod is deleted, the Deployment created a new Pod ➊. Similarly,
    if we change the number of replicas for the Deployment, Pods are automatically
    updated. Let’s add another replica:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The first command sets the number of replicas to four. As a result, Kubernetes
    needs to start a new identical Pod to meet the number we requested ➊. We can scale
    the Deployment by updating the YAML file and re-running `kubectl apply`, or we
    can use the `kubectl scale` command to edit the Deployment directly. Either way,
    this is a declarative approach; we are updating the Deployment’s resource declaration;
    Kubernetes then updates the actual state of the cluster to match.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, scaling the Deployment down causes Pods to be automatically deleted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: When we scale down, Kubernetes selects two Pods to terminate. These Pods take
    a moment to finish shutting down, at which point we have only two NGINX Pods running.
  prefs: []
  type: TYPE_NORMAL
- en: Autoscaling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For an application that is receiving real requests from users, we would choose
    the number of replicas necessary to provide a quality application, while scaling
    down when possible to reduce the amount of resources used by our application.
    Of course, the load on our application is constantly changing, and it would be
    tedious to monitor each component of our application continually to scale it independently.
    Instead, we can have the cluster perform the monitoring and scaling for us using
    a *HorizontalPodAutoscaler*. The term *horizontal* in this case just refers to
    the fact that the autoscaler can update the number of replicas of the same Pod
    managed by a controller.
  prefs: []
  type: TYPE_NORMAL
- en: 'To configure autoscaling, we create a new resource with a reference to our
    Deployment. The cluster then monitors resources used by the Pods and reconfigures
    the Deployment as needed. We could add a HorizontalPodAutoscaler to our Deployment
    using the `kubectl autoscale` command, but using a YAML resource file so that
    we can keep the autoscale configuration under version control is better. Here’s
    the YAML file:'
  prefs: []
  type: TYPE_NORMAL
- en: '*nginx-scaler.yaml*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `metadata` field, we add the label `app: nginx`. This does not change
    the behavior of the resource; its only purpose is to ensure that this resource
    shows up if we use an `app=nginx` label selector in a `kubectl get` command. This
    style of tagging the components of an application with consistent metadata is
    a good practice to help others understand what resources go together and to make
    debugging easier.'
  prefs: []
  type: TYPE_NORMAL
- en: This YAML configuration uses version 2 of the autoscaler configuration ➊. Providing
    new versions of API resource groups is how Kubernetes accommodates future capability
    without losing any of its backward compatibility. Generally, alpha and beta versions
    are released for new resource groups before the final configuration is released,
    and there is at least one version of overlap between the beta version and the
    final release to enable seamless upgrades.
  prefs: []
  type: TYPE_NORMAL
- en: Version 2 of the autoscaler supports multiple resources. Each resource is used
    to calculate a vote on the desired number of Pods, and the largest number wins.
    Adding support for multiple resources requires a change in the YAML layout, which
    is a common reason for the Kubernetes maintainers to create a new resource version.
  prefs: []
  type: TYPE_NORMAL
- en: We specify our NGINX Deployment ➋ as the target for the autoscaler using its
    API resource group, kind, and name, which is enough to uniquely identify any resource
    in a Kubernetes cluster. We then tell the autoscaler to monitor the CPU utilization
    of the Pods that belong to the Deployment ➍. The autoscaler will work to keep
    average CPU utilization by the Pods close to 50 percent over the long run, scaling
    up or down as necessary. However, the number of replicas will never go beyond
    the range we specify ➌.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create our autoscaler using this configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We can query the cluster to see that it was created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The output shows the autoscaler’s target reference, the current and desired
    resource utilization, and the maximum, minimum, and current number of replicas.
  prefs: []
  type: TYPE_NORMAL
- en: We use `hpa` as an abbreviation for `horizontalpodautoscaler`. Kubernetes allows
    us to use either singular or plural names and provides abbreviations for most
    of its resources to save typing. For example, we can type `deploy` for `deployment`
    and even `po` for `pods`. Every extra keystroke counts!
  prefs: []
  type: TYPE_NORMAL
- en: 'The autoscaler uses CPU utilization data that the `kubelet` is already collecting
    from the container engine. This data is centralized by the metrics server we installed
    as a cluster add-on. Without that cluster add-on, there would be no utilization
    data, and the autoscaler would not make any changes to the Deployment. In this
    case, because we’re not really using our NGINX server instances, they aren’t consuming
    any CPU, and the Deployment is scaled down to a single Pod, the minimum we specified:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The autoscaler has calculated that only one Pod is needed and has scaled the
    Deployment to match. The Deployment then selected a Pod to terminate to reach
    the desired scale.
  prefs: []
  type: TYPE_NORMAL
- en: For accuracy, the autoscaler will not use CPU data from the Pod if it recently
    started running, and it has logic to prevent it from scaling up or down too often,
    so if you ran through these examples very quickly you might need to wait a few
    minutes before you see it scale.
  prefs: []
  type: TYPE_NORMAL
- en: We explore Kubernetes resource utilization metrics in more detail when we look
    at limiting resource usage in [Chapter 14](ch14.xhtml#ch14).
  prefs: []
  type: TYPE_NORMAL
- en: Other Controllers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deployments are the most generic and commonly used controller, but Kubernetes
    has some other useful options. In this section, we explore *Job*s and *CronJob*s,
    *StatefulSets*, and *DaemonSets*.
  prefs: []
  type: TYPE_NORMAL
- en: Jobs and CronJobs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Deployments are great for application components because we usually want one
    or more instances to stay running indefinitely. However, for cases for which we
    need to run a command, either once or on a schedule, we can use a Job. The primary
    difference is a Deployment ensures that any container that stops running is restarted,
    whereas a Job can check the exit code of the main process and restart only if
    the exit code is non-zero, indicating failure.
  prefs: []
  type: TYPE_NORMAL
- en: 'A Job definition looks very similar to a Deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '*sleep-job.yaml*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The `restartPolicy` can be set to `OnFailure`, in which case the container will
    be restarted for a non-zero exit code, or to `Never`, in which case the Job will
    be completed when the container exits regardless of the exit code.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create and view the Job and the Pod it has created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The Job has created a Pod per the specification provided in the YAML file. The
    Job reflects `0/1` completions because it is waiting for its Pod to exit successfully.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the Pod has been running for 30 seconds, it exits with a code of zero,
    indicating success, and the Job and Pod status are updated accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The Pod is still available, which means that we could review its logs if desired,
    but it shows a status of `Completed`, so Kubernetes will not try to restart the
    exited container.
  prefs: []
  type: TYPE_NORMAL
- en: 'A CronJob is a controller that creates Jobs on a schedule. For example, we
    could set up our sleep Job to run once per day:'
  prefs: []
  type: TYPE_NORMAL
- en: '*sleep-cronjob.yaml*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The entire contents of the Job specification are embedded inside the `jobTemplate`
    field ➋. To this, we add a `schedule` ➊ that follows the standard format for the
    Unix `cron` command. In this case, `0 3 * * *` indicates that a Job should be
    created at 3:00 AM every day.
  prefs: []
  type: TYPE_NORMAL
- en: One of Kubernetes’ design principles is that anything could go down at any time.
    For a CronJob, if the cluster has an issue during the time the Job would be scheduled,
    the Job might not be scheduled, or it might be scheduled twice, this means that
    you should take care to write Jobs in an idempotent way so that they can handle
    missing or duplicated scheduling.
  prefs: []
  type: TYPE_NORMAL
- en: If we create this CronJob
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'it now exists in the cluster, but it does not immediately create a Job or a
    Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Instead, the CronJob will create a new Job each time its schedule is triggered.
  prefs: []
  type: TYPE_NORMAL
- en: StatefulSets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: So far, we’ve been looking at controllers that create identical Pods. With both
    Deployments and Jobs, we don’t really care which Pod is which, or where it is
    deployed, as long as we run enough instances at the right time. However, that
    doesn’t always match the behavior we want. For example, even though a Deployment
    can create Pods with persistent storage, the storage must either be brand new
    for each new Pod, or the same storage must be shared across all Pods. That doesn’t
    align well with a “primary and secondary” architecture such as a database. For
    those cases, we want specific storage to be attached to specific Pods.
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, because Pods can come and go due to hardware failures or upgrades,
    we need a way to manage the replacement of a Pod so that each Pod is attached
    to the right storage. This is the purpose of a *StatefulSet*. A StatefulSet identifies
    each Pod with a number, starting at zero, and each Pod receives matching persistent
    storage. When a Pod must be replaced, the new Pod is assigned the same numeric
    identifier and is attached to the same storage. Pods can look at their hostname
    to determine their identifier, so a StatefulSet is useful both for cases with
    a fixed primary instance as well as cases for which a primary instance is dynamically
    chosen.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll explore a lot more details related to Kubernetes StatefulSets in the next
    several chapters, including persistent storage and Services. For this chapter,
    we’ll look at a basic example of a StatefulSet and then build on it as we introduce
    other important concepts.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this simple example, let’s create two Pods and show how they each get unique
    storage that stays in place even if the Pod is replaced. We’ll use this YAML resource:'
  prefs: []
  type: TYPE_NORMAL
- en: '*sleep-set.yaml*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: There are a few important differences here compared to a Deployment or a Job.
    First, we must declare a `serviceName` to tie this StatefulSet to a Kubernetes
    Service ➊. This connection is used to create a Domain Name Service (DNS) entry
    for each Pod. We must also provide a template for the StatefulSet to use to request
    persistent storage ➌ and then tell Kubernetes where to mount that storage in our
    container ➋.
  prefs: []
  type: TYPE_NORMAL
- en: The actual *sleep-set.yaml* file that the automation scripts install includes
    the `sleep` Service definition. We cover Services in detail in [Chapter 9](ch09.xhtml#ch09).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create the `sleep` StatefulSet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The StatefulSet creates two Pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The persistent storage for each Pod is brand new, so it starts empty. Let’s
    create some content. The easiest way to do that is from within the container itself,
    using `kubectl exec`, which allows us to run commands inside a container, similar
    to `crictl`. The `kubectl exec` command works no matter what host the container
    is on, even if we’re connecting to our Kubernetes API server from outside the
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s write each container’s hostname to a file and print it out so that we
    can verify it worked:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Each of our Pods now has unique content in its persistent storage. Let’s delete
    one of the Pods and verify that its replacement inherits its predecessor’s storage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: After deleting `sleep-0`, we see a new Pod created with the same name, which
    is different from the Deployment for which a random name was generated for every
    new Pod. Additionally, for this new Pod, the file we created previously is still
    present because the StatefulSet attached the same persistent storage to the new
    Pod it created when the old one was deleted.
  prefs: []
  type: TYPE_NORMAL
- en: Daemon Sets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The *DaemonSet* controller is like a StatefulSet in that the DaemonSet also
    runs a specific number of Pods, each with a unique identity. However, the DaemonSet
    runs exactly one Pod per node, which is useful primarily for control plane and
    add-on components for a cluster, such as a network or storage plug-in.
  prefs: []
  type: TYPE_NORMAL
- en: Our cluster already has multiple DaemonSets installed, so let’s look at the
    `calico-node` DaemonSet that’s already running, which runs on each node to provide
    network configuration for all containers on that node.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `calico-node` DaemonSet is in the `calico-system` Namespace, so we’ll specify
    that Namespace to request information about the DaemonSet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Our cluster has three nodes, so the `calico-node` DaemonSet has created three
    instances. Here’s the configuration of this DaemonSet in YAML format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The `-o yaml` parameter to `kubectl get` prints out the configuration and status
    of one or more resources in YAML format, allowing us to inspect Kubernetes resources
    in detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'The selector for this DaemonSet expects a label called `k8s-app` to be set
    to `calico-node`. We can use this to show just the Pods that this DaemonSet creates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The DaemonSet has created three Pods, each of which is assigned to one of the
    nodes in our cluster. If we add additional nodes to our cluster, the DaemonSet
    will schedule a Pod on the new nodes as well.
  prefs: []
  type: TYPE_NORMAL
- en: Final Thoughts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This chapter explored Kubernetes from the perspective of a regular cluster user,
    creating controllers that in turn create Pods with containers. Having this core
    knowledge of controller resource types is essential for building our applications.
    At the same time, it’s important to remember that Kubernetes is using the container
    technology we explored in [Part I](part01.xhtml#part01).
  prefs: []
  type: TYPE_NORMAL
- en: One key aspect of container technology is the ability to isolate containers
    in separate network namespaces. Running containers in a Kubernetes cluster adds
    additional requirements for networking because we now need to connect containers
    running on different cluster nodes. In the next chapter, we consider multiple
    approaches to make this work as we look at overlay networks.
  prefs: []
  type: TYPE_NORMAL
