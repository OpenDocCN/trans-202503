- en: '**30'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**30'
- en: LIMITED LABELED DATA**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**有限标签数据**'
- en: '![Image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/common.jpg)'
- en: Suppose we plot a learning curve (as shown in [Figure 5-1](ch05.xhtml#ch5fig1)
    on [page 24](ch05.xhtml#ch5fig1), for example) and find the machine learning model
    overfits and could benefit from more training data. What are some different approaches
    for dealing with limited labeled data in supervised machine learning settings?
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们绘制一条学习曲线（例如，如[图5-1](ch05.xhtml#ch5fig1)所示，位于[第24页](ch05.xhtml#ch5fig1)），并发现机器学习模型出现过拟合，可能需要更多的训练数据。有哪些不同的方法可以处理监督学习中有限标签数据的问题？
- en: In lieu of collecting more data, there are several methods related to regular
    supervised learning that we can use to improve model performance in limited labeled
    data regimes.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在无法收集更多数据的情况下，我们可以使用一些与常规监督学习相关的方法，在有限标签数据的情况下提高模型性能。
- en: '**Improving Model Performance with Limited Labeled Data**'
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**在有限标签数据下提高模型性能**'
- en: The following sections explore various machine learning paradigms that help
    in scenarios where training data is limited.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 以下章节探讨了在训练数据有限的情况下，有助于解决问题的各种机器学习范式。
- en: '***Labeling More Data***'
  id: totrans-7
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***标签更多数据***'
- en: Collecting additional training examples is often the best way to improve the
    performance of a model (a learning curve is a good diagnostic for this). However,
    this is often not feasible in practice, because acquiring high-quality data can
    be costly, computational resources and storage might be insufficient, or the data
    may be hard to access.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 收集更多的训练样本通常是提高模型性能的最佳方法（学习曲线是诊断这一点的好工具）。然而，在实际中，这往往是不可行的，因为获取高质量的数据可能很昂贵，计算资源和存储可能不足，或者数据可能难以获取。
- en: '***Bootstrapping the Data***'
  id: totrans-9
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***引导数据***'
- en: Similar to the techniques for reducing overfitting discussed in [Chapter 5](ch05.xhtml),
    it can be helpful to “bootstrap” the data by generating modified (augmented) or
    artificial (synthetic) training examples to boost the performance of the predictive
    model. Of course, improving the quality of data can also lead to the improved
    predictive performance of a model, as discussed in [Chapter 21](ch21.xhtml).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于[第5章](ch05.xhtml)中讨论的减少过拟合的技术，通过生成修改过的（增强的）或人工的（合成的）训练样本来“引导”数据，能够提升预测模型的性能。当然，提升数据质量也可以提高模型的预测性能，正如在[第21章](ch21.xhtml)中讨论的那样。
- en: '***Transfer Learning***'
  id: totrans-11
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***迁移学习***'
- en: Transfer learning describes training a model on a general dataset (for example,
    ImageNet) and then fine-tuning the pretrained target dataset (for example, a dataset
    consisting of different bird species), as outlined in [Figure 30-1](ch30.xhtml#ch30fig1).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习描述了在通用数据集（例如ImageNet）上训练一个模型，然后对预训练的目标数据集（例如一个包含不同鸟类物种的数据集）进行微调，如[图30-1](ch30.xhtml#ch30fig1)中所示。
- en: '![Image](../images/30fig01.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/30fig01.jpg)'
- en: '*Figure 30-1: The process of transfer learning*'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*图30-1：迁移学习的过程*'
- en: Transfer learning is usually done in the context of deep learning, where model
    weights can be updated. This is in contrast to tree-based methods, since most
    decision tree algorithms are nonparametric models that do not support iterative
    training or parameter updates.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习通常是在深度学习的背景下进行的，其中模型的权重可以被更新。这与基于树的方法不同，因为大多数决策树算法是非参数模型，不支持迭代训练或参数更新。
- en: '***Self-Supervised Learning***'
  id: totrans-16
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***自监督学习***'
- en: Similar to transfer learning, in self-supervised learning, the model is pre-trained
    on a different task before being fine-tuned to a target task for which only limited
    data exists. However, self-supervised learning usually relies on label information
    that can be directly and automatically extracted from unlabeled data. Hence, self-supervised
    learning is also often called *unsupervised pretraining*.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于迁移学习，在自监督学习中，模型会在不同的任务上进行预训练，然后微调以适应目标任务，而该任务只有有限的数据。然而，自监督学习通常依赖于可以直接从未标注数据中自动提取的标签信息。因此，自监督学习通常也被称为*无监督预训练*。
- en: 'Common examples of self-supervised learning include the *next word* (used in
    GPT, for example) or *masked word* (used in BERT, for example) pre-training tasks
    in language modeling, covered in more detail in [Chapter 17](ch17.xhtml). Another
    intuitive example from computer vision includes *inpainting*: predicting the missing
    part of an image that was randomly removed, illustrated in [Figure 30-2](ch30.xhtml#ch30fig2).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督学习的常见例子包括语言建模中的*下一个词*（例如 GPT 使用）或*掩蔽词*（例如 BERT 使用）预训练任务，更多细节请参见[第 17 章](ch17.xhtml)。另一个来自计算机视觉的直观例子是*图像修复*：预测随机移除的图像缺失部分，如[图
    30-2](ch30.xhtml#ch30fig2)所示。
- en: '![Image](../images/30fig02.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/30fig02.jpg)'
- en: '*Figure 30-2: Inpainting for self-supervised learning*'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 30-2：自监督学习中的图像修复*'
- en: For more detail on self-supervised learning, see [Chapter 2](ch02.xhtml).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 关于自监督学习的更多细节，请参见[第 2 章](ch02.xhtml)。
- en: '***Active Learning***'
  id: totrans-22
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***主动学习***'
- en: In active learning, illustrated in [Figure 30-3](ch30.xhtml#ch30fig3), we typically
    involve manual labelers or users for feedback during the learning process. However,
    instead of labeling the entire dataset up front, active learning includes a prioritization
    scheme for suggesting unlabeled data points for labeling to maximize the machine
    learning model’s performance.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在主动学习中，如[图 30-3](ch30.xhtml#ch30fig3)所示，我们通常涉及人工标注者或用户在学习过程中的反馈。然而，主动学习并不是一开始就标注整个数据集，而是通过优先级方案建议未标注的数据点进行标注，以最大化机器学习模型的性能。
- en: '![Image](../images/30fig03.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/30fig03.jpg)'
- en: '*Figure 30-3: In active learning, a model queries an oracle for labels.*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 30-3：在主动学习中，模型向 oracle 请求标签。*'
- en: The term *active learning* refers to the fact that the model actively selects
    data for labeling. For example, the simplest form of active learning selects data
    points with high prediction uncertainty for labeling by a human annotator (also
    referred to as an *oracle*).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*主动学习*一词指的是模型主动选择数据进行标注。例如，最简单的主动学习形式是选择预测不确定性较高的数据点，由人工标注员进行标注（也称为*oracle*）。'
- en: '***Few-Shot Learning***'
  id: totrans-27
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***少量样本学习***'
- en: In a few-shot learning scenario, we often deal with extremely small datasets
    that include only a handful of examples per class. In research contexts, 1-shot
    (one example per class) and 5-shot (five examples per class) learning scenarios
    are very common. An extreme case of few-shot learning is zero-shot learning, where
    no labels are provided. Popular examples of zero-shot learning include GPT-3 and
    related language models, where the user has to provide all the necessary information
    via the input prompt, as illustrated in [Figure 30-4](ch30.xhtml#ch30fig4).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在少量样本学习的场景中，我们通常处理的是包含每个类别仅少量示例的极小数据集。在研究中，1-shot（每个类别一个示例）和5-shot（每个类别五个示例）学习场景非常常见。少量样本学习的极端案例是零样本学习，在这种情况下没有提供标签。零样本学习的流行例子包括GPT-3及相关语言模型，在这些模型中，用户必须通过输入提示提供所有必要的信息，如[图
    30-4](ch30.xhtml#ch30fig4)所示。
- en: '![Image](../images/30fig04.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/30fig04.jpg)'
- en: '*Figure 30-4: Zero-shot classification with ChatGPT*'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 30-4：使用 ChatGPT 进行零样本分类*'
- en: For more detail on few-shot learning, see [Chapter 3](ch03.xhtml).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 关于少量样本学习的更多细节，请参见[第 3 章](ch03.xhtml)。
- en: '***Meta-Learning***'
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***元学习***'
- en: Meta-learning involves developing methods that determine how machine learning
    algorithms can best learn from data. We can therefore think of meta-learning as
    “learning to learn.” The machine learning community has developed several approaches
    for meta-learning. Within the machine learning community, the term *meta-learning*
    doesn’t just represent multiple subcategories and approaches; it is also occasionally
    employed to describe related yet distinct processes, leading to nuances in its
    interpretation and application.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 元学习涉及开发方法来确定机器学习算法如何最好地从数据中学习。因此，我们可以将元学习看作是“学习如何学习”。机器学习社区已经开发了几种元学习方法。在机器学习社区中，*元学习*不仅代表多个子类别和方法；有时它也用来描述相关但不同的过程，导致其在解释和应用上的细微差别。
- en: Meta-learning is one of the main subcategories of few-shot learning. Here, the
    focus is on learning a good feature extraction module, which converts support
    and query images into vector representations. These vector representations are
    optimized for determining the predicted class of the query example via comparisons
    with the training examples in the support set. (This form of meta-learning is
    illustrated in [Chapter 3](ch03.xhtml) on [page 17](ch03.xhtml#ch3fig2).)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 元学习是少样本学习的主要子类别之一。在这里，重点是学习一个好的特征提取模块，该模块将支持集和查询图像转换为向量表示。这些向量表示通过与支持集中的训练示例进行比较，优化用于确定查询示例的预测类别。（这种元学习形式在[第3章](ch03.xhtml)的[第17页](ch03.xhtml#ch3fig2)中有说明。）
- en: Another branch of meta-learning unrelated to the few-shot learning approach
    is focused on extracting metadata (also called *meta-features*) from datasets
    for supervised learning tasks, as illustrated in [Figure 30-5](ch30.xhtml#ch30fig5).
    The meta-features are descriptions of the dataset itself. For example, these can
    include the number of features and statistics of the different features (kurtosis,
    range, mean, and so on).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个与少样本学习方法无关的元学习分支，专注于从数据集中提取元数据（也称为*元特征*）用于监督学习任务，如[图 30-5](ch30.xhtml#ch30fig5)所示。元特征是数据集本身的描述。例如，这些可以包括特征的数量和不同特征的统计数据（峰度、范围、均值等）。
- en: '![Image](../images/30fig05.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/30fig05.jpg)'
- en: '*Figure 30-5: The meta-learning process involving the extraction of metadata*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 30-5：涉及提取元数据的元学习过程*'
- en: The extracted meta-features provide information for selecting a machine learning
    algorithm for the dataset at hand. Using this approach, we can narrow down the
    algorithm and hyperparameter search spaces, which helps reduce overfitting when
    the dataset is small.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 提取的元特征为选择适合当前数据集的机器学习算法提供了信息。通过这种方法，我们可以缩小算法和超参数的搜索空间，从而帮助减少数据集较小时的过拟合。
- en: '***Weakly Supervised Learning***'
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***弱监督学习***'
- en: Weakly supervised learning, illustrated in [Figure 30-6](ch30.xhtml#ch30fig6),
    involves using an external label source to generate labels for an unlabeled dataset.
    Often, the labels created by a weakly supervised labeling function are more noisy
    or inaccurate than those produced by a human or domain expert, hence the term
    *weakly* supervised. We can develop or adopt a rule-based classifier to create
    the labels in weakly supervised learning; these rules usually cover only a subset
    of the unlabeled dataset.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 弱监督学习，如[图 30-6](ch30.xhtml#ch30fig6)所示，涉及使用外部标签源为未标记的数据集生成标签。通常，弱监督标签函数生成的标签比人工或领域专家生成的标签更嘈杂或不准确，因此称为*弱监督*。我们可以开发或采用基于规则的分类器来创建弱监督学习中的标签；这些规则通常只涵盖未标记数据集的一个子集。
- en: '![Image](../images/30fig06.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/30fig06.jpg)'
- en: '*Figure 30-6: Weakly supervised learning uses external labeling functions to
    train machine learning models.*'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 30-6：弱监督学习使用外部标签函数来训练机器学习模型。*'
- en: Let’s return to the example of email spam classification from [Chapter 23](ch23.xhtml)
    to illustrate a rule-based approach for data labeling. In weak supervision, we
    could design a rule-based classifier based on the keyword *SALE* in the email
    subject header line to identify a subset of spam emails. Note that while we may
    use this rule to label certain emails as spam positive, we should not apply this
    rule to label emails without *SALE* as non-spam. Instead, we should either leave
    those unlabeled or apply a different rule to them.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到[第23章](ch23.xhtml)中的电子邮件垃圾邮件分类示例，来说明一种基于规则的数据标注方法。在弱监督下，我们可以基于电子邮件主题行中的关键词*SALE*设计一个基于规则的分类器来识别一部分垃圾邮件。请注意，虽然我们可以使用此规则将某些电子邮件标记为垃圾邮件正例，但不应将没有*SALE*的邮件标记为非垃圾邮件。相反，我们应该将这些邮件保持未标记状态，或者为它们应用不同的规则。
- en: There is a subcategory of weakly supervised learning referred to as PU-learning.
    In *PU-learning*, which is short for *positive-unlabeled learning*, we label and
    learn only from positive examples.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 弱监督学习有一个子类别称为PU学习。在*PU学习*中，*PU*是*正样本-未标记学习*的缩写，我们只标记和学习正样本。
- en: '***Semi-Supervised Learning***'
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***半监督学习***'
- en: 'Semi-supervised learning is closely related to weakly supervised learning:
    it also involves creating labels for unlabeled instances in the dataset. The main
    difference between these two methods lies in *how* we create the labels. In weak
    supervision, we create labels using an external labeling function that is often
    noisy, inaccurate, or covers only a subset of the data. In semi-supervision, we
    do not use an external label function; instead, we leverage the structure of the
    data itself. We can, for example, label additional data points based on the density
    of neighboring labeled data points, as illustrated in [Figure 30-7](ch30.xhtml#ch30fig7).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督学习与弱监督学习密切相关：它也涉及为数据集中未标记的实例创建标签。这两种方法之间的主要区别在于*如何*创建标签。在弱监督中，我们使用外部标注函数创建标签，这些函数通常是嘈杂、不准确的，或者仅覆盖数据的一个子集。在半监督中，我们不使用外部标注函数；相反，我们利用数据本身的结构。例如，我们可以根据邻近已标记数据点的密度为额外的数据点打标签，如[图
    30-7](ch30.xhtml#ch30fig7)所示。
- en: '![Image](../images/30fig07.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/30fig07.jpg)'
- en: '*Figure 30-7: Semi-supervised learning*'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 30-7：半监督学习*'
- en: While we can apply weak supervision to an entirely unlabeled dataset, semi-supervised
    learning requires at least a portion of the data to be labeled. In practice, it
    is possible first to apply weak supervision to label a subset of the data and
    then to use semi-supervised learning to label instances that were not captured
    by the labeling functions.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们可以将弱监督应用于完全未标记的数据集，半监督学习则要求至少有一部分数据是已标记的。在实践中，通常先应用弱监督对数据的一个子集进行标注，然后使用半监督学习对未被标注函数捕捉到的实例进行标注。
- en: Thanks to their close relationship, semi-supervised learning is sometimes referred
    to as a subcategory of weakly supervised learning, and vice versa.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它们之间的紧密关系，半监督学习有时被称为弱监督学习的一个子类别，反之亦然。
- en: '***Self-Training***'
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***自训练***'
- en: Self-training falls somewhere between semi-supervised learning and weakly supervised
    learning. For this technique, we train a model to label the dataset or adopt an
    existing model to do the same. This model is also referred to as a *pseudo-labeler*.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 自训练介于半监督学习和弱监督学习之间。对于这种技术，我们训练一个模型来标注数据集，或者采用现有模型来完成同样的任务。这个模型也被称为*伪标签生成器*。
- en: Self-training does not guarantee accurate labels and is thus related to weakly
    supervised learning. Moreover, while we use or adopt a machine learning model
    for this pseudo-labeling, self-training is also related to semi-supervised learning.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 自训练并不能保证标签的准确性，因此它与弱监督学习相关。此外，尽管我们使用或采用机器学习模型进行伪标签生成，自训练也与半监督学习相关。
- en: An example of self-training is knowledge distillation, discussed in [Chapter
    6](ch06.xhtml).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 自训练的一个例子是知识蒸馏，在[第 6 章](ch06.xhtml)中讨论过。
- en: '***Multi-Task Learning***'
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***多任务学习***'
- en: Multi-task learning trains neural networks on multiple, ideally related tasks.
    For example, if we are training a classifier to detect spam emails, spam classification
    is the main task. In multi-task learning, we can add one or more related tasks
    for the model to solve, referred to as *auxiliary tasks*. For the spam email example,
    an auxiliary task could be classifying the email’s topic or language.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务学习训练神经网络来处理多个理想上相关的任务。例如，如果我们训练一个分类器来检测垃圾邮件，那么垃圾邮件分类就是主要任务。在多任务学习中，我们可以为模型添加一个或多个相关任务来解决，这些被称为*辅助任务*。对于垃圾邮件的例子，一个辅助任务可能是对邮件的主题或语言进行分类。
- en: Typically, multi-task learning is implemented via multiple loss functions that
    have to be optimized simultaneously, with one loss function for each task. The
    auxiliary tasks serve as an inductive bias, guiding the model to prioritize hypotheses
    that can explain multiple tasks. This approach often results in models that perform
    better on unseen data.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，多任务学习是通过多个损失函数来实现的，这些损失函数需要同时优化，每个任务对应一个损失函数。辅助任务作为归纳偏置，引导模型优先选择能够解释多个任务的假设。这种方法通常会使模型在未见数据上的表现更好。
- en: 'There are two subcategories of multi-task learning: multi-task learning with
    hard parameter sharing and multi-task learning with soft parameter sharing. [Figure
    30-8](ch30.xhtml#ch30fig8) illustrates the difference between these two methods.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务学习有两个子类别：硬参数共享的多任务学习和软参数共享的多任务学习。[图 30-8](ch30.xhtml#ch30fig8)展示了这两种方法之间的区别。
- en: '![Image](../images/30fig08.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/30fig08.jpg)'
- en: '*Figure 30-8: The two main types of multi-task learning*'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 30-8：多任务学习的两种主要类型*'
- en: In *hard* parameter sharing, as shown in [Figure 30-8](ch30.xhtml#ch30fig8),
    only the output layers are task specific, while all the tasks share the same hidden
    layers and neural network backbone architecture. In contrast, *soft* parameter
    sharing uses separate neural networks for each task, but regularization techniques
    such as distance minimization between parameter layers are applied to encourage
    similarity among the networks.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在*硬*参数共享中，如[图30-8](ch30.xhtml#ch30fig8)所示，只有输出层是特定于任务的，而所有任务共享相同的隐藏层和神经网络骨干架构。相比之下，*软*参数共享使用独立的神经网络处理每个任务，但会应用正则化技术，如在参数层之间最小化距离，以鼓励网络间的相似性。
- en: '***Multimodal Learning***'
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***多模态学习***'
- en: While multi-task learning involves training a model with multiple tasks and
    loss functions, multimodal learning focuses on incorporating multiple types of
    input data.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务学习涉及使用多个任务和损失函数训练一个模型，而多模态学习则侧重于融合多种类型的输入数据。
- en: Common examples of multimodal learning are architectures that take both image
    and text data as input (though multimodal learning is not restricted to only two
    modalities and can be used for any number of input modalities). Depending on the
    task, we may employ a matching loss that forces the embedding vectors between
    related images and text to be similar, as shown in [Figure 30-9](ch30.xhtml#ch30fig9).
    (See [Chapter 1](ch01.xhtml) for more on embedding vectors.)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态学习的常见示例是同时处理图像和文本数据的架构（尽管多模态学习不限于两种模态，可以应用于任意数量的输入模态）。根据任务的不同，我们可以使用匹配损失，迫使相关图像和文本之间的嵌入向量相似，如[图30-9](ch30.xhtml#ch30fig9)所示。（有关嵌入向量的更多信息，请参见[第1章](ch01.xhtml)。）
- en: '![Image](../images/30fig09.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/30fig09.jpg)'
- en: '*Figure 30-9: Multimodal learning with a matching loss*'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*图30-9：带有匹配损失的多模态学习*'
- en: '[Figure 30-9](ch30.xhtml#ch30fig9) shows image and text encoders as separate
    components. The image encoder can be a convolutional backbone or a vision transformer,
    and the language encoder can be a recurrent neural network or language transformer.
    However, it’s common nowadays to use a single transformer-based module that can
    simultaneously process image and text data. For example, the VideoBERT model has
    a joint module that processes both video and text for action classification and
    video captioning.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[图30-9](ch30.xhtml#ch30fig9)展示了图像和文本编码器作为独立组件。图像编码器可以是卷积骨干网络或视觉变换器，而语言编码器可以是循环神经网络或语言变换器。然而，现如今，通常使用一个基于变换器的模块，可以同时处理图像和文本数据。例如，VideoBERT模型有一个联合模块，处理视频和文本，用于动作分类和视频字幕生成。'
- en: Optimizing a matching loss, as shown in [Figure 30-9](ch30.xhtml#ch30fig9),
    can be useful for learning embeddings that can be applied to various tasks, such
    as image classification or summarization. However, it is also possible to directly
    optimize the target loss, like classification or regression, as [Figure 30-10](ch30.xhtml#ch30fig10)
    illustrates.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图30-9](ch30.xhtml#ch30fig9)所示，优化匹配损失对于学习可应用于各种任务的嵌入是有用的，如图像分类或摘要。然而，也可以直接优化目标损失，如[图30-10](ch30.xhtml#ch30fig10)所示，进行分类或回归。
- en: '![Image](../images/30fig10.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/30fig10.jpg)'
- en: '*Figure 30-10: Multimodal learning for optimizing a supervised learning objective*'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '*图30-10：用于优化监督学习目标的多模态学习*'
- en: '[Figure 30-10](ch30.xhtml#ch30fig10) shows data being collected from two different
    sensors. One could be a thermometer and the other could be a video camera. The
    signal encoders convert the information into embeddings (sharing the same number
    of dimensions), which are then concatenated to form the input representation for
    the model.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[图30-10](ch30.xhtml#ch30fig10)展示了从两个不同传感器收集的数据。一个可能是温度计，另一个可能是视频摄像机。信号编码器将信息转换为嵌入（共享相同的维度数），然后将它们连接起来，形成模型的输入表示。'
- en: Intuitively, models that combine data from different modalities generally perform
    better than unimodal models because they can leverage more information. Moreover,
    recent research suggests that the key to the success of multimodal learning is
    the improved quality of the latent space representation.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 直观上，结合来自不同模态的数据的模型通常比单一模态模型表现更好，因为它们可以利用更多的信息。此外，最近的研究表明，多模态学习成功的关键在于潜在空间表示质量的提高。
- en: '***Inductive Biases***'
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***归纳偏差***'
- en: Choosing models with stronger inductive biases can help lower data requirements
    by making assumptions about the structure of the data. For example, due to their
    inductive biases, convolutional networks require less data than vision transformers,
    as discussed in [Chapter 13](ch13.xhtml).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 选择具有更强归纳偏差的模型可以通过对数据结构做出假设，从而帮助减少数据需求。例如，由于卷积网络的归纳偏差，它们比视觉变换器需要更少的数据，正如[第13章](ch13.xhtml)中讨论的那样。
- en: '**Recommendations**'
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**建议**'
- en: Of all these techniques for reducing data requirements, how should we decide
    which ones to use in a given situation?
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些减少数据需求的技术中，我们应如何决定在特定情况下使用哪些方法？
- en: Techniques like collecting more data, data augmentation, and feature engineering
    are compatible with all the methods discussed in this chapter. Multi-task learning
    and multimodal inputs can also be used with the learning strategies outlined here.
    If the model suffers from overfitting, we should also include techniques discussed
    in [Chapters 5](ch05.xhtml) and [6](ch06.xhtml).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 像收集更多数据、数据增强和特征工程这样的技术与本章讨论的所有方法兼容。多任务学习和多模态输入也可以与这里概述的学习策略结合使用。如果模型出现过拟合，我们还应该使用[第5章](ch05.xhtml)和[第6章](ch06.xhtml)中讨论的技术。
- en: But how can we choose between active learning, few-shot learning, transfer learning,
    self-supervised learning, semi-supervised learning, and weakly supervised learning?
    Deciding which supervised learning technique(s) to try is highly context dependent.
    You can use the diagram in [Figure 30-11](ch30.xhtml#ch30fig11) as a guide to
    choosing the best method for your particular project.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们如何在主动学习、少样本学习、迁移学习、自监督学习、半监督学习和弱监督学习之间做出选择呢？决定尝试哪种监督学习技术非常依赖于具体的情境。你可以参考[图30-11](ch30.xhtml#ch30fig11)中的图示，帮助你选择最适合你项目的方法。
- en: '![Image](../images/30fig11.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/30fig11.jpg)'
- en: '*Figure 30-11: Recommendations for choosing a supervised learning technique*'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*图30-11：选择监督学习技术的建议*'
- en: Note that the dark boxes in [Figure 30-11](ch30.xhtml#ch30fig11) are not terminal
    nodes but arc back to the second box, “Evaluate model performance”; additional
    arrows were omitted to avoid visual clutter.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，[图30-11](ch30.xhtml#ch30fig11)中的黑色框不是终端节点，而是回到第二个框“评估模型性能”；为了避免视觉杂乱，省略了额外的箭头。
- en: '**Exercises**'
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**练习**'
- en: '**30-1.** Suppose we are given the task of constructing a machine learning
    model that utilizes images to detect manufacturing defects on the outer shells
    of tablet devices similar to iPads. We have access to millions of images of various
    computing devices, including smartphones, tablets, and computers, which are not
    labeled; thousands of labeled pictures of smart-phones depicting various types
    of damage; and hundreds of labeled images specifically related to the target task
    of detecting manufacturing defects on tablet devices. How could we approach this
    problem using self-supervised learning or transfer learning?'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**30-1.** 假设我们被要求构建一个机器学习模型，该模型利用图像检测类似 iPad 的平板设备外壳上的制造缺陷。我们有数百万张未标记的各类计算设备图像，包括智能手机、平板电脑和计算机；成千上万张标记了不同类型损坏的智能手机图片；以及数百张专门与检测平板设备制造缺陷相关的标记图像。我们该如何通过自监督学习或迁移学习来解决这个问题？'
- en: '**30-2.** In active learning, selecting difficult examples for human inspection
    and labeling is often based on confidence scores. Neural networks can provide
    such scores by using the logistic sigmoid or softmax function in the output layer
    to calculate class-membership probabilities. However, it is widely recognized
    that deep neural networks exhibit overconfidence on out-of-distribution data,
    rendering their use in active learning ineffective. What are some other methods
    to obtain confidence scores using deep neural networks for active learning?'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**30-2.** 在主动学习中，选择困难的样本供人工检查和标注通常基于置信度评分。神经网络可以通过在输出层使用逻辑 sigmoid 或 softmax
    函数来计算类别归属概率，从而提供这样的评分。然而，普遍认为，深度神经网络在面对分布外数据时表现出过度自信，这使得它们在主动学习中的应用变得低效。对于主动学习，如何使用深度神经网络获取置信度评分的其他方法有哪些？'
- en: '**References**'
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: 'While decision trees for incremental learning are not commonly implemented,
    algorithms for training decision trees in an iterative fashion do exist: *[https://en.wikipedia.org/wiki/Incremental_decision_tree](https://en.wikipedia.org/wiki/Incremental_decision_tree)*.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管增量学习的决策树并不常见，但确实存在用于以迭代方式训练决策树的算法：*[https://en.wikipedia.org/wiki/Incremental_decision_tree](https://en.wikipedia.org/wiki/Incremental_decision_tree)*。
- en: 'Models trained with multi-task learning often outperform models trained on
    a single task: Rich Caruana, “Multitask Learning” (1997), *[https://doi.org/10.1023%2FA%3A1007379606734](https://doi.org/10.1023%2FA%3A1007379606734)*.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用多任务学习训练的模型通常优于单一任务训练的模型：Rich Caruana，“多任务学习”（1997），*[https://doi.org/10.1023%2FA%3A1007379606734](https://doi.org/10.1023%2FA%3A1007379606734)*。
- en: 'A single transformer-based module that can simultaneously process image and
    text data: Chen Sun et al., “VideoBERT: A Joint Model for Video and Language Representation
    Learning” (2019), *[https://arxiv.org/abs/1904.01766](https://arxiv.org/abs/1904.01766)*.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个基于Transformer的单一模块，可以同时处理图像和文本数据：Chen Sun等人，“VideoBERT：一种用于视频和语言表示学习的联合模型”（2019），*[https://arxiv.org/abs/1904.01766](https://arxiv.org/abs/1904.01766)*。
- en: 'The aforementioned research suggesting the key to the success of multimodal
    learning is the improved quality of the latent space representation: Yu Huang
    et al., “What Makes Multi-Modal Learning Better Than Single (Provably)” (2021),
    *[https://arxiv.org/abs/2106.04538](https://arxiv.org/abs/2106.04538)*.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上述研究表明，多模态学习成功的关键是潜在空间表示质量的提升：Yu Huang等人，“是什么使多模态学习比单一模态更好（可以证明）”（2021），*[https://arxiv.org/abs/2106.04538](https://arxiv.org/abs/2106.04538)*。
- en: 'For more information on active learning: Zhen et al., “A Comparative Survey
    of Deep Active Learning” (2022), *[https://arxiv.org/abs/2203.13450](https://arxiv.org/abs/2203.13450)*.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欲了解更多关于主动学习的信息：Zhen等人，“深度主动学习的比较调查”（2022），*[https://arxiv.org/abs/2203.13450](https://arxiv.org/abs/2203.13450)*。
- en: 'For a more detailed discussion on how out-of-distribution data can lead to
    overconfidence in deep neural networks: Anh Nguyen, Jason Yosinski, and Jeff Clune,
    “Deep Neural Networks Are Easily Fooled: High Confidence Predictions for Unrecognizable
    Images” (2014), *[https://arxiv.org/abs/1412.1897](https://arxiv.org/abs/1412.1897)*.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于如何分布外数据可能导致深度神经网络过度自信的详细讨论：Anh Nguyen, Jason Yosinski和Jeff Clune，“深度神经网络容易被愚弄：对于无法识别图像的高置信度预测”（2014），*[https://arxiv.org/abs/1412.1897](https://arxiv.org/abs/1412.1897)*。
