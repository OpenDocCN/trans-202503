<html><head></head><body><div id="sbo-rt-content"><section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" title="117" id="Page_117"/>8</span><br/>
<span class="ChapterTitle">Asynchronous Programming</span></h1>
</header>
<figure class="opener">
<img src="Images/chapterart.png" alt="" width="206" height="206"/>
</figure>
<p class="ChapterIntro">Asynchronous programming is, as the name implies, programming that is not synchronous. At a high level, an asynchronous operation is one that executes in the background—the program won’t wait for the asynchronous operation to complete but will instead continue to the next line of code immediately. If you’re not already familiar with asynchronous programming, that definition may feel insufficient as it doesn’t actually explain what asynchronous programming <em>is</em>. To really understand the asynchronous programming model and how it works in Rust, we have to first dig into what the alternative is. That is, we need to understand the <em>synchronous</em> programming model before we can understand the <em>asynchronous</em> one. This is important in both clarifying the concepts and demonstrating the trade-offs of using asynchronous programming: an asynchronous solution is not always the right one! We’ll start this chapter by taking a quick journey through what <span epub:type="pagebreak" title="118" id="Page_118"/>motivates asynchronous programming as a concept in the first place; then we’ll dig into how asynchrony in Rust actually works under the hood.</p>
<h2 id="h1--0001">What’s the Deal with Asynchrony?</h2>
<p class="BodyFirst">Before we get to the details of the synchronous and asynchronous programming models, we first need to take a quick look at what your computer is actually doing when it runs your programs.</p>
<p>Computers are fast. Really fast. So fast, in fact, that they spend most of their time waiting for things to happen. Unless you’re decompressing files, encoding audio, or crunching numbers, chances are that your CPU mostly sits idle, waiting for operations to complete. It’s waiting for a network packet to arrive, for the mouse to move, for the disk to finish writing some bytes, or maybe even just for a read from main memory to complete. From the CPU’s perspective, eons go by between most such events. When one does occur, the CPU runs a few more instructions, then goes back to waiting again. Take a look at your CPU utilization—it’s probably somewhere in the low single digits, and that’s likely where it hovers the majority of the time.</p>
<h3 id="h2--0001">Synchronous Interfaces</h3>
<p class="BodyFirst">Synchronous interfaces allow your program (or rather, a single thread in your program) to execute only a single operation at a time; each operation has to wait for the previous synchronous operation to finish before it gets to run. Most interfaces you see in the wild are synchronous: you call them, they go do some stuff, and eventually they return when the operation has completed and your program can continue from there. The reason for this, as we’ll see later in this chapter, is that making an operation asynchronous takes a fair bit of extra machinery. Unless you need the benefits of asynchrony, sticking to the synchronous model requires much less pomp and circumstance.</p>
<p>Synchronous interfaces hide all this waiting; the application calls a function that says “write these bytes to this file,” and some time later, that function completes and the next line of code executes. Behind the scenes, what really happens is that the operating system queues up a write operation to the disk and then puts the application to sleep until the disk reports that it has finished the write. The application experiences this as the function taking a long time to execute, but in reality it isn’t really executing at all, just waiting.</p>
<p>An interface that performs operations sequentially in this way is also often referred to as <em>blocking</em>, since the operation in the interface that has to wait for some external event to happen in order for it to make progress <em>blocks</em> further execution until that event happens. Whether you refer to an interface as synchronous or blocking, the basic idea is the same: the application does not move on until the current operation finishes. While the operation is waiting, so is the application.</p>
<p>Synchronous interfaces are usually considered to be easy to work with and simple to reason about, since your code executes just one line at a time. <span epub:type="pagebreak" title="119" id="Page_119"/>But they also allow the application to do only one thing at a time. That means if you want your program to wait for either user input or a network packet, you’re out of luck unless your operating system provides an operation specifically for that. Similarly, even if your application could do some other useful work while the disk is writing a file, it doesn’t have that option as the file write operation blocks the execution!</p>
<h3 id="h2--0002">Multithreading</h3>
<p class="BodyFirst">By far the most common solution to allowing concurrent execution is to use <em>multithreading</em>. In a multithreaded program, each thread is responsible for executing a particular independent sequence of blocking operations, and the operating system multiplexes among the threads so that if any thread can make progress, progress is made. If one thread blocks, some other thread may still be runnable, and so the application can continue to do useful work.</p>
<p>Usually, these threads communicate with each other using a synchronization primitive like a lock or a channel so that the application can still coordinate their efforts. For example, you might have one thread that waits for user input, one thread that waits for network packets, and another thread that waits for either of those threads to send a message on a channel shared between all three threads.</p>
<p>Multithreading gives you <em>concurrency</em>—the ability to have multiple independent operations that can be executed at any one time. It’s up to the system running the application (in this case, the operating system) to choose among the threads that aren’t blocked and decide which to execute next. If one thread is blocked, it can choose to run another one that can make progress instead.</p>
<p>Multithreading combined with blocking interfaces gets you quite far, and large swaths of production-ready software are built in this way. But this approach is not without its shortcomings. First, keeping track of all these threads quickly gets cumbersome; if you have to spin up a thread for every concurrent task, including simple ones like waiting for keyboard input, the threads add up fast, and so does the additional complexity needed to keep track of how all those threads interact, communicate, and coordinate.</p>
<p>Second, switching between threads gets costly the more of them there are. Every time one thread stops running and another one starts back up in its place, you need to do a round-trip to the operating system scheduler, and that’s not free. On some platforms, spawning new threads is also a fairly heavyweight process. Applications with high performance needs often mitigate this cost by reusing threads and using operating system calls that allow you to block on many related operations, but ultimately you are left with the same problem: blocking interfaces require that you have as many threads as the number of blocking calls you want to make.</p>
<p>Finally, threads introduce <em>parallelism</em> into your program. The distinction between concurrency and parallelism is subtle, but important: concurrency means that the execution of your tasks is interleaved, whereas parallelism means that multiple tasks are executing at the same time. If you have two tasks, their execution expressed in ASCII might look like <code>_-_-_</code> <span epub:type="pagebreak" title="120" id="Page_120"/>(concurrency) versus <code>=====</code> (parallelism). Multithreading does not necessarily imply parallelism—even though you have many threads, you might have only a single core, so only one thread is executing at a given time—but the two usually go hand in hand. You can make two threads mutually exclusive in their execution by using a <code>Mutex</code> or other synchronization primitive, but that introduces additional complexity—threads want to run in parallel. And while parallelism is often a good thing—who doesn’t want their program to run faster on more cores—it also means that your program must handle truly simultaneous access to shared data structures. This means moving from <code>Rc</code>, <code>Cell</code>, and <code>RefCell</code> to the more powerful but also slower <code>Arc</code> and <code>Mutex</code>. While you <em>may</em> want to use the latter types in your concurrent program to enable parallelism, threading <em>forces</em> you to use them. We’ll look at multithreading in much greater detail in <span class="xref" itemid="xref_target_Chapter 10">Chapter 10</span>.</p>
<h3 id="h2--0003">Asynchronous Interfaces</h3>
<p class="BodyFirst">Now that we’ve explored synchronous interfaces, we can look at the alternative: asynchronous or <em>nonblocking</em> interfaces. An asynchronous interface is one that may not yield a result straightaway, and may instead indicate that the result will be available at some later time. This gives the caller the opportunity to do something else in the meantime rather than having to go to sleep until that particular operation completes. In Rust parlance, an asynchronous interface is a method that returns a <code>Poll</code>, as defined in <a href="#listing8-1" id="listinganchor8-1">Listing 8-1</a>.</p>
<pre><code>enum Poll&lt;T&gt; {
    Ready(T),
    Pending
}</code></pre>
<p class="CodeListingCaption"><a id="listing8-1">Listing 8-1</a>: The core of asynchrony: the “here you are or come back later” type</p>
<p><code>Poll</code> usually shows up in the return type of functions whose names start with <code>poll</code>—these are methods that signal they can attempt an operation without blocking. We’ll get into how exactly they do that later in this chapter, but in general they attempt to perform as much as they can of the operation before they would normally block, and then return. And crucially, they remember where they left off so that they can resume execution later when additional progress can again be made.</p>
<p>These nonblocking functions allow us to easily perform multiple tasks concurrently. For example, if you want to read from either the network or the user’s keyboard, whichever has an event available first, all you have to do is poll both in a loop until one of them returns <code>Poll::Ready</code>. No need for any additional threads or synchronization!</p>
<p>The word <em>loop</em> here should make you a little nervous. You don’t want your program to burn through a loop three billion times a second when it may be minutes until the next input occurs. In the world of blocking interfaces, this wasn’t a problem since the operating system simply put the thread to sleep and then took care of waking it up when a relevant event occurred, but how do we avoid burning cycles while waiting in this brave new nonblocking world? That’s what much of the remainder of this chapter will be about.</p>
<h3 id="h2--0004"><span epub:type="pagebreak" title="121" id="Page_121"/>Standardized Polling</h3>
<p class="BodyFirst">To get to a world where every library can be used in a nonblocking fashion, we could have every library author cook up their own <code>poll</code> methods, all with slightly different names, signatures, and return types—but that would quickly get unwieldy. Instead, in Rust, polling is standardized through the <code>Future</code> trait. A simplified version of <code>Future</code> is shown in <a href="#listing8-2" id="listinganchor8-2">Listing 8-2</a> (we’ll get back to the real one later in this chapter).</p>
<pre><code>trait Future {
    type Output;
    fn poll(&amp;mut self) -&gt; Poll&lt;Self::Output&gt;;
}</code></pre>
<p class="CodeListingCaption"><a id="listing8-2">Listing 8-2</a>: A simplified view of the <code>Future</code> trait</p>
<p>Types that implement the <code>Future</code> trait are known as <em>futures</em> and represent values that may not be available yet. A future could represent the next time a network packet comes in, the next time the mouse cursor moves, or just the point at which some amount of time has elapsed. You can read <code>Future&lt;Output = Foo&gt;</code> as “a type that will produce a <code>Foo</code> in the future.” Types like this are often referred to in other languages as <em>promises</em>—they promise that they will eventually yield the indicated type. When a future eventually returns <code>Poll::Ready(T)</code>, we say that the future <em>resolves</em> into a <code>T</code>.</p>
<p>With this trait in place, we can generalize the pattern of providing <code>poll</code> methods. Instead of having methods like <code>poll_recv</code> and <code>poll_keypress</code>, we can have methods like <code>recv</code> and <code>keypress</code> that both return <code>impl Future</code> with an appropriate <code>Output</code> type. This doesn’t change the fact that you have to poll them—we’ll deal with that later—but it does mean that at least there is a standardized interface to these kinds of pending values, and we don’t need to use the <code>poll_</code> prefix everywhere.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p>	In general, you should not poll a <em>future</em> again after it has returned <code>Poll::Ready</code>. If you do, the <em>future</em> is well within its rights to panic. A <em>future</em> that is safe to poll after it has returned <code>Ready</code> is sometimes referred to as a <em>fused</em> <var>future</var>.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<h2 id="h1--0002">Ergonomic Futures</h2>
<p class="BodyFirst">Writing a type that implements <code>Future</code> in the way I’ve described so far is quite a pain. To see why, first take a look at the fairly straightforward asynchronous code block in <a href="#listing8-3" id="listinganchor8-3">Listing 8-3</a> that simply tries to forward messages from the input channel <code>rx</code> to the output channel <code>tx</code>.</p>
<pre><code>async fn forward&lt;T&gt;(rx: Receiver&lt;T&gt;, tx: Sender&lt;T&gt;) {
    while let Some(t) = rx.next().await {
        tx.send(t).await;
    }
}</code></pre>
<p class="CodeListingCaption"><a id="listing8-3">Listing 8-3</a>: Implementing a channel-forwarding future using <code>async</code> and <code>await</code></p>
<p><span epub:type="pagebreak" title="122" id="Page_122"/>This code, written using <code>async</code> and await syntax, looks very similar to its equivalent synchronous code and is easy to read. We simply send each message we receive in a loop until there are no more messages, and each <code>await</code> point corresponds to a place where a synchronous variant might block. Now think about if you instead had to express this code by manually implementing the <code>Future</code> trait. Since each call to <code>poll</code> starts at the top of the function, you’d need to package the necessary state to continue from the last place the code yielded. The result is fairly grotesque, as <a href="#listing8-4" id="listinganchor8-4">Listing 8-4</a> demonstrates.</p>
<pre><code>enum Forward&lt;T&gt; { <span class="CodeAnnotationCode" aria-label="annotation1">1</span> 
    WaitingForReceive(ReceiveFuture&lt;T&gt;, Option&lt;Sender&lt;T&gt;&gt;),
    WaitingForSend(SendFuture&lt;T&gt;, Option&lt;Receiver&lt;T&gt;&gt;),
}

impl&lt;T&gt; Future for Forward&lt;T&gt; {
    type Output = (); <span class="CodeAnnotationCode" aria-label="annotation2">2</span> 
    fn poll(&amp;mut self) -&gt; Poll&lt;Self::Output&gt; {
        match self { <span class="CodeAnnotationCode" aria-label="annotation3">3</span> 
            Forward::WaitingForReceive(recv, tx) =&gt; {
                if let Poll::Ready((rx, v)) = recv.poll() {
                    if let Some(v) = v {
                        let tx = tx.take().unwrap(); <span class="CodeAnnotationCode" aria-label="annotation4">4</span> 
                        *self = Forward::WaitingForSend(tx.send(v), Some(rx)); <span class="CodeAnnotationCode" aria-label="annotation5">5</span> 
                        <span class="LiteralGray">// Try to make progress on sending.</span>
                        return self.poll(); <span class="CodeAnnotationCode" aria-label="annotation6">6</span> 
                    } else {
                        <span class="LiteralGray">// No more items.</span>
                        Poll::Ready(())
                    }
                } else {
                    Poll::Pending
                }
            }
            Forward::WaitingForSend(send, rx) =&gt; {
                if let Poll::Ready(tx) = send.poll() {
                    let rx = rx.take().unwrap();
                    *self = Forward::WaitingForReceive(rx.receive(), Some(tx));
                    <span class="LiteralGray">// Try to make progress on receiving.</span>
                    return self.poll();
                } else {
                    Poll::Pending
                }
            }
        }
    }
}</code></pre>
<p class="CodeListingCaption"><a id="listing8-4">Listing 8-4</a>: Manually implementing a channel-forwarding future</p>
<p>You’ll rarely have to write code like this in Rust anymore, but it gives important insight into how things work under the hood, so let’s walk through it. First, we define our future type as an <code>enum</code> <span class="CodeAnnotation" aria-label="annotation1">1</span>, which we’ll use to keep track of what we’re currently waiting on. This is a consequence of the fact that <span epub:type="pagebreak" title="123" id="Page_123"/>when we return <code>Poll::Pending</code>, the next call to <code>poll</code> will start at the top of the function again. We need some way to know what we were in the middle of so that we know which operation to continue on. Furthermore, we need to keep track of different information depending on what we’re doing: if we’re waiting for a <code>receive</code> to finish, we need to keep that <code>ReceiveFuture</code> (the definition of which is not shown in this example) so that we can poll it the next time we are polled ourselves, and the same goes for <code>SendFuture</code>. The <code>Option</code>s here might strike you as weird too; we’ll get back to those shortly.</p>
<p>When we implement <code>Future</code> for <code>Forward</code>, we declare its output type as <code>()</code> <span class="CodeAnnotation" aria-label="annotation2">2</span> because this future doesn’t actually return anything. Instead, the future resolves (with no result) when it has finished forwarding everything from the input channel to the output channel. In a more complete example, the <code>Output</code> of our forwarding type might be a <code>Result</code> so that it could communicate errors from <code>receive()</code> and <code>send()</code> back up the stack to the function that’s polling for the completion of the forwarding. But this code is complicated enough already, so we’ll leave that for another day.</p>
<p>When <code>Forward</code> is polled, it needs to resume wherever it last left off, which we find out by matching on the enum variant currently held in <code>self</code> <span class="CodeAnnotation" aria-label="annotation3">3</span>. For whichever branch we go into, the first step is to poll the future that blocks progress for the current operation; if we’re trying to receive, we poll the <code>ReceiveFuture</code>, and if we’re trying to send, we poll the <code>SendFuture</code>. If that call to <code>poll</code> returns <code>Poll::Pending</code>, then we can make no progress, and we return <code>Poll::Pending</code> ourselves. But if the current future resolves, we have work to do!</p>
<p>When one of the inner futures resolves, we need to update what the current operation is by switching which enum variant is stored in <code>self</code>. In order to do so, we have to move out of <code>self</code> to call <code>Receiver::receive</code> or <code>Sender::send</code>—but we can’t do that because all we have is <code>&amp;mut self</code>. So, we store the state we have to move in an <code>Option</code>, which we move out of with <code>Option::take</code> <span class="CodeAnnotation" aria-label="annotation4">4</span>. This is silly since we’re about to overwrite <code>self</code> anyway <span class="CodeAnnotation" aria-label="annotation5">5</span>, and hence the <code>Option</code>s will always be <code>Some</code>, but sometimes tricks are needed to make the borrow checker happy.</p>
<p>Finally, if we do make progress, we then poll <code>self</code> again <span class="CodeAnnotation" aria-label="annotation6">6</span> so that if we can immediately make progress on the pending send or receive, we do so. This is actually necessary for correctness when implementing the real <code>Future</code> trait, which we’ll get back to later, but for now think of this as an optimization.</p>
<p>We just hand-wrote a <em>state machine</em>: a type that has a number of possible states and moves between them in response to particular events. This was a fairly simple state machine, at that. Imagine having to write code like this for more complicated use cases where you have additional intermediate steps!</p>
<p>Beyond writing the unwieldy state machine, we have to know the types of the futures that <code>Sender::send</code> and <code>Receiver::receive</code> return so that we can store them in our type. If those methods instead returned <code>impl Future</code>, we’d have no way to write out the types for our variants. The <code>send</code> and <code>receive</code> methods also have to take ownership of the sender and the receiver; if they did not, the lifetimes of the futures they returned would be tied to the <span epub:type="pagebreak" title="124" id="Page_124"/>borrow of <code>self</code>, which would end when we return from <code>poll</code>. But that would not work, since we’re trying to store those futures <em>in </em><code>self</code>.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p>	You may have noticed that <code>Receiver</code> looks a lot like an asynchronous version of <code>Iterator</code>. Others have noticed the same thing, and the standard library is on its way to adding a trait specifically for types that can meaningfully implement <code>poll_next</code>. Down the line, these asynchronous iterators (often referred to as <em>streams</em>) may end up with first-class language support, such as the ability to loop over them directly!</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p class="BodyFirst">Ultimately, this code is hard to write, hard to read, and hard to change. If we wanted to add error handling, for example, the code complexity would increase significantly. Luckily, there’s a better way!</p>
<h3 id="h2--0005">async/await</h3>
<p class="BodyFirst">Rust 1.39 gave us the <code>async</code> keyword and the closely related <code>await</code> postfix operator, which we used in the original example in <a href="#listing8-3">Listing 8-3</a>. Together, they provide a much more convenient mechanism for writing asynchronous state machines like the one in <a href="#listing8-5" id="listinganchor8-5">Listing 8-5</a>. Specifically, they let you write the code in such a way that it doesn’t even look like a state machine!</p>
<pre><code>async fn forward&lt;T&gt;(rx: Receiver&lt;T&gt;, tx: Sender&lt;T&gt;) {
    while let Some(t) = rx.next().await {
        tx.send(t).await;
    }
}</code></pre>
<p class="CodeListingCaption"><a id="listing8-5">Listing 8-5</a>: Implementing a channel-forwarding future using <code>async</code> and <code>await</code>, repeated from <a href="#listing8-3">Listing 8-3</a></p>
<p>If you don’t have much experience with <code>async</code> and <code>await</code>, the difference between <a href="#listing8-4">Listing 8-4</a> and <a href="#listing8-5">Listing 8-5</a> might give you an idea of why the Rust community was so excited to see them land. But since this is an intermediate book, let’s dive a little deeper to understand just how this short segment of code can replace the much longer manual implementation. To do that, we first need to talk about <em>generators</em>—the mechanism by which <code>async</code> and <code>await</code> are implemented.</p>
<h4 id="h3--0001">Generators</h4>
<p class="BodyFirst">Briefly described, a generator is a chunk of code with some extra compiler-generated bits that enables it to stop, or <em>yield</em>, its execution midway through and then resume from where it last yielded later on. Take the <code>forward</code> function in <a href="#listing8-3">Listing 8-3</a>, for example. Imagine that it gets to the call to <code>send</code>, but the channel is currently full. The function can’t make any more progress, but it also cannot block (this is nonblocking code, after all), so it needs to return. Now suppose the channel eventually clears and we want to proceed with the send. If we call <code>forward </code>again from the top, it’ll call <code>next</code> again and the item we previously tried to send will be lost, so that’s no good. Instead, we turn <code>forward</code> into a generator.</p>
<p><span epub:type="pagebreak" title="125" id="Page_125"/>Whenever the <code>forward</code> generator cannot make progress anymore, it needs to store its current state somewhere so that when its execution eventually resumes, it resumes in the right place with the right state. It saves the state through an associated data structure that’s generated by the compiler, which contains all the state of the generator at a given point in time. A method on that data structure (also generated) then allows the generator to resume from its current state, stored in <code>&amp;mut self</code>, and updates the state again when the generator again cannot make progress.</p>
<p>This “return but allow me to resume later” operation is called <em>yielding</em>, which effectively means it returns while keeping some extra state on the side. When we later want to resume a call to <code>forward</code>, we invoke the known entry point into the generator (the <em>resume method</em>, which is <code>poll</code> for <code>async</code> generators), and the generator inspects the previously stored state in <code>self</code> to decide what to do next. This is exactly the same thing we did manually in <a href="#listing8-4">Listing 8-4</a>! In other words, the code in <a href="#listing8-5">Listing 8-5</a> loosely desugars to the hypothetical code shown in <a href="#listing8-6" id="listinganchor8-6">Listing 8-6</a>.</p>
<pre><code>generator fn forward&lt;T&gt;(rx: Receiver&lt;T&gt;, tx: Sender&lt;T&gt;) {
    loop {
        let mut f = rx.next();
        let r = if let Poll::Ready(r) = f.poll() { r } else { yield };
        if let Some(t) = r {
            let mut f = tx.send(t);
            let _ = if let Poll::Ready(r) = f.poll() { r } else { yield };
        } else { break Poll::Ready(()); }
    }
}</code></pre>
<p class="CodeListingCaption"><a id="listing8-6">Listing 8-6</a>: Desugaring <code>async</code>/<code>await</code> into a generator</p>
<p>At the time of writing, generators are not actually usable in Rust—they are only used internally by the compiler to implement <code>async</code>/<code>await</code>—but that may change in the future. Generators come in handy in a number of cases, such as to implement iterators without having to carry around a <code>struct</code> or to implement an <code>impl Iterator</code> that figures out how to yield items one at a time.</p>
<p>If you look closely at Listings 8-5 and 8-6, they may seem a little magical once you know that every <code>await</code> or <code>yield</code> is really a return from the function. After all, there are several local variables in the function, and it’s not clear how they’re restored when we resume later on. This is where the compiler-generated part of generators comes into play. The compiler transparently injects code to persist those variables into and read them from the generator’s associated data structure, rather than the stack, at the time of execution. So if you declare, write to, or read from some local variable <code>a</code>, you are really operating on something akin to <code>self.a</code>. Problem solved! It’s all really quite marvelous.</p>
<p>One subtle but important difference between the manual <code>forward </code>implementation and the <code>async</code>/<code>await</code> version is that the latter can hold references across yield points. This enables functions like <code>Receiver::next</code> and <code>Sender::send</code> in <a href="#listing8-5">Listing 8-5</a> to take <code>&amp;mut self</code> rather than the <code>self</code> they took in <a href="#listing8-4">Listing 8-4</a>. If we tried to use a <code>&amp;mut self</code> receiver for these methods in the manual state <span epub:type="pagebreak" title="126" id="Page_126"/>machine implementation, the borrow checker would have no way to enforce that the <code>Receiver</code> stored inside <code>Forward</code> cannot be referenced between when <code>Receiver::next</code> is called and when the future it returns resolves, and so it would reject the code. Only by moving the <code>Receiver</code> into the future can we convince the compiler that the <code>Receiver</code> is not otherwise accessible. Meanwhile, with <code>async</code>/<code>await</code>, the borrow checker can inspect the code before the compiler turns it into a state machine and verify that <code>rx</code> is indeed not accessed again until after the future is dropped, when the <code>await</code> on it returns.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="box">
<h2>The Size of Generators</h2>
<p class="BoxBodyFirst">The data structure used to back a generator’s state must be able to hold the combined state at any one yield point. If your <code>async fn</code> contains, say, a <code>[u8; 8192]</code>, those 8KiB must be stored in the generator itself. Even if your <code>async fn</code> contains only smaller local variables, it must also contain any future that it awaits, since it needs to be able to poll such a future later, when <code>poll</code> is invoked.</p>
<p>This nesting means that generators, and thus futures based on <code>async</code> functions and blocks, can get quite large without any visible indicator of that increased size in your code. This can in turn impact your program’s runtime performance, since those giant generators may have to be copied across function calls and in and out of data structures, which amounts to a fair amount of memory copying. In fact, you can usually identify when the size of your generator-based futures is affecting performance by looking for excessive amounts of time spent in the <code>memcpy</code> function in your application’s performance profiles!</p>
<p>Finding these large futures isn’t always easy, however, and often requires manually identifying long or complex chains of <code>async</code> functions. Clippy may be able to help with this in the future, but at the time of writing, you’re on your own. When you do find a particularly large future, you have two options: you can try to reduce the amount of local state the <code>async</code> functions need, or you can move the future to the heap (with <code>Box::pin</code>) so that moving the future just requires moving the pointer to it. The latter is by far the easiest way to go, but it also introduces an extra allocation and a pointer indirection. Your best bet is usually to put the problematic future on the heap, measure your performance, and then use your performance benchmarks to guide you from there.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<h3 id="h2--0006">Pin and Unpin</h3>
<p class="BodyFirst">We’re not quite done. While generators are neat, a challenge arises from the technique as I’ve described it so far. In particular, it’s not clear what happens if the code in the generator (or, equivalently, the <code>async</code> block) takes a reference to a local variable. In the code from <a href="#listing8-5">Listing 8-5</a>, the future that <code>rx.next()</code> returns must necessarily hold a reference to <code>rx</code> if a next message is not immediately available so that it knows where to try again when the generator next resumes. When the generator yields, the future and the reference the future contains get stashed away inside the generator. But what <span epub:type="pagebreak" title="127" id="Page_127"/>now happens if the generator is moved? Specifically, look at the code in <a href="#listing8-7" id="listinganchor8-7">Listing 8-7</a>, which calls <code>forward</code>.</p>
<pre><code>async fn try_forward&lt;T&gt;(rx: Receiver&lt;T&gt;, tx: Sender&lt;T&gt;) -&gt; Option&lt;impl Future&gt; {
    let mut f = forward(rx, tx);
    if f.poll().is_pending() { Some(f) } else { None }
}</code></pre>
<p class="CodeListingCaption"><a id="listing8-7">Listing 8-7</a>: Moving a future after polling it</p>
<p>The <code>try_forward</code> function polls <code>forward</code> only once, to forward as many messages as possible without blocking. If the receiver may still produce more messages (that is, if it returned <code>Poll::Pending</code> instead of <code>Poll::Ready(None)</code>), those messages are deferred to be forwarded at some later time by returning the forwarding future to the caller, which may choose to poll again at a time when it sees fit.</p>
<p>Let’s work through what happens here with what we know about <code>async</code> and <code>await</code> so far. When we poll the <code>forward</code> generator, it goes through the <code>while</code> loop some unknown number of times and eventually returns either <code>Poll::Ready(())</code> if the receiver ended, or <code>Poll::Pending</code> otherwise. If it returns <code>Poll::Pending</code>, the generator contains a future returned from either <code>rx.next()</code> or <code>tx.send(t)</code>. Those futures both contain a reference to one of the arguments initially provided to <code>forward </code>(<code>rx</code> and <code>tx</code>, respectively), which must also be stored in the generator. But when <code>try_forward</code> returns the entire generator, the fields of the generator also move. Thus, <code>rx</code> and <code>tx</code> no longer reside at the same locations in memory, and the references stored in the stashed-away future are no longer pointing to the right data!</p>
<p>What we’ve run into here is a case of a <em>self-referential</em> data structure: one that holds both data and references to that data. With generators, these self-referential structures are very easy to construct, and being unable to support them would be a significant blow to ergonomics because it would mean you wouldn’t be able to hold references across any yield point. The (ingenious) solution for supporting self-referential data structures in Rust comes in the form of the <code>Pin</code> type and the <code>Unpin</code> trait. Very briefly, <code>Pin</code> is a wrapper type that prevents the wrapped type from being (safely) moved, and <code>Unpin</code> is a marker trait that says the implementing type <em>can</em> be removed safely from a <code>Pin</code>.</p>
<h4 id="h3--0002">Pin</h4>
<p class="BodyFirst">There’s a lot of nuance to cover here, so let’s start with a concrete use of the <code>Pin</code> wrapper. <a href="#listing8-2">Listing 8-2</a> gave you a simplified version of the <code>Future</code> trait, but we’re now ready to peel back one part of the simplification. <a href="#listing8-8" id="listinganchor8-8">Listing 8-8</a> shows the <code>Future</code> trait somewhat closer to its final form.</p>
<pre><code>trait Future {
    type Output;
    fn poll(self: Pin&lt;&amp;mut Self&gt;) -&gt; Poll&lt;Self::Output&gt;;
}</code></pre>
<p class="CodeListingCaption"><a id="listing8-8">Listing 8-8</a>: A less simplified view of the <code>Future</code> trait with <code>Pin</code></p>
<p><span epub:type="pagebreak" title="128" id="Page_128"/>In particular, this definition requires that you call <code>poll</code> on <code>Pin&lt;&amp;mut Self&gt;</code>. Once you have a value behind a <code>Pin</code>, that constitutes a contract that that value will never move again. This means that you can construct self-references internally to your heart’s delight, exactly as you want for generators.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p>	While <code>Future</code> makes use of <code>Pin</code>, <code>Pin</code> is not tied to the <code>Future</code> trait—you can use <code>Pin</code> for <em>any</em> self-referential data structure.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>But how do you get a <code>Pin</code> to call <code>poll</code>? And how can <code>Pin</code> ensure that the contained value won’t move? To see how this magic works, let’s look at the definition of <code>std::pin::Pin</code> and some of its key methods, shown in <a href="#listing8-9" id="listinganchor8-9">Listing 8-9</a>.</p>
<pre><code>struct Pin&lt;P&gt; { pointer: P }
impl&lt;P&gt; Pin&lt;P&gt; where P: Deref {
    pub unsafe fn new_unchecked(pointer: P) -&gt; Self;
}
impl&lt;'a, T&gt; Pin&lt;&amp;'a mut T&gt; {
    pub unsafe fn get_unchecked_mut(self) -&gt; &amp;'a mut T;
}
impl&lt;P&gt; Deref for Pin&lt;P&gt; where P: Deref {
    type Target = P::Target;
    fn deref(&amp;self) -&gt; &amp;Self::Target;
}</code></pre>
<p class="CodeListingCaption"><a id="listing8-9">Listing 8-9</a>: <code>std::pin::Pin</code> and its key methods</p>
<p>There’s a lot to unpack here, and we’re going to have to go over the definition in <a href="#listing8-9">Listing 8-9</a> a few times before all the bits make sense, so please bear with me.</p>
<p>First, you’ll notice that <code>Pin</code> holds a <em>pointer type</em>. That is, rather than hold some <code>T</code> directly, it holds a type <code>P</code> that dereferences through <code>Deref</code> into <code>T</code>. This means that rather than have a <code>Pin&lt;MyType&gt;</code>, you’ll have a <code>Pin&lt;Box&lt;MyType&gt;&gt;</code> or <code>Pin&lt;Rc&lt;MyType&gt;&gt;</code> or <code>Pin&lt;&amp;mut MyType&gt;</code>. The reason for this design is simple—<code>Pin</code>’s primary goal is to make sure that once you place a <code>T</code> behind a <code>Pin</code>, that <code>T</code> won’t move, as doing so might invalidate self-references stored in the <code>T</code>. If the <code>Pin</code> just held a <code>T</code> directly, then simply moving the <code>Pin</code> would be enough to invalidate that invariant! In the remainder of this section, I’ll refer to <code>P</code> as the <em>pointer</em> type and <code>T</code> as the <em>target</em> type.</p>
<p>Next, notice that <code>Pin</code>’s constructor, <code>new_unchecked</code>, is unsafe. This is because the compiler has no way to actually check that the pointer type indeed promises that the pointed-to (target) type won’t move again. Consider, for example, a variable <code>foo</code> on the stack. If <code>Pin</code>’s constructor were safe, we could do <code>Pin::new(&amp;mut foo)</code>, call a method that requires <code>Pin&lt;&amp;mut Self&gt;</code> (and thus assumes that <code>Self</code> won’t move again), and then drop the <code>Pin</code>. At this point, we could modify <code>foo</code> as much as we liked, since it is no longer borrowed—including moving it! We could then pin it again and call the same method, which would be none the wiser that any self-referential pointers it may have constructed the first time around would now be invalid.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="box">
<h2><span epub:type="pagebreak" title="129" id="Page_129"/>Pin Constructor Safety</h2>
<p class="BoxBodyFirst">The other reason the constructor for <code>Pin</code> is unsafe is that its safety depends on the implementation of traits that are themselves safe. For example, the way that <code>Pin&lt;P&gt;</code> implements <code>get_unchecked_mut</code><code> </code>is to use the implementation of <code>DerefMut::deref_mut</code> for <code>P</code>. While the call to <code>get_unchecked_mut</code><code> </code>is unsafe, the <code>im</code><code>pl DerefMut for P</code> is not. Yet it receives a <code>&amp;mut self</code>, and can thus freely (and without unsafe code) move the <code>T</code>. The same thing applies to <code>Drop</code>. The safety requirement for <code>Pin::new_unchecked</code> is therefore not only that the pointer type will not let the target type be moved again (like in the <code>Pin&lt;&amp;mut T&gt;</code> example), but also that its <code>Deref</code>, <code>DerefMut</code>, and <code>Drop</code> implementations do not move the pointed-to value behind the <code>&amp;mut self</code> they receive.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>We then get to the <code>get_unchecked_mut</code> method, which gives you a mutable reference to the <code>T</code> behind the <code>Pin</code>’s pointer type. This method is also unsafe, because once we give out a <code>&amp;mut T</code>, the caller has to promise it won’t use that <code>&amp;mut T</code> to move the <code>T</code> or otherwise invalidate its memory, lest any self-references be invalidated. If this method weren’t unsafe, a caller could call a method that takes <code>Pin&lt;&amp;mut Self&gt;</code> and then call the safe variant of <code>get_unchecked_mut</code> on two <code>Pin&lt;&amp;mut _&gt;</code>s, then use <code>mem::swap</code> to swap the values behind the <code>Pin</code>. If we were to then call a method that takes <code>Pin&lt;&amp;mut Self&gt;</code> again on either <code>Pin</code>, its assumption that the <code>Self</code> hasn’t moved would be violated, and any internal references it stored would be invalid!</p>
<p>Perhaps surprisingly, <code>Pin&lt;P&gt;</code> always implements <code>Deref&lt;Target = T&gt;</code>, and that is entirely safe. The reason for this is that a <code>&amp;T</code> does not let you move <code>T</code> without writing other unsafe code (<code>UnsafeCell</code>, for example, as we’ll discuss in <span class="xref" itemid="xref_target_Chapter 9">Chapter 9</span>). This is a good example of why the scope of an unsafe block extends beyond just the code it contains. If you wrote some code in one part of the application that (unsafely) replaced a <code>T</code> behind an <code>&amp;</code> using <code>UnsafeCell</code>, then it <em>could</em> be that that <code>&amp;T</code> initially came from a <code>Pin&lt;&amp;mut T&gt;</code>, and that you have now violated the invariant that the <code>T</code> behind the <code>Pin</code> may never move, even though the place where you unsafely replaced the <code>&amp;T</code> did not even mention <code>Pin</code>!</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p>	If you’ve browsed through the <code>Pin</code> documentation while reading this chapter, you may have noticed <code>Pin::set</code>, which takes a <code>&amp;mut self</code> and a <code>&lt;P as </code><code>Deref&gt;::Target</code> and safely changes the value behind the <code>Pin</code>. This is possible because <code>set</code> does not return the value that was previously pinned—it simply drops it in place and stores the new value there instead. Therefore, it does not violate the pinning invariants: the old value was never accessed outside of a <code>Pin</code> after it was placed there.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<h4 id="h3--0003">Unpin: The Key to Safe Pinning</h4>
<p class="BodyFirst">At this point you might ask: given that getting a mutable reference is unsafe anyway, why not have <code>Pin</code> hold a <code>T</code> directly? That is, rather than require an <span epub:type="pagebreak" title="130" id="Page_130"/>indirection through a pointer type, you could instead make the contract for <code>get_unchecked_mut</code> that it is only safe to call if you haven’t moved the <code>Pin</code>. The answer to that question lies in a neat safe use of <code>Pin</code> that the pointer design enables. Recall that the whole reason we want <code>Pin</code> in the first place is so we can have target types that may contain references to themselves (like a generator) and give their methods a guarantee that the target type hasn’t moved and thus that internal self-references remain valid. <code>Pin</code> lets us use the type system to enforce that guarantee, which is great. But unfortunately, with the design so far, <code>Pin</code> is very unwieldy to work with. This is because it always requires unsafe code, even if you are working with a target type that doesn’t contain any self-references, and so doesn’t care whether it’s been moved or not.</p>
<p>This is where the marker trait <code>Unpin</code> comes into play. An implementation of <code>Unpin</code> for a type simply asserts that the type is safe to move out of a <code>Pin</code> when used as a target type. That is, the type promises that it will never use any of <code>Pin</code>’s guarantees about the referent not moving again when used as a target type, and thus those guarantees may be broken. <code>Unpin</code> is an auto-trait, like <code>Send</code> and <code>Sync</code>, and so is auto-implemented by the compiler for any type that contains only <code>Unpin</code> members. Only types that explicitly opt out of <code>Unpin</code> (like generators) and types that contain those types are <code>!Unpin</code>.</p>
<p>For target types that are <code>Unpin</code>, we can provide a much simpler safe interface to <code>Pin</code>, as shown in <a href="#listing8-10" id="listinganchor8-10">Listing 8-10</a>.</p>
<pre><code>impl&lt;P&gt; Pin&lt;P&gt; where P: Deref, P::Target: Unpin {
    pub fn new(pointer: P) -&gt; Self;
}
impl&lt;P&gt; DerefMut for Pin&lt;P&gt; where P: DerefMut, P::Target: Unpin {
    fn deref_mut(&amp;mut self) -&gt; &amp;mut Self::Target;
}</code></pre>
<p class="CodeListingCaption"><a id="listing8-10">Listing 8-10</a>: The safe API to <code>Pin</code> for <code>Unpin</code> target types</p>
<p>To make sense of the safe API in <a href="#listing8-10">Listing 8-10</a>, think about the safety requirements of the unsafe methods from <a href="#listing8-9">Listing 8-9</a>: the function <code>Pin::new_unchecked</code> is unsafe because the caller must promise that the referent cannot be moved outside of the <code>Pin</code>, and that the implementations of <code>Deref</code>, <code>DerefMut</code>, and <code>Drop</code> for the pointer type do not move the referent through the reference they receive. Those requirements are there to ensure that once we give out a <code>Pin</code> to a <code>T</code>, we never move that <code>T</code> again. But if the <code>T</code> is <code>Unpin</code>, it has declared that it does not care if it is moved even if it was previously pinned, so it’s fine if the caller does not satisfy any of those requirements! </p>
<p>Similarly, <code>get_unchecked_mut</code> is unsafe because the caller must guarantee that it doesn’t move the <code>T</code> out of the <code>&amp;mut T</code>—but with <code>T: Unpin</code>, <code>T</code> has declared that it’s fine being moved even after being pinned, so that safety requirement is no longer important. This means that for <code>Pin&lt;P&gt; where P::Target: Unpin</code>, we can simply provide safe variants of both those methods (<code>DerefMut</code> being the safe version of <code>get_unchecked_mut</code>). In fact, we can even provide a <code>Pin::into_inner</code> that simply gives back the owned <code>P</code> if the target type is <code>Unpin</code>, since the <code>Pin</code> is essentially irrelevant!</p>
<h4 id="h3--0004"><span epub:type="pagebreak" title="131" id="Page_131"/>Ways of Obtaining a Pin</h4>
<p class="BodyFirst">With our new understanding of <code>Pin</code> and <code>Unpin</code>, we can now make progress toward using the new <code>Future</code> definition from <a href="#listing8-8">Listing 8-8</a> that requires <code>Pin&lt;&amp;mut Self&gt;</code>. The first step is to construct the required type. If the future type is <code>Unpin</code>, that step is easy—we just use <code>Pin::new(&amp;mut future)</code>. If it is not <code>Unpin</code>, we can pin the future in one of two main ways: by pinning to the heap or pinning to the stack.</p>
<p>Let’s start with pinning to the heap. The primary contract of <code>Pin</code> is that once something has been pinned, it cannot move. The pinning API takes care of honoring that contract for all methods and traits on <code>Pin</code>, so the main role of any function that constructs a <code>Pin</code> is to ensure that if the <code>Pin</code> <em>itself</em> moves, the referent value does not move too. The easiest way to ensure that is to place the referent on the heap, and then place just a pointer to the referent in the <code>Pin</code>. You can then move the <code>Pin</code> to your heart’s delight, but the target will remain where it was. This is the rationale behind the (safe) method <code>Box::pin</code>, which takes a <code>T</code> and returns a <code>Pin&lt;Box&lt;T&gt;&gt;</code>. There’s no magic to it; it simply asserts that <code>Box</code> follows the <code>Pin</code> constructor, <code>Deref</code>, and <code>Drop</code> contracts.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="box">
<h2>Unpin Box</h2>
<p class="BoxBodyFirst">While we’re on the topic of <code>Box</code>, take a look at the implementation of <code>Unpin</code> for <code>Box</code>. The <code>Box</code> type unconditionally implements <code>Unpin</code> for any <code>T</code>, even if that <code>T</code> is not <code>Unpin</code>. This might strike you as odd, given the earlier assertion that <code>Unpin</code> is an auto-trait that is generally implemented for a type only if all of the type’s members are also <code>Unpin</code>. <code>Box</code> is an exception to this for the same reason that it can provide a safe <code>Pin</code> constructor: if you move a <code>Box&lt;T&gt;</code>, you do not move the <code>T</code>. In other words, the unconditional implementation asserts that you can move a <code>Box&lt;T&gt;</code> out of a <code>Pin</code> even if <code>T</code> cannot be moved out of a <code>Pin</code>. Note, however, that this does <em>not</em> enable you to move a <code>T</code> that is <code>!Unpin</code> out of a <code>Pin&lt;Box&lt;T&gt;&gt;</code>.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>The other option, pinning to the stack, is a little more involved, and at the time of writing requires a smidgen of unsafe code. We have to ensure that the pinned value cannot be accessed after the <code>Pin</code> with a <code>&amp;mut</code> to it has been dropped. We accomplish that by shadowing the value as shown in the macro in <a href="#listing8-11" id="listinganchor8-11">Listing 8-11</a> or by using one of the crates that provide exactly this macro. One day it may even make it into the standard library!</p>
<pre><code>macro_rules! pin_mut {
    ($var:ident) =&gt; {
        let mut $var = $var;
        let mut $var = unsafe { Pin::new_unchecked(&amp;mut $var) };
    }
}</code></pre>
<p class="CodeListingCaption"><a id="listing8-11">Listing 8-11</a>: Macro for pinning to the stack</p>
<p><span epub:type="pagebreak" title="132" id="Page_132"/>By taking the name of the variable to pin to the stack, the macro ensures that the caller has the value it wants to pin somewhere on the stack already. The shadowing of <code>$var</code> ensures that the caller cannot drop the <code>Pin</code> and continue to use the unpinned value (which would breach the <code>Pin</code> contract for any target type that’s <code>!Unpin</code>). By moving the value stored in <code>$var</code>, the macro also ensures that the caller cannot drop the <code>$var</code> binding the macro declarations without also dropping the original variable. Specifically, without that line, the caller could write (note the extra scope):</p>
<pre><code>let foo = /* */; { pin_mut!(foo); foo.poll() }; foo.mut_self_method();</code></pre>
<p>Here, we give a pinned instance of <code>foo</code> to <code>poll</code>, but then we later use a <code>&amp;mut</code> to <code>foo</code> without a <code>Pin</code>, which violates the <code>Pin</code> contract. With the extra reassignment, on the other hand, that code would also move <code>foo</code> into the new scope, rendering it unusable after the scope ends.</p>
<p>Pinning on the stack therefore requires unsafe code, unlike <code>Box::pin</code>, but avoids the extra allocation that <code>Box</code> introduces and also works in <code>no_std</code> environments.</p>
<h4 id="h3--0005">Back to the Future</h4>
<p class="BodyFirst">We now have our pinned future, and we know what that means. But you may have noticed that none of this important pinning stuff shows up in most asynchronous code you write with <code>async</code> and <code>await</code>. And that’s because the compiler hides it from you.</p>
<p>Think back to when we discussed <a href="#listing8-5">Listing 8-5</a>, when I told you that <code>&lt;expr&gt;.await</code> desugars into something like:</p>
<pre><code>loop { if let Poll::Ready(r) = expr.poll() { break r } else { yield } }</code></pre>
<p>That was an ever-so-slight simplification because, as we’ve seen, you can call <code>Future::poll</code> only if you have a <code>Pin&lt;&amp;mut Self&gt;</code> for the future. The desugaring is actually a bit more sophisticated, as shown in <a href="#listing8-12" id="listinganchor8-12">Listing 8-12</a>.</p>
<pre><code><span class="CodeAnnotationCode" aria-label="annotation1">1</span> match expr {
      mut pinned =&gt; loop {
        <span class="CodeAnnotationCode" aria-label="annotation2">2</span> match unsafe { Pin::new_unchecked(&amp;mut pinned) }.poll() {
              Poll::Ready(r) =&gt; break r,
              Poll::Pending =&gt; yield,
          }
    }
}</code></pre>
<p class="CodeListingCaption"><a id="listing8-12">Listing 8-12</a>: A more accurate desugaring of <code>&lt;expr&gt;.await</code></p>
<p>The match <span class="CodeAnnotation" aria-label="annotation1">1</span> is a neat shorthand to not only ensure that the expansion remains a valid expression, but also move the expression result into a variable that we can then pin on the stack. Beyond that, the main new addition is the call to <code>Pin::new_unchecked</code> <span class="CodeAnnotation" aria-label="annotation2">2</span>. That call is safe because for the containing async block to be polled, it must already be pinned due to the signature of <code>Future::poll</code>. And the async block was polled for us to reach <span epub:type="pagebreak" title="133" id="Page_133"/>the call to <code>Pin::new_unchecked</code>, so the generator state is pinned. Since <code>pinned</code> is stored in the generator that corresponds to the async block (it must be so that <code>yield</code> will resume correctly), we know that <code>pinned</code> will not move again. Furthermore, <code>pinned</code> is not accessible except through a <code>Pin</code> once we’re in the loop, so no code is able to move out of the value in <code>pinned</code>. Thus, we meet all the safety requirements of <code>Pin::new_unchecked</code>, and the code is safe.</p>
<h2 id="h1--0003">Going to Sleep</h2>
<p class="BodyFirst">We went pretty deep into the weeds with <code>Pin</code>, but now that we’re out the other side, there is another issue around futures that may have been making your brain itch. If a call to <code>Future::poll</code> returns <code>Poll::Pending</code>, you need something to call <code>poll</code> again at a later time to check whether you can make progress yet. That something is usually called the <em>executor</em>. Your executor could be a simple loop that polls all the futures you are waiting on until they’ve all returned <code>Poll::Ready</code>, but that would burn a lot of CPU cycles you could probably have used for other, more useful things, like running your web browser. Instead, we want the executor to do whatever useful work it can do, and then go to sleep. It should stay asleep until one of the futures can make progress, and only then wake up to do another pass, before going to sleep again.</p>
<h3 id="h2--0007">Waking Up</h3>
<p class="BodyFirst">The condition that determines when to check back with a given future varies widely. It might be “when a network packet arrives on this port,” “when the mouse cursor moves,” “when someone sends on this channel,” “when the CPU receives a particular interrupt,” or even “after this much time has passed.” On top of that, developers can write their own futures that wrap multiple other futures, and thus, they may have several wake-up conditions. Some futures may even introduce their own entirely custom wake events.</p>
<p>To accommodate these many use cases, Rust introduces the notion of a <code>Waker</code>: a way to wake the executor to signal that progress can be made. The <code>Waker</code> is what makes the whole machinery around futures work. The executor constructs a <code>Waker</code> that integrates with the mechanism the executor uses to go to sleep, and passes the <code>Waker</code> in to any <code>Future</code> it polls. How? With the additional parameter to <code>Future::poll</code> that I’ve hidden from you so far. Sorry about that. <a href="#listing8-13" id="listinganchor8-13">Listing 8-13</a> gives the final and true definition for <code>Future</code>—no more lies!</p>
<pre><code>trait Future {
    type Output;
    fn poll(self: Pin&lt;&amp;mut Self&gt;, cx: &amp;mut Context&lt;'_&gt;) -&gt; Poll&lt;Self::Output&gt;;
}</code></pre>
<p class="CodeListingCaption"><a id="listing8-13">Listing 8-13</a>: The actual <code>Future</code> trait with <code>Context</code></p>
<p>The <code>&amp;mut Context</code> contains the <code>Waker</code>. The argument is a <code>Context</code>, not a <code>Waker</code> directly, so that we can augment the asynchronous ecosystem with additional context for futures should that be deemed necessary.</p>
<p><span epub:type="pagebreak" title="134" id="Page_134"/>The primary method on <code>Waker</code> is <code>wake</code> (and the by-reference variant <code>wake_by_ref</code>), which should be called when the future can again make progress. The <code>wake</code> method takes no arguments, and its effects are entirely defined by the executor that constructed the <code>Waker</code>. You see, <code>Waker</code> is secretly generic over the executor. Or, more precisely, whatever constructed the <code>Waker</code> gets to dictate what happens when <code>Waker::wake</code> is called, when a <code>Waker</code> is cloned, and when a <code>Waker</code> is dropped. This all happens through a manually implemented vtable, which functions similarly to the dynamic dispatch we discussed way back in <span class="xref" itemid="xref_target_Chapter 2">Chapter 2</span>. </p>
<p>It’s a somewhat involved process to construct a <code>Waker</code>, and the mechanics of it aren’t all that important for using one, but you can see the building blocks in the <code>RawWakerVTable</code> type in the standard library. It has a constructor that takes the function pointers for <code>wake</code> and <code>wake_by_ref</code> as well as <code>Clone</code> and <code>Drop</code>. The <code>RawWakerVTable</code>, which is usually shared among all of an executor’s wakers, is bundled up with a raw pointer intended to hold data specific to each <code>Waker</code> instance (like which future it’s for) and is turned into a <code>RawWaker</code>. That is in turn passed to <code>Waker::from_raw</code> to produce a safe <code>Waker</code> that can be passed to <code>Future::poll</code>.</p>
<h3 id="h2--0008">Fulfilling the Poll Contract</h3>
<p class="BodyFirst">So far we’ve skirted around what a future actually does with a <code>Waker</code>. The idea is fairly simple: if <code>Future::poll</code> returns <code>Poll::Pending</code>, it is the future’s responsibility to ensure that <em>something</em> calls <code>wake</code> on the provided <code>Waker</code> when the future is next able to make progress. Most futures uphold this property by returning <code>Poll::Pending</code> only if some other future also returned <code>Poll::Pending</code>; in this way, it trivially fulfills the contract of <code>poll</code> since the inner future must follow that same contract. But there can’t be turtles all the way down. At some point, you reach a future that does not poll other futures but instead does something like write to a network socket or attempt to receive on a channel. These are commonly referred to as <em>leaf futures</em> since they have no children. A leaf future has no inner future but instead directly represents some resource that may not yet be ready to return a result.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p>	The poll contract is the reason why the recursive <code>poll</code> call <span class="CodeAnnotation" aria-label="annotation6">6</span> back in <a href="#listing8-4">Listing 8-4</a> is necessary for correctness.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>Leaf futures typically come in one of two shapes: those that wait for events that originate within the same process (like a channel receiver), and those that wait for events external to the process (like a TCP packet read). Those that wait for internal events all tend to follow the same pattern: store the <code>Waker</code> where the code that will wake you up can find it, and have that code call <code>wake</code> on the <code>Waker</code> when it generates the relevant event. For example, consider a leaf future that has to wait for a message on an in-memory channel. It stores its <code>Waker</code> inside the part of the channel that is shared between the sender and the receiver and then returns <code>Poll::Pending</code>. When a sender later comes along and injects a message into the channel, it notices the <code>Waker</code> left there by the waiting receiver and calls <code>wake</code> on the <code>Waker</code> before returning from <code>send</code>. Now the receiver is awoken, and the poll contract is upheld.</p>
<p><span epub:type="pagebreak" title="135" id="Page_135"/>Leaf futures that deal with external events are more involved, as the code that generates the event they’re waiting for knows nothing of futures or wakers. Most often the generating code is the operating system kernel, which knows when a disk is ready or a timer expires, but it could also be a C library that invokes a callback into Rust when an operation completes or some other such external entity. A leaf future wrapping an external resource like this could spin up a thread that executes a blocking system call (or waits for the C callback) and then use the internal waking mechanism, but that would be wasteful; you would spin up a thread every time an operation had to wait and be left with lots of single-use threads sitting around waiting for things.</p>
<p>Instead, executors tend to provide implementations of leaf futures that communicate behind the scenes with the executor to arrange for the appropriate interaction with the operating system. How exactly this is orchestrated depends on the executor and the operating system, but roughly speaking the executor keeps track of all the event sources that it should listen for the next time it goes to sleep. When a leaf future realizes it must wait for an external event, it updates that executor’s state (which it knows about since it’s provided by the executor crate) to include that external event source alongside its <code>Waker</code>. When the executor can no longer make progress, it gathers all of the event sources the various pending leaf futures are waiting for and does a big blocking call to the operating system, telling it to return when <em>any</em> of the resources the leaf futures are waiting on have a new event. On Linux, this is usually achieved with the <code>epoll</code> system call; Windows, the BSDs, macOS, and pretty much every other operating system provide similar mechanisms. When that call returns, the executor calls <code>wake</code> on all the wakers associated with event sources that the operating system reported events for, and thus the poll contract is fulfilled.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p>	A <em>reactor</em><em> </em>is the part of an executor that leaf futures register event sources with and that the executor waits on when it has no more useful work to do. It is possible to separate the executor and the reactor, though bundling them together often improves performance as the two can be co-optimized more readily.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>A knock-on effect of the tight integration between leaf futures and the executor is that leaf futures from one executor crate often cannot be used with a different executor. Or at least, they cannot be used unless the leaf future’s executor is <em>also</em> running. When the leaf future goes to store its <code>Waker</code> and register the event source it’s waiting for, the executor it was built against needs to have that state set up and needs to be running so that the event source will actually be monitored and <code>wake</code> eventually called. There are ways around this, such as having leaf futures spawn an executor if one is not already running, but this is not always advisable as it means that an application can transparently end up with multiple executors running at the same time, which can reduce performance and mean you must inspect the state of multiple executors when debugging.</p>
<p>Library crates that wish to support multiple executors have to be generic over their leaf resources. For example, instead of using a particular executor’s <code/><span epub:type="pagebreak" title="136" id="Page_136"/>TcpStream or <code>File</code> future type, a library can store a generic <code>T: AsyncRead + AsyncWrite</code>. However, the ecosystem has yet to settle on exactly what these traits should look like and which traits are needed, so for the moment it’s fairly difficult to make code truly generic over the executor. For example, while <code>AsyncRead</code> and <code>AsyncWrite</code> are somewhat common across the ecosystem (or can be easily adapted if necessary), no traits currently exist for running a future in the background (<em>spawning</em>, which we’ll discuss later) or for representing a timer.</p>
<h3 id="h2--0009">Waking Is a Misnomer</h3>
<p class="BodyFirst">You may already have realized that <code>Waker::wake</code> doesn’t necessarily seem to <em>wake</em> anything. For example, for external events (as described in the previous section), the executor is already awake, and it might seem silly for it to then call <code>wake</code> on a <code>Waker</code> that belongs to that executor anyway! The reality is that <code>Waker::wake</code> is a bit of a misnomer—in reality, it signals that a particular future is <em>runnable</em>. That is, it tells the executor that it should make sure to poll this particular future when it gets around to it rather than go to sleep again, since this future can make progress. This might wake the executor if it is currently sleeping so it will go poll that future, but that’s more of a side effect than its primary purpose.</p>
<p>It is important for the executor to know which futures are runnable for two reasons. First, it needs to know when it can stop polling a future and go to sleep; it’s not sufficient to just poll each future until it returns <code>Poll::Pending</code>, since polling a later future might make it possible to progress an earlier future. Consider the case where two futures bounce messages back and forth on channels to one another. When you poll one, the other becomes ready, and vice versa. In this case, the executor should never go to sleep, as there is always more work to do.</p>
<p>Second, knowing which futures are runnable lets the executor avoid polling futures unnecessarily. If an executor manages thousands of pending futures, it shouldn’t poll all of them just because an event made one of them runnable. If it did, executing asynchronous code would get very slow indeed.</p>
<h3 id="h2--0010">Tasks and Subexecutors</h3>
<p class="BodyFirst">The futures in an asynchronous program form a tree: a future may contain any number of other futures, which in turn may contain other futures, all the way down to the leaf futures that interact with wakers. The root of each tree is the future you give to whatever the executor’s main “run” function is. These root futures are called <em>tasks</em>, and they are the only point of contact between the executor and the futures tree. The executor calls <code>poll</code> on the task, and from that point forward the code of each contained future must figure out which inner future(s) to poll in response, all the way down to the relevant leaf.</p>
<p>Executors generally construct a separate <code>Waker</code> for each task they poll so that when <code>wake</code> is later called, they know which task was just made runnable and can mark it as such. That is what the raw pointer in <code>RawWaker</code> is for—to differentiate between tasks while sharing the code for the various <code>Waker</code> methods.</p>
<p>When the executor eventually polls a task, that task starts running from the top of its implementation of <code>Future::poll</code> and must decide from there how <span epub:type="pagebreak" title="137" id="Page_137"/>to get to the future deeper down that can now make progress. Since each future knows only about its own fields, and nothing about the whole tree, this all happens through calls to <code>poll</code> that each traverse one edge in the tree.</p>
<p>The choice of which inner future to poll is often obvious, but not always. In the case of <code>async</code>/<code>await</code>, the future to poll is the one we’re blocked waiting for. But in a future that waits for the first of several futures to make progress (often called a <em>select</em>), or for all of a set of futures (often called a <em>join</em>), there are many options. A future that has to make such a choice is basically a subexecutor. It could poll all of its inner futures, but doing so could be quite wasteful. Instead, these subexecutors often wrap the <code>Waker</code> they receive in <code>poll</code>’s <code>Context</code> with their own <code>Waker</code> type before they invoke <code>poll</code> on any inner future. In the wrapping code, they mark the future they just polled as runnable in their own state before they call <code>wake</code> on the original <code>Waker</code>. That way, when the executor eventually polls the subexecutor future again, the subexecutor can consult its own internal state to figure out which of its inner futures caused the current call to <code>poll</code>, and then only poll those.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="box">
<h2>Blocking in async Code</h2>
<p class="BoxBodyFirst">You must be careful about calling synchronous code from asynchronous code, since any time an executor thread spends executing the current task is time it’s <em>not</em> spending running other tasks. If a task occupies the current thread for a prolonged period of time without yielding back to the executor, which might happen when executing a blocking system call (like <code>std::sync::sleep</code>), running a subexecutor that doesn’t yield occasionally, or running in a tight loop with no awaits, then other tasks the current executor thread is responsible for won’t get to run during that time. Usually, this manifests as long delays between when certain tasks can make progress (such as when a client connects) and when they actually get to execute.</p>
<p>Some multithreaded executors implement <em>work-stealing</em> techniques, where idle executor threads steal tasks from busy executor threads, but this is more of a mitigation than a solution. Ultimately, you could end up in a situation where all the executor threads are blocked, and thus no tasks get run until one of the blocking operations completes.</p>
<p>In general, you should be very careful with executing compute-intensive operations or calling functions that could block in an asynchronous context. Such operations should either be converted to asynchronous operations where possible or executed on dedicated threads that then communicate using a primitive that does support asynchrony, like a channel. Some executors also provide mechanisms for indicating that a particular segment of asynchronous code might block or for yielding voluntarily in the context of loops that might otherwise not yield, which can compose part of the solution. A good rule of thumb is that no future should be able to run for more than 1 ms without returning <code>Poll::Pending</code>.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<h2 id="h1--0004"><span epub:type="pagebreak" title="138" id="Page_138"/>Tying It All Together with spawn</h2>
<p class="BodyFirst">When working with asynchronous executors, you may come across an operation that spawns a future. We’re now in a position to explore what that means! Let’s do so by way of example. First, consider the simple server implementation in <a href="#listing8-14" id="listinganchor8-14">Listing 8-14</a>.</p>
<pre><code>async fn handle_client(socket: TcpStream) -&gt; Result&lt;()&gt; {
    <span class="LiteralGray">// Interact with the client over the given socket.</span>
}

async fn server(socket: TcpListener) -&gt; Result&lt;()&gt; {
    while let Some(stream) = socket.accept().await? {
        handle_client(stream).await?;
    }
}</code></pre>
<p class="CodeListingCaption"><a id="listing8-14">Listing 8-14</a>: Handling connections sequentially</p>
<p>The top-level <code>server</code> function is essentially one big future that listens for new connections and does something when a new connection arrives. You hand that future to the executor and say “run this,” and since you don’t want your program to then exit immediately, you’ll probably have the executor block on that future. That is, the call to the executor to run the server future will not return until the server future resolves, which may be never (another client could always arrive later).</p>
<p>Now, every time a new client connection comes in, the code in <a href="#listing8-14">Listing 8-14</a> makes a new future (by calling <code>handle_client</code>) to handle that connection. Since the handling is itself a future, we <code>await</code> it and then move on to the next client connection.</p>
<p>The downside of this approach is that we only ever handle one connection at a time—there is no concurrency. Once the server accepts a connection, the <code>handle_client</code> function is called, and since we <code>await</code> it, we don’t go around the loop again until <code>handle_client</code>’s return future resolves (presumably when that client has left).</p>
<p>We could improve on this by keeping a set of all the client futures and having the loop in which the server accepts new connections also check all the client futures to see if any can make progress. <a href="#listing8-15" id="listinganchor8-15">Listing 8-15</a> shows what that might look like.</p>
<pre><code>async fn server(socket: TcpListener) -&gt; Result&lt;()&gt; {
    let mut clients = Vec::new();
    loop {
        poll_client_futures(&amp;mut clients)?;
        if let Some(stream) = socket.try_accept()? {
            clients.push(handle_client(stream));
        }
    }
}</code></pre>
<p class="CodeListingCaption"><a id="listing8-15">Listing 8-15</a>: Handling connections with a manual executor</p>
<p><span epub:type="pagebreak" title="139" id="Page_139"/>This at least handles many connections concurrently, but it’s quite convoluted. It’s also not very efficient because the code now busy-loops, switching between handling the connections we already have and accepting new ones. And it has to check each connection each time, since it won’t know which ones can make progress (if any). It also can’t <code>await</code> at any point, since that would prevent the other futures from making progress. You could implement your own wakers to ensure that the code polls only the futures that can make progress, but ultimately this is going down the path of developing your own mini-executor.</p>
<p>Another downside of sticking with just the one task for the server that internally contains the futures for all of the client connections is that the server ends up being single-threaded. There is just the one task and to poll it the code must hold an exclusive reference to the task’s future (<code>poll</code> takes <code>Pin&lt;&amp;mut Self&gt;</code>), which only one thread can hold at a time.</p>
<p>The solution is to make each client future its own task and leave it to the executor to multiplex among all the tasks. Which, you guessed it, you do by spawning the future. The executor will continue to block on the server future, but if it cannot make progress on that future, it will use its execution machinery to make progress on the other tasks in the meantime behind the scenes. And best of all, if the executor is multithreaded and your client futures are <code>Send</code>, it can run them in parallel since it can hold <code>&amp;mut</code>s to the separate tasks concurrently. <a href="#listing8-16" id="listinganchor8-16">Listing 8-16</a> gives an example of what this might look like.</p>
<pre><code>async fn server(socket: TcpListener) -&gt; Result&lt;()&gt; {
    while let Some(stream) = socket.accept().await? {
        <span class="LiteralGray">// Spawn a new task with the Future that represents this client.</span>
        <span class="LiteralGray">// The current task will continue to just poll for more connections</span>
        <span class="LiteralGray">// and will run concurrently (and possibly in parallel) with handle_client.</span>
        spawn(handle_client(stream));
    }
}</code></pre>
<p class="CodeListingCaption"><a id="listing8-16">Listing 8-16</a>: Spawning futures to create more tasks that can be polled concurrently</p>
<p>When you spawn a future and thus make it a task, it’s sort of like spawning a thread. The future continues running in the background and is multiplexed concurrently with any other tasks given to the executor. However, unlike a spawned thread, spawned tasks still depend on being polled by the executor. If the executor stops running, either because you drop it or because your code no longer runs the executor’s code, those spawned tasks will stop making progress. In the server example, imagine what will happen if the main server future resolves for some reason. Since the executor has returned control back to your code, it cannot continue doing, well, anything. Multi-threaded executors often spawn background threads that continue to poll tasks even if the executor yields control back to the user’s code, but not all executors do this, so check your executor before you rely on that behavior!</p>
<h2 id="h1--0005"><span epub:type="pagebreak" title="140" id="Page_140"/>Summary</h2>
<p class="BodyFirst">In this chapter, we’ve taken a look behind the scenes of the asynchronous constructs available in Rust. We’ve seen how the compiler implements generators and self-referential types, and why that work was necessary to support what we now know as <code>async</code>/<code>await</code>. We’ve also explored how futures are executed, and how wakers allow executors to multiplex among tasks when only some of them can make progress at any given moment. In the next chapter, we’ll tackle what is perhaps the deepest and most discussed area of Rust: unsafe code. Take a deep breath, and then turn the page.</p>
</section>
</div></body></html>