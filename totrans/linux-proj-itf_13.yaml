- en: Chapter 13. File I/O Buffering
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 13 章 文件 I/O 缓存
- en: In the interests of speed and efficiency, I/O system calls (i.e., the kernel)
    and the I/O functions of the standard C library (i.e., the *stdio* functions)
    buffer data when operating on disk files. In this chapter, we describe both types
    of buffering and consider how they affect application performance. We also look
    at various techniques for influencing and disabling both types of buffering, and
    look at a technique called direct I/O, which is useful for bypassing kernel buffering
    in certain circumstances.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高速度和效率，I/O 系统调用（即内核）和标准 C 库的 I/O 函数（即 *stdio* 函数）在操作磁盘文件时会缓存数据。在本章中，我们将描述这两种缓存方式，并考虑它们如何影响应用程序的性能。我们还将探讨如何影响和禁用这两种缓存的各种技术，并介绍一种称为直接
    I/O 的技术，在某些情况下，它对绕过内核缓存非常有用。
- en: 'Kernel Buffering of File I/O: The Buffer Cache'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内核文件 I/O 缓存：缓冲区缓存
- en: 'When working with disk files, the *read()* and *write()* system calls don’t
    directly initiate disk access. Instead, they simply copy data between a user-space
    buffer and a buffer in the kernel *buffer cache*. For example, the following call
    transfers 3 bytes of data from a buffer in user-space memory to a buffer in kernel
    space:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理磁盘文件时，*read()* 和 *write()* 系统调用不会直接发起磁盘访问。相反，它们只是将数据从用户空间的缓冲区复制到内核的 *缓冲区缓存*
    中。例如，下面的调用将 3 个字节的数据从用户空间内存中的缓冲区传输到内核空间中的缓冲区：
- en: '[PRE0]'
  id: totrans-4
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: At this point, *write()* returns. At some later point, the kernel writes (flushes)
    its buffer to the disk. (Hence, we say that the system call is not *synchronized*
    with the disk operation.) If, in the interim, another process attempts to read
    these bytes of the file, then the kernel automatically supplies the data from
    the buffer cache, rather than from (the outdated contents of) the file.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在此时，*write()* 返回。稍后，内核将其缓冲区（刷新）写入磁盘。（因此，我们说系统调用与磁盘操作并不 *同步*。）如果在此期间，另一个进程尝试读取这些字节的文件，那么内核将自动从缓冲区缓存中提供数据，而不是从（过时的）文件内容中提供。
- en: Correspondingly, for input, the kernel reads data from the disk and stores it
    in a kernel buffer. Calls to *read()* fetch data from this buffer until it is
    exhausted, at which point the kernel reads the next segment of the file into the
    buffer cache. (This is a simplification; for sequential file access, the kernel
    typically performs read-ahead to try to ensure that the next blocks of a file
    are read into the buffer cache before the reading process requires them. We say
    a bit more about read-ahead in Section 13.5.)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 相应地，对于输入，内核从磁盘读取数据并将其存储在内核缓冲区中。对 *read()* 的调用从该缓冲区获取数据，直到缓冲区被耗尽，此时内核会将文件的下一部分读取到缓冲区缓存中。（这是简化的描述；对于顺序文件访问，内核通常会执行预读操作，试图确保在读取过程需要这些文件的下一块时，下一块已经被读取到缓冲区缓存中。我们将在第
    13.5 节进一步讨论预读操作。）
- en: The aim of this design is to allow *read()* and *write()* to be fast, since
    they don’t need to wait on a (slow) disk operation. This design is also efficient,
    since it reduces the number of disk transfers that the kernel must perform.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 该设计的目的是让 *read()* 和 *write()* 操作快速进行，因为它们不需要等待（慢速的）磁盘操作。这个设计也是高效的，因为它减少了内核必须执行的磁盘传输次数。
- en: The Linux kernel imposes no fixed upper limit on the size of the buffer cache.
    The kernel will allocate as many buffer cache pages as are required, limited only
    by the amount of available physical memory and the demands for physical memory
    for other purposes (e.g., holding the text and data pages required by running
    processes). If available memory is scarce, then the kernel flushes some modified
    buffer cache pages to disk, in order to free those pages for reuse.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Linux 内核对缓冲区缓存的大小没有固定的上限。内核将根据需要分配足够的缓冲区缓存页面，仅受可用物理内存量以及其他用途（例如，运行中的进程所需的文本和数据页面）对物理内存的需求限制。如果可用内存紧张，内核将把一些已修改的缓冲区缓存页面刷新到磁盘，以便释放这些页面供再次使用。
- en: Note
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Speaking more precisely, from kernel 2.4 onward, Linux no longer maintains a
    separate buffer cache. Instead, file I/O buffers are included in the page cache,
    which, for example, also contains pages from memory-mapped files. Nevertheless,
    in the discussion in the main text, we use the term *buffer cache*, since that
    term is historically common on UNIX implementations.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 更准确地说，从内核 2.4 版本开始，Linux 不再维护一个独立的缓冲区缓存。相反，文件 I/O 缓冲区被包含在页面缓存中，例如，页面缓存还包含来自内存映射文件的页面。然而，在主文本中，我们使用术语
    *缓冲区缓存*，因为这个术语在 UNIX 实现中历史悠久。
- en: Effect of buffer size on I/O system call performance
  id: totrans-11
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 缓冲区大小对 I/O 系统调用性能的影响
- en: The kernel performs the same number of disk accesses, regardless of whether
    we perform 1000 writes of a single byte or a single write of a 1000 bytes. However,
    the latter is preferable, since it requires a single system call, while the former
    requires 1000\. Although much faster than disk operations, system calls nevertheless
    take an appreciable amount of time, since the kernel must trap the call, check
    the validity of the system call arguments, and transfer data between user space
    and kernel space (refer to [System Calls](ch03.html#system_calls "System Calls")
    for further details).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 内核执行的磁盘访问次数是相同的，无论我们是进行1000次单字节写入，还是一次1000字节的写入。然而，后者更可取，因为它只需要一次系统调用，而前者需要1000次。尽管比磁盘操作快得多，系统调用仍然需要一定时间，因为内核必须捕获调用，检查系统调用参数的有效性，并在用户空间和内核空间之间传输数据（有关详细信息，请参见[系统调用](ch03.html#system_calls
    "系统调用")）。
- en: 'The impact of performing file I/O using different buffer sizes can be seen
    by running the program in [Example 4-1](ch04.html#using_i_solidus_o_system_calls
    "Example 4-1. Using I/O system calls") (in [Universality of I/O](ch04.html#universality_of_i_solidus_o
    "Universality of I/O")) with different `BUF_SIZE` values. (The `BUF_SIZE` constant
    specifies how many bytes are transferred by each call to *read()* and *write()*.)
    [Table 13-1](ch13.html#time_required_to_duplicate_a_file_of_100 "Table 13-1. Time
    required to duplicate a file of 100 million bytes") shows the time that this program
    requires to copy a file of 100 million bytes on a Linux *ext2* file system using
    different `BUF_SIZE` values. Note the following points concerning the information
    in this table:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 使用不同缓冲区大小进行文件I/O操作的影响可以通过运行[示例4-1](ch04.html#using_i_solidus_o_system_calls
    "示例4-1：使用I/O系统调用")（在[输入输出的普遍性](ch04.html#universality_of_i_solidus_o "输入输出的普遍性")）来观察，使用不同的`BUF_SIZE`值。（`BUF_SIZE`常量指定每次调用*read()*和*write()*时传输的字节数。）[表13-1](ch13.html#time_required_to_duplicate_a_file_of_100
    "表13-1：复制100百万字节文件所需的时间")显示了该程序在Linux *ext2*文件系统上使用不同`BUF_SIZE`值复制100百万字节文件所需的时间。请注意，关于此表格中的信息，有以下几点：
- en: The *Elapsed* and *Total CPU* time columns have the obvious meanings. The *User
    CPU* and *System CPU* columns show a breakdown of the *Total CPU* time into, respectively,
    the time spent executing code in user mode and the time spent executing kernel
    code (i.e., system calls).
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*经过的时间*和*总CPU*时间列有显而易见的含义。*用户CPU*和*系统CPU*列将*总CPU*时间分别细分为执行用户模式代码的时间和执行内核代码（即系统调用）的时间。'
- en: The tests shown in the table were performed using a vanilla 2.6.30 kernel on
    an *ext2* file system with a block size of 4096 bytes.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表格中显示的测试是在使用*ext2*文件系统、块大小为4096字节的原生2.6.30内核上进行的。
- en: Note
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: When we talk about a *vanilla kernel*, we mean an unpatched mainline kernel.
    This is in contrast to kernels that are supplied by most distributors, which often
    include various patches to fix bugs or add features.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈到*原生内核*时，我们指的是未打补丁的主线内核。这与大多数发行版提供的内核不同，后者通常会包含各种补丁以修复bug或添加功能。
- en: Each row shows the average of 20 runs for the given buffer size. In these tests,
    as in other tests shown later in this chapter, the file system was unmounted and
    remounted between each execution of the program to ensure that the buffer cache
    for the file system was empty. Timing was done using the shell *time* command.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每一行显示了给定缓冲区大小的20次运行的平均值。在这些测试中，和本章后面展示的其他测试一样，文件系统在每次程序执行之间都被卸载并重新挂载，以确保文件系统的缓冲区缓存为空。计时是使用shell的*time*命令完成的。
- en: Table 13-1. Time required to duplicate a file of 100 million bytes
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 表13-1：复制100百万字节文件所需的时间
- en: '| `BUF_SIZE` | Time (seconds) |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| `BUF_SIZE` | 时间（秒） |'
- en: '| --- | --- |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Elapsed | Total CPU | User CPU | System CPU |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 经过时间 | 总CPU | 用户CPU | 系统CPU |'
- en: '| --- | --- | --- | --- |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| `1` | `107.43` | `107.32` | `8.20` | `99.12` |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| `1` | `107.43` | `107.32` | `8.20` | `99.12` |'
- en: '| `2` | `54.16` | `53.89` | `4.13` | `49.76` |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| `2` | `54.16` | `53.89` | `4.13` | `49.76` |'
- en: '| `4` | `31.72` | `30.96` | `2.30` | `28.66` |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| `4` | `31.72` | `30.96` | `2.30` | `28.66` |'
- en: '| `8` | `15.59` | `14.34` | `1.08` | `13.26` |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| `8` | `15.59` | `14.34` | `1.08` | `13.26` |'
- en: '| `16` | `7.50` | `7.14` | `0.51` | `6.63` |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| `16` | `7.50` | `7.14` | `0.51` | `6.63` |'
- en: '| `32` | `3.76` | `3.68` | `0.26` | `3.41` |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| `32` | `3.76` | `3.68` | `0.26` | `3.41` |'
- en: '| `64` | `2.19` | `2.04` | `0.13` | `1.91` |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| `64` | `2.19` | `2.04` | `0.13` | `1.91` |'
- en: '| `128` | `2.16` | `1.59` | `0.11` | `1.48` |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| `128` | `2.16` | `1.59` | `0.11` | `1.48` |'
- en: '| `256` | `2.06` | `1.75` | `0.10` | `1.65` |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| `256` | `2.06` | `1.75` | `0.10` | `1.65` |'
- en: '| `512` | `2.06` | `1.03` | `0.05` | `0.98` |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| `512` | `2.06` | `1.03` | `0.05` | `0.98` |'
- en: '| `1024` | `2.05` | `0.65` | `0.02` | `0.63` |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| `1024` | `2.05` | `0.65` | `0.02` | `0.63` |'
- en: '| `4096` | `2.05` | `0.38` | `0.01` | `0.38` |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| `4096` | `2.05` | `0.38` | `0.01` | `0.38` |'
- en: '| `16384` | `2.05` | `0.34` | `0.00` | `0.33` |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| `16384` | `2.05` | `0.34` | `0.00` | `0.33` |'
- en: '| `65536` | `2.06` | `0.32` | `0.00` | `0.32` |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| `65536` | `2.06` | `0.32` | `0.00` | `0.32` |'
- en: Since the total amount of data transferred (and hence the number of disk operations)
    is the same for the various buffer sizes, what [Table 13-1](ch13.html#time_required_to_duplicate_a_file_of_100
    "Table 13-1. Time required to duplicate a file of 100 million bytes") illustrates
    is the overhead of making *read()* and *write()* calls. With a buffer size of
    1 byte, 100 million calls are made to *read()* and *write()*. With a buffer size
    of 4096 bytes, the number of invocations of each system call falls to around 24,000,
    and near optimal performance is reached. Beyond this point, there is no significant
    performance improvement, because the cost of making *read()* and *write()* system
    calls becomes negligible compared to the time required to copy data between user
    space and kernel space, and to perform actual disk I/O.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 由于不同缓冲区大小的情况下传输的数据总量（因此磁盘操作的次数）是相同的，[表 13-1](ch13.html#time_required_to_duplicate_a_file_of_100
    "表 13-1. 复制一个 1 亿字节文件所需的时间")展示的是进行 *read()* 和 *write()* 调用的开销。使用 1 字节的缓冲区时，会进行
    1 亿次 *read()* 和 *write()* 调用。使用 4096 字节的缓冲区时，每个系统调用的调用次数降到大约 24,000 次，并且接近最优性能。超过这一点后，性能不再显著提高，因为与在用户空间和内核空间之间复制数据以及执行实际磁盘
    I/O 所需的时间相比，进行 *read()* 和 *write()* 系统调用的成本变得微不足道。
- en: Note
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The final rows of [Table 13-1](ch13.html#time_required_to_duplicate_a_file_of_100
    "Table 13-1. Time required to duplicate a file of 100 million bytes") allow us
    to make rough estimates of the times required for data transfer between user space
    and kernel space, and for file I/O. Since the number of system calls in these
    cases is relatively small, their contribution to the elapsed and CPU times is
    negligible. Thus, we can say that the *System CPU* time is essentially measuring
    the time for data transfers between user space and kernel space. The *Elapsed*
    time value gives us an estimate of the time required for data transfer to and
    from the disk. (As we’ll see in a moment, this is mainly the time required for
    disk reads.)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 13-1](ch13.html#time_required_to_duplicate_a_file_of_100 "表 13-1. 复制一个 1
    亿字节文件所需的时间")的最后几行使我们能够大致估算用户空间和内核空间之间的数据传输时间，以及文件 I/O 所需的时间。由于这些情况中的系统调用数量相对较少，它们对经过时间和
    CPU 时间的贡献可以忽略不计。因此，我们可以说，*系统 CPU* 时间实际上是在测量用户空间和内核空间之间的数据传输时间。*经过* 时间值则给出了数据传输进出磁盘所需时间的估算。（正如我们稍后看到的，这主要是磁盘读取所需的时间。）'
- en: In summary, if we are transferring a large amount of data to or from a file,
    then by buffering data in large blocks, and thus performing fewer system calls,
    we can greatly improve I/O performance.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，如果我们正在向文件中传输大量数据，或者从文件中传输大量数据，那么通过将数据缓冲在大块中，从而减少系统调用次数，我们可以显著提高 I/O 性能。
- en: 'The data in [Table 13-1](ch13.html#time_required_to_duplicate_a_file_of_100
    "Table 13-1. Time required to duplicate a file of 100 million bytes") measures
    a range of factors: the time to perform *read()* and *write()* system calls, the
    time to transfer data between buffers in kernel space and user space, and the
    time to transfer data between kernel buffers and the disk. Let’s consider the
    last factor further. Obviously, transferring the contents of the input file into
    the buffer cache is unavoidable. However, we already saw that *write()* returns
    immediately after transferring data from user space to the kernel buffer cache.
    Since the RAM size on the test system (4 GB) far exceeds the size of the file
    being copied (100 MB), we can assume that by the time the program completes, the
    output file has not actually been written to disk. Therefore, as a further experiment,
    we ran a program that simply wrote arbitrary data to a file using different *write()*
    buffer sizes. The results are shown in [Table 13-2](ch13.html#time_required_to_write_a_file_of_100_mil
    "Table 13-2. Time required to write a file of 100 million bytes").'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 13-1](ch13.html#time_required_to_duplicate_a_file_of_100 "表 13-1. 写入 1 亿字节文件所需时间")中的数据衡量了一系列因素：执行
    *read()* 和 *write()* 系统调用的时间、在内核空间和用户空间之间传输数据的时间，以及在内核缓冲区和磁盘之间传输数据的时间。我们进一步考虑最后一个因素。显然，将输入文件的内容传输到缓冲区缓存是不可避免的。然而，我们已经看到，*write()*
    在将数据从用户空间传输到内核缓冲区缓存后立即返回。由于测试系统上的 RAM 大小（4 GB）远大于正在复制的文件的大小（100 MB），我们可以假设在程序完成时，输出文件实际上并未写入磁盘。因此，作为进一步实验，我们运行了一个程序，简单地使用不同的
    *write()* 缓冲区大小将任意数据写入文件。结果如 [表 13-2](ch13.html#time_required_to_write_a_file_of_100_mil
    "表 13-2. 写入 1 亿字节文件所需时间") 所示。'
- en: Again, the data shown in [Table 13-2](ch13.html#time_required_to_write_a_file_of_100_mil
    "Table 13-2. Time required to write a file of 100 million bytes") was obtained
    from kernel 2.6.30, on an *ext2* file system with a 4096-byte block size, and
    each row shows the average of 20 runs. We don’t show the test program (`filebuff/write_bytes.c`),
    but it is available in the source code distribution for this book.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，数据显示在 [表 13-2](ch13.html#time_required_to_write_a_file_of_100_mil "表 13-2.
    写入 1 亿字节文件所需时间") 中是从内核 2.6.30 中获得的，在一个 *ext2* 文件系统上，块大小为 4096 字节，每行显示的是 20 次运行的平均值。我们没有展示测试程序（`filebuff/write_bytes.c`），但它可以在本书的源代码发行版中找到。
- en: Table 13-2. Time required to write a file of 100 million bytes
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 表 13-2. 写入 1 亿字节文件所需时间
- en: '| `BUF_SIZE` | Time (seconds) |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| `BUF_SIZE` | 时间（秒） |'
- en: '| --- | --- |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Elapsed | Total CPU | User CPU | System CPU |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 经过时间 | 总 CPU | 用户 CPU | 系统 CPU |'
- en: '| --- | --- | --- | --- |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| `1` | `72.13` | `72.11` | `5.00` | `67.11` |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| `1` | `72.13` | `72.11` | `5.00` | `67.11` |'
- en: '| `2` | `36.19` | `36.17` | `2.47` | `33.70` |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| `2` | `36.19` | `36.17` | `2.47` | `33.70` |'
- en: '| `4` | `20.01` | `19.99` | `1.26` | `18.73` |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| `4` | `20.01` | `19.99` | `1.26` | `18.73` |'
- en: '| `8` | `9.35` | `9.32` | `0.62` | `8.70` |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| `8` | `9.35` | `9.32` | `0.62` | `8.70` |'
- en: '| `16` | `4.70` | `4.68` | `0.31` | `4.37` |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| `16` | `4.70` | `4.68` | `0.31` | `4.37` |'
- en: '| `32` | `2.39` | `2.39` | `0.16` | `2.23` |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| `32` | `2.39` | `2.39` | `0.16` | `2.23` |'
- en: '| `64` | `1.24` | `1.24` | `0.07` | `1.16` |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| `64` | `1.24` | `1.24` | `0.07` | `1.16` |'
- en: '| `128` | `0.67` | `0.67` | `0.04` | `0.63` |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| `128` | `0.67` | `0.67` | `0.04` | `0.63` |'
- en: '| `256` | `0.38` | `0.38` | `0.02` | `0.36` |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| `256` | `0.38` | `0.38` | `0.02` | `0.36` |'
- en: '| `512` | `0.24` | `0.24` | `0.01` | `0.23` |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| `512` | `0.24` | `0.24` | `0.01` | `0.23` |'
- en: '| `1024` | `0.17` | `0.17` | `0.01` | `0.16` |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| `1024` | `0.17` | `0.17` | `0.01` | `0.16` |'
- en: '| `4096` | `0.11` | `0.11` | `0.00` | `0.11` |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| `4096` | `0.11` | `0.11` | `0.00` | `0.11` |'
- en: '| `16384` | `0.10` | `0.10` | `0.00` | `0.10` |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| `16384` | `0.10` | `0.10` | `0.00` | `0.10` |'
- en: '| `65536` | `0.09` | `0.09` | `0.00` | `0.09` |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| `65536` | `0.09` | `0.09` | `0.00` | `0.09` |'
- en: '[Table 13-2](ch13.html#time_required_to_write_a_file_of_100_mil "Table 13-2. Time
    required to write a file of 100 million bytes") shows the costs just for making
    *write()* system calls and transferring data from user space to the kernel buffer
    cache using different *write()* buffer sizes. For larger buffer sizes, we see
    significant differences from the data shown in [Table 13-1](ch13.html#time_required_to_duplicate_a_file_of_100
    "Table 13-1. Time required to duplicate a file of 100 million bytes"). For example,
    for a 65,536-byte buffer size, the elapsed time in [Table 13-1](ch13.html#time_required_to_duplicate_a_file_of_100
    "Table 13-1. Time required to duplicate a file of 100 million bytes") is 2.06
    seconds, while for [Table 13-2](ch13.html#time_required_to_write_a_file_of_100_mil
    "Table 13-2. Time required to write a file of 100 million bytes") it is 0.09 seconds.
    This is because no actual disk I/O is being performed in the latter case. In other
    words, the majority of the time required for the large buffer cases in [Table 13-1](ch13.html#time_required_to_duplicate_a_file_of_100
    "Table 13-1. Time required to duplicate a file of 100 million bytes") is due to
    the disk reads.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[表13-2](ch13.html#time_required_to_write_a_file_of_100_mil "表13-2. 写入1亿字节文件所需时间")展示了仅进行*write()*系统调用并使用不同的*write()*缓冲区大小将数据从用户空间传输到内核缓冲区缓存的成本。对于较大的缓冲区大小，我们看到与[表13-1](ch13.html#time_required_to_duplicate_a_file_of_100
    "表13-1. 复制1亿字节文件所需时间")中显示的数据存在显著差异。例如，对于65,536字节的缓冲区大小，[表13-1](ch13.html#time_required_to_duplicate_a_file_of_100
    "表13-1. 复制1亿字节文件所需时间")中的耗时为2.06秒，而在[表13-2](ch13.html#time_required_to_write_a_file_of_100_mil
    "表13-2. 写入1亿字节文件所需时间")中为0.09秒。这是因为在后者的情况下没有实际的磁盘I/O操作。换句话说，[表13-1](ch13.html#time_required_to_duplicate_a_file_of_100
    "表13-1. 复制1亿字节文件所需时间")中的大多数时间都是由于磁盘读取造成的。'
- en: As we’ll see in [Controlling Kernel Buffering of File I/O](ch13.html#controlling_kernel_buffering_of_file_i_s
    "Controlling Kernel Buffering of File I/O"), when we force output operations to
    block until data is transferred to the disk, the times for *write()* calls rise
    significantly.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[控制内核缓存文件I/O](ch13.html#controlling_kernel_buffering_of_file_i_s "控制内核缓存文件I/O")中看到的，当我们强制输出操作阻塞直到数据被传输到磁盘时，*write()*
    调用的时间会显著增加。
- en: Finally, it is worth noting that the information in [Table 13-2](ch13.html#time_required_to_write_a_file_of_100_mil
    "Table 13-2. Time required to write a file of 100 million bytes") (and later,
    in [Table 13-3](ch13.html#impact_of_the_o_underscore_sync_flag_on "Table 13-3. Impact
    of the O_SYNC flag on the speed of writing 1 million bytes")) represents just
    one form of (naive) benchmark for a file system. Furthermore, the results will
    probably show some variation across file systems. File systems can be measured
    by various other criteria, such as performance under heavy multiuser load, speed
    of file creation and deletion, time required to search for a file in a large directory,
    space required to store small files, or maintenance of file integrity in the event
    of a system crash. Where the performance of I/O or other file-system operations
    is critical, there is no substitute for application-specific benchmarks on the
    target platform.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，值得注意的是，[表13-2](ch13.html#time_required_to_write_a_file_of_100_mil "表13-2.
    写入1亿字节文件所需时间")中的信息（以及后面的[表13-3](ch13.html#impact_of_the_o_underscore_sync_flag_on
    "表13-3. O_SYNC标志对写入100万字节速度的影响")）仅代表文件系统的一个（天真的）基准测试形式。此外，结果可能会在不同的文件系统之间有所差异。文件系统还可以通过其他各种标准进行衡量，例如在重负载多用户情况下的性能、文件创建和删除的速度、大目录中查找文件所需的时间、存储小文件所需的空间，或者在系统崩溃时维护文件完整性。当I/O或其他文件系统操作的性能至关重要时，没有什么能替代针对目标平台进行的特定应用程序基准测试。
- en: Buffering in the *stdio* Library
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*stdio*库中的缓冲'
- en: Buffering of data into large blocks to reduce system calls is exactly what is
    done by the C library I/O functions (e.g., *fprintf()*, *fscanf()*, *fgets()*,
    *fputs()*, *fputc()*, *fgetc()*) when operating on disk files. Thus, using the
    *stdio* library relieves us of the task of buffering data for output with *write()*
    or input via *read()*.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据缓冲到大块以减少系统调用，正是C库I/O函数（例如，*fprintf()*, *fscanf()*, *fgets()*, *fputs()*,
    *fputc()*, *fgetc()*）在操作磁盘文件时所做的。因此，使用*stdio*库让我们免去了通过*write()*输出数据或通过*read()*输入数据时手动缓冲数据的任务。
- en: Setting the buffering mode of a *stdio* stream
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 设置*stdio*流的缓冲模式
- en: The *setvbuf()* function controls the form of buffering employed by the *stdio*
    library.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*setvbuf()* 函数控制*stdio*库所采用的缓冲形式。'
- en: '[PRE1]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Returns 0 on success, or nonzero on error
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 成功时返回0，出错时返回非零值
- en: The *stream* argument identifies the file stream whose buffering is to be modified.
    After the stream has been opened, the *setvbuf()* call must be made before calling
    any other *stdio* function on the stream. The *setvbuf()* call affects the behavior
    of all subsequent *stdio* operations on the specified stream.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '*stream*参数标识要修改缓冲区的文件流。在流打开之后，必须在调用任何其他*stdio*函数之前调用*setvbuf()*。*setvbuf()*调用会影响对指定流的所有后续*stdio*操作的行为。'
- en: Note
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The streams used by the *stdio* library should not be confused with the STREAMS
    facility of System V. The System V STREAMS facility is not implemented in the
    mainline Linux kernel.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*stdio*库使用的流不应与System V的STREAMS功能混淆。System V的STREAMS功能并未在主线Linux内核中实现。'
- en: 'The *buf* and *size* arguments specify the buffer to be used for *stream*.
    These arguments may be specified in two ways:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*buf*和*size*参数指定用于*stream*的缓冲区。这些参数可以通过两种方式指定：'
- en: If *buf* is non-`NULL`, then it points to a block of memory of *size* bytes
    that is to be used as the buffer for *stream*. Since the buffer pointed to by
    *buf* is then used by the *stdio* library, it should be either statically allocated
    or dynamically allocated on the heap (using *malloc()* or similar). It should
    not be allocated as a local function variable on the stack, since chaos will result
    when that function returns and its stack frame is deallocated.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果*buf*非`NULL`，则它指向一个大小为*size*字节的内存块，作为*stream*的缓冲区使用。由于*buf*指向的缓冲区将由*stdio*库使用，因此它应该是静态分配的或通过堆动态分配的（使用*malloc()*或类似的函数）。不应该将其作为局部函数变量分配在栈上，因为当该函数返回并且栈帧被释放时会导致混乱。
- en: If *buf* is `NULL`, then the *stdio* library automatically allocates a buffer
    for use with *stream* (unless we select unbuffered I/O, as described below). SUSv3
    permits, but does not require, an implementation to use *size* to determine the
    size for this buffer. In the *glibc* implementation, *size* is ignored in this
    case.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果*buf*为`NULL`，则*stdio*库会自动为*stream*分配一个缓冲区（除非我们选择无缓冲I/O，如下所述）。SUSv3允许，但不要求，实现使用*size*来确定此缓冲区的大小。在*glibc*实现中，在这种情况下，*size*会被忽略。
- en: 'The *mode* argument specifies the type of buffering and has one of the following
    values:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '*mode*参数指定缓冲类型，并具有以下值之一：'
- en: '`_IONBF`'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '`_IONBF`'
- en: Don’t buffer I/O. Each *stdio* library call results in an immediate *write()*
    or *read()* system call. The *buf* and *size* arguments are ignored, and can be
    specified as `NULL` and 0, respectively. This is the default for *stderr*, so
    that error output is guaranteed to appear immediately.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 不使用缓冲I/O。每个*stdio*库调用都会立即触发*write()*或*read()*系统调用。*buf*和*size*参数会被忽略，并且可以分别指定为`NULL`和0。这是*stderr*的默认行为，因此错误输出保证会立即显示。
- en: '`_IOLBF`'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`_IOLBF`'
- en: Employ line-buffered I/O. This flag is the default for streams referring to
    terminal devices. For output streams, data is buffered until a newline character
    is output (unless the buffer fills first). For input streams, data is read a line
    at a time.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 使用行缓冲I/O。这个标志是指向终端设备的流的默认模式。对于输出流，数据会缓冲，直到输出一个换行符（除非缓冲区先满）。对于输入流，数据会一行一行地读取。
- en: '`_IOFBF`'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`_IOFBF`'
- en: Employ fully buffered I/O. Data is read or written (via calls to *read()* or
    *write()*) in units equal to the size of the buffer. This mode is the default
    for streams referring to disk files.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 使用全缓冲I/O。数据通过*read()*或*write()*函数以等于缓冲区大小的单位读取或写入。这种模式是指向磁盘文件的流的默认模式。
- en: 'The following code demonstrates the use of *setvbuf()*:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码演示了*setvbuf()*的使用：
- en: '[PRE2]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note that *setvbuf()* returns a nonzero value (not necessarily -1) on error.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，*setvbuf()*在发生错误时返回一个非零值（不一定是-1）。
- en: The *setbuf()* function is layered on top of *setvbuf()*, and performs a similar
    task.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*setbuf()*函数是*setvbuf()*的封装，执行类似的任务。'
- en: '[PRE3]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Other than the fact that it doesn’t return a function result, the call *setbuf(fp,
    buf)* is equivalent to:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 除了没有返回函数结果之外，调用*setbuf(fp, buf)*等同于：
- en: '[PRE4]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The *buf* argument is specified either as `NULL`, for no buffering, or as a
    pointer to a caller-allocated buffer of `BUFSIZ` bytes. (`BUFSIZ` is defined in
    `<stdio.h>`. In the *glibc* implementation, this constant has the value 8192,
    which is typical.)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '*buf*参数可以指定为`NULL`，表示不使用缓冲，或者指定为指向调用者分配的`BUFSIZ`字节的缓冲区指针。(`BUFSIZ`在`<stdio.h>`中定义。在*glibc*实现中，常量的值为8192，这是典型的值。)'
- en: The *setbuffer()* function is similar to *setbuf()*, but allows the caller to
    specify the size of *buf*.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*setbuffer()*函数类似于*setbuf()*，但允许调用者指定*buf*的大小。'
- en: '[PRE5]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The call *setbuffer(fp, buf, size)* is equivalent to the following:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 调用 *setbuffer(fp, buf, size)* 相当于以下操作：
- en: '[PRE6]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The *setbuffer()* function is not specified in SUSv3, but is available on most
    UNIX implementations.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*setbuffer()* 函数在 SUSv3 中没有规定，但在大多数 UNIX 实现中是可用的。'
- en: Flushing a *stdio* buffer
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 刷新 *stdio* 缓冲区
- en: Regardless of the current buffering mode, at any time, we can force the data
    in a *stdio* output stream to be written (i.e., flushed to a kernel buffer via
    *write()*) using the *fflush()* library function. This function flushes the output
    buffer for the specified *stream*.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 无论当前的缓冲模式如何，随时我们都可以强制将 *stdio* 输出流中的数据写入（即通过 *write()* 刷新到内核缓冲区），这可以通过 *fflush()*
    库函数实现。此函数会刷新指定 *stream* 的输出缓冲区。
- en: '[PRE7]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注释
- en: Returns 0 on success, `EOF` on error
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 成功时返回 0，出错时返回 `EOF`。
- en: If *stream* is `NULL`, *fflush()* flushes all *stdio* buffers.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 *stream* 为 `NULL`，则 *fflush()* 会刷新所有 *stdio* 缓冲区。
- en: The *fflush()* function can also be applied to an input stream. This causes
    any buffered input to be discarded. (The buffer will be refilled when the program
    next tries to read from the stream.)
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*fflush()* 函数也可以应用于输入流。这会导致任何缓存的输入被丢弃。（当程序下一次尝试从流中读取时，缓冲区将被重新填充。）'
- en: A *stdio* buffer is automatically flushed when the corresponding stream is closed.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 当相应的流被关闭时，*stdio* 缓冲区会自动刷新。
- en: 'In many C library implementations, including *glibc*, if *stdin* and *stdout*
    refer to a terminal, then an implicit *fflush(stdout)* is performed whenever input
    is read from *stdin*. This has the effect of flushing any prompts written to *stdout*
    that don’t include a terminating newline character (e.g., *printf(“Date: ”)*).
    However, this behavior is not specified in SUSv3 or C99 and is not implemented
    in all C libraries. Portable programs should use explicit *fflush(stdout)* calls
    to ensure that such prompts are displayed.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '在许多 C 库实现中，包括 *glibc*，如果 *stdin* 和 *stdout* 引用的是终端，那么每次从 *stdin* 读取输入时，会隐式执行
    *fflush(stdout)*。这样会刷新任何写入 *stdout* 且未包含终止换行符的提示信息（例如，*printf("Date: ")*）。然而，这种行为并未在
    SUSv3 或 C99 中规定，也不是所有 C 库都实现了此行为。便携式程序应该显式调用 *fflush(stdout)*，以确保这些提示信息能够显示出来。'
- en: Note
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注释
- en: The C99 standard makes two requirements if a stream is opened for both input
    and output. First, an output operation can’t be directly followed by an input
    operation without an intervening call to *fflush()* or one of the file-positioning
    functions (*fseek()*, *fsetpos()*, or *rewind()*). Second, an input operation
    can’t be directly followed by an output operation without an intervening call
    to one of the file-positioning functions, unless the input operation encountered
    end-of-file.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个流既用于输入又用于输出，C99标准规定了两个要求。首先，输出操作不能直接跟在输入操作后面，除非中间调用了*fflush()* 或者某些文件定位函数（*fseek()*、*fsetpos()*
    或 *rewind()*）。其次，输入操作不能直接跟在输出操作后面，除非中间调用了某些文件定位函数，除非输入操作遇到了文件末尾（end-of-file）。
- en: Controlling Kernel Buffering of File I/O
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 控制文件 I/O 的内核缓冲
- en: It is possible to force flushing of kernel buffers for output files. Sometimes,
    this is necessary if an application (e.g., a database journaling process) must
    ensure that output really has been written to the disk (or at least to the disk’s
    hardware cache) before continuing.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 可以强制刷新输出文件的内核缓冲区。有时，当应用程序（例如，数据库日志处理程序）必须确保输出确实已写入磁盘（或至少写入磁盘的硬件缓存）后再继续时，这是必要的。
- en: Before we describe the system calls used to control kernel buffering, it is
    useful to consider a few relevant definitions from SUSv3.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在描述用于控制内核缓冲的系统调用之前，了解一些来自 SUSv3 的相关定义是很有用的。
- en: Synchronized I/O data integrity and synchronized I/O file integrity
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 同步I/O数据完整性和同步I/O文件完整性
- en: SUSv3 defines the term *synchronized I/O completion* to mean “an I/O operation
    that has either been successfully transferred [to the disk] or diagnosed as unsuccessful.”
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: SUSv3 定义了 *synchronized I/O completion*（同步I/O完成）为“一个I/O操作，要么已经成功地转移到[磁盘]，要么被诊断为不成功”。
- en: SUSv3 defines two different types of synchronized I/O completion. The difference
    between the types involves the *metadata* (“data about data”) describing the file,
    which the kernel stores along with the data for a file. We consider file metadata
    in detail when we look at file i-nodes in [I-nodes](ch14.html#i-nodes "I-nodes"),
    but for now, it is sufficient to note that the file metadata includes information
    such as the file owner and group; file permissions; file size; number of (hard)
    links to the file; timestamps indicating the time of the last file access, last
    file modification, and last metadata change; and file data block pointers.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: SUSv3 定义了两种不同类型的同步 I/O 完成。这些类型的区别在于 *元数据*（即“关于数据的数据”），描述文件的元数据，内核将其与文件数据一起存储。我们在查看文件
    i-nodes 时会详细讨论文件元数据，参见 [I-nodes](ch14.html#i-nodes "I-nodes")，但现在仅需注意，文件元数据包括文件所有者和组、文件权限、文件大小、文件的（硬）链接数、最后访问时间、最后修改时间和最后元数据更改时间等时间戳，以及文件数据块指针等信息。
- en: The first type of synchronized I/O completion defined by SUSv3 is *synchronized
    I/O data integrity completion*. This is concerned with ensuring that a file data
    update transfers sufficient information to allow a later retrieval of that data
    to proceed.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: SUSv3 定义的第一种同步 I/O 完成类型是 *同步 I/O 数据完整性完成*。这涉及确保文件数据更新传输足够的信息，以便以后能够成功检索该数据。
- en: For a read operation, this means that the requested file data has been transferred
    (from the disk) to the process. If there were any pending write operations affecting
    the requested data, these are transferred to the disk before performing the read.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于读操作，这意味着请求的文件数据已经从磁盘传输到进程。如果有任何待处理的写操作影响了请求的数据，这些数据会在执行读取操作之前传输到磁盘。
- en: For a write operation, this means that the data specified in the write request
    has been transferred (to the disk) and all file metadata required to retrieve
    that data has also been transferred. The key point to note here is that not all
    modified file metadata attributes need to be transferred to allow the file data
    to be retrieved. An example of a modified file metadata attribute that would need
    to be transferred is the file size (if the write operation extended the file).
    By contrast, modified file timestamps would not need to be transferred to disk
    before a subsequent data retrieval could proceed.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于写操作，这意味着写请求中指定的数据已经传输（到磁盘），并且检索该数据所需的所有文件元数据也已传输。需要注意的关键点是，并非所有修改过的文件元数据属性都需要被传输才能允许文件数据被检索。一个需要传输的修改文件元数据属性的例子是文件大小（如果写操作扩展了文件）。相比之下，修改的文件时间戳在后续的数据检索之前并不需要传输到磁盘。
- en: The other type of synchronized I/O completion defined by SUSv3 is *synchronized
    I/O file integrity completion*, which is a superset of synchronized I/O data integrity
    completion. The difference with this mode of I/O completion is that during a file
    update, *all* updated file metadata is transferred to disk, even if it is not
    necessary for the operation of a subsequent read of the file data.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: SUSv3 定义的另一种同步 I/O 完成类型是 *同步 I/O 文件完整性完成*，这是同步 I/O 数据完整性完成的超集。与此模式的 I/O 完成的区别在于，在文件更新期间，*所有*
    更新的文件元数据都将被传输到磁盘，即使这些数据在后续读取文件数据时并不需要。
- en: System calls for controlling kernel buffering of file I/O
  id: totrans-120
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 控制内核文件 I/O 缓冲的系统调用
- en: The *fsync()* system call causes the buffered data and all metadata associated
    with the open file descriptor *fd* to be flushed to disk. Calling *fsync()* forces
    the file to the synchronized I/O file integrity completion state.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '*fsync()* 系统调用会将与打开的文件描述符 *fd* 关联的缓冲数据及所有元数据刷新到磁盘。调用 *fsync()* 会强制文件达到同步 I/O
    文件完整性完成状态。'
- en: '[PRE8]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Returns 0 on success, or -1 on error
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 成功时返回 0，错误时返回 -1
- en: An *fsync()* call returns only after the transfer to the disk device (or at
    least its cache) has completed.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '*fsync()* 调用只有在数据传输到磁盘设备（或至少是其缓存）完成后才会返回。'
- en: The *fdatasync()* system call operates similarly to *fsync()*, but only forces
    the file to the synchronized I/O data integrity completion state.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '*fdatasync()* 系统调用的操作与 *fsync()* 类似，但仅将文件强制同步到 I/O 数据完整性完成状态。'
- en: '[PRE9]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Returns 0 on success, or -1 on error
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 成功时返回 0，错误时返回 -1
- en: Using *fdatasync()* potentially reduces the number of disk operations from the
    two required by *fsync()* to one. For example, if the file data has changed, but
    the file size has not, then calling *fdatasync()* only forces the data to be updated.
    (We noted above that changes to file metadata attributes such as the last modification
    timestamp don’t need to be transferred for synchronized I/O data completion.)
    By contrast, calling *fsync()* would also force the metadata to be transferred
    to disk.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 *fdatasync()* 可以将磁盘操作的数量从 *fsync()* 所需的两个操作减少为一个。例如，如果文件数据发生了变化，但文件大小没有变化，那么调用
    *fdatasync()* 只会强制更新数据。（我们之前提到过，文件元数据属性（如最后修改时间戳）的更改不需要传输，以完成同步 I/O 数据。）相比之下，调用
    *fsync()* 会强制将元数据也传输到磁盘。
- en: 'Reducing the number of disk I/O operations in this manner is useful for certain
    applications in which performance is crucial and the accurate maintenance of certain
    metadata (such as timestamps) is not essential. This can make a considerable performance
    difference for applications that are making multiple file updates: because the
    file data and metadata normally reside on different parts of the disk, updating
    them both would require repeated seek operations backward and forward across the
    disk.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式减少磁盘 I/O 操作的数量对于某些性能至关重要且不需要精确维护某些元数据（如时间戳）的应用程序非常有用。这对于进行多次文件更新的应用程序可以带来显著的性能提升：因为文件数据和元数据通常存储在磁盘的不同区域，更新它们都需要在磁盘上反复进行寻道操作。
- en: In Linux 2.2 and earlier, *fdatasync()* is implemented as a call to *fsync()*,
    and thus carries no performance gain.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Linux 2.2 及更早版本中，*fdatasync()* 被实现为调用 *fsync()*，因此没有性能上的提升。
- en: Note
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Starting with kernel 2.6.17, Linux provides the nonstandard *sync_file_range()*
    system call, which allows more precise control than *fdatasync()* when flushing
    file data. The caller can specify the file region to be flushed, and specify flags
    controlling whether the system call blocks on disk writes. See the *sync_file_range(2)*
    manual page for further details.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 从内核 2.6.17 开始，Linux 提供了非标准的 *sync_file_range()* 系统调用，它在刷新文件数据时提供比 *fdatasync()*
    更精确的控制。调用者可以指定需要刷新的文件区域，并指定控制系统调用是否在磁盘写入时阻塞的标志。有关更多详细信息，请参阅 *sync_file_range(2)*
    手册页。
- en: The *sync()* system call causes all kernel buffers containing updated file information
    (i.e., data blocks, pointer blocks, metadata, and so on) to be flushed to disk.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '*sync()* 系统调用会使所有包含更新文件信息的内核缓冲区（即数据块、指针块、元数据等）刷新到磁盘。'
- en: '[PRE10]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In the Linux implementation, *sync()* returns only after all data has been transferred
    to the disk device (or at least to its cache). However, SUSv3 permits an implementation
    of *sync()* to simply schedule the I/O transfer and return before it has completed.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Linux 实现中，*sync()* 只有在所有数据都已传输到磁盘设备（或至少传输到其缓存）后才会返回。然而，SUSv3 允许 *sync()* 的实现仅仅调度
    I/O 传输，并在完成之前返回。
- en: Note
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: A permanently running kernel thread ensures that modified kernel buffers are
    flushed to disk if they are not explicitly synchronized within 30 seconds. This
    is done to ensure that buffers don’t remain unsynchronized with the corresponding
    disk file (and thus vulnerable to loss in the event of a system crash) for long
    periods. In Linux 2.6, this task is performed by the *pdflush* kernel thread.
    (In Linux 2.4, it is performed by the *kupdated* kernel thread.)
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 一个永久运行的内核线程确保如果修改后的内核缓冲区在 30 秒内未被显式同步，它们会被刷新到磁盘。这样做是为了确保缓冲区不会长时间与相应的磁盘文件不同步（从而在系统崩溃时容易丢失）。在
    Linux 2.6 中，这项任务由 *pdflush* 内核线程执行。（在 Linux 2.4 中，由 *kupdated* 内核线程执行。）
- en: The file `/proc/sys/vm/dirty_expire_centisecs` specifies the age (in hundredths
    of a second) that a dirty buffer must reach before it is flushed by *pdflush*.
    Additional files in the same directory control other aspects of the operation
    of *pdflush*.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 文件 `/proc/sys/vm/dirty_expire_centisecs` 指定脏缓冲区在被 *pdflush* 刷新之前必须达到的时间（以百分之一秒为单位）。同一目录中的其他文件控制
    *pdflush* 操作的其他方面。
- en: 'Making all writes synchronous: `O_SYNC`'
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使所有写操作同步：`O_SYNC`
- en: 'Specifying the `O_SYNC` flag when calling *open()* makes all subsequent output
    *synchronous*:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用 *open()* 时指定 `O_SYNC` 标志会使所有后续的输出变为 *同步*：
- en: '[PRE11]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: After this *open()* call, every *write()* to the file automatically flushes
    the file data and metadata to the disk (i.e., writes are performed according to
    synchronized I/O file integrity completion).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在此 *open()* 调用之后，每次对文件的 *write()* 操作都会自动将文件数据和元数据刷新到磁盘（即，按照同步 I/O 文件完整性完成的要求执行写入）。
- en: Note
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Older BSD systems used the `O_FSYNC` flag to provide `O_SYNC` functionality.
    In *glibc*, `O_FSYNC` is defined as a synonym for `O_SYNC`.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 旧版BSD系统使用`O_FSYNC`标志提供`O_SYNC`功能。在*glibc*中，`O_FSYNC`被定义为`O_SYNC`的同义词。
- en: Performance impact of `O_SYNC`
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '`O_SYNC`的性能影响'
- en: Using the `O_SYNC` flag (or making frequent calls to *fsync()*, *fdatasync()*,
    or *sync()*) can strongly affect performance. [Table 13-3](ch13.html#impact_of_the_o_underscore_sync_flag_on
    "Table 13-3. Impact of the O_SYNC flag on the speed of writing 1 million bytes")
    shows the time required to write 1 million bytes to a newly created file (on an
    *ext2* file system) for a range of buffer sizes with and without `O_SYNC`. The
    results were obtained (using the `filebuff/write_bytes.c` program provided in
    the source code distribution for this book) using a vanilla 2.6.30 kernel and
    an *ext2* file system with a block size of 4096 bytes. Each row shows the average
    of 20 runs for the given buffer size.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`O_SYNC`标志（或频繁调用*fsync()*、*fdatasync()*或*sync()*）会显著影响性能。[表13-3](ch13.html#impact_of_the_o_underscore_sync_flag_on
    "表13-3. O_SYNC标志对写入100万字节速度的影响")展示了在不同缓冲区大小下，使用和不使用`O_SYNC`时，将100万字节写入新创建的文件（在*ext2*文件系统上）所需的时间。结果是通过（使用本书源码分发包中提供的`filebuff/write_bytes.c`程序）在2.6.30版本的内核和*ext2*文件系统（块大小为4096字节）下获得的。每行显示的是给定缓冲区大小下，进行20次运行的平均时间。
- en: As can be seen from the table, `O_SYNC` increases elapsed times enormously—in
    the 1-byte buffer case, by a factor of more than 1000\. Note also the large differences
    between the elapsed and CPU times for writes with `O_SYNC`. This is a consequence
    of the program being blocked while each buffer is actually transferred to disk.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 从表中可以看出，`O_SYNC`显著增加了经过的时间——在1字节缓冲区的情况下，增加了超过1000倍的时间。此外，注意到使用`O_SYNC`时，写入操作的经过时间和CPU时间之间的巨大差异。这是因为程序在每个缓冲区实际传输到磁盘时被阻塞。
- en: The results shown in [Table 13-3](ch13.html#impact_of_the_o_underscore_sync_flag_on
    "Table 13-3. Impact of the O_SYNC flag on the speed of writing 1 million bytes")
    omit a further factor that affects performance when using `O_SYNC`. Modern disk
    drives have large internal caches, and by default, `O_SYNC` merely causes data
    to be transferred to the cache. If we disable caching on the disk (using the command
    *hdparm -W0*), then the performance impact of `O_SYNC` becomes even more extreme.
    In the 1-byte case, the elapsed time rises from 1030 seconds to around 16,000
    seconds. In the 4096-byte case, the elapsed time rises from 0.34 seconds to 4
    seconds.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[表13-3](ch13.html#impact_of_the_o_underscore_sync_flag_on "表13-3. O_SYNC标志对写入100万字节速度的影响")中显示的结果省略了使用`O_SYNC`时影响性能的另一个因素。现代磁盘驱动器拥有较大的内部缓存，并且默认情况下，`O_SYNC`只是将数据传输到缓存。如果我们禁用磁盘缓存（使用命令*hdparm
    -W0*），则`O_SYNC`的性能影响将变得更加极端。在1字节的情况下，经过时间从1030秒增加到大约16,000秒。在4096字节的情况下，经过时间从0.34秒增加到4秒。'
- en: In summary, if we need to force flushing of kernel buffers, we should consider
    whether we can design our application to use large *write()* buffer sizes or make
    judicious use of occasional calls to *fsync()* or *fdatasync()*, instead of using
    the `O_SYNC` flag when opening the file.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，如果我们需要强制刷新内核缓冲区，应考虑是否能够设计我们的应用程序，使用较大的*write()*缓冲区大小，或合理利用偶尔调用*fsync()*或*fdatasync()*，而不是在打开文件时使用`O_SYNC`标志。
- en: Table 13-3. Impact of the `O_SYNC` flag on the speed of writing 1 million bytes
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 表13-3. `O_SYNC`标志对写入100万字节速度的影响
- en: '| `BUF_SIZE` | Time required (seconds) |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| `BUF_SIZE` | 所需时间（秒） |'
- en: '| --- | --- |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Without `O_SYNC` | With `O_SYNC` |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 不使用`O_SYNC` | 使用`O_SYNC` |'
- en: '| --- | --- |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Elapsed | Total CPU | Elapsed | Total CPU |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 经过时间 | 总CPU时间 | 经过时间 | 总CPU时间 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| `1` | `0.73` | `0.73` | `1030` | `98.8` |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| `1` | `0.73` | `0.73` | `1030` | `98.8` |'
- en: '| `16` | `0.05` | `0.05` | `65.0` | `0.40` |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| `16` | `0.05` | `0.05` | `65.0` | `0.40` |'
- en: '| `256` | `0.02` | `0.02` | `4.07` | `0.03` |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| `256` | `0.02` | `0.02` | `4.07` | `0.03` |'
- en: '| `4096` | `0.01` | `0.01` | `0.34` | `0.03` |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| `4096` | `0.01` | `0.01` | `0.34` | `0.03` |'
- en: The `O_DSYNC` and `O_RSYNC` flags
  id: totrans-163
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '`O_DSYNC`和`O_RSYNC`标志'
- en: 'SUSv3 specifies two further open file status flags related to synchronized
    I/O: `O_DSYNC` and `O_RSYNC`.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: SUSv3规范规定了与同步I/O相关的另外两个文件打开状态标志：`O_DSYNC`和`O_RSYNC`。
- en: The `O_DSYNC` flag causes writes to be performed according to the requirements
    of synchronized I/O data integrity completion (like *fdatasync()*). This contrasts
    with `O_SYNC`, which causes writes to be performed according to the requirements
    of synchronized I/O file integrity completion (like *fsync()*).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '`O_DSYNC`标志使得写操作按照同步I/O数据完整性完成的要求执行（类似于*fdatasync()*）。这与`O_SYNC`形成对比，后者使得写操作按照同步I/O文件完整性完成的要求执行（类似于*fsync()*）。'
- en: The `O_RSYNC` flag is specified in conjunction with either `O_SYNC` or `O_DSYNC`,
    and extends the write behaviors of these flags to read operations. Specifying
    both `O_RSYNC` and `O_DSYNC` when opening a file means that all subsequent reads
    are completed according to the requirements of synchronized I/O data integrity
    (i.e., prior to performing the read, all pending file writes are completed as
    though carried out with `O_DSYNC`). Specifying both `O_RSYNC` and `O_SYNC` when
    opening a file means that all subsequent reads are completed according to the
    requirements of synchronized I/O file integrity (i.e., prior to performing the
    read, all pending file writes are completed as though carried out with `O_SYNC`).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '`O_RSYNC`标志与`O_SYNC`或`O_DSYNC`一同指定，扩展了这些标志的写操作行为到读取操作。当打开文件时同时指定`O_RSYNC`和`O_DSYNC`，意味着所有后续的读取操作都会按照同步I/O数据完整性（即在执行读取之前，所有挂起的文件写入将像使用`O_DSYNC`进行的那样完成）的要求来完成。当同时指定`O_RSYNC`和`O_SYNC`打开文件时，意味着所有后续的读取操作都会按照同步I/O文件完整性（即在执行读取之前，所有挂起的文件写入将像使用`O_SYNC`进行的那样完成）的要求来完成。'
- en: Before kernel 2.6.33, the `O_DSYNC` and `O_RSYNC` flags were not implemented
    on Linux, and the *glibc* headers defined these constants to be the same as `O_SYNC`.
    (This isn’t actually correct in the case of `O_RSYNC`, since `O_SYNC` doesn’t
    provide any functionality for read operations.)
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在内核2.6.33之前，Linux并没有实现`O_DSYNC`和`O_RSYNC`标志，*glibc*头文件将这些常量定义为与`O_SYNC`相同。（在`O_RSYNC`的情况下，这实际上是不正确的，因为`O_SYNC`并未为读取操作提供任何功能。）
- en: Starting with kernel 2.6.33, Linux implements `O_DSYNC`, and an implementation
    of `O_RSYNC` is likely to be added in a future kernel release.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 从内核2.6.33开始，Linux实现了`O_DSYNC`，并且未来的内核版本可能会加入`O_RSYNC`的实现。
- en: Note
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Before kernel 2.6.33, Linux didn’t fully implement `O_SYNC` semantics. Instead,
    `O_SYNC` was implemented as `O_DSYNC`. To maintain consistent behavior for applications
    that were built for older kernels, applications that were linked against older
    versions of the GNU C library continue to provide `O_DSYNC` semantics for `O_SYNC`,
    even on Linux 2.6.33 and later.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在内核2.6.33之前，Linux并未完全实现`O_SYNC`语义。相反，`O_SYNC`被实现为`O_DSYNC`。为了保持针对旧内核构建的应用程序的行为一致，针对旧版本GNU
    C库链接的应用程序，继续为`O_SYNC`提供`O_DSYNC`语义，即便是在Linux 2.6.33及以后的版本中。
- en: Summary of I/O Buffering
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I/O缓冲概述
- en: '[Figure 13-1](ch13.html#summary_of_i_solidus_o_bufferin "Figure 13-1. Summary
    of I/O buffering") provides an overview of the buffering employed (for output
    files) by the *stdio* library and the kernel, along with the mechanisms for controlling
    each type of buffering. Traveling downward through the middle of this diagram,
    we see the transfer of user data by the *stdio* library functions to the *stdio*
    buffer, which is maintained in user memory space. When this buffer is filled,
    the *stdio* library invokes the *write()* system call, which transfers the data
    into the kernel buffer cache (maintained in kernel memory). Eventually, the kernel
    initiates a disk operation to transfer the data to the disk.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '[图13-1](ch13.html#summary_of_i_solidus_o_bufferin "图13-1. I/O缓冲概览")提供了*stdio*库和内核所采用的缓冲概述（用于输出文件），以及控制每种缓冲机制的方式。沿着图表中间向下，我们可以看到用户数据通过*stdio*库函数传输到*stdio*缓冲区，该缓冲区保存在用户内存空间中。当该缓冲区被填满时，*stdio*库会调用*write()*系统调用，将数据传输到内核缓冲区缓存中（保存在内核内存中）。最终，内核会启动磁盘操作，将数据传输到磁盘。'
- en: The left side of [Figure 13-1](ch13.html#summary_of_i_solidus_o_bufferin "Figure 13-1. Summary
    of I/O buffering") shows the calls that can be used at any time to explicitly
    force a flush of either of the buffers. The right side shows the calls that can
    be used to make flushing automatic, either by disabling buffering in the *stdio*
    library or by making file output system calls synchronous, so that each *write()*
    is immediately flushed to the disk.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 13-1](ch13.html#summary_of_i_solidus_o_bufferin "图 13-1：I/O 缓冲区总结")的左侧展示了可以在任何时候显式强制刷新缓冲区的调用。右侧则展示了可以通过禁用
    *stdio* 库的缓冲，或通过使文件输出系统调用同步，来使刷新变为自动的调用，从而确保每个 *write()* 操作立即刷新到磁盘。'
- en: '![Summary of I/O buffering](figs/web/13-1_FILEBUFF-buffering.png.jpg)Figure 13-1. Summary
    of I/O buffering'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '![I/O 缓冲区总结](figs/web/13-1_FILEBUFF-buffering.png.jpg)图 13-1：I/O 缓冲区总结'
- en: Advising the Kernel About I/O Patterns
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向内核提供 I/O 模式建议
- en: The *posix_fadvise()* system call allows a process to inform the kernel about
    its likely pattern for accessing file data.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '*posix_fadvise()* 系统调用允许进程向内核报告其访问文件数据的可能模式。'
- en: '[PRE12]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Returns 0 on success, or a positive error number on error
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 成功时返回 0，出错时返回一个正的错误号。
- en: The kernel may (but is not obliged to) use the information provided by *posix_fadvise()*
    to optimize its use of the buffer cache, thereby improving I/O performance for
    the process and for the system as a whole. Calling *posix_fadvise()* has no effect
    on the semantics of a program.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 内核可能（但不强制）使用 *posix_fadvise()* 提供的信息来优化其缓冲区缓存的使用，从而改善进程和系统整体的 I/O 性能。调用 *posix_fadvise()*
    对程序的语义没有影响。
- en: 'The *fd* argument is a file descriptor identifying the file about whose access
    patterns we wish to inform the kernel. The *offset* and *len* arguments identify
    the region of the file about which advice is being given: *offset* specifies the
    starting offset of the region, and *len* specifies the size of the region in bytes.
    A *len* value of 0 means all bytes from *offset* through to the end of the file.
    (In kernels before 2.6.6, a *len* of 0 was interpreted literally as zero bytes.)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '*fd* 参数是一个文件描述符，用于标识我们希望向内核报告其访问模式的文件。*offset* 和 *len* 参数标识文件中给出建议的区域：*offset*
    指定该区域的起始偏移量，*len* 指定该区域的大小（以字节为单位）。*len* 的值为 0 意味着从 *offset* 开始直到文件末尾的所有字节。（在
    2.6.6 之前的内核中，*len* 为 0 会被字面理解为零字节。）'
- en: 'The *advice* argument indicates the process’s expected pattern of access for
    the file. It is specified as one of the following:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '*advice* 参数指示进程对文件的预期访问模式。它被指定为以下选项之一：'
- en: '`POSIX_FADV_NORMAL`'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '`POSIX_FADV_NORMAL`'
- en: The process has no special advice to give about access patterns. This is the
    default behavior if no advice is given for the file. On Linux, this operation
    sets the file read-ahead window to the default size (128 kB).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 该进程没有特殊的访问模式建议。如果没有为文件提供任何建议，这是默认行为。在 Linux 上，此操作将文件预读窗口设置为默认大小（128 kB）。
- en: '`POSIX_FADV_SEQUENTIAL`'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '`POSIX_FADV_SEQUENTIAL`'
- en: The process expects to read data sequentially from lower offsets to higher offsets.
    On Linux, this operation sets the file read-ahead window to the twice the default
    size.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 该进程预计会顺序地从较低偏移量读取数据到较高偏移量。在 Linux 上，此操作会将文件的预读窗口设置为默认大小的两倍。
- en: '`POSIX_FADV_RANDOM`'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '`POSIX_FADV_RANDOM`'
- en: The process expects to access the data in random order. On Linux, this option
    disables file read-ahead.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 该进程预计会随机顺序访问数据。在 Linux 上，此选项会禁用文件预读。
- en: '`POSIX_FADV_WILLNEED`'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '`POSIX_FADV_WILLNEED`'
- en: The process expects to access the specified file region in the near future.
    The kernel performs read-ahead to populate the buffer cache with file data in
    the range specified by *offset* and *len*. Subsequent *read()* calls on the file
    don’t block on disk I/O; instead, they simply fetch data from the buffer cache.
    The kernel provides no guarantees about how long the data fetched from the file
    will remain resident in the buffer cache. If other processes or kernel activities
    place a sufficiently strong demand on memory, then the pages will eventually be
    reused. In other words, if memory pressure is high, then we should ensure that
    the elapsed time between the *posix_fadvise()* call and the subsequent *read()*
    call(s) is short. (The Linux-specific *readahead()* system call provides functionality
    equivalent to the `POSIX_FADV_WILLNEED` operation.)
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 进程预计在不久的将来会访问指定的文件区域。内核会执行预读操作，将文件数据填充到缓冲区缓存中，缓存的文件范围由*offset*和*len*指定。随后的*read()*调用不会阻塞磁盘
    I/O；相反，它们会直接从缓冲区缓存中获取数据。内核无法保证从文件获取的数据在缓冲区缓存中保持的时间。如果其他进程或内核活动对内存的需求足够强烈，那么这些页面最终将被重新使用。换句话说，如果内存压力很大，我们应该确保*posix_fadvise()*调用和随后的*read()*调用之间的时间间隔尽量短。（Linux
    特有的*readahead()*系统调用提供了等效于`POSIX_FADV_WILLNEED`操作的功能。）
- en: '`POSIX_FADV_DONTNEED`'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '`POSIX_FADV_DONTNEED`'
- en: The process expects not to access the specified file region in the near future.
    This advises the kernel that it can free the corresponding cache pages (if there
    are any). On Linux, this operation is performed in two steps. First, if the underlying
    device is not currently congested with a series of queued write operations, the
    kernel flushes any modified pages in the specified region. Second, the kernel
    attempts to free any cache pages for the region. For modified pages in the region,
    this second step will succeed only if the pages have been written to the underlying
    device in the first step—that is, if the device’s write queue was not congested.
    Since congestion on the device can’t be controlled by the application, an alternate
    way of ensuring that the cache pages can be freed is to precede the `POSIX_FADV_DONTNEED`
    operation with a *sync()* or *fdatasync()* call that specifies *fd*.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 进程预计在不久的将来不会访问指定的文件区域。这向内核发出提示，可以释放相应的缓存页面（如果有）。在 Linux 上，此操作分为两步。首先，如果底层设备当前没有被一系列排队的写操作阻塞，内核会刷新指定区域中所有修改过的页面。其次，内核尝试释放该区域的缓存页面。对于该区域中的修改页面，第二步只有在第一步中页面已经写入底层设备时才能成功——即如果设备的写队列没有被阻塞。由于应用程序无法控制设备上的拥堵，确保缓存页面可以被释放的另一种方式是，在执行`POSIX_FADV_DONTNEED`操作之前，调用*sync()*或*fdatasync()*并指定*fd*。
- en: '`POSIX_FADV_NOREUSE`'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '`POSIX_FADV_NOREUSE`'
- en: The process expects to access data in the specified file region once, and then
    not to reuse it. This hint tells the kernel that it can free the pages after they
    have been accessed once. On Linux, this operation currently has no effect.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 进程预计只会访问指定文件区域一次，然后不再重复使用该数据。这个提示告诉内核，它可以在访问过该数据之后释放相关页面。在 Linux 上，此操作当前没有效果。
- en: The specification of *posix_fadvise()* is new in SUSv3, and not all UNIX implementations
    support this interface. Linux provides *posix_fadvise()* since kernel 2.6.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '*posix_fadvise()*的规范是 SUSv3 中新增的，并非所有 UNIX 实现都支持此接口。自 2.6 内核起，Linux 提供了*posix_fadvise()*。'
- en: 'Bypassing the Buffer Cache: Direct I/O'
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 绕过缓冲区缓存：直接 I/O
- en: Starting with kernel 2.4, Linux allows an application to bypass the buffer cache
    when performing disk I/O, thus transferring data directly from user space to a
    file or disk device. This is sometimes termed *direct I/O* or *raw I/O*.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 从 2.4 内核开始，Linux 允许应用程序在执行磁盘 I/O 时绕过缓冲区缓存，从用户空间直接将数据传输到文件或磁盘设备。这有时被称为*直接 I/O*或*原始
    I/O*。
- en: Note
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The details described here are Linux-specific and are not standardized by SUSv3\.
    Nevertheless, most UNIX implementations provide some form of direct I/O access
    to devices and files.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这里描述的细节是特定于 Linux 的，并未由 SUSv3 标准化。然而，大多数 UNIX 实现都提供某种形式的直接 I/O 访问设备和文件。
- en: Direct I/O is sometimes misunderstood as being a means of obtaining fast I/O
    performance. However, for most applications, using direct I/O can considerably
    degrade performance. This is because the kernel applies a number of optimizations
    to improve the performance of I/O done via the buffer cache, including performing
    sequential read-ahead, performing I/O in clusters of disk blocks, and allowing
    processes accessing the same file to share buffers in the cache. All of these
    optimizations are lost when we use direct I/O. Direct I/O is intended only for
    applications with specialized I/O requirements. For example, database systems
    that perform their own caching and I/O optimizations don’t need the kernel to
    consume CPU time and memory performing the same tasks.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 直接I/O有时被误解为一种获得高速I/O性能的手段。然而，对于大多数应用程序来说，使用直接I/O可能会显著降低性能。这是因为内核对通过缓冲区缓存执行的I/O进行了许多优化，包括执行顺序预读、以磁盘块集群方式执行I/O，以及允许访问同一文件的进程共享缓存中的缓冲区。所有这些优化在使用直接I/O时都会丧失。直接I/O仅适用于具有特殊I/O需求的应用程序。例如，执行自己缓存和I/O优化的数据库系统不需要内核浪费CPU时间和内存来执行相同的任务。
- en: We can perform direct I/O either on an individual file or on a block device
    (e.g., a disk). To do this, we specify the `O_DIRECT` flag when opening the file
    or device with *open()*.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在单个文件或块设备（例如磁盘）上执行直接I/O。为此，我们在使用*open()*打开文件或设备时指定`O_DIRECT`标志。
- en: The `O_DIRECT` flag is effective since kernel 2.4.10\. Not all Linux file systems
    and kernel versions support the use of this flag. Most native file systems support
    `O_DIRECT`, but many non-UNIX file systems (e.g., VFAT) do not. It may be necessary
    to test the file system concerned (if a file system doesn’t support `O_DIRECT`,
    then *open()* fails with the error `EINVAL`) or read the kernel source code to
    check for this support.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '`O_DIRECT`标志自内核2.4.10版本起生效。并非所有Linux文件系统和内核版本都支持使用此标志。大多数本地文件系统支持`O_DIRECT`，但许多非UNIX文件系统（例如VFAT）不支持。可能需要测试相关的文件系统（如果文件系统不支持`O_DIRECT`，则*open()*将因错误`EINVAL`失败），或阅读内核源代码以检查是否支持此功能。'
- en: Note
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: If a file is opened with `O_DIRECT` by one process, and opened normally (i.e.,
    so that the buffer cache is used) by another process, then there is no coherency
    between the contents of the buffer cache and the data read or written via direct
    I/O. Such scenarios should be avoided.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个文件被一个进程以`O_DIRECT`方式打开，而另一个进程以正常方式打开（即使用缓冲区缓存），则缓冲区缓存中的内容与通过直接I/O读取或写入的数据之间没有一致性。这种情况应避免。
- en: The *raw(8)* manual page describes an older (now deprecated) technique for obtaining
    raw access to a disk device.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '*raw(8)*手册页描述了一种较旧（现已弃用）的方法，用于获得对磁盘设备的原始访问权限。'
- en: Alignment restrictions for direct I/O
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 直接I/O的对齐限制
- en: 'Because direct I/O (on both disk devices and files) involves direct access
    to the disk, we must observe a number of restrictions when performing I/O:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 由于直接I/O（无论是磁盘设备还是文件）涉及对磁盘的直接访问，因此在执行I/O时必须遵守一些限制：
- en: The data buffer being transferred must be aligned on a memory boundary that
    is a multiple of the block size.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正在传输的数据缓冲区必须与内存边界对齐，且该边界必须是块大小的倍数。
- en: The file or device offset at which data transfer commences must be a multiple
    of the block size.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据传输开始时的文件或设备偏移量必须是块大小的倍数。
- en: The length of the data to be transferred must be a multiple of the block size.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要传输的数据长度必须是块大小的倍数。
- en: Failure to observe any of these restrictions results in the error `EINVAL`.
    In the above list, *block size* means the physical block size of the device (typically
    512 bytes).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 未遵守任何这些限制将导致错误`EINVAL`。在上述列表中，*块大小*指的是设备的物理块大小（通常为512字节）。
- en: Note
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'When performing direct I/O, Linux 2.4 is more restrictive than Linux 2.6: the
    alignment, length, and offset must be multiples of the *logical* block size of
    the underlying file system. (Typical file system logical block sizes are 1024,
    2048, or 4096 bytes.)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行直接I/O时，Linux 2.4比Linux 2.6更为严格：对齐、长度和偏移量必须是底层文件系统的*逻辑*块大小的倍数。（典型的文件系统逻辑块大小为1024、2048或4096字节。）
- en: Example program
  id: totrans-214
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例程序
- en: '[Example 13-1](ch13.html#using_o_underscore_direct_to_bypass_the "Example 13-1. Using
    O_DIRECT to bypass the buffer cache") provides a simple example of the use of
    `O_DIRECT` while opening a file for reading. This program takes up to four command-line
    arguments specifying, in order, the file to be read, the number of bytes to be
    read from the file, the offset to which the program should seek before reading
    from the file, and the alignment of the data buffer passed to *read()*. The last
    two arguments are optional, and default to offset 0 and 4096 bytes, respectively.
    Here are some examples of what we see when we run this program:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 13-1](ch13.html#using_o_underscore_direct_to_bypass_the "示例 13-1：使用 O_DIRECT
    绕过缓冲区缓存") 提供了一个简单的示例，展示了在打开文件进行读取时使用 `O_DIRECT`。此程序最多接受四个命令行参数，依次指定：要读取的文件、要从文件中读取的字节数、程序在读取文件之前应定位的偏移量以及传递给
    *read()* 的数据缓冲区对齐方式。最后两个参数是可选的，默认为偏移量 0 和 4096 字节。以下是运行该程序时可能看到的几个示例：'
- en: '[PRE13]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Note
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The program in [Example 13-1](ch13.html#using_o_underscore_direct_to_bypass_the
    "Example 13-1. Using O_DIRECT to bypass the buffer cache") uses the *memalign()*
    function to allocate a block of memory aligned on a multiple of its first argument.
    We describe *memalign()* in [Other Methods of Allocating Memory on the Heap](ch07.html#other_methods_of_allocating_memory_on_th
    "Other Methods of Allocating Memory on the Heap").
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 13-1](ch13.html#using_o_underscore_direct_to_bypass_the "示例 13-1：使用 O_DIRECT
    绕过缓冲区缓存") 中的程序使用 *memalign()* 函数分配一个与第一个参数倍数对齐的内存块。我们在 [堆上分配内存的其他方法](ch07.html#other_methods_of_allocating_memory_on_th
    "堆上分配内存的其他方法") 中描述了 *memalign()*。'
- en: Example 13-1. Using `O_DIRECT` to bypass the buffer cache
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 13-1：使用 `O_DIRECT` 绕过缓冲区缓存
- en: '[PRE14]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Mixing Library Functions and System Calls for File I/O
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混合使用库函数和系统调用进行文件 I/O
- en: It is possible to mix the use of system calls and the standard C library functions
    to perform I/O on the same file. The *fileno()* and *fdopen()* functions assist
    us with this task.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 可以混合使用系统调用和标准 C 库函数对同一文件执行 I/O 操作。*fileno()* 和 *fdopen()* 函数帮助我们完成这项任务。
- en: '[PRE15]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Returns file descriptor on success, or -1 on error
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 成功时返回文件描述符，出错时返回 -1
- en: '[PRE16]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Note
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Returns (new) file pointer on success, or `NULL` on error
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 成功时返回（新的）文件指针，出错时返回 `NULL`
- en: Given a stream, *fileno()* returns the corresponding file descriptor (i.e.,
    the one that the *stdio* library has opened for this stream). This file descriptor
    can then be used in the usual way with I/O system calls such as *read()*, *write()*,
    *dup()*, and *fcntl()*.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个流，*fileno()* 会返回相应的文件描述符（即 *stdio* 库为此流打开的文件描述符）。然后，可以像通常那样使用该文件描述符与 I/O
    系统调用进行交互，例如 *read()*、*write()*、*dup()* 和 *fcntl()*。
- en: The *fdopen()* function is the converse of *fileno()*. Given a file descriptor,
    it creates a corresponding stream that uses this descriptor for its I/O. The *mode*
    argument is the same as for *fopen()*; for example, *r* for read, *w* for write,
    or *a* for append. If this argument is not consistent with the access mode of
    the file descriptor *fd*, then *fdopen()* fails.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '*fdopen()* 函数是 *fileno()* 的逆操作。给定一个文件描述符，它会创建一个相应的流，使用该描述符进行 I/O 操作。*mode*
    参数与 *fopen()* 相同；例如，*r* 表示读取，*w* 表示写入，*a* 表示追加。如果此参数与文件描述符 *fd* 的访问模式不一致，则 *fdopen()*
    会失败。'
- en: The *fdopen()* function is especially useful for descriptors referring to files
    other than regular files. As we’ll see in later chapters, the system calls for
    creating sockets and pipes always return file descriptors. To use the *stdio*
    library with these file types, we must use *fdopen()* to create a corresponding
    file stream.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '*fdopen()* 函数特别适用于指向非常规文件的描述符。正如我们在后面的章节中将看到的，创建套接字和管道的系统调用总是返回文件描述符。为了在这些文件类型上使用
    *stdio* 库，我们必须使用 *fdopen()* 来创建一个相应的文件流。'
- en: 'When using the *stdio* library functions in conjunction with I/O system calls
    to perform I/O on disk files, we must keep buffering issues in mind. I/O system
    calls transfer data directly to the kernel buffer cache, while the *stdio* library
    waits until the stream’s user-space buffer is full before calling *write()* to
    transfer that buffer to the kernel buffer cache. Consider the following code used
    to write to standard output:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在结合使用 *stdio* 库函数与 I/O 系统调用对磁盘文件进行 I/O 操作时，我们必须注意缓冲问题。I/O 系统调用将数据直接传输到内核缓冲区缓存，而
    *stdio* 库则在流的用户空间缓冲区满时，才调用 *write()* 将缓冲区数据传送到内核缓冲区缓存。考虑以下用于写入标准输出的代码：
- en: '[PRE17]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'In the usual case, the output of the *printf()* will typically appear *after*
    the output of the *write()*, so that this code yields the following output:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在常见情况下，*printf()*的输出通常会在*write()*的输出之后出现，因此这段代码会产生如下输出：
- en: '[PRE18]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: When intermingling I/O system calls and *stdio* functions, judicious use of
    *fflush()* may be required to avoid this problem. We could also use *setvbuf()*
    or *setbuf()* to disable buffering, but doing so might impact I/O performance
    for the application, since each output operation would then result in the execution
    of a *write()* system call.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在混合使用 I/O 系统调用和*stdio*函数时，可能需要谨慎使用*fflush()*以避免此问题。我们也可以使用*setvbuf()*或*setbuf()*来禁用缓冲，但这样做可能会影响应用程序的I/O性能，因为每次输出操作都会导致执行*write()*系统调用。
- en: Note
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: SUSv3 goes to some length specifying the requirements for an application to
    be able to mix the use of I/O system calls and *stdio* functions. See the section
    headed *Interaction of File Descriptors and Standard I/O Streams* under the chapter
    *General Information* in the *System Interfaces* (XSH) volume for details.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: SUSv3 对应用程序能够混合使用 I/O 系统调用和*stdio*函数做了详细规定。有关详细信息，请参见*系统接口*（XSH）卷中*文件描述符与标准
    I/O 流的交互*部分，位于*常规信息*章节下。
- en: Summary
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Buffering of input and output data is performed by the kernel, and also by the
    *stdio* library. In some cases, we may wish to prevent buffering, but we need
    to be aware of the impact this has on application performance. Various system
    calls and library functions can be used to control kernel and *stdio* buffering
    and to perform one-off buffer flushes.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 输入和输出数据的缓冲由内核和*stdio*库共同执行。在某些情况下，我们可能希望禁用缓冲，但需要注意这样做对应用程序性能的影响。可以使用各种系统调用和库函数来控制内核和*stdio*缓冲，并执行一次性缓冲区刷新。
- en: A process can use *posix_fadvise()* to advise the kernel of its likely pattern
    for accessing data from a specified file. The kernel may use this information
    to optimize the use of the buffer cache, thus improving I/O performance.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 进程可以使用*posix_fadvise()*向内核建议其访问指定文件数据的模式。内核可能会利用这些信息来优化缓冲区缓存的使用，从而提高I/O性能。
- en: The Linux-specific *open()* `O_DIRECT` flag allows specialized applications
    to bypass the buffer cache.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: Linux 特有的*open()* `O_DIRECT`标志允许专用应用程序绕过缓冲区缓存。
- en: The *fileno()* and *fdopen()* functions assist us with the task of mixing system
    calls and standard C library functions to perform I/O on the same file. Given
    a stream, *fileno()* returns the corresponding file descriptor; *fdopen()* performs
    the converse operation, creating a new stream that employs a specified open file
    descriptor.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '*fileno()*和*fdopen()*函数帮助我们将系统调用和标准 C 库函数混合使用，以在同一文件上执行 I/O。给定一个流，*fileno()*返回相应的文件描述符；*fdopen()*执行反向操作，创建一个使用指定打开的文件描述符的新流。'
- en: Further information
  id: totrans-244
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 进一步的信息
- en: '[Bach, 1986] describes the implementation and advantages of the buffer cache
    on System V. [Goodheart & Cox, 1994] and [Vahalia, 1996] also describe the rationale
    and implementation of the System V buffer cache. Further relevant information
    specific to Linux can be found in [Bovet & Cesati, 2005] and [Love, 2010].'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '[Bach, 1986]描述了System V中缓冲区缓存的实现和优点。[Goodheart & Cox, 1994]和[Vahalia, 1996]也描述了System
    V缓冲区缓存的原理和实现。有关Linux的进一步相关信息，可以参考[Bovet & Cesati, 2005]和[Love, 2010]。'
- en: Exercises
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习
- en: Using the *time* built-in command of the shell, try timing the operation of
    the program in [Example 4-1](ch04.html#using_i_solidus_o_system_calls "Example 4-1. Using
    I/O system calls") (`copy.c`) on your system.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Shell的*time*内置命令，尝试计时在系统中运行[示例 4-1](ch04.html#using_i_solidus_o_system_calls
    "示例 4-1. 使用 I/O 系统调用")（`copy.c`）程序的操作。
- en: Experiment with different file and buffer sizes. You can set the buffer size
    using the *-DBUF_SIZE=nbytes* option when compiling the program.
  id: totrans-248
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试不同的文件和缓冲区大小。你可以在编译程序时使用*-DBUF_SIZE=nbytes*选项来设置缓冲区大小。
- en: Modify the *open()* system call to include the `O_SYNC` flag. How much difference
    does this make to the speed for various buffer sizes?
  id: totrans-249
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改*open()*系统调用，添加`O_SYNC`标志。对于不同的缓冲区大小，这对速度的影响有多大？
- en: Try performing these timing tests on a range of file systems (e.g., *ext3*,
    *XFS*, *Btrfs*, and *JFS*). Are the results similar? Are the trends the same when
    going from small to large buffer sizes?
  id: totrans-250
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试在不同的文件系统上执行这些计时测试（例如，*ext3*，*XFS*，*Btrfs*和*JFS*）。结果是否相似？从小缓冲区到大缓冲区时，趋势是否一致？
- en: Time the operation of the `filebuff/write_bytes.c` program (provided in the
    source code distribution for this book) for various buffer sizes and file systems.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对`filebuff/write_bytes.c`程序（本书源代码中提供）在不同缓冲区大小和文件系统下的操作进行计时。
- en: What is the effect of the following statements?
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下语句的效果是什么？
- en: '[PRE19]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Explain why the output of the following code differs depending on whether standard
    output is redirected to a terminal or to a disk file.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释为什么以下代码的输出会因标准输出是重定向到终端还是磁盘文件而有所不同。
- en: '[PRE20]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The command *tail [ -n num ] file* prints the last *num* lines (ten by default)
    of the named file. Implement this command using I/O system calls (*lseek()*, *read()*,
    *write()*, and so on). Keep in mind the buffering issues described in this chapter,
    in order to make the implementation efficient.
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 命令*tail [ -n num ] file*打印指定文件的最后*num*行（默认是十行）。使用 I/O 系统调用（*lseek()*, *read()*,
    *write()*等）来实现此命令。请记住本章描述的缓冲问题，以便使实现更加高效。
