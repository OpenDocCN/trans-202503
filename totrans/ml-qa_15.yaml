- en: '**13'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**13'
- en: LARGE TRAINING SETS FOR VISION TRANSFORMERS**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**视觉变换器的大型训练集**'
- en: '![Image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/common.jpg)'
- en: Why do vision transformers (ViTs) generally require larger training sets than
    convolutional neural networks (CNNs)?
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么视觉变换器（ViTs）通常需要比卷积神经网络（CNNs）更大的训练集？
- en: Each machine learning algorithm and model encodes a particular set of assumptions
    or prior knowledge, commonly referred to as *inductive biases*, in its design.
    Some inductive biases are workarounds to make algorithms computationally more
    feasible, other inductive biases are based on domain knowledge, and some inductive
    biases are both.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 每个机器学习算法和模型在其设计中都编码了一组特定的假设或先验知识，通常称为*归纳偏差*。有些归纳偏差是为了使算法在计算上更可行，其他归纳偏差则基于领域知识，还有一些归纳偏差同时包含两者。
- en: CNNs and ViTs can be used for the same tasks, including image classification,
    object detection, and image segmentation. CNNs are mainly composed of convolutional
    layers, while ViTs consist primarily of multi-head attention blocks (discussed
    in [Chapter 8](ch08.xhtml) in the context of transformers for natural language
    inputs).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs和ViTs可以用于相同的任务，包括图像分类、目标检测和图像分割。CNNs主要由卷积层组成，而ViTs主要由多头注意力块构成（在[第8章](ch08.xhtml)中讨论了针对自然语言输入的变换器）。
- en: CNNs have more inductive biases that are hardcoded as part of the algorithmic
    design, so they generally require less training data than ViTs. In a sense, ViTs
    are given more degrees of freedom and can or must learn certain inductive biases
    from the data (assuming that these biases are conducive to optimizing the training
    objective). However, everything that needs to be learned requires more training
    examples.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs具有更多硬编码的归纳偏差，这些偏差是算法设计的一部分，因此它们通常比ViTs需要更少的训练数据。从某种意义上说，ViTs被赋予了更多的自由度，可以或必须从数据中学习某些归纳偏差（假设这些偏差有助于优化训练目标）。然而，一切需要学习的内容都需要更多的训练样本。
- en: The following sections explain the main inductive biases encountered in CNNs
    and how ViTs work well without them.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 以下章节将解释在CNNs中遇到的主要归纳偏差，并说明视觉变换器（ViTs）如何在没有这些偏差的情况下仍能良好工作。
- en: '**Inductive Biases in CNNs**'
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**CNN中的归纳偏差**'
- en: 'The following are the primary inductive biases that largely define how CNNs
    function:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是主要的归纳偏差，它们在很大程度上定义了卷积神经网络（CNNs）的工作方式：
- en: '**Local connectivity** In CNNs, each unit in a hidden layer is connected to
    only a subset of neurons in the previous layer. We can justify this restriction
    by assuming that neighboring pixels are more relevant to each other than pixels
    that are farther apart. As an intuitive example, consider how this assumption
    applies to the context of recognizing edges or contours in an image.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**局部连接** 在CNNs中，每个隐藏层中的单元只与前一层中的一部分神经元相连。我们可以通过假设相邻像素比远距离像素更相关来合理化这一限制。作为一个直观的例子，考虑一下这种假设如何应用于图像中识别边缘或轮廓的场景。'
- en: '**Weight sharing** Via the convolutional layers, we use the same small set
    of weights (the kernels or filters) throughout the whole image. This reflects
    the assumption that the same filters are useful for detecting the same patterns
    in different parts of the image.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**权重共享** 通过卷积层，我们在整个图像中使用相同的小组权重（即卷积核或滤波器）。这反映了这样一个假设：相同的滤波器在图像的不同部分用于检测相同的模式。'
- en: '**Hierarchical processing** CNNs consist of multiple convolutional layers to
    extract features from the input image. As the network progresses from the input
    to the output layers, low-level features are successively combined to form increasingly
    complex features, ultimately leading to the recognition of more complex objects
    and shapes. Furthermore, the convolutional filters in these layers learn to detect
    specific patterns and features at different levels of abstraction.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**层次处理** CNNs由多个卷积层组成，用于从输入图像中提取特征。随着网络从输入层到输出层的推进，低级特征依次结合形成越来越复杂的特征，最终导致对更复杂物体和形状的识别。此外，这些层中的卷积滤波器学习在不同的抽象层次上检测特定的模式和特征。'
- en: '**Spatial invariance** CNNs exhibit the mathematical property of spatial invariance,
    meaning the output of a model remains consistent even if the input signal is shifted
    to a different location within the spatial domain. This characteristic arises
    from the combination of local connectivity, weight sharing, and the hierarchical
    architecture mentioned earlier.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**空间不变性** CNNs表现出空间不变性的数学特性，这意味着即使输入信号在空间域内被移动到不同位置，模型的输出仍然保持一致。这一特性源于局部连接、权重共享和先前提到的层次结构的结合。'
- en: The combination of local connectivity, weight sharing, and hierarchical processing
    in a CNN leads to spatial invariance, allowing the model to recognize the same
    pattern or feature regardless of its location in the input image.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 中的局部连接性、权重共享和层次处理的结合导致了空间不变性，使得模型能够识别图像中无论其位置如何的相同模式或特征。
- en: '*Translation invariance* is a specific case of spatial invariance in which
    the output remains the same after a shift or translation of the input signal in
    the spatial domain. In this context, the emphasis is solely on moving an object
    to a different location within an image without any rotations or alterations of
    its other attributes.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*平移不变性*是空间不变性的一个特例，其中输出在输入信号在空间域内发生平移或移位后保持不变。在这种情况下，重点仅在于将对象移动到图像中的不同位置，而不涉及其其他属性的旋转或变化。'
- en: In reality, convolutional layers and networks are not truly translation-invariant;
    rather, they achieve a certain level of translation equivariance. What is the
    difference between translation invariance and equivariance? *Translation invariance*
    means that the output does not change with an input shift, while *translation
    equivariance* implies that the output shifts with the input in a corresponding
    manner. In other words, if we shift the input object to the right, the results
    will correspondingly shift to the right, as illustrated in [Figure 13-1](ch13.xhtml#ch13fig1).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，卷积层和网络并不是真正的平移不变；它们实现的是某种程度的平移等变性。那么，平移不变性和平移等变性有什么区别呢？*平移不变性*意味着输出在输入平移时不会改变，而*平移等变性*则意味着输出会以相应的方式随着输入的平移而改变。换句话说，如果我们将输入对象向右平移，结果也会相应地向右平移，如[图
    13-1](ch13.xhtml#ch13fig1)所示。
- en: '![Image](../images/13fig01.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/13fig01.jpg)'
- en: '*Figure 13-1: Equivariance under different image translations*'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 13-1：不同图像平移下的等变性*'
- en: 'As [Figure 13-1](ch13.xhtml#ch13fig1) shows, under translation invariance,
    we get the same output pattern regardless of the order in which we apply the operations:
    transformation followed by translation or translation followed by transformation.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图 13-1](ch13.xhtml#ch13fig1)所示，在平移不变性的情况下，无论我们以何种顺序应用操作（先变换后平移，或先平移后变换），都能得到相同的输出模式。
- en: As mentioned earlier, CNNs achieve translation equivariance through a combination
    of their local connectivity, weight sharing, and hierarchical processing properties.
    [Figure 13-2](ch13.xhtml#ch13fig2) depicts a convolutional operation to illustrate
    the local connectivity and weight-sharing priors. This figure demonstrates the
    concept of translation equivariance in CNNs, in which a convolutional filter captures
    the input signal (the two dark blocks) irrespective of where it is located in
    the input.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，CNN 通过其局部连接性、权重共享和层次处理属性的结合实现了平移等变性。[图 13-2](ch13.xhtml#ch13fig2) 描绘了一个卷积操作，以说明局部连接性和权重共享的先验。这张图展示了
    CNN 中平移等变性的概念，其中卷积滤波器捕捉输入信号（两个深色块），无论它位于输入的哪个位置。
- en: '![Image](../images/13fig02.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/13fig02.jpg)'
- en: '*Figure 13-2: Convolutional filters and translation equivariance*'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 13-2：卷积滤波器和平移等变性*'
- en: '[Figure 13-2](ch13.xhtml#ch13fig2) shows a 3*×*3 input image that consists
    of two nonzero pixel values in the upper-left corner (top portion of the figure)
    or upper-right corner (bottom portion of the figure). If we apply a 2*×*2 convolutional
    filter to these two input image scenarios, we can see that the output feature
    maps contain the same extracted pattern, which is on either the left (top of the
    figure) or the right (bottom of the figure), demonstrating the translation equivariance
    of the convolutional operation.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 13-2](ch13.xhtml#ch13fig2) 显示了一个 3*×*3 的输入图像，该图像由左上角（图像上部分）或右上角（图像下部分）两个非零像素值组成。如果我们对这两种输入图像情况应用一个
    2*×*2 的卷积滤波器，可以看到输出特征图包含相同的提取模式，这个模式分别位于左侧（图像上方）或右侧（图像下方），从而展示了卷积操作的平移等变性。'
- en: For comparison, a fully connected network such as a multilayer perceptron lacks
    this spatial invariance or equivariance. To illustrate this point, picture a multilayer
    perceptron with one hidden layer. Each pixel in the input image is connected with
    each value in the resulting output. If we shift the input by one or more pixels,
    a different set of weights will be activated, as illustrated in [Figure 13-3](ch13.xhtml#ch13fig3).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做对比，像多层感知机这样的全连接网络缺乏空间不变性或等变性。为了说明这一点，设想一个具有一个隐藏层的多层感知机。输入图像中的每个像素都与输出中的每个值连接。如果我们将输入图像平移一个或多个像素，将会激活一组不同的权重，如[图13-3](ch13.xhtml#ch13fig3)所示。
- en: '![Image](../images/13fig03.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/13fig03.jpg)'
- en: '*Figure 13-3: Location-specific weights in fully connected layers*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*图13-3：全连接层中的位置特定权重*'
- en: Like fully connected networks, ViT architecture (and transformer architecture
    in general) lacks the inductive bias for spatial invariance or equi-variance.
    For instance, the model produces different outputs if we place the same object
    in two different spatial locations within an image. This is not ideal, as the
    semantic meaning of an object (the concept that an object represents or conveys)
    remains the same based on its location. Consequently, it must learn these invariances
    directly from the data. To facilitate learning useful patterns present in CNNs
    requires pretraining over a larger dataset.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 和全连接网络一样，ViT架构（以及一般的transformer架构）缺乏空间不变性或等变性的归纳偏置。例如，如果我们将相同的物体放置在图像中的两个不同位置，模型会产生不同的输出。这并不理想，因为物体的语义意义（物体所代表或传达的概念）在其位置变化时应该保持不变。因此，模型必须直接从数据中学习这些不变性。为了便于学习CNN中有用的模式，需要在更大的数据集上进行预训练。
- en: A common workaround for adding positional information in ViTs is to use relative
    positional embeddings (also known as *relative positional encodings*) that consider
    the relative distance between two tokens in the input sequence. However, while
    relative embeddings encode information that helps transformers keep track of the
    relative location of tokens, the transformer still needs to learn from the data
    whether and how far spatial information is relevant for the task at hand.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在ViT中，常见的添加位置信息的变通方法是使用相对位置嵌入（也称为*相对位置编码*），它考虑输入序列中两个标记之间的相对距离。然而，尽管相对嵌入编码了有助于transformer跟踪标记相对位置的信息，transformer仍然需要从数据中学习空间信息是否以及如何与当前任务相关。
- en: '**ViTs Can Outperform CNNs**'
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**ViT 可以超越 CNN**'
- en: The hardcoded assumptions via the inductive biases discussed in previous sections
    reduce the number of parameters in CNNs substantially compared to fully connected
    layers. On the other hand, ViTs tend to have larger numbers of parameters than
    CNNs, which require more training data. (Refer to [Chapter 11](ch11.xhtml) for
    a refresher on how to precisely calculate the number of parameters in fully connected
    and convolutional layers.)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 通过前面章节中讨论的归纳偏置所做的硬编码假设，相较于全连接层，显著减少了CNN中的参数数量。另一方面，ViT通常比CNN具有更多的参数，因此需要更多的训练数据。（有关如何精确计算全连接层和卷积层中参数数量的内容，请参考[第11章](ch11.xhtml)。）
- en: ViTs may underperform compared to popular CNN architectures without extensive
    pretraining, but they can perform very well with a sufficiently large pretraining
    dataset. In contrast to language transformers, where unsupervised pretraining
    (such as self-supervised learning, discussed in [Chapter 2](ch02.xhtml)) is a
    preferred choice, vision transformers are often pretrained using large, labeled
    datasets like ImageNet, which provides millions of labeled images for training,
    and regular supervised learning.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 与流行的CNN架构相比，ViT在没有广泛预训练的情况下可能表现不佳，但在足够大的预训练数据集下，它们的表现可以非常好。与语言transformer不同，后者通常使用无监督预训练（如[第2章](ch02.xhtml)中讨论的自监督学习），视觉transformer通常使用像ImageNet这样的标签数据集进行预训练，ImageNet提供了数百万张带标签的图像用于训练，并进行常规的监督学习。
- en: 'An example of ViTs surpassing the predictive performance of CNNs, given enough
    data, can be observed from initial research on the ViT architecture, as shown
    in the paper “An Image Is Worth 16x16 Words: Transformers for Image Recognition
    at Scale.” This study compared ResNet, a type of convolutional network, with the
    original ViT design using different dataset sizes for pretraining. The findings
    also showed that the ViT model excelled over the convolutional approach only after
    being pretrained on a minimum of 100 million images.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 一个ViTs超越CNN预测性能的例子，可以从ViT架构的初步研究中看到，如论文《一张图像值16x16个词：用于大规模图像识别的变换器》中所示。该研究比较了卷积网络的一种类型ResNet与原始ViT设计，在不同数据集规模下进行预训练。研究结果还表明，ViT模型只有在预训练了至少1亿张图像后，才能在预测性能上超越卷积方法。
- en: '**Inductive Biases in ViTs**'
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**ViTs中的归纳偏差**'
- en: ViTs also possess some inductive biases. For example, vision transformers *patchify*
    the input image to process each input patch individually. Here, each patch can
    attend to all other patches so that the model learns relationships between far-apart
    patches in the input image, as illustrated in [Figure 13-4](ch13.xhtml#ch13fig4).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ViTs也具有一些归纳偏差。例如，视觉变换器*将输入图像切分*，以便分别处理每个输入补丁。在这里，每个补丁可以关注所有其他补丁，从而使模型学习到输入图像中相距较远的补丁之间的关系，如[图13-4](ch13.xhtml#ch13fig4)所示。
- en: '![Image](../images/13fig04.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/13fig04.jpg)'
- en: '*Figure 13-4: How a vision transformer operates on image patches*'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*图13-4：视觉变换器如何处理图像补丁*'
- en: The patchify inductive bias allows ViTs to scale to larger image sizes without
    increasing the number of parameters in the model, which can be computationally
    expensive. By processing smaller patches individually, ViTs can efficiently capture
    spatial relationships between image regions while benefiting from the global context
    captured by the self-attention mechanism.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 切分补丁的归纳偏差使得ViTs能够扩展到更大的图像尺寸，而无需增加模型中的参数数量，这样可以减少计算开销。通过分别处理较小的补丁，ViTs能够有效捕捉图像区域之间的空间关系，同时从自注意力机制捕捉的全局上下文中获益。
- en: 'This raises another question: how and what do ViTs learn from the training
    data? ViTs learn more uniform feature representations across all layers, with
    self-attention mechanisms enabling early aggregation of global information. In
    addition, the residual connections in ViTs strongly propagate features from lower
    to higher layers, in contrast to the more hierarchical structure of CNNs.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这引出了另一个问题：ViTs从训练数据中学习到什么，如何学习？ViTs在所有层中学习到更均匀的特征表示，自注意力机制使得全局信息能够更早地汇聚。此外，ViTs中的残差连接强烈地将特征从较低层传播到较高层，这与CNN的层次结构不同。
- en: ViTs tend to focus more on global than local relationships because their self-attention
    mechanism allows the model to consider long-range dependencies between different
    parts of the input image. Consequently, the self-attention layers in ViTs are
    often considered low-pass filters that focus more on shapes and curvature.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ViTs往往更关注全局而非局部关系，因为它们的自注意力机制使模型能够考虑输入图像不同部分之间的远距离依赖关系。因此，ViTs中的自注意力层通常被认为是低通滤波器，更关注形状和曲率。
- en: In contrast, the convolutional layers in CNNs are often considered high-pass
    filters that focus more on texture. However, keep in mind that convolutional layers
    can act as both high-pass and low-pass filters, depending on the learned filters
    at each layer. High-pass filters detect an image’s edges, fine details, and texture,
    while low-pass filters capture more global, smooth features and shapes. CNNs achieve
    this by applying convolutional kernels of varying sizes and learning different
    filters at each layer.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，CNN中的卷积层通常被认为是高通滤波器，更注重纹理。然而，请记住，卷积层可以充当高通和低通滤波器，这取决于每层学习到的滤波器。高通滤波器检测图像的边缘、细节和纹理，而低通滤波器则捕捉更多全局的、平滑的特征和形状。CNN通过应用不同大小的卷积核，并在每一层学习不同的滤波器来实现这一点。
- en: '**Recommendations**'
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**建议**'
- en: ViTs have recently begun outperforming CNNs if enough data is available for
    pretraining. However, this doesn’t make CNNs obsolete, as methods such as the
    popular EfficientNetV2 CNN architecture are less memory and data hungry.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有足够的数据进行预训练，ViTs最近已经开始超越CNN。然而，这并不意味着CNN会被淘汰，因为像流行的EfficientNetV2 CNN架构这样的技术更不那么依赖于内存和数据。
- en: Moreover, recent ViT architectures don’t rely solely on large datasets, parameter
    numbers, and self-attention. Instead, they have taken inspiration from CNNs and
    added soft convolutional inductive biases or even complete convolutional layers
    to get the best of both worlds.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，最近的 ViT 架构不仅仅依赖于大数据集、参数数量和自注意力。相反，它们从卷积神经网络（CNN）中汲取灵感，增加了软卷积归纳偏置，甚至加入了完整的卷积层，从而获得两者的优势。
- en: In short, vision transformer architectures without convolutional layers generally
    have fewer spatial and locality inductive biases than convolutional neural networks.
    Consequently, vision transformers need to learn data-related concepts such as
    local relationships among pixels. Thus, vision transformers require more training
    data to achieve good predictive performance and produce acceptable visual representations
    in generative modeling contexts.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，没有卷积层的视觉变换器架构通常比卷积神经网络具有更少的空间和局部归纳偏置。因此，视觉变换器需要学习与数据相关的概念，例如像素之间的局部关系。因此，视觉变换器需要更多的训练数据才能达到良好的预测性能，并在生成建模场景中产生可接受的视觉表示。
- en: '**Exercises**'
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**练习**'
- en: '**13-1.** Consider the patchification of the input images shown in [Figure
    13-4](ch13.xhtml#ch13fig4). The size of the resulting patches controls a computational
    and predictive performance trade-off. The optimal patch size depends on the application
    and desired trade-off between computational cost and model performance. Do smaller
    patches typically result in higher or lower computational costs?'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**13-1.** 请考虑[图13-4](ch13.xhtml#ch13fig4)中显示的输入图像的块化。生成的块的大小控制了计算和预测性能之间的权衡。最佳的块大小取决于应用程序以及计算成本和模型性能之间的期望权衡。较小的块通常会导致更高还是更低的计算成本？'
- en: '**13-2.** Following up on the previous question, do smaller patches typically
    lead to a higher or lower prediction accuracy?'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**13-2.** 接着上一问题，较小的块通常会导致更高还是更低的预测准确度？'
- en: '**References**'
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: 'The paper proposing the original vision transformer model: Alexey Dosovitskiy
    et al., “An Image Is Worth 16x16 Words: Transformers for Image Recognition at
    Scale” (2020), *[https://arxiv.org/abs/2010.11929](https://arxiv.org/abs/2010.11929)*.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提出原始视觉变换器模型的论文：Alexey Dosovitskiy 等人，“一张图片值 16x16 个词：用于大规模图像识别的变换器”（2020），*[https://arxiv.org/abs/2010.11929](https://arxiv.org/abs/2010.11929)*。
- en: 'A workaround for adding positional information in ViTs is to use relative positional
    embeddings: Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani, “Self-Attention with
    Relative Position Representations” (2018), *[https://arxiv.org/abs/1803.02155](https://arxiv.org/abs/1803.02155)*.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加位置编码信息的一个解决方法是使用相对位置嵌入：Peter Shaw、Jakob Uszkoreit 和 Ashish Vaswani，“带有相对位置表示的自注意力”（2018），*[https://arxiv.org/abs/1803.02155](https://arxiv.org/abs/1803.02155)*。
- en: 'Residual connections in ViTs strongly propagate features from lower to higher
    layers, in contrast to the more hierarchical structure of CNNs: Maithra Raghu
    et al., “Do Vision Transformers See Like Convolutional Neural Networks?” (2021),
    *[https://arxiv.org/abs/2108.08810](https://arxiv.org/abs/2108.08810)*.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ViT 中的残差连接强烈地将特征从低层传播到高层，这与 CNN 更加分层的结构形成对比：Maithra Raghu 等人，“视觉变换器是否像卷积神经网络一样看待问题？”（2021），*[https://arxiv.org/abs/2108.08810](https://arxiv.org/abs/2108.08810)*。
- en: 'A detailed research article covering the EfficientNetV2 CNN architecture: Mingxing
    Tan and Quoc V. Le, “EfficientNetV2: Smaller Models and Faster Training” (2021),
    *[https://arxiv.org/abs/2104.00298](https://arxiv.org/abs/2104.00298)*.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 详细介绍 EfficientNetV2 CNN 架构的研究文章：Mingxing Tan 和 Quoc V. Le，“EfficientNetV2：更小的模型和更快的训练”（2021），*[https://arxiv.org/abs/2104.00298](https://arxiv.org/abs/2104.00298)*。
- en: 'A ViT architecture that also incorporates convolutional layers: Stéphane d’Ascoli
    et al., “ConViT: Improving Vision Transformers with Soft Convolutional Inductive
    Biases” (2021), *[https://arxiv.org/abs/2103.10697](https://arxiv.org/abs/2103.10697)*.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种也包含卷积层的 ViT 架构：Stéphane d’Ascoli 等人，“ConViT：通过软卷积归纳偏置改进视觉变换器”（2021），*[https://arxiv.org/abs/2103.10697](https://arxiv.org/abs/2103.10697)*。
- en: 'Another example of a ViT using convolutional layers: Haiping Wu et al., “CvT:
    Introducing Convolutions to Vision Transformers” (2021), *[https://arxiv.org/abs/2103.15808](https://arxiv.org/abs/2103.15808)*.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一种使用卷积层的 ViT 示例：Haiping Wu 等人，“CvT：向视觉变换器引入卷积”（2021），*[https://arxiv.org/abs/2103.15808](https://arxiv.org/abs/2103.15808)*。
