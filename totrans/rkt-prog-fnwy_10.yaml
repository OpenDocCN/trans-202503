- en: '10'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '10'
- en: 'TRAC: THE RACKET ALGEBRAIC CALCULATOR'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'TRAC: RACKET 代数计算器'
- en: '![Image](../images/common01.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/common01.jpg)'
- en: Racket provides an ecosystem for language-oriented programming. It has extensive
    built-in capabilities to construct macros, lexers, and parser generators. In this
    final chapter, we unfortunately won’t have time to explore all of these enticing
    topics. However, we’ll explore a number of new topics in computer science and
    utilize many of the topics introduced in previous chapters (and especially leverage
    a number of the computing machine concepts introduced in the previous chapter).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Racket 提供了一个面向语言编程的生态系统。它拥有广泛的内建功能，可以构建宏、词法分析器和解析器生成器。在本章的最后，我们遗憾地没有时间探索所有这些引人入胜的主题。然而，我们将探索计算机科学中的一些新话题，并利用前面章节中介绍的许多主题（尤其是前一章中介绍的计算机概念）。
- en: In the process, we’ll build a command line program called TRAC (The Racket Algebraic
    Calculator), which will take a string of characters representing an algebraic
    expression and compute its value. TRAC is, in fact, a stripped-down version of
    a programming language. If desired, it can be extended in a number of ways to
    implement a full-fledged programming language.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，我们将构建一个名为 TRAC（Racket 代数计算器）的命令行程序，它将接收一个表示代数表达式的字符字符串并计算其值。事实上，TRAC
    是一种简化版的编程语言。如果需要，它可以通过多种方式扩展，以实现一个完整的编程语言。
- en: 'This program will be able accommodate a dialog such as the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这个程序将能够处理如下对话：
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The TRAC Pipeline
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TRAC 流程
- en: To build TRAC, we’ll make use of the following pipeline, which processes the
    input in stages in order to compute the output.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建 TRAC，我们将使用以下的处理流程，该流程分阶段处理输入，以便计算输出。
- en: '![Image](../images/p0276-01.jpg)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/p0276-01.jpg)'
- en: 'The lexer (or *lexical analyzer*) is responsible for taking the input string
    and breaking it into a list of tokens that can then be passed to the parser for
    further processing. Take the following string, for example:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 词法分析器（或 *词法分析器*）负责接收输入字符串，并将其分解为一系列可以传递给解析器进一步处理的标记。例如，考虑以下字符串：
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Given the above string, the lexical analyzer will return an output list similar
    to this:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 给定上述字符串，词法分析器将返回类似于以下的输出列表：
- en: '[PRE2]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Once we’ve produced the token list, we can pass it on to the parser. The job
    of the parser is to determine the structure of the input by building something
    called the *abstract syntax tree* (or *AST*). An AST is a description of the structure
    of an expression. Mathematical expressions such as the one just introduced have
    an inverted tree-like structure. The AST for our example expression is shown in
    [Figure 10-1](ch10.xhtml#ch10fig1).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们生成了标记列表，我们就可以将其传递给解析器。解析器的任务是通过构建一种称为 *抽象语法树*（或 *AST*）的结构，来确定输入的结构。AST 是表达式结构的描述。像刚刚引入的数学表达式具有一种倒置的树状结构。我们示例表达式的
    AST 如 [图 10-1](ch10.xhtml#ch10fig1) 所示。
- en: '![Image](../images/10fig01.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/10fig01.jpg)'
- en: '*Figure 10-1: AST for (*x* + 1.2)*(7.7 / *y*)*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 10-1：(*x* + 1.2)*(7.7 / *y*)* 的 AST'
- en: We can then pass the AST on to the *interpreter* to evaluate the expression
    and calculate the result. If we were building a full-blown computer language,
    the AST would be passed on to a compiler and optimizer, where it would be reduced
    to machine code for efficient execution. Strictly speaking, if the intent were
    to only build an interpreter, it wouldn’t be necessary to build an AST, since
    the parser could simply perform any required computations on the fly, but we’ll
    see later that having the AST available will allow us to manipulate it to derive
    other useful results.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以将 AST 传递给 *解释器*，以评估表达式并计算结果。如果我们在构建一个完全的计算机语言，AST 将被传递给编译器和优化器，在那里它将被转换成机器代码，以便高效执行。严格来说，如果仅仅是构建一个解释器，则不需要构建
    AST，因为解析器可以直接在运行时进行任何必要的计算，但稍后我们会看到，拥有 AST 使我们能够操作它，从而推导出其他有用的结果。
- en: The processing pipeline (lexer, parser, interpreter), in addition to providing
    a clear separation of duties, allows us to plug in different modules optimized
    for specific tasks. For example, an interpreter works well for interactive computations
    but not so much for long running calculations. In such an instance, we’d want
    to substitute a compiler for the interpreter. This would permit our code to be
    converted to machine code and run at full speed by being executed directly by
    the CPU.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 处理流水线（词法分析器、解析器、解释器）除了提供明确的职责分离外，还允许我们插入针对特定任务优化的不同模块。例如，解释器适合交互式计算，但不适合长期运行的计算。在这种情况下，我们希望将解释器替换为编译器。这将允许我们的代码被转换为机器码，并通过CPU直接执行以全速运行。
- en: We’ll discuss and implement each of these components in turn, until we have
    a working algebraic calculator; then we’ll look at a few ways to improve TRAC.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将依次讨论并实现这些组件，直到我们拥有一个功能齐全的代数计算器；然后，我们将研究几种改进TRAC的方法。
- en: The Lexical Analyzer
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词法分析器
- en: In order to split the input into tokens, the lexical analyzer scans the input
    one character at a time looking for certain patterns. At a high level, a token
    is just some sequence of characters that can be categorized in a certain way.
    For example, a string of digits such a 19876 can be categorized as an integer
    token. Strings of characters that start with a letter and are followed by zero
    or more letters and digits (such as “AVG1” or “SIN") can be categorized as identifier
    tokens. Lexical analyzers typically ignore nonessential characters such as spaces
    and tabs (the language Python is a notable exception).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将输入分割成标记，词法分析器一次扫描一个字符，寻找特定的模式。从高层次来看，标记就是一些可以以某种方式分类的字符序列。例如，一串数字如19876可以被归类为整数标记。以字母开头，后面跟着零个或多个字母和数字的字符串（如“AVG1”或“SIN”）可以被归类为标识符标记。词法分析器通常会忽略不必要的字符，如空格和制表符（Python语言是一个显著的例外）。
- en: Each pattern can be represented by a finite-state machine, or FSM (see [Chapter
    9](ch09.xhtml)). One such FSM that we’ll use is a recognizer for unsigned integers.
    In the discussion that follows, certain sets of characters, when grouped together,
    are referred to as a *character class*. One such class we’ll need is the characters
    consisting of the digits from 0 to 9, which we simply designate as the *digit*
    class. An unsigned integer is exclusively composed of a string of digits from
    the digit class, so we can represent its recognizer by the following FSM shown
    in [Figure 10-2](ch10.xhtml#ch10fig2), where the digit class is represented by
    an uppercase italic *D*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模式都可以通过有限状态机（FSM）来表示（参见[第9章](ch09.xhtml)）。我们将使用的一个FSM是无符号整数的识别器。在接下来的讨论中，某些字符集组合在一起时，称为*字符类*。我们需要的一个字符类是由0到9的数字组成的字符，我们将其简单地指定为*数字*类。无符号整数完全由数字类中的数字串组成，因此我们可以通过下图所示的FSM来表示它的识别器，在该FSM中，数字类由一个大写斜体字母*D*表示。
- en: '![Image](../images/10fig02.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/10fig02.jpg)'
- en: '*Figure 10-2: FSM to recognize digits*'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*图10-2：用于识别数字的有限状态机（FSM）*'
- en: This diagram indicates that an unsigned integer always starts with a digit,
    and may be followed by any number of trailing digits. An alternative method for
    representing an unsigned integer is with a *syntax diagram*, such as the one given
    in [Figure 10-3](ch10.xhtml#ch10fig3).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 该图表显示无符号整数总是以数字开始，后面可以跟任意数量的尾随数字。表示无符号整数的另一种方法是使用*语法图*，例如[图10-3](ch10.xhtml#ch10fig3)所示。
- en: '![Image](../images/10fig03.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/10fig03.jpg)'
- en: '*Figure 10-3: Syntax diagram to recognize digits*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*图10-3：用于识别数字的语法图*'
- en: 'In this case, the digit class is represented with a typewriter font like this:
    `digit`. A syntax diagram can sometimes provide a more intuitive representation
    of the pattern being recognized. The syntax diagram shows that, after accepting
    a digit, the analyzer can optionally loop back to accept another digit.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，数字类使用类似打字机字体的样式表示，如`digit`。语法图有时可以提供一种更直观的模式识别表示方式。语法图显示，在接受一个数字后，分析器可以选择性地循环回去接受另一个数字。
- en: To be truly useful, TRAC will need to be able to recognize more than just integers.
    The following syntax diagram in [Figure 10-4](ch10.xhtml#ch10fig4) illustrates
    a recognizer that will accept numbers that consist of unsigned integers, as well
    as floating-point numbers entered with a decimal point and numbers entered in
    scientific notation with an embedded `e`.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/10fig04.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10-4: Syntax diagram to recognize numbers*'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that syntax diagrams can be nested: the boxes in [Figure 10-4](ch10.xhtml#ch10fig4)
    encapsulate the recognizer from [Figure 10-3](ch10.xhtml#ch10fig3). We leave it
    as an exercise for the reader to construct the corresponding FSM.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to recognizing numbers, TRAC recognizes identifiers (like `x` in
    `let x = 4`). TRAC identifiers always start with a letter, followed by any number
    of letters or digits. We’ll designate the letter class with an italic uppercase
    *L*. As such, the following FSM (see [Figure 10-5](ch10.xhtml#ch10fig5)) will
    be used to recognize TRAC identifiers:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/10fig05.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10-5: FSM to recognize identifiers*'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Here’s the corresponding syntax diagram in [Figure 10-6](ch10.xhtml#ch10fig6).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/10fig06.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10-6: Syntax diagram to recognize identifiers*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '***Regular Expressions***'
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: So far the discussion has been at a somewhat abstract level. The question now
    becomes, *How does one actually obtain an FSM that recognizes various character
    patterns?* The answer is *regular expressions*. A regular expression is essentially
    a special language used to build finite-state machines (in this case Racket builds
    the FSM for us, given the regular expression). Our tokens (for example, strings
    of digits constituting integers) are in fact regular languages. Recall from the
    last chapter that a regular language is one where there exists an FSM that can
    accept the entire set of strings. A regular expression is something a bit different.
    A regular expression (as distinct from a regular language) is really a specification
    used to build an FSM that recognizes a regular language.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a regular expression that can be used to recognize unsigned integers:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '`[0-9][0-9]*`'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: 'The expression in square brackets is a character class. In this case, it’s
    the class of digits from 0 to 9\. This regular expression contains two character
    classes, both for recognizing digits. The way to interpret this is that the first
    class will recognize a single digit, but the second, since it’s immediately followed
    by an asterisk, will recognize zero or more additional digits (the asterisk is
    called the *Kleene star* in honor of Stephen Kleene, who formalized the concept
    of regular expressions). A more succinct way to do this is with the following
    regular expression:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '`[0-9]+`'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: The trailing plus sign (called the *Kleene plus*) indicates that we want to
    recognize a string of one or more characters in the class.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 'The Kleene star and Kleene plus are called *quantifiers*. One additional regular
    expression quantifier is the question mark, `?`. The question mark matches zero
    or one occurrence of a regular expression. If we wanted to capture numbers that
    have exactly one or two digits, we could specify it this way:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 克利尼星号和克利尼加号被称为 *量词*。另外一个正则表达式量词是问号 `?`。问号匹配零次或一次正则表达式的出现。如果我们想捕捉恰好有一位或两位数字的数字，可以这样指定：
- en: '`[0-9][0-9]?`'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`[0-9][0-9]?`'
- en: There are a number of additional ways to specify a regular expression class.
    The version we’ve seen for digits specifies a range of values, with the dash (`-`)
    separating the start and end characters. It’s possible to specify multiple ranges
    in a class. For example, to specify a class for both the upper- and lowercase
    characters, one could use `[A-Za-z]`. A class can also contain any arbitrary set
    of characters—for example `[abCD]`.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他方式可以指定正则表达式类。我们已经看到的数字版本指定了一个范围，其中连字符 (`-`) 用于分隔起始字符和结束字符。还可以在一个类中指定多个范围。例如，要指定一个同时包含大写字母和小写字母的类，可以使用
    `[A-Za-z]`。一个类还可以包含任何任意字符集合——例如 `[abCD]`。
- en: 'For our purposes, we’ll define a class consisting of the arithmetic operators:
    `[-+/*^]`. There are a couple of items to note about this particular class. The
    first is that since the class starts with a dash, the dash isn’t used to specify
    a range, so it’s treated as an ordinary character. The second is that the circumflex
    (`^`) would be treated differently if it was the first item in the class. For
    example, the regular expression `[^abc]` would match all characters *except* `a`,
    `b`, or `c`.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的目的，我们将定义一个包含算术运算符的类：`[-+/*^]`。关于这个特定类，有几点需要注意。首先，由于类以连字符开始，因此连字符不会用于指定范围，它被当作普通字符处理。第二点是，如果连字符
    (`^`) 是类中的第一个项目，它将被特殊处理。例如，正则表达式 `[^abc]` 会匹配除 `a`、`b` 或 `c` 外的所有字符。
- en: These are just the basics. Given this overview, let’s look at how Racket implements
    regular expressions and in the process dig deeper into the capability of regular
    expressions.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是基础知识。了解了这些概述后，让我们来看看 Racket 如何实现正则表达式，并在此过程中更深入地挖掘正则表达式的能力。
- en: '***Regular Expressions in Racket***'
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***Racket 中的正则表达式***'
- en: 'Racket builds regular expressions with the `regexp` function, which takes a
    string and converts it to a regular expression:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Racket 使用 `regexp` 函数构建正则表达式，该函数接受一个字符串并将其转换为正则表达式：
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: There’s also a special literal regexp value that starts with `#rx`. For example,
    a regexp value that recognizes unsigned integers is `#rx"[0-9]+"` (or `#rx"[0-9][0-9]*"`
    if you like typing). This syntax is a shorthand method of constructing regular
    expressions.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个特殊的文字正则表达式值，它以 `#rx` 开头。例如，一个识别无符号整数的正则表达式值是 `#rx"[0-9]+"`（或者如果你喜欢打字，可以用
    `#rx"[0-9][0-9]*"`）。这种语法是一种构建正则表达式的简写方法。
- en: 'Regexp values are used in conjunction with the functions `regexp-match` and
    `regexp-match-positions`. Suppose we wanted to find the integer embedded in the
    string `"Is the number 1234 an integer?"`. One way to do it would be with the
    following:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式值与函数 `regexp-match` 和 `regexp-match-positions` 一起使用。假设我们想在字符串 `"Is the
    number 1234 an integer?"` 中查找嵌入的整数。可以通过以下方式之一来实现：
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The match is returned in a list. The reason for this is that regular expressions
    can contain subexpressions that will result in additional matches being returned.
    We’ll touch on this a bit later. The `regexp-match-positions` functions works
    in a similar fashion to `regexp-match`. The difference is that `regexp-match-positions`
    doesn’t return the matched string; instead, it returns the indices that can be
    used with `substring` to extract the match. Here’s an example.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 匹配结果以列表的形式返回。之所以这样做，是因为正则表达式可以包含子表达式，这会导致额外的匹配项被返回。我们稍后会讨论这一点。`regexp-match-positions`
    函数的工作方式与 `regexp-match` 类似。不同之处在于，`regexp-match-positions` 不返回匹配的字符串；相反，它返回可以与
    `substring` 一起使用的索引，以提取匹配的内容。下面是一个示例。
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: These functions have a number of useful optional parameters. Instead of searching
    the entire string, the range can be limited by specifying start and stop positions.
    Here are some examples.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数有许多有用的可选参数。通过指定起始位置和停止位置，可以限定搜索范围，而不是搜索整个字符串。以下是一些示例。
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Notice in the second example that `regexp-match-positions` always returns the
    position of the match from the start of the string and not from the specified
    starting position. The ending position is optional, and if not specified, the
    search continues until the end of the string is reached, as seen in the third
    example.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个例子中，注意 `regexp-match-positions` 总是返回从字符串开始位置的匹配位置，而不是从指定的起始位置开始。结束位置是可选的，如果没有指定，搜索会一直进行到字符串的末尾，就像第三个例子中所示。
- en: 'Probably the most basic regular expressions are just literal letters and digits.
    For example, to determine whether a string contains the string `"gizmo"`, one
    could form this query:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 可能最基本的正则表达式就是字面上的字母和数字。例如，要判断一个字符串是否包含字符串 `"gizmo"`，可以形成这样的查询：
- en: '[PRE7]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Of course this type of functionality could be obtained from `string-contains?`,
    but regular expressions are much more powerful. Used in conjunction with the Kleene
    star and plus operators, we can form much more sophisticated queries.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这种功能也可以通过 `string-contains?` 来实现，但正则表达式要强大得多。与 Kleene 星号和加号操作符配合使用时，我们可以形成更加复杂的查询。
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The period in a regular expression will match any single character, so the regular
    expression above will match any substring that has the string `"cats"` followed
    somewhere else by the string `"dogs"`.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式中的句点（`.`）会匹配任何单一字符，因此上面的正则表达式会匹配任何包含字符串 `"cats"`，并且在其他地方跟随字符串 `"dogs"`
    的子字符串。
- en: What if we just want to know if the string contains `"cats"` *or* `"dogs"`?
    This is where the regular expression *or* operator, which consists of a vertical
    bar (`|`), comes into play.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只想知道字符串中是否包含 `"cats"` *或* `"dogs"`，该怎么办？这时正则表达式的*或*操作符，即竖线（`|`），就派上用场了。
- en: '[PRE9]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The circumflex (`^`) and and dollar sign (`$`) characters are special regular
    expression markers. The circumflex indicates that the match must start at the
    beginning of the string or, if a start position is specified, at the start position.
    Likewise, the dollar sign indicates that the match must extend to the end of the
    string or the ending position, if specified.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 上箭头符号（`^`）和美元符号（`$`）是特殊的正则表达式标记。上箭头表示匹配必须从字符串的开始处开始，或者如果指定了起始位置，则从起始位置开始。同样，美元符号表示匹配必须延伸到字符串的末尾或结束位置（如果指定了的话）。
- en: '[PRE10]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[Table 10-1](ch10.xhtml#ch10tab1) provides a summary description of the various
    regular expression operators. The string “…” in the table represents an arbitrary
    list of characters.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[Table 10-1](ch10.xhtml#ch10tab1) 提供了各种正则表达式操作符的摘要描述。表格中的字符串“…”代表一组任意字符。'
- en: '**Table 10-1**: Regular Expression Operators'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**Table 10-1**：正则表达式操作符'
- en: '| **Operator** | **Description** |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| **Operator** | **描述** |'
- en: '| --- | --- |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| . | Match any character |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| . | 匹配任何字符 |'
- en: '| *x** | Match *x* zero or more times |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| *x** | 匹配 *x* 零次或多次 |'
- en: '| *x*+ | Match *x* one or more times |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| *x*+ | 匹配 *x* 一次或多次 |'
- en: '| *x*? | Match *x* zero or one time |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| *x*? | 匹配 *x* 零次或一次 |'
- en: '| *x*∣*y* | Match *x* or *y* |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| *x*∣*y* | 匹配 *x* 或 *y* |'
- en: '| ^ | Match from start of string |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| ^ | 匹配字符串的开始 |'
- en: '| $ | Match to end of string |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| $ | 匹配字符串的结束 |'
- en: '| […] | Define character class |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| […] | 定义字符类 |'
- en: '| [^…] | Define excluded character class |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| [^…] | 定义排除字符类 |'
- en: One thing that’s not obvious in the discussion so far is that each letter and
    digit is in fact a regular expression. A string such as `"abc"` is actually the
    concatenation of the letters `a`, `b`, and `c`. Much like multiplication in a
    mathematical expression such as 3*a*, concatenation is implicit in regular expressions.
    Also like multiplication versus addition, concatenation has a higher precedence
    than the or (`|`) operator. This means that an expression like `"abc|def"` is
    interpreted as `"(abc)|(def)"` instead of `"ab(c|d)ef"` (note the parentheses
    in these last two strings are just examples of how the regular expression `"abc|def"`
    is *interpreted*, but see the following for more on how parentheses play into
    regular expressions).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止讨论中有一个不明显的地方，那就是每个字母和数字实际上都是一个正则表达式。像 `"abc"` 这样的字符串实际上是字母 `a`、`b` 和 `c`
    的连接。就像数学表达式中的乘法（例如 3*a*）一样，连接在正则表达式中是隐式的。并且像乘法与加法一样，连接的优先级高于或（`|`）操作符。这意味着像 `"abc|def"`
    这样的表达式会被解释为 `"(abc)|(def)"`，而不是 `"ab(c|d)ef"`（注意，最后两个字符串中的括号只是用来说明正则表达式 `"abc|def"`
    是如何被*解释*的，但请参见下面关于括号在正则表达式中的作用）。
- en: Parentheses are used in regular expressions to group subexpressions together
    and to specify order of evaluation. Let’s see how this plays out.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 括号在正则表达式中用于将子表达式组合在一起，并指定评估的顺序。让我们看看这如何发挥作用。
- en: '[PRE11]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The first two examples return the first part of the string that matches either
    `"abc"` or `"def"`.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个示例返回匹配 `"abc"` 或 `"def"` 的字符串的第一部分。
- en: 'The third example, using subexpressions, returns three values. The first is
    the expected match for the overall regular expression. The second value represents
    the answer to the question: within the first returned value, what is the match
    for the subexpression `"(abc)"`? In this case, the value is just the string `"(abc)"`.
    The third value answers this question: within the first returned value, what is
    the match for the subexpression `"(def)"`? In this case there’s no match, so it
    returns `#f`.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个示例，使用子表达式，返回三个值。第一个是符合整体正则表达式的预期匹配。第二个值表示对这个问题的回答：在第一个返回值中，子表达式 `"(abc)"`
    的匹配是什么？在这个案例中，值就是字符串 `"(abc)"`。第三个值回答这个问题：在第一个返回值中，子表达式 `"(def)"` 的匹配是什么？在这个案例中没有匹配，因此返回
    `#f`。
- en: In the fourth example, the match fails because the regular expression is looking
    for a string with either `c` or `d`, but not both. In the last example, the entire
    string was matched, which is reflected in the first return value, but the second
    value reflects the fact that only the `"c"` from the subexpression `"(c|d)"` was
    matched.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在第四个示例中，匹配失败，因为正则表达式在查找包含 `c` 或 `d` 的字符串，但不能同时包含两者。在最后一个示例中，整个字符串被匹配，这在第一个返回值中有所体现，但第二个返回值反映了只有子表达式
    `"(c|d)"` 中的 `"c"` 被匹配。
- en: In our lexical analyzer, we’ll want to use subexpressions, but we’ll only want
    to know whether the overall regular expression found a match, and we won’t be
    interested in individual subexpression matches (that is, we’re mainly using it
    to control evaluation). In this case, we’ll use a special parentheses syntax,
    `"(?>...)"`, which indicates that we only want the overall match without bothering
    to return matched subexpressions (note that `?:` works in a similar way to `?>`,
    but `?:` allows specifying matching modes, like whether or not the match is case
    sensitive—see the Racket Documentation for specifics).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的词法分析器中，我们希望使用子表达式，但我们只关心整体正则表达式是否找到匹配，而不关心各个子表达式的匹配（也就是说，我们主要使用它来控制评估）。在这种情况下，我们将使用特殊的括号语法
    `"(?>...)"`，表示我们只想要整体匹配，而不返回匹配的子表达式（请注意，`?:` 的作用与 `?>` 类似，但 `?:` 允许指定匹配模式，如是否区分大小写——具体请参见
    Racket 文档）。
- en: '[PRE12]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: One interesting variant of `regexp-match` is `regexp-match*`. This particular
    function (although we won’t have a need for it in our application) returns the
    subexpression matches only.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`regexp-match` 的一个有趣变种是 `regexp-match*`。这个特定的函数（虽然我们在应用中不需要它）只返回子表达式的匹配项。'
- en: '[PRE13]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Notice that `regexp-match` only matches `"abc"`, but `regexp-match*` returns
    a list of all matches, so both `"abc"` and `"def"` are returned. See the Racket
    Documentation for more on `regexp-match*`.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`regexp-match` 只匹配 `"abc"`，而 `regexp-match*` 返回所有匹配的列表，因此会返回 `"abc"` 和 `"def"`。更多信息请参见
    Racket 文档中的 `regexp-match*`。
- en: '**NOTE**'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*Racket provides an additional form of regular expressions that conform to
    the ones used in the Perl programming language. The function used to create regular
    expressions of this form is called `pregexp`. There’s also a literal syntax, similar
    to the `#rx` form, but starting with `#px` instead. The Perl syntax provides a
    number of useful extensions, including predefined character classes. Since our
    needs are fairly simple, we’ll stick with the basic syntax outlined above.*'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*Racket 提供了一种附加的正则表达式形式，符合 Perl 编程语言中使用的正则表达式。用于创建这种正则表达式的函数叫做 `pregexp`。还有一种文字语法，类似于
    `#rx` 形式，但以 `#px` 开头。Perl 语法提供了一些有用的扩展，包括预定义的字符类。由于我们的需求相对简单，我们将坚持使用上述基本语法。*'
- en: '***Regular Expressions in TRAC***'
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***TRAC中的正则表达式***'
- en: In TRAC (or any calculator for that matter), we need to identify valid numeric
    strings (floating-point numbers, to be exact). In addition we’ll want to define
    variables, so that means we’ll need to be able to define identifiers. We’ll need
    to specify mathematical operators for addition, subtraction, and so on as well
    as a judicious set of elementary function names. These items all dictate the use
    of regular expressions.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TRAC（或者任何计算器中），我们需要识别有效的数字字符串（准确来说是浮动小数点数）。此外，我们还需要定义变量，这意味着我们需要能够定义标识符。我们还需要指定数学运算符，如加法、减法等，并定义一组合理的基本函数名。这些都需要使用正则表达式。
- en: 'For the purposes of our TRAC application, we’ll always specify the starting
    position for the regular expression search, so each regular expression will start
    with `^`. The recognizer for identifiers is defined as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了我们TRAC应用的需要，我们将始终指定正则表达式搜索的起始位置，因此每个正则表达式都会以`^`开始。标识符的识别器定义如下：
- en: '[PRE14]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: It should be clear from the information above that this will match any string
    that starts with a letter and is followed by zero or more letters or digits.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的信息应该能清楚地看出，这将匹配任何以字母开头，后面跟着零个或多个字母或数字的字符串。
- en: The recognizer for numbers (below) is a bit more involved, but the only new
    element is the portion with `\\.`. Since the period (`.`) is a regular expression
    that matches any character, it needs to be *escaped* so that it can be treated
    as a regular character (if a character has a special meaning in regular expressions,
    escaping is a means of removing, or *escaping*, that special meaning). To avoid
    having to escape the period, we could also have specified `\\.` as `[.]`, which
    might be easier to read in some contexts. The regular expression escape character
    is the backslash (`\`), and since it’s embedded in a Racket string, it must also
    be escaped by prefixing it with another slash.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 数字的识别器（如下所示）稍微复杂一点，但唯一的新元素是带有`\\.`的部分。由于句点（`.`）是一个匹配任意字符的正则表达式，它需要被*转义*，以便将其视为普通字符（如果字符在正则表达式中具有特殊含义，转义就是去除或*转义*这种特殊含义的一种方式）。为了避免转义句点，我们也可以将`\\.`指定为`[.]`，这在某些上下文中可能更容易阅读。正则表达式的转义字符是反斜杠（`\`），而且由于它嵌入在Racket字符串中，因此必须通过在前面加一个斜杠来转义。
- en: '[PRE15]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: While this is a bit long, it closely mirrors the definition specified by the
    syntax diagram given earlier in [Figure 10-4](ch10.xhtml#ch10fig4). Let’s review
    a few test cases.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这有点长，但它与前面[图 10-4](ch10.xhtml#ch10fig4)中给出的语法图定义非常相似。让我们回顾几个测试用例。
- en: '[PRE16]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Notice in the last expression that the match didn’t include the decimal point,
    since we specified that a decimal point must be followed by at least one digit.
    This is in line with the syntax diagram, since the match is up to but doesn’t
    include the decimal point. If the regular expression had ended with `$`, this
    match would have failed. Notice the following.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在最后的表达式中，匹配没有包括小数点，因为我们规定小数点后必须至少跟随一个数字。这与语法图一致，因为匹配是直到但不包括小数点。如果正则表达式以`$`结尾，这个匹配将会失败。请注意以下内容。
- en: '[PRE17]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In this case, the entire string is matched. Here are a few more examples.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，整个字符串都会被匹配。这里有几个更多的例子。
- en: '[PRE18]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Again, the match didn’t include the `e`, since we specified that an `e` must
    be followed by at least one digit.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，匹配没有包括`e`，因为我们规定`e`后必须跟随至少一个数字。
- en: '[PRE19]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The definition for arithmetic operators is obvious.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 算术运算符的定义很明显。
- en: '[PRE20]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We’ll want to skip over any space characters, so we add this to our toolbox:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望跳过任何空格字符，因此我们将其添加到我们的工具箱中：
- en: '[PRE21]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: To make TRAC truly useful, we include the usual transcendental functions.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让TRAC真正有用，我们包括了常见的超越函数。
- en: '[PRE22]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Finally, to facilitate variable assignment, we create a regular expression for
    keywords. For now, `let` is our only keyword.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了方便变量赋值，我们为关键字创建了一个正则表达式。目前，`let`是我们唯一的关键字。
- en: '[PRE23]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '***The Lexer***'
  id: totrans-120
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***词法分析器***'
- en: 'With the essential definitions in place, we move on to actually defining the
    lexical analyzer. Rather than just returning a list of tokens, we’re going to
    supplement each token value with its type. For example, if an identifier is matched,
    we’re going to return a pair: the first element of the pair is the token type,
    in this case `identifier`, and the second element is the matched string. This
    additional information will make the job of the parser a bit easier.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义了必要的定义后，我们继续实际定义词法分析器。我们不仅仅返回一个令牌列表，而是将每个令牌值与其类型进行补充。例如，如果匹配到了一个标识符，我们将返回一个对：对的第一个元素是令牌类型，在这种情况下是`identifier`，第二个元素是匹配的字符串。这些额外的信息将使解析器的工作稍微容易一点。
- en: 'The lexer is (conceptually) fairly simple: it just sequentially tries to match
    each token type while keeping track of the position of the matched string. If
    no match is found, the process fails. If a match is found, the token and its position
    are recorded, and the process repeats at the next position. This continues until
    the entire input string is consumed.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 词法分析器（从概念上讲）相当简单：它只是顺序地尝试匹配每种令牌类型，并跟踪匹配字符串的位置。如果没有找到匹配项，过程会失败。如果找到匹配项，则会记录令牌及其位置，并在下一个位置重复该过程。这个过程会一直持续，直到整个输入字符串被消耗完。
- en: Another point of interest is that we’re using `regexp-match-positions` as our
    matching function. This will allow us to easily get the position of the next location
    once a match has been made.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: The TRAC lexical analyzer is a function called `tokenize`, as given below. The
    main body of the code is a few lines long (see the `cond` block ➌); the rest of
    the code is composed of a few helper functions to manage some of the bookkeeping.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: At each iteration of the loop beginning on the second line, the variable `i`
    has the current starting position within the input string `instr`. After initializing
    `str-len` and `next-pos`, the function reads past any whitespace ➊. The `match-reg`
    function executes the regular expression passed to it in `regex` and sets `next-pos`
    to the next position in the string if there is a match; otherwise it’s set to
    `#f`. If there’s a match, `next-pos` is returned; otherwise the function returns
    `#f`. The `classify` function ➋ merges the token type and the token value into
    a Racket `cons` cell. If the token is a number, it also converts the string value
    to the corresponding numeric value. The `at-end` function tests whether the tokenizer
    is at the end of a keyword or function. A string like `sine` is a valid variable
    name, but wouldn’t be valid as the function name `sin`, so `at-end` allows the
    tokenizer to differentiate one input type from another.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: With these functions available, the actual logic to tokenize the string is fairly
    straightforward. A check is made ➌ to see if we’re at the end of the string, and
    if so, the empty list is returned. Next is a series of checks ➍ to see whether
    the text at the current position in the string matches any one of the specified
    regular expressions; if so, the matching token is packaged up in a `cons` cell
    by `classify` and returned. If no match is found, the `cond` statement returns
    `#f`, which results in an error being generated ➑. If the value of `token` is
    anything other than `#f`, it’s added to the returned list ➐. We didn’t bother
    setting up regular expressions for parentheses, since they can be handled easily
    ➎ ➏.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: The order in which the regular expressions are evaluated is important. If `regex-ident`
    were evaluated before `regex-fname`, a function name like `cos` could mistakenly
    be interpreted as an ordinary variable name instead of the cosine function (this
    could be dealt with in the parser, but it’s better to offload as much work as
    possible to the lexical analyzer).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of the output:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The Parser
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our next major TRAC component is the parser. The parser takes the token list
    from the lexical analyzer and outputs an abstract syntax tree that can be further
    processed by either an interpreter or a compiler. We first provide a formal definition
    of our grammar, which will be used as a guide in the construction of the parser.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '***TRAC Grammar Specification***'
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Computer languages are often specified by a metasyntax (a syntax that describes
    another syntax) called *extended Backus–Naur form (EBNF)*. You’ll notice many
    similarities between EBNF and regular expressions, but EBNF has more expressive
    power. EBNF can be used to describe *context-free grammars*, or *CFG* (see [“A
    Few Words About Languages” on page 272](ch09.xhtml#ch00lev1sec_54)), which are
    out of the reach of regular expressions. (TRAC utilizes a CFG.) This notation
    will be used to give a formal definition to TRAC. We’re going to begin simply,
    by formally defining what’s meant by `digit` (we’re actually going to use the
    lexical analyzer to recognize numbers and identifiers, but for the sake of introducing
    simple examples of EBNF, we also define them here).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: digit = "1" | "2" | "3" | "4" | "5" | "6" | "7" | "8" | "9";
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: This is called a *production rule*. As in regular expressions, the vertical
    bar (`|`) means *or*. Items in quotation marks (`"`) are called *terminals*, and
    the identifier `digit` is called a *nonterminal*. A terminal is a sequence of
    actual characters (such as what you type on your computer terminal). A nonterminal
    is a label for a rule, such as `digit` above. The definition for `letter` is similar,
    but we don’t show it here, because you can figure it out.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: 'The production for `unsigned` follows directly from `digit`:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: unsigned = digit , { digit };
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: In EBNF, curly brackets `{` and `}` function almost exactly like the Kleene
    star (except that they also allow grouping items together). This means the items
    within curly brackets can be repeated zero or more times. The comma (`,`) is the
    concatenation operator.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 'With these entities established, we define `identifier` as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: identifier = letter , { letter | digit };
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'The production for `number` is as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: number = unsigned , [ "."  unsigned ]
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: ', [ "e",  [ "+" | "-" ] , unsigned ];'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: This production introduces the use of square brackets `[` and `]`. Much like
    the regular expression `?`, square brackets enclose optional items.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: 'Function names are defined as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: fname = "sin" | "cos" | "tan" | "asin" | "acos" | "atan"
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '| "log" | "ln";'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: All these productions have regular expression equivalents, so the implementation
    is managed by the lexer. The parser will implement more complex production rules.
    Arithmetic expressions typically contain several levels of nested parenthetical
    expressions; such expressions constitute a context-free grammar. As mentioned
    in the previous chapter, parsing such expressions exceeds the capability of an
    FSA (and by extension, regular expressions). Therefore, we now need the expressive
    power of EBNF to complete our definitions.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: With these preliminaries out of the way, we can now give the rest of the definition
    of the TRAC grammar. Since we only use production names without spaces, commas
    will be omitted, and therefore concatenation is implicit.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: statement = "let" identifier "=" expr
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '| expr;'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: expr = term { [ "+" | "-" ] term };
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: term = neg { [ "*" | "/" ] neg };
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: neg = "-" neg
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: neg = "-" neg
- en: '| pow;'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '| pow;'
- en: pow = factor | factor "^" pow;
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: pow = factor | factor "^" pow;
- en: factor = number
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: factor = number
- en: '| identifier'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '| identifier'
- en: '| "(" expr ")"'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '| "(" expr ")"'
- en: '| fname  "(" expr ")";'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '| fname  "(" expr ")";'
- en: These rules are written in such a way that the higher-precedence operators are
    nested further down. Because of how EBNF is evaluated (example below), this ensures
    that multiplication and division occurs before addition and subtraction. Likewise,
    exponentiation occurs ahead of multiplication and division. Note also that the
    `pow` production is defined recursively, with the recursive call to the right
    of the operator. This makes exponentiation right-associative, which is how it’s
    normally handled (that is, `a^b^c` is interpreted as `a^(b^c)`, where the rightmost
    exponentiation is performed first).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这些规则是这样编写的，使得优先级更高的运算符被嵌套在更深的地方。因为EBNF的求值方式（见下例），这确保了乘法和除法在加法和减法之前发生。同样，指数运算发生在乘法和除法之前。还要注意，`pow`产生式是递归定义的，递归调用位于运算符的右侧。这使得指数运算是右结合的，这是正常的处理方式（即，`a^b^c`被解释为`a^(b^c)`，其中最右侧的指数运算首先进行）。
- en: '[Table 10-2](ch10.xhtml#ch10tab2) illustrates how the productions are expanded
    for the expression *a* * (1 + *b*).'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 10-2](ch10.xhtml#ch10tab2)展示了如何为表达式*a* * (1 + *b*)扩展产生式。'
- en: '**Table 10-2**: Expansion of *a* * (1 + *b*)'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 10-2**：*a* * (1 + *b*)的展开'
- en: '![Image](../images/p0290.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/p0290.jpg)'
- en: Standard typeface is used to designate terminal tokens, and italics are used
    to designate nonterminal rules. The notation *expr-op* refers to the expression
    operators `+` and `-`, and *term-op* refers to the term operators `*` and `/`.
    Notice that only the leftmost production is expanded until a terminal value is
    recognized. Expansion starts with the *statement* rule on row 1\. A *statement*
    can be an *expr*, which in turn can be a *term*; this is reflected on rows 2 and
    3.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 标准字体用于表示终结符，而斜体用于表示非终结符规则。符号*expr-op*表示表达式运算符`+`和`-`，而*term-op*表示项运算符`*`和`/`。请注意，只有最左侧的产生式会展开，直到识别出一个终结符值。展开从第1行的*statement*规则开始。一个*statement*可以是一个*expr*，而*expr*又可以是一个*term*；这在第2行和第3行中得到了体现。
- en: A *term* can be a *neg* followed by a *term-op* followed by a *neg*. This is
    shown on row 4\. Expansion continues in this fashion until we get to row 7\. Notice
    that our leftmost rule is *identifier*. We now have a terminal, `a`, that satisfies
    this rule. The expansion of this rule is shown on row 8\. The leftmost rule on
    this row is *term-op*, which can be expanded to the terminal `*`. Expansion continues
    in this way until we have parsed the entire string on row 22.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 一个*term*可以是一个*neg*，后跟一个*term-op*，再后跟一个*neg*。这一点在第4行展示。展开按这种方式继续，直到我们到达第7行。请注意，我们的最左侧规则是*identifier*。现在我们有一个终结符`a`，它满足这个规则。这个规则的展开显示在第8行。此行的最左侧规则是*term-op*，它可以展开为终结符`*`。展开继续进行，直到我们在第22行解析完整个字符串。
- en: This grammar is designed in such a way that it’s an *LL(1) grammar*. The term
    LL(1) means that it scans its input (the list of tokens from the lexer) left to
    right, using a leftmost derivation (as we did in the walk-through above), with
    a lookahead (lookahead just defines how far ahead we need to look into the list
    of input tokens) of one symbol (token). This particular type of grammar allows
    parsers to be constructed in such a way that no backtracking is required to parse
    the input stream. LL(1) grammars are recognized by *recursive descent parsers*
    in which each nonterminal production has a procedure (or function) that’s responsible
    for recognizing its portion of the grammar and returns the corresponding portion
    of the syntax tree (or generating an error if the input is incorrect).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这个语法是设计成一个*LL(1)语法*。LL(1)中的LL表示它从左到右扫描输入（来自词法分析器的符号列表），使用最左推导（就像我们在上面的讲解中做的那样），并且具有一个向前看（向前看定义了我们需要查看输入符号列表的多远）的范围为一个符号（符号）。这种特定类型的语法允许解析器以一种无需回溯即可解析输入流的方式构建。LL(1)语法由*递归下降解析器*识别，其中每个非终结符产生式都有一个过程（或函数），负责识别其语法部分，并返回相应的语法树部分（如果输入不正确，则生成错误）。
- en: '***The TRAC Parser***'
  id: totrans-169
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***TRAC解析器***'
- en: As mentioned in the previous section, TRAC will use a recursive descent parser.
    A recursive descent parser is mainly a set of mutually recursive functions where
    there’s a function for each grammar rule. There’s always a starting function (corresponding
    to the top-level rule—which is why this is called a top-down parser), which calls
    other functions as defined by the grammar. The *descent* part of the definition
    comes about due to the fact that the rules continue to nest down until a terminal
    (or error) is encountered.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一节所述，TRAC将使用递归下降解析器。递归下降解析器主要是一组相互递归的函数，每个语法规则都有一个对应的函数。总是有一个起始函数（对应于顶层规则——这就是为什么它被称为自顶向下解析器），该函数根据语法规则调用其他函数。定义中的*descent*部分之所以存在，是因为规则会继续嵌套，直到遇到终结符（或错误）。
- en: We need a few global variables to keep track of the tokens during the parsing
    process.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一些全局变量来跟踪解析过程中的令牌。
- en: '[PRE26]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Next are the predicates used to test various operator types.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是用于测试各种运算符类型的谓词。
- en: '[PRE27]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The following procedure updates the token info whenever the next token value
    is requested.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 以下过程会在每次请求下一个标记值时更新令牌信息。
- en: '[PRE28]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The `accept` function tests whether the input token is of the expected type
    and, if so, reads in the next token and returns `#t`; otherwise, it returns `#f`.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '`accept`函数测试输入的令牌是否为预期类型，如果是，则读取下一个令牌并返回`#t`；否则返回`#f`。'
- en: '[PRE29]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The `expect` function tests whether the input token is of the expected type
    and, if so, reads in the next token and returns `#t`; otherwise, it generates
    an error.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '`expect`函数测试输入的令牌是否为预期类型，如果是，则读取下一个令牌并返回`#t`；否则，它会产生错误。'
- en: '[PRE30]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The reason we have both `accept` and `expect` is that in some cases we need
    to test for various token types without generating an error. For example, the
    *factor* rule accepts a number of different token types. We don’t want to generate
    an error if we’re testing for a number and the current token is an identifier,
    because if the number test fails, we still want to test for an identifier, so
    we use `accept`. On the other hand, if the expected token *must* be of a particular
    type, we use the `expect` function, which generates an error if the current token
    isn’t of the expected type.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们同时使用`accept`和`expect`的原因是，在某些情况下，我们需要测试各种令牌类型，而不产生错误。例如，*factor*规则接受多种不同类型的令牌。如果我们在测试一个数字时，当前令牌是一个标识符，我们不希望产生错误，因为即使数字测试失败，我们仍然希望测试标识符，因此使用`accept`。另一方面，如果预期的令牌*必须*是某种特定类型，我们使用`expect`函数，如果当前令牌不是预期的类型，它将产生错误。
- en: 'We’re now able to define the functions that correspond to each grammar production.
    Even though recursive descent parsers are top-down parsers, we’re going to present
    the code from the bottom up. Since there are fewer dependencies that way, it should
    be easier to understand. Given that, the first function is `factor`:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以定义与每个语法生成式对应的函数。尽管递归下降解析器是自顶向下的解析器，但我们将从底向上展示代码。这样依赖关系更少，应该更容易理解。基于此，第一个函数是`factor`：
- en: '[PRE31]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Note that we need to save the current token value in `val` (which is set in
    the first line of `let`). Once `accept` is called and a match is found, the variable
    `token-value` is set to the value of the next token, which isn’t what we need
    in the return value in the `cond` section of the code. The correspondence between
    the various `cond` tests and the production for `factor` should be self-evident.
    As a bit of explanation for the third condition branch ➊, if we look back at our
    rule for `factor`, we find `"(" expr ")"` as an accepted production. So we see
    that this portion of the code accepts a left parenthesis, calls `expr` to parse
    that part of the rule, and then *expects* a right parenthesis (and errors out
    if that isn’t the current token).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们需要将当前的令牌值保存在`val`中（它在`let`的第一行被设置）。一旦调用了`accept`并找到匹配项，变量`token-value`会被设置为下一个令牌的值，但这不是我们在代码的`cond`部分返回值所需要的内容。各种`cond`测试与`factor`生成式之间的对应关系应该是显而易见的。关于第三个条件分支➊的简要说明，如果我们回顾一下`factor`的规则，会发现`"("
    expr ")"`是一个有效的生成式。因此，我们看到这段代码接受一个左括号，调用`expr`来解析该部分规则，然后*期望*一个右括号（如果当前标记不是右括号，则会报错）。
- en: For each accepted value, a `cons` cell is created where the first element is
    a symbol identifying the node type and the second element is the value. The function
    call portion of the `factor` rule (`fname "(" expr ")"`) wasn’t given a name,
    but we specify ’`func-call` here to identify the node type. This pattern of defining
    functions for rules will be replicated in all the productions, with the end result
    being the desired parser to construct the syntax tree.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 'Next up is the code for `pow`:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This is written in such a way to enforce the grammar rule that requires it to
    be right-associative. This is managed by the recursive call to `pow` ➊. The value
    returned for `pow` is either just the value returned from `factor` or a new pair
    (if the symbol `^` is recognized). The first element of this new pair is the character
    `^` and the second element is another pair, where the first element is the base
    number (from `e1`) and the second element is the power it’s being raised to (from
    a recursive call to `pow`).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: The code for `neg` (unary minus) is quite simple. If needed, it appends a negation
    operator to the return value from `pow` to generate a node for unary minus.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Multiplication and division are handled by the next function, `term`. As long
    as it keeps recognizing other `term` operators (`*` or `/`), it loops, gathering
    values from `neg`. Notice how this differs from the code for `pow`: this code
    makes `term` operators left-associative whereas the code for `pow` makes exponentiation
    right-associative.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Addition and subtraction are managed by `expr`. This function works analogously
    to `term`.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Finally, we get to the top level, where most of the work that needs to be done
    is to set things up to parse the assignment statement.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The actual parser just has to call `tokenize` (the lexer) to convert the input
    string to a list of tokens and kick off the parsing process by calling `statement`.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Notice that if there’s anything left in `token-list`, an error is generated.
    Without this, an input that starts with a valid expression, but has some dangling
    tokens. For example, the following would return a partial result (in this case
    ’`(ident . "x")`) without alerting the user that the input was invalid.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Here it goes with a test input expression:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: It seems to work, but it’s a bit difficult to decipher what’s actually going
    on with this output. We need a procedure that will take the syntax tree and print
    it in a way that makes the structure more obvious. So here it is!
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: It’s essentially one big `match` statement that matches against the node type
    of the tree. The indentation varies depending on the depth of the node in the
    tree. This will provide a visual representation of how the child nodes are lined
    up. With this, we can generate output that is a bit more decipherable.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The parser creates a syntax tree of an input string, and `print-tree` prints
    out a visual representation of the tree. It turns out that `print-tree` provides
    a framework with which to build a routine that can reconstruct the input string
    from the syntax tree. This can be useful for debugging purposes, since it allows
    us to see whether an output string constructed from the AST corresponds to the
    input string. We reverse the process by first creating a token list from the syntax
    tree, and then we create an output string by appending the tokens together.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 解析器创建了输入字符串的语法树，而`print-tree`打印出树的可视化表示。事实证明，`print-tree`提供了一个框架，通过它可以构建一个例程，从语法树中重构输入字符串。这对于调试非常有用，因为它允许我们检查从AST构建的输出字符串是否与输入字符串对应。我们通过首先从语法树创建一个标记列表，然后将这些标记拼接在一起生成输出字符串，来逆转这个过程。
- en: The biggest issue in creating a tree-to-string conversion function lies in deciding
    when to add parentheses around an expression. We certainly want to include them
    when required, but we don’t want to include unnecessary parentheses when they
    aren’t required. To facilitate this, we create a function that returns the precedence
    and associativity of each operator. This is needed to determine whether or not
    parentheses are required (for example, operators with lower precedence will require
    parentheses, and if the precedence is the same, the need for parentheses is dictated
    by the associativity).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 创建树到字符串的转换函数的最大难点在于决定何时在表达式周围加上括号。我们当然希望在需要时包含括号，但在不需要时我们不希望加入不必要的括号。为了解决这个问题，我们创建了一个返回每个操作符优先级和结合性的函数。这是为了判断是否需要括号（例如，优先级较低的操作符需要括号，如果优先级相同，则由结合性决定是否需要括号）。
- en: '[PRE42]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: If a symbol isn’t in the table, the second λ expression returns a default value
    of `(info 90` ’`n)`.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 如果符号不在表中，第二个λ表达式将返回默认值`(info 90` ’`n)`。
- en: 'With this function at hand, we can produce `ast->string`:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个功能，我们可以生成`ast->string`：
- en: '[PRE43]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'A local function called `push` has been defined that adds a token to the output
    string port (`expr-port`). One major difference between this code and `print-tree`
    is that all the `print` statements have been changed to `push` statements. In
    addition, the function that handles the various operators, `push-op` (instead
    of `print-op`), has been expanded to decide when to include parentheses. Aside
    from these changes, the structural similarities, starting with the `match` statement,
    between `ast->string` and `print-tree` should be fairly obvious. So now we can
    go full circle: input string to abstract syntax tree and back to input string:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了一个本地函数`push`，它将一个标记添加到输出字符串端口（`expr-port`）。这段代码与`print-tree`的主要区别在于，所有的`print`语句都被改成了`push`语句。此外，处理各种操作符的函数`push-op`（取代了`print-op`）被扩展，以决定何时加入括号。除了这些变化之外，从`match`语句开始，`ast->string`和`print-tree`之间的结构相似性应该是显而易见的。那么现在我们可以完整回环：从输入字符串到抽象语法树，再到输入字符串：
- en: '[PRE44]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: TRAC
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TRAC
- en: Once the syntax tree has been created, the rest of the work is smooth sailing.
    The main remaining components are a dictionary to hold our variable values and
    the code that actually evaluates our input expressions and produces a numeric
    value. Before we wrap up, we’ll look at a few enhancements, such as adding complex
    numbers and setting the angular mode.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦语法树创建完成，其余的工作就变得轻松了。剩下的主要部分是一个字典，用于保存变量值，以及实际计算输入表达式并生成数值的代码。在我们结束之前，我们将看一些改进，例如添加复数和设置角度模式。
- en: Adding a Dictionary
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 添加字典
- en: Since TRAC has the capability to assign values to variables, we’ll need a dictionary
    to hold the values. We’re actually going to create this in the form of a function,
    where we pass it an action (for example, `get` to retrieve a value and `set` to
    assign a value). This will make it easier to extend its functionality without
    cluttering up the namespace with additional definitions. This also provides an
    example of using a single *rest-id* in a lambda expression. A rest-id is a parameter
    that takes all the arguments supplied to the function in a single list. The `args`
    parameter in the code below is the rest-id that accepts a list of arguments. Notice
    that it’s not surrounded by parentheses.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Observe that this code actually uses a closure to construct the dictionary (that
    is, `vars`, in the form of a hash table). This function returns a function that
    has the dictionary embedded in it.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: With a dictionary to hold variable values in place, we can now define the expression
    evaluator.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Notice that it follows a pattern similar to `ast->string` and `print-tree`;
    the difference is that now, instead of returning or printing a string, it traverses
    the syntax tree and computes the numerical values of the nodes.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Let’s walk through what happens. Given the AST, we extract the parsed symbol
    (`sym`) and value (`val`) ➊. We then match the symbol ➍ and take the appropriate
    action. If we’re given a literal number, we simply return the value. If we have
    an identifier, then we extract the value from the dictionary using `(var` ’`get
    val)`. An arithmetic operation will result in calling `eval-op` ➋, which first
    recursively extracts arguments `n1` and `n2`. It then matches the input symbol
    to determine which operation to perform. A function call ➎ recursively extracts
    its argument via `(loop (cdr val))` and calls `eval-func` ➌ to actually perform
    the computation.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: We’re now in a position to actually perform some calculations.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: To keep from having to call `parse` and `eval-ast` every time, we need to set
    up a read-evaluate-print loop (REPL). To do this, we create a `start` function
    that kicks off the process and sets up a few predefined variables.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Now we can exercise TRAC in a more natural way.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '***A Few Enhancements***'
  id: totrans-231
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We’ve now established the basic functionality of TRAC, but to make it truly
    useful, we’ll add a few enhancements. One important enhancement is to have it
    fail gracefully if the user makes an input error. It might also be nice to provide
    advanced users the ability to work with complex numbers. We’ll explore these topics
    and more in the sections that follow.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '**Exception Handling**'
  id: totrans-233
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'As it stands, TRAC is quite fragile. The slightest misstep will cause it to
    fail:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: It should be more forgiving of erroneous input (we’re human, after all). To
    alleviate this situation, we leverage Racket’s *exception handling* capability.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: When an error occurs in executing Racket code, an exception is raised. An exception
    will either have a type of `exn` or one of its subtypes. The exceptions raised
    by `error` have a type of `exn:fail`. To trap such errors, one wraps the code
    in a `with-handlers` form. A modified version of `start` that uses `with-handlers`
    is given here.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: The `with-handlers` form can trap any number of different types of error. In
    this case, we use the `exn:fail?` predicate to trap generated `exn:fail` errors
    generated by the `error` form. Each trapped error type has a corresponding function
    to manage the trapped error.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Here we use a lambda expression to generate the somewhat uninformative `"An
    error occurred."` message. Evaluating the expression with the missing right parenthesis
    now produces the following outcome.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Observe that this time, even though an error has occurred, the `>` prompt appears,
    indicating that the program is still running. The user now has an opportunity
    to re-enter the expression and continue working.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we want to provide a more informative error message, like the one provided
    by Racket. The `e` parameter handed to the exception handling function is an `exn`
    structure. This structure has a `message` field that contains the actual text
    string of the raised error. So to print the text of the error message, we need
    to modify the lambda function to read as follows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'With this change in place, a session with an erroneous entry would proceed
    as follows:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Notice that evaluating an expression such as `sqrt(-1)` will produce the complex
    number `0+1i`. This may be confusing to users not familiar with complex numbers.
    In this case, it may be preferable to raise an error instead of returning a result.
    To accommodate this, the `start` procedure could be modified as follows:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'With this change in place, evaluating an expression that returns a complex
    number would produce the following result:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '**Complex Numbers**'
  id: totrans-251
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In the previous section, we mentioned throwing an exception if a calculation
    produces a complex number. If users *are* familiar with complex numbers, the lexer
    could be modified to accept complex numbers, in which case the original `start`
    procedure could be kept in place. It’s not extremely difficult to modify TRAC’s
    lexical analyzer such that it works with complex numbers. One might be tempted
    to create a regular expression that recognizes a complex number such as `1+2i`.
    That would be a big mistake. If one evaluates an expression such as `2*1+2i`,
    the expected result is `2+2i` since multiplication has a higher precedence than
    addition. If the lexer returns the entire expression as a number, the parser will
    treat the expression `2*1+2i` as `2*(1+2i)`, which will give the result `2+4i`.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'The actual solution is quite simple. Instead of recognizing the entire complex
    number, we only recognize the imaginary part. That is, the regular expression
    for a number becomes as follows:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Notice that the only change in the expression is the inclusion of `i?` at the
    end, which means we accept an optional `i` at the end of a numeric input.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we make a small modification to `classify` (which is embedded in
    `tokenize`) to handle imaginary numbers.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'With these changes in place, we can compute the following in TRAC:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '**Mode, Reset, and Help Commands**'
  id: totrans-260
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Most calculators allow the user to compute trigonometric functions using either
    degrees or radians. We’d be remiss to omit this capability from TRAC. This will
    require a global variable to contain the trigonometric mode:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: TRAC currently handles numeric entries exactly as Racket would. That is, if
    an exact value is divided by an exact value, a fraction results. For example entering
    `2/4` would return a result of `1/2`. This is typically not what’s expected for
    run-of-the-mill calculations. We’ll thus modify TRAC to give the user the option
    to treat all entries as floating-point numbers or to retain fractional entries.
    To enable this, we’ll use a global variable to maintain the numeric mode.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: It would also be nice to allow the user to reset TRAC to its default start-up
    state, so TRAC is given a new keyword called `reset`, which requires the following
    change to `regex-keyword`.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: The question mark at the end will allow TRAC to have a mini–help system, which
    is accessed by entering `?` on the command line (more on this shortly).
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 'Entering `reset` will result in clearing previous entries in the TRAC dictionary
    and priming it with the default values. These actions are bundled up into a `reset`
    procedure:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The `start` procedure then becomes as follows:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'To accommodate the new `reset` and `?` keywords, the `statement` portion of
    the parser is updated as follows:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: If `reset` or `?` is entered for input, the function returns immediately without
    drilling down into the parser so that the expression evaluator can handle these
    commands directly.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: Of course we still need to modify the trigonometric functions to work properly
    depending on the current mode. The handling of numeric entries will also need
    to be adjusted to ensure that they honor the current numeric mode. Here’s the
    tweaked version of `ast-eval`.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'The actual changes to the trigonometric functions are minor: just a multiplication
    or division by `mode` does the trick (observe how `trig-mode` is handled ➊). Code
    is also added to properly convert exact values to inexact when the mode is set
    to `FLOAT` ➋. Most of the remaining changes involve modifying the assignment statement
    to trap changes to `TrigMode` ➎ and `NumMode` ➏ to ensure that they can only be
    assigned proper values. Note the additions for `reset` ➌ and `help` ➍. The `print-help`
    procedure is provided here:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Here’s a session illustrating the new functionality.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Pretty cool, eh?
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '***Making Sure TRAC Works Properly***'
  id: totrans-282
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Given the nature of this application, it would be nice to have some degree of
    comfort that it’s performing the calculations properly. If you were using this
    to calculate the landing trajectory of a spaceship to the moon, it wouldn’t do
    to have it return a calculation that results in the spaceship flying out into
    empty space instead.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: Of course one could sit down and manually enter a large number of test equations
    into TRAC and verify the results by entering the same equations on some other
    calculator and seeing if the results are the same. This clearly wouldn’t be much
    fun (or very efficient, for that matter). No, we want an automated process where
    we can have the computer do all the work. The approach we’re going to take is
    to build a procedure that will generate a random Racket expression. This expression
    can be evaluated using the Racket `eval` function to get a numeric value. In addition
    we’ll need a function that converts the Racket expression into a TRAC expression
    string. We can evaluate the TRAC expression to see if it returns the same value.
    We can then have the computer repeat this process thousands of times to make sure
    we don’t produce any mismatches.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: Here’s the code for the random Racket expression generator.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: An operator from the `ops` vector is randomly selected by the `gen-racket` function.
    Values in `ops` include both the operator symbol and the number of arguments it’s
    expecting (this is called its *arity*). Notice that both `log` and minus (`-`)
    have two different arities. The function call `log(x)` (base-10 logarithm) in
    TRAC is the same as `(log x 10)` in Racket. Then `gen-racket` will build an expression
    containing from one to five random operations or functions with random floating-point
    numeric arguments. The result is an actual Racket expression instead of an AST,
    where its arguments and functions are populated with random values.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: Here’s a look at some of the expressions that `gen-racket` produces.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Most of the work involves converting the Racket expressions into TRAC expressions.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: This is largely an adaptation of the `ast->string` function, but using the randomly
    generated Racket expressions created by `gen-racket` as input instead of the TRAC
    syntax tree. We’ve had to make some accommodations to account for the multiple
    arities of `-` and `log`. We also match against the literal function symbols.
    Aside from these considerations, the code should closely mirror that of `ast->string`.
    Here are a few samples of its output.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'The basic idea is to automate the following process:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'So here’s our test bench:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: The result of a computation could potentially result in a complex number (for
    example, `(sqrt -1)`), so we use `magnitude` to get the absolute value size of
    the difference between the values.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: And here’s the output from an initial test run, which in fact indicated that
    the TRAC evaluation routine wasn’t always producing the correct results.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: The common thread in all the mismatches was the exponentiation operator `^`
    (mapped from Racket’s `expt` function), which was inadvertently defined with the
    division operator in `eval-ast` (the `eval-ast` code given above is correct, but
    you can introduce the same error if you want to test this). Once the correction
    was made, another test run produced the following result.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: In this case, no news is *good* news.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '***Making an Executable***'
  id: totrans-304
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There’s really no need to have TRAC dependent on the DrRacket environment. Only
    a few additional steps are required to create an executable file that can be launched
    without starting DrRacket. The first step is to simply add the `(start)` command
    to the last line of the definitions file (see below) so that the program starts
    executing immediately when launched.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Racket supports three different types of executables:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: '**Launcher** This type of executable will execute the current version of your
    `.rkt` source file, so it will include the path to the source file in the executable.
    This will allow your executable to immediately reflect any enhancements to your
    program. The downside is that you can’t move the source file elsewhere or easily
    share the executable with someone else.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '**Standalone** This version embeds the source file in the executable, so there’s
    no problem moving it to another location on your machine. A standalone executable
    still depends on the installed Racket DLLs, so it may not work properly if moved
    to a different machine.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '**Distribution archive** A distribution archive bundles all needed files into
    an install file. The install file can be used to install TRAC on another machine
    as long as the destination machine uses the same operating system as the one the
    archive was created on.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: Before you create an executable, it’s recommended that debugging be turned off.
    This can be done by going to the **Choose Language . . .** dialog (from the Language
    option on the main menu) and pressing the **Show Details** button. This will open
    a panel where you should select **No Debugging**. Once this is done, go to the
    Racket main menu, and from there select **Create Executable . . .** . In the dialog
    box, you may select which of the three different types of executables you want
    to create. It’s even possible to select a custom icon to give TRAC that personal
    touch.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 10-7](ch10.xhtml#ch10fig7) is a screenshot of TRAC running on our machine.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/10fig07.jpg)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10-7: TRAC in action*'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-315
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter, we leveraged our knowledge of abstract computing machines and
    various automata (introduced in the previous chapter) to build an interactive
    command line expression calculator. Along the way we learned about lexers (and
    using regular expressions to construct them), parsers (which construct abstract
    syntax trees), and interpreters. We used EBNF (extended Backus–Naur form) to specify
    our calculator grammar. Once we had our basic calculator built, we enhanced it
    with additional capabilities, such as handling complex numbers and hand degrees
    or radians. Just to be sure our calculator doesn’t give us bogus numbers, we built
    a simple test bench to make certain our code was robust.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: Well, that just about concludes our Racket journey for now. But we’ve only scratched
    the tip of the iceberg. There’s much more capability that we haven’t even hinted
    at. We encourage you to further explore Racket on your own via the Racket website
    and other available literature. Happy learning!
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
