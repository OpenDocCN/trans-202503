- en: '10'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'TRAC: THE RACKET ALGEBRAIC CALCULATOR'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Racket provides an ecosystem for language-oriented programming. It has extensive
    built-in capabilities to construct macros, lexers, and parser generators. In this
    final chapter, we unfortunately won’t have time to explore all of these enticing
    topics. However, we’ll explore a number of new topics in computer science and
    utilize many of the topics introduced in previous chapters (and especially leverage
    a number of the computing machine concepts introduced in the previous chapter).
  prefs: []
  type: TYPE_NORMAL
- en: In the process, we’ll build a command line program called TRAC (The Racket Algebraic
    Calculator), which will take a string of characters representing an algebraic
    expression and compute its value. TRAC is, in fact, a stripped-down version of
    a programming language. If desired, it can be extended in a number of ways to
    implement a full-fledged programming language.
  prefs: []
  type: TYPE_NORMAL
- en: 'This program will be able accommodate a dialog such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The TRAC Pipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To build TRAC, we’ll make use of the following pipeline, which processes the
    input in stages in order to compute the output.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/p0276-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The lexer (or *lexical analyzer*) is responsible for taking the input string
    and breaking it into a list of tokens that can then be passed to the parser for
    further processing. Take the following string, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Given the above string, the lexical analyzer will return an output list similar
    to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Once we’ve produced the token list, we can pass it on to the parser. The job
    of the parser is to determine the structure of the input by building something
    called the *abstract syntax tree* (or *AST*). An AST is a description of the structure
    of an expression. Mathematical expressions such as the one just introduced have
    an inverted tree-like structure. The AST for our example expression is shown in
    [Figure 10-1](ch10.xhtml#ch10fig1).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/10fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10-1: AST for (*x* + 1.2)*(7.7 / *y*)*'
  prefs: []
  type: TYPE_NORMAL
- en: We can then pass the AST on to the *interpreter* to evaluate the expression
    and calculate the result. If we were building a full-blown computer language,
    the AST would be passed on to a compiler and optimizer, where it would be reduced
    to machine code for efficient execution. Strictly speaking, if the intent were
    to only build an interpreter, it wouldn’t be necessary to build an AST, since
    the parser could simply perform any required computations on the fly, but we’ll
    see later that having the AST available will allow us to manipulate it to derive
    other useful results.
  prefs: []
  type: TYPE_NORMAL
- en: The processing pipeline (lexer, parser, interpreter), in addition to providing
    a clear separation of duties, allows us to plug in different modules optimized
    for specific tasks. For example, an interpreter works well for interactive computations
    but not so much for long running calculations. In such an instance, we’d want
    to substitute a compiler for the interpreter. This would permit our code to be
    converted to machine code and run at full speed by being executed directly by
    the CPU.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll discuss and implement each of these components in turn, until we have
    a working algebraic calculator; then we’ll look at a few ways to improve TRAC.
  prefs: []
  type: TYPE_NORMAL
- en: The Lexical Analyzer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to split the input into tokens, the lexical analyzer scans the input
    one character at a time looking for certain patterns. At a high level, a token
    is just some sequence of characters that can be categorized in a certain way.
    For example, a string of digits such a 19876 can be categorized as an integer
    token. Strings of characters that start with a letter and are followed by zero
    or more letters and digits (such as “AVG1” or “SIN") can be categorized as identifier
    tokens. Lexical analyzers typically ignore nonessential characters such as spaces
    and tabs (the language Python is a notable exception).
  prefs: []
  type: TYPE_NORMAL
- en: Each pattern can be represented by a finite-state machine, or FSM (see [Chapter
    9](ch09.xhtml)). One such FSM that we’ll use is a recognizer for unsigned integers.
    In the discussion that follows, certain sets of characters, when grouped together,
    are referred to as a *character class*. One such class we’ll need is the characters
    consisting of the digits from 0 to 9, which we simply designate as the *digit*
    class. An unsigned integer is exclusively composed of a string of digits from
    the digit class, so we can represent its recognizer by the following FSM shown
    in [Figure 10-2](ch10.xhtml#ch10fig2), where the digit class is represented by
    an uppercase italic *D*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/10fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10-2: FSM to recognize digits*'
  prefs: []
  type: TYPE_NORMAL
- en: This diagram indicates that an unsigned integer always starts with a digit,
    and may be followed by any number of trailing digits. An alternative method for
    representing an unsigned integer is with a *syntax diagram*, such as the one given
    in [Figure 10-3](ch10.xhtml#ch10fig3).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/10fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10-3: Syntax diagram to recognize digits*'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the digit class is represented with a typewriter font like this:
    `digit`. A syntax diagram can sometimes provide a more intuitive representation
    of the pattern being recognized. The syntax diagram shows that, after accepting
    a digit, the analyzer can optionally loop back to accept another digit.'
  prefs: []
  type: TYPE_NORMAL
- en: To be truly useful, TRAC will need to be able to recognize more than just integers.
    The following syntax diagram in [Figure 10-4](ch10.xhtml#ch10fig4) illustrates
    a recognizer that will accept numbers that consist of unsigned integers, as well
    as floating-point numbers entered with a decimal point and numbers entered in
    scientific notation with an embedded `e`.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/10fig04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10-4: Syntax diagram to recognize numbers*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that syntax diagrams can be nested: the boxes in [Figure 10-4](ch10.xhtml#ch10fig4)
    encapsulate the recognizer from [Figure 10-3](ch10.xhtml#ch10fig3). We leave it
    as an exercise for the reader to construct the corresponding FSM.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to recognizing numbers, TRAC recognizes identifiers (like `x` in
    `let x = 4`). TRAC identifiers always start with a letter, followed by any number
    of letters or digits. We’ll designate the letter class with an italic uppercase
    *L*. As such, the following FSM (see [Figure 10-5](ch10.xhtml#ch10fig5)) will
    be used to recognize TRAC identifiers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/10fig05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10-5: FSM to recognize identifiers*'
  prefs: []
  type: TYPE_NORMAL
- en: Here’s the corresponding syntax diagram in [Figure 10-6](ch10.xhtml#ch10fig6).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/10fig06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10-6: Syntax diagram to recognize identifiers*'
  prefs: []
  type: TYPE_NORMAL
- en: '***Regular Expressions***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: So far the discussion has been at a somewhat abstract level. The question now
    becomes, *How does one actually obtain an FSM that recognizes various character
    patterns?* The answer is *regular expressions*. A regular expression is essentially
    a special language used to build finite-state machines (in this case Racket builds
    the FSM for us, given the regular expression). Our tokens (for example, strings
    of digits constituting integers) are in fact regular languages. Recall from the
    last chapter that a regular language is one where there exists an FSM that can
    accept the entire set of strings. A regular expression is something a bit different.
    A regular expression (as distinct from a regular language) is really a specification
    used to build an FSM that recognizes a regular language.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a regular expression that can be used to recognize unsigned integers:'
  prefs: []
  type: TYPE_NORMAL
- en: '`[0-9][0-9]*`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The expression in square brackets is a character class. In this case, it’s
    the class of digits from 0 to 9\. This regular expression contains two character
    classes, both for recognizing digits. The way to interpret this is that the first
    class will recognize a single digit, but the second, since it’s immediately followed
    by an asterisk, will recognize zero or more additional digits (the asterisk is
    called the *Kleene star* in honor of Stephen Kleene, who formalized the concept
    of regular expressions). A more succinct way to do this is with the following
    regular expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '`[0-9]+`'
  prefs: []
  type: TYPE_NORMAL
- en: The trailing plus sign (called the *Kleene plus*) indicates that we want to
    recognize a string of one or more characters in the class.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Kleene star and Kleene plus are called *quantifiers*. One additional regular
    expression quantifier is the question mark, `?`. The question mark matches zero
    or one occurrence of a regular expression. If we wanted to capture numbers that
    have exactly one or two digits, we could specify it this way:'
  prefs: []
  type: TYPE_NORMAL
- en: '`[0-9][0-9]?`'
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of additional ways to specify a regular expression class.
    The version we’ve seen for digits specifies a range of values, with the dash (`-`)
    separating the start and end characters. It’s possible to specify multiple ranges
    in a class. For example, to specify a class for both the upper- and lowercase
    characters, one could use `[A-Za-z]`. A class can also contain any arbitrary set
    of characters—for example `[abCD]`.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our purposes, we’ll define a class consisting of the arithmetic operators:
    `[-+/*^]`. There are a couple of items to note about this particular class. The
    first is that since the class starts with a dash, the dash isn’t used to specify
    a range, so it’s treated as an ordinary character. The second is that the circumflex
    (`^`) would be treated differently if it was the first item in the class. For
    example, the regular expression `[^abc]` would match all characters *except* `a`,
    `b`, or `c`.'
  prefs: []
  type: TYPE_NORMAL
- en: These are just the basics. Given this overview, let’s look at how Racket implements
    regular expressions and in the process dig deeper into the capability of regular
    expressions.
  prefs: []
  type: TYPE_NORMAL
- en: '***Regular Expressions in Racket***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Racket builds regular expressions with the `regexp` function, which takes a
    string and converts it to a regular expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: There’s also a special literal regexp value that starts with `#rx`. For example,
    a regexp value that recognizes unsigned integers is `#rx"[0-9]+"` (or `#rx"[0-9][0-9]*"`
    if you like typing). This syntax is a shorthand method of constructing regular
    expressions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Regexp values are used in conjunction with the functions `regexp-match` and
    `regexp-match-positions`. Suppose we wanted to find the integer embedded in the
    string `"Is the number 1234 an integer?"`. One way to do it would be with the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The match is returned in a list. The reason for this is that regular expressions
    can contain subexpressions that will result in additional matches being returned.
    We’ll touch on this a bit later. The `regexp-match-positions` functions works
    in a similar fashion to `regexp-match`. The difference is that `regexp-match-positions`
    doesn’t return the matched string; instead, it returns the indices that can be
    used with `substring` to extract the match. Here’s an example.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: These functions have a number of useful optional parameters. Instead of searching
    the entire string, the range can be limited by specifying start and stop positions.
    Here are some examples.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Notice in the second example that `regexp-match-positions` always returns the
    position of the match from the start of the string and not from the specified
    starting position. The ending position is optional, and if not specified, the
    search continues until the end of the string is reached, as seen in the third
    example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Probably the most basic regular expressions are just literal letters and digits.
    For example, to determine whether a string contains the string `"gizmo"`, one
    could form this query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Of course this type of functionality could be obtained from `string-contains?`,
    but regular expressions are much more powerful. Used in conjunction with the Kleene
    star and plus operators, we can form much more sophisticated queries.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The period in a regular expression will match any single character, so the regular
    expression above will match any substring that has the string `"cats"` followed
    somewhere else by the string `"dogs"`.
  prefs: []
  type: TYPE_NORMAL
- en: What if we just want to know if the string contains `"cats"` *or* `"dogs"`?
    This is where the regular expression *or* operator, which consists of a vertical
    bar (`|`), comes into play.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The circumflex (`^`) and and dollar sign (`$`) characters are special regular
    expression markers. The circumflex indicates that the match must start at the
    beginning of the string or, if a start position is specified, at the start position.
    Likewise, the dollar sign indicates that the match must extend to the end of the
    string or the ending position, if specified.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[Table 10-1](ch10.xhtml#ch10tab1) provides a summary description of the various
    regular expression operators. The string “…” in the table represents an arbitrary
    list of characters.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 10-1**: Regular Expression Operators'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Operator** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| . | Match any character |'
  prefs: []
  type: TYPE_TB
- en: '| *x** | Match *x* zero or more times |'
  prefs: []
  type: TYPE_TB
- en: '| *x*+ | Match *x* one or more times |'
  prefs: []
  type: TYPE_TB
- en: '| *x*? | Match *x* zero or one time |'
  prefs: []
  type: TYPE_TB
- en: '| *x*∣*y* | Match *x* or *y* |'
  prefs: []
  type: TYPE_TB
- en: '| ^ | Match from start of string |'
  prefs: []
  type: TYPE_TB
- en: '| $ | Match to end of string |'
  prefs: []
  type: TYPE_TB
- en: '| […] | Define character class |'
  prefs: []
  type: TYPE_TB
- en: '| [^…] | Define excluded character class |'
  prefs: []
  type: TYPE_TB
- en: One thing that’s not obvious in the discussion so far is that each letter and
    digit is in fact a regular expression. A string such as `"abc"` is actually the
    concatenation of the letters `a`, `b`, and `c`. Much like multiplication in a
    mathematical expression such as 3*a*, concatenation is implicit in regular expressions.
    Also like multiplication versus addition, concatenation has a higher precedence
    than the or (`|`) operator. This means that an expression like `"abc|def"` is
    interpreted as `"(abc)|(def)"` instead of `"ab(c|d)ef"` (note the parentheses
    in these last two strings are just examples of how the regular expression `"abc|def"`
    is *interpreted*, but see the following for more on how parentheses play into
    regular expressions).
  prefs: []
  type: TYPE_NORMAL
- en: Parentheses are used in regular expressions to group subexpressions together
    and to specify order of evaluation. Let’s see how this plays out.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The first two examples return the first part of the string that matches either
    `"abc"` or `"def"`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The third example, using subexpressions, returns three values. The first is
    the expected match for the overall regular expression. The second value represents
    the answer to the question: within the first returned value, what is the match
    for the subexpression `"(abc)"`? In this case, the value is just the string `"(abc)"`.
    The third value answers this question: within the first returned value, what is
    the match for the subexpression `"(def)"`? In this case there’s no match, so it
    returns `#f`.'
  prefs: []
  type: TYPE_NORMAL
- en: In the fourth example, the match fails because the regular expression is looking
    for a string with either `c` or `d`, but not both. In the last example, the entire
    string was matched, which is reflected in the first return value, but the second
    value reflects the fact that only the `"c"` from the subexpression `"(c|d)"` was
    matched.
  prefs: []
  type: TYPE_NORMAL
- en: In our lexical analyzer, we’ll want to use subexpressions, but we’ll only want
    to know whether the overall regular expression found a match, and we won’t be
    interested in individual subexpression matches (that is, we’re mainly using it
    to control evaluation). In this case, we’ll use a special parentheses syntax,
    `"(?>...)"`, which indicates that we only want the overall match without bothering
    to return matched subexpressions (note that `?:` works in a similar way to `?>`,
    but `?:` allows specifying matching modes, like whether or not the match is case
    sensitive—see the Racket Documentation for specifics).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: One interesting variant of `regexp-match` is `regexp-match*`. This particular
    function (although we won’t have a need for it in our application) returns the
    subexpression matches only.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Notice that `regexp-match` only matches `"abc"`, but `regexp-match*` returns
    a list of all matches, so both `"abc"` and `"def"` are returned. See the Racket
    Documentation for more on `regexp-match*`.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Racket provides an additional form of regular expressions that conform to
    the ones used in the Perl programming language. The function used to create regular
    expressions of this form is called `pregexp`. There’s also a literal syntax, similar
    to the `#rx` form, but starting with `#px` instead. The Perl syntax provides a
    number of useful extensions, including predefined character classes. Since our
    needs are fairly simple, we’ll stick with the basic syntax outlined above.*'
  prefs: []
  type: TYPE_NORMAL
- en: '***Regular Expressions in TRAC***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In TRAC (or any calculator for that matter), we need to identify valid numeric
    strings (floating-point numbers, to be exact). In addition we’ll want to define
    variables, so that means we’ll need to be able to define identifiers. We’ll need
    to specify mathematical operators for addition, subtraction, and so on as well
    as a judicious set of elementary function names. These items all dictate the use
    of regular expressions.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the purposes of our TRAC application, we’ll always specify the starting
    position for the regular expression search, so each regular expression will start
    with `^`. The recognizer for identifiers is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: It should be clear from the information above that this will match any string
    that starts with a letter and is followed by zero or more letters or digits.
  prefs: []
  type: TYPE_NORMAL
- en: The recognizer for numbers (below) is a bit more involved, but the only new
    element is the portion with `\\.`. Since the period (`.`) is a regular expression
    that matches any character, it needs to be *escaped* so that it can be treated
    as a regular character (if a character has a special meaning in regular expressions,
    escaping is a means of removing, or *escaping*, that special meaning). To avoid
    having to escape the period, we could also have specified `\\.` as `[.]`, which
    might be easier to read in some contexts. The regular expression escape character
    is the backslash (`\`), and since it’s embedded in a Racket string, it must also
    be escaped by prefixing it with another slash.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: While this is a bit long, it closely mirrors the definition specified by the
    syntax diagram given earlier in [Figure 10-4](ch10.xhtml#ch10fig4). Let’s review
    a few test cases.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Notice in the last expression that the match didn’t include the decimal point,
    since we specified that a decimal point must be followed by at least one digit.
    This is in line with the syntax diagram, since the match is up to but doesn’t
    include the decimal point. If the regular expression had ended with `$`, this
    match would have failed. Notice the following.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the entire string is matched. Here are a few more examples.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Again, the match didn’t include the `e`, since we specified that an `e` must
    be followed by at least one digit.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The definition for arithmetic operators is obvious.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll want to skip over any space characters, so we add this to our toolbox:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: To make TRAC truly useful, we include the usual transcendental functions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Finally, to facilitate variable assignment, we create a regular expression for
    keywords. For now, `let` is our only keyword.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '***The Lexer***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'With the essential definitions in place, we move on to actually defining the
    lexical analyzer. Rather than just returning a list of tokens, we’re going to
    supplement each token value with its type. For example, if an identifier is matched,
    we’re going to return a pair: the first element of the pair is the token type,
    in this case `identifier`, and the second element is the matched string. This
    additional information will make the job of the parser a bit easier.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The lexer is (conceptually) fairly simple: it just sequentially tries to match
    each token type while keeping track of the position of the matched string. If
    no match is found, the process fails. If a match is found, the token and its position
    are recorded, and the process repeats at the next position. This continues until
    the entire input string is consumed.'
  prefs: []
  type: TYPE_NORMAL
- en: Another point of interest is that we’re using `regexp-match-positions` as our
    matching function. This will allow us to easily get the position of the next location
    once a match has been made.
  prefs: []
  type: TYPE_NORMAL
- en: The TRAC lexical analyzer is a function called `tokenize`, as given below. The
    main body of the code is a few lines long (see the `cond` block ➌); the rest of
    the code is composed of a few helper functions to manage some of the bookkeeping.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: At each iteration of the loop beginning on the second line, the variable `i`
    has the current starting position within the input string `instr`. After initializing
    `str-len` and `next-pos`, the function reads past any whitespace ➊. The `match-reg`
    function executes the regular expression passed to it in `regex` and sets `next-pos`
    to the next position in the string if there is a match; otherwise it’s set to
    `#f`. If there’s a match, `next-pos` is returned; otherwise the function returns
    `#f`. The `classify` function ➋ merges the token type and the token value into
    a Racket `cons` cell. If the token is a number, it also converts the string value
    to the corresponding numeric value. The `at-end` function tests whether the tokenizer
    is at the end of a keyword or function. A string like `sine` is a valid variable
    name, but wouldn’t be valid as the function name `sin`, so `at-end` allows the
    tokenizer to differentiate one input type from another.
  prefs: []
  type: TYPE_NORMAL
- en: With these functions available, the actual logic to tokenize the string is fairly
    straightforward. A check is made ➌ to see if we’re at the end of the string, and
    if so, the empty list is returned. Next is a series of checks ➍ to see whether
    the text at the current position in the string matches any one of the specified
    regular expressions; if so, the matching token is packaged up in a `cons` cell
    by `classify` and returned. If no match is found, the `cond` statement returns
    `#f`, which results in an error being generated ➑. If the value of `token` is
    anything other than `#f`, it’s added to the returned list ➐. We didn’t bother
    setting up regular expressions for parentheses, since they can be handled easily
    ➎ ➏.
  prefs: []
  type: TYPE_NORMAL
- en: The order in which the regular expressions are evaluated is important. If `regex-ident`
    were evaluated before `regex-fname`, a function name like `cos` could mistakenly
    be interpreted as an ordinary variable name instead of the cosine function (this
    could be dealt with in the parser, but it’s better to offload as much work as
    possible to the lexical analyzer).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The Parser
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our next major TRAC component is the parser. The parser takes the token list
    from the lexical analyzer and outputs an abstract syntax tree that can be further
    processed by either an interpreter or a compiler. We first provide a formal definition
    of our grammar, which will be used as a guide in the construction of the parser.
  prefs: []
  type: TYPE_NORMAL
- en: '***TRAC Grammar Specification***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Computer languages are often specified by a metasyntax (a syntax that describes
    another syntax) called *extended Backus–Naur form (EBNF)*. You’ll notice many
    similarities between EBNF and regular expressions, but EBNF has more expressive
    power. EBNF can be used to describe *context-free grammars*, or *CFG* (see [“A
    Few Words About Languages” on page 272](ch09.xhtml#ch00lev1sec_54)), which are
    out of the reach of regular expressions. (TRAC utilizes a CFG.) This notation
    will be used to give a formal definition to TRAC. We’re going to begin simply,
    by formally defining what’s meant by `digit` (we’re actually going to use the
    lexical analyzer to recognize numbers and identifiers, but for the sake of introducing
    simple examples of EBNF, we also define them here).
  prefs: []
  type: TYPE_NORMAL
- en: digit = "1" | "2" | "3" | "4" | "5" | "6" | "7" | "8" | "9";
  prefs: []
  type: TYPE_NORMAL
- en: This is called a *production rule*. As in regular expressions, the vertical
    bar (`|`) means *or*. Items in quotation marks (`"`) are called *terminals*, and
    the identifier `digit` is called a *nonterminal*. A terminal is a sequence of
    actual characters (such as what you type on your computer terminal). A nonterminal
    is a label for a rule, such as `digit` above. The definition for `letter` is similar,
    but we don’t show it here, because you can figure it out.
  prefs: []
  type: TYPE_NORMAL
- en: 'The production for `unsigned` follows directly from `digit`:'
  prefs: []
  type: TYPE_NORMAL
- en: unsigned = digit , { digit };
  prefs: []
  type: TYPE_NORMAL
- en: In EBNF, curly brackets `{` and `}` function almost exactly like the Kleene
    star (except that they also allow grouping items together). This means the items
    within curly brackets can be repeated zero or more times. The comma (`,`) is the
    concatenation operator.
  prefs: []
  type: TYPE_NORMAL
- en: 'With these entities established, we define `identifier` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: identifier = letter , { letter | digit };
  prefs: []
  type: TYPE_NORMAL
- en: 'The production for `number` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: number = unsigned , [ "."  unsigned ]
  prefs: []
  type: TYPE_NORMAL
- en: ', [ "e",  [ "+" | "-" ] , unsigned ];'
  prefs: []
  type: TYPE_NORMAL
- en: This production introduces the use of square brackets `[` and `]`. Much like
    the regular expression `?`, square brackets enclose optional items.
  prefs: []
  type: TYPE_NORMAL
- en: 'Function names are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: fname = "sin" | "cos" | "tan" | "asin" | "acos" | "atan"
  prefs: []
  type: TYPE_NORMAL
- en: '| "log" | "ln";'
  prefs: []
  type: TYPE_NORMAL
- en: All these productions have regular expression equivalents, so the implementation
    is managed by the lexer. The parser will implement more complex production rules.
    Arithmetic expressions typically contain several levels of nested parenthetical
    expressions; such expressions constitute a context-free grammar. As mentioned
    in the previous chapter, parsing such expressions exceeds the capability of an
    FSA (and by extension, regular expressions). Therefore, we now need the expressive
    power of EBNF to complete our definitions.
  prefs: []
  type: TYPE_NORMAL
- en: With these preliminaries out of the way, we can now give the rest of the definition
    of the TRAC grammar. Since we only use production names without spaces, commas
    will be omitted, and therefore concatenation is implicit.
  prefs: []
  type: TYPE_NORMAL
- en: statement = "let" identifier "=" expr
  prefs: []
  type: TYPE_NORMAL
- en: '| expr;'
  prefs: []
  type: TYPE_NORMAL
- en: expr = term { [ "+" | "-" ] term };
  prefs: []
  type: TYPE_NORMAL
- en: term = neg { [ "*" | "/" ] neg };
  prefs: []
  type: TYPE_NORMAL
- en: neg = "-" neg
  prefs: []
  type: TYPE_NORMAL
- en: '| pow;'
  prefs: []
  type: TYPE_NORMAL
- en: pow = factor | factor "^" pow;
  prefs: []
  type: TYPE_NORMAL
- en: factor = number
  prefs: []
  type: TYPE_NORMAL
- en: '| identifier'
  prefs: []
  type: TYPE_NORMAL
- en: '| "(" expr ")"'
  prefs: []
  type: TYPE_NORMAL
- en: '| fname  "(" expr ")";'
  prefs: []
  type: TYPE_NORMAL
- en: These rules are written in such a way that the higher-precedence operators are
    nested further down. Because of how EBNF is evaluated (example below), this ensures
    that multiplication and division occurs before addition and subtraction. Likewise,
    exponentiation occurs ahead of multiplication and division. Note also that the
    `pow` production is defined recursively, with the recursive call to the right
    of the operator. This makes exponentiation right-associative, which is how it’s
    normally handled (that is, `a^b^c` is interpreted as `a^(b^c)`, where the rightmost
    exponentiation is performed first).
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 10-2](ch10.xhtml#ch10tab2) illustrates how the productions are expanded
    for the expression *a* * (1 + *b*).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 10-2**: Expansion of *a* * (1 + *b*)'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/p0290.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Standard typeface is used to designate terminal tokens, and italics are used
    to designate nonterminal rules. The notation *expr-op* refers to the expression
    operators `+` and `-`, and *term-op* refers to the term operators `*` and `/`.
    Notice that only the leftmost production is expanded until a terminal value is
    recognized. Expansion starts with the *statement* rule on row 1\. A *statement*
    can be an *expr*, which in turn can be a *term*; this is reflected on rows 2 and
    3.
  prefs: []
  type: TYPE_NORMAL
- en: A *term* can be a *neg* followed by a *term-op* followed by a *neg*. This is
    shown on row 4\. Expansion continues in this fashion until we get to row 7\. Notice
    that our leftmost rule is *identifier*. We now have a terminal, `a`, that satisfies
    this rule. The expansion of this rule is shown on row 8\. The leftmost rule on
    this row is *term-op*, which can be expanded to the terminal `*`. Expansion continues
    in this way until we have parsed the entire string on row 22.
  prefs: []
  type: TYPE_NORMAL
- en: This grammar is designed in such a way that it’s an *LL(1) grammar*. The term
    LL(1) means that it scans its input (the list of tokens from the lexer) left to
    right, using a leftmost derivation (as we did in the walk-through above), with
    a lookahead (lookahead just defines how far ahead we need to look into the list
    of input tokens) of one symbol (token). This particular type of grammar allows
    parsers to be constructed in such a way that no backtracking is required to parse
    the input stream. LL(1) grammars are recognized by *recursive descent parsers*
    in which each nonterminal production has a procedure (or function) that’s responsible
    for recognizing its portion of the grammar and returns the corresponding portion
    of the syntax tree (or generating an error if the input is incorrect).
  prefs: []
  type: TYPE_NORMAL
- en: '***The TRAC Parser***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As mentioned in the previous section, TRAC will use a recursive descent parser.
    A recursive descent parser is mainly a set of mutually recursive functions where
    there’s a function for each grammar rule. There’s always a starting function (corresponding
    to the top-level rule—which is why this is called a top-down parser), which calls
    other functions as defined by the grammar. The *descent* part of the definition
    comes about due to the fact that the rules continue to nest down until a terminal
    (or error) is encountered.
  prefs: []
  type: TYPE_NORMAL
- en: We need a few global variables to keep track of the tokens during the parsing
    process.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Next are the predicates used to test various operator types.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The following procedure updates the token info whenever the next token value
    is requested.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The `accept` function tests whether the input token is of the expected type
    and, if so, reads in the next token and returns `#t`; otherwise, it returns `#f`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The `expect` function tests whether the input token is of the expected type
    and, if so, reads in the next token and returns `#t`; otherwise, it generates
    an error.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The reason we have both `accept` and `expect` is that in some cases we need
    to test for various token types without generating an error. For example, the
    *factor* rule accepts a number of different token types. We don’t want to generate
    an error if we’re testing for a number and the current token is an identifier,
    because if the number test fails, we still want to test for an identifier, so
    we use `accept`. On the other hand, if the expected token *must* be of a particular
    type, we use the `expect` function, which generates an error if the current token
    isn’t of the expected type.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’re now able to define the functions that correspond to each grammar production.
    Even though recursive descent parsers are top-down parsers, we’re going to present
    the code from the bottom up. Since there are fewer dependencies that way, it should
    be easier to understand. Given that, the first function is `factor`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Note that we need to save the current token value in `val` (which is set in
    the first line of `let`). Once `accept` is called and a match is found, the variable
    `token-value` is set to the value of the next token, which isn’t what we need
    in the return value in the `cond` section of the code. The correspondence between
    the various `cond` tests and the production for `factor` should be self-evident.
    As a bit of explanation for the third condition branch ➊, if we look back at our
    rule for `factor`, we find `"(" expr ")"` as an accepted production. So we see
    that this portion of the code accepts a left parenthesis, calls `expr` to parse
    that part of the rule, and then *expects* a right parenthesis (and errors out
    if that isn’t the current token).
  prefs: []
  type: TYPE_NORMAL
- en: For each accepted value, a `cons` cell is created where the first element is
    a symbol identifying the node type and the second element is the value. The function
    call portion of the `factor` rule (`fname "(" expr ")"`) wasn’t given a name,
    but we specify ’`func-call` here to identify the node type. This pattern of defining
    functions for rules will be replicated in all the productions, with the end result
    being the desired parser to construct the syntax tree.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next up is the code for `pow`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: This is written in such a way to enforce the grammar rule that requires it to
    be right-associative. This is managed by the recursive call to `pow` ➊. The value
    returned for `pow` is either just the value returned from `factor` or a new pair
    (if the symbol `^` is recognized). The first element of this new pair is the character
    `^` and the second element is another pair, where the first element is the base
    number (from `e1`) and the second element is the power it’s being raised to (from
    a recursive call to `pow`).
  prefs: []
  type: TYPE_NORMAL
- en: The code for `neg` (unary minus) is quite simple. If needed, it appends a negation
    operator to the return value from `pow` to generate a node for unary minus.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Multiplication and division are handled by the next function, `term`. As long
    as it keeps recognizing other `term` operators (`*` or `/`), it loops, gathering
    values from `neg`. Notice how this differs from the code for `pow`: this code
    makes `term` operators left-associative whereas the code for `pow` makes exponentiation
    right-associative.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Addition and subtraction are managed by `expr`. This function works analogously
    to `term`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we get to the top level, where most of the work that needs to be done
    is to set things up to parse the assignment statement.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The actual parser just has to call `tokenize` (the lexer) to convert the input
    string to a list of tokens and kick off the parsing process by calling `statement`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Notice that if there’s anything left in `token-list`, an error is generated.
    Without this, an input that starts with a valid expression, but has some dangling
    tokens. For example, the following would return a partial result (in this case
    ’`(ident . "x")`) without alerting the user that the input was invalid.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Here it goes with a test input expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: It seems to work, but it’s a bit difficult to decipher what’s actually going
    on with this output. We need a procedure that will take the syntax tree and print
    it in a way that makes the structure more obvious. So here it is!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: It’s essentially one big `match` statement that matches against the node type
    of the tree. The indentation varies depending on the depth of the node in the
    tree. This will provide a visual representation of how the child nodes are lined
    up. With this, we can generate output that is a bit more decipherable.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: The parser creates a syntax tree of an input string, and `print-tree` prints
    out a visual representation of the tree. It turns out that `print-tree` provides
    a framework with which to build a routine that can reconstruct the input string
    from the syntax tree. This can be useful for debugging purposes, since it allows
    us to see whether an output string constructed from the AST corresponds to the
    input string. We reverse the process by first creating a token list from the syntax
    tree, and then we create an output string by appending the tokens together.
  prefs: []
  type: TYPE_NORMAL
- en: The biggest issue in creating a tree-to-string conversion function lies in deciding
    when to add parentheses around an expression. We certainly want to include them
    when required, but we don’t want to include unnecessary parentheses when they
    aren’t required. To facilitate this, we create a function that returns the precedence
    and associativity of each operator. This is needed to determine whether or not
    parentheses are required (for example, operators with lower precedence will require
    parentheses, and if the precedence is the same, the need for parentheses is dictated
    by the associativity).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: If a symbol isn’t in the table, the second λ expression returns a default value
    of `(info 90` ’`n)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this function at hand, we can produce `ast->string`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'A local function called `push` has been defined that adds a token to the output
    string port (`expr-port`). One major difference between this code and `print-tree`
    is that all the `print` statements have been changed to `push` statements. In
    addition, the function that handles the various operators, `push-op` (instead
    of `print-op`), has been expanded to decide when to include parentheses. Aside
    from these changes, the structural similarities, starting with the `match` statement,
    between `ast->string` and `print-tree` should be fairly obvious. So now we can
    go full circle: input string to abstract syntax tree and back to input string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: TRAC
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once the syntax tree has been created, the rest of the work is smooth sailing.
    The main remaining components are a dictionary to hold our variable values and
    the code that actually evaluates our input expressions and produces a numeric
    value. Before we wrap up, we’ll look at a few enhancements, such as adding complex
    numbers and setting the angular mode.
  prefs: []
  type: TYPE_NORMAL
- en: Adding a Dictionary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since TRAC has the capability to assign values to variables, we’ll need a dictionary
    to hold the values. We’re actually going to create this in the form of a function,
    where we pass it an action (for example, `get` to retrieve a value and `set` to
    assign a value). This will make it easier to extend its functionality without
    cluttering up the namespace with additional definitions. This also provides an
    example of using a single *rest-id* in a lambda expression. A rest-id is a parameter
    that takes all the arguments supplied to the function in a single list. The `args`
    parameter in the code below is the rest-id that accepts a list of arguments. Notice
    that it’s not surrounded by parentheses.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Observe that this code actually uses a closure to construct the dictionary (that
    is, `vars`, in the form of a hash table). This function returns a function that
    has the dictionary embedded in it.
  prefs: []
  type: TYPE_NORMAL
- en: With a dictionary to hold variable values in place, we can now define the expression
    evaluator.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Notice that it follows a pattern similar to `ast->string` and `print-tree`;
    the difference is that now, instead of returning or printing a string, it traverses
    the syntax tree and computes the numerical values of the nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s walk through what happens. Given the AST, we extract the parsed symbol
    (`sym`) and value (`val`) ➊. We then match the symbol ➍ and take the appropriate
    action. If we’re given a literal number, we simply return the value. If we have
    an identifier, then we extract the value from the dictionary using `(var` ’`get
    val)`. An arithmetic operation will result in calling `eval-op` ➋, which first
    recursively extracts arguments `n1` and `n2`. It then matches the input symbol
    to determine which operation to perform. A function call ➎ recursively extracts
    its argument via `(loop (cdr val))` and calls `eval-func` ➌ to actually perform
    the computation.
  prefs: []
  type: TYPE_NORMAL
- en: We’re now in a position to actually perform some calculations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: To keep from having to call `parse` and `eval-ast` every time, we need to set
    up a read-evaluate-print loop (REPL). To do this, we create a `start` function
    that kicks off the process and sets up a few predefined variables.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Now we can exercise TRAC in a more natural way.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '***A Few Enhancements***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We’ve now established the basic functionality of TRAC, but to make it truly
    useful, we’ll add a few enhancements. One important enhancement is to have it
    fail gracefully if the user makes an input error. It might also be nice to provide
    advanced users the ability to work with complex numbers. We’ll explore these topics
    and more in the sections that follow.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exception Handling**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'As it stands, TRAC is quite fragile. The slightest misstep will cause it to
    fail:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: It should be more forgiving of erroneous input (we’re human, after all). To
    alleviate this situation, we leverage Racket’s *exception handling* capability.
  prefs: []
  type: TYPE_NORMAL
- en: When an error occurs in executing Racket code, an exception is raised. An exception
    will either have a type of `exn` or one of its subtypes. The exceptions raised
    by `error` have a type of `exn:fail`. To trap such errors, one wraps the code
    in a `with-handlers` form. A modified version of `start` that uses `with-handlers`
    is given here.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: The `with-handlers` form can trap any number of different types of error. In
    this case, we use the `exn:fail?` predicate to trap generated `exn:fail` errors
    generated by the `error` form. Each trapped error type has a corresponding function
    to manage the trapped error.
  prefs: []
  type: TYPE_NORMAL
- en: Here we use a lambda expression to generate the somewhat uninformative `"An
    error occurred."` message. Evaluating the expression with the missing right parenthesis
    now produces the following outcome.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Observe that this time, even though an error has occurred, the `>` prompt appears,
    indicating that the program is still running. The user now has an opportunity
    to re-enter the expression and continue working.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we want to provide a more informative error message, like the one provided
    by Racket. The `e` parameter handed to the exception handling function is an `exn`
    structure. This structure has a `message` field that contains the actual text
    string of the raised error. So to print the text of the error message, we need
    to modify the lambda function to read as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'With this change in place, a session with an erroneous entry would proceed
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that evaluating an expression such as `sqrt(-1)` will produce the complex
    number `0+1i`. This may be confusing to users not familiar with complex numbers.
    In this case, it may be preferable to raise an error instead of returning a result.
    To accommodate this, the `start` procedure could be modified as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'With this change in place, evaluating an expression that returns a complex
    number would produce the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '**Complex Numbers**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In the previous section, we mentioned throwing an exception if a calculation
    produces a complex number. If users *are* familiar with complex numbers, the lexer
    could be modified to accept complex numbers, in which case the original `start`
    procedure could be kept in place. It’s not extremely difficult to modify TRAC’s
    lexical analyzer such that it works with complex numbers. One might be tempted
    to create a regular expression that recognizes a complex number such as `1+2i`.
    That would be a big mistake. If one evaluates an expression such as `2*1+2i`,
    the expected result is `2+2i` since multiplication has a higher precedence than
    addition. If the lexer returns the entire expression as a number, the parser will
    treat the expression `2*1+2i` as `2*(1+2i)`, which will give the result `2+4i`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The actual solution is quite simple. Instead of recognizing the entire complex
    number, we only recognize the imaginary part. That is, the regular expression
    for a number becomes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the only change in the expression is the inclusion of `i?` at the
    end, which means we accept an optional `i` at the end of a numeric input.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we make a small modification to `classify` (which is embedded in
    `tokenize`) to handle imaginary numbers.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'With these changes in place, we can compute the following in TRAC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '**Mode, Reset, and Help Commands**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Most calculators allow the user to compute trigonometric functions using either
    degrees or radians. We’d be remiss to omit this capability from TRAC. This will
    require a global variable to contain the trigonometric mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: TRAC currently handles numeric entries exactly as Racket would. That is, if
    an exact value is divided by an exact value, a fraction results. For example entering
    `2/4` would return a result of `1/2`. This is typically not what’s expected for
    run-of-the-mill calculations. We’ll thus modify TRAC to give the user the option
    to treat all entries as floating-point numbers or to retain fractional entries.
    To enable this, we’ll use a global variable to maintain the numeric mode.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: It would also be nice to allow the user to reset TRAC to its default start-up
    state, so TRAC is given a new keyword called `reset`, which requires the following
    change to `regex-keyword`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: The question mark at the end will allow TRAC to have a mini–help system, which
    is accessed by entering `?` on the command line (more on this shortly).
  prefs: []
  type: TYPE_NORMAL
- en: 'Entering `reset` will result in clearing previous entries in the TRAC dictionary
    and priming it with the default values. These actions are bundled up into a `reset`
    procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'The `start` procedure then becomes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'To accommodate the new `reset` and `?` keywords, the `statement` portion of
    the parser is updated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: If `reset` or `?` is entered for input, the function returns immediately without
    drilling down into the parser so that the expression evaluator can handle these
    commands directly.
  prefs: []
  type: TYPE_NORMAL
- en: Of course we still need to modify the trigonometric functions to work properly
    depending on the current mode. The handling of numeric entries will also need
    to be adjusted to ensure that they honor the current numeric mode. Here’s the
    tweaked version of `ast-eval`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'The actual changes to the trigonometric functions are minor: just a multiplication
    or division by `mode` does the trick (observe how `trig-mode` is handled ➊). Code
    is also added to properly convert exact values to inexact when the mode is set
    to `FLOAT` ➋. Most of the remaining changes involve modifying the assignment statement
    to trap changes to `TrigMode` ➎ and `NumMode` ➏ to ensure that they can only be
    assigned proper values. Note the additions for `reset` ➌ and `help` ➍. The `print-help`
    procedure is provided here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Here’s a session illustrating the new functionality.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: Pretty cool, eh?
  prefs: []
  type: TYPE_NORMAL
- en: '***Making Sure TRAC Works Properly***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Given the nature of this application, it would be nice to have some degree of
    comfort that it’s performing the calculations properly. If you were using this
    to calculate the landing trajectory of a spaceship to the moon, it wouldn’t do
    to have it return a calculation that results in the spaceship flying out into
    empty space instead.
  prefs: []
  type: TYPE_NORMAL
- en: Of course one could sit down and manually enter a large number of test equations
    into TRAC and verify the results by entering the same equations on some other
    calculator and seeing if the results are the same. This clearly wouldn’t be much
    fun (or very efficient, for that matter). No, we want an automated process where
    we can have the computer do all the work. The approach we’re going to take is
    to build a procedure that will generate a random Racket expression. This expression
    can be evaluated using the Racket `eval` function to get a numeric value. In addition
    we’ll need a function that converts the Racket expression into a TRAC expression
    string. We can evaluate the TRAC expression to see if it returns the same value.
    We can then have the computer repeat this process thousands of times to make sure
    we don’t produce any mismatches.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s the code for the random Racket expression generator.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: An operator from the `ops` vector is randomly selected by the `gen-racket` function.
    Values in `ops` include both the operator symbol and the number of arguments it’s
    expecting (this is called its *arity*). Notice that both `log` and minus (`-`)
    have two different arities. The function call `log(x)` (base-10 logarithm) in
    TRAC is the same as `(log x 10)` in Racket. Then `gen-racket` will build an expression
    containing from one to five random operations or functions with random floating-point
    numeric arguments. The result is an actual Racket expression instead of an AST,
    where its arguments and functions are populated with random values.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s a look at some of the expressions that `gen-racket` produces.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: Most of the work involves converting the Racket expressions into TRAC expressions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: This is largely an adaptation of the `ast->string` function, but using the randomly
    generated Racket expressions created by `gen-racket` as input instead of the TRAC
    syntax tree. We’ve had to make some accommodations to account for the multiple
    arities of `-` and `log`. We also match against the literal function symbols.
    Aside from these considerations, the code should closely mirror that of `ast->string`.
    Here are a few samples of its output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'The basic idea is to automate the following process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'So here’s our test bench:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: The result of a computation could potentially result in a complex number (for
    example, `(sqrt -1)`), so we use `magnitude` to get the absolute value size of
    the difference between the values.
  prefs: []
  type: TYPE_NORMAL
- en: And here’s the output from an initial test run, which in fact indicated that
    the TRAC evaluation routine wasn’t always producing the correct results.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: The common thread in all the mismatches was the exponentiation operator `^`
    (mapped from Racket’s `expt` function), which was inadvertently defined with the
    division operator in `eval-ast` (the `eval-ast` code given above is correct, but
    you can introduce the same error if you want to test this). Once the correction
    was made, another test run produced the following result.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: In this case, no news is *good* news.
  prefs: []
  type: TYPE_NORMAL
- en: '***Making an Executable***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There’s really no need to have TRAC dependent on the DrRacket environment. Only
    a few additional steps are required to create an executable file that can be launched
    without starting DrRacket. The first step is to simply add the `(start)` command
    to the last line of the definitions file (see below) so that the program starts
    executing immediately when launched.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Racket supports three different types of executables:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Launcher** This type of executable will execute the current version of your
    `.rkt` source file, so it will include the path to the source file in the executable.
    This will allow your executable to immediately reflect any enhancements to your
    program. The downside is that you can’t move the source file elsewhere or easily
    share the executable with someone else.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Standalone** This version embeds the source file in the executable, so there’s
    no problem moving it to another location on your machine. A standalone executable
    still depends on the installed Racket DLLs, so it may not work properly if moved
    to a different machine.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Distribution archive** A distribution archive bundles all needed files into
    an install file. The install file can be used to install TRAC on another machine
    as long as the destination machine uses the same operating system as the one the
    archive was created on.'
  prefs: []
  type: TYPE_NORMAL
- en: Before you create an executable, it’s recommended that debugging be turned off.
    This can be done by going to the **Choose Language . . .** dialog (from the Language
    option on the main menu) and pressing the **Show Details** button. This will open
    a panel where you should select **No Debugging**. Once this is done, go to the
    Racket main menu, and from there select **Create Executable . . .** . In the dialog
    box, you may select which of the three different types of executables you want
    to create. It’s even possible to select a custom icon to give TRAC that personal
    touch.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 10-7](ch10.xhtml#ch10fig7) is a screenshot of TRAC running on our machine.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/10fig07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10-7: TRAC in action*'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter, we leveraged our knowledge of abstract computing machines and
    various automata (introduced in the previous chapter) to build an interactive
    command line expression calculator. Along the way we learned about lexers (and
    using regular expressions to construct them), parsers (which construct abstract
    syntax trees), and interpreters. We used EBNF (extended Backus–Naur form) to specify
    our calculator grammar. Once we had our basic calculator built, we enhanced it
    with additional capabilities, such as handling complex numbers and hand degrees
    or radians. Just to be sure our calculator doesn’t give us bogus numbers, we built
    a simple test bench to make certain our code was robust.
  prefs: []
  type: TYPE_NORMAL
- en: Well, that just about concludes our Racket journey for now. But we’ve only scratched
    the tip of the iceberg. There’s much more capability that we haven’t even hinted
    at. We encourage you to further explore Racket on your own via the Racket website
    and other available literature. Happy learning!
  prefs: []
  type: TYPE_NORMAL
