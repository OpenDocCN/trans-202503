<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><h2 class="h2" id="ch10"><span epub:type="pagebreak" id="page_59"/><strong><span class="big">10</span><br/>SOURCES OF RANDOMNESS</strong></h2>&#13;
<div class="image1"><img src="../images/common.jpg" alt="Image" width="252" height="252"/></div>&#13;
<p class="noindent">What are the common sources of randomness when training deep neural networks that can cause non-reproducible behavior during training and inference?</p>&#13;
<p class="indent">When training or using machine learning models such as deep neural networks, several sources of randomness can lead to different results every time we train or run these models, even though we use the same overall settings. Some of these effects are accidental and some are intended. The following sections categorize and discuss these various sources of randomness.</p>&#13;
<p class="indent">Optional hands-on examples for most of these categories are provided in the <em>supplementary/q10-random-sources</em> subfolder at <em><a href="https://github.com/rasbt/MachineLearning-QandAI-book">https://github.com/rasbt/MachineLearning-QandAI-book</a></em>.</p>&#13;
<h3 class="h3" id="ch00lev45"><strong>Model Weight Initialization</strong></h3>&#13;
<p class="noindent">All common deep neural network frameworks, including TensorFlow and PyTorch, randomly initialize the weights and bias units at each layer by default. This means that the final model will be different every time we start the training. The reason these trained models will differ when we start with different random weights is the nonconvex nature of the loss, as illustrated <span epub:type="pagebreak" id="page_60"/>in <a href="ch10.xhtml#ch10fig1">Figure 10-1</a>. As the figure shows, the loss will converge to different local minima depending on where the initial starting weights are located.</p>&#13;
<div class="image"><img id="ch10fig1" src="../images/10fig01.jpg" alt="Image" width="544" height="506"/></div>&#13;
<p class="figcap"><em>Figure 10-1: Different starting weights can lead to different final weights.</em></p>&#13;
<p class="indent">In practice, it is therefore recommended to run the training (if the computational resources permit) at least a handful of times; unlucky initial weights can sometimes cause the model not to converge or to converge to a local minimum corresponding to poorer predictive accuracy.</p>&#13;
<p class="indent">However, we can make the random weight initialization deterministic by seeding the random generator. For instance, if we set the seed to a specific value like 123, the weights will still initialize with small random values. Nonetheless, the neural network will consistently initialize with the same small random weights, enabling accurate reproduction of results.</p>&#13;
<h3 class="h3" id="ch00lev46"><strong>Dataset Sampling and Shuffling</strong></h3>&#13;
<p class="noindent">When we train and evaluate machine learning models, we usually start by dividing a dataset into training and test sets. This requires random sampling since we have to decide which examples we put into a training set and which examples we put into a test set.</p>&#13;
<p class="indent">In practice, we often use model evaluation techniques such as <em>k</em>-fold cross-validation or holdout validation. In holdout validation, we split the training set into training, validation, and test datasets, which are also sampling procedures influenced by randomness. Similarly, unless we use a fixed random seed, we get a different model each time we partition the dataset or tune or evaluate the model using <em>k</em>-fold cross-validation since the training partitions will differ.<span epub:type="pagebreak" id="page_61"/></p>&#13;
<h3 class="h3" id="ch00lev47"><strong>Nondeterministic Algorithms</strong></h3>&#13;
<p class="noindent">We may include random components and algorithms depending on the architecture and hyperparameter choices. A popular example of this is <em>dropout</em>.</p>&#13;
<p class="indent">Dropout works by randomly setting a fraction of a layer’s units to zero during training, which helps the model learn more robust and generalized representations. This “dropping out” is typically applied at each training iteration with a probability <em>p</em>, a hyperparameter that controls the fraction of units dropped out. Typical values for <em>p</em> are in the range of 0.2 to 0.8.</p>&#13;
<p class="indent">To illustrate this concept, <a href="ch10.xhtml#ch10fig2">Figure 10-2</a> shows a small neural network where dropout randomly drops a subset of the hidden layer nodes in each forward pass during training.</p>&#13;
<div class="image"><img id="ch10fig2" src="../images/10fig02.jpg" alt="Image" width="804" height="333"/></div>&#13;
<p class="figcap"><em>Figure 10-2: In dropout, hidden nodes are intermittently and randomly disabled during each forward pass in training.</em></p>&#13;
<p class="indent">To create reproducible training runs, we must seed the random generator before training with dropout (analogous to seeding the random generator before initializing the model weights). During inference, we need to disable dropout to guarantee deterministic results. Each deep learning framework has a specific setting for that purpose—a PyTorch example is included in the <em>supplementary/q10-random-sources</em> subfolder at <em><a href="https://github.com/rasbt/MachineLearning-QandAI-book">https://github.com/rasbt/MachineLearning-QandAI-book</a></em>.</p>&#13;
<h3 class="h3" id="ch00lev48"><strong>Different Runtime Algorithms</strong></h3>&#13;
<p class="noindent">The most intuitive or simplest implementation of an algorithm or method is not always the best one to use in practice. For example, when training deep neural networks, we often use efficient alternatives and approximations to gain speed and resource advantages during training and inference.</p>&#13;
<p class="indent">A popular example is the convolution operation used in convolutional neural networks. There are several possible ways to implement the convolution operation:</p>&#13;
<p class="noindentin"><strong>The classic direct convolution</strong> The common implementation of discrete convolution via an element-wise product between the input and the window, followed by summing the result to get a single number. (See <a href="ch12.xhtml">Chapter 12</a> for a discussion of the convolution operation.)</p>&#13;
<p class="noindentin"><span epub:type="pagebreak" id="page_62"/><strong>FFT-based convolution</strong> Uses fast Fourier transform (FFT) to convert the convolution into an element-wise multiplication in the frequency domain.</p>&#13;
<p class="noindentin"><strong>Winograd-based convolution</strong> An efficient algorithm for small filter sizes (like 3<em>×</em>3) that reduces the number of multiplications required for the convolution.</p>&#13;
<p class="indentt">Different convolution algorithms have different trade-offs in terms of memory usage, computational complexity, and speed. By default, libraries such as the CUDA Deep Neural Network library (cuDNN), which are used in PyTorch and TensorFlow, can choose different algorithms for performing convolution operations when running deep neural networks on GPUs. However, the deterministic algorithm choice has to be explicitly enabled. In PyTorch, for example, this can be done by setting</p>&#13;
<pre class="pre">torch.use_deterministic_algorithms(True)</pre>&#13;
<p class="indent">While these approximations yield similar results, subtle numerical differences can accumulate during training and cause the training to converge to slightly different local minima.</p>&#13;
<h3 class="h3" id="ch00lev49"><strong>Hardware and Drivers</strong></h3>&#13;
<p class="noindent">Training deep neural networks on different hardware can also produce different results due to small numeric differences, even when the same algorithms are used and the same operations are executed. These differences may sometimes be due to different numeric precision for floating-point operations. However, small numeric differences may also arise due to hardware and software optimization, even at the same precision.</p>&#13;
<p class="indent">For instance, different hardware platforms may have specialized optimizations or libraries that can slightly alter the behavior of deep learning algorithms. To give one example of how different GPUs can produce different modeling results, the following is a quotation from the official NVIDIA documentation: “Across different architectures, no cuDNN routines guarantee bit-wise reproducibility. For example, there is no guarantee of bit-wise reproducibility when comparing the same routine run on NVIDIA Volta<sup>TM</sup> and NVIDIA Turing<sup>TM</sup> [. . .] and NVIDIA Ampere architecture.”</p>&#13;
<h3 class="h3" id="ch00lev50"><strong>Randomness and Generative AI</strong></h3>&#13;
<p class="noindent">Besides the various sources of randomness mentioned earlier, certain models may also exhibit random behavior during inference that we can think of as “randomness by design.” For instance, generative image and language models may create different results for identical prompts to produce a diverse sample of results. For image models, this is often so that users can <span epub:type="pagebreak" id="page_63"/>select the most accurate and aesthetically pleasing image. For language models, this is often to vary the responses, for example, in chat agents, to avoid repetition.</p>&#13;
<p class="indent">The intended randomness in generative image models during inference is often due to sampling different noise values at each step of the reverse process. In diffusion models, a noise schedule defines the noise variance added at each step of the diffusion process.</p>&#13;
<p class="indent">Autoregressive LLMs like GPT tend to create different outputs for the same input prompt (GPT will be discussed at greater length in <a href="ch14.xhtml">Chapters 14</a> and <a href="ch17.xhtml">17</a>). The ChatGPT user interface even has a Regenerate Response button for that purpose. The ability to generate different results is due to the sampling strategies these models employ. Techniques such as top-<em>k</em> sampling, nucleus sampling, and temperature scaling influence the model’s output by controlling the degree of randomness. This is a feature, not a bug, since it allows for diverse responses and prevents the model from producing overly deterministic or repetitive outputs. (See <a href="ch09.xhtml">Chapter 9</a> for a more in-depth overview of generative AI and deep learning models; see <a href="ch17.xhtml">Chapter 17</a> for more detail on autoregressive LLMs.)</p>&#13;
<p class="indent"><em>Top-</em>k <em>sampling</em>, illustrated in <a href="ch10.xhtml#ch10fig3">Figure 10-3</a>, works by sampling tokens from the top <em>k</em> most probable candidates at each step of the next-word generation process.</p>&#13;
<div class="image"><img id="ch10fig3" src="../images/10fig03.jpg" alt="Image" width="647" height="364"/></div>&#13;
<p class="figcap"><em>Figure 10-3: Top-</em>k <em>sampling</em></p>&#13;
<p class="indent">Given an input prompt, the language model produces a probability distribution over the entire vocabulary (the candidate words) for the next token. Each token in the vocabulary is assigned a probability based on the model’s understanding of the context. The selected top-<em>k</em> tokens are then renormalized so that the probabilities sum to 1. Finally, a token is sampled from the renormalized top-<em>k</em> probability distribution and is appended to the input prompt. This process is repeated for the desired length of the generated text or until a stop condition is met.</p>&#13;
<p class="indent"><em>Nucleus sampling</em> (also known as <em>top-</em>p <em>sampling</em>), illustrated in <a href="ch10.xhtml#ch10fig4">Figure 10-4</a>, is an alternative to top-<em>k</em> sampling.<span epub:type="pagebreak" id="page_64"/></p>&#13;
<div class="image"><img id="ch10fig4" src="../images/10fig04.jpg" alt="Image" width="806" height="390"/></div>&#13;
<p class="figcap"><em>Figure 10-4: Nucleus sampling</em></p>&#13;
<p class="indent">Similar to top-<em>k</em> sampling, the goal of nucleus sampling is to balance diversity and coherence in the output. However, nucleus and top-<em>k</em> sampling differ in how to select the candidate tokens for sampling at each step of the generation process. Top-<em>k</em> sampling selects the <em>k</em> most probable tokens from the probability distribution produced by the language model, regardless of their probabilities. The value of <em>k</em> remains fixed throughout the generation process. Nucleus sampling, on the other hand, selects tokens based on a probability threshold <em>p</em>, as shown in <a href="ch10.xhtml#ch10fig4">Figure 10-4</a>. It then accumulates the most probable tokens in descending order until their cumulative probability meets or exceeds the threshold <em>p</em>. In contrast to top-<em>k</em> sampling, the size of the candidate set (nucleus) can vary at each step.</p>&#13;
<h3 class="h3" id="ch00lev51"><strong>Exercises</strong></h3>&#13;
<p class="number1"><strong>10-1.</strong> Suppose we train a neural network with top-<em>k</em> or nucleus sampling where <em>k</em> and <em>p</em> are hyperparameter choices. Can we make the model behave deterministically during inference without changing the code?</p>&#13;
<p class="number1"><strong>10-2.</strong> In what scenarios might random dropout behavior during inference be desired?<span epub:type="pagebreak" id="page_65"/></p>&#13;
<h3 class="h3" id="ch00lev52"><strong>References</strong></h3>&#13;
<ul>&#13;
<li class="noindent">For more about different data sampling and model evaluation techniques, see my article: “Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning” (2018), <em><a href="https://arxiv.org/abs/1811.12808">https://arxiv.org/abs/1811.12808</a></em>.</li>&#13;
<li class="noindent">The paper that originally proposed the dropout technique: Nitish Srivastava et al., “Dropout: A Simple Way to Prevent Neural Networks from Overfitting” (2014), <em><a href="https://jmlr.org/papers/v15/srivastava14a.html">https://jmlr.org/papers/v15/srivastava14a.html</a></em>.</li>&#13;
<li class="noindent">A detailed paper on FFT-based convolution: Lu Chi, Borui Jiang, and Yadong Mu, “Fast Fourier Convolution” (2020), <em><a href="https://dl.acm.org/doi/abs/10.5555/3495724.3496100">https://dl.acm.org/doi/abs/10.5555/3495724.3496100</a></em>.</li>&#13;
<li class="noindent">Details on Winograd-based convolution: Syed Asad Alam et al., “Winograd Convolution for Deep Neural Networks: Efficient Point Selection” (2022), <em><a href="https://arxiv.org/abs/2201.10369">https://arxiv.org/abs/2201.10369</a></em>.</li>&#13;
<li class="noindent">More information about the deterministic algorithm settings in Py-Torch: <em><a href="https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html">https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html</a></em>.</li>&#13;
<li class="noindent">For details on the deterministic behavior of NVIDIA graphics cards, see the “Reproducibility” section of the official NVIDIA documentation: <em><a href="https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#reproducibility">https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#reproducibility</a></em>.<span epub:type="pagebreak" id="page_66"/></li>&#13;
</ul>&#13;
</div>
</div>
</body></html>