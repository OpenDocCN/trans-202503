- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Homotopy Algorithms
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/book_art/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: 'In this chapter, we’ll explore algorithms related to homotopy, a way to classify
    topological objects based on path types around the object, including homotopy-based
    calculations of regression parameters. Local minima and maxima often plague datasets:
    they provide suboptimal stopping points for algorithms that explore solution spaces
    locally. In the next few pages, we’ll see how homotopy solves this problem.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Homotopy
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Two paths or functions are *homotopic* to each other if they can be continuously
    deformed into each other within the space of interest. Imagine a golf course and
    a pair of golfers, one who is a better putter than the other. The ball can travel
    to the hole along many different paths. Imagine tracing out the path of each shot
    on the green with rope. One path might be rather direct. The other might meander
    quite a bit before finding the hole, particularly if the green is hilly. A bad
    golfer may have to make many shots, resulting in a long, jagged path. But no matter
    how many hills exist or how many strokes it takes for the golfer’s ball to make
    it into the hole, we could shorten each of these paths by deforming the rope,
    as depicted in [Figure 8-1](#figure8-1).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c08/f08001_m.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8-1: A long path to the hole of a golf course (left) deformed to a shorter
    path to the hole (right)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Let’s stretch the analogy somewhat and imagine a sinkhole has appeared in the
    golf course. Topological objects and spaces with holes can complicate this deformation
    process and lead to many different possible paths from one point to another. Paths
    can connect two points on an object. Depending on the object’s properties, these
    paths can sometimes “wiggle” enough to overlap with another path without having
    to cut the path into pieces to get around an obstacle (usually a hole). Winding
    paths around holes presents a problem to continuous deformation of one path into
    another. It’s not possible for the path to wind or wiggle around a hole, such
    that a path between points will necessarily overlap with another path. Different
    types of paths begin to emerge as holes and paths around holes are added. One
    path might make only one loop around a hole before connecting two points. Another
    might make several loops around a hole before connecting two points. Imagine golfing
    again. Let’s say that the green has an obstacle (such as a rock or a water hazard)
    in the middle of it, creating a torus with tricky hills around it that can force
    a bad shot to require circumnavigating the rock to get back to the hole, as you
    can see in [Figure 8-2](#figure8-2).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c08/f08002_m.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8-2: Two paths with the same start and end points on a torus course
    (donut) that cannot be morphed into each other without being cut or having the
    inner hole removed'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: In this scenario, we can no longer deform paths into each other without cutting
    the line or removing the hole. As more holes or holes of larger dimension are
    added, more classes of equivalent paths begin to emerge, with equivalent paths
    having the same number of loops around one or more holes. A two-dimensional course
    will have fewer possible paths from the tee to the hole than a three-dimensional
    course, as fewer possible types of obstacles and holes in the course exist. A
    space with many holes or obstacles in many different dimensions presents a lot
    of obstacles that paths can wind around. This means many unique paths exist for
    that space.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Given that datasets often contain holes of varying dimension, many different
    classes of paths may exist in the data. Random walks on the data, common in Bayesian
    analyses and robotic navigation path-finding tasks, may not be equivalent. This
    can be an advantage in navigation problems, allowing the system to choose from
    a set of different paths with different cost weights related to length, resource
    allocation, and ease of movement. For instance, in the path-finding problem in
    [Figure 8-3](#figure8-3), perhaps obstacle 2 has sharp ends that could harm the
    system should it get too close, making the leftmost path the ideal one for the
    system to take.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c08/f08003.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8-3: An example obstacle course with navigation from a start point to
    a finish point with several possible solutions'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 8-3](#figure8-3) shows three paths, and none of them can be deformed
    into another of the paths without moving an obstacle or cutting the path. These
    are unique paths in the space. By counting the total number of unique paths, we
    can classify the space topologically.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Homotopy-Based Regression
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned, datasets often contain obstacles in the form of local optima,
    that is, local maximums and minimums. Gradient descent algorithms and other stepwise
    optimization algorithms can get stuck there. You can think of this as the higher-dimensional
    version of hills and valleys (saddle points, which are higher-dimensional inflection
    points, can also pose optimization issues). Getting stuck in a local optimum provides
    less-than-ideal solutions to an optimization problem.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Homotopy-based algorithms can help with the estimation of parameters in high-dimensional
    data containing many local optima, under which conditions many common algorithms
    such as gradient descent can struggle. Finding a solution in a space with fewer
    local optima and then continuously deforming that solution to the original space
    can lead to better accuracy of estimates and variables selected in a model.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: To provide more insight, consider a blindfolded person trying to navigate through
    an industrial complex ([Figure 8-4](#figure8-4)). Without a tether, they are sure
    to bump into obstacles and potentially think they have hit their target when they
    are stopped by one of the larger obstacles.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c08/f08004.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8-4: A blindfolded person navigating an obstacle course'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-4：一个蒙眼的人在障碍课程中导航
- en: However, if they are given a rope connecting their starting point to their ending
    point, they can navigate between the points a bit better and know that any obstacle
    they encounter is likely not the true ending point. There are many possible ways
    to connect the start and finish points. [Figure 8-5](#figure8-5) shows one possible
    rope configuration.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果他们有一根绳子将起点和终点连接起来，他们就能更好地在这些点之间导航，并且知道他们遇到的任何障碍物可能都不是最终的终点。连接起点和终点的方式有很多种。[图
    8-5](#figure8-5)展示了其中一种可能的绳子配置。
- en: '![](image_fi/503083c08/f08005.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c08/f08005.png)'
- en: 'Figure 8-5: A blindfolded person navigating an obstacle course with a guide
    rope'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-5：一个蒙眼的人在带有引导绳的障碍课程中导航
- en: A blindfolded person struggling to avoid physical obstacles is analogous to
    a machine learning algorithm avoiding local optima. For example, let’s consider
    a function of two variables with a global maximum and minimum but other local
    optima, as derived in [Listing 8-1](#listing8-1).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一个蒙眼的人努力避免物理障碍物，可以类比为一个机器学习算法避免局部最优解。例如，我们可以考虑一个具有全局最大值和最小值，但存在其他局部最优解的二元函数，如[示例
    8-1](#listing8-1)中所推导的。
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Listing 8-1: A script that creates a function of two variables with a global
    minimum and maximum but many other local optima'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 8-1：一个创建二元函数的脚本，该函数具有全局最小值和最大值，但有许多其他局部最优解
- en: The code in [Listing 8-1](#listing8-1) produces the plot in [Figure 8-6](#figure8-6),
    from which we can see many minima and maxima. The other optima are local optima,
    some of which are very close to the global minimum or maximum.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 8-1](#listing8-1)中的代码生成了[图 8-6](#figure8-6)中的图表，从中我们可以看到许多极小值和极大值。其他的最优解是局部最优解，其中一些非常接近全局最小值或最大值。'
- en: '![](image_fi/503083c08/f08006.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c08/f08006.png)'
- en: 'Figure 8-6: A scatterplot of three-dimensional data, namely, a function with
    many local optima'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-6：三维数据的散点图，即一个具有多个局部最优解的函数
- en: An algorithm trying to optimize this function will likely get stuck in one of
    the local optima, as the values near the local optima are increasing or decreasing
    from that optimum’s value. Some algorithms that have been known to struggle with
    this type of optimization include gradient descent and the expectation-maximization
    (EM) algorithm, among others. Optimization strategies such as evolutionary algorithms
    will also likely take a long time to find global solutions, making them less ideal
    for this type of data.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一个试图优化这个函数的算法很可能会陷入其中一个局部最优解，因为局部最优解附近的值会从该最优解的值上升或下降。一些已知在这种优化类型中表现不佳的算法包括梯度下降法和期望最大化（EM）算法等。像进化算法这样的优化策略也很可能需要很长时间才能找到全局解，这使得它们对于这种数据来说不太理想。
- en: Homotopy-based calculations provide an effective solution to this problem of
    local optima traps; algorithms employing homotopy-based calculations can wiggle
    around or out of local optima. In essence, these algorithms start with an easy
    optimization problem, in which no local optima are present, and deform the solution
    slowly according to the dataset and its geometry, avoiding local optima as the
    deformation proceeds.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 基于同伦的计算为这个局部最优解陷阱问题提供了有效的解决方案；使用基于同伦的计算的算法可以在局部最优解之间摆动，或者脱离局部最优解。实质上，这些算法从一个简单的优化问题开始，在这个问题中没有局部最优解，并根据数据集及其几何形状缓慢变形解，随着变形的进行，避免局部最优解。
- en: 'Homotopy-based optimization methods commonly used these days in machine learning
    include support vector machines, Lasso, and even neural networks. The lasso2 package
    of R is one package that implements homotopy-based models; in this case, lasso2
    implements a homotopy-based model for the Lasso algorithm. Let’s first explore
    model fit and solutions for the data generated in [Listing 8-1](#listing8-1),
    in which the outcome has many local optima and the predictors are collinear, a
    problem for many machine learning algorithms. Add the following to the code in
    [Listing 8-1](#listing8-1):'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 目前在机器学习中常用的基于同伦的优化方法包括支持向量机、Lasso，甚至神经网络。R的lasso2包是一个实现基于同伦模型的包；在这个案例中，lasso2实现了一个基于同伦的Lasso算法模型。让我们首先探讨在[示例
    8-1](#listing8-1)中生成的数据的模型拟合和解，其中结果有多个局部最优解，并且预测变量是共线性的，这对于许多机器学习算法来说是一个问题。将以下代码添加到[示例
    8-1](#listing8-1)中的代码：
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, the model is ready to be built and tested. The outcome of interest (our
    variable `z`) is not normally distributed, but a Gaussian distribution is the
    closest available distribution for use in the model. In the following addition
    to the script in [Listing 8-1](#listing8-1), the `etastart` parameter needs to
    be set to null before starting the model iterations, and a bound needs to be in
    place to guide the homotopy-based parameter search. Generally, a lower setting
    is best:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This script now fits a homotopy-based Lasso model to the training data and
    then predicts test data outcomes based on this model, allowing us to assess the
    model fit. The mean square error for this sample, calculated in the final line,
    should be near 2.30\. (Again, results may vary with R versions, as the seeding
    and sampling algorithms changed.) The results of the model suggest that one term
    dominates the behavior of the function:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: These results, which may vary for readers with different versions of R, show
    that only one variable is selected as important to the model. `x` contributes
    more to the prediction of `z` than `y` contributes, according to our model. Linear
    regression isn’t a great tool to use on this problem, given the nonlinear relationships
    between `x` or `y` and `z`, but it does find some consistency in the relationship.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: 'To compare with another method, let’s create a linear regression model and
    add it to [Listing 8-1](#listing8-1):'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This code trains a linear model on the training data and predicts test set
    outcomes, similar to how the homotopy-based model was fit with the previous code.
    You may get a warning with your regression model, as there is covarying behavior
    of `x` and `y` (which presents issues to linear regression models per the assumption
    of noncollinearity). Let’s take a look at this model’s results:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The mean square error (MSE) for this sample should be near 2.30, which is the
    same as the homotopy-based model. MSE accounts for both variance and bias in the
    estimator, giving a balanced view of how well the algorithm is performing on a
    regression task. However, the collinearity is problematic for the linear regression
    model. Penalized models avoid this issue, including homotopy-based Lasso models.
    Of note, the coefficients found by the linear regression and the homotopy-based
    Lasso model are identical. Typically, models with different optimization strategies
    will vary a bit on their estimates. In this case, the sample size is probably
    large and the number of predictors few enough for both algorithms to converge
    to a global optimum.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Comparing Results on a Sample Dataset
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s return to our self-reported educational dataset and explore the relationships
    between school experiences, IQ, and self-reported depression. Because we don’t
    know what the function between these predictors and depression should be, we don’t
    know what sort of local optima might exist. However, we do know that a training
    dataset with 7 predictors and 16 individuals (70 percent of the data) will be
    sparse, and it’s possible that local optima are a problem in the dataset. There
    is evidence that geometry-based linear regression models work better on sparse
    datasets than other algorithms, and it’s possible that our homotopy-based Lasso
    model will work better, as well.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Let’s create [Listing 8-2](#listing8-2) and partition the data into training
    and test sets.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Listing 8-2: A script that loads and then analyzes the Quora IQ sample'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s run a homotopy-based Lasso model and a logistic regression model
    to compare results on this small, real-world dataset:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: From running the models in the previous script addition, we should see that
    the homotopy-based Lasso model has a higher accuracy (~85 percent) than the logistic
    regression (~70 percent); additionally, the logistic regression model spits out
    a warning message about fitted probabilities of 0 or 1 occurring. This means the
    data is quite separated into groups, which can happen when small data with strong
    relationships to an outcome is split. Depending on your version of R or GUI, you
    may end up with a different sample and, thus, somewhat different fit statistics
    and results. Because this is a relatively small sample to begin with, it’s possible
    that you’ll have slightly different results than the ones presented here. Some
    samples may not have any instances of a given predictor within the dataset. Larger
    samples would create more stable models across samples.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look more closely at the homotopy-based Lasso model and its coefficients:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: From the previous output, we can see that, for this sample, higher IQ, endorsement
    of boredom, and being put in a remedial class increase the likelihood of self-reported
    depression. However, outside learning has a strong protective effect. In fact,
    outside learning can completely counterbalance the risk from boredom and being
    placed in a remedial course. This suggests that parents of profoundly gifted children
    who are experiencing school issues may be able to mitigate some of the potential
    adverse outcomes, such as depression, by providing outside learning opportunities,
    such as college courses in the evening, tutoring outside of school, or other opportunities
    for the child to learn. The role of outside learning opportunities has been explored
    to some extent in the giftedness literature with similar results, but more research
    is needed on this topic.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s compare these results with the results of the logistic regression
    model:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Examining the previous output, it seems that the logistic regression model could
    not handle the dataset, giving errors and spitting out very large coefficients.
    This is likely related to the smallness of the data, where the linear system is
    underdetermined; however, this is not a situation where the number of predictors
    outnumber the number of individuals in the sample, so it is likely a function
    of the data itself rather than purely sample size.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Note the model fails to find any significant predictors of self-reported depression.
    Linear regression can’t handle this dataset very well, and the results are not
    reliable. For some samples, certain variables may not be computable in the linear
    regression model at all. Homotopy-based models (and other types of penalized models)
    often work better on small datasets, and there is some evidence that they perform
    better for datasets with many local optima. While this dataset is a bit small
    for fitting a model, it does demonstrate the power of homotopy-based optimization
    (and penalized regression, in general) on very small datasets, and its results
    make a lot more sense than the linear regression model’s results.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we gave you an overview of homotopy and its applications in
    machine learning, including through an example of homotopy as an extension of
    regression-based algorithms on a simulated problem and a real dataset. Homotopy
    can help regression algorithms avoid local optima that often trap local optimizers.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Other uses of homotopy algorithms in the field of artificial intelligence include
    navigational problems. For instance, an autonomous cart may need to navigate the
    halls and obstacles of a hospital by weighting different possible paths from its
    current location to its destination. Homotopy algorithms are often used to generate
    the possible paths, which are then weighted by time cost or hazard cost of the
    route. Bounds can also be placed to avoid generating paths that obviously aren’t
    viable (such as going through areas where the cart can’t physically go or wouldn’t
    be wanted—such as an operating room). It’s likely that this branch of topological
    data analysis will grow in the coming years, and we encourage you to explore other
    uses of homotopy in machine learning, robotics, differential equations, and engineering.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
