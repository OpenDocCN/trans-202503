- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Homotopy Algorithms
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/book_art/chapterart.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this chapter, we’ll explore algorithms related to homotopy, a way to classify
    topological objects based on path types around the object, including homotopy-based
    calculations of regression parameters. Local minima and maxima often plague datasets:
    they provide suboptimal stopping points for algorithms that explore solution spaces
    locally. In the next few pages, we’ll see how homotopy solves this problem.'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Homotopy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Two paths or functions are *homotopic* to each other if they can be continuously
    deformed into each other within the space of interest. Imagine a golf course and
    a pair of golfers, one who is a better putter than the other. The ball can travel
    to the hole along many different paths. Imagine tracing out the path of each shot
    on the green with rope. One path might be rather direct. The other might meander
    quite a bit before finding the hole, particularly if the green is hilly. A bad
    golfer may have to make many shots, resulting in a long, jagged path. But no matter
    how many hills exist or how many strokes it takes for the golfer’s ball to make
    it into the hole, we could shorten each of these paths by deforming the rope,
    as depicted in [Figure 8-1](#figure8-1).
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c08/f08001_m.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8-1: A long path to the hole of a golf course (left) deformed to a shorter
    path to the hole (right)'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s stretch the analogy somewhat and imagine a sinkhole has appeared in the
    golf course. Topological objects and spaces with holes can complicate this deformation
    process and lead to many different possible paths from one point to another. Paths
    can connect two points on an object. Depending on the object’s properties, these
    paths can sometimes “wiggle” enough to overlap with another path without having
    to cut the path into pieces to get around an obstacle (usually a hole). Winding
    paths around holes presents a problem to continuous deformation of one path into
    another. It’s not possible for the path to wind or wiggle around a hole, such
    that a path between points will necessarily overlap with another path. Different
    types of paths begin to emerge as holes and paths around holes are added. One
    path might make only one loop around a hole before connecting two points. Another
    might make several loops around a hole before connecting two points. Imagine golfing
    again. Let’s say that the green has an obstacle (such as a rock or a water hazard)
    in the middle of it, creating a torus with tricky hills around it that can force
    a bad shot to require circumnavigating the rock to get back to the hole, as you
    can see in [Figure 8-2](#figure8-2).
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c08/f08002_m.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8-2: Two paths with the same start and end points on a torus course
    (donut) that cannot be morphed into each other without being cut or having the
    inner hole removed'
  prefs: []
  type: TYPE_NORMAL
- en: In this scenario, we can no longer deform paths into each other without cutting
    the line or removing the hole. As more holes or holes of larger dimension are
    added, more classes of equivalent paths begin to emerge, with equivalent paths
    having the same number of loops around one or more holes. A two-dimensional course
    will have fewer possible paths from the tee to the hole than a three-dimensional
    course, as fewer possible types of obstacles and holes in the course exist. A
    space with many holes or obstacles in many different dimensions presents a lot
    of obstacles that paths can wind around. This means many unique paths exist for
    that space.
  prefs: []
  type: TYPE_NORMAL
- en: Given that datasets often contain holes of varying dimension, many different
    classes of paths may exist in the data. Random walks on the data, common in Bayesian
    analyses and robotic navigation path-finding tasks, may not be equivalent. This
    can be an advantage in navigation problems, allowing the system to choose from
    a set of different paths with different cost weights related to length, resource
    allocation, and ease of movement. For instance, in the path-finding problem in
    [Figure 8-3](#figure8-3), perhaps obstacle 2 has sharp ends that could harm the
    system should it get too close, making the leftmost path the ideal one for the
    system to take.
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c08/f08003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8-3: An example obstacle course with navigation from a start point to
    a finish point with several possible solutions'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 8-3](#figure8-3) shows three paths, and none of them can be deformed
    into another of the paths without moving an obstacle or cutting the path. These
    are unique paths in the space. By counting the total number of unique paths, we
    can classify the space topologically.'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Homotopy-Based Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned, datasets often contain obstacles in the form of local optima,
    that is, local maximums and minimums. Gradient descent algorithms and other stepwise
    optimization algorithms can get stuck there. You can think of this as the higher-dimensional
    version of hills and valleys (saddle points, which are higher-dimensional inflection
    points, can also pose optimization issues). Getting stuck in a local optimum provides
    less-than-ideal solutions to an optimization problem.
  prefs: []
  type: TYPE_NORMAL
- en: Homotopy-based algorithms can help with the estimation of parameters in high-dimensional
    data containing many local optima, under which conditions many common algorithms
    such as gradient descent can struggle. Finding a solution in a space with fewer
    local optima and then continuously deforming that solution to the original space
    can lead to better accuracy of estimates and variables selected in a model.
  prefs: []
  type: TYPE_NORMAL
- en: To provide more insight, consider a blindfolded person trying to navigate through
    an industrial complex ([Figure 8-4](#figure8-4)). Without a tether, they are sure
    to bump into obstacles and potentially think they have hit their target when they
    are stopped by one of the larger obstacles.
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c08/f08004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8-4: A blindfolded person navigating an obstacle course'
  prefs: []
  type: TYPE_NORMAL
- en: However, if they are given a rope connecting their starting point to their ending
    point, they can navigate between the points a bit better and know that any obstacle
    they encounter is likely not the true ending point. There are many possible ways
    to connect the start and finish points. [Figure 8-5](#figure8-5) shows one possible
    rope configuration.
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c08/f08005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8-5: A blindfolded person navigating an obstacle course with a guide
    rope'
  prefs: []
  type: TYPE_NORMAL
- en: A blindfolded person struggling to avoid physical obstacles is analogous to
    a machine learning algorithm avoiding local optima. For example, let’s consider
    a function of two variables with a global maximum and minimum but other local
    optima, as derived in [Listing 8-1](#listing8-1).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 8-1: A script that creates a function of two variables with a global
    minimum and maximum but many other local optima'
  prefs: []
  type: TYPE_NORMAL
- en: The code in [Listing 8-1](#listing8-1) produces the plot in [Figure 8-6](#figure8-6),
    from which we can see many minima and maxima. The other optima are local optima,
    some of which are very close to the global minimum or maximum.
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c08/f08006.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8-6: A scatterplot of three-dimensional data, namely, a function with
    many local optima'
  prefs: []
  type: TYPE_NORMAL
- en: An algorithm trying to optimize this function will likely get stuck in one of
    the local optima, as the values near the local optima are increasing or decreasing
    from that optimum’s value. Some algorithms that have been known to struggle with
    this type of optimization include gradient descent and the expectation-maximization
    (EM) algorithm, among others. Optimization strategies such as evolutionary algorithms
    will also likely take a long time to find global solutions, making them less ideal
    for this type of data.
  prefs: []
  type: TYPE_NORMAL
- en: Homotopy-based calculations provide an effective solution to this problem of
    local optima traps; algorithms employing homotopy-based calculations can wiggle
    around or out of local optima. In essence, these algorithms start with an easy
    optimization problem, in which no local optima are present, and deform the solution
    slowly according to the dataset and its geometry, avoiding local optima as the
    deformation proceeds.
  prefs: []
  type: TYPE_NORMAL
- en: 'Homotopy-based optimization methods commonly used these days in machine learning
    include support vector machines, Lasso, and even neural networks. The lasso2 package
    of R is one package that implements homotopy-based models; in this case, lasso2
    implements a homotopy-based model for the Lasso algorithm. Let’s first explore
    model fit and solutions for the data generated in [Listing 8-1](#listing8-1),
    in which the outcome has many local optima and the predictors are collinear, a
    problem for many machine learning algorithms. Add the following to the code in
    [Listing 8-1](#listing8-1):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the model is ready to be built and tested. The outcome of interest (our
    variable `z`) is not normally distributed, but a Gaussian distribution is the
    closest available distribution for use in the model. In the following addition
    to the script in [Listing 8-1](#listing8-1), the `etastart` parameter needs to
    be set to null before starting the model iterations, and a bound needs to be in
    place to guide the homotopy-based parameter search. Generally, a lower setting
    is best:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This script now fits a homotopy-based Lasso model to the training data and
    then predicts test data outcomes based on this model, allowing us to assess the
    model fit. The mean square error for this sample, calculated in the final line,
    should be near 2.30\. (Again, results may vary with R versions, as the seeding
    and sampling algorithms changed.) The results of the model suggest that one term
    dominates the behavior of the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: These results, which may vary for readers with different versions of R, show
    that only one variable is selected as important to the model. `x` contributes
    more to the prediction of `z` than `y` contributes, according to our model. Linear
    regression isn’t a great tool to use on this problem, given the nonlinear relationships
    between `x` or `y` and `z`, but it does find some consistency in the relationship.
  prefs: []
  type: TYPE_NORMAL
- en: 'To compare with another method, let’s create a linear regression model and
    add it to [Listing 8-1](#listing8-1):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This code trains a linear model on the training data and predicts test set
    outcomes, similar to how the homotopy-based model was fit with the previous code.
    You may get a warning with your regression model, as there is covarying behavior
    of `x` and `y` (which presents issues to linear regression models per the assumption
    of noncollinearity). Let’s take a look at this model’s results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The mean square error (MSE) for this sample should be near 2.30, which is the
    same as the homotopy-based model. MSE accounts for both variance and bias in the
    estimator, giving a balanced view of how well the algorithm is performing on a
    regression task. However, the collinearity is problematic for the linear regression
    model. Penalized models avoid this issue, including homotopy-based Lasso models.
    Of note, the coefficients found by the linear regression and the homotopy-based
    Lasso model are identical. Typically, models with different optimization strategies
    will vary a bit on their estimates. In this case, the sample size is probably
    large and the number of predictors few enough for both algorithms to converge
    to a global optimum.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing Results on a Sample Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s return to our self-reported educational dataset and explore the relationships
    between school experiences, IQ, and self-reported depression. Because we don’t
    know what the function between these predictors and depression should be, we don’t
    know what sort of local optima might exist. However, we do know that a training
    dataset with 7 predictors and 16 individuals (70 percent of the data) will be
    sparse, and it’s possible that local optima are a problem in the dataset. There
    is evidence that geometry-based linear regression models work better on sparse
    datasets than other algorithms, and it’s possible that our homotopy-based Lasso
    model will work better, as well.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s create [Listing 8-2](#listing8-2) and partition the data into training
    and test sets.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 8-2: A script that loads and then analyzes the Quora IQ sample'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s run a homotopy-based Lasso model and a logistic regression model
    to compare results on this small, real-world dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: From running the models in the previous script addition, we should see that
    the homotopy-based Lasso model has a higher accuracy (~85 percent) than the logistic
    regression (~70 percent); additionally, the logistic regression model spits out
    a warning message about fitted probabilities of 0 or 1 occurring. This means the
    data is quite separated into groups, which can happen when small data with strong
    relationships to an outcome is split. Depending on your version of R or GUI, you
    may end up with a different sample and, thus, somewhat different fit statistics
    and results. Because this is a relatively small sample to begin with, it’s possible
    that you’ll have slightly different results than the ones presented here. Some
    samples may not have any instances of a given predictor within the dataset. Larger
    samples would create more stable models across samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look more closely at the homotopy-based Lasso model and its coefficients:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: From the previous output, we can see that, for this sample, higher IQ, endorsement
    of boredom, and being put in a remedial class increase the likelihood of self-reported
    depression. However, outside learning has a strong protective effect. In fact,
    outside learning can completely counterbalance the risk from boredom and being
    placed in a remedial course. This suggests that parents of profoundly gifted children
    who are experiencing school issues may be able to mitigate some of the potential
    adverse outcomes, such as depression, by providing outside learning opportunities,
    such as college courses in the evening, tutoring outside of school, or other opportunities
    for the child to learn. The role of outside learning opportunities has been explored
    to some extent in the giftedness literature with similar results, but more research
    is needed on this topic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s compare these results with the results of the logistic regression
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Examining the previous output, it seems that the logistic regression model could
    not handle the dataset, giving errors and spitting out very large coefficients.
    This is likely related to the smallness of the data, where the linear system is
    underdetermined; however, this is not a situation where the number of predictors
    outnumber the number of individuals in the sample, so it is likely a function
    of the data itself rather than purely sample size.
  prefs: []
  type: TYPE_NORMAL
- en: Note the model fails to find any significant predictors of self-reported depression.
    Linear regression can’t handle this dataset very well, and the results are not
    reliable. For some samples, certain variables may not be computable in the linear
    regression model at all. Homotopy-based models (and other types of penalized models)
    often work better on small datasets, and there is some evidence that they perform
    better for datasets with many local optima. While this dataset is a bit small
    for fitting a model, it does demonstrate the power of homotopy-based optimization
    (and penalized regression, in general) on very small datasets, and its results
    make a lot more sense than the linear regression model’s results.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we gave you an overview of homotopy and its applications in
    machine learning, including through an example of homotopy as an extension of
    regression-based algorithms on a simulated problem and a real dataset. Homotopy
    can help regression algorithms avoid local optima that often trap local optimizers.
  prefs: []
  type: TYPE_NORMAL
- en: Other uses of homotopy algorithms in the field of artificial intelligence include
    navigational problems. For instance, an autonomous cart may need to navigate the
    halls and obstacles of a hospital by weighting different possible paths from its
    current location to its destination. Homotopy algorithms are often used to generate
    the possible paths, which are then weighted by time cost or hazard cost of the
    route. Bounds can also be placed to avoid generating paths that obviously aren’t
    viable (such as going through areas where the cart can’t physically go or wouldn’t
    be wanted—such as an operating room). It’s likely that this branch of topological
    data analysis will grow in the coming years, and we encourage you to explore other
    uses of homotopy in machine learning, robotics, differential equations, and engineering.
  prefs: []
  type: TYPE_NORMAL
