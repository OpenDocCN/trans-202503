- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Behind the Curtain
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/book_art/chapterart.png)'
  prefs: []
  type: TYPE_IMG
- en: Maybe you follow the newest and hippest technologies as soon as they hit the
    market. Maybe you’re too busy busting Windows domains to keep up with the latest
    trends outside your niche. But whether you were living like a pariah for the last
    couple of years or touring from one conference to another, you must have heard
    rumors and whispers of some magical new beast called *Kubernetes*, the ultimate
    container orchestrator and deployment solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kube fanatics will tell you that this technology solves all the greatest challenges
    of admins and DevOps. That it just works out of the box. Magic, they claim. Sure,
    give a helpless individual a wing suit, point to a tiny hole far in the mountains,
    and push them over the edge. Kubernetes is no magic. It’s complex. It’s a messy
    spaghetti of dissonant ingredients somehow entangled together and bound by everyone’s
    worst nemeses: iptables and DNS.'
  prefs: []
  type: TYPE_NORMAL
- en: The best part for us hackers? It took a team of very talented engineers two
    full years *after the first public release* to roll out security features. One
    could argue over their sense of priority, but I, for one, am grateful. If qualified,
    overpaid engineers were designing unauthenticated APIs and insecure systems in
    2017, who am I to argue? Any help is much appreciated, folks.
  prefs: []
  type: TYPE_NORMAL
- en: Having said that, I believe that Kubernetes is a powerful and disruptive technology.
    It’s probably here to stay and has the potential to play such a critical role
    in a company’s architecture that I feel compelled to present a crash course on
    its internal workings. If you’ve already deployed clusters from scratch or written
    your own controller, you can skip this chapter. Otherwise, stick around. You may
    not become a Kube expert, but you will know enough to hack one, that I can promise
    you.
  prefs: []
  type: TYPE_NORMAL
- en: Hackers cannot be satisfied with the “magic” argument. We will break Kube apart,
    explore its components, and learn to spot some common misconfigurations. MXR Ads
    will be the perfect terrain for that. Get pumped to hack some Kube!
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Kubernetes is the answer to the question, “How can I efficiently manage a thousand
    containers?” If you play a little bit with the containers in the infrastructure
    we set up in Chapter 3, you will quickly hit some frustrating limits. For instance,
    to deploy a new version of a container image, you have to alter the user data
    and restart or roll out a new machine. Think about that: to reset a handful of
    processes, an operation that should take mere seconds, you have to provision a
    whole new machine. Similarly, the only way to scale out the environment dynamically—say,
    if you wanted to double the number of containers—is to multiply machines and hide
    them behind a load balancer. Our application comes in containers, but we can only
    act at the machine level.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Kube solves this and many more issues by providing an environment to run, manage,
    and schedule containers efficiently across multiple machines. Want to add two
    more Nginx containers? No problem. That’s literally one command away:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Want to update the version of the Nginx container deployed in production? Now
    there’s no need to redeploy machines. Just ask Kube to roll out the new update,
    with no downtime:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Want to have an immediate shell on container number 7543 running on machine
    i-1b2ac87e65f15 somewhere on the VPC vpc-b95e4bdf? Forget about fetching the host’s
    IP, injecting a private key, SSH, `docker exec`, and so on. It’s not 2012 anymore!
    A simple `kubectl exec` from your laptop will suffice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: No wonder this behemoth conquered the hearts and brains of everyone in the DevOps
    community. It’s elegant, efficient, and, until fairly recently, so very insecure!
    There was a time, barely a couple of years ago, when you could just point to a
    single URL and perform all of the aforementioned actions and much more without
    a whisper of authentication. *Nichts*, *zilch*, *nada*. And that was just one
    entry point; three others gave similar access. It was brutal.
  prefs: []
  type: TYPE_NORMAL
- en: In the last two years or so, however, Kubernetes has implemented many new security
    features, from role-based access control to network filtering. While some companies
    are still stuck with clusters older than 1.8, most are running reasonably up-to-date
    versions, so we will tackle a fully patched and hardened Kubernetes cluster to
    spice things up.
  prefs: []
  type: TYPE_NORMAL
- en: For the remainder of this chapter, imagine that we have a set of a hundred machines
    provisioned, courtesy of AWS, that are fully subjected to the whim and folly of
    Kubernetes. The whole lot form what we commonly call a *Kubernetes cluster*. We
    will play with some rudimentary commands before deconstructing the whole thing,
    so indulge some partial information in the next few paragraphs. It will all come
    together in the end.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Pods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our journey into Kubernetes starts with a container running an application.
    This application heavily depends on a second container with a small local database
    to answer queries. That’s where pods enter the scene. A *pod* is essentially one
    or many containers considered by Kubernetes as a single unit. All containers within
    a pod will be scheduled together, spawned together, and terminated together (see
    [Figure 7-1](#figure7-1)).
  prefs: []
  type: TYPE_NORMAL
- en: The most common way you interact with Kubernetes is by submitting *manifest
    files*. These files describe the *desired state* of the infrastructure, such as
    which pods should run, which image they use, how they communicate with each other,
    and so on. Everything in Kubernetes revolves around that desired state. In fact,
    Kube’s main mission is to make that desired state a reality and keep it that way.
  prefs: []
  type: TYPE_NORMAL
- en: '![f07001](image_fi/501263c07/f07001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7-1: A pod composed of Nginx and Redis containers'
  prefs: []
  type: TYPE_NORMAL
- en: 'In [Listing 7-1](#listing7-1), we create a manifest file that stamps the label
    `app: myapp` on a pod composed of two containers: an Nginx server listening on
    port 8080 and a Redis database available on port 6379\. Here is the YAML syntax
    to describe this setup:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 7-1: The manifest file to create a pod comprising two containers'
  prefs: []
  type: TYPE_NORMAL
- en: We send this manifest using the kubectl utility, which is the flagship program
    used to interact with a Kubernetes cluster. You’ll need to download kubectl from
    [https://kubernetes.io/docs/tasks/tools/install-kubectl/](https://kubernetes.io/docs/tasks/tools/install-kubectl/).
  prefs: []
  type: TYPE_NORMAL
- en: 'We update the kubectl config file *~/.kube/config* to point to our cluster
    (more on that later) and then submit the manifest file in [Listing 7-1](#listing7-1):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Our pod consisting of two containers is now successfully running on one of the
    100 machines in the cluster. Containers in the same pod are treated as a single
    unit, so Kube makes them share the same volume and network namespaces. The result
    is that our Nginx and database containers have the same IP address (10.0.2.3)
    picked from the network bridge IP pool (see “Resources” on page 119 for a pointer
    to more info on that) and can talk to each other using their namespace-isolated
    localhost (127.0.0.1) address, as depicted in [Figure 7-2](#figure7-2). Pretty
    handy.
  prefs: []
  type: TYPE_NORMAL
- en: '![f07002](image_fi/501263c07/f07002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7-2: Network configuration of the pod, containers, and the host machine
    (node)'
  prefs: []
  type: TYPE_NORMAL
- en: Each pod has an IP address and lives on a virtual or bare-metal machine called
    a *node*. Each machine in our cluster is a node, so the cluster has 100 nodes.
    Each node hosts a Linux distribution with some special Kubernetes tools and programs
    to synchronize with the rest of the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: One pod is great, but two are better, especially for resilience so the second
    can act as a backup should the first fail. What should we do? Submit the same
    manifest twice? Nah, we create a *deployment* object that can replicate pods,
    as depicted in [Figure 7-3](#figure7-3).
  prefs: []
  type: TYPE_NORMAL
- en: '![f07003](image_fi/501263c07/f07003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7-3: A Kube deployment object'
  prefs: []
  type: TYPE_NORMAL
- en: A deployment describes how many pods should be running at any given time and
    oversees the replication strategy. It will automatically respawn pods if they
    go down, but its key feature is rolling updates. If we decide to update the container’s
    image, for instance, and thus submit an updated deployment manifest, it will strategically
    replace pods in a way that guarantees the continuous availability of the application
    during the update process. If anything goes wrong, the new deployment rolls back
    to the previous version of the desired state.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s delete our previous stand-alone pod so we can re-create it as part of
    a deployment object instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: To create the pod as a deployment object, we push a new manifest file of type
    `Deployment`, specify the labels of the containers to replicate, and append the
    previous pod’s configuration in its manifest file (see [Listing 7-2](#listing7-2)).
    Pods are almost always created as part of deployment resources.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 7-2: Re-creating our pod as a deployment object'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we submit the manifest file and check the details of the new deployment
    pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 7-4](#figure7-4) shows these two pods running.'
  prefs: []
  type: TYPE_NORMAL
- en: '![f07004](image_fi/501263c07/f07004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7-4: Two pods running, each composed of two containers'
  prefs: []
  type: TYPE_NORMAL
- en: All pods and nodes that are part of the same Kubernetes cluster can freely communicate
    with each other without having to use masquerading techniques such as Network
    Address Translation (NAT). This free communication is one of the defining network
    features of Kubernetes. Our pod A on machine B should be able to reach pod C on
    machine D by following normal routes defined at the machine/router/subnet/VPC
    level. These routes are automatically created by tools setting up the Kube cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Balancing Traffic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now we want to balance traffic to these two pods. If one of them goes down,
    the packets should be automatically routed to the remaining pod while a new one
    is respawned. The object that describes this configuration is called a *service*
    and is depicted in [Figure 7-5](#figure7-5).
  prefs: []
  type: TYPE_NORMAL
- en: '![f07005](image_fi/501263c07/f07005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7-5: A cluster service object'
  prefs: []
  type: TYPE_NORMAL
- en: A service’s manifest file is composed of metadata adding tags to this service
    and its routing rules, which state which pods to target and port to listen on
    (see [Listing 7-3](#listing7-3)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 7-3: The service manifest file'
  prefs: []
  type: TYPE_NORMAL
- en: 'We then submit this manifest file to create the service, and our service gets
    assigned a *cluster IP* that is reachable only from within the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: A pod on another machine that wants to communicate with our Nginx server will
    send its request to that cluster IP on port 80, which will then forward the traffic
    to port 8080 on one of the two containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s quickly spring up a temporary container using the Docker public image
    `curlimages/curl` to test this setup and ping the cluster IP:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Excellent, we can reach the Nginx container from within the cluster. With me
    so far? Great.
  prefs: []
  type: TYPE_NORMAL
- en: Opening the App to the World
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Up until this point, our application is still closed to the outside world. Only
    internal pods and nodes know how to contact the cluster IP or directly reach the
    pods. Our computer sitting on a different network does not have the necessary
    routing information to reach any of the resources we just created. The last step
    in this crash tutorial is to make this service callable from the outside world
    using a *NodePort*. This object exposes a port on every node of the cluster that
    will randomly point to one of the two pods we created (we’ll go into this a bit
    more later). We preserve the resilience feature even for external access.
  prefs: []
  type: TYPE_NORMAL
- en: 'We add `type: NodePort` to the previous service definition in the manifest
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we resubmit the service manifest once more:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Any request to the external IP of any node on port 31357 will reach one of
    the two Nginx pods at random. Here’s a quick test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Phew . . . all done. We could also add another layer of networking by creating
    a load balancer to expose more common ports like 443 and 80 that will route traffic
    to this node port, but let’s just stop here for now.
  prefs: []
  type: TYPE_NORMAL
- en: Kube Under the Hood
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have a resilient, loosely load-balanced, containerized application running
    somewhere. Now to the fun part. Let’s deconstruct what just happened and uncover
    the dirty secrets that every online tutorial seems to hastily slip under the rug.
  prefs: []
  type: TYPE_NORMAL
- en: When I first started playing with Kubernetes, that cluster IP address we get
    when creating a service bothered me. A lot. Where did it come from? The nodes’
    subnet is 192.168.0.0/16\. The containers are swimming in their own 10.0.0.0/16
    pool. Where the hell did that IP come from?
  prefs: []
  type: TYPE_NORMAL
- en: We can list every interface of every node in our cluster without ever finding
    that IP address. Because it does not exist. Literally. It’s simply an iptables
    target rule. The rule is pushed to all nodes and instructs them to forward all
    requests targeting this nonexistent IP to one of the two pods we created. That’s
    it. That’s what a service object is—a bunch of iptables rules that are orchestrated
    by a component called *kube-proxy*.
  prefs: []
  type: TYPE_NORMAL
- en: Kube-proxy is also a pod, but a very special one indeed. It runs on every node
    of the cluster, secretly orchestrating the network traffic. Despite its name,
    it does not actually forward packets, not in recent releases anyway. It silently
    creates and updates iptables rules on all nodes to make sure network packets reach
    their destinations.
  prefs: []
  type: TYPE_NORMAL
- en: 'When a packet reaches (or tries to leave) a node, it automatically gets sent
    to the `KUBE-SERVICES` iptables chain, which we can explore using the `iptables-save`
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This chain tries to match the packet against multiple rules based on its destination
    IP and port (`-d` and `--dport` flags):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'There is our naughty cluster IP! Any packet sent to the 10.100.172.183 address
    is forwarded to the chain `KUBE-SVC-NPJ`, which is defined a few lines further
    down:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Each rule in this chain randomly matches the packet 50 percent of the time
    and forwards it to a different chain that ultimately sends the packet to one of
    the two pods running. The resilience of the service object is nothing more than
    a reflection of iptables’ statistic module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'A packet sent to the node port will follow the same processing chain, except
    that it will fail to match any cluster IP rule, so it automatically gets forwarded
    to the `KUBE-NODEPORTS` chain. If the destination port matches a predeclared node
    port, the packet is forwarded to the load-balancing chain (`KUBE-SVC-NPJI`) we
    saw that distributes it randomly among the pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'That’s all there is to it: a clever chain of iptables rules and network routes.'
  prefs: []
  type: TYPE_NORMAL
- en: In Kubernetes, every little task is performed by a dedicated component. Kube-proxy
    is in charge of the networking configuration. It is special in that it runs as
    a pod on every node, while the rest of the core components run inside multiple
    pods on a select group of nodes called *master nodes*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Out of the 100 nodes we made when we created the cluster of 100 machines, the
    one master node will host a collection of pods that make up the spinal cord of
    Kubernetes: the API server, kube-scheduler, and controller manager (see [Figure
    7-6](#figure7-6)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![f07006](image_fi/501263c07/f07006.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7-6: Pods running on the master node versus those running on regular
    nodes'
  prefs: []
  type: TYPE_NORMAL
- en: 'We actually already interacted with the master node when using `kubectl` `apply`
    commands to send manifest files. Kubectl is a wrapper that sends HTTP requests
    to the all-important API server pod, the main entry point to retrieve and persist
    the famous desired state of the cluster. Here is a typical configuration one may
    use to reach the Kube cluster *(~/.kube/config)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Our API server URL in this case is *https://192.168.99.100*. Think of it this
    way: the API server is the only pod allowed to read/write the desired state in
    the database. Want to list pods? Ask the API server. Want to report a pod failure?
    Tell the API server. It is the main orchestrator that conducts the complex symphony
    that is Kubernetes.'
  prefs: []
  type: TYPE_NORMAL
- en: When we submitted our deployment file to the API server through kubectl (HTTP),
    it made a series of checks (authentication and authorization, which we will cover
    in Chapter 8) and then wrote that deployment object in the *etcd* database, which
    is a key-value database that maintains a consistent and coherent state across
    multiple nodes (or pods) using the Raft consensus algorithm. In the case of Kube,
    etcd describes the desired state of the cluster, such as how many pods there are,
    their manifest files, service descriptions, node descriptions, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the API server writes the deployment object to etcd, the desired state
    has officially been altered. It notifies the callback handler that subscribed
    to this particular event: the *deployment controller*, another component running
    on the master node.'
  prefs: []
  type: TYPE_NORMAL
- en: All Kube interactions are based on this type of event-driven behavior, which
    is a reflection of etcd’s watch feature. The API server receives a notification
    or an action. It reads or modifies the desired state in etcd, which triggers an
    event delivered to the corresponding handler.
  prefs: []
  type: TYPE_NORMAL
- en: The deployment controller asks the API server to send back the new desired state,
    notices that a deployment has been initialized, but does not find any reference
    to the group of pods it is supposed to manage. It resolves this discrepancy by
    creating a *ReplicaSet*, an object describing the replication strategy of a group
    of pods.
  prefs: []
  type: TYPE_NORMAL
- en: This operation goes through the API server again, which updates the state once
    more. This time, however, the event is sent to the ReplicaSet controller, which
    in turn notices a mismatch between the desired state (a group of two pods) and
    reality (no pods). It proceeds to create the definition of the containers.
  prefs: []
  type: TYPE_NORMAL
- en: This process (you guessed it) goes through the API server again, which, after
    modifying the state, triggers a callback for pod creation, which is monitored
    by the kube-scheduler (a dedicated pod running on the master node).
  prefs: []
  type: TYPE_NORMAL
- en: The scheduler sees two pods in the database in a pending state. Unacceptable.
    It runs its scheduling algorithm to find suitable nodes to host these two pods,
    updates the pods’ descriptions with the corresponding nodes, and submits the lot
    to the API server to be stored in the database.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final piece of this bureaucratic madness is the *kubelet*: a process (not
    a pod!) running on each worker node that routinely pulls the list of pods it ought
    to be running from the API server. The kubelet finds out that its host should
    be running two additional containers, so it proceeds to launch them through the
    container runtime (usually Docker). Our pods are finally alive.'
  prefs: []
  type: TYPE_NORMAL
- en: Complex? Told you so. But one cannot deny the beauty of this synchronization
    scheme. Though we covered only one workflow out of many possible interactions,
    rest assured that you should be able to follow along with almost every article
    you read about Kube. We are even ready to take this to the next step—because,
    lest you forget, we still have a real cluster waiting for us at MXR Ads.
  prefs: []
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'More detail on bridges and bridge pools can be found in the Docker documentation:
    [https://docs.docker.com/network/bridge/](https://docs.docker.com/network/bridge/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pods on Amazon Elastic Kubernetes Service (EKS) directly plug into the Elastic
    network interface instead of using a bridged network; for details see [https://amzn.to/37Rff5c](https://amzn.to/37Rff5c).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more about Kubernetes pod-to-pod networking, see [http://bit.ly/3a0hJjX](http://bit.ly/3a0hJjX).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here’s an overview of other ways to access the cluster from the outside:[http://bit.ly/30aGqFU](http://bit.ly/30aGqFU).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information about etcd, see [http://bit.ly/36MAjKr](http://bit.ly/36MAjKr)
    and[http://bit.ly/2sds4bg](http://bit.ly/2sds4bg).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hacking Kubernetes through unauthenticated APIs is covered at [http://bit.ly/36NBk4S](http://bit.ly/36NBk4S).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
