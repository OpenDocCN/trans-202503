- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Behind the Curtain
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 幕后
- en: '![](image_fi/book_art/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/book_art/chapterart.png)'
- en: Maybe you follow the newest and hippest technologies as soon as they hit the
    market. Maybe you’re too busy busting Windows domains to keep up with the latest
    trends outside your niche. But whether you were living like a pariah for the last
    couple of years or touring from one conference to another, you must have heard
    rumors and whispers of some magical new beast called *Kubernetes*, the ultimate
    container orchestrator and deployment solution.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 也许你总是在市场上最新最潮的技术发布时就跟进。也许你太忙于破解Windows域，没时间关注自己领域之外的最新趋势。但无论你这几年是过得像个弃儿，还是在各大会议之间巡回演讲，你一定听过关于某种神奇新生物的传闻和低语——那就是*Kubernetes*，终极容器编排和部署解决方案。
- en: 'Kube fanatics will tell you that this technology solves all the greatest challenges
    of admins and DevOps. That it just works out of the box. Magic, they claim. Sure,
    give a helpless individual a wing suit, point to a tiny hole far in the mountains,
    and push them over the edge. Kubernetes is no magic. It’s complex. It’s a messy
    spaghetti of dissonant ingredients somehow entangled together and bound by everyone’s
    worst nemeses: iptables and DNS.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Kube的狂热者会告诉你，这项技术解决了管理员和DevOps面临的所有重大挑战。它号称开箱即用，简直是魔法，他们这么说。是的，给一个无助的人一套翼装，指向远处山脉中的一个小洞，然后把他推下去。Kubernetes可不是什么魔法。它很复杂。它是由各种不协调的成分交织在一起，像一团乱麻，最终被每个人最头痛的敌人：iptables和DNS绑定在一起。
- en: The best part for us hackers? It took a team of very talented engineers two
    full years *after the first public release* to roll out security features. One
    could argue over their sense of priority, but I, for one, am grateful. If qualified,
    overpaid engineers were designing unauthenticated APIs and insecure systems in
    2017, who am I to argue? Any help is much appreciated, folks.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们这些黑客来说，最棒的部分是什么？在首次公开发布后，花了一个非常有才华的工程师团队整整两年时间*才推出安全功能*。有人可能会对他们的优先级提出质疑，但我个人是感激的。如果合格的、高薪的工程师在2017年设计了未经认证的API和不安全的系统，那我又能说什么呢？任何帮助都非常感激，伙计们。
- en: Having said that, I believe that Kubernetes is a powerful and disruptive technology.
    It’s probably here to stay and has the potential to play such a critical role
    in a company’s architecture that I feel compelled to present a crash course on
    its internal workings. If you’ve already deployed clusters from scratch or written
    your own controller, you can skip this chapter. Otherwise, stick around. You may
    not become a Kube expert, but you will know enough to hack one, that I can promise
    you.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，我相信Kubernetes是一项强大且具有颠覆性的技术。它可能会长期存在，并且有潜力在公司架构中扮演至关重要的角色，以至于我觉得有必要为大家呈现一场关于它内部运作的速成课程。如果你已经从零部署过集群，或者编写过自己的控制器，那么你可以跳过这一章。否则，请继续阅读。你可能不会成为Kube专家，但我敢保证，你会学到足够的知识来破解它。
- en: Hackers cannot be satisfied with the “magic” argument. We will break Kube apart,
    explore its components, and learn to spot some common misconfigurations. MXR Ads
    will be the perfect terrain for that. Get pumped to hack some Kube!
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 黑客们不会满足于“魔法”这一说法。我们将拆解Kube，探索它的各个组件，学习识别一些常见的错误配置。MXR Ads将是完美的实践场所。准备好来破解一些Kube吧！
- en: Kubernetes Overview
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes概述
- en: 'Kubernetes is the answer to the question, “How can I efficiently manage a thousand
    containers?” If you play a little bit with the containers in the infrastructure
    we set up in Chapter 3, you will quickly hit some frustrating limits. For instance,
    to deploy a new version of a container image, you have to alter the user data
    and restart or roll out a new machine. Think about that: to reset a handful of
    processes, an operation that should take mere seconds, you have to provision a
    whole new machine. Similarly, the only way to scale out the environment dynamically—say,
    if you wanted to double the number of containers—is to multiply machines and hide
    them behind a load balancer. Our application comes in containers, but we can only
    act at the machine level.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes是解答“如何高效管理成千上万个容器？”这一问题的答案。如果你在第三章中设置的基础设施上稍微玩一下容器，你很快就会遇到一些令人沮丧的限制。例如，要部署一个新的容器镜像版本，你必须修改用户数据并重启或推出新机器。想想看：为了重置一些进程，这个本应仅需几秒钟的操作，你却需要配置一台全新的机器。同样，唯一的动态扩展环境的方式——比如说，如果你想将容器数量加倍——就是增加机器并将它们隐藏在负载均衡器后面。我们的应用程序以容器形式存在，但我们只能在机器级别进行操作。
- en: 'Kube solves this and many more issues by providing an environment to run, manage,
    and schedule containers efficiently across multiple machines. Want to add two
    more Nginx containers? No problem. That’s literally one command away:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Kube 通过提供一个运行、管理和调度容器的环境，解决了这个以及更多问题，使得多个机器之间的容器管理变得高效。想要再添加两个 Nginx 容器？没问题。只需一个命令：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Want to update the version of the Nginx container deployed in production? Now
    there’s no need to redeploy machines. Just ask Kube to roll out the new update,
    with no downtime:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 想要更新生产环境中部署的 Nginx 容器版本吗？现在不需要重新部署机器了。只需请求 Kube 滚动发布新更新，无需停机：
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Want to have an immediate shell on container number 7543 running on machine
    i-1b2ac87e65f15 somewhere on the VPC vpc-b95e4bdf? Forget about fetching the host’s
    IP, injecting a private key, SSH, `docker exec`, and so on. It’s not 2012 anymore!
    A simple `kubectl exec` from your laptop will suffice:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 想要立即在 VPC vpc-b95e4bdf 上某个机器 i-1b2ac87e65f15 上运行的容器编号 7543 上获得 shell 吗？忘掉获取主机
    IP、注入私钥、SSH、`docker exec` 等等吧。现在可不是 2012 年了！只需从你的笔记本电脑上执行一个简单的 `kubectl exec`
    命令即可：
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: No wonder this behemoth conquered the hearts and brains of everyone in the DevOps
    community. It’s elegant, efficient, and, until fairly recently, so very insecure!
    There was a time, barely a couple of years ago, when you could just point to a
    single URL and perform all of the aforementioned actions and much more without
    a whisper of authentication. *Nichts*, *zilch*, *nada*. And that was just one
    entry point; three others gave similar access. It was brutal.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 难怪这个庞然大物征服了所有 DevOps 社区的心智。它优雅、高效，直到最近，曾经是如此不安全！几年前，你只需指向一个 URL，就可以执行上述所有操作以及更多操作，而无需任何身份验证。*Nichts*，*zilch*，*nada*。而且那只是一个入口点，另外三个入口也提供类似的访问。那真是残酷。
- en: In the last two years or so, however, Kubernetes has implemented many new security
    features, from role-based access control to network filtering. While some companies
    are still stuck with clusters older than 1.8, most are running reasonably up-to-date
    versions, so we will tackle a fully patched and hardened Kubernetes cluster to
    spice things up.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在过去两年左右的时间里，Kubernetes 实现了许多新的安全功能，从基于角色的访问控制到网络过滤。虽然一些公司仍然停留在 1.8 之前的集群版本，但大多数公司都在运行比较现代的版本，因此我们将使用一个完全修补和加固的
    Kubernetes 集群来增加难度。
- en: For the remainder of this chapter, imagine that we have a set of a hundred machines
    provisioned, courtesy of AWS, that are fully subjected to the whim and folly of
    Kubernetes. The whole lot form what we commonly call a *Kubernetes cluster*. We
    will play with some rudimentary commands before deconstructing the whole thing,
    so indulge some partial information in the next few paragraphs. It will all come
    together in the end.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分，假设我们有一百台由 AWS 提供的机器，完全受 Kubernetes 的支配。这些机器组成了我们常说的 *Kubernetes 集群*。我们将在解构整个过程之前先使用一些基本命令，所以接下来的几段请容忍部分信息。这一切最终都会理顺。
- en: Introducing Pods
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 引入 Pods
- en: Our journey into Kubernetes starts with a container running an application.
    This application heavily depends on a second container with a small local database
    to answer queries. That’s where pods enter the scene. A *pod* is essentially one
    or many containers considered by Kubernetes as a single unit. All containers within
    a pod will be scheduled together, spawned together, and terminated together (see
    [Figure 7-1](#figure7-1)).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 Kubernetes 之旅从一个运行应用程序的容器开始。这个应用程序严重依赖于第二个容器，后者包含一个小型本地数据库来响应查询。这时，pods
    登场了。*Pod* 本质上是一个或多个容器，Kubernetes 将它们视为一个整体。Pod 中的所有容器将一起调度、一起启动、一起终止（参见 [图 7-1](#figure7-1)）。
- en: The most common way you interact with Kubernetes is by submitting *manifest
    files*. These files describe the *desired state* of the infrastructure, such as
    which pods should run, which image they use, how they communicate with each other,
    and so on. Everything in Kubernetes revolves around that desired state. In fact,
    Kube’s main mission is to make that desired state a reality and keep it that way.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你与 Kubernetes 交互的最常见方式是提交 *清单文件*。这些文件描述了基础设施的 *期望状态*，例如哪些 pod 应该运行，使用哪个镜像，它们如何相互通信，等等。在
    Kubernetes 中，一切都围绕着那个期望状态展开。实际上，Kube 的主要任务就是将这个期望状态变为现实并保持不变。
- en: '![f07001](image_fi/501263c07/f07001.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![f07001](image_fi/501263c07/f07001.png)'
- en: 'Figure 7-1: A pod composed of Nginx and Redis containers'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7-1：由 Nginx 和 Redis 容器组成的一个 pod
- en: 'In [Listing 7-1](#listing7-1), we create a manifest file that stamps the label
    `app: myapp` on a pod composed of two containers: an Nginx server listening on
    port 8080 and a Redis database available on port 6379\. Here is the YAML syntax
    to describe this setup:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '在[清单 7-1](#listing7-1)中，我们创建一个清单文件，为由两个容器组成的 Pod 打上标签`app: myapp`：一个 Nginx
    服务器监听 8080 端口，另一个是可用 6379 端口的 Redis 数据库。以下是描述此设置的 YAML 语法：'
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Listing 7-1: The manifest file to create a pod comprising two containers'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 7-1：创建包含两个容器的 Pod 的清单文件
- en: We send this manifest using the kubectl utility, which is the flagship program
    used to interact with a Kubernetes cluster. You’ll need to download kubectl from
    [https://kubernetes.io/docs/tasks/tools/install-kubectl/](https://kubernetes.io/docs/tasks/tools/install-kubectl/).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 kubectl 工具发送这个清单，kubectl 是与 Kubernetes 集群交互的旗舰程序。你需要从[https://kubernetes.io/docs/tasks/tools/install-kubectl/](https://kubernetes.io/docs/tasks/tools/install-kubectl/)
    下载 kubectl。
- en: 'We update the kubectl config file *~/.kube/config* to point to our cluster
    (more on that later) and then submit the manifest file in [Listing 7-1](#listing7-1):'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们更新 kubectl 配置文件*~/.kube/config*，使其指向我们的集群（稍后会详细介绍），然后提交[清单 7-1](#listing7-1)中的清单文件：
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Our pod consisting of two containers is now successfully running on one of the
    100 machines in the cluster. Containers in the same pod are treated as a single
    unit, so Kube makes them share the same volume and network namespaces. The result
    is that our Nginx and database containers have the same IP address (10.0.2.3)
    picked from the network bridge IP pool (see “Resources” on page 119 for a pointer
    to more info on that) and can talk to each other using their namespace-isolated
    localhost (127.0.0.1) address, as depicted in [Figure 7-2](#figure7-2). Pretty
    handy.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 由两个容器组成的 Pod 现在已成功运行在集群中 100 台机器中的一台上。位于同一 Pod 中的容器被视为一个整体，因此 Kube 使它们共享相同的卷和网络命名空间。结果是，我们的
    Nginx 和数据库容器具有相同的 IP 地址（10.0.2.3），该地址从网络桥接 IP 池中选择（有关详细信息，请参见第 119 页的“资源”部分），并且它们可以使用其命名空间隔离的本地主机地址（127.0.0.1）互相通信，如[图
    7-2](#figure7-2)所示。这很方便。
- en: '![f07002](image_fi/501263c07/f07002.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![f07002](image_fi/501263c07/f07002.png)'
- en: 'Figure 7-2: Network configuration of the pod, containers, and the host machine
    (node)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7-2：Pod、容器和宿主机（节点）的网络配置
- en: Each pod has an IP address and lives on a virtual or bare-metal machine called
    a *node*. Each machine in our cluster is a node, so the cluster has 100 nodes.
    Each node hosts a Linux distribution with some special Kubernetes tools and programs
    to synchronize with the rest of the cluster.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 Pod 都有一个 IP 地址，并运行在一个虚拟或裸金属机器上，称为*节点*。我们集群中的每台机器都是一个节点，因此集群有 100 个节点。每个节点都托管着一个带有一些特殊
    Kubernetes 工具和程序的 Linux 发行版，用于与集群中的其他节点同步。
- en: One pod is great, but two are better, especially for resilience so the second
    can act as a backup should the first fail. What should we do? Submit the same
    manifest twice? Nah, we create a *deployment* object that can replicate pods,
    as depicted in [Figure 7-3](#figure7-3).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 Pod 很棒，但两个更好，特别是为了提高弹性，第二个 Pod 可以在第一个失败时作为备份。那么我们该怎么办呢？提交相同的清单两次？不，我们创建一个*部署*对象，可以复制
    Pod，如[图 7-3](#figure7-3)所示。
- en: '![f07003](image_fi/501263c07/f07003.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![f07003](image_fi/501263c07/f07003.png)'
- en: 'Figure 7-3: A Kube deployment object'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7-3：一个 Kube 部署对象
- en: A deployment describes how many pods should be running at any given time and
    oversees the replication strategy. It will automatically respawn pods if they
    go down, but its key feature is rolling updates. If we decide to update the container’s
    image, for instance, and thus submit an updated deployment manifest, it will strategically
    replace pods in a way that guarantees the continuous availability of the application
    during the update process. If anything goes wrong, the new deployment rolls back
    to the previous version of the desired state.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一个部署描述了在任何给定时间应该运行多少个 Pod，并监督复制策略。如果 Pod 发生故障，它将自动重启；但它的关键特性是滚动更新。例如，如果我们决定更新容器的镜像，并提交一个更新的部署清单，它将以一种策略性方式替换
    Pod，确保在更新过程中应用的持续可用性。如果出现问题，新的部署将回滚到之前的期望状态。
- en: 'Let’s delete our previous stand-alone pod so we can re-create it as part of
    a deployment object instead:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们删除之前的独立 Pod，以便将其作为部署对象的一部分重新创建：
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: To create the pod as a deployment object, we push a new manifest file of type
    `Deployment`, specify the labels of the containers to replicate, and append the
    previous pod’s configuration in its manifest file (see [Listing 7-2](#listing7-2)).
    Pods are almost always created as part of deployment resources.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 要将 pod 创建为部署对象，我们推送一个新的类型为`Deployment`的清单文件，指定要复制的容器标签，并在清单文件中附加前一个 pod 的配置（参见[列表
    7-2](#listing7-2)）。Pod 通常作为部署资源的一部分进行创建。
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Listing 7-2: Re-creating our pod as a deployment object'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7-2：将我们的 pod 重新创建为部署对象
- en: 'Now we submit the manifest file and check the details of the new deployment
    pods:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们提交清单文件并查看新部署的 pod 详情：
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[Figure 7-4](#figure7-4) shows these two pods running.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7-4](#figure7-4)展示了这两个 pod 正在运行。'
- en: '![f07004](image_fi/501263c07/f07004.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![f07004](image_fi/501263c07/f07004.png)'
- en: 'Figure 7-4: Two pods running, each composed of two containers'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7-4：两个正在运行的 pod，每个 pod 由两个容器组成
- en: All pods and nodes that are part of the same Kubernetes cluster can freely communicate
    with each other without having to use masquerading techniques such as Network
    Address Translation (NAT). This free communication is one of the defining network
    features of Kubernetes. Our pod A on machine B should be able to reach pod C on
    machine D by following normal routes defined at the machine/router/subnet/VPC
    level. These routes are automatically created by tools setting up the Kube cluster.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 所有属于同一 Kubernetes 集群的 pod 和节点可以自由通信，而不需要使用像网络地址转换（NAT）这样的伪装技术。这种自由通信是 Kubernetes
    网络功能的标志之一。位于 B 机器上的 pod A 应该能够通过机器/路由器/子网/VPC 层定义的正常路由，访问位于 D 机器上的 pod C。这些路由是由设置
    Kube 集群的工具自动创建的。
- en: Balancing Traffic
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 流量均衡
- en: Now we want to balance traffic to these two pods. If one of them goes down,
    the packets should be automatically routed to the remaining pod while a new one
    is respawned. The object that describes this configuration is called a *service*
    and is depicted in [Figure 7-5](#figure7-5).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们想要将流量均衡到这两个 pod。如果其中一个 pod 停止运行，数据包应自动路由到剩下的 pod，同时重新生成一个新的 pod。描述这个配置的对象叫做*服务*，如[图
    7-5](#figure7-5)所示。
- en: '![f07005](image_fi/501263c07/f07005.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![f07005](image_fi/501263c07/f07005.png)'
- en: 'Figure 7-5: A cluster service object'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7-5：集群服务对象
- en: A service’s manifest file is composed of metadata adding tags to this service
    and its routing rules, which state which pods to target and port to listen on
    (see [Listing 7-3](#listing7-3)).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一个服务的清单文件由元数据组成，这些元数据为服务和其路由规则添加标签，路由规则指定要访问的 pod 和监听的端口（参见[列表 7-3](#listing7-3)）。
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Listing 7-3: The service manifest file'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7-3：服务清单文件
- en: 'We then submit this manifest file to create the service, and our service gets
    assigned a *cluster IP* that is reachable only from within the cluster:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们提交这个清单文件以创建服务，服务会被分配一个*集群 IP*，这个 IP 只能从集群内部访问：
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: A pod on another machine that wants to communicate with our Nginx server will
    send its request to that cluster IP on port 80, which will then forward the traffic
    to port 8080 on one of the two containers.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个机器上的 pod 如果想与我们的 Nginx 服务器通信，将把请求发送到集群 IP 的 80 端口，然后将流量转发到两个容器中的一个的 8080
    端口。
- en: 'Let’s quickly spring up a temporary container using the Docker public image
    `curlimages/curl` to test this setup and ping the cluster IP:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 Docker 公共镜像`curlimages/curl`快速启动一个临时容器来测试这个设置，并 ping 集群 IP：
- en: '[PRE10]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Excellent, we can reach the Nginx container from within the cluster. With me
    so far? Great.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 很棒，我们可以从集群内访问 Nginx 容器。跟得上吗？太好了。
- en: Opening the App to the World
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将应用暴露给外部世界
- en: Up until this point, our application is still closed to the outside world. Only
    internal pods and nodes know how to contact the cluster IP or directly reach the
    pods. Our computer sitting on a different network does not have the necessary
    routing information to reach any of the resources we just created. The last step
    in this crash tutorial is to make this service callable from the outside world
    using a *NodePort*. This object exposes a port on every node of the cluster that
    will randomly point to one of the two pods we created (we’ll go into this a bit
    more later). We preserve the resilience feature even for external access.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们的应用仍然对外界封闭。只有内部的 pod 和节点知道如何联系集群 IP 或直接访问 pod。我们所在的计算机位于不同的网络上，缺少必要的路由信息来访问我们刚刚创建的任何资源。本教程的最后一步是通过*NodePort*使该服务能够从外部访问。该对象会在集群的每个节点上暴露一个端口，该端口会随机指向我们创建的两个
    pod 之一（我们稍后会详细介绍）。即使是外部访问，我们也保留了弹性功能。
- en: 'We add `type: NodePort` to the previous service definition in the manifest
    file:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在清单文件中的先前服务定义中添加 `type: NodePort`：'
- en: '[PRE11]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Then we resubmit the service manifest once more:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们再次提交服务清单：
- en: '[PRE12]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Any request to the external IP of any node on port 31357 will reach one of
    the two Nginx pods at random. Here’s a quick test:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 任何请求到达任何节点外部 IP 上的 31357 端口时，都会随机地到达两个 Nginx Pod 之一。这里是一个快速测试：
- en: '[PRE13]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Phew . . . all done. We could also add another layer of networking by creating
    a load balancer to expose more common ports like 443 and 80 that will route traffic
    to this node port, but let’s just stop here for now.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 呼……完成了。我们还可以通过创建一个负载均衡器，暴露更多常见的端口，如 443 和 80，将流量路由到此节点端口，从而添加更多网络层次，但暂时就停在这里吧。
- en: Kube Under the Hood
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kube 底层揭秘
- en: We have a resilient, loosely load-balanced, containerized application running
    somewhere. Now to the fun part. Let’s deconstruct what just happened and uncover
    the dirty secrets that every online tutorial seems to hastily slip under the rug.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个具有弹性、松散负载均衡、容器化的应用程序正在某处运行。接下来是有趣的部分。让我们拆解一下刚刚发生的事情，揭开每个在线教程似乎匆忙掩盖的肮脏秘密。
- en: When I first started playing with Kubernetes, that cluster IP address we get
    when creating a service bothered me. A lot. Where did it come from? The nodes’
    subnet is 192.168.0.0/16\. The containers are swimming in their own 10.0.0.0/16
    pool. Where the hell did that IP come from?
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 当我第一次开始玩 Kubernetes 时，创建服务时得到的集群 IP 地址让我困扰了很久。很多。它是从哪里来的？节点的子网是 192.168.0.0/16。容器们在它们自己的
    10.0.0.0/16 池中游泳。那个 IP 是怎么来的？
- en: We can list every interface of every node in our cluster without ever finding
    that IP address. Because it does not exist. Literally. It’s simply an iptables
    target rule. The rule is pushed to all nodes and instructs them to forward all
    requests targeting this nonexistent IP to one of the two pods we created. That’s
    it. That’s what a service object is—a bunch of iptables rules that are orchestrated
    by a component called *kube-proxy*.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以列出集群中每个节点的每个接口，但永远也找不到那个 IP 地址。因为它根本不存在。字面上讲。它只是一个 iptables 目标规则。这个规则会被推送到所有节点，指示它们将所有针对这个不存在的
    IP 的请求转发到我们创建的两个 Pod 之一。就这样。这就是一个服务对象——一堆由名为 *kube-proxy* 的组件编排的 iptables 规则。
- en: Kube-proxy is also a pod, but a very special one indeed. It runs on every node
    of the cluster, secretly orchestrating the network traffic. Despite its name,
    it does not actually forward packets, not in recent releases anyway. It silently
    creates and updates iptables rules on all nodes to make sure network packets reach
    their destinations.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Kube-proxy 也是一个 Pod，但确实是一个非常特殊的 Pod。它运行在集群的每个节点上，默默地编排网络流量。尽管名字是代理（proxy），但实际上它并不转发数据包，至少在近期版本中不是。它悄悄地在所有节点上创建和更新
    iptables 规则，以确保网络包能够到达目的地。
- en: 'When a packet reaches (or tries to leave) a node, it automatically gets sent
    to the `KUBE-SERVICES` iptables chain, which we can explore using the `iptables-save`
    command:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个数据包到达（或试图离开）节点时，它会自动发送到 `KUBE-SERVICES` iptables 链，我们可以使用 `iptables-save`
    命令查看该链：
- en: '[PRE14]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This chain tries to match the packet against multiple rules based on its destination
    IP and port (`-d` and `--dport` flags):'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这个链尝试根据数据包的目标 IP 和端口（`-d` 和 `--dport` 标志）将其与多个规则匹配：
- en: '[PRE15]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'There is our naughty cluster IP! Any packet sent to the 10.100.172.183 address
    is forwarded to the chain `KUBE-SVC-NPJ`, which is defined a few lines further
    down:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们调皮的集群 IP！任何发送到 10.100.172.183 地址的包都会被转发到链 `KUBE-SVC-NPJ`，该链在稍后的几行中定义：
- en: '[PRE16]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Each rule in this chain randomly matches the packet 50 percent of the time
    and forwards it to a different chain that ultimately sends the packet to one of
    the two pods running. The resilience of the service object is nothing more than
    a reflection of iptables’ statistic module:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 该链中的每条规则会随机匹配包 50% 的时间，并将其转发到一个不同的链，最终将包发送到正在运行的两个 Pod 之一。服务对象的韧性无非是 iptables
    统计模块的反映：
- en: '[PRE17]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'A packet sent to the node port will follow the same processing chain, except
    that it will fail to match any cluster IP rule, so it automatically gets forwarded
    to the `KUBE-NODEPORTS` chain. If the destination port matches a predeclared node
    port, the packet is forwarded to the load-balancing chain (`KUBE-SVC-NPJI`) we
    saw that distributes it randomly among the pods:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 发送到节点端口的数据包将遵循相同的处理链，只是它将无法匹配任何集群 IP 规则，因此会自动转发到 `KUBE-NODEPORTS` 链。如果目标端口匹配预定义的节点端口，数据包就会被转发到我们看到的负载均衡链（`KUBE-SVC-NPJI`），该链将数据包随机分配到
    Pod 中：
- en: '[PRE18]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'That’s all there is to it: a clever chain of iptables rules and network routes.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这么简单：一串巧妙的 iptables 规则和网络路由。
- en: In Kubernetes, every little task is performed by a dedicated component. Kube-proxy
    is in charge of the networking configuration. It is special in that it runs as
    a pod on every node, while the rest of the core components run inside multiple
    pods on a select group of nodes called *master nodes*.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中，每一个小任务都由一个专门的组件执行。Kube-proxy 负责网络配置。它的特殊之处在于它作为一个 pod 在每个节点上运行，而其余核心组件则在一个特定节点组（称为
    *master nodes*）上的多个 pods 中运行。
- en: 'Out of the 100 nodes we made when we created the cluster of 100 machines, the
    one master node will host a collection of pods that make up the spinal cord of
    Kubernetes: the API server, kube-scheduler, and controller manager (see [Figure
    7-6](#figure7-6)).'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们创建 100 台机器的集群时，100 个节点中有一个主节点将承载一组组成 Kubernetes 脊柱的 pods：API 服务器、kube-scheduler
    和 controller manager（参见 [图 7-6](#figure7-6)）。
- en: '![f07006](image_fi/501263c07/f07006.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![f07006](image_fi/501263c07/f07006.png)'
- en: 'Figure 7-6: Pods running on the master node versus those running on regular
    nodes'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7-6：运行在主节点上的 pods 与运行在普通节点上的 pods
- en: 'We actually already interacted with the master node when using `kubectl` `apply`
    commands to send manifest files. Kubectl is a wrapper that sends HTTP requests
    to the all-important API server pod, the main entry point to retrieve and persist
    the famous desired state of the cluster. Here is a typical configuration one may
    use to reach the Kube cluster *(~/.kube/config)*:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，当我们使用 `kubectl` `apply` 命令发送清单文件时，已经与主节点进行了交互。Kubectl 是一个封装器，它向至关重要的 API
    服务器 pod 发送 HTTP 请求，这是获取和持久化集群所需状态的主要入口点。这里是一个典型的配置，可能会用来访问 Kube 集群（*~/.kube/config*）：
- en: '[PRE19]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Our API server URL in this case is *https://192.168.99.100*. Think of it this
    way: the API server is the only pod allowed to read/write the desired state in
    the database. Want to list pods? Ask the API server. Want to report a pod failure?
    Tell the API server. It is the main orchestrator that conducts the complex symphony
    that is Kubernetes.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们的 API 服务器 URL 是 *https://192.168.99.100*。你可以这样理解：API 服务器是唯一允许读取/写入数据库中所需状态的
    pod。想列出 pods？请询问 API 服务器。想报告 pod 故障？告诉 API 服务器。它是主控者，负责协调 Kubernetes 中复杂的交响乐。
- en: When we submitted our deployment file to the API server through kubectl (HTTP),
    it made a series of checks (authentication and authorization, which we will cover
    in Chapter 8) and then wrote that deployment object in the *etcd* database, which
    is a key-value database that maintains a consistent and coherent state across
    multiple nodes (or pods) using the Raft consensus algorithm. In the case of Kube,
    etcd describes the desired state of the cluster, such as how many pods there are,
    their manifest files, service descriptions, node descriptions, and so on.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们通过 kubectl 向 API 服务器提交部署文件（HTTP）时，它进行了系列检查（身份验证和授权，我们将在第 8 章讨论），然后将该部署对象写入
    *etcd* 数据库，这是一种使用 Raft 共识算法在多个节点（或 pods）之间保持一致和协调状态的键值数据库。在 Kube 中，etcd 描述了集群的所需状态，例如有多少个
    pods，它们的清单文件，服务描述，节点描述等。
- en: 'Once the API server writes the deployment object to etcd, the desired state
    has officially been altered. It notifies the callback handler that subscribed
    to this particular event: the *deployment controller*, another component running
    on the master node.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 API 服务器将部署对象写入 etcd，所需状态就正式改变了。它会通知订阅了此特定事件的回调处理程序：*deployment controller*，这是另一个在主节点上运行的组件。
- en: All Kube interactions are based on this type of event-driven behavior, which
    is a reflection of etcd’s watch feature. The API server receives a notification
    or an action. It reads or modifies the desired state in etcd, which triggers an
    event delivered to the corresponding handler.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 所有 Kube 交互都基于这种事件驱动的行为，这反映了 etcd 的 watch 功能。API 服务器接收到通知或执行某个操作。它读取或修改 etcd
    中的所需状态，这会触发事件并将其传递给相应的处理程序。
- en: The deployment controller asks the API server to send back the new desired state,
    notices that a deployment has been initialized, but does not find any reference
    to the group of pods it is supposed to manage. It resolves this discrepancy by
    creating a *ReplicaSet*, an object describing the replication strategy of a group
    of pods.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 部署控制器要求 API 服务器返回新的所需状态，发现部署已初始化，但没有找到它应管理的 pod 群组的任何参考。它通过创建一个 *ReplicaSet*
    来解决这个差异，ReplicaSet 是描述一组 pod 复制策略的对象。
- en: This operation goes through the API server again, which updates the state once
    more. This time, however, the event is sent to the ReplicaSet controller, which
    in turn notices a mismatch between the desired state (a group of two pods) and
    reality (no pods). It proceeds to create the definition of the containers.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这个操作再次经过 API 服务器，后者再次更新状态。不过，这一次，事件被发送到 ReplicaSet 控制器，控制器发现期望的状态（两组 pod）与现实情况（没有
    pod）不匹配。它继续创建容器定义。
- en: This process (you guessed it) goes through the API server again, which, after
    modifying the state, triggers a callback for pod creation, which is monitored
    by the kube-scheduler (a dedicated pod running on the master node).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程（你猜对了）再次经过 API 服务器，服务器在修改状态后触发 pod 创建的回调， kube-scheduler（运行在主节点上的专用 pod）会监控该回调。
- en: The scheduler sees two pods in the database in a pending state. Unacceptable.
    It runs its scheduling algorithm to find suitable nodes to host these two pods,
    updates the pods’ descriptions with the corresponding nodes, and submits the lot
    to the API server to be stored in the database.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 调度器在数据库中看到两个处于待处理状态的 pod。无法接受。它运行调度算法以找到合适的节点来托管这两个 pod，更新 pod 的描述并为其分配相应的节点，然后将这批
    pod 提交到 API 服务器，存储在数据库中。
- en: 'The final piece of this bureaucratic madness is the *kubelet*: a process (not
    a pod!) running on each worker node that routinely pulls the list of pods it ought
    to be running from the API server. The kubelet finds out that its host should
    be running two additional containers, so it proceeds to launch them through the
    container runtime (usually Docker). Our pods are finally alive.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这一系列官僚式的疯狂过程的最后一部分是 *kubelet*：一个在每个工作节点上运行的进程（不是 pod！），定期从 API 服务器拉取应该运行的 pod
    列表。kubelet 发现它的主机应该运行两个额外的容器，于是它通过容器运行时（通常是 Docker）启动这些容器。我们的 pod 终于活了起来。
- en: Complex? Told you so. But one cannot deny the beauty of this synchronization
    scheme. Though we covered only one workflow out of many possible interactions,
    rest assured that you should be able to follow along with almost every article
    you read about Kube. We are even ready to take this to the next step—because,
    lest you forget, we still have a real cluster waiting for us at MXR Ads.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂吗？我早就说过了。但不可否认，这种同步方案的美妙。虽然我们只讲解了众多可能的交互中的一个工作流，但放心，你应该能跟得上几乎所有关于 Kube 的文章。我们甚至准备将其推向下一个阶段——因为，别忘了，我们在
    MXR Ads 还有一个真实的集群等着我们。
- en: Resources
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: 'More detail on bridges and bridge pools can be found in the Docker documentation:
    [https://docs.docker.com/network/bridge/](https://docs.docker.com/network/bridge/).'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更多关于桥接和桥接池的细节可以在 Docker 文档中找到：[https://docs.docker.com/network/bridge/](https://docs.docker.com/network/bridge/)。
- en: Pods on Amazon Elastic Kubernetes Service (EKS) directly plug into the Elastic
    network interface instead of using a bridged network; for details see [https://amzn.to/37Rff5c](https://amzn.to/37Rff5c).
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亚马逊弹性 Kubernetes 服务（EKS）上的 pod 直接连接到弹性网络接口，而不是使用桥接网络；详情请见 [https://amzn.to/37Rff5c](https://amzn.to/37Rff5c)。
- en: For more about Kubernetes pod-to-pod networking, see [http://bit.ly/3a0hJjX](http://bit.ly/3a0hJjX).
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更多关于 Kubernetes pod 到 pod 网络的信息，请参见 [http://bit.ly/3a0hJjX](http://bit.ly/3a0hJjX)。
- en: Here’s an overview of other ways to access the cluster from the outside:[http://bit.ly/30aGqFU](http://bit.ly/30aGqFU).
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是一个关于从外部访问集群的其他方式的概述：[http://bit.ly/30aGqFU](http://bit.ly/30aGqFU)。
- en: For more information about etcd, see [http://bit.ly/36MAjKr](http://bit.ly/36MAjKr)
    and[http://bit.ly/2sds4bg](http://bit.ly/2sds4bg).
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更多关于 etcd 的信息，请参见 [http://bit.ly/36MAjKr](http://bit.ly/36MAjKr) 和 [http://bit.ly/2sds4bg](http://bit.ly/2sds4bg)。
- en: Hacking Kubernetes through unauthenticated APIs is covered at [http://bit.ly/36NBk4S](http://bit.ly/36NBk4S).
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于通过未经认证的 API 攻击 Kubernetes，详情见 [http://bit.ly/36NBk4S](http://bit.ly/36NBk4S)。
