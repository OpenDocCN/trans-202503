<html><head></head><body>
<h2 class="h" id="ch13"><span epub:type="pagebreak" id="page_405" class="calibre1"/><strong class="calibre2"><span class="big">13</span><br class="calibre3"/>SCIENTIFIC MACHINE LEARNING</strong></h2>
<div class="bq">
<p class="center"><em class="calibre11">The bewilderments of the eyes are of two kinds, and arise from two causes, either from coming out of the light or from going into the light.</em></p>
<p class="center1">—Socrates</p>
</div>
<div class="image"><img alt="Image" src="../images/common.jpg" class="calibre6"/></div>
<p class="noindent">The topic of this chapter is a rather new approach to solving scientific problems through computation. Much of the recent development in the field of scientific machine learning (SciML) has taken place within the Julia ecosystem and has been led by researchers using Julia in science. Relatively little has been published explaining how to apply the new techniques in a form digestible by those not conversant with machine learning jargon. I hope to fill at least part of that gap here through the selection of simple but concrete examples that clarify the concepts involved so that readers can apply them to a variety of problems.</p>
<p class="indent">Scientific machine learning is not machine learning. <em class="calibre11">Machine learning (ML)</em> is a branch of artificial intelligence in which computers train themselves (usually guided by human supervision), by practicing on a large corpus <span epub:type="pagebreak" id="page_406"/>of data, to recognize patterns and make classifications. ML techniques are applied to such problems as detecting fraudulent financial transactions or trying to guess what movie you want to watch next. The training replaces the traditional coding of a specific model or algorithm.</p>
<p class="indent"><em class="calibre11">SciML</em> extracts several key techniques from ML and applies them to a different class of problem. In SciML, we assume that the system we’re studying is described by a particular model, often expressed as a set of differential equations. Certain parameters or other aspects of the model, however, are unknown. If we have data about how the system behaves, SciML techniques allow us to infer the values of these unknown parameters efficiently.</p>
<h3 class="h2" id="ch13lev1"><strong class="calibre2">Automatic Differentiation in a Physics Problem</strong></h3>
<p class="noindent">Along with concepts from statistics and probability theory, SciML borrows <em class="calibre11">automatic differentiation</em> from ML. This technique is critical to both ML and SciML. Traditionally, differentiation is a mathematical procedure from calculus that finds the slope of a curve (in one dimension) or a surface (in two or more dimensions). We call a derivative of a surface, which involves dealing with several variables, a <em class="calibre11">gradient</em>. If your kitchen sink is installed correctly, the negative gradient of its surface points toward the drain at every point, so that when you pull the plug all the water drains out and you’re not left with any puddles.</p>
<p class="indent">Automatic differentiation is the calculation of a derivative or gradient of a function expressed in a programming language, rather than in mathematical notation. The programmed function can be the direct translation of a mathematical expression. Often, when the expression is complicated, its analytic derivative will involve many terms and be expensive to calculate in the traditional way. Automatic differentiation can be faster. We can even use automatic differentiation to calculate gradients that have no analytic form: the function being differentiated can include nearly any computation, including those not expressible in mathematical notation. Automatic differentiation is not numerical differentiation; it’s not a finite-difference calculation. Neither is it symbolic differentiation, as explored in <a href="ch12.xhtml" class="calibre10">Chapter 12</a>. It applies knowledge of calculus, such as the chain rule for derivatives, with knowledge of the derivatives of specific functions and numerical techniques to differentiate efficiently and accurately.</p>
<p class="indent">ML uses automatic differentiation to guide its models in the direction of the correct solutions, and it’s used within the SciML machinery in a similar way. We can also use it explicitly for efficient calculations of derivatives in mathematical models, as shown in “Calculating Forces from Potentials” on <a href="ch13.xhtml#ch13lev1sec2" class="calibre10">page 408</a>.</p>
<h4 class="h3" id="ch13lev1sec1"><strong class="calibre2"><em class="calibre4">Differentiating with ForwardDiff</em></strong></h4>
<p class="noindent">We can meet our automatic differentiation needs in this chapter with the <code>derivative()</code> function from the <code>ForwardDiff</code> package, which I’ll assume has been imported in the following examples. Its use is simple: we supply a <span epub:type="pagebreak" id="page_407"/>function and a value, and <code>ForwardDiff.derivative()</code> returns the derivative of the function evaluated at the supplied value:</p>
<pre class="calibre13">julia&gt; <span class="codestrong">ForwardDiff.derivative(sin, 0.0)</span>
1.0</pre>
<p class="noindent">The result is correct: the derivative of sin(<em class="calibre11">x</em>) is cos(<em class="calibre11">x</em>), and cos(0) = 1.</p>
<p class="indent">The <code>ForwardDiff.derivative()</code> function can also handle functions defined in Julia that may contain almost any type of computation:</p>
<pre class="calibre13">julia&gt; <span class="codestrong">function fdst(x)
           (x - floor(x))^2 / ceil(x)
       end</span>
fdst (generic function with 1 method)

julia&gt; <span class="codestrong">plot(fdst, 0, 5; label="fdst(x)", xlabel="x", lw=2);</span>

julia&gt; <span class="codestrong">plot!(x -&gt; ForwardDiff.derivative(fdst, x); label="fdst'(x)", lw=2, ls=:dash)</span></pre>
<p class="indent">The <code>floor()</code> and <code>ceil()</code> functions round their arguments to the closest smaller or larger whole number. The <code>fdst()</code> function defined in the example is not something that we can look up in a table of derivatives or handle with the familiar techniques of calculus, but Julia’s automatic differentiation routine calculates the derivative correctly. <a href="ch13.xhtml#ch13fig1" class="calibre10">Figure 13-1</a> shows the result.</p>
<div class="image1"><img alt="Image" id="ch13fig1" src="../images/ch13fig01.jpg" class="calibre6"/></div>
<p class="figcap"><em class="calibre11">Figure 13-1: Automatic differentiation of a strange function</em></p>
<p class="indent">In <a href="ch13.xhtml#ch13fig1" class="calibre10">Figure 13-1</a>, the legend uses a prime to indicate a derivative. The dashed line shows the result of the automatic differentiation function, which is not troubled by the existence of discontinuities.</p>
<h4 class="h3" id="ch13lev1sec2"><span epub:type="pagebreak" id="page_408" class="calibre1"/><strong class="calibre2"><em class="calibre4">Calculating Forces from Potentials</em></strong></h4>
<p class="noindent">In physics, the force on a body is the negative gradient of its potential energy. If the potential energy depends on only one variable, this is simply the negative of its derivative with respect to that variable. Let’s revisit the finite-angle pendulum problem from <a href="ch09.xhtml" class="calibre10">Chapter 9</a>.</p>
<p class="indent"><a href="ch13.xhtml#ch13lis1" class="calibre10">Listing 13-1</a> recapitulates the problem in one place for convenience.</p>
<pre class="calibre13">   using ForwardDiff
   using DifferentialEquations
   const L = 1.0
   const g = 9.8
   const m = 1.0

<span class="ent">➊</span> function ppot(θ)
       return m*g*L*(1-cos(θ))
   end

   function pendulum!(du, u, p, t)
       L, g = p
       θ, ω = u
       du[1] = ω
    <span class="ent">➋</span> du[2] = -ForwardDiff.derivative(ppot, u[1])/m
   end

   function pendulumF!(du, u, p, t)
       L, g = p
       θ, ω = u
       du[1] = ω
       du[2] = -g/L * sin(θ)
   end
   p = [L, g] #  &lt;- Parameters

   u0 = [deg2rad(175), 0]
              #  θ   ω  &lt;- Initial conditions

   tspan = (0, 20)

   prob = ODEProblem(pendulum!, u0, tspan, p)
   probF = ODEProblem(pendulumF!, u0, tspan, p)

<span class="ent">➌</span> sol5d = solve(prob)
   sol5dF = solve(probF)</pre>
<p class="list" id="ch13lis1"><em class="calibre11">Listing 13-1: Revisiting the finite-angle pendulum</em></p>
<p class="indent"><a href="ch13.xhtml#ch13lis1" class="calibre10">Listing 13-1</a> contains something extra, however: the <code>ppot()</code> function, which gives the gravitational potential energy of the pendulum as a function of height <span class="ent">➊</span>. The <code>pendulum!()</code> function now sets up the problem using <span epub:type="pagebreak" id="page_409"/>automatic differentiation to calculate the (negative) derivative of the potential <span class="ent">➋</span> to derive the force, rather than using the force function directly. A second function, <code>pendulumF!()</code>, sets up the problem as before, using the force function. We proceed just as we did in <a href="ch09.xhtml" class="calibre10">Chapter 9</a>, but we find two numerical solutions: once using the potential <span class="ent">➌</span> and again using the force.</p>
<p class="indent"><a href="ch13.xhtml#ch13fig2" class="calibre10">Figure 13-2</a> compares the two methods of solution.</p>
<div class="image1"><img alt="Image" id="ch13fig2" src="../images/ch13fig02.jpg" class="calibre6"/></div>
<p class="figcap"><em class="calibre11">Figure 13-2: The finite-angle pendulum computed two ways</em></p>
<p class="indent">The two solutions agree exactly. Clearly it wasn’t necessary to reach for the <code>ForwardDiff</code> package to handle this problem, but we did so to verify that it works as expected. When applying a new technique, it’s essential to test it on a relatively simple problem with a known solution first, to gain confidence in our understanding of how to use it, and to confirm that we understand how it works.</p>
<p class="indent">Physicists usually think in terms of potentials rather than forces, so when conducting numerical experiments, we’re more likely to try different potentials rather than tweak the force function directly. Having a solution program that differentiates the potential for us is more convenient than deriving a new force field at each iteration. Also, the potential functions we work with have a simpler form than the force functions derived from them. This is the case in the next example.</p>
<p class="indent">Imagine that we’ve discovered a new particle with a potential that is strongly repulsive at short range, has a well at a particular distance, and is weakly repulsive at longer ranges. The potential</p>
<div class="image"><img alt="Image" src="../images/409math.jpg" class="calibre6"/></div>
<p class="noindent"><span epub:type="pagebreak" id="page_410"/>where <em class="calibre11">r</em> is the distance from the particle, has these properties, as shown in <a href="ch13.xhtml#ch13fig3" class="calibre10">Figure 13-3</a>.</p>
<div class="image"><img alt="Image" id="ch13fig3" src="../images/ch13fig03.jpg" class="calibre6"/></div>
<p class="figcap"><em class="calibre11">Figure 13-3: The potential of an imaginary particle</em></p>
<p class="indent"><a href="ch13.xhtml#ch13fig3" class="calibre10">Figure 13-3</a> shows the potential well at <em class="calibre11">r</em> ≈ 1.3. This is a location at which an interacting particle can be trapped if it lacks the energy to escape.</p>
<p class="indent">The system will contain two of these particles, fixed at <em class="calibre11">r</em> = 0 and <em class="calibre11">r</em> = 20. We’ll place a moving particle between them, and use units where its mass is 1. <a href="ch13.xhtml#ch13fig4" class="calibre10">Figure 13-4</a> shows the combined potential of the two fixed particles.</p>
<div class="image"><img alt="Image" id="ch13fig4" src="../images/ch13fig04.jpg" class="calibre6"/></div>
<p class="figcap"><em class="calibre11">Figure 13-4: The total potential of two imaginary particles</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_411"/>We’ll insert the moving particle into the system at <em class="calibre11">r</em> = 5<em class="calibre11">.</em>0, with an initial velocity of 0.2035. This positive velocity starts the particle moving to the right at <em class="calibre11">t</em> = 0. With a zero initial velocity, it would oscillate within the shallow well centered on <em class="calibre11">x</em> = 10, between <em class="calibre11">x</em> = 5 and <em class="calibre11">x</em> = 15. Its particular initial velocity gives the particle barely enough energy to surmount the potential hill near <em class="calibre11">x</em> = 16.</p>
<p class="indent">In <a href="ch13.xhtml#ch13lis2" class="calibre10">Listing 13-2</a>, we proceed as in the revisited pendulum problem in <a href="ch13.xhtml#ch13lis1" class="calibre10">Listing 13-1</a>.</p>
<pre class="calibre13">   using DifferentialEquations
   using ForwardDiff

   U(r) = exp(-(exp((-0.4*(r-1)^2))))/sqrt(r+1)

   function particle!(du, u, p, t)
       x1, x2 = p
       r, v = u
       du[1] = v
    <span class="ent">➊</span> du[2] = -ForwardDiff.derivative(U, abs(r - x1)) +
                ForwardDiff.derivative(U, abs(r - x2))
   end

<span class="ent">➋</span> p = [0.0, 20.0]
<span class="ent">➌</span> u0 = [5.0, 0.2035]

   tspan = (0, 650)

   prob = ODEProblem(particle!, u0, tspan, p)
   sol = solve(prob)</pre>
<p class="list" id="ch13lis2"><em class="calibre11">Listing 13-2: Solving for the motion between two imaginary particles</em></p>
<p class="indent">We derive the forces by applying automatic differentiation to the potential function, which is the sum of the two contributions from the two fixed particles <span class="ent">➊</span>, evaluating the derivatives at the distance from each particle. The <code>p</code> array holds the positions of these two particles <span class="ent">➋</span>, and the <code>u0</code> array contains the initial position and initial velocity of the moving particle <span class="ent">➌</span>. After establishing a time span for the solution, we define the ordinary differential equation (ODE) problem and store its solution in <code>sol</code> as before.</p>
<p class="indent">A first attempt at a solution is shown in <a href="ch13.xhtml#ch13fig5" class="calibre10">Figure 13-5</a>, which shows the position of the moving particle as a function of time.</p>
<div class="image"><img alt="Image" id="ch13fig5" src="../images/ch13fig05.jpg" class="calibre6"/></div>
<p class="figcap"><em class="calibre11">Figure 13-5: An inaccurate solution</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_412"/>We extract the position variable from the solution as explained in <a href="ch09.xhtml" class="calibre10">Chapter 9</a>.</p>
<p class="indent">Scientists should always cast a critical eye over purported numerical solutions to differential equations. Our first instinct should be to examine the output of the solver in light of everything we know about how the solution should behave. In this case, we know that the solution should be periodic, as nothing in the definition of the problem can add or remove energy. The result in <a href="ch13.xhtml#ch13fig5" class="calibre10">Figure 13-5</a> is clearly not accurately periodic.</p>
<p class="indent">The <code>DifferentialEquations</code> package provides many options for solution methods and exposes several parameters for tweaking the behavior of the solvers. See “Further Reading” on <a href="ch13.xhtml#fur13" class="calibre10">page 427</a> for a link to the relevant part of the documentation. As the differential equation set up in <a href="ch13.xhtml#ch13lis2" class="calibre10">Listing 13-2</a> is not of a difficult type, we can probably stick with the default solver. The accuracy issue is most likely caused by the nature of the potential and the initial velocity, which, as mentioned, is near a critical value that determines whether the particle will surmount a local potential maximum. This suggests that simply applying an error bound may be sufficient. The <code>reltol</code> parameter, supplied as a keyword argument to <code>solve()</code>, adjusts the adaptive timestepping as needed to limit the local error to the value that we supply, as described in “Parametric Instability” on <a href="ch09.xhtml#ch09lev1sec19" class="calibre10">page 300</a>. Its default is 0.001, which is probably not stringent enough for this problem. Smaller changes in the initial velocity have a large effect on the particle’s motion. If we try again using <code>sol = solve(prob; reltol=1e-6)</code>, we get the solution shown in <a href="ch13.xhtml#ch13fig6" class="calibre10">Figure 13-6</a>.</p>
<div class="image"><img alt="Image" id="ch13fig6" src="../images/ch13fig06.jpg" class="calibre6"/></div>
<p class="figcap"><em class="calibre11">Figure 13-6: An accurate solution</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_413"/>The new solution appears to be accurately periodic. Furthermore, reducing <code>reltol</code> further doesn’t change the solution, which supplies some reassurance that it’s converged to the right answer.</p>
<p class="indent">The derivative of U happens to be</p>
<div class="image"><img alt="Image" src="../images/413math.jpg" class="calibre6"/></div>
<p class="noindent">which would be somewhat more annoying to work with directly.</p>
<h3 class="h2" id="ch13lev2"><strong class="calibre2">Probabilistic Programming</strong></h3>
<p class="noindent">This section introduces the <code>Turing</code> package through several examples. This package allows us to infer likely causes given observed effects. We’ll assume some comfort with several of the ideas discussed in <a href="ch10.xhtml" class="calibre10">Chapter 10</a>—in particular, probability and probability distributions. We’ll need to be familiar with these ideas to understand the output from <code>Turing</code> and to interpret its results.</p>
<h4 class="h3" id="ch13lev1sec3"><strong class="calibre2"><em class="calibre4">Testing for Fairness of a Coin</em></strong></h4>
<p class="noindent">This simple example introduces the basic concepts and procedures for using <code>Turing</code> in probabilistic programming.</p>
<p class="indent">Suppose we flip a coin <em class="calibre11">L</em> number of times and observe that we get a total of <em class="calibre11">Nheads</em> heads. We want to assess whether what we observed shows that the coin is <em class="calibre11">fair</em> or not, where <em class="calibre11">fair</em> means that the probability of coming up heads is 1/2, or very close to it. This is the type of question that probabilistic <span epub:type="pagebreak" id="page_414"/>programming claims to be able to answer: given an effect, or a set of observations, what was the cause? Here the effect is the proportion of heads, and the cause is the probability of heads.</p>
<div class="note">
<p class="notet"><strong class="calibre2"><span class="notes">NOTE</span></strong></p>
<p class="notep"><em class="calibre11">I’m cognizant that the foregoing brief analysis may not please everyone, but wish to avoid becoming mired in metaphysics. The actual causes of our observations will be the physical details of the coin’s construction and the method of tossing. The probability of heads represents a summary of the cumulative effect of this myriad of unknown details; the description of cause as a probability reflects our incomplete knowledge.</em></p>
</div>
<p class="indent">The first step in using <code>Turing</code> is to construct a probabilistic model describing the probability distributions of each of the random variables in the problem. For some variables, these distributions are unknown, in which case we need to assume something reasonable, such as a uniform or normal distribution that includes all possible values, perhaps centered on the value that we think is most likely. For others, the description of the problem implies a particular distribution, one that is usually parameterized by observations or the values of some of the other variables.</p>
<p class="indent">In this example we have one unknown random variable, <em class="calibre11">Pheads</em>. We’ll assume that it can have any value from 0 to 1, uniformly distributed. This assumption means that we don’t have any a priori belief about the nature of the coin. If we had reason to think that it was almost certainly fair, we could instead assert that it was normally distributed with a mean of 1/2 and a small variance.</p>
<p class="indent">In <code>Turing</code> models, we represent assertions about the distributions of random variables using the <code>~</code> operator. Our assumption about the distribution of the probability of heads takes the form <code>Pheads ~ Uniform(0, 1)</code>. The <code>Uniform()</code> function comes from <code>Distributions.jl</code>, which <code>Turing</code> automatically imports (see “Distributions” on <a href="ch10.xhtml#ch10lev7" class="calibre10">page 321</a>).</p>
<p class="indent"><a href="ch13.xhtml#ch13lis3" class="calibre10">Listing 13-3</a> shows the complete <code>Turing</code> model.</p>
<pre class="calibre13">julia&gt; <span class="codestrong">using Turing, StatsPlots</span>

julia&gt; <span class="codestrong">@model function coin(Nheads, L)
           Pheads ~ Uniform(0, 1)
        <span class="ent">➊</span> Nheads ~ Binomial(L, Pheads)
       end;</span></pre>
<p class="list" id="ch13lis3"><em class="calibre11">Listing 13-3: A simple probabilistic program</em></p>
<p class="indent">After importing <code>Turing</code> and <code>StatsPlots</code>, which will be useful for visualizing the output, we use the <code>@model</code> macro from <code>Turing</code> to define the model. We can call the function that <code>@model</code> acts on anything we want; the macro understands the <code>~</code> operator and transforms the function into a <code>Turing</code> model.</p>
<p class="indent">The inputs are the observed number of heads and <code>L</code>, the total number of flips. As mentioned, we assume a uniform distribution for <code>Pheads</code>, the quantity that we’re trying to infer. The number of heads observed when we flip a coin <code>L</code> times is a random variable that we know has a binomial distribution <span epub:type="pagebreak" id="page_415"/>parametrized by <code>L</code> and <code>Pheads</code> <span class="ent">➊</span> (see “Further Reading” on <a href="ch13.xhtml#fur13" class="calibre10">page 427</a> for a link to a brief introduction).</p>
<p class="indent">To understand, in outline, how <code>Turing</code> carries out its inductive process to infer the unknowns in the model (<code>Pheads</code> in this case) from the observations, we’ll imagine how we might do it manually. For the simple problem here, we might choose a series of <code>Pheads</code> values from 0 to 1, either deterministically or randomly, perhaps using <code>rand()</code>. For each of these values for <code>Pheads</code>, we can calculate the expectation value, or mean, of <code>Nheads</code> from its binomial distribution. The expectation value closest to the observed value of <code>Nheads</code> is our inferred value for <code>Pheads</code>.</p>
<p class="indent">This inference procedure would be fairly efficient because we have a simple formula for the mean of the binomial distribution. If we were dealing with less tractable distributions, including ones depending on many parameters, each with its own distribution, the only way to extract the expectation value would be through the numerical experiment of sampling from the distribution. As pointed out in “Random Numbers in Julia” on <a href="ch10.xhtml#ch10lev2" class="calibre10">page 307</a>, the <code>rand()</code> function allows us to sample directly from a distribution. However, as we’ll see soon, a more realistic problem may include thousands of random variables and thousands of distributions. Naive sampling from each of them would take a prohibitively long time.</p>
<p class="indent">This is the problem that <code>Turing</code> solves. It allows us to do no more than tell it what the probability distributions are, then it samples from them efficiently, calculates expectation values as needed, and reports the results and their uncertainties and error estimates. We won’t go into the details of how <code>Turing</code> accomplishes this feat, except to say that it implements the technology of Markov chain Monte Carlo (MCMC) sampling, a starting point for readers who are interested in investigating the theoretical background.</p>
<p class="indent">To tell <code>Turing</code> to generate a report about its inferences, we issue one command using its <code>sample()</code> function:</p>
<pre class="calibre13">julia&gt; <span class="codestrong">flips = sample(coin(60, 100), SMC(), 1000)</span></pre>
<p class="indent">Here <code>coin()</code> is the model function from <a href="ch13.xhtml#ch13lis3" class="calibre10">Listing 13-3</a>. Its arguments are the number of heads and the total number of flips—in this case 60 heads out of 100 coin tosses. The next argument selects a sampling strategy from among the handful supplied by the <code>Turing</code> package. The initials <code>SMC</code> stand for sequential Monte Carlo, which performs well on simple problems. The choice of sampler can be a matter of trial and error; different samplers are best suited to different problems. (See “Further Reading” on <a href="ch13.xhtml#fur13" class="calibre10">page 427</a> for links to some documentation for <code>Turing</code>’s samplers.) The final argument, <code>1000</code>, is the number of sampling experiments to conduct. Each one produces an estimate for <code>Pheads</code>, and <code>Turing</code> reports the mean of these estimates, which is its most likely value, as shown in <a href="ch13.xhtml#ch13lis4" class="calibre10">Listing 13-4</a>.</p>
<pre class="calibre13">Chains MCMC chain (1000×3×1 Array{Float64, 3}):

Log evidence      = -4.5014682572661195
Iterations        = 1:1:1000
<span epub:type="pagebreak" id="page_416"/>Number of chains  = 1
Samples per chain = 1000
Wall duration     = 12.73 seconds
Compute duration  = 12.73 seconds
parameters        = Pheads
internals         = lp, weight

Summary Statistics
  parameters      mean       std   naive_se      mcse        ess      rhat   ess_per_sec
      Symbol   Float64   Float64    Float64   Float64    Float64   Float64       Float64

      Pheads    0.6024    0.0460     0.0015    0.0023   410.5088    1.0002       32.2499

Quantiles
  parameters      2.5%     25.0%     50.0%     75.0%     97.5%
      Symbol   Float64   Float64   Float64   Float64   Float64

      Pheads    0.5058    0.5719    0.6092    0.6319    0.6862</pre>
<p class="list" id="ch13lis4"><em class="calibre11">Listing 13-4: The report from</em> <span class="codeitalic">Turing</span></p>
<p class="indent">The report, which appears after 12.73 seconds on my laptop, contains a lot of information, but only a few numbers are essential for us. Under <code>Summary Statistics</code>, the <code>Symbol</code>s are the random variables whose inferred values we want: in this case, only <code>Pheads</code>. The best guess that <code>Turing</code> has for <code>Pheads</code> is 0.6024. Another number to keep an eye on is <code>rhat</code>, which is 1.0002 in this example. If this number is far from 1.0, the sampling process did not converge properly, and we need to try a different sampler or alter the controls passed to the sampler, if it’s one that accepts parameters.</p>
<p class="indent">Now we can think about addressing the question implied in the title of this section: is the coin fair? We can gain some insight by looking at the distribution of the 1,000 inferences for <code>Pheads</code> resulting from the sampling procedure. The <code>histogram()</code> function (see “Distributions” on <a href="ch10.xhtml#ch10lev7" class="calibre10">page 321</a>) gains the power to plot this with a simple call to <code>histogram(flips; normalize=true)</code> courtesy of the <code>Turing</code> and <code>StatsPlots</code> packages. We’ll plot the histogram with a normal distribution curve on the same graph with the following:</p>
<pre class="calibre13">julia&gt; <span class="codestrong">histogram(flips; normalize=true)</span>
julia&gt; <span class="codestrong">plot!(Normal(0.6024, 0.0460); lw=2)</span></pre>
<p class="indent">The parameters in the normal distribution, plotted in the second line, are the mean and standard deviation taken from the report in <a href="ch13.xhtml#ch13lis4" class="calibre10">Listing 13-4</a>. <a href="ch13.xhtml#ch13fig7" class="calibre10">Figure 13-7</a> shows the result, where we can see that the sampling distribution from <code>Turing</code> is quite a good approximation to the normal distribution with the parameters that it reports.</p>
<div class="image"><img alt="Image" id="ch13fig7" src="../images/ch13fig07.jpg" class="calibre6"/></div>
<p class="figcap"><em class="calibre11">Figure 13-7: The distribution of inferences for</em> <span class="codeitalic">Pheads</span></p>
<p class="indent"><span epub:type="pagebreak" id="page_417"/>Why should the distribution of the mean values of <code>Pheads</code> be normal? After all, we set <code>Pheads</code> up with a uniform distribution in the model. The answer is that the distribution in <a href="ch13.xhtml#ch13fig7" class="calibre10">Figure 13-7</a> is the distribution of <em class="calibre11">mean values</em> of the random variable <code>Pheads</code>. As demonstrated in “The Normal Distribution” on <a href="ch10.xhtml#ch10lev1sec3" class="calibre10">page 323</a> (using the same uniform distribution), the distribution of the means will be normal (Gaussian). This will be true regardless of the underlying distribution of the variable itself, which is an important theorem in probability theory and the fundamental reason for the ubiquity of the normal distribution.</p>
<p class="indent">We can apply any criterion we choose to decide whether this coin is fair after examining the sampling results. Although the most likely value for the probability of heads is very close to 0.6, strongly suggesting that we have a biased coin, it’s <em class="calibre11">possible</em> that the coin is fair. We can estimate the probability that <code>Pheads</code> is 1/2 directly from the normalized histogram. The two bars surrounding 0.5 on the horizontal axis have an area of about (0.52 - 0.48) × 0.8 = 0.32, yielding a probability of 3.2 percent that the coin is fair. The value of 0.8 comes from visually estimating the average height of the two relevant bins. We can also calculate this from the normal distribution:</p>
<pre class="calibre13">julia&gt; <span class="codestrong">cdf(Normal(0.6024, 0.0460), 0.52) - cdf(Normal(0.6024, 0.0460), 0.48)</span>
0.032725277247186525</pre>
<p class="indent">The <code>cdf()</code> function, which stands for <em class="calibre11">cumulative density function</em>, returns the integral of the distribution supplied in the first argument from negative infinity to the value supplied in the second argument. Therefore, to extract the probability that a random variable governed by the distribution lies between two values, we need merely to subtract the results from two calls to <code>cdf()</code>. The value of 3.3 percent agrees pretty well with our estimate for the same interval from the histogram.</p>
<p class="indent">This coin has only a 3.3 percent chance of being fair. Is that strong enough evidence to convict it of bias? That’s up to us.</p>
<p class="indent"><span epub:type="pagebreak" id="page_418"/>Flipping the coin 100 times provides pretty strong evidence of its shady character. Intuitively, we understand that if we had flipped it only 10 times, and happened to observe six heads, that wouldn’t be strong evidence of any non-fairness in the coin. Similarly, an observation of 600 heads after tossing the coin 1,000 times would be pretty conclusive.</p>
<p class="indent">We can see the results of these two scenarios by calling <code>sample()</code> twice and passing the result directly to <code>histogram()</code>:</p>
<pre class="calibre13">julia&gt; <span class="codestrong">histogram(sample(coin(6, 10), SMC(), 1000); normalize=:probability, fc=:lightgray)</span>
julia&gt; <span class="codestrong">histogram!(sample(coin(600, 1000), SMC(), 1000); normalize=:probability, fc=:gray)</span></pre>
<p class="indent">This is a quick way to compare the distributions when we’re not interested in the detailed report.</p>
<div class="note">
<p class="notet"><strong class="calibre2"><span class="notes">NOTE</span></strong></p>
<p class="noindent"><em class="calibre11">Because the results returned by</em> <span class="codeitalic1">sample()</span> <em class="calibre11">are generated partly through random sampling, the details will be different every time. Everyone running the code samples in this section will observe slightly different distributions and means, although the over-all conclusions should be invariant. In an important problem, a good practice would be to run more than one sampling experiment, try different samplers, and perhaps vary some of the details in the model concerning assumed distributions.</em></p>
</div>
<p class="indent"><a href="ch13.xhtml#ch13fig8" class="calibre10">Figure 13-8</a> shows the result.</p>
<div class="image"><img alt="Image" id="ch13fig8" src="../images/ch13fig08.jpg" class="calibre6"/></div>
<p class="figcap"><em class="calibre11">Figure 13-8: Weak and strong evidence</em></p>
<p class="indent">The lighter histogram, showing the inferences from 10 flips, clearly indicates that we have no evidence for bias in the coin. It’s about as likely that <code>Pheads</code> is 1/2 as it is 6/10. However, the observation using 1,000 flips is unambiguous: 600 heads in that experiment makes it nearly impossible for the coin to be fair. The darker gray overlay of the second histogram shows a narrow distribution around <code>Pheads</code> = 0.6.</p>
<h4 class="h3" id="ch13lev1sec4"><span epub:type="pagebreak" id="page_419" class="calibre1"/><strong class="calibre2"><em class="calibre4">Inferring Model Parameters from Series Observations</em></strong></h4>
<p class="noindent">In most applications of probabilistic programming, scientists are interested in inferring the causes of a series of observations taken over time, rather than merely a single number. We can extend the approach in the previous section to handle time series by considering the data gathered at each point in time to be a separate measurement with a distribution around some predicted value. The values can be predictions from nearly any type of model, as long as we can express it as a Julia function.</p>
<h5 class="h4" id="ch13sec1sec1"><strong class="calibre2">A Simple Mathematical Model</strong></h5>
<p class="noindent">To demonstrate the approach, we’ll first consider the problem of fitting a pair of parameters in a simple expression that we assume to be the cause of a series of observations. The model is a sine function and the two unknown parameters are its amplitude <code>A</code> and its frequency <code>f</code>, as shown in <a href="ch13.xhtml#ch13lis5" class="calibre10">Listing 13-5</a>.</p>
<pre class="calibre13">const t = 0:π/50:4π;
A0 = 3.4; f0 = 2.7;
data = A0*sin.(f0*t) + 0.5 .* randn(length(t));

@model function wave(data)
    f ~ Uniform(0, 3)
    A ~ Uniform(0, 4)
 <span class="ent">➊</span> prediction = A*sin.(f*t)
    for i in eachindex(t)
     <span class="ent">➋</span> data[i] ~ Normal(prediction[i], 0.5)
    end
end;</pre>
<p class="list" id="ch13lis5"><em class="calibre11">Listing 13-5: The sine function with unknown frequency and amplitude as a model</em></p>
<p class="indent">After defining a series of times, we pick values for the <code>A</code> (amplitude) and <code>f</code> (frequency) parameters. We use these to generate some simulated observations, containing normally distributed errors, that we store in <code>data</code>. Our plan is to pretend we don’t know the values of <code>A</code> and <code>f</code> and to use the data, along with the assumed sinewave dependence, to infer their values.</p>
<p class="indent">In the model, we assert a priori uniform distributions for the frequency and amplitude that establish limits for their possible values. For each possible set of values, we have a prediction <span class="ent">➊</span> for the time series that would result. We consider the data passed to the model to be a set of physical measurements, so we assume that the observation at each time is normally distributed around the “true” (predicted) value at that time, with a standard deviation of 0.5 <span class="ent">➋</span>.</p>
<p class="indent">The inference through sampling proceeds as in the previous section. However, the <code>SMC</code> sampler seems to work poorly for this class of problems. The <code>MH</code> sampler (for Metropolis-Hastings) works far more reliably, and it’s <span epub:type="pagebreak" id="page_420"/>quite fast as well, but is a poor performer in other problems. (As mentioned earlier, we may need to experiment with a variety of sampling algorithms and their input parameters.) <a href="ch13.xhtml#ch13lis6" class="calibre10">Listing 13-6</a> shows the sampling command and its truncated output.</p>
<pre class="calibre13">julia&gt; <span class="codestrong">wavesample = sample(wave(data), MH(), 1000)</span>
Chains MCMC chain (1000×3×1 Array{Float64, 3}):

Iterations        = 1:1:1000
Number of chains  = 1
Samples per chain = 1000
Wall duration     = 0.92 seconds
Compute duration  = 0.92 seconds
parameters        = f, A
internals         = lp

Summary Statistics
  parameters      mean       std   naive_se      mcse       ess      rhat   ess_per_sec
      Symbol   Float64   Float64    Float64   Float64   Float64   Float64       Float64

           f    2.6876    0.0247     0.0008    0.0039    8.9062    1.1077        9.7230
           A    3.4323    0.3867     0.0122    0.0681    2.5378    2.1700        2.7706</pre>
<p class="list" id="ch13lis6"><em class="calibre11">Listing 13-6: Inferring the values of parameters</em></p>
<p class="indent">The sampler returns reasonable results in less than one second. This is impressive, considering that the algorithm is sampling two parameters 1,000 times and using 200 data points, each with its own distribution, to infer the final distributions of <code>A</code> and <code>f</code> and their expectation values.</p>
<p class="indent">Let’s visualize the inferred solution using the returned means of <code>A</code> and <code>f</code> superimposed on the simulated data and what we know is the true solution:</p>
<pre class="calibre13">julia&gt; <span class="codestrong">plot(t, A0*sin.(f0*t); lw=2, legend=false, ylabel="A(t)", xlabel="t")</span>
julia&gt; <span class="codestrong">plot!(t, data)</span>
julia&gt; <span class="codestrong">A1 = 3.4323; f1 = 2.6876;</span>
julia&gt; <span class="codestrong">plot!(t, A1*sin.(f1*t); ls=:dot)</span></pre>
<p class="indent">In the first two lines, we plot the model with a thick line and the noisy data with a thinner line. The final plot command plots a sinewave using the inferences for <code>A</code> and <code>f</code> as a dotted line. <a href="ch13.xhtml#ch13fig9" class="calibre10">Figure 13-9</a> shows the combined plot.</p>
<div class="image"><img alt="Image" id="ch13fig9" src="../images/ch13fig09.jpg" class="calibre6"/></div>
<p class="figcap"><em class="calibre11">Figure 13-9: Model parameters recovered from noisy data</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_421"/><a href="ch13.xhtml#ch13fig9" class="calibre10">Figure 13-9</a> shows how the correct signal was recovered from the noisy observations. The periodic nature of the model means that the slight error in the inferred frequency will cause the curves to diverge further at later times.</p>
<h5 class="h4" id="ch13sec1sec2"><strong class="calibre2">An ODE Model</strong></h5>
<p class="noindent">The model used for generating the prediction need not be a known function; it can be a set of differential equations. This is possible because <code>Turing</code> and <code>DifferentialEquations</code> are composable, another benefit of Julia’s type system. The combination is immensely powerful, and opens up new arenas for research. In science our models often take the form of differential equations that encode, in general terms, our hypotheses about how the system works. Some of the details of the system may remain as parameters with unknown, or partially known, values. Probabilistic programming, using the general procedure outlined in “Inferring Model Parameters from Series Observations” on <a href="ch13.xhtml#ch13lev1sec4" class="calibre10">page 419</a>, allows us to infer the most likely values of these parameters and then check, quantitatively, how well our purported model performs.</p>
<p class="indent"><span epub:type="pagebreak" id="page_422"/>For example, we may measure the trajectory of a cannonball and think we know that its path is governed by Newton’s laws of motion and the forces of gravity and air resistance. But we might not know the correct value of the gravitational acceleration on our planet or the coefficient of drag for the cannonball in its atmosphere. Assuming our differential equations are correct, we can use <code>Turing</code> and <code>DifferentialEquations</code> to infer the values of those two numbers from the observed trajectory, and then plug them back into the model to see whether we can reproduce the data. This approach eliminates a huge amount of trial and error, and it lets us iterate fluidly over variations in our models.</p>
<p class="indent">Returning to the parametric instability problem from <a href="ch09.xhtml" class="calibre10">Chapter 9</a>, let’s go backward: assume that we know we have a pendulum in a gravitational field, with a varying string length, and that we know the values for gravity, the pendulum mass, and the mean length of the string, but that the frequency and amplitude of the oscillation in the string’s length are unknown. We will, however, assume that the function defining that oscillation is a sin(<em class="calibre11">t</em>), where, as before, <em class="calibre11">t</em> is time.</p>
<p class="indent">This example will show how we can work backward from data about the pendulum’s behavior to an estimate of the driving frequency and amplitude, using the assumption of the underlying physical model behind the data. Naively, we might approach this problem by solving the differential equation, using the techniques from <a href="ch09.xhtml" class="calibre10">Chapter 9</a>, multiple times, with various values of the unknown parameters, until we hit upon a solution that is close enough to the data. But this process will be computationally expensive and may not provide systematic knowledge of the uncertainty in the final result.</p>
<p class="indent"><a href="ch13.xhtml#ch13lis7" class="calibre10">Listing 13-7</a> shows the problem set up for solution by the <code>Differential</code> <code>Equations</code> package, assembled here for convenience from <a href="ch09.xhtml#ch9lis8" class="calibre10">Listings 9-8</a> and <a href="ch09.xhtml#ch9lis8" class="calibre10">9-9</a>.</p>
<pre class="calibre13">   using DifferentialEquations

   function pendulum!(du, u, p, t)
       L, g = p
       θ, ω = u
       du[1] = ω
       du[2] = -g/L(t) * sin(θ)
   end

<span class="ent">➊</span> g = 9.8; A = 0.2; f = 0.97
   L(t) = 1.0 + A * cos(f*2*sqrt(g)*t)
   p = [L, g]

   u0 = [deg2rad(5), 0]
   #  θ   ω  &lt;- Initial conditions

   tspan = (0, 80)

   sol = solve(ODEProblem(pendulum!, u0, tspan, p); saveat=0.1)</pre>
<p class="list" id="ch13lis7"><em class="calibre11">Listing 13-7: The parametrically driven pendulum</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_423"/>In this example, the driving frequency is set to 3 percent smaller than the parametric resonance frequency <span class="ent">➊</span>.</p>
<p class="indent">Before we proceed to apply <code>Turing</code> to this problem, let’s take a look at how varying the <code>f</code> and <code>A</code> values affects the results. First, we’ll plot the solution at resonance and slightly “detuned,” at 0.95 resonance:</p>
<pre class="calibre13">g = 9.8; A = 0.2; f = 1.0
L(t) = 1.0 + A * cos(f*2*sqrt(g)*t)
p = [L, g]
plot(solve(ODEProblem(pendulum!, u0, tspan, p)); idxs=1,
     legend=false, ylabel="A(t)")

f = 0.95
L(t) = 1.0 + A * cos(f*2*sqrt(g)*t)
p = [L, g]
plot!(solve(ODEProblem(pendulum!, u0, tspan, p)); idxs=1, lw=2)

annotate!(40, 1, ("Thin line:\nparametric forcing at resonance", 8))
annotate!(40, -0.5, ("Thick line:\n5% detuning", 8))</pre>
<p class="indent"><a href="ch13.xhtml#ch13fig10" class="calibre10">Figure 13-10</a> shows the two solutions.</p>
<div class="image"><img alt="Image" id="ch13fig10" src="../images/ch13fig10.jpg" class="calibre6"/></div>
<p class="figcap"><em class="calibre11">Figure 13-10: The parametrically driven pendulum at two driving frequencies</em></p>
<p class="indent">As <a href="ch13.xhtml#ch13fig10" class="calibre10">Figure 13-10</a> makes clear, the solution is quite sensitive to the driving frequency.</p>
<p class="indent">Changing the driving amplitude also has a strong effect on the solution. <a href="ch13.xhtml#ch13fig11" class="calibre10">Figure 13-11</a> shows the effect of two different forcing amplitudes at the same frequency.</p>
<div class="image"><img alt="Image" id="ch13fig11" src="../images/ch13fig11.jpg" class="calibre6"/></div>
<p class="figcap"><em class="calibre11">Figure 13-11: The parametrically driven pendulum at two driving amplitudes</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_424"/>Changing the forcing amplitude alone changes the envelope amplitude, the envelope timescale, and the frequency of the response.</p>
<p class="indent">When we compare these solutions, we can see that amplitude and frequency are interdependent. It’s not a simple matter to infer either driving parameter from the response. Let’s see how well probabilistic programming with <code>Turing</code> does with this problem. First we’ll define a model with <code>A</code> and <code>f</code> uniformly distributed within reasonable intervals:</p>
<pre class="calibre13">using Turing

@model function pdpen(observation)
    A ~ Uniform(0.0, 0.3)
    f ~ Uniform(0.9, 1.1)
    g = 9.8
    L(t) = 1.0 + A * cos(2*f*sqrt(g)*t)
    p = [L, g]
    prediction = Array(solve(ODEProblem(pendulum!, u0, tspan, p); saveat=0.1))[1, :]
    mstd = 0.1 * maximum(abs.(prediction))
    for i in eachindex(prediction)
        observation[i] ~ Normal(prediction[i], mstd)
    end
end</pre>
<p class="indent"><span epub:type="pagebreak" id="page_425"/>As in the simple sinewave model, we’ll generate some noisy simulated data from the solution returned by <code>DifferentialEquations</code> for given values of <code>A</code> and <code>f</code>, and then use the <code>Turing</code> model to try to infer those numbers from the data. The program in the following listing goes through this procedure for a small set of values for <code>A</code> and <code>f</code> and plots the inferred numbers with the known values:</p>
<pre class="calibre13">plot(; xrange=(0, 0.3), yrange=(0.9, 1.1), legend=false,
       xlabel="A", ylabel="f")
for A in range(0.02, 0.25; length=3)
    for f in range(0.95, 1.05; length=3)
     <span class="ent">➊</span> L(t) = 1.0 + A * cos(2*f*sqrt(g)*t)
        p = [L, g]
     <span class="ent">➋</span> sol = solve(ODEProblem(pendulum!, u0, tspan, p); saveat=0.1)
        mstd = 0.1 * maximum(abs.(Array(sol)[1, :]))
        observation = Array(sol)[1, :] + mstd * randn(length(sol))
     <span class="ent">➌</span> psamples = sample(pdpen(observation), MH(), 3000)
        scatter!([A], [f]; mc=:lightgray, ms=9)
        scatter!([mean(psamples[:A])], [mean(psamples[:f])];
                 xerror=std(psamples[:A]), yerror=std(psamples[:f]),
                 mc=:black, shape=:hexagon, ms=9)
    end
end
plot!()</pre>
<p class="indent">For each <code>A,f</code> pair, the program defines a forcing function <span class="ent">➊</span> and generates a solution <span class="ent">➋</span> from the differential equation. We tell the solver to save solution points at regular intervals using the <code>saveat</code> keyword argument and scale the simulated noise to the amplitude of the solution. The purpose of the solution is to generate the simulated noisy observations, which we then feed to the sampler <span class="ent">➌</span>. The next command places a mark on the <code>A</code>-<code>f</code> plane of the plot corresponding to the true values of <code>A</code> and <code>f</code>. Then we place a mark for the inferred values, with error bars taken from the standard deviations of the distributions returned by <code>sample()</code>.</p>
<p class="indent">We can access the sampling results for individual parameters using indexing on the name of the parameter as a symbol, so <code>psamples[:A]</code> is an array of all 3,000 values for <code>A</code> in the distribution generated by the sampler. The mean of this array is its expectation value (and the value printed in the report printed in the REPL). The <code>std()</code> function calculates the standard deviation of an array, returning the same number as in the report under <code>std</code>.</p>
<p class="indent"><a href="ch13.xhtml#ch13fig12" class="calibre10">Figure 13-12</a> shows the result.</p>
<div class="image"><img alt="Image" id="ch13fig12" src="../images/ch13fig12.jpg" class="calibre6"/></div>
<p class="figcap"><em class="calibre11">Figure 13-12: Inference of forcing parameters in the parametric pendulum</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_426"/>The experiment works well using 3,000 samples; however, the same program run with 1,000 samples performs distinctly worse. <a href="ch13.xhtml#ch13fig12" class="calibre10">Figure 13-12</a> shows that each inferred value is correct within its reported standard deviation, and most of those spreads are small. Despite the complexity and sensitivity of this problem, <code>Turing</code> and <code>DifferentialEquations</code> were able to work together to confirm the faithfulness of the model and accurately induce the correct model parameters. Doubtless with further tuning of the sampling method, we could improve the results even further.</p>
<h3 class="h2" id="ch13lev3"><strong class="calibre2">Conclusion</strong></h3>
<p class="noindent">The field of scientific machine learning is making impressive strides and expanding rapidly as I write this. Julia users are perfectly positioned to take advantage of recent research in this field, as it finds application in the packages of the SciML ecosystem. Scientific machine learning selects some of the technologies developed in ML that can be fruitfully applied to science and engineering concerns. A survey of the entire field would be a book in itself. In this chapter we’ve explored a few central ideas and applied them to problems that, while interesting in themselves, are simple enough not to obscure the working of the SciML machinery with too much incidental detail. These ideas and techniques can be applied to all areas of quantitative science. This is an exciting field to follow. Wherever it goes, it will inevitably become a pillar of computational science.</p>
<div class="box">
<p class="boxtitle-d" id="fur13"><span epub:type="pagebreak" id="page_427" class="calibre1"/><strong class="calibre2">FURTHER READING</strong></p>
<ul class="calibre12">
<li class="noindent1">See “The Essential Tools of Scientific Machine Learning (Scientific ML)” by Christopher Rackauckas for an introduction to existing open source tools: <a href="http://www.stochasticlifestyle.com/the-essential-tools-of-scientific-machine-learning-scientific-ml/" class="calibre10"><em class="calibre11">http://www.stochasticlifestyle.com/the-essential-tools-of-scientific-machine-learning-scientific-ml/</em></a>.</li>
<li class="noindent1">A solid mathematical introduction to automatic differentiation is available at <a href="http://www.ams.org/publicoutreach/feature-column/fc-2017-12" class="calibre10"><em class="calibre11">http://www.ams.org/publicoutreach/feature-column/fc-2017-12</em></a>.</li>
<li class="noindent1">Here is a hub for Julia’s SciML documentation: <a href="https://docs.sciml.ai/" class="calibre10"><em class="calibre11">https://docs.sciml.ai/</em></a>.</li>
<li class="noindent1">For a description of the various solver options for the <code>Differential</code> <code>Equations.jl</code> package, visit <a href="https://docs.sciml.ai/DiffEqDocs/stable/basics/common_solver_opts/" class="calibre10"><em class="calibre11">https://docs.sciml.ai/DiffEqDocs/stable/basics/common_solver_opts/</em></a>.</li>
<li class="noindent1">Details on the binomial distribution can be found at <a href="https://www.itl.nist.gov/div898/handbook/eda/section3/eda366i.htm" class="calibre10"><em class="calibre11">https://www.itl.nist.gov/div898/handbook/eda/section3/eda366i.htm</em></a>.</li>
<li class="noindent1">Documentation for the <code>Turing</code> package resides at <a href="https://turinglang.org/dev/docs/using-turing/get-started" class="calibre10"><em class="calibre11">https://turinglang.org/dev/docs/using-turing/get-started</em></a>.</li>
<li class="noindent1">For a tutorial on the use of <code>Turing</code>, visit <a href="https://turinglang.org/dev/docs/using-turing/guide" class="calibre10"><em class="calibre11">https://turinglang.org/dev/docs/using-turing/guide</em></a>.<span epub:type="pagebreak" id="page_428"/></li>
</ul>
</div>
</body></html>