- en: '**4'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**4'
- en: 'NEURAL NETWORKS: BRAIN-LIKE AI**'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络：类脑 AI**
- en: '![Image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/common.jpg)'
- en: Connectionism seeks to provide a substrate from which intelligence might emerge.
    Today, connectionism means neural networks, with *neural* being a nod to biological
    neurons. Despite the name, however, the relationship between the two is superficial.
    Biological neurons and artificial neurons may possess a similar configuration,
    but they operate in an entirely different manner.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 联结主义试图提供一个智能可能从中涌现的基质。今天，联结主义意味着神经网络，其中“神经”一词指的是生物神经元。尽管如此，它们之间的关系却只是表面上的。生物神经元和人工神经元可能具有类似的结构，但它们的运作方式完全不同。
- en: Biological neurons accept input on their dendrites, and when a sufficient number
    of inputs are active they “fire” to produce a short-lived voltage spike on their
    axons. In other words, biological neurons are off until they’re on. Some 800 million
    years of animal evolution have made the process considerably more complex, but
    that’s the essence.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 生物神经元通过树突接受输入，当足够多的输入被激活时，它们会“放电”，在轴突上产生短暂的电压脉冲。换句话说，生物神经元处于关闭状态，直到它们被激活。经过约
    8 亿年的动物进化，这一过程变得复杂得多，但这就是其本质。
- en: The artificial neurons of a neural network likewise possess inputs and outputs,
    but instead of firing, the neurons are mathematical functions with continuous
    behavior. Some models spike like biological neurons, but we ignore them in this
    book. The neural networks powering the AI revolution operate continuously.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的人工神经元也具有输入和输出，但与生物神经元不同，神经元是具有连续行为的数学函数，而不是通过放电来工作。有些模型像生物神经元一样会发生放电，但我们在本书中忽略了它们。推动
    AI 革新的神经网络是连续运作的。
- en: Think of a biological neuron like a light switch. It’s off until there is a
    reason (sufficient input) to turn it on. The biological neuron doesn’t turn on
    and stay on but flashes on and off, like flicking the switch. An artificial neuron
    is akin to a light with a dimmer switch. Turn the switch a tiny amount to produce
    a small amount of light; turn the switch further, and the light’s brightness changes
    proportionally. This analogy isn’t accurate in all cases, but it conveys the essential
    notion that artificial neurons are not all or nothing. Instead, they produce output
    in proportion to their input according to some function. The fog will lift as
    we work through the chapter, so don’t worry if this makes little sense at present.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将生物神经元想象成一个电灯开关。它处于关闭状态，直到有足够的理由（输入）将其打开。生物神经元不是打开了就一直开着，而是像切换开关一样，开与关交替。人工神经元类似于带调光器的灯泡。稍微调节开关，就会产生微弱的光；进一步调节，光的亮度会按比例变化。这个类比并不在所有情况下都准确，但它传达了人工神经元不是“全开”或“全关”的基本概念。相反，它们根据某种函数，按比例输出与输入相对应的结果。随着我们继续学习这一章，迷雾将会逐渐消散，所以即使目前这部分内容有些难以理解，也不用担心。
- en: '****'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '****'
- en: '[Figure 4-1](ch04.xhtml#ch04fig01) is the most critical figure in the book.
    It’s also one of the simplest, as is to be expected if the connectionist approach
    is on the right track. If we understand what [Figure 4-1](ch04.xhtml#ch04fig01)
    represents and how it operates, we have the core understanding necessary to make
    sense of modern AI.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-1](ch04.xhtml#ch04fig01)是本书中最关键的图，也是最简单的图之一，正如如果联结主义方法是正确的那样应该预期的。如果我们理解[图
    4-1](ch04.xhtml#ch04fig01)所代表的内容以及它是如何运作的，我们就掌握了理解现代 AI 所必需的核心知识。'
- en: '![Image](../images/ch04fig01.jpg)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/ch04fig01.jpg)'
- en: '*Figure 4-1: The humble (artificial) neuron*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-1：简朴的（人工）神经元*'
- en: '[Figure 4-1](ch04.xhtml#ch04fig01) contains three squares, a circle, five arrows,
    and labels like “*x*[0]” and “Output.” Let’s examine each in turn, beginning with
    the squares on the left.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-1](ch04.xhtml#ch04fig01)包含三个方框、一个圆圈、五个箭头，以及像“*x*[0]”和“输出”这样的标签。我们将依次检查每个元素，从左侧的方框开始。'
- en: Standard practice presents neural networks with the inputs on the left and data
    flow to the right. In [Figure 4-1](ch04.xhtml#ch04fig01), the three squares labeled
    *x*[0], *x*[1], and *x*[2] are the inputs to the neuron. They are the three features
    of a feature vector, what we want the neuron to process to give us an output leading
    to a class label.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 标准做法是将神经网络的输入放在左侧，数据流向右侧。在[图 4-1](ch04.xhtml#ch04fig01)中，三个标记为 *x*[0]、*x*[1]
    和 *x*[2] 的方框是神经元的输入。它们是特征向量的三个特征，我们希望神经元处理这些输入，从而给出一个输出，最终得到一个类别标签。
- en: The circle is labeled *h*, a standard notation for the [*activation function*](glossary.xhtml#glo1).
    The activation function’s job is to accept input to the neuron and produce an
    output value, the arrow heading off to the right in [Figure 4-1](ch04.xhtml#ch04fig01).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 圆圈被标记为*h*，这是[*激活函数*](glossary.xhtml#glo1)的标准符号。激活函数的任务是接收神经元的输入并生成输出值，输出箭头指向右侧，见[图4-1](ch04.xhtml#ch04fig01)。
- en: The three input squares are connected to the circle (the [*node*](glossary.xhtml#glo74))
    by arrows, one from each input square. The arrows’ labels—*w*[0], *w*[1], and
    *w*[2]—are the [*weights*](glossary.xhtml#glo100). Every input to the neuron has
    an associated weight. The lone *b* linked to the circle by an arrow is the [*bias*](glossary.xhtml#glo13).
    It’s a number, as are the weights, the input *x*s, and the output. For this neuron,
    three numbers come in, and one number goes out.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 三个输入方框通过箭头连接到圆圈（[*节点*](glossary.xhtml#glo74)），每个输入方框都有一条箭头。箭头的标签——*w*[0]、*w*[1]和*w*[2]——表示[*权重*](glossary.xhtml#glo100)。每个输入都对应一个权重。与圆圈通过箭头连接的单独*b*是[*偏置*](glossary.xhtml#glo13)。它是一个数字，和权重、输入*x*及输出一样。对于这个神经元，三个数字进入，一个数字输出。
- en: 'The neuron operates like this:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元是这样运作的：
- en: Multiply every input value, *x*[0], *x*[1], and *x*[2], by its associated weight,
    *w*[0], *w*[1], and *w*[2].
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个输入值*x*[0]、*x*[1]和*x*[2]与其相应的权重*w*[0]、*w*[1]和*w*[2]相乘。
- en: Add all the products from step 1 together along with the bias value, *b*. This
    produces a single number.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将步骤1中的所有乘积与偏置值*b*相加，得到一个单一的数字。
- en: Give the single number to *h*, the activation function, to produce the output,
    also a single number.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将该单一数字传递给*h*，即激活函数，生成输出，也是一个单一数字。
- en: 'That’s all a neuron does: it multiplies its inputs by the weights, sums the
    products, adds the bias value, and passes that total to the activation function
    to produce the output.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是神经元的全部功能：它将输入与权重相乘，求和，添加偏置值，然后将总和传递给激活函数，以生成输出。
- en: Virtually all the fantastic accomplishments of modern AI are due to this primitive
    construct. String enough of these together in the correct configuration, and you
    have a model that can learn to identify dog breeds, drive a car, or translate
    from French to English. Well, you do if you have the magic weight and bias values,
    which training gives us. These values are so important to neural networks that
    one company has adopted “Weights & Biases” as its name; see [*https://www.wandb.ai*](https://www.wandb.ai).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现代人工智能几乎所有的伟大成就都归功于这个原始构造。将足够多的这些基本构件按照正确的配置连接在一起，就可以构建一个可以学会识别狗品种、驾驶汽车或将法语翻译成英语的模型。嗯，前提是你有魔法般的权重和偏置值，这些值是通过训练得到的。这些值对于神经网络如此重要，以至于有一家公司将“权重与偏置”作为其名称；见[*https://www.wandb.ai*](https://www.wandb.ai)。
- en: 'We have choices for the activation function, but in modern networks it’s most
    often the rectified linear unit (ReLU) mentioned in [Chapter 2](ch02.xhtml). The
    ReLU is a question: is the input (the sum of the inputs multiplied by the weights
    plus the bias) less than zero? If so, the output is zero; otherwise, it’s whatever
    the input is.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以选择激活函数，但在现代网络中，最常用的是[第2章](ch02.xhtml)中提到的整流线性单元（ReLU）。ReLU的问题是：输入（即输入的加权和加上偏置）是否小于零？如果是，输出为零；否则，输出就是输入值。
- en: Can something as straightforward as a lone neuron be useful? It can. As an experiment,
    I trained the neuron in [Figure 4-1](ch04.xhtml#ch04fig01) using three features
    from the iris flower dataset from [Chapter 1](ch01.xhtml) as input. Recall, this
    dataset contains measurements of the parts of three different species of iris.
    After training, I tested the neuron with an unused test set that had 30 feature
    vectors. The neuron correctly classified 28, for an accuracy of 93 percent.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 像单个神经元这样的简单构件能有用吗？它能。作为实验，我使用[第4-1图](ch04.xhtml#ch04fig01)中的神经元，利用来自[第1章](ch01.xhtml)的鸢尾花数据集的三个特征作为输入进行训练。回想一下，这个数据集包含了三种不同鸢尾花的各部分测量值。训练后，我使用一个未用过的测试集对神经元进行了测试，该测试集包含30个特征向量。神经元正确分类了28个样本，准确率为93%。
- en: I trained the neuron by searching for a set of three weights and a bias value
    producing an output that, when rounded to the nearest whole number, matched the
    class label for an iris flower—either 0, 1, or 2\. This is not the standard way
    to train a neural network, but it works for something as modest as a single neuron.
    We’ll discuss standard network training later in the chapter.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我通过寻找一组三个权重和一个偏置值来训练神经元，使得输出经过四舍五入到最接近的整数后，匹配鸢尾花的类别标签——0、1或2。这不是训练神经网络的标准方法，但对于像单个神经元这样的简单情况是有效的。我们将在本章后面讨论标准的网络训练方法。
- en: A single neuron can learn, but complex inputs baffle it. Complex inputs imply
    we need a more complex model. Let’s give our single neuron some friends.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 单个神经元可以学习，但复杂的输入会让它感到困惑。复杂的输入意味着我们需要一个更复杂的模型。让我们给我们的单个神经元一些朋友。
- en: Convention arranges neurons in layers, with the outputs from the previous layer
    the inputs to the following layer. Consider [Figure 4-2](ch04.xhtml#ch04fig02),
    which shows networks with two, three, and eight nodes in the layer after the input.
    Arranging the network in layers simplifies the implementation in code and facilitates
    the standard training procedure. That said, there is no requirement to use layers
    if an alternative way to train the model can be found.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，神经元按层排列，前一层的输出作为后一层的输入。考虑[图4-2](ch04.xhtml#ch04fig02)，它展示了输入之后层中分别有两个、三个和八个节点的网络。将网络按层排列简化了代码实现，并有助于标准的训练过程。不过，如果能找到替代方法来训练模型，也没有强制要求必须使用层。
- en: '![Image](../images/ch04fig02.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/ch04fig02.jpg)'
- en: '*Figure 4-2: Two-, three-, and eight-node networks*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4-2：两节点、三节点和八节点网络*'
- en: Let’s begin with the two-node network at the upper left. The three inputs (squares)
    are there, but this time there are two circles in the middle layer and a single
    circle on the right. The inputs are fully connected to the two nodes in the middle
    layer, meaning a line connects each input square to each middle layer node. The
    middle layer outputs are connected to a single node on the far right, from which
    the network’s output comes.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从左上角的两个节点网络开始。三个输入（方框）在那里，但这次中间层有两个圆圈，右边有一个单一的圆圈。输入完全连接到中间层的两个节点，这意味着每个输入方框都与每个中间层节点之间有一条连接线。中间层的输出连接到最右边的单一节点，网络的输出来自这个节点。
- en: The middle layers of a neural network between the input on the left and the
    output on the right are known as [*hidden layers*](glossary.xhtml#glo54). For
    example, the networks of [Figure 4-2](ch04.xhtml#ch04fig02) each have one hidden
    layer with 2, 3, and 8 nodes, respectively.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中位于左侧输入和右侧输出之间的中间层被称为[*隐藏层*](glossary.xhtml#glo54)。例如，[图4-2](ch04.xhtml#ch04fig02)中的网络分别有一个隐藏层，包含2个、3个和8个节点。
- en: A network with this configuration is suitable for a binary classification task,
    class 0 versus class 1, where the output is a single number representing the model’s
    belief that the input is a member of class 1\. Therefore, the rightmost node uses
    a different activation function known as a [*sigmoid*](glossary.xhtml#glo90) (also
    called a logistic). The sigmoid produces an output between 0 and 1\. This is also
    the range used to represent a probability, so many people refer to the output
    of a node with a sigmoid activation function as a probability. This is not generally
    accurate, but we can live with the sloppiness. The nodes of the hidden layer all
    use ReLU activation functions.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 具有这种配置的网络适用于二元分类任务，即类别0与类别1，其中输出是一个单一的数字，表示模型认为输入属于类别1的概率。因此，最右侧的节点使用一种不同的激活函数，称为[*sigmoid*](glossary.xhtml#glo90)（也叫做逻辑斯蒂函数）。Sigmoid函数的输出范围在0到1之间。这也是表示概率的范围，因此许多人将带有Sigmoid激活函数的节点输出称为概率。虽然这通常并不准确，但我们可以接受这种不严谨。隐藏层的节点全部使用ReLU激活函数。
- en: How many weights and biases must we learn to implement the two-node network
    in [Figure 4-2](ch04.xhtml#ch04fig02)? We need one weight for each line (except
    the output arrow) and one bias value for each node. Therefore, we need eight weights
    and three bias values. For the model at the lower left, we need 12 weights and
    4 biases. Finally, for the 8-node model, we need to learn 32 weights and 9 bias
    values. As the number of nodes in a layer increases, the number of weights increases
    even faster. This fact alone restrained neural networks for years, as potentially
    useful models were too big for a single computer’s memory. Of course, model size
    is relative. OpenAI’s GPT-3 has over 175 billion weights, and while they aren’t
    talking about how large GPT-4 is, rumor puts it at 1.7 *trillion* weights.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现[图4-2](ch04.xhtml#ch04fig02)中的两个节点网络，我们必须学习多少权重和偏置？我们需要为每条线（输出箭头除外）设置一个权重，并为每个节点设置一个偏置值。因此，我们需要八个权重和三个偏置值。对于左下角的模型，我们需要12个权重和4个偏置。最后，对于8节点模型，我们需要学习32个权重和9个偏置值。随着层中节点数量的增加，权重的数量增长得更快。仅这一事实就限制了神经网络的发展多年，因为潜在有用的模型对于单台计算机的内存来说过于庞大。当然，模型的大小是相对的。OpenAI的GPT-3有超过1750亿个权重，尽管他们没有透露GPT-4的具体大小，但传闻称它有1.7
    *万亿*个权重。
- en: We need a two-class dataset to explore the models in [Figure 4-2](ch04.xhtml#ch04fig02).
    The dataset we’ll use is a classic one that attempts to distinguish between two
    cultivars of grapes used to make wine in a particular region of Italy. Unfortunately,
    the wines represented by the dataset are, it seems, no longer known. (That’s how
    old the dataset is.) However, we know that models don’t care about the labels—they
    use numbers—so we’ll use 0 and 1 as the labels.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个二分类数据集来探索[图 4-2](ch04.xhtml#ch04fig02)中的模型。我们将使用的数据集是一个经典的数据集，旨在区分意大利某一特定地区用于酿酒的两种葡萄品种。不幸的是，似乎这个数据集所代表的葡萄酒现在已经不再知名了。（数据集的历史就是这么久。）不过，我们知道模型不关心标签——它们使用的是数字——所以我们将使用0和1作为标签。
- en: We need three features, *x*[0], *x*[1], and *x*[2]. The features we’ll use are
    alcohol content in percent, malic acid, and total phenols. The goal is to train
    the models in [Figure 4-2](ch04.xhtml#ch04fig02) to see how well each performs
    when identifying an unknown wine given measurements of the three features.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要三个特征，*x*[0]，*x*[1]，和 *x*[2]。我们将使用的特征是酒精含量（百分比）、苹果酸和总酚类物质。目标是训练[图 4-2](ch04.xhtml#ch04fig02)中的模型，看看在给定这三种特征的测量值时，每个模型在识别未知葡萄酒时表现如何。
- en: I trained the two-neuron model using a training set of 104 samples and a test
    set of 26 samples. This means I used 104 triplets of measured alcohol content,
    malic acid level, and total phenols, knowing the proper output label, class 0
    or class 1\. The training set conditioned the two-neuron model to give values
    to all eight weights and three biases. I promise we will discuss how training
    works, but for now, assume it happens so we can explore how neural networks behave.
    The trained model achieved an accuracy on the test set of 81 percent, meaning
    it was right better than 8 times out of 10\. That’s not too bad for such a small
    model and training set.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用包含104个样本的训练集和26个样本的测试集训练了双神经元模型。这意味着我使用了104个测量的酒精含量、苹果酸水平和总酚类物质的三元组，并且知道正确的输出标签，类别0或类别1。训练集使双神经元模型为所有八个权重和三个偏差赋值。我保证我们会讨论训练是如何工作的，但现在请假设它已经发生，这样我们就可以探索神经网络的行为。经过训练的模型在测试集上的准确率为81%，意味着它的判断正确率超过了10次中的8次。对于如此小的模型和训练集来说，这还算不错。
- en: '[Figure 4-3](ch04.xhtml#ch04fig03) presents the trained two-neuron model. I
    added the weights to the links and the biases to the nodes so you can see them.
    I think it’s worth looking at the numbers at least once, and it’s best to do that
    with a simple model.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-3](ch04.xhtml#ch04fig03)展示了训练后的双神经元模型。我在连接处添加了权重，在节点上添加了偏差，这样你就可以看到它们。我认为至少要看一次这些数字，最好是用一个简单的模型来做。'
- en: '![Image](../images/ch04fig03.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/ch04fig03.jpg)'
- en: '*Figure 4-3: The two-neuron model trained on the wine dataset*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-3：基于葡萄酒数据集训练的双神经元模型*'
- en: 'Let’s use the model with two test samples to understand the process. The two
    test samples consist of three numbers each, the values of the features, (*x*[0],
    *x*[1], *x*[2]):'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用两个测试样本来理解这个过程。这两个测试样本各由三个数字组成，分别是特征值（*x*[0]，*x*[1]，*x*[2]）：
- en: '| **Sample 1** | (–0.7359, 0.9795, –0.1333) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| **样本 1** | (–0.7359, 0.9795, –0.1333) |'
- en: '| **Sample 2** | ( 0.0967, –1.2138, –1.0500) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| **样本 2** | ( 0.0967, –1.2138, –1.0500) |'
- en: You may have a question at this point. I said the features were alcohol content
    in percent, malic acid level, and total phenols. While I have no idea what the
    units are for measuring malic acid or total phenols, a percent is a percent, so
    why is *x*[0] for the first sample a small negative number? We can’t have a negative
    percentage of alcohol.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会有一个问题。我说特征是酒精含量（百分比）、苹果酸水平和总酚类物质。虽然我不知道苹果酸或总酚类物质的测量单位是什么，但百分比就是百分比，那么为什么第一个样本的
    *x*[0] 是一个小的负数呢？我们不可能有负的酒精百分比。
- en: The answer has to do with [*preprocessing*](glossary.xhtml#glo82). Raw data,
    like the percent alcohol, is seldom used with machine learning models as is. Instead,
    each feature is adjusted by subtracting the average value of the feature over
    the training set and dividing that result by a measure of how scattered the data
    is around the average value (the standard deviation). The original alcohol content
    was 12.29 percent, a reasonable value for wine, but after scaling, it became –0.7359.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 答案与[*预处理*](glossary.xhtml#glo82)有关。原始数据，比如酒精含量，通常不会直接用于机器学习模型。相反，每个特征都会通过减去训练集中特征的平均值，并将结果除以数据围绕平均值的分散程度（标准差）来进行调整。原始的酒精含量是12.29%，这是一个合理的葡萄酒酒精含量值，但经过缩放后，它变成了
    –0.7359。
- en: Let’s classify sample 1 using the learned weights and biases in [Figure 4-3](ch04.xhtml#ch04fig03).
    The input to the top neuron is each feature multiplied by the weight on the line
    connecting that feature to the neuron, then summed with the bias value. The first
    feature gives us 0.4716 × –0.7359, the second 0.0399 × 0.9795, and the third –0.3902
    × –0.1333, with bias value 0.0532\. Adding all of these together gives – 0.2028\.
    This is the number passed to the activation function, a ReLU. Since it is negative,
    the ReLU returns 0, meaning the output from the top node is 0\. Repeating the
    calculation for the bottom node gives 0.1720 as the input to the ReLU. That’s
    a positive number, so the ReLU returns 0.1720 as the output.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用[图4-3](ch04.xhtml#ch04fig03)中学习到的权重和偏置对样本1进行分类。顶部神经元的输入是每个特征与连接该特征与神经元的权重相乘，然后与偏置值求和。第一个特征给我们0.4716
    × –0.7359，第二个给我们0.0399 × 0.9795，第三个给我们–0.3902 × –0.1333，加上偏置值0.0532。将这些加在一起得到–0.2028。这是传递给激活函数ReLU的数值。由于它是负数，ReLU返回0，这意味着顶部节点的输出是0。对底部节点进行相同的计算得到0.1720作为传递给ReLU的输入。由于这是一个正数，ReLU返回0.1720作为输出。
- en: The outputs of the two nodes in the middle layer are now used as the inputs
    to the final node on the right. As before, we multiply the outputs by the weights,
    add them along with the bias value, and pass that to the activation function.
    In this case, the activation function is not a ReLU but a sigmoid.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 中间层两个节点的输出现在作为输入传递给右侧的最终节点。和之前一样，我们将输出与权重相乘，进行求和，并加上偏置值，然后传递给激活函数。在这个例子中，激活函数不是ReLU，而是Sigmoid。
- en: The top node’s output is 0, and the bottom’s output is 0.1720\. Multiplying
    these by their respective weights, summing, and adding the bias value of 2.2277
    gives us 1.9502 as the argument to the sigmoid activation function, producing
    0.8755 as the network’s output for the first input sample.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 顶部节点的输出是0，底部节点的输出是0.1720。将这些与各自的权重相乘，求和，再加上偏置值2.2277，得到1.9502作为传递给Sigmoid激活函数的输入，产生0.8755作为网络对第一个输入样本的输出。
- en: 'How should we interpret this output? Here’s where we learn an important aspect
    of neural networks:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该如何解释这个输出呢？在这里，我们学到神经网络的一个重要方面：
- en: Neural networks don’t tell us the actual class label for the input, but only
    their confidence in one label relative to another.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络不会告诉我们输入的实际类别标签，而只是告诉我们一个标签相对于另一个标签的置信度。
- en: Binary models output a confidence value that we’re interpreting as the probability
    of the input belonging to class 1\. Probabilities are numbers between 0 (no chance)
    and 1 (absolutely assured). Humans are generally more comfortable with percentages,
    which we get by multiplying the probability by 100\. Therefore, we can say that
    the network is a little more than 87 percent confident that this input represents
    an instance of class 1.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 二分类模型输出一个置信值，我们将其解释为输入属于类别1的概率。概率是介于0（没有机会）和1（完全确定）之间的数字。人类通常对百分比更为熟悉，我们可以通过将概率乘以100来得到百分比。因此，我们可以说网络对该输入属于类别1的信心大约为87%。
- en: In practice, we use a threshold—a cutoff value—to decide which label to assign.
    The most common approach for binary models is a threshold of 50 percent. If the
    output exceeds 50 percent (probability 0.5), we assign the input to class 1\.
    This output is above 50 percent, so we assign “class 1” as the label. This sample
    is from class 1, meaning the network’s assigned label is correct.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们使用一个阈值——一个截止值——来决定分配哪个标签。对于二分类模型，最常见的做法是使用50%的阈值。如果输出超过50%（概率为0.5），我们将输入分配给类别1。这个输出超过了50%，所以我们将“类别1”作为标签分配给它。这个样本来自类别1，意味着网络分配的标签是正确的。
- en: We can repeat these calculations for the second input sample, (0.0967, –1.2138,
    –1.0500). I’ll leave walking through it to you as an exercise, but the network’s
    output for sample 2 is 0.4883\. In other words, the network’s confidence that
    this sample belongs to class 1 is 49 percent. The cutoff is 50 percent, so we
    reject the class 1 label and assign this input to class 0\. The actual class is
    class 1, so, in this instance, the network is wrong—it assigned a class 1 sample
    to class 0\. Oops.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对第二个输入样本（0.0967, –1.2138, –1.0500）重复这些计算。我将把详细的计算过程留给你作为练习，但是网络对样本2的输出是0.4883。换句话说，网络对这个样本属于类别1的信心是49%。截止值是50%，所以我们拒绝类别1标签，并将此输入分配给类别0。实际的类别是类别1，因此在这个实例中，网络是错误的——它将一个类别1的样本分配给了类别0。哎呀。
- en: Is this a useful model? The answer depends on the context. We’re classifying
    wine by cultivar. If the model’s output is wrong 20 percent of the time, which
    is one time in five, is that acceptable? I suspect not, but there might be other
    tasks where a model with this level of accuracy is acceptable.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个有用的模型吗？答案取决于具体情境。我们正在按葡萄品种对葡萄酒进行分类。如果模型的输出每五次中有一次错误，误差率为 20%，这能接受吗？我怀疑不能，但也可能有些任务对于这种精度水平的模型是可以接受的。
- en: Neural networks offer some control over how their outputs are interpreted. For
    example, we might not use 50 percent as the cutoff. If we make it lower, say,
    40 percent, we’ll capture more class 1 samples, but at the expense of mistakenly
    identifying more actual class 0 samples as class 1\. In other words, we get to
    trade off one kind of error for another.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络提供了某种程度的控制，来决定如何解释它们的输出。例如，我们可能不会使用 50% 作为判断的分界线。如果我们把这个值设得更低，比如 40%，那么我们会捕获更多的类别
    1 样本，但也会因此错误地将更多的实际类别 0 样本识别为类别 1。换句话说，我们可以在不同类型的错误之间进行权衡。
- en: 'Let’s bring the other models in [Figure 4-2](ch04.xhtml#ch04fig02) into the
    mix. I trained all three models using the same training and test sets used for
    [Figure 4-3](ch04.xhtml#ch04fig03). I repeated the process 240 times for each
    of the three models. Here are the average accuracies:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把[图 4-2](ch04.xhtml#ch04fig02)中的其他模型也考虑进来。我使用与[图 4-3](ch04.xhtml#ch04fig03)相同的训练集和测试集训练了这三个模型。每个模型的训练过程重复了
    240 次。以下是每个模型的平均精度：
- en: '| **2-node** | 81.5 percent |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| **2节点** | 81.5 百分比 |'
- en: '| **3-node** | 83.6 percent |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| **3节点** | 83.6 百分比 |'
- en: '| **8-node** | 86.2 percent |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| **8节点** | 86.2 百分比 |'
- en: The model’s performance improves as the number of nodes in the hidden layer
    increases. This makes intuitive sense, as a more complex model (more nodes) implies
    the ability to learn more complex associations hidden within the training set.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层中的节点数增加时，模型的性能有所提升。这是直观合理的，因为一个更复杂的模型（更多的节点）意味着能够学习到训练集中更复杂的关联。
- en: 'I suspect you now have a new question: why did I train each model 240 times
    and report the average accuracy over all 240 models? Here’s another critical thing
    to understand about neural networks:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我怀疑你现在有了一个新问题：为什么我每个模型都训练了 240 次，并报告了这 240 个模型的平均精度？这是理解神经网络时的另一个关键点：
- en: Neural networks are randomly initialized, such that repeated training leads
    to differently performing models even when using the same training data.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是随机初始化的，因此即使使用相同的训练数据，重复训练也会导致表现不同的模型。
- en: The phrase “randomly initialized” demands clarification. Look again at [Figure
    4-3](ch04.xhtml#ch04fig03). The numbers representing the weights and biases came
    from an iterative process. This means that an initial set of weights and biases
    are updated repeatedly, each time moving the network toward a better and better
    approximation of whatever function it is that links the input feature vectors
    and the output labels. Approximating this function well is what we want the network
    to do.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: “随机初始化”这一说法需要澄清。再看一下[图 4-3](ch04.xhtml#ch04fig03)。表示权重和偏置的数字来自一个迭代过程。这意味着一组初始的权重和偏置会被反复更新，每次都将网络推向更接近输入特征向量与输出标签之间的函数的近似值。准确地近似这个函数是我们希望网络完成的任务。
- en: Why not initialize all the weights to the same value? The answer is that doing
    so forces the weights to learn similar characteristics of the data, which is something
    we don’t want, and in the end the model will perform poorly. If we set all of
    the initial weights to zero, the model does not learn at all.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么不将所有权重初始化为相同的值呢？答案是，这样做会迫使权重学习数据的相似特征，这正是我们不希望发生的，最终模型的表现会很差。如果我们将所有初始权重都设置为零，模型根本无法学习。
- en: An initial set of values are necessary for the iterative process to work. How
    should we pick the initial values? That’s an important question, and the answer
    for our current level of understanding is “at random,” meaning we roll dice, in
    a sense, to get the initial value for each weight and bias. The iterative process
    then refines these values to arrive at the final set in [Figure 4-3](ch04.xhtml#ch04fig03).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代过程需要一组初始值才能正常工作。我们应该如何选择初始值呢？这是一个重要的问题，对于我们当前的理解水平，答案是“随机”，意思是我们在某种程度上是通过掷骰子来为每个权重和偏置选择初始值。然后，迭代过程会不断优化这些值，最终得到如[图
    4-3](ch04.xhtml#ch04fig03)所示的最终结果。
- en: 'However, the iterative process doesn’t always end in the same place. Pick a
    different random set of initial weights and biases, and the network will converge
    to a different set of final values. For example, the network in [Figure 4-3](ch04.xhtml#ch04fig03)
    achieved an accuracy of 81 percent, as mentioned previously. Here are 10 more
    accuracies for the same network trained and tested on the same data:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这个迭代过程并不总是以相同的结果结束。选择不同的初始随机权重和偏置集，网络将收敛到不同的一组最终值。例如，前面提到的[图4-3](ch04.xhtml#ch04fig03)中的网络达到了81%的准确率。以下是同一网络在相同数据上训练和测试后获得的另外10个准确率：
- en: 89, 85, 73, 81, 81, 81, 81, 85, 85, 85
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 89, 85, 73, 81, 81, 81, 81, 85, 85, 85
- en: The accuracies range from a high of 89 percent to a low of 73 percent. All that
    changed between each training session was the collection of initial weights and
    biases. This is an often overlooked issue with neural networks. Networks should
    be trained multiple times, if feasible, to gather data on their effectiveness
    or, as with the 73 percent version of the network, to understand that a bad set
    of initial values was used purely by chance. I should also mention that the wide
    variation in the accuracy of this network is related to its being relatively small
    and containing only a few weights and biases. Larger models tend to be more consistent
    when trained repeatedly.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这些准确率从最高的89%到最低的73%不等。每次训练会话之间唯一变化的只是初始权重和偏置的集合。这是神经网络中一个经常被忽视的问题。如果可能的话，网络应该进行多次训练，以收集关于其有效性的数据，或者如同在73%版本的网络中那样，理解纯粹由于偶然使用了一个糟糕的初始值集。我还应该提到，这个网络准确率的广泛变化与其相对较小，并且只包含少数权重和偏置有关。较大的模型在反复训练时往往更为一致。
- en: 'We’ve already covered a lot of ground, so a recap is in order:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经覆盖了很多内容，所以需要做一个总结：
- en: The fundamental unit of a neural network is the neuron, also called a node.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络的基本单元是神经元，也叫节点。
- en: Neurons multiply their inputs by weights, sum those products, add a bias value,
    and pass all of that to the activation function to produce an output value.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经元将输入与权重相乘，将这些乘积求和，添加一个偏置值，然后将所有这些传递给激活函数以产生输出值。
- en: Neural networks are collections of individual neurons, typically arranged in
    layers with the output of the current layer the input to the following layer.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络是由单个神经元组成的集合，通常按层排列，当前层的输出是下一层的输入。
- en: Training a neural network assigns values to the weights and biases by iteratively
    adjusting an initial, randomly selected set.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练神经网络是通过迭代调整初始的、随机选择的一组权重和偏置值来赋予权重和偏置值。
- en: Binary neural networks produce an output that roughly corresponds to the probability
    of the input belonging to class 1.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二值神经网络产生的输出大致对应于输入属于类别1的概率。
- en: '****'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '****'
- en: 'Now that we know what a neural network is and how it’s used, we finally come
    to the crux of the matter: where do the magic weights and biases come from in
    the first place? In [Chapter 2](ch02.xhtml), I briefly mentioned that neural networks
    improved in the 1980s thanks to two essential algorithms: backpropagation and
    gradient descent. These are the algorithms at the heart of neural network training.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了什么是神经网络以及它是如何使用的，我们终于来到了问题的核心：这些神奇的权重和偏置到底是从哪里来的？在[第2章](ch02.xhtml)中，我简要提到，神经网络在1980年代得到了提升，这要归功于两个关键的算法：反向传播和梯度下降。这些算法是神经网络训练的核心。
- en: We discussed optimization, the process of finding the best of something according
    to some criteria, in [Chapter 3](ch03.xhtml) in reference to support vector machines.
    Training a neural network is also an optimization process, involving learning
    the weights and biases that best fit the training data. Care must be taken, however,
    to make it more likely that the learned weights and biases fit general trends
    in the training data rather than the details of the specific training data itself.
    What I mean by that will become apparent as we learn more about the training process.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第3章](ch03.xhtml)中讨论了优化，指的是根据某些标准寻找最佳解的过程，具体是在支持向量机的背景下。训练神经网络也是一个优化过程，涉及学习最佳拟合训练数据的权重和偏置。然而，必须小心，确保学习到的权重和偏置更可能符合训练数据中的一般趋势，而不是特定训练数据本身的细节。随着我们对训练过程了解得更多，这一点将变得更加明显。
- en: 'The general training algorithm is:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 一般的训练算法是：
- en: Select the model’s architecture, including the number of hidden layers, nodes
    per layer, and activation function.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择模型的架构，包括隐藏层的数量、每层的节点数以及激活函数。
- en: Randomly but intelligently initialize all the weights and biases associated
    with the selected architecture.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机但智能地初始化与所选架构相关的所有权重和偏置。
- en: Run the training data, or a subset, through the model and calculate the average
    error. This is the [*forward pass*](glossary.xhtml#glo45).
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练数据或其子集通过模型，并计算平均误差。这就是[*前向传播*](glossary.xhtml#glo45)。
- en: Use backpropagation to determine how much each weight and bias contributes to
    that error.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用反向传播来确定每个权重和偏置对误差的贡献。
- en: Update the weights and biases according to the gradient descent algorithm. This
    and the previous step make up the [*backward pass*](glossary.xhtml#glo11).
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据梯度下降算法更新权重和偏置。这一步和前一步构成了[*反向传播*](glossary.xhtml#glo11)。
- en: Repeat from step 3 until the network is considered “good enough.”
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从第3步开始重复，直到网络被认为是“足够好”。
- en: These six steps include many important terms. It’s worth our time to ensure
    that we have an idea of what each means. In this chapter, [*architecture*](glossary.xhtml#glo3)
    refers to the number of layers, typically hidden layers, used by the network.
    We have our input feature vector, and we can imagine each hidden layer working
    collectively to accept an input vector and produce an output vector, which then
    becomes the input to the next layer, and so on. For binary classifiers, the network’s
    output is a single node producing a value from 0 to 1\. We’ll learn later in the
    book that this idea can be extended to multiclass outputs.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这六个步骤包含了许多重要的术语。我们花时间理解每个术语的含义是值得的。在本章中，[*架构*](glossary.xhtml#glo3)指的是网络使用的层数，通常是隐藏层。我们有输入特征向量，可以想象每个隐藏层共同工作，接收输入向量并生成输出向量，随后该输出成为下一个层的输入，依此类推。对于二分类器，网络的输出是一个节点，输出值在0到1之间。稍后在本书中，我们将学习到这个概念可以扩展到多类别输出。
- en: The algorithm indicates that training is an iterative process that repeats many
    times. Iterative processes have a starting point. If you want to walk from point
    A to point B, place one foot in front of the other. That’s the iterative part.
    Point A is the starting point. For a neural network, the architecture implies
    a set of weights and biases. The initial values assigned to those weights and
    biases are akin to point A, with training akin to placing one foot in front of
    the other.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 算法表明，训练是一个重复多次的迭代过程。迭代过程有一个起点。如果你想从A点走到B点，就得一步一步走。这就是迭代的部分。A点是起点。对于神经网络，架构意味着一组权重和偏置。这些权重和偏置的初始值类似于A点，训练过程就像是一步步往前走。
- en: The algorithm uses the phrase “average error.” What error? Here’s where a new
    concept enters the picture. Intuitively, we can see that simply picking some initial
    values for the weights and biases is not likely to lead to a network able to classify
    the training data accurately. Remember, we know the inputs and the expected outputs
    for the training data.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 算法使用了“平均误差”这个术语。什么是误差？这里引入了一个新概念。直观地看，我们可以理解，仅仅为权重和偏置选择一些初始值，不太可能让网络准确地对训练数据进行分类。记住，我们知道训练数据的输入和预期输出。
- en: Say we push training sample 1 through the network to give us an output value,
    perhaps 0.44\. If we know that sample 1 belongs to class 1, the error made by
    the network is the difference between the expected output and the actual output.
    Here, that’s 1 – 0.44, or 0.56\. A good model might instead have produced an output
    of 0.97 for this sample, giving an error of only 0.03\. The smaller the error,
    the better the model is at classifying the sample. If we push all the training
    data through the network, or a representative subset of it, we can calculate the
    error for each training sample and find the average over the entire training set.
    That’s the measure used by the (to be described) backpropagation and gradient
    descent algorithms to update the weights and biases.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们将训练样本1通过网络，得到一个输出值，可能是0.44。如果我们知道样本1属于类别1，那么网络的误差就是预期输出和实际输出之间的差值。在这里，就是1
    – 0.44，等于0.56。一个好的模型可能会为这个样本输出0.97，产生的误差仅为0.03。误差越小，模型在分类该样本时越好。如果我们将所有训练数据或其代表性子集通过网络，我们可以计算每个训练样本的误差，并求出整个训练集的平均误差。这就是反向传播和梯度下降算法用来更新权重和偏置的标准。
- en: Finally, the training algorithm says to push data through the network, get an
    error, update the weights and biases, and repeat until the network is “good enough.”
    In a way, good enough is when the error, also called the [*loss*](glossary.xhtml#glo63),
    is as close to zero as possible. If the network produces 0 as the output for all
    class 0 samples and 1 as the output for all class 1 samples, then it performs
    perfectly on the training data, and the error will be zero. That’s certainly good
    enough, but we must be careful. Sometimes when that happens the network is [*overfitting*](glossary.xhtml#glo79),
    meaning it’s learned all the details of the training data without actually learning
    the general trends of the data that will allow it to perform well when used with
    unknown inputs in the wild.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，训练算法会要求将数据输入到网络中，得到误差，更新权重和偏差，然后重复这个过程，直到网络表现得“足够好”。从某种意义上说，“足够好”就是当误差，也就是[*损失*](glossary.xhtml#glo63)，尽可能接近零时。如果网络对所有类别0的样本输出0，对所有类别1的样本输出1，那么它在训练数据上的表现是完美的，误差为零。这当然是“足够好”的，但我们必须小心。有时，当这种情况发生时，网络会[*过拟合*](glossary.xhtml#glo79)，意味着它学习了训练数据中的所有细节，但没有学习到能够让它在面对未知输入时仍能良好表现的数据的普遍趋势。
- en: In practice, overfitting is addressed in several ways, the best of which is
    acquiring more training data. We use the training data as a stand-in for all the
    possible data that could be produced by whatever process we are trying to model.
    Therefore, more training data means a better representation of that data collection.
    It’s the interpolate versus extrapolate issue we discussed in [Chapter 1](ch01.xhtml).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，过拟合问题有多种解决方法，其中最有效的一种是获取更多的训练数据。我们使用训练数据来代表所有可能由我们试图建模的过程生成的数据。因此，更多的训练数据意味着对这些数据集合的更好表示。这正是我们在[第1章](ch01.xhtml)中讨论的插值与外推的问题。
- en: However, getting more training data might not be possible. Alternatives include
    tweaking the training algorithm to introduce things that keep the network from
    focusing on irrelevant details of the training data while learning. One such technique
    you may hear mentioned is *weight decay*, which penalizes the network if it makes
    the weight values too large.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，获取更多的训练数据可能并不可行。替代方法包括调整训练算法，引入一些技术，防止网络在学习过程中专注于训练数据中无关的细节。你可能听说过的一种技术是*权重衰减*，它会惩罚网络，如果它使权重值过大。
- en: Another common approach is [*data augmentation*](glossary.xhtml#glo26). Out
    of training data? No worries, data augmentation will invent some by slightly modifying
    the data you already have. Data augmentation takes the existing training data
    and mutates it to produce new data that might plausibly have been created by the
    same process that made the actual training data. For example, if the training
    sample is a picture of a dog, it will still be a picture of a dog if you rotate
    it, shift it up a few pixels, flip it left to right, and so on. Each transformation
    produces a new training sample. It might seem like cheating, but in practice,
    data augmentation is a powerful [*regularizer*](glossary.xhtml#glo86) that keeps
    the network from overfitting during training.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的方法是[*数据增强*](glossary.xhtml#glo26)。训练数据不够？别担心，数据增强会通过稍微修改你已有的数据来“发明”一些新的数据。数据增强会将现有的训练数据进行变换，产生新的数据，这些新数据可能是由与实际训练数据相同的过程所生成的。例如，如果训练样本是一张狗的图片，那么如果你对其进行旋转、上移几个像素、左右翻转等操作，它依然会是一张狗的图片。每次变换都会生成一个新的训练样本。这看起来可能像是作弊，但实际上，数据增强是一种强大的[*正则化方法*](glossary.xhtml#glo86)，可以防止网络在训练过程中发生过拟合。
- en: Let’s return for a moment to initialization, as its importance was not sufficiently
    appreciated for many years.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们暂时回到初始化问题，因为它的重要性在多年来并未得到充分重视。
- en: At first, weight initialization meant nothing more than “pick a small random
    number” like 0.001 or –0.0056\. That worked much of the time. However, it didn’t
    work consistently, and when it did work, the network’s behavior wasn’t stellar.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，权重初始化仅仅意味着“选择一个小的随机数”，比如0.001或-0.0056\。这种方法大多数情况下都有效。但它并不总是有效，而且当它有效时，网络的表现也不一定出色。
- en: 'Shortly after the advent of deep learning, researchers revisited the “small
    random value” idea in search of a more principled approach to initialization.
    The fruit of those efforts is the way neural networks are initialized to this
    day. Three factors need to be considered: the form of the activation function,
    the number of connections coming from the layer below (*fan-in*), and the number
    of outputs to the layer above (*fan-out*). Formulas were devised to use all three
    factors to select the initial weights for each layer. Bias values are usually
    initialized to zero. It isn’t difficult to demonstrate that networks so initialized
    perform better than those initialized the old-fashioned way.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习出现不久后，研究人员重新审视了“随机小值”的思路，寻求一种更有原则的初始化方法。这些努力的成果就是神经网络至今使用的初始化方法。需要考虑三个因素：激活函数的形式、来自下层的连接数量（*fan-in*）以及到上层的输出数量（*fan-out*）。为了选定每层的初始权重，研究人员设计了公式，结合这三个因素进行选择。偏置值通常初始化为零。证明起来并不困难，使用这种初始化方法的网络性能优于传统方式初始化的网络。
- en: 'We have two steps of the training algorithm yet to discuss: backpropagation
    and gradient descent. Backpropagation is often presented first because its output
    is necessary for gradient descent. However, I think it’s more intuitive to understand
    what gradient descent is doing, then fill in the missing piece it needs with what
    backpropagation provides. Despite the unfamiliar names, I am certain you already
    understand the essence of both algorithms.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还有两个训练算法的步骤没有讨论：反向传播和梯度下降。反向传播通常先介绍，因为它的输出是梯度下降所必需的。然而，我认为理解梯度下降在做什么更为直观，然后通过反向传播提供的内容来填补缺失的部分。尽管这些名称可能不太熟悉，但我相信你已经理解了这两种算法的本质。
- en: '****'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '****'
- en: You’re standing in a vast, open grassland of rolling hills. How did you get
    here? You strain your brain, but no answer comes. Then, finally, you spy a small
    village to the north, in the valley far below. Perhaps the people there can give
    you some answers. But what’s the best way to get there?
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 你站在一个广阔的、起伏的草原上。你是怎么到这里的？你绞尽脑汁，但没有答案。然后，终于，你发现北方远处山谷里有一个小村庄，也许那里的居民能给你一些答案。但前往那里最好的方法是什么呢？
- en: You want to go north and down, in general, but you must also respect the contour
    of the land. You always want to move from a higher to a lower position. You can’t
    go due north because a large hill is in your way. You could head northeast; the
    terrain is flatter there, but going that way will make your journey a long one,
    as the land drops slowly. So, you decide to head northwest, as that moves you
    both north and down more steeply than to the east. You take a step to the northwest,
    then pause to reassess your position to decide which direction to move in next.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，你想要向北和向下移动，但也必须尊重地形的轮廓。你总是想从高处移动到低处。你不能直接往北走，因为一座大山阻碍了你的路。你可以朝东北走；那里的地势更平坦，但走那条路会让你的旅程变得漫长，因为地面下坡很缓慢。所以，你决定朝西北走，因为这条路既能让你向北又能更陡地向下。你向西北迈出一步，然后停下来重新评估自己的位置，决定下一步该往哪个方向走。
- en: Repeating this two-stage process of examining your current position to determine
    the direction that best moves you both northward and downward, then taking a step
    in that direction, is your best bet for reaching the village in the valley. You
    may not make it; you might get stuck in a small canyon out of which you can’t
    climb. But overall, you’ll make progress toward your goal by consistently moving
    in a direction that is north and down relative to your current position.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 重复这个两阶段的过程：先检查你当前的位置，以确定最佳的方向，既能向北又能向下移动，然后朝这个方向迈步，这是你到达山谷中村庄的最佳选择。你可能不会成功；你可能会被困在一个小峡谷里，无法攀爬出来。但总体而言，通过持续朝着一个相对你当前位置既向北又向下的方向移动，你会向目标取得进展。
- en: Following this process, known as [*gradient descent*](glossary.xhtml#glo52),
    lets us adjust a neural network’s initial weights and biases to give us ever better-performing
    models. In other words, gradient descent trains the model.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循这个过程，称为[*梯度下降*](glossary.xhtml#glo52)，使我们能够调整神经网络的初始权重和偏置，从而获得表现越来越优秀的模型。换句话说，梯度下降训练了模型。
- en: The three-dimensional world of the grassland surrounding the village corresponds
    to the *n*-dimensional world of the network, where *n* is the total number of
    weights and biases whose values we are trying to learn. Choosing a direction to
    head in from your current position and then moving some distance in that direction
    is a gradient descent step. Repeated gradient descent steps move you closer and
    closer to the village.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 围绕村庄的三维草原世界对应着网络的*n*维世界，其中*n*是我们试图学习的所有权重和偏置的总数。从当前位置选择一个方向并沿该方向移动一定距离，就是一个梯度下降步骤。重复的梯度下降步骤会使你越来越接近村庄。
- en: Gradient descent seeks the minimum position, the village in the valley—but the
    minimum of what? For a neural network, gradient descent aims to adjust the weights
    and biases of the network to minimize the error over the training set.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降寻求最小值的位置，即山谷中的村庄——但最小的是哪个值呢？对于神经网络而言，梯度下降的目标是调整网络的权重和偏置，以最小化训练集上的误差。
- en: The vast, open grassland of rolling hills represents the error function, the
    average error over the training data when using the weight and bias values corresponding
    to your current position. This means that each position in the grassland implies
    a complete set of network weights and biases. The position of the village corresponds
    to the smallest error the network can make on the training set. The hope is that
    a model that has a small error on its training set will make few errors on unknown
    inputs when used in the wild. Gradient descent is the algorithm that moves through
    the space of weights and biases to minimize the error.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 起伏的广袤草原代表了误差函数，即在使用当前权重和偏置值时，训练数据上的平均误差。这意味着草原中的每个位置都对应一组完整的网络权重和偏置。村庄的位置对应网络在训练集上能够达到的最小误差。我们的期望是，一个在训练集上有小误差的模型在实际应用中对未知输入的误差也会很少。梯度下降是一个通过权重和偏置的空间移动，以最小化误差的算法。
- en: Gradient descent is an optimization algorithm, again telling us that training
    a neural network is an optimization problem, a problem where we need to find the
    best set of something. While this is true, it is also true that training a neural
    network is subtly different from other optimization problems. As mentioned previously,
    we don’t necessarily want the smallest possible error on the training data, but
    rather the model that best generalizes to unknown inputs. We want to avoid overfitting.
    I’ll demonstrate visually what that means later in the chapter.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是一种优化算法，再次告诉我们训练神经网络是一个优化问题，一个我们需要找到最佳一组参数的问题。虽然这是真的，但训练神经网络与其他优化问题也有细微的不同。如前所述，我们不一定要在训练数据上得到最小的误差，而是要得到一个能够最好地泛化到未知输入的模型。我们希望避免过拟合。我将在本章稍后通过图示演示这意味着什么。
- en: Gradient descent moves through the landscape of the error function. In everyday
    use, a gradient is a change in something, like the steepness of a road or a color
    gradient varying smoothly from one shade to another. Mathematically, a gradient
    is the multidimensional analog of the slope of a curve at a point. The steepest
    direction to move is down the maximum gradient. The slope of a line at a point
    on a curve is a helpful representation of the gradient, so contemplating slopes
    is a worthy use of our time.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降在误差函数的地形中移动。在日常使用中，梯度指的是某种事物的变化，比如道路的坡度或颜色渐变平滑地从一种色调变到另一种色调。从数学上讲，梯度是曲线在某一点的斜率的多维类比。最陡的移动方向是沿着最大梯度下降。曲线在某一点的斜率是梯度的一个有用表示，因此思考斜率是值得我们花时间的。
- en: '[Figure 4-4](ch04.xhtml#ch04fig04) shows a curve with four lines touching it
    at different points. The lines represent the slope at those points. The slope
    indicates how quickly the value of the function changes in the vicinity of the
    point. The steeper the line, the faster the function’s value changes as you move
    along the *x*-axis.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4-4](ch04.xhtml#ch04fig04)展示了一条曲线，四条线在不同的点触及曲线。这些线代表了这些点的斜率。斜率表示函数值在该点附近变化的速度。线越陡，沿着*x*轴移动时，函数值变化得越快。'
- en: '![Image](../images/ch04fig04.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/ch04fig04.jpg)'
- en: '*Figure 4-4: A curve with the slope at various points marked*'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4-4：标出不同点斜率的曲线*'
- en: Line B marks the lowest point on the curve. This is the [*global minimum*](glossary.xhtml#glo51)
    and the point that an optimization algorithm seeks to find. Notice that the line
    touching this point is entirely horizontal. Mathematically, this means that the
    slope of line B is zero. This is true at the minima (and maxima) of functions.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 线B标记了曲线的最低点。这就是[*全局最小值*](glossary.xhtml#glo51)，也是优化算法试图找到的点。注意，接触该点的线完全水平。从数学角度来看，这意味着线B的坡度为零。这在函数的最小值（和最大值）处成立。
- en: The point touched by line B is the global minimum, but there are three other
    minima in the plot. These are *local minima*, points where the slope of the line
    touching those points is also zero. Ideally, an optimization algorithm would avoid
    these points, favoring the global minimum.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 线B接触的点是全局最小值，但图中还有三个其他最小值。这些是*局部最小值*，即接触这些点的线的坡度也为零。理想情况下，优化算法应避免这些点，优先选择全局最小值。
- en: Line A is steep and points toward the global minimum. Therefore, if we were
    at the point on the curve touched by line A, we could move quickly toward the
    global minimum by taking steps in the indicated direction. Moreover, as the slope
    is steep here, we can take reasonably large steps down to the valley.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 线A的坡度较陡，指向全局最小值。因此，如果我们在曲线上位于线A接触的点，我们可以通过朝着指示方向迈步，迅速朝着全局最小值移动。此外，由于此处坡度较陡，我们可以迈出相对较大的步伐，迅速下降到谷底。
- en: Line C is also steep but heads toward one of the local minima, the one just
    beyond 3 on the *x*-axis. A gradient descent algorithm that only knows how to
    move down the gradient will locate that local minimum and become stuck there.
    The same applies to line D, which heads toward the local minimum between 4 and
    5 on the *x*-axis.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 线C的坡度也很陡，但它指向其中一个局部最小值，即* x *轴上3右侧的最小值。一个只知道沿着梯度下降的梯度下降算法将定位到该局部最小值并被困在那里。同样，线D也指向*
    x *轴上4与5之间的局部最小值。
- en: What are the takeaways from [Figure 4-4](ch04.xhtml#ch04fig04)? First, gradient
    descent moves down the gradient, or slope, from some point. Here the curve is
    one- dimensional, so the point is a specific value of *x*. Gradient descent uses
    the value of the slope at that point to pick a direction and a step size proportional
    to the steepness of the slope. A steep slope means we can take a larger step to
    end up at a new *x* value closer to a minimum. A shallow slope implies a smaller
    step.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 从[图4-4](ch04.xhtml#ch04fig04)中可以得出什么结论？首先，梯度下降是沿着某一点的梯度或坡度向下移动的。这里曲线是一维的，因此该点是*
    x *的一个特定值。梯度下降利用该点的坡度值来选择一个方向，并根据坡度的陡峭程度确定步长。坡度陡峭意味着我们可以迈出较大的步伐，最终到达更接近最小值的新*
    x *值。坡度较浅则意味着步伐较小。
- en: For example, suppose we are initially at the point where line A touches the
    curve. The slope is steep, so we take a big step toward the global minimum. After
    the step, we look at the slope again, but this time it’s the slope at the new
    point on the *x*-axis. Using that slope, we take another step, then another, and
    another until we get to a point where the slope is essentially zero. That’s the
    minimum, so we stop.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们最初位于线A与曲线接触的点。此时的坡度较陡，因此我们朝着全局最小值迈出大步。迈出这一步后，我们再次查看坡度，不过这次是新点上* x *轴的坡度。利用这个坡度，我们再迈出一步，然后是下一步，直到我们到达坡度基本为零的点。那就是最小值，所以我们停下。
- en: The one-dimensional case is straightforward enough because at each point there
    is only one slope, so there is only one direction to go. However, recalling the
    vast, open grassland, we know that from any point there are an infinite number
    of directions we might head in, many of which are useful in that they move us
    northward and downward. One of these directions, the direction of the maximum
    gradient, is the steepest and moves us most quickly toward our desired destination,
    and that’s the direction we step in. Repeating the process, using the maximum
    gradient direction each time, accomplishes in multiple dimensions what we did
    in one dimension. To be precise, we step in the direction *opposite* the maximum
    gradient because the maximum gradient points away from the minimum, not toward
    it.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 一维情况比较简单，因为每个点只有一个坡度，因此只有一个方向可以前进。然而，回想起广袤的草原，我们知道从任何一点出发，我们都可以朝着无数方向前进，其中许多方向都很有用，因为它们能带我们向北和向下移动。所有这些方向中，最大梯度的方向是最陡的，能够最快地带我们走向期望的目标，这也是我们迈出的方向。通过重复这个过程，每次使用最大梯度方向，我们就能在多维空间中实现一维空间中的目标。准确来说，我们应该朝着与最大梯度方向*相反*的方向迈步，因为最大梯度指向远离最小值的方向，而不是指向它。
- en: '[Figure 4-5](ch04.xhtml#ch04fig05) presents gradient descent in two dimensions.
    The figure shows a contour plot. Imagine an open pit mine with terraced levels:
    the lighter the shade, the deeper into the mine, but also the flatter the slope.
    That is, lighter shades imply shallower slopes.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-5](ch04.xhtml#ch04fig05)展示了二维中的梯度下降法。图中显示了一个等高线图。可以想象一个露天矿井的阶梯式层次：颜色越浅，表示矿井越深，但坡度也越平缓。也就是说，颜色越浅表示坡度越浅。'
- en: '![Image](../images/ch04fig05.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch04fig05.jpg)'
- en: '*Figure 4-5: Gradient descent in two dimensions*'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-5：二维中的梯度下降法*'
- en: 'The figure shows the path taken by gradient descent for three starting positions:
    the circle, the triangle, and the square. Initially, the slopes are steep, so
    the step sizes are big, but the slopes become shallow as the minimum is approached,
    implying smaller steps. Eventually, gradient descent reaches the minimum, regardless
    of the starting point.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图中显示了梯度下降法从三个起始位置出发的路径：圆形、三角形和正方形。最初，坡度较陡，因此步长较大，但随着逼近最小值，坡度变浅，意味着步长变小。最终，梯度下降法不论起点如何，都会到达最小值。
- en: We’ve discussed gradient descent in one and two dimensions because we can visualize
    the process. We understand now that we have always known the algorithm and used
    it ourselves whenever we walk from a higher elevation to a lower one. Honestly,
    this is all that training a neural network does. The initial set of weights and
    biases is nothing more than a single starting point in an *n*-dimensional space.
    Gradient descent uses the maximum gradient from that initial starting position
    to march toward a minimum. Each new position in the *n*-dimensional space is a
    new set of the *n* weights and biases generated from the previous set based on
    the steepness of the gradient. When the gradient gets very small, we claim victory
    and fix the weights and biases, believing the network to be trained.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在一维和二维中讨论了梯度下降法，因为我们可以直观地看到这个过程。现在我们明白了，我们一直都知道这个算法，并且在自己从高处走到低处时也在使用它。说实话，这就是训练神经网络的全部内容。初始的权重和偏置集不过是*n*维空间中的一个起始点。梯度下降法利用从该起始位置出发的最大梯度，向最小值迈进。在*n*维空间中的每一个新位置，都是根据梯度的陡峭程度，从上一组权重和偏置中生成的一组新的权重和偏置。当梯度非常小时，我们宣告胜利并固定权重和偏置，认为网络已经训练完成。
- en: Gradient descent depends on slopes, on the value of the gradient. But where
    do the gradients come from? Gradient descent minimizes the loss function, or the
    error made by the network. The error over the training set is a function of each
    weight and bias value in the network. The gradient represents how much each weight
    and bias contributes to the overall error.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降法依赖于坡度，即梯度的值。那么，梯度从哪里来呢？梯度下降法是通过最小化损失函数，或者说最小化网络所犯的错误，来实现的。训练集上的误差是每个网络中权重和偏置值的函数。梯度表示每个权重和偏置对整体误差的贡献程度。
- en: For example, suppose we know how much weight 3 (whatever weight that labels)
    contributes to the network’s error as measured by the mistakes the network makes
    on the training set. In that case, we know the steepness of the gradient should
    we change weight 3’s value, keeping all other weights and biases the same. That
    steepness, multiplied by a step size, gives us a value to subtract from weight
    3’s current value. By subtracting, we move in the direction opposite to the maximum
    gradient. Repeating the calculation for every weight and bias in the network takes
    a step in the *n*-dimensional space. This is what gradient descent does during
    training.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，假设我们知道权重 3（无论是什么权重）的值对网络错误的贡献有多大，这个错误是通过网络在训练集上所犯的错误来衡量的。这样，我们就知道如果改变权重
    3 的值，而保持其他权重和偏置不变，梯度的陡峭程度应该是多少。这个陡峭程度，乘以步长，给我们一个值，从权重 3 当前的值中减去。通过减去这个值，我们就朝着与最大梯度方向相反的方向移动。对网络中每个权重和偏置重复这个计算，就完成了在*n*维空间中的一次步伐。这就是梯度下降法在训练过程中所做的。
- en: '[*Backpropagation*](glossary.xhtml#glo10) is the algorithm that gives us the
    steepness values per weight and bias. Backpropagation is an application of a well-known
    rule from differential calculus, the branch of mathematics telling us how one
    thing changes as another changes. Speed is an example. Speed indicates how distance
    changes with time. It’s even in how we talk about speed: miles per hour or kilometers
    per hour. Backpropagation gives us the “speed” representing how the network’s
    error changes with a change in any weight or bias value. Gradient descent uses
    these “speeds,” multiplied by a scale factor known as the [*learning rate*](glossary.xhtml#glo61),
    to step to the next position in the *n*-dimensional space represented by the *n*
    weights and biases of the network.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[*反向传播*](glossary.xhtml#glo10)是给我们每个权重和偏置提供陡度值的算法。反向传播是微积分中一个著名规则的应用，微积分是数学的一个分支，告诉我们一个事物如何随着另一个事物的变化而变化。速度就是一个例子。速度表示距离随着时间的变化。我们谈论速度时，也是这样表达的：每小时多少英里或多少公里。反向传播给我们提供了“速度”，表示网络的误差如何随着任何权重或偏置值的变化而变化。梯度下降使用这些“速度”，乘以一个称为[*学习率*](glossary.xhtml#glo61)的比例因子，步进到由网络的*n*个权重和偏置所代表的*n*维空间中的下一个位置。'
- en: For example, the “big” network in [Figure 4-2](ch04.xhtml#ch04fig02) has 32
    weights and 9 biases; therefore, training that network with gradient descent means
    moving through a 41-dimensional space to find the 41 weight and bias values giving
    us the smallest error averaged over the training set.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，[图4-2](ch04.xhtml#ch04fig02)中的“巨大”网络有32个权重和9个偏置；因此，使用梯度下降训练该网络意味着在一个41维的空间中移动，以找到41个权重和偏置值，从而在训练集上得到最小的平均误差。
- en: The algorithm is called “backpropagation” because it calculates the “speed”
    values for each weight and bias, beginning with the network’s output layer and
    then moving backward, layer by layer, to the input layer. That is, it moves backward
    through the network to propagate the error from a layer to the previous layer.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法被称为“反向传播”，因为它计算每个权重和偏置的“速度”值，首先从网络的输出层开始，然后逐层向后移动，直到输入层。也就是说，它通过网络反向传播误差，从一层传播到前一层。
- en: 'The take-home message is this:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 结论是这样的：
- en: Gradient descent uses the gradient direction supplied by backpropagation to
    iteratively update the weights and biases to minimize the network’s error over
    the training set.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降使用反向传播提供的梯度方向，迭代地更新权重和偏置，以最小化网络在训练集上的误差。
- en: And that, in a nutshell, is how neural networks are trained.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是神经网络训练的核心原理。
- en: '****'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '****'
- en: The ability to train a neural network with backpropagation and gradient descent
    is a bit of a fluke. It shouldn’t work. Gradient descent with backpropagation
    is a *first-order* optimization approach. First-order optimization works best
    with simple functions, and the error surfaces of a neural network are anything
    but. However, Fortuna has smiled upon us, and it does work, and rather well at
    that. There is as yet no rigorous mathematical explanation beyond the realization
    that the local minima of the error function are all pretty much the same, meaning
    if you land in one and can’t get out, that’s often just fine.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 使用反向传播和梯度下降训练神经网络的能力有点像偶然的运气。理论上它不应该有效。带有反向传播的梯度下降是一种*一阶*优化方法。一阶优化适用于简单的函数，而神经网络的误差曲面可远非简单。然而，幸运之神眷顾了我们，它确实有效，而且效果相当不错。到目前为止，还没有严格的数学解释，除了认识到误差函数的局部极小值几乎是相同的，这意味着如果你落入其中并无法逃脱，通常也没关系。
- en: There is another empirical explanation, but to understand that, we must learn
    more about the training process. The six-step training algorithm I gave earlier
    in the chapter talks about running the training set, or a subset of it, through
    the network, and repeating until things are “good enough.” Let me expand on the
    process implied by these steps.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 还有另一种经验性的解释，但要理解这一点，我们必须更多地了解训练过程。我在本章前面给出的六步训练算法讲述了如何将训练集或其子集输入到网络中，并反复执行，直到达到“足够好”的效果。让我来扩展一下这些步骤所暗示的过程。
- en: Each pass of training data through the network, a forward pass followed by a
    backward pass, results in a gradient descent step as shown in [Figure 4-5](ch04.xhtml#ch04fig05).
    If the training set is small, all of it is used in the forward pass, meaning all
    of it is used by gradient descent to decide where to step next. A complete pass
    through the training data is called an [*epoch*](glossary.xhtml#glo37); therefore,
    using all the training data in the forward and backward passes results in one
    gradient descent step per epoch.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 每次将训练数据通过网络传递时，首先进行前向传递，然后进行反向传递，最终会得到一个梯度下降步骤，如[图4-5](ch04.xhtml#ch04fig05)所示。如果训练集较小，则会在前向传递中使用所有数据，这意味着所有数据都会被梯度下降用来决定下一步的方向。对训练数据的完整传递称为[*epoch*](glossary.xhtml#glo37)；因此，在前向传递和反向传递中使用所有训练数据会导致每个epoch执行一个梯度下降步骤。
- en: Modern machine learning datasets are often massive, making it computationally
    infeasible to use all of the training data for each gradient descent step. Instead,
    a small, randomly selected subset of the data, known as a [*minibatch*](glossary.xhtml#glo67),
    is passed through the network for the forward and backward passes. Using minibatches
    dramatically reduces the computational overhead during gradient descent, resulting
    in many steps per epoch. Minibatches also provide another benefit that helps overcome
    the “this approach to training shouldn’t work” issue.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现代机器学习数据集通常非常庞大，这使得在每次梯度下降步骤中使用所有训练数据变得在计算上不可行。相反，会有一个小的、随机选择的数据子集，称为[*小批量*](glossary.xhtml#glo67)，通过网络进行前向和反向传递。使用小批量显著减少了梯度下降中的计算开销，从而在每个epoch中执行多个步骤。小批量还提供了另一个好处，帮助克服“这种训练方法不该有效”的问题。
- en: Suppose we had a mathematical function representing the error made by the network.
    In that case, we could use centuries-old calculus techniques to find the exact
    form of each weight and bias’s contribution to the error; gradient descent would
    know the best direction to step each time. Unfortunately, the world isn’t that
    kind. We don’t know the mathematical form of the error function (there isn’t likely
    one to know), so we have to approximate with our training data. This approximation
    improves when using more training data to determine the error. This fact argues
    for using all the training data for each gradient descent step. However, we already
    know this is computationally extremely taxing in many cases.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个数学函数，表示网络的误差。在这种情况下，我们可以使用几个世纪前的微积分技术来找到每个权重和偏差对误差的确切贡献；梯度下降将知道每次应该朝哪个方向前进。不幸的是，世界并非如此。我们不知道误差函数的数学形式（很可能也没有一个可以知道的形式），因此我们必须使用训练数据进行近似。当使用更多训练数据来确定误差时，这种近似会得到改善。这一事实支持了在每次梯度下降步骤中使用所有训练数据。然而，我们已经知道，在许多情况下，这在计算上是极其繁重的。
- en: The compromise is to use minibatches for each gradient descent step. The calculations
    are no longer too taxing, but the approximation of the actual gradient is worse
    because we are estimating it with fewer data points. Randomly selecting something
    is often attached to the word “stochastic,” so training with minibatches is known
    as [*stochastic gradient descent*](glossary.xhtml#glo91). Stochastic gradient
    descent, in one form or another, is the standard training approach used by virtually
    all modern AI.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 妥协的做法是，在每次梯度下降步骤中使用小批量。这样，计算量不再太大，但由于我们使用较少的数据点来估算梯度，实际梯度的近似会变差。随机选择某些东西通常与“随机”一词相关联，因此，使用小批量进行训练被称为[*随机梯度下降*](glossary.xhtml#glo91)。随机梯度下降，某种形式或另一种形式，是几乎所有现代AI使用的标准训练方法。
- en: At first blush, stochastic gradient descent sounds like a losing proposition.
    Sure, we can calculate many gradient descent steps before the heat death of the
    universe, but our gradient fidelity is low, and we’re likely moving in the wrong
    direction through the error space. That can’t be good, can it?
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 一开始，随机梯度下降听起来像是一个失败的方案。当然，我们可以在宇宙的热寂之前计算出很多梯度下降步骤，但我们的梯度精度较低，而且我们可能正在沿着错误的方向穿越误差空间。这肯定不行，对吧？
- en: Here’s where Fortuna smiles on humanity a second time. Not only has she given
    us the ability to train complex models with first-order gradient descent because
    local minima are (assumed) roughly equivalent; she’s also arranged things so that
    the “wrong” gradient direction found by stochastic gradient descent is often what
    we need to avoid local minima early in the training process. In other words, walking
    slightly northeast when we should head due north is a blessing in disguise that
    allows us to train large neural networks.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 就在此时，幸运女神再次眷顾了人类。她不仅赋予了我们通过一阶梯度下降训练复杂模型的能力，因为局部最小值（假设）大致是等价的；她还安排了事情，使得随机梯度下降找到的“错误”梯度方向往往正是我们需要的，可以帮助我们避免在训练初期进入局部最小值。换句话说，当我们应该朝正北走时，稍微朝东北走是一种暗藏的祝福，能够让我们训练大型神经网络。
- en: '****'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '****'
- en: We’re ready to move on to the next chapter. However, before we do, let’s apply
    traditional neural networks to the dinosaur footprint dataset. We’ll compare the
    results to the classical models of [Chapter 3](ch03.xhtml).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们准备进入下一章。然而，在此之前，让我们将传统神经网络应用于恐龙足迹数据集。我们将把结果与[第3章](ch03.xhtml)的经典模型进行比较。
- en: 'We need first to select an architecture: that is, the number of hidden layers,
    the number of nodes per layer, and the type of activation function for each node.
    The dinosaur footprint dataset has two classes: ornithischian (class 0) and theropod
    (class 1). Therefore, the output node should use a sigmoid activation function
    to give us a likelihood of class 1 membership. The network’s output value estimates
    the probability that the input image represents a theropod. If the probability
    is above 50 percent, we’ll assign the input to class 1; otherwise, into class
    0 it goes. We’ll stick with rectified linear unit activations for the hidden layer
    nodes, as we have for all the models in this chapter. All that remains is to select
    the number of hidden layers and the number of nodes per layer.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要选择一个架构：即隐藏层的数量、每层的节点数以及每个节点的激活函数类型。恐龙足迹数据集有两个类别：鸟臀目（类别0）和兽脚亚目（类别1）。因此，输出节点应使用sigmoid激活函数，以给出类别1的可能性。网络的输出值估算输入图像属于兽脚亚目的概率。如果概率超过50％，我们将把输入分配到类别1；否则，将其归类为类别0。我们将继续使用修正线性单元（ReLU）激活函数来处理隐藏层节点，就像本章中的所有模型一样。剩下的就是选择隐藏层的数量和每层节点的数量。
- en: There are 1,336 training samples in the footprints dataset. That’s not a lot,
    and we aren’t augmenting the dataset, so we need a smallish model. Large models,
    meaning many nodes and layers, require large training sets; otherwise, there are
    too many weights and biases to learn relative to the number of training samples.
    Therefore, we’ll limit ourselves to trying at most two hidden layer models for
    the footprints dataset. As for the number of nodes in the hidden layers, we’ll
    let the first hidden layer vary from very small to nearly twice the input size
    of 1,600 features (the 40×40-pixel image unraveled). If we try a second hidden
    layer, we’ll restrict the number of nodes to no more than half the number in the
    first hidden layer.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 恐龙足迹数据集中有1,336个训练样本。这并不多，并且我们没有对数据集进行增强，所以我们需要一个较小的模型。大模型意味着很多节点和层，需要大量的训练集；否则，学习的权重和偏置相对于训练样本数量来说就会过多。因此，我们将限制自己最多尝试两层隐藏层的模型。至于隐藏层中的节点数，我们将让第一隐藏层的节点数从非常小到接近输入大小的两倍（1,600个特征，即40×40像素图像展开后）。如果我们尝试第二隐藏层，我们将限制第二层的节点数不超过第一层节点数的一半。
- en: First, we’ll train a collection of one- and two-layer architectures. Second,
    we’ll train the best performing of those 100 times to give us an average level
    of performance. [Table 4-1](ch04.xhtml#ch04tab1) presents the trial models’ results.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将训练一组一层和两层的架构。然后，我们将训练表现最好的模型100次，以获得平均性能水平。[表4-1](ch04.xhtml#ch04tab1)展示了试验模型的结果。
- en: '**Table 4-1:** Trial Architectures with the Dinosaur Footprint Dataset'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '**表4-1：** 恐龙足迹数据集的试验架构'
- en: '| **Accuracy (%)** | **Architecture** | **Weights and biases** |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| **准确度（%）** | **架构** | **权重和偏置** |'
- en: '| --- | --- | --- |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 59.4 | 10 | 16,021 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 59.4 | 10 | 16,021 |'
- en: '| 77.0 | 400 | 640,801 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 77.0 | 400 | 640,801 |'
- en: '| 76.7 | 800 | 1,281,601 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 76.7 | 800 | 1,281,601 |'
- en: '| **81.2** | **2,400** | **3,844,801** |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| **81.2** | **2,400** | **3,844,801** |'
- en: '| 75.8 | 100, 50 | 165,201 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 75.8 | 100, 50 | 165,201 |'
- en: '| **81.2** | **800, 100** | **1,361,001** |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| **81.2** | **800, 100** | **1,361,001** |'
- en: '| 77.9 | 2,400, 800 | 5,764,001 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 77.9 | 2,400, 800 | 5,764,001 |'
- en: The network with a mere 10 nodes in its hidden layer was the worst, returning
    an accuracy of about 60 percent. A binary classifier that does nothing but flips
    a coin is correct about 50 percent of the time, so the 10-node network is performing
    only slightly above chance. We don’t want that one. Most of the other networks
    return accuracies in the mid- to upper 70s.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 只有10个节点的网络表现最差，准确率约为60%。一个仅仅抛硬币的二分类器，正确率大约是50%，所以10节点网络的表现只比随机稍好一些。我们不想要这个模型。其他大部分网络的准确率都在中高70%的区间。
- en: The two models in **bold** each produced just over 81 percent accuracy. The
    first used a single hidden layer of 2,400 nodes. The second used a hidden layer
    of 800 nodes, followed by another with 100 nodes. Both models produced the same
    accuracy on the test set, but the 2,400-node model had nearly three times as many
    weights and biases as the two-layer model, so we’ll go with the two-layer model.
    (Bear in mind that the results in [Table 4-1](ch04.xhtml#ch04tab1) represent a
    single training session, not the average of many. We’ll fix that shortly.)
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种**粗体**模型的准确率均略超过81%。第一种模型使用了一个包含2,400个节点的单隐层。第二种模型使用了一个包含800个节点的隐层，接着是一个包含100个节点的隐层。两种模型在测试集上得到了相同的准确率，但2,400节点模型的权重和偏置几乎是两层模型的三倍，所以我们选择使用两层模型。（请记住，[表4-1](ch04.xhtml#ch04tab1)中的结果仅代表一次训练会话，而不是多次训练的平均值。稍后我们会修正这一点。）
- en: The two-layer model is still relatively large. We’re trying to learn 1.4 million
    parameters to condition the model to correctly classify the dinosaur footprint
    images. That’s a lot of parameters to learn, especially with a training set of
    only 1,336 samples. Fully connected neural networks grow quickly in terms of the
    number of parameters required. We’ll revisit this observation in [Chapter 5](ch05.xhtml)
    when discussing convolutional neural networks.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 两层模型依然相对较大。我们试图学习140万个参数，以便将模型调整到可以正确分类恐龙足迹图像。这是一个需要学习的大量参数，尤其是在训练集只有1,336个样本的情况下。完全连接的神经网络在所需参数数量上增长非常快。我们将在[第5章](ch05.xhtml)讨论卷积神经网络时重新回顾这一观察。
- en: 'We have our architecture: two hidden layers using rectified linear activation
    functions with 800 and 100 nodes, respectively, followed by a single node using
    a sigmoid to give us a likelihood of class 1 membership. Training the model 100
    times on the footprints dataset returned an average accuracy of 77.4 percent,
    with a minimum of 69.3 percent and a maximum of 81.5 percent. Let’s put this result
    in its proper relation to those of [Chapter 3](ch03.xhtml); see [Table 4-2](ch04.xhtml#ch04tab2).'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的架构是：两个隐层分别使用800个和100个节点的修正线性激活函数，然后是一个单节点，使用sigmoid函数输出类别1的概率。对足迹数据集进行100次训练后，模型的平均准确率为77.4%，最低为69.3%，最高为81.5%。让我们将这一结果与[第3章](ch03.xhtml)中的结果做一个对比；见[表4-2](ch04.xhtml#ch04tab2)。
- en: '**Table 4-2:** Dinosaur Footprint Models'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '**表4-2：** 恐龙足迹模型'
- en: '| **Model** | **Accuracy (%)** |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **准确率（%）** |'
- en: '| --- | --- |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| RF300 | 83.3 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| RF300 | 83.3 |'
- en: '| RBF SVM | 82.4 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| RBF SVM | 82.4 |'
- en: '| 7-NN | 80.0 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 7-NN | 80.0 |'
- en: '| 3-NN | 77.6 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 3-NN | 77.6 |'
- en: '| **MLP** | **77.4** |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| **MLP** | **77.4** |'
- en: '| 1-NN | 76.1 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 1-NN | 76.1 |'
- en: '| Linear SVM | 70.7 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 线性SVM | 70.7 |'
- en: Recall that RF300 means a random forest with 300 trees, SVM refers to a support
    vector machine, and, somewhat confusingly, NN refers to a nearest neighbor classifier.
    I’m using MLP (multilayer perceptron) as a stand-in for our neural network. [*Multilayer
    perceptron*](glossary.xhtml#glo70) is an old but still common name for the traditional
    neural networks we’ve been discussing in this chapter—notice the link back to
    Rosenblatt’s original Perceptron from the late 1950s.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 请回忆一下，RF300指的是一个包含300棵树的随机森林，SVM指的是支持向量机，而NN则有点令人困惑，指的是最近邻分类器。我这里使用MLP（多层感知机）作为神经网络的代替。[*多层感知机*](glossary.xhtml#glo70)是传统神经网络的一个旧名称，至今仍然常见——注意这里的链接指回到罗森布拉特1950年代末期的原始感知机。
- en: Our neural network wasn’t the best performer on this dataset. In fact, it was
    one of the worst. Additional tweaking might move it up a place or two on the list,
    but this level of performance is typical, in my experience, and contributed to
    the general perception (pun intended) before the deep learning revolution that
    neural networks are “meh” models—run-of-the-mill, nothing to write home about.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的神经网络在这个数据集上的表现并不是最好的。事实上，它是最差的之一。通过进一步调整，它的排名可能会有所提升，但根据我的经验，这种表现水平是典型的，也有助于深度学习革命前人们普遍认为神经网络是“平庸”的模型——平凡无奇，没什么值得大书特书的。
- en: '****'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '****'
- en: 'This chapter introduced the fundamental ideas behind modern neural networks.
    The remainder of the book builds on the basic concepts covered in this chapter.
    Here are the principal takeaways:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了现代神经网络背后的基本思想。本书的其余部分将在本章覆盖的基本概念基础上展开。以下是主要要点：
- en: Neural networks are collections of nodes (neurons) that accept multiple inputs
    and produce a single number as output.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络是由多个节点（神经元）组成的集合，它们接受多个输入并输出一个数字。
- en: Neural networks are often arranged in layers so that the current layer’s input
    is the previous layer’s output.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络通常按层排列，使得当前层的输入为上一层的输出。
- en: Neural networks are randomly initialized, so repeated training leads to differently
    performing models.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络是随机初始化的，因此重复训练会导致不同表现的模型。
- en: Neural networks are trained by gradient descent, using the gradient direction
    supplied by backpropagation to update the weights and biases iteratively.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络通过梯度下降进行训练，使用反向传播提供的梯度方向迭代地更新权重和偏差。
- en: Now, let’s press on to investigate convolutional neural networks, the architecture
    that ushered in the deep learning revolution. This chapter brought us to the early
    2000s. The next moves us to 2012 and beyond.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续研究卷积神经网络，这种架构引领了深度学习革命。本章带我们走进了2000年代初期，接下来我们将进入2012年及以后的发展。
- en: '**KEY TERMS**'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '**关键词**'
- en: activation function, architecture, backward pass, bias, data augmentation, epoch,
    forward pass, global minimum, gradient descent, hidden layer, learning rate, local
    minimum, loss, minibatch, multilayer perceptron, neuron, node, overfitting, preprocessing,
    rectified linear unit, regularizer, sigmoid, stochastic gradient descent, weight
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数、架构、反向传播、偏差、数据增强、周期（epoch）、前向传播、全局最小值、梯度下降、隐藏层、学习率、局部最小值、损失、迷你批次（minibatch）、多层感知机（multilayer
    perceptron）、神经元、节点、过拟合、预处理、修正线性单元（rectified linear unit）、正则化器、sigmoid、随机梯度下降、权重
