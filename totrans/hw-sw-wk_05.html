<html><head></head><body>
<h2 class="h2" id="ch05"><a id="page_85"/><strong><span class="big">5</span></strong><br/><strong>Game Graphics</strong></h2>
<div class="image1"><img alt="image" src="graphics/common-01.jpg"/></div>
<p class="noindent">A modern video game is like a modern movie—a big production that requires expertise in many different technical areas. Teams of programmers develop code for audio, artificial intelligence, network connectivity, and so on. Still, the first thing you notice about a video game is the graphics.</p>
<p class="indent">Early video game systems like the Atari 2600 and Sega Genesis relied on premade bitmap graphics; that is, there was no rendering, not even the 2D rendering described in the previous chapter. Instead, if a video game needed to show the game’s hero walking, an artist would draw several bitmaps to be shown in a repeating sequence. Backgrounds, too, were hand-drawn. Displays were low resolution and offered only a few choices for pixel colors.</p>
<p class="indent">As the quality of displays improved, game developers turned to other techniques to produce their bitmaps. Fighting games like <em>Mortal Kombat</em> would scan photographs of stunt actors in costume or at least use them for <a id="page_86"/>reference. Some games in this era would actually use rendered graphics, but not real-time rendering; instead they would prerender the bitmaps on more powerful systems over a longer period of time. The 3D game as we know it today was unknown outside of a few early experiments.</p>
<p class="indent">That started to change in the mid-1990s. Game consoles like the Sony PlayStation were built around 3D graphics capabilities instead of bitmaps. PC gamers began to purchase what were then called <em>graphics accelerators</em>— plug-in hardware to assist in the creation of 3D graphics. Those early 3D games were crude, both graphically and otherwise, compared to games today. Also, few 3D games were made for the PC because Microsoft had yet to build DirectX, a standardized interface between game software and graphics hardware, which meant that games had to include different code to match each manufacturer’s graphics accelerator.</p>
<p class="indent">Even so, gamers were hooked on the new 3D gaming, and each succeeding generation of graphics hardware blew away the capabilities of the previous one. Nowhere was this generational leap more apparent than in <em>cut scenes</em>—short, prerendered videos shown at the beginning of the game to set the scene, or at critical points during the game to advance the plot. Because these videos were prerendered on expensive hardware, just like the movie CGI we discussed in <a href="ch04.html#ch04">Chapter 4</a>, early cut scenes were much more impressive than the graphics during actual gameplay. As the hardware advanced, though, gameplay visuals began to match or even exceed the cut scenes of earlier games.</p>
<p class="indent">These days, few games use prerendered cut scenes. Although the game may still include noninteractive “movie” sequences to set up or advance the plot, they’re much more likely to be rendered in real time, just like the rest of the game. That’s because the real-time rendering looks so good, it’s not worth it for game developers to do anything else.</p>
<p class="indent">And that, I think, is why I find video game graphics so amazing. They look as good as or better than the prerendered graphics I saw in earlier video games, or even in early CGI movies, and they’re being produced in real time. Those two words—<em>real time</em>—look innocent enough, but they encapsulate an enormous challenge for a game renderer. To put it into numbers: if your typical gamer wants a refresh rate of 60 frames per second, each image must be rendered in a mere <sup>1</sup>/<sub>60</sub> of a second.</p>
<h3 class="h3" id="ch05lev1sec01"><strong>Hardware for Real-Time Graphics</strong></h3>
<p class="noindent">The increasing quality of real-time graphics is tied to advancements in graphics hardware. Today’s graphics hardware is powerful and optimized for the tasks involved in 3D graphical rendering. Although this book is about software, a brief discussion of hardware is necessary to understand why game graphics work the way they do.</p>
<p class="indent">The main processor inside a computer or video game console is the <em>central processing unit (CPU)</em>. These processors might have multiple <em>cores</em>, or independent processing subunits. Think of a core as an office worker. The cores inside a CPU are like fast, widely trained workers. They are good at <a id="page_87"/>doing just about any task, and doing it very quickly. However, they are so expensive that you can afford to have only a few of them, usually eight or fewer in a typical desktop processor, although this number will continue to rise.</p>
<p class="indent">By contrast, a <em>graphics processing unit (GPU)</em> will have hundreds or even thousands of cores. These cores are much simpler, and individually slower, than the cores in a CPU. Think of them as workers who can do only a few tasks well, and don’t do those tasks especially fast, but they are so affordable that you can have an army of them. This hardware approach for GPUs was adopted because there’s only so much improvement that can be made to the speed of individual cores. Even though the raw speed of cores increased with each generation, that wasn’t nearly enough to close the performance gap to allow high-quality real-time rendering; the only solution was more cores.</p>
<p class="indent">CPUs, then, are great at tasks with steps that have to be completed in a specified order, like filling in a tax form. GPUs, though, are better at tasks that can be easily divided among many workers, like painting the outside of a house. Game renderers are designed to keep all of the GPU cores as busy as possible.</p>
<h3 class="h3" id="ch05lev1sec02"><strong>Why Games Don’t Ray Trace</strong></h3>
<p class="noindent">We saw in the preceding chapter how ray tracing can produce amazing graphics. But games don’t ray trace, because it’s too slow for real-time rendering. There are several reasons for this.</p>
<p class="indent">One reason is that ray tracing doesn’t match up well with the “army of workers” GPU design. For example, ray tracing sends out a beam of light for each pixel, determines where that beam strikes, and from that point of impact, sends out a bunch more light beams, determines where they strike, and so on. This job is better suited for a CPU, because the renderer must determine each point of impact before it knows what beams to check next.</p>
<p class="indent">More broadly, realtime renders should expend computational effort where the result makes a difference to the viewer. Consider a computer-generated scene in which you face a chair in the middle of a polished wooden floor. A ray tracer, pinballing light around the room, would still indirectly determine the color of every point on the back of the chair, because that data is necessary for proper global illumination of the floor. A game renderer, though, could never afford the luxury of coloring a surface that won’t be directly seen.</p>
<h3 class="h3" id="ch05lev1sec03"><strong>All Lines and No Curves</strong></h3>
<p class="noindent">To understand how a video game renders without ray tracing, we start with the basic building block of game graphics: the triangle. In the previous chapter we learned how CGI models in movies are made of lines and curves. In game rendering, models are normally made exclusively of lines. If you remember graphing parabolas in high school algebra, you’ll recall that the math for describing curves is a lot more complicated than the math <a id="page_88"/>for describing lines, and there’s just not enough time to deal with curves in a game. That’s why game renderers use lines, and this means that the surfaces defined by the control points are flat. The simplest flat surface is a triangle, defined by three points in space.</p>
<p class="indent">Triangles are ubiquitous in games. In a game, whatever you think you’re looking at, you’re actually looking at millions of triangles, joined at angles to create surfaces and shapes. Triangles used in rendering are often generically called <em>polygons</em>, even though almost all the polygons are simple triangles.</p>
<p class="indent">Games simulate curved surfaces by using lots and lots of triangles. A round tumbler, for example, can be approximated as a ring of interlocking triangles, as shown in <a href="ch05.html#ch5fig1">Figure 5-1</a>. On the right, the outlines of each triangle are shown for clarity.</p>
<div class="image"><img alt="image" src="graphics/f05-01.jpg"/></div>
<p class="figuret"><a id="ch5fig1"/><em>Figure 5-1: A curved tumbler approximated with triangles</em></p>
<h3 class="h3" id="ch05lev1sec04"><strong>Projection Without Ray Tracing</strong></h3>
<p class="noindent">To render the triangles in the scene models, the renderer must project the control points that define the triangle to locate these points on the screen. Ray tracing projects by following an imaginary beam of light through the center of each pixel, but in this case we have to do something different.</p>
<p class="indent">The good news is that a direct mathematical relationship exists between world coordinates and screen coordinates, and this makes mapping the points fairly straightforward. We know the location—the x, y, and z world coordinates—of the viewpoint and of the point on the model we want to project. We also know the location of the virtual projection screen. <a href="ch05.html#ch5fig2">Figure 5-2</a> shows how we use these locations to determine the exact y-coordinate where the line aimed at the model point crosses the projection screen. In this example, the depth (the distance from the viewpoint along the z-coordinate) of the projection screen is four-tenths of the depth from the viewpoint to the point on the model, as shown by the large blocks along the bottom. Knowing this proportion, we can calculate the x- and <a id="page_89"/>y-coordinates of the projected point. The y-coordinate of the projected point is four-tenths of the distance between the y-coordinate of the viewpoint and the y-coordinate of the point on the model, as shown by the shaded boxes on the projection screen. Also, though we can’t see this from the perspective of <a href="ch05.html#ch5fig2">Figure 5-2</a>, the x-coordinate of the projected point will be four-tenths of the distance between the x-coordinates of the viewpoint and model point.</p>
<div class="image"><img alt="image" src="graphics/f05-02.jpg"/></div>
<p class="figuret"><a id="ch5fig2"/><em>Figure 5-2: Projecting a point in the virtual world to the screen</em></p>
<p class="indent">Note that the position of the imaginary projection screen in the virtual world affects the resulting projection. To see this effect, make a rectangle using the forefinger and thumb of both hands and look through it while moving your hands close and then farther away. The farther away your hands are from your eyes, the narrower your <em>field of view</em>. In the same way, games can adjust field of view by altering the distance between the viewpoint and the projection screen in the virtual world. For example, games that let you look through binoculars or a gun scope accomplish the zoom effect by moving the projection screen deeper into the scene.</p>
<h3 class="h3" id="ch05lev1sec05"><strong>Rendering Triangles</strong></h3>
<p class="noindent">With all three points of a triangle located in screen space, rendering a triangle follows the same rasterization process we saw in <a href="ch04.html#ch04">Chapter 4</a> to make a bitmap out of a 2D model. In <a href="ch05.html#ch5fig3">Figure 5-3</a>, the pixel centers inside the triangle boundaries are colored gray.</p>
<p class="indent">From reading the previous chapter, you probably have some objections to this simple method of triangle rendering. First, how can we just color every pixel the same—what about all those lighting effects? And second, look at those jaggies— how do we get rid of them?</p>
<div class="image"><img alt="image" src="graphics/f05-03.jpg"/></div>
<p class="figuret"><a id="ch5fig3"/><em>Figure 5-3: With the vertices of a triangle located on the screen, the triangle can be rendered.</em></p>
<p class="indent">These questions will be answered, but first we have to deal with a more fundamental problem. Simply determining where every triangle is located on the screen and coloring its pixels doesn’t work because every pixel on the screen will probably be <a id="page_90"/>inside more than one triangle. Consider the image shown in <a href="ch05.html#ch5fig4">Figure 5-4</a>. The flowerpot is behind a cube, which is behind a tall cup. Pixel A lies within four different triangles: one on the front of the cup, one on the back of the cup, one on the front of the cube, and one on the side of the cube. Likewise, four triangles enclose pixel B. In each case, only one triangle should actually determine the color of the pixel. In order to render the image correctly, the renderer must always map each pixel to the model surface in the scene that is closest to the viewpoint. Ray tracing already finds the closest intersection point between the light beam and a model in the scene, so this problem is handled without any additional effort. Without ray tracing, though, what should the renderer do?</p>
<div class="image"><img alt="image" src="graphics/f05-04.jpg"/></div>
<p class="figuret"><a id="ch5fig4"/><em>Figure 5-4: Three overlapping models in a scene</em></p>
<h4 class="h4" id="ch05lev2sec01"><strong><em>The Painter’s Algorithm</em></strong></h4>
<p class="noindent">A simple solution is known as the <em>painter’s algorithm</em>. First, all of the triangles in the scene are ordered according to their distance from the viewpoint. Then the models are “painted” back to front, the way Bob Ross would paint a landscape on <em>The Joy of Painting</em>. This algorithm is easy for the programmer to implement, but it has several problems.</p>
<p class="indent">First, it’s highly inefficient: the renderer will wind up coloring the same pixel over and over again as foreground models are rendered over previous background models, which is a huge waste of effort.</p>
<p class="indent">Second, it doesn’t allow for easy subdivision to keep the army of workers busy on the GPU. The painter’s algorithm requires the models to be drawn in a certain order, so it’s difficult to effectively divide the work among separate processing units.</p>
<p class="indent">Third, there’s not always an easy way to determine which of two triangles is farther way from the viewpoint. <a href="ch05.html#ch5fig5">Figure 5-5</a> shows a perspective view of two triangles, with numbers indicating the depth of each vertex. The top view makes it clear which triangle is in front, but because the depths of one <a id="page_91"/>triangle’s vertices are between those of the other triangle, there’s no easy way to figure out which triangle is closer by direct comparison of the vertex depths.</p>
<div class="image"><img alt="image" src="graphics/f05-05.jpg"/></div>
<p class="figuret"><a id="ch5fig5"/><em>Figure 5-5: Perspective and top views of two triangles</em></p>
<h4 class="h4" id="ch05lev2sec02"><strong><em>Depth Buffering</em></strong></h4>
<p class="noindent">Because of all the deficiencies of the painter’s algorithm, the most common solution to projection in games is a method known as <em>depth buffering</em>. As introduced in the previous chapter, computer graphics require a bitmap called a display buffer to store the color of each pixel in a display. This technique also uses a corresponding <em>depth buffer</em> to track the depth of each pixel—how far away it is from the viewpoint. Of course, a screen is flat, so pixels don’t really have depth. What the depth buffer actually stores is the depth of the point in the scene that was used to determine the color of that pixel. This allows the renderer to process the objects in the scene in any order.</p>
<p class="indent">Here’s how depth buffering would work with the example scene from <a href="ch05.html#ch5fig4">Figure 5-4</a>. Initially, the depth of each pixel would be set to some maximal value that’s greater than the depth of any actual object in the scene—let’s say 100,000 virtual feet. If the cup is drawn first, the depth of those pixels in the depth buffer is set to the corresponding distances from the viewpoint. Suppose the flowerpot is drawn next; the renderer then sets the depth of its pixels. We can picture the depth buffer as a grayscale image, where pixels are darker the closer they are to the viewpoint. The depth buffer at this stage is shown in <a href="ch05.html#ch5fig6">Figure 5-6</a>.</p>
<p class="indent">The depth buffer solves the problem of projecting the right point onto the pixel. Before rendering a pixel, the renderer checks the depth buffer value for that pixel’s location to see if the new pixel would be in front of or behind the pixel that’s already in the display buffer. When a new pixel appears behind the pixel in that location in the display buffer, the renderer skips it and moves on. Continuing with our example, when the cube is drawn, the pixels on the left side of the cube that overlap with the cup are not drawn, because the values in the depth buffer show that the cup’s pixels are in front of the cube. The cube would overwrite the pixels of the flowerpot, because the depth of the flowerpot pixels is greater than those of the cube.</p>
<div class="image"><a id="page_92"/><img alt="image" src="graphics/f05-06.jpg"/></div>
<p class="figuret"><a id="ch5fig6"/><em>Figure 5-6: A depth buffer with two objects drawn. Darker colors are closer to the viewpoint.</em></p>
<p class="indent">Depth buffering is an efficient solution to projection because less work is thrown away. Models can be roughly preordered so that they are painted approximately front to back, to minimize overwritten pixels. Also, because depth buffers allow for rendering models in any order, work can more easily be divided among the cores of the graphics processor. In our example, different cores can be working on the cup, cube, and flowerpot at the same time, and the right model will be projected to each pixel in the final rendered image.</p>
<h3 class="h3" id="ch05lev1sec06"><strong>Real-Time Lighting</strong></h3>
<p class="noindent">Now that the renderer knows which triangle each pixel belongs to, the pixel must be colored. In real-time rendering this is known as <em>pixel shading</em>. Once a particular pixel has passed the depth buffer test, all the data needed to color the pixel is processed by an algorithm called a pixel shader. Because each pixel can be independently colored, pixel shading is a great way to keep the army of workers busy inside the GPU.</p>
<p class="indent">The data needed by the shader will vary based on the complexity of the lighting model, including the location, direction, and color of the lights in the scene. Without a method like ray tracing, a full global illumination model, in which reflections from near surfaces color each other, isn’t possible. However, shaders can include the basic effects of distance, diffuse reflections, and specular reflections.</p>
<p class="indent">In <a href="ch05.html#ch5fig7">Figure 5-7</a>, a beam of light represented by the solid arrow reflects from a triangle. The dashed arrow represents the <em>normal</em> (or <em>surface normal</em>) of the triangle in that location; a normal is simply a perpendicular line pointing away from the surface. In <a href="ch04.html#ch04">Chapter 4</a> we learned how the angles between light beams, surfaces, and viewpoints affect diffuse and specular reflections. The normal is used by the pixel shader for these calculations; <a id="page_93"/>so, for example, in <a href="ch05.html#ch5fig7">Figure 5-7</a>, if the dark arrow represents a light beam, this would have high diffuse reflection because the angle between the light and the normal is small.</p>
<div class="image"><img alt="image" src="graphics/f05-07.jpg"/></div>
<p class="figuret"><a id="ch5fig7"/><em>Figure 5-7: A triangle with a surface normal (dashed arrow) perpendicular to the triangle surface, and a light beam (dark arrow) striking the surface.</em></p>
<p class="indent">In <a href="ch05.html#ch5fig7">Figure 5-7</a>, the normal points straight up, meaning it is perpendicular to the plane of the triangle. Triangles with straight-up normals for every point on the surface are completely flat, which makes the individual triangles clearly visible in the rendering. For example, with straight-up normals, the tumbler in <a href="ch05.html#ch5fig8">Figure 5-8</a> appears faceted like a gemstone.</p>
<p class="indent">For a more rounded appearance, the normals are bent as shown in <a href="ch05.html#ch5fig9">Figure 5-9</a>. Here, the normals at the corners are bent outward, and the normal at any location inside the triangle is a weighted average of the normals at the corner. Because the normal at the point of impact for the light beam no longer points straight up, the light beam reflects more sharply. If this were part of a diffuse lighting calculation, the resulting color would be brighter.</p>
<div class="image"><img alt="image" src="graphics/f05-08.jpg"/></div>
<p class="figuret"><a id="ch5fig8"/><em>Figure 5-8: If the normals for each location on a triangle point the same way, this model will be rendered as a series of flat triangles.</em></p>
<div class="image"><img alt="image" src="graphics/f05-09.jpg"/></div>
<p class="figuret"><a id="ch5fig9"/><em>Figure 5-9: The normal at the point of light impact is affected by the bent corner normals, which changes the angle of reflection.</em></p>
<p class="indent"><a id="page_94"/>Bending normals allows the flat triangle to reflect light as though it were the bent triangle shown in <a href="ch05.html#ch5fig10">Figure 5-10</a>.</p>
<div class="image"><img alt="image" src="graphics/f05-10.jpg"/></div>
<p class="figuret"><a id="ch5fig10"/><em>Figure 5-10: Bending the normals gives the triangle a bent shape so far as the lighting calculations are concerned.</em></p>
<p class="indent">This goes only so far in fixing the problem, though, because the underlying shape is unchanged. Bending normals doesn’t affect which pixels are matched to which triangle; it affects only the lighting calculations in the pixel shader. Therefore, the illusion breaks down along the edges of a model. With our tumbler, bending normals helps the sides of the tumbler to appear smooth, but it doesn’t affect the tumbler’s silhouette, and the rim is still a series of straight lines. Smoother model renderings require additional techniques that we’ll see later in this chapter.</p>
<h3 class="h3" id="ch05lev1sec07"><strong>Shadows</strong></h3>
<p class="noindent">Shadowing plays an important part in convincing the viewer to accept the reality of an image by giving models weight and realism. Producing shadows requires tracing beams of light; a shadow is, after all, the outline of an object between a light source and a surface. Game renderers don’t have time for full ray tracing, so they use clever shortcuts to produce convincing shadow effects.</p>
<p class="indent">Consider the scene outline shown in <a href="ch05.html#ch5fig11">Figure 5-11</a>. This scene will be rendered in a nighttime environment, so the lamppost on the left will cast strong shadows. To render the shadows properly, the renderer must determine which pixels visible from this viewpoint would be illuminated by the lamppost and which will be lit only by other light sources. In this example, the renderer must determine that the point labeled Scene-A is not visible from the lamppost, but Scene-B is.</p>
<div class="image"><img alt="image" src="graphics/f05-11.jpg"/></div>
<p class="figuret"><a id="ch5fig11"/><em>Figure 5-11: The light from the lamppost should cast shadows in this scene.</em></p>
<p class="indent"><a id="page_95"/>A common solution to this problem in games is a <em>shadow map</em>, a quickly rendered image from the point of view of a light source looking into the scene that calculates only the depth buffer, not the display buffer. <a href="ch05.html#ch5fig12">Figure 5-12</a> is a shadow map for the lamppost in <a href="ch05.html#ch5fig11">Figure 5-11</a>, showing the distance from the lamppost to every point in the scene; as with the depth buffer, this is shown in grayscale with closer pixels colored darker.</p>
<div class="image"><img alt="image" src="graphics/f05-12.jpg"/></div>
<p class="figuret"><a id="ch5fig12"/><em>Figure 5-12: The depth buffer from a rendering of the viewpoint of the lamppost</em></p>
<p class="indent">Shadow maps are created for each light source before scene pixels are colored. When coloring a pixel, the pixel shader checks each light’s shadow map to determine if the point being rendered is visible from that light. Consider the points Scene-A and Scene-B in <a href="ch05.html#ch5fig11">Figure 5-11</a>. The shader computes the distance from each of these points to the top of the lamppost and compares this distance to the depth of the same points projected onto the shadow map, labeled Shadow-A and Shadow-B in <a href="ch05.html#ch5fig12">Figure 5-12</a>. In this case, the depth of Shadow-A in <a href="ch05.html#ch5fig12">Figure 5-12</a> is less than the distance between Scene-A and the lamppost in <a href="ch05.html#ch5fig11">Figure 5-11</a>, which means something is blocking that light from reaching Scene-A. In contrast, the depth of Shadow-B matches the distance from Scene-B to the lamppost. So Scene-A is in shadow, but Scene-B is not.</p>
<p class="indent">I deliberately gave the shadow map in <a href="ch05.html#ch5fig12">Figure 5-12</a> a blocky appearance; to improve performance, shadow maps are often created at lower resolutions, making blocky shadows. If a game offers a “shadow quality” setting, this setting most likely controls the resolution of the shadow maps.</p>
<h3 class="h3" id="ch05lev1sec08"><a id="page_96"/><strong>Ambient Light and Ambient Occlusion</strong></h3>
<p class="noindent">The simpler lighting model in real-time rendering tends to produce images that are too dark. It’s easy to overlook the effect of indirect lighting in the world around us. For example, standing outside in the daytime, you’ll have enough light to read even if you stand in a solid shadow, because of indirect sunlight bouncing off nearby surfaces.</p>
<p class="indent">To produce images with natural-looking light levels, a game renderer will typically apply a simple <em>ambient light</em> model. This lighting is omnipresent, illuminating the surface of every model without regard to light beams or angles of incidence, so that even surfaces missed by in-scene lighting are not totally dark. Ambient lighting is used throughout games, even for indoor scenes. This is a situation where a little fakery produces a more realistic result.</p>
<p class="indent">Ambient lighting can also be used to adjust the mood of a scene. When you leave behind a golden, autumnal field to enter a dusky forest in an open-world game like <em>World of Warcraft</em>, a large part of the effect is the ambient lighting changing from bright yellow to dim blue.</p>
<p class="indent">Although the simple ambient lighting model keeps the rendering from being too dark, the method doesn’t produce any shadows, which hurts a scene’s realism. <em>Ambient occlusion</em> methods fake shadows from ambient light by following the observation that such shadows should occur in crevices, cracks, holes, and the like. <a href="ch05.html#ch5fig13">Figure 5-13</a> shows the key idea. Point A is much less occluded than point B because the angle through which light can reach the point is much larger, letting more light through. Therefore, ambient light should have a greater influence on point A than point B.</p>
<p class="indent">For a renderer to measure the occlusion precisely, though, it would have to send out light beams in every direction, much like the scattering of light from diffuse lighting, but we already know that tracing light beams is not an option for real-time rendering. Instead, a technique called <em>screen space ambient occlusion (SSAO)</em> approximates the amount of occlusion for each pixel after the main rendering is over, using data that was already computed earlier in the rendering process.</p>
<p class="indent">In <a href="ch05.html#ch5fig14">Figure 5-14</a> we see SSAO approximation in action. Note that the viewpoint is looking straight down at the surface. The dashed arrow is the normal for the point on the surface. The gray area is a hemisphere aligned with that normal, shown as a semicircle in this 2D representation. The shader examines a scattering of points inside the hemisphere. Each point is projected into screen coordinates, just like the projection of the model point shown back in <a href="ch05.html#ch5fig2">Figure 5-2</a>. Then the depth of the point is compared to he depth buffer for the pixel location, which tells the shader whether the point is in front of (shown in white) or behind (black) the model surface. The percentage of points behind the surface is a good approximation of the amount of ambient occlusion.</p>
<div class="image"><a id="page_97"/><img alt="image" src="graphics/f05-13.jpg"/></div>
<p class="figuret"><a id="ch5fig13"/><em>Figure 5-13: Measuring the occlusion at given points</em></p>
<div class="image"><img alt="image" src="graphics/f05-14.jpg"/></div>
<p class="figuret"><a id="ch5fig14"/><em>Figure 5-14: Screen space ambient occlusion approximates the degree of occlusion by the percentage of points behind the model surface.</em></p>
<p class="indent">SSAO is heavy work for the renderer because it requires projecting and examining a lot of extra points—at least 16 per pixel for acceptable results. However, the calculations for each pixel are independent, which allows the work to be easily divided among the army of worker cores. If a gamer has the hardware to handle it, SSAO produces believable ambient shadowing.</p>
<h3 class="h3" id="ch05lev1sec09"><strong>Texture Mapping</strong></h3>
<p class="noindent">Throughout these discussions of graphics, we have discussed models as though their surfaces were one solid color, but that describes few surfaces in the actual world. Tigers have stripes, rugs have patterns, wood has grain, and so on. To reproduce surfaces with complex coloring, pixel shaders employ <em>texture mapping</em>, which conceptually wraps a flat image onto the surface of a model, much like an advertising wrap on the side of a city bus. To be clear, texture mapping is not just for game rendering; movie CGI employs it extensively, too. But texture mapping is a special problem for games, in which textures have to be applied in milliseconds. The sheer number of textures and texture operations needed for a single frame presents one of the greatest challenges of game rendering.</p>
<p class="indent"><a href="ch05.html#ch5fig15">Figure 5-15</a> shows a texture bitmap (an image of a zigzag pattern) and a scene in which the pattern has been applied. Bitmap images used for texture mapping are called <em>textures</em>. In this case, the surface of the rug rectangle is covered by a single large texture, although for regular patterns like the one on this rug, a smaller texture can be applied repeatedly to tile the surface.</p>
<p class="indent">The pixel shader is responsible for choosing the base color of the pixel using the associated texture; this base color is later modified by the lighting model. Because the textured surface is an arbitrary distance from the viewpoint, and at an arbitrary orientation, there’s not a one-to-one correspondence between pixels in the texture and pixels on the model’s surface. Choosing pixel colors in a textured area based on the applied texture is known as <em>sampling</em>.</p>
<div class="image"><a id="page_98"/><img alt="image" src="graphics/f05-15.jpg"/></div>
<p class="figuret"><a id="ch5fig15"/><em>Figure 5-15: Texture mapping. The zigzag texture on top is applied to the rug object under the chair.</em></p>
<p class="indent">To illustrate the decisions involved in sampling, let’s start with a bitmap of a robot with a hat, shown in <a href="ch05.html#ch5fig16">Figure 5-16</a>. The pixels in a texture are called <em>texels</em>. This 20×20 texture has 400 texels.</p>
<p class="indent">In this example, this texture will appear as a painting in the frame on the wall in <a href="ch05.html#ch5fig17">Figure 5-17</a>.</p>
<p class="indent">Suppose that the area inside the frame fills a 10×10 block of pixels in the rendered image. The texture will be applied head-on without any adjustment for perspective, which means all the renderer has to do is shrink the 20×20 block of texels to fit the 10×10 block of pixels in the final image.</p>
<div class="image"><img alt="image" src="graphics/f05-16.jpg"/></div>
<p class="figuret"><a id="ch5fig16"/><em>Figure 5-16: A texture of a robot wearing a hat</em></p>
<div class="image"><a id="page_99"/><img alt="image" src="graphics/f05-17.jpg"/></div>
<p class="figuret"><a id="ch5fig17"/><em>Figure 5-17: In this scene, the texture of <a href="ch05.html#ch5fig16">Figure 5-16</a> will be applied inside the picture frame on the wall.</em></p>
<h4 class="h4" id="ch05lev2sec03"><strong><em>Nearest-Neighbor Sampling</em></strong></h4>
<p class="noindent">Because 10×10 pixels are needed to fill the textured area, let’s imagine a grid of 100 sample points overlaying the texture. <a href="ch05.html#ch5fig18">Figure 5-18</a> shows a closeup section of the original robot texture from <a href="ch05.html#ch5fig16">Figure 5-16</a>. Here, the centers of the texels are shown as squares, and the crosses represent the sample points for the pixels in the scene. Sampling resolves this mismatch of pixels to texels.</p>
<p class="indent">The simplest method of sampling is choosing the color of the nearest texel, an approach known as <em>nearest-neighbor sampling</em>. This approach is easy to implement and fast to compute, but tends to look horrible. In this example, each of four texels is equally close to the pixel centers, so I’ve arbitrarily chosen the texel in the lower right of each pixel center. <a href="ch05.html#ch5fig19">Figure 5-19</a> shows the texels chosen by this sampling method, and the 10×10-pixel block that would appear in the final image.</p>
<p class="indent">As you can see, the result looks more like a skeletal aerobics instructor than a robot with a hat. If you’ve ever looked closely at an oil painting, you may guess why the nearest-neighbor technique produces such an unattractive result. Up close, an oil painting reveals a wealth of detail, a multitude of individual brushstrokes. Take a few steps back, though, and the strokes vanish as the colors blend together in the eye. In the same way, when a texture is represented with fewer pixels, the colors of neighboring texels should blend. Nearest-neighbor sampling, though, simply picks the color of one texel with no blending; in our example, three out of four texels have no influence on the result at all.</p>
<div class="image"><a id="page_100"/><img alt="image" src="graphics/f05-18.jpg"/></div>
<p class="figuret"><a id="ch5fig18"/><em>Figure 5-18: A close-up section of the <a href="ch05.html#ch5fig16">Figure 5-16</a> texture. Squares are texel centers; crosses are sample points.</em></p>
<div class="image"><img alt="image" src="graphics/f05-19.jpg"/></div>
<p class="figuret"><a id="ch5fig19"/><em>Figure 5-19: The result of 10×10 nearest-neighbor sampling on <a href="ch05.html#ch5fig16">Figure 5-16</a>. On the left are the selected texels of the original texture, and on the right is the resulting bitmap.</em></p>
<p class="indent">When a texture is expanded to fill a larger area, the results are just as ugly. In this case, some of the texels will simply be repeated in the textured area, producing a blocky result. To see the problem, let’s start with a triangle and its representation as a 16×16 anti-aliased texture, as shown in <a href="ch05.html#ch5fig20">Figure 5-20</a>.</p>
<div class="image"><img alt="image" src="graphics/f05-20.jpg"/></div>
<p class="figuret"><a id="ch5fig20"/><em>Figure 5-20: A triangle and its representation as an anti-aliased 16×16-pixel texture.</em></p>
<p class="indent">Now suppose this texture is applied over a 32×32 area. Ideally, it should look smoother than the original, smaller texture; the greater resolution offers the opportunity for a finer edge. As shown in <a href="ch05.html#ch5fig21">Figure 5-21</a>, though, nearest-neighbor sampling puts four sample points in each texel, so every texel in the original 16×16 texture simply becomes four identically colored pixels at the larger size.</p>
<div class="image"><img alt="image" src="graphics/f05-21.jpg"/></div>
<p class="figuret"><a id="ch5fig21"/><em>Figure 5-21: When used to enlarge textures, nearest-neighbor sampling merely duplicates pixels.</em></p>
<h4 class="h4" id="ch05lev2sec04"><a id="page_101"/><strong><em>Bilinear Filtering</em></strong></h4>
<p class="noindent">A better-looking sampling method is <em>bilinear filtering</em>. Instead of taking the color of the nearest texel, each texture sample is a proportional blend of the four nearest texels. The method is called bilinear because it uses the position of the sample point along two axes within the square formed by the four nearest texels. For example, in <a href="ch05.html#ch5fig22">Figure 5-22</a>, the sample point toward the bottom and just left of center results in the mixing percentages shown. The final color of this sample is computed from the colors of the texels at the given percentages.</p>
<p class="indent"><a href="ch05.html#ch5fig23">Figure 5-23</a> shows the robot texture after reduction via bilinear filtering. With only a fourth of the original pixels, the reduced version necessarily lacks detail, but if you hold the original at arm’s length and compare to the reduced version held close, you’ll see the reduction is a good representation, and much better than the nearest-neighbor result.</p>
<div class="image"><img alt="image" src="graphics/f05-22.jpg"/></div>
<p class="figuret"><a id="ch5fig22"/><em>Figure 5-22: Bilinear filtering measures the position of a sample point vertically and horizontally within the square of neighboring texels, and uses these positions to determine the percentage that each texel influences the sample color.</em></p>
<div class="image"><img alt="image" src="graphics/f05-23.jpg"/></div>
<p class="figuret"><a id="ch5fig23"/><em>Figure 5-23: The robot texture reduced through bilinear filtering</em></p>
<p class="indent"><a href="ch05.html#ch5fig24">Figure 5-24</a> shows a 32×32 area blown up from the 16×16 triangle texture using bilinear filtering—a clear improvement over the chunky nearest-neighbor sampling.</p>
<div class="image"><img alt="image" src="graphics/f05-24.jpg"/></div>
<p class="figuret"><a id="ch5fig24"/><em>Figure 5-24: The triangle texture expanded through bilinear filtering</em></p>
<h4 class="h4" id="ch05lev2sec05"><a id="page_102"/><strong><em>Mipmaps</em></strong></h4>
<p class="noindent">The examples in the previous section show the limit of what is possible with bilinear filtering. For bilinear filtering to look good, the texture needs to be at least half, but no more than twice, the resolution of the textured area. If the texture is any smaller, bilinear filtering still produces blocky results. If the texture is too large, even though four texels are used per sample, some texels won’t contribute to any samples.</p>
<p class="indent">Avoiding these problems requires a set of different-sized bitmaps for each texture: a large, full-resolution version for viewing up close, and smaller versions for when the textured area is also small. This collection of progressively smaller textures is known as a <em>mipmap</em>. An example is shown in <a href="ch05.html#ch5fig25">Figure 5-25</a>. Each texture in the mipmap is one-quarter of the area of the next larger texture.</p>
<div class="image"><img alt="image" src="graphics/f05-25.jpg"/></div>
<p class="figuret"><a id="ch5fig25"/><em>Figure 5-25: A mipmap is a collection of textures, each one-quarter the size of the previous.</em></p>
<p class="indent">With a mipmap, the renderer can always find a texture that will produce good results with bilinear filtering. If a 110×110 texture is needed, for example, the 128×128 texture is shrunk. If a 70×70 texture is required, the 64×64 texture is magnified.</p>
<h4 class="h4" id="ch05lev2sec06"><strong><em>Trilinear Filtering</em></strong></h4>
<p class="noindent">While bilinear filtering and mipmaps work reasonably well, they introduce a distracting visual anomaly when transitioning from one mipmap texture to another. Suppose, in a first-person game, you’re running toward a brick wall that uses a mipmapped texture. As you get closer to the wall, the smaller texture will get blown up more and more until you reach the point where you get a shrunk-down version of the next larger texture in the mipmap. Unfortunately, a larger texture that has been reduced through bilinear filtering doesn’t quite match a smaller version of the same texture that has been expanded, so at the moment of this transition the texture will “pop.” The problem can also occur with no movement at all on a surface that stretches out to the distance, such as a long rug in a corridor, that has been tiled with a repeating texture; because the parts of rug at different distances are covered by different textures in the mipmap, seams will be clearly visible where the textures touch.</p>
<p class="indent">To smooth over the texture transition, the renderer can blend samples from different textures in addition to blending between texels in a texture. Suppose the area to be textured is 70×70, a size that falls between the 64×64 and 128×128 textures in a mipmap. Instead of just using bilinear filtering on <a id="page_103"/>the nearer-sized 64×64 texture, the renderer can use bilinear filtering on both the larger and smaller textures, then blend the two resulting samples. As with the bilinear filtering itself, this final step is proportional: in our example, the color would be mostly determined by the result from the 64×64 texture, with a little of the 128×128 result mixed in. Because we are filtering in two dimensions on each texture, then blending the results, this technique is known as <em>trilinear filtering</em>. It is demonstrated in <a href="ch05.html#ch5fig26">Figure 5-26</a>.</p>
<p class="indent">Trilinear filtering eliminates popping and seaming between textures in a mipmap, but because it requires two bilinear samples and then a final blend, it does over twice as much work as bilinear filtering.</p>
<div class="image"><img alt="image" src="graphics/f05-26.jpg"/></div>
<p class="figuret"><a id="ch5fig26"/><em>Figure 5-26: Trilinear filtering takes bilinear samples from the larger and smaller textures in a mipmap and blends the results.</em></p>
<h3 class="h3" id="ch05lev1sec10"><strong>Reflections</strong></h3>
<p class="noindent">As discussed in <a href="ch04.html#ch04">Chapter 4</a>, ray tracing naturally captures all the effects of light reflecting from one surface to another. Unfortunately, the subtle influence of colors of nearby surfaces is nearly impossible to capture without ray tracing, but game renderers do have a way to fake what I’ll call <em>clear reflections</em>: the more obvious, mirror-like reflections on such surfaces as polished countertops, windows, and of course mirrors themselves.</p>
<p class="indent">Games limit which surfaces produce clear reflections. Having just a few objects with such reflections maintains the realism of the scene at a much lower computational cost. To reduce the workload further, renderers use <em>environment mapping</em>, in which shiny objects are conceptually placed inside cubes that are texture-mapped with a previously rendered image of the object’s surroundings.</p>
<p class="indent"><a href="ch05.html#ch5fig27">Figure 5-27</a> shows a sample situation: a shiny sports car on a showroom turntable. To compute the effect of clear reflections, the renderer conceptually places the car in a cube; the cube itself is not rendered, but used only to map reflections. The inside of the cube is texture-mapped with an image of the showroom interior, as shown in <a href="ch05.html#ch5fig28">Figure 5-28</a>. Because the reflected images will be somewhat distorted anyway by the surface of the car body, viewers won’t notice that the reflections don’t perfectly match the rendered world in which the car is placed.</p>
<div class="image"><a id="page_104"/><img alt="image" src="graphics/f05-27.jpg"/></div>
<p class="figuret"><a id="ch5fig27"/><em>Figure 5-27: For realism, the shiny car body should reflect the showroom.</em></p>
<div class="image"><img alt="image" src="graphics/f05-28.jpg"/></div>
<p class="figuret"><a id="ch5fig28"/><em>Figure 5-28: For the purpose of mapping reflections, the car is considered to be in a cube, the insides of which are covered by a bitmap image of the showroom.</em></p>
<p class="indent"><a id="page_105"/>Instead of tracing light as it pinballs around the scene, mapping reflections becomes an indirect texture-map reference, a relatively simple calculation. Of course, the surface of the car is probably also texture-mapped, which means that adding reflections is at least doubling the per-pixel effort, but the gain in realism is usually worth the extra work.</p>
<p class="indent">The job becomes harder when a reflecting model is moving, as would happen if our car were racing down a desert road in a driving game. The renderer can’t simply paste a static image of a desert inside a cube and expect this to fool the viewer. Because the viewpoint will be moving with the car as the car travels down the road, the reflections must likewise travel— or at least give that appearance.</p>
<p class="indent">There’s an old Hollywood trick that was used to convey the illusion of sideways movement in relation to the camera. An actor would stand on a treadmill so he could walk without going anywhere. Behind him an illustration of scenery on a continuous roll would slide past at the same speed as the treadmill. As long as the audience didn’t notice the same trees going by, it looked as though the actor was actually moving sideways.</p>
<p class="indent">The same idea can be applied inside the cube around the shiny car. A portion of a wide continuous image is selected, as shown in <a href="ch05.html#ch5fig29">Figure 5-29</a>. Sliding the selection “window” across the wide image to match the movement of the car creates the illusion that the car is reflecting the arid mountains depicted in the scene.</p>
<div class="image"><img alt="image" src="graphics/f05-29.jpg"/></div>
<p class="figuret"><a id="ch5fig29"/><em>Figure 5-29: Sliding a window down a wide, continuous image creates the effect of movement in mapped reflections.</em></p>
<h3 class="h3" id="ch05lev1sec11"><strong>Faking Curves</strong></h3>
<p class="noindent">Nothing in a video game destroys realism faster than a model with easily recognizable triangles trying to represent a rounded shape. Early 3D games were filled with car tires shaped like octagons and human characters that looked like they were made of toy bricks. We’ve already seen one part of the solution to this problem—bending the normals of triangle vertices—but producing smooth models requires a whole set of techniques.</p>
<h4 class="h4" id="ch05lev2sec07"><strong><em>Distant Impostors</em></strong></h4>
<p class="noindent">An obvious solution to the problem of flat triangles is to break models down into so many small triangles that the individual facets are too small to be recognized. That works in theory, but even though triangles are <a id="page_106"/>simple shapes, there’s still a limit to how many can be rendered in the time allowed. Trying to design each model at the highest possible detail would slow rendering to a crawl.</p>
<p class="indent">A renderer could, however, use lots of extra triangles to smooth out just those models closest to the viewpoint. This is the idea behind <em>distant impostors</em>. Here, each object in a game is modeled twice—a fully detailed high-triangle model and a simplified model with relatively few triangles. This simplified model is the “impostor” of the original, and is swapped in for the high-quality model whenever the model gets beyond a certain distance from the viewpoint.</p>
<p class="indent">Distant impostors make effective use of rendering time, but because the two models are so dissimilar, if a player is watching a particular model while moving closer to it, the transition between the models can be visually jarring. Ideally, you’d like to give the viewer the feeling that the distant object is revealing greater detail as it comes closer, but in practice the two models are so different that the replacement looks like one object magically transforming into another.</p>
<h4 class="h4" id="ch05lev2sec08"><strong><em>Bump Mapping</em></strong></h4>
<p class="noindent">Another technique for smoothing models keeps the triangle count the same, but alters the lighting calculations at each pixel to give the appearance of an irregular surface.</p>
<p class="indent">To understand why this <em>bump mapping</em> method can be so effective, imagine a game featuring a hacienda with stucco walls. To get the appearance of stucco, the renderer can apply a texture made from an image of an actual stucco wall to the walls of the hacienda model. Because stucco is wavy, its undulations should be visible under the scene lighting. Merely applying a texture to a flat wall wouldn’t convince the eye; it would look like a flat wall with a picture of stucco on it.</p>
<p class="indent">Bump mapping allows flat surfaces to react to light as though they were wavy like stucco, bumpy like popcorn ceilings, crumpled, louvered, or anything else. The process starts with a grayscale bitmap the same size as the texture that will be applied to the model surface. This bitmap is known as a <em>height map</em>, because the brightness of each pixel indicates the height of the surface.</p>
<p class="indent">The height map allows a pixel shader to approximate the surface normal at each pixel location. This is easiest to understand in 2D. <a href="ch05.html#ch5fig30">Figure 5-30</a> shows a row of 10 pixels. The numbers at the bottom represent the height of each pixel. The 10 points are shown at proportionate heights, along with the surface normals. I’ve added gray lines to show how the normals for the fourth and seventh points are computed. An imaginary line is drawn between the two points on either side of a chosen point; then, the normal for the chosen point is set perpendicular to this line.</p>
<div class="image"><a id="page_107"/><img alt="image" src="graphics/f05-30.jpg"/></div>
<p class="figuret"><a id="ch5fig30"/><em>Figure 5-30: A row of pixels with light calculations altered by bump mapping. The numbers indicate the artificial height of each pixel. The renderer determines the normal at each pixel based on the heights of neighboring pixels.</em></p>
<p class="indent">These bent normals affect the calculations for both diffuse and specular lighting, allowing a flat surface to react to light as though it were rough or wavy. As with previous tricks that involved bending normals, though, a surface with a bump map is still a flat surface. The points on the surface are not actually raised or lowered, but merely react to light as though they were pointing in different directions. As a player moving through a 3D scene passes a bump-mapped model, the lighting on the surface will change in a realistic manner, but the edges of the model will still be straight, possibly giving the game away. Just as the rim of the tumbler back in <a href="ch05.html#ch5fig8">Figure 5-8</a> betrayed the straight lines on the model, the outside corners of our bump-mapped hacienda will be perfectly straight when they should be wavy, because bump mapping doesn’t alter the shape of the flat wall.</p>
<h4 class="h4" id="ch05lev2sec09"><strong><em>Tessellation</em></strong></h4>
<p class="noindent">Suppose you’re playing a fantasy game, and all your attention is focused on a huge ogre slowly approaching with an axe in his hands. As a gamer, you want this ogre to look as good as possible even as he gets close enough to nearly fill the screen, but you don’t want him made out of so many triangles that the frame rate is too low for you to effectively fight him.</p>
<p class="indent">If the renderer uses a distant impostor, though, there will be a jarring transition that will remind you that you’re just playing a game. If the renderer bump-maps the ogre model, the light will reflect realistically off the rivets in his armor, but the neat lighting effect won’t hide the fact that the model just has too few triangles to be viewed up close.</p>
<p class="indent">A process known as <em>tessellation</em> solves this problem. First, each triangle in the ogre model is subdivided into more triangles. The corners of these new triangles are then manipulated independently inward or outward (that is, up or down in relation to the original triangle) using a height map. Instead of merely bending normals to trick the lighting model as bump mapping does, tessellation actually produces a model with more detail. <a href="ch05.html#ch5fig31">Figure 5-31</a> demonstrates the process for a single triangle.</p>
<p class="indent">This method is a great way to cover up the straight lines of triangles and is a clear improvement in appearance over bump mapping and distant impostors. Because the model is actually deformed into a new, more complicated shape, even the edges of the model are properly affected, unlike <a id="page_108"/>with bump mapping. Also, unlike the distant impostor technique, the model improves gradually as the distance from the viewpoint decreases, avoiding the sharp transition when models are swapped.</p>
<div class="image"><img alt="image" src="graphics/f05-31.jpg"/></div>
<p class="figuret"><a id="ch5fig31"/><em>Figure 5-31: A triangle is tessellated, producing a web of smaller triangles. These new triangle vertices are then manipulated using a height map to produce the more complex surface on the bottom.</em></p>
<p class="indent">Though you might think that tessellation is used extensively in games, it’s not, because it inflicts a much larger performance hit than the simpler methods discussed earlier. Creating more complex models on the fly is a lot more work than accessing one of several premade models as in the distant impostor method, or adjusting normals in bump mapping.</p>
<p class="indent">Tessellation is therefore used where the results are most obvious. For example, in a game set outdoors, the ground beneath the avatar’s feet may stretch far into the distance. Modeling the ground in great detail would require a huge number of triangles, creating a performance bottleneck, but if the ground model has a low triangle count, the ground closest to the viewer will have an unrealistic, angular appearance. Tessellation can smooth out just the closest part of the ground.</p>
<h3 class="h3" id="ch05lev1sec12"><strong>Anti-Aliasing in Real Time</strong></h3>
<p class="noindent">All of the renderer’s hard work can go down the drain if individual pixels become clearly visible through aliasing. As with movie CGI, games need some form of full-screen anti-aliasing to smooth over the edges of models <a id="page_109"/>and surfaces. With ray tracing, anti-aliasing is conceptually simple: send out more beams than pixels and blend the results. Game renderers, though, must use more efficient techniques.</p>
<h4 class="h4" id="ch05lev2sec10"><strong><em>Supersampling</em></strong></h4>
<p class="noindent">The most direct approximation to casting multiple beams is known as <em>supersampling anti-aliasing (SSAA)</em>. Instead of casting multiple beams per pixel, supersampling renders an intermediate image that is much larger than the desired final image. The color of each pixel in the final image is a blend of a sample of pixels from the larger image.</p>
<p class="indent">Consider the two white triangles covered by a gray triangle shown in <a href="ch05.html#ch5fig32">Figure 5-32</a>. Note that the edges of the white triangles won’t be visible in the rendered image but are shown here for clarity.</p>
<div class="image"><img alt="image" src="graphics/f05-32.jpg"/></div>
<p class="figuret"><a id="ch5fig32"/><em>Figure 5-32: An arrangement of three triangles</em></p>
<p class="indent"><a href="ch05.html#ch5fig33">Figure 5-33</a> demonstrates a basic rendering of these triangles at an 8×4 resolution. Each pixel is colored gray or white depending on whether the pixel center lies within the area of the gray triangle in the foreground.</p>
<div class="image"><img alt="image" src="graphics/f05-33.jpg"/></div>
<p class="figuret"><a id="ch5fig33"/><em>Figure 5-33: Coloring pixels without anti-aliasing</em></p>
<p class="indent">To produce an 8×4 supersampled image, the triangles are first rendered at a 16×8 resolution as shown in <a href="ch05.html#ch5fig34">Figure 5-34</a>.</p>
<div class="image"><a id="page_110"/><img alt="image" src="graphics/f05-34.jpg"/></div>
<p class="figuret"><a id="ch5fig34"/><em>Figure 5-34: Supersampling the three triangles. Here, each pixel in the final bitmap is represented by four subpixels with scattered sample points.</em></p>
<p class="indent">As you can see, each pixel in <a href="ch05.html#ch5fig33">Figure 5-33</a> has become four smaller pixels in <a href="ch05.html#ch5fig34">Figure 5-34</a>. These smaller pixels are called <em>subpixels</em>. Using this higher-resolution rendering, the color of each pixel in the final rendering is a proportional blend of the colors of its four subpixels, as shown in <a href="ch05.html#ch5fig35">Figure 5-35</a>.</p>
<div class="image"><img alt="image" src="graphics/f05-35.jpg"/></div>
<p class="figuret"><a id="ch5fig35"/><em>Figure 5-35: Coloring each pixel by blending subpixels</em></p>
<p class="indent">Supersampling does a nice job of smoothing out the jaggies, but as you might expect, rendering the image at a much higher resolution incurs a large performance penalty. Sampling four pixels to make one pixel in the final image is four times as much work for the pixel shader. In this example, I’ve kept things simple by assigning a flat color to each triangle, but in a typical game render each subpixel represents, at a minimum, a texture map sample followed by lighting calculations. Although earlier generations of video games commonly used SSAA, it’s rare to see this method now.</p>
<h4 class="h4" id="ch05lev2sec11"><strong><em>Multisampling</em></strong></h4>
<p class="noindent">In the previous example you can see that when all four subpixels are inside the same triangle, supersampling doesn’t accomplish anything. To reduce the <a id="page_111"/>performance hit of anti-aliasing, the subpixel work can be limited to the edges of triangles where the jaggies occur, a technique known as <em>multisample anti-aliasing (MSAA)</em>.</p>
<p class="indent"><a href="ch05.html#ch5fig36">Figure 5-36</a> demonstrates one version of this concept. Two pixels lie across the edge between two triangles. With supersampling, each of the eight subpixels is texture-sampled and individually colored by scene lighting. With multisampling, there are still eight subpixels for the two pixels, but not eight samples. Instead, the renderer first determines which triangle contains each subpixel. Each of the four subpixels that lie within the same triangle is given the same color, which has been sampled from a point midway between the subpixel sample points. So while supersampling colors eight subpixels A through H, multisampling colors only four subpixels A through D, which means substantially less work in texture mapping and lighting.</p>
<div class="image"><img alt="image" src="graphics/f05-36.jpg"/></div>
<p class="figuret"><a id="ch5fig36"/><em>Figure 5-36: Comparing supersampling and multisampling</em></p>
<p class="indent">When all four subpixels lie within the interior of the same triangle, multisampling colors only one subpixel per final pixel, introducing little computational overhead. Multisampling puts in extra effort where it is most needed—reducing jaggies at edges—and thus is an efficient use of rendering time.</p>
<h4 class="h4" id="ch05lev2sec12"><strong><em>Post-Process Anti-Aliasing</em></strong></h4>
<p class="noindent">Performance can be improved even further by delaying anti-aliasing until the image is rendered, an idea known as <em>post-process anti-aliasing</em>. That is, the image is first rendered normally at the desired final resolution, and then the jaggies are identified and smoothed over. In essence, a post-process anti-aliasing technique decides that some of the pixels in an image are colored incorrectly based on nothing more than the colors of the pixels themselves.</p>
<p class="indent">One such method is called <em>fast approximate anti-aliasing</em>, or <em>FXAA</em>. (Why that wouldn’t be F<em>A</em>AA is perhaps a question we’re not supposed to ask.) The idea behind FXAA is to find pixels that are likely to be along the edge between overlapping triangles, and then blend neighboring pixel colors to smooth the jarring transition.</p>
<p class="indent">FXAA examines each pixel in the image separately—let’s call the pixel under examination the <em>current</em> pixel. The process starts by computing the perceived brightness of the current pixel and its four immediate neighbors, similar to examining a black-and-white version of the image. The brightest and dimmest pixels in the neighborhood are selected, as shown in <a href="ch05.html#ch5fig37">Figure 5-37</a>, <a id="page_112"/>and their difference is compared to a cut-off value. This test ensures that the anti-aliasing is applied only to pixel neighborhoods of high contrast—areas where the difference between the brightest and dimmest pixels is large.</p>
<div class="image"><img alt="image" src="graphics/f05-37.jpg"/></div>
<p class="figuret"><a id="ch5fig37"/><em>Figure 5-37: Checking the level of contrast in a pixel’s neighborhood</em></p>
<p class="indent">These high-contrast areas likely represent jagged edges that need to be smoothed, and each such area is further examined as shown in <a href="ch05.html#ch5fig38">Figure 5-38</a>. The 3×3 block of pixels centered on the current pixel is considered both as a set of three columns and a set of three rows to determine whether this is a horizontal or vertical edge. In this example, because the columns are similar to each other but one row strongly contrasts with the other two, this would be classified as a horizontal edge.</p>
<div class="image"><img alt="image" src="graphics/f05-38.jpg"/></div>
<p class="figuret"><a id="ch5fig38"/><em>Figure 5-38: Looking for contrast in the columns and rows of a pixel neighborhood</em></p>
<p class="indent">Because this is a horizontal edge, the next step is to compare the pixels above and below the current pixel to find which contrasts the most with the current pixel. In this case, the pixel above is much brighter than the current pixel, while the pixel below is quite similar. This means the detected edge is between the current pixel and its topside neighbor. To anti-alias this edge, the current pixel will be replaced by a bilinear sample between the pixel centers, shown as the white circle in <a href="ch05.html#ch5fig39">Figure 5-39</a>. FXAA examines other pixels along the edge to determine how jagged <a id="page_113"/>the edge is, adjusting the degree of blending by placing the sample point farther from the center of the current pixel.</p>
<div class="image"><img alt="image" src="graphics/f05-39.jpg"/></div>
<p class="figuret"><a id="ch5fig39"/><em>Figure 5-39: To smooth this edge, FXAA will replace the color of the center pixel with a bilinear sample at the circle point.</em></p>
<p class="indent">A post-process anti-aliasing method like FXAA is very fast compared to supersampling or even multisampling because it doesn’t create any sub-pixels at all. However, the results of FXAA are not always as impressive as other methods. In particular, FXAA can sometimes blur areas that weren’t actually aliased; unlike supersampling, post-process methods like FXAA are only guessing where the edges are, so areas of high contrast within textures may fool the algorithm.</p>
<h3 class="h3" id="ch05lev1sec13"><strong>The Rendering Budget</strong></h3>
<p class="noindent">The trade-offs that accompany different anti-aliasing techniques mean that developers of real-time graphics applications must choose between best quality and best performance. Is FXAA good enough for this situation? Or is MSAA necessary? This choice, though, is not made in isolation. More broadly, game developers must review all the techniques available for real-time rendering—lighting and shadows and anti-aliasing, and lots of other possibilities we don’t have the space to discuss, like motion blur and particle systems—and select a set that maximizes the quality of the images without exceeding the time allowed for rendering. Within that <sup>1</sup>/<sub>60</sub> of a second, a surprising amount of work can be done, but all of the best-looking techniques can’t be used, so sacrifices have to be made somewhere.</p>
<p class="indent">On a console or in a mobile game, these choices are usually all made by the game designer. On PCs, a degree of choice is usually afforded to the user, who is given controls to raise or lower the resolution of textures, select the method of texture filtering, choose among anti-aliasing methods, turn shadows and reflections on or off, and tweak the renderer in a host of other ways. In part, this control is given so the user can adjust the render workload to match the performance of the particular system, since the PC in question might be top of the line, or an aging clunker.</p>
<p class="indent">Beyond that, though, detailed rendering options reflect the truth that beauty is subjective: what impresses one viewer might have no effect on another. Some gamers are horrified by jagged edges, for example, and always crank up anti-aliasing to the maximum, while others wouldn’t dream of devoting precious processor cycles to removing jaggies when there are more realistic shadows to be had instead. In a sense, video games are all about placing ourselves inside believable illusions, and what we believe is up to us.</p>
<h3 class="h3" id="ch05lev1sec14"><strong>What’s Next for Game Graphics</strong></h3>
<p class="noindent">So where do game graphics go from here? We can expect game programmers to continue to be challenged by advancements in displays. Monitors keep increasing in resolution, eating away some of the benefit of each new GPU generation. A special challenge will come from virtual reality (VR) headsets, which combine displays mounted inside helmets with sensors to <a id="page_114"/>track the gamer’s head movements. VR headsets can be trouble if the display lags behind the movement—our brains don’t like conflicting information, and when our eyes are saying one thing, and our inner ear something else, the result for many people is nausea. In a game played on a normal flat screen, gamers would prefer a consistently high frame rate but don’t get too bent out of shape by sporadic dips in the number; with VR devices, an absolutely rock-steady frame rate is imperative.</p>
<p class="indent">Beyond matching the needs of displays, it’s difficult to predict exactly how game graphics will progress. Over the past decade, every time I’ve played a new AAA game (as the industry calls the biggest-budget titles), I find myself thinking the graphics can’t get any better, that whatever improvements the next generation of hardware brings will be insignificant. And every time, I’ve been proven wrong. So I’m confident that I’ll continue to be blown away by the advances in game graphics, even if I can’t be sure what those advances will be.</p>
<p class="indent">Raw hardware power is only part of the equation. Buying a new GPU with twice as many cores as an older GPU means the hardware can process twice as many triangles in the same allotment of time, but once triangle counts get high enough, doubling them doesn’t improve the resulting images very much. Indeed, at some point, models may get so detailed and triangle counts so high that the average triangle will occupy less than a one-pixel area on the screen. When that happens, it will call into question the whole idea of rendering the scene as a series of triangles. Rather than projecting three triangle vertices to determine the color of one pixel, renderers may replace triangles with single points of fixed volume—imagine building a sculpture out of tiny marshmallows.</p>
<p class="indent">What ultimately drives advancements in game graphics, though, isn’t hardware, but the creativity of graphics programmers. Many of the techniques in <a href="ch04.html#ch04">Chapter 4</a> are about making accurate, or at least plausible, simulations of how light and vision work in the real world. Game graphics are just about making results that look good. That gives programmers enormous leeway to experiment, to find new ways to spend part of the precious rendering budget, to find new tricks to put silly grins on the faces of gamers. I don’t know for sure what game developers are cooking up for the next generation of games, but I’m sure that they’ll continue to put my GPU to work in ways that will thrill and amaze.</p>
</body></html>