- en: '**13'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SCIENTIFIC MACHINE LEARNING**
  prefs: []
  type: TYPE_NORMAL
- en: '*The bewilderments of the eyes are of two kinds, and arise from two causes,
    either from coming out of the light or from going into the light.*'
  prefs: []
  type: TYPE_NORMAL
- en: —Socrates
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The topic of this chapter is a rather new approach to solving scientific problems
    through computation. Much of the recent development in the field of scientific
    machine learning (SciML) has taken place within the Julia ecosystem and has been
    led by researchers using Julia in science. Relatively little has been published
    explaining how to apply the new techniques in a form digestible by those not conversant
    with machine learning jargon. I hope to fill at least part of that gap here through
    the selection of simple but concrete examples that clarify the concepts involved
    so that readers can apply them to a variety of problems.
  prefs: []
  type: TYPE_NORMAL
- en: Scientific machine learning is not machine learning. *Machine learning (ML)*
    is a branch of artificial intelligence in which computers train themselves (usually
    guided by human supervision), by practicing on a large corpus of data, to recognize
    patterns and make classifications. ML techniques are applied to such problems
    as detecting fraudulent financial transactions or trying to guess what movie you
    want to watch next. The training replaces the traditional coding of a specific
    model or algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '*SciML* extracts several key techniques from ML and applies them to a different
    class of problem. In SciML, we assume that the system we’re studying is described
    by a particular model, often expressed as a set of differential equations. Certain
    parameters or other aspects of the model, however, are unknown. If we have data
    about how the system behaves, SciML techniques allow us to infer the values of
    these unknown parameters efficiently.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Automatic Differentiation in a Physics Problem**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Along with concepts from statistics and probability theory, SciML borrows *automatic
    differentiation* from ML. This technique is critical to both ML and SciML. Traditionally,
    differentiation is a mathematical procedure from calculus that finds the slope
    of a curve (in one dimension) or a surface (in two or more dimensions). We call
    a derivative of a surface, which involves dealing with several variables, a *gradient*.
    If your kitchen sink is installed correctly, the negative gradient of its surface
    points toward the drain at every point, so that when you pull the plug all the
    water drains out and you’re not left with any puddles.
  prefs: []
  type: TYPE_NORMAL
- en: 'Automatic differentiation is the calculation of a derivative or gradient of
    a function expressed in a programming language, rather than in mathematical notation.
    The programmed function can be the direct translation of a mathematical expression.
    Often, when the expression is complicated, its analytic derivative will involve
    many terms and be expensive to calculate in the traditional way. Automatic differentiation
    can be faster. We can even use automatic differentiation to calculate gradients
    that have no analytic form: the function being differentiated can include nearly
    any computation, including those not expressible in mathematical notation. Automatic
    differentiation is not numerical differentiation; it’s not a finite-difference
    calculation. Neither is it symbolic differentiation, as explored in [Chapter 12](ch12.xhtml).
    It applies knowledge of calculus, such as the chain rule for derivatives, with
    knowledge of the derivatives of specific functions and numerical techniques to
    differentiate efficiently and accurately.'
  prefs: []
  type: TYPE_NORMAL
- en: ML uses automatic differentiation to guide its models in the direction of the
    correct solutions, and it’s used within the SciML machinery in a similar way.
    We can also use it explicitly for efficient calculations of derivatives in mathematical
    models, as shown in “Calculating Forces from Potentials” on [page 408](ch13.xhtml#ch13lev1sec2).
  prefs: []
  type: TYPE_NORMAL
- en: '***Differentiating with ForwardDiff***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We can meet our automatic differentiation needs in this chapter with the `derivative()`
    function from the `ForwardDiff` package, which I’ll assume has been imported in
    the following examples. Its use is simple: we supply a function and a value, and
    `ForwardDiff.derivative()` returns the derivative of the function evaluated at
    the supplied value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is correct: the derivative of sin(*x*) is cos(*x*), and cos(0) =
    1.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `ForwardDiff.derivative()` function can also handle functions defined in
    Julia that may contain almost any type of computation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `floor()` and `ceil()` functions round their arguments to the closest smaller
    or larger whole number. The `fdst()` function defined in the example is not something
    that we can look up in a table of derivatives or handle with the familiar techniques
    of calculus, but Julia’s automatic differentiation routine calculates the derivative
    correctly. [Figure 13-1](ch13.xhtml#ch13fig1) shows the result.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch13fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-1: Automatic differentiation of a strange function*'
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 13-1](ch13.xhtml#ch13fig1), the legend uses a prime to indicate a
    derivative. The dashed line shows the result of the automatic differentiation
    function, which is not troubled by the existence of discontinuities.
  prefs: []
  type: TYPE_NORMAL
- en: '***Calculating Forces from Potentials***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In physics, the force on a body is the negative gradient of its potential energy.
    If the potential energy depends on only one variable, this is simply the negative
    of its derivative with respect to that variable. Let’s revisit the finite-angle
    pendulum problem from [Chapter 9](ch09.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 13-1](ch13.xhtml#ch13lis1) recapitulates the problem in one place
    for convenience.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 13-1: Revisiting the finite-angle pendulum*'
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 13-1](ch13.xhtml#ch13lis1) contains something extra, however: the
    `ppot()` function, which gives the gravitational potential energy of the pendulum
    as a function of height ➊. The `pendulum!()` function now sets up the problem
    using automatic differentiation to calculate the (negative) derivative of the
    potential ➋ to derive the force, rather than using the force function directly.
    A second function, `pendulumF!()`, sets up the problem as before, using the force
    function. We proceed just as we did in [Chapter 9](ch09.xhtml), but we find two
    numerical solutions: once using the potential ➌ and again using the force.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 13-2](ch13.xhtml#ch13fig2) compares the two methods of solution.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch13fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-2: The finite-angle pendulum computed two ways*'
  prefs: []
  type: TYPE_NORMAL
- en: The two solutions agree exactly. Clearly it wasn’t necessary to reach for the
    `ForwardDiff` package to handle this problem, but we did so to verify that it
    works as expected. When applying a new technique, it’s essential to test it on
    a relatively simple problem with a known solution first, to gain confidence in
    our understanding of how to use it, and to confirm that we understand how it works.
  prefs: []
  type: TYPE_NORMAL
- en: Physicists usually think in terms of potentials rather than forces, so when
    conducting numerical experiments, we’re more likely to try different potentials
    rather than tweak the force function directly. Having a solution program that
    differentiates the potential for us is more convenient than deriving a new force
    field at each iteration. Also, the potential functions we work with have a simpler
    form than the force functions derived from them. This is the case in the next
    example.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that we’ve discovered a new particle with a potential that is strongly
    repulsive at short range, has a well at a particular distance, and is weakly repulsive
    at longer ranges. The potential
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/409math.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where *r* is the distance from the particle, has these properties, as shown
    in [Figure 13-3](ch13.xhtml#ch13fig3).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch13fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-3: The potential of an imaginary particle*'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 13-3](ch13.xhtml#ch13fig3) shows the potential well at *r* ≈ 1.3\.
    This is a location at which an interacting particle can be trapped if it lacks
    the energy to escape.'
  prefs: []
  type: TYPE_NORMAL
- en: The system will contain two of these particles, fixed at *r* = 0 and *r* = 20\.
    We’ll place a moving particle between them, and use units where its mass is 1\.
    [Figure 13-4](ch13.xhtml#ch13fig4) shows the combined potential of the two fixed
    particles.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch13fig04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-4: The total potential of two imaginary particles*'
  prefs: []
  type: TYPE_NORMAL
- en: We’ll insert the moving particle into the system at *r* = 5*.*0, with an initial
    velocity of 0.2035\. This positive velocity starts the particle moving to the
    right at *t* = 0\. With a zero initial velocity, it would oscillate within the
    shallow well centered on *x* = 10, between *x* = 5 and *x* = 15\. Its particular
    initial velocity gives the particle barely enough energy to surmount the potential
    hill near *x* = 16.
  prefs: []
  type: TYPE_NORMAL
- en: In [Listing 13-2](ch13.xhtml#ch13lis2), we proceed as in the revisited pendulum
    problem in [Listing 13-1](ch13.xhtml#ch13lis1).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 13-2: Solving for the motion between two imaginary particles*'
  prefs: []
  type: TYPE_NORMAL
- en: We derive the forces by applying automatic differentiation to the potential
    function, which is the sum of the two contributions from the two fixed particles
    ➊, evaluating the derivatives at the distance from each particle. The `p` array
    holds the positions of these two particles ➋, and the `u0` array contains the
    initial position and initial velocity of the moving particle ➌. After establishing
    a time span for the solution, we define the ordinary differential equation (ODE)
    problem and store its solution in `sol` as before.
  prefs: []
  type: TYPE_NORMAL
- en: A first attempt at a solution is shown in [Figure 13-5](ch13.xhtml#ch13fig5),
    which shows the position of the moving particle as a function of time.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch13fig05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-5: An inaccurate solution*'
  prefs: []
  type: TYPE_NORMAL
- en: We extract the position variable from the solution as explained in [Chapter
    9](ch09.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: Scientists should always cast a critical eye over purported numerical solutions
    to differential equations. Our first instinct should be to examine the output
    of the solver in light of everything we know about how the solution should behave.
    In this case, we know that the solution should be periodic, as nothing in the
    definition of the problem can add or remove energy. The result in [Figure 13-5](ch13.xhtml#ch13fig5)
    is clearly not accurately periodic.
  prefs: []
  type: TYPE_NORMAL
- en: The `DifferentialEquations` package provides many options for solution methods
    and exposes several parameters for tweaking the behavior of the solvers. See “Further
    Reading” on [page 427](ch13.xhtml#fur13) for a link to the relevant part of the
    documentation. As the differential equation set up in [Listing 13-2](ch13.xhtml#ch13lis2)
    is not of a difficult type, we can probably stick with the default solver. The
    accuracy issue is most likely caused by the nature of the potential and the initial
    velocity, which, as mentioned, is near a critical value that determines whether
    the particle will surmount a local potential maximum. This suggests that simply
    applying an error bound may be sufficient. The `reltol` parameter, supplied as
    a keyword argument to `solve()`, adjusts the adaptive timestepping as needed to
    limit the local error to the value that we supply, as described in “Parametric
    Instability” on [page 300](ch09.xhtml#ch09lev1sec19). Its default is 0.001, which
    is probably not stringent enough for this problem. Smaller changes in the initial
    velocity have a large effect on the particle’s motion. If we try again using `sol
    = solve(prob; reltol=1e-6)`, we get the solution shown in [Figure 13-6](ch13.xhtml#ch13fig6).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch13fig06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-6: An accurate solution*'
  prefs: []
  type: TYPE_NORMAL
- en: The new solution appears to be accurately periodic. Furthermore, reducing `reltol`
    further doesn’t change the solution, which supplies some reassurance that it’s
    converged to the right answer.
  prefs: []
  type: TYPE_NORMAL
- en: The derivative of U happens to be
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/413math.jpg)'
  prefs: []
  type: TYPE_IMG
- en: which would be somewhat more annoying to work with directly.
  prefs: []
  type: TYPE_NORMAL
- en: '**Probabilistic Programming**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section introduces the `Turing` package through several examples. This
    package allows us to infer likely causes given observed effects. We’ll assume
    some comfort with several of the ideas discussed in [Chapter 10](ch10.xhtml)—in
    particular, probability and probability distributions. We’ll need to be familiar
    with these ideas to understand the output from `Turing` and to interpret its results.
  prefs: []
  type: TYPE_NORMAL
- en: '***Testing for Fairness of a Coin***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This simple example introduces the basic concepts and procedures for using `Turing`
    in probabilistic programming.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we flip a coin *L* number of times and observe that we get a total
    of *Nheads* heads. We want to assess whether what we observed shows that the coin
    is *fair* or not, where *fair* means that the probability of coming up heads is
    1/2, or very close to it. This is the type of question that probabilistic programming
    claims to be able to answer: given an effect, or a set of observations, what was
    the cause? Here the effect is the proportion of heads, and the cause is the probability
    of heads.'
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*I’m cognizant that the foregoing brief analysis may not please everyone, but
    wish to avoid becoming mired in metaphysics. The actual causes of our observations
    will be the physical details of the coin’s construction and the method of tossing.
    The probability of heads represents a summary of the cumulative effect of this
    myriad of unknown details; the description of cause as a probability reflects
    our incomplete knowledge.*'
  prefs: []
  type: TYPE_NORMAL
- en: The first step in using `Turing` is to construct a probabilistic model describing
    the probability distributions of each of the random variables in the problem.
    For some variables, these distributions are unknown, in which case we need to
    assume something reasonable, such as a uniform or normal distribution that includes
    all possible values, perhaps centered on the value that we think is most likely.
    For others, the description of the problem implies a particular distribution,
    one that is usually parameterized by observations or the values of some of the
    other variables.
  prefs: []
  type: TYPE_NORMAL
- en: In this example we have one unknown random variable, *Pheads*. We’ll assume
    that it can have any value from 0 to 1, uniformly distributed. This assumption
    means that we don’t have any a priori belief about the nature of the coin. If
    we had reason to think that it was almost certainly fair, we could instead assert
    that it was normally distributed with a mean of 1/2 and a small variance.
  prefs: []
  type: TYPE_NORMAL
- en: In `Turing` models, we represent assertions about the distributions of random
    variables using the `~` operator. Our assumption about the distribution of the
    probability of heads takes the form `Pheads ~ Uniform(0, 1)`. The `Uniform()`
    function comes from `Distributions.jl`, which `Turing` automatically imports (see
    “Distributions” on [page 321](ch10.xhtml#ch10lev7)).
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 13-3](ch13.xhtml#ch13lis3) shows the complete `Turing` model.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 13-3: A simple probabilistic program*'
  prefs: []
  type: TYPE_NORMAL
- en: After importing `Turing` and `StatsPlots`, which will be useful for visualizing
    the output, we use the `@model` macro from `Turing` to define the model. We can
    call the function that `@model` acts on anything we want; the macro understands
    the `~` operator and transforms the function into a `Turing` model.
  prefs: []
  type: TYPE_NORMAL
- en: The inputs are the observed number of heads and `L`, the total number of flips.
    As mentioned, we assume a uniform distribution for `Pheads`, the quantity that
    we’re trying to infer. The number of heads observed when we flip a coin `L` times
    is a random variable that we know has a binomial distribution parametrized by
    `L` and `Pheads` ➊ (see “Further Reading” on [page 427](ch13.xhtml#fur13) for
    a link to a brief introduction).
  prefs: []
  type: TYPE_NORMAL
- en: To understand, in outline, how `Turing` carries out its inductive process to
    infer the unknowns in the model (`Pheads` in this case) from the observations,
    we’ll imagine how we might do it manually. For the simple problem here, we might
    choose a series of `Pheads` values from 0 to 1, either deterministically or randomly,
    perhaps using `rand()`. For each of these values for `Pheads`, we can calculate
    the expectation value, or mean, of `Nheads` from its binomial distribution. The
    expectation value closest to the observed value of `Nheads` is our inferred value
    for `Pheads`.
  prefs: []
  type: TYPE_NORMAL
- en: This inference procedure would be fairly efficient because we have a simple
    formula for the mean of the binomial distribution. If we were dealing with less
    tractable distributions, including ones depending on many parameters, each with
    its own distribution, the only way to extract the expectation value would be through
    the numerical experiment of sampling from the distribution. As pointed out in
    “Random Numbers in Julia” on [page 307](ch10.xhtml#ch10lev2), the `rand()` function
    allows us to sample directly from a distribution. However, as we’ll see soon,
    a more realistic problem may include thousands of random variables and thousands
    of distributions. Naive sampling from each of them would take a prohibitively
    long time.
  prefs: []
  type: TYPE_NORMAL
- en: This is the problem that `Turing` solves. It allows us to do no more than tell
    it what the probability distributions are, then it samples from them efficiently,
    calculates expectation values as needed, and reports the results and their uncertainties
    and error estimates. We won’t go into the details of how `Turing` accomplishes
    this feat, except to say that it implements the technology of Markov chain Monte
    Carlo (MCMC) sampling, a starting point for readers who are interested in investigating
    the theoretical background.
  prefs: []
  type: TYPE_NORMAL
- en: 'To tell `Turing` to generate a report about its inferences, we issue one command
    using its `sample()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Here `coin()` is the model function from [Listing 13-3](ch13.xhtml#ch13lis3).
    Its arguments are the number of heads and the total number of flips—in this case
    60 heads out of 100 coin tosses. The next argument selects a sampling strategy
    from among the handful supplied by the `Turing` package. The initials `SMC` stand
    for sequential Monte Carlo, which performs well on simple problems. The choice
    of sampler can be a matter of trial and error; different samplers are best suited
    to different problems. (See “Further Reading” on [page 427](ch13.xhtml#fur13)
    for links to some documentation for `Turing`’s samplers.) The final argument,
    `1000`, is the number of sampling experiments to conduct. Each one produces an
    estimate for `Pheads`, and `Turing` reports the mean of these estimates, which
    is its most likely value, as shown in [Listing 13-4](ch13.xhtml#ch13lis4).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 13-4: The report from* Turing'
  prefs: []
  type: TYPE_NORMAL
- en: 'The report, which appears after 12.73 seconds on my laptop, contains a lot
    of information, but only a few numbers are essential for us. Under `Summary Statistics`,
    the `Symbol`s are the random variables whose inferred values we want: in this
    case, only `Pheads`. The best guess that `Turing` has for `Pheads` is 0.6024\.
    Another number to keep an eye on is `rhat`, which is 1.0002 in this example. If
    this number is far from 1.0, the sampling process did not converge properly, and
    we need to try a different sampler or alter the controls passed to the sampler,
    if it’s one that accepts parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can think about addressing the question implied in the title of this
    section: is the coin fair? We can gain some insight by looking at the distribution
    of the 1,000 inferences for `Pheads` resulting from the sampling procedure. The
    `histogram()` function (see “Distributions” on [page 321](ch10.xhtml#ch10lev7))
    gains the power to plot this with a simple call to `histogram(flips; normalize=true)`
    courtesy of the `Turing` and `StatsPlots` packages. We’ll plot the histogram with
    a normal distribution curve on the same graph with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The parameters in the normal distribution, plotted in the second line, are the
    mean and standard deviation taken from the report in [Listing 13-4](ch13.xhtml#ch13lis4).
    [Figure 13-7](ch13.xhtml#ch13fig7) shows the result, where we can see that the
    sampling distribution from `Turing` is quite a good approximation to the normal
    distribution with the parameters that it reports.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch13fig07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-7: The distribution of inferences for* Pheads'
  prefs: []
  type: TYPE_NORMAL
- en: Why should the distribution of the mean values of `Pheads` be normal? After
    all, we set `Pheads` up with a uniform distribution in the model. The answer is
    that the distribution in [Figure 13-7](ch13.xhtml#ch13fig7) is the distribution
    of *mean values* of the random variable `Pheads`. As demonstrated in “The Normal
    Distribution” on [page 323](ch10.xhtml#ch10lev1sec3) (using the same uniform distribution),
    the distribution of the means will be normal (Gaussian). This will be true regardless
    of the underlying distribution of the variable itself, which is an important theorem
    in probability theory and the fundamental reason for the ubiquity of the normal
    distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can apply any criterion we choose to decide whether this coin is fair after
    examining the sampling results. Although the most likely value for the probability
    of heads is very close to 0.6, strongly suggesting that we have a biased coin,
    it’s *possible* that the coin is fair. We can estimate the probability that `Pheads`
    is 1/2 directly from the normalized histogram. The two bars surrounding 0.5 on
    the horizontal axis have an area of about (0.52 - 0.48) × 0.8 = 0.32, yielding
    a probability of 3.2 percent that the coin is fair. The value of 0.8 comes from
    visually estimating the average height of the two relevant bins. We can also calculate
    this from the normal distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `cdf()` function, which stands for *cumulative density function*, returns
    the integral of the distribution supplied in the first argument from negative
    infinity to the value supplied in the second argument. Therefore, to extract the
    probability that a random variable governed by the distribution lies between two
    values, we need merely to subtract the results from two calls to `cdf()`. The
    value of 3.3 percent agrees pretty well with our estimate for the same interval
    from the histogram.
  prefs: []
  type: TYPE_NORMAL
- en: This coin has only a 3.3 percent chance of being fair. Is that strong enough
    evidence to convict it of bias? That’s up to us.
  prefs: []
  type: TYPE_NORMAL
- en: Flipping the coin 100 times provides pretty strong evidence of its shady character.
    Intuitively, we understand that if we had flipped it only 10 times, and happened
    to observe six heads, that wouldn’t be strong evidence of any non-fairness in
    the coin. Similarly, an observation of 600 heads after tossing the coin 1,000
    times would be pretty conclusive.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see the results of these two scenarios by calling `sample()` twice and
    passing the result directly to `histogram()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This is a quick way to compare the distributions when we’re not interested in
    the detailed report.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Because the results returned by* sample() *are generated partly through random
    sampling, the details will be different every time. Everyone running the code
    samples in this section will observe slightly different distributions and means,
    although the over-all conclusions should be invariant. In an important problem,
    a good practice would be to run more than one sampling experiment, try different
    samplers, and perhaps vary some of the details in the model concerning assumed
    distributions.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 13-8](ch13.xhtml#ch13fig8) shows the result.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch13fig08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-8: Weak and strong evidence*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The lighter histogram, showing the inferences from 10 flips, clearly indicates
    that we have no evidence for bias in the coin. It’s about as likely that `Pheads`
    is 1/2 as it is 6/10\. However, the observation using 1,000 flips is unambiguous:
    600 heads in that experiment makes it nearly impossible for the coin to be fair.
    The darker gray overlay of the second histogram shows a narrow distribution around
    `Pheads` = 0.6.'
  prefs: []
  type: TYPE_NORMAL
- en: '***Inferring Model Parameters from Series Observations***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In most applications of probabilistic programming, scientists are interested
    in inferring the causes of a series of observations taken over time, rather than
    merely a single number. We can extend the approach in the previous section to
    handle time series by considering the data gathered at each point in time to be
    a separate measurement with a distribution around some predicted value. The values
    can be predictions from nearly any type of model, as long as we can express it
    as a Julia function.
  prefs: []
  type: TYPE_NORMAL
- en: '**A Simple Mathematical Model**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To demonstrate the approach, we’ll first consider the problem of fitting a pair
    of parameters in a simple expression that we assume to be the cause of a series
    of observations. The model is a sine function and the two unknown parameters are
    its amplitude `A` and its frequency `f`, as shown in [Listing 13-5](ch13.xhtml#ch13lis5).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 13-5: The sine function with unknown frequency and amplitude as a
    model*'
  prefs: []
  type: TYPE_NORMAL
- en: After defining a series of times, we pick values for the `A` (amplitude) and
    `f` (frequency) parameters. We use these to generate some simulated observations,
    containing normally distributed errors, that we store in `data`. Our plan is to
    pretend we don’t know the values of `A` and `f` and to use the data, along with
    the assumed sinewave dependence, to infer their values.
  prefs: []
  type: TYPE_NORMAL
- en: In the model, we assert a priori uniform distributions for the frequency and
    amplitude that establish limits for their possible values. For each possible set
    of values, we have a prediction ➊ for the time series that would result. We consider
    the data passed to the model to be a set of physical measurements, so we assume
    that the observation at each time is normally distributed around the “true” (predicted)
    value at that time, with a standard deviation of 0.5 ➋.
  prefs: []
  type: TYPE_NORMAL
- en: The inference through sampling proceeds as in the previous section. However,
    the `SMC` sampler seems to work poorly for this class of problems. The `MH` sampler
    (for Metropolis-Hastings) works far more reliably, and it’s quite fast as well,
    but is a poor performer in other problems. (As mentioned earlier, we may need
    to experiment with a variety of sampling algorithms and their input parameters.)
    [Listing 13-6](ch13.xhtml#ch13lis6) shows the sampling command and its truncated
    output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 13-6: Inferring the values of parameters*'
  prefs: []
  type: TYPE_NORMAL
- en: The sampler returns reasonable results in less than one second. This is impressive,
    considering that the algorithm is sampling two parameters 1,000 times and using
    200 data points, each with its own distribution, to infer the final distributions
    of `A` and `f` and their expectation values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s visualize the inferred solution using the returned means of `A` and `f`
    superimposed on the simulated data and what we know is the true solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In the first two lines, we plot the model with a thick line and the noisy data
    with a thinner line. The final plot command plots a sinewave using the inferences
    for `A` and `f` as a dotted line. [Figure 13-9](ch13.xhtml#ch13fig9) shows the
    combined plot.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch13fig09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-9: Model parameters recovered from noisy data*'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 13-9](ch13.xhtml#ch13fig9) shows how the correct signal was recovered
    from the noisy observations. The periodic nature of the model means that the slight
    error in the inferred frequency will cause the curves to diverge further at later
    times.'
  prefs: []
  type: TYPE_NORMAL
- en: '**An ODE Model**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The model used for generating the prediction need not be a known function; it
    can be a set of differential equations. This is possible because `Turing` and
    `DifferentialEquations` are composable, another benefit of Julia’s type system.
    The combination is immensely powerful, and opens up new arenas for research. In
    science our models often take the form of differential equations that encode,
    in general terms, our hypotheses about how the system works. Some of the details
    of the system may remain as parameters with unknown, or partially known, values.
    Probabilistic programming, using the general procedure outlined in “Inferring
    Model Parameters from Series Observations” on [page 419](ch13.xhtml#ch13lev1sec4),
    allows us to infer the most likely values of these parameters and then check,
    quantitatively, how well our purported model performs.
  prefs: []
  type: TYPE_NORMAL
- en: For example, we may measure the trajectory of a cannonball and think we know
    that its path is governed by Newton’s laws of motion and the forces of gravity
    and air resistance. But we might not know the correct value of the gravitational
    acceleration on our planet or the coefficient of drag for the cannonball in its
    atmosphere. Assuming our differential equations are correct, we can use `Turing`
    and `DifferentialEquations` to infer the values of those two numbers from the
    observed trajectory, and then plug them back into the model to see whether we
    can reproduce the data. This approach eliminates a huge amount of trial and error,
    and it lets us iterate fluidly over variations in our models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Returning to the parametric instability problem from [Chapter 9](ch09.xhtml),
    let’s go backward: assume that we know we have a pendulum in a gravitational field,
    with a varying string length, and that we know the values for gravity, the pendulum
    mass, and the mean length of the string, but that the frequency and amplitude
    of the oscillation in the string’s length are unknown. We will, however, assume
    that the function defining that oscillation is a sin(*t*), where, as before, *t*
    is time.'
  prefs: []
  type: TYPE_NORMAL
- en: This example will show how we can work backward from data about the pendulum’s
    behavior to an estimate of the driving frequency and amplitude, using the assumption
    of the underlying physical model behind the data. Naively, we might approach this
    problem by solving the differential equation, using the techniques from [Chapter
    9](ch09.xhtml), multiple times, with various values of the unknown parameters,
    until we hit upon a solution that is close enough to the data. But this process
    will be computationally expensive and may not provide systematic knowledge of
    the uncertainty in the final result.
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 13-7](ch13.xhtml#ch13lis7) shows the problem set up for solution by
    the `Differential` `Equations` package, assembled here for convenience from [Listings
    9-8](ch09.xhtml#ch9lis8) and [9-9](ch09.xhtml#ch9lis8).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 13-7: The parametrically driven pendulum*'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the driving frequency is set to 3 percent smaller than the
    parametric resonance frequency ➊.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we proceed to apply `Turing` to this problem, let’s take a look at how
    varying the `f` and `A` values affects the results. First, we’ll plot the solution
    at resonance and slightly “detuned,” at 0.95 resonance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 13-10](ch13.xhtml#ch13fig10) shows the two solutions.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch13fig10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-10: The parametrically driven pendulum at two driving frequencies*'
  prefs: []
  type: TYPE_NORMAL
- en: As [Figure 13-10](ch13.xhtml#ch13fig10) makes clear, the solution is quite sensitive
    to the driving frequency.
  prefs: []
  type: TYPE_NORMAL
- en: Changing the driving amplitude also has a strong effect on the solution. [Figure
    13-11](ch13.xhtml#ch13fig11) shows the effect of two different forcing amplitudes
    at the same frequency.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch13fig11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-11: The parametrically driven pendulum at two driving amplitudes*'
  prefs: []
  type: TYPE_NORMAL
- en: Changing the forcing amplitude alone changes the envelope amplitude, the envelope
    timescale, and the frequency of the response.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we compare these solutions, we can see that amplitude and frequency are
    interdependent. It’s not a simple matter to infer either driving parameter from
    the response. Let’s see how well probabilistic programming with `Turing` does
    with this problem. First we’ll define a model with `A` and `f` uniformly distributed
    within reasonable intervals:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'As in the simple sinewave model, we’ll generate some noisy simulated data from
    the solution returned by `DifferentialEquations` for given values of `A` and `f`,
    and then use the `Turing` model to try to infer those numbers from the data. The
    program in the following listing goes through this procedure for a small set of
    values for `A` and `f` and plots the inferred numbers with the known values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: For each `A,f` pair, the program defines a forcing function ➊ and generates
    a solution ➋ from the differential equation. We tell the solver to save solution
    points at regular intervals using the `saveat` keyword argument and scale the
    simulated noise to the amplitude of the solution. The purpose of the solution
    is to generate the simulated noisy observations, which we then feed to the sampler
    ➌. The next command places a mark on the `A`-`f` plane of the plot corresponding
    to the true values of `A` and `f`. Then we place a mark for the inferred values,
    with error bars taken from the standard deviations of the distributions returned
    by `sample()`.
  prefs: []
  type: TYPE_NORMAL
- en: We can access the sampling results for individual parameters using indexing
    on the name of the parameter as a symbol, so `psamples[:A]` is an array of all
    3,000 values for `A` in the distribution generated by the sampler. The mean of
    this array is its expectation value (and the value printed in the report printed
    in the REPL). The `std()` function calculates the standard deviation of an array,
    returning the same number as in the report under `std`.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 13-12](ch13.xhtml#ch13fig12) shows the result.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch13fig12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-12: Inference of forcing parameters in the parametric pendulum*'
  prefs: []
  type: TYPE_NORMAL
- en: The experiment works well using 3,000 samples; however, the same program run
    with 1,000 samples performs distinctly worse. [Figure 13-12](ch13.xhtml#ch13fig12)
    shows that each inferred value is correct within its reported standard deviation,
    and most of those spreads are small. Despite the complexity and sensitivity of
    this problem, `Turing` and `DifferentialEquations` were able to work together
    to confirm the faithfulness of the model and accurately induce the correct model
    parameters. Doubtless with further tuning of the sampling method, we could improve
    the results even further.
  prefs: []
  type: TYPE_NORMAL
- en: '**Conclusion**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The field of scientific machine learning is making impressive strides and expanding
    rapidly as I write this. Julia users are perfectly positioned to take advantage
    of recent research in this field, as it finds application in the packages of the
    SciML ecosystem. Scientific machine learning selects some of the technologies
    developed in ML that can be fruitfully applied to science and engineering concerns.
    A survey of the entire field would be a book in itself. In this chapter we’ve
    explored a few central ideas and applied them to problems that, while interesting
    in themselves, are simple enough not to obscure the working of the SciML machinery
    with too much incidental detail. These ideas and techniques can be applied to
    all areas of quantitative science. This is an exciting field to follow. Wherever
    it goes, it will inevitably become a pillar of computational science.
  prefs: []
  type: TYPE_NORMAL
- en: '**FURTHER READING**'
  prefs: []
  type: TYPE_NORMAL
- en: 'See “The Essential Tools of Scientific Machine Learning (Scientific ML)” by
    Christopher Rackauckas for an introduction to existing open source tools: [*http://www.stochasticlifestyle.com/the-essential-tools-of-scientific-machine-learning-scientific-ml/*](http://www.stochasticlifestyle.com/the-essential-tools-of-scientific-machine-learning-scientific-ml/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A solid mathematical introduction to automatic differentiation is available
    at [*http://www.ams.org/publicoutreach/feature-column/fc-2017-12*](http://www.ams.org/publicoutreach/feature-column/fc-2017-12).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is a hub for Julia’s SciML documentation: [*https://docs.sciml.ai/*](https://docs.sciml.ai/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a description of the various solver options for the `Differential` `Equations.jl`
    package, visit [*https://docs.sciml.ai/DiffEqDocs/stable/basics/common_solver_opts/*](https://docs.sciml.ai/DiffEqDocs/stable/basics/common_solver_opts/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Details on the binomial distribution can be found at [*https://www.itl.nist.gov/div898/handbook/eda/section3/eda366i.htm*](https://www.itl.nist.gov/div898/handbook/eda/section3/eda366i.htm).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for the `Turing` package resides at [*https://turinglang.org/dev/docs/using-turing/get-started*](https://turinglang.org/dev/docs/using-turing/get-started).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a tutorial on the use of `Turing`, visit [*https://turinglang.org/dev/docs/using-turing/guide*](https://turinglang.org/dev/docs/using-turing/guide).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
