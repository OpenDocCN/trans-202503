<html><head></head><body>
<h2 class="h2" id="app01"><span epub:type="pagebreak" id="page_405"/><strong><span class="big">A</span><br/>ALGORITHM RUNTIME</strong></h2>
<div class="image1"><img alt="Image" src="../images/common01.jpg"/></div>
<p class="noindent">Each competitive programming problem that we solve in this book specifies a time limit on how long our program will be allowed to run. If our program exceeds the time limit, then the judge terminates our program with a “Time-Limit Exceeded” error. A time limit is designed to prevent algorithmically naive solutions from passing the test cases. The problem author has some model solutions in mind and sets the time limit as an arbiter of whether we have demonstrated those solution ideas. As such, in addition to being correct, we need our programs to be fast.</p>
<h3 class="h3" id="lev69">The Case for Timing . . . and Something Else</h3>
<p class="noindent">Most books on algorithms do not use time limits when discussing runtime. Time limits and execution times do, however, appear frequently in this book. The primary reason is that such times can give us intuitive understanding of the efficiency of our programs. We can run a program and measure how <span epub:type="pagebreak" id="page_406"/>long it takes. If our program is too slow, according to the time limit for the problem, then we know that we need to optimize the current code or find a wholly new approach. We don’t know what kind of computer the judge is using, but running the program on our own computer is still informative. Say that we run our program on our laptop and it takes 30 seconds on some small test case. If the problem time limit is three seconds, we can be confident that our program is simply not fast enough.</p>
<p class="indentb">An exclusive focus on execution times, however, is limiting. Here are five reasons why:</p>
<p class="block95"><strong>Execution time depends on the computer.</strong> As just suggested, timing our program tells us only how long our program takes on one computer. That’s very specific information, and it gives us little in the way of understanding what to expect when it is run on other computers. When working through the book, you may also notice that the time taken by a program varies from run to run, even on the same computer. For example, you might run a program on a test case and find that it takes 3 seconds; you might then run it again, on the same test case, and find that it takes 2.5 seconds or 3.5 seconds. The reason for this difference is that your operating system is managing your computing resources, shunting them around to different tasks as needed. The decisions that your operating system makes influence the runtime of your program.</p>
<p class="block95"><strong>Execution time depends on the test case.</strong> Timing our program on a test case tells us only how long our program takes on that test case. Suppose that our program takes one second to run on a small test case. That may seem fast, but here’s the truth about small test cases: every reasonable solution for a problem will be able to solve those. If I ask you to sort a few numbers, or optimally schedule a few events, or whatever, you can quickly do it with the first correct idea that you have. What’s interesting, then, are large test cases. They are the ones where algorithmic ingenuity pays off. How long will our program take on a large test case or on a huge test case? We don’t know. We’d have to run our program on those test cases, too. Even if we did that, there could be specific kinds of test cases that trigger poorer performance. We may be led to believe that our program is faster than it is.</p>
<p class="block95"><strong>The program requires implementation.</strong> We can’t time something that we don’t implement. Suppose that we’re thinking about a problem and come up with an idea for how to solve it. Is it fast? Although we could implement it to find out, it would be nice to know, in advance, whether or not the idea is likely to lead to a fast program. You would not implement a program that you knew, at the outset, would be incorrect. It would similarly be nice to know, at the outset, that a program would be too slow.</p>
<p class="block95"><strong>Timing doesn’t explain slowness.</strong> If we find that our program is too slow, then our next task is to design a faster one. However, simply timing a program gives us no insight into why our program is slow. It just <span epub:type="pagebreak" id="page_407"/>is. Further, if we manage to think up a possible improvement to our program, we’d need to implement it to see whether or not it helps.</p>
<p class="block95"><strong>Execution time is not easily communicated.</strong> For many of the reasons above, it’s difficult to use execution time to talk to other people about the efficiency of algorithms. “My program takes two seconds to run on this computer that I bought last year, on a test case with eight chickens and four eggs, using a program that I wrote in C. How about yours?”</p>
<p class="indent">Not to worry: computer scientists have devised a notation that addresses these shortcomings of timing. It’s independent of the computer, independent of test case, and independent of a particular implementation. It signals why a slow program is slow. It’s easily communicated. It’s called <em>big O</em>, and it’s coming right up.</p>
<h3 class="h3" id="lev70">Big O Notation</h3>
<p class="noindent">Big O is a notation that computer scientists use to concisely describe the efficiency of algorithms. It assigns each algorithm to one of a small number of efficiency classes. An efficiency class tells you how fast an algorithm is or, equivalently, how much work it does. The faster an algorithm, the less work it does; the slower an algorithm, the more work it does. Each algorithm belongs to an efficiency class; the efficiency class tells you how much work that algorithm does relative to the amount of input that it must process. To understand big O, we need to understand these efficiency classes. I’ll introduce three of them here: linear time, constant time, and quadratic time.</p>
<h4 class="h4" id="sec174"><em>Linear Time</em></h4>
<p class="noindent">Suppose that we are provided an array of integers in increasing order, and we want to return its maximum integer. For example, given the array</p>
<pre>[1, 3, 8, 10, 21]</pre>
<p class="noindent">we want to return <code>21</code>.</p>
<p class="indent">One way to do this is to keep track of the maximum value that we have found so far. Whenever we find a larger value than the maximum, we update the maximum. <a href="app01.xhtml#app01ex01">Listing A-1</a> implements this idea.</p>
<pre>int find_max(int nums[], int n) {
  int i, max;
  max = nums[0];
  for (i = 0; i &lt; n; i++)
    if (nums[i] &gt; max)
      max = nums[i];
  return max;
}</pre>
<p class="excap" id="app01ex01"><em>Listing A-1: Finding the maximum in an array of increasing integers</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_408"/>The code sets <code>max</code> to the value at index <code>0</code> of <code>nums</code>, and then loops through the array, looking for larger values. Don’t worry that the first iteration of the loop compares <code>max</code> to itself: that’s just one iteration of unnecessary work.</p>
<p class="indent">Rather than timing specific test cases, let’s think about the amount of work that this algorithm does as a function of the size of the array. Suppose that the array has five elements. What does our program do? It performs one variable assignment above the loop, then iterates five times in the loop, and then returns the result. If the array has 10 elements, then our program does similarly, except now it iterates 10 times in the loop rather than 5. What about a million elements? Our program iterates a million times. Now we see that the assignment above the loop and return below the loop pale in comparison to the amount of work done by the loop. What matters, especially as the test case gets large, is the number of iterations of the loop.</p>
<p class="indent">If our array has <em>n</em> elements, then the loop iterates <em>n</em> times. In big O notation, we say that this algorithm is <em>O</em>(<em>n</em>). Interpret this as follows: for an array of <em>n</em> elements, the algorithm does work proportional to <em>n</em>. An <em>O</em>(<em>n</em>) algorithm is called a <em>linear-time algorithm</em> because there is a linear relationship between the problem size and the amount of work done. If we double the problem size, then we double the work done and thereby double the runtime. For example, if it takes one second to run on an array with two million elements, we can expect it to take about two seconds to run on an array of four million elements.</p>
<p class="indent">Notice that we didn’t have to run the code to arrive at this insight. We didn’t even have to write the code out. (Well . . . yeah, I did write the code, but that was just to make the algorithm clear.) Saying that an algorithm is <em>O</em>(<em>n</em>) offers us the fundamental relationship between the problem size and the growth in runtime. It’s true no matter what computer we use or which test case we look at.</p>
<h4 class="h4" id="sec175"><em>Constant Time</em></h4>
<p class="noindent">We know something about our arrays that we didn’t exploit yet: that the integers are in increasing order. The biggest integer will therefore be found at the end of the array. Let’s just return that directly, rather than eventually finding it through an exhaustive search of the array. <a href="app01.xhtml#app01ex02">Listing A-2</a> presents this new idea.</p>
<pre>int find_max(int nums[], int n) {
  return nums[n - 1];
}</pre>
<p class="excap" id="app01ex02"><em>Listing A-2: Finding the maximum in an array of increasing integers</em></p>
<p class="indent">How much work does this algorithm do as a function of the size of the array? Interestingly, array size no longer matters! The algorithm accesses and returns <code>nums[n - 1]</code>, the final element of the array, no matter if it has 5 elements or 10 or a million. The algorithm doesn’t care. In big O notation, we say that this algorithm is <em>O</em>(1). It’s called a <em>constant-time algorithm</em> because <span epub:type="pagebreak" id="page_409"/>the amount of work it does is constant, not increasing as the problem size increases.</p>
<p class="indent">This is the best kind of algorithm. No matter how large our array, we can expect about the same runtime. It’s surely better than a linear-time algorithm, which gets slower as the problem size increases. Not many interesting problems can be solved by constant-time algorithms, though. For example, if we were given the array in arbitrary order, rather than increasing order, then constant-time algorithms are out. There’s no way we could look at a fixed number of array elements and hope to be guaranteed to find the maximum.</p>
<h4 class="h4" id="sec176"><em>Another Example</em></h4>
<p class="noindent">Consider the algorithm in <a href="app01.xhtml#app01ex03">Listing A-3</a>: is it <em>O</em>(<em>n</em>) or <em>O</em>(1) or something else? (Notice that I’ve left out the function and variable definitions so that we’re not tempted to compile and run this.)</p>
<pre>total = 0;
for (i = 0; i &lt; n; i++)
  total = total + nums[i];
for (i = 0; i &lt; n; i++)
  total = total + nums[i];</pre>
<p class="excap" id="app01ex03"><em>Listing A-3: What kind of algorithm is this?</em></p>
<p class="indent">Suppose that array <code>nums</code> has <em>n</em> elements. The first loop iterates <em>n</em> times, and the second loop iterates <em>n</em> times. That’s 2<em>n</em> iterations in total. As a first attempt, it’s natural to say that this algorithm is <em>O</em>(2<em>n</em>). While saying that is technically true, computer scientists would ignore the 2, simply writing <em>O</em>(<em>n</em>).</p>
<p class="indent">This may seem weird, since this algorithm is twice as slow as the one in <a href="app01.xhtml#app01ex01">Listing A-1</a>, yet we declare both to be <em>O</em>(<em>n</em>). The reason comes down to a balancing act between simplicity and expressiveness of our notation. If we kept the 2, then we’d perhaps be more accurate, but we’d obscure the fact that this is a linear-time algorithm. Whether it’s 2<em>n</em> or 3<em>n</em> or anything times <em>n</em>, it’s fundamental linear runtime growth does not change.</p>
<h4 class="h4" id="sec177"><em>Quadratic Time</em></h4>
<p class="noindent">We have now seen linear-time algorithms (which are very fast in practice) and constant-time algorithms (which are even faster than linear-time algorithms). Now let’s look at something slower than linear time. The code is in <a href="app01.xhtml#app01ex04">Listing A-4</a>.</p>
<pre>total = 0;
for (i = 0; i &lt; n; i++)
  for (j = 0; j &lt; n; j++)
    total = total + nums[j];</pre>
<p class="excap" id="app01ex04"><em>Listing A-4: A quadratic-time algorithm</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_410"/>Compared to <a href="app01.xhtml#app01ex03">Listing A-3</a>, notice that the loops are now nested rather than sequential. Each iteration of the outer loop causes <em>n</em> iterations of the inner loop. The outer loop iterates <em>n</em> times. Therefore, the total number of iterations for the inner loop, and the number of times that we update <code>total</code>, is <em>n</em><sup>2</sup>. (The first iteration of the outer loop costs <em>n</em> work, the second costs <em>n</em> work, the third costs <em>n</em> work, and so on. The total is <em>n</em> + <em>n</em> + <em>n</em> + . . . + <em>n</em>, where the number of times we add <em>n</em> is <em>n</em>.)</p>
<p class="indent">In big O notation, we say that this algorithm is <em>O</em>(<em>n</em><sup>2</sup>). It’s called a <em>quadratic-time algorithm</em> because quadratic is the mathematical term referring to a power of 2.</p>
<p class="indent">Let’s now probe why quadratic-time algorithms are slower than linear-time algorithms. Suppose that we have a quadratic-time algorithm that takes <em>n</em><sup>2</sup> steps. On a problem size of 5, it would take 5<sup>2</sup> = 25 steps; on a problem size of 10, it would take 10<sup>2</sup> = 100 steps; and on a problem size of 20, it would take 20<sup>2</sup> = 400 steps. Notice what’s happening when we double the problem size: the work done <em>quadruples</em>. That’s far worse than linear-time algorithms, where doubling the problem size leads to only a doubling of work done.</p>
<p class="indent">Don’t be surprised that an algorithm that takes 2<em>n</em><sup>2</sup> steps, 3<em>n</em><sup>2</sup> steps, and so on is also classified as a quadratic-time algorithm. The big O notation hides what’s in front of the <em>n</em><sup>2</sup> term, just as it hides what’s in front of the <em>n</em> term in a linear-time algorithm.</p>
<p class="indent">What if we have an algorithm that we find takes 2<em>n</em><sup>2</sup> + 6<em>n</em> steps? This, too, is a quadratic-time algorithm. We’re taking a quadratic runtime of 2<em>n</em><sup>2</sup> and adding a linear runtime of 6<em>n</em> to it. The result is still a quadratic-time algorithm: the quadrupling behavior of the quadratic part quickly comes to dominate the doubling behavior of the linear part.</p>
<h4 class="h4" id="sec178"><em>Big O in This Book</em></h4>
<p class="noindent">There’s much more that can be said about big O. It has a formal mathematical basis used by computer scientists to rigorously analyze the runtime of their algorithms. There are other efficiency classes besides the three that I’ve introduced here (and I’ll introduce the few others that appear in this book as needed). There is certainly more to learn if you are interested in going further, but what I’ve presented here is enough for our purposes.</p>
<p class="indent">Big O generally arises in this book on an as-needed basis. We may pursue an initial solution for a problem, only to find that we get a “Time-Limit Exceeded” error from the judge. In those cases, we need to understand where we went wrong, and the first step in such an analysis is to appreciate the way that our runtime grows as a function of problem size. A big O analysis not only confirms that slow code is slow, but it often uncovers the particular bottlenecks in our code. We can then use that enhanced understanding to design a more efficient solution.</p>
</body></html>