- en: '**15'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'FROM PARAMETER ESTIMATION TO HYPOTHESIS TESTING: BUILDING A BAYESIAN A/B TEST**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this chapter, we’re going to build our first hypothesis test, an *A/B test*.
    Companies often use A/B tests to try out product web pages, emails, and other
    marketing materials to determine which will work best for customers. In this chapter,
    we’ll test our belief that removing an image from an email will increase the *click-through
    rate* against the belief that removing it will hurt the click-through rate.
  prefs: []
  type: TYPE_NORMAL
- en: Since we already know how to estimate a single unknown parameter, all we need
    to do for our test is estimate both parameters—that is, the conversion rates of
    each email. Then we’ll use R to run a Monte Carlo simulation and determine which
    hypothesis is likely to perform better—in other words, which variant, A or B,
    is superior. A/B tests can be performed using classical statistical techniques
    such as *t*-tests, but building our test the Bayesian way will help us understand
    each part of it intuitively and give us more useful results as well.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve covered the basics of parameter estimation pretty well at this point.
    We’ve seen how to use the PDF, CDF, and quantile functions to learn the likelihood
    of certain values, and we’ve seen how to add a Bayesian prior to our estimate.
    Now we want to use our estimates to compare *two* unknown parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '**Setting Up a Bayesian A/B Test**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Keeping with our email example from the previous chapter, imagine we want to
    see whether adding an image helps or hurts the conversion rate for our blog. Previously,
    the weekly email has included some image. For our test we’re going to send one
    variant with images like usual, and another without images. The test is called
    an *A/B test* because we are comparing variant A (with image) and variant B (without)
    to determine which one performs better.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s assume at this point we have 600 blog subscribers. Because we want to
    exploit the knowledge gained during this experiment, we’re only going to be running
    our test on 300 of them; that way, we can send the remaining 300 subscribers what
    we believe to be the most effective variant of the email.
  prefs: []
  type: TYPE_NORMAL
- en: The 300 people we’re going to test will be split up into two groups, A and B.
    Group A will receive the usual email with a big picture at the top, and group
    B will receive an email with no picture. The hope is that a simpler email will
    feel less “spammy” and encourage users to click through to the content.
  prefs: []
  type: TYPE_NORMAL
- en: '***Finding Our Prior Probability***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Next, we need to figure out what prior probability we’re going to use. We’ve
    run an email campaign every week, so from that data we have a reasonable expectation
    that the probability of clicking the link to the blog on any given email should
    be around 30 percent. To make things simple, we’ll use the same prior for both
    variants. We’ll also choose a pretty weak version of our prior distribution, meaning
    that it considers a wider range of conversion rates to be probable. We’re using
    a weak prior because we don’t really know how well we expect B to do, and this
    is a new email campaign, so other factors could cause a better or worse conversion.
    We’ll settle on Beta(3,7) for our prior probability distribution. This distribution
    allows us to represent a beta distribution where 0.3 is the mean, but a wide range
    of possible alternative rates are considered. We can see this distribution in
    [Figure 15-1](ch15.xhtml#ch15fig01).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/15fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-1: Visualizing our prior probability distribution*'
  prefs: []
  type: TYPE_NORMAL
- en: All we need now is our likelihood, which means we need to collect data.
  prefs: []
  type: TYPE_NORMAL
- en: '***Collecting Data***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We send out our emails and get the results in [Table 15-1](ch15.xhtml#ch15tab01).
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 15-1:** Email Click-through Rates'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Clicked** | **Not clicked** | **Observed conversion rate** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Variant A** | 36 | 114 | 0.24 |'
  prefs: []
  type: TYPE_TB
- en: '| **Variant B** | 50 | 100 | 0.33 |'
  prefs: []
  type: TYPE_TB
- en: We can treat each of these variants as a separate parameter we’re trying to
    estimate. In order to arrive at a posterior distribution for each, we need to
    combine both their likelihood distribution and prior distribution. We’ve already
    decided that the prior for these distributions should be Beta(3,7), representing
    a relatively weak belief in what possible values we expect the conversion rate
    to be, given no additional information. We say this is a weak belief because we
    don’t believe very strongly in a particular range of values, and consider all
    possible rates with a reasonably high probability. For the likelihood of each,
    we’ll again use the beta distribution, making α the number of times the link was
    clicked through and β the number of times it was not.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that:'
  prefs: []
  type: TYPE_NORMAL
- en: Beta(α[posterior], β[posterior]) = Beta(α[prior] + α[likelihood], β[prior] +
    β[likelihood])
  prefs: []
  type: TYPE_NORMAL
- en: Variant A will be represented by Beta(36+3,114+7) and variant B by Beta(50+3,100+7).
    [Figure 15-2](ch15.xhtml#ch15fig02) shows the estimates for each parameter side
    by side.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/15fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-2: Beta distributions for our estimates for both variants of our
    email*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Clearly, our data suggests that variant B is superior, in that it garners a
    higher conversion rate. However, from our earlier discussion on parameter estimation,
    we know that the true conversion rate is one of a range of possible values. We
    can also see here that there’s an overlap between the possible true conversion
    rates for A and B. What if we were just unlucky in our A responses, and A’s true
    conversion rate is in fact much higher? What if we were also just lucky with B,
    and its conversion rate is in fact much lower? It’s easy to see a possible world
    in which A is actually the better variant, even though it did worse on our test.
    So the real question is: how sure can we be that B is the better variant? This
    is where the Monte Carlo simulation comes in.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Monte Carlo Simulations**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The accurate answer to which email variant generates a higher click-through
    rate lies somewhere in the intersection of the distributions of A and B. Fortunately,
    we have a way to figure it out: a Monte Carlo simulation. A *Monte Carlo simulation*
    is any technique that makes use of random sampling to solve a problem. In this
    case, we’re going to randomly sample from the two distributions, where each sample
    is chosen based on its probability in the distribution so that samples in a high-probability
    region will appear more frequently. For example, as we can see in [Figure 15-2](ch15.xhtml#ch15fig02),
    a value *greater* than 0.2 is far more likely to be sampled from A than a value
    less than 0.2\. However, a random sample from distribution B is nearly certain
    to be above 0.2\. In our random sampling, we might pick out a value of 0.2 for
    variant A and 0.35 for variant B. Each sample is random, and based on the relative
    probability of values in the A and B distributions. The values 0.2 for A and 0.35
    for B both could be the true conversion rate for each variant based on the evidence
    we’ve observed. This individual sampling from the two distributions confirms the
    belief that variant B is, in fact, superior to A, since 0.35 is larger than 0.2.'
  prefs: []
  type: TYPE_NORMAL
- en: However, we could also sample 0.3 for variant A and 0.27 for variant B, both
    of which are reasonably likely to be sampled from their respective distributions.
    These are also both realistic possible values for the true conversion rate of
    each variant, but in this case, they indicate that variant B is actually worse
    than variant A.
  prefs: []
  type: TYPE_NORMAL
- en: We can imagine that the posterior distribution represents all the worlds that
    could exist based on our current state of beliefs regarding each conversion rate.
    Every time we sample from each distribution, we’re seeing what one possible world
    could look like. We can tell visually in [Figure 15-1](ch15.xhtml#ch15fig01) that
    we should expect more worlds where B is truly the better variant. The more frequently
    we sample, the more precisely we can tell in exactly how many worlds, of all the
    worlds we’ve sampled from, B is the better variant. Once we have our samples,
    we can look at the ratio of worlds where B is the best to the total number of
    worlds we’ve looked at and get an exact probability that B is in fact greater
    than A.
  prefs: []
  type: TYPE_NORMAL
- en: '***In How Many Worlds Is B the Better Variant?***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now we just have to write the code that will perform this sampling. R’s `rbeta()`
    function allows us to automatically sample from a beta distribution. We can consider
    each comparison of two samples a single trial. The more trials we run, the more
    precise our result will be, so we’ll start with 100,000 trials by assigning this
    value to the variable `n.trials`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we’ll put our prior alpha and beta values into variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we need to collect samples from each variant. We’ll use `rbeta()` for
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We’re saving the results of the `rbeta()` samples into variables, too, so we
    can access them more easily. For each variant, we input the number of people who
    clicked through to the blog and the number of people who didn’t.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we compare how many times the `b.samples` are greater than the `a.samples`
    and divide that number by `n.trials`, which will give us the percentage of the
    total trials where variant B was greater than variant A:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The result we end up with is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: What we see here is that in 96 percent of the 100,000 trials, variant B was
    superior. We can imagine this as looking at 100,000 possible worlds. Based on
    the distribution of possible conversion rates for each variant, in 96 percent
    of the worlds variant B was the better of the two. This result shows that, even
    with a relatively small number of observed samples, we have a pretty strong belief
    that B is the better variant. If you’ve ever done *t*-tests in classical statistics,
    this is roughly equivalent—if we used a Beta(1,1) prior—to getting a *p*-value
    of 0.04 from a single-tailed *t*-test (often considered “statistically significant”).
    However, the beauty of our approach is that we were able to build this test from
    scratch using just our knowledge of probability and a straightforward simulation.
  prefs: []
  type: TYPE_NORMAL
- en: '***How Much Better Is Each Variant B Than Each Variant A?***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now we can say precisely how certain we are that B is the superior variant.
    However, if this email campaign were for a real business, simply saying “B is
    better” wouldn’t be a very satisfactory answer. Don’t you really want to know
    *how much better*?
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the real power of our Monte Carlo simulation. We can take the exact
    results from our last simulation and test how much better variant B is likely
    to be by looking at how many times greater the B samples are than the A samples.
    In other words, we can look at this ratio:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0154-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In R, if we take the `a.samples` and `b.samples` from before, we can compute
    `b.samples`/`a.samples`. This will give us a distribution of the relative improvements
    from variant A to variant B. When we plot out this distribution as a histogram,
    as shown in [Figure 15-3](ch15.xhtml#ch15fig03), we can see how much we expect
    variant B to improve our click-through rate.
  prefs: []
  type: TYPE_NORMAL
- en: From this histogram we can see that variant B will most likely be about a 40
    percent improvement (ratio of 1.4) over A, although there is an entire range of
    possible values. As we discussed in [Chapter 13](ch13.xhtml#ch13), the cumulative
    distribution function (CDF) is much more useful than a histogram for reasoning
    about our results. Since we’re working with data rather than a mathematical function,
    we’ll compute the *empirical* cumulative distribution function with R’s `ecdf()`
    function. The eCDF is illustrated in [Figure 15-4](ch15.xhtml#ch15fig04).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/15fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-3: A histogram of possible improvements we might see*'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/15fig04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-4: A distribution of possible improvements we might see*'
  prefs: []
  type: TYPE_NORMAL
- en: Now we can see our results more clearly. There is really just a small, small
    chance that A is better, and even if it is better, it’s not going to be by much.
    We can also see that there’s about a 25 percent chance that variant B is a 50
    percent or more improvement over A, and even a reasonable chance it could be more
    than double the conversion rate! Now, in choosing B over A, we can actually reason
    about our risk by saying, “The chance that B is 20 percent worse is roughly the
    same that it’s 100 percent better.” Sounds like a good bet to me, and a much better
    statement of our knowledge than, “There is a statistically significant difference
    between B and A.”
  prefs: []
  type: TYPE_NORMAL
- en: '**Wrapping Up**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter we saw how parameter estimation naturally extends to a form
    of hypothesis testing. If the hypothesis we want to test is “variant B has a better
    conversion rate than variant A,” we can start by first doing parameter estimation
    for the possible conversion rates of each variant. Once we know those estimates,
    we can use the Monte Carlo simulation in order to sample from them. By comparing
    these samples, we can come up with a probability that our hypothesis is true.
    Finally, we can take our test one step further by seeing how well our new variant
    performs in these possible worlds, estimating not only whether the hypothesis
    is true, but also how much improvement we are likely to see.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercises**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Try answering the following questions to see how well you understand running
    A/B tests. The solutions can be found at *[https://nostarch.com/learnbayes/](https://nostarch.com/learnbayes/)*.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose a director of marketing with many years of experience tells you he believes
    very strongly that the variant without images (B) won’t perform any differently
    than the original variant. How could you account for this in our model? Implement
    this change and see how your final conclusions change as well.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The lead designer sees your results and insists that there’s no way that variant
    B should perform better with no images. She feels that you should assume the conversion
    rate for variant B is closer to 20 percent than 30 percent. Implement a solution
    for this and again review the results of our analysis.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Assume that being 95 percent certain means that you’re more or less “convinced”
    of a hypothesis. Also assume that there’s no longer any limit to the number of
    emails you can send in your test. If the true conversion for A is 0.25 and for
    B is 0.3, explore how many samples it would take to convince the director of marketing
    that B was in fact superior. Explore the same for the lead designer. You can generate
    samples of conversions with the following snippet of R:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
