<html><head></head><body>
<h2 class="h2" id="ch08"><span epub:type="pagebreak" id="page_129"/><span class="big">8</span><br/>OVERLAY NETWORKS</h2>&#13;
<div class="image1"><img alt="image" src="../images/common01.jpg"/></div>&#13;
<p class="noindent">Container networking is complex enough when all of the containers are on a single host, as we saw in <a href="ch04.xhtml#ch04">Chapter 4</a>. When we scale up to a cluster of nodes, all of which run containers, the complexity increases substantially. Not only must we provide each container with its own virtual network devices and manage IP addresses, dynamically creating new network namespaces and devices when containers are created, but we also need to ensure that containers on one node can communicate with containers on all the other nodes.</p>&#13;
<p class="indent">In this chapter, we’ll describe how <em>overlay networks</em> are used to provide the appearance of a single container network across all nodes in a Kubernetes cluster. We’ll consider two different approaches for routing container traffic across a host network, examining the network configuration and traffic flows for each. Finally, we’ll explore how Kubernetes uses the Container Network Interface (CNI) standard to configure networking as a separate plug-in, making it easy to shift to new technology as it becomes available and allowing for custom solutions where needed.</p>&#13;
<h3 class="h3" id="ch00lev1sec36"><span epub:type="pagebreak" id="page_130"/>Cluster Networking</h3>&#13;
<p class="noindent">The fundamental goal of a Kubernetes cluster is to treat a set of hosts (physical or virtual machines) as a single computing resource that can be allocated as needed to run containers. From a networking standpoint, this means Kubernetes should be able to schedule a Pod onto any node without worrying about connectivity to Pods on other nodes. It also means that Kubernetes should have a way to dynamically allocate IP addresses to Pods in a way that supports that cluster-wide network connectivity.</p>&#13;
<p class="indent">As we’ll see in this chapter, Kubernetes uses a plug-in design to allow any compatible network software to allocate IP addresses and provide cross-node network connectivity. All plug-ins must follow a couple of important rules. First, Pod IP addresses should come from a single pool of IP addresses, although this pool can be subdivided by node. This means that we can treat all Pods as part of a single flat network, no matter where the Pods run. Second, traffic should be routable such that all Pods can see all other Pods and the control plane.</p>&#13;
<h4 class="h4" id="ch00lev2sec57">CNI Plug-ins</h4>&#13;
<p class="noindent">Plug-ins communicate with the Kubernetes cluster, specifically with <span class="literal">kubelet</span>, using the CNI standard. CNI specifies how <span class="literal">kubelet</span> finds and invokes CNI plug-ins. When a new Pod is created, <span class="literal">kubelet</span> first allocates the network namespace. It then invokes the CNI plug-in, providing it a reference to the network namespace. The CNI plug-in adds network devices to the namespace, assigns an IP address, and passes that IP address back to <span class="literal">kubelet</span>.</p>&#13;
<p class="indent">Let’s see that process in action. To do so, our examples for this chapter include two different environments with two different CNI plug-ins: Calico and WeaveNet. Both of these plug-ins provide networking for Pods but with different cross-node networking. We’ll begin with the Calico environment.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>The example repository for this book is at</em> <a href="https://github.com/book-of-kubernetes/examples">https://github.com/book-of-kubernetes/examples</a>. <em>See “Running Examples” on <a href="ch00.xhtml#ch00lev1sec2">page xx</a> for details on getting set up.</em></p>&#13;
</div>&#13;
<p class="indent">By default, CNI plug-in information is kept in <em>/etc/cni/net.d</em>. We can see the Calico configuration in that directory:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">ls /etc/cni/net.d</span>&#13;
10-calico.conflist  calico-kubeconfig</pre>&#13;
<p class="indent">The file <em>10-calico.conflist</em> contains the actual Calico configuration. The file <em>calico-kubeconfig</em> is used by Calico components to authenticate with the control plane; it was created based on a service account created during Calico installation. The configuration filename has the <em>10-</em> prefix because <span class="literal">kubelet</span> sorts any configuration files it finds and uses the first one.</p>&#13;
<p class="indent"><a href="ch08.xhtml#ch08list1">Listing 8-1</a> shows the configuration file, which is in JSON format and identifies the network plug-ins to use.</p>&#13;
<pre><span epub:type="pagebreak" id="page_131"/>root@host01:~# <span class="codestrong1">cat /etc/cni/net.d/10-calico.conflist</span> &#13;
{&#13;
  "name": "k8s-pod-network",&#13;
  "cniVersion": "0.3.1",&#13;
  "plugins": [&#13;
    {&#13;
      "type": "calico",&#13;
...&#13;
    },&#13;
    {&#13;
      "type": "bandwidth",&#13;
      "capabilities": {"bandwidth": true}&#13;
    },&#13;
    {"type": "portmap", "snat": true, "capabilities": {"portMappings": true}}&#13;
  ]&#13;
}</pre>&#13;
<p class="caption" id="ch08list1"><em>Listing 8-1: Calico configuration</em></p>&#13;
<p class="indent">The most important field is <span class="literal">type</span>; it specifies which plug-in to run. In this case, we’re running three plug-ins: <span class="literal">calico</span>, which handles Pod networking; <span class="literal">bandwidth</span>, which we can use to configure network limits; and <span class="literal">portmap</span>, which is used to expose container ports to the host network. These two plug-ins inform <span class="literal">kubelet</span> of their purposes using the <span class="literal">capabilities</span> field; as a result, when <span class="literal">kubelet</span> invokes them, it passes in the relevant bandwidth and port mapping configuration so that the plug-in can make the necessary network configuration changes.</p>&#13;
<p class="indent">To run these plug-ins, <span class="literal">kubelet</span> needs to know where they are located. The default location for the actual plug-in executables is <em>/opt/cni/bin</em>, and the name of the plug-in matches the <span class="literal">type</span> field:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">ls /opt/cni/bin</span>&#13;
bandwidth  calico-ipam  flannel      install   macvlan  sbr     vlan&#13;
bridge     dhcp         host-device  ipvlan    portmap  static&#13;
calico     firewall     host-local   loopback  ptp      tuning</pre>&#13;
<p class="indent">Here, we see a common set of network plug-ins that were installed by <span class="literal">kubeadm</span> along with our Kubernetes cluster. We also see <span class="literal">calico</span>, which was added to this directory by the Calico DaemonSet we installed after cluster initialization.</p>&#13;
<h4 class="h4" id="ch00lev2sec58">Pod Networking</h4>&#13;
<p class="noindent">Let’s look at an example Pod to get a glimpse of how the CNI plug-ins configure the Pod’s network namespace. The behavior is very similar to the work we did in <a href="ch04.xhtml#ch04">Chapter 4</a>, adding virtual network devices into network namespaces to enable communication between containers and with the host network.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_132"/>Let’s create a basic Pod:</p>&#13;
<p class="noindent6"><em>pod.yaml</em></p>&#13;
<pre>---&#13;
apiVersion: v1&#13;
kind: Pod&#13;
metadata:&#13;
  name: pod&#13;
spec:&#13;
  containers:&#13;
  - name: pod&#13;
    image: busybox&#13;
    command: &#13;
      - "sleep"&#13;
      - "infinity"&#13;
  nodeName: host01</pre>&#13;
<p class="indent">We’ve added the extra field <span class="literal">nodeName</span> to force this Pod to run on <span class="literal">host01</span>, which will make it easier to find and examine how its networking is configured.</p>&#13;
<p class="indent">We start the Pod via the usual command:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/pod.yaml</span>&#13;
pod/pod created</pre>&#13;
<p class="indent">Next, check to see that it’s running:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get pods</span>&#13;
NAME   READY   STATUS    RESTARTS   AGE&#13;
pod    1/1     Running   0          2m32s</pre>&#13;
<p class="indent">After it’s running, we can use <span class="literal">crictl</span> to capture its unique ID:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">POD_ID=$(crictl pods --name pod -q)</span>&#13;
root@host01:~# <span class="codestrong1">echo $POD_ID</span>&#13;
b7d2391320e07f97add7ccad2ad1a664393348f1dcb6f803f701318999ed0295</pre>&#13;
<p class="indent">At this point, using the Pod ID, we can find its network namespace. In <a href="ch08.xhtml#ch08list2">Listing 8-2</a>, we use <span class="literal">jq</span> to extract only the data we want, just as we did in <a href="ch04.xhtml#ch04">Chapter 4</a>. We’ll then assign it to a variable.</p>&#13;
<pre>root@host01:~# <span class="codestrong1">NETNS_PATH=$(crictl inspectp $POD_ID |</span>&#13;
  <span class="codestrong1">jq -r '.info.runtimeSpec.linux.namespaces[]|select(.type=="network").path')</span>&#13;
root@host01:~# <span class="codestrong1">echo $NETNS_PATH</span>&#13;
/var/run/netns/cni-7cffed61-fb56-9be1-0548-4813d4a8f996&#13;
root@host01:~# <span class="codestrong1">NETNS=$(basename $NETNS_PATH)</span>&#13;
root@host01:~# <span class="codestrong1">echo $NETNS</span>&#13;
cni-7cffed61-fb56-9be1-0548-4813d4a8f996</pre>&#13;
<p class="caption" id="ch08list2"><em>Listing 8-2: Network namespace</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_133"/>We now can explore the network namespace to see how Calico set up the IP address and network routing for this Pod. First, as expected, this network namespace is being used for our Pod:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">ps $(ip netns pids $NETNS)</span>&#13;
    PID TTY      STAT   TIME COMMAND&#13;
  35574 ?        Ss     0:00 /pause&#13;
  35638 ?        Ss     0:00 sleep infinity</pre>&#13;
<p class="indent">We see the two processes that we should expect. The first is a pause container that is always created whenever we create a Pod. This is a permanent container to hold the network namespace. The second is our BusyBox container running <span class="literal">sleep</span>, as we configured in the Pod YAML file.</p>&#13;
<p class="indent">Now, let’s see the configured network interfaces:</p>&#13;
<pre>root@host03:~# <span class="codestrong1">ip netns exec $NETNS ip addr</span>&#13;
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN ...&#13;
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00&#13;
    inet 127.0.0.1/8 scope host lo&#13;
       valid_lft forever preferred_lft forever&#13;
    inet6 ::1/128 scope host &#13;
       valid_lft forever preferred_lft forever&#13;
3: <span class="ent">➊</span> eth0@if16: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 ... state UP ...&#13;
    link/ether 7a:9e:6c:e2:30:47 brd ff:ff:ff:ff:ff:ff link-netnsid 0&#13;
    inet <span class="ent">➋</span> 172.31.239.205/32 brd 172.31.25.202 scope global eth0&#13;
       valid_lft forever preferred_lft forever&#13;
    inet6 fe80::789e:6cff:fee2:3047/64 scope link &#13;
       valid_lft forever preferred_lft forever</pre>&#13;
<p class="indent">Calico has created the network device <span class="literal">eth0@if16</span> in the network namespace <span class="ent">➊</span> and given it an IP address of <span class="literal">172.31.239.205</span> <span class="ent">➋</span>. Note that the network length for that IP address is <span class="literal">/32</span>, which indicates that any traffic must go through a configured router. This is different from how our bridged container networking worked in <a href="ch04.xhtml#ch04">Chapter 4</a>. It is necessary so that Calico can provide firewall capabilities via network policies.</p>&#13;
<p class="indent">The choice of IP address for this Pod was ultimately up to Calico. Calico is configured with <span class="literal">172.31.0.0/16</span> for use as the IP address space for Pods. Calico decides how to divide this address space up between nodes and then allocates IP addresses to each Pod from the range allocated to the node. Calico then passes this IP address back to <span class="literal">kubelet</span> so that it can update the Pod’s status:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get pods -o wide</span>&#13;
NAME   READY   STATUS    RESTARTS   AGE   IP                NODE    ...&#13;
pod    1/1     Running   0          16m   172.31.239.205   host01   ...</pre>&#13;
<p class="indent">When Calico created the network interface in the Pod, it created it as part of a virtual Ethernet (veth) pair. The veth pair acts as a virtual network wire that creates a connection to a network interface in the root namespace, <span epub:type="pagebreak" id="page_134"/>allowing connections outside the Pod. <a href="ch08.xhtml#ch08list3">Listing 8-3</a> lets us have a look at both halves of the veth pair.</p>&#13;
<pre>root@host01:~# <span class="codestrong1">ip netns exec $NETNS ip link</span>&#13;
...&#13;
3: eth0@if13: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue ... &#13;
    link/ether 6e:4c:3a:41:d0:54 brd ff:ff:ff:ff:ff:ff link-netnsid 0&#13;
root@host01:~# <span class="codestrong1">ip link | grep -B 1 $NETNS</span>&#13;
13: cali9381c30abed@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 ... &#13;
    link/ether ee:ee:ee:ee:ee:ee ... link-netns cni-7cffed61-fb56-9be1-0548-4813d4a8f996</pre>&#13;
<p class="caption" id="ch08list3"><em>Listing 8-3: Calico veth pair</em></p>&#13;
<p class="indent">The first command prints the network interfaces inside the namespace, whereas the second prints the interfaces on the host. Each contains the field <span class="literal">link-netns</span> pointing to the corresponding network namespace of the other interface, showing that these two interfaces create a link between our Pod’s namespace and the root namespace.</p>&#13;
<h3 class="h3" id="ch00lev1sec37">Cross-Node Networking</h3>&#13;
<p class="noindent">So far, the configuration of the virtual network devices in the container looks very similar to the container networking in <a href="ch04.xhtml#ch04">Chapter 4</a>, where there was no Kubernetes cluster installed. The difference in this case is that the network plug-in is configured not just to connect containers on a single node, but to connect containers running anywhere in the cluster.</p>&#13;
<div class="box5">&#13;
<p class="boxtitle-d"><strong>WHY NOT NAT?</strong></p>&#13;
<p class="noindents">Regular container networking does, of course, provide connectivity to the host network. However, as we’ve discussed, it accomplishes this using Network Address Translation (NAT). This is fine for containers running individual client applications, as connection tracking enables Linux to route server responses all the way into the originating container. It does not work for containers that need to act as servers, which is a key use case for a Kubernetes cluster.</p>&#13;
<p class="noindents">For most private networks that use NAT to connect to a broader network, port forwarding is used to expose specific services from within the private network. That isn’t a good solution for every container in every Pod, as we would quickly run out of ports to allocate. The network plug-ins do end up using NAT, but only to connect containers acting as clients to make connections to networks outside the cluster. In addition, we will see port forwarding behavior in <a href="ch09.xhtml#ch09">Chapter 9</a>, where it will be one possible way to expose Services outside the cluster.</p>&#13;
</div>&#13;
<p class="indent">The challenge in cross-node networking is that the Pod network has a different range of IP addresses from the host network, so the host network does not know how to route this traffic. There are a couple of different ways that network plug-ins work around this. We’ll begin by continuing with our cluster running Calico. Then, we’ll show a different cross-node networking technology using WeaveNet.</p>&#13;
<h4 class="h4" id="ch00lev2sec59"><span epub:type="pagebreak" id="page_135"/>Calico Networking</h4>&#13;
<p class="noindent">Calico performs cross-node networking using Layer 3 routing. This means that it routes based on IP addresses, configuring IP routing tables on each host and in the Pod to ensure that traffic is sent to the correct host and then to the correct Pod. Thus, at the host level, we see the Pod IP addresses as the source and destination. Because Calico relies on the built-in routing capabilities of Linux, we don’t need to configure our host network switch to route the traffic, but we do need to configure any security controls on the host network switch to allow Pod IP addresses to travel across the network.</p>&#13;
<p class="indent">To explore Calico cross-node networking, it helps to have two Pods: one on <span class="literal">host01</span> and the other on <span class="literal">host02</span>. We’ll use this resource file:</p>&#13;
<p class="noindent6"><em>two-pods.yaml</em></p>&#13;
<pre>---&#13;
apiVersion: v1&#13;
kind: Pod&#13;
metadata:&#13;
  name: pod1&#13;
spec:&#13;
  containers:&#13;
  - name: pod1&#13;
    image: busybox&#13;
    command: &#13;
      - "sleep"&#13;
      - "infinity"&#13;
  nodeName: host01&#13;
---&#13;
apiVersion: v1&#13;
kind: Pod&#13;
metadata:&#13;
  name: pod2&#13;
spec:&#13;
  containers:&#13;
  - name: pod2&#13;
    image: busybox&#13;
    command: &#13;
      - "sleep"&#13;
      - "infinity"&#13;
  nodeName: host02</pre>&#13;
<p class="indent">As always, these files have been loaded into the <em>/opt</em> directory by the automated scripts for this chapter.</p>&#13;
<p class="indent">The <span class="literal">---</span> separator allows us to put two different Kubernetes resources in the same file so that we can manage them together. The only difference in configuration with these two Pods is that they each have a <span class="literal">nodeName</span> field to ensure that they are assigned to the correct node.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_136"/>Let’s delete our existing Pod and replace it with the two that we need:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl delete -f /opt/pod.yaml</span>&#13;
pod "pod" deleted&#13;
root@host01:~# <span class="codestrong1">kubectl apply -f /opt/two-pods.yaml</span> &#13;
pod/pod1 created&#13;
pod/pod2 created</pre>&#13;
<p class="indent">After these Pods are running, we’ll need to collect their IP addresses:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">IP1=$(kubectl get po pod1 -o json | jq -r '.status.podIP')</span>&#13;
root@host01:~# <span class="codestrong1">IP2=$(kubectl get po pod2 -o json | jq -r '.status.podIP')</span>&#13;
root@host01:~# <span class="codestrong1">echo $IP1</span>&#13;
172.31.239.216&#13;
root@host01:~# <span class="codestrong1">echo $IP2</span>&#13;
172.31.89.197</pre>&#13;
<p class="indent">We’re able to extract the Pod IP using a simple <span class="literal">jq</span> filter because our <span class="literal">kubectl get</span> command is guaranteed to return only one item. If we were running <span class="literal">kubectl get</span> without a filter, or with a filter that might match multiple Pods, the JSON output would be a list and we would need to change the <span class="literal">jq</span> filter accordingly.</p>&#13;
<p class="indent">Let’s quickly verify that we have connectivity between these two Pods:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl exec -ti pod1 -- ping -c 3 $IP2</span>&#13;
PING 172.31.89.197 (172.31.89.197): 56 data bytes&#13;
64 bytes from 172.31.89.197: seq=0 ttl=62 time=2.867 ms&#13;
64 bytes from 172.31.89.197: seq=1 ttl=62 time=0.916 ms&#13;
64 bytes from 172.31.89.197: seq=2 ttl=62 time=1.463 ms&#13;
&#13;
--- 172.31.89.197 ping statistics ---&#13;
3 packets transmitted, 3 packets received, 0% packet loss&#13;
round-trip min/avg/max = 0.916/1.748/2.867 ms</pre>&#13;
<p class="indent">The <span class="literal">ping</span> command shows that all three packets arrived successfully, so we know the Pods can communicate across nodes.</p>&#13;
<p class="indent">As in our earlier example, each of these Pods has a network interface with a network length of <span class="literal">/32</span>, meaning that all traffic must go through a router. For example, here is the IP configuration and route table for <span class="literal">pod1</span>:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl exec -ti pod1 -- ip addr</span>&#13;
...&#13;
3: eth0@if17: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1450 qdisc noqueue &#13;
    link/ether f2:ed:e8:04:00:cc brd ff:ff:ff:ff:ff:ff&#13;
    inet 172.31.239.216/32 brd 172.31.239.216 scope global eth0&#13;
...&#13;
root@host01:~# <span class="codestrong1">kubectl exec -ti pod1 -- ip route</span>&#13;
default via 169.254.1.1 dev eth0 &#13;
169.254.1.1 dev eth0 scope link</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_137"/>Based on this configuration, when we run our <span class="literal">ping</span> command, the networking stack recognizes that the destination IP is not local to any interface. It therefore looks up <span class="literal">169.254.1.1</span> in its Address Resolution Protocol (ARP) table to determine where to send the “next hop.” If we try to find an interface either in the container or on the host that has the address <span class="literal">169.254.1.1</span>, we won’t be successful. Rather than actually assign that address to an interface, Calico just configures “proxy ARP” so that the packet will be sent through the <span class="literal">eth0</span> end of the veth pair. As a result, there is an entry for <span class="literal">169.254.1.1</span> in the ARP table inside the container:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl exec -ti pod1 -- arp -n</span>&#13;
? (169.254.1.1) at ee:ee:ee:ee:ee:ee [ether]  on eth0&#13;
...</pre>&#13;
<p class="indent">As shown in <a href="ch08.xhtml#ch08list3">Listing 8-3</a>, the hardware address <span class="literal">ee:ee:ee:ee:ee:ee</span> belongs to the host side of the veth pair, so this is sufficient to get the packet out of the container and into the root network namespace. From there, IP routing takes over.</p>&#13;
<p class="indent">Calico has already configured the routing table to send packets to other cluster nodes based on the destination IP address range for that node and to send packets to local containers based on their individual IP addresses. We can see the result of this in the IP routing table on the host:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">ip route</span>&#13;
...&#13;
172.31.25.192/26 via 192.168.61.13 dev enp0s8 proto 80 onlink &#13;
172.31.89.192/26 via 192.168.61.12 dev enp0s8 proto 80 onlink &#13;
172.31.239.216 dev calice0906292e2 scope link &#13;
...</pre>&#13;
<p class="indent">Because the destination address for the ping is within the <span class="literal">172.31.89.192/26</span> network, the packet now is routed to <span class="literal">192.168.61.12</span>, which is <span class="literal">host02</span>.</p>&#13;
<p class="indent">Let’s look at the routing table on <span class="literal">host02</span> so that we can follow along with the next step:</p>&#13;
<pre>root@host02:~# <span class="codestrong1">ip route</span>&#13;
...&#13;
172.31.239.192/26 via 192.168.61.11 dev enp0s8 proto 80 onlink &#13;
172.31.25.192/26 via 192.168.61.13 dev enp0s8 proto 80 onlink &#13;
172.31.89.197 dev calibd2348b4f67 scope link &#13;
...</pre>&#13;
<p class="indent">If you want to run this command for yourself, make sure you run it from <span class="literal">host02</span>. When our packet arrives at <span class="literal">host02</span>, it has a route for the specific IP address that is the destination of the <span class="literal">ping</span>. This route sends the packet into the veth pair that is attached to the <span class="literal">pod2</span> network namespace.</p>&#13;
<p class="indent">Now that the ping has arrived, the network stack inside <span class="literal">pod2</span> sends back a reply. The reply goes through the same process to reach the root network namespace of <span class="literal">host02</span>. Based on the <span class="literal">host02</span> routing table, it is sent to <span class="literal">host01</span>, <span epub:type="pagebreak" id="page_138"/>where a routing table entry for <span class="literal">172.31.239.216</span> is used to send it to the appropriate container.</p>&#13;
<p class="indent">Because Calico is using Layer 3 routing, the host network sees the actual container IP addresses. We can confirm that using <span class="literal">tcpdump</span>. We’ll switch back to <span class="literal">host01</span> for this.</p>&#13;
<p class="indent">First, let’s kick off <span class="literal">tcpdump</span> in the background:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">tcpdump -n -w pings.pcap -i any icmp &amp;</span>&#13;
[1] 70949&#13;
tcpdump: listening on any ...</pre>&#13;
<p class="indent">The <span class="literal">-n</span> flag tells <span class="literal">tcpdump</span> to avoid trying to lookup hostnames in DNS for any IP addresses; this saves time. The <span class="literal">-w pings.pcap</span> flag tells <span class="literal">tcpdump</span> to write its data to the file <em>pings.pcap</em>; the <span class="literal">-i any</span> flag tells it to listen on all network interfaces; the <span class="literal">icmp</span> filter tells it to listen only to ICMP traffic; and finally, <span class="literal">&amp;</span> at the end puts it in the background.</p>&#13;
<p class="indent">The <em>pcap</em> filename extension is important because our Ubuntu host system will only allow <span class="literal">tcpdump</span> to read files with that extension.</p>&#13;
<p class="indent">Now, let’s run <span class="literal">ping</span> again:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl exec -ti pod1 -- ping -c 3 $IP2</span>&#13;
...&#13;
3 packets transmitted, 3 packets received, 0% packet loss&#13;
round-trip min/avg/max = 0.928/0.991/1.115 ms</pre>&#13;
<p class="indent">The ICMP requests and replies have been collected, but they are being buffered in memory.</p>&#13;
<p class="indent">To get them dumped to the file, we’ll shut down <span class="literal">tcpdump</span>:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">killall tcpdump</span>&#13;
12 packets captured&#13;
12 packets received by filter&#13;
0 packets dropped by kernel</pre>&#13;
<p class="indent">There were three pings, and each ping consists of a request and a reply. Thus, we might have expected six packets, but in fact we captured 12. To see why, let’s print the details of the packets that <span class="literal">tcpdump</span> collected:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">tcpdump -enr pings.pcap</span>&#13;
reading from file pings.pcap, link-type LINUX_SLL (Linux cooked v1)&#13;
00:16:23...  In f2:ed:e8:04:00:cc <span class="ent">➊</span> ... 172.31.239.216 &gt; 172.31.89.197: ICMP echo request ...&#13;
00:16:23... Out 08:00:27:b7:ef:ef <span class="ent">➋</span> ... 172.31.239.216 &gt; 172.31.89.197: ICMP echo request ...&#13;
00:16:23...  In 08:00:27:fc:d2:36 <span class="ent">➌</span> ... 172.31.89.197 &gt; 172.31.239.216: ICMP echo reply ...&#13;
00:16:23... Out ee:ee:ee:ee:ee:ee <span class="ent">➍</span> ... 172.31.89.197 &gt; 172.31.239.216: ICMP echo reply ...&#13;
...</pre>&#13;
<p class="indent">The <span class="literal">-e</span> flag to <span class="literal">tcpdump</span> prints the hardware addresses; otherwise, we wouldn’t be able to tell some of the packets apart. The first hardware address <span class="ent">➊</span> is the hardware address of <span class="literal">eth0</span> inside the Pod. Next is the same packet again, but this time the hardware address is the host interface <span class="ent">➋</span>. We <span epub:type="pagebreak" id="page_139"/>then see the reply, first arriving at the host interface and labeled with the hardware address for <span class="literal">host02</span> <span class="ent">➌</span>. Finally, the packet is routed into the Calico network interface corresponding to our Pod <span class="ent">➍</span>, and our <span class="literal">ping</span> has made its round trip.</p>&#13;
<p class="indent">We’re now done with these two Pods, so let’s delete them:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl delete -f /opt/two-pods.yaml</span>&#13;
pod "pod1" deleted&#13;
pod "pod2" deleted</pre>&#13;
<p class="indent">Using Layer 3 routing is an elegant solution to cross-node networking for a Kubernetes cluster, as it takes advantage of the routing and traffic forwarding capabilities that are native to Linux. However, it does mean that the host network sees the Pods’ IP addresses, which may require security rule changes. For example, the automated scripts that set up virtual machines in Amazon Web Services (AWS) for use with this book not only configure a security group to allow all traffic in the Pod IP address space, but they also turn off the “source/destination check” for the virtual machine instances. Otherwise, the underlying AWS network infrastructure would refuse to pass traffic with unexpected IP addresses to our cluster’s nodes.</p>&#13;
<h4 class="h4" id="ch00lev2sec60">WeaveNet</h4>&#13;
<p class="noindent">Layer 3 routing is not the only solution for cross-node networking. Another option is to “encapsulate” the container packets into a packet that is sent explicitly host to host. This is the approach taken by popular network plug-ins such as Flannel and WeaveNet. We’ll look at a WeaveNet example, but the traffic using Flannel looks very similar.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>Larger clusters based on Calico also use encapsulation for some traffic between networks. For example, a cluster that spans multiple regions, or Availability Zones, in AWS would likely need to configure Calico to use encapsulation, given that it may not be possible or practical to configure all of the routers between the regions or Availability Zones with the necessary Pod IP routes for the cluster.</em></p>&#13;
</div>&#13;
<p class="indent">Because everything you might want to do in networking has some defined standard, it’s not surprising that there is a standard for encapsulation: Virtual Extensible LAN (VXLAN). In VXLAN, each packet is wrapped in a UDP datagram and sent to the destination.</p>&#13;
<p class="indent">We’ll use the same <em>two-pods.yaml</em> configuration file to create two Pods in our Kubernetes cluster, this time using a cluster built from the <em>weavenet</em> directory from this chapter’s examples. As before, we end up with one Pod on <span class="literal">host01</span> and the other on <span class="literal">host02</span>:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/two-pods.yaml</span>&#13;
pod/pod1 created&#13;
pod/pod2 created</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_140"/>Let’s check that these Pods are running and allocated correctly to their different hosts:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get po -o wide</span>&#13;
NAME   READY   STATUS    ... IP           NODE     ...&#13;
pod1   1/1     Running   ... 10.46.0.8    host01   ...&#13;
pod2   1/1     Running   ... 10.40.0.21   host02   ...</pre>&#13;
<p class="indent">After these Pods are running, we can collect their IP addresses using the same commands shown earlier:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">IP1=$(kubectl get po pod1 -o json | jq -r '.status.podIP')</span>&#13;
root@host01:~# <span class="codestrong1">IP2=$(kubectl get po pod2 -o json | jq -r '.status.podIP')</span>&#13;
root@host01:~# <span class="codestrong1">echo $IP1</span>&#13;
10.46.0.8&#13;
root@host01:~# <span class="codestrong1">echo $IP2</span>&#13;
10.40.0.21</pre>&#13;
<p class="indent">Note that the IP addresses assigned look nothing like the Calico example. Further exploration shows that the address and routing configuration is also different, as demonstrated in <a href="ch08.xhtml#ch08list4">Listing 8-4</a>.</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl exec -ti pod1 -- ip addr</span>&#13;
...&#13;
25: eth0@if26: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1376 qdisc noqueue &#13;
    link/ether e6:78:69:44:3d:a4 brd ff:ff:ff:ff:ff:ff&#13;
    inet 10.46.0.8/12 brd 10.47.255.255 scope global eth0&#13;
       valid_lft forever preferred_lft forever&#13;
...&#13;
root@host01:~# <span class="codestrong1">kubectl exec -ti pod1 -- ip route</span>&#13;
default via 10.46.0.0 dev eth0 &#13;
10.32.0.0/12 dev eth0 scope link  src 10.46.0.8</pre>&#13;
<p class="caption" id="ch08list4"><em>Listing 8-4: WeaveNet networking</em></p>&#13;
<p class="indent">This time, our Pods are getting IP addresses in a massive <span class="literal">/12</span> network, corresponding to more than one million possible addresses on a single network. In this case, our Pod’s networking stack is going to expect to be able to use ARP to directly identify the hardware address of any other Pod on the network rather than routing traffic to a gateway as we saw with Calico.</p>&#13;
<p class="indent">As before, we do have connectivity between these two Pods:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl exec -ti pod1 -- ping -c 3 $IP2</span>&#13;
PING 10.40.0.21 (10.40.0.21): 56 data bytes&#13;
64 bytes from 10.40.0.21: seq=0 ttl=64 time=0.981 ms&#13;
64 bytes from 10.40.0.21: seq=1 ttl=64 time=0.963 ms&#13;
64 bytes from 10.40.0.21: seq=2 ttl=64 time=0.871 ms&#13;
<span epub:type="pagebreak" id="page_141"/>--- 10.40.0.21 ping statistics ---&#13;
3 packets transmitted, 3 packets received, 0% packet loss&#13;
round-trip min/avg/max = 0.871/0.938/0.981 ms</pre>&#13;
<p class="indent">And now that we’ve run this <span class="literal">ping</span> command, we should expect that the ARP table in the <span class="literal">pod1</span> networking stack is populated with the hardware address of the <span class="literal">pod2</span> network interface:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl exec -ti pod1 -- arp -n</span>&#13;
? (10.40.0.21) at ba:75:e6:db:7c:c6 [ether]  on eth0&#13;
? (10.46.0.0) at 1a:72:78:64:36:c6 [ether]  on eth0</pre>&#13;
<p class="indent">As expected, <span class="literal">pod1</span> has an ARP table entry for <span class="literal">pod2</span>’s IP address, corresponding to the virtual network interface inside <span class="literal">pod2</span>:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl exec -ti pod2 -- ip addr</span>&#13;
...&#13;
53: eth0@if54: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1376 qdisc noqueue &#13;
    link/ether <span class="ent">➊</span> ba:75:e6:db:7c:c6 brd ff:ff:ff:ff:ff:ff&#13;
    inet 10.40.0.21/12 brd 10.47.255.255 scope global eth0&#13;
       valid_lft forever preferred_lft forever&#13;
...</pre>&#13;
<p class="indent">The hardware address in the <span class="literal">pod1</span> ARP table matches the hardware address of the virtual network device in <span class="literal">pod2</span> <span class="ent">➊</span>. To make this happen, WeaveNet is routing the ARP request over the network so that the network stack in <span class="literal">pod2</span> can respond.</p>&#13;
<p class="indent">Let’s look at how the cross-node routing of ARP and ICMP traffic is happening. First, although the IP address management may be different, one important similarity between Calico and WeaveNet is that both are using veth pairs to connect containers to the host. If you want to explore that, use the commands shown in <a href="ch08.xhtml#ch08list2">Listing 8-2</a> and <a href="ch08.xhtml#ch08list3">Listing 8-3</a> to determine the network namespace for <span class="literal">pod1</span>, and then use <span class="literal">ip addr</span> on <span class="literal">host01</span> to verify that there is a <span class="literal">veth</span> device with a <span class="literal">link-netns</span> field that corresponds to that network namespace.</p>&#13;
<p class="indent">For our purposes, because we’ve seen that before, we’ll take it as a given that the traffic goes through the virtual network wire created by the veth pair and gets to the host. Let’s start there and trace the ICMP traffic between the two Pods.</p>&#13;
<p class="indent">If we use the same <span class="literal">tcpdump</span> capture as we did with Calico, we’ll be able to capture the ICMP traffic, but that will get us only so far. Let’s go ahead and look at that:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">tcpdump -w pings.pcap -i any icmp &amp;</span>&#13;
[1] 55999&#13;
tcpdump: listening on any, link-type LINUX_SLL (Linux cooked v1) ...&#13;
root@host01:~# <span class="codestrong1">kubectl exec -ti pod1 -- ping -c 3 $IP2</span>&#13;
...&#13;
<span epub:type="pagebreak" id="page_142"/>3 packets transmitted, 3 packets received, 0% packet loss&#13;
round-trip min/avg/max = 0.824/1.691/3.053 ms&#13;
root@host01:~# <span class="codestrong1">killall tcpdump</span>&#13;
24 packets captured&#13;
24 packets received by filter&#13;
0 packets dropped by kernel</pre>&#13;
<p class="indent">As before, we ran <span class="literal">tcpdump</span> in the background to capture ICMP on all network interfaces, ran our <span class="literal">ping</span>, and then stopped <span class="literal">tcpdump</span> so that it would write out the packets it captured. This time we have 24 packets to look at, but they still don’t tell the whole story:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">tcpdump -enr pings.pcap</span>&#13;
reading from file pings.pcap, link-type LINUX_SLL (Linux cooked v1)&#13;
16:22:08.211499   P e6:78:69:44:3d:a4 ... 10.46.0.8 &gt; 10.40.0.21: ICMP echo request ...&#13;
16:22:08.211551 Out e6:78:69:44:3d:a4 ... 10.46.0.8 &gt; 10.40.0.21: ICMP echo request ...&#13;
16:22:08.211553   P e6:78:69:44:3d:a4 ... 10.46.0.8 &gt; 10.40.0.21: ICMP echo request ...&#13;
16:22:08.211745 Out e6:78:69:44:3d:a4 ... 10.46.0.8 &gt; 10.40.0.21: ICMP echo request ...&#13;
16:22:08.212917   P ba:75:e6:db:7c:c6 ... 10.40.0.21 &gt; 10.46.0.8: ICMP echo reply ...&#13;
16:22:08.213704 Out ba:75:e6:db:7c:c6 ... 10.40.0.21 &gt; 10.46.0.8: ICMP echo reply ...&#13;
16:22:08.213708   P ba:75:e6:db:7c:c6 ... 10.40.0.21 &gt; 10.46.0.8: ICMP echo reply ...&#13;
16:22:08.213724 Out ba:75:e6:db:7c:c6 ... 10.40.0.21 &gt; 10.46.0.8: ICMP echo reply ...&#13;
...</pre>&#13;
<p class="indent">These lines show four packets for a single <span class="literal">ping</span> request and reply, but the hardware addresses aren’t changing. What’s happening is that these ICMP packets are being handed between network interfaces unmodified. However, we’re still not seeing the actual traffic that’s going between <span class="literal">host01</span> and <span class="literal">host02</span>, because we never see any hardware addresses that correspond to host interfaces.</p>&#13;
<p class="indent">To see the host-level traffic, we need to tell <span class="literal">tcpdump</span> to capture UDP and then treat it as VXLAN, which enables <span class="literal">tcpdump</span> to identify the fact that an ICMP packet is inside.</p>&#13;
<p class="indent">Let’s start the capture again, this time looking for UDP traffic:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">tcpdump -w vxlan.pcap -i any udp &amp;</span>&#13;
...&#13;
root@host01:~# <span class="codestrong1">kubectl exec -ti pod1 -- ping -c 3 $IP2</span>&#13;
...&#13;
3 packets transmitted, 3 packets received, 0% packet loss&#13;
round-trip min/avg/max = 1.139/1.364/1.545 ms&#13;
root@host01:~# <span class="codestrong1">killall tcpdump</span>&#13;
22 packets captured&#13;
24 packets received by filter&#13;
0 packets dropped by kernel</pre>&#13;
<p class="indent">This time we saved the packet data in <em>vxlan.pcap</em>. In this example, <span class="literal">tcpdump</span> captured 22 packets. Because there is lots of cross-Pod traffic in our cluster, not just ICMP traffic, you might see a different number.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_143"/>The packets we captured cover all of the UDP traffic on <span class="literal">host01</span>, not just our ICMP, so in printing out the packets shown in <a href="ch08.xhtml#ch08list5">Listing 8-5</a>, we’ll need to be selective.</p>&#13;
<pre>root@host01:~# <span class="codestrong1">tcpdump -enr vxlan.pcap -T vxlan | grep -B 1 ICMP</span>&#13;
reading from file vxlan.pcap, link-type LINUX_SLL (Linux cooked v1)&#13;
16:45:47.307949 Out 08:00:27:32:a0:28 ... &#13;
  length 150: 192.168.61.11.50200 &gt; 192.168.61.12.6784: VXLAN ...&#13;
e6:78:69:44:3d:a4 &gt; ba:75:e6:db:7c:c6 ... &#13;
  length 98: 10.46.0.8 &gt; 10.40.0.21: ICMP echo request ...&#13;
16:45:47.308699  In 08:00:27:67:b9:da ... &#13;
  length 150: 192.168.61.12.43489 &gt; 192.168.61.11.6784: VXLAN ... &#13;
ba:75:e6:db:7c:c6 &gt; e6:78:69:44:3d:a4 ... &#13;
  length 98: 10.40.0.21 &gt; 10.46.0.8: ICMP echo reply ...&#13;
16:45:48.308240 Out 08:00:27:32:a0:28 ... &#13;
  length 150: 192.168.61.11.50200 &gt; 192.168.61.12.6784: VXLAN ... &#13;
...</pre>&#13;
<p class="caption" id="ch08list5"><em>Listing 8-5: VXLAN capture</em></p>&#13;
<p class="indent">The <span class="literal">-T vxlan</span> flag tells <span class="literal">tcpdump</span> to treat the packet data it sees as VXLAN data. This causes <span class="literal">tcpdump</span> to look inside and pull out data from the encapsulated packets, enabling it to identify ICMP packets when those are hidden inside. We then use <span class="literal">grep</span> with a <span class="literal">-B 1</span> flag to find those ICMP packets and also print the line immediately previous so that we can see the VXLAN wrapper.</p>&#13;
<p class="indent">This capture shows the host’s hardware address, which informs us that we’ve managed to capture the traffic moving between hosts. Each ICMP packet is wrapped in a UDP datagram and sent across the host network. The IP source and destination for these datagrams are the host network IP addresses <span class="literal">192.168.61.11</span> and <span class="literal">192.168.61.12</span>, so the host network never sees the Pod IP addresses. However, that information is still there, in the encapsulated ICMP packet, thus when the datagram arrives at its destination, WeaveNet can send the ICMP packet to the correct destination.</p>&#13;
<p class="indent">The advantage of encapsulation is that all of our cross-node traffic looks like ordinary UDP datagrams between hosts. Typically, we don’t need to do any additional network configuration to allow this traffic. However, we do pay a price. As you can see in <a href="ch08.xhtml#ch08list5">Listing 8-5</a>, each ICMP packet is 98 bytes, but the encapsulated packet is 150 bytes. The wrapper needed for encapsulation creates network overhead that we have to pay with each packet we send.</p>&#13;
<p class="indent">Look back at <a href="ch08.xhtml#ch08list4">Listing 8-4</a> for another consequence. The virtual network interface inside the Pod has a maximum transmission unit (MTU) of 1,376. This represents the largest packet that can be sent; anything bigger must to be fragmented into multiple packets and reassembled at the destination. This MTU of 1,376 is considerably smaller than the standard MTU of 1,500 on our host network. The smaller MTU on the Pod interface ensures that the Pod’s network stack will do any required fragmenting. This way, we can guarantee that we don’t exceed 1,500 at the host layer, even after the wrapper is added. For this reason, if you are using a network plug-in that uses <span epub:type="pagebreak" id="page_144"/>encapsulation, it might be worth exploring how to configure jumbo frames to enable an MTU larger than 1,500 on the host network.</p>&#13;
<h4 class="h4" id="ch00lev2sec61">Choosing a Network Plug-in</h4>&#13;
<p class="noindent">Network plug-ins can use different approaches to cross-node networking. As is universal in engineering, though, there are trade-offs with each approach. Layer 3 routing uses native capabilities of Linux and is efficient in its use of the network bandwidth, but it may require customization of the underlying host network. Encapsulation with VXLAN works in any network where we can send UDP datagrams between hosts, but it adds overhead with each packet.</p>&#13;
<p class="indent">Either way, however, our Pods are getting what they need, which is the ability to communicate with other Pods, wherever in the cluster they may be. And in practice, the configuration effort and performance difference tends to be small. For this reason, the best way to choose a network plug-in is to start with the plug-in that is recommended for or installed by default with your particular Kubernetes distribution. If you find specific use cases for which the performance doesn’t meet your requirements, you’ll then be able to test an alternative plug-in based on real network traffic rather than guesswork.</p>&#13;
<h3 class="h3" id="ch00lev1sec38">Network Customization</h3>&#13;
<p class="noindent">Some scenarios may require cluster networking that is more complex than a single Pod network connected across all cluster nodes. For example, some regulated industries require certain data, such as security audit logs, to travel across a separated network. Other systems may have specialized hardware so that application components that interface with that hardware must be placed on a specific network or virtual LAN (VLAN).</p>&#13;
<p class="indent">One of the advantages of a plug-in architecture for networking is that a Kubernetes cluster can accommodate these specialized networking scenarios. As long as Pods have an interface that can reach (and is reachable from) the rest of the cluster, Pods can have additional network interfaces that provide specialized connectivity.</p>&#13;
<p class="indent">Let’s look at an example. We’ll configure two Pods on the same node so they have a local host-only network they can use for intercommunication. Being a host-only network, it doesn’t provide connectivity to the rest of the cluster, so we’ll also use Calico to provide cluster networking for Pods.</p>&#13;
<p class="indent">Because of the need to configure both Calico and our host-only network, we’ll be invoking two separate CNI plug-ins that will create virtual network interfaces in our Pods’ network namespaces. As we saw in <a href="ch08.xhtml#ch08list1">Listing 8-1</a>, it’s possible to configure multiple CNI plug-ins in a single configuration file. However, <span class="literal">kubelet</span> expects only one of these CNI plug-ins to actually assign a network interface and IP address. To work around this, we’ll use Multus, a CNI plug-in that is designed to invoke multiple plug-ins but will treat one as primary for purposes of reporting IP address information back to <span class="literal">kubelet</span>. <span epub:type="pagebreak" id="page_145"/>Multus also allows us to be selective as to what CNI plug-ins are applied to each Pod.</p>&#13;
<p class="indent">We’ll begin by installing Multus into the <span class="literal">calico</span> example cluster for this chapter:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/multus-daemonset.yaml</span>&#13;
customresourcedefinition.../network-attachment-definitions... created&#13;
clusterrole.rbac.authorization.k8s.io/multus created&#13;
clusterrolebinding.rbac.authorization.k8s.io/multus created&#13;
serviceaccount/multus created&#13;
configmap/multus-cni-config created&#13;
daemonset.apps/kube-multus-ds created</pre>&#13;
<p class="indent">As the filename implies, the primary resource in this YAML file is a DaemonSet that runs a Multus container on every host. However, this file installs several other resources, including a <em>CustomResourceDefinition</em>. This CustomResourceDefinition will allow us to configure network attachment resources to tell Multus what CNI plug-ins to use for a given Pod.</p>&#13;
<p class="indent">We’ll look at CustomResourceDefinitions in detail in <a href="ch17.xhtml#ch17">Chapter 17</a>. For now, in <a href="ch08.xhtml#ch08list6">Listing 8-6</a> we’ll just see the NetworkAttachmentDefinition that we’ll use to configure Multus.</p>&#13;
<p class="noindent6"><em>netattach.yaml</em></p>&#13;
<pre>---&#13;
apiVersion: k8s.cni.cncf.io/v1&#13;
kind: NetworkAttachmentDefinition&#13;
metadata:&#13;
  name: macvlan-conf&#13;
spec:&#13;
  config: '{&#13;
      "cniVersion": "0.3.0",&#13;
      "type": "macvlan",&#13;
      "mode": "bridge",&#13;
      "ipam": {&#13;
        "type": "host-local",&#13;
        "subnet": "10.244.0.0/24",&#13;
        "rangeStart": "10.244.0.1",&#13;
        "rangeEnd": "10.244.0.254"&#13;
      }&#13;
    }'</pre>&#13;
<p class="caption" id="ch08list6"><em>Listing 8-6: Network attachment</em></p>&#13;
<p class="indent">The <span class="literal">config</span> field in the <span class="literal">spec</span> looks a lot like a CNI configuration file, which isn’t surprising, as Multus needs to use this information to invoke the <span class="literal">macvlan</span> CNI plug-in when we ask for it to be added to a Pod.</p>&#13;
<p class="indent">We need to add this NetworkAttachmentDefinition to the cluster:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/netattach.yaml</span> &#13;
networkattachmentdefinition.k8s.cni.cncf.io/macvlan-conf created</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_146"/>This definition doesn’t immediately affect any of our Pods; it just provides a Multus configuration for future use.</p>&#13;
<p class="indent">Of course, to use this configuration, Multus must be invoked. How does that happen when we’ve already installed Calico into this cluster? The answer is in the <em>/etc/cni/net.d</em> directory, which the Multus DaemonSet modified on all of our cluster nodes as part of its initialization:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">ls /etc/cni/net.d</span>&#13;
00-multus.conf  10-calico.conflist  calico-kubeconfig  multus.d</pre>&#13;
<p class="indent">Multus left the existing Calico configuration files in place, but added its own <em>00-multus.conf</em> configuration file and a <em>multus.d</em> directory. Because the <em>00-multus.conf</em> file is ahead of <em>10-calico.conflist</em> in an alphabetic sort, <span class="literal">kubelet</span> will start to use it the next time it creates a new Pod.</p>&#13;
<p class="indent">Here’s <em>00-multus.conf</em>:</p>&#13;
<p class="noindent6"><em>00-multus.conf</em></p>&#13;
<pre>{&#13;
  "cniVersion": "0.3.1",&#13;
  "name": "multus-cni-network",&#13;
  "type": "multus",&#13;
  "capabilities": {&#13;
    "portMappings": true,&#13;
    "bandwidth": true&#13;
  },&#13;
  "kubeconfig": "/etc/cni/net.d/multus.d/multus.kubeconfig",&#13;
  "delegates": [&#13;
    {&#13;
      "name": "k8s-pod-network",&#13;
      "cniVersion": "0.3.1",&#13;
      "plugins": [&#13;
        {&#13;
          "type": "calico",&#13;
...&#13;
          }&#13;
        },&#13;
        {&#13;
          "type": "bandwidth",&#13;
...&#13;
        },&#13;
        {&#13;
          "type": "portmap",&#13;
...&#13;
        }&#13;
      ]&#13;
    }&#13;
  ]&#13;
}</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_147"/>The <span class="literal">delegates</span> field is pulled from the Calico configuration that Multus found. This field is used to determine the default CNI plug-ins that Multus always uses when it is invoked. The top-level <span class="literal">capabilities</span> field is needed to ensure that Multus will get all the correct configuration data from <span class="literal">kubelet</span> to be able to invoke the <span class="literal">portmap</span> and <span class="literal">bandwidth</span> plug-ins.</p>&#13;
<p class="indent">Now that Multus is fully set up, let’s use it to add a host-only network to two Pods. The Pods are defined as follows:</p>&#13;
<p class="noindent6"><em>local-pods.yaml</em></p>&#13;
<pre>---&#13;
apiVersion: v1&#13;
kind: Pod&#13;
metadata:&#13;
  name: pod1&#13;
  annotations:&#13;
    k8s.v1.cni.cncf.io/networks: macvlan-conf&#13;
spec:&#13;
  containers:&#13;
  - name: pod1&#13;
    image: busybox&#13;
    command: &#13;
      - "sleep"&#13;
      - "infinity"&#13;
  nodeName: host01&#13;
---&#13;
apiVersion: v1&#13;
kind: Pod&#13;
metadata:&#13;
  name: pod2&#13;
  annotations:&#13;
    k8s.v1.cni.cncf.io/networks: macvlan-conf&#13;
spec:&#13;
  containers:&#13;
  - name: pod2&#13;
    image: busybox&#13;
    command: &#13;
      - "sleep"&#13;
      - "infinity"&#13;
  nodeName: host01</pre>&#13;
<p class="indent">This time we need both Pods to wind up on <span class="literal">host01</span> so that the host-only networking functions. In addition, we add the <span class="literal">k8s.v1.cni.cncf.io/networks</span> annotation to each Pod. Multus uses this annotation to identify what additional CNI plug-ins it should run. The name <span class="literal">macvlan-conf</span> matches the name we provided in the NetworkAttachmentDefinition in <a href="ch08.xhtml#ch08list6">Listing 8-6</a>.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_148"/>Let’s create these two Pods:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/local-pods.yaml</span>&#13;
pod/pod1 created&#13;
pod/pod2 created</pre>&#13;
<p class="indent">After these Pods are running, we can check that they each have an extra network interface:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl exec -ti pod1 -- ip addr</span>&#13;
...&#13;
3: eth0@if12: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1450 qdisc noqueue &#13;
    link/ether 9a:a1:db:ec:c7:91 brd ff:ff:ff:ff:ff:ff&#13;
    inet 172.31.239.198/32 brd 172.31.239.198 scope global eth0&#13;
       valid_lft forever preferred_lft forever&#13;
...&#13;
4: net1@if2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue &#13;
    link/ether 9e:4f:c4:47:40:07 brd ff:ff:ff:ff:ff:ff&#13;
    inet 10.244.0.2/24 brd 10.244.0.255 scope global net1&#13;
       valid_lft forever preferred_lft forever&#13;
...&#13;
root@host01:~# <span class="codestrong1">kubectl exec -ti pod2 -- ip addr</span>&#13;
...&#13;
3: eth0@if13: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1450 qdisc noqueue &#13;
    link/ether 52:08:99:a7:d2:bc brd ff:ff:ff:ff:ff:ff&#13;
    inet 172.31.239.199/32 brd 172.31.239.199 scope global eth0&#13;
       valid_lft forever preferred_lft forever&#13;
...&#13;
4: net1@if2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue &#13;
    link/ether a6:e5:01:82:81:82 brd ff:ff:ff:ff:ff:ff&#13;
    inet 10.244.0.3/24 brd 10.244.0.255 scope global net1&#13;
       valid_lft forever preferred_lft forever&#13;
...</pre>&#13;
<p class="indent">The <span class="literal">macvlan</span> CNI plug-in has added the additional <span class="literal">net1</span> network interface, using the IP address management configuration we provided in the NetworkAttachmentDefinition.</p>&#13;
<p class="indent">These two Pods are now able to communicate with each other using these interfaces:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl exec -ti pod1 -- ping -c 3 10.244.0.3</span>&#13;
PING 10.244.0.3 (10.244.0.3): 56 data bytes&#13;
64 bytes from 10.244.0.3: seq=0 ttl=64 time=3.125 ms&#13;
64 bytes from 10.244.0.3: seq=1 ttl=64 time=0.192 ms&#13;
64 bytes from 10.244.0.3: seq=2 ttl=64 time=0.085 ms&#13;
&#13;
--- 10.244.0.3 ping statistics ---&#13;
3 packets transmitted, 3 packets received, 0% packet loss&#13;
round-trip min/avg/max = 0.085/1.134/3.125 ms</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_149"/>This communication goes over the bridge created by the <span class="literal">macvlan</span> CNI plug-in, as opposed to travelling via Calico.</p>&#13;
<p class="indent">Keep in mind that our purpose here is solely to demonstrate custom networking without requiring any particular VLAN or complex setup outside our cluster hosts. For a real cluster, this kind of host-only network is of limited value because it constrains where Pods can be deployed. In this kind of situation, it might be preferable to place the two containers into the same Pod so that they will always be scheduled together and can use <span class="literal">localhost</span> to communicate.</p>&#13;
<h3 class="h3" id="ch00lev1sec39">Final Thoughts</h3>&#13;
<p class="noindent">We’ve looked at a lot of network interfaces and traffic flows in this chapter. Most of the time, it’s enough to know that every Pod in the cluster is allocated an IP address from a Pod network, and also that any Pod in the cluster can reach and is reachable from any other Pod. Any of the Kubernetes network plug-ins provide this capability, whether they use Layer 3 routing or VXLAN encapsulation, or possibly both.</p>&#13;
<p class="indent">At the same time, networking issues do occur in a cluster, and it’s essential for cluster administrators and cluster users to understand how the traffic is flowing between hosts and what that traffic looks like to the host network in order to debug issues with switch and host configuration, or simply to build applications that make best use of the cluster.</p>&#13;
<p class="indent">We’re not yet done with the networking layers that are needed to have a fully functioning Kubernetes cluster. In the next chapter, we’ll look at how Kubernetes provides a Service layer on top of Pod networking to provide load balancing and automated failover, and then uses the Service networking layer together with Ingress networking to make container services accessible outside the cluster.<span epub:type="pagebreak" id="page_150"/></p>&#13;
</body></html>