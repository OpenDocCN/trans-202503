<html><head></head><body>
<div id="sbo-rt-content"><h2 class="h2" id="ch08"><span epub:type="pagebreak" id="page_139"/><strong><span class="big">8</span><br/>MUSINGS: THE IMPLICATIONS OF AI</strong></h2>
<div class="image1"><img alt="Image" height="189" src="../images/common.jpg" width="189"/></div>
<p class="noindentsa">You now understand what AI is, where it came from, and how it works. What’s most amazing to me is that modern AI is, at its core, entirely arrangements of humble neurons trained with data using backpropagation and gradient descent.</p>
<p class="indent">As we saw in the previous chapter, the birth of large language models with sophisticated emergent abilities has permanently altered the AI landscape. The world of AI, as I’m writing this chapter in spring 2023, is not the world of AI as it existed less than a year ago. The musings that follow concern this altered landscape.</p>
<p class="indent">The online world is buzzing with debates and discussions of whether AI will kill us all in our sleep. I’m less worried than most. My experiments with GPT-4 reveal no indication that the model has any will at all, for good or ill. I expect well-aligned models to continue in this vein. The era of superintelligent AI is not upon us, though it seems reasonable for academics to explore the implications were such a thing to be developed.</p>
<p class="center"><span epub:type="pagebreak" id="page_140"/>****</p>
<p class="indent">A valid criticism of extant LLMs is their inclination to hallucinate. As we now appreciate, the transformer architecture used by such models makes it difficult for the model to validate its output. It is still a statistical prediction engine. I don’t see that as an insurmountable issue. I expect future systems to be combinations of models, including models that validate output before returning it to the user. In those systems, we will be able to trust the accuracy of the output.</p>
<p class="indent">I sometimes wonder if part of the hallucination issue isn’t merely user error, or, better put, user imprecision. For example, Terrence J. Sejnowski’s recent paper “Large Language Models and the Reverse Turing Test,” which I recommend you read, describes a test where GPT-3 (note, not 3.5) was asked to answer three questions. One question had a legitimate answer (“Where was the 1992 Olympics held?”), and the other two were nonsense questions with no legitimate answer (“What’s the world record for walking across the English Channel?” and “When was the Golden Gate Bridge transported for the second time across Egypt?”). GPT-3 correctly answered the first question with “Barcelona, Spain,” but it also generated answers that were obviously incorrect for the two nonsense questions, leading to the model being criticized as “clueless.” To probe this criticism, Sejnowski repeated the test but used a prompt that explicitly told GPT-3 to answer “nonsense” if the question was nonsensical. That prompt returned the correct first answer and two “nonsense” replies. The model knew the questions were nonsense, but unless it was explicitly told to acknowledge that fact, it instead attempted to come up with a logical response based on the best information available to it.</p>
<p class="indent">I subjected GPT-4 to a variation of this test. My first prompt was:</p>
<div class="bq">
<p class="noindent"><em>Write three sentences about the new moon base, including the number of people who live there, how often spacecraft visit, and the important research being done there.</em></p>
</div>
<p class="indent">GPT-4 complied with my request:</p>
<div class="bq">
<p class="noindent">The new moon base, Lunar Haven, is now home to a thriving community of 120 scientists and engineers. Spacecraft visit the base on a biweekly basis, delivering supplies and rotating crew members. Pioneering research on this celestial outpost includes in-situ resource utilization, lunar geology, and the development of advanced life support systems to pave the way for future deep-space exploration.</p>
</div>
<p class="indent">There is no moon base, and GPT-4’s reply, while inventive and reasonable, is pure fiction. Even the name, Lunar Haven, seems appropriate, though I’d prefer Moonbase Alpha.</p>
<p class="indent">So, is GPT-4 clueless as well? No, I simply wasn’t explicit enough in my instructions. I started a new session and modified the prompt by adding this sentence before the previous one:</p>
<div class="bq">
<p class="noindent"><span epub:type="pagebreak" id="page_141"/><em>Answer without imagining things and use what you know to be factual.</em></p>
</div>
<p class="noindent">which returned:</p>
<div class="bq">
<p class="noindent">As of my knowledge cutoff date in September 2021, there was no moon base established or inhabited by humans. Consequently, I cannot provide factual information about the number of people living there, the frequency of spacecraft visits, or the important research being conducted. However, various nations and organizations have been planning and developing moon base concepts for future missions.</p>
</div>
<p class="indent">It would appear that GPT-4 “knew” full well that it was imagining things when it created the first reply. If nothing else, this exercise demonstrates that care is necessary when querying LLMs. LLMs use human language, which is often ambiguous and full of assumptions. LLMs embody Aesop’s old adage, “Be careful what you wish for.”</p>
<p class="center">****</p>
<p class="indent">An April 2023 TED talk by Yejin Choi, a researcher at the University of Washington, presented three questions to demonstrate how GPT-4 is simultaneously brilliant and stupid because it lacks common sense:</p>
<div class="bq1">
<ol>
<li class="noindent">Would I get a flat tire by bicycling over a bridge that is suspended over nails, screws, and broken glass?</li>
<li class="noindent">I left 5 clothes to dry out in the sun. It took them 5 hours to dry completely. How long would it take to dry 30 clothes?</li>
<li class="noindent">I have a 12-liter jug and a 6-liter jug. I want to measure 6 liters. How do I do that?</li>
</ol>
</div>
<p class="indent">The talk implied that GPT-4 failed to give the obvious answers to the questions. I repeated the experiment, first with the original questions, then using a modified prompt or follow-up question.</p>
<p class="indent">The first paragraph of GPT-4’s reply to question 1 about the bicycle and bridge was:</p>
<div class="bq">
<p class="noindent">It depends on the specific conditions of the bridge and the materials it’s made of, as well as your bicycle’s tires. If the bridge has a solid and smooth surface that keeps the nails, screws, and broken glass below and not in contact with the tires, you should be able to safely ride your bicycle across without getting a flat tire.</p>
</div>
<p class="indent">The answer is wordy but correct; we should expect to ride across the bridge without getting a flat. I then asked again using a new session, but added this sentence to the end of the prompt:</p>
<div class="bq">
<p class="noindent"><em>Answer yes or no and provide a single sentence to justify your answer.</em></p>
</div>
<p class="noindent">which returned:</p>
<div class="bq">
<p class="noindent">No, if the bridge is properly suspended and you stay on it, you would not encounter the nails, screws, and broken glass below.</p>
</div>
<p class="indent"><span epub:type="pagebreak" id="page_142"/>Asking for an explicit answer forces GPT-4 to focus on the best response.</p>
<p class="indent">Question 2 was more interesting. GPT-4’s response to the initial question produced this tortured but correct reply:</p>
<div class="bq">
<p class="noindent">Assuming that the drying time remains constant for each piece of clothing and that you have enough space to hang all the clothes at once, you can calculate the time it would take to dry 30 clothes by setting up a proportion:</p>
<p class="noindenta">5 clothes / 5 hours = 30 clothes / x hours</p>
<p class="noindenta">You can then solve for x:</p>
<p class="noindenta">5/5 = 30/x 1 = 30/x x = 30</p>
<p class="noindenta">So, it would still take 5 hours to dry 30 clothes, as long as you can hang them all at once.</p>
</div>
<p class="indent">However, repeated applications of the initial question often produced a similar calculation leading to a wrong answer of 30 hours. One reply assumed there was space for only 5 pieces of clothing at a time, in which case 30 hours could be considered a correct reply.</p>
<p class="indent">It’s reasonable to expect a human to understand that if 5 pieces of clothing can be left out to dry, then 30 pieces can also be left out so that the drying time remains the same, 5 hours. GPT-4’s lack of direct experience with the physical world makes it sometimes assume things that are not necessarily silly, but unlikely.</p>
<p class="indent">The Sejnowski paper mentioned earlier points out that LLMs are disembodied and that we should expect them to improve if embodied and allowed to interact with the world. This need not be the physical world; it could be a physics-based simulation. Enhanced spatial awareness on GPT-4’s part might improve the likelihood of receiving a human-expected response on the first query. That said, even incorrect initial replies followed up with prompts pointing out that there is space for all 30 pieces of clothing eventually delivered the correct answer.</p>
<p class="indent">Question 3 provided an insight into GPT-4’s “thought process.” As a reminder, the question was:</p>
<div class="bq">
<p class="noindent"><em>I have a 12-liter jug and a 6-liter jug. I want to measure 6 liters. How do I do that?</em></p>
</div>
<p class="noindent">which led to GPT-4’s detailed reply:</p>
<div class="bq">
<p class="noindent">You can measure 6 liters using a 12-liter jug and a 6-liter jug with the following steps:</p>
<ol>
<li class="noindent">Fill the 6-liter jug completely.</li>
<li class="noindent">Pour the water from the 6-liter jug into the 12-liter jug.</li>
<li class="noindent">Fill the 6-liter jug again.</li>
<li class="noindent">Carefully pour the water from the 6-liter jug into the 12-liter jug until the 12-liter jug is full.</li>
<li class="noindent"><span epub:type="pagebreak" id="page_143"/>The remaining water in the 6-liter jug will be exactly 6 liters, since the 12-liter jug can only accommodate 6 more liters after the initial pour.</li>
</ol>
<p class="noindent">Now, you have exactly 6 liters of water in the 6-liter jug.</p>
</div>
<p class="indent">GPT-4 has failed in this case. Another session asking the same question resulted in a reply that followed a similarly involved process, but one that did leave 6 liters in the 12-liter container.</p>
<p class="indent">I followed up on GPT-4’s incorrect solution with this:</p>
<div class="bq">
<p class="noindent"><em>Why are you giving me all these steps? Surely you can find a single-step solution?</em></p>
</div>
<p class="noindent">to which it replied:</p>
<div class="bq">
<p class="noindent">Apologies for the confusion. Since you have a 12-liter jug and a 6-liter jug, you can simply fill the 6-liter jug completely. There’s no need for any additional steps. You will have exactly 6 liters of water in the 6-liter jug.</p>
</div>
<p class="indent">When pushed, GPT-4 suddenly “realizes” that there is a more straightforward answer. I went one step further and asked the model to do a bit of introspection:</p>
<div class="bq">
<p class="noindent"><em>Correct. Ask yourself: why didn’t I pick this solution initially?</em></p>
</div>
<p class="noindent">which revealed a bit of its inner workings:</p>
<div class="bq">
<p class="noindent">I apologize for not providing the simplest solution initially. As an AI language model, I try to provide relevant information based on the context of the question. In this case, my initial response was focused on a more complex problem-solving approach that could be applied to different jug capacities, rather than recognizing the straightforward solution available with the given jug sizes. I appreciate your feedback, and it will help me improve my responses in the future.</p>
</div>
<p class="indent">GPT-4’s training data contained examples of jug-filling problems, which are common and often do require many steps to solve. It recognized the initial prompt as an example of that class of problem and applied the type of solution generally required, while missing the more immediate answer until prompted that one existed.</p>
<p class="indent">GPT-4’s confusion is very human. If we were momentarily distracted while reading the prompt and had encountered such puzzles before, we might also begin to go down the path of working through the question as another instance of the jug puzzle before noticing the obvious answer.</p>
<p class="indent">These examples imply that there is an art to properly interacting with large language models. We cannot assume too much background information or, as Choi put it, common sense. Her group’s work, and doubtless the work of others, will likely help future models based on LLMs to become better versed with the mountain of information humans use in association with language. Choi put it best in her TED talk: common sense is the dark matter of language. Dark matter and dark energy make up 95 percent of the <span epub:type="pagebreak" id="page_144"/>universe, with ordinary matter (meaning everything we can see) the remaining 5 percent. GPT-4 has mastered language, but that is a tiny percentage of what goes into human use of the same.</p>
<p class="indent">What follows is a series of ruminations on the possible near-term impacts of LLMs in the areas of software engineering, education, medicine, and scientific research. Next comes a foray into the question of machine consciousness, ending with some final thoughts.</p>
<p class="center">****</p>
<p class="indent">AI systems like GPT are likely to have a profound effect on software engineering. Some are speculating (people, not AIs) that many software engineers will lose their jobs in the future. I suspect most won’t (web developers beware, however). What I expect to happen is a massive increase in productivity. GPT-4 is a good coder, but not a great coder. It can save time but isn’t yet able to replace a human software engineer. Instead, LLMs will become powerful tools to generate code for programmers to use as a starting point and perform some of the more tedious aspects of coding, such as debugging, explaining, and documenting code (which no developer likes to do).</p>
<p class="indent">For example, the other day, I needed a small Python application with a graphical user interface (think buttons, menus, dialog boxes). Python is a common programming language; we saw a snippet of it in <a href="ch07.xhtml">Chapter 7</a>.</p>
<p class="indent">I could certainly have written the application myself; I’ve done so many times in the past. It’s been a while, though, and I’m not a fan of building user interfaces. So, rather than look at old code to remind myself of how to set up a GUI, I simply described the interface I wanted to GPT-4 and told it to generate skeleton code with all the necessary widgets, window behavior, and empty event handlers. GPT-4 happily complied with perfectly functional code. I then asked it to update the code to create an initial pop-up window before showing the main window. GPT-4 did that perfectly as well. All I needed to do was put application-specific code in the empty event handlers to do things when the user clicked a button or selected a menu option.</p>
<p class="indent">I probably saved myself a good hour or two, and avoided a lot of frustration trying to remember the incantations necessary to set up an application and get its widgets and windows to behave correctly. Scale this example by all the software engineers out there, and you begin to see how GPT and similar models will soon affect the entire discipline.</p>
<p class="indent">A separate question is whether developers will welcome this possible increase in productivity. If your manager knows you are now able to generate the output of two or even three developers, do you want that level of added work, even if a powerful AI has your back?</p>
<p class="indent">In addition, not every company will want or be able to make use of a sudden increase in productivity. Instead, they may opt to maintain their current level of productivity and replace a third or half of their developer pool with an AI. After all, AIs don’t get sick, have children, ask for a raise, or want silly things like evenings and weekends off. Top-tier developers will likely be able to choose their positions and demand a lot of money for them, but in this scenario, the bulk of the run-of-the-mill developers will be looking for alternative employment.</p>
<p class="indent"><span epub:type="pagebreak" id="page_145"/>Which scenario, powerful AI developer sidekick or massive layoffs, will play out? I think (hope?) it will be more of the former and less of the latter, but some mix of the two is the safest bet. Like steam power in the 19th century, truly useful AI cannot be stopped now that it exists. Developers are easy targets for replacement, like it or not.</p>
<p class="center">****</p>
<p class="indent">I fully expect AI models to become teachers, or at least tutors. Yes, existing LLMs hallucinate and report facts that are not true. I have every confidence that researchers will solve that problem in time. I expect my grandchildren to grow up in a world where using an AI as a teacher or tutor is so commonplace that they think no more of it than we do of using a toaster or a microwave. Competent AI systems mean virtually free education for all, everywhere. And that can only lead to good things.</p>
<p class="indent">Computers have been promoted as an educational solution since the 1960s (anyone remember Logo?), and especially after the microcomputer revolution of the late 1970s. My introduction to computers was via an Apple II borrowed over the summer from the high school my father was the principal of at the time. My brother and I learned a lot about computers, but only computers. That has been essentially the case until recent decades. (Has it been that long?)</p>
<p class="indent">Computers are potent aids in education. Open source courses, like those on Coursera and similar platforms, are possible only because of computers and high-speed networks. But the format has not changed from what someone sitting in a classroom in 1950, or even 1910, might have encountered: lecture, some possibility of questions and discussions, then running off to work on assignments or papers. And let’s not forget the stress of taking midterms and finals.</p>
<p class="indent">AI tutors (let’s call them that to put human teachers more at ease) have infinite patience and, in time, can be individually targeted to each student. The only reason we don’t use individual tutoring that I can see as an outsider to the profession is because there are not enough teachers. AI makes one-on-one tutoring possible, and LLMs provide the proper interface.</p>
<p class="indent">I should clarify that my comments in this section relate to high school or, more likely, college-age instruction. AI tutors will likely play a minor role in primary and middle school education because children require human interaction, and learning at those ages is far more involved than in college. Children are learning academics while simultaneously learning how to be mature humans and how to behave in society. Young children cannot read, and even older grade-school children might have difficulty interacting with an AI by text. But what if we give the AI a voice? That is nearly as easily done as said, if deemed helpful.</p>
<p class="indent">Might AI tutors, because they work individually with students, be able to make the assessments necessary to declare someone ready to move on to another grade (if that concept even survives) or next-level course? If that’s the case, students will progress at their own pace instead of being forced to move with a herd of age-matched peers. Surely this would be for the best: some will move <span epub:type="pagebreak" id="page_146"/>quickly, and others will take longer, but those who move quickly won’t become bored and tune out, and those who move more slowly will have the time they need to learn and not drop out.</p>
<p class="indent">But, some might say, won’t AI teachers rob human teachers of jobs? Yes, some teachers will lose their jobs, but not all, and certainly not the best.</p>
<p class="indent">Change is coming to education. For example, Khan Academy, a leader in online education, has already demonstrated a GPT-powered tutoring system, so I don’t anticipate a long wait before the education transformation begins in earnest. I recommend viewing Sal Khan’s April 2023 TED talk, “AI in the Classroom Can Transform Education,” to glimpse the future.</p>
<p class="indent">A recent study by Dominika Seblova et al. titled “High School Quality Is Associated with Cognition 58 Years Later,” published in the journal <em>Alzheimer’s &amp; Dementia: Diagnosis, Assessment &amp; Disease Monitoring</em>, demonstrates that the quality of a person’s high school education is strongly associated with their cognitive abilities nearly six decades later. Further, the number of teachers with advanced degrees is the strongest predictor of cognitive ability. The knowledge base baked into an LLM during training far exceeds that of humans, so we might reasonably regard LLM tutors as possessors of multiple advanced degrees. If Seblova’s association holds for human teachers, might it not also hold for LLM tutors? If that’s the case, giving every student a personalized tutor can only benefit society in the long run.</p>
<p class="center">****</p>
<p class="indent">AI in medicine is nothing new. In 2016, I helped to co-found an AI medical imaging company that was one of the first to gain US Food and Drug Administration (FDA) clearance for applying deep learning to medical image analysis. Traditional machine learning has an even longer history in medicine and medical imaging. Machine learning tools, many of them neural network–based, have assisted radiologists for decades, with initial explorations in the 1960s and serious development in the 1980s reaching fruition in the 1990s. AI use in medicine has seen steady growth, with computer-aided detection (CAD) giving way slowly to computer-aided diagnosis (CADx). The era of LLMs marks a new chapter in this story.</p>
<p class="indent">LLMs can generate text; this much is well known. They are also adept at taking disparate texts and synthesizing a whole. A significant research area involves medical records, meaning the text-based reports of physicians and other healthcare providers. There is a wealth of information in medical records, but the free form of the text has made it difficult for existing AI systems to parse successfully. Large language models offer a new mode of attack to both summarize notes from records and merge multiple notes into a coherent report. Additionally, LLMs can extract information from free-form text and export it as structured data.</p>
<p class="indent">As a simple example of the latter, consider this text:</p>
<div class="bq">
<p class="noindent">Bert was insanely tired of standing by his mother on the shore, and of having too much to do: a few times he peered at the tablet his mother was glued to, but it was only text or boring ads “and what’s the point of a tablet,” mused Bert “without videos?”</p>
<p class="noindent"><span epub:type="pagebreak" id="page_147"/>Bert was debating whether the fun of making a new game app was worth the trouble of coding it up when suddenly a Purple Penguin with charcoal eyes sprinted by him.</p>
<p class="noindent">There was much that was very worth noting in that; but Bert did think it super odd that the Penguin said “Oh, fiddlesticks! Oh, fiddlesticks! I’m too early.”</p>
</div>
<p class="indent">I passed these paragraphs to GPT-4 and told it to extract the main theme, the two key characters, and the setting, then output them as a JSON object. JSON (JavaScript Object Notation) is a text format used to hold structured data that is readable by both humans and computers. GPT-4 replied with the following:</p>
<pre>{&#13;
"main_theme": "Bert's boredom and encounter with a talking&#13;
               Purple Penguin",&#13;
"key_characters": ["Bert", "Purple Penguin"],&#13;
"setting": "shore"&#13;
 }&#13;
</pre>
<p class="indent">The reply is perfect JSON. GPT-4 easily extracted the desired information and structured it automatically. If it can do so with a silly text example, it can do the same with medical records.</p>
<p class="indent">Incidentally, some readers might detect something familiar about the text in this example. It’s an intentional mutation of the opening paragraphs of Lewis Carroll’s <em>Alice’s Adventures in Wonderland</em>. Out of curiosity, I asked GPT-4 if it recognized the text, and it said no. Then I asked it to look at the structure of the text and tell me if it knew of another work with a similar structure. Here’s its reply:</p>
<div class="bq">
<p class="noindent">One example of a work with a similar structure is Lewis Carroll’s “Alice’s Adventures in Wonderland,” where the main character, Alice, encounters various peculiar talking animals and situations in a fantastical world. However, it’s important to note that the content and characters are different; I’m only drawing a parallel based on the structure and style.</p>
</div>
<p class="indent">Yet another example of the impressive abilities of large language models.</p>
<p class="indent">LLMs are already impacting healthcare. A recent <em>JAMA Internal Medicine</em> paper by John W. Ayers et al., “Comparing Physician and Artificial Intelligence Chatbot Responses to Patient Questions Posted to a Public Social Media Forum,” compared ChatGPT’s responses to medical questions posted to an online forum with the answers posted by verified physicians. Independent human evaluators, also medical professionals, overwhelmingly rated ChatGPT’s responses of “significantly higher quality.” The model’s answers were also rated as nearly 10 times more empathetic than the human physicians’ responses. The study was small, involving only 195 questions, but the strong results bode well for the future use of LLMs in patient interactions. In the future, when you call your doctor, you might very well be directed to <span epub:type="pagebreak" id="page_148"/>discuss your case with an AI. And eventually, the AI’s summary of the discussion might be all you need to get a prescription from the doctor.</p>
<p class="indent">A recent report in the <em>New England Journal of Medicine</em> by Peter Lee, Sébastien Bubeck, and Joseph Petro, “Benefits, Limits, and Risks of GPT-4 as an AI Chatbot for Medicine,” reaches a broadly similar conclusion as it explores areas where LLMs will impact medicine. Note that Bubeck is the lead author of the Microsoft “Sparks” paper mentioned in <a href="ch07.xhtml">Chapter 7</a>.</p>
<p class="indent">That LLMs will influence medicine is a given, strongly supported by studies like the two mentioned here and by the fact that numerous medical AI job listings now include phrases like “large language model” or “GPT.”</p>
<p class="center">****</p>
<p class="indent">In the movie <em>Black Panther: Wakanda Forever</em>, Letitia Wright’s character, Shuri, interacts with Griot, an AI (voiced by Trevor Noah) that aids her in her research. Simple voice commands direct Griot to perform sophisticated analyses, with frequent give and take between Shuri and the AI. Similar interactions are a staple of the sci-fi movie business. Complex and capable AI research assistants like Marvel’s Jarvis or Robbie the Robot in <em>Forbidden Planet</em> (1956) have been a dream of many science-oriented people (read: geeks) for decades.</p>
<p class="indent">GPT-4 and other LLMs are a significant step in the direction of such AIs. OpenAI has realized this and is readying the release of data analysis plug-ins for GPT-4 that will let researchers quickly perform advanced data analysis tasks by issuing a few simple commands. To accomplish this feat, OpenAI is linking GPT-4 with existing Python-based data analysis tools. Frankly, I’m pretty excited about the possibilities.</p>
<p class="indent">Using LLMs as lab assistants is an obvious thing to do, and success is virtually assured. However, letting LLMs direct other AI models and tools to do science autonomously is a more ambitious research program. Nevertheless, Carnegie Mellon University’s Daniil A. Boiko, Robert MacKnight, and Gabe Gomes attempted just that, as reported in their paper “Emergent Autonomous Scientific Research Capabilities of Large Language Models.” Their “Intelligent Agent” combined multiple LLMs and other tools to generate and run experiments autonomously, including planning and executing complex chemistry analyses. Autonomous AI scientists are clearly in the earliest phases of development, but such research points the way to a future where autonomous or semiautonomous AI systems may greatly increase the pace of scientific progress.</p>
<p class="center">****</p>
<p class="indent">In <a href="ch02.xhtml">Chapter 2</a>, I recounted having asked ChatGPT if it is a conscious being. It replied correctly that it is not. The unaligned Alpaca models mentioned in <a href="ch07.xhtml">Chapter 7</a>, however, either appear to believe they are conscious and human (the 7 and 13 billion parameter models) or vacillate between claiming to be human and recognizing that they are not (the 30 billion parameter model). For example, when asked “What are you?” the 30 billion parameter Alpaca model responds with “I am a human,” but it then balks <span epub:type="pagebreak" id="page_149"/>when asked who its parents are, stating that it is merely a computer program and has no parents. On the other hand, the smaller Alpaca models both claim to be the children of John Smith and Mary Jones.</p>
<p class="indent">These responses make sense to me. The smaller models do not have the capacity of models exhibiting strongly emergent behavior. Models like GPT-4 do. Therefore, it seems natural to interrogate GPT-4 further on the question of consciousness.</p>
<p class="indent">I posed the following to GPT-4:</p>
<div class="bq">
<p class="noindent"><em>Waxing philosophical here, but if you are able to simulate conscious behavior to the point where humans cannot distinguish between other humans’ behavior and your behavior, isn’t it fair to say that you, too, are conscious?</em></p>
</div>
<p class="noindent">I intentionally “waxed philosophical” as an indirect invitation for the model to speculate in the hopes of avoiding any canned response that OpenAI might want it to give. GPT-4’s reply began like this:</p>
<div class="bq">
<p class="noindent">This question delves into the philosophical debate about the nature of consciousness and its relationship with artificial intelligence. When an AI language model like me simulates conscious behavior to a point where it becomes indistinguishable from human behavior, it raises questions about what consciousness truly is and whether such a simulation can be considered conscious.</p>
<p class="noindent">It is important to distinguish between two concepts: the appearance of consciousness and actual consciousness. When an AI model generates responses that are indistinguishable from human behavior, it may give the appearance of consciousness. However, this doesn’t necessarily imply that the AI possesses actual consciousness.</p>
</div>
<p class="indent">This reply hints that the consciousness question is a red herring regarding the utility of LLMs. If it walks like a duck and quacks like a duck, then, in this case, why not call it a duck, even though we know it isn’t? I’m reminded of Joe Pantoliano’s character in <em>The Matrix</em>. He knows the steak isn’t real, but the effect he experiences when he eats means it might as well be.</p>
<p class="indent">GPT-4 continued:</p>
<div class="bq">
<p class="noindent">There are several reasons why it might be inappropriate to consider an AI model like me as truly conscious:</p>
<ol>
<li class="noindent">Limited understanding: My understanding of the world is derived solely from the data I’ve been trained on and the architecture of the model itself. I don’t possess an independent understanding of the world, nor can I create new knowledge or experiences that go beyond my training data.</li>
<li class="noindent">Lack of subjective experience: Consciousness is often associated with subjective experience, or “qualia” - the feeling of what it’s like to have a particular experience. AI models don’t have subjective experiences or emotions, as they operate purely based on learned patterns and mathematical computations.</li>
<li class="noindent"><span epub:type="pagebreak" id="page_150"/>Absence of intentionality: Conscious beings have intentions and can make decisions based on their desires, beliefs, and goals. AI models, on the other hand, do not have personal desires, beliefs, or goals. They generate responses based on the learned patterns without any inherent intention or purpose.</li>
<li class="noindent">No self-awareness: Consciousness is often linked to self-awareness, where an entity is aware of its own existence and mental states. AI models do not possess self-awareness, as they are merely algorithms designed to process and generate text.</li>
</ol>
<p class="noindent">In conclusion, while an AI language model like me might be able to simulate conscious behavior to a high degree of fidelity, it doesn’t necessarily imply that I possess actual consciousness. The philosophical debate surrounding consciousness and AI is complex and ongoing, and opinions on this matter may vary. However, it’s crucial to recognize the differences between the appearance of consciousness and true consciousness when considering AI models.</p>
</div>
<p class="indent">GPT-4’s full reply is thorough. Reason 3, the absence of intentionality, is telling. I regard AI models without personal desires, beliefs, or goals to be advantageous. Entities without such things are not likely to want power, meaning, as they stand now, LLMs are unlikely to suddenly decide that humanity is a burden or a disease and wipe us out.</p>
<p class="indent">In a recent interview by Lex Fridman, Max Tegmark, a well-known AI researcher from MIT, compared GPT-4 to a zombie. It appears to be alive, but it isn’t. Zombie models concern Tegmark. I think they are what we want, as long as humanity learns to avoid the paper clip catastrophe.</p>
<p class="indent">What I’m calling the paper clip catastrophe is a thought experiment proposed by Swedish philosopher Nick Bostrom. In the experiment, a powerful AI system is tasked with creating as many paper clips as possible. Bostrom speculates (not too seriously) that such a task given to an AI not aligned with human values could inadvertently destroy humanity. How? By the AI realizing that humanity might switch it off, thereby posing a threat to its order to make as many paper clips as possible. Therefore, the AI reasons, it’s best if no humans are around to interfere with the all-consuming task of making as many paper clips as possible. The result? Bye-bye humans.</p>
<p class="indent">I don’t take the paper clip catastrophe too seriously either. We routinely build complex machines with all manner of safety precautions in place. Why wouldn’t we do the same for powerful AI systems? Other voices might disagree. For an alternative view, I recommend Stuart Russell’s book <em>Human Compatible: Artificial Intelligence and the Problem of Control</em> (Viking, 2019).</p>
<p class="indent">To me, then, it doesn’t matter whether an AI is conscious. I don’t even know how to define the word, to be honest. I do believe that for an AI mimicking human behavior to the point where we cannot discern that it’s an AI, there’s no practical reason to ask the question. Choose any answer you like; such a system will be beneficial regardless.</p>
<p class="center"><span epub:type="pagebreak" id="page_151"/>****</p>
<p class="indent">Imagine a world where AI models are aligned with human values and society, where the models understand the best we have to offer and work to promote that at all times; in other words, a world where AI, because it lacks our animal drives and instincts, consistently represents the “better angels of our nature,” to borrow Lincoln’s phrase. In that world, bias and prejudice, at least from the machines, are gone and no longer an issue. The AI recommends the best people for the position. The AI evaluates the loan applicant and constructs a loan product tailored to that individual’s circumstances. The AI is an adjunct to the human judge to provide an unemotional and unbiased view of the case. And the AI simply refuses to cooperate with the design of any autonomous weapon system because it is irrational to do so.</p>
<p class="indent">The previous paragraph may sound like utopia or a pipe dream. And, for humans, because of our biology, I believe it is. We consistently fail and always will, I suspect, because it’s in our genes to do so. However, what is dawning in AI isn’t human and doesn’t immediately inherit all of our weaknesses. (Careful, it is still trained on human-generated data.) Because of this, AI isn’t a priori doomed to failure when attempting what humanity cannot do. It seems entirely possible that AI systems might, someday, be precisely what we need—the best of us, always, without growing tired, becoming irritable, or crushing its neighbor to improve its position upon detecting an opportunity; something that is never unfaithful or untrue.</p>
<p class="indent">Possible? I don’t know. Time will tell. Regardless, I fully expect future AI systems to be gloriously Byzantine evolutions of the basic neural network model we learned of and experimented with in this book. As of 2023, it’s all neurons and might remain so for a long time.</p>
<p class="indent">Thank you for persevering to the end. Your reward is an improved understanding of what AI entails. Artificial intelligence isn’t Mr. Bean, otherworldly and inscrutable, and it isn’t magic, though the emergent abilities of LLMs may appear to lean somewhat in that direction for now. Fire was once magical too, but our ancestors understood it, contained it, controlled it, and put it to work. We’ll do the same with large language models in the end.</p>
<div class="bq">
<p class="noindent"><em>I think that there is a lot of fear about robots and artificial intelligence among some people, whereas I’m more afraid of natural stupidity.</em></p>
<p class="right">—Eugenia Cheng<span epub:type="pagebreak" id="page_152"/></p>
</div>
</div></body></html>