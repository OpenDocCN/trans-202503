<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" epub:prefix="index: http://www.index.com/" lang="en" xml:lang="en">
<head>
<title>Chapter 8: Homotopy Algorithms</title>
<link href="NSTemplate_v1.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:ea5eeba6-9dea-4463-bfe4-b91d6c6b5861" name="Adept.expected.resource"/>
</head>
<body epub:type="bodymatter chapter">
<section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" id="Page_167" title="167"/><a class="XrefDestination" id="8"/><span class="XrefDestination" id="xref-503083c08-001"/>8</span><br/>
<span class="ChapterTitle"><a class="XrefDestination" id="HomotopyAlgorithms"/><span class="XrefDestination" id="xref-503083c08-002"/>Homotopy Algorithms</span></h1>
</header>
<figure class="opener">
<img alt="" src="image_fi/book_art/chapterart.png"/>
</figure>
<p class="ChapterIntro">In this chapter, we’ll explore algorithms related to homotopy, a way to classify topological objects based on path types around the object, including homotopy-based calculations of regression parameters. Local minima and maxima often plague datasets: they provide suboptimal stopping points for algorithms that explore solution spaces locally. In the next few pages, we’ll see how homotopy solves this problem.</p>
<h2 id="h1-503083c08-0001"><a class="XrefDestination" id="IntroducingHomotopy"/><span class="XrefDestination" id="xref-503083c08-003"/>Introducing Homotopy</h2>
<p class="BodyFirst">Two paths or functions are <em>homotopic</em> to each other if they can be continuously deformed into each other within the space of interest. Imagine a golf course and a pair of golfers, one who is a better putter than the other. The ball can travel to the hole along many different paths. Imagine tracing out <span epub:type="pagebreak" id="Page_168" title="168"/>the path of each shot on the green with rope. One path might be rather direct. The other might meander quite a bit before finding the hole, particularly if the green is hilly. A bad golfer may have to make many shots, resulting in a long, jagged path. But no matter how many hills exist or how many strokes it takes for the golfer’s ball to make it into the hole, we could shorten each of these paths by deforming the rope, as depicted in <a href="#figure8-1" id="figureanchor8-1">Figure 8-1</a>.</p>
<figure>
<img alt="" class="" src="image_fi/503083c08/f08001_m.png"/>
<figcaption><p><a id="figure8-1">Figure 8-1</a>: A long path to the hole of a golf course (left) deformed to a shorter path to the hole (right)</p></figcaption>
</figure>
<p>Let’s stretch the analogy somewhat and imagine a sinkhole has appeared in the golf course. Topological objects and spaces with holes can complicate this deformation process and lead to many different possible paths from one point to another. Paths can connect two points on an object. Depending on the object’s properties, these paths can sometimes “wiggle” enough to overlap with another path without having to cut the path into pieces to get around an obstacle (usually a hole). Winding paths around holes presents a problem to continuous deformation of one path into another. It’s not possible for the path to wind or wiggle around a hole, such that a path between points will necessarily overlap with another path. Different types of paths begin to emerge as holes and paths around holes are added. One path might make only one loop around a hole before connecting two points. Another might make several loops around a hole before connecting two points. Imagine golfing again. Let’s say that the green has an obstacle (such as a rock or a water hazard) in the middle of it, creating a torus with tricky hills around it that can force a bad shot to require circumnavigating the rock to get back to the hole, as you can see in <a href="#figure8-2" id="figureanchor8-2">Figure 8-2</a>.</p>
<figure>
<img alt="" class="" src="image_fi/503083c08/f08002_m.png"/>
<figcaption><p><a id="figure8-2">Figure 8-2</a>: Two paths with the same start and end points on a torus course (donut) that cannot be morphed into each other without being cut or having the inner hole removed</p></figcaption>
</figure>
<p><span epub:type="pagebreak" id="Page_169" title="169"/>In this scenario, we can no longer deform paths into each other without cutting the line or removing the hole. As more holes or holes of larger dimension are added, more classes of equivalent paths begin to emerge, with equivalent paths having the same number of loops around one or more holes. A two-dimensional course will have fewer possible paths from the tee to the hole than a three-dimensional course, as fewer possible types of obstacles and holes in the course exist. A space with many holes or obstacles in many different dimensions presents a lot of obstacles that paths can wind around. This means many unique paths exist for that space.</p>
<p>Given that datasets often contain holes of varying dimension, many different classes of paths may exist in the data. Random walks on the data, common in Bayesian analyses and robotic navigation path-finding tasks, may not be equivalent. This can be an advantage in navigation problems, allowing the system to choose from a set of different paths with different cost weights related to length, resource allocation, and ease of movement. For instance, in the path-finding problem in <a href="#figure8-3" id="figureanchor8-3">Figure 8-3</a>, perhaps obstacle 2 has sharp ends that could harm the system should it get too close, making the leftmost path the ideal one for the system to take.</p>
<figure>
<img alt="" class="" src="image_fi/503083c08/f08003.png"/>
<figcaption><p><a id="figure8-3">Figure 8-3</a>: An example obstacle course with navigation from a start point to a finish point with several possible solutions</p></figcaption>
</figure>
<p><a href="#figure8-3">Figure 8-3</a> shows three paths, and none of them can be deformed into another of the paths without moving an obstacle or cutting the path. These are unique paths in the space. By counting the total number of unique paths, we can classify the space topologically.</p>
<h2 id="h1-503083c08-0002"><a class="XrefDestination" id="IntroducingHomotopy-BasedRegression"/><span class="XrefDestination" id="xref-503083c08-004"/>Introducing Homotopy-Based Regression</h2>
<p class="BodyFirst">As mentioned, datasets often contain obstacles in the form of local optima, that is, local maximums and minimums. Gradient descent algorithms and other stepwise optimization algorithms can get stuck there. You can think of this as the higher-dimensional version of hills and valleys (saddle points, which are higher-dimensional inflection points, can also pose optimization issues). Getting stuck in a local optimum provides less-than-ideal solutions to an optimization problem.</p>
<p><span epub:type="pagebreak" id="Page_170" title="170"/>Homotopy-based algorithms can help with the estimation of parameters in high-dimensional data containing many local optima, under which conditions many common algorithms such as gradient descent can struggle. Finding a solution in a space with fewer local optima and then continuously deforming that solution to the original space can lead to better accuracy of estimates and variables selected in a model.</p>
<p>To provide more insight, consider a blindfolded person trying to navigate through an industrial complex (<a href="#figure8-4" id="figureanchor8-4">Figure 8-4</a>). Without a tether, they are sure to bump into obstacles and potentially think they have hit their target when they are stopped by one of the larger obstacles.</p>
<figure>
<img alt="" class="" src="image_fi/503083c08/f08004.png"/>
<figcaption><p><a id="figure8-4">Figure 8-4</a>: A blindfolded person navigating an obstacle course</p></figcaption>
</figure>
<p>However, if they are given a rope connecting their starting point to their ending point, they can navigate between the points a bit better and know that any obstacle they encounter is likely not the true ending point. There are many possible ways to connect the start and finish points. <a href="#figure8-5" id="figureanchor8-5">Figure 8-5</a> shows one possible rope configuration.</p>
<figure>
<img alt="" class="" src="image_fi/503083c08/f08005.png"/>
<figcaption><p><a id="figure8-5">Figure 8-5</a>: A blindfolded person navigating an obstacle course with a guide rope</p></figcaption>
</figure>
<p>A blindfolded person struggling to avoid physical obstacles is analogous to a machine learning algorithm avoiding local optima. For example, let’s <span epub:type="pagebreak" id="Page_171" title="171"/>consider a function of two variables with a global maximum and minimum but other local optima, as derived in <a href="#listing8-1" id="listinganchor8-1">Listing 8-1</a>.</p>
<pre><code>#load plot library and create the function
library(scatterplot3d)
x&lt;-seq(-10,10,0.01)
y&lt;-seq(-10,10,0.01)
z&lt;-2*sin(y)-cos(x)
which(z==min(z))
which(z==max(z))
scatterplot3d(x,y,z,main="Scatterplot of 3-Dimensional Data")</code></pre>
<p class="CodeListingCaption"><a id="listing8-1">Listing 8-1</a>: A script that creates a function of two variables with a global minimum and maximum but many other local optima</p>
<p>The code in <a href="#listing8-1">Listing 8-1</a> produces the plot in <a href="#figure8-6" id="figureanchor8-6">Figure 8-6</a>, from which we can see many minima and maxima. The other optima are local optima, some of which are very close to the global minimum or maximum.</p>
<figure>
<img alt="" class="" src="image_fi/503083c08/f08006.png"/>
<figcaption><p><a id="figure8-6">Figure 8-6</a>: A scatterplot of three-dimensional data, namely, a function with many local optima</p></figcaption>
</figure>
<p>An algorithm trying to optimize this function will likely get stuck in one of the local optima, as the values near the local optima are increasing or decreasing from that optimum’s value. Some algorithms that have been known to struggle with this type of optimization include gradient descent and the expectation-maximization (EM) algorithm, among others. Optimization strategies such as evolutionary algorithms will also likely take a long time to find global solutions, making them less ideal for this type of data.</p>
<p>Homotopy-based calculations provide an effective solution to this problem of local optima traps; algorithms employing homotopy-based calculations can wiggle around or out of local optima. In essence, these <span epub:type="pagebreak" id="Page_172" title="172"/>algorithms start with an easy optimization problem, in which no local optima are present, and deform the solution slowly according to the dataset and its geometry, avoiding local optima as the deformation proceeds.</p>
<p>Homotopy-based optimization methods commonly used these days in machine learning include support vector machines, Lasso, and even neural networks. The lasso2 package of R is one package that implements homotopy-based models; in this case, lasso2 implements a homotopy-based model for the Lasso algorithm. Let’s first explore model fit and solutions for the data generated in <a href="#listing8-1">Listing 8-1</a>, in which the outcome has many local optima and the predictors are collinear, a problem for many machine learning algorithms. Add the following to the code in <a href="#listing8-1">Listing 8-1</a>:</p>
<pre><code>#partition into training and test samples
mydata&lt;-as.data.frame(cbind(x,y,z))
set.seed(10)
s&lt;-sample(1:2001,0.7*2001)
train&lt;-mydata[s,]
test&lt;-mydata[-s,]</code></pre>
<p>Now, the model is ready to be built and tested. The outcome of interest (our variable <code>z</code>) is not normally distributed, but a Gaussian distribution is the closest available distribution for use in the model. In the following addition to the script in <a href="#listing8-1">Listing 8-1</a>, the <code>etastart</code> parameter needs to be set to null before starting the model iterations, and a bound needs to be in place to guide the homotopy-based parameter search. Generally, a lower setting is best:</p>
<pre><code>#load package and create model
library(lasso2)
etastart&lt;-NULL
las1&lt;-gl1ce(z~.,train,family=gaussian(),bound=0.5,standardize=F)
lpred1&lt;-predict(las1,test)
sum((lpred1-test$z)^2)/601</code></pre>
<p>This script now fits a homotopy-based Lasso model to the training data and then predicts test data outcomes based on this model, allowing us to assess the model fit. The mean square error for this sample, calculated in the final line, should be near 2.30. (Again, results may vary with R versions, as the seeding and sampling algorithms changed.) The results of the model suggest that one term dominates the behavior of the function:</p>
<pre><code>&gt; <b>las1</b>
Call:
gl1ce(formula=z ~ .,data=train,family=gaussian(),standardize=F,
    bound=0.5)

Coefficients:
(Intercept)           x           y
 0.05068903  0.04355682  0.00000000

<span epub:type="pagebreak" id="Page_173" title="173"/>Family:

Family: gaussian
Link function: identity


The absolute L1 bound was       :  0.5
The Lagrangian for the bound is :  1.305622e-13</code></pre>
<p>These results, which may vary for readers with different versions of R, show that only one variable is selected as important to the model. <code>x</code> contributes more to the prediction of <code>z</code> than <code>y</code> contributes, according to our model. Linear regression isn’t a great tool to use on this problem, given the nonlinear relationships between <code>x</code> or <code>y</code> and <code>z</code>, but it does find some consistency in the relationship.</p>
<p>To compare with another method, let’s create a linear regression model and add it to <a href="#listing8-1">Listing 8-1</a>:</p>
<pre><code>#create linear model
l&lt;-lm(z~.,train)
summary(l)
lpred&lt;-predict(l,test)
sum((lpred1-test$z)^2)/601</code></pre>
<p>This code trains a linear model on the training data and predicts test set outcomes, similar to how the homotopy-based model was fit with the previous code. You may get a warning with your regression model, as there is covarying behavior of <code>x</code> and <code>y</code> (which presents issues to linear regression models per the assumption of noncollinearity). Let’s take a look at this model’s results:</p>
<pre><code>&gt; <b>summary(l)</b>
 Call:
lm(formula=z ~ .,data=train)

Residuals:
     Min       1Q   Median       3Q      Max
-2.51261 -1.48663  0.07368  1.48680  2.37086

Coefficients: (1 not defined because of singularities)
            Estimate Std. Error t value Pr(&gt;|t|)
(Intercept) 0.050689   0.041267   1.228     0.22
x           0.043557   0.007112   6.124 1.18e-09 ***
y                 NA                    NA
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.544 on 1398 degrees of freedom
Multiple R-squared:  0.02613,   Adjusted R-squared:  0.02543
F-statistic: 37.51 on 1 and 1398 DF,  p-value: 1.183e-09</code></pre>
<p><span epub:type="pagebreak" id="Page_174" title="174"/>The mean square error (MSE) for this sample should be near 2.30, which is the same as the homotopy-based model. MSE accounts for both variance and bias in the estimator, giving a balanced view of how well the algorithm is performing on a regression task. However, the collinearity is problematic for the linear regression model. Penalized models avoid this issue, including homotopy-based Lasso models. Of note, the coefficients found by the linear regression and the homotopy-based Lasso model are identical. Typically, models with different optimization strategies will vary a bit on their estimates. In this case, the sample size is probably large and the number of predictors few enough for both algorithms to converge to a global optimum.</p>
<h2 id="h1-503083c08-0003"><a class="XrefDestination" id="UsingHomotopyonOurSchoolData"/><span class="XrefDestination" id="xref-503083c08-005"/>Comparing Results on a Sample Dataset</h2>
<p class="BodyFirst">Let’s return to our self-reported educational dataset and explore the relationships between school experiences, IQ, and self-reported depression. Because we don’t know what the function between these predictors and depression should be, we don’t know what sort of local optima might exist. However, we do know that a training dataset with 7 predictors and 16 individuals (70 percent of the data) will be sparse, and it’s possible that local optima are a problem in the dataset. There is evidence that geometry-based linear regression models work better on sparse datasets than other algorithms, and it’s possible that our homotopy-based Lasso model will work better, as well.</p>
<p>Let’s create <a href="#listing8-2" id="listinganchor8-2">Listing 8-2</a> and partition the data into training and test sets.</p>
<pre><code>#load data and set seed
mydata&lt;-read.csv("QuoraSample.csv")
set.seed(1)

#sample data to split into two datasets with stratified sampling
#to ensure more balance in the training set with respect to depression
m1&lt;-mydata[mydata$Depression==1,]
m2&lt;-mydata[mydata$Depression==0,]
s1&lt;-sample(1:4,3)
s2&lt;-sample(1:18,6)
train&lt;-rbind(m1[s1,],m2[s2,])
test&lt;-rbind(m1[-s1,],m2[-s2,])</code></pre>
<p class="CodeListingCaption"><a id="listing8-2">Listing 8-2</a>: A script that loads and then analyzes the Quora IQ sample</p>
<p>Now, let’s run a homotopy-based Lasso model and a logistic regression model to compare results on this small, real-world dataset:</p>
<pre><code>#run the homotopy-based Lasso model
las1&lt;-gl1ce(factor(Depression)~.,train,family=binomial(),bound=2,standardize=F)
lpred1&lt;-round(predict(las1,test,type="response"))
length(which(lpred1==test$Depression))/length(test$Depression)

<span epub:type="pagebreak" id="Page_175" title="175"/>#run the logistic regression model
gl&lt;-glm(factor(Depression)~.,train,family=binomial(link="logit"))
glpred&lt;-round(predict(gl,test,type="response"))
length(which(glpred==test$Depression))/length(test$Depression)</code></pre>
<p>From running the models in the previous script addition, we should see that the homotopy-based Lasso model has a higher accuracy (~85 percent) than the logistic regression (~70 percent); additionally, the logistic regression model spits out a warning message about fitted probabilities of 0 or 1 occurring. This means the data is quite separated into groups, which can happen when small data with strong relationships to an outcome is split. Depending on your version of R or GUI, you may end up with a different sample and, thus, somewhat different fit statistics and results. Because this is a relatively small sample to begin with, it’s possible that you’ll have slightly different results than the ones presented here. Some samples may not have any instances of a given predictor within the dataset. Larger samples would create more stable models across samples.</p>
<p>Let’s look more closely at the homotopy-based Lasso model and its coefficients:</p>
<pre><code>&gt; <b>las1</b>
Call:
gl1ce(formula=factor(Depression) ~ .,data=train,family=binomial(),
    standardize=F,bound=2)

Coefficients:
           (Intercept)                     IQ               Bullying
           -8.31322182             0.04551602             0.00000000
     Teacher.Hostility                Boredom     Lack.of.Motivation
            0.00000000             0.51213722             0.00000000
      Outside.Learning Put.in.Remedial.Course
           -1.01281345             0.42953330

Family:

Family: binomial
Link function: logit


The absolute L1 bound was       :  2
The Lagrangian for the bound is :  0.4815216</code></pre>
<p>From the previous output, we can see that, for this sample, higher IQ, endorsement of boredom, and being put in a remedial class increase the likelihood of self-reported depression. However, outside learning has a strong protective effect. In fact, outside learning can completely counterbalance the risk from boredom and being placed in a remedial course. This suggests that parents of profoundly gifted children who are experiencing school issues may be able to mitigate some of the potential adverse outcomes, such as depression, by providing outside learning opportunities, such as college courses in the evening, tutoring outside of school, or other opportunities for the child to <span epub:type="pagebreak" id="Page_176" title="176"/>learn. The role of outside learning opportunities has been explored to some extent in the giftedness literature with similar results, but more research is needed on this topic.</p>
<p>Now, let’s compare these results with the results of the logistic regression model:</p>
<pre><code>&gt; <b>summary(gl)</b>
CALL:
glm(formula=factor(Depression) ~ .,family=binomial(link="logit"),
    data=train)

Deviance Residuals:
        20          21           6          16          18           4
 3.971e-06   1.060e-05    -3.971e-06  -3.971e-06  -2.110e-08
        11           2          19
-8.521e-06  -1.060e-05  -1.060e-05

Coefficients:
                         Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)            -7.297e+02  1.229e+06  -0.001        1
IQ                      3.934e+00  6.742e+03   0.001        1
Bullying               -1.421e+01  5.193e+05   0.000        1
Teacher.Hostility       2.798e+01  3.424e+05   0.000        1
Boredom                -1.967e+01  8.765e+04   0.000        1
Lack.of.Motivation      4.174e+01  2.496e+05   0.000        1
Outside.Learning       -6.535e+01  2.765e+05   0.000        1
Put.in.Remedial.Course  1.121e+02  2.712e+05   0.000        1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1.1457e+01  on 8  degrees of freedom
Residual deviance: 5.6954e-10  on 1  degrees of freedom
AIC: 16
Number of Fisher Scoring iterations: 24</code></pre>
<p>Examining the previous output, it seems that the logistic regression model could not handle the dataset, giving errors and spitting out very large coefficients. This is likely related to the smallness of the data, where the linear system is underdetermined; however, this is not a situation where the number of predictors outnumber the number of individuals in the sample, so it is likely a function of the data itself rather than purely sample size.</p>
<p>Note the model fails to find any significant predictors of self-reported depression. Linear regression can’t handle this dataset very well, and the results are not reliable. For some samples, certain variables may not be computable in the linear regression model at all. Homotopy-based models (and other types of penalized models) often work better on small datasets, and there is some evidence that they perform better for datasets with many local optima. While this dataset is a bit small for fitting a model, it does demonstrate the power of homotopy-based optimization (and penalized regression, in general) on very small datasets, and its results make a lot more sense than the linear regression model’s results.</p>
<h2 id="h1-503083c08-0004"><span epub:type="pagebreak" id="Page_177" title="177"/><a class="XrefDestination" id="Summary"/><span class="XrefDestination" id="xref-503083c08-006"/>Summary</h2>
<p class="BodyFirst">In this chapter, we gave you an overview of homotopy and its applications in machine learning, including through an example of homotopy as an extension of regression-based algorithms on a simulated problem and a real dataset. Homotopy can help regression algorithms avoid local optima that often trap local optimizers.</p>
<p>Other uses of homotopy algorithms in the field of artificial intelligence include navigational problems. For instance, an autonomous cart may need to navigate the halls and obstacles of a hospital by weighting different possible paths from its current location to its destination. Homotopy algorithms are often used to generate the possible paths, which are then weighted by time cost or hazard cost of the route. Bounds can also be placed to avoid generating paths that obviously aren’t viable (such as going through areas where the cart can’t physically go or wouldn’t be wanted—such as an operating room). It’s likely that this branch of topological data analysis will grow in the coming years, and we encourage you to explore other uses of homotopy in machine learning, robotics, differential equations, and engineering.</p>
</section>
</body>
</html>