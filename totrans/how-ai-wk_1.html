<html><head></head><body>
<div id="sbo-rt-content"><h2 class="h2" id="ch01"><span epub:type="pagebreak" id="page_5"/><strong><span class="big">1</span><br/>AND AWAY WE GO: AN AI OVERVIEW</strong></h2>
<div class="image1"><img alt="Image" height="189" src="../images/common.jpg" width="189"/></div>
<p class="noindentsa"><a href="glossary.xhtml#glo5"><em>Artificial intelligence</em></a> attempts to coax a machine, typically a computer, to behave in ways humans judge to be intelligent. The phrase was coined in the 1950s by prominent computer scientist John McCarthy (1927–2011).</p>
<p class="indent">This chapter aims to clarify what AI is and its relationship to <a href="glossary.xhtml#glo64"><em>machine learning</em></a> and <a href="glossary.xhtml#glo29"><em>deep learning</em></a>, two terms you may have heard in recent years. We’ll dive in with an example of machine learning in action. Think of this chapter as an overview of AI as a whole. Later chapters will build on and review the concepts introduced here.</p>
<p class="center">****</p>
<p class="indent">Computers are <em>programmed</em> to carry out a particular task by giving them a sequence of instructions, a <em>program</em>, which embodies an <a href="glossary.xhtml#glo2"><em>algorithm</em></a>, or the recipe that the program causes the computer to execute.</p>
<p class="indent">The word <a href="glossary.xhtml#glo2"><em>algorithm</em></a> is cast about often these days, though it isn’t new; it’s a corruption of <em>Al-Khwarizmi</em>, referring to ninth-century Persian mathematician Muhammad ibn Musa al-Khwarizmi, whose primary gift to the world was the mathematics we call <em>algebra</em>.</p>
<p class="center"><span epub:type="pagebreak" id="page_6"/>****</p>
<p class="indent">Let’s begin with a story.</p>
<p class="indent">Tonya owns a successful hot sauce factory. The hot sauce recipe is Tonya’s own, and she guards it carefully. It’s literally her secret sauce, and only she understands the process of making it.</p>
<p class="indent">Tonya employs one worker for each step of the hot sauce–making process. These are human workers, but Tonya treats them as if they were machines because she’s worried they’ll steal her hot sauce recipe—and because Tonya is a bit of a monster. In truth, the workers don’t mind much because she pays them well, and they laugh at her behind her back.</p>
<p class="indent">Tonya’s recipe is an algorithm; it’s the set of steps that must be followed to create the hot sauce. The collection of instructions Tonya uses to tell her workers how to make the hot sauce is a program. The program embodies the algorithm in a way that the workers (the machine) can follow step by step. Tonya has programmed her workers to implement her algorithm to create hot sauce. The sequence looks something like this:</p>
<div class="image1"><img alt="Image" height="49" src="../images/ch01fig00.jpg" width="522"/></div>
<p class="indent">There are a few things to note about this scenario. First, Tonya is definitely a monster for treating human beings as machines. Second, at no point in the process of making hot sauce does any worker need to understand why they do what they do. Third, the programmer (Tonya) knows why the machine (the workers) does what it does, even if the machine doesn’t.</p>
<p class="center">****</p>
<p class="indent">What I’ve just described is how we’ve controlled virtually all computers, going back to the first conceptual machines envisioned by Alan Turing in the 1930s and even earlier to the 19th-century Analytical Engine of Charles Babbage. A human conceives an algorithm, then translates that algorithm into a sequence of steps (a program). The machine executes the program, thereby implementing the algorithm. The machine doesn’t understand what it’s doing; it’s simply performing a series of primitive instructions.</p>
<p class="indent">The genius of Babbage and Turing lay in the realization that there could be a general-purpose machine capable of executing arbitrary algorithms via programs. However, I would argue that it was Ada Lovelace, a friend of Babbage’s often regarded as the world’s first programmer, who initially understood the far-reaching possibilities of what we now call a computer. We’ll talk more about Turing, Babbage, and Lovelace in <a href="ch02.xhtml">Chapter 2</a>.</p>
<div class="note">
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>
<p class="notep"><em>In Lovelace’s day, a “computer” was not a machine but a human being who calculated by hand. Hence, Babbage’s Engine was a mechanical computer.</em></p>
</div>
<p class="indent">Let’s take a moment to explore the relationship between the terms <em>AI</em>, <a href="glossary.xhtml#glo64"><em>machine learning</em></a>, and <a href="glossary.xhtml#glo29"><em>deep learning</em></a>. On the one hand, all three have become synonymous as referring to modern AI. This is wrong, but convenient. <a href="ch01.xhtml#ch01fig01">Figure 1-1</a> shows the proper relationship between the terms.</p>
<div class="image"><img alt="Image" height="202" id="ch01fig01" src="../images/ch01fig01.jpg" width="502"/></div>
<p class="figcap"><em>Figure 1-1: The relationship between artificial intelligence, machine learning, and deep learning</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_7"/>Deep learning is a subfield of machine learning, which is a subfield of artificial intelligence. This relationship implies that AI involves concepts that are neither machine learning nor deep learning. We’ll call those concepts <em>old-school AI</em>, which includes the algorithms and approaches developed from the 1950s onward. Old-school AI is not what people currently mean when discussing AI. Going forward, we’ll entirely (and unfairly) ignore this portion of the AI universe.</p>
<p class="indent"><a href="glossary.xhtml#glo64"><em>Machine learning</em></a> builds models from data. For us, a <a href="glossary.xhtml#glo69"><em>model</em></a> is an abstract notion of something that accepts inputs and generates outputs, where the inputs and outputs are related in some meaningful way. The primary goal of machine learning is to condition a model using <em>known</em> data so that the model produces meaningful output when given <em>unknown</em> data. That’s about as clear as muddy water, but bear with me; the mud will settle in time.</p>
<p class="indent"><a href="glossary.xhtml#glo29"><em>Deep learning</em></a> uses large models of the kind previously too big to make useful. More muddy water, but I’m going to argue that there’s no strict definition of deep learning other than that it involves neural networks with many layers. <a href="ch04.xhtml">Chapter 4</a> will clarify.</p>
<p class="indent">In this book, we’ll be sloppy but in accord with popular usage, even by experts, and take “deep learning” to mean large neural networks (yet to be formally defined), “machine learning” to mean models conditioned by data, and “AI” to be a catchall for both machine learning and deep learning—remembering that there is more to AI than what we discuss here.</p>
<p class="indent">Data is everything in AI. I can’t emphasize this enough. Models are blank slates that data must condition to make them suitable for a task. If the data is bad, the model is bad. Throughout the book, we’ll return to this notion of “good” and “bad” data.</p>
<p class="indent">For now, let’s focus on what a model is, how it’s made useful by conditioning, and how it’s used after conditioning. All this talk of conditioning and using sounds dark and sinister, if not altogether evil, but, I assure you, it’s not, even though we have ways of making the model talk.</p>
<p class="center">****</p>
<p class="indent">A machine learning model is a black box that accepts an input, usually a collection of numbers, and produces an output, typically a label like “dog” or “cat,” or a continuous value like the probability of being a “dog” or the value of a house with the characteristics given to the model (size, number of bathrooms, ZIP code, and so on).</p>
<p class="indent"><span epub:type="pagebreak" id="page_8"/>The model has <a href="glossary.xhtml#glo80"><em>parameters</em></a>, which control the model’s output. Conditioning a model, known as <a href="glossary.xhtml#glo96"><em>training</em></a>, seeks to set the model’s parameters in such a way that they produce the correct output for a given input.</p>
<p class="indent">Training implies that we have a collection of inputs, and the outputs the model should produce when given those inputs. At first blush, this seems a bit silly; why do we want the model to give us an output we already have? The answer is that we will, at some future point, have inputs for which we don’t already have the output. This is the entire point of making the model: to use it with unknown inputs and to believe the model when it gives us an output.</p>
<p class="indent">Training uses the collection of known inputs and outputs to adjust the model’s parameters to minimize mistakes. If we can do that, we begin to believe the model’s outputs when given new, unknown inputs.</p>
<p class="indent">Training a model is fundamentally different from programming. In programming, we implement the algorithm we want by instructing the computer step by step. In training, we use data to teach the model to adjust its parameters to produce correct output. There is no programming because, most of the time, we have no idea what the algorithm should be. We only know or believe a relationship exists between the inputs and the desired outputs. We hope a model can approximate that relationship well enough to be useful.</p>
<p class="indent">It’s worth remembering the sage words of British statistician George Box, who said that all models are wrong, but some are useful. At the time, he was referring to other kinds of mathematical models, but the wisdom applies to machine learning.</p>
<p class="indent">Now we understand why the field is called machine learning: we teach the machine (model) by giving it data. We don’t program the machine; we instruct it.</p>
<p class="indent">Here, then, is the machine learning algorithm:</p>
<ol>
<li class="noindent">Gather a training dataset consisting of a collection of inputs to the model and the outputs we expect from the model for those inputs.</li>
<li class="noindent">Select the type of model we want to train.</li>
<li class="noindent">Train the model by presenting the training inputs and adjusting the model’s parameters when it gets the outputs wrong.</li>
<li class="noindent">Repeat step 3 until we are satisfied with the model’s performance.</li>
<li class="noindent">Use the now-trained model to produce outputs for new, unknown inputs.</li>
</ol>
<p class="indent">Most of machine learning follows this algorithm. Since we’re using known <em>labeled data</em> to train the model, this approach is called <em>supervised learning</em>: we supervise the model while it learns to produce correct output. In a sense, we punish the model until it gets it right. This is a dark enterprise, after all.</p>
<p class="indent">We’re ready for an example, but let’s first summarize the story so far. We want a system where, for an unknown input, we get a meaningful output. To make the system, we train a machine learning model using a collection of inputs and their known outputs. Training conditions the model by modifying <span epub:type="pagebreak" id="page_9"/>its parameters to minimize the mistakes it makes on the training data. When we’re satisfied with the model’s performance, we use the model with unknown inputs because we now believe the model when it gives us an output (at least, most of the time).</p>
<p class="indent">Our first example comes from a famous dataset consisting of measurements of the parts of iris flowers. This dataset is from the 1930s, indicating how long people have contemplated what we now call machine learning.</p>
<p class="indent">The goal is a model that, for an input collection of measurements, outputs the specific species of iris flower. The full dataset has four measurements for three iris species. We’ll keep it simple and use two measurements and two species: petal length and width in centimeters (cm) for <em>I. setosa</em> versus <em>I. versicolor</em>. Therefore, we want the model to accept two measurements as input and give us an output we can interpret as <em>I. setosa</em> or <em>I. versicolor</em>. <em>Binary models</em> like this decide between two possible outputs and are common in AI. If the model decides between more than two categories, it’s a <em>multiclass</em> model.</p>
<p class="indent">We have 100 samples in our dataset: 100 pairs of petal measurements, and the corresponding iris flower types. We’ll call <em>I. setosa</em> class 0 and <em>I. versicolor</em> class 1, where <em>class</em> labels the input categories.</p>
<p class="indent">Models often want numeric class labels, which tells us that models don’t know what their inputs and outputs mean; they only make associations between sets of inputs and outputs. Models don’t “think” using any commonly accepted definition of the word. (The models of <a href="ch07.xhtml">Chapter 7</a> might beg to differ, but more on that then.)</p>
<p class="center">****</p>
<p class="indent">Here we must pause to introduce some critical terminology. I know, not what you want to read, but it’s essential to all that follows. Artificial intelligence makes frequent use of vectors and matrices (singular “matrix”). A <em>vector</em> is a string of numbers treated as a single entity. For example, the four measurements of each iris flower mean we can represent the flower as a string of four numbers, say, (4.5, 2.3, 1.3, 0.3). The flower described by this vector has a sepal length of 4.5 cm, sepal width of 2.3 cm, petal length of 1.3 cm, and petal width of 0.3 cm. By grouping these measurements together, we can refer to them as a single entity.</p>
<p class="indent">The number of elements in a vector determines its dimensionality; for example, the iris dataset uses four-dimensional vectors, the four measurements of the flower. AI often works with inputs that have hundreds or even thousands of dimensions. If the input is an image, every pixel of that image is one dimension, meaning a small 28-pixel-square image becomes an input vector of 28×28, or 784 dimensions. The concept is the same in 3 dimensions or 33,000 dimensions: it remains a string of numbers treated as a single entity. But an image has rows and columns, making it a two-dimensional array of numbers, not a string. Two-dimensional arrays of numbers are <em>matrices</em>. In machine learning, we often represent datasets as matrices, where the rows are vectors representing the elements of the dataset, like an iris <span epub:type="pagebreak" id="page_10"/>flower, and the columns are the measurements. For example, the first five flowers in the iris dataset form the following matrix:</p>
<div class="image1"><img alt="Image" height="162" src="../images/math10.jpg" width="188"/></div>
<p class="indent">Each row is a flower. Notice that the first row matches the vector example. The remaining rows list the measurements for other flowers.</p>
<p class="indent">While you’re reading, keep these thoughts in the back of your mind:</p>
<ul>
<li class="noindent">Vectors are strings of numbers often representing measurements in a dataset.</li>
<li class="noindent">Matrices are two-dimensional arrays of numbers often representing datasets (stacks of vectors).</li>
</ul>
<p class="indent">As we continue our exploration of AI, the differences between vectors and matrices will come into focus. Now, let’s return to our story.</p>
<p class="center">****</p>
<p class="indent">The inputs to a model are its <a href="glossary.xhtml#glo42"><em>features</em></a>. Our iris flower dataset has two features, the petal’s length and width, which are grouped into <a href="glossary.xhtml#glo43"><em>feature vectors</em></a> (or <em>samples</em>). A single feature vector serves as the model’s input. A binary model’s output is typically a number relating to the model’s belief that the input belongs to class 1. For our example, we’ll give the model a feature vector consisting of two features and expect an output that lets us decide whether we should call the input <em>I. versicolor</em>. If not, we declare the input to be <em>I. setosa</em> because we <em>assume</em> that inputs will always be one or the other.</p>
<p class="indent">Machine learning etiquette states that we should test our model; otherwise, how will we know it’s working? You might think it’s working when it gets all the training samples right, but experience has taught practitioners this isn’t always the case. The proper way to test a model is to keep some of the labeled training data to use after training. The model’s performance on this held-out test dataset better indicates how well the model has learned. We’ll use 80 labeled samples for training and keep 20 of them for testing, making sure that both the training and test sets contain an approximately even mix of both classes (flower types). This is also essential in practice, as far as possible. If we never show the model examples of a particular class of input, how can it learn to distinguish that class from others?</p>
<p class="indent">Using a held-out test set to judge the performance of a model isn’t just etiquette. It addresses a foundational issue in machine learning: generalization. Some machine learning models follow a process quite similar to a widely used approach known as <em>optimization</em>. Scientists and engineers use optimization to fit measured data to known functions; machine learning models also use optimization to condition their parameters, but the goal is <span epub:type="pagebreak" id="page_11"/>different. Fitting data to a function, like a line, seeks to create the best possible <em>fit</em>, or the line that best explains the measured data. In machine learning, we instead want a model that learns the general characteristics of the training data to <em>generalize</em> to new data. That’s why we evaluate the model with the held-out test set. To the model, the test set contains new, unseen data it didn’t use to modify its parameters. The model’s performance on the test set is a clue to its generalization abilities.</p>
<p class="indent">Our example has two input features, meaning the feature vectors are two-dimensional. Since we have two dimensions, we can opt to make a plot of the training dataset. (If we have two or three features in a feature vector, we can plot the feature vectors. However, most feature vectors have hundreds to thousands of features. I don’t know about you, but I can’t visualize a thousand-dimensional space.)</p>
<p class="indent"><a href="ch01.xhtml#ch01fig02">Figure 1-2</a> displays the two-dimensional iris training data; the <em>x</em>-axis is petal length, and the <em>y</em>-axis is petal width. The circles correspond to instances of <em>I. setosa</em> and the squares <em>I. versicolor</em>. Each circle or square represents a single training sample, the petal length and width for a specific flower. To place each point, find the petal length on the <em>x</em>-axis and the petal width on the <em>y</em>-axis. Then, move up from the <em>x</em>-axis and to the right from the <em>y</em>-axis. Where your fingers meet is the point representing that flower. If the flower is <em>I. setosa</em>, make the point a circle; otherwise, make it a square.</p>
<div class="image"><img alt="Image" height="375" id="ch01fig02" src="../images/ch01fig02.jpg" width="501"/></div>
<p class="figcap"><em>Figure 1-2: The iris training data</em></p>
<p class="indent">The plot in <a href="ch01.xhtml#ch01fig02">Figure 1-2</a> shows the <em>feature space</em> of the training set. In this case, we can visualize the training set directly, because we only have two features. When that’s not possible, all is not lost. Advanced algorithms exist that allow us to make plots like <a href="ch01.xhtml#ch01fig02">Figure 1-2</a> where the points in two or three dimensions reflect the distribution of the samples in the much higher-dimensional space. Here, the word <em>space</em> means much the same as it does in everyday parlance.</p>
<p class="indent"><span epub:type="pagebreak" id="page_12"/>Look carefully at <a href="ch01.xhtml#ch01fig02">Figure 1-2</a>. Does anything jump out at you? Are the different classes mixed or well separated? Every circle inhabits the lower-left corner of the plot, while all of the squares are in the upper right. There is no overlap between the classes, meaning they are entirely separate in the feature space.</p>
<p class="indent">How can we use this fact to make a <a href="glossary.xhtml#glo15"><em>classifier</em></a>, a model that classifies iris flowers? (While <a href="glossary.xhtml#glo69"><em>model</em></a> is the more general term, as not all models place their inputs into categories, when they do, use the term <em>classifier</em>.)</p>
<p class="indent">We have many model types to choose from for our classifier, including <a href="glossary.xhtml#glo28"><em>decision trees</em></a>, which generate a series of yes/no questions related to the features used to decide the class label to output for a given input. When the questions are laid out visually, they form a structure reminiscent of an upside-down tree. Think of a decision tree as a computer-generated version of the game <em>20 Questions</em>.</p>
<p class="indent">Even though we have two features, petal length and petal width, we can classify new iris flowers by asking a single question: is the petal length less than 2.5 cm? If the answer is “yes,” then return class 0, <em>I. setosa</em>; otherwise, return class 1, <em>I. versicolor</em>. To classify the training data correctly, we need only the answer to this simple question.</p>
<p class="indent">Did you catch what I did just now? I said that the question correctly classifies all the <em>training</em> data. What about the 20 test samples we didn’t use? Is our single-question classifier sufficient to give each of them the correct label? In practice, that’s what we want to know, and that is what we would report as the classifier’s performance.</p>
<p class="indent"><a href="ch01.xhtml#ch01fig03">Figure 1-3</a> shows the training data again, along with the test data we didn’t use to make our single-question classifier. The solid circles and squares represent the test data.</p>
<div class="image"><img alt="Image" height="501" id="ch01fig03" src="../images/ch01fig03.jpg" width="668"/></div>
<p class="figcap"><em>Figure 1-3: The iris training data with the held-out test data (solid)</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_13"/>None of the test data violates our rule; we still get correct class labels by asking if the petal length is less than 2.5 cm. Therefore, our model is perfect; it makes no mistakes. Congratulations, you just created your first machine learning model!</p>
<p class="indent">We should be happy, but not too happy. Let’s repeat this exercise, replacing <em>I. setosa</em> with the remaining iris species, <em>I. virginica</em>. This leads to <a href="ch01.xhtml#ch01fig04">Figure 1-4</a>, where the triangles are instances of <em>I. virginica</em>.</p>
<div class="image"><img alt="Image" height="375" id="ch01fig04" src="../images/ch01fig04.jpg" width="501"/></div>
<p class="figcap"><em>Figure 1-4: The new iris training data</em></p>
<p class="indent">Hmm, things are not as clear-cut now. The obvious gap between the classes is gone, and they overlap.</p>
<p class="indent">I trained a decision tree using this new iris dataset. As before, there were 80 samples for training and 20 held back for testing. This time, the model wasn’t perfect. It correctly labeled 18 of the 20 samples, for an accuracy of 9 out of 10, or 90 percent. This roughly means that when this model assigns a flower to a particular class, there is a 90 percent chance it’s correct. The previous sentence, to be rigorous, needs careful clarification, but for now, you get the idea—machine learning models are not always perfect; they (quite frequently) make mistakes.</p>
<p class="indent"><a href="ch01.xhtml#ch01fig05">Figure 1-5</a> shows the learned decision tree. Begin at the top, which is the root, and answer the question in that box. If the answer is “yes,” move to the box on the left; otherwise, move to the right. Keep answering and moving in this way until you arrive at a leaf: a box with no arrows. The label in this box is assigned to the input.</p>
<div class="image"><img alt="Image" height="253" id="ch01fig05" src="../images/ch01fig05.jpg" width="503"/></div>
<p class="figcap"><em>Figure 1-5: The decision tree for</em> I. virginica <em>versus</em> I. versicolor</p>
<p class="indent"><span epub:type="pagebreak" id="page_14"/>The first decision tree classifier was trivial, as the answer to a single question was sufficient to decide class membership. The second decision tree classifier is more common. Most machine learning models are not particularly simple. Though their operation is comprehensible, understanding why they act as they do is an entirely different matter. Decision trees are among the few model types that readily explain themselves. For any input, the path traversed from root to leaf in <a href="ch01.xhtml#ch01fig05">Figure 1-5</a> explains in detail why the input received a particular label. The neural networks behind modern AI are not so transparent.</p>
<p class="center">****</p>
<p class="indent">For a model to perform well “in the wild,” meaning when used in the real world, the data used to train the model must cover the entire range of inputs that the model might encounter. For example, say we want a model to identify pictures of dogs, and our training set contains images of only dogs and parrots. While the model performs well on our held-out test set, which also includes pictures of dogs and parrots, what might happen when we deploy the model and it comes across a picture of a wolf? Intuitively, we might expect the model to say “it’s a dog,” just as a small child might before they learn what a wolf is. This is precisely what most machine learning models would do.</p>
<p class="indent">To illustrate this, let’s try an experiment. A popular dataset used by all AI researchers consists of tens of thousands of small images containing handwritten digits, 0 through 9. It goes by the uninspiring name of MNIST (Modified NIST) because it was derived in the late 1990s from a dataset constructed by the National Institute of Standards and Technology (NIST), the division of the United States Department of Commerce tasked with implementing all manner of standards for just about everything in the commercial and industrial realm.</p>
<p class="indent"><a href="ch01.xhtml#ch01fig06">Figure 1-6</a> presents some typical MNIST digit samples. Our goal is to build a neural network that learns to identify the digits 0, 1, 3, and 9. We can train neural networks without knowing how they work because of powerful, open source toolkits like scikit-learn that are available to everyone. On the one hand, this democratizes AI; on the other, a little knowledge is often a dangerous thing. Models may appear to be good when they’re flawed in <span epub:type="pagebreak" id="page_15"/>reality, and lack of knowledge about how the models work might prevent us from realizing that fact before it’s too late.</p>
<div class="image"><img alt="Image" height="56" id="ch01fig06" src="../images/ch01fig06.jpg" width="488"/></div>
<p class="figcap"><em>Figure 1-6: Sample MNIST digits</em></p>
<p class="indent">After the classifier is trained, we’ll throw it a few curveballs by handing it images of fours and sevens—inputs the AI never saw during training. What might the model do with such inputs?</p>
<p class="indent">I trained the digits model using an open source toolkit. For now, all we need to know about the dataset is that the input feature vectors are unraveled digit images; the first row of pixels is followed by the second row, then the third row, and so on, until the entire image is unraveled into one long vector, a string of numbers. The digit images are 28×28 pixels, making the feature vector 784 numbers long. We’re asking the neural network to learn about things in a 784-dimensional space, rather than the simple 2-dimensional space we used previously, but machine learning is up to the challenge.</p>
<p class="indent">The training set used to condition the neural network model contained 24,745 samples, roughly 6,000 of each digit type (0, 1, 3, and 9). This is likely enough to fairly represent the types of digits the model might encounter when used, but we need to try it to know. AI is a largely empirical science.</p>
<p class="indent">The held-out test set, also containing the digits 0, 1, 3, and 9, had 4,134 samples (about 1,000 for each digit).</p>
<p class="indent">We’ll use a <a href="glossary.xhtml#glo19"><em>confusion matrix</em></a>, a two-dimensional table of numbers, to evaluate the model. Confusion matrices are the most common way to evaluate a model because they show how it behaves on the test data.</p>
<p class="indent">In this case, the confusion matrix for our digit classifier is shown in <a href="ch01.xhtml#ch01tab1">Table 1-1</a>.</p>
<p class="tabcap" id="ch01tab1"><strong>Table 1-1:</strong> The Digit Classifier Confusion Matrix</p>
<div class="bqparan">
<table class="all">
<colgroup>
<col style="width:10%"/>
<col style="width:20%"/>
<col style="width:20%"/>
<col style="width:25%"/>
<col style="width:25%"/>
</colgroup>
<thead>
<tr>
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"> </p></th>
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"><strong>0</strong></p></th>
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"><strong>1</strong></p></th>
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"><strong>3</strong></p></th>
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"><strong>9</strong></p></th>
</tr>
</thead>
<tbody>
<tr>
<td class="gray" style="vertical-align: top"><p class="noindent"><strong>0</strong></p></td>
<td class="gray" style="vertical-align: top"><p class="noindent">978</p></td>
<td class="gray" style="vertical-align: top"><p class="noindent">0</p></td>
<td class="gray" style="vertical-align: top"><p class="noindent">1</p></td>
<td class="gray" style="vertical-align: top"><p class="noindent">1</p></td>
</tr>
<tr>
<td style="vertical-align: top"><p class="noindent-tab"><strong>1</strong></p></td>
<td style="vertical-align: top"><p class="noindent-tab">2</p></td>
<td style="vertical-align: top"><p class="noindent-tab">1,128</p></td>
<td style="vertical-align: top"><p class="noindent-tab">3</p></td>
<td style="vertical-align: top"><p class="noindent-tab">2</p></td>
</tr>
<tr>
<td class="gray" style="vertical-align: top"><p class="noindent"><strong>3</strong></p></td>
<td class="gray" style="vertical-align: top"><p class="noindent">5</p></td>
<td class="gray" style="vertical-align: top"><p class="noindent">0</p></td>
<td class="gray" style="vertical-align: top"><p class="noindent">997</p></td>
<td class="gray" style="vertical-align: top"><p class="noindent">8</p></td>
</tr>
<tr>
<td style="vertical-align: top"><p class="noindent-tab"><strong>9</strong></p></td>
<td style="vertical-align: top"><p class="noindent-tab">5</p></td>
<td style="vertical-align: top"><p class="noindent-tab">1</p></td>
<td style="vertical-align: top"><p class="noindent-tab">8</p></td>
<td style="vertical-align: top"><p class="noindent-tab">995</p></td>
</tr>
</tbody>
</table>
</div>
<p class="indent">The matrix rows represent the true labels for the samples given to the model. The columns are the model’s responses. The values in the table are counts, the number of times each possible combination of input class and model-assigned label happened.</p>
<p class="indent">For example, the first row represents the zeros in the test set. Of those 980 inputs, the model returned a label of zero for 978 of them, but it said the input was a three once and a nine another time. Therefore, when zero <span epub:type="pagebreak" id="page_16"/>was the input, the model’s output was correct 978 out of 980 times. That’s encouraging.</p>
<p class="indent">Similarly, when the input was a one, the model returned the correct label 1,128 times. It was right 997 times for threes and 995 times for nines. When a classifier is good, the numbers along the diagonal of the confusion matrix from upper left to lower right are high, and there are almost no numbers off that diagonal. Off-diagonal numbers are errors made by the model.</p>
<p class="indent">Overall, the digits model is 99 percent accurate. We have a solid, well-performing model—that is, if we can ensure that all inputs to the model are indeed a 0, 1, 3, or 9. But what if they aren’t?</p>
<p class="indent">I handed the model 982 fours. The model replied like this:</p>
<table class="table3">
<colgroup>
<col style="width:25%"/>
<col style="width:25%"/>
<col style="width:25%"/>
<col style="width:25%"/>
</colgroup>
<thead>
<tr>
<th class="table-h1aa-n" style="vertical-align: top"><p class="noindent-tab-c"><strong>0</strong></p></th>
<th class="table-h1aa-n" style="vertical-align: top"><p class="noindent-tab-c"><strong>1</strong></p></th>
<th class="table-h1aa-n" style="vertical-align: top"><p class="noindent-tab-c"><strong>3</strong></p></th>
<th class="table-h1aa-n" style="vertical-align: top"><p class="noindent-tab-c"><strong>9</strong></p></th>
</tr>
</thead>
<tbody>
<tr>
<td class="table-h1aa-na" style="vertical-align: top"><p class="noindent-tab-c">48</p></td>
<td class="table-h1aa-na" style="vertical-align: top"><p class="noindent-tab-c">9</p></td>
<td class="table-h1aa-na" style="vertical-align: top"><p class="noindent-tab-c">8</p></td>
<td class="table-h1aa-na" style="vertical-align: top"><p class="noindent-tab-c">917</p></td>
</tr>
</tbody>
</table>
<p class="indent">In other words, the model returned a label of 9 for 917 of the 982 fours, a label of 1 for 48 fours, and labels of 1 or 3 for the rest. How about sevens?</p>
<table class="table3">
<colgroup>
<col style="width:25%"/>
<col style="width:25%"/>
<col style="width:25%"/>
<col style="width:25%"/>
</colgroup>
<thead>
<tr>
<th class="table-h1aa-n" style="vertical-align: top"><p class="noindent-tab-c"><strong>0</strong></p></th>
<th class="table-h1aa-n" style="vertical-align: top"><p class="noindent-tab-c"><strong>1</strong></p></th>
<th class="table-h1aa-n" style="vertical-align: top"><p class="noindent-tab-c"><strong>3</strong></p></th>
<th class="table-h1aa-n" style="vertical-align: top"><p class="noindent-tab-c"><strong>9</strong></p></th>
</tr>
</thead>
<tbody>
<tr>
<td class="table-h1aa-na" style="vertical-align: top"><p class="noindent-tab-c">19</p></td>
<td class="table-h1aa-na" style="vertical-align: top"><p class="noindent-tab-c">20</p></td>
<td class="table-h1aa-na" style="vertical-align: top"><p class="noindent-tab-c">227</p></td>
<td class="table-h1aa-na" style="vertical-align: top"><p class="noindent-tab-c">762</p></td>
</tr>
</tbody>
</table>
<p class="indent">The model still favored calling sevens nines, but it often called them threes as well. Neural networks are loath to give up their secrets when explaining their actions, but in this case, of the 227 sevens labeled as threes, 47 of them were European-style sevens with a slash. A random sample of 227 sevens from the entire dataset turned up only 24 European-style sevens. The comparison isn’t rigorous mathematically, but it hints that sevens with a slash are often close enough to a three to fool the model.</p>
<p class="indent">The model was never taught to recognize fours or sevens, so it did the next best thing and placed them in a nearby category. Depending on how they’re written, people might sometimes confuse fours and sevens for nines, for example. The model is making the kind of mistakes people make, which is interesting—but, more significantly, the model is poor because it wasn’t trained on the full range of inputs it might encounter. It has no way of saying “I don’t know,” and getting a model to reliably say this can be tricky.</p>
<p class="indent">This is a simple exercise, but the implications are profound. Instead of digits, what if the model was looking for cancer in medical images but was never trained to recognize an important category of lesion or all the forms that lesion might take? A properly constructed and comprehensive dataset might mean the difference between life and death.</p>
<p class="center">****</p>
<p class="indent">We can also think about the digits example in terms of interpolation and extrapolation. <em>Interpolation</em> approximates within the range of known data, and <em>extrapolation</em> goes beyond known data.</p>
<p class="indent">For the digits example, interpolation might refer to encountering a tilted zero in the wild when none of the zeros in the training set were particularly tilted. The model must interpolate, in a sense, to respond correctly. <span epub:type="pagebreak" id="page_17"/>Extrapolation is more like classifying a zero with a slash through it, which is something unseen during training time. To better understand these terms, let’s model the world population from 1950 through 2020.</p>
<p class="indent">First, we’ll fit a line to the data from 1950 through 1970. Fitting a line is a form of curve fitting; think of it as machine learning’s less sophisticated cousin. To fit a line, find two numbers: the slope and the intercept. The slope tells us how steep the line is. If the slope is positive, the line is increasing as we move from left to right along the <em>x</em>-axis of a graph. A negative slope means the line decreases as we move along the <em>x</em>-axis. The intercept is where the line intersects the <em>y</em>-axis; that is, the value when the input is zero.</p>
<p class="indent">To fit a line, we use an algorithm to find the slope and intercept that best characterize the data (here, world population from 1950 through 1970). <a href="ch01.xhtml#ch01fig07">Figure 1-7</a> shows a plot of the line and the actual populations by year, denoted by plus signs. The line passes through or near to most of the plus signs, so the fit is reasonable. Notice that the population is in billions.</p>
<div class="image"><img alt="Image" height="374" id="ch01fig07" src="../images/ch01fig07.jpg" width="501"/></div>
<p class="figcap"><em>Figure 1-7: World population from 1950 through 1970</em></p>
<p class="indent">Once we have the line, we can use the slope and intercept to estimate the population for any year. Estimating for years between 1950 and 1970 is interpolating, because we used data from that range of years to create the line. If we estimate populations for years before 1950 or after 1970, we are extrapolating. <a href="ch01.xhtml#ch01tab2">Table 1-2</a> shows our results when interpolating.</p>
<p class="tabcap" id="ch01tab2"><strong>Table 1-2:</strong> Interpolating the Population Between 1950 and 1970</p>
<div class="bqparan">
<table class="all">
<colgroup>
<col style="width:30%"/>
<col style="width:45%"/>
<col style="width:25%"/>
</colgroup>
<thead>
<tr>
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"><strong>Year</strong></p></th>
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"><strong>Interpolated</strong></p></th>
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"><strong>Actual</strong></p></th>
</tr>
</thead>
<tbody>
<tr>
<td class="gray" style="vertical-align: top"><p class="noindent">1954</p></td>
<td class="gray" style="vertical-align: top"><p class="noindent">2.71</p></td>
<td class="gray" style="vertical-align: top"><p class="noindent">2.72</p></td>
</tr>
<tr>
<td style="vertical-align: top"><p class="noindent-tab">1960</p></td>
<td style="vertical-align: top"><p class="noindent-tab">3.06</p></td>
<td style="vertical-align: top"><p class="noindent-tab">3.03</p></td>
</tr>
<tr>
<td class="gray" style="vertical-align: top"><p class="noindent">1966</p></td>
<td class="gray" style="vertical-align: top"><p class="noindent">3.41</p></td>
<td class="gray" style="vertical-align: top"><p class="noindent">3.41</p></td>
</tr>
</tbody>
</table>
</div>
<p class="indent"><span epub:type="pagebreak" id="page_18"/>The interpolated population values are quite close to the actual population values, meaning the model (here the line fit to the data) is doing well. Now, let’s extrapolate to dates outside the fit range, as shown in <a href="ch01.xhtml#ch01tab3">Table 1-3</a>.</p>
<p class="tabcap" id="ch01tab3"><strong>Table 1-3:</strong> Extrapolating the Population After 1970</p>
<div class="bqparan">
<table class="all">
<colgroup>
<col style="width:30%"/>
<col style="width:35%"/>
<col style="width:35%"/>
</colgroup>
<thead>
<tr>
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"><strong>Year</strong></p></th>
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"><strong>Extrapolated</strong></p></th>
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"><strong>Actual</strong></p></th>
</tr>
</thead>
<tbody>
<tr>
<td class="gray" style="vertical-align: top"><p class="noindent">1995</p></td>
<td class="gray" style="vertical-align: top"><p class="noindent">5.10</p></td>
<td class="gray" style="vertical-align: top"><p class="noindent">5.74</p></td>
</tr>
<tr>
<td style="vertical-align: top"><p class="noindent-tab">2010</p></td>
<td style="vertical-align: top"><p class="noindent-tab">5.98</p></td>
<td style="vertical-align: top"><p class="noindent-tab">6.96</p></td>
</tr>
<tr>
<td class="gray" style="vertical-align: top"><p class="noindent">2020</p></td>
<td class="gray" style="vertical-align: top"><p class="noindent">6.56</p></td>
<td class="gray" style="vertical-align: top"><p class="noindent">7.79</p></td>
</tr>
</tbody>
</table>
</div>
<p class="indent">The difference between the extrapolated population values and the actual population is increasing with each year. The model isn’t doing well. Plotting the full range from 1950 through 2020 reveals the problem; see <a href="ch01.xhtml#ch01fig08">Figure 1-8</a>.</p>
<div class="image"><img alt="Image" height="372" id="ch01fig08" src="../images/ch01fig08.jpg" width="501"/></div>
<p class="figcap"><em>Figure 1-8: World population from 1950 through 2020</em></p>
<p class="indent">As time goes by, the fit line becomes increasingly wrong because the data is not linear after all. That is, the rate of growth is not constant and doesn’t follow a straight line.</p>
<p class="indent">When extrapolating, we might have reason to believe that the data will continue to fit the line; if that’s a valid assumption, then the line will continue to be a good fit. However, in the real world, we usually have no such assurance. So, as a slogan, we might say interpolation good, extrapolation bad.</p>
<p class="indent">Fitting a line to some data is an example of <em>curve fitting</em>. What is true for curve fitting is also true for AI. The handwritten digits model did well when given inputs close to the data it was trained to recognize. The digits in the test data were all instances of 0, 1, 3, and 9, so the test data was like the training data. The two datasets are from the same <em>distribution</em>, and the <span epub:type="pagebreak" id="page_19"/>same data-generating process created both. We can therefore claim that the model was, in a way, interpolating in those cases. However, when we forced the model to make decisions about fours and sevens, we were extrapolating by having the model make decisions about data it never saw during training.</p>
<p class="indent">It bears repeating: interpolation good, extrapolation bad. Bad datasets lead to bad models; good datasets lead to good models, which behave badly when forced to extrapolate. And, for good measure: all models are wrong, but some are useful.</p>
<p class="center">****</p>
<p class="indent">Along the same lines of Hilaire Belloc’s 1907 book <em>Cautionary Tales for Children</em>—an amusing and somewhat horrifying look at foolish things children do that could lead to an unfortunate end—let’s examine some cautionary tales that AI practitioners should be aware of when training, testing, and, most of all, deploying models.</p>
<p class="indent">In 2016, I attended a conference talk where the presenter demonstrated research into understanding why a neural network chooses the way it does. This is not yet a solved problem, but progress has been made. In this case, the research marked parts of the input images that influenced the model’s decision.</p>
<p class="indent">The speaker displayed pictures of huskies and wolves and discussed his classifier for differentiating between the two. He showed how well it performed on a test set and asked the audience of machine learning researchers if this was a good model. Many people said yes, but with hesitation because they expected a trap. They were right to be hesitant. The speaker then marked the images to show the parts that the neural network focused on when making its decisions. The model wasn’t paying attention to the dogs or the wolves. Instead, the model noticed that all the wolf training images had snow in the background, while none of the dog images contained snow. The model learned nothing about dogs and wolves but only about snow and no snow. Careless acceptance of the model’s behavior wouldn’t have revealed that fact, and the model might have been deployed only to fail in the wild.</p>
<p class="indent">A similar tale is told of a very early machine learning system from the 1950s or 1960s. This one is likely apocryphal, though I have read a paper from that period that might be the origin of the urban legend. In this case, the images were bird’s-eye views of forests. Some images contained a tank, while others did not.</p>
<p class="indent">A model trained to detect tanks seemed to work well on the training data but failed miserably when set loose in the wild. It was eventually realized that one set of training images had been taken on a sunny day and the other on a cloudy day. The model had learned nothing that its creators assumed it had.</p>
<p class="indent">More recent examples of this phenomenon exist with more advanced machine learning models. Some have even fooled experts into believing the model had learned something fundamental about language or the like when, instead, it had learned extremely subtle correlations in the training data that no human could (easily) detect.</p>
<p class="indent"><span epub:type="pagebreak" id="page_20"/>The word <em>correlation</em> has a strict mathematical meaning, but we capture its essence with the phrase “correlation does not imply causation.” Correlation is when two things are linked so that the occurrence of one implies the occurrence of the other, often in a particular order. More concretely, correlation measures how strongly a change in one thing is associated with a change in another. If both increase, they are positively correlated. If one increases while the other decreases, they are negatively correlated.</p>
<p class="indent">For example, a rooster crows, and the sun comes up. The two events are time-dependent: the rooster first, then the sun. This correlation does not imply causation, as the rooster crowing doesn’t cause the sun to rise, but if such a correlation is observed often enough, the human mind begins to see one as causing the other, even when there is no real evidence of this. Why humans act this way isn’t hard to understand. Evolution favored early humans who made such associations because, sometimes, the associations led to behavior beneficial for survival.</p>
<p class="indent">“Correlation does not imply causation” also applies to AI. The aforementioned models learned to detect things in the training data that correlated with the intended targets (dogs, wolves, tanks) but didn’t learn about the targets themselves. Savvy machine learning practitioners are always on the lookout for such spurious correlations. Using a large and highly diverse dataset for training and testing can defend against this effect, though this isn’t always possible in practice.</p>
<p class="indent">We must ask whether our models have learned what we assume they have. And, as we saw with the MNIST digits, we must ensure that our models have seen all the kinds of inputs they will encounter in the wild—they should interpolate, not extrapolate.</p>
<p class="indent">This matters more than it might initially appear. Google learned this lesson in 2015 when it deployed a feature for Google Photos, wherein the model was insufficiently trained on human faces and made incorrect and inappropriate associations. Bias, in both the generic and social senses, is a real issue in AI.</p>
<p class="indent">Let’s perform another experiment with MNIST digits. This time, the model has a seemingly simple decision to make: is the input digit a nine? The model is the same neural network used previously. If trained on a dataset where every image is either a nine or any other digit except four or seven (that is, no fours or sevens are in the training data), then the model is 99 percent accurate, as the confusion matrix shows:</p>
<table class="table3">
<colgroup>
<col style="width:30%"/>
<col style="width:35%"/>
<col style="width:35%"/>
</colgroup>
<thead>
<tr>
<th class="table-h1aa" style="vertical-align: top"><p class="noindent-tab"> </p></th>
<th class="table-h1ab" style="vertical-align: top"><p class="noindent-tab"><strong>Not 9</strong></p></th>
<th class="table-h1aa" style="vertical-align: top"><p class="noindent-tab"><strong>9</strong></p></th>
</tr>
</thead>
<tbody>
<tr>
<td class="border" style="vertical-align: top"><p class="noindent-tab"><strong>Not 9</strong></p></td>
<td style="vertical-align: top"><p class="noindent-tab">  9,754</p></td>
<td style="vertical-align: top"><p class="noindent-tab">23</p></td>
</tr>
<tr>
<td class="border" style="vertical-align: top"><p class="noindent-tab">        <strong>9</strong></p></td>
<td style="vertical-align: top"><p class="noindent-tab">  38</p></td>
<td style="vertical-align: top"><p class="noindent-tab">1,362</p></td>
</tr>
</tbody>
</table>
<p class="indent">The confusion matrix tells us that the model correctly labeled 9,754 out of 9,777 test images that were not a nine. The model’s label was also correct for 1,362 of the 1,400 nines. While the model performs well on the test set, the set does not contain fours or sevens.</p>
<p class="indent">In this case, the confusion matrix is small because the model has only two classes: nine or not nine. In other words, this is a binary model.</p>
<p class="indent"><span epub:type="pagebreak" id="page_21"/>The 23 in the upper-right corner of the matrix represents 23 times when the input wasn’t a nine, but the model said it was. For a binary model, class 1 is usually considered the class of interest, or the positive class. Therefore, these 23 inputs represent <a href="glossary.xhtml#glo41"><em>false positives</em></a>, because the model said “it’s a nine” when it wasn’t. Similarly, the 38 samples at the lower left are <a href="glossary.xhtml#glo40"><em>false negatives</em></a> because the model said “it’s not a nine” when the input actually was. We want models with no false positives or negatives, but sometimes it’s more important to minimize one than the other.</p>
<p class="indent">For example, if a model is to detect breast cancer in mammograms, a false positive represents a case where the model says, “it might be cancer,” even though it isn’t. That’s scary to hear, but further testing will show that the model was wrong. However, a false negative represents a missed cancer. We might tolerate a model with more false positives if it also has virtually no false negatives, as a false positive is less catastrophic than a false negative. We’re beginning to appreciate how important it is to fully train, characterize, test, and <em>understand</em> our machine learning models.</p>
<p class="center">****</p>
<p class="indent">All right, back to our experiment. The “is it a nine” classifier, like our earlier MNIST model, knows nothing about fours or sevens. When shown fours and sevens, the MNIST model typically called them nines. Will this model do the same? Here’s what I received when I gave the model fours and sevens:</p>
<div class="bqparan">
<table class="table3">
<colgroup>
<col style="width:30%"/>
<col style="width:35%"/>
<col style="width:35%"/>
</colgroup>
<thead>
<tr>
<th class="table-h1aa" style="vertical-align: top"><p class="noindent-tab"> </p></th>
<th class="table-h1ab" style="vertical-align: top"><p class="noindent-tab"><strong>Not 9</strong></p></th>
<th class="table-h1aa" style="vertical-align: top"><p class="noindent-tab"><strong>9</strong></p></th>
</tr>
</thead>
<tbody>
<tr>
<td class="border" style="vertical-align: top"><p class="noindent-tab"><strong>Not 9</strong></p></td>
<td style="vertical-align: top"><p class="noindent-tab">  5,014</p></td>
<td style="vertical-align: top"><p class="noindent-tab">9,103</p></td>
</tr>
</tbody>
</table>
</div>
<p class="indent">The model marked 9,103 of the 14,117 fours and sevens as nines. That’s slightly more than 65 percent, or roughly two out of every three. This mimics the case where we present the model with inputs of a type it was never trained to detect.</p>
<p class="indent">Let’s help the model by adding fours and sevens to the training set. Hopefully, providing examples that say, “It looks like a nine, but it isn’t,” formally known as <em>hard negatives</em>, will improve the model. I made 3 percent of the training data fours and sevens. The overall model was just as accurate as before, 99 percent, and here’s what happened when I gave it fours and sevens it had never seen before:</p>
<div class="bqparan">
<table class="table3">
<colgroup>
<col style="width:30%"/>
<col style="width:35%"/>
<col style="width:35%"/>
</colgroup>
<thead>
<tr>
<th class="table-h1aa" style="vertical-align: top"><p class="noindent-tab"> </p></th>
<th class="table-h1ab" style="vertical-align: top"><p class="noindent-tab"><strong>Not 9</strong></p></th>
<th class="table-h1aa" style="vertical-align: top"><p class="noindent-tab"><strong>9</strong></p></th>
</tr>
</thead>
<tbody>
<tr>
<td class="border" style="vertical-align: top"><p class="noindent-tab"><strong>Not 9</strong></p></td>
<td style="vertical-align: top"><p class="noindent-tab">  9,385</p></td>
<td style="vertical-align: top"><p class="noindent-tab">3,321</p></td>
</tr>
</tbody>
</table>
</div>
<p class="indent">That’s better. Instead of calling two-thirds of four or seven inputs a nine, the model labeled only one in four as a nine. Even a few examples of things that look like the positive class but aren’t can help. If I boost the proportion of fours and sevens in the training set to 18 percent, the model misclassifies fours and sevens less than 1 percent of the time. Because models learn from data, we <em>must</em> use datasets that are as complete as possible so our models interpolate and do not extrapolate.</p>
<div class="note">
<p class="notet"><span epub:type="pagebreak" id="page_22"/><strong><span class="notes">NOTE</span></strong></p>
<p class="notep"><em>To be completely accurate, recent research shows that modern deep learning models are almost always extrapolating, but the more similar the inputs are to the data on which the model was trained, the better the performance, so I feel justified in using the analogy.</em></p>
</div>
<p class="indent">Everyone who seeks to understand, let alone work with, AI must take the warnings about the quality of the data used to train AI models to heart. A 2021 research article published in the journal <em>Nature Machine Intelligence</em> by Michael Roberts et al., “Common Pitfalls and Recommendations for Using Machine Learning to Detect and Prognosticate for COVID-19 Using Chest Radiographs and CT Scans,” is a sobering example. The authors assessed the performance of machine learning models designed to detect COVID-19 in chest X-rays and CT scans, reducing the initial candidate pool of over 2,000 studies (models) to 62 for rigorous testing. In the end, the authors declared <em>none</em> of the models fit for clinical use because of flaws in construction, bias in the datasets, or both.</p>
<p class="indent">Results like these have led to the creation of <a href="glossary.xhtml#glo39"><em>explainable AI</em></a>, a subfield that seeks to give models the ability to explain themselves.</p>
<p class="indent">Look at your data and try to understand, as far as humanly possible, what your model is doing and <em>why</em>.</p>
<p class="center">****</p>
<p class="indent">This chapter’s title, “And Away We Go,” was comedian Jackie Gleason’s tagline. It’s often good to dive into a subject to get an overview before coming back to understand things at a deeper level. In other words, we rush in to get a feel for the topic before exploring more methodically.</p>
<p class="indent">You’ll find the many new terms and concepts introduced in this chapter in the glossary at the end of the book. My goal isn’t for you to understand them all now, let alone retain them, but to plant seeds so that the next time you encounter one of these terms or concepts, you’ll be more likely to think, “Ah, I know that one.” Later chapters reinforce them, and you’ll learn the important ones via repeated exposure.</p>
<p class="indent">There are two categories of takeaways from this chapter. The first has to do with what AI is and its essential pieces. The second is about building intuition about what AI offers and how we should respond.</p>
<p class="indent">AI involves models, as yet nebulous entities we can condition with data to perform some desired task. There are many types of AI models, and this chapter introduced two: decision trees and neural networks. I won’t say much more about decision trees, but neural networks occupy most of the remainder of the book.</p>
<p class="indent">Models are often best thought of as functions, like the mathematical functions you may remember from school or the functions that form the core of most computer programs. Both can be considered black boxes, where something goes in (the input) and something comes out (the output). In AI, the input is a feature vector, a collection of whatever is appropriate for the task at hand. In this chapter, we used two feature vectors: measurements of a flower and images of a handwritten digit.</p>
<p class="indent"><span epub:type="pagebreak" id="page_23"/>Training conditions the model by altering its parameters to make it as accurate as possible. It’s necessary to exercise caution when training most models to learn the general features of the data and not spurious correlations or the minute details of the training set (a concept known as <a href="glossary.xhtml#glo79"><em>overfitting</em></a>, which we’ll discuss in <a href="ch04.xhtml">Chapter 4</a>).</p>
<p class="indent">Proper development of machine learning models means we must have a test set, a collection of known input and output pairs that we do not use when training. We use this set after training to evaluate the model. If the dataset is constructed correctly, the test set provides an idea of how well we can expect the model to perform in the wild.</p>
<p class="indent">The second takeaway relates to what AI offers and how we should respond to it. While AI is powerful, it doesn’t think as we do (though the models of <a href="ch07.xhtml">Chapter 7</a> might disagree). AI lives and dies by data and is only as good as the data we feed to it. If the dataset is biased, the AI is biased. If the dataset neglects to include examples of the types of inputs it will encounter when used, the AI will fail to handle such inputs properly.</p>
<p class="indent">The chapter’s examples warn us to be careful when assuming AI operates as intended. Did the model learn what we wanted it to learn? Was it influenced by correlations in the data that we didn’t notice or, worse still, that we are too limited to discern? Think back to the huskies versus wolves example.</p>
<p class="indent">Because AI is only as good as the data fed to it, it’s on us to make datasets fair and unbiased and to understand what the AI has truly learned without assumptions.</p>
<p class="indent">AI first appeared in the 1950s, so why is it now suddenly everywhere we look? The next chapter answers this question.</p>
<div class="box5">
<p class="boxtitle-c"><strong>KEY TERMS</strong></p>
<p class="noindent">algorithm, artificial intelligence, classifier, class label, confusion matrix, dataset, decision tree, deep learning, explainable AI, feature, feature vector, machine learning, model, neural network, parameters, testing, training<span epub:type="pagebreak" id="page_24"/></p>
</div>
</div></body></html>