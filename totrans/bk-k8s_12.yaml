- en: '10'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '10'
- en: WHEN THINGS GO WRONG
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当事情出错时
- en: '![image](../images/common01.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/common01.jpg)'
- en: So far our installation and configuration of Kubernetes has gone as planned,
    and our controllers have had no problem creating Pods and starting containers.
    Of course, in the real world, it’s rarely that easy. Although showing everything
    that might go wrong with a complex application deployment isn’t possible, we can
    look at some of the most common problems. Most important, we can explore debugging
    tools that will help us diagnose any issue.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们的 Kubernetes 安装和配置进展顺利，控制器在创建 Pods 和启动容器方面没有问题。当然，在现实世界中，事情很少这么简单。虽然无法展示复杂应用部署中可能出现的所有问题，但我们可以看看一些最常见的问题。最重要的是，我们可以探索一些调试工具，帮助我们诊断任何问题。
- en: In this chapter, we’ll look at how to diagnose problems with application containers
    that we deploy on top of Kubernetes. We’ll work our way through the life cycle
    for scheduling and running containers, examining potential problems at each step
    as well as how to diagnose and fix them.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将研究如何诊断在 Kubernetes 上部署的应用容器的问题。我们将循序渐进地了解调度和运行容器的生命周期，检查每个步骤可能出现的问题，以及如何诊断和解决它们。
- en: Scheduling
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 调度
- en: Scheduling is the first activity Kubernetes performs on a Pod and its containers.
    When a Pod is first created, the Kubernetes scheduler assigns it to a node. Normally,
    this happens quickly and automatically, but some issues can prevent scheduling
    from happening successfully.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 调度是 Kubernetes 对 Pod 及其容器执行的第一个操作。当一个 Pod 被创建时，Kubernetes 调度器会将其分配给一个节点。通常，这个过程会很快自动完成，但某些问题可能会阻止调度的成功执行。
- en: No Available Nodes
  id: totrans-7
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 无可用节点
- en: One possibility is that the scheduler simply doesn’t have any nodes available.
    This situation might occur because our cluster doesn’t have any nodes configured
    for regular application containers or because all nodes have failed.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 一种可能性是调度器根本没有可用的节点。这种情况可能是因为我们的集群没有配置任何用于常规应用容器的节点，或者因为所有节点都已失败。
- en: To illustrate the case in which no nodes are available for assignment, let’s
    create a Pod with a *node selector*. A node selector specifies one or more node
    labels that are required for a Pod to be scheduled on that node. Node selectors
    are useful when some nodes in our cluster are different from others (for example,
    when some nodes have newer CPUs with support for more advanced instruction sets
    needed by some of our containers).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明没有可用节点进行分配的情况，我们将创建一个带有 *节点选择器* 的 Pod。节点选择器指定一个或多个节点标签，Pod 必须在匹配这些标签的节点上进行调度。节点选择器在集群中的某些节点与其他节点有所不同时很有用（例如，当一些节点拥有更新的
    CPU，支持容器所需的更高级指令集时）。
- en: '**NOTE**'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*The example repository for this book is at* [https://github.com/book-of-kubernetes/examples](https://github.com/book-of-kubernetes/examples).
    *See “Running Examples” on [page xx](ch00.xhtml#ch00lev1sec2) for details on getting
    set up.*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*本书的示例仓库位于* [https://github.com/book-of-kubernetes/examples](https://github.com/book-of-kubernetes/examples)。*有关设置的详细信息，请参见
    [第 xx 页](ch00.xhtml#ch00lev1sec2) 中的“运行示例”部分。*'
- en: 'We’ll begin with a Pod definition that has a node selector that doesn’t match
    any of our nodes:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一个具有节点选择器的 Pod 定义开始，这个选择器与我们的任何节点都不匹配：
- en: '*nginx-selector.yaml*'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*nginx-selector.yaml*'
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The node selector ➊ tells Kubernetes to assign this Pod only to a node with
    a label called `purpose` whose value is equal to `special`. Even though none of
    our nodes currently match, we can still create this Pod:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 节点选择器 ➊ 告诉 Kubernetes 只将这个 Pod 分配给一个标签为 `purpose` 且值为 `special` 的节点。尽管我们当前没有节点匹配该标签，我们仍然可以创建这个
    Pod：
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'However, Kubernetes is stuck trying to schedule the Pod, because it can’t find
    a matching node:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Kubernetes 在尝试调度 Pod 时遇到了问题，因为它找不到匹配的节点：
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We see a status of `Pending` and a node assignment of `<none>`. This is because
    Kubernetes has not yet scheduled this Pod onto a node.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到的状态是 `Pending`，节点分配为 `<none>`。这是因为 Kubernetes 还没有将这个 Pod 调度到一个节点上。
- en: 'The `kubectl get` command is typically the first command we should run to see
    whether there are issues with a resource we’ve deployed to our cluster. If we
    have an issue, as we do in this case, the next step is to view the detailed status
    and event log using `kubectl describe`:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubectl get` 命令通常是我们应该运行的第一个命令，用于查看我们部署到集群中的资源是否存在问题。如果出现问题，就像在本例中一样，下一步是使用
    `kubectl describe` 查看详细的状态和事件日志：'
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The event log informs us as to exactly what the issue is: the Pod can’t be
    scheduled because none of the nodes matched the selector.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 事件日志告诉我们具体问题所在：Pod 无法调度，因为没有节点匹配选择器。
- en: 'Let’s add the necessary label to one of our nodes:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们向其中一个节点添加必要的标签：
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We first list the three nodes we have available and then apply the necessary
    label to one of them. As soon as we apply this label, Kubernetes can now schedule
    the Pod:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先列出可用的三个节点，然后将必要的标签应用到其中一个节点上。一旦我们应用了这个标签，Kubernetes 现在可以调度该 Pod：
- en: '[PRE5]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As expected, the Pod was scheduled onto the node where we applied the label.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，Pod 被调度到了我们应用了标签的节点上。
- en: This example, like the others we’ll see in this chapter, illustrates debugging
    in Kubernetes. After we’ve created the resources that we need, we query the cluster
    state to make sure the actual deployment of those resources was successful. When
    we find issues, we can correct those issues and our resources will be started
    as desired without having to reinstall our application components.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例，与本章中我们将看到的其他示例一样，展示了如何在 Kubernetes 中进行调试。在我们创建了所需的资源后，我们查询集群状态，以确保这些资源的实际部署成功。当我们发现问题时，可以纠正这些问题，我们的资源将按照预期启动，而无需重新安装我们的应用组件。
- en: 'Let’s clean up this NGINX Pod:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们清理一下这个 NGINX Pod：
- en: '[PRE6]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let’s also remove the label from the node. We remove the label by appending
    a minus sign to it to identify it:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也从节点中移除标签。我们通过在标签后添加一个减号来移除它，以便标识：
- en: '[PRE7]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We’ve covered one issue with the scheduler, but there’s still another we need
    to look at.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经解决了一个关于调度器的问题，但还有另一个问题需要我们关注。
- en: Insufficient Resources
  id: totrans-34
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 资源不足
- en: When choosing a node to host a Pod, the scheduler also considers the resources
    that are available on each node and the resources the Pod requires. We explore
    resource limits in detail in [Chapter 14](ch14.xhtml#ch14); for now it’s enough
    to know that each container in a Pod can request the resources it needs, and the
    scheduler will ensure that it is scheduled onto a node that has those resources
    available. Of course, if there aren’t any nodes with enough room, the scheduler
    won’t be able to schedule the Pod. Instead the Pod will wait in a `Pending` state.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择节点来托管 Pod 时，调度器还会考虑每个节点上可用的资源以及 Pod 所需的资源。我们在[第14章](ch14.xhtml#ch14)中详细探讨了资源限制；目前只需要知道，每个容器都可以请求它所需的资源，调度器将确保它被调度到一个有这些资源可用的节点。当然，如果没有节点有足够的资源，调度器将无法调度该
    Pod。相反，Pod 会处于 `Pending` 状态等待。
- en: 'Let’s look at an example Pod definition to illustrate this:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个示例 Pod 定义来说明这一点：
- en: '*sleep-multiple.yaml*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*sleep-multiple.yaml*'
- en: '[PRE8]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In this YAML definition, we create two containers in the same Pod. Each container
    requests two CPUs. Because all of the containers in a Pod must be on the same
    host in order to share some Linux namespace types (especially the network namespace
    so that they can use `localhost` for communication), the scheduler needs to find
    a single node with four CPUs available. In our small cluster, that can’t happen,
    as we can see if we try to deploy the Pod:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个 YAML 定义中，我们在同一个 Pod 中创建了两个容器。每个容器请求两个 CPU。因为 Pod 中的所有容器必须在同一个主机上，以便共享某些
    Linux 命名空间类型（尤其是网络命名空间，这样它们可以使用 `localhost` 进行通信），所以调度器需要找到一个有四个 CPU 可用的单一节点。在我们的一个小集群中，这是不可能的，正如我们尝试部署该
    Pod 时所看到的那样：
- en: '[PRE9]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As before, `kubectl describe` gives us the event log that reveals the issue:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如之前所述，`kubectl describe` 给出了事件日志，揭示了问题：
- en: '[PRE10]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Notice that it doesn’t matter how heavily loaded our nodes actually are:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，无论我们的节点实际负载有多重，都无关紧要：
- en: '[PRE11]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Nor does it matter how much CPU our containers will actually use. The scheduler
    allocates Pods purely based on what it requested; this way, we don’t suddenly
    overwhelm a CPU when load increases.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 容器实际使用多少 CPU 也无关紧要。调度器完全根据请求来分配 Pod；通过这种方式，当负载增加时，我们不会突然让 CPU 超负荷。
- en: 'We can’t magically provide our nodes with more CPUs, so to get this Pod scheduled,
    we’re going to need to specify a lower CPU usage for our two containers. Let’s
    use a more sensible figure of 0.1 CPU:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能神奇地为我们的节点提供更多 CPU，因此，要让这个 Pod 被调度，我们需要为两个容器指定较低的 CPU 使用量。我们可以使用一个更合理的值：0.1
    CPU：
- en: '*sleep-sensible.yaml*'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*sleep-sensible.yaml*'
- en: '[PRE12]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The value `100m` ➊ equates to “one hundred millicpu” or one-tenth (0.1) of a
    CPU.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 值 `100m` ➊ 等同于“100 毫 CPU”或 CPU 的十分之一 (0.1)。
- en: 'Even though this is a separate file, it declares the same resource, so Kubernetes
    will treat it as an update. However, if we try to apply this as a change to the
    existing Pod, it will fail:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 即使这是一个单独的文件，它声明了相同的资源，因此 Kubernetes 会将其视为更新。然而，如果我们尝试将其应用为对现有 Pod 的更改，它将失败：
- en: '[PRE13]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We are not allowed to change the resource request of an existing Pod, which
    makes sense given that a Pod is allocated to a node only once on creation, and
    a resource usage change might cause the node to be overly full.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不允许更改已存在 Pod 的资源请求，这也合乎逻辑，因为 Pod 在创建时只会分配给节点一次，改变资源使用可能会导致节点过载。
- en: 'If we were using a controller such as a Deployment, the controller could handle
    replacing the Pods for us. Because we created a Pod directly, we need to manually
    delete and then re-create it:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用的是像 Deployment 这样的控制器，控制器可以为我们处理替换 Pods 的操作。由于我们是直接创建 Pod，因此需要手动删除然后重新创建它：
- en: '[PRE14]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Our new Pod has no trouble with node allocation:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的新 Pod 在节点分配上没有问题：
- en: '[PRE15]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'And if we run `kubectl describe` on the node, we can see how our new Pod has
    been allocated some of the node’s CPU:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在节点上运行 `kubectl describe`，可以看到我们的新 Pod 已经分配到节点的一些 CPU：
- en: '[PRE16]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Be sure to use the correct node name for the node where your Pod was deployed.
    Because our Pod has two containers, each requesting `100m`, its total request
    is `200m` ➊.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 请确保使用正确的节点名称，指向部署 Pod 的节点。因为我们的 Pod 有两个容器，每个请求 `100m`，所以它的总请求为 `200m` ➊。
- en: 'Let’s finish by cleaning up this Pod:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们最后清理这个 Pod：
- en: '[PRE17]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Other errors can prevent a Pod from being scheduled, but these are the most
    common. Most important, the commands we used here apply in all cases. First, use
    `kubectl get` to determine the Pod’s current status, followed by `kubectl describe`
    to view the event log. These two commands are always a good first step when something
    doesn’t seem to be working properly.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 其他错误可能会阻止 Pod 被调度，但这些是最常见的问题。最重要的是，我们在这里使用的命令适用于所有情况。首先，使用 `kubectl get` 来确定
    Pod 的当前状态，然后使用 `kubectl describe` 查看事件日志。这两个命令在出现问题时总是一个不错的起点。
- en: Pulling Images
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 拉取镜像
- en: 'After a Pod is scheduled onto a node, the local `kubelet` service interacts
    with the underlying container runtime to create an isolated environment and start
    containers. However, there’s still one application misconfiguration that can cause
    our Pod to become stuck in the `Pending` phase: inability to pull the container
    image.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 被调度到节点上后，本地的 `kubelet` 服务会与底层的容器运行时交互，创建一个隔离的环境并启动容器。然而，仍然有一个应用配置错误可能导致我们的
    Pod 停留在 `Pending` 阶段：无法拉取容器镜像。
- en: 'Three main issues can prevent the container runtime from pulling an image:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 三个主要问题可能会导致容器运行时无法拉取镜像：
- en: Failure to connect to the container image registry
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无法连接到容器镜像注册表
- en: Authorization issue with the requested image
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请求的镜像授权问题
- en: Image is missing from the registry
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 镜像在注册表中缺失
- en: As we described in [Chapter 5](ch05.xhtml#ch05), an image registry is a web
    server. Often, the image registry is outside the cluster, and the nodes need to
    be able to connect to an external network or the internet to reach the registry.
    Additionally, most registries support publishing private images that require authentication
    and authorization to access. And, of course, if there is no image published under
    the name we specify, the container runtime is not going to be able to pull it
    from the registry.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在 [第 5 章](ch05.xhtml#ch05) 中所描述的，镜像注册表是一个 Web 服务器。通常，镜像注册表位于集群外部，节点需要能够连接到外部网络或互联网才能访问注册表。此外，大多数注册表支持发布需要身份验证和授权才能访问的私有镜像。当然，如果没有发布我们指定名称的镜像，容器运行时将无法从注册表拉取它。
- en: 'All of these errors behave the same way in our Kubernetes cluster, with differences
    only in the message in the event log, so we’ll need to explore only one of them.
    We’ll look at what is probably the most common issue: a missing image caused by
    a typo in the image name.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些错误在我们的 Kubernetes 集群中表现相同，仅在事件日志中的信息有所不同，因此我们只需要探索其中一个错误。我们将重点讨论可能最常见的问题：由于镜像名称中的拼写错误导致镜像缺失。
- en: 'Let’s try to create a Pod using this YAML file:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用这个 YAML 文件创建一个 Pod：
- en: '*nginx-typo.yaml*'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*nginx-typo.yaml*'
- en: '[PRE18]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Because there is no image in Docker Hub called `nginz`, it won’t be possible
    to pull this image. Let’s explore what happens when we add this resource to the
    cluster:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 因为在 Docker Hub 中没有名为 `nginz` 的镜像，所以无法拉取这个镜像。让我们看看将此资源添加到集群时会发生什么：
- en: '[PRE19]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Our Pod has status `ImagePullBackOff`, which immediately signals two things.
    First, this Pod is not yet getting to the point at which the containers are running,
    because it has not yet pulled the container images. Second, as with all errors,
    Kubernetes will continue attempting the action, but it will use a *back-off* algorithm
    to avoid overwhelming our cluster’s resources. Pulling an image involves reaching
    out over the network to communicate with the image registry, and it would be rude
    and a waste of network bandwidth to flood the registry with many requests in a
    short amount of time. Moreover, the cause of the failure may be transient, so
    the cluster will keep trying in hopes that the problem will be resolved.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 Pod 状态是`ImagePullBackOff`，这立即传达了两个信息。首先，这个 Pod 尚未到达容器运行的阶段，因为它还没有拉取容器镜像。其次，与所有错误一样，Kubernetes
    会继续尝试该操作，但会使用*退避*算法来避免让我们的集群资源过载。拉取镜像涉及通过网络与镜像仓库通信，如果在短时间内频繁发起请求，这对仓库来说既不礼貌也浪费网络带宽。此外，故障的原因可能是暂时性的，因此集群将继续尝试，希望问题能够解决。
- en: The fact that Kubernetes uses a back-off algorithm for retrying errors is important
    for debugging. In this case, we obviously are not going to publish an `nginz`
    image to Docker Hub to fix the problem. But for cases in which we do fix the issue
    by publishing an image, or by changing the permissions for the image, it’s important
    to know that Kubernetes will not pick up that change immediately, because the
    amount of delay between tries increases with each failure.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 使用退避算法来重试错误，这对于调试非常重要。在这种情况下，我们显然不会将一个`nginx`镜像发布到 Docker Hub 来解决问题。但在一些我们通过发布镜像或更改镜像权限来修复问题的情况下，了解
    Kubernetes 不会立即获取到这些更改也很重要，因为每次失败后，重试之间的延迟时间会增加。
- en: 'Let’s explore the event log so that we can see this back-off in action:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看事件日志，以便看到这个退避过程的实际效果：
- en: '[PRE20]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: As before, our Pod is stuck in a `Pending` status ➊. In this case, however,
    the Pod has gotten past the scheduling activity and has moved on to pulling the
    image. For security reasons, the registry does not distinguish between a private
    image for which we don’t have permission to access and a missing image, so Kubernetes
    can tell us only that the issue is one or the other ➋. Finally, we can see that
    Kubernetes has tried to pull the image seven times during the two minutes since
    we created this Pod ➌, and it last tried to pull the image one second ago.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们的 Pod 仍然处于`Pending`状态 ➊。然而，这时 Pod 已经完成了调度活动，并且开始拉取镜像。出于安全考虑，镜像仓库并不区分我们没有权限访问的私有镜像和缺失的镜像，因此
    Kubernetes 只能告诉我们问题是两者之一 ➋。最后，我们可以看到 Kubernetes 在我们创建 Pod 的两分钟内尝试了七次拉取镜像 ➌，并且最后一次尝试是在一秒钟前。
- en: 'If we wait a few minutes and then run the same `kubectl describe` command again,
    focusing on the back-off behavior, we can see that a long amount of time elapses
    between tries:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们等待几分钟，然后再次运行相同的`kubectl describe`命令，重点观察退避行为，我们可以看到每次重试之间的时间间隔变得非常长：
- en: '[PRE21]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Kubernetes has now tried to pull the image 65 times over the course of 19 minutes.
    However, the amount of delay has grown over time and has reached the maximum of
    five minutes between each attempt. This means that as we debug this issue, we
    will need to wait up to five minutes each time to see whether the problem has
    been resolved.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在 Kubernetes 已经在 19 分钟内尝试了 65 次拉取镜像。然而，随着时间的推移，延迟已经增加，且每次尝试之间的最大延迟达到了五分钟。这意味着在我们调试这个问题时，每次都需要等待最多五分钟来查看问题是否已解决。
- en: 'Let’s go ahead and fix the issue so that we can see this in action. We could
    fix the YAML file and run `kubectl apply` again, but we can also fix it using
    `kubectl set`:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续解决这个问题，以便能够看到实际效果。我们可以修复 YAML 文件并再次运行`kubectl apply`，但我们也可以使用`kubectl set`来修复它：
- en: '[PRE22]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The `kubectl set` command requires us to specify the resource type and name;
    in this case `pod nginx`. We then specify `nginx=nginx` to provide the name of
    the container to modify (because a Pod can have multiple containers) along with
    the new image.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubectl set`命令要求我们指定资源类型和名称；在本例中是`pod nginx`。然后我们指定`nginx=nginx`来提供要修改的容器名称（因为一个
    Pod 可以有多个容器）以及新镜像。'
- en: 'We fixed the image name, but the Pod is still showing `ImagePullBackOff` because
    we must wait for the five-minute timer to elapse before Kubernetes tries again.
    Upon the next try, the pull is successful and the Pod starts running:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们修正了镜像名称，但 Pod 仍然显示`ImagePullBackOff`，因为我们必须等待五分钟的计时器结束，Kubernetes 才会再次尝试。下一次尝试时，镜像拉取成功，Pod
    开始运行：
- en: '[PRE23]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let’s clean up the Pod before moving on:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们先清理一下 Pod：
- en: '[PRE24]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Again, we were able to solve this using `kubectl get` and `kubectl describe`.
    However, when we get to the point that the container is running, that won’t be
    sufficient.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 再次说明，我们通过使用 `kubectl get` 和 `kubectl describe` 解决了问题。然而，当容器运行起来时，这些命令就不足以提供帮助了。
- en: Running Containers
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行中的容器
- en: After instructing the container runtime to pull any images needed, `kubelet`
    then tells the runtime to start the containers. For the rest of the examples in
    this chapter, we’ll assume that the container runtime is working as expected.
    At this point, then, the main problem we’ll run into is the case in which the
    container does not start as expected. Let’s begin with a simpler example of debugging
    a container that fails to run, and then we’ll look at a more complex example.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在指示容器运行时拉取所需镜像后，`kubelet` 会告诉运行时启动容器。对于本章中的其他示例，我们假设容器运行时按预期工作。此时，我们将面临的主要问题是容器未按预期启动。让我们从一个简单的调试示例开始，看看容器无法运行的情况，然后再看一个更复杂的示例。
- en: Debugging Using Logs
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用日志进行调试
- en: 'For our simple example, we first need a Pod definition with a container that
    fails on startup. Here’s a Pod definition for PostgreSQL that will do what we
    want:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的简单示例，我们首先需要一个 Pod 定义，里面的容器在启动时会失败。下面是一个会导致失败的 PostgreSQL Pod 定义：
- en: '*postgres-misconfig.yaml*'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*postgres-misconfig.yaml*'
- en: '[PRE25]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: It might not seem like there are any issues with this definition, but PostgreSQL
    has some required configuration when running in a container.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这个定义看起来似乎没有问题，但 PostgreSQL 在容器中运行时有一些必要的配置。
- en: 'We can create the Pod using `kubectl apply`:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `kubectl apply` 来创建 Pod：
- en: '[PRE26]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'After a minute or so to allow time to pull the image, we can check the status
    with `kubectl get`, and we’ll notice a status we haven’t seen before:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 等待大约一分钟以便拉取镜像后，我们可以使用 `kubectl get` 检查状态，这时我们会看到一个之前没有见过的状态：
- en: '[PRE27]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The `CrashLoopBackOff` status indicates that a container in the Pod has exited.
    As this is not a Kubernetes Job, it doesn’t expect the container to exit, so it’s
    considered a crash.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`CrashLoopBackOff` 状态表示 Pod 中的一个容器已经退出。由于这不是一个 Kubernetes 作业，它并不期望容器退出，所以它被认为是崩溃。'
- en: 'If you catch the Pod at the right time, you might see an `Error` status rather
    than `CrashLoopBackOff`. This is temporary: the Pod transitions through that status
    immediately after crashing.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在合适的时间查看 Pod，可能会看到 `Error` 状态，而不是 `CrashLoopBackOff`。这是暂时的：Pod 在崩溃后会立即过渡到该状态。
- en: 'Like the `ImagePullBackOff` status, a `CrashLoopBackOff` uses an algorithm
    to retry the failure, increasing the time between retries with every failure,
    to avoid overwhelming the cluster. We can see this back-off if we wait a few minutes
    and then print the status again:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 与 `ImagePullBackOff` 状态类似，`CrashLoopBackOff` 使用一种算法来重试失败，每次失败时增加重试之间的时间，以避免给集群带来过大负担。我们可以等待几分钟，再次打印状态来查看这种退避情况：
- en: '[PRE28]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: After five restarts, we’re already up to more than a minute of wait time between
    retries. The wait time will continue to increase until we reach five minutes,
    and then Kubernetes will continue to retry every five minutes thereafter indefinitely.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在经过五次重启后，我们已经进入了每次重试之间需要等待超过一分钟的状态。等待时间将继续增加，直到达到五分钟，然后 Kubernetes 会继续每五分钟重试一次，直到无限期地进行下去。
- en: 'Let’s use `kubectl describe`, as usual, to try to get more information about
    this failure:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们像往常一样使用 `kubectl describe` 尝试获取更多关于此失败的信息：
- en: '[PRE29]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The `kubectl describe` command does give us one piece of useful information:
    the exit code for the container. However, that really just tells us there was
    an error of some kind; it isn’t enough to fully debug the failure. To establish
    why the container is failing, we’ll look at the container logs using the `kubectl
    logs` command:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubectl describe` 命令确实为我们提供了一个有用的信息：容器的退出代码。然而，这只是告诉我们发生了某种错误；它不足以完全调试失败的原因。为了弄清楚容器失败的原因，我们将使用
    `kubectl logs` 命令查看容器日志：'
- en: '[PRE30]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: We can see the logs even though the container has already stopped, because the
    container runtime has captured them.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 即使容器已经停止，我们依然可以查看日志，因为容器运行时已经捕获了它们。
- en: 'This message comes directly from PostgreSQL itself. Fortunately, it tells us
    exactly what the issue is: we are missing a required environment variable. We
    can quickly fix this with an update to the YAML resource file:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这个消息直接来自 PostgreSQL 本身。幸运的是，它告诉我们问题的具体原因：我们缺少一个必需的环境变量。我们可以通过更新 YAML 资源文件快速修复这个问题：
- en: '*postgres-fixed.yaml*'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '*postgres-fixed.yaml*'
- en: '[PRE31]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The `env` field ➊ adds a configuration to pass in the required environment variable.
    Of course, in a real system we would not put this directly in a YAML file in plaintext.
    We look at how to secure this kind of information in [Chapter 16](ch16.xhtml#ch16).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: 'To apply this change, we first need to delete the Pod definition and then apply
    the new resource configuration to the cluster:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: As before, if we were using a controller such as a Deployment, we could just
    update the Deployment, and it would handle deleting the old Pod and creating a
    new one for us.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we’ve fixed the configuration, our PostgreSQL container starts as
    expected:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Let’s clean up this Pod before we continue to our next example:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Most well-written applications will print log messages before terminating, but
    we need to be prepared for more difficult cases. Let’s look at one more example
    that includes two new debugging approaches.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Debugging Using Exec
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For this example, we’ll need an application that behaves badly. We’ll use a
    C program that does some very naughty memory access. This program is packaged
    into an Alpine Linux container so that we can run it as a container in Kubernetes.
    Here’s the C source code:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '*crasher.c*'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The first line of code creates a pointer to a string that is two characters
    long; the second line then tries to write to the non-existent third character,
    causing the program to terminate immediately.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: 'This C program can be compiled on any system by using `gcc` to create a `crasher`
    executable. If you build it on a host Linux system, use this `gcc` command:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The `-g` argument ensures that debugging symbols are available. We’ll use those
    in a moment. The `-static` argument is the most important; we want to package
    this as a standalone application inside an Alpine container image. If we are building
    on a different Linux distribution, such as Ubuntu, the standard libraries are
    based on a different toolchain, and dynamic linking will fail. For this reason,
    we want our executable to have all of its dependencies statically linked. Finally,
    we use `-o` to specify the output executable name and then provide the name of
    our C source file.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, you can just use the container image that’s already been built
    and published to Docker Hub under the name `bookofkubernetes/crasher: stable`.
    This image is built and published automatically using GitHub Actions based on
    the code in the repository *[https://github.com/book-of-kubernetes/crasher](https://github.com/book-of-kubernetes/crasher)*.
    Here’s the *Dockerfile* from that repository:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '*Dockerfile*'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: This *Dockerfile* takes advantage of Docker’s multistage builds capability to
    reduce the final image size. To compile inside an Alpine container, we need `gcc`
    and the core C include files and libraries. However, these have the effect of
    making the container image significantly larger. We only need them at compile
    time, so we want to avoid having that extra content in the final image.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: When we run this build using the `docker build` command that we saw in [Chapter
    5](ch05.xhtml#ch05), Docker will create one container based on Alpine Linux, copy
    our source code into it, install the developer tools, and compile the application.
    Docker will then start over with a fresh Alpine Linux container and will copy
    the resulting executable from the first container. The final container image is
    captured from this second container, so we avoid adding the developer tools to
    the final image.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用在[第5章](ch05.xhtml#ch05)中看到的 `docker build` 命令来构建时，Docker 会基于 Alpine Linux
    创建一个容器，将我们的源代码复制到其中，安装开发工具，并编译应用程序。然后，Docker 会使用一个新的 Alpine Linux 容器重新开始，并将第一个容器中生成的可执行文件复制到新的容器中。最终的容器镜像来自第二个容器，因此我们避免将开发工具添加到最终镜像中。
- en: 'Let’s run this image in our Kubernetes cluster. We’ll use a Deployment resource
    this time so that we can illustrate editing it to work around the crashing container:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在 Kubernetes 集群中运行这个镜像。这次我们将使用 Deployment 资源，以便可以演示如何编辑它来解决崩溃的容器问题：
- en: '*crasher-deploy.yaml*'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '*crasher-deploy.yaml*'
- en: '[PRE38]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: This basic Deployment is very similar to what we saw when we introduced Deployments
    in [Chapter 7](ch07.xhtml#ch07). We specify the `image` field to match the location
    where the image is published.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这个基本的 Deployment 与我们在[第7章](ch07.xhtml#ch07)中介绍的 Deployment 非常相似。我们指定 `image`
    字段来匹配镜像发布的位置。
- en: 'We can add this Deployment to the cluster in the usual way:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像往常一样将这个 Deployment 添加到集群中：
- en: '[PRE39]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'As soon as Kubernetes has had a chance to schedule the Pod and pull the image,
    it starts crashing, as expected:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 Kubernetes 有机会调度 Pod 并拉取镜像，它就会开始崩溃，正如预期的那样：
- en: '[PRE40]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'As before, using `kubectl describe` tells us only the exit code of the container.
    There’s another way to get this exit code; we can use the JSON output format of
    `kubectl get` and the `jq` tool to capture just the exit code:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，使用 `kubectl describe` 只能告诉我们容器的退出代码。还有另一种获取退出代码的方法；我们可以使用 `kubectl get`
    的 JSON 输出格式和 `jq` 工具来捕获退出代码：
- en: '[PRE41]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Be sure to use the correct name for your Pod based on the output of `kubectl
    get pods`. The path to the specific field we need is based on how Kubernetes tracks
    this resource internally; with some practice it becomes easier to craft a path
    to `jq` to capture a specific field, which is a very handy trick in scripting.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 一定要使用 `kubectl get pods` 输出中的正确 Pod 名称。我们需要的特定字段路径是基于 Kubernetes 内部如何跟踪该资源的；通过一些实践，构建路径并使用
    `jq` 捕获特定字段会变得更加容易，这是在脚本编写中非常有用的技巧。
- en: 'The exit code of 139 tells us that the container terminated with a segmentation
    fault. However, the logs are unhelpful in diagnosing the problem, because our
    program didn’t print anything before it crashed:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 退出代码 139 告诉我们容器因段错误而终止。然而，日志对于诊断问题没有帮助，因为我们的程序在崩溃之前没有打印任何信息：
- en: '[PRE42]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: We have quite a problem. The logs aren’t helpful, so the next step would be
    to use `kubectl exec` to get inside the container. However, the container stops
    immediately when our application crashes and is not around long enough for us
    to do any debugging work.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遇到了一个大问题。日志没有帮助，所以下一步是使用 `kubectl exec` 进入容器。然而，容器在应用程序崩溃后立即停止，没能维持足够长的时间让我们进行调试工作。
- en: To fix this, we need a way to start this container without running the crashing
    program. We can do that by overriding the default command to have our container
    remain running. Because we built on an Alpine Linux image, the `sleep` command
    is available to us for this purpose.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们需要一种方法来启动容器而不运行崩溃的程序。我们可以通过覆盖默认命令来让容器保持运行状态。由于我们是基于 Alpine Linux
    镜像构建的，`sleep` 命令可供我们使用。
- en: 'We could edit our YAML file and update the Deployment that way, but we can
    also edit the Deployment directly using `kubectl edit`, which will bring up the
    current definition in an editor, and any changes we make will be saved to the
    cluster:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以编辑 YAML 文件并更新 Deployment，但也可以直接使用 `kubectl edit` 命令编辑 Deployment，这样当前的定义会在编辑器中打开，我们所做的任何更改都会保存到集群中：
- en: '[PRE43]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: This will bring up vi in an editor window with the Deployment resource in YAML
    format. The resource will include a lot more fields than we provided when we created
    it because Kubernetes will show us the status of the resource as well as some
    fields with default values.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这将会在编辑器窗口中打开 vi，里面包含以 YAML 格式表示的 Deployment 资源。该资源会包含比我们创建时更多的字段，因为 Kubernetes
    会向我们展示资源的状态以及一些带有默认值的字段。
- en: If you don’t like vi, you can preface the `kubectl edit` command with `KUBE_EDITOR=nano`
    to use the Nano editor, instead.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不喜欢 vi，可以在 `kubectl edit` 命令前加上 `KUBE_EDITOR=nano` 来使用 Nano 编辑器。
- en: 'Within the file, find these lines:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在文件中，找到这些行：
- en: '[PRE44]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'You will see the `imagePullPolicy` line even though it wasn’t in the YAML resource,
    as Kubernetes has added the default policy to the resource automatically. Add
    a new line between `image` and `imagePullPolicy` so that the result looks like
    this:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 即使`imagePullPolicy`行未出现在YAML资源中，你仍会看到它，因为Kubernetes已自动将默认策略添加到资源中。在`image`和`imagePullPolicy`之间添加一行，使结果如下所示：
- en: '[PRE45]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'This added line overrides the default command for the container so that it
    runs `sleep` instead of running our crashing program. Save and exit the editor,
    and `kubectl` will pick up the new definition:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这行新增的代码覆盖了容器的默认命令，使其运行`sleep`，而不是运行我们的崩溃程序。保存并退出编辑器，`kubectl`将会加载新的定义：
- en: '[PRE46]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'After `kubectl` applies this change to the cluster, the Deployment must delete
    the old Pod and create a new one. This is done automatically, so the only difference
    we’ll notice is the automatically generated part of the Pod name. Of course, we’ll
    also see the Pod running:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在`kubectl`将此更改应用到集群后，Deployment必须删除旧的Pod并创建一个新的Pod。这个过程会自动完成，因此我们唯一能注意到的区别是Pod名称中自动生成的部分。当然，我们还会看到Pod正在运行：
- en: '[PRE47]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Our Pod is now running, but it’s only running `sleep`. We still need to debug
    our actual application. To do that, we can now get a shell prompt inside our container:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的Pod现在正在运行，但它只是在运行`sleep`。我们仍然需要调试我们的实际应用程序。为此，我们现在可以在容器内获取一个Shell提示符：
- en: '[PRE48]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The Deployment replaced the Pod when we changed the definition, so the name
    has changed. As before, use the correct name for your Pod. At this point we can
    try out our crashing program manually:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们更改定义时，Deployment替换了Pod，因此名称发生了变化。如前所述，请使用正确的Pod名称。此时我们可以手动尝试我们的崩溃程序：
- en: '[PRE49]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: In many cases, the ability to run a program this way, playing with different
    environment variables and command line options, may be enough to find and fix
    the problem. Alternatively, we could try running the program with `strace`, which
    would tell us what system calls the program is trying to make and what files it
    is trying to open prior to crashing. In this case, we know that the program is
    crashing with a segmentation fault, meaning that the problem is likely a programming
    error, so our best approach is to connect a debugging tool to the application
    using port forwarding.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，能够以这种方式运行程序，调整不同的环境变量和命令行选项，可能足以找到并修复问题。或者，我们可以尝试使用`strace`来运行程序，它会告诉我们程序在崩溃之前尝试进行哪些系统调用以及尝试打开哪些文件。在这种情况下，我们知道程序因段错误崩溃，这意味着问题很可能是编程错误，因此我们最好的方法是通过端口转发将调试工具连接到应用程序。
- en: Debugging Using Port Forwarding
  id: totrans-170
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用端口转发进行调试
- en: 'We’ll illustrate port forwarding using the text-based debugger `gdb`, but any
    debugger that can connect via a network port will work. First, we need to get
    our application created inside the container using a debugger that will listen
    on a network port and wait before it runs the code. To do that, we’ll need to
    install `gdb` inside our container. Because this is an Alpine container, we’ll
    use `apk`:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用基于文本的调试器`gdb`来演示端口转发，但任何可以通过网络端口连接的调试器都可以使用。首先，我们需要使用一个调试器在容器内创建我们的应用程序，该调试器将在网络端口上监听，并在运行代码之前等待。为此，我们需要在容器内安装`gdb`。由于这是一个Alpine容器，我们将使用`apk`：
- en: '[PRE50]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: The version of `gdb` we installed includes `gdbserver`, which enables us to
    start a networked debug session.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们安装的`gdb`版本包含`gdbserver`，它使我们能够启动一个网络调试会话。
- en: Because `gdb` is a text-based debugger, we could obviously just start it directly
    to debug our application, but it is often nicer to use a debugger with a GUI,
    making it easier for us to step through source, set breakpoints, and watch variables.
    For this reason, I’m showing the process for connecting a debugger over the network.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`gdb`是一个基于文本的调试器，我们显然可以直接启动它来调试应用程序，但使用带有GUI的调试器通常更为方便，因为它使我们更容易逐步调试源代码、设置断点和观察变量。因此，我将展示如何通过网络连接调试器的过程。
- en: 'Let’s start `gdbserver` and set it up to listen on port `2345`:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们启动`gdbserver`并设置它监听端口`2345`：
- en: '[PRE51]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Note that we told `gdbserver` to listen to the `localhost` interface. We’ll
    still be able to connect to the debugger because we’ll have Kubernetes provide
    us with port forwarding with the `kubectl port-forward` command. This command
    causes `kubectl` to connect to the API server and request it to forward traffic
    to a specific port on a specific Pod. The advantage is that we can use this port
    forwarding capability from anywhere we can connect to the API server, even outside
    the cluster.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Using port forwarding specifically to run a remote debugger may not be an everyday
    occurrence for either a Kubernetes cluster administrator or the developer of a
    containerized application, but it’s a valuable skill to have when there’s no other
    way to find the bug. It’s also a great way to illustrate the power of port forwarding
    to reach a Pod.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'Because we have our debugger running in our first terminal, we’ll need another
    terminal tab or window for the port forwarding, which can be done from any of
    the hosts in our cluster. Let’s use `host01`:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: This `kubectl` command starts listening on port `2345` and forwards all traffic
    through the API server to the Pod we specified. Because this command keeps running,
    we need yet another terminal window or tab for our final step, which is to run
    the debugger we’ll use to connect to our debug server running in the container.
    This must be done from the same host as our `kubectl port-forward` command because
    that program is listening only on local interfaces.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we could run any debugger that knows how to talk to the debug
    server. For simplicity, we’ll use `gdb` again. We’ll begin by changing to the
    */opt* directory because our C source file is there:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Now we can kick off `gdb` and use it to connect to the debug server:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Our debug session connects successfully and is waiting for us to start the
    program, which we’ll do by using the `continue` command:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: With the debugger, we’re able to see exactly which line of our source code is
    causing the segmentation fault, and now we can figure out how to fix it.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Final Thoughts
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When we move our application components into container images and run them in
    a Kubernetes cluster, we gain substantial benefits in scalability and automated
    failover, but we introduce a number of new possibilities that can go wrong when
    getting our application running, and we introduce new challenges in debugging
    those problems. In this chapter, we’ve looked at how to use Kubernetes commands
    to systematically track our application startup and operation to determine what
    is preventing it from working correctly. With these commands, we can debug any
    kind of issue happening at the application level, even if an application component
    won’t start correctly in its containerized environment.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a clear picture of running containers using Kubernetes, we
    can begin to look in depth into the capabilities of the cluster itself. As we
    do this, we’ll be sure to explore how each component works so as to have the tools
    needed to diagnose problems. We’ll start in the next chapter by looking in detail
    at the Kubernetes control plane.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经清楚地了解了如何使用 Kubernetes 运行容器，接下来我们可以深入研究集群本身的功能。在这个过程中，我们将确保探讨每个组件的工作原理，以便拥有诊断问题所需的工具。我们将在下一章开始，详细了解
    Kubernetes 控制平面。
