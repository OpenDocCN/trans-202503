- en: '10'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: WHEN THINGS GO WRONG
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/common01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: So far our installation and configuration of Kubernetes has gone as planned,
    and our controllers have had no problem creating Pods and starting containers.
    Of course, in the real world, it’s rarely that easy. Although showing everything
    that might go wrong with a complex application deployment isn’t possible, we can
    look at some of the most common problems. Most important, we can explore debugging
    tools that will help us diagnose any issue.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll look at how to diagnose problems with application containers
    that we deploy on top of Kubernetes. We’ll work our way through the life cycle
    for scheduling and running containers, examining potential problems at each step
    as well as how to diagnose and fix them.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Scheduling is the first activity Kubernetes performs on a Pod and its containers.
    When a Pod is first created, the Kubernetes scheduler assigns it to a node. Normally,
    this happens quickly and automatically, but some issues can prevent scheduling
    from happening successfully.
  prefs: []
  type: TYPE_NORMAL
- en: No Available Nodes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One possibility is that the scheduler simply doesn’t have any nodes available.
    This situation might occur because our cluster doesn’t have any nodes configured
    for regular application containers or because all nodes have failed.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate the case in which no nodes are available for assignment, let’s
    create a Pod with a *node selector*. A node selector specifies one or more node
    labels that are required for a Pod to be scheduled on that node. Node selectors
    are useful when some nodes in our cluster are different from others (for example,
    when some nodes have newer CPUs with support for more advanced instruction sets
    needed by some of our containers).
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*The example repository for this book is at* [https://github.com/book-of-kubernetes/examples](https://github.com/book-of-kubernetes/examples).
    *See “Running Examples” on [page xx](ch00.xhtml#ch00lev1sec2) for details on getting
    set up.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll begin with a Pod definition that has a node selector that doesn’t match
    any of our nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '*nginx-selector.yaml*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The node selector ➊ tells Kubernetes to assign this Pod only to a node with
    a label called `purpose` whose value is equal to `special`. Even though none of
    our nodes currently match, we can still create this Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'However, Kubernetes is stuck trying to schedule the Pod, because it can’t find
    a matching node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We see a status of `Pending` and a node assignment of `<none>`. This is because
    Kubernetes has not yet scheduled this Pod onto a node.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `kubectl get` command is typically the first command we should run to see
    whether there are issues with a resource we’ve deployed to our cluster. If we
    have an issue, as we do in this case, the next step is to view the detailed status
    and event log using `kubectl describe`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The event log informs us as to exactly what the issue is: the Pod can’t be
    scheduled because none of the nodes matched the selector.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s add the necessary label to one of our nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We first list the three nodes we have available and then apply the necessary
    label to one of them. As soon as we apply this label, Kubernetes can now schedule
    the Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As expected, the Pod was scheduled onto the node where we applied the label.
  prefs: []
  type: TYPE_NORMAL
- en: This example, like the others we’ll see in this chapter, illustrates debugging
    in Kubernetes. After we’ve created the resources that we need, we query the cluster
    state to make sure the actual deployment of those resources was successful. When
    we find issues, we can correct those issues and our resources will be started
    as desired without having to reinstall our application components.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s clean up this NGINX Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s also remove the label from the node. We remove the label by appending
    a minus sign to it to identify it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We’ve covered one issue with the scheduler, but there’s still another we need
    to look at.
  prefs: []
  type: TYPE_NORMAL
- en: Insufficient Resources
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When choosing a node to host a Pod, the scheduler also considers the resources
    that are available on each node and the resources the Pod requires. We explore
    resource limits in detail in [Chapter 14](ch14.xhtml#ch14); for now it’s enough
    to know that each container in a Pod can request the resources it needs, and the
    scheduler will ensure that it is scheduled onto a node that has those resources
    available. Of course, if there aren’t any nodes with enough room, the scheduler
    won’t be able to schedule the Pod. Instead the Pod will wait in a `Pending` state.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at an example Pod definition to illustrate this:'
  prefs: []
  type: TYPE_NORMAL
- en: '*sleep-multiple.yaml*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In this YAML definition, we create two containers in the same Pod. Each container
    requests two CPUs. Because all of the containers in a Pod must be on the same
    host in order to share some Linux namespace types (especially the network namespace
    so that they can use `localhost` for communication), the scheduler needs to find
    a single node with four CPUs available. In our small cluster, that can’t happen,
    as we can see if we try to deploy the Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'As before, `kubectl describe` gives us the event log that reveals the issue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that it doesn’t matter how heavily loaded our nodes actually are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Nor does it matter how much CPU our containers will actually use. The scheduler
    allocates Pods purely based on what it requested; this way, we don’t suddenly
    overwhelm a CPU when load increases.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can’t magically provide our nodes with more CPUs, so to get this Pod scheduled,
    we’re going to need to specify a lower CPU usage for our two containers. Let’s
    use a more sensible figure of 0.1 CPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '*sleep-sensible.yaml*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The value `100m` ➊ equates to “one hundred millicpu” or one-tenth (0.1) of a
    CPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'Even though this is a separate file, it declares the same resource, so Kubernetes
    will treat it as an update. However, if we try to apply this as a change to the
    existing Pod, it will fail:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We are not allowed to change the resource request of an existing Pod, which
    makes sense given that a Pod is allocated to a node only once on creation, and
    a resource usage change might cause the node to be overly full.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we were using a controller such as a Deployment, the controller could handle
    replacing the Pods for us. Because we created a Pod directly, we need to manually
    delete and then re-create it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Our new Pod has no trouble with node allocation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'And if we run `kubectl describe` on the node, we can see how our new Pod has
    been allocated some of the node’s CPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Be sure to use the correct node name for the node where your Pod was deployed.
    Because our Pod has two containers, each requesting `100m`, its total request
    is `200m` ➊.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s finish by cleaning up this Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Other errors can prevent a Pod from being scheduled, but these are the most
    common. Most important, the commands we used here apply in all cases. First, use
    `kubectl get` to determine the Pod’s current status, followed by `kubectl describe`
    to view the event log. These two commands are always a good first step when something
    doesn’t seem to be working properly.
  prefs: []
  type: TYPE_NORMAL
- en: Pulling Images
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After a Pod is scheduled onto a node, the local `kubelet` service interacts
    with the underlying container runtime to create an isolated environment and start
    containers. However, there’s still one application misconfiguration that can cause
    our Pod to become stuck in the `Pending` phase: inability to pull the container
    image.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Three main issues can prevent the container runtime from pulling an image:'
  prefs: []
  type: TYPE_NORMAL
- en: Failure to connect to the container image registry
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authorization issue with the requested image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image is missing from the registry
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we described in [Chapter 5](ch05.xhtml#ch05), an image registry is a web
    server. Often, the image registry is outside the cluster, and the nodes need to
    be able to connect to an external network or the internet to reach the registry.
    Additionally, most registries support publishing private images that require authentication
    and authorization to access. And, of course, if there is no image published under
    the name we specify, the container runtime is not going to be able to pull it
    from the registry.
  prefs: []
  type: TYPE_NORMAL
- en: 'All of these errors behave the same way in our Kubernetes cluster, with differences
    only in the message in the event log, so we’ll need to explore only one of them.
    We’ll look at what is probably the most common issue: a missing image caused by
    a typo in the image name.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try to create a Pod using this YAML file:'
  prefs: []
  type: TYPE_NORMAL
- en: '*nginx-typo.yaml*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Because there is no image in Docker Hub called `nginz`, it won’t be possible
    to pull this image. Let’s explore what happens when we add this resource to the
    cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Our Pod has status `ImagePullBackOff`, which immediately signals two things.
    First, this Pod is not yet getting to the point at which the containers are running,
    because it has not yet pulled the container images. Second, as with all errors,
    Kubernetes will continue attempting the action, but it will use a *back-off* algorithm
    to avoid overwhelming our cluster’s resources. Pulling an image involves reaching
    out over the network to communicate with the image registry, and it would be rude
    and a waste of network bandwidth to flood the registry with many requests in a
    short amount of time. Moreover, the cause of the failure may be transient, so
    the cluster will keep trying in hopes that the problem will be resolved.
  prefs: []
  type: TYPE_NORMAL
- en: The fact that Kubernetes uses a back-off algorithm for retrying errors is important
    for debugging. In this case, we obviously are not going to publish an `nginz`
    image to Docker Hub to fix the problem. But for cases in which we do fix the issue
    by publishing an image, or by changing the permissions for the image, it’s important
    to know that Kubernetes will not pick up that change immediately, because the
    amount of delay between tries increases with each failure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s explore the event log so that we can see this back-off in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: As before, our Pod is stuck in a `Pending` status ➊. In this case, however,
    the Pod has gotten past the scheduling activity and has moved on to pulling the
    image. For security reasons, the registry does not distinguish between a private
    image for which we don’t have permission to access and a missing image, so Kubernetes
    can tell us only that the issue is one or the other ➋. Finally, we can see that
    Kubernetes has tried to pull the image seven times during the two minutes since
    we created this Pod ➌, and it last tried to pull the image one second ago.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we wait a few minutes and then run the same `kubectl describe` command again,
    focusing on the back-off behavior, we can see that a long amount of time elapses
    between tries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Kubernetes has now tried to pull the image 65 times over the course of 19 minutes.
    However, the amount of delay has grown over time and has reached the maximum of
    five minutes between each attempt. This means that as we debug this issue, we
    will need to wait up to five minutes each time to see whether the problem has
    been resolved.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go ahead and fix the issue so that we can see this in action. We could
    fix the YAML file and run `kubectl apply` again, but we can also fix it using
    `kubectl set`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The `kubectl set` command requires us to specify the resource type and name;
    in this case `pod nginx`. We then specify `nginx=nginx` to provide the name of
    the container to modify (because a Pod can have multiple containers) along with
    the new image.
  prefs: []
  type: TYPE_NORMAL
- en: 'We fixed the image name, but the Pod is still showing `ImagePullBackOff` because
    we must wait for the five-minute timer to elapse before Kubernetes tries again.
    Upon the next try, the pull is successful and the Pod starts running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s clean up the Pod before moving on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Again, we were able to solve this using `kubectl get` and `kubectl describe`.
    However, when we get to the point that the container is running, that won’t be
    sufficient.
  prefs: []
  type: TYPE_NORMAL
- en: Running Containers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After instructing the container runtime to pull any images needed, `kubelet`
    then tells the runtime to start the containers. For the rest of the examples in
    this chapter, we’ll assume that the container runtime is working as expected.
    At this point, then, the main problem we’ll run into is the case in which the
    container does not start as expected. Let’s begin with a simpler example of debugging
    a container that fails to run, and then we’ll look at a more complex example.
  prefs: []
  type: TYPE_NORMAL
- en: Debugging Using Logs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For our simple example, we first need a Pod definition with a container that
    fails on startup. Here’s a Pod definition for PostgreSQL that will do what we
    want:'
  prefs: []
  type: TYPE_NORMAL
- en: '*postgres-misconfig.yaml*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: It might not seem like there are any issues with this definition, but PostgreSQL
    has some required configuration when running in a container.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create the Pod using `kubectl apply`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'After a minute or so to allow time to pull the image, we can check the status
    with `kubectl get`, and we’ll notice a status we haven’t seen before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The `CrashLoopBackOff` status indicates that a container in the Pod has exited.
    As this is not a Kubernetes Job, it doesn’t expect the container to exit, so it’s
    considered a crash.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you catch the Pod at the right time, you might see an `Error` status rather
    than `CrashLoopBackOff`. This is temporary: the Pod transitions through that status
    immediately after crashing.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Like the `ImagePullBackOff` status, a `CrashLoopBackOff` uses an algorithm
    to retry the failure, increasing the time between retries with every failure,
    to avoid overwhelming the cluster. We can see this back-off if we wait a few minutes
    and then print the status again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: After five restarts, we’re already up to more than a minute of wait time between
    retries. The wait time will continue to increase until we reach five minutes,
    and then Kubernetes will continue to retry every five minutes thereafter indefinitely.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use `kubectl describe`, as usual, to try to get more information about
    this failure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The `kubectl describe` command does give us one piece of useful information:
    the exit code for the container. However, that really just tells us there was
    an error of some kind; it isn’t enough to fully debug the failure. To establish
    why the container is failing, we’ll look at the container logs using the `kubectl
    logs` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: We can see the logs even though the container has already stopped, because the
    container runtime has captured them.
  prefs: []
  type: TYPE_NORMAL
- en: 'This message comes directly from PostgreSQL itself. Fortunately, it tells us
    exactly what the issue is: we are missing a required environment variable. We
    can quickly fix this with an update to the YAML resource file:'
  prefs: []
  type: TYPE_NORMAL
- en: '*postgres-fixed.yaml*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The `env` field ➊ adds a configuration to pass in the required environment variable.
    Of course, in a real system we would not put this directly in a YAML file in plaintext.
    We look at how to secure this kind of information in [Chapter 16](ch16.xhtml#ch16).
  prefs: []
  type: TYPE_NORMAL
- en: 'To apply this change, we first need to delete the Pod definition and then apply
    the new resource configuration to the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: As before, if we were using a controller such as a Deployment, we could just
    update the Deployment, and it would handle deleting the old Pod and creating a
    new one for us.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we’ve fixed the configuration, our PostgreSQL container starts as
    expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s clean up this Pod before we continue to our next example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Most well-written applications will print log messages before terminating, but
    we need to be prepared for more difficult cases. Let’s look at one more example
    that includes two new debugging approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Debugging Using Exec
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For this example, we’ll need an application that behaves badly. We’ll use a
    C program that does some very naughty memory access. This program is packaged
    into an Alpine Linux container so that we can run it as a container in Kubernetes.
    Here’s the C source code:'
  prefs: []
  type: TYPE_NORMAL
- en: '*crasher.c*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The first line of code creates a pointer to a string that is two characters
    long; the second line then tries to write to the non-existent third character,
    causing the program to terminate immediately.
  prefs: []
  type: TYPE_NORMAL
- en: 'This C program can be compiled on any system by using `gcc` to create a `crasher`
    executable. If you build it on a host Linux system, use this `gcc` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The `-g` argument ensures that debugging symbols are available. We’ll use those
    in a moment. The `-static` argument is the most important; we want to package
    this as a standalone application inside an Alpine container image. If we are building
    on a different Linux distribution, such as Ubuntu, the standard libraries are
    based on a different toolchain, and dynamic linking will fail. For this reason,
    we want our executable to have all of its dependencies statically linked. Finally,
    we use `-o` to specify the output executable name and then provide the name of
    our C source file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, you can just use the container image that’s already been built
    and published to Docker Hub under the name `bookofkubernetes/crasher: stable`.
    This image is built and published automatically using GitHub Actions based on
    the code in the repository *[https://github.com/book-of-kubernetes/crasher](https://github.com/book-of-kubernetes/crasher)*.
    Here’s the *Dockerfile* from that repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Dockerfile*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: This *Dockerfile* takes advantage of Docker’s multistage builds capability to
    reduce the final image size. To compile inside an Alpine container, we need `gcc`
    and the core C include files and libraries. However, these have the effect of
    making the container image significantly larger. We only need them at compile
    time, so we want to avoid having that extra content in the final image.
  prefs: []
  type: TYPE_NORMAL
- en: When we run this build using the `docker build` command that we saw in [Chapter
    5](ch05.xhtml#ch05), Docker will create one container based on Alpine Linux, copy
    our source code into it, install the developer tools, and compile the application.
    Docker will then start over with a fresh Alpine Linux container and will copy
    the resulting executable from the first container. The final container image is
    captured from this second container, so we avoid adding the developer tools to
    the final image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s run this image in our Kubernetes cluster. We’ll use a Deployment resource
    this time so that we can illustrate editing it to work around the crashing container:'
  prefs: []
  type: TYPE_NORMAL
- en: '*crasher-deploy.yaml*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: This basic Deployment is very similar to what we saw when we introduced Deployments
    in [Chapter 7](ch07.xhtml#ch07). We specify the `image` field to match the location
    where the image is published.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can add this Deployment to the cluster in the usual way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'As soon as Kubernetes has had a chance to schedule the Pod and pull the image,
    it starts crashing, as expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'As before, using `kubectl describe` tells us only the exit code of the container.
    There’s another way to get this exit code; we can use the JSON output format of
    `kubectl get` and the `jq` tool to capture just the exit code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Be sure to use the correct name for your Pod based on the output of `kubectl
    get pods`. The path to the specific field we need is based on how Kubernetes tracks
    this resource internally; with some practice it becomes easier to craft a path
    to `jq` to capture a specific field, which is a very handy trick in scripting.
  prefs: []
  type: TYPE_NORMAL
- en: 'The exit code of 139 tells us that the container terminated with a segmentation
    fault. However, the logs are unhelpful in diagnosing the problem, because our
    program didn’t print anything before it crashed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: We have quite a problem. The logs aren’t helpful, so the next step would be
    to use `kubectl exec` to get inside the container. However, the container stops
    immediately when our application crashes and is not around long enough for us
    to do any debugging work.
  prefs: []
  type: TYPE_NORMAL
- en: To fix this, we need a way to start this container without running the crashing
    program. We can do that by overriding the default command to have our container
    remain running. Because we built on an Alpine Linux image, the `sleep` command
    is available to us for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: 'We could edit our YAML file and update the Deployment that way, but we can
    also edit the Deployment directly using `kubectl edit`, which will bring up the
    current definition in an editor, and any changes we make will be saved to the
    cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: This will bring up vi in an editor window with the Deployment resource in YAML
    format. The resource will include a lot more fields than we provided when we created
    it because Kubernetes will show us the status of the resource as well as some
    fields with default values.
  prefs: []
  type: TYPE_NORMAL
- en: If you don’t like vi, you can preface the `kubectl edit` command with `KUBE_EDITOR=nano`
    to use the Nano editor, instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'Within the file, find these lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see the `imagePullPolicy` line even though it wasn’t in the YAML resource,
    as Kubernetes has added the default policy to the resource automatically. Add
    a new line between `image` and `imagePullPolicy` so that the result looks like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'This added line overrides the default command for the container so that it
    runs `sleep` instead of running our crashing program. Save and exit the editor,
    and `kubectl` will pick up the new definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'After `kubectl` applies this change to the cluster, the Deployment must delete
    the old Pod and create a new one. This is done automatically, so the only difference
    we’ll notice is the automatically generated part of the Pod name. Of course, we’ll
    also see the Pod running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Our Pod is now running, but it’s only running `sleep`. We still need to debug
    our actual application. To do that, we can now get a shell prompt inside our container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The Deployment replaced the Pod when we changed the definition, so the name
    has changed. As before, use the correct name for your Pod. At this point we can
    try out our crashing program manually:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: In many cases, the ability to run a program this way, playing with different
    environment variables and command line options, may be enough to find and fix
    the problem. Alternatively, we could try running the program with `strace`, which
    would tell us what system calls the program is trying to make and what files it
    is trying to open prior to crashing. In this case, we know that the program is
    crashing with a segmentation fault, meaning that the problem is likely a programming
    error, so our best approach is to connect a debugging tool to the application
    using port forwarding.
  prefs: []
  type: TYPE_NORMAL
- en: Debugging Using Port Forwarding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We’ll illustrate port forwarding using the text-based debugger `gdb`, but any
    debugger that can connect via a network port will work. First, we need to get
    our application created inside the container using a debugger that will listen
    on a network port and wait before it runs the code. To do that, we’ll need to
    install `gdb` inside our container. Because this is an Alpine container, we’ll
    use `apk`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: The version of `gdb` we installed includes `gdbserver`, which enables us to
    start a networked debug session.
  prefs: []
  type: TYPE_NORMAL
- en: Because `gdb` is a text-based debugger, we could obviously just start it directly
    to debug our application, but it is often nicer to use a debugger with a GUI,
    making it easier for us to step through source, set breakpoints, and watch variables.
    For this reason, I’m showing the process for connecting a debugger over the network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start `gdbserver` and set it up to listen on port `2345`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Note that we told `gdbserver` to listen to the `localhost` interface. We’ll
    still be able to connect to the debugger because we’ll have Kubernetes provide
    us with port forwarding with the `kubectl port-forward` command. This command
    causes `kubectl` to connect to the API server and request it to forward traffic
    to a specific port on a specific Pod. The advantage is that we can use this port
    forwarding capability from anywhere we can connect to the API server, even outside
    the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Using port forwarding specifically to run a remote debugger may not be an everyday
    occurrence for either a Kubernetes cluster administrator or the developer of a
    containerized application, but it’s a valuable skill to have when there’s no other
    way to find the bug. It’s also a great way to illustrate the power of port forwarding
    to reach a Pod.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because we have our debugger running in our first terminal, we’ll need another
    terminal tab or window for the port forwarding, which can be done from any of
    the hosts in our cluster. Let’s use `host01`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: This `kubectl` command starts listening on port `2345` and forwards all traffic
    through the API server to the Pod we specified. Because this command keeps running,
    we need yet another terminal window or tab for our final step, which is to run
    the debugger we’ll use to connect to our debug server running in the container.
    This must be done from the same host as our `kubectl port-forward` command because
    that program is listening only on local interfaces.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we could run any debugger that knows how to talk to the debug
    server. For simplicity, we’ll use `gdb` again. We’ll begin by changing to the
    */opt* directory because our C source file is there:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can kick off `gdb` and use it to connect to the debug server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Our debug session connects successfully and is waiting for us to start the
    program, which we’ll do by using the `continue` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: With the debugger, we’re able to see exactly which line of our source code is
    causing the segmentation fault, and now we can figure out how to fix it.
  prefs: []
  type: TYPE_NORMAL
- en: Final Thoughts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When we move our application components into container images and run them in
    a Kubernetes cluster, we gain substantial benefits in scalability and automated
    failover, but we introduce a number of new possibilities that can go wrong when
    getting our application running, and we introduce new challenges in debugging
    those problems. In this chapter, we’ve looked at how to use Kubernetes commands
    to systematically track our application startup and operation to determine what
    is preventing it from working correctly. With these commands, we can debug any
    kind of issue happening at the application level, even if an application component
    won’t start correctly in its containerized environment.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a clear picture of running containers using Kubernetes, we
    can begin to look in depth into the capabilities of the cluster itself. As we
    do this, we’ll be sure to explore how each component works so as to have the tools
    needed to diagnose problems. We’ll start in the next chapter by looking in detail
    at the Kubernetes control plane.
  prefs: []
  type: TYPE_NORMAL
