<html><head></head><body>
<h2 class="h2" id="ch18"><span epub:type="pagebreak" id="page_295"/><span class="big">18</span><br/>AFFINITY AND DEVICES</h2>&#13;
<div class="image1"><img alt="image" src="../images/common01.jpg"/></div>&#13;
<p class="noindent">The ideal application exhibits complete simplicity. It is simple to design. It is simple to develop. It is simple to deploy. Its individual components are stateless, so it’s easy to scale to serve as many users as needed. The individual service endpoints act as pure functions where the output is determined solely by the input. The application operates on a reasonable amount of data, with modest CPU and memory requirements, and requests and responses easily fit into a JSON structure that is at most a couple of kilobytes.</p>&#13;
<p class="indent">Of course, outside of tutorials, ideal applications don’t exist. Real-world applications store state, both in long-term persistent storage and in caches that can be accessed quickly. Real-world applications have data security and authorization concerns, so they need to authenticate users, remember who those users are, and limit access accordingly. And many real-world applications need to access specialized hardware rather than just using idealized CPU, memory, storage, and network resources.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_296"/>We want to deploy real-world applications on our Kubernetes cluster, not just idealized applications. This means that we need to make smart decisions about how to deploy the application components that move us away from an ideal world in which the cluster decides how many container instances to run and where to schedule them. However, we don’t want to create an application architecture that is so rigid that we lose our cluster’s scalability and resiliency. Instead, we want to work within the cluster to give it hints about how to deploy our application components while still maintaining as much flexibility as possible. In this chapter, we’ll explore how our application components can enforce a little bit of coupling to other components or to specialized hardware without losing the benefits of Kubernetes.</p>&#13;
<h3 class="h3" id="ch00lev1sec73">Affinity and Anti-affinity</h3>&#13;
<p class="noindent">We’ll begin by looking at the case in which we want to manage the scheduling of Pods so that we can prefer or avoid co-locating multiple containers on the same node. For example, if we have two containers that consume significant network bandwidth communicating with each other, we might want those two containers to run together on a node to reduce latency and avoid slowing down the rest of the cluster. Or, if we want to ensure that a highly available component can survive the loss of a node in the cluster, we may want to split Pod instances so they run on as many different cluster nodes as possible.</p>&#13;
<p class="indent">One way to co-locate containers is to combine multiple separate containers into a single Pod specification. That is a great solution for cases in which two processes are completely dependent on each other. However, it removes the ability to scale the instances separately. For example, in a web application backed by distributed storage, we might need many more instances of the web server process than we would need of the storage process. We need to place those application components in different Pods to be able to scale them separately.</p>&#13;
<p class="indent">In <a href="ch08.xhtml#ch08">Chapter 8</a>, when we wanted to guarantee that a Pod ran on a specified node, we added the <span class="literal">nodeName</span> field to the Pod specification to override the scheduler. That was fine for an example, but for a real application it would eliminate the scaling and failover that are essential for performance and reliability. Instead, we’ll use the Kubernetes concept of <em>affinity</em> to give the scheduler hints about how to allocate Pods without forcing any Pod to run on a specific node.</p>&#13;
<p class="indent">Affinity allows us to restrict where a Pod should be scheduled based on the presence of other Pods. Let’s look at an example using the <span class="literal">iperf3</span> network testing application.</p>&#13;
<div class="box5">&#13;
<p class="boxtitle-d"><span epub:type="pagebreak" id="page_297"/><strong>CLUSTER ZONES</strong></p>&#13;
<p class="noindents">Pod affinity is most valuable for large clusters that span multiple networks. For example, we might deploy a Kubernetes cluster to multiple different data centers to eliminate single points of failure. In those cases, we would configure affinity based on a zone, which might contain many nodes. Here, we have only a small example cluster, so we’ll treat each node in our cluster as a separate zone.</p>&#13;
</div>&#13;
<h4 class="h4" id="ch00lev2sec105">Anti-affinity</h4>&#13;
<p class="noindent">Let’s start with the opposite of affinity: <em>anti-affinity</em>. Anti-affinity causes the Kubernetes scheduler to avoid co-locating Pods. In this case, we’ll create a Deployment with three separate <span class="literal">iperf3</span> server Pods, but we’ll use anti-affinity to distribute those three Pods across our nodes so that each node gets a Pod.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>The example repository for this book is at</em> <a href="https://github.com/book-of-kubernetes/examples">https://github.com/book-of-kubernetes/examples</a>. <em>See “Running Examples” on <a href="ch00.xhtml#ch00lev1sec2">page xx</a> for details on getting set up.</em></p>&#13;
</div>&#13;
<p class="indent">Here’s the YAML definition we need:</p>&#13;
<p class="noindent6"><em>ipf-server.yaml</em></p>&#13;
<pre>---&#13;
kind: Deployment&#13;
apiVersion: apps/v1&#13;
metadata:&#13;
  name: iperf-server&#13;
spec:&#13;
  replicas: 3&#13;
  selector:&#13;
    matchLabels:&#13;
      app: iperf-server&#13;
  template:&#13;
    metadata:&#13;
      labels:&#13;
        app: iperf-server&#13;
    spec:&#13;
   <span class="ent">➊</span> affinity:&#13;
        podAntiAffinity:&#13;
       <span class="ent">➋</span> requiredDuringSchedulingIgnoredDuringExecution:&#13;
          - labelSelector:&#13;
              matchExpressions:&#13;
              - key: app&#13;
                operator: In&#13;
                values:&#13;
                - iperf-server&#13;
         <span class="ent">➌</span> topologyKey: "kubernetes.io/hostname"&#13;
<span epub:type="pagebreak" id="page_298"/>      containers:&#13;
      - name: iperf&#13;
        image: bookofkubernetes/iperf3:stable&#13;
        env:&#13;
        - name: IPERF_SERVER&#13;
          value: "1"</pre>&#13;
<p class="indent">This Deployment resource is typical except for the new <span class="literal">affinity</span> section <span class="ent">➊</span>. We specify an anti-affinity rule that is based on the same label that the Deployment uses to manage its Pods. With this rule, we specify that we don’t want a Pod to be scheduled into a zone that already has a Pod with the <span class="literal">app=iperf-server</span> label.</p>&#13;
<p class="indent">The <span class="literal">topologyKey</span> <span class="ent">➌</span> specifies the size of the zone. In this case, each node in the cluster has a different <span class="literal">hostname</span> label, so each node is considered to be a different zone. The anti-affinity rule therefore prevents <span class="literal">kube-scheduler</span> from placing a second Pod onto a node after the first Pod has already been scheduled there.</p>&#13;
<p class="indent">Finally, because we specified the rule using <span class="literal">requiredDuringScheduling</span> <span class="ent">➋</span>, it’s a <em>hard</em> anti-affinity rule, which means that the scheduler won’t schedule the Pod unless it can satisfy the rule. It is also possible to use <span class="literal">preferredDuringScheduling</span> and assign a weight to give the scheduler a hint without preventing Pod scheduling if the rule can’t be satisfied.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>The <span class="codeitalic">topologyKey</span> can be based on any label that’s applied on the node. Cloud-based Kubernetes distributions typically automatically apply labels to each node based on the availability zone for that node, making it easy to use anti-affinity to spread Pods across availability zones for redundancy.</em></p>&#13;
</div>&#13;
<p class="indent">Let’s apply this Deployment and see the result:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/ipf-server.yaml</span> &#13;
deployment.apps/iperf-server created</pre>&#13;
<p class="indent">As soon as our Pods are running, we see that a Pod has been allocated to each node in the cluster:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get po -o wide</span>&#13;
NAME                            READY   STATUS    ... NODE     ...&#13;
iperf-server-7666fb76d8-7rz8j   1/1     Running   ... host01   ...&#13;
iperf-server-7666fb76d8-cljkh   1/1     Running   ... host02   ...&#13;
iperf-server-7666fb76d8-ktk92   1/1     Running   ... host03   ...</pre>&#13;
<p class="indent">Because we have three nodes and three instances, it’s essentially identical to using a DaemonSet, but this approach is more flexible because it doesn’t require an instance on every node. In a large cluster, we still might need only a few Pod instances to meet demand for this service. Using anti-affinity with zones based on hostnames allows us to specify the correct scale for our Deployment while still distributing each Pod to a distinct node for higher availability. And anti-affinity can be used to distribute Pods across other types of zones as well.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_299"/>Before we continue, let’s create a Service with which our <span class="literal">iperf3</span> clients will be able to find a server instance. Here’s the YAML:</p>&#13;
<p class="noindent6"><em>ipf-svc.yaml</em></p>&#13;
<pre>---&#13;
kind: Service&#13;
apiVersion: v1&#13;
metadata:&#13;
  name: iperf-server&#13;
spec:&#13;
  selector:&#13;
    app: iperf-server&#13;
  ports:&#13;
  - protocol: TCP&#13;
    port: 5201&#13;
    targetPort: 5201</pre>&#13;
<p class="indent">Let’s apply this to the cluster:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/ipf-svc.yaml</span> &#13;
service/iperf-server created</pre>&#13;
<p class="indent">The Service picks up all three Pods:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get ep iperf-server</span>&#13;
NAME           ENDPOINTS                                                 ...&#13;
iperf-server   172.31.239.207:5201,172.31.25.214:5201,172.31.89.206:5201 ...</pre>&#13;
<p class="indent">The <span class="literal">ep</span> is short for <span class="literal">endpoints</span>. Each Service has an associated Endpoint object that records the current Pods that are receiving traffic for the Service.</p>&#13;
<h4 class="h4" id="ch00lev2sec106">Affinity</h4>&#13;
<p class="noindent">We’re now ready to deploy our <span class="literal">iperf3</span> client to use these server instances. We would like to distribute the clients to each node in the same way, but we want to make sure that each client is deployed to a node that has a server instance. To do this, we’ll use both an affinity and an anti-affinity rule:</p>&#13;
<p class="noindent6"><em>ipf-client.yaml</em></p>&#13;
<pre>---&#13;
kind: Deployment&#13;
apiVersion: apps/v1&#13;
metadata:&#13;
  name: iperf&#13;
spec:&#13;
  replicas: 3&#13;
  selector:&#13;
    matchLabels:&#13;
      app: iperf&#13;
  template:&#13;
    metadata:&#13;
      labels:&#13;
<span epub:type="pagebreak" id="page_300"/>        app: iperf&#13;
    spec:&#13;
      affinity:&#13;
        podAntiAffinity:&#13;
          requiredDuringSchedulingIgnoredDuringExecution:&#13;
          - labelSelector:&#13;
              matchExpressions:&#13;
              - key: app&#13;
                operator: In&#13;
                values:&#13;
                - iperf&#13;
            topologyKey: "kubernetes.io/hostname"&#13;
        <span class="ent">➊</span> podAffinity:&#13;
          requiredDuringSchedulingIgnoredDuringExecution:&#13;
          - labelSelector:&#13;
              matchExpressions:&#13;
              - key: app&#13;
                operator: In&#13;
                values:&#13;
                - iperf-server&#13;
            topologyKey: "kubernetes.io/hostname"&#13;
      containers:&#13;
      - name: iperf&#13;
        image: bookofkubernetes/iperf3:stable</pre>&#13;
<p class="indent">The additional <span class="literal">podAffinity</span> rule <span class="ent">➊</span> ensures that each client instance is deployed to a node only if a server instance is already present. The fields in an affinity rule work the same way as an anti-affinity rule.</p>&#13;
<p class="indent">Let’s deploy the client instances:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/ipf-client.yaml</span> &#13;
deployment.apps/iperf created</pre>&#13;
<p class="indent">After these Pods are running, we can see that they have also been distributed across all three nodes in the cluster:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get po -o wide</span>&#13;
NAME                            READY   STATUS    ... NODE     ... &#13;
iperf-c8d4566f-btppf            1/1     Running   ... host02   ... &#13;
iperf-c8d4566f-s6rpn            1/1     Running   ... host03   ... &#13;
iperf-c8d4566f-v9v8m            1/1     Running   ... host01   ... &#13;
...</pre>&#13;
<p class="indent">It may seem like we’ve deployed our <span class="literal">iperf3</span> client and server in a way that enables each client to talk to its local server instance, maximizing the bandwidth between client and server. However, that’s not actually the case. Because the <span class="literal">iperf-server</span> Service is configured with all three Pods, each client Pod is connecting to a random server. As a result, our clients may not behave correctly. You might see logs indicating that a client is able to connect <span epub:type="pagebreak" id="page_301"/>to a server, but you might also see client Pods in the <span class="literal">Error</span> or <span class="literal">CrashLoopBackOff</span> state, with log output like this:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl logs</span> <span class="codestrong1"><span class="codeitalic1">iperf-c8d4566f-v9v8m</span></span>&#13;
iperf3: error - the server is busy running a test. try again later&#13;
iperf3 error - exiting</pre>&#13;
<p class="indent">This indicates that a client is connecting to a server that already has a client connected, which means that we must have at least two clients using the same server.</p>&#13;
<h3 class="h3" id="ch00lev1sec74">Service Traffic Routing</h3>&#13;
<p class="noindent">We would like to configure our client Pods with the ability to access the local server Pod we deployed rather than a server Pod on a different node. Let’s start by confirming that traffic is being routed randomly across all three server Pods. We can examine the <span class="literal">iptables</span> rules created by <span class="literal">kube-proxy</span> for this Service:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">iptables-save | grep iperf-server</span>&#13;
...&#13;
-A KUBE-SVC-KN2SIRYEH2IFQNHK -m comment --comment "default/iperf-server" &#13;
  -m statistic --mode random --probability 0.33333333349 -j KUBE-SEP-IGBNNG5F5VCPRRWI&#13;
-A KUBE-SVC-KN2SIRYEH2IFQNHK -m comment --comment "default/iperf-server" &#13;
  -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-FDPADR4LUNHDJSPL&#13;
-A KUBE-SVC-KN2SIRYEH2IFQNHK -m comment --comment "default/iperf-server" &#13;
  -j KUBE-SEP-TZDPKVKUEZYBFM3V</pre>&#13;
<p class="indent">We’re running this command on <em>host01</em>, and we see that there are three separate <span class="literal">iptables</span> rules, with a random selection of the destination. This means that the <span class="literal">iperf3</span> client on <em>host01</em> could potentially be routed to any server Pod.</p>&#13;
<p class="indent">To fix that, we need to change the internal traffic policy configuration of our Service. By default, the policy is <span class="literal">Cluster</span>, indicating that all Pods in the cluster are valid destinations. We can change the policy to <span class="literal">Local</span>, which restricts the Service to route only to Pods on the same node.</p>&#13;
<p class="indent">Let’s patch the Service to change this policy:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl patch svc iperf-server -p '{"spec":{"internalTrafficPolicy":"Local"}}'</span>&#13;
service/iperf-server patched</pre>&#13;
<p class="indent">The change takes effect immediately, as we can see by looking at the <span class="literal">iptables</span> rules again:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">iptables-save | grep iperf-server</span>&#13;
...&#13;
-A KUBE-SVC-KN2SIRYEH2IFQNHK -m comment --comment "default/iperf-server" \&#13;
  -j KUBE-SEP-IGBNNG5F5VCPRRWI</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_302"/>This time, only one possible destination is configured on <em>host01</em>, as there is only one local Pod instance for this Service.</p>&#13;
<p class="indent">After a few minutes, the <span class="literal">iperf3</span> clients now show the kind of output we expect to see:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl logs</span> <span class="codestrong1"><span class="codeitalic1">iperf-c8d4566f-btppf</span></span>&#13;
Connecting to host iperf-server, port 5201&#13;
...&#13;
[ ID] Interval           Transfer     Bitrate         Retr&#13;
[  5]   0.00-10.00  sec  8.67 GBytes  7.45 Gbits/sec  1250             sender&#13;
[  5]   0.00-10.00  sec  8.67 GBytes  7.45 Gbits/sec                  receiver&#13;
...</pre>&#13;
<p class="indent">Not only are all of the clients able to connect to a unique server, but the performance is consistently high as the network connection is local to each node.</p>&#13;
<p class="indent">Before we go further, let’s clean up these resources:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl delete svc/iperf-server deploy/iperf deploy/iperf-server</span>&#13;
service "iperf-server" deleted&#13;
deployment.apps "iperf" deleted&#13;
deployment.apps "iperf-server" deleted</pre>&#13;
<p class="indent">Although the <span class="literal">Local</span> internal traffic policy is useful for maximizing bandwidth between client and server, it has a major limitation. If a node does not contain a healthy Pod instance, clients on that node will not be able to access the Service at all, even if there are healthy instances on other nodes. It is critical when using this design pattern to also configure a readiness probe, as described in <a href="ch13.xhtml#ch13">Chapter 13</a>, that checks not only the Pod itself but also its Service dependencies. This way, if a Service is inaccessible on a particular node, the client on that node will also report itself to be unhealthy so that no traffic will be routed to it.</p>&#13;
<p class="indent">The affinity and anti-affinity capabilities we’ve seen allows us to give hints to the scheduler without losing the scalability and resilience we want for our application components. However, even though it might be tempting to use these features whenever we have closely connected components in our application architecture, it’s probably best to allow the scheduler to work unhindered and add affinity only for cases in which real performance testing shows that it makes a significant difference.</p>&#13;
<p class="indent">Service routing for improved performance is an active area of development in Kubernetes. For clusters running across multiple zones, a new feature called Topology Aware Hints can enable Kubernetes to route connections to Services to the closest instances wherever possible, improving network performance while still allowing cross-zone traffic where necessary.</p>&#13;
<h3 class="h3" id="ch00lev1sec75"><span epub:type="pagebreak" id="page_303"/>Hardware Resources</h3>&#13;
<p class="noindent">Affinity and anti-affinity allow us to control where Pods are scheduled but should be used only if necessary. But what about cases for which a Pod needs access to some specialized hardware that is available only on some nodes? For example, we might have processing that would benefit from a graphics processing unit (GPU), but we might limit the number of GPU nodes in the cluster to reduce cost. In that case, it is absolutely necessary to ensure that the Pod is scheduled in the right place.</p>&#13;
<p class="indent">As before, we could tie our Pod directly to a node using <span class="literal">nodeName</span>. But we might have many nodes in our cluster with the right hardware, so what we really want is to be able to tell Kubernetes about the requirement and then let the scheduler decide how to satisfy it.</p>&#13;
<p class="indent">Kubernetes provides two related methods to address this need: device plug-ins and extended resources. A device plug-in provides the most complete functionality, but the plug-in itself must exist for the hardware device. Meanwhile, extended resources can be used for any hardware device, but the Kubernetes cluster only tracks allocation of the resource; it doesn’t actually manage its availability in the container.</p>&#13;
<p class="indent">Implementing a device plug-in requires close collaboration with <span class="literal">kubelet</span>. Similar to the storage plug-in architecture we saw in <a href="ch15.xhtml#ch15">Chapter 15</a>, a device plug-in registers itself with the <span class="literal">kubelet</span> instance running on a node, identifying any devices it manages. Pods identify any devices they require, and the device manager tells <span class="literal">kubelet</span> how to make the device available inside the container (typically by mounting the device from the host into the container’s filesystem).</p>&#13;
<p class="indent">Because we’re operating in a virtualized example cluster, we don’t have any specialized hardware to demonstrate a device plug-in, but an extended resource works identically from an allocation standpoint, so we can still get a feel for the overall approach.</p>&#13;
<p class="indent">Let’s begin by updating the cluster to indicate that one of the nodes has an example extended resource. We do this by patching the <span class="literal">status</span> for the node. Ideally, we could do this with <span class="literal">kubectl patch</span>, but unfortunately it’s not possible to update the <span class="literal">status</span> of a resource with that command, so we’re reduced to using <span class="literal">curl</span> to call the Kubernetes API directly. The <em>/opt</em> directory has a script to make this easy. <a href="ch18.xhtml#ch18list1">Listing 18-1</a> presents the relevant part.</p>&#13;
<p class="noindent6"><em>add-hw.sh</em></p>&#13;
<pre>#!/bin/bash&#13;
...&#13;
patch='&#13;
[&#13;
  {&#13;
    "op": "add", &#13;
    "path": "/status/capacity/bookofkubernetes.com~1special-hw", &#13;
    "value": "3"&#13;
  }&#13;
]&#13;
'&#13;
<span epub:type="pagebreak" id="page_304"/>curl --cacert $ca --cert $cert --key $key \&#13;
  -H "Content-Type: application/json-patch+json" \&#13;
  -X PATCH -d "$patch" \&#13;
  https://192.168.61.10:6443/api/v1/nodes/host02/status&#13;
...</pre>&#13;
<p class="caption" id="ch18list1"><em>Listing 18-1: Special hardware script</em></p>&#13;
<p class="indent">This <span class="literal">curl</span> command sends a JSON patch object to update the <span class="literal">status</span> field for the node, adding an entry called <span class="literal">bookofkubernetes.com/special-hw</span> under <span class="literal">capacity</span>. The <span class="literal">~1</span> acts as a slash character.</p>&#13;
<p class="indent">Run the script to update the node:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">/opt/add-hw.sh</span> &#13;
...</pre>&#13;
<p class="indent">The response from the API server includes the entire Node resource. Let’s double-check just the field we care about to make sure it applied:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get node host02 -o json | jq .status.capacity</span>&#13;
{&#13;
  "bookofkubernetes.com/special-hw": "3",&#13;
  "cpu": "2",&#13;
  "ephemeral-storage": "40593612Ki",&#13;
  "hugepages-2Mi": "0",&#13;
  "memory": "2035228Ki",&#13;
  "pods": "110"&#13;
}</pre>&#13;
<p class="indent">The extended resource shows up alongside the standard resources for the node. We can now request this resource similar to how we request standard resources, as we saw in <a href="ch14.xhtml#ch14">Chapter 14</a>.</p>&#13;
<p class="indent">Here’s a Pod that requests the special hardware:</p>&#13;
<p class="noindent6"><em>hw.yaml</em></p>&#13;
<pre>---&#13;
apiVersion: v1&#13;
kind: Pod&#13;
metadata:&#13;
  name: sleep&#13;
spec:&#13;
  containers:&#13;
  - name: sleep&#13;
    image: busybox&#13;
    command: ["/bin/sleep", "infinity"]&#13;
    resources:&#13;
      limits:&#13;
        bookofkubernetes.com/special-hw: 1</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_305"/>We specify the requirement for the special hardware using the <span class="literal">resources</span> field. The resource is either allocated or not allocated; thus, there’s no distinction between requests and limits, so Kubernetes expects us to specify it using <span class="literal">limits</span>. When we apply this to the cluster, the Kubernetes scheduler will ensure that this Pod runs on a node that can meet this requirement:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/hw.yaml</span> &#13;
pod/sleep created</pre>&#13;
<p class="indent">As a result, the Pod ends up on <span class="literal">host02</span>:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get po -o wide</span>&#13;
NAME    READY   STATUS    ... NODE     ...&#13;
sleep   1/1     Running   ... host02   ...</pre>&#13;
<p class="indent">Additionally, the node status now reflects an allocation for this extended resource:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl describe node host02</span>&#13;
Name:               host02&#13;
...&#13;
Allocated resources:&#13;
...&#13;
  Resource                         Requests     Limits&#13;
  --------                         --------     ------&#13;
...&#13;
  bookofkubernetes.com/special-hw  1            1&#13;
...</pre>&#13;
<p class="indent">Both the available quantity of three <span class="literal">special-hw</span> that we specified when we added the extended resource in <a href="ch18.xhtml#ch18list1">Listing 18-1</a> and the allocation of that resource to our Pod are arbitrary. The extended resource acts like a semaphore in preventing too many users from using the same resource, but we would need to add additional processing to deconflict multiple users if we really had three separate special hardware devices on the same node.</p>&#13;
<p class="indent">If we do try to over-allocate based on what we specified is available, the Pod won’t be scheduled. We can confirm this if we try to add another Pod that needs all three of our special hardware devices:</p>&#13;
<p class="noindent6"><em>hw3.yaml</em></p>&#13;
<pre>---&#13;
apiVersion: v1&#13;
kind: Pod&#13;
metadata:&#13;
  name: sleep3&#13;
spec:&#13;
  containers:&#13;
  - name: sleep&#13;
    image: busybox&#13;
    command: ["/bin/sleep", "infinity"]&#13;
    resources:&#13;
<span epub:type="pagebreak" id="page_306"/>      limits:&#13;
        bookofkubernetes.com/special-hw: 3</pre>&#13;
<p class="indent">Let’s try to add this Pod to the cluster:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/hw3.yaml</span> &#13;
pod/sleep created</pre>&#13;
<p class="indent">Because there aren’t enough special hardware devices available, this Pod stays in the Pending state:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get po -o wide</span>&#13;
NAME    READY   STATUS    ... NODE     ...&#13;
sleep   1/1     Running   ... host02   ...&#13;
sleep3  0/1     Pending   ... &lt;none&gt;   ...</pre>&#13;
<p class="indent">The Pod will wait for the hardware to be available. Let’s delete our original Pod to free up room:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl delete pod sleep</span> &#13;
pod/sleep deleted</pre>&#13;
<p class="indent">Our new Pod will now start running:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get po -o wide</span>&#13;
NAME    READY   STATUS    ... NODE     ...&#13;
sleep3  1/1     Running   ... host02   ...</pre>&#13;
<p class="indent">As before, the Pod was scheduled onto <span class="literal">host02</span> because of the special hardware requirement.</p>&#13;
<p class="indent">Device drivers work identically from an allocation standpoint. In both cases, we use the <span class="literal">limits</span> field to identify our hardware requirements. The only difference is that we don’t need to patch the node manually to record the resource, because <span class="literal">kubelet</span> updates the node’s status automatically when the device driver registers. Additionally, <span class="literal">kubelet</span> invokes the device driver to perform any necessary allocation and configuration of the hardware when a container is created.</p>&#13;
<h3 class="h3" id="ch00lev1sec76">Final Thoughts</h3>&#13;
<p class="noindent">Unlike ideal applications, in the real world we often must deal with closely coupled application components and the need for specialized hardware. It’s critical that we account for those application requirements without losing the flexibility and resiliency that we gain from deploying our application to a Kubernetes cluster. In this chapter, we’ve seen how affinity and device drivers allow us to provide hints and resource requirements to the scheduler while still allowing it the flexibility to manage the application at scale dynamically.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_307"/>Scheduling is not the only concern we might have as we consider how to obtain the desired behavior and performance from real-world applications. In the next chapter, we’ll see how we can shape the processing and memory allocation for our Pods through the use of quality-of-service classes.<span epub:type="pagebreak" id="page_308"/></p>&#13;
</body></html>