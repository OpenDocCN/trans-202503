<html><head></head><body>
<h2 class="h2" id="ch10"><span epub:type="pagebreak" id="page_271"/><strong><span class="big">10</span><br/>EXPERIMENTAL DESIGN</strong></h2>
<div class="image1"><img alt="Image" src="../images/common.jpg"/></div>
<p class="noindent">The <em>scientific method</em> is the backbone of science. It involves the creation of theories from hypotheses that are tested through experiments and supported by evidence gleaned from those experiments. In this chapter, we’ll explore the design of experiments, or <em>experimental design</em>, a foundational part of the scientific method. Randomness is critical to successful experimental design for two main reasons. First, many measurements, regardless of the field, involve uncertainty or other factors outside of the researcher’s control, called <em>random noise</em>. We use randomness in experimental design to combat noise, like fighting fire with fire. Second, randomness makes the results conform to the expectations of statistics.</p>
<p class="indent">This chapter uses simulation (<a href="ch03.xhtml">Chapter 3</a>) to explore three common approaches to randomization in experimental design. Our running example mimics medical research, but the concepts involved apply everywhere.</p>
<h3 class="h3" id="ch00lev1_61"><span epub:type="pagebreak" id="page_272"/><strong>Randomization in Experiments</strong></h3>
<p class="noindent">Say a researcher wants to know the general public’s opinion regarding a ballot proposal to enact a noise ordinance restricting loud parties after 8 <small>PM</small>. He decides to use a phone survey, picking 100 names at random from the phone book and calling each on a Wednesday afternoon. He gets 64 answering machines, 36 pickups, and 17 willing to talk. Of those 17, 15 support the ordinance and 2 oppose it. Are these results a fair representation of the general public’s stance on this issue?</p>
<p class="indent">I suspect your answer is no because there are many possible sources of bias in the results. The researcher used the phone book, which generally lists only landlines; relied on those called to be willing to offer their opinion; and called on a weekday in the afternoon.</p>
<p class="indent">His sample is strongly biased toward retired people who tend to be older and (stereotypically) less inclined to have parties, so his results do not necessarily reflect the population as a whole. His sample excludes the group most likely to be affected by the ordinance—young people who use cell phones and were probably at school or work during the time he collected his data.</p>
<p class="indent">Avoiding such <em>sample bias</em> is an excellent reason to use randomness when conducting experiments. However, bias in surveys and polling is tough to correct and often leads to contrary results. More subtle, and perhaps more dangerous, is sample bias in selecting cohorts for medical studies.</p>
<p class="indent">The code in <em>bad_sample.py</em> generates a population where individuals are a combination of four randomly selected characteristics: age, income, smoker or not, and the average number of alcoholic drinks per week. The characteristics are linked to the person’s age, so an older person is likelier to have a higher income, not smoke, and drink less alcohol.</p>
<p class="indent">The <span class="literal">Population</span> function generates a population of individuals as a NumPy array:</p>
<pre class="pre">def Population(npop):
    pop = []
    for i in range(npop):
        age = 20 + int(55*rng.random())
        income = int(age*200 + age*1000*rng.random())
        income = int(income/1000)
        smoker = 0
        if (rng.random() &lt; (0.75 - age/100)):
            smoker = 1
        drink = 1.0 - age/100
        drink = int(14*drink*rng.random())
        pop.append([age, income, smoker, drink])
    return np.array(pop)</pre>
<p class="indent">The code loops append four-element lists of characteristics, each derived from the selected <span class="literal">age</span>. Notice, <span class="literal">age</span> is in years, <span class="literal">income</span> is in thousands, <span class="literal">smoker</span> is binary, and <span class="literal">drink</span> is average number of alcoholic drinks per week.</p>
<p class="indent"><span epub:type="pagebreak" id="page_273"/>The code selects a random subset of the population to simulate a random sample as a cohort for a medical study. For example, here’s one run of <em>bad_sample.py</em>:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 bad_sample.py 1000 10 1 mt19937 4004</span>
age   : 47.22  42.20  (t= 0.9782, p=0.32823)
income: 33.20  24.80  (t= 1.4284, p=0.15350)
smoker:  0.27   0.10  (t= 1.2126, p=0.22555)
drink :  3.14   3.20  (t=-0.0714, p=0.94309)</pre>
<p class="noindent">The population size is 1,000, from which we randomly select 10. The <span class="literal">1</span> argument samples once. As usual, the randomness source and seed follow.</p>
<p class="indent">We sort the output by age, income, smoker, and drink. The first column shows the mean of these values for the entire population. The second is the mean for the 10-person random sample. The values in parentheses are t-test results where <span class="literal">t</span> is the <em>t</em> statistic and <span class="literal">p</span> the p-value. A significant difference between the population and sample produces a low p-value. The sign of the <em>t</em> statistic is such that a positive <em>t</em> signifies that the population mean exceeds the sample mean.</p>
<p class="indent">In this case, the 10-person random sample was similar to the population regarding drinking and overall age. However, income was different, which might impact the results of a study that claims this sample is a suitable stand-in for the general population.</p>
<p class="indent">Let’s run the same command again:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 bad_sample.py 1000 10 1 mt19937 6502</span>
age   : 47.35  56.10  (t=-1.7258, p=0.08468)
income: 32.43  31.80  (t= 0.1118, p=0.91097)
smoker:  0.29   0.10  (t= 1.3255, p=0.18530)
drink :  3.20   2.60  (t= 0.7774, p=0.43713)</pre>
<p class="noindent">This random sample is significantly older than the population and is consequently much less likely to smoke. Try a few runs of <em>bad_sample.py</em>. While some samples are similar to the population, others deviate significantly.</p>
<p class="indent">Run <em>bad_sample.py</em>, but change the number of samples from 1 to 40:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 bad_sample.py 1000 10 40 mt19937 8080</span>
10 6.36529487 0.49581353</pre>
<p class="noindent">Each person in the population is a 4-tuple (age, income, smoker, drink). As such, they become a point in a four-dimensional space. The entire population becomes a single point in space if we consider the means of the individual age, income, smoker, and drink characters. The <em>bad_sample.py</em> file reports the mean (± standard error) Euclidean distance between the mean point of the population and the mean point of the sample, for each of the 40 samples taken. In this case, with a sample size of 10, the mean distance was 6.37 ± 0.50.</p>
<p class="indent"><span epub:type="pagebreak" id="page_274"/>Let’s increase the sample size to 100:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 bad_sample.py 1000 100 40 mt19937 8080</span>
100 2.12064926 0.23245698</pre>
<p class="noindent">The mean distance between the population and the samples decreases. If we have a larger sample, it should be a better representation of the population as a whole. This effect is the motivation behind working with as much data as possible, both for scientific research and machine learning models.</p>
<p class="indent">The file <em>bad_sample_test.py</em> repeats the process for a population of 10,000 and sample sizes from 10 to 5,000. For each sample size, we collect 40 samples. The results are amenable to plotting, as <a href="ch010.xhtml#ch010fig01">Figure 10-1</a> illustrates.</p>
<div class="image"><img alt="Image" id="ch010fig01" src="../images/10fig01.jpg"/></div>
<p class="figcap"><em>Figure 10-1: The Euclidean distance from the population mean and sample mean as a function of the sample size</em></p>
<p class="indent">The <em>x</em>-axis in <a href="ch010.xhtml#ch010fig01">Figure 10-1</a> is the number of samples in each set. We plot the average deviation of the mean of those samples from the population mean with error bars representing the standard error. I included end caps on the error bars to make the range easier to spot. Each point is the mean over 40 population-sample pairs.</p>
<p class="indent">Larger sample sizes lead to a smaller distance between the population and sample means, indicating that larger samples are more likely to be better representations of the population than smaller samples. Note that the error bars shrink as the sample size increases. The noise in the samples, or the deviation from the population mean, drops to nearly zero.</p>
<p class="indent">The error bars for a sample size of 10 are large, and the mean deviation is about 7, roughly seven times the mean deviation for samples of size 500. Notice also how quickly the curve decreases as the sample size increases. A large sample size might not be necessary for many experiments, but the strength of the effect being investigated influences sample size choice. More is always better.</p>
<p class="indent"><span epub:type="pagebreak" id="page_275"/>Experiments often report mean results along with the standard deviation or standard error of the mean. Then they perform a significance test, like a t-test; if the p-value is below the (arbitrary) threshold of 0.05, they declare victory and label the result as statistically significant. However, this is only part of the story. If the sample sizes are substantial, as might be the case in some cohort studies based on widely collected health data, then it’s entirely possible that the result is <em>p</em> &lt; 0.05, but, because of the large cohort, the <em>effect size</em> as measured by Cohen’s <em>d</em> is small. In other words, the effect is real but perhaps relatively meaningless in practice because the effect size is small. The moral of the story is to report effect sizes whenever possible.</p>
<p class="indent">The noise in <a href="ch010.xhtml#ch010fig01">Figure 10-1</a> for small sample sizes justifies my rant in the previous paragraph. If the effect is small, and we expect a slight deviation from a population mean, then a small sample size is unlikely to capture the effect because it’s swamped by the inherent noise in the sample itself. We’ll return to this notion later in the chapter when using experimental design techniques to compensate for selection bias.</p>
<p class="indent">Finally, the very thing that makes small sample sizes in <a href="ch010.xhtml#ch010fig01">Figure 10-1</a> a poor choice for research purposes is what drives evolution via genetic drift. In <a href="ch03.xhtml">Chapter 3</a>, we saw that the random mix of characters in a subpopulation separated by chance from a larger population led to a drift that ultimately produced a new species. In that sense, evolution is a poor researcher but a clever tinkerer able to take sample bias and turn it into something useful in the long run.</p>
<p class="indent">Now that we know sample bias can lead to poor research results, let’s try to find ways to compensate for it.</p>
<p class="indent">Randomization in experimental design comes in three broad categories. The code we’ll use throughout the remainder of the chapter supports all three approaches.</p>
<h4 class="h4" id="ch00lev2_78"><em><strong>Simple</strong></em></h4>
<p class="noindent">If the experiment is testing one outcome—such as the effectiveness of a particular treatment—we might build the treatment and control groups by recruiting members one at a time, flipping a coin to decide group assignment. We can simulate such <em>simple randomization</em> with a snippet of Python code:</p>
<pre class="pre">&gt;&gt;&gt; <span class="codestrong1">from RE import *</span> 
&gt;&gt;&gt; <span class="codestrong1">RE(mode="int", low=0, high=2).random(10)</span>
array([1, 1, 1, 0, 0, 0, 0, 1, 0, 1])
&gt;&gt;&gt; <span class="codestrong1">RE(mode="int", low=0, high=2).random(10)</span>
array([0, 1, 1, 0, 1, 1, 0, 0, 1, 1])</pre>
<p class="indent">We configure <span class="literal">RE</span> to return coin flips, 0 or 1. Both calls return 10 flips each. Therefore, we have the assignments for 20 participants: 11 for the treatment group and 9 for the control group.</p>
<p class="indent">Simple randomization seems like a good approach, but it has an obvious drawback, similar to what we observed when working with <em>bad_sample.py</em>.</p>
<p class="noindent"><span epub:type="pagebreak" id="page_276"/>Let’s try one more assignment for an experiment with 10 participants:</p>
<pre class="pre">&gt;&gt;&gt; <span class="codestrong1">RE(mode="int", low=0, high=2).random(10)</span>
array([0, 1, 1, 1, 0, 0, 1, 1, 1, 1])</pre>
<p class="noindent">If we follow simple randomization, we’ll have seven in the treatment group and only three in the control group—this seems unwise. Note that this isn’t a contrived example; the output is from a single run of that line of code.</p>
<p class="indent">While simple randomization isn’t the best approach when the study size is small, it often works well for larger study sizes:</p>
<pre class="pre">&gt;&gt;&gt; <span class="codestrong1">s = RE(mode="int", low=0, high=2).random(10000)</span>
&gt;&gt;&gt; <span class="codestrong1">np.bincount(s)</span>
array([4852, 5148])</pre>
<p class="noindent">In this case, the difference between treatment and control group size is a smaller fraction of the total study size, so we might expect each group to be similar.</p>
<p class="indent">Simple randomization’s weakness, especially for small study sizes, is that the number of subjects in the treatment and control groups might be highly imbalanced. We can remedy this with block randomization.</p>
<h4 class="h4" id="ch00lev2_79"><em><strong>Block</strong></em></h4>
<p class="noindent"><em>Block randomization</em> ensures that the number of subjects in each group is the same. For a binary experiment with only two groups—treatment and control—we first select a block size, usually between 4 and 6. We’ll use a constant block size of 4 for our example. Next, create all possible blocks of four subjects where the assignments are balanced. For us, this means all combinations of four-digit binary numbers where the number of 1s and 0s is the same:</p>
<p class="center">1100, 1010, 1001, 0110, 0101, 0011</p>
<p class="noindent">Each block has two 1s and two 0s.</p>
<p class="indent">We select as many blocks as needed to cover the subjects to generate the final group assignments. For completely balanced groups, the number of subjects must be a multiple of the block size, in this case, a multiple of four. So, for 32 subjects total, we need a sequence of eight blocks because 32 / 4 = 8. We randomly select the eight blocks from the set, but no matter which blocks we select, the total number of 1s and 0s will be the same.</p>
<p class="indent">For example, here’s a randomly selected sequence of eight blocks:</p>
<pre class="pre">&gt;&gt;&gt; <span class="codestrong1">from RE import *</span> 
&gt;&gt;&gt; <span class="codestrong1">b = ["1100","1010","1001","0110","0101","0011"]</span>
&gt;&gt;&gt; <span class="codestrong1">r = RE(mode="int", low=0, high=6)</span>
&gt;&gt;&gt; <span class="codestrong1">"".join([b[i] for i in r.random(8)])</span>
'11000011101010101010100100110110'</pre>
<p class="noindent"><span epub:type="pagebreak" id="page_277"/>There are 16 subjects assigned to both the treatment and the control groups. Run the code again and the sequence will be different, but the number assigned to each group is still 16. Apply the sequence to the 32 subjects selected for the study, perhaps by matching the assigned subject ID number in order with the sequence. See <a href="ch010.xhtml#ch010tab01">Table 10-1</a>.</p>
<p class="tabcap" id="ch010tab01"><strong>Table 10-1:</strong> Matching Subject ID and Assigned Group</p>
<table class="table-h">
<colgroup>
<col style="width:10%"/>
<col style="width:10%"/>
<col style="width:10%"/>
<col style="width:10%"/>
<col style="width:10%"/>
<col style="width:10%"/>
<col style="width:10%"/>
<col style="width:10%"/>
<col style="width:10%"/>
<col style="width:10%"/>
</colgroup>
<thead>
<tr>
<th class="tab_th"/>
<th class="tab_th"><strong>0</strong></th>
<th class="tab_th"><strong>1</strong></th>
<th class="tab_th"><strong>2</strong></th>
<th class="tab_th"><strong>3</strong></th>
<th class="tab_th"><strong>4</strong></th>
<th class="tab_th"><strong>5</strong></th>
<th class="tab_th"><strong>6</strong></th>
<th class="tab_th"><strong>7</strong></th>
<th class="tab_th"><strong>. . .</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="bg"><strong>Subject</strong></td>
<td class="bg">0</td>
<td class="bg">1</td>
<td class="bg">2</td>
<td class="bg">3</td>
<td class="bg">4</td>
<td class="bg">5</td>
<td class="bg">6</td>
<td class="bg">7</td>
<td class="bg">. . .</td>
</tr>
<tr>
<td class="bg"><strong>Group</strong></td>
<td class="bg">1</td>
<td class="bg">1</td>
<td class="bg">0</td>
<td class="bg">0</td>
<td class="bg">0</td>
<td class="bg">0</td>
<td class="bg">1</td>
<td class="bg">1</td>
<td class="bg">. . .</td>
</tr>
</tbody>
</table>
<p class="noindent">This continues for all 32 subjects.</p>
<p class="indent">Block randomization is an improvement over simple randomization in that it balances the number of subjects in the treatment and control groups. However, neither approach pays attention to other characteristics that may impact or mask treatment effects. If the outcome of the treatment is dramatic and broadly applicable, both simple and block randomization will show it. But if the treatment effect is weaker or relevant only to a specific demographic, then neither approach on its own is desirable. Enter stratified randomization.</p>
<h4 class="h4" id="ch00lev2_80"><em><strong>Stratified</strong></em></h4>
<p class="noindent">In <em>stratified randomization</em>, we assign subjects to treatment and control groups such that each group contains a consistent mix of subjects with other characteristics, called <em>covariates</em>, that we (the experimenters) believe are likely to influence the results. For example, if we use a block design for a research project that tests running endurance after taking a supplement for three months, and our treatment group happens to have an abundance of non-smokers under age 25, there will likely be a strong difference in endurance between the treatment and control groups at the end of the experiment.</p>
<p class="indent">Stratified randomization manages the group composition as much as possible by matching the covariates of the controls to the treatment group.</p>
<p class="indent">There are different approaches to stratified randomization. In our simulation, we’ll pick a treatment group subject at random from a large population of potential subjects. Based on the covariates for that subject, we’ll search for a matching subject to put in the control group. This way, we have balanced numbers in the treatment and control groups, and we’ll know that every person in the treatment group has a matched control to make the overall characteristics of both groups the same. In other words, we’ll adjust for variation in the covariates.</p>
<p class="indent">Our experience with <em>bad_sample.py</em> demonstrated that small study sizes are susceptible to dramatic variation in factors like age, income, smoking, and drinking. These are the covariates we’ll use in our simulation.</p>
<h3 class="h3" id="ch00lev1_62"><span epub:type="pagebreak" id="page_278"/><strong>Defining the Simulation</strong></h3>
<p class="noindent">The remainder of the chapter works with the code in <em>design.py</em>. As always, I recommend reading the code before continuing. We’ll do a detailed walkthrough in the next section; in this section, we’ll define what we hope to accomplish with the simulation.</p>
<p class="indent">Here’s the scenario: we want to evaluate the impact a novel supplement has on overall health after one year of use. To do this, we first select a treatment group that takes the supplement for a year. Then, we compare their overall health score to the score of a control group that did not take the supplement.</p>
<div class="note">
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>
<p class="notep"><em>Modern clinical trials often use</em> placebos<em>, or “fake” treatments, with the control group. Here the placebo would be a sugar pill that looks like the supplement. Neither the participants nor the researchers would be aware of who received what, thereby making the study double-blind. Alternatively, the control group might not be told anything about the supplement group. They would be queried or tested at the beginning of the study and again at the end, with no placebo given. While we’ll use the latter approach, either will work in this case.</em></p>
</div>
<p class="indent">The simulation is rigged; there will always be a positive health benefit for the treatment group. The strength of the effect is under our control. Naturally, there is no supplement; the point of the simulation is to understand how each of the three experimental design choices affects the experiment’s outcome. Is it easy to detect the treatment effect? How convinced might we be if we see an improvement that the measured effect is real? We’ll seek to answer these questions.</p>
<p class="indent">As we did with <em>bad_sample.py</em>, we’ll generate populations of individuals using the same covariates: age, income, smoking, and drinking. Unlike <em>bad_sample.py</em>, we won’t use age to influence the other covariates, and we’ll immediately place covariates into bins. In other words, we won’t say this person is 48 or 11; instead, we’ll select from three age bins, meaning age will be 0, 1, or 2. We’ll do the same for income and drinks per week. Smoking is binary, 0 or 1.</p>
<p class="indent">Each run of the code accumulates statistics for a user-selected number of experiments. We randomly generate a population, and select treatment and control groups according to the desired randomization scheme. We then apply the treatment (supplement), and use the health score for both groups to generate metrics. When all experiments are complete, we use the collected information to create output that provides insight into how well the randomization scheme worked.</p>
<p class="indent">As with all simulations, we must be careful to convince ourselves that the thing being simulated is a fair approximation of reality to the level we require. Remember Box’s adage: all models are wrong, but some are useful.</p>
<h3 class="h3" id="ch00lev1_63"><span epub:type="pagebreak" id="page_279"/><strong>Implementing the Simulation</strong></h3>
<p class="noindent">Let’s understand the essential parts of <em>design.py</em>, beginning with the main loop near the bottom of the file. Then, as needed, we’ll discuss functions that loop calls. After that comes the code to analyze the results. Algorithmically, the main loop repeats the following for the desired number of experiments (<span class="literal">nsimulations</span>):</p>
<ol>
<li class="noindent">Create a population (a list of <span class="literal">Person</span> objects).</li>
<li class="noindent">Select <span class="literal">control</span> and <span class="literal">treatment</span> cohorts according to the user-supplied randomization scheme (<span class="literal">typ</span>).</li>
<li class="noindent">Apply the treatment to the <span class="literal">treatment</span> group by calling the <span class="literal">Treat</span> method, passing the desired treatment effect size (<span class="literal">beta</span>).</li>
<li class="noindent">Accumulate stats on the <span class="literal">control</span> and <span class="literal">treatment</span> cohorts. This includes the per-subject health score and the average value of each covariate.</li>
<li class="noindent">Append the collected data to the <span class="literal">results</span> list, with each element being a dictionary that holds the results for that simulated experiment.</li>
</ol>
<p class="indent">The main loop is in <a href="ch010.xhtml#ch010list01">Listing 10-1</a>.</p>
<pre class="pre">results = []
for nsim in range(nsimulations):
    pop = []
    for i in range(npop):
        pop.append(Person())

    control, treatment = [Simple, Block, Stratified][typ](pop, nsubj)

    for subject in treatment:
        subject.Treat(beta)

    ch, c_age, c_income, c_smoker, c_drink = Summarize(control)
    th, t_age, t_income, t_smoker, t_drink = Summarize(treatment)

    results.append({
        "c_age": c_age,
        "c_income": c_income,
        "c_smoker": c_smoker,
        "c_drink": c_drink,
        "t_age": t_age,
        "t_income": t_income,
        "t_smoker": t_smoker,
        "t_drink": t_drink,
        "ttest": ttest_ind(th,ch),
        "d": Cohen_d(th,ch),
    })</pre>
<p class="list" id="ch010list01"><em>Listing 10-1: The main loop</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_280"/>There are five code paragraphs. The first creates <span class="literal">pop</span>, a list of <span class="literal">Person</span> objects from which we select <span class="literal">control</span> and <span class="literal">treatment</span> subjects.</p>
<p class="indent">The second code paragraph, a single line, selects the proper randomization function passing the population and the total number of subjects in the experiment (<span class="literal">nsubj</span>). For <span class="literal">Block</span> and <span class="literal">Stratified</span> randomization, the number of subjects in each group is always half the desired total. Both <span class="literal">control</span> and <span class="literal">treatment</span> are lists of <span class="literal">Person</span> objects selected from <span class="literal">pop</span>.</p>
<p class="indent">The next code paragraph calls the <span class="literal">Treat</span> method on each subject in the <span class="literal">treatment</span> group. The argument, <span class="literal">beta</span>, [0, 1), is the user-supplied treatment effect size, where a higher <span class="literal">beta</span> implies a stronger positive treatment effect. In other words, the supplement’s effectiveness improves as <span class="literal">beta</span> increases.</p>
<p class="indent">The fourth code paragraph calculates the per-subject health scores and covariate averages for the <span class="literal">control</span> and <span class="literal">treatment</span> groups.</p>
<p class="indent">The last code paragraph appends the results of this experiment to the <span class="literal">results</span> list. The call to <span class="literal">ttest_ind</span> performs a t-test between the treatment and control health scores while <span class="literal">Cohen_d</span> calculates Cohen’s <em>d</em> to measure the effect size.</p>
<p class="indent">Let’s explore the supporting cast called by the main loop, followed by each randomization function. We want to simulate the latter correctly.</p>
<h4 class="h4" id="ch00lev2_81"><em><strong>Functions and Classes</strong></em></h4>
<p class="noindent">Populations are lists of <span class="literal">Person</span> instances (<a href="ch010.xhtml#ch010list02">Listing 10-2</a>).</p>
<pre class="pre">class Person():
    def __init__(self):
        self.age = int(3*rng.random())
        self.income = int(3*rng.random())
        self.smoker = 0
        if (rng.random() &lt; 0.2):
            self.smoker = 1
        self.drink = int(3*rng.random())
        self.adj = 2*(rng.random() - 0.5)

    def Health(self):
        return 3*(2-self.age) + 2*self.income - 2*self.smoker - self.drink + self.adj

    def Treat(self, beta=0.03):
        self.adj += 3*binomial(300, beta, rng) / 300  # [0,1]</pre>
<p class="list" id="ch010list02"><em>Listing 10-2: The</em> <span class="codeitalic">Person</span> <em>class</em></p>
<p class="indent">A person is a collection of five characteristics. Three of them are integers in [0, 2]: <span class="literal">age</span>, <span class="literal">income</span>, and <span class="literal">drink</span>. Notice the call to the globally defined <span class="literal">rng</span>, an instance of the <span class="literal">RE</span> class according to the supplied randomness source and seed. The <span class="literal">smoker</span> characteristic is binary, so a person has a 20 percent chance of being a smoker. The fifth characteristic, <span class="literal">adj</span>, is a random float in [–1, 1]. We’ll learn why we use this binning scheme in the next section.</p>
<p class="indent"><span epub:type="pagebreak" id="page_281"/>The <span class="literal">Health</span> method returns a float indicating the person’s overall health:</p>
<p class="center"><span class="literal">Health</span> = 3 × (2 – <span class="literal">age</span>) + 2 × <span class="literal">income</span> – 2 × <span class="literal">smoker</span> – <span class="literal">drink</span> + <span class="literal">adj</span></p>
<p class="noindent">Better health (a higher float) is associated with younger age, higher income bracket, not being a smoker, and minimized drinking, plus the random adjustment.</p>
<p class="indent">The <span class="literal">Treat</span> method applies the treatment by altering <span class="literal">adj</span> based on <span class="literal">beta</span>, the desired treatment effect. It does this by increasing <span class="literal">adj</span> by three times a random draw from a binomial distribution using a fixed number of trials (300).</p>
<p class="indent">We haven’t worked with binomial distributions before. If there’s an event with a probability <em>p</em> of happening on any given trial—think of flipping a loaded coin with probability <em>p</em> of landing heads up—and there are <em>n</em> trials, then the number of successful events in <em>n</em> trials follows a binomial distribution for an expected number of outcomes, [0, <em>n</em>].</p>
<p class="indent">For example, if the probability of an event is <em>p</em> = 0.7, or 70 percent, and there are <em>n</em> = 10 trials, then the expected number of events for repetitions follows the distribution in <a href="ch010.xhtml#ch010fig02">Figure 10-2</a>.</p>
<div class="image"><img alt="Image" id="ch010fig02" src="../images/10fig02.jpg"/></div>
<p class="figcap"><em>Figure 10-2: The binomial distribution for 10 trials and 70 percent probability of success per trial</em></p>
<p class="indent">The most frequent number of successes in 10 trials at 70 percent probability of success is 7, as we might expect. However, about 2.5 percent of the time, every trial was successful.</p>
<p class="indent">As the probability of success increases, so does the number of possible successful events over a given number of trials. In <span class="literal">Treat</span>, we scale the number of successful trials for probability <span class="literal">beta</span> by 300; thus, the entire expression becomes a number in [0, 1], which we multiply by 3 and add to <span class="literal">adj</span>. When the number of trials is large, the binomial distribution looks like a narrow normal distribution, where the treatment effect is narrowly centered on the supplied <span class="literal">beta</span> value.</p>
<p class="indent"><span epub:type="pagebreak" id="page_282"/>The <span class="literal">binomial</span> function simulates a draw from a binomial distribution using the uniform distribution supplied by <span class="literal">rng</span>. This algorithm is inefficient because it uses many calls to <span class="literal">rng</span> to return one sample from the desired binomial distribution. Still, it’s good enough for us (<a href="ch010.xhtml#ch010list03">Listing 10-3</a>).</p>
<pre class="pre">def binomial(n,a,rng):
    k = 0
    p = a if (a &lt;= 0.5) else 1.0-a
    for i in range(n):
        if (rng.random() &lt;= p):
            k += 1
    return k if (a &lt;= 0.5) else n-k</pre>
<p class="list" id="ch010list03"><em>Listing 10-3: Sampling from a binomial distribution</em></p>
<p class="noindent">The number of successful trials is in <span class="literal">k</span>. The conditional return and assignment to <span class="literal">p</span> makes use of symmetry in the binomial distribution to return the proper sample for any desired probability of success for a given probability (<span class="literal">a</span>).</p>
<p class="indent">The main loop calls two more functions, <span class="literal">Cohen_d</span> and <span class="literal">Summarize</span>, as in <a href="ch010.xhtml#ch010list04">Listing 10-4</a>.</p>
<pre class="pre">def Cohen_d(a,b):
    s1 = np.std(a, ddof=1)**2
    s2 = np.std(b, ddof=1)**2
    return (a.mean()-b.mean())/np.sqrt(0.5*(s1+s2))

def Summarize(subjects):
    h = []
    age = income = smoker = drink = 0.0
    for subject in subjects:
        h.append(subject.Health())
        age += subject.age
        income += subject.income
        smoker += subject.smoker
        drink += subject.drink
    age /= len(subjects)
    income /= len(subjects)
    smoker /= len(subjects)
    drink /= len(subjects)
    return np.array(h), age, income, smoker, drink</pre>
<p class="list" id="ch010list04"><em>Listing 10-4: Additional helper functions</em></p>
<p class="indent">The <span class="literal">Cohen_d</span> function measures the effect size by calculating the difference in the means of two datasets along with the square root of their averaged variances.</p>
<p class="indent">The <span class="literal">Summarize</span> function queries a collection of subjects from either the control or treatment group to return a vector of subject health scores along with the mean age, income, smoke, and drink values over the subjects.</p>
<p class="indent"><span epub:type="pagebreak" id="page_283"/>Let’s get to the heart of the matter and discuss the different randomization schemes themselves.</p>
<h4 class="h4" id="ch00lev2_82"><em><strong>Schemes</strong></em></h4>
<p class="noindent"><a href="ch010.xhtml#ch010list05">Listing 10-5</a> uses simple randomization to flip a coin, assigning selected subjects to either the treatment or control group.</p>
<pre class="pre">def Simple(pop, nsubj):
    order = np.argsort(rng.random(len(pop)))
    c = []; t = []
    for k in range(nsubj):
        if (rng.random() &lt; 0.5):
            c.append(pop[order[k]])
        else:
            t.append(pop[order[k]])
    return c,t</pre>
<p class="list" id="ch010list05"><em>Listing 10-5: Simple random assignment</em></p>
<p class="indent">The <span class="literal">Simple</span> function first generates a random ordering of the population. This isn’t strictly necessary, as we randomly generated <span class="literal">pop</span>, but we shouldn’t assume that if we don’t need to.</p>
<p class="indent">The <span class="literal">for</span> loop picks subjects sequentially and, based on the coin flip, assigns them to the treatment or the control group. Note that the coin flip means the number of subjects in each group might not be the same. When done, we return both lists.</p>
<p class="indent">Block randomization ensures balanced treatment and control group sizes, as shown in <a href="ch010.xhtml#ch010list06">Listing 10-6</a>.</p>
<pre class="pre">def Block(pop, nsubj):
    ns = 4*(nsubj//4)
    blocks = ["1100","1010","1001","0110","0101","0011"]
    nblocks = ns//4
    seq = ""
    for i in range(nblocks):
        n = int(len(blocks)*rng.random())
        seq += blocks[n]
    order = np.argsort(rng.random(len(pop)))
    c = [];  t = []
    for i in range(ns):
        if (seq[i] == "1"):
            t.append(pop[order[i]])
        else:
            c.append(pop[order[i]])
    return c,t</pre>
<p class="list" id="ch010list06"><em>Listing 10-6: Block randomization</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_284"/>The first line of <span class="literal">Block</span> uses integer division to set <span class="literal">ns</span> to the nearest multiple of 4 less than or equal to <span class="literal">nsubj</span>. Then come the block definitions as strings of binary numbers. These blocks are the six ways we can split four items evenly among two groups, treatment (1) and control (0).</p>
<p class="indent">We create the <span class="literal">seq</span> string with the number of blocks (<span class="literal">nblocks</span>) by randomly concatenating block definitions until the string is <span class="literal">ns</span> characters long.</p>
<p class="indent">The final <span class="literal">for</span> loop mimics <span class="literal">Simple</span> in randomizing the order of <span class="literal">pop</span> before assigning subjects to treatment and control groups based on the current character of <span class="literal">seq</span>.</p>
<p class="indent">Our approach to stratified randomization assigns a random subject from the population to the treatment group, searches the population for an unassigned person with the same characteristics, and assigns them to the control group. This way, both groups are balanced in number and in overall characteristics. In code, this becomes <a href="ch010.xhtml#ch010list07">Listing 10-7</a>.</p>
<pre class="pre">def Stratified(pop, nsubj):
    def match(n,m,pop,selected):
        if (selected[m]):
            return False
        if (pop[n].age != pop[m].age):
            return False
        if (pop[n].income != pop[m].income):
            return False
        if (pop[n].smoker != pop[m].smoker):
            return False
        if (pop[n].drink != pop[m].drink):
            return False
        return True
    
    selected = np.zeros(len(pop), dtype="uint8")
    c = [];  t = []
    while (len(t) &lt; nsubj//2):
        n = int(len(pop)*rng.random())
        while (selected[n] == 1):
            n = int(len(pop)*rng.random())
        selected[n] = 1
        t.append(pop[n])
        m = int(len(pop)*rng.random())
        while (not match(n,m, pop, selected)):
            m = int(len(pop)*rng.random())
        selected[m] = 1
        c.append(pop[m])
    return c,t</pre>
<p class="list" id="ch010list07"><em>Listing 10-7: Stratified randomization</em></p>
<p class="indent"><a href="ch010.xhtml#ch010list07">Listing 10-7</a> creates a flag vector, <span class="literal">selected</span>, to mark members of the population already assigned to a group. The <span class="literal">while</span> loop then runs until the treatment group contains half the requested number of subjects. The body of <span epub:type="pagebreak" id="page_285"/>the <span class="literal">while</span> loop sets <span class="literal">n</span> to the index of an unselected member of the population, who becomes a member of the treatment group. We update <span class="literal">selected</span> once <span class="literal">n</span> is found, and append the person to the treatment group (<span class="literal">t</span>).</p>
<p class="indent">Next, we find another unselected member of the population who has characteristics matching those of the person we just added to the treatment group; this is why the <span class="literal">Person</span> class uses bins for characteristics. There are only 3 × 3 × 2 × 3 = 54 possible combinations of characters. With a large population, we’re virtually assured of finding a match, which happens when the <span class="literal">match</span> embedded function returns <span class="literal">True</span>. After we append the match to the control group, the outer <span class="literal">while</span> loop continues until it assigns all subjects.</p>
<p class="indent">The remaining code in <em>design.py</em> analyzes the results of multiple experiments. While I won’t walk through it, I’ll describe the analysis in the next section.</p>
<h3 class="h3" id="ch00lev1_64"><strong>Exploring the Simulation</strong></h3>
<p class="noindent">We’re ready to run some experiments. First, let’s orient ourselves with a quick recap:</p>
<ul>
<li class="noindent">We are simulating experiments using a known positive treatment effect of a size we supply.</li>
<li class="noindent">We are supplying the randomization scheme: simple, block, or stratified.</li>
<li class="noindent">We are evaluating the results of many experiments to understand the randomization scheme’s influence.</li>
</ul>
<p class="indent">We aren’t simulating a single experiment, but dozens to hundreds. We’ll evaluate the combined results from these experiments to (hopefully) arrive at a picture of the subtle influence randomization schemes have on experiment outcomes. The <em>design.py</em> file is a stand-in for an entire universe of treatment studies, all performed using the given randomization scheme.</p>
<p class="indent">Let’s dive in. The <em>design.py</em> file expects several command line arguments:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 design.py</span>
design &lt;npop&gt; &lt;nsubj&gt; &lt;beta&gt; &lt;nsim&gt; &lt;type&gt; &lt;plot&gt; [&lt;kind&gt; | &lt;kind&gt; &lt;seed&gt;]

  &lt;npop&gt;  -  population size (e.g. 1000)
  &lt;nsubj&gt; -  number of subjects in the experiment (e.g. 40)
  &lt;beta&gt;  -  supplement effect strength [0..1]
  &lt;nsim&gt;  -  number of simulations to run (e.g. 100)
  &lt;typ&gt;   -  selection type: 0=simple, 1=block, 2=stratified
  &lt;plot&gt;  -  1=show plot, 0=no plot
  &lt;kind&gt;  -  randomness source
  &lt;seed&gt;  -  seed value</pre>
<p class="indent">We must supply the population size (<span class="literal">npop</span>), number of subjects in each experiment (<span class="literal">nsubj</span>), treatment effect strength (<span class="literal">beta</span>), number of experiments <span epub:type="pagebreak" id="page_286"/>to simulate (<span class="literal">nsim</span>), and type of randomization to use (<span class="literal">type</span>). As always, the randomness source and optional seed value are last.</p>
<h4 class="h4" id="ch00lev2_83"><em><strong>Simple</strong></em></h4>
<p class="noindent">We can run our first simulation like so:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 design.py 10000 32 0.3 40 0 1 minstd 6809</span>

mean p-value (lowest) : 0.01138
mean p-value (highest): 0.82919

mean Cohen's (lowest) : 1.00568
mean Cohen's (highest): 0.07732

delta age   : (high, low, t, p) = (0.16556, 0.41945, -3.27589, 0.01691)
delta income: (high, low, t, p) = (0.21191, 0.30570, -0.64742, 0.54132)
delta smoker: (high, low, t, p) = (0.12377, 0.11255,  0.21945, 0.83357)
delta drink : (high, low, t, p) = (0.38284, 0.22193,  0.83403, 0.43620)</pre>
<p class="indent">A plot should appear along with the text output; we’ll get to it momentarily. We asked for 40 simulations of an experiment where we selected 32 subjects from a pool of 10,000 using simple randomization (0). The treatment effect size was 0.3, meaning the <span class="literal">Treat</span> method on each <span class="literal">Person</span> object added a random value centered on 0.3 to the overall health of the individual.</p>
<p class="indent">These stats are drawn from the results of the simulations. The first two lines show the mean p-value for the lowest and highest 10 percent of experiments. Recall, the p-value for an experiment is from the health scores for the treatment and control groups. In this case, each experiment had approximately 16 subjects per group and there were 40 simulated experiments, so the highest and lowest 10 percent p-values correspond to 4 experiments. Because we used simple randomization, the treatment and control group size for a particular simulated experiment isn’t always 16.</p>
<p class="indent">The next two lines report the mean Cohen’s <em>d</em> for the same set of experiments. Cohen’s <em>d</em> is a scaled version of the difference in the means between the treatment and control groups. Therefore, as we’ll see when using stratified randomization, we might expect the mean effect size for at least the lowest 10 percent group to be about the same as the requested effect size. Here, we asked for a smaller effect size of 0.3, but the mean was a substantial 1.0. For the highest 10 percent group, the effect size was insignificant, which matches the p-value of 0.83. For this group of experiments, we found no meaningful difference between the treatment and control groups.</p>
<p class="indent">The final four lines of output present the means of the per-experiment covariate <em>differences</em> for each group, low p-value and high p-value. We use a t-test here to see if the covariate differences between treatment and control significantly differed between the groups. For this run, the age difference was significant.</p>
<p class="indent"><span epub:type="pagebreak" id="page_287"/>The <em>t</em> statistic for age is negative, meaning the low p-value experiments consisted of treatment and control groups with large age differences. In other words, for the low p-value experiments, the age covariate for the treatment and control groups was more likely to be different than for the high p-value experiments. Age is the most prominent factor in the health score, so a significant imbalance between treatment and control groups strongly affects measured results.</p>
<p class="indent">So, what should we make of these results? We ran many simulations, and, even for the best performing of them—those that showed a significant difference after treatment—we shouldn’t believe the results, as the covariates of those experiments were quite different. If we ran a single experiment, we’d likely see no effect and judge the treatment as a failure. For those cases where we did see an effect, it was likely artificially inflated because the treatment group was quite different from the control group in an important covariate.</p>
<p class="indent">Remember that these results are for 40 experiments, meaning 32 subjects and simple random sampling into treatment and control groups tend toward unreliable results that either exaggerate the effectiveness of the treatment or mask it entirely.</p>
<p class="indent">The left side of <a href="ch010.xhtml#ch010fig03">Figure 10-3</a> shows the <em>design.py</em> plot, a histogram displaying the t-test p-values between the health of the treatment and control groups for the 40 simulated experiments. The results to the left of the vertical dashed line at 0.05 indicate results that would, typically, be heralded as statistically significant.</p>
<div class="image"><img alt="Image" id="ch010fig03" src="../images/10fig03.jpg"/></div>
<p class="figcap"><em>Figure 10-3: Example plot output from</em> design.py <em>by randomization type</em></p>
<p class="indent">From the leftmost plot in <a href="ch010.xhtml#ch010fig03">Figure 10-3</a>, we see that about 12.5 percent of the experiments yielded a statistically significant difference. These are the experiments we’d likely read about in a scientific journal. The remaining 87.5 percent of the experiments didn’t result in a statistically significant treatment effect. Proper scientific adherence states that these experiments should be published as negative results, but such papers seldom appear—a known issue in the scientific literature.</p>
<p class="indent">While we don’t know a priori that the treatment effect is present and positive in the real world, in our case we do—and that all 40 simulated experiments should have found a positive result. So, why wasn’t it found? The size of the cohort is a contributing factor that we’ll experiment with shortly, but <span epub:type="pagebreak" id="page_288"/>as shown in the simulation, so is the imbalance of covariates due to simple randomization, both in masking and exaggerating the effect.</p>
<h4 class="h4" id="ch00lev2_84"><em><strong>Block</strong></em></h4>
<p class="noindent">Switching to block randomization requires changing a 0 to a 1 on the command line:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 design.py 10000 32 0.3 40 1 1 minstd 6809</span>

mean p-value (lowest) : 0.02021
mean p-value (highest): 0.94917

mean Cohen's (lowest) : 0.89363
mean Cohen's (highest): 0.00087

delta age   : (high, low, t, p) = (0.15625, 0.45312, -2.12870, 0.07735)
delta income: (high, low, t, p) = (0.21875, 0.42188, -1.53562, 0.17554)
delta smoker: (high, low, t, p) = (0.10938, 0.14062, -0.35729, 0.73310)
delta drink : (high, low, t, p) = (0.37500, 0.17188,  1.80858, 0.12051)</pre>
<p class="indent">Not much has changed when compared to the simple randomization results. Selecting balanced treatment and control group sizes for each experiment has eliminated any possible effect due to bad splits. Still, that effect is unlikely to show up at the level of many experiments, which we’re considering here. The middle plot in <a href="ch010.xhtml#ch010fig03">Figure 10-3</a> is much like the simple randomization plot; in both, 12.5 percent of experiments were statistically significant at the <em>p</em> &lt; 0.05 level. However, age is again highly different between the lowest and highest p-value experiment groups, though not at the level it was for the simple randomization case. We might improve this by increasing the number of subjects, but let’s switch to stratified randomization first and see what happens.</p>
<h4 class="h4" id="ch00lev2_85"><em><strong>Stratified</strong></em></h4>
<p class="noindent">To switch to stratified randomization, change the 1 on the command line to a 2:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 design.py 10000 32 0.3 40 2 1 minstd 6809</span>

mean p-value (lowest) : 0.19833
mean p-value (highest): 0.64602

mean Cohen's (lowest) : 0.46910
mean Cohen's (highest): 0.16422</pre>
<p class="indent">Note that the output lacks any “delta” lines because treatment and control groups have the same covariate makeup in stratified randomization, so all the deltas are zero; there is nothing to report. Recall, the “delta” lines <span epub:type="pagebreak" id="page_289"/>refer to the means of the individual experiment differences between treatment and control groups. Those differences are zero because we selected a matched control group subject for each treatment group subject.</p>
<p class="indent">The plot is on the right in <a href="ch010.xhtml#ch010fig03">Figure 10-3</a>. It doesn’t look like the simple or block randomization plots, as the covariate effect has been compensated for by the stratified (matching) selection process. What the plot shows is due to other factors.</p>
<p class="indent">However, no experiment produced a p-value below the 0.05 threshold. After properly accounting for the covariates we believe influence health, the treatment effect, while present, is too weak to detect with only 32 subjects in the study.</p>
<p class="indent">Cohen’s <em>d</em> for the lowest p-value group is 0.47—not too far from the 0.3 we should get if we detect the treatment effect consistently. To improve matters further, we’ll adjust the study size by increasing the number of subjects.</p>
<h5 class="h5"><strong>Increasing the Number of Subjects</strong></h5>
<p class="noindent">Let’s move from 32 subjects to 128 subjects:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 design.py 10000 128 0.3 40 2 1 minstd 6809</span>

mean p-value (lowest) : 0.06149
mean p-value (highest): 0.20789

mean Cohen's (lowest) : 0.33364
mean Cohen's (highest): 0.22381</pre>
<p class="noindent">The lowest 10 percent of experiments has a mean p-value of 0.06 and a mean <em>d</em> of 0.334, so we’re getting closer, but no experiments passed the magic threshold of 0.05.</p>
<p class="indent">Let’s double the study size again:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 design.py 10000 256 0.3 40 2 1 minstd 6809</span>

mean p-value (lowest) : 0.00781
mean p-value (highest): 0.05495

mean Cohen's (lowest) : 0.33582
mean Cohen's (highest): 0.24168</pre>
<p class="noindent">Now even the highest p-value group is near 0.05, and both <em>d</em> values are in the 0.3 ballpark. The plot (not shown) illustrates that almost all experiments produce <em>p</em> &lt; 0.05 results.</p>
<p class="indent">With stratified randomization, we’ve accounted for covariates, so we’re seeing the actual treatment effect in these means. Still, we needed a sufficiently large number of subjects to tease the treatment effect out of the data. Let’s find out how many subjects we need for a given desired effect size.</p>
<p class="indent"><span epub:type="pagebreak" id="page_290"/>The code in <em>cohen_d_test.py</em> uses <em>design.py</em> to estimate the number of subjects necessary to achieve a mean p-value of 0.05 or less in the highest 10 percent of experiments. A run of the code produces:</p>
<pre class="pre">0.9:   50 subjects, p=0.03350000, d=0.7079
0.8:   60 subjects, p=0.03402000, d=0.6172
0.7:   80 subjects, p=0.02784000, d=0.5373
0.6:   90 subjects, p=0.03872000, d=0.4702
0.5:  110 subjects, p=0.04107000, d=0.4141
0.4:  170 subjects, p=0.04417000, d=0.3213
0.3:  280 subjects, p=0.04682000, d=0.2431
0.2:  580 subjects, p=0.04017000, d=0.1724
0.1: 2370 subjects, p=0.04922000, d=0.0811</pre>
<p class="indent">The requested effect size is on the left, followed by the estimated number of subjects necessary, the p-value, and <em>d</em> for the highest 10 percent of simulated experiments. My code run took about five hours, mostly working the <em>d</em> = 0.1 case. Generally, the printed <em>d</em> values are close to the requested values, which is a good sign.</p>
<p class="indent">The code is a loop over the desired effect sizes with an inner <span class="literal">while</span> loop that increments the number of subjects until the mean p-value is 0.05 or less:</p>
<pre class="pre">def RunTest(beta, nsubj):
    cmd = "python3 design.py 100000 %d %0.1f 20 2 minstd 6809 &gt;/tmp/xyzzy"
    os.system(cmd % (nsubj,beta))
    lines = [i[:-1] for i in open("/tmp/xyzzy")]
    pv = float(lines[2].split()[-1])
    d =  float(lines[5].split()[-1])
    return pv,d

base = 10
for beta in [0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2,0.1]:
    pvalue = 10.0
    k = 1
    while (pvalue &gt; 0.05):
        pvalue,d = RunTest(beta, k*base)
        k += 1
    print("%0.1f: %3d subjects, p=%0.8f, d=%0.4f" % (beta, k*base, pvalue, d), flush=True)</pre>
<p class="indent">The number of subjects starts at 10 (<span class="literal">base</span>) and is multiplied by a value <span class="literal">k</span> that increments by 1 inside the <span class="literal">while</span> loop until the p-value is at or below 0.05. The <span class="literal">RunTest</span> function constructs the <em>design.py</em> command line, executes it, and parses the output file to get <em>p</em> and <em>d</em>.</p>
<h5 class="h5"><strong>Calculating the Simulation’s Statistical Power</strong></h5>
<p class="noindent">Let’s use <span class="literal">TTestIndPower</span> from the <span class="literal">statsmodels</span> package to see whether the simulation captures the correct number of subjects necessary for a desired <em>d</em> value.</p>
<p class="indent"><span epub:type="pagebreak" id="page_291"/>The <span class="literal">TTestIndPower</span> class performs a <em>power</em> analysis for two independent samples, the treatment and control groups. See <em>power_analysis.py</em>, which produces output for the same set of effect sizes as <em>cohen_d_test.py</em>. Comparing the two in terms of the number of subjects results in <a href="ch010.xhtml#ch010tab02">Table 10-2</a>.</p>
<p class="tabcap" id="ch010tab02"><strong>Table 10-2:</strong> Comparing the Calculated and Estimated Number of Subjects</p>
<table class="table-h">
<colgroup>
<col style="width:25%"/>
<col style="width:45%"/>
<col style="width:30%"/>
</colgroup>
<thead>
<tr>
<th class="tab_th"><em><strong>d</strong></em></th>
<th class="tab_th"><strong>Calculated</strong></th>
<th class="tab_th"><strong>Estimated</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="bg1">0.9</td>
<td class="bg1">26</td>
<td class="bg1">50</td>
</tr>
<tr>
<td class="bg">0.8</td>
<td class="bg">33</td>
<td class="bg">60</td>
</tr>
<tr>
<td class="bg1">0.7</td>
<td class="bg1">43</td>
<td class="bg1">80</td>
</tr>
<tr>
<td class="bg">0.6</td>
<td class="bg">59</td>
<td class="bg">90</td>
</tr>
<tr>
<td class="bg1">0.5</td>
<td class="bg1">85</td>
<td class="bg1">110</td>
</tr>
<tr>
<td class="bg">0.4</td>
<td class="bg">132</td>
<td class="bg">170</td>
</tr>
<tr>
<td class="bg1">0.3</td>
<td class="bg1">234</td>
<td class="bg1">280</td>
</tr>
<tr>
<td class="bg">0.2</td>
<td class="bg">526</td>
<td class="bg">580</td>
</tr>
<tr>
<td class="bg1">0.1</td>
<td class="bg1">2,102</td>
<td class="bg1">2,370</td>
</tr>
</tbody>
</table>
<p class="noindent">The calculated results are for a threshold of 0.05 (alpha) and a power of 0.9, meaning we want a 90 percent chance of achieving at least 0.05 as the p-value for the t-test between the treatment and control groups. I selected a power of 0.9 because the empirical code seeks to make the mean of the top 10 percent of the highest p-values 0.05 or less, implying that almost all simulated experiments will find a result at a p-value of 0.05 or lower. The agreement between the calculated and estimated numbers of subjects is rough but reasonable, which validates the simulation.</p>
<h3 class="h3" id="ch00lev1_65"><strong>Exercises</strong></h3>
<p class="noindent">This chapter has only two exercises, both of which are challenging.</p>
<h4 class="h4" id="ch00lev2_86"><em><strong>Working with Two Treatments</strong></em></h4>
<p class="noindent">Apply block randomization to a study with three conditions: control, treatment 1, and treatment 2. The block size should be a multiple of the number of conditions; fix the block size at 6, meaning each block will use each condition twice. For example, <span class="literal">[0,0,1,1,2,2]</span> is a valid block of six subjects where 0 is control, 1 receives treatment 1, and 2 receives treatment 2.</p>
<p class="indent">We need a way to select at random from the set containing all possible permutations of the set of three conditions used twice. In other words, we’d like a NumPy array where each row is a valid block. Use code like this:</p>
<pre class="pre">import numpy as np
from itertools import permutations
b = [0,0,1,1,2,2]
l = list(set(permutations(b)))
blocks = np.array(l)</pre>
<p class="noindent"><span epub:type="pagebreak" id="page_292"/>The <span class="literal">blocks</span> array is a 90×6 matrix with 90 unique blocks to select from when building a balanced cohort. The <span class="literal">permutations</span> function returns an iterable that <span class="literal">set</span> uses to return all unique permutations of the basic block, <span class="literal">b</span>. To make a NumPy array, convert the set into a list before passing it to <span class="literal">array</span>.</p>
<p class="indent">Your task: alter a copy of <em>design.py</em> to select subjects for simple and block randomization for the control, treatment 1, and treatment 2 groups. For simple randomization, roll a three-sided die. Ignore stratified randomization for now.</p>
<h4 class="h4" id="ch00lev2_87"><em><strong>Combining Block and Stratified Randomization</strong></em></h4>
<p class="noindent">To combine block and stratified randomization, create blocks where all subjects are matched according to the desired covariates, and each block includes all conditions in a balanced manner. If there are three conditions—as in the previous exercise—and the blocks have six subjects each, then the subjects in each block must be matched to some level in terms of covariates.</p>
<p class="indent">Alter the copy of <em>design.py</em> from Possibility 1, but account for the 56 possible combinations of the four covariates. The subjects placed into a block must all have the same covariate values.</p>
<p class="indent">If the block is <span class="literal">[0,1,2,1,2,0]</span> and the covariate set is <span class="literal">[2,1,0,1]</span>, for example, then search the population for six subjects in age group 2, income group 1, smoker 0, and drink 1, assigning them to the conditions as dictated by the block. Continue for <em>n</em> subjects where <em>n</em> is a multiple of six, the block size. Add this capability to your altered <em>design.py</em> from Exercise 1.</p>
<h3 class="h3" id="ch00lev1_66"><strong>Summary</strong></h3>
<p class="noindent">This chapter focused on randomization in experimental design, particularly during subject selection. After briefly discussing the need for randomization, we explored the most common types: simple, block, and stratified.</p>
<p class="indent">Simple randomization uses coin flips or dice rolls to place subjects in treatment or control groups. While this process can introduce bias due to unequal group sizes, block randomization removes this possibility by ensuring that treatment and control group sizes are balanced.</p>
<p class="indent">We learned about covariates, factors outside the study scope that may affect the results by enhancing or masking any possible treatment effect. Stratified randomization uses covariates to construct treatment and control groups with matching characteristics.</p>
<p class="indent">We then created a simulation to test randomization schemes on the outcome of an experiment where we knew the treatment effect we should see. We learned that small cohorts using simple randomization are prone to bias; while block randomization removes bias due to imbalanced group sizes, stratified randomization corrects for bias in the treatment and control groups. This allows us to measure the effect we expect from the simulation, provided the number of subjects is of some minimal size.</p>
<p class="indent"><span epub:type="pagebreak" id="page_293"/>Lastly, we matched the number of subjects necessary in a stratified setting to achieve the desired effect size at a <em>p</em> &lt; 0.05 level for most of our simulated experiments to the number of subjects indicated by a t-test-based power analysis. The empirical number was in reasonably good agreement with the model, so we claimed this as validation of our approach.</p>
<p class="indent">Let’s continue our journey with randomized algorithms, an essential class of algorithms in computer science.<span epub:type="pagebreak" id="page_294"/></p>
</body></html>