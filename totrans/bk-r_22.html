<html><head></head><body>
<h2 class="h2" id="ch18"><span epub:type="pagebreak" id="page_385"/><span class="big"><strong>18</strong></span><br/><strong>HYPOTHESIS TESTING</strong></h2>&#13;
<div class="image"><img src="../images/common-01.jpg" alt="image"/></div>&#13;
<p class="noindent">In this chapter, you’ll build on your experience with confidence intervals and sampling distributions to make more formal statements about the value of a true, unknown parameter of interest. For this, you’ll learn about frequentist <em>hypothesis testing</em>, where a probability from a relevant sampling distribution is used as evidence against some claim about the true value. When a probability is used in this way, it is referred to as a <em>p</em>-value. In this chapter, I talk about interpreting results for relatively basic statistics, but you can apply the same concepts to statistics arising from more complicated methods (such as regression modeling in <a href="ch19.xhtml#ch19">Chapter 19</a>).</p>&#13;
<h3 class="h3" id="ch18lev1sec54"><strong>18.1 Components of a Hypothesis Test</strong></h3>&#13;
<p class="noindent">To give you an example of hypothesis testing, suppose I told you that 7 percent of a certain population was allergic to peanuts. You then randomly selected 20 individuals from that population and found that 18 of them were allergic to peanuts. Assuming your sample was unbiased and truly reflective of the population, what would you then think about my claim that the true proportion of allergic individuals is 7 percent?</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_386"/>Naturally, you would doubt the correctness of my claim. In other words, there is such a small probability of observing 18 or more successes out of 20 trials for a set success rate of 0.07 that you can state that you have statistical evidence against the claim that the true rate is 0.07. Indeed, when defining <em>X</em> as the number of allergic individuals out of 20 by assuming <em>X</em> ∼ BIN(20,0.07), evaluating Pr(<em>X</em> ≥ 18) gives you the precise <em>p</em>-value, which is tiny.</p>&#13;
<pre>R&gt; dbinom(18,size=20,prob=0.07) + dbinom(19,size=20,prob=0.07) +<br/>   dbinom(20,size=20,prob=0.07)<br/>[1] 2.69727e-19</pre>&#13;
<p class="indent">This <em>p</em>-value represents the probability of observing the results in your sample, <em>X</em> = 18, or a more extreme outcome (<em>X</em> = 19 or <em>X</em> = 20), if the chance of success was truly 7 percent.</p>&#13;
<p class="indent">Before looking at specific hypothesis tests and their implementation in R, this section will introduce terminology that you’ll come across often in the reporting of such tests.</p>&#13;
<h4 class="h4" id="ch18lev2sec153"><strong><em>18.1.1 Hypotheses</em></strong></h4>&#13;
<p class="noindent">As the name would suggest, in hypothesis testing, formally stating a claim and the subsequent hypothesis test is done with a <em>null</em> and an <em>alternative</em> hypothesis. The null hypothesis is interpreted as the <em>baseline</em> or <em>no-change</em> hypothesis and is the claim that is assumed to be true. The alternative hypothesis is the conjecture that you’re testing for, against the null hypothesis.</p>&#13;
<p class="indentb">In general, null and alternative hypotheses are denoted H<sub>0</sub> and H<sub>A</sub>, respectively, and they are written as follows:</p>&#13;
<p class="center1">H<sub>0</sub> : . . .</p>&#13;
<p class="center1">H<sub>A</sub> : . . .</p>&#13;
<p class="indenttb">The null hypothesis is often (but not always) defined as an equality, =, to a null value. Conversely, the alternative hypothesis (the situation you’re testing for) is often defined in terms of an inequality to the null value.</p>&#13;
<p class="bull">• When H<sub>A</sub> is defined in terms of a less-than statement, with &lt;, it is <em>one-sided</em>; this is also called a <em>lower-tailed test</em>.</p>&#13;
<p class="bull">• When H<sub>A</sub> is defined in terms of a greater-than statement, with &gt;, it is <em>one-sided</em>; this is also called an <em>upper-tailed test</em>.</p>&#13;
<p class="bull">• When H<sub>A</sub> is merely defined in terms of a different-to statement, with ≠, it is <em>two-sided</em>; this is also called a <em>two-tailed test</em>.</p>&#13;
<p class="indentt">These test variants are entirely situation specific and depend upon the problem at hand.</p>&#13;
<h4 class="h4" id="ch18lev2sec154"><span epub:type="pagebreak" id="page_387"/><strong><em>18.1.2 Test Statistic</em></strong></h4>&#13;
<p class="noindent">Once the hypotheses are formed, sample data are collected, and statistics are calculated according to the parameters detailed in the hypotheses. The <em>test statistic</em> is the statistic that’s compared to the appropriate standardized sampling distribution to yield the <em>p</em>-value.</p>&#13;
<p class="indent">A test statistic is typically a standardized or rescaled version of the sample statistic of interest. The distribution and extremity (that is, distance from zero) of the test statistic are the sole drivers of the smallness of the <em>p</em>-value (which indicates the strength of the evidence against the null hypothesis—see <a href="ch18.xhtml#ch18lev2sec155">Section 18.1.3</a>). Specifically, the test statistic is determined by both the difference between the original sample statistic and the null value and the standard error of the sample statistic.</p>&#13;
<h4 class="h4" id="ch18lev2sec155"><strong><em>18.1.3 p-value</em></strong></h4>&#13;
<p class="noindent">The <em>p</em>-value is the probability value that’s used to quantify the amount of evidence, if any, against the null hypothesis. More formally, the <em>p</em>-value is found to be the probability of observing the test statistic, or something more extreme, assuming the null hypothesis is true.</p>&#13;
<p class="indentb">The exact nature of calculating a <em>p</em>-value is dictated by the type of statistics being tested and the nature of H<sub>A</sub>. In reference to this, you’ll see the following terms:</p>&#13;
<p class="bull">• A lower-tailed test implies the <em>p</em>-value is a left-hand tail probability from the sampling distribution of interest.</p>&#13;
<p class="bull">• For an upper-tailed test, the <em>p</em>-value is a right-hand tail probability.</p>&#13;
<p class="bull">• For a two-sided test, the <em>p</em>-value is the sum of a left-hand tail probability and right-hand tail probability. When the sampling distribution is symmetric (for example, normal or <em>t</em>, as in all examples coming up in <a href="ch18.xhtml#ch18lev1sec55">Sections 18.2</a> and <a href="ch18.xhtml#ch18lev1sec56">18.3</a>), this is equivalent to two times the area in one of those tails.</p>&#13;
<p class="indentt">Put simply, the more extreme the test statistic, the smaller the <em>p</em>-value. The smaller the <em>p</em>-value, the greater the amount of statistical evidence against the assumed truth of H<sub>0</sub>.</p>&#13;
<h4 class="h4" id="ch18lev2sec156"><strong><em>18.1.4 Significance Level</em></strong></h4>&#13;
<p class="noindentb">For every hypothesis test, a <em>significance level</em>, denoted <em>α</em>, is assumed. This is used to qualify the result of the test. The significance level defines a cutoff point, at which you decide whether there is sufficient evidence to view H<sub>0</sub> as incorrect and favor H<sub>A</sub> instead.</p>&#13;
<p class="bull">• If the <em>p</em>-value is greater than or equal to <em>α</em>, then you conclude there is insufficient evidence against the null hypothesis, and therefore you <em>retain</em> H<sub>0</sub> when compared to H<sub>A</sub>.</p>&#13;
<p class="bull"><span epub:type="pagebreak" id="page_388"/>• If the <em>p</em>-value is less than <em>α</em>, then the result of the test is <em>statistically significant</em>. This implies there is sufficient evidence against the null hypothesis, and therefore you <em>reject</em> H<sub>0</sub> in favor of H<sub>A</sub>.</p>&#13;
<p class="indentt">Common or conventional values of <em>α</em> are <em>α</em> = 0.1, <em>α</em> = 0.05, and <em>α</em> = 0.01.</p>&#13;
<h4 class="h4" id="ch18lev2sec157"><strong><em>18.1.5 Criticisms of Hypothesis Testing</em></strong></h4>&#13;
<p class="noindent">The terminology just presented becomes easier to understand once you look at some examples in the upcoming sections. However, even at this early stage, it’s important to recognize that hypothesis testing is susceptible to justifiable criticism. The end result of any hypothesis test is to either retain or reject the null hypothesis, a decision that is solely dependent upon the rather arbitrary choice of significance level <em>α</em>; this is most often simply set at one of the conventionally used values.</p>&#13;
<p class="indent">Before you begin looking at examples, it is also important to note that a <em>p</em>-value never provides “proof” of either H<sub>0</sub> or H<sub>A</sub> being truly correct. It can only ever quantify evidence against the null hypothesis, which one rejects given a sufficiently small <em>p</em>-value &lt; <em>α</em>. In other words, rejecting a null hypothesis is not the same as disproving it. Rejecting H<sub>0</sub> merely implies that the sample data suggest H<sub>A</sub> ought to be preferred, and the <em>p</em>-value merely indicates the strength of this preference.</p>&#13;
<p class="indent">In recent years, there has been a push against emphasizing these aspects of statistical inference in some introductory statistics courses owing at least in part to the overuse, and even misuse, of <em>p</em>-values in some areas of applied research. A particularly good article by Sterne and Smith (<a href="ref.xhtml#ref63">2001</a>) discusses the role of, and problems surrounding, hypothesis testing from the point of view of medical research. Another good reference is Reinhart (<a href="ref.xhtml#ref53">2015</a>), which discusses common misinterpretations of <em>p</em>-values in statistics.</p>&#13;
<p class="indent">That being said, probabilistic inference with respect to sampling distributions is, and will always remain, a cornerstone of frequentist statistical practice. The best way to improve the use and interpretation of statistical tests and modeling is with a sound introduction to the relevant ideas and methods so that, from the outset, you understand statistical significance and what it can and cannot tell you.</p>&#13;
<h3 class="h3" id="ch18lev1sec55"><strong>18.2 Testing Means</strong></h3>&#13;
<p class="noindent">The validity of hypothesis tests involving sample means is dependent upon the same assumptions and conditions mentioned in <a href="ch17.xhtml#ch17lev2sec146">Section 17.1.1</a>. In particular, throughout this section you should assume that the central limit theorem holds, and if the sample sizes are small (in other words, roughly less than 30), the raw data are normally distributed. You’ll also focus on examples where the sample standard deviation <em>s</em> is used to estimate the true standard deviation, <em>σ</em><sub><em>X</em></sub>, because this is the most common situation you’ll encounter in practice. Again, mirroring <a href="ch17.xhtml#ch17lev2sec146">Section 17.1.1</a>, this means you need to use the <em>t</em>-distribution instead of the normal distribution when calculating the critical values and <em>p</em>-values.</p>&#13;
<h4 class="h4" id="ch18lev2sec158"><span epub:type="pagebreak" id="page_389"/><strong><em>18.2.1 Single Mean</em></strong></h4>&#13;
<p class="noindent">As you’ve already met the standard error formula, <img class="middle" src="../images/f0379-01.jpg" alt="image"/>, and the R functionality needed to obtain quantiles and probabilities from the <em>t</em>-distribution (<code>qt</code> and <code>pt</code>), the only new concepts to introduce here are related to the definition of the hypotheses themselves and the interpretation of the result.</p>&#13;
<h5 class="h5" id="ch18lev3sec67"><strong>Calculation: One-Sample t-Test</strong></h5>&#13;
<p class="noindent">Let’s dive straight into an example—a one-sample <em>t</em>-test. Recall the problem in <a href="ch16.xhtml#ch16lev2sec142">Section 16.2.2</a> where a manufacturer of a snack was interested in the mean net weight of contents in an advertised 80-gram pack. Say that a consumer calls in with a complaint—over time they have bought and precisely weighed the contents of 44 randomly selected 80-gram packs from different stores and recorded the weights as follows:</p>&#13;
<pre>R&gt; snacks &lt;- c(87.7,80.01,77.28,78.76,81.52,74.2,80.71,79.5,77.87,81.94,80.7,<br/>               82.32,75.78,80.19,83.91,79.4,77.52,77.62,81.4,74.89,82.95,<br/>               73.59,77.92,77.18,79.83,81.23,79.28,78.44,79.01,80.47,76.23,<br/>               78.89,77.14,69.94,78.54,79.7,82.45,77.29,75.52,77.21,75.99,<br/>               81.94,80.41,77.7)</pre>&#13;
<p class="indent">The customer claims that they’ve been shortchanged because their data cannot have arisen from a distribution with mean <em>μ</em> = 80, so the true mean weight must be less than 80. To investigate this claim, the manufacturer conducts a hypothesis test using a significance level of <em>α</em> = 0.05.</p>&#13;
<p class="indent">First, the hypotheses must be defined, with a null value of 80 grams. Remember, the alternative hypothesis is “what you’re testing for”; in this case, H<sub>A</sub> is that <em>μ</em> is smaller than 80. The null hypothesis, interpreted as “no change,” will be defined as <em>μ</em> = 80: that the true mean is in fact 80 grams. These hypotheses are formalized like this:</p>&#13;
<div class="imagec"><a id="ch18eq1"/><img src="../images/e18-1.jpg" alt="image"/></div>&#13;
<p class="indent">Second, the mean and standard deviation must be estimated from the sample.</p>&#13;
<pre>R&gt; n &lt;- length(snacks)<br/>R&gt; snack.mean &lt;- mean(snacks)<br/>R&gt; snack.mean<br/>[1] 78.91068<br/>R&gt; snack.sd &lt;- sd(snacks)<br/>R&gt; snack.sd<br/>[1] 3.056023</pre>&#13;
<p class="indent">The question your hypotheses seek to answer is this: given the estimated standard deviation, what’s the probability of observing a sample mean (when <em>n</em> = 44) of 78.91 grams or less if the true mean is 80 grams? To answer this, you need to calculate the relevant test statistic.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_390"/>Formally, the test statistic <em>T</em> in a hypothesis test for a single mean with respect to a null value of <em>μ</em><sub>0</sub> is given as</p>&#13;
<div class="imagec"><a id="ch18eq2"/><img src="../images/e18-2.jpg" alt="image"/></div>&#13;
<p class="noindent">based on a sample of size <em>n</em>, a sample mean of <em><span class="ent">x̄</span></em>, and a sample standard deviation of <em>s</em> (the denominator is the estimated standard error of the mean). Assuming the relevant conditions have been met, <em>T</em> follows a <em>t</em>-distribution with <em>ν</em> = <em>n</em> − 1 degrees of freedom.</p>&#13;
<p class="indent">In R, the following provides you with the standard error of the sample mean for the snacks data:</p>&#13;
<pre>R&gt; snack.se &lt;- snack.sd/sqrt(n)<br/>R&gt; snack.se<br/>[1] 0.4607128</pre>&#13;
<p class="indent">Then, <em>T</em> can be calculated as follows:</p>&#13;
<pre>R&gt; snack.T &lt;- (snack.mean-80)/snack.se<br/>R&gt; snack.T<br/>[1] -2.364419</pre>&#13;
<p class="indent">Finally, the test statistic is used to obtain the <em>p</em>-value. Recall that the <em>p</em>-value is the probability that you observe <em>T</em> or something more extreme. The nature of “more extreme” is determined by the alternative hypothesis H<sub>A</sub>, which, as a less-than statement, directs you to find a left-hand, lower-tail probability as the <em>p</em>-value. In other words, the <em>p</em>-value is provided as the area under the sampling distribution (a <em>t</em>-distribution with 43 df in the current example) to the left of a vertical line at <em>T</em>. From <a href="ch16.xhtml#ch16lev2sec143">Section 16.2.3</a>, this is easily done, as shown here:</p>&#13;
<pre>R&gt; pt(snack.T,df=n-1)<br/>[1] 0.01132175</pre>&#13;
<p class="indent">Your result states that if the H<sub>0</sub> were true, there would be only a little more than a 1 percent chance that you’d observe the customer’s sample mean of <em><span class="ent">x̄</span></em> = 78.91, or less, as a random phenomenon. Since this <em>p</em>-value is smaller than the predefined significance level of <em>α</em> = 0.05, the manufacturer concludes that there is sufficient evidence to reject the null hypothesis in favor of the alternative, suggesting the true value of <em>μ</em> is in fact less than 80 grams.</p>&#13;
<p class="indent">Note that if you find the corresponding 95 percent CI for the single sample mean, as described in <a href="ch17.xhtml#ch17lev2sec149">Section 17.2.1</a> and given by</p>&#13;
<pre>R&gt; snack.mean+c(-1,1)*qt(0.975,n-1)*snack.se<br/>[1] 77.98157 79.83980</pre>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_391"/>it does <em>not</em> include the null value of 80, mirroring the result of the hypothesis test at the 0.05 level.</p>&#13;
<h5 class="h5" id="ch18lev3sec68"><strong>R Function: t.test</strong></h5>&#13;
<p class="noindent">The result of the one-sample <em>t</em>-test can also be found with the built-in <code>t.test</code> function.</p>&#13;
<pre>R&gt; t.test(x=snacks,mu=80,alternative="less")<br/><br/>        One Sample t-test<br/><br/>data:  snacks<br/>t = -2.3644, df = 43, p-value = 0.01132<br/>alternative hypothesis: true mean is less than 80<br/>95 percent confidence interval:<br/>     -Inf 79.68517<br/>sample estimates:<br/>mean of x<br/> 78.91068</pre>&#13;
<p class="indent">The function takes the raw data vector as <code>x</code>, the null value for the mean as <code>mu</code>, and the direction of the test (in other words, how to find the <em>p</em>-value under the appropriate <em>t</em>-curve) as <code>alternative</code>. The <code>alternative</code> argument has three available options: <code>"less"</code> for H<sub>A</sub> with &lt;; <code>"greater"</code> for H<sub>A</sub> with &gt;; and <code>"two.sided"</code> for H<sub>A</sub> with ≠. The default value of <em>α</em> is 0.05. If you want a different significance level than 0.05, this must be provided to <code>t.test</code> as 1 − <em>α</em>, passed to the argument <code>conf.level</code>.</p>&#13;
<p class="indent">Note that the value of <em>T</em> is reported in the output of <code>t.test</code>, as are the degrees of freedom and the <em>p</em>-value. You also get a 95 percent “interval,” but its values of <code>-Inf</code> and <code>79.68517</code> do not match the interval calculated just a moment ago. The manually calculated interval is in fact a two-sided interval—a bounded interval formed by using an error component that’s equal on both sides.</p>&#13;
<p class="indent">The CI in the <code>t.test</code> output, on the other hand, takes instruction from the <code>alternative</code> argument. It provides a <em>one-sided confidence bound</em>. For a lower-tailed test, it provides an upper bound on the statistic such that the entire lower-tail area of the sampling distribution of interest is 0.95, as opposed to a <em>central</em> area as the traditional two-sided interval does. One-sided bounds are less frequently used than the fully bounded two-sided interval, which can be obtained (as the component <code>conf.int</code>) from a relevant call to <code>t.test</code> by setting <code>alternative="two.sided"</code>.</p>&#13;
<pre>R&gt; t.test(x=snacks,mu=80,alternative="two.sided")$conf.int<br/>[1] 77.98157 79.83980<br/>attr(,"conf.level")<br/>[1] 0.95</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_392"/>This result matches your manually computed version from earlier. Note also that the corresponding confidence level, 1 − <em>α</em>, is stored alongside this component as an attribute (refer to <a href="ch06.xhtml#ch06lev2sec59">Section 6.2.1</a>).</p>&#13;
<p class="indent">In examining the result for the snack example, with a <em>p</em>-value of around 0.011, remember to be careful when interpreting hypothesis tests. With <em>α</em> set at 0.05 for this particular test, H<sub>0</sub> is rejected. But what if the test were carried out with <em>α</em> = 0.01? The <em>p</em>-value is greater than 0.01, so in that case, H<sub>0</sub> would be retained, for no reason other than the arbitrary movement of the value of <em>α</em>. In these situations, it’s helpful to comment on the perceived strength of the evidence against the null hypothesis. For the current example, you could reasonably state that there exists some evidence to support H<sub>A</sub> but that this evidence is not especially strong.</p>&#13;
<div class="ex">&#13;
<p class="ext"><a id="ch18exc1"/><strong>Exercise 18.1</strong></p>&#13;
<ol type="a">&#13;
<li><p class="noindents">Adult domestic cats of a certain breed are said to have an average weight of 3.5 kilograms. A feline enthusiast disagrees and collects a sample of 73 weights of cats of this breed. From her sample, she calculates a mean of 3.97 kilograms and a standard deviation of 2.21 kilograms. Perform a hypothesis test to test her claim that the true mean weight <em>μ</em> is <em>not</em> 3.5 kilograms by setting up the appropriate hypothesis, carrying out the analysis, and interpreting the <em>p</em>-value (assume the significance level is <em>α</em> = 0.05).</p></li>&#13;
<li><p class="noindents">Suppose it was previously believed that the mean magnitude of seismic events off the coast of Fiji is 4.3 on the Richter scale. Use the data in the <code>mag</code> variable of the ready-to-use <code>quakes</code> data set, providing 1,000 sampled seismic events in that area, to test the claim that the true mean magnitude is in fact <em>greater</em> than 4.3. Set up appropriate hypotheses, use <code>t.test</code> (conduct the test at a significance level of <em>α</em> = 0.01), and draw a conclusion.</p></li>&#13;
<li><p class="noindents">Manually compute a two-sided confidence interval for the true mean of (b).</p></li>&#13;
</ol>&#13;
</div>&#13;
<h4 class="h4" id="ch18lev2sec159"><strong><em>18.2.2 Two Means</em></strong></h4>&#13;
<p class="noindent">Often, testing a single sample mean isn’t enough to answer the question you’re interested in. In many settings, a researcher wants to directly compare the means of two distinct groups of measurements, which boils down to a hypothesis test for the true difference between two means; call them <em><em>μ<sub>1</sub></em></em> and <em><em>μ<sub>2</sub></em></em>.</p>&#13;
<p class="indent">The way in which two groups of data relate to each other affects the specific form of standard error for the difference between two sample means and therefore the test statistic itself. The actual comparison of the two <span epub:type="pagebreak" id="page_393"/>means, however, is often of the same nature—the typical null hypothesis is usually defined as <em><em>μ<sub>1</sub></em></em> and <em><em>μ<sub>2</sub></em></em> being equal. In other words, the null value of the difference between the two means is often zero.</p>&#13;
<h5 class="h5" id="ch18lev3sec69"><strong>Unpaired/Independent Samples: Unpooled Variances</strong></h5>&#13;
<p class="noindent">The most general case is where the two sets of measurements are based on two independent, separate groups (also referred to as <em>unpaired</em> samples). You compute the sample means and sample standard deviations of both data sets, define the hypotheses of interest, and then calculate the test statistic.</p>&#13;
<p class="indent">When you cannot assume the variances of the two populations are equal, then you perform the <em>unpooled</em> version of the two-sample <em>t</em>-test; this will be discussed first. If, however, you can safely assume equal variances, then you can perform a <em>pooled</em> two-sample <em>t</em>-test, which improves the precision of the results. You’ll look at the pooled version of the test in a moment.</p>&#13;
<p class="indent">For an unpooled example, return to the 80-gram snack packet example from <a href="ch18.xhtml#ch18lev2sec158">Section 18.2.1</a>. After collecting a sample of 44 packs from the original manufacturer (label this sample size <em>n</em><sub>1</sub>), the disgruntled consumer goes out and collects <em>n<sub>2</sub></em> = 31 randomly selected 80-gram packs from a rival snack manufacturer. This second set of measurements is stored as <code>snacks2</code>.</p>&#13;
<pre>R&gt; snacks2 &lt;- c(80.22,79.73,81.1,78.76,82.03,81.66,80.97,81.32,80.12,78.98,<br/>                79.21,81.48,79.86,81.06,77.96,80.73,80.34,80.01,81.82,79.3,<br/>                79.08,79.47,78.98,80.87,82.24,77.22,80.03,79.2,80.95,79.17,81)</pre>&#13;
<p class="indent">From <a href="ch18.xhtml#ch18lev2sec158">Section 18.2.1</a>, you already know the mean and standard deviation of the first sample of size <em>n<sub>1</sub></em> = 44—these are stored as <code>snack.mean</code> (around 78.91) and <code>snack.sd</code> (around 3.06), respectively—think of these as <em><span class="ent">x̄</span><sub>1</sub></em> and <em>s</em><sub>1</sub>. Compute the same quantities, <em><span class="ent">x̄</span><sub>2</sub></em> and <em>s</em><sub>2</sub>, respectively, for the new data.</p>&#13;
<pre>R&gt; snack2.mean &lt;- mean(snacks2)<br/>R&gt; snack2.mean<br/>[1] 80.1571<br/>R&gt; snack2.sd &lt;- sd(snacks2)<br/>R&gt; snack2.sd<br/>[1] 1.213695</pre>&#13;
<p class="indentb">Let the true mean of the original sample be denoted with <em><em>μ<sub>1</sub></em></em> and the true mean of the new sample from the rival company packs be denoted with <em><em>μ<sub>2</sub></em></em>. You’re now interested in testing whether there is statistical evidence to support the claim that <em><em>μ<sub>2</sub></em></em> is greater than <em><em>μ<sub>1</sub></em></em>. This suggests the hypotheses of H<sub>0</sub> : <em><em>μ<sub>1</sub></em></em> = <em><em>μ<sub>2</sub></em></em> and H<sub>A</sub> : <em><em>μ<sub>1</sub></em></em> &lt; <em><em>μ<sub>2</sub></em></em>, which can be written as follows:</p>&#13;
<p class="center1">H<sub>0</sub> : <em><em>μ<sub>2</sub></em></em> − <em><em>μ<sub>1</sub></em></em> = 0</p>&#13;
<p class="center1">H<sub>A</sub> : <em><em>μ<sub>2</sub></em></em> − <em><em>μ<sub>1</sub></em></em> &gt; 0</p>&#13;
<p class="indentt">That is, the difference between the true mean of the rival company packs and the original manufacturer’s packs, when the original is subtracted <span epub:type="pagebreak" id="page_394"/>from the rival, is bigger than zero. The “no-change” scenario, the null hypothesis, is that the two means are the same, so their difference is truly zero.</p>&#13;
<p class="indent">Now that you’ve constructed the hypotheses, let’s look at how to actually test them. The difference between two means is the quantity of interest. For two independent samples arising from populations with true means <em><em>μ<sub>1</sub></em></em> and <em><em>μ<sub>2</sub></em></em>, sample means <em><span class="ent">x̄</span><sub>1</sub></em> and <em><span class="ent">x̄</span></em><sub>2</sub>, and sample standard deviations <em>s<sub>1</sub></em> and <em>s</em><sub>2</sub>, respectively (and that meet the relevant conditions for the validity of the <em>t</em>-distribution), the standardized test statistic <em>T</em> for testing the difference between <em><em>μ<sub>2</sub></em></em> and <em><em>μ<sub>1</sub></em></em>, in that order, is given as</p>&#13;
<div class="imagec"><a id="ch18eq3"/><img src="../images/e18-3.jpg" alt="image"/></div>&#13;
<p class="noindent">whose distribution is approximated by a <em>t</em>-distribution with <em>ν</em> degrees of freedom, where</p>&#13;
<div class="imagec"><a id="ch18eq4"/><img src="../images/e18-4.jpg" alt="image"/></div>&#13;
<p class="indent">In (18.3), <em>μ</em><sub>0</sub> is the null value of interest—typically zero in tests concerned with “difference” statistics. This term would therefore disappear from the numerator of the test statistic. The denominator of <em>T</em> is the standard error of the difference between two means in this setting.</p>&#13;
<p class="indent">The <span class="ent">└</span> · <span class="ent">┘</span> on the right of <a href="ch18.xhtml#ch18eq4">Equation (18.4)</a> denotes a <em>floor</em> operation—rounding strictly down to the nearest integer.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>This two-sample</em> t<em>-test, conducted using <a href="ch18.xhtml#ch18eq3">Equation (18.3)</a>, is also called</em> Welch’s <em>t-</em>test<em>. This refers to use of <a href="ch18.xhtml#ch18eq4">Equation (18.4)</a>, called the</em> Welch-Satterthwaite equation<em>. Crucially, it assumes that the two samples have different true variances, which is why it’s called the</em> unpooled variance <em>version of the test.</em></p>&#13;
</div>&#13;
<p class="indent">It’s important to be consistent when defining your two sets of parameters and when constructing the hypotheses. In this example, since the test aims to find evidence for <em><em>μ<sub>2</sub></em></em> being greater than <em><em>μ<sub>1</sub></em></em>, a difference of <em><em>μ<sub>2</sub></em></em>−<em><em>μ<sub>1</sub></em></em> &gt; 0 forms H<sub>A</sub> (a greater-than, upper-tailed test), and this order of subtraction is mirrored when calculating <em>T</em>. The same test could be carried out if you defined the difference the other way around. In that case, your alternative hypothesis would suggest a lower-tailed test because if you’re testing for <em><em>μ<sub>2</sub></em></em> being bigger than <em><em>μ<sub>1</sub></em></em>, H<sub>A</sub> would correctly be written as <em><em>μ<sub>1</sub></em></em> − <em><em>μ<sub>2</sub></em></em> &lt; 0. Again, this would modify the order of subtraction in the numerator of <a href="ch18.xhtml#ch18eq3">Equation (18.3)</a> accordingly.</p>&#13;
<p class="indent">The same care must apply to the use of <code>t.test</code> for two-sample comparisons. The two samples must be supplied as the arguments <code>x</code> and <code>y</code>, but the function interprets <code>x</code> as greater than <code>y</code> when doing an upper-tailed test and interprets <code>x</code> as less than <code>y</code> when doing a lower-tailed test. Therefore, when <span epub:type="pagebreak" id="page_395"/>performing the test with <code>alternative="greater"</code> for the snack pack example, it’s <code>snacks2</code> that must be supplied to <code>x</code>:</p>&#13;
<pre>R&gt; t.test(x=snacks2,y=snacks,alternative="greater",conf.level=0.9)<br/><br/>        Welch Two Sample t-test<br/><br/>data:  snacks2 and snacks<br/>t = 2.4455, df = 60.091, p-value = 0.008706<br/>alternative hypothesis: true difference in means is greater than 0<br/>90 percent confidence interval:<br/> 0.5859714       Inf<br/>sample estimates:<br/>mean of x mean of y<br/> 80.15710  78.91068</pre>&#13;
<p class="indent">With a small <em>p</em>-value of 0.008706, you’d conclude that there is sufficient evidence to reject H<sub>0</sub> in favor of H<sub>A</sub> (indeed, the <em>p</em>-value is certainly smaller than the stipulated <em>α</em> = 0.1 significance level as implied by <code>conf.level=0.9</code>). The evidence suggests that the mean net weight of snacks from the rival manufacturer’s 80-gram packs is greater than the mean net weight for the original manufacturer.</p>&#13;
<p class="indent">Note that the output from <code>t.test</code> has reported a df value of 60.091, which is the unfloored result of (18.4). You also receive a one-sided confidence bound (based on the aforementioned confidence level), triggered by the one-sided nature of this test. Again, the more common two-sided 90 percent interval is also useful; knowing that <em>ν</em> = <span class="ent">└</span> 60.091 <span class="ent">┘</span> = 60 and using the statistic and the standard error of interest (numerator and denominator of <a href="ch18.xhtml#ch18eq3">Equation (18.3)</a>, respectively), you can calculate it.</p>&#13;
<pre>R&gt; (snack2.mean-snack.mean) +<br/>   c(-1,1)*qt(0.95,df=60)*sqrt(snack.sd^2/44+snack2.sd^2/31)<br/>[1] 0.3949179 2.0979120</pre>&#13;
<p class="indent">Here, you’ve used the previously stored sample statistics <code>snack.mean</code>, <code>snack.sd</code> (the mean and standard deviation of the 44 raw measurements from the original manufacturer’s sample), <code>snack2.mean</code>, and <code>snack2.sd</code> (the same quantities for the 31 observations corresponding to the rival manufacturer). Note that the CI takes the same form as detailed by <a href="ch17.xhtml#ch17eq2">Equation (17.2)</a> on <a href="ch17.xhtml#page_378">page 378</a> and that to provide the correct 1 − <em>α</em> central area, the <code>q</code>-function for the appropriate <em>t</em>-distribution requires 1 − <em>α</em>/2 as its supplied probability value. You can interpret this as being “90 percent confident that the true difference in mean net weight between the rival and the original manufacturer (in that order) is somewhere between 0.395 and 2.098 grams.” The fact that zero isn’t included in the interval, and that the interval is wholly positive, supports the conclusion from the hypothesis test.</p>&#13;
<h5 class="h5" id="ch18lev3sec70"><span epub:type="pagebreak" id="page_396"/><strong>Unpaired/Independent Samples: Pooled Variance</strong></h5>&#13;
<p class="noindent">In the unpooled variance example just passed, there was no assumption that the variances of the two populations whose means were being compared were equal. This is an important note to make because it leads to the use of (18.3) for the test statistic calculation and (18.4) for the associated degrees of freedom in the corresponding <em>t</em>-distribution. However, if you <em>can</em> assume equivalence of variances, the precision of the test is improved—you use a different formula for the standard error of the difference and for calculating the associated df.</p>&#13;
<p class="indent">Again, the quantity of interest is the difference between two means, written as <em><em>μ<sub>2</sub></em></em> − <em><em>μ<sub>1</sub></em></em>. Assume you have two independent samples of sizes <em>n<sub>1</sub></em> and <em>n<sub>2</sub></em> arising from populations with true means <em><em>μ<sub>1</sub></em></em> and <em><em>μ<sub>2</sub></em></em>, sample means <em><span class="ent">x̄</span><sub>1</sub></em> and <em><span class="ent">x̄</span></em><sub>2</sub>, and sample standard deviations <em>s<sub>1</sub></em> and <em>s</em><sub>2</sub>, respectively, and assume that the relevant conditions for the validity of the <em>t</em>-distribution have been met. Additionally, assume that the true variances of the samples, <img class="middle" src="../images/f0396-01.jpg" alt="image"/> and <img class="middle" src="../images/f0396-02.jpg" alt="image"/>, are equal such that <img class="middle" src="../images/f0396-03.jpg" alt="image"/>.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>There is a simple rule of thumb to check the validity of the “equal variance” assumption. If the ratio of the larger sample standard deviation to the smaller sample standard deviation is less than</em> 2<em>, you can assume equal variances. For example, if s</em><sub>1</sub> &gt; <em>s</em><sub>2</sub><em>, then if</em> <img class="middle" src="../images/f0396-07.jpg" alt="image"/> &lt; 2, <em>you can use the pooled variance test statistic that follows.</em></p>&#13;
</div>&#13;
<p class="indent">The standardized test statistic <em>T</em> for this scenario is given as</p>&#13;
<div class="imagec"><a id="ch18eq5"/><img src="../images/e18-5.jpg" alt="image"/></div>&#13;
<p class="noindent">whose distribution is a <em>t</em>-distribution with <em>ν</em> = <em>n<sub>1</sub></em> + <em>n<sub>2</sub></em> − 2 degrees of freedom, where</p>&#13;
<div class="imagec"><a id="ch18eq6"/><img src="../images/e18-6.jpg" alt="image"/></div>&#13;
<p class="noindent">is the <em>pooled estimate of the variance</em> of all the raw measurements. This is substituted in place of <em>s<sub>1</sub></em> and <em>s<sub>2</sub></em> in the denominator of <a href="ch18.xhtml#ch18eq3">Equation (18.3)</a>, resulting in <a href="ch18.xhtml#ch18eq5">Equation (18.5)</a>.</p>&#13;
<p class="indent">All other aspects of the two-sample <em>t</em>-test remain as earlier, including the construction of appropriate hypotheses, the typical null value of <em>μ</em><sub>0</sub>, and the calculation and interpretation of the <em>p</em>-value.</p>&#13;
<p class="indent">For the comparison of the two means in the snack pack example, you’d find it difficult to justify using the pooled version of the <em>t</em>-test. Applying the rule of thumb, the two estimated standard deviations (<em>s</em><sub>1</sub> <span class="ent">≊</span> 3.06 and <em>s<sub>2</sub></em> <span class="ent">≊</span> 1.21 for the original and rival manufacturer’s samples, respectively) have a large-to-small ratio that is greater than 2.</p>&#13;
<pre>R&gt; snack.sd/snack2.sd<br/>[1] 2.51795</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_397"/>Though this is rather informal, if the assumption cannot reasonably be made, it’s best to stick with the unpooled version of the test.</p>&#13;
<p class="indentb">To illustrate this, let’s consider a new example. The intelligence quotient (IQ) is a quantity commonly used to measure how clever a person is. IQ scores are reasonably assumed to be normally distributed, and the average IQ of the population is said to be 100. Say that you’re interested in assessing whether there is a difference in mean IQ scores between men and women, suggesting the following hypotheses where you have <em>n<sub>men</sub></em> = 12 and <em>n</em><small><sub>women</sub></small> = 20:</p>&#13;
<p class="center1">H<sub>0</sub> : <em>μ<sub>men</sub></em> − <em>μ</em><small>women</small> = 0</p>&#13;
<p class="center1">H<sub>A</sub> : <em>μ<sub>men</sub></em> − <em>μ</em><small>women</small> ≠ 0</p>&#13;
<p class="indentt">You randomly sample the following data:</p>&#13;
<pre>R&gt; men &lt;- c(102,87,101,96,107,101,91,85,108,67,85,82)<br/>R&gt; women &lt;- c(73,81,111,109,143,95,92,120,93,89,119,79,90,126,62,92,77,106,<br/>              105,111)</pre>&#13;
<p class="indent">As usual, let’s calculate the basic statistics required.</p>&#13;
<pre>R&gt; mean(men)<br/>[1] 92.66667<br/>R&gt; sd(men)<br/>[1] 12.0705<br/>R&gt; mean(women)<br/>[1] 98.65<br/>R&gt; sd(women)<br/>[1] 19.94802</pre>&#13;
<p class="indent">These give the sample averages <em><span class="ent">x̄</span><sub>men</sub></em> and <em><span class="ent">x̄</span></em><small><sub>women</sub></small>, as well as their respective sample standard deviations <em>s<sub>men</sub></em> and <em>s</em><small><sub>women</sub></small>. Enter the following to quickly check the ratio of the standard deviations:</p>&#13;
<pre>R&gt; sd(women)/sd(men)<br/>[1] 1.652626</pre>&#13;
<p class="indent">You can see that the ratio of the larger sample standard deviation to the smaller is less than 2, so you could assume equal variances in carrying out the hypothesis test.</p>&#13;
<p class="indent">The <code>t.test</code> command also enables the pooled two-sample <em>t</em>-test as per <a href="ch18.xhtml#ch18eq5">Equations (18.5)</a> and <a href="ch18.xhtml#ch18eq6">(18.6)</a>. To execute it, you provide the optional argument <code>var.equal=TRUE</code> (as opposed to the default <code>var.equal=FALSE</code>, which triggers Welch’s <em>t</em>-test).</p>&#13;
<pre>R&gt; t.test(x=men,y=women,alternative="two.sided",conf.level=0.95,var.equal=TRUE)<br/><br/>        Two Sample t-test<br/>data:  men and women<br/>t = -0.9376, df = 30, p-value = 0.3559<br/>alternative hypothesis: true difference in means is not equal to 0<br/>95 percent confidence interval:<br/> -19.016393   7.049727<br/>sample estimates:<br/>mean of x mean of y<br/> 92.66667  98.65000</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_398"/>Note also that H<sub>A</sub> for this example implies a two-tailed test, hence the provision of <code>alternative="two.sided"</code>.</p>&#13;
<p class="indent">The resulting <em>p</em>-value of this test, 0.3559, is certainly larger than the conventional cutoff level of 0.05. Thus, your conclusion here is that there is no evidence to reject H<sub>0</sub>—there is insufficient evidence to support a true difference in the mean IQ scores of men compared to women.</p>&#13;
<h5 class="h5" id="ch18lev3sec71"><strong>Paired/Dependent Samples</strong></h5>&#13;
<p class="noindent">Finally, we’ll look comparing two means in <em>paired</em> data. This setting is distinctly different from that of both unpaired <em>t</em>-tests because it concerns the way the data have been collected. The issue concerns <em>dependence</em> between pairs of observations across the two groups of interest—previously, the measurements in each group have been defined as independent. This notion has important consequences for how the test can be carried out.</p>&#13;
<p class="indent">Paired data occur if the measurements forming the two sets of observations are recorded on the same individual or if they are related in some other important or obvious way. A classic example of this is “before” and “after” observations, such as two measurements made on each person before and after some kind of intervention treatment. These situations still focus on the difference between the mean outcomes in each group, but rather than working with the two data sets separately, a paired <em>t</em>-test works with a single mean—the true mean of the individual paired differences <em>μ<sub>d</sub></em>.</p>&#13;
<p class="indent">As an example, consider a company interested in the efficacy of a drug designed to reduce resting heart rates in beats per minute (bpm). The resting heart rates of 16 individuals are measured. The individuals are then administered a course of the treatment, and their resting heart rates are again measured. The data are provided in the two vectors <code>rate.before</code> and <code>rate.after</code> as follows:</p>&#13;
<pre>R&gt; rate.before &lt;- c(52,66,89,87,89,72,66,65,49,62,70,52,75,63,65,61)<br/>R&gt; rate.after &lt;- c(51,66,71,73,70,68,60,51,40,57,65,53,64,56,60,59)</pre>&#13;
<p class="indent">It quickly becomes clear why any test comparing these two groups must take dependence into account. Heart rate is affected by an individual’s age, build, and level of physical fitness. An unfit individual older than 60 is likely to have a higher baseline resting heart rate than a fit 20-year-old, and if both are given the same drug to lower their heart rate, their final heart rates are <span epub:type="pagebreak" id="page_399"/>still likely to reflect the baselines. Any true effect of the drug therefore has the potential to be hidden if you approached the analysis using either of the unpaired <em>t</em>-tests.</p>&#13;
<p class="indent">To overcome this problem, the paired two-sample <em>t</em>-test considers the difference between each pair of values. Labeling one set of <em>n</em> measurements as <em>x</em><sub>1</sub>, ..., <em>x</em><sub><em>n</em></sub> and the other set of <em>n</em> observations as <em>y</em><sub>1</sub>, ..., <em>y</em><sub><em>n</em></sub>, the difference, <em>d</em>, is defined as <em>d</em><sub><em>i</em></sub> = <em>y</em><sub><em>i</em></sub> − <em>x</em><sub><em>i</em></sub>; <em>i</em> = 1, ..., <em>n</em>. In R, you can easily compute the pairwise differences:</p>&#13;
<pre>R&gt; rate.d &lt;- rate.after-rate.before<br/>R&gt; rate.d<br/> [1]  -1   0 -18 -14 -19  -4  -6 -14  -9  -5  -5   1 -11  -7  -5  -2</pre>&#13;
<p class="indent">The following code calculates the sample mean <img src="../images/d.jpg" alt="image"/> and standard deviation <em>s<sub>d</sub></em> of these differences:</p>&#13;
<pre>R&gt; rate.dbar &lt;- mean(rate.d)<br/>R&gt; rate.dbar<br/>[1] -7.4375<br/>R&gt; rate.sd &lt;- sd(rate.d)<br/>R&gt; rate.sd<br/>[1] 6.196437</pre>&#13;
<p class="indentb">You want to see how much the heart rate is reduced by, so the test at hand will be concerned with the following hypotheses:</p>&#13;
<p class="center1">H<sub>0</sub> : <em>μ<sub>d</sub></em> = 0</p>&#13;
<p class="center1">H<sub>A</sub> : <em>μ<sub>d</sub></em> &lt; 0</p>&#13;
<p class="indentt">Given the order or subtraction used to obtain the differences, detection of a successful reduction in heart rate will be represented by an “after” mean that is smaller than the “before” mean.</p>&#13;
<p class="indent">Expressing all this mathematically, the value of interest is the true mean difference, <em>μ<sub>d</sub></em>, between two means of dependent pairs of measurements. There are two sets of <em>n</em> measurements, <em>x</em><sub>1</sub>, ..., <em>x</em><sub><em>n</em></sub> and <em>y</em><sub>1</sub>, ..., <em>y</em><sub><em>n</em></sub>, with pairwise differences <em>d</em><sub>1</sub>, ..., <em>d</em><sub><em>n</em></sub>. The relevant conditions for the validity of the <em>t</em>-distribution must be met; in this case, if the number of pairs <em>n</em> is less than 30, then you must be able to assume the raw data are normally distributed. The test statistic <em>T</em> is given as</p>&#13;
<div class="imagec"><a id="ch18eq7"/><img src="../images/e18-7.jpg" alt="image"/></div>&#13;
<p class="noindent">where <img class="middle" src="../images/d.jpg" alt="image"/> is the mean of the pairwise differences, <em>s<sub>d</sub></em> is the sample standard deviation of the pairwise differences, and <em>μ</em><sub>0</sub> is the null value (usually zero). The statistic <em>T</em> follows a <em>t</em>-distribution with <em>n</em> − 1 df.</p>&#13;
<p class="indent">The form of <a href="ch18.xhtml#ch18eq7">Equation (18.7)</a> is actually the same as the form of the test statistic in (18.2), once the sample statistics for the individual paired <span epub:type="pagebreak" id="page_400"/>differences have been calculated. Furthermore, it’s important to note that <em>n</em> represents the total number of <em>pairs</em>, not the total number of individual observations.</p>&#13;
<p class="indent">For the current example hypotheses, you can find the test statistic and <em>p</em>-value with <code>rate.dbar</code> and <code>rate.sd</code>.</p>&#13;
<pre>R&gt; rate.T &lt;- rate.dbar/(rate.sd/sqrt(16))<br/>R&gt; rate.T<br/>[1] -4.801146<br/>R&gt; pt(rate.T,df=15)<br/>[1] 0.000116681</pre>&#13;
<p class="indent">These results suggest evidence to reject H<sub>0</sub>. In <code>t.test</code>, the optional logical argument <code>paired</code> must be set to <code>TRUE</code>.</p>&#13;
<pre>R&gt; t.test(x=rate.after,y=rate.before,alternative="less",conf.level=0.95,<br/>          paired=TRUE)<br/><br/>        Paired t-test<br/><br/>data:  rate.after and rate.before<br/>t = -4.8011, df = 15, p-value = 0.0001167<br/>alternative hypothesis: true difference in means is less than 0<br/>95 percent confidence interval:<br/>      -Inf -4.721833<br/>sample estimates:<br/>mean of the differences<br/>                -7.4375</pre>&#13;
<p class="indent">Note that the order you supply your data vectors to the <code>x</code> and <code>y</code> arguments follows the same rules as for the unpaired tests, given the desired value of <code>alternative</code>. The same <em>p</em>-value as was calculated manually is confirmed through the use of <code>t.test</code>, and since this is less than an assumed conventional significance level of <em>α</em> = 0.05, a valid conclusion would be to state that there is statistical evidence that the medication does reduce the mean resting heart rate. You could go on to say you’re 95 percent confident that the true mean difference in heart rate after taking the course of medication lies somewhere between</p>&#13;
<pre>R&gt; rate.dbar-qt(0.975,df=15)*(rate.sd/sqrt(16))<br/>[1] -10.73935</pre>&#13;
<p class="noindent">and</p>&#13;
<pre>R&gt; rate.dbar+qt(0.975,df=15)*(rate.sd/sqrt(16))<br/>[1] -4.135652</pre>&#13;
<div class="note">&#13;
<p class="notet"><span epub:type="pagebreak" id="page_401"/><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>On some occasions, such as when your data strongly indicate non-normality, you may not be comfortable assuming the validity of the CLT (refer back to <a href="ch17.xhtml#ch17lev2sec146">Section 17.1.1</a>). An alternative approach to the tests discussed here is to employ a</em> nonparametric <em>technique that relaxes these distributional requirements. In the two-sample case, you could employ the</em> Mann-Whitney U test <em>(also known as the</em> Wilcoxon rank-sum test<em>). This is a hypothesis test that compares two medians, as opposed to two means. You can use the R function</em> <code><span class="codeitalic">wilcox.test</code></span> <em>to access this methodology; its help page provides useful commentary and references on the particulars of the technique.</em></p>&#13;
</div>&#13;
<div class="ex">&#13;
<p class="ext"><a id="ch18exc2"/><strong>Exercise 18.2</strong></p>&#13;
<p class="noindentz">In the package <code>MASS</code> you’ll find the data set <code>anorexia</code>, which contains data on pre- and post-treatment weights (in pounds) of 72 young women suffering from the disease, obtained from Hand et al. (<a href="ref.xhtml#ref28">1994</a>). One group of women is the control group (in other words, no intervention), and the other two groups are the cognitive behavioral program and family support intervention program groups. Load the library and ensure you can access the data frame and understand its contents. Let <em>μ<sub>d</sub></em> denote the mean difference in weight, computed as (<em>post-weight</em> − <em>pre-weight</em>).</p>&#13;
<ol type="a">&#13;
<li><p class="noindents">Regardless of which treatment group the participants fall into, conduct and conclude an appropriate hypothesis test with <em>α</em> = 0.05 for the entire set of weights for the following hypotheses:</p>&#13;
<p class="center1">H<sub>0</sub> : <em>μ<sub>d</sub></em> = 0</p>&#13;
<p class="center1">H<sub>A</sub> : <em>μ<sub>d</sub></em> &gt; 0</p></li>&#13;
<li><p class="noindents">Next, conduct three separate hypothesis tests using the same defined hypotheses, based on which treatment group the participants fall into. What do you notice?</p></li>&#13;
</ol>&#13;
<p class="noindentz">Another ready-to-use data set in R is <code>PlantGrowth</code> (<a href="ref.xhtml#ref19">Dobson, 1983</a>), which records a continuous measure of the yields of a certain plant, looking at the potential effect of two supplements administered during growth to increase the yield when compared to a control group with no supplement.</p>&#13;
<ol type="a" start="3">&#13;
<li><p class="noindents">Set up hypotheses to test whether the mean yield for the control group is less than the mean yield from a plant given either of the treatments. Determine whether this test should proceed using a pooled estimate of the variance or whether Welch’s <em>t</em>-test would be more appropriate.</p></li>&#13;
<li><p class="noindents">Conduct the test and make a conclusion (assuming normality of the raw observations).</p></li>&#13;
</ol>&#13;
<p class="noindentz"><span epub:type="pagebreak" id="page_402"/>As discussed, there is a rule of thumb for deciding whether to use a pooled estimate of the variance in an unpaired <em>t</em>-test.</p>&#13;
<ol type="a" start="5">&#13;
<li><p class="noindents">Your task is to write a <em>wrapper</em> function that calls <code>t.test</code> after deciding whether it should be executed with <code>var.equal=FALSE</code> according to the rule of thumb. Use the following guidelines:</p>&#13;
<p class="dash">– Your function should take four defined arguments: <code>x</code> and <code>y</code> with no defaults, to be treated in the same way as the same arguments in <code>t.test</code>; and <code>var.equal</code> and <code>paired</code>, with defaults that are the same as the defaults of <code>t.test</code>.</p>&#13;
<p class="dash">– An ellipsis (<a href="ch09.xhtml#ch09lev2sec86">Section 9.2.5</a>) should be included to represent any additional arguments to be passed to <code>t.test</code>.</p>&#13;
<p class="dash">– Upon execution, the function should determine whether <code>paired=FALSE</code>.</p>&#13;
<p class="d-star">* If <code>paired</code> is <code>TRUE</code>, then there is no need to proceed with the check of a pooled variance.</p>&#13;
<p class="d-star">* If <code>paired</code> is <code>FALSE</code>, then the function should determine the value for <code>var.equal</code> automatically by using the rule of thumb.</p>&#13;
<p class="dash">– If the value of <code>var.equal</code> was set automatically, you can assume it will override any value of this argument initially supplied by the user.</p>&#13;
<p class="dash">– Then, call <code>t.test</code> appropriately.</p></li>&#13;
<li><p class="noindents">Try your new function on all three examples in the text of <a href="ch18.xhtml#ch18lev2sec159">Section 18.2.2</a>, ensuring you reach identical results.</p></li>&#13;
</ol>&#13;
</div>&#13;
<h3 class="h3" id="ch18lev1sec56"><strong>18.3 Testing Proportions</strong></h3>&#13;
<p class="noindent">A focus on means is especially common in statistical modeling and hypothesis testing, and therefore you must also consider sample proportions, interpreted as the mean of a series of <em>n</em> binary trials, in which the results are success (1) or failure (0). This section focuses on the parametric tests of proportions, which assume normality of the target sampling distributions (otherwise referred to as <em>Z</em>-tests).</p>&#13;
<p class="indent">The general rules regarding the setup and interpretation of hypothesis tests for sample proportions remain the same as for sample means. In this introduction to <em>Z</em>-tests, you can consider these as tests regarding the true value of a single proportion or the difference between two proportions.</p>&#13;
<h4 class="h4" id="ch18lev2sec160"><strong><em>18.3.1 Single Proportion</em></strong></h4>&#13;
<p class="noindent"><a href="ch17.xhtml#ch17lev2sec147">Section 17.1.2</a> introduced the sampling distribution of a single sample proportion to be normally distributed, with a mean centered on the true proportion <em>π</em> and with a standard error of <img class="middle" src="../images/f0402-01.jpg" alt="image"/>. Provided the trials are independent and that <em>n</em> isn’t “too small” and <em>π</em> isn’t “too close” to 0 or 1, those formulas are applicable here.</p>&#13;
<div class="note">&#13;
<p class="notet"><span epub:type="pagebreak" id="page_403"/><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>A rule of thumb to check the latter condition on n and</em> <em>π</em> <em>simply involves checking that n</em> <img class="middle" src="../images/p.jpg" alt="image"/> <em>and n</em>(1 − <img class="middle" src="../images/p.jpg" alt="image"/>) <em>are both greater than 5, where</em> <img class="middle" src="../images/p.jpg" alt="image"/> <em>is the sample estimate of</em> <em>π.</em></p>&#13;
</div>&#13;
<p class="indent">It’s worth noting that the standard error in the case of hypothesis tests involving proportions is itself dependent upon <em>π</em>. This is important—remember that any hypothesis test assumes satisfaction of H<sub>0</sub> in the relevant calculations. In dealing with proportions, that means when computing the test statistic, the standard error must make use of the null value <em>π</em><sub>0</sub> rather than the estimated sample proportion <img class="middle" src="../images/p.jpg" alt="image"/>.</p>&#13;
<p class="indentb">I’ll clarify this in an example. Suppose an individual fond of a particular fast-food chain notices that he tends to have an upset stomach within a certain amount of time after having his usual lunch. He comes across the website of a blogger who believes that the chance of getting an upset stomach shortly after eating that particular food is 20 percent. The individual is curious to determine whether his true rate of stomach upset <em>π</em> is any different from the blogger’s quoted value and, over time, visits these fast-food outlets for lunch on <em>n</em> = 29 separate occasions, recording the success (<code>TRUE</code>) or failure (<code>FALSE</code>) of experiencing an upset stomach. This suggests the following pair of hypotheses:</p>&#13;
<p class="center1">H<sub>0</sub> : <em>π</em> = 0.2</p>&#13;
<p class="center1">H<sub>A</sub> : <em>π</em> ≠ 0.2</p>&#13;
<p class="indentt">These may be tested according to the general rules discussed in the following sections.</p>&#13;
<h5 class="h5" id="ch18lev3sec72"><strong>Calculation: One-Sample Z-Test</strong></h5>&#13;
<p class="noindent">In testing for the true value of some proportion of success, <em>π</em>, let <img class="middle" src="../images/p.jpg" alt="image"/> be the sample proportion over <em>n</em> trials, and let the null value be denoted with <em>π</em><sub>0</sub>. You find the test statistic with the following:</p>&#13;
<div class="imagec"><a id="ch18eq8"/><img src="../images/e18-8.jpg" alt="image"/></div>&#13;
<p class="indent">Provided the aforementioned conditions on the size of <em>n</em> and the value of <em>π</em> can be assumed, <em>Z</em> ∼ N(0,1).</p>&#13;
<p class="indent">The denominator of <a href="ch18.xhtml#ch18eq8">Equation (18.8)</a>, the standard error of the proportion, is calculated with respect to the null value <em>π</em><sub>0</sub>, not <img class="middle" src="../images/p.jpg" alt="image"/>. As mentioned just a moment ago, this is to satisfy the assumption of “truth” of H<sub>0</sub> as the test is carried out, so it allows interpretation of the resulting <em>p</em>-value as usual. The standard normal distribution is used to find the <em>p</em>-value with respect to <em>Z</em>; the direction underneath this curve is governed by the nature of H<sub>A</sub> just as before.</p>&#13;
<p class="indent">Getting back to the fast-food example, suppose these are the observed data, where <code>1</code> is recorded for an upset stomach and is <code>0</code> otherwise.</p>&#13;
<pre>sick &lt;- c(0,0,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,1,1,0,0,0,1)</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_404"/>The number of successes and probability of success in this sample are as follows:</p>&#13;
<pre>R&gt; sum(sick)<br/>[1] 8<br/>R&gt; p.hat &lt;- mean(sick)<br/>R&gt; p.hat<br/>[1] 0.2758621</pre>&#13;
<p class="indent">A quick check indicates that as per the rule of thumb, the test is reasonable to carry out:</p>&#13;
<pre>R&gt; 29*0.2<br/>[1] 5.8<br/>R&gt; 29*0.8<br/>[1] 23.2</pre>&#13;
<p class="indent">Following <a href="ch18.xhtml#ch18eq8">Equation (18.8)</a>, the test statistic <em>Z</em> for this example is as follows:</p>&#13;
<pre>R&gt; Z &lt;- (p.hat-0.2)/sqrt(0.2*0.8/29)<br/>R&gt; Z<br/>[1] 1.021324</pre>&#13;
<p class="indent">The alternative hypothesis is two-sided, so you compute the corresponding <em>p</em>-value as a two-tailed area under the standard normal curve. With a positive test statistic, this can be evaluated by doubling the upper-tailed area from <em>Z</em>.</p>&#13;
<pre>R&gt; 2*(1-pnorm(Z))<br/>[1] 0.3071008</pre>&#13;
<p class="indent">Assume a conventional <em>α</em> level of 0.05. The high <em>p</em>-value given as 0.307 suggests the results in the sample of size 29 are not unusual enough, under the assumption that the null hypothesis is true, to reject H<sub>0</sub>. There is insufficient evidence to suggest that the proportion of instances of an upset stomach that this individual experiences is any different from 0.2 as noted by the blogger.</p>&#13;
<p class="indent">You can support this conclusion with a confidence interval. At the level of 95 percent, you calculate the CI:</p>&#13;
<pre>R&gt; p.hat+c(-1,1)*qnorm(0.975)*sqrt(p.hat*(1-p.hat)/29)<br/>[1] 0.1131927 0.4385314</pre>&#13;
<p class="indent">This interval easily includes the null value of 0.2.</p>&#13;
<h5 class="h5" id="ch18lev3sec73"><span epub:type="pagebreak" id="page_405"/><strong>R Function: prop.test</strong></h5>&#13;
<p class="noindent">Once more, R rescues you from tedious step-by-step calculation. The ready-to-use <code>prop.test</code> function allows you to perform, among other things, a single sample proportion test. The function actually performs the test in a slightly different way, using the chi-squared distribution (which will be explored more in <a href="ch18.xhtml#ch18lev1sec57">Section 18.4</a>). However, the test is equivalent, and the resulting <em>p</em>-value from <code>prop.test</code> is identical to the one reached using the <em>Z-</em>based test.</p>&#13;
<p class="indent">To the <code>prop.test</code> function, as used for a single sample test of a proportion, you provide the number of successes observed as <code>x</code>, the total number of trials as <code>n</code>, and the null value as <code>p</code>. The two further arguments, <code>alternative</code> (defining the nature of H<sub>A</sub>) and <code>conf.level</code> (defining 1 − <em>α</em>), are identical to the same arguments in <code>t.test</code> and have defaults of <code>"two.sided"</code> and <code>0.95</code>, respectively. Lastly, it is recommended to explicitly set the optional argument <code>correct=FALSE</code> if your data satisfy the <em>n</em> <img class="middle" src="../images/p.jpg" alt="image"/> and <em>n</em>(1 − <img class="middle" src="../images/p.jpg" alt="image"/>) rule of thumb.</p>&#13;
<p class="indent">For the current example, you perform the test with this code:</p>&#13;
<pre>R&gt; prop.test(x=sum(sick),n=length(sick),p=0.2,correct=FALSE)<br/><br/>        1-sample proportions test without continuity correction<br/><br/>data:  sum(sick) out of length(sick), null probability 0.2<br/>X-squared = 1.0431, df = 1, p-value = 0.3071<br/>alternative hypothesis: true p is not equal to 0.2<br/>95 percent confidence interval:<br/> 0.1469876 0.4571713<br/>sample estimates:<br/>        p<br/>0.2758621</pre>&#13;
<p class="indent">The <em>p</em>-value is the same as you got earlier. Note, however, that the reported CI is not quite the same (the normal-based interval, dependent upon the CLT). The CI produced by <code>prop.test</code> is referred to as the <em>Wilson score interval</em>, which takes into account the direct association that a “probability of success” has with the binomial distribution. For simplicity, you’ll continue to work with normal-based intervals when performing hypothesis tests involving proportions here.</p>&#13;
<p class="indent">Note also that, just like <code>t.test</code>, any one-sided test performed with <code>prop.test</code> will provide only a single-limit confidence bound; you’ll see this in the following example.</p>&#13;
<h4 class="h4" id="ch18lev2sec161"><strong><em>18.3.2 Two Proportions</em></strong></h4>&#13;
<p class="noindent">With a basic extension to the previous procedure, by way of a modification to the standard error, you can compare <em>two</em> estimated proportions from independent populations. As with the difference between two means, you’re often testing whether the two proportions are the same and thus have a difference of zero. Therefore, the typical null value is zero.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_406"/>For an example, consider a group of students taking a statistics exam. In this group are <em>n<sub>1</sub></em> = 233 students majoring in psychology, of whom <em>x<sub>1</sub></em> = 180 pass, and <em>n<sub>2</sub></em> = 197 students majoring in geography, of whom 175 pass. Suppose it is claimed that the geography students have a higher pass rate in statistics than the psychology students.</p>&#13;
<p class="indentb">Representing the true pass rates for psychology students as <em>π</em><sub>1</sub> and geography students as <em>π</em><sub>2</sub>, this claim can be statistically tested using a pair of hypotheses defined as follows:</p>&#13;
<p class="center1">H<sub>0</sub> : <em>π</em><sub>2</sub> − <em>π</em><sub>1</sub> = 0</p>&#13;
<p class="center1">H<sub>A</sub> : <em>π</em><sub>2</sub> − <em>π</em><sub>1</sub> &gt; 0</p>&#13;
<p class="indentt">Just as with a comparison between two means, it’s important to keep the order of differencing consistent throughout the test calculations. This example shows an upper-tailed test.</p>&#13;
<h5 class="h5" id="ch18lev3sec74"><strong>Calculation: Two-Sample Z-Test</strong></h5>&#13;
<p class="noindent">In testing for the true difference between two proportions mathematically, <em>π</em><sub>1</sub> and <em>π</em><sub>2</sub>, let <img class="middle" src="../images/p.jpg" alt="image"/><sub>1</sub> = <em>x</em><sub>1</sub>/<em>n</em><sub>1</sub> be the sample proportion for <em>x<sub>1</sub></em> successes in <em>n<sub>1</sub></em> trials corresponding to <em>π</em><sub>1</sub>, and the same quantities as <img class="middle" src="../images/p.jpg" alt="image"/><sub>2</sub> = <em>x</em><sub>2</sub>/<em>n</em><sub>2</sub> for <em>π</em><sub>2</sub>. With a null value of the difference denoted <em>π</em><sub>0</sub>, the test statistic is given by the following:</p>&#13;
<div class="imagec"><a id="ch18eq9"/><img src="../images/e18-9.jpg" alt="image"/></div>&#13;
<p class="indent">Provided you can assume to apply the aforementioned conditions for a proportion with respect to <em>n</em><sub>1</sub>, <em>n<sub>2</sub></em> and <em>π</em><sub>1</sub>, <em>π</em><sub>2</sub>, you can treat <em>Z</em> ∼ N(0,1).</p>&#13;
<p class="indent">There is a new quantity, <em>p</em><sup>∗</sup>, present in the denominator of (18.9). This is a <em>pooled</em> proportion, given as follows:</p>&#13;
<div class="imagec"><a id="ch18eq10"/><img src="../images/e18-10.jpg" alt="image"/></div>&#13;
<p class="indent">As noted, in this kind of test it is common for the null value, the true difference in proportions, to be set to zero (in other words, <em>π</em><sub>0</sub> = 0).</p>&#13;
<p class="indent">The denominator of <a href="ch18.xhtml#ch18eq9">Equation (18.9)</a> is itself the standard error of the difference between two proportions as used in a hypothesis test. The need to use <em>p</em><sup>∗</sup> lies once more in the fact that H<sub>0</sub> is assumed to be true. Using <img class="middle" src="../images/p1.jpg" alt="image"/> and <img class="middle" src="../images/p2.jpg" alt="image"/> separately in the denominator of (18.9), in the form of <img class="middle" src="../images/f0406-01.jpg" alt="image"/> (the standard error of the difference between two proportions outside the confines of a hypothesis test), would violate the assumed “truth” of H<sub>0</sub>.</p>&#13;
<p class="indent">So, returning to the statistics exam taken by the psychology and geography students, you can evaluate the required quantities as such:</p>&#13;
<pre>R&gt; x1 &lt;- 180<br/>R&gt; n1 &lt;- 233<br/>R&gt; p.hat1 &lt;- x1/n1<br/>R&gt; p.hat1<br/>[1] 0.7725322<br/>R&gt; x2 &lt;- 175<br/>R&gt; n2 &lt;- 197<br/>R&gt; p.hat2 &lt;- x2/n2<br/>R&gt; p.hat2<br/>[1] 0.8883249</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_407"/>The results indicate sample pass rates of around 77.2 percent for the psychology students and 88.8 percent for the geography students; this is a difference of roughly 11.6 percent. From examining the values of <img class="middle" src="../images/p1.jpg" alt="image"/>, <em>n<sub>1</sub></em> and <img class="middle" src="../images/p2.jpg" alt="image"/>, <em>n</em><sub>2</sub>, you can see that the rule of thumb is satisfied for this test; again, assume a standard significance level of <em>α</em> = 0.05.</p>&#13;
<p class="indent">The pooled proportion <em>p</em><sup>∗</sup>, following (18.10), is as follows:</p>&#13;
<pre>R&gt; p.star &lt;- (x1+x2)/(n1+n2)<br/>R&gt; p.star<br/>[1] 0.8255814</pre>&#13;
<p class="indent">With that you calculate the test statistic <em>Z</em> as per <a href="ch18.xhtml#ch18eq9">Equation (18.9)</a> with the following:</p>&#13;
<pre>R&gt; Z &lt;- (p.hat2-p.hat1)/sqrt(p.star*(1-p.star)*(1/n1+1/n2))<br/>R&gt; Z<br/>[1] 3.152693</pre>&#13;
<p class="indent">In light of the hypotheses, you find the corresponding <em>p</em>-value as a right-hand, upper-tail area from <em>Z</em> underneath the standard normal curve as follows:</p>&#13;
<pre>R&gt; 1-pnorm(Z)<br/>[1] 0.0008088606</pre>&#13;
<p class="indent">You observe a <em>p</em>-value that’s substantially smaller than <em>α</em>, so the formal decision is of course to reject the null hypothesis in favor of the alternative. The sample data provide sufficient evidence against H<sub>0</sub> such that you can conclude that evidence exists to support the pass rate for geography students being higher than the pass rate for psychology students.</p>&#13;
<h5 class="h5" id="ch18lev3sec75"><strong>R Function: prop.test</strong></h5>&#13;
<p class="noindent">Once more, R allows you to perform the test with one line of code using <code>prop.test</code>. For comparisons of two proportions, you pass the number of successes in each group as a vector of length 2 to <code>x</code> and the respective sample sizes as another vector of length 2 to <code>n</code>. Note that the order of the entries must reflect the order of <code>alternative</code> if this is one-sided (in other words, <span epub:type="pagebreak" id="page_408"/>here, the proportion that is to be tested as “greater” corresponds to the first elements of <code>x</code> and <code>n</code>). Once more, <code>correct</code> is set to <code>FALSE</code>.</p>&#13;
<pre>R&gt; prop.test(x=c(x2,x1),n=c(n2,n1),alternative="greater",correct=FALSE)<br/><br/>   2-sample test for equality of proportions without continuity correction<br/><br/>data:  c(x2, x1) out of c(n2, n1)<br/>X-squared = 9.9395, df = 1, p-value = 0.0008089<br/>alternative hypothesis: greater<br/>95 percent confidence interval:<br/> 0.05745804 1.00000000<br/>sample estimates:<br/>   prop 1    prop 2<br/>0.8883249 0.7725322</pre>&#13;
<p class="indent">The <em>p</em>-value is identical to the one generated by the previous series of calculations, suggesting a rejection of H<sub>0</sub>. Since <code>prop.test</code> was called as a one-sided test, the confidence interval returned provides a single bound. To provide a two-sided CI for the true difference, it makes sense, considering the outcome of the test, to construct this using the separate <img class="middle" src="../images/p1.jpg" alt="image"/> and <img class="middle" src="../images/p2.jpg" alt="image"/> instead of using the denominator of (18.9) specifically (which assumes truth of H<sub>0</sub>). The “separate-estimate” version of the standard error of the difference between two proportions was given earlier (in the text beneath <a href="ch18.xhtml#ch18eq10">Equation (18.10)</a>), and a 95 percent CI is therefore calculated with the following:</p>&#13;
<pre>R&gt; (p.hat2-p.hat1) +<br/>   c(-1,1)*qnorm(0.975)*sqrt(p.hat1*(1-p.hat1)/n1+p.hat2*(1-p.hat2)/n2)<br/>[1] 0.04628267 0.18530270</pre>&#13;
<p class="indent">With that, you’re 95 percent confident that the true difference between the proportion of geography students passing the exam and the proportion of psychology students passing the exam lies somewhere between 0.046 and 0.185. Naturally, the interval also reflects the result of the hypothesis test—it doesn’t include the null value of zero and is wholly positive.</p>&#13;
<div class="ex">&#13;
<p class="ext"><a id="ch18exc3"/><strong>Exercise 18.3</strong></p>&#13;
<p class="noindentz">An advertisement for a skin cream claims nine out of ten women who use it would recommend it to a friend. A skeptical salesperson in a department store believes the true proportion of women users who’d recommend it, <em>π</em>, is much smaller than 0.9. She follows up with 89 random customers who had purchased the skin cream and asks if they would recommend it to others, to which 71 answer yes.</p>&#13;
<ol type="a">&#13;
<li><p class="noindents"><span epub:type="pagebreak" id="page_409"/>Set up an appropriate pair of hypotheses for this test and determine whether it will be valid to carry out using the normal distribution.</p></li>&#13;
<li><p class="noindents">Compute the test statistic and the <em>p</em>-value and state your conclusion for the test using a significance level of <em>α</em> = 0.1.</p></li>&#13;
<li><p class="noindents">Using your estimated sample proportion, construct a two-sided 90 percent confidence interval for the true proportion of women who would recommend the skin cream.</p></li>&#13;
</ol>&#13;
<p class="noindentz">The political leaders of a particular country are curious as to the proportion of citizens in two of its states that support the decriminalization of marijuana. A small pilot survey taken by officials reveals that 97 out of 445 randomly sampled voting-age citizens residing in state 1 support the decriminalization and that 90 out of 419 voting-age citizens residing in state 2 support the same notion.</p>&#13;
<ol type="a" start="4">&#13;
<li><p class="noindents">Letting <em>π</em><sub>1</sub> denote the true proportion of citizens in support of decriminalization in state 1, and <em>π</em><sub>2</sub> the same measure in state 2, conduct and conclude a hypothesis test under a significance level of <em>α</em> = 0.05 with reference to the following hypotheses:</p>&#13;
<p class="center1">H<sub>0</sub> : <em>π</em><sub>2</sub> − <em>π</em><sub>1</sub> = 0</p>&#13;
<p class="center1">H<sub>A</sub> : <em>π</em><sub>2</sub> − <em>π</em><sub>1</sub> ≠ 0</p></li>&#13;
<li><p class="noindents">Compute and interpret a corresponding CI.</p></li>&#13;
</ol>&#13;
<p class="noindentz">Though there is standard, ready-to-use R functionality for the <em>t</em>-test, at the time of this writing, there is no similar function for the <em>Z</em>-test (in other words, the normal-based test of proportions described here) except in contributed packages.</p>&#13;
<ol type="a" start="6">&#13;
<li><p class="noindents">Your task is to write a relatively simple R function, <code>Z.test</code>, that can perform a one- or two-sample <em>Z</em>-test, using the following guidelines:</p>&#13;
<p class="dash">– The function should take the following arguments: <code>p1</code> and <code>n1</code> (no default) to pose as the estimated proportion and sample size; <code>p2</code> and <code>n2</code> (both defaulting to <code>NULL</code>) that contain the second sample proportion and sample size in the event of a two-sample test; <code>p0</code> (no default) as the null value; and <code>alternative</code> (default <code>"two.sided"</code>) and <code>conf.level</code> (default <code>0.95</code>), to be used in the same way as in <code>t.test</code>.</p>&#13;
<p class="dash">– When conducting a two-sample test, it should be <code>p1</code> that is tested as being smaller or larger than <code>p2</code> when <code>alternative="less"</code> or <code>alternative="greater"</code>, the same as in the use of <code>x</code> and <code>y</code> in <code>t.test</code>.</p>&#13;
<p class="dash">– The function should perform a one-sample <em>Z</em>-test using <code>p1</code>, <code>n1</code>, and <code>p0</code> if either <code>p2</code> or <code>n2</code> (or both) is <code>NULL</code>.</p>&#13;
<p class="dash"><span epub:type="pagebreak" id="page_410"/>– The function should contain a check for the rule of thumb to ensure the validity of the normal distribution in both one- and two-sample settings. If this is violated, the function should still complete but should issue an appropriate warning message (see <a href="ch12.xhtml#ch12lev2sec106">Section 12.1.1</a>).</p>&#13;
<p class="dash">– All that need be returned is a list containing the members <code>Z</code> (test statistic), <code>P</code> (appropriate <em>p</em>-value—this can be determined by <code>alternative</code>; for a two-sided test, determining whether <code>Z</code> is positive or not can help), and <code>CI</code> (two-sided CI with respect to <code>conf.level</code>).</p></li>&#13;
<li><p class="noindents">Replicate the two examples in the text of <a href="ch18.xhtml#ch18lev2sec160">Sections 18.3.1</a> and <a href="ch18.xhtml#ch18lev2sec161">18.3.2</a> using <code>Z.test</code>; ensure you reach identical results.</p></li>&#13;
<li><p class="noindents">Call <code>Z.test(p1=0.11,n1=10,p0=0.1)</code> to try your warning message in the one-sample setting.</p></li>&#13;
</ol>&#13;
</div>&#13;
<h3 class="h3" id="ch18lev1sec57"><strong>18.4 Testing Categorical Variables</strong></h3>&#13;
<p class="noindent">The normal-based <em>Z</em>-test is particular to data that are binary in nature. To statistically test claims regarding more general categorical variables, with more than two distinct levels, you use the ubiquitous <em>chi-squared test</em>. Pronounced <em>kai</em>, “chi” refers to the Greek symbol χ and is sometimes noted in shorthand as the χ<sup>2</sup> <em>test</em>.</p>&#13;
<p class="indent">There are two common variants of the chi-squared test. The first—a chi-squared test of distribution, also called a <em>goodness of fit (GOF)</em> test—is used when assessing the frequencies in the levels of a single categorical variable. The second—a chi-squared test of <em>independence</em>—is employed when you’re investigating the relationship between frequencies in the levels of two such variables.</p>&#13;
<h4 class="h4" id="ch18lev2sec162"><strong><em>18.4.1 Single Categorical Variable</em></strong></h4>&#13;
<p class="noindent">Like the <em>Z</em>-test, the one-dimensional chi-squared test is also concerned with comparing proportions but in a setting where there are more than two proportions. A chi-squared test is used when you have <em>k</em> levels (or categories) of a categorical variable and want to hypothesize about their relative frequencies to find out what proportion of <em>n</em> observations fall into each defined category. In the following examples, it must be assumed that the categories are <em>mutually exclusive</em> (in other words, an observation cannot take more than one of the possible categories) and <em>exhaustive</em> (in other words, the <em>k</em> categories cover all possible outcomes).</p>&#13;
<p class="indent">I’ll illustrate how hypotheses are constructed and introduce the relevant ideas and methods with the following example. Suppose a researcher in sociology is interested in the dispersion of rates of facial hair in men of his local city and whether they are uniformly represented in the male population. He defines a categorical variable with three levels: clean shaven (1), beard only <span epub:type="pagebreak" id="page_411"/><em>or</em> moustache only (2), and beard <em>and</em> moustache (3). He collects data on 53 randomly selected men and finds the following outcomes:</p>&#13;
<pre>R&gt; hairy &lt;- c(2,3,2,3,2,1,3,3,2,2,3,2,2,2,3,3,3,2,3,2,2,2,1,3,2,2,2,1,2,2,3,<br/>              2,2,2,2,1,2,1,1,1,2,2,2,3,1,2,1,2,1,2,1,3,3)</pre>&#13;
<p class="indent">Now, the research question asks whether the proportions in each category are equally represented. Let <em>π</em><sub>1</sub>, <em>π</em><sub>2</sub>, and <em>π</em><sub>3</sub> represent the true proportion of men in the city who fall into groups 1, 2, and 3, respectively. You therefore seek to test these hypotheses:</p>&#13;
<div class="imagec"><img src="../images/f0411-01.jpg" alt="image"/></div>&#13;
<p class="indent">For this test, use a standard significance level of 0.05.</p>&#13;
<p class="indent">The appearance of the alternative hypothesis is a little different from what you’ve seen so far but is an accurate reflection of the interpretation of a chi-squared goodness of fit test. In these types of problems, H<sub>0</sub> is always that the proportions in each group are equal to the stated values, and H<sub>A</sub> is that the data, as a whole, do not match the proportions defined in the null. The test is conducted assuming the null hypothesis is true, and evidence against the no-change, baseline setting will be represented as a small <em>p</em>-value.</p>&#13;
<h5 class="h5" id="ch18lev3sec76"><strong>Calculation: Chi-Squared Test of Distribution</strong></h5>&#13;
<p class="noindent">The quantities of interest are the proportion of <em>n</em> observations in each of <em>k</em> categories, <em>π</em><sub>1</sub>, ..., <em>π</em><sub>k</sub>, for a single mutually exclusive and exhaustive categorical variable. The null hypothesis defines hypothesized null values for each proportion; label these respectively as <em>π</em><sub>0</sub><sub>(</sub><sub>1</sub><sub>)</sub>, ..., <em>π</em><sub>0</sub><sub>(</sub><sub>k</sub><sub>)</sub>. The test statistic χ<sup>2</sup> is given as</p>&#13;
<div class="imagec"><a id="ch18eq11"/><img src="../images/e18-11.jpg" alt="image"/></div>&#13;
<p class="noindent">where <em>O<sub>i</sub></em> is the <em>observed</em> count and <em>E<sub>i</sub></em> is the <em>expected</em> count in the <em>i</em>th category; <em>i</em> = 1, ..., <em>k</em>. The <em>O<sub>i</sub></em> are obtained directly from the raw data, and the expected counts, <em>E<sub>i</sub></em> = <em>nπ</em><sub>0</sub><em><sub>(</sub><sub>i</sub><sub>)</sub></em>, are merely the product of the overall sample size <em>n</em> with the respective null proportion for each category. The result of χ<sup>2</sup> follows a <em>chi-squared distribution</em> (explained further momentarily) with <em>ν</em> = <em>k</em> − 1 degrees of freedom. You usually consider the test to be valid based on an informal rule of thumb stating that at least 80 percent of the expected counts <em>E<sub>i</sub></em> should be at least 5.</p>&#13;
<p class="indentb">In this type of chi-squared test, it is important to note the following:</p>&#13;
<p class="bull">• The term <em>goodness of fit</em> refers to the proximity of the observed data to the distribution hypothesized in H<sub>0</sub>.</p>&#13;
<p class="bull"><span epub:type="pagebreak" id="page_412"/>• Positive extremity of the result of (18.11) provides evidence against H<sub>0</sub>. As such, the corresponding <em>p-value is always computed as an upper-tail area</em>.</p>&#13;
<p class="bull">• As in the current example, a test for uniformity simplifies the null hypothesis slightly by having equivalent null proportions <em>π</em><sub>0</sub> = <em>π</em><sub>0</sub><sub>(</sub><sub>1</sub><sub>)</sub> = ... = <em>π</em><sub>0</sub><em><sub>(</sub><sub>k</sub><sub>)</sub></em>.</p>&#13;
<p class="bull">• A rejected H<sub>0</sub> doesn’t tell you about the true values of <em>π<sub>i</sub></em>. It merely suggests that they do not follow H<sub>0</sub> specifically.</p>&#13;
<p class="indentt">The chi-squared distribution relies on specification of a degree of freedom, much like the <em>t</em>-distribution. Unlike a <em>t</em> curve, however, a chi-squared curve is unidirectional in nature, being defined for non-negative values and with a positive (right-hand) horizontal asymptote (tail going to zero).</p>&#13;
<p class="indent">It’s this unidirectional distribution that leads to <em>p</em>-values being defined as upper-tail areas only; decisions like one- or two-tailed areas have no relevance in these types of chi-squared tests. To get an idea of what the density functions actually look like, <a href="ch18.xhtml#ch18fig1">Figure 18-1</a> shows three particular curves defined with <em>ν</em> = 1, <em>ν</em> = 5, and <em>ν</em> = 10 degrees of freedom.</p>&#13;
<div class="image"><img src="../images/f18-01.jpg" alt="image"/></div>&#13;
<p class="figt"><em><a id="ch18fig1"/>Figure 18-1: Three instances of the chi-squared density function using differing degrees of freedom values. Note the positive domain of the function and the “flattening” and “right-extending” behavior as</em> <em>ν</em> <em>is increased.</em></p>&#13;
<p class="indent">This image was produced using the relevant <code>d</code>-function, <code>dchisq</code>, with <em>ν</em> passed to the argument <code>df</code>.</p>&#13;
<pre>R&gt; x &lt;- seq(0,20,length=100)<br/>R&gt; plot(x,dchisq(x,df=1),type="l",xlim=c(0,15),ylim=c(0,0.5),ylab="density")<br/>R&gt; lines(x,dchisq(x,df=5),lty=2)<br/>R&gt; lines(x,dchisq(x,df=10),lty=3)<br/>R&gt; abline(h=0,col="gray")<br/>R&gt; abline(v=0,col="gray")<br/>R&gt; legend("topright",legend=c("df=1","df=5","df=10"),lty=1:3)</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_413"/>The current facial hair example is a test for the uniformity of the distribution of frequencies in the three categories. You can obtain the observed counts and corresponding proportions with <code>table</code>.</p>&#13;
<pre>R&gt; n &lt;- length(hairy)<br/>R&gt; n<br/>[1] 53<br/>R&gt; hairy.tab &lt;- table(hairy)<br/>R&gt; hairy.tab<br/>hairy<br/> 1  2  3<br/>11 28 14<br/>R&gt; hairy.tab/n<br/>hairy<br/>        1         2         3<br/>0.2075472 0.5283019 0.2641509</pre>&#13;
<p class="indent">For computation of the test statistic χ<sup>2</sup>, you have the observed counts <em>O<sub>i</sub></em> in <code>hairy.tab</code>. The expected count <em>E<sub>i</sub></em> is a straightforward arithmetic calculation of the total number of observations multiplied by the null proportion 1/3 (the result stored as <code>expected</code>), giving you the same value for each category.</p>&#13;
<p class="indent">These, as well as the contribution of each category to the test statistic, are nicely presented in a matrix constructed with <code>cbind</code> (<a href="ch03.xhtml#ch03lev2sec25">Section 3.1.2</a>).</p>&#13;
<pre>R&gt; expected &lt;- 1/3*n<br/>R&gt; expected<br/>[1] 17.66667<br/>R&gt; hairy.matrix &lt;- cbind(1:3,hairy.tab,expected,<br/>                         (hairy.tab-expected)^2/expected)<br/>R&gt; dimnames(hairy.matrix) &lt;- list(c("clean","beard OR mous.",<br/>                                    "beard AND mous."),<br/>                                  c("i","Oi","Ei","(Oi-Ei)^2/Ei"))<br/>R&gt; hairy.matrix<br/>                i Oi       Ei (Oi-Ei)^2/Ei<br/>clean           1 11 17.66667    2.5157233<br/>beard OR mous.  2 28 17.66667    6.0440252<br/>beard AND mous. 3 14 17.66667    0.7610063</pre>&#13;
<p class="indent">Note that all the expected counts are comfortably greater than 5, which satisfies the informal rule of thumb mentioned earlier. In terms of R coding, note also that the single number <code>expected</code> is implicitly recycled to match the length of the other vectors supplied to <code>cbind</code> and that you’ve used the <code>dimnames</code> attribute (refer to <a href="ch06.xhtml#ch06lev2sec59">Section 6.2.1</a>) to annotate the rows and columns.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_414"/>The test statistic, as per (18.11), is given as the sum of the (<em>O<sub>i</sub></em> − <em>E<sub>i</sub></em>)<sup>2</sup>/<em>E</em><sub>i</sub> contributions in the fourth column of <code>hairy.matrix</code>.</p>&#13;
<pre>R&gt; X2 &lt;- sum(hairy.matrix[,4])<br/>R&gt; X2<br/>[1] 9.320755</pre>&#13;
<p class="indent">The corresponding <em>p</em>-value is the appropriate upper-tail area from the chi-squared distribution with <em>ν</em> = 3 − 1 = 2 degrees of freedom.</p>&#13;
<pre>R&gt; 1-pchisq(X2,df=2)<br/>[1] 0.009462891</pre>&#13;
<p class="indent">This small <em>p</em>-value provides evidence to suggest that the true frequencies in the defined categories of male facial hair are not uniformly distributed in a 1/3,1/3,1/3 fashion. Remember that the test result doesn’t give you the true proportions but only suggests that they do not follow those in H<sub>0</sub>.</p>&#13;
<h5 class="h5" id="ch18lev3sec77"><strong>R Function: chisq.test</strong></h5>&#13;
<p class="noindent">Like <code>t.test</code> and <code>prop.test</code>, R provides a quick-use function for performing a chi-squared GOF test. The <code>chisq.test</code> function takes the vector of observed frequencies as its first argument <code>x</code>. For the facial hair example, this simple line therefore provides the same results as found previously:</p>&#13;
<pre>R&gt; chisq.test(x=hairy.tab)<br/><br/>        Chi-squared test for given probabilities<br/><br/>data:  hairy.tab<br/>X-squared = 9.3208, df = 2, p-value = 0.009463</pre>&#13;
<p class="indentb">By default, the function performs a test for uniformity, taking the number of categories as the length of the vector supplied to <code>x</code>. However, suppose that the researcher collecting the facial hair data realizes that he was doing so in November, a month during which many men grow mustaches in support of “Mo-vember” to raise awareness of men’s health. This changes thoughts on the true rates in terms of his clean-shaven (1), beard-only <em>or</em> moustache-only (2), and beard <em>and</em> moustache (3) categories. He now wants to test the following:</p>&#13;
<p class="center2">H<sub>0</sub> : <em>π</em><sub>0</sub><sub>(</sub><sub>1</sub><sub>)</sub> = 0.25; <em>π</em><sub>0</sub><sub>(</sub><sub>2</sub><sub>)</sub> = 0.5; <em>π</em><sub>0</sub><sub>(</sub><sub>3</sub><sub>)</sub> = 0.25</p>&#13;
<p class="center2">H<sub>A</sub> : H<sub>0</sub> is incorrect.</p>&#13;
<p class="indentt">If a GOF test of uniformity is not desired, when the “true” rates across the categories are not all the same, the <code>chisq.test</code> function requires you to supply the null proportions as a vector of the same length as <code>x</code> to the <code>p</code> argument. Naturally, each entry in <code>p</code> must correspond to the categories tabulated in <code>x</code>.</p>&#13;
<pre><span epub:type="pagebreak" id="page_415"/>R&gt; chisq.test(x=hairy.tab,p=c(0.25,0.5,0.25))<br/><br/>        Chi-squared test for given probabilities<br/><br/>data:  hairy.tab<br/>X-squared = 0.5094, df = 2, p-value = 0.7751</pre>&#13;
<p class="indent">With a very high <em>p</em>-value, there is no evidence to reject H<sub>0</sub> in this scenario. In other words, there is no evidence to suggest that the proportions hypothesized in H<sub>0</sub> are incorrect.</p>&#13;
<h4 class="h4" id="ch18lev2sec163"><strong><em>18.4.2 Two Categorical Variables</em></strong></h4>&#13;
<p class="noindentb">The chi-squared test can also apply to the situation in which you have <em>two</em> mutually exclusive and exhaustive categorical variables at hand—call them variable <em>A</em> and variable <em>B</em>. It is used to detect whether there might be some influential relationship (in other words, <em>dependence</em>) between <em>A</em> and <em>B</em> by looking at the way in which the distribution of frequencies change together with respect to their categories. If there is no relationship, the distribution of frequencies in variable <em>A</em> will have nothing to do with the distribution of frequencies in variable <em>B</em>. As such, this particular variant of the chi-squared test is called a <em>test of independence</em> and is always performed with the following hypotheses:</p>&#13;
<p class="hang">H<sub>0</sub> : Variables <em>A</em> and <em>B</em> are independent.</p>&#13;
<p class="hangp">(<em>or</em>, There is no relationship between <em>A</em> and <em>B</em>.)</p>&#13;
<p class="hang">H<sub>A</sub> : Variables <em>A</em> and <em>B</em> are <em>not</em> independent.</p>&#13;
<p class="hangp">(<em>or</em>, There <em>is</em> a relationship between <em>A</em> and <em>B</em>.)</p>&#13;
<p class="indentt">To carry out the test, therefore, you compare the observed data to the counts you’d expect to see if the distributions were completely unrelated (satisfying the assumption that H<sub>0</sub> is true). An overall large departure from the expected frequencies will result in a small <em>p</em>-value and thus provide evidence against the null.</p>&#13;
<p class="indent">So, how are such data best presented? For two categorical variables, a two-dimensional structure is appropriate; in R, this is a standard matrix. For example, suppose some dermatologists at a certain clinical practice are interested in their successes in treating a common skin affliction. Their records show <em>N</em> = 355 patients have been treated at their clinic using one of four possible treatments—a course of tablets, a series of injections, a laser treatment, and an herbal-based remedy. The level of success in curing the affliction is also recorded—none, partial success, and full success. The data are given in the constructed matrix <code>skin</code>.</p>&#13;
<pre>R&gt; skin &lt;- matrix(c(20,32,8,52,9,72,8,32,16,64,30,12),4,3,<br/>                  dimnames=list(c("Injection","Tablet","Laser","Herbal"),<br/>                  c("None","Partial","Full")))<br/>R&gt; skin<br/>          None Partial Full<br/>Injection   20       9   16<br/>Tablet      32      72   64<br/>Laser        8       8   30<br/>Herbal      52      32   12</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_416"/>A two-dimensional table presenting frequencies in this fashion is called a <em>contingency table</em>.</p>&#13;
<h5 class="h5" id="ch18lev3sec78"><strong>Calculation: Chi-Squared Test of Independence</strong></h5>&#13;
<p class="noindent">To compute the test statistic, presume data are presented as a <em>k<sub>r</sub></em> × <em>k<sub>c</sub></em> contingency table, in other words, a matrix of counts, based on two categorical variables (both mutually exclusive and exhaustive). The focus of the test is the way in which the frequencies of <em>N</em> observations between the <em>k<sub>r</sub></em> levels of the “row” variable and the <em>k<sub>c</sub></em> levels of the “column” variable are jointly distributed. The test statistic χ<sup>2</sup> is given with</p>&#13;
<div class="imagec"><a id="ch18eq12"/><img src="../images/e18-12.jpg" alt="image"/></div>&#13;
<p class="noindent">where <em>O</em><sub>[</sub><em><sub>i</sub><sub>,</sub> <sub>j</sub></em><small>]</small> is the observed count and <em>E</em><sub>[</sub><em><sub>i</sub><sub>,</sub> <sub>j</sub></em><small>]</small> is the expected count at row position <em>i</em> and column position <em>j</em>. Each <em>E</em><sub>[</sub><em><sub>i</sub><sub>,</sub> <sub>j</sub></em><small>]</small> is found as the sum total of row <em>i</em> multiplied by the sum total of column <em>j</em>, all divided by <em>N</em>.</p>&#13;
<div class="imagec"><a id="ch18eq13"/><img src="../images/e18-13.jpg" alt="image"/></div>&#13;
<p class="indent">The result, χ<sup>2</sup>, follows a chi-squared distribution with <em>ν</em> = (<em>k<sub>r</sub></em> − 1) × (<em>k<sub>c</sub></em> − 1) degrees of freedom. Again, the <em>p</em>-value is always an upper-tailed area, and you can consider the test valid with the satisfaction of the condition that at least 80 percent of the <em>E</em><sub>[</sub><em><sub>i</sub><sub>,</sub> <sub>j</sub></em><small>]</small> are at least 5.</p>&#13;
<p class="indentb">For this calculation, it’s important to note the following:</p>&#13;
<p class="bull">• It’s not necessary to assume that <em>k<sub>r</sub></em> = <em>k<sub>c</sub></em>.</p>&#13;
<p class="bull">• The functionality of <a href="ch18.xhtml#ch18eq12">Equation (18.12)</a> is the same as that of (18.11)—an overall sum involving the squared differences between the observed and expected values of each cell.</p>&#13;
<p class="bull">• The double-sum in (18.12) just represents the total sum over all the cells, in the sense that you can compute the total sample size <em>N</em> with <img class="middle" src="../images/f0416-03.jpg" alt="image"/>.</p>&#13;
<p class="bull">• A rejected H<sub>0</sub> doesn’t tell you about the nature of <em>how</em> the frequencies depend on one another, just that there is evidence to suggest that some kind of dependency between the two categorical variables exists.</p>&#13;
<p class="indentt"><span epub:type="pagebreak" id="page_417"/>Continuing with the example, the dermatologists want to determine whether their records suggest there is statistical evidence to indicate some relationship between the type of treatment and the level of success in curing the skin affliction. For convenience, store the total number of categories <em>k<sub>r</sub></em> and <em>k<sub>c</sub></em> for the row and column variables, respectively.</p>&#13;
<pre>R&gt; kr &lt;- nrow(skin)<br/>R&gt; kc &lt;- ncol(skin)</pre>&#13;
<p class="indent">You have the <em>O</em><sub>[</sub><em><sub>i</sub><sub>,</sub> <sub>j</sub></em><small>]</small> in <code>skin</code>, so now you must now compute the <em>E</em><sub>[</sub><em><sub>i</sub><sub>,</sub> <sub>j</sub></em><small>]</small>. In light of <a href="ch18.xhtml#ch18eq13">Equation (18.13)</a>, which deals with row and column sums, you can evaluate these using the built-in <code>rowSums</code> and <code>colSums</code> functions.</p>&#13;
<pre>R&gt; rowSums(skin)<br/>Injection    Tablet    Laser    Herbal<br/>       45       168       46        96<br/>R&gt; colSums(skin)<br/>   None Partial    Full<br/>    112     121     122</pre>&#13;
<p class="indent">These results indicate the totals in each group, regardless of the other variable. To get the expected counts for all cells of the matrix, <a href="ch18.xhtml#ch18eq13">Equation (18.13)</a> requires each row sum to be multiplied by each column sum once. You could write a <code>for</code> loop, but this would be inefficient and rather inelegant. It is better to use <code>rep</code> with the optional <code>each</code> argument (refer to <a href="ch02.xhtml#ch02lev2sec21">Section 2.3.2</a>). By repeating each element of the column totals (level of success) four times, you can then use vector-oriented behavior to multiply that repeated vector by the shorter vector produced by <code>rowSums</code>. You can then call <code>sum(skin)</code> to divide this by <em>N</em> and rearrange it into a matrix. The following lines show how this example works step-by-step:</p>&#13;
<pre>R&gt; rep(colSums(skin),each=kr)<br/>   None    None    None    None Partial Partial Partial Partial    Full<br/>    112     112     112     112     121     121     121     121     122<br/>   Full    Full    Full<br/>    122     122     122<br/>R&gt; rep(colSums(skin),each=kr)*rowSums(skin)<br/>   None    None    None    None Partial Partial Partial Partial   Full<br/>   5040   18816    5152   10752    5445   20328    5566   11616   5490<br/>   Full    Full    Full<br/>  20496    5612   11712<br/>R&gt; rep(colSums(skin),each=kr)*rowSums(skin)/sum(skin)<br/>    None     None     None     None  Partial  Partial  Partial  Partial<br/>14.19718 53.00282 14.51268 30.28732 15.33803 57.26197 15.67887 32.72113<br/>    Full     Full     Full     Full<br/>15.46479 57.73521 15.80845 32.99155<br/>R&gt; skin.expected &lt;- matrix(rep(colSums(skin),each=kr)<sub>*</sub>rowSums(skin)/sum(skin),<br/>                           nrow=kr,ncol=kc,dimnames=dimnames(skin))<br/>R&gt; skin.expected<br/>              None  Partial     Full<br/>Injection 14.19718 15.33803 15.46479<br/>Tablet    53.00282 57.26197 57.73521<br/>Laser     14.51268 15.67887 15.80845<br/>Herbal    30.28732 32.72113 32.99155</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_418"/>Note that all the expected values are greater than 5, as preferred.</p>&#13;
<p class="indent">It’s best to construct a single object to hold the results of the different stages of calculations leading to the test statistic, as you did for the one-dimensional example. Since each stage is a matrix, you can bind the relevant matrices together with <code>cbind</code> and produce an array of the appropriate dimensions (refer to <a href="ch03.xhtml#ch03lev1sec15">Section 3.4</a> for a refresher).</p>&#13;
<pre>R&gt; skin.array &lt;- array(data=cbind(skin,skin.expected,<br/>                                  (skin-skin.expected)^2/skin.expected),<br/>                       dim=c(kr,kc,3),<br/>                       dimnames=list(dimnames(skin)[[1]],dimnames(skin)[[2]],<br/>                                     c("O[i,j]","E[i,j]",<br/>                                       "(O[i,j]-E[i,j])^2/E[i,j]")))<br/>R&gt; skin.array<br/>, , O[i,j]<br/><br/>          None Partial Full<br/>Injection   20       9   16<br/>Tablet      32      72   64<br/>Laser        8       8   30<br/>Herbal      52      32   12<br/><br/>, , E[i,j]<br/><br/>              None  Partial     Full<br/>Injection 14.19718 15.33803 15.46479<br/>Tablet    53.00282 57.26197 57.73521<br/>Laser     14.51268 15.67887 15.80845<br/>Herbal    30.28732 32.72113 32.99155<br/><br/>, , (O[i,j]-E[i,j])^2/E[i,j]<br/><br/>                None   Partial        Full<br/>Injection   2.371786 2.6190199  0.01852279<br/>Tablet      8.322545 3.7932587  0.67978582<br/>Laser       2.922614 3.7607992 12.74002590<br/>Herbal     15.565598 0.0158926 13.35630339</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_419"/>The final steps are easy—the test statistic given by (18.12) is just the grand total of all elements of the matrix that is the third layer of <code>skin.array</code>.</p>&#13;
<pre>R&gt; X2 &lt;- sum(skin.array[,,3])<br/>R&gt; X2<br/>[1] 66.16615</pre>&#13;
<p class="indent">The corresponding <em>p</em>-value for this test of independence is as follows:</p>&#13;
<pre>R&gt; 1-pchisq(X2,df=(kr-1)<sub>*</sub>(kc-1))<br/>[1] 2.492451e-12</pre>&#13;
<p class="indent">Recall that the relevant degrees of freedom are defined as <em>ν</em> = (<em>k<sub>r</sub></em> − 1) × (<em>k<sub>c</sub></em> − 1).</p>&#13;
<p class="indent">The extremely small <em>p</em>-value provides strong evidence against the null hypothesis. The appropriate conclusion would be to reject H<sub>0</sub> and state that there does appear to be a relationship between the type of treatment for the skin affliction and the level of success in curing it.</p>&#13;
<h5 class="h5" id="ch18lev3sec79"><strong>R Function: chisq.test</strong></h5>&#13;
<p class="noindent">Yet once more, no section in this chapter would be complete without showcasing the built-in functionality R possesses for these fundamental procedures. The default behavior of <code>chisq.test</code>, when supplied a matrix as <code>x</code>, is to perform a chi-squared test of independence with respect to the row and column frequencies—just as performed manually here for the skin affliction example. The following result easily confirms your previous calculations:</p>&#13;
<pre>R&gt; chisq.test(x=skin)<br/><br/>        Pearson's Chi-squared test<br/><br/>data:  skin<br/>X-squared = 66.1662, df = 6, p-value = 2.492e-12</pre>&#13;
<div class="ex">&#13;
<p class="ext"><a id="ch18exc4"/><strong>Exercise 18.4</strong></p>&#13;
<p class="noindentz"><code>HairEyeColor</code> is a ready-to-use data set in R that you haven’t yet come across. This 4 × 4 × 2 array provides frequencies of hair and eye colors of 592 statistics students, split by sex (<a href="ref.xhtml#ref61">Snee, 1974</a>).</p>&#13;
<ol type="a">&#13;
<li><p class="noindents">Perform and interpret, at a significance level of <em>α</em> = 0.01, a chi-squared test of independence for hair against eye color for all students, regardless of their sex.</p></li>&#13;
</ol>&#13;
<p class="noindentz"><span epub:type="pagebreak" id="page_420"/>In <a href="ch08.xhtml#ch8exc1">Exercise 8.1</a> on <a href="ch08.xhtml#page_161">page 161</a>, you accessed the <code>Duncan</code> data set of the contributed package <code>car</code>, which contains markers of job prestige collected in 1950. Install the package if you haven’t already and load the data frame.</p>&#13;
<ol type="a" start="2">&#13;
<li><p class="noindents">The first column of <code>Duncan</code> is the variable <code>type</code>, recording the type of job as a factor with three levels: <code>prof</code> (professional or managerial), <code>bc</code> (blue collar), and <code>wc</code> (white collar). Construct appropriate hypotheses and perform a chi-squared GOF test to determine whether the three job types are equally represented in the data set.</p>&#13;
<ol type="i">&#13;
<li><p class="noindents">Interpret the resulting <em>p</em>-value with respect to a significance level of <em>α</em> = 0.05.</p></li>&#13;
<li><p class="noindents">What conclusion would you reach if you used a significance level of <em>α</em> = 0.01?</p></li>&#13;
</ol></li>&#13;
</ol>&#13;
</div>&#13;
<h3 class="h3" id="ch18lev1sec58"><strong>18.5 Errors and Power</strong></h3>&#13;
<p class="noindent">In discussing all these forms of statistical hypothesis testing, there has been one common thread: the interpretation of a <em>p</em>-value and what it tells you about your problem in terms of the hypotheses. Frequentist statistical hypothesis testing is ubiquitous in many fields of research, so it is important to at least briefly explore directly related concepts.</p>&#13;
<h4 class="h4" id="ch18lev2sec164"><strong><em>18.5.1 Hypothesis Test Errors</em></strong></h4>&#13;
<p class="noindent">Hypothesis testing is performed with the objective of obtaining a <em>p</em>-value in order to quantify evidence against the null statement H<sub>0</sub>. This is rejected in favor of the alternative, H<sub>A</sub>, if the <em>p</em>-value is itself less than a predefined significance level <em>α</em>, which is conventionally 0.05 or 0.01. As touched upon, this approach is justifiably criticized since the choice of <em>α</em> is essentially arbitrary; a decision to reject or retain H<sub>0</sub> can change depending solely upon the <em>α</em> value.</p>&#13;
<p class="indent">Consider for the moment, given a specific test, what the <em>correct</em> outcome is. If H<sub>0</sub> is really true, then you’d want to retain it. If H<sub>A</sub> is really true, you’d want to reject the null. This “truth,” one way or another, is impossible to know in practice. That being said, it’s useful to consider in a theoretical sense just how good (or bad) a given hypothesis test is at yielding a result that leads to the correct conclusion.</p>&#13;
<p class="indentb">To be able to test the validity of your rejection or retention of the null hypothesis, you must be able to identify two kinds of errors:</p>&#13;
<p class="bull">• A <em>Type I error</em> occurs when you incorrectly reject a true H<sub>0</sub>. In any given hypothesis test, the probability of a Type I error is equivalent to the significance level <em>α</em>.</p>&#13;
<p class="bull"><span epub:type="pagebreak" id="page_421"/>• A <em>Type II error</em> occurs when you incorrectly retain a false H<sub>0</sub> (in other words, fail to accept a true H<sub>A</sub>). Since this depends upon what the true H<sub>A</sub> actually is, the probability of committing such an error, labeled <em>β</em>, is not usually known in practice.</p>&#13;
<h4 class="h4" id="ch18lev2sec165"><strong><em>18.5.2 Type I Errors</em></strong></h4>&#13;
<p class="noindent">If your <em>p</em>-value is less than <em>α</em>, you reject the null statement. If the null is really true, though, the <em>α</em> directly defines the probability that you <em>incorrectly</em> reject it. This is referred to as a <em>Type I error</em>.</p>&#13;
<p class="indent"><a href="ch18.xhtml#ch18fig2">Figure 18-2</a> provides a conceptual illustration of a Type I error probability for a supposed hypothesis test of a sample mean, where the hypotheses are set up as H<sub>0</sub> : <em>μ</em> = <em>μ</em><sub>0</sub> and H<sub>A</sub> : <em>μ</em> &gt; <em>μ</em><sub>0</sub>.</p>&#13;
<div class="image"><img src="../images/f18-02.jpg" alt="image"/></div>&#13;
<p class="figt"><em><a id="ch18fig2"/>Figure 18-2: A conceptual diagram of the Type I error probability</em> <em>α</em></p>&#13;
<p class="indent">The null hypothesis distribution is centered on the null value <em>μ</em><sub>0</sub>; the alternative hypothesis distribution is centered to its right at some mean <em>μ</em><sub>A</sub> in <a href="ch18.xhtml#ch18fig2">Figure 18-2</a>. As you can see, if the null hypothesis is really true, then the probability it is incorrectly rejected for this test will be equal to the significance level <em>α</em>, located in the upper tail of the null distribution.</p>&#13;
<h5 class="h5" id="ch18lev3sec80"><strong>Simulating Type I Errors</strong></h5>&#13;
<p class="noindent">To demonstrate the Type I error rate via <em>numerical simulation</em> (here, this refers to randomly generating hypothetical data samples), you can write code that does the equivalent of repeating a hypothesis test under known conditions. So that you can use this code multiple times, in the R script editor define the following function:</p>&#13;
<pre>typeI.tester &lt;- function(mu0,sigma,n,alpha,ITERATIONS=10000){<br/>    pvals &lt;- rep(NA,ITERATIONS)<br/>    for(i in 1:ITERATIONS){<br/>        temporary.sample &lt;- rnorm(n=n,mean=mu0,sd=sigma)<br/>        temporary.mean &lt;- mean(temporary.sample)<br/>        temporary.sd &lt;- sd(temporary.sample)<br/>        pvals[i] &lt;- 1-pt((temporary.mean-mu0)/(temporary.sd/sqrt(n)),df=n-1)<br/>    }<br/>    return(mean(pvals&lt;alpha))<br/>}</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_422"/>The <code>typeI.tester</code> function is designed to generate <code>ITERATIONS</code> samples from a particular normal distribution. With each sample, you’ll perform an upper-tailed test of the mean (refer to <a href="ch18.xhtml#ch18lev2sec158">Section 18.2.1</a>) in the spirit of <a href="ch18.xhtml#ch18fig2">Figure 18-2</a>, assuming the hypotheses of H<sub>0</sub> : <em>μ</em> = <em>μ</em><sub>0</sub> and H<sub>A</sub> : <em>μ</em> &gt; <em>μ</em><sub>0</sub>.</p>&#13;
<p class="indent">You can decrease <code>ITERATIONS</code> to generate fewer entire samples, and this will speed up computation time but will result in simulated rates that are more variable. Each entire sample of size <code>n</code> of hypothetical raw measurements is generated using <code>rnorm</code> with the mean equal to the <code>mu0</code> argument (and standard deviation equal to the <code>sigma</code> argument). The desired significance level is set by <code>alpha</code>. In the <code>for</code> loop, the sample mean and sample standard deviation are calculated for each generated sample.</p>&#13;
<p class="indent">Were each sample subjected to a “real” hypothesis test, the <em>p</em>-value would be taken from the right-hand area of the <em>t</em>-distribution with <code>n-1</code> degrees of freedom (using <code>pt</code>), with respect to the standardized test statistic given earlier in <a href="ch18.xhtml#ch18eq2">Equation (18.2)</a>.</p>&#13;
<p class="indent">The calculated <em>p</em>-value, at each iteration, is stored in a predefined vector <code>pvals</code>. The logical vector <code>pvals&lt;alpha</code> therefore contains corresponding <code>TRUE</code>/<code>FALSE</code> values; the former logical value flags rejection of the null hypothesis, and the latter flags retention. The Type I error rate is determined by calling <code>mean</code> on that logical vector, which yields the proportion of <code>TRUE</code>s (in other words, the overall proportion of “null hypothesis rejections”) arising from the simulated samples. Remember, the samples are generated randomly, so your results are liable to change slightly each time you run the function.</p>&#13;
<p class="indent">This function works because, by definition of the problem, the samples that are being generated come from a distribution that truly has the mean set at the null value, in other words, <em>μ</em><sub>A</sub> = <em>μ</em><sub>0</sub>. Therefore, any statistical rejection of this statement, obtained with a <em>p</em>-value less than the significance level <em>α</em>, is clearly incorrect and is purely a result of random variation.</p>&#13;
<p class="indent">To try this, import the function and execute it generating the default <code>ITERATIONS=10000</code> samples. Use the standard normal as the null (and “true” in this case!) distribution; make each sample of size 40 and set the significance level at the conventional <em>α</em> = 0.05. Here’s an example:</p>&#13;
<pre>R&gt; typeI.tester(mu0=0,sigma=1,n=40,alpha=0.05)<br/>[1] 0.0489</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_423"/>This indicates that 10,000 × 0.0489 = 489 of the samples taken yielded a corresponding test statistic that provided a <em>p</em>-value, which would incorrectly result in rejection of H<sub>0</sub>. This simulated Type I error rate lies close to the preset <code>alpha=0.05</code>.</p>&#13;
<p class="indent">Here’s another example, this time for nonstandard normal data samples with <em>α</em> = 0.01:</p>&#13;
<pre>R&gt; typeI.tester(mu0=-4,sigma=0.3,n=60,alpha=0.01)<br/>[1] 0.0108</pre>&#13;
<p class="indent">Note that again, the numerically simulated rate of Type I error reflects the significance level.</p>&#13;
<p class="indent">These results are not difficult to understand theoretically—if the true distribution does indeed have a mean equal to the null value, you’ll naturally observe those “extreme” test statistic values in practice at a rate equal to <em>α</em>. The catch, of course, is that in practice the true distribution is unknown, highlighting once more the fact that a rejection of any H<sub>0</sub> can never be interpreted as proof of the truth of H<sub>A</sub>. It might simply be that the sample you observed followed the null hypothesis but produced an extreme test statistic value by chance, however small that chance might be.</p>&#13;
<h5 class="h5" id="ch18lev3sec81"><strong>Bonferroni Correction</strong></h5>&#13;
<p class="noindent">The fact that Type I errors naturally occur because of random variation is particularly important and leads us to consider the <em>multiple testing problem</em>. If you’re conducting many hypothesis tests, you should be cautious in simply reporting the “number of statistically significant outcomes”—as you increase the number of hypothesis tests, you increase the chance of receiving an erroneous result. In, say, 20 tests conducted under <em>α</em> = 0.05, on average one will be a so-called false positive; if you conduct 40 or 60 tests, you are inevitably more likely to find more false positives.</p>&#13;
<p class="indent">When several hypothesis tests are conducted, you can curb the multiple testing problem with respect to committing a Type I error by using the <em>Bonferroni correction</em>. The Bonferroni correction suggests that when performing a total of <em>N</em> independent hypothesis tests, each under a significance level of <em>α</em>, you should instead use <em>α</em><sub>B</sub> = <em>α</em>/<em>N</em> for any interpretation of statistical significance. Be aware, however, that this correction to the level of significance represents the simplest solution to the multiple testing problem and can be criticized for its conservative nature, which is potentially problematic when <em>N</em> is large.</p>&#13;
<p class="indent">The Bonferroni and other corrective measures were developed in an attempt to formalize remedies to making a Type I error in multiple tests. In general, though, it suffices to be aware of the possibility that H<sub>0</sub> may be true, even if the <em>p</em>-value is considered small.</p>&#13;
<h4 class="h4" id="ch18lev2sec166"><span epub:type="pagebreak" id="page_424"/><strong><em>18.5.3 Type II Errors</em></strong></h4>&#13;
<p class="noindent">The issues with Type I errors might suggest that it’s desirable to perform a hypothesis test with a smaller <em>α</em> value. Unfortunately, it’s not quite so simple; reducing the significance level for any given test leads directly to an increase in the chance of committing a Type II error.</p>&#13;
<p class="indent">A Type II error refers to incorrect retention of the null hypothesis—in other words, obtaining a <em>p</em>-value greater than the significance level when it’s the alternative hypothesis that’s actually true. For the same scenario you’ve been looking at so far (an upper-tailed test for a single sample mean), <a href="ch18.xhtml#ch18fig3">Figure 18-3</a> illustrates the probability of a Type II error, shaded and denoted <em>β</em>.</p>&#13;
<div class="image"><img src="../images/f18-03.jpg" alt="image"/></div>&#13;
<p class="figt"><em><a id="ch18fig3"/>Figure 18-3: A conceptual diagram of the Type II error probability <em>β</em></em></p>&#13;
<p class="indent">It’s not as easy to find <em>β</em> as it is to find the probability of making a Type I error because <em>β</em> depends, among other things, on what the true value of <em>μ</em><sub>A</sub> is (which in general you won’t know). If <em>μ</em><sub>A</sub> is closer to the hypothesized null value of <em>μ</em><sub>0</sub>, you can imagine the alternative distribution in <a href="ch18.xhtml#ch18fig3">Figure 18-3</a> translating (shifting) to the left, resulting in an increase in <em>β</em>. Similarly, staying with <a href="ch18.xhtml#ch18fig3">Figure 18-3</a>, imagine decreasing the significance level <em>α</em>. Doing so means the vertical dashed line (denoting the corresponding critical value) moves to the right, also increasing the shaded area of <em>β</em>. Intuitively, this makes sense—the closer the true alternative value is to the null and/or the smaller the significance level, the harder H<sub>A</sub> is to detect by rejection of H<sub>0</sub>.</p>&#13;
<p class="indent">As noted, <em>β</em> usually can’t be calculated in practice because of the need to know what the true distribution actually is. This quantity is, however, useful in giving you an idea of how prone a test is to the incorrect retention of a null hypothesis under particular conditions. Suppose, for example, you’re performing a one-sample <em>t</em>-test for H<sub>0</sub> : <em>μ</em> = <em>μ</em><sub>0</sub> and H<sub>A</sub> : <em>μ</em> &gt; <em>μ</em><sub>0</sub> with <em>μ</em><sub>0</sub> = 0 but that the (true) alternative distribution of the raw measurements has mean <em>μ</em><sub>A</sub> = 0.5 and standard deviation <em>σ</em> = 1. Given a random sample of size <em>n</em> = 30 and using <em>α</em> = 0.05, what is the probability of committing a Type II error in any given hypothesis test (using the same standard deviation <span epub:type="pagebreak" id="page_425"/>for the null distribution)? To answer this, look again at <a href="ch18.xhtml#ch18fig3">Figure 18-3</a>; you need the critical value marked off by the significance level (the dashed vertical line). If you assume <em>σ</em> is known, then the sampling distribution of interest will be normal with mean <em>μ</em><sub>0</sub> = 0 and a standard error of <img class="middle" src="../images/f0425-01a.jpg" alt="image"/> (see <a href="ch17.xhtml#ch17lev2sec146">Section 17.1.1</a>). Therefore, with an upper-tail area of 0.05, you can find the critical value with the following:</p>&#13;
<pre>R&gt; critval &lt;- qnorm(1-0.05,mean=0,sd=1/sqrt(30))<br/>R&gt; critval<br/>[1] 0.3003078</pre>&#13;
<p class="indent">This represents the vertical dashed line in this specific setting (see <a href="ch16.xhtml#ch16lev2sec142">Section 16.2.2</a> for a refresher on use of <code>qnorm</code>). The Type II error in this example is found as the left-hand tail area under the alternative, “true” distribution, from that critical value:</p>&#13;
<pre>R&gt; pnorm(critval,mean=0.5,sd=1/sqrt(30))<br/>[1] 0.1370303</pre>&#13;
<p class="indent">From this, you can see that a hypothesis test under these conditions has roughly a 13.7 percent chance of incorrect retention of the null.</p>&#13;
<h5 class="h5" id="ch18lev3sec82"><strong>Simulating Type II Errors</strong></h5>&#13;
<p class="noindent">Simulation is especially useful here. In the editor, consider the function <code>typeII.tester</code> defined as follows:</p>&#13;
<pre>typeII.tester &lt;- function(mu0,muA,sigma,n,alpha,ITERATIONS=10000){<br/>    pvals &lt;- rep(NA,ITERATIONS)<br/>    for(i in 1:ITERATIONS){<br/>        temporary.sample &lt;- rnorm(n=n,mean=muA,sd=sigma)<br/>        temporary.mean &lt;- mean(temporary.sample)<br/>        temporary.sd &lt;- sd(temporary.sample)<br/>        pvals[i] &lt;- 1-pt((temporary.mean-mu0)/(temporary.sd/sqrt(n)),df=n-1)<br/>    }<br/>    return(mean(pvals&gt;=alpha))<br/>}</pre>&#13;
<p class="indent">This function is similar to <code>typeI.tester</code>. The null value, standard deviation of raw measurements, sample size, significance level, and number of iterations are all as before. Additionally, you now have <code>muA</code>, providing the “true” mean <em>μ</em><sub>A</sub> under which to generate the samples. Again, at each iteration, a random sample of size <code>n</code> is generated, its mean and standard deviation are calculated, and the appropriate <em>p</em>-value for the test is computed using <code>pt</code> from the usual standardized test statistic with <code>df=n-1</code>. (Remember, since you’re estimating the true standard deviation of the measurements <em>σ</em> with the sample standard deviation <em>s</em>, it’s technically correct to use the <span epub:type="pagebreak" id="page_426"/><em>t</em>-distribution.) Following completion of the <code>for</code> loop, the proportion of <em>p</em>-values that were greater than or equal to the significance level <code>alpha</code> is returned.</p>&#13;
<p class="indent">After importing the function into the workspace, you can simulate <em>β</em> for this test.</p>&#13;
<pre>R&gt; typeII.tester(mu0=0,muA=0.5,sigma=1,n=30,alpha=0.05)<br/>[1] 0.1471</pre>&#13;
<p class="indent">My result indicates something close to the theoretical <em>β</em> evaluated previously, albeit slightly larger because of the additional uncertainty that is naturally present when using a <em>t</em>-based sampling distribution instead of a normal. Again, each time you run <code>typeII.tester</code>, the results will vary slightly since everything is based on randomly generated hypothetical data samples.</p>&#13;
<p class="indent">Turning your attention to <a href="ch18.xhtml#ch18fig3">Figure 18-3</a>, you can see (in line with a comment made earlier) that if, in an effort to decrease the chance of a Type I error, you use <em>α</em> = 0.01 instead of 0.05, the vertical line moves to the right, thereby increasing the probability of a Type II error, with all other conditions being held constant.</p>&#13;
<pre>R&gt; typeII.tester(mu0=0,muA=0.5,sigma=1,n=30,alpha=0.01)<br/>[1] 0.3891</pre>&#13;
<h5 class="h5" id="ch18lev3sec83"><strong>Other Influences on the Type II Error Rate</strong></h5>&#13;
<p class="noindent">The significance level isn’t the only contributing factor in driving <em>β</em>. Keeping <em>α</em> at 0.01, this time see what happens if the standard deviation of the raw measurements is increased from <em>σ</em> = 1 to <em>σ</em> = 1.1 and then <em>σ</em> = 1.2.</p>&#13;
<pre>R&gt; typeII.tester(mu0=0,muA=0.5,sigma=1.1,n=30,alpha=0.01)<br/>[1] 0.4815<br/>R&gt; typeII.tester(mu0=0,muA=0.5,sigma=1.2,n=30,alpha=0.01)<br/>[1] 0.5501</pre>&#13;
<p class="indent">Increasing the variability of the measurements, without touching anything else in the scenario, also increases the chance of a Type II error. You can imagine the curves in <a href="ch18.xhtml#ch18fig3">Figure 18-3</a> becoming flatter and more widely dispersed owing to a larger standard error of the mean, which would result in more probability weight in the left-hand tail marked off by the critical value. Conversely, if the variability of the raw measurements is smaller, then the sampling distributions of the sample mean will be taller and skinnier, meaning a reduction in <em>β</em>.</p>&#13;
<p class="indent">A smaller or larger sample size will have a similar impact. Located in the denominator of the standard error formula, a smaller <em>n</em> will result in a larger standard error and hence that flatter curve and an increased <em>β</em>; a larger sample size will have the opposite effect. If you remain with the latest values of <em>μ</em><sub>0</sub> = 0, <em>μ</em><sub>A</sub> = 0.5, <em>σ</em> = 1.2, and <em>α</em> = 0.01, note that reducing <span epub:type="pagebreak" id="page_427"/>the sample size to 20 (from 30) results in an increased simulated Type II error rate compared with the most recent result of 0.5501, but increasing the sample size to 40 improves the rate.</p>&#13;
<pre>R&gt; typeII.tester(mu0=0,muA=0.5,sigma=1.2,n=20,alpha=0.01)<br/>[1] 0.7319<br/>R&gt; typeII.tester(mu0=0,muA=0.5,sigma=1.2,n=40,alpha=0.01)<br/>[1] 0.4219</pre>&#13;
<p class="indent">Finally, as noted at the beginning of the discussion, the specific value of <em>μ</em><sub>A</sub> itself affects <em>β</em> just as you’d expect. Again, keeping the latest values for all other components, which resulted in my case in <em>β</em> = 0.4219, note that shifting the “true” mean closer to <em>μ</em><sub>0</sub> by changing from <em>μ</em><sub>A</sub> = 0.5 to <em>μ</em><sub>A</sub> = 0.4 means the probability of committing a Type II error is increased; the opposite is true if the difference is increased to <em>μ</em><sub>A</sub> = 0.6.</p>&#13;
<pre>R&gt; typeII.tester(mu0=0,muA=0.4,sigma=1.2,n=40,alpha=0.01)<br/>[1] 0.6147<br/>R&gt; typeII.tester(mu0=0,muA=0.6,sigma=1.2,n=40,alpha=0.01)<br/>[1] 0.2287</pre>&#13;
<p class="indent">To summarize, although these simulated rates have been applied to the specific situation in which the hypothesis test is an upper-tailed test for a single mean, the general concepts and ideas discussed here hold for any hypothesis test. It’s easy to establish that the Type I error rate matches the predefined significance level and so can be decreased by reducing <em>α</em>. In contrast, controlling the Type II error rate is a complex balancing act that can involve sample size, significance level, observation variability, and magnitude of the difference between the true value and the null. This problem is largely academic since the “truth” is typically unknown in practice. However, the Type II error rate’s direct relationship to statistical power often plays a critical role in preparing for data collection, especially when you’re considering sample size requirements, as you’ll see in the next section.</p>&#13;
<div class="ex">&#13;
<p class="ext"><a id="ch18exc5"/><strong>Exercise 18.5</strong></p>&#13;
<ol type="a">&#13;
<li><p class="noindents">Write a new version of <code>typeI.tester</code> called <code>typeI.mean</code>. The new function should be able to simulate the Type I error rate for tests of a single mean in any direction (in other words, one-or two-sided). The new function should take an additional argument, <code>test</code>, which takes a character string <code>"less"</code>, <code>"greater"</code>, or <code>"two.sided"</code> depending on the type of desired test. You can achieve this by modifying <code>typeI.tester</code> as follows:</p>&#13;
<p class="dash">– Instead of calculating and storing the <em>p</em>-values directly in the <code>for</code> loop, simply store the test statistic.</p>&#13;
<p class="dash"><span epub:type="pagebreak" id="page_428"/>– When the loop is complete, set up stacked <code>if</code>-<code>else</code> statements that cater to each of the three types of test, calculating the <em>p</em>-value as appropriate.</p>&#13;
<p class="dash">– For the two-sided test, remember that the <em>p</em>-value is defined as twice the area “more extreme” than the null. Computationally, this means you must use the upper-tail area if the test statistic is positive and the lower-tail area otherwise. If this area is less than half of <em>α</em> (since it is subsequently multiplied by 2 in a “real” hypothesis test), then a rejection of the null should be flagged.</p>&#13;
<p class="dash">– If the value of <code>test</code> is not one of the three possibilities, the function should throw an appropriate error using <code>stop</code>.</p>&#13;
<ol type="i">&#13;
<li><p class="noindents">Experiment with your function using the first example setting in the text with <em>μ</em><sub>0</sub> = 0, <em>σ</em> = 1, <em>n</em> = 40, and <em>α</em> = 0.05. Call <code>typeI.mean</code> three times, using each of the three possible options for <code>test</code>. You should find that all simulated results sit close to 0.05.</p></li>&#13;
<li><p class="noindents">Repeat (i) using the second example setting in the text with <em>μ</em><sub>0</sub> = −4, <em>σ</em> = 0.3, <em>n</em> = 60, and <em>α</em> = 0.01. Again, you should find that all simulated results sit close to the value of <em>α</em>.</p></li>&#13;
</ol></li>&#13;
<li><p class="noindents">Modify <code>typeII.tester</code> in the same way as you did <code>typeI.tester</code>; call the new function <code>typeII.mean</code>. Simulate the Type II error rates for the following hypothesis tests. As per the text, assume <em>μ</em><sub>A</sub>, <em>σ</em>, <em>α</em>, and <em>n</em> denote the true mean, standard deviation of raw observations, significance level, and sample size, respectively.</p>&#13;
<ol type="i">&#13;
<li><p class="noindent">H<sub>0</sub> : <em>μ</em> = −3.2; H<sub>A</sub> : <em>μ</em> ≠ −3.2</p>&#13;
<p class="noindent">with <em>μ</em><sub>A</sub> = −3.3, <em>σ</em> = 0.1, <em>α</em> = 0.05, and <em>n</em> = 25.</p></li>&#13;
<li><p class="noindent">H<sub>0</sub> : <em>μ</em> = 8994; H<sub>A</sub> : <em>μ</em> &lt; 8994</p>&#13;
<p class="noindent">with <em>μ</em><sub>A</sub> = 5600, <em>σ</em> = 3888, <em>α</em> = 0.01, and <em>n</em> = 9.</p></li>&#13;
<li><p class="noindent">H<sub>0</sub> : <em>μ</em> = 0.44; H<sub>A</sub> : <em>μ</em> &gt; 0.44</p>&#13;
<p class="noindent">with <em>μ</em><sub>A</sub> = 0.4, <em>σ</em> = 2.4, <em>α</em> = 0.05, and <em>n</em> = 68.</p></li>&#13;
</ol></li>&#13;
</ol>&#13;
</div>&#13;
<h4 class="h4" id="ch18lev2sec167"><strong><em>18.5.4 Statistical Power</em></strong></h4>&#13;
<p class="noindent">For any hypothesis test, it is useful to consider its potential statistical power. <em>Power</em> is the probability of correctly rejecting a null hypothesis that is untrue. For a test that has a Type II error rate of <em>β</em>, the statistical power is found simply with 1 − <em>β</em>. It’s desirable for a test to have a power that’s as high as possible. The simple relationship with the Type II error probability means that all factors impacting the value of <em>β</em> also directly affect power.</p>&#13;
<p class="indent">For the same one-sided H<sub>0</sub> : <em>μ</em> = <em>μ</em><sub>0</sub> and H<sub>A</sub> : <em>μ</em> &gt; <em>μ</em><sub>0</sub> example discussed in the previous section, <a href="ch18.xhtml#ch18fig4">Figure 18-4</a> shades the power of the test—the complement to the Type II error rate. By convention, a hypothesis test that has a power greater than 0.8 is considered <em>statistically powerful</em>.</p>&#13;
<div class="image"><span epub:type="pagebreak" id="page_429"/><img src="../images/f18-04.jpg" alt="image"/></div>&#13;
<p class="figt"><em><a id="ch18fig4"/>Figure 18-4: A conceptual diagram of statistical power 1 − <em>β</em></em></p>&#13;
<p class="indent">You can numerically evaluate power under specific testing conditions using simulation. For the previous discussion on Type II errors, you’re able to subtract all simulated results of <em>β</em> from 1 to evaluate the power of that particular test. For example, the power of detection of <em>μ</em><sub>A</sub> = 0.6 when <em>μ</em><sub>0</sub> = 0, taking samples of size <em>n</em> = 40 and with <em>σ</em> = 1.2 and <em>α</em> = 0.01, is simulated as 1 − 0.2287 = 0.7713 (using my most recent result of <em>β</em> from earlier). This means there’s roughly a 77 percent chance of correctly detecting the true mean of 0.6 in a hypothesis test based on a sample of measurements generated under those conditions.</p>&#13;
<p class="indent">Researchers are often interested in the relationship between power and sample size (though it is important to bear in mind that this is only one of the influential ingredients in the determination of power). Before you begin to collect data to examine a particular hypothesis, you might have an idea of the potential true value of the parameter of interest from past research or pilot studies. This is useful in helping to determine your sample size, such as in helping to answer questions like “How big does my sample need to be in order to be able to conduct a statistically powerful test to correctly reject H<sub>0</sub>, if the true mean is actually <em>μ</em><sub>A</sub>?”</p>&#13;
<h5 class="h5" id="ch18lev3sec84"><strong>Simulating Power</strong></h5>&#13;
<p class="noindent">For the most recent testing conditions, with a sample size of <em>n</em> = 40, you’ve seen that there’s a power of around 0.77 of detecting <em>μ</em><sub>A</sub> = 0.6. For the purposes of this example, let’s say you want to find how much you should increase <em>n</em> by in order to conduct a statistically powerful test. To answer this, define the following function <code>power.tester</code> in the editor:</p>&#13;
<pre>power.tester &lt;- function(nvec,...){<br/>    nlen &lt;- length(nvec)<br/>    result &lt;- rep(NA,nlen)<br/>    pbar &lt;- txtProgressBar(min=0,max=nlen,style=3)<br/>    for(i in 1:nlen){<br/>        result[i] &lt;- 1-typeII.tester(n=nvec[i],...)<br/>        setTxtProgressBar(pbar,i)<br/>    }<br/>    close(pbar)<br/>    return(result)<br/>}</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_430"/>The <code>power.tester</code> function uses the <code>typeII.tester</code> function defined in <a href="ch18.xhtml#ch18lev2sec166">Section 18.5.3</a> to evaluate the power of a given upper-tailed hypothesis test of a single sample mean. It takes a vector of sample sizes supplied as the <code>nvec</code> argument (you pass all other arguments to <code>typeII.tester</code> using an ellipsis—refer to <a href="ch11.xhtml#ch11lev2sec102">Section 11.2.4</a>). A <code>for</code> loop defined in <code>power.tester</code> cycles through the entries of <code>nvec</code> one at a time, simulates the power for each sample size, and stores them in a corresponding vector that’s returned to the user. Remember, through <code>typeII.tester</code>, this function is using random generation of hypothetical data samples, so there may be some fluctuation in the results you observe each time you run <code>power.tester</code>.</p>&#13;
<p class="indent">There can be a slight delay when evaluating the power for many individual sample sizes, so this function also provides a good opportunity to showcase a progress bar in a practical implementation (refer to <a href="ch12.xhtml#ch12lev2sec108">Section 12.2.1</a> for details).</p>&#13;
<p class="indent">Set up the following vector, which uses the colon operator (see <a href="ch02.xhtml#ch02lev2sec21">Section 2.3.2</a>) to construct a sequence of integers between 5 and 100 inclusive for the sample sizes to be examined:</p>&#13;
<pre>R&gt; sample.sizes &lt;- 5:100</pre>&#13;
<p class="indent">Importing the <code>power.tester</code> function, you can then simulate the power for each of these integers for this particular test (<code>ITERATIONS</code> is halved to <code>5000</code> to reduce the overall completion time).</p>&#13;
<pre>R&gt; pow &lt;- power.tester(nvec=sample.sizes,<br/>                       mu0=0,muA=0.6,sigma=1.2,alpha=0.01,ITERATIONS=5000)<br/>  |====================================================================| 100%<br/>R&gt; pow<br/> [1] 0.0630 0.0752 0.1018 0.1226 0.1432 0.1588 0.1834 0.2162 0.2440 0.2638<br/>[11] 0.2904 0.3122 0.3278 0.3504 0.3664 0.3976 0.4232 0.4478 0.4680 0.4920<br/>[21] 0.5258 0.5452 0.5552 0.5616 0.5916 0.6174 0.6326 0.6438 0.6638 0.6844<br/>[31] 0.6910 0.7058 0.7288 0.7412 0.7552 0.7718 0.7792 0.7950 0.8050 0.8078<br/>[41] 0.8148 0.8316 0.8480 0.8524 0.8600 0.8702 0.8724 0.8800 0.8968 0.8942<br/>[51] 0.8976 0.9086 0.9116 0.9234 0.9188 0.9288 0.9320 0.9378 0.9370 0.9448<br/>[61] 0.9436 0.9510 0.9534 0.9580 0.9552 0.9648 0.9656 0.9658 0.9684 0.9756<br/>[71] 0.9742 0.9770 0.9774 0.9804 0.9806 0.9804 0.9806 0.9854 0.9848 0.9844<br/>[81] 0.9864 0.9886 0.9890 0.9884 0.9910 0.9894 0.9906 0.9930 0.9926 0.9938<br/>[91] 0.9930 0.9946 0.9948 0.9942 0.9942 0.9956</pre>&#13;
<p class="indent">As expected, the power of detection rises steadily as <em>n</em> increases; the conventional cutoff of 80 percent is visible in these results as lying between <span epub:type="pagebreak" id="page_431"/>0.7950 and 0.8050. If you don’t want to identify the value visually, you can find which entry of <code>sample.sizes</code> corresponds to the 80 percent cutoff by first using <code>which</code> to identify the indexes of <code>pow</code> that are at least 0.8 and then returning the lowest value in that category with <code>min</code>. The identified index may then be specified in square brackets to <code>sample.sizes</code> to give you the value of <em>n</em> that corresponds to that simulated power (0.8050 in this case). These commands can be nested as follows:</p>&#13;
<pre>R&gt; minimum.n &lt;- sample.sizes[min(which(pow&gt;=0.8))]<br/>R&gt; minimum.n<br/>[1] 43</pre>&#13;
<p class="indent">The result indicates that if your sample size is at least 43, a hypothesis test under these particular conditions should be statistically powerful (based on the randomly simulated output in <code>pow</code> in this instance).</p>&#13;
<p class="indent">What if the significance level for this test were relaxed? Say you wanted to conduct the test (still upper-tailed under the condition of <em>μ</em><sub>0</sub> = 0, <em>μ</em><sub>A</sub> = 0.6, and <em>σ</em> = 1.2) using a significance level of <em>α</em> = 0.05 rather than 0.01. If you look again at <a href="ch18.xhtml#ch18fig4">Figure 18-4</a>, this alteration means the vertical line (critical value) moves to the left, decreasing <em>β</em> and so increasing power. That would therefore suggest you’d require a smaller sample size than earlier, in other words, <em>n</em> &lt; 43, in order to perform a statistically powerful test when <em>α</em> is increased.</p>&#13;
<p class="indent">To simulate this situation for the same range of sample sizes and store the resulting powers in <code>pow2</code>, examine the following:</p>&#13;
<pre>R&gt; pow2 &lt;- power.tester(nvec=sample.sizes,<br/>                        mu0=0,muA=0.6,sigma=1.2,alpha=0.05,ITERATIONS=5000)<br/>  |====================================================================| 100%<br/>R&gt; minimum.n2 &lt;- sample.sizes[min(which(pow2&gt;0.8))]<br/>R&gt; minimum.n2<br/>[1] 27</pre>&#13;
<p class="indent">This result indicates a sample size of at least 27 is required, which is a noticeable reduction from the 43 noted if <em>α</em> = 0.01. However, relaxing <em>α</em> means an increased risk of committing a Type I error!</p>&#13;
<h5 class="h5" id="ch18lev3sec85"><strong>Power Curves</strong></h5>&#13;
<p class="noindent">For comparison, you can plot your simulated powers as a kind of power curve using both <code>pow</code> and <code>pow2</code> with the following code:</p>&#13;
<pre>R&gt; plot(sample.sizes,pow,xlab="sample size n",ylab="simulated power")<br/>R&gt; points(sample.sizes,pow2,col="gray")<br/>R&gt; abline(h=0.8,lty=2)<br/>R&gt; abline(v=c(minimum.n,minimum.n2),lty=3,col=c("black","gray"))<br/>R&gt; legend("bottomright",legend=c("alpha=0.01","alpha=0.05"),<br/>          col=c("black","gray"),pch=1)</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_432"/>My particular image is given in <a href="ch18.xhtml#ch18fig5">Figure 18-5</a>. A horizontal line marks off the power of 0.8, and the vertical line marks the minimum sample size values identified and stored in <code>minimum.n</code> and <code>minimum.n2</code>. As a final touch, a legend is added to reference the <em>α</em> values of each curve.</p>&#13;
<div class="image"><img src="../images/f18-05.jpg" alt="image"/></div>&#13;
<p class="figt"><em><a id="ch18fig5"/>Figure 18-5: Simulated power curves for the upper-tailed hypothesis test of a single sample mean</em></p>&#13;
<p class="indent">The curves themselves indicate exactly what you’d expect—the power of detection increases as the sample size is incremented. You can also note the flattening off occurring as the power rises ever closer to the “perfect” rate of 1, which is typical of a power curve. For <em>α</em> = 0.05, the curve sits almost consistently above the curve for <em>α</em> = 0.01, though the difference looks negligible as <em>n</em> rises above 75 or so.</p>&#13;
<p class="indent">The preceding discussion of errors and power highlights the need for care in interpreting the results of even the most basic of statistical tests. A <em>p</em>-value is merely a probability, and as such, no matter how small it may be in any circumstance, it can never prove or disprove a claim on its own. Issues surrounding the quality of a hypothesis test (parametric or otherwise) should be considered, though this is arguably difficult in practice. Nevertheless, an awareness of Type I and Type II errors, as well as the concept of statistical power, is extremely useful in the implementation and appraisal of any formal statistical testing procedure.</p>&#13;
<div class="ex">&#13;
<p class="ext"><span epub:type="pagebreak" id="page_433"/><a id="ch18exc6"/><strong>Exercise 18.6</strong></p>&#13;
<ol type="a">&#13;
<li><p class="noindents">For this exercise you’ll need to have written <code>typeII.mean</code> from <a href="ch18.xhtml#ch18exc5">Exercise 18.5</a> (b). Using this function, modify <code>power.tester</code> so that a new function, <code>power.mean</code>, calls <code>typeII.mean</code> instead of calling <code>typeII.tester</code>.</p>&#13;
<ol type="i">&#13;
<li><p class="noindents">Confirm that the power of the test given by H<sub>0</sub> : <em>μ</em> = 10; H<sub>A</sub> : <em>μ</em> ≠ 10, with <em>μ</em><sub>A</sub> = 10.5, <em>σ</em> = 0.9, <em>α</em> = 0.01, and <em>n</em> = 50, is roughly 88 percent.</p></li>&#13;
<li><p class="noindentsb">Remember the hypothesis test in <a href="ch18.xhtml#ch18lev2sec158">Section 18.2.1</a> for the mean net weight of an 80-gram pack of snacks, based on the <em>n</em> = 44 observations provided in the <code>snack</code> vector. The hypotheses were as follows:</p>&#13;
<p class="center1">H<sub>0</sub> : <em>μ</em> = 80</p>&#13;
<p class="center1">H<sub>A</sub> : <em>μ</em> &lt; 80</p>&#13;
<p class="indentt">If the true mean is <em>μ</em><sub>A</sub> = 78.5 g and the true standard deviation of the weights is <em>σ</em> = 3.1 g, use <code>power.mean</code> to determine whether the test is statistically powerful, assuming <em>α</em> = 0.05. Does your answer to this change if <em>α</em> = 0.01?</p></li>&#13;
</ol></li>&#13;
<li><p class="noindents">Staying with the snacks hypothesis test, using the <code>sample.sizes</code> vector from the text, determine the minimum sample size required for a statistically powerful test using both <em>α</em> = 0.05 and <em>α</em> = 0.01. Produce a plot showing the two power curves.</p></li>&#13;
</ol>&#13;
</div>&#13;
<h5 class="h5" id="ch18lev3sec86"><strong>Important Code in This Chapter</strong></h5>&#13;
<table class="topbot">&#13;
<thead>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table_th"><p class="table"><strong>Function/operator</strong></p></td>&#13;
<td style="vertical-align: top;" class="table_th"><p class="table"><strong>Brief description</strong></p></td>&#13;
<td style="vertical-align: top;" class="table_th"><p class="table"><strong>First occurrence</strong></p></td>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><code>t.test</code></p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">One- and two-sample <em>t</em>-test</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><a href="ch18.xhtml#ch18lev2sec158">Section 18.2.1</a>, <a href="ch18.xhtml#page_391">p. 391</a></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><code>prop.test</code></p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">One- and two-sample <em>Z</em>-test</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><a href="ch18.xhtml#ch18lev2sec160">Section 18.3.1</a>, <a href="ch18.xhtml#page_405">p. 405</a></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><code>pchisq</code></p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">χ<sup>2</sup> cumulative problems</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><a href="ch18.xhtml#ch18lev2sec162">Section 18.4.1</a>, <a href="ch18.xhtml#page_414">p. 414</a></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><code>chisq.test</code></p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">χ<sup>2</sup> test of distribution/independence</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><a href="ch18.xhtml#ch18lev2sec162">Section 18.4.1</a>, <a href="ch18.xhtml#page_414">p. 414</a></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><code>rowSums</code></p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">Matrix row totals</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><a href="ch18.xhtml#ch18lev2sec163">Section 18.4.2</a>, <a href="ch18.xhtml#page_417">p. 417</a></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><code>colSums</code></p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">Matrix column totals</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><a href="ch18.xhtml#ch18lev2sec163">Section 18.4.2</a>, <a href="ch18.xhtml#page_417">p. 417</a><span epub:type="pagebreak" id="page_434"/></p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
</body></html>