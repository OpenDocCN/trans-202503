<html><head></head><body><div id="sbo-rt-content"><section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" title="85" id="Page_85"/>6</span><br/>
<span class="ChapterTitle">Testing</span></h1>
</header>
<figure class="opener">
<img src="Images/chapterart.png" alt="" width="206" height="206"/>
</figure>
<p class="ChapterIntro">In this chapter, we’ll look at the various ways in which you can extend Rust’s testing capabilities and what other kinds of testing you may want to add into your testing mix. Rust comes with a number of built-in testing facilities that are well covered in <em>The Rust Programming Language</em>, represented primarily by the <code>#[test]</code> attribute and the <em>tests/</em> directory. These will serve you well across a wide range of applications and scales and are often all you need when you are getting started with a project. However, as the codebase develops and your testing needs grow more elaborate, you may need to go beyond just tagging <code>#[test]</code> onto individual functions. </p>
<p>This chapter is divided into two main sections. The first part covers Rust testing mechanisms, like the standard testing harness and conditional testing code. The second looks at other ways to evaluate the correctness of your Rust code, such as benchmarking, linting, and fuzzing.</p>
<h2 id="h1-501850c07-0001"><span epub:type="pagebreak" title="86" id="Page_86"/>Rust Testing Mechanisms</h2>
<p class="BodyFirst">To understand the various testing mechanisms Rust provides, you must first understand how Rust builds and runs tests. When you run <code>cargo test --lib</code>, the only special thing Cargo does is pass the <code>--test</code> flag to <code>rustc</code>. This flag tells <code>rustc</code> to produce a test binary that runs all the unit tests, rather than just compiling the crate’s library or binary. Behind the scenes, <code>--test</code> has two primary effects. First, it enables <code>cfg(test)</code> so that you can conditionally include testing code (more on that in a bit). Second, it makes the compiler generate a <em>test harness</em>: a carefully generated <code>main</code> function that invokes each <code>#[test]</code> function in your program when it’s run.</p>
<h3 id="h2-501850c07-0001">The Test Harness</h3>
<p class="BodyFirst">The compiler generates the test harness <code>main</code> function through a mix of procedural macros, which we’ll discuss in greater depth in <span class="xref" itemid="xref_target_Chapter 7">Chapter 7</span>, and a light sprinkling of magic. Essentially, the harness transforms every function annotated by <code>#[test]</code> into a test <em>descriptor</em>—this is the procedural macro part. It then exposes the path of each of the descriptors to the generated <code>main</code> function—this is the magic part. The descriptor includes information like the test’s name, any additional options it has set (like <code>#[should_panic]</code>), and so on. At its core, the test harness iterates over the tests in the crate, runs them, captures their results, and prints the results. So, it also includes logic to parse command line arguments (for things like <code>--test-threads=1</code>), capture test output, run the listed tests in parallel, and collect test results.</p>
<p>As of this writing, Rust developers are working on making the magic part of test harness generation a publicly available API so that developers can build their own test harnesses. This work is still at the experimental stage, but the proposal aligns fairly closely with the model as it exists today. Part of the magic that needs to be figured out is how to ensure that <code>#[test]</code> functions are available to the generated <code>main</code> function even if they are inside private submodules.</p>
<p>Integration tests (the tests in <em>tests/</em>) follow the same process as unit tests, with the one exception that they are each compiled as their own separate crate, meaning they can access only the main crate’s public interface and are run against the main crate compiled without <code>#[cfg(test)]</code>. A test harness is generated for each file in <em>tests/</em>. Test harnesses are not generated for files in subdirectories under <em>tests/</em> to allow you to have shared submodules for your tests.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p>	If you explicitly want a test harness for a file in a subdirectory, you can opt in to that by calling the file <em>main.rs</em>.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>Rust does not require that you use the default test harness. You can instead opt out of it and implement your own <code>main</code> method that represents the test runner by setting <code>harness = false</code> for a given integration test in <em>Cargo.toml</em>, as shown in <a href="#listing6-1" id="listinganchor6-1">Listing 6-1</a>. The <code>main</code> method that you define will then be invoked to run the test.</p>
<pre><code><span epub:type="pagebreak" title="87" id="Page_87"/>[[test]]
name = "custom"
path = "tests/custom.rs"
harness = false</code></pre>
<p class="CodeListingCaption"><a id="listing6-1">Listing 6-1</a>: Opting out of the standard test harness</p>
<p>Without the test harness, none of the magic around <code>#[test]</code> happens. Instead, you’re expected to write your own <code>main</code> function to run the testing code you want to execute. Essentially, you’re writing a normal Rust binary that just happens to be run by <code>cargo test</code>. That binary is responsible for handling all the things that the default harness normally does (if you want to support them), such as command line flags. The <code>harness</code> property is set separately for each integration test, so you can have one test file that uses the standard harness and one that does not.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="box">
<h2>Arguments to the Default Test Harness</h2>
<p class="BoxBodyFirst">The default test harness supports a number of command line arguments to configure how the tests are run. These aren’t passed to <code>cargo test</code> directly but rather to the test binary that Cargo compiles and runs for you when you run <code>cargo test</code>. To access that set of flags, pass <code>--</code> to <code>cargo test</code>, followed by the arguments to the test binary. For example, to see the help text for the test binary, you’d run <code>cargo test -- --help</code>.</p>
<p>A number of handy configuration options are available through these command line arguments. The <code>--nocapture</code> flag disables the output capturing that normally happens when you run Rust tests. This is useful if you want to observe a test’s output in real time rather than all at once after the test has failed. You can use the <code>--test-threads</code> option to limit how many tests run concurrently, which is helpful if you have a test that hangs or segfaults and you want to figure out which one it is by running the tests sequentially. There’s also a <code>--skip</code> option for skipping tests that match a certain pattern, <code>--ignored</code> to run tests that would normally be ignored (such as those that require an external program to be running), and <code>--list</code> to just list all the available tests.</p>
<p>Keep in mind that these arguments are all implemented by the default test harness, so if you disable it (with <code>harness = false</code>), you’ll have to implement the ones you need yourself in your <code>main</code> function!</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>Integration tests without a harness are primarily useful for benchmarks, as we’ll see later, but they also come in handy when you want to run tests that don’t fit the standard “one function, one test” model. For example, you’ll frequently see harnessless tests used with fuzzers, model checkers, and tests that require a custom global setup (like under WebAssembly or when working with custom targets).</p>
<h3 id="h2-501850c07-0002"><span epub:type="pagebreak" title="88" id="Page_88"/>#[cfg(test)]</h3>
<p class="BodyFirst">When Rust builds code for testing, it sets the compiler configuration flag <code>test</code>, which you can then use with conditional compilation to have code that is compiled out unless it is specifically being tested. On the surface, this may seem odd: don’t you want to test exactly the same code that’s going into production? You do, but having code exclusively available when testing allows you to write better, more thorough tests, in a few ways.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="box">
<h2>Mocking</h2>
<p class="BoxBodyFirst">When writing tests, you often want tight control over the code you’re testing as well as any other types that your code may interact with. For example, if you are testing a network client, you probably do not want to run your unit tests over a real network but instead want to directly control what bytes are emitted by the “network” and when. Or, if you’re testing a data structure, you want your test to use types that allow you to control what each method returns on each invocation. You may also want to gather metrics such as how often a given method was called or whether a given byte sequence was emitted.</p>
<p>These “fake” types and implementations are known as <em>mocks</em>, and they are a key feature of any extensive unit test suite. While you can often do the work needed to get this kind of control manually, it’s nicer to have a library take care of most of the nitty-gritty details for you. This is where automated mocking comes into play. A mocking library will have facilities for generating types (including functions) with particular properties or signatures, as well as mechanisms to control and introspect those generated items during a test execution.</p>
<p>Mocking in Rust generally happens through generics—as long as your program, data structure, framework, or tool is generic over anything you might want to mock (or takes a trait object), you can use a mocking library to generate conforming types that will instantiate those generic parameters. You then write your unit tests by instantiating your generic constructs with the generated mock types, and you’re off to the races!</p>
<p>In situations where generics are inconvenient or inappropriate, such as if you want to avoid making a particular aspect of your type generic to users, you can instead encapsulate the state and behavior you want to mock in a dedicated struct. You would then generate a mocked version of that struct and its methods and use conditional compilation to use either the real or mocked implementation depending on <code>cfg(test)</code> or a test-only feature like <code>cfg(feature = "test_mock_foo")</code>.</p>
<p>At the moment, there isn’t a single mocking library, or even a single mocking approach, that has emerged as the One True Answer in the Rust community. The most extensive and thorough mocking library I know of is the <code>mockall</code> crate, but that is still under active development, and there are many other contenders.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<h4 id="h3-501850c07-0001"><span epub:type="pagebreak" title="89" id="Page_89"/>Test-Only APIs</h4>
<p class="BodyFirst">First, having test-only code allows you to expose additional methods, fields, and types to your (unit) tests so the tests can check not only that the public API behaves correctly but also that the internal state is correct. For example, consider the <code>HashMap</code> type from <code>hashbrown</code>, the crate that implements the standard library <code>HashMap</code>. The <code>HashMap</code> type is really just a wrapper around a <code>RawTable</code> type, which is what implements most of the hash table logic. Suppose that after doing a <code>HashMap::insert</code> on an empty map, you want to check that a single bucket in the map is nonempty, as shown in <a href="#listing6-2" id="listinganchor6-2">Listing 6-2</a>.</p>
<pre><code>#[test]
fn insert_just_one() {
  let mut m = HashMap::new();
  m.insert(42, ());
  let full = m.table.buckets.iter().filter(Bucket::is_full).count();
  assert_eq!(full, 1);
}</code></pre>
<p class="CodeListingCaption"><a id="listing6-2">Listing 6-2</a>: A test that accesses inaccessible internal state and thus does not compile</p>
<p>This code will not compile as written, because while the test code can access the private <code>table</code> field of <code>HashMap</code>, it cannot access the also private <code>buckets</code> field of <code>RawTable</code>, as <code>RawTable</code> lives in a different module. We could fix this by making the <code>buckets</code> field visibility <code>pub(crate)</code>, but we really don’t want <code>HashMap</code> to be able to touch <code>buckets</code> in general, as it could accidentally corrupt the internal state of the <code>RawTable</code>. Even making <code>buckets</code> available as read-only could be problematic, as new code in <code>HashMap</code> may then start depending on the internal state of <code>RawTable</code>, making future modifications more difficult. </p>
<p>The solution is to use <code>#[cfg(test)]</code>. We can add a method to <code>RawTable</code> that allows access to <code>buckets</code> only while testing, as shown in <a href="#listing6-3" id="listinganchor6-3">Listing 6-3</a>, and thereby avoid adding footguns for the rest of the code. The code from <a href="#listing6-2">Listing 6-2</a> can then be updated to call <code>buckets()</code> instead of accessing the private <code>buckets</code> field.</p>
<pre><code>impl RawTable {
  #[cfg(test)]
  pub(crate) fn buckets(&amp;self) -&gt; &amp;[Bucket] {
    &amp;self.buckets
  }
}</code></pre>
<p class="CodeListingCaption"><a id="listing6-3">Listing 6-3</a>: Using <code>#[cfg(test)]</code> to make internal state accessible in the testing context</p>
<h4 id="h3-501850c07-0002">Bookkeeping for Test Assertions</h4>
<p class="BodyFirst">The second benefit of having code that exists only during testing is that you can augment the program to perform additional runtime bookkeeping that can then be inspected by tests. For example, imagine you’re writing your own version of the <code>BufWriter</code> type from the standard library. When testing it, you want to make sure that <code>BufWriter</code> does not issue system calls <span epub:type="pagebreak" title="90" id="Page_90"/>unnecessarily. The most obvious way to do so is to have the <code>BufWriter</code> keep track of how many times it has invoked <code>write</code> on the underlying <code>Write</code>. However, in production this information isn’t important, and keeping track of it introduces (marginal) performance and memory overhead. With <code>#[cfg(test)]</code>, you can have the bookkeeping happen only when testing, as shown in <a href="#listing6-4" id="listinganchor6-4">Listing 6-4</a>.</p>
<pre><code>struct BufWriter&lt;T&gt; {
  #[cfg(test)]
  write_through: usize,
  <span class="LiteralGray">// other fields...</span>
}

impl&lt;T: Write&gt; Write for BufWriter&lt;T&gt; {
  fn write(&amp;mut self, buf: &amp;[u8]) -&gt; Result&lt;usize&gt; {
    <span class="LiteralGray">// ...</span>
    if self.full() {
      #[cfg(test)]
      self.write_through += 1;
      let n = self.inner.write(&amp;self.buffer[..])?;
    <span class="LiteralGray">// ...</span>
  }
}</code></pre>
<p class="CodeListingCaption"><a id="listing6-4">Listing 6-4</a>: Using <code>#[cfg(test)]</code> to limit bookkeeping to the testing context</p>
<p>Keep in mind that <code>test</code> is set only for the crate that is being compiled as a test. For unit tests, this is the crate being tested, as you would expect. For integration tests, however, it is the integration test binary being compiled as a test—the crate you are testing is just compiled as a library and so will not have <code>test</code> set.</p>
<h3 id="h2-501850c07-0003">Doctests</h3>
<p class="BodyFirst">Rust code snippets in documentation comments are automatically run as test cases. These are commonly referred to as <em>doctests</em>. Because doctests appear in the public documentation of your crate, and users are likely to mimic what they contain, they are run as integration tests. This means that the doctests don’t have access to private fields and methods, and <code>test</code> is not set on the main crate’s code. Each doctest is compiled as its own dedicated crate and is run in isolation, just as if the user had copy-pasted the doctest into their own program.</p>
<p>Behind the scenes, the compiler performs some preprocessing on doctests to make them more concise. Most importantly, it automatically adds an <code>fn main</code> around your code. This allows doctests to focus only on the important bits that the user is likely to care about, like the parts that actually use types and methods from your library, without including unnecessary boilerplate.</p>
<p>You can opt out of this auto-wrapping by defining your own <code>fn main</code> in the doctest. You may want to do this, for example, if you want to write an <span epub:type="pagebreak" title="91" id="Page_91"/>asynchronous <code>main</code> function using something like <code>#[tokio::main] async fn main</code>, or if you want to add additional modules to the doctest. </p>
<p>To use the <code>?</code> operator in your doctest, you don’t normally have to use a custom <code>main</code> function as <code>rustdoc</code> includes some heuristics to set the return type to<code> Result&lt;(), impl Debug&gt;</code> if your code looks like it makes use of <code>?</code> (for example, if it ends with <code>Ok(())</code>). If type inference gives you a hard time about the error type for the function, you can disambiguate it by changing the last line of the doctest to be explicitly typed, like this: <code>Ok::&lt;(), T&gt;(())</code>.</p>
<p>Doctests have a number of additional features that come in handy as you write documentation for more complex interfaces. The first is the ability to hide individual lines. If you prefix a line of a doctest with a <code>#</code>, that line is included when the doctest is compiled and run, but it is not included in the code snippet generated in the documentation. This lets you easily hide details that are not important to the current example, such as implementing traits for dummy types or generating values. It is also useful if you wish to present a sequence of examples without showing the same leading code each time. <a href="#listing6-5" id="listinganchor6-5">Listing 6-5</a> gives an example of what a doctest with hidden lines might look like.</p>
<pre><code><span class="LiteralGray">/// Completely frobnifies a number through I/O.</span>
<span class="LiteralGray">///</span>
<span class="LiteralGray">/// In this first example we hide the value generation.</span>
<span class="LiteralGray">/// ```</span>
<span class="LiteralGray">/// # let unfrobnified_number = 0;</span>
<span class="LiteralGray">/// # let already_frobnified = 1;</span>
<span class="LiteralGray">/// assert!(frobnify(unfrobnified_number).is_ok());</span>
<span class="LiteralGray">/// assert!(frobnify(already_frobnified).is_err());</span>
<span class="LiteralGray">/// ```</span>
<span class="LiteralGray">///</span>
<span class="LiteralGray">/// Here's an example that uses ? on multiple types</span>
<span class="LiteralGray">/// and thus needs to declare the concrete error type,</span>
<span class="LiteralGray">/// but we don't want to distract the user with that.</span>
<span class="LiteralGray">/// We also hide the use that brings the function into scope.</span>
<span class="LiteralGray">/// ```</span>
<span class="LiteralGray">/// # use mylib::frobnify;</span>
<span class="LiteralGray">/// frobnify("0".parse()?)?;</span>
<span class="LiteralGray">/// # Ok::&lt;(), anyhow::Error&gt;(())</span>
<span class="LiteralGray">/// ```</span>
<span class="LiteralGray">///</span>
<span class="LiteralGray">/// You could even replace an entire block of code completely,</span>
<span class="LiteralGray">/// though use this _very_ sparingly:</span>
<span class="LiteralGray">/// ```</span>
<span class="LiteralGray">/// # /*</span>
<span class="LiteralGray">/// let i = ...;</span>
<span class="LiteralGray">/// # */</span>
<span class="LiteralGray">/// # let i = 42;</span>
<span class="LiteralGray">/// frobnify(i)?;</span>
<span class="LiteralGray">/// ```</span>
fn frobnify(i: usize) -&gt; std::io::Result&lt;()&gt; {</code></pre>
<p class="CodeListingCaption"><a id="listing6-5">Listing 6-5</a>: Hiding lines in a doctest with <code>#</code></p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<span epub:type="pagebreak" title="92" id="Page_92"/><h2><span class="NoteHead">Note</span></h2>
<p>	Use this feature with care; it can be frustrating to users if they copy-paste an example and then it doesn’t work because of required steps that you’ve hidden.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>Much like <code>#[test]</code> functions, doctests also support attributes that modify how the doctest is run. These attributes go immediately after the triple-backtick used to denote a code block, and multiple attributes can be separated by commas. </p>
<p>Like with test functions, you can specify the <code>should_panic</code> attribute to indicate that the code in a particular doctest should panic when run, or <code>ignore</code> to check the code segment only if <code>cargo test</code> is run with the <code>--ignored</code> flag. You can also use the <code>no_run</code> attribute to indicate that a given doctest should compile but should not be run.</p>
<p>The attribute <code>compile_fail</code> tells <code>rustdoc</code> that the code in the documentation example should not compile. This indicates to the user that a particular use is not possible and serves as a useful test to remind you to update the documentation should the relevant aspect of your library change. You can also use this attribute to check that certain static properties hold for your types. <a href="#listing6-6" id="listinganchor6-6">Listing 6-6</a> shows an example of how you can use <code>compile_fail</code> to check that a given type does not implement <code>Send</code>, which may be necessary to uphold safety guarantees in unsafe code.</p>
<pre><code>```compile_fail
# struct MyNonSendType(std::rc::Rc&lt;()&gt;);
fn is_send&lt;T: Send&gt;() {}
is_send::&lt;MyNonSendType&gt;();
```</code></pre>
<p class="CodeListingCaption"><a id="listing6-6">Listing 6-6</a>: Testing that code fails to compile with <code>compile_fail</code></p>
<p><code>compile_fail</code> is a fairly crude tool in that it gives no indication of <em>why</em> the code does not compile. For example, if code doesn’t compile because of a missing semicolon, a <code>compile_fail</code> test will appear to have been successful. For that reason, you’ll usually want to add the attribute only after you have made sure that the test indeed fails to compile with the expected error. If you need more fine-grained tests for compilation errors, such as when developing macros, take a look at the <code>trybuild</code> crate.</p>
<h2 id="h1-501850c07-0002">Additional Testing Tools</h2>
<p class="BodyFirst">There’s a lot more to testing than just running test functions and seeing that they produce the expected result. A thorough survey of testing techniques, methodologies, and tools is outside the scope of this book, but there are some key Rust-specific pieces that you should know about as you expand your testing repertoire.</p>
<h3 id="h2-501850c07-0004">Linting</h3>
<p class="BodyFirst">You may not consider a linter’s checks to be tests, but in Rust they often can be. The Rust linter <em>clippy</em> categorizes a number of its lints as <em>correctness </em><span epub:type="pagebreak" title="93" id="Page_93"/>lints. These lints catch code patterns that compile but are almost certainly bugs. Some examples are<code> a = b; b = a</code>, which fails to swap <code>a</code> and <code>b</code>; <code>std::mem::forget(t)</code>, where <code>t</code> is a reference; and <code>for x in y.next()</code>, which will iterate only over the first element in <code>y</code>. If you are not running clippy as part of your CI pipeline already, you probably should be.</p>
<p>Clippy comes with a number of other lints that, while usually helpful, may be more opinionated than you’d prefer. For example, the <code>type_complexity</code> lint, which is on by default, issues a warning if you use a particularly involved type in your program, like <code>Rc&lt;Vec&lt;Vec&lt;Box&lt;(u32, u32, u32, u32)&gt;&gt;&gt;&gt;</code>. While that warning encourages you to write code that is easier to read, you may find it too pedantic to be broadly useful. If some part of your code erroneously triggers a particular lint, or you just want to allow a specific instance of it, you can opt out of the lint just for that piece of code with <code>#[allow(clippy::name_of_lint)]</code>.</p>
<p>The Rust compiler also comes with its own set of lints in the form of warnings, though these are usually more directed toward writing idiomatic code than checking for correctness. Instead, correctness lints in the compiler are simply treated as errors (take a look at <code>rustc -W help</code> for a list).</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p>	Not all compiler warnings are enabled by default. Those disabled by default are usually still being refined, or are more about style than content. A good example of this is the “idiomatic Rust 2018 edition” lint, which you can enable with<var> </var><code>#![warn(rust_2018_idioms)]</code>. When this lint is enabled, the compiler will tell you if you’re failing to take advantage of changes brought by the Rust 2018 edition. Some other lints that you may want to get into the habit of enabling when you start a new project are <code>missing_docs</code> and <code>missing_debug_implementations</code>, which warn you if you’ve forgotten to document any public items in your crate or add <code>Debug</code> implementations for any public types, respectively.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<h3 id="h2-501850c07-0005">Test Generation</h3>
<p class="BodyFirst">Writing a good test suite is a lot of work. And even when you do that work, the tests you write test only the particular set of behaviors you were considering at the time you wrote them. Luckily, you can take advantage of a number of test generation techniques to develop better and more thorough tests. These generate input for you to use to check your application’s correctness. Many such tools exist, each with their own strengths and weaknesses, so here I’ll cover only the main strategies used by these tools: fuzzing and property testing.</p>
<h4 id="h3-501850c07-0003">Fuzzing</h4>
<p class="BodyFirst">Entire books have been written about fuzzing, but at a high level the idea is simple: generate random inputs to your program and see if it crashes. If the program crashes, that’s a bug. For example, if you’re writing a URL parsing library, you can fuzz-test your program by systematically generating random strings and throwing them at the parsing function until it panics. Done <span epub:type="pagebreak" title="94" id="Page_94"/>naively, this would take a while to yield results: if the fuzzer starts with <code>a</code>, then <code>b</code>, then <code>c</code>, and so on, it will take it a long time to generate a tricky URL like <code>http://[:]</code>. In practice, modern fuzzers use code coverage metrics to explore different paths in your code, which lets them reach higher degrees of coverage faster than if the inputs were truly chosen at random.</p>
<p>Fuzzers are great at finding strange corner cases that your code doesn’t handle correctly. They require little setup on your part: you just point the fuzzer at a function that takes a “fuzzable” input, and off it goes. For example, <a href="#listing6-7" id="listinganchor6-7">Listing 6-7</a> shows an example of how you might fuzz-test a URL parser.</p>
<pre><code>libfuzzer_sys::fuzz_target!(|data: &amp;[u8]| {
  if let Ok(s) = std::str::from_utf8(data) {
      let _ = url::Url::parse(s);
  }
});</code></pre>
<p class="CodeListingCaption"><a id="listing6-7">Listing 6-7</a>: Fuzzing a URL parser with <code>libfuzzer</code></p>
<p>The fuzzer will generate semi-random inputs to the closure, and any that form valid UTF-8 strings will be passed to the parser. Notice that the code here doesn’t check whether the parsing succeeds or fails—instead, it’s looking for cases where the parser panics or otherwise crashes due to internal invariants that are violated.</p>
<p>The fuzzer keeps running until you terminate it, so most fuzzing tools come with a built-in mechanism to stop after a certain number of test cases have been explored. If your input isn’t a trivially fuzzable type—something like a hash table—you can usually use a crate like <code>arbitrary</code> to turn the byte string that the fuzzer generates into a more complex Rust type. It feels like magic, but under the hood it’s actually implemented in a very straightforward fashion. The crate defines an <code>Arbitrary</code> trait with a single method, <code>arbitrary</code>, that constructs the implementing type from a source of random bytes. Primitive types like <code>u32</code> or <code>bool</code> read the necessary number of bytes from that input to construct a valid instance of themselves, whereas more complex types like <code>HashMap</code> or <code>BTreeSet</code> produce one number from the input to dictate their length and then call <code>Arbitrary</code> that number of times on their inner types. There’s even an attribute, <code>#[derive(Arbitrary)]</code>, that implements <code>Arbitrary</code> by just calling <code>arbitrary</code> on each contained type! To explore fuzzing further, I recommend starting with <code>cargo-fuzz</code>.</p>
<h4 id="h3-501850c07-0004">Property-Based Testing</h4>
<p class="BodyFirst">Sometimes you want to check not only that your program doesn’t crash but also that it does what it’s expected to do. It’s great that your <code>add</code> function didn’t panic, but if it tells you that the result of <code>add(1, 4)</code> is <code>68</code>, it’s probably still wrong. This is where <em>property-based testing</em> comes into play; you describe a number of properties your code should uphold, and then the property testing framework generates inputs and checks that those properties indeed hold. </p>
<p><span epub:type="pagebreak" title="95" id="Page_95"/>A common way to use property-based testing is to first write a simple but naive version of the code you want to test that you are confident is correct. Then, for a given input, you give that input to both the code you want to test and the simplified but naive version. If the result or output of the two implementations is the same, your code is good—that is the correctness property you’re looking for—but if it’s not, you’ve likely found a bug. You can also use property-based testing to check for properties not directly related to correctness, such as whether operations take strictly less time for one implementation than another. The common principle is that you want any difference in outcome between the real and test versions to be informative and actionable so that every failure allows you to make improvements. The naive implementation might be one from the standard library that you’re trying to replace or augment (like <code>std::collections::VecDeque</code>), or it might be a simpler version of an algorithm that you’re trying optimize (like naive versus optimized matrix multiplication).</p>
<p>If this approach of generating inputs until some condition is met sounds a lot like fuzzing, that’s because it is—smarter people than I have argued that fuzzing is “just” property-based testing where the property you’re testing for is “it doesn’t crash.”</p>
<p>One downside of property-based testing is that it relies more heavily on the provided descriptions of the inputs. Whereas fuzzing will keep trying all possible inputs, property testing tends to be guided by developer annotations like “a number between 0 and 64” or “a string that contains three commas.” This allows property testing to more quickly reach cases that fuzzers may take a long time to encounter randomly, but it does require manual work and may miss important but niche buggy inputs. As fuzzers and property testers grow closer, however, fuzzers are starting to gain this kind of constraint-based searching capability as well.</p>
<p>If you’re curious about property-based test generation, I recommend starting with the <code>proptest</code> crate.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="box">
<h2>Testing Sequences of Operations</h2>
<p class="BoxBodyFirst">Since fuzzers and property testers allow you to generate arbitrary Rust types, you aren’t limited to testing a single function call in your crate. For example, say you want to test that some type <code>Foo</code> behaves correctly if you perform a particular sequence of operations on it. You could define an <code>enum Operation</code> that lists operations, and make your test function take a <code>Vec&lt;Operation&gt;</code>. Then you could instantiate a <code>Foo</code> and perform each operation on that <code>Foo</code>, one after the other. Most testers have support for minimizing inputs, so they will even search for the smallest sequence of operations that still violates a property if a property-violating input is found!</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<h3 id="h2-501850c07-0006"><span epub:type="pagebreak" title="96" id="Page_96"/>Test Augmentation</h3>
<p class="BodyFirst">Let’s say you have a magnificent test suite all set up, and your code passes all the tests. It’s glorious. But then, one day, one of the normally reliable tests inexplicably fails or crashes with a segmentation fault. There are two common reasons for these kinds of nondeterministic test failures: race conditions, where your test might fail only if two operations occur on different threads in a particular order, and undefined behavior in unsafe code, such as if some unsafe code reads a particular value out of uninitialized memory.</p>
<p>Catching these kinds of bugs with normal tests can be difficult—often you don’t have sufficient low-level control over thread scheduling, memory layout and content, or other random-ish system factors to write a reliable test. You could run each test many times in a loop, but even that may not catch the error if the bad case is sufficiently rare or unlikely. Luckily, there are tools that can help augment your tests to make catching these kinds of bugs much easier. </p>
<p>The first of these is the amazing tool <em>Miri</em>, an interpreter for Rust’s <em>mid-level intermediate representation (MIR)</em>. MIR<em> </em>is an internal, simplified representation of Rust that helps the compiler find optimizations and check properties without having to consider all of the syntax sugar of Rust itself. Running your tests through Miri is as simple as running <code>cargo miri test</code>. Miri <em>interprets</em> your code rather than compiling and running it like a normal binary, which makes the tests run a decent amount slower. But in return, Miri can keep track of the entire program state as each line of your code executes. This allows Miri to detect and report if your program ever exhibits certain types of undefined behavior, such as uninitialized memory reads, uses of values after they’ve been dropped, or out-of-bounds pointer accesses. Rather than having these operations yield strange program behaviors that may only sometimes result in observable test failures (like crashes), Miri detects them when they happen and tells you immediately. </p>
<p>For example, consider the very unsound code in <a href="#listing6-8" id="listinganchor6-8">Listing 6-8</a>, which creates two exclusive references to a value.</p>
<pre><code>let mut x = 42;
let x: *mut i32 = &amp;mut x;
let (x1, x2) = unsafe { (&amp;mut *x, &amp;mut *x) };
println!("{} {}", x1, x2);</code></pre>
<p class="CodeListingCaption"><a id="listing6-8">Listing 6-8</a>: Wildly unsafe code that Miri detects is incorrect</p>
<p>At the time of writing, if you run this code through Miri, you get an error that points out exactly what’s wrong:</p>
<pre><code>error: Undefined Behavior: trying to reborrow for Unique at alloc1383, but parent tag &lt;2772&gt; does not have an appropriate item in the borrow stack
 --&gt; src/main.rs:4:6
  |
4 | let (x1, x2) = unsafe { (&amp;mut *x, &amp;mut *x) };
  |      ^^ trying to reborrow for Unique at alloc1383, but parent tag &lt;2772&gt; does not have an appropriate item in the borrow stack</code></pre>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<span epub:type="pagebreak" title="97" id="Page_97"/><h2><span class="NoteHead">Note</span></h2>
<p>	Miri is still under development, and its error messages aren’t always the easiest to understand. This is a problem that’s being actively worked on, so by the time you read this, the error output may have already gotten much better!</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>Another tool worth looking at is <em>Loom</em>, a clever library that tries to ensure your tests are run with every relevant interleaving of concurrent operations. At a high level, Loom keeps track of all cross-thread synchronization points and runs your tests over and over, adjusting the order in which threads proceed from those synchronization points each time. So, if thread A and thread B both take the same <code>Mutex</code>, Loom will ensure that the test runs once with A taking it first and once with B taking it first. Loom also keeps track of atomic accesses, memory orderings, and accesses to <code>UnsafeCell</code> (which we’ll discuss in <span class="xref" itemid="xref_target_Chapter 9">Chapter 9</span>) and checks that threads do not access them inappropriately. If a test fails, Loom can give you an exact rundown of which threads executed in what order so you can determine how the crash happened.</p>
<h3 id="h2-501850c07-0007">Performance Testing</h3>
<p class="BodyFirst">Writing performance tests is difficult because it is often hard to accurately model a workload that reflects real-world usage of your crate. But having such tests is important; if your code suddenly runs 100 times slower, that really should be considered a bug, yet without a performance test you may not spot the regression. If your code runs 100 times <em>faster</em>, that might also indicate that something is off. Both of these are good reasons to have automated performance tests as part of your CI—if performance changes drastically in either direction, you should know about it.</p>
<p>Unlike with functional testing, performance tests do not have a common, well-defined output. A functional test will either succeed or fail, whereas a performance test may give you a throughput number, a latency profile, a number of processed samples, or any other metric that might be relevant to the application in question. Also, a performance test may require running a function in a loop a few hundred thousand times, or it might take hours running across a distributed network of multicore boxes. For that reason, it is difficult to speak about how to write performance tests in a general sense. Instead, in this section, we’ll look at some of the issues you may encounter when writing performance tests in Rust and how to mitigate them. Three particularly common pitfalls that are often overlooked are performance variance, compiler optimizations, and I/O overhead. Let’s explore each of these in turn.</p>
<h4 id="h3-501850c07-0005">Performance Variance</h4>
<p class="BodyFirst">Performance can vary for a huge variety of reasons, and many factors affect how fast a particular sequence of machine instructions run. Some are obvious, like the CPU and memory clock speed, or how loaded the machine otherwise is, but many are much more subtle. For example, your kernel version may change paging performance, the length of your username might change the <span epub:type="pagebreak" title="98" id="Page_98"/>layout of memory, and the temperature in the room might cause the CPU to clock down. Ultimately, it is highly unlikely that if you run a benchmark twice, you’ll get the same result. In fact, you may observe significant variance, even if you are using the same hardware. Or, viewed from another perspective, your code may have gotten slower or faster, but the effect may be invisible due to differences in the benchmarking environment.</p>
<p>There are no perfect ways to eliminate all variance in your performance results, unless you happen to be able to run benchmarks repeatedly on a highly diverse fleet of machines. Even so, it’s important to try to handle this measurement variance as best we can to extract a signal from the noisy measurements benchmarks give us. In practice, our best friend in combating variance is to run each benchmark many times and then look at the <em>distribution</em> of measurements rather than just a single one. Rust has tools that can help with this. For example, rather than ask “How long did this function take to run on average?” crates like <code>hdrhistogram</code> enable us to look at statistics like “What range of runtime covers 95% of the samples we observed?” To be even more rigorous, we can use techniques like null hypothesis testing from statistics to build some confidence that a measured difference indeed corresponds to a true change and is not just noise.</p>
<p>A lecture on statistical hypothesis testing is beyond the scope of this book, but luckily much of this work has already been done by others. The <code>criterion</code> crate, for instance, does all of this and more for you. All you have to do is give it a function that it can call to run one iteration of your benchmark, and it will run it the appropriate number of times to be fairly sure that the result is reliable. It then produces a benchmark report, which includes a summary of the results, analysis of outliers, and even graphical representations of trends over time. Of course, it can’t eliminate the effects of just testing on a particular configuration of hardware, but it at least categorizes the noise that is measurable across executions.</p>
<h4 id="h3-501850c07-0006">Compiler Optimizations</h4>
<p class="BodyFirst">Compilers these days are really clever. They eliminate dead code, compute complex expressions at compile time, unroll loops, and perform other dark magic to squeeze every drop of performance out of our code. Normally this is great, but when we’re trying to measure how fast a particular piece of code is, the compiler’s smartness can give us invalid results. For example, take the code to benchmark <code>Vec::push</code> in <a href="#listing6-9" id="listinganchor6-9">Listing 6-9</a>.</p>
<pre><code>let mut vs = Vec::with_capacity(4);
let start = std::time::Instant::now();
for i in 0..4 {
  vs.push(i);
}
println!("took {:?}", start.elapsed());</code></pre>
<p class="CodeListingCaption"><a id="listing6-9">Listing 6-9</a>: A suspiciously fast performance benchmark</p>
<p><span epub:type="pagebreak" title="99" id="Page_99"/>If you were to look at the assembly output of this code compiled in release mode using something like the excellent <em>godbolt.org </em>or <code>cargo-asm</code>, you’d immediately notice that something was wrong: the calls to <code>Vec::with_capacity</code> and <code>Vec::push</code>, and indeed the whole <code>for</code> loop, are nowhere to be seen. They have been optimized out completely. The compiler realized that nothing in the code actually required the vector operations to be performed and eliminated them as dead code. Of course, the compiler is completely within its rights to do so, but for benchmarking purposes, this is not particularly helpful.</p>
<p class="BodyFirst">To avoid these kinds of optimizations for benchmarking, the standard library provides <code>std::hint::black_box</code>. This function has been the topic of much debate and confusion and is still pending stabilization at the time of writing, but is so useful it’s worth discussing here nonetheless. At its core, it’s simply an identity function (one that takes <code>x</code> and returns <code>x</code>) that tells the compiler to assume that the argument to the function is used in arbitrary (legal) ways. It does not prevent the compiler from applying optimizations to the input argument, nor does it prevent the compiler from optimizing how the return value is used. Instead, it encourages the compiler to actually compute the argument to the function (under the assumption that it will be used) and to store that result somewhere accessible to the CPU such that <code>black_box</code> could be called with the computed value. The compiler is free to, say, compute the input argument at compile time, but it should still inject the result into the program.</p>
<p>This function is all we need for many, though admittedly not all, of our benchmarking needs. For example, we can annotate <a href="#listing6-9">Listing 6-9</a> so that the vector accesses are no longer optimized out, as shown in <a href="#listing6-10" id="listinganchor6-10">Listing 6-10</a>.</p>
<pre><code>let mut vs = Vec::with_capacity(4);
let start = std::time::Instant::now();
for i in 0..4 {
  black_box(vs.as_ptr());
  vs.push(i);<br/>  black_box(vs.as_ptr());
}
println!("took {:?}", start.elapsed());</code></pre>
<p class="CodeListingCaption"><a id="listing6-10">Listing 6-10</a>: A corrected version of <a href="#listing6-9">Listing 6-9</a></p>
<p>We’ve told the compiler to assume that <code>vs</code> is used in arbitrary ways on each iteration of the loop, both before and after the calls to <code>push</code>. This forces the compiler to perform each <code>push</code> in order, without merging or otherwise optimizing consecutive calls, since it has to assume that “arbitrary stuff that cannot be optimized out” (that’s the <code>black_box</code> part) may happen to <code>vs</code> between each call.</p>
<p>Note that we used <code>vs.as_ptr()</code> and not, say, <code>&amp;vs</code>. That’s because of the caveat that the compiler should assume <code>black_box</code> can perform any <em>legal</em> operation on its argument. It is not legal to mutate the <code>Vec</code> through a shared reference, so if we used <code>black_box(&amp;vs)</code>, the compiler might notice that <code>vs</code> will not change between iterations of the loop and implement optimizations based on that observation!</p>
<h4 id="h3-501850c07-0007"><span epub:type="pagebreak" title="100" id="Page_100"/>I/O Overhead Measurement</h4>
<p class="BodyFirst">When writing benchmarks, it’s easy to accidentally measure the wrong thing. For example, we often want to get information in real time about how far along the benchmark is. To do that, we might write code like that in <a href="#listing6-11" id="listinganchor6-11">Listing 6-11</a>, intended to measure how fast <code>my_function</code> runs:</p>
<pre><code>let start = std::time::Instant::now();
for i in 0..1_000_000 {
  println!("iteration {}", i);
  my_function();
}
println!("took {:?}", start.elapsed());</code></pre>
<p class="CodeListingCaption"><a id="listing6-11">Listing 6-11</a>: What are we really benchmarking here?</p>
<p>This may look like it achieves the goal, but in reality, it does not actually measure how fast <code>my_function</code> is. Instead, this loop is most likely to tell us how long it takes to print a million numbers. The <code>println!</code> in the body of the loop does a lot of work behind the scenes: it turns a binary integer into decimal digits for printing, locks standard output, writes out a sequence of UTF-8 code points using at least one system call, and then releases the standard output lock. Not only that, but the system call might block if your terminal is slow to print out the input it receives. That’s a lot of cycles! And the time it takes to call <code>my_function</code> might pale in comparison.</p>
<p>A similar thing happens when your benchmark uses random numbers. If you run <code>my_function(rand::random())</code> in a loop, you may well be mostly measuring the time it takes to generate a million random numbers. The story is the same for getting the current time, reading a configuration file, or starting a new thread—these things all take a long time, relatively speaking, and may end up overshadowing the time you actually wanted to measure.</p>
<p>Luckily, this particular issue is often easy to work around once you are aware of it. Make sure that the body of your benchmarking loop contains almost nothing but the particular code you want to measure. All other code should run either before the benchmark begins or outside of the measured part of the benchmark. If you’re using <code>criterion</code>, take a look at the different timing loops it provides—they’re all there to cater to benchmarking cases that require different measurement strategies!</p>
<h2 id="h1-501850c07-0003">Summary</h2>
<p class="BodyFirst">In this chapter, we explored the built-in testing capabilities that Rust offers in great detail. We also looked at a number of testing facilities and techniques that are useful when testing Rust code. This is the last chapter that focuses on higher-level aspects of intermediate Rust use in this book. Starting with the next chapter on declarative and procedural macros, we will be focusing much more on Rust code. See you on the next page!</p>
</section>
</div></body></html>