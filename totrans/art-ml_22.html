<html><head></head><body>
<div id="sbo-rt-content" class="calibre1"><h2 class="h1" id="appd"><span epub:type="pagebreak" id="page_229" class="calibre2"/><strong class="calibre3"><span class="big">D</span><br class="calibre18"/>PITFALL: BEWARE OF “P-HACKING”!</strong></h2>
<p class="noindent">In recent years there has been much concern over something that has acquired the name <em class="calibre13">p-hacking</em>. Though such issues have always been known and discussed, things really came to a head with the publication of John Ioannidis’s highly provocatively titled paper, “Why Most Published Research Findings Are False” (<em class="calibre13">PLOS Medicine</em>, August 30, 2005). One aspect of this controversy can be described as follows.</p>
<p class="indent">Say we have 250 coins, and we suspect that some are unbalanced. (Any coin is unbalanced to at least some degree, but let’s put that aside.) We toss each coin 100 times, and if a coin yields fewer than 40 or more than 60 heads, we will decide that it’s unbalanced. For those who know some statistics, this range was chosen so that a balanced coin would have only a 5 percent chance of straying more than 10 heads away from 50 out of 100. So, while this chance is only 5 percent for each particular coin, with 250 coins, the chances are high that at least one of them falls outside that [40,60] range, <em class="calibre13">even if none of the coins is unbalanced</em>. We will falsely declare some coins unbalanced. In reality, it was just a random accident that those coins look unbalanced.<span epub:type="pagebreak" id="page_230"/></p>
<p class="indent">Or, to give a somewhat frivolous example that still will make the point, say we are investigating whether there is any genetic component to sense of humor. Is there a humor gene? There are many, many genes to consider— many more than 250, actually. Testing each one for relation to sense of humor is like checking each coin for being unbalanced: even if there is no humor gene, eventually just by accident we’ll stumble upon one that seems to be related to humor.</p>
<p class="indent">In a complex scientific study, the analyst is testing many genes, or many risk factors for cancer, or many exoplanets for the possibility of life, or many economic inflation factors, and so on. The term <em class="calibre13">p-hacking</em> means that the analyst looks at so many different factors that one is likely to emerge as “statistically significant” even if no factor has any true impact. A common joke is that the analyst “beats the data until they confess,” alluding to a researcher testing so many factors that one finally comes out “significant.”</p>
<p class="indent">Cassie Kozyrkov, head of decision intelligence at Google, said it quite well:</p>
<p class="blockquote">What the mind does with inkblots, it also does with data. Complex datasets practically beg you to find false meaning in them.</p>
<p class="noindent"><em class="calibre13">This has major implications for ML analysis.</em> For instance, a popular thing in the ML community is to have competitions in which many analysts try their own tweaks on ML methods to outdo each other on a certain dataset. Typically these are classification problems, and “winning” means getting the lowest rate of misclassification.</p>
<p class="indent">The trouble is, having 250 ML analysts attacking the same dataset is like having 250 coins in our example above. Even if the 250 methods they try are all equally effective, one of them will emerge by accident as the victor, and it will be annointed as a “technological advance.”</p>
<p class="indent">Of course, it may well be that one of the 250 methods really is superior. But without careful statistical analysis of the 250 data points, it is not clear what’s real and what’s just accident. Note, too, that even if one of the 250 methods is in fact superior, there is a high probability that it won’t be the winner in the competition, again due to random variation.</p>
<p class="indent">The problem is exacerbated by the fact that a contestant will probably not even submit his entry if it appears unlikely to set a new record. This further biases the results.</p>
<p class="indent">As mentioned, this concept is second nature to statisticians, but it is seldom mentioned in ML circles. An exception is the blog post “AI Competitions Don’t Produce Useful Models” by Lauren Oakden-Rayner, whose excellent graphic is reproduced in <a href="app04.xhtml#appdfig1" class="calibre12">Figure D-1</a> with Dr. Oakden-Rayner’s permission.<sup class="calibre11"><a id="appdfn1b" class="calibre12"/><a href="footnote.xhtml#appdfn1" class="calibre12">1</a></sup><span epub:type="pagebreak" id="page_231"/></p>
<div class="image"><img alt="Image" id="appdfig01" src="../images/app04fig01.jpg" class="calibre110"/></div>
<p class="figcap" id="appdfig1"><em class="calibre13">Figure D-1: AI p-hacking</em></p>
<p class="indent">Rayner uses a simple statistical power analysis to analyze ImageNet, a contest in ML image classification. He reckons that at least those “new records” starting in 2014 are overfitting, or just noise. With more sophisticated statistical tools, a more refined analysis could be done, but the principle is clear.</p>
<p class="indent">This also has a big implication for the setting of tuning parameters. Let’s say we have four tuning parameters in an ML method, and we try 10 values of each. That’s 10<sup class="calibre11">4</sup> = 10000 possible combinations, a lot more than 250! So again, what seems to be the “best” setting for the tuning parameters may be illusory.</p>
<p class="indent">The <span class="literal">regtools</span> function <span class="literal">fineTuning()</span> takes steps to counter the possibility of p-hacking in searches for the best tuning parameter combination.<span epub:type="pagebreak" id="page_232"/></p>
</div></body></html>