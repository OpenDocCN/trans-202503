- en: '11'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**STORAGE AND THE MEMORY HIERARCHY**'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Although designing and implementing efficient algorithms is typically the *most*
    critical aspect of writing programs that perform well, there’s another, often
    overlooked factor that can have a major impact on performance: memory. Perhaps
    surprisingly, two algorithms with the same asymptotic performance (number of steps
    in the worst case) run on the same inputs might perform very differently in practice
    due to the organization of the hardware on which they execute. Such differences
    generally stem from the algorithms’ memory accesses, particularly where they store
    data and the kinds of patterns they exhibit when accessing it. These patterns
    are referred to as *memory locality*, and to achieve the best performance, a program’s
    access patterns need to align well with the hardware’s memory arrangement.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider the following two variations of a function for averaging
    the values in an *N* × *N* matrix. Despite both versions accessing the same memory
    locations an equal number of times (*N*²), Variation 1 executes about five times
    faster on real systems than Variation 2\. The difference arises from the patterns
    in which they access those memory locations. Toward the end of this chapter we
    analyze this example using the memory profiling tool *Cachegrind*.
  prefs: []
  type: TYPE_NORMAL
- en: Variation 1
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Variation 2
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Storage locations like registers, CPU caches, main memory, and files on disk
    all feature remarkably different access times, transfer rates, and storage capacities.
    When programming a high-performance application, it’s important to consider where
    data is stored and how frequently the program accesses each device’s data. For
    example, accessing a slow disk once as the program starts is rarely a major concern.
    On the other hand, accessing the disk frequently will slow down the program considerably.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter characterizes a diverse set of memory devices and describes how
    they’re organized in a modern PC. With that context, we’ll see how a collection
    of varied memory devices can be combined to exploit the locality found in a typical
    program’s memory access patterns.
  prefs: []
  type: TYPE_NORMAL
- en: 11.1 The Memory Hierarchy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we explore modern computer storage, a common pattern emerges: devices with
    higher capacities offer lower performance. Said another way, systems use devices
    that are fast and devices that store a large amount of data, but no single device
    does both. This trade-off between performance and capacity is known as the *memory
    hierarchy*, and [Figure 11-1](ch11.xhtml#ch11fig1) depicts the hierarchy visually.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Storage devices similarly trade cost and storage density: faster devices are
    more expensive, both in terms of bytes per dollar and operational costs (e.g.,
    energy usage). Consider that even though caches provide great performance, the
    cost (and manufacturing challenges) of building a CPU with a large enough cache
    to forego main memory makes such a design infeasible. Practical systems must utilize
    a combination of devices to meet the performance and capacity requirements of
    programs, and a typical system today incorporates most, if not all, of the devices
    described in [Figure 11-1](ch11.xhtml#ch11fig1).'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/11fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-1: The memory hierarchy*'
  prefs: []
  type: TYPE_NORMAL
- en: The reality of the memory hierarchy is unfortunate for programmers, who would
    prefer to not worry about the performance implications of where their data resides.
    For example, when declaring an integer *in most applications*, a programmer ideally
    wouldn’t need to agonize over the differences between data stored in a cache or
    main memory. Requiring a programmer to micromanage which type of memory each variable
    occupies would be burdensome, although it may occasionally be worth the effort
    for certain small, performance-critical sections of code.
  prefs: []
  type: TYPE_NORMAL
- en: Note that [Figure 11-1](ch11.xhtml#ch11fig1) categorizes *cache* as single entity,
    but most systems contain multiple levels of caches that form their own smaller
    hierarchy. For example, CPUs commonly incorporate a very small and fast *level
    one* (L1) cache, which sits relatively close to the ALU, and a larger and slower
    *level two* (L2) cache that resides farther away. Many multicore CPUs also share
    data between cores in a larger *level three* (L3) cache. Although the differences
    between the cache levels may matter to performance-conscious applications, this
    book considers just a single level of caching for simplicity.
  prefs: []
  type: TYPE_NORMAL
- en: Though this chapter primarily focuses on data movement between registers, CPU
    caches, and main memory, the next section characterizes common storage devices
    across the memory hierarchy. We examine disks and their role in the bigger picture
    of memory management later, in “Virtual Memory” on [page 639](ch13.xhtml#lev1_101).
  prefs: []
  type: TYPE_NORMAL
- en: 11.2 Storage Devices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Systems designers classify devices in the memory hierarchy according to how
    programs access their data. *Primary storage* devices can be accessed directly
    by a program on the CPU. That is, the CPU’s assembly instructions encode the exact
    location of the data that the instructions should retrieve. Examples of primary
    storage include CPU registers and main memory (RAM), which assembly instructions
    reference directly (e.g., in IA32 assembly as `%reg` and `(%reg)`, respectively).
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, CPU instructions cannot directly refer to *secondary storage* devices.
    To access the contents of a secondary storage device, a program must first request
    that the device copy its data into primary storage (typically memory). The most
    familiar types of secondary storage devices are disk devices (e.g., hard disk
    drives and solid-state drives), which persistently store file data. Other examples
    include floppy disks, magnetic tape cartridges, or even remote file servers.
  prefs: []
  type: TYPE_NORMAL
- en: Even though you may not have considered the distinction between primary and
    secondary storage in these terms before, it’s likely that you have encountered
    their differences in programs already. For example, after declaring and assigning
    ordinary variables (primary storage), a program can immediately use them in arithmetic
    operations. When working with file data (secondary storage), the program must
    read values from the file into memory variables before it can access them (see
    “File Input/Output” on [page 117](ch02.xhtml#lev2_33)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Several other important criteria for classifying memory devices arise from
    their performance and capacity characteristics. The three most interesting measures
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Capacity**  The amount of data a device can store. Capacity is typically
    measured in bytes.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Latency**  The amount of time it takes for a device to respond with data
    after it has been instructed to perform a data retrieval operation. Latency is
    typically measured in either fractions of a second (e.g., milliseconds or nanoseconds)
    or CPU cycles.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Transfer rate**  The amount of data that can be moved between the device
    and main memory over some interval of time. Transfer rate is also known as *throughput*
    and is typically measured in bytes per second.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exploring the variety of devices in a modern computer reveals a huge disparity
    in device performance across all three of these measures. The performance variance
    primarily arises from two factors: *distance* and *variations in the technologies*
    used to implement the devices.'
  prefs: []
  type: TYPE_NORMAL
- en: Distance contributes because, ultimately, any data that a program wants to use
    must be available to the CPU’s arithmetic components (e.g., the ALU) for processing.
    CPU designers place registers close to the ALU to minimize the time it takes for
    a signal to propagate between the two. Thus, while registers can store only a
    few bytes and there aren’t many of them, the values stored are available to the
    ALU almost immediately! In contrast, secondary storage devices like disks transfer
    data to memory through various controller devices that are connected by longer
    wires. The extra distance and intermediate processing slows down secondary storage
    considerably.
  prefs: []
  type: TYPE_NORMAL
- en: GRACE HOPPER’S “NANOSECONDS”
  prefs: []
  type: TYPE_NORMAL
- en: When speaking to an audience, computing pioneer and US Navy Admiral Grace Hopper
    frequently handed out 11.8-inch strands of wire to audience members. These strands
    represented the maximum distance that an electrical signal travels in one nanosecond
    and were called “Grace Hopper nano-seconds.” She used them to describe the latency
    limitations of satellite communication and to demonstrate why computing devices
    need to be small in order to be fast. Recordings of Grace Hopper presenting her
    nanoseconds are available on YouTube.^([1](ch11.xhtml#fn11_1))
  prefs: []
  type: TYPE_NORMAL
- en: The underlying technology also significantly affects device performance. Registers
    and caches are built from relatively simple circuits, consisting of just a few
    logic gates. Their small size and minimal complexity ensures that electrical signals
    can propagate through them quickly, reducing their latencies. On the opposite
    end of the spectrum, traditional hard disks contain spinning magnetic platters
    that store hundreds of gigabytes. Although they offer dense storage, their access
    latency is relatively high due to the requirements of mechanically aligning and
    rotating components into the correct positions.
  prefs: []
  type: TYPE_NORMAL
- en: The remainder of this section examines the details of primary and secondary
    storage devices and analyzes their performance characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2.1 Primary Storage
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Primary storage devices consist of *random access memory* (RAM), which means
    the time it takes to access data is not affected by the data’s location in the
    device. That is, RAM doesn’t need to worry about things like moving parts into
    the correct position or rewinding tape spools. There are two widely used types
    of RAM, *static RAM* (SRAM) and *dynamic RAM* (DRAM), and both play an important
    role in modern computers. [Table 11-1](ch11.xhtml#ch11tab1) characterizes the
    performance measures of common primary storage devices and the types of RAM they
    use.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 11-1:** Primary Storage Device Characteristics of a Typical 2022 Workstation'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Device** | **Capacity** | **Approx. Latency** | **RAM Type** |'
  prefs: []
  type: TYPE_TB
- en: '| Register | 4–8 bytes | < 1 ns | SRAM |'
  prefs: []
  type: TYPE_TB
- en: '| CPU cache | 1–32 megabytes | 5 ns | SRAM |'
  prefs: []
  type: TYPE_TB
- en: '| Main memory | 4–64 gigabytes | 100 ns | DRAM |'
  prefs: []
  type: TYPE_TB
- en: SRAM stores data in small electrical circuits (e.g., latches—see “RS Latch”
    on [page 257](ch05.xhtml#lev3_49)). SRAM is typically the fastest type of memory,
    and designers integrate it directly into a CPU to build registers and caches.
    SRAM is relatively expensive in its cost to build, cost to operate (e.g., power
    consumption), and in the amount of space it occupies. Collectively, those costs
    limit the amount of SRAM storage that a CPU can include.
  prefs: []
  type: TYPE_NORMAL
- en: DRAM stores data using electrical components called *capacitors* that hold an
    electrical charge. It’s called “dynamic” because a DRAM system must frequently
    refresh the charge of its capacitors to maintain a stored value. Modern systems
    use DRAM to implement main memory on modules that connect to the CPU via a high-speed
    interconnect called the *memory bus*.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 11-2](ch11.xhtml#ch11fig2) illustrates the positions of primary storage
    devices relative to the memory bus. To retrieve a value from memory, the CPU puts
    the address of the data it would like to retrieve on the memory bus and signals
    that the memory modules should perform a read. After a short delay, the memory
    module sends the value stored at the requested address across the bus to the CPU.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/11fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-2: Primary storage and memory bus architecture*'
  prefs: []
  type: TYPE_NORMAL
- en: Even though the CPU and main memory are physically just a few inches away from
    each other, data must travel through the memory bus when it moves between the
    CPU and main memory. The extra distance and circuitry between them increases the
    latency and reduces the transfer rate of main memory relative to on-CPU storage.
    As a result, the memory bus is sometimes referred to as the *von Neumann bottleneck*.
    Of course, despite its lower performance, main memory remains an essential component
    because it stores several orders of magnitude more data than can fit on the CPU.
    Consistent with other forms of storage, there’s a clear trade-off between capacity
    and speed.
  prefs: []
  type: TYPE_NORMAL
- en: '*CPU cache* (pronounced “cash”) occupies the middle ground between registers
    and main memory, both physically and in terms of its performance and capacity
    characteristics. A CPU cache typically stores a few kilobytes to megabytes of
    data directly on the CPU, but physically, caches are not quite as close to the
    ALU as registers. Thus, caches are faster to access than main memory, but they
    require a few more cycles than registers to make data available for computation.'
  prefs: []
  type: TYPE_NORMAL
- en: Rather than the programmer explicitly loading values into the cache, control
    circuitry within the CPU automatically stores a subset of the main memory’s contents
    in the cache. CPUs strategically control which subset of main memory they store
    in caches so that as many memory requests as possible can be serviced by the (much
    higher performance) cache. Later sections of this chapter describe the design
    decisions that go into cache construction and the algorithms that should govern
    which data they store.
  prefs: []
  type: TYPE_NORMAL
- en: Real systems incorporate multiple levels of caches that behave like their own
    miniature version of the memory hierarchy. That is, a CPU might have a very small
    and fast *L1 cache* that stores a subset of a slightly larger and slower *L2 cache*,
    which in turns stores a subset of a larger and slower *L3 cache*. The remainder
    of this section describes a system with just a single cache, but the interaction
    between caches on a real system behaves much like the interaction between a single
    cache and main memory detailed later.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note**'
  prefs: []
  type: TYPE_NORMAL
- en: If you’re curious about the sizes of the caches and main memory on your system,
    the `lscpu` command prints information about the CPU (including its cache capacities).
    Running `free -m` shows the system’s main memory capacity in megabytes.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2.2 Secondary Storage
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Physically, secondary storage devices connect to a system even farther away
    from the CPU than main memory. Compared to most other computer equipment, secondary
    storage devices have evolved dramatically over the years, and they continue to
    exhibit more diverse designs than other components. The iconic punch card^([2](ch11.xhtml#fn11_2))
    allowed a human operator to store data by making small holes in a thick piece
    of paper, similar to an index card. Punch cards, whose design dates back to the
    US census of 1890, faithfully stored user data (often programs) through the 1960s
    and into the 1970s.
  prefs: []
  type: TYPE_NORMAL
- en: A tape drive^([3](ch11.xhtml#fn11_3)) stores data on a spool of magnetic tape.
    Although they generally offer good storage density (lots of information in a small
    size) for a low cost, tape drives are slow to access because they must wind the
    spool to the correct location. Although most computer users don’t encounter them
    often anymore, tape drives are still frequently used for bulk storage operations
    (e.g., large data backups) in which reading the data back is expected to be rare.
    Modern tape drives arrange the magnetic tape spool into small cartridges for ease
    of use.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/11fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-3: Example photos of (a) a punch card, (b) a magnetic tape spool,
    and (c) a variety of floppy disk sizes. Images from Wikipedia.*'
  prefs: []
  type: TYPE_NORMAL
- en: Removable media like floppy disks^([4](ch11.xhtml#fn11_4)) and optical discs^([5](ch11.xhtml#fn11_5))
    are another popular form of secondary storage. Floppy disks contain a spindle
    of magnetic recording media that rotates over a disk head that reads and writes
    its contents. [Figure 11-3](ch11.xhtml#ch11fig3) shows photos of a punch card,
    a tape drive, and a floppy disk. Optical discs (e.g., CD, DVD, and Blu-ray) store
    information via small indentations on the disc. The drive reads a disc by shining
    a laser at it, and the presence or absence of indentations causes the beam to
    reflect (or not), encoding zeros and ones.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2.2.1 Modern Secondary Storage
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '**Table 11-2:** Secondary Storage Device Characteristics of a Typical 2022
    Workstation'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Device** | **Capacity** | **Latency** | **Transfer Rate** |'
  prefs: []
  type: TYPE_TB
- en: '| Flash disk | 0.5–2 terabytes | 0.1–1 ms | 200–3,000 megabytes/second |'
  prefs: []
  type: TYPE_TB
- en: '| Traditional hard disk | 0.5–10 terabytes | 5–10 ms | 100–200 megabytes/second
    |'
  prefs: []
  type: TYPE_TB
- en: '| Remote network server | Varies considerably | 20–200 ms | Varies considerably
    |'
  prefs: []
  type: TYPE_TB
- en: '[Table 11-2](ch11.xhtml#ch11tab2) characterizes the secondary storage devices
    commonly available to workstations today. [Figure 11-4](ch11.xhtml#ch11fig4) displays
    how the path from secondary storage to main memory generally passes through several
    intermediate device controllers. For example, a typical hard disk connects to
    a Serial ATA controller, which connects to the system I/O controller, which in
    turn connects to the memory bus. These intermediate devices make disks easier
    to use by abstracting the disk communication details from the OS and programmer.
    However, they also introduce transfer delays as data flows through the additional
    devices.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/11fig04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-4: Secondary storage and I/O bus architecture*'
  prefs: []
  type: TYPE_NORMAL
- en: The two most common secondary storage devices today are *hard disk drives* (HDDs)
    and flash-based *solid-state drives* (SSDs). A hard disk consists of a few flat,
    circular platters made from a material that allows for magnetic recording. The
    platters rotate quickly, typically at speeds between 5,000 and 15,000 revolutions
    per minute. As the platters spin, a small mechanical arm with a disk head at the
    tip moves across the platter to read or write data on concentric tracks (regions
    of the platter located at the same diameter).
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 11-5](ch11.xhtml#ch11fig5) illustrates the major components of a hard
    disk.^([6](ch11.xhtml#fn11_6)) Before accessing data, the disk must align the
    disk head with the track that contains the desired data. Alignment requires extending
    or retracting the arm until the head sits above the track. Moving the disk arm
    is called *seeking*, and because it requires mechanical motion, seeking introduces
    a small *seek time* delay to accessing data (a few milliseconds). When the arm
    is in the correct position, the disk must wait for the platter to rotate until
    the disk head is directly above the location that stores the desired data. This
    introduces another short delay (a few more milliseconds) known as *rotational
    latency*. Thus, due to their mechanical characteristics, hard disks exhibit significantly
    higher access latencies than the primary storage devices described earlier.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/11fig05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-5: The major components of a hard disk drive*'
  prefs: []
  type: TYPE_NORMAL
- en: In the past few years, SSDs, which have no moving parts (and thus lower latency),
    have quickly risen to prominence. They are known as solid-state drives because
    they don’t rely on mechanical movement. Although several solid-state technologies
    exist, flash memory^([7](ch11.xhtml#fn11_7)) reigns supreme in commercial SSD
    devices. The technical details of flash memory are beyond the scope of this book,
    but it suffices to say that flash-based devices allow for reading, writing, and
    erasing data at speeds faster than traditional hard disks. Though they don’t yet
    store data as densely as their mechanical counterparts, they’ve largely replaced
    spinning disks in most consumer devices like laptops.
  prefs: []
  type: TYPE_NORMAL
- en: 11.3 Locality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Because memory devices vary considerably in their performance characteristics
    and storage capacities, modern systems integrate several forms of storage. Luckily,
    most programs exhibit common memory access patterns, known as *locality*, and
    designers build hardware that exploits good locality to automatically move data
    into an appropriate storage location. Specifically, a system improves performance
    by moving the subset of data that a program is actively using into storage that
    lives close to the CPU’s computation circuitry (e.g., in a register or CPU cache).
    As necessary data moves up the hierarchy toward the CPU, unused data moves farther
    away to slower storage until the program needs it.
  prefs: []
  type: TYPE_NORMAL
- en: To a system designer, building a system that exploits locality represents an
    abstraction problem. The system provides an abstract view of memory devices such
    that it appears to programmers as if they have the sum of all memory capacities
    with the performance characteristics of fast on-chip storage. Of course, providing
    this rosy illusion to users can’t be accomplished perfectly, but by exploiting
    program locality, modern systems achieve good performance for most well-written
    programs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Systems primarily exploit two forms of locality:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Temporal locality**  Programs tend to access the same data repeatedly over
    time. That is, if a program has used a variable recently, it’s likely to use that
    variable again soon.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Spatial locality**  Programs tend to access data that is nearby other, previously
    accessed data. “Nearby” here refers to the data’s memory address. For example,
    if a program accesses data at addresses *N* and *N* + 4, it’s likely to access
    *N* + 8 soon.'
  prefs: []
  type: TYPE_NORMAL
- en: 11.3.1 Locality Examples in Code
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Fortunately, common programming patterns exhibit both forms of locality quite
    frequently. Take the following function, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In this code, the repetitive nature of the `for` loop introduces temporal locality
    for `i`, `len`, `sum`, and `array` (the base address of the array), as the program
    accesses each of these variables within every loop iteration. Exploiting this
    temporal locality allows a system to load each variable from main memory into
    the CPU cache only once. Every subsequent access can be serviced out of the significantly
    faster cache.
  prefs: []
  type: TYPE_NORMAL
- en: Accesses to the array’s contents also benefit from spatial locality. Even though
    the program accesses each array element only once, a modern system loads more
    than one `int` at a time from memory to the CPU cache. That is, accessing the
    first array index fills the cache with not only the first integer, but also the
    next few integers after it, too. Exactly *how many* additional integers get moved
    into the cache depends on the cache’s *block size*—the amount of data transferred
    into the cache at once.
  prefs: []
  type: TYPE_NORMAL
- en: For example, with a 16-byte block size, a system copies four integers from memory
    to the cache at a time. Thus, accessing the first integer incurs the relatively
    high cost of accessing main memory, but the accesses to the next three are served
    out of cache, even if the program has never accessed them previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'In many cases, a programmer can help a system by intentionally writing code
    that exhibits good locality patterns. For example, consider the nested loops that
    access every element of an *N* × *N* matrix (this same example appeared in this
    chapter’s introduction):'
  prefs: []
  type: TYPE_NORMAL
- en: Version 1
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Version 2
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In both versions, the loop variables (`i` and `j`) and the accumulator variable
    (`total`) exhibit good temporal locality because the loops repeatedly use them
    in every iteration. Thus, when executing this code, a system would store those
    variables in fast on-CPU storage locations to provide good performance.
  prefs: []
  type: TYPE_NORMAL
- en: However, due to the row-major order organization of a matrix in memory (see
    “Two-Dimensional Array Memory Layout” on [page 86](ch02.xhtml#lev3_13)), the first
    version of the code executes about five times faster than the second version.
    The disparity arises from the difference in spatial locality—the first version
    accesses the matrix’s values sequentially in memory (i.e., in order of consecutive
    memory addresses). Thus, it benefits from a system that loads large blocks from
    memory into the cache because it pays the cost of going to memory only once for
    every block of values.
  prefs: []
  type: TYPE_NORMAL
- en: The second version accesses the matrix’s values by repeatedly jumping between
    rows across nonsequential memory addresses. It *never* reads from the same cache
    block in subsequent memory accesses, so it looks to the cache like the block isn’t
    needed. Thus, it pays the cost of going to memory for every matrix value it reads.
  prefs: []
  type: TYPE_NORMAL
- en: This example illustrates how a programmer can affect the system-level costs
    of a program’s execution. Keep these principles in mind when writing high-performance
    applications, particularly those that access arrays in a regular pattern.
  prefs: []
  type: TYPE_NORMAL
- en: 11.3.2 From Locality to Caches
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To help illustrate how the concepts of temporal and spatial locality enable
    cache designs, we’ll adopt an example scenario with familiar real-world objects:
    books. Suppose that Fiona does all of her homework at a desk in her dorm room,
    and the desk has a small amount of space that can store only three books. Just
    outside her room she keeps a bookshelf, which has much more space than the desk.
    Finally, across campus her college has a library with a huge variety of books.
    The “book storage hierarchy” in this example might look something like [Figure
    11-6](ch11.xhtml#ch11fig6). Given this scenario, we’ll explore how locality can
    help guide which storage location Fiona should use to store her books.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/11fig06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-6: A hypothetical book storage hierarchy*'
  prefs: []
  type: TYPE_NORMAL
- en: 11.3.3 Temporal Locality
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Temporal locality suggests that, if there’s a book Fiona uses frequently, she
    should keep it as close to her desk as possible. If she occasionally needs to
    move it to the shelf to clear up temporary work space, the cost isn’t too high,
    but it would be silly to take a book back to the library if she’s just going to
    need it again the next day. The inverse is also true: if there’s a book taking
    up valuable space on her desk or shelf, and she hasn’t used it for quite a while,
    that book seems like a good candidate for returning to the library.'
  prefs: []
  type: TYPE_NORMAL
- en: So, which books should Fiona move to her precious desk space? In this example,
    real students would probably look at their upcoming assignments and select the
    books that they expect to be most useful. In other words, to make the best storage
    decision, they would ideally need information about *future usage*.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, hardware designers haven’t discovered how to build circuits that
    can predict the future. As an alternative to prediction, one could instead imagine
    a system that asks the programmer or user to inform the system in advance how
    a program will use data so that it’s placement could be optimized. Such a strategy
    may work well in specialized applications (e.g., large databases) that exhibit
    *very* regular access patterns. However, in a general-purpose system like a personal
    computer, requiring advance notice from the user is too large a burden—many users
    would not want to (or would be unable to) provide enough detail to help the system
    make good decisions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, instead of relying on future access information, systems look to the
    past as a predictor of what will *likely* happen in the future. Applying this
    idea to the book example suggests a relatively simple (but still quite effective)
    strategy for governing book storage spaces:'
  prefs: []
  type: TYPE_NORMAL
- en: When Fiona needs to use a book, she retrieves it from wherever it currently
    is and moves it to her desk.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the desk is already full, she moves the book that she used *least recently*
    (that is, the book that has been sitting on the desk untouched for the longest
    amount of time) to her shelf.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the shelf is full, she returns the shelf’s least recently used book to the
    library to free up space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Even though this scheme may not be perfect, the simplicity makes it attractive.
    All it requires is the ability to move books between storage locations and a small
    amount of metainformation regarding the order in which books were previously used.
    Furthermore, this scheme captures the two initial temporal locality objectives
    well:'
  prefs: []
  type: TYPE_NORMAL
- en: Frequently used books are likely to remain on the desk or shelf, preventing
    unnecessary trips to the library.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Infrequently used books eventually become the least recently used book, at which
    point returning them to the library makes sense.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Applying this strategy to primary storage devices looks remarkably similar
    to the book example: as data is loaded into CPU registers from main memory, make
    room for it in the CPU cache. If the cache is already full, make room in the cache
    by *evicting* the least recently used cache data to main memory. In the following
    caching section, we’ll explore the details of how such mechanisms are built in
    to modern caching systems.'
  prefs: []
  type: TYPE_NORMAL
- en: 11.3.4 Spatial Locality
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Spatial locality suggests that, when making a trip to the library, Fiona should
    retrieve more than one book to reduce the likelihood of future library trips.
    Specifically, she should retrieve additional books that are “nearby” the one she
    needs because those that are nearby seem like good candidates for books that might
    otherwise turn into additional library visits.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that she’s taking a literature course on the topic of Shakespeare’s
    histories. If in the first week of the course she’s assigned to read *Henry VI,
    Part I*, when she finds herself in the library to retrieve it, she’s likely to
    also find Parts II and III close by on the shelves. Even if she doesn’t yet know
    whether the course will assign those other two parts, it’s not unreasonable to
    think that she *might* need them. That is, the likelihood of needing them is much
    higher than a random book in the library, specifically because they are nearby
    the book she does need.
  prefs: []
  type: TYPE_NORMAL
- en: In this scenario, the likelihood increases due to the way libraries arrange
    books on shelves, and programs similarly organize data in memory. For example,
    a programming construct like an array or a `struct` stores a collection of related
    data in a contiguous region of memory. When iterating over consecutive elements
    in an array, there is clearly a spatial pattern in the accessed memory addresses.
    Applying these spatial locality lessons to primary storage devices implies that,
    when retrieving data from main memory, the system should also retrieve the data
    immediately surrounding it.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll characterize cache characteristics and describe mechanisms
    for the hardware to make identifying and exploiting locality happen automatically.
  prefs: []
  type: TYPE_NORMAL
- en: 11.4 CPU Caches
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Having characterized storage devices and recognized the important patterns
    of temporal and spatial locality, we’re ready to explore how CPU caches are designed
    and implemented. A *cache* is a small, fast storage device on a CPU that holds
    limited subsets of main memory. Caches face several important design questions:
    *Which* subsets of a program’s memory should the cache hold? *When* should the
    cache copy a subset of a program’s data from main memory to the cache, or vice
    versa? *How* can a system determine whether a program’s data is present in the
    cache?'
  prefs: []
  type: TYPE_NORMAL
- en: Before exploring these challenging questions, we need to introduce some cache
    behavior and terminology. Recall that when accessing data in memory, a program
    first computes the data’s memory address (see “Instruction Structure” on [page
    298](ch07.xhtml#lev2_116)). Ideally, the data at the desired address already resides
    in the cache, allowing the program to skip accessing main memory altogether. To
    maximize performance, the hardware simultaneously sends the desired address to
    *both* the cache and main memory. Because the cache is faster and closer to the
    ALU, the cache responds much more quickly than memory. If the data is present
    in the cache (a *cache hit*), the cache hardware cancels the pending memory access
    because the cache can serve the data faster than memory.
  prefs: []
  type: TYPE_NORMAL
- en: Otherwise, if the data isn’t in the cache (a *cache miss*), the CPU has no choice
    but to wait for memory to retrieve it. Critically though, when the request to
    main memory completes, the CPU loads the retrieved data into the cache so that
    subsequent requests for the same address (which are likely thanks to temporal
    locality) can be serviced quickly from the cache. Even if the memory access that
    misses is *writing* to memory, the CPU still loads the value into the cache on
    a miss because it’s likely that the program will attempt to access the same location
    again in the future.
  prefs: []
  type: TYPE_NORMAL
- en: When loading data into a cache after a miss, a CPU often finds that the cache
    doesn’t have enough free space available. In such cases, the cache must first
    *evict* some resident data to make room for the new data that it’s loading in.
    Because a cache stores subsets of data copied from main memory, evicting cached
    data that has been modified requires the cache to update the contents of main
    memory prior to evicting data from the cache.
  prefs: []
  type: TYPE_NORMAL
- en: To provide all the aforementioned functionality, cache designers employ one
    of three designs. This section begins by examining *direct-mapped caches*, which
    are less complex than the other designs.
  prefs: []
  type: TYPE_NORMAL
- en: 11.4.1 Direct-Mapped Caches
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A direct-mapped cache divides its storage space into units called *cache lines*.
    Depending on the size of a cache, it might hold dozens, hundreds, or even thousands
    of cache lines. In a direct-mapped cache, each cache line is independent of all
    the others and contains two important types of information: a *cache data block*
    and *metadata*.'
  prefs: []
  type: TYPE_NORMAL
- en: A *cache data block* (often shortened to *cache block*) stores a subset of program
    data from main memory. Cache blocks store multibyte chunks of program data to
    take advantage of spatial locality. The size of a cache block determines the unit
    of data transfer between the cache and main memory. That is, when loading a cache
    with data from memory, the cache always receives a chunk of data the size of a
    cache block. Cache designers balance a trade-off in choosing a cache’s block size.
    Given a fixed storage budget, a cache can store more smaller blocks or fewer larger
    blocks. Using larger blocks improves performance for programs that exhibit good
    spatial locality, whereas having more blocks gives a cache the opportunity to
    store a more diverse subset of memory. Ultimately, which strategy provides the
    best performance depends on the workload of applications. Since general-purpose
    CPUs can’t assume much about a system’s applications, a typical CPU cache today
    uses middle-of-the-road block sizes ranging from 16 to 64 bytes.
  prefs: []
  type: TYPE_NORMAL
- en: '*Metadata* stores information about the contents of the cache line’s data block.
    A cache line’s metadata does *not* contain program data. Instead, it maintains
    bookkeeping information for the cache line (e.g., to help identify which subset
    of memory the cache line’s data block holds).'
  prefs: []
  type: TYPE_NORMAL
- en: When a program attempts to access a memory address, a cache must know where
    to look to find the corresponding data, check whether the desired data is available
    at that cache location, and if so, retrieve a portion of the stored cache block
    to the requesting application. The following steps walk through the details of
    this process for finding data in a cache and retrieving it.
  prefs: []
  type: TYPE_NORMAL
- en: Locating Cached Data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A cache must be able to quickly determine whether the subset of memory corresponding
    to a requested address currently resides in the cache. To answer that question,
    a cache must first determine which cache line(s) to check. In a direct-mapped
    cache, each address in memory corresponds to *exactly* one cache line. This restriction
    explains the *direct-mapped* name—it maps every memory address directly to one
    cache line.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 11-7](ch11.xhtml#ch11fig7) shows how memory addresses map to cache
    lines in a small direct-mapped cache with four cache lines and a 32-byte cache
    block size. Recall that a cache’s block size represents the smallest unit of data
    transfer between a cache and main memory. Thus, every memory address falls within
    one 32-byte range, and each range maps to one cache line.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/11fig07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-7: An example mapping of memory addresses to cache lines in a four-line
    direct-mapped cache with 32-byte cache blocks*'
  prefs: []
  type: TYPE_NORMAL
- en: Note that although each region of memory maps to only one cache line, many memory
    ranges map to the *same* cache line. All of the memory regions that map the same
    cache line (i.e., chunks of the same color in [Figure 11-7](ch11.xhtml#ch11fig7))
    compete for space in the same cache line, so only one region of each color can
    reside in the cache at a time.
  prefs: []
  type: TYPE_NORMAL
- en: A cache maps a memory address to a cache line using a portion of the bits in
    the memory address. To spread data more evenly among cache lines, caches use bits
    taken from the *middle* of the memory address, known as the *index* portion of
    the address, to determine which line the address maps to. The number of bits used
    as the index (which varies) determines how many lines a cache will hold. [Figure
    11-8](ch11.xhtml#ch11fig8) shows the index portion of a memory address referring
    to a cache line.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/11fig08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-8: The middle index portion of a memory address identifies a cache
    line.*'
  prefs: []
  type: TYPE_NORMAL
- en: Using the middle of the address reduces competition for the same cache line
    when program data is clustered together, which is often the case for programs
    that exhibit good locality. That is, programs tend to store variables nearby one
    another in one of a few locations (e.g., on the stack or heap). Such clustered
    variables share the same high-order address bits. Thus, indexing with the high-order
    bits would cause the clustered variables to all map to the same cache lines, leaving
    the rest of the cache unused. By using bits from the middle of the address, caches
    spread data more evenly among the available cache lines.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying Cache Contents
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Next, having located the appropriate cache line, the cache must determine whether
    that line holds the requested address. Since multiple memory ranges map to the
    same cache line, the cache examines the line’s metadata to answer two important
    questions: Does this cache line hold a valid subset of memory? If so, which of
    the many subsets of memory that map to this cache line does it currently hold?'
  prefs: []
  type: TYPE_NORMAL
- en: To answer these questions, each cache line’s metadata includes a valid bit and
    a tag. The *valid bit* is a single bit that indicates whether a line is currently
    storing a valid subset of memory (if valid is set to 1). An invalid line (if valid
    is set to 0) never produces a cache hit because no data has been loaded into it.
    Invalid lines effectively represent free space in the cache.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to a valid bit, each cache line’s metadata stores a *tag* that uniquely
    identifies which subset of memory the line’s cache block holds. The tag field
    stores the high-order bits of the address range stored in the cache line and allows
    a cache line to track where in memory its data block came from. In other words,
    because many memory subsets map to the same cache line (those with the same index
    bits), the tag records which of those subsets is currently present in the cache
    line.
  prefs: []
  type: TYPE_NORMAL
- en: For a cache lookup to produce a hit, the tag field stored in the cache line
    must exactly match the tag portion (upper bits) of the program’s requested memory
    address. A tag mismatch indicates that a cache line’s data block does not contain
    the requested memory, even if the line stores valid data. [Figure 11-9](ch11.xhtml#ch11fig9)
    illustrates how a cache divides a memory address into a tag and an index, uses
    the index bits to select a target cache line, verifies a line’s valid bit, and
    checks the line’s tag for a match.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/11fig09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-9: After using the requested memory address’s index bits to locate
    the proper cache line, the cache simultaneously verifies the line’s valid bit
    and checks its tag against the requested address’s tag. If the line is valid with
    a matching tag, the lookup succeeds as a hit.*'
  prefs: []
  type: TYPE_NORMAL
- en: Retrieving Cached Data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Finally, after using the program’s requested memory address to find the appropriate
    cache line and verifying that the line holds a valid subset of memory containing
    that address, the cache sends the requested data to the CPU’s components that
    need it. Because a cache line’s data block size (e.g., 64 bytes) is typically
    much larger than the amount of data that programs request (e.g., 4 bytes), caches
    use the low-order bits of the requested address as an *offset* into the cached
    data block. [Figure 11-10](ch11.xhtml#ch11fig10) depicts how the offset portion
    of an address identifies which bytes of a cache block the program expects to retrieve.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/11fig10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-10: Given a cache data block, the offset portion of an address identifies
    which bytes the program wants to retrieve.*'
  prefs: []
  type: TYPE_NORMAL
- en: Memory Address Division
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The *dimensions* of a cache dictate how many bits to interpret as the offset,
    index, and tag portions of a memory address. Equivalently, the number of bits
    in each portion of an address imply what the dimensions of a cache must be. In
    determining which bits belong to each portion of an address, it’s helpful to consider
    the address from right to left (i.e., from least to most significant bit).
  prefs: []
  type: TYPE_NORMAL
- en: The rightmost portion of the address is the *offset*, and its length depends
    on a cache’s block size dimension. The offset portion of an address must contain
    enough bits to refer to every possible byte within a cache data block. For example,
    suppose that a cache stores 32-byte data blocks. Because a program might come
    along asking for any of those 32 bytes, the cache needs enough offset bits to
    describe exactly which of the 32 possible positions the program might want. In
    this case, it would need five bits for the offset because five bits are necessary
    to represent 32 unique values (log[2] 32 = 5). In the reverse direction, a cache
    that uses four bits for the offset must store 16-byte data blocks (2⁴ = 16).
  prefs: []
  type: TYPE_NORMAL
- en: The *index* portion of the address begins immediately to the left of the offset.
    To determine the number of index bits, consider the number of lines in the cache,
    given that the index needs enough bits to uniquely identify every cache line.
    Using similar logic to the offset, a cache with 1,024 lines needs 10 bits for
    the index (log[2] 1,024 = 10). Likewise, a cache that uses 12 bits for the index
    must have 4,096 lines (2^(12) = 4,096).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/11fig11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-11: The index portion of an address uniquely identifies a cache
    line, and the offset portion uniquely identifies a position in the line’s data
    block.*'
  prefs: []
  type: TYPE_NORMAL
- en: The remaining address bits form the tag. Because the tag must uniquely identify
    the subset of memory contained within a cache line, the tag must use *all* of
    the remaining, unclaimed bits of the address. For example, if a machine uses 32-bit
    addresses, a cache with 5 offset bits and 10 index bits uses the remaining 32
    – 15 = 17 bits of the address to represent the tag.
  prefs: []
  type: TYPE_NORMAL
- en: Direct-Mapped Read Examples
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Consider a CPU with the following characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: 16-bit memory addresses
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a direct-mapped cache with 128 cache lines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 32-byte cache data blocks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The cache starts empty (all lines are invalid), as shown in [Figure 11-12](ch11.xhtml#ch11fig12).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/11fig12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-12: An empty direct-mapped example cache*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose that a program running on this CPU accesses the following memory locations
    (see [Figures 11-13](ch11.xhtml#ch11fig13) through [11-16](ch11.xhtml#ch11fig16)):'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Read from address 1010000001100100.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Read from address 1010000001100111.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Read from address 1001000000100000.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Read from address 1111000001100101.
  prefs: []
  type: TYPE_NORMAL
- en: 'To put the entire sequence together, follow these steps when tracing the behavior
    of a cache:'
  prefs: []
  type: TYPE_NORMAL
- en: '1\. Divide the requested address into three portions, from right (low-order
    bits) to left (high-order bits): an offset within the cache data block, an index
    into the appropriate cache line, and a tag to identify which subset of memory
    the line stores.'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Index into the cache using the middle portion of the requested address to
    find the cache line to which the address maps.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Check the cache line’s valid bit. When invalid, the program can’t use a
    cache line’s contents (cache miss), regardless of what the tag might be.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Check the cache line’s tag. If the address’s tag matches the cache line’s
    tag and the line is valid, the cache line’s data block holds the data the program
    is looking for (cache hit). Otherwise, the cache must load the data from main
    memory at the identified index (cache miss).
  prefs: []
  type: TYPE_NORMAL
- en: 5\. On a hit, use the low-order offset bits of the address to extract the program’s
    desired data from the stored block. (Not shown in this example.)
  prefs: []
  type: TYPE_NORMAL
- en: Address Division
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Begin by determining how to divide the memory addresses into their *offset*,
    *index*, and *tag* portions. Consider the address portions from low-order to high-order
    bits (right to left):'
  prefs: []
  type: TYPE_NORMAL
- en: '*Offset*: A 32-byte block size implies that the rightmost five bits of the
    address (log[2] 32 = 5) comprise the offset portion. With five bits, the offset
    can uniquely identify any of the 32 bytes in block.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Index*: A cache with 128 lines implies that the next seven bits of the address
    (log[2] 128 = 7) comprise the index portion. With seven bits, the index can uniquely
    identify each cache line.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Tag*: The tag consists of any remaining address bits that don’t belong to
    the offset or index. Here, the address has four remaining bits left that form
    the tag (16 – (5 + 7) = 4).'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/11fig13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-13: Read from address 1010000001100100\. Index 0000011 (line 3)
    is invalid, so the request misses and the cache loads data from main memory.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/11fig14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-14: Read from address 1010000001100111\. Index 0000011 (line 3)
    is valid, and the tag (1010) matches, so the request hits. The cache yields data
    beginning at byte 7 (offset 0b00111) of its data block.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/11fig15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-15: Read from address 1001000000100000\. Index 0000001 (line 1)
    is invalid, so the request misses and the cache loads data from main memory.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/11fig16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-16: Read from address 1111000001100101\. Index 0000011 (line 3)
    is valid, but the tag doesn’t match, so the request misses and the cache loads
    data from main memory.*'
  prefs: []
  type: TYPE_NORMAL
- en: Writing to Cached Data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: So far, this section has primarily considered memory read operations for which
    a CPU performs lookups in the cache. Caches must also allow programs to store
    values, and they support store operations with one of two strategies.
  prefs: []
  type: TYPE_NORMAL
- en: In a *write-through cache*, a memory write operation modifies the value in the
    cache and simultaneously updates the contents of main memory. That is, a write
    operation *always* synchronizes the contents of the cache and main memory immediately.
  prefs: []
  type: TYPE_NORMAL
- en: In a *write-back cache*, a memory write operation modifies the value stored
    in the cache’s data block, but it does *not* update main memory. Thus, after updating
    the cache’s data, a write-back cache’s contents differ from the corresponding
    data in main memory.
  prefs: []
  type: TYPE_NORMAL
- en: To identify cache blocks whose contents differ from their main memory counterparts,
    each line in a write-back cache stores an additional bit of metadata, known as
    a *dirty bit*. When evicting the data block from a dirty cache line, the cache
    block’s data must first be written back to main memory to synchronize their contents.
    [Figure 11-17](ch11.xhtml#ch11fig17) shows a direct-mapped cache that includes
    a dirty bit to mark lines that must be written to memory upon eviction.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/11fig17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-17: Cache extended with a dirty bit*'
  prefs: []
  type: TYPE_NORMAL
- en: As usual, the difference between the designs reveals a trade-off. Write-through
    caches are less complex than write-back caches, and they avoid storing extra metadata
    in the form of a dirty bit for each line. On the other hand, write-back caches
    reduce the cost of repeated writes to the same location in memory.
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose that a program frequently updates the same variable without
    that variable’s memory ever being evicted from the cache. A write-through cache
    writes to main memory on every update, even though each subsequent update is just
    going to overwrite the previous one, whereas a write-back cache writes to memory
    only when eventually evicting the cache block. Because amortizing the cost of
    a memory access across many writes significantly improves performance, most modern
    caches opt for a write-back design.
  prefs: []
  type: TYPE_NORMAL
- en: Direct-Mapped Write Examples (Write-Back)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Writes to the cache behave like reads, except they also set the modified cache
    line’s dirty bit. When evicting a dirty cache line, the cache must write the modified
    data block to memory before discarding it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose that the previously described example scenario continues with two additional
    memory accesses (see [Figures 11-18](ch11.xhtml#ch11fig18) and [11-19](ch11.xhtml#ch11fig19)):'
  prefs: []
  type: TYPE_NORMAL
- en: '5\. Write to address: 1111000001100000.'
  prefs: []
  type: TYPE_NORMAL
- en: '6\. Write to address: 1010000001100100.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/11fig18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-18: Write to address 1111000001100000\. Index 0000011 (line 3) is
    valid, and the tag (1111) matches, so the request hits. Because this access is
    a write, the cache sets the line’s dirty bit to 1.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/11fig19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-19: Write to address 1010000001100100\. Index 0000011 (line 3) is
    valid, but the tag doesn’t match, so the request misses. Because the target line
    is both valid and dirty, the cache must save the existing data block to main memory
    before loading the new one. This access is a write, so the cache sets the newly
    loaded line’s dirty bit to 1.*'
  prefs: []
  type: TYPE_NORMAL
- en: In the fourth and sixth memory accesses of the example, the cache evicts data
    because two memory regions are competing for the same cache line. Next, we’ll
    explore a different cache design that aims to reduce this type of competition.
  prefs: []
  type: TYPE_NORMAL
- en: 11.4.2 Cache Misses and Associative Designs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Cache designers aim to maximize a cache’s hit rate to ensure that as many memory
    requests as possible can avoid going to main memory. Even though locality provides
    hope for achieving a good hit rate, real caches can’t expect to hit on every access
    for a variety of reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Compulsory** or **cold-start misses**: If a program has never accessed a
    memory location (or any location near it), it has little hope of finding that
    location’s data in the cache. Thus, programs often cannot avoid cache misses when
    first accessing new memory addresses.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Capacity misses**: A cache stores a subset of main memory, and ideally it
    stores *exactly* the subset of memory that a program is actively using. However,
    if a program is actively using more memory than fits in the cache, it can’t possibly
    find *all* of the data it wants in the cache, leading to *capacity misses*.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Conflict misses**: To reduce the complexity of finding data, some cache designs
    limit where in the cache data can reside, and those restrictions can lead to *conflict
    misses*. For example, even if a direct-mapped cache is not 100% full, a program
    might end up with the addresses of two frequently used variables mapping to the
    same cache location. In such cases, each access to one of those variables evicts
    the other from the cache as they compete for the same cache line.'
  prefs: []
  type: TYPE_NORMAL
- en: The relative frequency of each miss type depends on a program’s memory access
    pattern. In general though, without increasing the cache size, a cache’s design
    mainly affects its conflict miss rate. Although direct-mapped caches are less
    complex than other designs, they suffer the most from conflicts.
  prefs: []
  type: TYPE_NORMAL
- en: The alternative to a direct-mapped cache is an *associative* cache. An associative
    design gives a cache the flexibility to choose among more than one location to
    store a region of memory. Intuitively, having more storage location options reduces
    the likelihood of conflicts but also increases complexity due to more locations
    needing to be checked on every access.
  prefs: []
  type: TYPE_NORMAL
- en: A *fully associative* cache allows any memory region to occupy any cache location.
    Fully associative caches offer the most flexibility, but they also have the highest
    lookup and eviction complexity because every location needs to be simultaneously
    considered during any operation. Although fully associative caches are valuable
    in some small, specialized applications (e.g., translation look-aside buffers—see
    “Making Page Accesses Faster” on [page 655](ch13.xhtml#lev3_110)), their high
    complexity makes them generally unfit for a general-purpose CPU cache.
  prefs: []
  type: TYPE_NORMAL
- en: '*Set associative* caches occupy the middle ground between direct-mapped and
    fully associative designs, which makes them well suited for general-purpose CPUs.
    In a set associative cache, every memory region maps to exactly one *cache set*,
    but each set stores multiple cache lines. The number of lines allowed in a set
    is a fixed dimension of a cache, and set associative caches typically store two
    to eight lines per set.'
  prefs: []
  type: TYPE_NORMAL
- en: 11.4.3 Set Associative Caches
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A set associative design offers a good compromise between complexity and conflicts.
    The number of lines in a set limits how many places a cache needs to check during
    a lookup, and multiple memory regions that map to the same set don’t trigger conflict
    misses unless the entire set fills.
  prefs: []
  type: TYPE_NORMAL
- en: In a set associative cache, the *index* portion of a memory address maps the
    address to one set of cache lines. When performing an address lookup, the cache
    simultaneously checks every line in the set. [Figure 11-20](ch11.xhtml#ch11fig20)
    illustrates the tag and valid bit checks in a two-way set associative cache.
  prefs: []
  type: TYPE_NORMAL
- en: 'If any of a set’s valid lines contains a tag that matches the address’s tag
    portion, the matching line completes the lookup. When the lookup narrows the search
    to just one cache line, it proceeds like a direct-mapped cache: the cache uses
    the address’s *offset* to send the desired bytes from the line’s cache block to
    the CPU’s arithmetic components.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/11fig20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-20: Valid bit verification and tag matching in a two-way set associative
    cache*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The additional flexibility of multiple cache lines in a set reduces conflicts,
    but it also introduces a new wrinkle: when loading a value into a cache (and when
    evicting data already resident in the cache), the cache must decide *which* of
    the line options to use.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To help solve this selection problem, caches turn to the idea of locality.
    Specifically, temporal locality suggests that recently used data is likely to
    be used again. Therefore, caches adopt the same strategy that the previous section
    used to manage our example bookcase: when deciding which line in a set to evict,
    choose the least recently used (LRU) line. LRU is known as a *cache replacement
    policy* because it governs the cache’s eviction mechanism.'
  prefs: []
  type: TYPE_NORMAL
- en: The LRU policy requires each set to store additional bits of metadata to identify
    which line of the set was used least recently. As the number of lines in a set
    increases, so does the number of bits required to encode the LRU status of the
    set. These extra metadata bits contribute to the “higher complexity” of set associative
    designs compared to simpler direct-mapped variants.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 11-21](ch11.xhtml#ch11fig21) illustrates a two-way set associative
    cache, meaning each set contains two lines. With just two lines, each set requires
    one LRU metadata bit to keep track of which line was least recently used. In the
    figure, an LRU value of zero indicates the leftmost line was least recently used,
    and a value of one means the rightmost line was least recently used.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/11fig21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-21: A two-way set associative cache in which each set stores one
    bit of LRU metadata to inform eviction decisions*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Warning LRU BIT INTERPRETATION**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 11-21](ch11.xhtml#ch11fig21)’s choice that zero means “left” and one
    means “right” is arbitrary. The interpretation of LRU bits varies across caches.
    If you’re asked to work with caches on an assignment, don’t assume the assignment
    is using the same LRU encoding scheme!'
  prefs: []
  type: TYPE_NORMAL
- en: Set Associative Cache Examples
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Consider a CPU with the following characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: 16-bit memory addresses.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A two-way set associative cache with 64 sets. Note that making a cache two-way
    set associative doubles its storage capacity (two lines per set), so this example
    halves the number of sets so that it stores the same number of lines as the earlier
    direct-mapped example.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 32-byte cache blocks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An LRU cache replacement policy that indicates whether the leftmost line of
    the set was least recently used (LRU = 0) or the rightmost line of the set was
    least recently used (LRU = 1).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Initially, the cache is empty (all lines invalid and LRU bits 0), as shown in
    [Figure 11-22](ch11.xhtml#ch11fig22).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/11fig22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-22: An empty two-way set associative example cache*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose that a program running on this CPU accesses the following memory locations
    (same as the direct-mapped example) (see [Figures 11-23](ch11.xhtml#ch11fig23)
    through [11-28](ch11.xhtml#ch11fig28)):'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Read from address 1010000001100100.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Read from address 1010000001100111.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Read from address 1001000000100000.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Read from address 1111000001100101.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Write to address 1111000001100000.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Write to address 1010000001100100.
  prefs: []
  type: TYPE_NORMAL
- en: 'Begin by determining how to divide the memory addresses into their *offset*,
    *index*, and *tag* portions. Consider the address portions from low-order to high-order
    bits (right to left):'
  prefs: []
  type: TYPE_NORMAL
- en: '*Offset*: A 32-byte block size implies that the rightmost five bits of the
    address (log[2] 32 = 5) comprise the offset portion. Five bits allows the offset
    to uniquely identify any of the bytes in a block.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Index*: A 64-set cache implies that the next six bits of the address (log[2]
    64 = 6) comprise the index portion. Six bits allows the index to uniquely identify
    each set in the cache.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Tag*: The tag consists of any remaining bits of the address that don’t belong
    to the offset or index. Here, the address has five remaining bits left over for
    the tag (16 – (5 + 6) = 5).'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/11fig23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-23: Read from address 1010000001100100\. Both lines at index 000011
    (set 3) are invalid, so the request misses, and the cache loads data from main
    memory. The set’s LRU bit is 0, so the cache loads data into the left line and
    updates the LRU bit to 1.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/11fig24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-24: Read from address 1010000001100111\. The left line at index
    000011 (set 3) holds a matching tag, so the request hits.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/11fig25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-25: Read from address 1001000000100000\. Both lines at index 000001
    (set 1) are invalid, so the request misses, and the cache loads data from main
    memory. The set’s LRU bit is 0, so the cache loads data into the left line and
    updates the LRU bit to 1.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/11fig26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-26: Read from address 1111000001100101\. At index 000011 (set 3),
    one line’s tag doesn’t match, and the other line is invalid, so the request misses.
    The set’s LRU bit is 1, so the cache loads data into the right line and updates
    the LRU bit to 0.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/11fig27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-27: Write to address 1111000001100000\. The right line at index
    000011 (set 3) is valid and holds a matching tag, so the request hits. Because
    this access is a write, the cache sets the line’s dirty bit to 1\. The LRU bit
    remains 0 to indicate that the left line remains least recently used.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/11fig28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-28: Write to address 1010000001100100\. The left line at index 000011
    (set 3) is valid and holds a matching tag, so the request hits. Because this access
    is a write, the cache sets the line’s dirty bit to 1\. After accessing the left
    line, the cache sets the line’s LRU bit to 1.*'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the same memory access sequence that produced two conflict
    misses with a direct-mapped cache suffers from no conflicts with a two-way set
    associative cache.
  prefs: []
  type: TYPE_NORMAL
- en: 11.5 Cache Analysis and Valgrind
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Because caches significantly influence program performance, most systems provide
    profiling tools to measure a program’s use of the cache. One such tool is Valgrind’s
    `cachegrind` mode, which this section uses to evaluate cache performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following program that generates a random *N* × *N* matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Prior sections in this chapter introduced two functions for averaging every
    element of a matrix. They differ only in the way they index into the matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: Version 1
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Version 2
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This section uses cache profiling tools to quantify the differences between
    them.
  prefs: []
  type: TYPE_NORMAL
- en: '11.5.1 A First Cut: Theoretical Analysis and Benchmarking'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A theoretical analysis based on locality and the memory hierarchy suggests that
    the first version exhibits better spatial locality (on matrix `mat`) due to the
    fact that `mat` is stored in row-major order in memory (see the section “Two-Dimensional
    Array Memory Layout” on [page 86](ch02.xhtml#lev3_13)). The second solution has
    poor spatial locality because each element in the matrix is visited in column-major
    order. Recall that data is loaded into a cache in *blocks*. Traversing the matrix
    in column-major order will likely lead to more cache misses, resulting in poorer
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s modify the main function to include calls to the `gettimeofday` function
    to accurately measure the difference in performance between the two versions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Compiling the code and running it yields the following result (note that the
    times will vary based on the machine on which it’s run):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: That’s a big difference! In essence, the solution using row-major order is 4.61
    times faster than the second one!
  prefs: []
  type: TYPE_NORMAL
- en: '11.5.2 Cache Analysis in the Real World: Cachegrind'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Theoretically analyzing the two solutions and then running them verifies that
    the first version is faster than the second. However, it doesn’t confirm the details
    of the cache analysis. Fortunately, the Valgrind suite of tools can help. Earlier
    in the book, we discussed how Valgrind can help find memory leaks in a program
    (see “Debugging Memory with Valgrind” on [page 168](ch03.xhtml#lev1_22)). This
    section describes Cachegrind, Valgrind’s cache simulator. Cachegrind enables a
    programmer to study how a program or particular function affects the cache.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cachegrind simulates how a program interacts with the computer’s cache hierarchy.
    In many cases, Cachegrind can autodetect the cache organization of a machine.
    In the cases that it cannot, Cachegrind still simulates the first level (L1) cache
    and the last level (LL) cache. It assumes the first level cache has two independent
    components: the instruction cache and the data cache. The reason for this is that
    the last level cache has the most important implications for runtime. L1 caches
    also have the lowest level of associativity, so it’s important to ensure that
    programs interact well with it. These assumptions match the structure of most
    modern machines.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cachegrind collects and outputs the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: Instruction cache reads (`Ir`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: L1 instruction cache read misses (`I1mr`) and LL cache instruction read misses
    (`ILmr`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data cache reads (`Dr`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: D1 cache read misses (`D1mr`) and LL cache data misses (`DLmr`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data cache writes (`Dw`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: D1 cache write misses (`D1mw`) and LL cache data write misses (`DLmw`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that D1 total access is computed by `D1` = `D1mr` + `D1mw` and LL total
    access is given by `ILmr` + `DLmr` + `DLmw`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how well version 1 of the code operates under Cachegrind. To run
    it, execute Valgrind on the compiled code with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In this invocation, Valgrind’s `cachegrind` tool acts as a wrapper around the
    `cachex` executable. Choosing a smaller matrix size for Cachegrind aids in the
    speed of execution. Cachegrind outputs information about the number of cache hits
    and misses in the overall program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'However, this analysis is interested *specifically* in the hits and misses
    for the two versions of this averaging function. To view that information, use
    the Cachegrind tool `cg_annotate`. Running Cachegrind should have produced a file
    in the current working directory that looks similar to `cachegrind.out.n`, where
    `n` is some process ID number. To run `cg_annotate`, type in the following command
    (replacing `cachegrind.out.28657` with the name of the output file):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We’ve edited the output from this command slightly to focus on the two versions
    of the average function. This output shows that version 2 yields 1,062,996 data
    misses, compared to only `62688` misses in version 1\. Cachegrind provides solid
    proof that our analysis is correct!
  prefs: []
  type: TYPE_NORMAL
- en: '11.6 Looking Ahead: Caching on Multicore Processors'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far our discussion of caching has focused on a single level of cache memory
    on a single-core processor. Modern processors, however, are multicore with several
    levels of cache memory. Typically, each core maintains its own private cache memory
    at the highest level(s) of the memory hierarchy and shares a single cache with
    all cores at lower levels. [Figure 11-29](ch11.xhtml#ch11fig29) shows an example
    of the memory hierarchy on a four-core processor in which each core contains a
    private level 1 (L1) cache, and the level 2 (L2) cache is shared by all four cores.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/11fig29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-29: An example memory hierarchy on a multicore processor. Each of
    the four cores has its own private L1 cache, and all four cores share a single
    L2 cache that they access through a shared bus. The multicore processor connects
    to RAM via the memory bus.*'
  prefs: []
  type: TYPE_NORMAL
- en: Recall that higher levels of the memory hierarchy are faster to access and smaller
    in capacity than lower levels of the memory hierarchy. Thus, an L1 cache is smaller
    and faster than an L2 cache, which in turn is smaller and faster than RAM. Also
    recall that cache memory stores a copy of a value from a lower level in the memory
    hierarchy; a value stored in a L1 cache is a copy of the same value stored in
    the L2 cache, which is a copy of the same value stored in RAM. As a result, higher
    levels of the memory hierarchy serve as caches for lower levels. Thus, for the
    example in [Figure 11-29](ch11.xhtml#ch11fig29), the L2 cache is a cache of RAM
    contents, and each core’s L1 cache is a cache of the L2 cache contents.
  prefs: []
  type: TYPE_NORMAL
- en: Each core in a multicore processor simultaneously executes an independent stream
    of instructions, often from separate programs. Providing each core a private L1
    cache allows the core to store copies of the data and instructions exclusively
    from the instruction stream it’s executing in its fastest cache memory. In other
    words, each core’s L1 cache stores a copy of only those blocks of memory that
    are from its execution stream as opposed to competing for space in a single L1
    cache shared by all cores. This design yields a higher hit rate in each core’s
    private L1 cache (in its fastest cache memory) than one in which all cores share
    a single L1 cache.
  prefs: []
  type: TYPE_NORMAL
- en: Today’s processors often include more than two levels of cache. Three levels
    are common in desktop systems, with the highest level (L1) typically split into
    two separate L1 caches, one for program instructions and the other for program
    data. Lower-level caches are usually *unified caches*, meaning that they store
    both program data and instructions. Each core usually maintains a private L1 cache
    and shares a single L3 cache with all cores. The L2 cache layer, which sits between
    each core’s private L1 cache and the shared L3 cache, varies substantially in
    modern CPU designs. The L2 may be a private L2 cache, may be shared by all cores,
    or may be a hybrid organization with multiple L2 caches, each shared by a subset
    of cores.
  prefs: []
  type: TYPE_NORMAL
- en: PROCESSOR AND CACHE INFORMATION IN LINUX SYSTEMS
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’re curious about your CPU’s design, there are several ways to obtain
    information about a processor and its cache organization on your system. For example,
    the `lscpu` command displays information about the processor, including its number
    of cores and the levels and sizes of its caches:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This output shows that there are six total cores (the number of `Socket(s)`
    multiplied by the `Core(s) per socket`), and that each core is two-way hyper-
    threaded (`Thread(s) per core`) to make the six physical cores appear as 12 CPUs
    to the operating system (see ”Multicore and Hardware Multithreading” in [Chapter
    5](ch05.xhtml#ch05) for more information on hardware multithreading). Additionally,
    the output shows that there are three levels of cache (`L1`, `L2`, and `L3`),
    and that there are two separate L1 caches, one for caching data (`L1d`) and another
    for caching instructions (`L1i`).
  prefs: []
  type: TYPE_NORMAL
- en: In addition to `lscpu`, files in the `/proc` and `/sys` filesystems contain
    information about the processor. For example, the command `cat /proc/cpuinfo`
    outputs information about the processor, and the following command lists information
    about the caches for a specific processor core (note that these files are named
    in terms of a core’s hyperthreaded CPUs, and in this example `cpu0` and `cpu6`
    are the two hyperthreaded CPUs on core 0).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This output indicates that core 0 has four caches (`index0` to `index3`). To
    see the details of each cache, examine the index directory’s `type`, `level`,
    and `shared` `_cpu_list` files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The `type` output indicates that core 0 has separate data and instruction caches
    as well as two other unified caches. Correlating the `level` output with the `type`
    output reveals that the data and instruction caches are both L1 caches, whereas
    the unified caches are L2 and L3 caches, respectively. The `shared_cpu_list` further
    shows that the L1 and L2 caches are private to core 0 (shared only by CPU `0`
    and `6`, the two hyperthread CPUs on core 0), and that the L3 cache is shared
    by all six cores (by all 12 hyperthreaded CPUs, `0-11`).
  prefs: []
  type: TYPE_NORMAL
- en: 11.6.1 Cache Coherency
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Because programs typically exhibit a high degree of locality of reference, it
    is advantageous for each core to have its own L1 cache to store copies of the
    data and instructions from the instruction stream it executes. However, multiple
    L1 caches can result in *cache coherency* problems. Problems with cache coherency
    arise when the value of a copy of a block of memory stored in one core’s L1 cache
    is different than the value of a copy of the same block stored in another core’s
    L1 cache. This situation occurs when one core writes to a block cached in its
    L1 cache that is also cached in other core’s L1 caches. Because a cache block
    contains a copy of memory contents, the system needs to maintain a coherent single
    value of the memory contents across all copies of the cached block. implement
    a *cache-coherence protocol* to ensure a coherent view of memory that can be cached
    and accessed by multiple cores. A cache coherency protocol ensures that any core
    accessing a memory location sees the most recently modified value of that memory
    location rather than seeing an older (stale) copy of the value that may be stored
    in its L1 cache.
  prefs: []
  type: TYPE_NORMAL
- en: 11.6.2 The MSI Protocol
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are many different cache coherency protocols. Here, we discuss the details
    of one example, the MSI protocol. The *MSI protocol* (Modified, Shared, Invalid)
    adds three flags (or bits) to each cache line. A flag’s value is either clear
    (0) or set (1). The values of the three flags encode the state of its data block
    with respect to cache coherency with other cached copies of the block, and their
    values trigger cache coherency actions on read or write accesses to the data block
    in the cache line. The three flags used by the MSI protocol are:'
  prefs: []
  type: TYPE_NORMAL
- en: The *M* flag that, if set, indicates the block has been modified, meaning that
    this core has written to its copy of the cached value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *S* flag that, if set, indicates that the block is unmodified and can be
    safely shared, meaning that multiple L1 caches may safely store a copy of the
    block and read from their copy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *I* flag that, if set, indicates if the cached block is invalid or contains
    stale data (is an older copy of the data that does not reflect the current value
    of the block of memory).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The MSI protocol is triggered on read and write accesses to cache entries.
  prefs: []
  type: TYPE_NORMAL
- en: 'On a read access:'
  prefs: []
  type: TYPE_NORMAL
- en: If the cache block is in the M or S state, then the cached value is used to
    satisfy the read (its copy’s value is the most current value of the block of memory).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the cache block is in the I state, then the cached copy is out of date with
    a newer version of the block, and the block’s new value needs to be loaded into
    the cache line before the read can be satisfied.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If another core’s L1 cache stores the new value (it stores the value with the
    M flag set indicating that it stores a modified copy of the value), that other
    core must first write its value back to the lower level (e.g., to the L2 cache).
    After performing the write-back, it clears the M flag with the cache line (its
    copy and the copy in the lower-level are now consistent) and sets the S bit to
    indicate that the block in this cache line is in a state that can be safely cached
    by other cores (the L1 block is consistent with its copy in the L2 cache and the
    core read the current value of the block from this L1 copy).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The core that initiated the read access on an line with the I flag set can then
    load the new value of the block into its cache line. It clears the I flag indicating
    that the block is now valid and stores the new value of the block, sets the S
    flag indicating that the block can be safely shared (it stores the latest value
    and is consistent with other cached copies), and clears the M flag indicating
    that the L1 block’s value matches that of the copy stored in the L2 cache (a read
    does not modify the L1 cached copy of the memory).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'On a write access:'
  prefs: []
  type: TYPE_NORMAL
- en: If the block is in the M state, then write to the cached copy of the block.
    No changes to the flags are needed (the block remains in the M state).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the block is in the I or the S state, then notify other cores that the block
    is being written to (modified). Other L1 caches that have the block stored in
    the S state need to clear the S bit and set the I bit on their block (their copies
    of the block are now out of date with the copy that is being written to by the
    other core). If another L1 cache has the block in the M state, it will write its
    block back to the lower level and set its copy to I. The core writing will then
    load the new value of the block into its L1 cache, set the M flag (its copy will
    be modified by the write), and clear the I flags (its copy is now valid), and
    write to the cached block.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Figure 11-30](ch11.xhtml#ch11fig30) through [Figure 11-32](ch11.xhtml#ch11fig32)
    step through an example of the MSI protocol applied to ensure coherency of read
    and write accesses to a block of memory that is cached in two core’s private L1
    caches. In [Figure 11-30](ch11.xhtml#ch11fig30) our example starts with the shared
    data block copied into both core’s L1 cache with the S flag set, meaning that
    the L1 cached copies are the same as the value of the block in the L2 cache (all
    copies store the current value of the block, 6). At this point, both core 0 and
    core 1 can safely read from the copy stored in their L1 caches without triggering
    coherency actions (the S flag indicates that their shared copy is up to date).'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/11fig30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-30: At the start, both cores have a copy of the block in their private
    L1 caches with the S flag set (in Shared mode)*'
  prefs: []
  type: TYPE_NORMAL
- en: If core 0 next writes to the copy of the block stored in its L1 cache, its L1
    cache controller notifies the other L1 caches to invalidate their copy of the
    block. Core 1’s L1 cache controller then clears the S flag and sets the I flag
    on its copy, indicating that its copy of the block is stale. Core 0 writes to
    its copy of the block in its L1 cache (changing its value to 7 in our example)
    and sets the M flag and clears the S flag on the cache line to indicate that its
    copy has been modified and stores the current value of the block. At this point,
    the copy in the L2 cache and in core 1’s L1 cache are stale. The resulting cache
    state is shown in [Figure 11-31](ch11.xhtml#ch11fig31).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/11fig31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-31: The resulting state of the caches after Core 0 writes to its
    copy of the block*'
  prefs: []
  type: TYPE_NORMAL
- en: At this point, core 0 can safely read from its copy of the cached block because
    its copy is in the M state, meaning that it stores the most recently written value
    of the block.
  prefs: []
  type: TYPE_NORMAL
- en: If core 1 next reads from the memory block, the I flag on its L1 cached copy
    indicates that its L1 copy of the block is stale and cannot be used to satisfy
    the read. Core 1’s L1 cache controller must first load the new value of the block
    into the L1 cache before the read can be satisfied. To achieve this, core 0’s
    L1 cache controller must first write its modified value of the block back to the
    L2 cache, so that core 1’s L1 cache can read the new value of the block into its
    L1 cache. The result of these actions, (shown in [Figure 11-32](ch11.xhtml#ch11fig32)),
    is that core 0 and core 1’s L1 cached copies of the block are now both stored
    in the S state, indicating that each core’s L1 copy is up to data and can be safely
    used to satisfy subsequent reads to the block.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/11fig32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-32: The resulting state of the caches after Core 1 next reads the
    block*'
  prefs: []
  type: TYPE_NORMAL
- en: 11.6.3 Implementing Cache Coherency Protocols
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To implement a cache coherency protocol, a processor needs some mechanism to
    identify when accesses to the other cores’ L1 cache contents require coherency
    state changes involving the other cores’ L1 cache contents. One way this mechanism
    is implemented is through *snooping* on a bus that is shared by all L1 caches.
    A snooping cache controller listens (or snoops) on the bus for reads or writes
    to blocks that it caches. Because every read and write request is in terms of
    a memory address, a snooping L1 cache controller can identify any read or write
    from another L1 cache for a block it stores, and can then respond appropriately
    based on the coherency protocol. For example, it can set the I flag on a cache
    line when it snoops a write to the same address by another L1 cache. This example
    is how a *write-invalidate protocol* would be implemented with snooping.
  prefs: []
  type: TYPE_NORMAL
- en: MSI and other similar protocols such as MESI and MOESI are write-invalidate
    protocols; that is, protocols that invalidate copies of cached entries on writes.
    Snooping can also be used by write-update cache coherency protocols, where the
    new value of a data is snooped from the bus and applied to update all copies stored
    in other L1 caches.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of snooping, a directory-based cache coherence mechanism can be used
    to trigger cache coherency protocols. This method scales better than snooping
    due to performance limitations of multiple cores sharing a single bus. However,
    directory-based mechanisms require more state to detect when memory blocks are
    shared, and are slower than snooping.
  prefs: []
  type: TYPE_NORMAL
- en: 11.6.4 More About Multicore Caching
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The benefits to performance of each core of a multicore processor having its
    own separate cache(s) at the highest levels of the memory hierarchy, which are
    used to store copies of only the program data and instructions that it executes,
    is worth the added extra complexity of the processor needing to implement a cache
    coherency protocol.
  prefs: []
  type: TYPE_NORMAL
- en: Although cache coherency solves the memory coherency problem on multicore processors
    with separate L1 caches, there is another problem that can occur as a result of
    cache coherency protocols on multicore processors. This problem, called *false
    sharing*, may occur when multiple threads of a single multithreaded parallel program
    are running simultaneously across the multiple cores and are accessing memory
    locations that are near to those accessed by other threads. In section 14.5, we
    discuss the false sharing problem and some solutions to it.
  prefs: []
  type: TYPE_NORMAL
- en: For more information and details about hardware caching on multicore processors,
    including different protocols and how they are implemented, refer to a computer
    architecture textbook.^([8](ch11.xhtml#fn11_8))
  prefs: []
  type: TYPE_NORMAL
- en: 11.7 Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This chapter explored the characteristics of computer storage devices and their
    trade-offs with respect to key measures like access latency, storage capacity,
    transfer latency, and cost. Because devices embody many disparate design and performance
    trade-offs, they naturally form a memory hierarchy, which arranges them according
    to their capacity and access time. At the top of the hierarchy, primary storage
    devices like CPU caches and main memory quickly provide data directly to the CPU,
    but their capacity is limited. Lower in the hierarchy, secondary storage devices
    like solid-state drives and hard disks offer dense bulk storage at the cost of
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Because modern systems require both high capacity and good performance, system
    designers build computers with multiple forms of storage. Crucially, the system
    must manage which storage device holds any particular chunk of data. Systems aim
    to store data that’s being actively used in faster storage devices, and they relegate
    infrequently used data to slower storage devices.
  prefs: []
  type: TYPE_NORMAL
- en: 'To determine which data is being used, systems rely on program data access
    patterns known as *locality*. Programs exhibit two important types of locality:
    *temporal locality*, whereby programs tend to access the same data repeatedly
    over time, and *spatial locality*, whereby programs tend to access data that is
    nearby other, previously accessed data.'
  prefs: []
  type: TYPE_NORMAL
- en: Locality serves as the basis for CPU caches, which store a small subset of main
    memory in fast storage directly on the CPU chip. When a program attempts to access
    main memory, the CPU first checks for the data in the cache; if it finds the data
    there, it can avoid the more costly trip to main memory.
  prefs: []
  type: TYPE_NORMAL
- en: When a program issues a request to read or write memory, it provides the address
    of the memory location that it wants to access. CPU caches use three sections
    of the bits in a memory address to identify which subset of main memory a cache
    line stores. The middle *index* bits of an address map the address to a storage
    location in the cache, the high-order *tag* bits uniquely identify which subset
    of memory the cache location stores, and the low-order *offset* bits identify
    which bytes of stored data the program wants to access.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, this chapter concluded by demonstrating how the Cachegrind tool can
    enable cache performance profiling for a running program. Cachegrind simulates
    a program’s interaction with the cache hierarchy and collects statistics about
    a program’s use of the cache (e.g., the hit and miss rates).
  prefs: []
  type: TYPE_NORMAL
- en: Notes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[1.](ch11.xhtml#rfn11_1) *[https://www.youtube.com/watch?v=9eyFDBPk4Yw](https://www.youtube.com/watch?v=9eyFDBPk4Yw)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[2.](ch11.xhtml#rfn11_2) *[https://en.wikipedia.org/wiki/Punched_card](https://en.wikipedia.org/wiki/Punched_card)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[3.](ch11.xhtml#rfn11_3) *[https://en.wikipedia.org/wiki/Magnetic_tape_data_storage](https://en.wikipedia.org/wiki/Magnetic_tape_data_storage)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[4.](ch11.xhtml#rfn11_4) *[https://en.wikipedia.org/wiki/Floppy_disk](https://en.wikipedia.org/wiki/Floppy_disk)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[5.](ch11.xhtml#rfn11_5) *[https://en.wikipedia.org/wiki/Optical_disc](https://en.wikipedia.org/wiki/Optical_disc)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[6.](ch11.xhtml#rfn11_6) *[https://en.wikipedia.org/wiki/Hard_disk_drive](https://en.wikipedia.org/wiki/Hard_disk_drive)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[7.](ch11.xhtml#rfn11_7) *[https://en.wikipedia.org/wiki/Flash_memory](https://en.wikipedia.org/wiki/Flash_memory)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[8.](ch11.xhtml#rfn11_8) One suggestion is “Computer Organization and Design:
    The Hardware and Software Interface,” by David A. Patterson and John L. Hennessy.'
  prefs: []
  type: TYPE_NORMAL
