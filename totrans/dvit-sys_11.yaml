- en: '11'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '11'
- en: '**STORAGE AND THE MEMORY HIERARCHY**'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**存储与内存层次结构**'
- en: '![image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/common.jpg)'
- en: 'Although designing and implementing efficient algorithms is typically the *most*
    critical aspect of writing programs that perform well, there’s another, often
    overlooked factor that can have a major impact on performance: memory. Perhaps
    surprisingly, two algorithms with the same asymptotic performance (number of steps
    in the worst case) run on the same inputs might perform very differently in practice
    due to the organization of the hardware on which they execute. Such differences
    generally stem from the algorithms’ memory accesses, particularly where they store
    data and the kinds of patterns they exhibit when accessing it. These patterns
    are referred to as *memory locality*, and to achieve the best performance, a program’s
    access patterns need to align well with the hardware’s memory arrangement.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管设计和实现高效算法通常是编写高性能程序中*最*关键的方面，但还有一个常常被忽视的因素，可能对性能产生重大影响：内存。或许令人惊讶的是，两个具有相同渐近性能（最坏情况下的步骤数）的算法，在相同输入下，可能由于执行硬件的组织方式而表现出截然不同的性能。这种差异通常源于算法的内存访问，特别是它们存储数据的位置以及它们访问数据时所表现出的模式。这些模式被称为*内存局部性*，为了获得最佳性能，程序的访问模式需要与硬件的内存布局良好对接。
- en: For example, consider the following two variations of a function for averaging
    the values in an *N* × *N* matrix. Despite both versions accessing the same memory
    locations an equal number of times (*N*²), Variation 1 executes about five times
    faster on real systems than Variation 2\. The difference arises from the patterns
    in which they access those memory locations. Toward the end of this chapter we
    analyze this example using the memory profiling tool *Cachegrind*.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑以下两种计算 *N* × *N* 矩阵平均值的函数变体。尽管这两种变体访问相同的内存位置次数相同（*N*²），但变体 1 在实际系统上执行的速度大约是变体
    2 的五倍。这个差异源于它们访问这些内存位置的模式。本文末尾，我们将使用内存分析工具*Cachegrind*来分析这个例子。
- en: Variation 1
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 变体 1
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Variation 2
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 变体 2
- en: '[PRE1]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Storage locations like registers, CPU caches, main memory, and files on disk
    all feature remarkably different access times, transfer rates, and storage capacities.
    When programming a high-performance application, it’s important to consider where
    data is stored and how frequently the program accesses each device’s data. For
    example, accessing a slow disk once as the program starts is rarely a major concern.
    On the other hand, accessing the disk frequently will slow down the program considerably.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 存储位置，如寄存器、CPU缓存、主内存和磁盘上的文件，都具有显著不同的访问时间、传输速率和存储容量。在编写高性能应用程序时，考虑数据的存储位置以及程序访问每个设备数据的频率是非常重要的。例如，程序启动时访问一次缓慢的磁盘通常不会成为主要问题。另一方面，频繁访问磁盘则会显著降低程序的运行速度。
- en: This chapter characterizes a diverse set of memory devices and describes how
    they’re organized in a modern PC. With that context, we’ll see how a collection
    of varied memory devices can be combined to exploit the locality found in a typical
    program’s memory access patterns.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章描述了各种内存设备，并介绍了它们在现代PC中的组织方式。通过这个背景，我们将看到如何将不同的内存设备组合起来，利用典型程序中的内存访问模式的局部性。
- en: 11.1 The Memory Hierarchy
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1 内存层次结构
- en: 'As we explore modern computer storage, a common pattern emerges: devices with
    higher capacities offer lower performance. Said another way, systems use devices
    that are fast and devices that store a large amount of data, but no single device
    does both. This trade-off between performance and capacity is known as the *memory
    hierarchy*, and [Figure 11-1](ch11.xhtml#ch11fig1) depicts the hierarchy visually.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们探索现代计算机存储时，一个常见的模式浮现出来：具有更高容量的设备通常提供较低的性能。换句话说，系统使用既快速又能存储大量数据的设备，但没有单一设备能同时做到这两点。这种性能与容量之间的权衡被称为*内存层次结构*，[图
    11-1](ch11.xhtml#ch11fig1)以图示的方式呈现了这一层次结构。
- en: 'Storage devices similarly trade cost and storage density: faster devices are
    more expensive, both in terms of bytes per dollar and operational costs (e.g.,
    energy usage). Consider that even though caches provide great performance, the
    cost (and manufacturing challenges) of building a CPU with a large enough cache
    to forego main memory makes such a design infeasible. Practical systems must utilize
    a combination of devices to meet the performance and capacity requirements of
    programs, and a typical system today incorporates most, if not all, of the devices
    described in [Figure 11-1](ch11.xhtml#ch11fig1).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 存储设备同样在成本和存储密度之间进行权衡：速度更快的设备更贵，无论是在每美元字节数还是运营成本（例如，能耗）方面。考虑到虽然缓存提供了很好的性能，但要构建一个足够大的缓存以放弃主内存的
    CPU，其成本（和制造挑战）使得这种设计不可行。实际系统必须利用多种设备的组合，以满足程序的性能和容量需求，今天的典型系统通常包括图[11-1](ch11.xhtml#ch11fig1)中描述的大部分设备，甚至是所有设备。
- en: '![image](../images/11fig01.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/11fig01.jpg)'
- en: '*Figure 11-1: The memory hierarchy*'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-1：内存层次结构*'
- en: The reality of the memory hierarchy is unfortunate for programmers, who would
    prefer to not worry about the performance implications of where their data resides.
    For example, when declaring an integer *in most applications*, a programmer ideally
    wouldn’t need to agonize over the differences between data stored in a cache or
    main memory. Requiring a programmer to micromanage which type of memory each variable
    occupies would be burdensome, although it may occasionally be worth the effort
    for certain small, performance-critical sections of code.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 内存层次的现实对程序员来说是不幸的，因为他们希望不必担心数据存储位置对性能的影响。例如，在大多数应用中声明一个整数时，程序员理想情况下不需要为数据是存储在缓存中还是主内存中而苦恼。要求程序员细致管理每个变量占据的内存类型会非常繁琐，尽管在某些小型、对性能要求严格的代码段中，这样做偶尔是值得的。
- en: Note that [Figure 11-1](ch11.xhtml#ch11fig1) categorizes *cache* as single entity,
    but most systems contain multiple levels of caches that form their own smaller
    hierarchy. For example, CPUs commonly incorporate a very small and fast *level
    one* (L1) cache, which sits relatively close to the ALU, and a larger and slower
    *level two* (L2) cache that resides farther away. Many multicore CPUs also share
    data between cores in a larger *level three* (L3) cache. Although the differences
    between the cache levels may matter to performance-conscious applications, this
    book considers just a single level of caching for simplicity.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，[图 11-1](ch11.xhtml#ch11fig1)将 *缓存* 分类为单一实体，但大多数系统包含多个级别的缓存，这些缓存形成自己的较小层次结构。例如，CPU
    通常会集成一个非常小且快速的 *一级*（L1）缓存，它相对靠近算术逻辑单元（ALU），以及一个更大且较慢的 *二级*（L2）缓存，位于更远的位置。许多多核
    CPU 还会在更大的 *三级*（L3）缓存中共享核心之间的数据。尽管缓存级别之间的差异可能对性能敏感的应用程序很重要，本书为了简化，假设只考虑一个缓存级别。
- en: Though this chapter primarily focuses on data movement between registers, CPU
    caches, and main memory, the next section characterizes common storage devices
    across the memory hierarchy. We examine disks and their role in the bigger picture
    of memory management later, in “Virtual Memory” on [page 639](ch13.xhtml#lev1_101).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本章主要关注寄存器、CPU 缓存和主内存之间的数据传输，下一节将描述跨内存层次结构的常见存储设备。我们稍后将在“虚拟内存”一节中，讨论磁盘及其在内存管理中的作用，见[第639页](ch13.xhtml#lev1_101)。
- en: 11.2 Storage Devices
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2 存储设备
- en: Systems designers classify devices in the memory hierarchy according to how
    programs access their data. *Primary storage* devices can be accessed directly
    by a program on the CPU. That is, the CPU’s assembly instructions encode the exact
    location of the data that the instructions should retrieve. Examples of primary
    storage include CPU registers and main memory (RAM), which assembly instructions
    reference directly (e.g., in IA32 assembly as `%reg` and `(%reg)`, respectively).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 系统设计者根据程序如何访问数据，将内存层次结构中的设备进行分类。*主存储*设备可以被程序直接访问。也就是说，CPU 的汇编指令编码了指令应该获取的数据的确切位置。主存储的例子包括
    CPU 寄存器和主内存（RAM），汇编指令会直接引用它们（例如，在 IA32 汇编中分别是 `%reg` 和 `(%reg)`）。
- en: In contrast, CPU instructions cannot directly refer to *secondary storage* devices.
    To access the contents of a secondary storage device, a program must first request
    that the device copy its data into primary storage (typically memory). The most
    familiar types of secondary storage devices are disk devices (e.g., hard disk
    drives and solid-state drives), which persistently store file data. Other examples
    include floppy disks, magnetic tape cartridges, or even remote file servers.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，CPU 指令不能直接引用*二级存储*设备。为了访问二级存储设备的内容，程序必须先请求将设备中的数据复制到主存储器中（通常是内存）。最常见的二级存储设备是磁盘设备（例如硬盘驱动器和固态硬盘），它们持久存储文件数据。其他例子还包括软盘、磁带盒，甚至远程文件服务器。
- en: Even though you may not have considered the distinction between primary and
    secondary storage in these terms before, it’s likely that you have encountered
    their differences in programs already. For example, after declaring and assigning
    ordinary variables (primary storage), a program can immediately use them in arithmetic
    operations. When working with file data (secondary storage), the program must
    read values from the file into memory variables before it can access them (see
    “File Input/Output” on [page 117](ch02.xhtml#lev2_33)).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你之前可能没有以这些术语区分主存储和二级存储，但你可能已经在程序中遇到它们的区别。例如，在声明和赋值普通变量（主存储）之后，程序可以立即在算术操作中使用它们。而在处理文件数据（二级存储）时，程序必须先将文件中的值读取到内存变量中，才能访问它们（请参见
    [第117页](ch02.xhtml#lev2_33) 的“文件输入/输出”）。
- en: 'Several other important criteria for classifying memory devices arise from
    their performance and capacity characteristics. The three most interesting measures
    are:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 分类记忆设备的其他几个重要标准来源于它们的性能和容量特性。最有趣的三个衡量标准是：
- en: '**Capacity**  The amount of data a device can store. Capacity is typically
    measured in bytes.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**容量** 设备可以存储的数据量。容量通常以字节为单位进行测量。'
- en: '**Latency**  The amount of time it takes for a device to respond with data
    after it has been instructed to perform a data retrieval operation. Latency is
    typically measured in either fractions of a second (e.g., milliseconds or nanoseconds)
    or CPU cycles.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**延迟** 设备在接到执行数据检索操作的指令后，响应并提供数据所需的时间。延迟通常以秒的分数（例如毫秒或纳秒）或 CPU 周期为单位进行测量。'
- en: '**Transfer rate**  The amount of data that can be moved between the device
    and main memory over some interval of time. Transfer rate is also known as *throughput*
    and is typically measured in bytes per second.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**传输速率** 设备与主内存之间在某段时间内可以移动的数据量。传输速率也称为*吞吐量*，通常以每秒字节数为单位进行测量。'
- en: 'Exploring the variety of devices in a modern computer reveals a huge disparity
    in device performance across all three of these measures. The performance variance
    primarily arises from two factors: *distance* and *variations in the technologies*
    used to implement the devices.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 探索现代计算机中各种设备揭示了在这三项衡量标准上设备性能的巨大差异。性能差异主要来源于两个因素：*距离*和*用于实现这些设备的技术差异*。
- en: Distance contributes because, ultimately, any data that a program wants to use
    must be available to the CPU’s arithmetic components (e.g., the ALU) for processing.
    CPU designers place registers close to the ALU to minimize the time it takes for
    a signal to propagate between the two. Thus, while registers can store only a
    few bytes and there aren’t many of them, the values stored are available to the
    ALU almost immediately! In contrast, secondary storage devices like disks transfer
    data to memory through various controller devices that are connected by longer
    wires. The extra distance and intermediate processing slows down secondary storage
    considerably.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 距离的影响在于，最终，任何程序想要使用的数据必须能够提供给 CPU 的算术组件（例如 ALU）进行处理。CPU 设计师将寄存器放置在靠近 ALU 的位置，以最小化信号在两者之间传播所需的时间。因此，虽然寄存器只能存储少量字节且数量不多，但存储的值几乎可以立即提供给
    ALU！相比之下，像磁盘这样的二级存储设备通过各种控制器设备将数据传输到内存，这些控制器设备通过较长的电线连接。额外的距离和中间处理大大减慢了二级存储的速度。
- en: GRACE HOPPER’S “NANOSECONDS”
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**格雷斯·霍普的“纳秒”**'
- en: When speaking to an audience, computing pioneer and US Navy Admiral Grace Hopper
    frequently handed out 11.8-inch strands of wire to audience members. These strands
    represented the maximum distance that an electrical signal travels in one nanosecond
    and were called “Grace Hopper nano-seconds.” She used them to describe the latency
    limitations of satellite communication and to demonstrate why computing devices
    need to be small in order to be fast. Recordings of Grace Hopper presenting her
    nanoseconds are available on YouTube.^([1](ch11.xhtml#fn11_1))
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在与观众交流时，计算机先驱、美国海军上将格蕾丝·霍普尔经常向观众分发11.8英寸的电线。这些电线代表了电信号在一纳秒内传播的最大距离，被称为“格蕾丝·霍普尔纳秒”。她用这些电线来描述卫星通信的延迟限制，并展示为什么计算设备需要小型化才能提高速度。格蕾丝·霍普尔讲解她的纳秒理论的录音可以在YouTube上找到。^([1](ch11.xhtml#fn11_1))
- en: The underlying technology also significantly affects device performance. Registers
    and caches are built from relatively simple circuits, consisting of just a few
    logic gates. Their small size and minimal complexity ensures that electrical signals
    can propagate through them quickly, reducing their latencies. On the opposite
    end of the spectrum, traditional hard disks contain spinning magnetic platters
    that store hundreds of gigabytes. Although they offer dense storage, their access
    latency is relatively high due to the requirements of mechanically aligning and
    rotating components into the correct positions.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 底层技术也会显著影响设备的性能。寄存器和缓存由相对简单的电路构成，仅由少数逻辑门组成。它们的小尺寸和最小复杂性确保电信号能够快速传播，减少延迟。在对立的一端，传统的硬盘包含旋转的磁性盘片，能够存储数百GB的数据。尽管它们提供了密集的存储，但由于需要机械地调整和旋转部件以对准到正确的位置，因此其访问延迟相对较高。
- en: The remainder of this section examines the details of primary and secondary
    storage devices and analyzes their performance characteristics.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的其余部分将详细探讨主存储和次存储设备，并分析它们的性能特征。
- en: 11.2.1 Primary Storage
  id: totrans-33
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.2.1 主存储器
- en: Primary storage devices consist of *random access memory* (RAM), which means
    the time it takes to access data is not affected by the data’s location in the
    device. That is, RAM doesn’t need to worry about things like moving parts into
    the correct position or rewinding tape spools. There are two widely used types
    of RAM, *static RAM* (SRAM) and *dynamic RAM* (DRAM), and both play an important
    role in modern computers. [Table 11-1](ch11.xhtml#ch11tab1) characterizes the
    performance measures of common primary storage devices and the types of RAM they
    use.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 主存储设备由*随机存取内存*（RAM）组成，这意味着访问数据所需的时间不受数据在设备中的位置影响。也就是说，RAM不需要担心像将部件移到正确位置或倒带磁带卷轴这样的事情。RAM有两种广泛使用的类型，*静态RAM*（SRAM）和*动态RAM*（DRAM），它们在现代计算机中都扮演着重要角色。[表11-1](ch11.xhtml#ch11tab1)列出了常见主存储设备的性能衡量标准及其使用的RAM类型。
- en: '**Table 11-1:** Primary Storage Device Characteristics of a Typical 2022 Workstation'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**表11-1：** 典型2022工作站的主存储设备特性'
- en: '| **Device** | **Capacity** | **Approx. Latency** | **RAM Type** |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| **设备** | **容量** | **大致延迟** | **RAM类型** |'
- en: '| Register | 4–8 bytes | < 1 ns | SRAM |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 寄存器 | 4–8字节 | < 1 纳秒 | SRAM |'
- en: '| CPU cache | 1–32 megabytes | 5 ns | SRAM |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| CPU缓存 | 1–32兆字节 | 5纳秒 | SRAM |'
- en: '| Main memory | 4–64 gigabytes | 100 ns | DRAM |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 主内存 | 4–64吉字节 | 100纳秒 | DRAM |'
- en: SRAM stores data in small electrical circuits (e.g., latches—see “RS Latch”
    on [page 257](ch05.xhtml#lev3_49)). SRAM is typically the fastest type of memory,
    and designers integrate it directly into a CPU to build registers and caches.
    SRAM is relatively expensive in its cost to build, cost to operate (e.g., power
    consumption), and in the amount of space it occupies. Collectively, those costs
    limit the amount of SRAM storage that a CPU can include.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: SRAM将数据存储在小型电路中（例如，锁存器——参见“RS锁存器”在[第257页](ch05.xhtml#lev3_49)）。SRAM通常是最快的内存类型，设计师将其直接集成到CPU中，以构建寄存器和缓存。SRAM在制造成本、运营成本（如功耗）以及所占空间方面相对昂贵。总体而言，这些成本限制了CPU可以包含的SRAM存储量。
- en: DRAM stores data using electrical components called *capacitors* that hold an
    electrical charge. It’s called “dynamic” because a DRAM system must frequently
    refresh the charge of its capacitors to maintain a stored value. Modern systems
    use DRAM to implement main memory on modules that connect to the CPU via a high-speed
    interconnect called the *memory bus*.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: DRAM使用名为*电容器*的电子元件存储数据，这些电容器能够储存电荷。它被称为“动态”是因为DRAM系统必须频繁刷新电容器的电荷才能保持存储的值。现代系统使用DRAM来实现主内存，这些内存模块通过一种称为*内存总线*的高速互连与CPU连接。
- en: '[Figure 11-2](ch11.xhtml#ch11fig2) illustrates the positions of primary storage
    devices relative to the memory bus. To retrieve a value from memory, the CPU puts
    the address of the data it would like to retrieve on the memory bus and signals
    that the memory modules should perform a read. After a short delay, the memory
    module sends the value stored at the requested address across the bus to the CPU.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11-2](ch11.xhtml#ch11fig2)说明了相对于内存总线的主存储设备的位置。为了从内存中检索值，CPU将要检索数据的地址放在内存总线上，并发出信号，要求内存模块执行读取操作。经过短暂延迟后，内存模块将存储在请求地址处的值通过总线发送到CPU。'
- en: '![image](../images/11fig02.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/11fig02.jpg)'
- en: '*Figure 11-2: Primary storage and memory bus architecture*'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-2：主存储和内存总线架构*'
- en: Even though the CPU and main memory are physically just a few inches away from
    each other, data must travel through the memory bus when it moves between the
    CPU and main memory. The extra distance and circuitry between them increases the
    latency and reduces the transfer rate of main memory relative to on-CPU storage.
    As a result, the memory bus is sometimes referred to as the *von Neumann bottleneck*.
    Of course, despite its lower performance, main memory remains an essential component
    because it stores several orders of magnitude more data than can fit on the CPU.
    Consistent with other forms of storage, there’s a clear trade-off between capacity
    and speed.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管CPU和主存储器在物理上相距几英寸，但数据在CPU和主存储器之间移动时必须经过内存总线。它们之间的额外距离和电路增加了延迟，并减少了相对于CPU存储的主存储器传输速率。因此，内存总线有时被称为*冯·诺依曼瓶颈*。当然，尽管性能较低，主存储器仍然是一个必不可少的组件，因为它存储的数据量比CPU能容纳的多几个数量级。与其他存储形式一致，容量和速度之间存在明显的权衡。
- en: '*CPU cache* (pronounced “cash”) occupies the middle ground between registers
    and main memory, both physically and in terms of its performance and capacity
    characteristics. A CPU cache typically stores a few kilobytes to megabytes of
    data directly on the CPU, but physically, caches are not quite as close to the
    ALU as registers. Thus, caches are faster to access than main memory, but they
    require a few more cycles than registers to make data available for computation.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*CPU缓存*（发音为“缓存”）位于寄存器和主存储器之间的中间地带，无论从物理上还是性能和容量特性上都是如此。CPU缓存通常直接存储几千字节到几兆字节的数据在CPU上，但物理上，缓存不像寄存器那样接近ALU。因此，缓存访问速度比主存储器快，但相比寄存器，它们需要更多周期将数据提供给计算使用。'
- en: Rather than the programmer explicitly loading values into the cache, control
    circuitry within the CPU automatically stores a subset of the main memory’s contents
    in the cache. CPUs strategically control which subset of main memory they store
    in caches so that as many memory requests as possible can be serviced by the (much
    higher performance) cache. Later sections of this chapter describe the design
    decisions that go into cache construction and the algorithms that should govern
    which data they store.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: CPU内部的控制电路自动将主存储器内容的一个子集存储到缓存中，而不是程序员显式地将值加载到缓存中。CPU策略性地控制将主存储器的哪个子集存储在缓存中，以便尽可能多地通过（性能更高的）缓存服务内存请求。本章后续部分描述了影响缓存构建和管理存储数据算法的设计决策。
- en: Real systems incorporate multiple levels of caches that behave like their own
    miniature version of the memory hierarchy. That is, a CPU might have a very small
    and fast *L1 cache* that stores a subset of a slightly larger and slower *L2 cache*,
    which in turns stores a subset of a larger and slower *L3 cache*. The remainder
    of this section describes a system with just a single cache, but the interaction
    between caches on a real system behaves much like the interaction between a single
    cache and main memory detailed later.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 真实系统包含多个层次的缓存，它们表现得像自己的小型内存层次结构版本。也就是说，CPU可能有一个非常小而快速的*L1缓存*，它存储稍大而较慢的*L2缓存*的子集，后者又存储了更大而更慢的*L3缓存*的子集。本节其余部分描述了一个仅具有单个缓存的系统，但是实际系统中缓存之间的交互行为与稍后详细描述的单个缓存与主存储器之间的交互行为非常相似。
- en: '**Note**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: If you’re curious about the sizes of the caches and main memory on your system,
    the `lscpu` command prints information about the CPU (including its cache capacities).
    Running `free -m` shows the system’s main memory capacity in megabytes.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对系统中缓存和主存储器的大小感兴趣，可以使用`lscpu`命令打印有关CPU（包括其缓存容量）的信息。运行`free -m`可以显示系统的主存储器容量（以兆字节为单位）。
- en: 11.2.2 Secondary Storage
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.2.2 次级存储
- en: Physically, secondary storage devices connect to a system even farther away
    from the CPU than main memory. Compared to most other computer equipment, secondary
    storage devices have evolved dramatically over the years, and they continue to
    exhibit more diverse designs than other components. The iconic punch card^([2](ch11.xhtml#fn11_2))
    allowed a human operator to store data by making small holes in a thick piece
    of paper, similar to an index card. Punch cards, whose design dates back to the
    US census of 1890, faithfully stored user data (often programs) through the 1960s
    and into the 1970s.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 从物理上讲，二级存储设备与主存相比距离CPU更远。与大多数其他计算机设备相比，二级存储设备多年来经历了显著的演变，并且它们的设计通常比其他组件更加多样化。标志性的打孔卡^([2](ch11.xhtml#fn11_2))允许操作员通过在厚纸上打孔来存储数据，类似于索引卡。打孔卡的设计可追溯到1890年美国人口普查，在1960年代和1970年代，打孔卡一直忠实地存储用户数据（通常是程序）。
- en: A tape drive^([3](ch11.xhtml#fn11_3)) stores data on a spool of magnetic tape.
    Although they generally offer good storage density (lots of information in a small
    size) for a low cost, tape drives are slow to access because they must wind the
    spool to the correct location. Although most computer users don’t encounter them
    often anymore, tape drives are still frequently used for bulk storage operations
    (e.g., large data backups) in which reading the data back is expected to be rare.
    Modern tape drives arrange the magnetic tape spool into small cartridges for ease
    of use.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 磁带驱动器^([3](ch11.xhtml#fn11_3))将数据存储在磁带卷轴上。尽管它们通常以较低的成本提供良好的存储密度（在小尺寸中存储大量信息），但由于需要将卷轴绕到正确位置，磁带驱动器的访问速度较慢。尽管如今大多数计算机用户不再经常接触到它们，磁带驱动器仍然广泛用于大规模存储操作（如大规模数据备份），在这些操作中，读取数据的频率较低。现代磁带驱动器将磁带卷轴安排成小型卡带，便于使用。
- en: '![image](../images/11fig03.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/11fig03.jpg)'
- en: '*Figure 11-3: Example photos of (a) a punch card, (b) a magnetic tape spool,
    and (c) a variety of floppy disk sizes. Images from Wikipedia.*'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*图11-3：打孔卡（a）、磁带卷轴（b）和各种软盘尺寸（c）的示例照片。图片来自Wikipedia。*'
- en: Removable media like floppy disks^([4](ch11.xhtml#fn11_4)) and optical discs^([5](ch11.xhtml#fn11_5))
    are another popular form of secondary storage. Floppy disks contain a spindle
    of magnetic recording media that rotates over a disk head that reads and writes
    its contents. [Figure 11-3](ch11.xhtml#ch11fig3) shows photos of a punch card,
    a tape drive, and a floppy disk. Optical discs (e.g., CD, DVD, and Blu-ray) store
    information via small indentations on the disc. The drive reads a disc by shining
    a laser at it, and the presence or absence of indentations causes the beam to
    reflect (or not), encoding zeros and ones.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 可移动介质，如软盘^([4](ch11.xhtml#fn11_4))和光盘^([5](ch11.xhtml#fn11_5))是另一种流行的二级存储形式。软盘包含一个旋转的磁性记录介质轴，磁头在其上方读取和写入数据。[图11-3](ch11.xhtml#ch11fig3)展示了打孔卡、磁带驱动器和软盘的照片。光盘（如CD、DVD和Blu-ray）通过在盘面上做小的凹痕来存储信息。驱动器通过向光盘照射激光来读取盘片，凹痕的存在与否会影响激光束的反射（或不反射），从而编码成零和一。
- en: 11.2.2.1 Modern Secondary Storage
  id: totrans-57
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 11.2.2.1 现代二级存储
- en: '**Table 11-2:** Secondary Storage Device Characteristics of a Typical 2022
    Workstation'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**表11-2：** 典型2022年工作站的二级存储设备特性'
- en: '| **Device** | **Capacity** | **Latency** | **Transfer Rate** |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| **设备** | **容量** | **延迟** | **传输速率** |'
- en: '| Flash disk | 0.5–2 terabytes | 0.1–1 ms | 200–3,000 megabytes/second |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 闪存 | 0.5–2 terabytes | 0.1–1 毫秒 | 200–3,000 兆字节/秒 |'
- en: '| Traditional hard disk | 0.5–10 terabytes | 5–10 ms | 100–200 megabytes/second
    |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 传统硬盘 | 0.5–10 terabytes | 5–10 毫秒 | 100–200 兆字节/秒 |'
- en: '| Remote network server | Varies considerably | 20–200 ms | Varies considerably
    |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 远程网络服务器 | 可变 | 20–200 毫秒 | 可变 |'
- en: '[Table 11-2](ch11.xhtml#ch11tab2) characterizes the secondary storage devices
    commonly available to workstations today. [Figure 11-4](ch11.xhtml#ch11fig4) displays
    how the path from secondary storage to main memory generally passes through several
    intermediate device controllers. For example, a typical hard disk connects to
    a Serial ATA controller, which connects to the system I/O controller, which in
    turn connects to the memory bus. These intermediate devices make disks easier
    to use by abstracting the disk communication details from the OS and programmer.
    However, they also introduce transfer delays as data flows through the additional
    devices.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[表11-2](ch11.xhtml#ch11tab2) 描述了今天工作站常见的二级存储设备。[图11-4](ch11.xhtml#ch11fig4)
    展示了从二级存储到主内存的路径通常如何通过几个中间设备控制器。例如，典型的硬盘连接到串行ATA控制器，再连接到系统I/O控制器，最后连接到内存总线。这些中间设备通过将磁盘通信的细节从操作系统和程序员中抽象出来，使磁盘更易于使用。然而，它们也引入了传输延迟，因为数据需要通过额外的设备流动。'
- en: '![image](../images/11fig04.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/11fig04.jpg)'
- en: '*Figure 11-4: Secondary storage and I/O bus architecture*'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '*图11-4：二级存储和I/O总线架构*'
- en: The two most common secondary storage devices today are *hard disk drives* (HDDs)
    and flash-based *solid-state drives* (SSDs). A hard disk consists of a few flat,
    circular platters made from a material that allows for magnetic recording. The
    platters rotate quickly, typically at speeds between 5,000 and 15,000 revolutions
    per minute. As the platters spin, a small mechanical arm with a disk head at the
    tip moves across the platter to read or write data on concentric tracks (regions
    of the platter located at the same diameter).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 今天最常见的两种二级存储设备是*硬盘驱动器*（HDD）和基于闪存的*固态硬盘*（SSD）。硬盘由几个平坦的圆盘组成，这些圆盘由可进行磁性记录的材料制成。盘片快速旋转，通常的转速在每分钟5,000到15,000转之间。当盘片旋转时，一只小型机械臂上带有一个磁头，磁头在盘片的同心轨道（位于相同直径的区域）上移动，以读取或写入数据。
- en: '[Figure 11-5](ch11.xhtml#ch11fig5) illustrates the major components of a hard
    disk.^([6](ch11.xhtml#fn11_6)) Before accessing data, the disk must align the
    disk head with the track that contains the desired data. Alignment requires extending
    or retracting the arm until the head sits above the track. Moving the disk arm
    is called *seeking*, and because it requires mechanical motion, seeking introduces
    a small *seek time* delay to accessing data (a few milliseconds). When the arm
    is in the correct position, the disk must wait for the platter to rotate until
    the disk head is directly above the location that stores the desired data. This
    introduces another short delay (a few more milliseconds) known as *rotational
    latency*. Thus, due to their mechanical characteristics, hard disks exhibit significantly
    higher access latencies than the primary storage devices described earlier.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11-5](ch11.xhtml#ch11fig5) 展示了硬盘的主要组件。^([6](ch11.xhtml#fn11_6)) 在访问数据之前，磁盘必须将磁头与包含所需数据的轨道对齐。对齐需要通过伸缩机械臂，直到磁头正好位于轨道上方。移动磁盘臂被称为*寻道*，由于它需要机械运动，寻道过程会引入一个小的*寻道时间*延迟（几毫秒）。当臂到达正确的位置时，磁盘必须等待盘片旋转，直到磁头直接位于存储所需数据的位置上方。这又引入了另一个短暂的延迟（几毫秒），称为*旋转延迟*。因此，由于其机械特性，硬盘的访问延迟显著高于前面描述的主要存储设备。'
- en: '![image](../images/11fig05.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/11fig05.jpg)'
- en: '*Figure 11-5: The major components of a hard disk drive*'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*图11-5：硬盘驱动器的主要组件*'
- en: In the past few years, SSDs, which have no moving parts (and thus lower latency),
    have quickly risen to prominence. They are known as solid-state drives because
    they don’t rely on mechanical movement. Although several solid-state technologies
    exist, flash memory^([7](ch11.xhtml#fn11_7)) reigns supreme in commercial SSD
    devices. The technical details of flash memory are beyond the scope of this book,
    but it suffices to say that flash-based devices allow for reading, writing, and
    erasing data at speeds faster than traditional hard disks. Though they don’t yet
    store data as densely as their mechanical counterparts, they’ve largely replaced
    spinning disks in most consumer devices like laptops.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 过去几年中，固态硬盘（SSD）由于没有活动部件（因此延迟较低），迅速崭露头角。它们被称为固态硬盘，因为它们不依赖机械运动。虽然存在多种固态技术，但闪存^([7](ch11.xhtml#fn11_7))
    在商业SSD设备中占据主导地位。闪存的技术细节超出了本书的范围，但可以简单地说，基于闪存的设备能够以比传统硬盘更快的速度读取、写入和擦除数据。尽管它们的存储密度尚不及机械硬盘，但在大多数消费类设备（如笔记本电脑）中，它们已经大体取代了旋转硬盘。
- en: 11.3 Locality
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.3 本地性
- en: Because memory devices vary considerably in their performance characteristics
    and storage capacities, modern systems integrate several forms of storage. Luckily,
    most programs exhibit common memory access patterns, known as *locality*, and
    designers build hardware that exploits good locality to automatically move data
    into an appropriate storage location. Specifically, a system improves performance
    by moving the subset of data that a program is actively using into storage that
    lives close to the CPU’s computation circuitry (e.g., in a register or CPU cache).
    As necessary data moves up the hierarchy toward the CPU, unused data moves farther
    away to slower storage until the program needs it.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 由于内存设备在性能特征和存储容量上差异很大，现代系统集成了多种存储形式。幸运的是，大多数程序展示了常见的内存访问模式，称为*局部性*，设计者构建硬件来利用良好的局部性，自动将数据移到适当的存储位置。具体来说，系统通过将程序正在积极使用的数据子集移到靠近CPU计算电路的存储中（例如，在寄存器或CPU缓存中）来提高性能。随着必要数据向CPU靠近，未使用的数据则移向更远、速度较慢的存储，直到程序需要它。
- en: To a system designer, building a system that exploits locality represents an
    abstraction problem. The system provides an abstract view of memory devices such
    that it appears to programmers as if they have the sum of all memory capacities
    with the performance characteristics of fast on-chip storage. Of course, providing
    this rosy illusion to users can’t be accomplished perfectly, but by exploiting
    program locality, modern systems achieve good performance for most well-written
    programs.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 对于系统设计师来说，构建一个能够利用局部性的系统代表着一个抽象问题。系统提供了内存设备的抽象视图，使得程序员看起来他们拥有所有内存容量，并且具有快速片上存储的性能特征。当然，向用户提供这种美好的假象无法做到完美，但通过利用程序的局部性，现代系统能为大多数写得很好的程序提供良好的性能。
- en: 'Systems primarily exploit two forms of locality:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 系统主要利用两种局部性：
- en: '**Temporal locality**  Programs tend to access the same data repeatedly over
    time. That is, if a program has used a variable recently, it’s likely to use that
    variable again soon.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**时间局部性** 程序往往会在一段时间内反复访问相同的数据。也就是说，如果程序最近使用过一个变量，那么很可能会很快再次使用该变量。'
- en: '**Spatial locality**  Programs tend to access data that is nearby other, previously
    accessed data. “Nearby” here refers to the data’s memory address. For example,
    if a program accesses data at addresses *N* and *N* + 4, it’s likely to access
    *N* + 8 soon.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**空间局部性** 程序倾向于访问与其他之前访问过的数据相邻的数据。“相邻”指的是数据的内存地址。例如，如果程序访问了地址为*N*和*N* + 4的数据，那么它很可能会很快访问*N*
    + 8的数据。'
- en: 11.3.1 Locality Examples in Code
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.3.1 代码中的局部性示例
- en: 'Fortunately, common programming patterns exhibit both forms of locality quite
    frequently. Take the following function, for example:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，常见的编程模式经常表现出这两种局部性。例如，考虑以下函数：
- en: '[PRE2]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In this code, the repetitive nature of the `for` loop introduces temporal locality
    for `i`, `len`, `sum`, and `array` (the base address of the array), as the program
    accesses each of these variables within every loop iteration. Exploiting this
    temporal locality allows a system to load each variable from main memory into
    the CPU cache only once. Every subsequent access can be serviced out of the significantly
    faster cache.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，`for`循环的重复性特征为`i`、`len`、`sum`和`array`（数组的基地址）引入了时间局部性，因为程序在每次循环迭代中都会访问这些变量。利用这种时间局部性，系统可以仅将每个变量从主内存加载到CPU缓存中一次。之后的每次访问都可以从速度更快的缓存中服务。
- en: Accesses to the array’s contents also benefit from spatial locality. Even though
    the program accesses each array element only once, a modern system loads more
    than one `int` at a time from memory to the CPU cache. That is, accessing the
    first array index fills the cache with not only the first integer, but also the
    next few integers after it, too. Exactly *how many* additional integers get moved
    into the cache depends on the cache’s *block size*—the amount of data transferred
    into the cache at once.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 对数组内容的访问同样受益于空间局部性。尽管程序每次只访问一次数组元素，现代系统通常会一次性从内存加载多个`int`到CPU缓存中。也就是说，访问第一个数组索引时，不仅会加载第一个整数，还会加载紧随其后的几个整数。具体加载多少个额外的整数进入缓存，取决于缓存的*块大小*——即每次向缓存转移的数据量。
- en: For example, with a 16-byte block size, a system copies four integers from memory
    to the cache at a time. Thus, accessing the first integer incurs the relatively
    high cost of accessing main memory, but the accesses to the next three are served
    out of cache, even if the program has never accessed them previously.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，使用 16 字节的块大小，系统一次将四个整数从内存复制到缓存中。因此，访问第一个整数会产生相对较高的主内存访问成本，但接下来的三个整数会从缓存中提供，即使程序之前从未访问过它们。
- en: 'In many cases, a programmer can help a system by intentionally writing code
    that exhibits good locality patterns. For example, consider the nested loops that
    access every element of an *N* × *N* matrix (this same example appeared in this
    chapter’s introduction):'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，程序员可以通过故意编写展示良好局部性模式的代码来帮助系统。例如，考虑访问每个元素的嵌套循环，一个 *N* × *N* 矩阵（这个例子在本章的介绍中也有出现）：
- en: Version 1
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 版本 1
- en: '[PRE3]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Version 2
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 版本 2
- en: '[PRE4]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In both versions, the loop variables (`i` and `j`) and the accumulator variable
    (`total`) exhibit good temporal locality because the loops repeatedly use them
    in every iteration. Thus, when executing this code, a system would store those
    variables in fast on-CPU storage locations to provide good performance.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两个版本中，循环变量（`i` 和 `j`）以及累加变量（`total`）展示了良好的时间局部性，因为这些变量在每次迭代中都会被反复使用。因此，当执行这段代码时，系统会将这些变量存储在快速的
    CPU 存储位置，以提供良好的性能。
- en: However, due to the row-major order organization of a matrix in memory (see
    “Two-Dimensional Array Memory Layout” on [page 86](ch02.xhtml#lev3_13)), the first
    version of the code executes about five times faster than the second version.
    The disparity arises from the difference in spatial locality—the first version
    accesses the matrix’s values sequentially in memory (i.e., in order of consecutive
    memory addresses). Thus, it benefits from a system that loads large blocks from
    memory into the cache because it pays the cost of going to memory only once for
    every block of values.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于矩阵在内存中的行主序组织（参见 [第 86 页](ch02.xhtml#lev3_13) 的“二维数组内存布局”），第一版本的代码执行速度大约比第二版本快五倍。这个差异源自空间局部性的不同——第一版本按顺序在内存中访问矩阵的值（即，按照连续的内存地址顺序）。因此，它受益于系统从内存中加载大块数据到缓存中，因为每访问一块值时，它只需一次访问内存。
- en: The second version accesses the matrix’s values by repeatedly jumping between
    rows across nonsequential memory addresses. It *never* reads from the same cache
    block in subsequent memory accesses, so it looks to the cache like the block isn’t
    needed. Thus, it pays the cost of going to memory for every matrix value it reads.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 第二版本通过在非连续的内存地址之间反复跳跃来访问矩阵的值。它 *从未* 在随后的内存访问中读取同一个缓存块，因此看起来缓存不需要该块数据。因此，每次读取矩阵值时，它都需要去访问内存。
- en: This example illustrates how a programmer can affect the system-level costs
    of a program’s execution. Keep these principles in mind when writing high-performance
    applications, particularly those that access arrays in a regular pattern.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子说明了程序员如何影响程序执行时的系统级成本。在编写高性能应用程序时，特别是那些以规则模式访问数组的程序，牢记这些原则。
- en: 11.3.2 From Locality to Caches
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.3.2 从局部性到缓存
- en: 'To help illustrate how the concepts of temporal and spatial locality enable
    cache designs, we’ll adopt an example scenario with familiar real-world objects:
    books. Suppose that Fiona does all of her homework at a desk in her dorm room,
    and the desk has a small amount of space that can store only three books. Just
    outside her room she keeps a bookshelf, which has much more space than the desk.
    Finally, across campus her college has a library with a huge variety of books.
    The “book storage hierarchy” in this example might look something like [Figure
    11-6](ch11.xhtml#ch11fig6). Given this scenario, we’ll explore how locality can
    help guide which storage location Fiona should use to store her books.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助说明时间局部性和空间局部性如何支持缓存设计，我们将采用一个具有实际世界物品的例子场景：书籍。假设 Fiona 在她宿舍的桌子上做所有作业，而桌子空间有限，只能存放三本书。在她房间外面，她有一排书架，比桌子有更多的存储空间。最后，在校园的另一边，她所在的大学有一个图书馆，里面有大量种类繁多的书籍。在这个例子中，“书籍存储层次结构”可能看起来像是
    [图 11-6](ch11.xhtml#ch11fig6)。基于这个场景，我们将探讨局部性如何帮助指导 Fiona 应该选择哪个存储位置来存放她的书籍。
- en: '![image](../images/11fig06.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/11fig06.jpg)'
- en: '*Figure 11-6: A hypothetical book storage hierarchy*'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-6：假设的书籍存储层次结构*'
- en: 11.3.3 Temporal Locality
  id: totrans-96
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.3.3 时间局部性
- en: 'Temporal locality suggests that, if there’s a book Fiona uses frequently, she
    should keep it as close to her desk as possible. If she occasionally needs to
    move it to the shelf to clear up temporary work space, the cost isn’t too high,
    but it would be silly to take a book back to the library if she’s just going to
    need it again the next day. The inverse is also true: if there’s a book taking
    up valuable space on her desk or shelf, and she hasn’t used it for quite a while,
    that book seems like a good candidate for returning to the library.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 时间局部性表明，如果有一本书是 Fiona 经常使用的，她应该将它尽可能靠近她的桌子。如果她偶尔需要将它移到书架上以腾出临时工作空间，这个成本不高，但如果她第二天就要再用这本书，把它拿回图书馆就显得很愚蠢。反过来也是如此：如果有一本书占据了她桌子或书架上的宝贵空间，而她已经很久没有使用它，那本书就像是一个适合归还图书馆的候选书籍。
- en: So, which books should Fiona move to her precious desk space? In this example,
    real students would probably look at their upcoming assignments and select the
    books that they expect to be most useful. In other words, to make the best storage
    decision, they would ideally need information about *future usage*.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，Fiona 应该将哪些书移到她宝贵的桌面空间呢？在这个例子中，真正的学生可能会查看他们即将进行的任务，并选择他们预计最有用的书籍。换句话说，为了做出最佳存储决策，他们理想情况下需要关于*未来使用*的信息。
- en: Unfortunately, hardware designers haven’t discovered how to build circuits that
    can predict the future. As an alternative to prediction, one could instead imagine
    a system that asks the programmer or user to inform the system in advance how
    a program will use data so that it’s placement could be optimized. Such a strategy
    may work well in specialized applications (e.g., large databases) that exhibit
    *very* regular access patterns. However, in a general-purpose system like a personal
    computer, requiring advance notice from the user is too large a burden—many users
    would not want to (or would be unable to) provide enough detail to help the system
    make good decisions.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，硬件设计师尚未发现如何构建能够预测未来的电路。作为预测的替代方法，可以设想一个系统，要求程序员或用户提前告知系统程序如何使用数据，以便优化其放置。这种策略在专门的应用程序（例如，大型数据库）中可能效果很好，因为这些应用展示了*非常*规律的访问模式。然而，在像个人计算机这样的通用系统中，要求用户提前通知就显得负担过重——许多用户可能不愿意（或者无法）提供足够的细节来帮助系统做出正确的决策。
- en: 'Thus, instead of relying on future access information, systems look to the
    past as a predictor of what will *likely* happen in the future. Applying this
    idea to the book example suggests a relatively simple (but still quite effective)
    strategy for governing book storage spaces:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，系统并不是依赖未来的访问信息，而是看向过去，作为预测未来*可能*发生事情的依据。将这个想法应用到书籍的例子中，提出了一种相对简单（但仍然非常有效）的方法来管理书籍存储空间：
- en: When Fiona needs to use a book, she retrieves it from wherever it currently
    is and moves it to her desk.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当 Fiona 需要使用一本书时，她从当前所在的位置取出这本书，并把它移到她的桌子上。
- en: If the desk is already full, she moves the book that she used *least recently*
    (that is, the book that has been sitting on the desk untouched for the longest
    amount of time) to her shelf.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果桌子已经满了，她会将最近*最不常用*的书（即那本在桌子上放得最久、没有被动过的书）移到书架上。
- en: If the shelf is full, she returns the shelf’s least recently used book to the
    library to free up space.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果书架已满，她会将书架上最不常用的书归还到图书馆，以腾出空间。
- en: 'Even though this scheme may not be perfect, the simplicity makes it attractive.
    All it requires is the ability to move books between storage locations and a small
    amount of metainformation regarding the order in which books were previously used.
    Furthermore, this scheme captures the two initial temporal locality objectives
    well:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 即使这个方案可能并不完美，但它的简洁性使其具有吸引力。它所需要的只是能够在存储位置之间移动书籍的能力，以及少量关于书籍先前使用顺序的元信息。此外，这个方案很好地捕捉了最初的两个时间局部性目标：
- en: Frequently used books are likely to remain on the desk or shelf, preventing
    unnecessary trips to the library.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经常使用的书很可能会一直留在桌子或书架上，避免了不必要的去图书馆的往返。
- en: Infrequently used books eventually become the least recently used book, at which
    point returning them to the library makes sense.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 很少使用的书最终会成为最近最不常用的书，这时把它们归还到图书馆就显得有意义。
- en: 'Applying this strategy to primary storage devices looks remarkably similar
    to the book example: as data is loaded into CPU registers from main memory, make
    room for it in the CPU cache. If the cache is already full, make room in the cache
    by *evicting* the least recently used cache data to main memory. In the following
    caching section, we’ll explore the details of how such mechanisms are built in
    to modern caching systems.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 将这一策略应用于主存储设备，看起来与书本中的示例非常相似：当数据从主内存加载到 CPU 寄存器时，为其在 CPU 缓存中腾出空间。如果缓存已经满了，则通过*驱逐*最近最少使用的缓存数据到主内存来腾出空间。在接下来的缓存部分中，我们将探讨现代缓存系统是如何构建这些机制的细节。
- en: 11.3.4 Spatial Locality
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.3.4 空间局部性
- en: Spatial locality suggests that, when making a trip to the library, Fiona should
    retrieve more than one book to reduce the likelihood of future library trips.
    Specifically, she should retrieve additional books that are “nearby” the one she
    needs because those that are nearby seem like good candidates for books that might
    otherwise turn into additional library visits.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 空间局部性表明，当去图书馆时，Fiona 应该取更多的书，以减少未来再次去图书馆的可能性。具体来说，她应该取一些“邻近”她所需要的书的书，因为那些邻近的书看起来是避免额外去图书馆的好候选。
- en: Suppose that she’s taking a literature course on the topic of Shakespeare’s
    histories. If in the first week of the course she’s assigned to read *Henry VI,
    Part I*, when she finds herself in the library to retrieve it, she’s likely to
    also find Parts II and III close by on the shelves. Even if she doesn’t yet know
    whether the course will assign those other two parts, it’s not unreasonable to
    think that she *might* need them. That is, the likelihood of needing them is much
    higher than a random book in the library, specifically because they are nearby
    the book she does need.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 假设她正在上一个关于莎士比亚历史剧的文学课程。如果在课程的第一周，她被分配阅读*亨利六世 第一部分*，当她在图书馆取书时，她很可能会在书架上找到第二部分和第三部分。即使她还不知道课程是否会分配那两部分，认为她*可能*需要它们也是合情合理的。也就是说，需要它们的可能性远高于图书馆里随机一本书，特别是因为它们就近于她确实需要的那本书。
- en: In this scenario, the likelihood increases due to the way libraries arrange
    books on shelves, and programs similarly organize data in memory. For example,
    a programming construct like an array or a `struct` stores a collection of related
    data in a contiguous region of memory. When iterating over consecutive elements
    in an array, there is clearly a spatial pattern in the accessed memory addresses.
    Applying these spatial locality lessons to primary storage devices implies that,
    when retrieving data from main memory, the system should also retrieve the data
    immediately surrounding it.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，由于图书馆排列书籍的方式，可能性会增加，程序也类似地在内存中组织数据。例如，像数组或`struct`这样的编程结构将一组相关数据存储在内存中的连续区域。当遍历数组中的连续元素时，访问的内存地址显然具有空间模式。将这些空间局部性的经验应用于主存储设备，意味着在从主内存检索数据时，系统也应该检索周围的数据。
- en: In the next section, we’ll characterize cache characteristics and describe mechanisms
    for the hardware to make identifying and exploiting locality happen automatically.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分中，我们将描述缓存的特性，并描述硬件如何自动识别和利用局部性的机制。
- en: 11.4 CPU Caches
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.4 CPU 缓存
- en: 'Having characterized storage devices and recognized the important patterns
    of temporal and spatial locality, we’re ready to explore how CPU caches are designed
    and implemented. A *cache* is a small, fast storage device on a CPU that holds
    limited subsets of main memory. Caches face several important design questions:
    *Which* subsets of a program’s memory should the cache hold? *When* should the
    cache copy a subset of a program’s data from main memory to the cache, or vice
    versa? *How* can a system determine whether a program’s data is present in the
    cache?'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在描述了存储设备并认识到时间和空间局部性的关键模式之后，我们准备探讨 CPU 缓存的设计和实现。*缓存*是 CPU 上的一种小型快速存储设备，存储着主内存的有限子集。缓存面临几个重要的设计问题：*哪个*程序内存的子集应该被缓存？*何时*缓存应将程序数据的子集从主内存复制到缓存，或反之？*如何*系统能够判断程序的数据是否存在于缓存中？
- en: Before exploring these challenging questions, we need to introduce some cache
    behavior and terminology. Recall that when accessing data in memory, a program
    first computes the data’s memory address (see “Instruction Structure” on [page
    298](ch07.xhtml#lev2_116)). Ideally, the data at the desired address already resides
    in the cache, allowing the program to skip accessing main memory altogether. To
    maximize performance, the hardware simultaneously sends the desired address to
    *both* the cache and main memory. Because the cache is faster and closer to the
    ALU, the cache responds much more quickly than memory. If the data is present
    in the cache (a *cache hit*), the cache hardware cancels the pending memory access
    because the cache can serve the data faster than memory.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在探讨这些具有挑战性的问题之前，我们需要介绍一些缓存行为和术语。回想一下，当访问内存中的数据时，程序首先计算数据的内存地址（请参见[第298页](ch07.xhtml#lev2_116)的“指令结构”）。理想情况下，所需地址处的数据已经存在于缓存中，允许程序完全跳过对主内存的访问。为了最大化性能，硬件同时将所需地址发送到*缓存*和主内存。由于缓存比内存更快且离算术逻辑单元（ALU）更近，缓存的响应速度远快于内存。如果数据存在于缓存中（*缓存命中*），缓存硬件将取消挂起的内存访问，因为缓存能比内存更快地提供数据。
- en: Otherwise, if the data isn’t in the cache (a *cache miss*), the CPU has no choice
    but to wait for memory to retrieve it. Critically though, when the request to
    main memory completes, the CPU loads the retrieved data into the cache so that
    subsequent requests for the same address (which are likely thanks to temporal
    locality) can be serviced quickly from the cache. Even if the memory access that
    misses is *writing* to memory, the CPU still loads the value into the cache on
    a miss because it’s likely that the program will attempt to access the same location
    again in the future.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，如果数据不在缓存中（*缓存未命中*），CPU别无选择，只能等待从内存中检索数据。然而，关键的是，当访问主内存的请求完成后，CPU会将检索到的数据加载到缓存中，以便后续对同一地址的请求（由于时间局部性，这些请求可能会发生）能够从缓存中快速服务。即使未命中的内存访问是*写入*内存，CPU仍然会在未命中时将值加载到缓存中，因为程序很可能会在未来再次访问相同的位置。
- en: When loading data into a cache after a miss, a CPU often finds that the cache
    doesn’t have enough free space available. In such cases, the cache must first
    *evict* some resident data to make room for the new data that it’s loading in.
    Because a cache stores subsets of data copied from main memory, evicting cached
    data that has been modified requires the cache to update the contents of main
    memory prior to evicting data from the cache.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 当在缓存未命中的情况下加载数据时，CPU通常会发现缓存没有足够的空闲空间可用。在这种情况下，缓存必须首先*逐出*一些已驻留的数据，为正在加载的新数据腾出空间。由于缓存存储的是从主内存复制的数据子集，逐出已修改的缓存数据时，缓存必须在逐出数据之前更新主内存中的内容。
- en: To provide all the aforementioned functionality, cache designers employ one
    of three designs. This section begins by examining *direct-mapped caches*, which
    are less complex than the other designs.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供上述所有功能，缓存设计师采用了三种设计方案之一。本节首先讨论*直接映射缓存*，它比其他设计方案更简单。
- en: 11.4.1 Direct-Mapped Caches
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.4.1 直接映射缓存
- en: 'A direct-mapped cache divides its storage space into units called *cache lines*.
    Depending on the size of a cache, it might hold dozens, hundreds, or even thousands
    of cache lines. In a direct-mapped cache, each cache line is independent of all
    the others and contains two important types of information: a *cache data block*
    and *metadata*.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 直接映射缓存将其存储空间划分为称为*缓存行*的单元。根据缓存的大小，它可能包含几十、几百甚至几千个缓存行。在直接映射缓存中，每个缓存行都是独立的，并包含两种重要类型的信息：*缓存数据块*和*元数据*。
- en: A *cache data block* (often shortened to *cache block*) stores a subset of program
    data from main memory. Cache blocks store multibyte chunks of program data to
    take advantage of spatial locality. The size of a cache block determines the unit
    of data transfer between the cache and main memory. That is, when loading a cache
    with data from memory, the cache always receives a chunk of data the size of a
    cache block. Cache designers balance a trade-off in choosing a cache’s block size.
    Given a fixed storage budget, a cache can store more smaller blocks or fewer larger
    blocks. Using larger blocks improves performance for programs that exhibit good
    spatial locality, whereas having more blocks gives a cache the opportunity to
    store a more diverse subset of memory. Ultimately, which strategy provides the
    best performance depends on the workload of applications. Since general-purpose
    CPUs can’t assume much about a system’s applications, a typical CPU cache today
    uses middle-of-the-road block sizes ranging from 16 to 64 bytes.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '*缓存数据块*（通常缩写为*缓存块*）存储来自主内存的程序数据子集。缓存块存储多字节的数据块，以利用空间局部性。缓存块的大小决定了缓存和主存之间的数据传输单位。也就是说，当从内存加载数据到缓存时，缓存始终接收一个与缓存块大小相等的数据块。缓存设计师在选择缓存块大小时需要权衡各种因素。给定固定的存储预算，缓存可以存储更多的小块，或者更少的大块。使用更大的块可以提高具有良好空间局部性的程序的性能，而更多的块则使缓存能够存储更为多样化的内存子集。最终，哪种策略提供最佳性能取决于应用程序的工作负载。由于通用
    CPU 无法假设系统应用程序的特性，当前典型的 CPU 缓存使用的是介于 16 到 64 字节之间的中等大小块。'
- en: '*Metadata* stores information about the contents of the cache line’s data block.
    A cache line’s metadata does *not* contain program data. Instead, it maintains
    bookkeeping information for the cache line (e.g., to help identify which subset
    of memory the cache line’s data block holds).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '*元数据*存储有关缓存行数据块内容的信息。缓存行的元数据*不*包含程序数据。相反，它维护缓存行的账目信息（例如，帮助识别缓存行的数据块包含内存的哪个子集）。'
- en: When a program attempts to access a memory address, a cache must know where
    to look to find the corresponding data, check whether the desired data is available
    at that cache location, and if so, retrieve a portion of the stored cache block
    to the requesting application. The following steps walk through the details of
    this process for finding data in a cache and retrieving it.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 当程序尝试访问一个内存地址时，缓存必须知道从哪里查找对应的数据，检查该缓存位置是否有所需的数据，如果有，就将缓存块的一部分数据返回给请求的应用程序。以下步骤详细介绍了在缓存中查找并检索数据的过程。
- en: Locating Cached Data
  id: totrans-124
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 定位缓存数据
- en: A cache must be able to quickly determine whether the subset of memory corresponding
    to a requested address currently resides in the cache. To answer that question,
    a cache must first determine which cache line(s) to check. In a direct-mapped
    cache, each address in memory corresponds to *exactly* one cache line. This restriction
    explains the *direct-mapped* name—it maps every memory address directly to one
    cache line.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存必须能够快速判断对应请求地址的内存子集是否当前存在于缓存中。为了解答这个问题，缓存必须首先确定检查哪些缓存行。在直接映射缓存中，内存中的每个地址都对应*恰好*一个缓存行。这一限制解释了*直接映射*这个名称——它将每个内存地址直接映射到一个缓存行。
- en: '[Figure 11-7](ch11.xhtml#ch11fig7) shows how memory addresses map to cache
    lines in a small direct-mapped cache with four cache lines and a 32-byte cache
    block size. Recall that a cache’s block size represents the smallest unit of data
    transfer between a cache and main memory. Thus, every memory address falls within
    one 32-byte range, and each range maps to one cache line.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11-7](ch11.xhtml#ch11fig7)展示了内存地址如何映射到一个小型直接映射缓存中的缓存行，缓存行有四行，缓存块大小为 32 字节。回想一下，缓存的块大小表示缓存和主存之间数据传输的最小单位。因此，每个内存地址都落在一个
    32 字节的范围内，并且每个范围映射到一个缓存行。'
- en: '![image](../images/11fig07.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/11fig07.jpg)'
- en: '*Figure 11-7: An example mapping of memory addresses to cache lines in a four-line
    direct-mapped cache with 32-byte cache blocks*'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-7：在一个四行直接映射缓存中，内存地址到缓存行的映射示例，缓存块大小为 32 字节*'
- en: Note that although each region of memory maps to only one cache line, many memory
    ranges map to the *same* cache line. All of the memory regions that map the same
    cache line (i.e., chunks of the same color in [Figure 11-7](ch11.xhtml#ch11fig7))
    compete for space in the same cache line, so only one region of each color can
    reside in the cache at a time.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，虽然每个内存区域只映射到一个缓存行，但许多内存范围会映射到*相同*的缓存行。所有映射到同一缓存行的内存区域（即[图 11-7](ch11.xhtml#ch11fig7)中相同颜色的块）都在同一缓存行中争夺空间，因此每次只能有一个颜色的区域驻留在缓存中。
- en: A cache maps a memory address to a cache line using a portion of the bits in
    the memory address. To spread data more evenly among cache lines, caches use bits
    taken from the *middle* of the memory address, known as the *index* portion of
    the address, to determine which line the address maps to. The number of bits used
    as the index (which varies) determines how many lines a cache will hold. [Figure
    11-8](ch11.xhtml#ch11fig8) shows the index portion of a memory address referring
    to a cache line.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存使用内存地址中的一部分位将内存地址映射到缓存行。为了使数据更均匀地分布在缓存行中，缓存使用取自内存地址*中间*部分的位，这部分称为地址的*索引*部分，用于确定该地址映射到哪个缓存行。作为索引的位数（会有所变化）决定了缓存能容纳多少个缓存行。[图
    11-8](ch11.xhtml#ch11fig8) 显示了内存地址的索引部分是如何指向缓存行的。
- en: '![image](../images/11fig08.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/11fig08.jpg)'
- en: '*Figure 11-8: The middle index portion of a memory address identifies a cache
    line.*'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-8：内存地址的中间索引部分标识缓存行。*'
- en: Using the middle of the address reduces competition for the same cache line
    when program data is clustered together, which is often the case for programs
    that exhibit good locality. That is, programs tend to store variables nearby one
    another in one of a few locations (e.g., on the stack or heap). Such clustered
    variables share the same high-order address bits. Thus, indexing with the high-order
    bits would cause the clustered variables to all map to the same cache lines, leaving
    the rest of the cache unused. By using bits from the middle of the address, caches
    spread data more evenly among the available cache lines.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 使用地址的中间部分可以减少程序数据聚集时对同一缓存行的竞争，这通常发生在具有良好局部性的程序中。也就是说，程序倾向于将变量存储在彼此靠近的地方，通常是在几个位置之一（例如，栈或堆上）。这些聚集的变量共享相同的高位地址。因此，如果使用高位进行索引，则会导致这些聚集的变量全部映射到同一缓存行，从而使其余缓存行未被使用。通过使用地址中间部分的位，缓存能更均匀地分配数据到可用的缓存行。
- en: Identifying Cache Contents
  id: totrans-134
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 缓存内容识别
- en: 'Next, having located the appropriate cache line, the cache must determine whether
    that line holds the requested address. Since multiple memory ranges map to the
    same cache line, the cache examines the line’s metadata to answer two important
    questions: Does this cache line hold a valid subset of memory? If so, which of
    the many subsets of memory that map to this cache line does it currently hold?'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，定位到适当的缓存行后，缓存必须确定该行是否包含请求的地址。由于多个内存范围映射到同一缓存行，缓存会检查该行的元数据，以回答两个重要问题：该缓存行是否持有有效的内存子集？如果是，它当前持有哪些映射到此缓存行的内存子集？
- en: To answer these questions, each cache line’s metadata includes a valid bit and
    a tag. The *valid bit* is a single bit that indicates whether a line is currently
    storing a valid subset of memory (if valid is set to 1). An invalid line (if valid
    is set to 0) never produces a cache hit because no data has been loaded into it.
    Invalid lines effectively represent free space in the cache.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这些问题，每个缓存行的元数据包括一个有效位和一个标签。*有效位*是一个单个位，表示该行当前是否存储有效的内存子集（如果有效位为1）。无效行（如果有效位为0）永远不会产生缓存命中，因为没有数据被加载到其中。无效行实际上表示缓存中的空闲空间。
- en: In addition to a valid bit, each cache line’s metadata stores a *tag* that uniquely
    identifies which subset of memory the line’s cache block holds. The tag field
    stores the high-order bits of the address range stored in the cache line and allows
    a cache line to track where in memory its data block came from. In other words,
    because many memory subsets map to the same cache line (those with the same index
    bits), the tag records which of those subsets is currently present in the cache
    line.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 除了有效位，每个缓存行的元数据还存储一个*标签*，该标签唯一标识该缓存块所持有的内存子集。标签字段存储缓存行中地址范围的高位，并允许缓存行追踪其数据块来自内存的哪个位置。换句话说，由于许多内存子集映射到同一缓存行（那些具有相同索引位的子集），标签记录了当前存储在缓存行中的子集。
- en: For a cache lookup to produce a hit, the tag field stored in the cache line
    must exactly match the tag portion (upper bits) of the program’s requested memory
    address. A tag mismatch indicates that a cache line’s data block does not contain
    the requested memory, even if the line stores valid data. [Figure 11-9](ch11.xhtml#ch11fig9)
    illustrates how a cache divides a memory address into a tag and an index, uses
    the index bits to select a target cache line, verifies a line’s valid bit, and
    checks the line’s tag for a match.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使缓存查找产生命中，存储在缓存行中的标签字段必须与程序请求的内存地址的标签部分（高位）完全匹配。标签不匹配表明缓存行的数据块不包含请求的内存，即使该行存储有效数据。[图11-9](ch11.xhtml#ch11fig9)说明了缓存如何将内存地址划分为标签和索引，使用索引位选择目标缓存行，验证行的有效位，并检查行的标签是否匹配。
- en: '![image](../images/11fig09.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/11fig09.jpg)'
- en: '*Figure 11-9: After using the requested memory address’s index bits to locate
    the proper cache line, the cache simultaneously verifies the line’s valid bit
    and checks its tag against the requested address’s tag. If the line is valid with
    a matching tag, the lookup succeeds as a hit.*'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '*图11-9：在使用请求的内存地址的索引位定位正确的缓存行后，缓存同时验证该行的有效位并检查其标签与请求地址的标签是否匹配。如果该行有效且标签匹配，则查找成功，命中。*'
- en: Retrieving Cached Data
  id: totrans-141
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 检索缓存数据
- en: Finally, after using the program’s requested memory address to find the appropriate
    cache line and verifying that the line holds a valid subset of memory containing
    that address, the cache sends the requested data to the CPU’s components that
    need it. Because a cache line’s data block size (e.g., 64 bytes) is typically
    much larger than the amount of data that programs request (e.g., 4 bytes), caches
    use the low-order bits of the requested address as an *offset* into the cached
    data block. [Figure 11-10](ch11.xhtml#ch11fig10) depicts how the offset portion
    of an address identifies which bytes of a cache block the program expects to retrieve.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在使用程序请求的内存地址找到合适的缓存行并验证该行包含有效的内存子集（其中包含该地址）之后，缓存将请求的数据发送到需要这些数据的CPU组件。由于缓存行的数据块大小（例如，64字节）通常比程序请求的数据量（例如，4字节）要大得多，缓存使用请求地址的低位作为*偏移量*，以进入缓存的数据块。[图11-10](ch11.xhtml#ch11fig10)展示了地址的偏移量部分是如何确定程序期望检索缓存块中的哪些字节的。
- en: '![image](../images/11fig10.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/11fig10.jpg)'
- en: '*Figure 11-10: Given a cache data block, the offset portion of an address identifies
    which bytes the program wants to retrieve.*'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '*图11-10：给定一个缓存数据块，地址的偏移量部分确定程序想要检索的字节。*'
- en: Memory Address Division
  id: totrans-145
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 内存地址划分
- en: The *dimensions* of a cache dictate how many bits to interpret as the offset,
    index, and tag portions of a memory address. Equivalently, the number of bits
    in each portion of an address imply what the dimensions of a cache must be. In
    determining which bits belong to each portion of an address, it’s helpful to consider
    the address from right to left (i.e., from least to most significant bit).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存的*维度*决定了将多少位解释为内存地址中的偏移量、索引和标签部分。同样，地址中每个部分的位数也暗示了缓存的维度。为了确定哪个位属于地址的哪一部分，考虑从右到左（即从最低有效位到最高有效位）来看地址是有帮助的。
- en: The rightmost portion of the address is the *offset*, and its length depends
    on a cache’s block size dimension. The offset portion of an address must contain
    enough bits to refer to every possible byte within a cache data block. For example,
    suppose that a cache stores 32-byte data blocks. Because a program might come
    along asking for any of those 32 bytes, the cache needs enough offset bits to
    describe exactly which of the 32 possible positions the program might want. In
    this case, it would need five bits for the offset because five bits are necessary
    to represent 32 unique values (log[2] 32 = 5). In the reverse direction, a cache
    that uses four bits for the offset must store 16-byte data blocks (2⁴ = 16).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 地址的最右部分是*偏移量*，其长度取决于缓存的数据块大小维度。地址的偏移量部分必须包含足够的位，以指向缓存数据块中的每个可能字节。例如，假设一个缓存存储32字节的数据块。因为程序可能会请求这些32字节中的任何一个，缓存需要足够的偏移量位来准确描述程序可能请求的32个位置中的哪个位置。在这种情况下，偏移量需要5个位，因为5个位足以表示32个唯一的值（log[2]
    32 = 5）。反过来，如果一个缓存使用四个位作为偏移量，那么它必须存储16字节的数据块（2⁴ = 16）。
- en: The *index* portion of the address begins immediately to the left of the offset.
    To determine the number of index bits, consider the number of lines in the cache,
    given that the index needs enough bits to uniquely identify every cache line.
    Using similar logic to the offset, a cache with 1,024 lines needs 10 bits for
    the index (log[2] 1,024 = 10). Likewise, a cache that uses 12 bits for the index
    must have 4,096 lines (2^(12) = 4,096).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 地址的*索引*部分从偏移的左侧立即开始。要确定索引位数，考虑缓存中的行数，因为索引需要足够的位数唯一标识每个缓存行。使用与偏移类似的逻辑，具有1,024行的缓存需要10位索引（log[2]
    1,024 = 10）。同样，使用12位索引的缓存必须有4,096行（2^(12) = 4,096）。
- en: '![image](../images/11fig11.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/11fig11.jpg)'
- en: '*Figure 11-11: The index portion of an address uniquely identifies a cache
    line, and the offset portion uniquely identifies a position in the line’s data
    block.*'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '*图11-11：地址的索引部分唯一标识一个缓存行，偏移部分唯一标识行数据块中的位置。*'
- en: The remaining address bits form the tag. Because the tag must uniquely identify
    the subset of memory contained within a cache line, the tag must use *all* of
    the remaining, unclaimed bits of the address. For example, if a machine uses 32-bit
    addresses, a cache with 5 offset bits and 10 index bits uses the remaining 32
    – 15 = 17 bits of the address to represent the tag.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余的地址位形成标记。因为标记必须唯一标识包含在缓存行中的内存子集，所以标记必须使用地址剩余未使用的所有位。例如，如果一台机器使用32位地址，具有5位偏移和10位索引的缓存使用剩余的32
    - 15 = 17位地址表示标记。
- en: Direct-Mapped Read Examples
  id: totrans-152
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 直接映射读取示例
- en: 'Consider a CPU with the following characteristics:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑具有以下特性的CPU：
- en: 16-bit memory addresses
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 16位内存地址
- en: a direct-mapped cache with 128 cache lines
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有128个缓存行的直接映射缓存
- en: 32-byte cache data blocks.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 32字节缓存数据块。
- en: The cache starts empty (all lines are invalid), as shown in [Figure 11-12](ch11.xhtml#ch11fig12).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存从空开始（所有行都无效），如[图11-12](ch11.xhtml#ch11fig12)所示。
- en: '![image](../images/11fig12.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/11fig12.jpg)'
- en: '*Figure 11-12: An empty direct-mapped example cache*'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '*图11-12：一个空的直接映射示例缓存*'
- en: 'Suppose that a program running on this CPU accesses the following memory locations
    (see [Figures 11-13](ch11.xhtml#ch11fig13) through [11-16](ch11.xhtml#ch11fig16)):'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 假设运行在此CPU上的程序访问以下内存位置（参见[图11-13](ch11.xhtml#ch11fig13)至[11-16](ch11.xhtml#ch11fig16)）：
- en: 1\. Read from address 1010000001100100.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 从地址1010000001100100读取。
- en: 2\. Read from address 1010000001100111.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 从地址1010000001100111读取。
- en: 3\. Read from address 1001000000100000.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 从地址1001000000100000读取。
- en: 4\. Read from address 1111000001100101.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 从地址1111000001100101读取。
- en: 'To put the entire sequence together, follow these steps when tracing the behavior
    of a cache:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 要将整个序列放在一起，在跟踪缓存行为时，请按以下步骤操作：
- en: '1\. Divide the requested address into three portions, from right (low-order
    bits) to left (high-order bits): an offset within the cache data block, an index
    into the appropriate cache line, and a tag to identify which subset of memory
    the line stores.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 将请求的地址从右侧（低位）到左侧（高位）分成三部分：缓存数据块内的偏移量、适当缓存行的索引以及标记，以标识存储行的内存子集。
- en: 2\. Index into the cache using the middle portion of the requested address to
    find the cache line to which the address maps.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 使用请求地址的中间部分索引到缓存，以找到地址映射到的缓存行。
- en: 3\. Check the cache line’s valid bit. When invalid, the program can’t use a
    cache line’s contents (cache miss), regardless of what the tag might be.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 检查缓存行的有效位。当无效时，程序无法使用缓存行的内容（缓存未命中），无论标记是什么。
- en: 4\. Check the cache line’s tag. If the address’s tag matches the cache line’s
    tag and the line is valid, the cache line’s data block holds the data the program
    is looking for (cache hit). Otherwise, the cache must load the data from main
    memory at the identified index (cache miss).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 检查缓存行的标记。如果地址的标记与缓存行的标记匹配并且该行有效，则缓存行的数据块包含程序正在查找的数据（缓存命中）。否则，缓存必须从主存加载数据到指定的索引（缓存未命中）。
- en: 5\. On a hit, use the low-order offset bits of the address to extract the program’s
    desired data from the stored block. (Not shown in this example.)
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. 命中时，使用地址的低位偏移位从存储的块中提取程序所需数据。（本例中未显示。）
- en: Address Division
  id: totrans-171
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 地址划分
- en: 'Begin by determining how to divide the memory addresses into their *offset*,
    *index*, and *tag* portions. Consider the address portions from low-order to high-order
    bits (right to left):'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 首先确定如何将内存地址划分为其*偏移*、*索引*和*标记*部分。考虑地址部分从低位到高位（从右到左）：
- en: '*Offset*: A 32-byte block size implies that the rightmost five bits of the
    address (log[2] 32 = 5) comprise the offset portion. With five bits, the offset
    can uniquely identify any of the 32 bytes in block.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '*偏移量*：一个 32 字节的块大小意味着地址的最右边五位（log[2] 32 = 5）构成偏移量部分。通过五位，偏移量可以唯一标识块中的任何一个 32
    字节。'
- en: '*Index*: A cache with 128 lines implies that the next seven bits of the address
    (log[2] 128 = 7) comprise the index portion. With seven bits, the index can uniquely
    identify each cache line.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '*索引*：一个有 128 行的缓存意味着地址的接下来的七位（log[2] 128 = 7）构成索引部分。通过七位，索引可以唯一标识每个缓存行。'
- en: '*Tag*: The tag consists of any remaining address bits that don’t belong to
    the offset or index. Here, the address has four remaining bits left that form
    the tag (16 – (5 + 7) = 4).'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '*标签*：标签由地址中不属于偏移量或索引的剩余位组成。在这里，地址中剩下四位构成标签（16 – (5 + 7) = 4）。'
- en: '![image](../images/11fig13.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/11fig13.jpg)'
- en: '*Figure 11-13: Read from address 1010000001100100\. Index 0000011 (line 3)
    is invalid, so the request misses and the cache loads data from main memory.*'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-13：从地址 1010000001100100 读取。索引 0000011（第 3 行）无效，因此请求未命中，缓存从主内存加载数据。*'
- en: '![image](../images/11fig14.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/11fig14.jpg)'
- en: '*Figure 11-14: Read from address 1010000001100111\. Index 0000011 (line 3)
    is valid, and the tag (1010) matches, so the request hits. The cache yields data
    beginning at byte 7 (offset 0b00111) of its data block.*'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-14：从地址 1010000001100111 读取。索引 0000011（第 3 行）有效，并且标签（1010）匹配，因此请求命中。缓存返回数据块中从字节
    7（偏移量 0b00111）开始的数据。*'
- en: '![image](../images/11fig15.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/11fig15.jpg)'
- en: '*Figure 11-15: Read from address 1001000000100000\. Index 0000001 (line 1)
    is invalid, so the request misses and the cache loads data from main memory.*'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-15：从地址 1001000000100000 读取。索引 0000001（第 1 行）无效，因此请求未命中，缓存从主内存加载数据。*'
- en: '![image](../images/11fig16.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/11fig16.jpg)'
- en: '*Figure 11-16: Read from address 1111000001100101\. Index 0000011 (line 3)
    is valid, but the tag doesn’t match, so the request misses and the cache loads
    data from main memory.*'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-16：从地址 1111000001100101 读取。索引 0000011（第 3 行）有效，但标签不匹配，因此请求未命中，缓存从主内存加载数据。*'
- en: Writing to Cached Data
  id: totrans-184
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 写入缓存数据
- en: So far, this section has primarily considered memory read operations for which
    a CPU performs lookups in the cache. Caches must also allow programs to store
    values, and they support store operations with one of two strategies.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，本节主要考虑了 CPU 在缓存中执行查找的内存读取操作。缓存还必须允许程序存储值，并支持两种策略之一的存储操作。
- en: In a *write-through cache*, a memory write operation modifies the value in the
    cache and simultaneously updates the contents of main memory. That is, a write
    operation *always* synchronizes the contents of the cache and main memory immediately.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *写直达缓存* 中，内存写操作修改缓存中的值，并同时更新主内存的内容。也就是说，写操作 *总是* 立即同步缓存和主内存的内容。
- en: In a *write-back cache*, a memory write operation modifies the value stored
    in the cache’s data block, but it does *not* update main memory. Thus, after updating
    the cache’s data, a write-back cache’s contents differ from the corresponding
    data in main memory.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *写回缓存* 中，内存写操作修改存储在缓存数据块中的值，但 *不* 更新主内存。因此，在更新缓存的数据后，写回缓存的内容与主内存中的相应数据不同。
- en: To identify cache blocks whose contents differ from their main memory counterparts,
    each line in a write-back cache stores an additional bit of metadata, known as
    a *dirty bit*. When evicting the data block from a dirty cache line, the cache
    block’s data must first be written back to main memory to synchronize their contents.
    [Figure 11-17](ch11.xhtml#ch11fig17) shows a direct-mapped cache that includes
    a dirty bit to mark lines that must be written to memory upon eviction.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 为了识别那些其内容与主内存对应部分不同的缓存块，写回缓存中的每一行都会存储一个额外的元数据位，称为 *脏位*。当从脏缓存行中驱逐数据块时，必须先将缓存块的数据写回主内存，以同步其内容。[图
    11-17](ch11.xhtml#ch11fig17) 显示了一个包含脏位的直接映射缓存，用于标记需要在驱逐时写入内存的行。
- en: '![image](../images/11fig17.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/11fig17.jpg)'
- en: '*Figure 11-17: Cache extended with a dirty bit*'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-17：缓存扩展了一个脏位*'
- en: As usual, the difference between the designs reveals a trade-off. Write-through
    caches are less complex than write-back caches, and they avoid storing extra metadata
    in the form of a dirty bit for each line. On the other hand, write-back caches
    reduce the cost of repeated writes to the same location in memory.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如常所见，设计之间的差异揭示了一个权衡。写透缓存比写回缓存更简单，并且避免为每行存储额外的元数据，如脏位。另一方面，写回缓存通过减少对同一内存位置的重复写操作的开销来提高性能。
- en: For example, suppose that a program frequently updates the same variable without
    that variable’s memory ever being evicted from the cache. A write-through cache
    writes to main memory on every update, even though each subsequent update is just
    going to overwrite the previous one, whereas a write-back cache writes to memory
    only when eventually evicting the cache block. Because amortizing the cost of
    a memory access across many writes significantly improves performance, most modern
    caches opt for a write-back design.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设一个程序频繁更新相同的变量，而该变量的内存从未被逐出缓存。写透缓存（write-through cache）在每次更新时都会写入主内存，即使每次更新只是覆盖了前一个值，而写回缓存（write-back
    cache）只有在最终逐出缓存块时才会写入内存。由于将内存访问的开销分摊到多次写操作中能显著提高性能，因此大多数现代缓存选择写回设计。
- en: Direct-Mapped Write Examples (Write-Back)
  id: totrans-193
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 直接映射写示例（写回）
- en: Writes to the cache behave like reads, except they also set the modified cache
    line’s dirty bit. When evicting a dirty cache line, the cache must write the modified
    data block to memory before discarding it.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 写入缓存的行为类似于读取，但它们还会设置修改过的缓存行的脏位。在逐出脏缓存行时，缓存必须在丢弃它之前将修改过的数据块写回内存。
- en: 'Suppose that the previously described example scenario continues with two additional
    memory accesses (see [Figures 11-18](ch11.xhtml#ch11fig18) and [11-19](ch11.xhtml#ch11fig19)):'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 假设前述示例场景继续进行，并增加了两个额外的内存访问（见 [图 11-18](ch11.xhtml#ch11fig18) 和 [图 11-19](ch11.xhtml#ch11fig19)）：
- en: '5\. Write to address: 1111000001100000.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. 写入地址：1111000001100000。
- en: '6\. Write to address: 1010000001100100.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 6\. 写入地址：1010000001100100。
- en: '![image](../images/11fig18.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/11fig18.jpg)'
- en: '*Figure 11-18: Write to address 1111000001100000\. Index 0000011 (line 3) is
    valid, and the tag (1111) matches, so the request hits. Because this access is
    a write, the cache sets the line’s dirty bit to 1.*'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-18：写入地址 1111000001100000。索引 0000011（第 3 行）有效，且标签（1111）匹配，因此请求命中。由于这是写操作，缓存将该行的脏位设置为
    1。*'
- en: '![image](../images/11fig19.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/11fig19.jpg)'
- en: '*Figure 11-19: Write to address 1010000001100100\. Index 0000011 (line 3) is
    valid, but the tag doesn’t match, so the request misses. Because the target line
    is both valid and dirty, the cache must save the existing data block to main memory
    before loading the new one. This access is a write, so the cache sets the newly
    loaded line’s dirty bit to 1.*'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-19：写入地址 1010000001100100。索引 0000011（第 3 行）有效，但标签不匹配，因此请求未命中。由于目标行既有效又脏，缓存必须在加载新数据之前将现有数据块保存到主内存。这是一个写操作，因此缓存将新加载行的脏位设置为
    1。*'
- en: In the fourth and sixth memory accesses of the example, the cache evicts data
    because two memory regions are competing for the same cache line. Next, we’ll
    explore a different cache design that aims to reduce this type of competition.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在示例中的第四次和第六次内存访问中，缓存逐出了数据，因为两个内存区域竞争同一缓存行。接下来，我们将探讨一种旨在减少这种竞争的缓存设计。
- en: 11.4.2 Cache Misses and Associative Designs
  id: totrans-203
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.4.2 缓存缺失和关联设计
- en: 'Cache designers aim to maximize a cache’s hit rate to ensure that as many memory
    requests as possible can avoid going to main memory. Even though locality provides
    hope for achieving a good hit rate, real caches can’t expect to hit on every access
    for a variety of reasons:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存设计者的目标是最大化缓存的命中率，以确保尽可能多的内存请求能够避免访问主内存。尽管局部性提供了实现良好命中率的希望，但由于各种原因，实际缓存无法期待每次访问都能命中：
- en: '**Compulsory** or **cold-start misses**: If a program has never accessed a
    memory location (or any location near it), it has little hope of finding that
    location’s data in the cache. Thus, programs often cannot avoid cache misses when
    first accessing new memory addresses.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '**强制性** 或 **冷启动缺失**：如果一个程序从未访问过某个内存位置（或与该位置相邻的任何位置），它几乎无法在缓存中找到该位置的数据。因此，程序在第一次访问新的内存地址时，往往无法避免缓存缺失。'
- en: '**Capacity misses**: A cache stores a subset of main memory, and ideally it
    stores *exactly* the subset of memory that a program is actively using. However,
    if a program is actively using more memory than fits in the cache, it can’t possibly
    find *all* of the data it wants in the cache, leading to *capacity misses*.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '**容量未命中**：缓存存储的是主存储器的一个子集，理想情况下它存储的是程序正在积极使用的*确切*内存子集。然而，如果一个程序正在积极使用的内存超过了缓存的容量，那么它不可能在缓存中找到*所有*它需要的数据，从而导致*容量未命中*。'
- en: '**Conflict misses**: To reduce the complexity of finding data, some cache designs
    limit where in the cache data can reside, and those restrictions can lead to *conflict
    misses*. For example, even if a direct-mapped cache is not 100% full, a program
    might end up with the addresses of two frequently used variables mapping to the
    same cache location. In such cases, each access to one of those variables evicts
    the other from the cache as they compete for the same cache line.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '**冲突未命中**：为了减少查找数据的复杂性，一些缓存设计限制了数据在缓存中存储的位置，这些限制可能导致*冲突未命中*。例如，即使一个直接映射的缓存没有完全满，程序也可能会把两个常用变量的地址映射到同一个缓存位置。在这种情况下，每次访问其中一个变量都会将另一个从缓存中驱逐，因为它们竞争相同的缓存行。'
- en: The relative frequency of each miss type depends on a program’s memory access
    pattern. In general though, without increasing the cache size, a cache’s design
    mainly affects its conflict miss rate. Although direct-mapped caches are less
    complex than other designs, they suffer the most from conflicts.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 每种未命中类型的相对频率取决于程序的内存访问模式。一般而言，在不增加缓存大小的情况下，缓存的设计主要影响其冲突未命中率。虽然直接映射缓存比其他设计更简单，但它们最容易受到冲突的影响。
- en: The alternative to a direct-mapped cache is an *associative* cache. An associative
    design gives a cache the flexibility to choose among more than one location to
    store a region of memory. Intuitively, having more storage location options reduces
    the likelihood of conflicts but also increases complexity due to more locations
    needing to be checked on every access.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 直接映射缓存的替代方案是*关联*缓存。关联设计使缓存能够在多个位置之间选择来存储一个内存区域。从直观上讲，拥有更多存储位置选项可以减少冲突的可能性，但也由于每次访问时需要检查更多位置而增加了复杂性。
- en: A *fully associative* cache allows any memory region to occupy any cache location.
    Fully associative caches offer the most flexibility, but they also have the highest
    lookup and eviction complexity because every location needs to be simultaneously
    considered during any operation. Although fully associative caches are valuable
    in some small, specialized applications (e.g., translation look-aside buffers—see
    “Making Page Accesses Faster” on [page 655](ch13.xhtml#lev3_110)), their high
    complexity makes them generally unfit for a general-purpose CPU cache.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '*全关联*缓存允许任何内存区域占据任何缓存位置。全关联缓存提供了最大的灵活性，但它们也具有最高的查找和驱逐复杂性，因为在任何操作期间，每个位置都需要同时考虑。尽管全关联缓存在一些小型专用应用中（例如，转换旁路缓存——见《加快页面访问》[第655页](ch13.xhtml#lev3_110)）非常有价值，但它们的高复杂性使得它们通常不适合用于通用CPU缓存。'
- en: '*Set associative* caches occupy the middle ground between direct-mapped and
    fully associative designs, which makes them well suited for general-purpose CPUs.
    In a set associative cache, every memory region maps to exactly one *cache set*,
    but each set stores multiple cache lines. The number of lines allowed in a set
    is a fixed dimension of a cache, and set associative caches typically store two
    to eight lines per set.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '*集合关联*缓存占据了直接映射和全关联设计之间的中间地带，这使得它们非常适合用于通用CPU。在集合关联缓存中，每个内存区域映射到一个*缓存集合*，但每个集合存储多个缓存行。集合中的行数是缓存的固定维度，集合关联缓存通常每个集合存储两到八行。'
- en: 11.4.3 Set Associative Caches
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.4.3 集合关联缓存
- en: A set associative design offers a good compromise between complexity and conflicts.
    The number of lines in a set limits how many places a cache needs to check during
    a lookup, and multiple memory regions that map to the same set don’t trigger conflict
    misses unless the entire set fills.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 集合关联设计在复杂性和冲突之间提供了一个良好的折中。集合中的行数限制了缓存查找时需要检查的位置数量，并且映射到同一集合的多个内存区域只有在整个集合填满时才会触发冲突未命中。
- en: In a set associative cache, the *index* portion of a memory address maps the
    address to one set of cache lines. When performing an address lookup, the cache
    simultaneously checks every line in the set. [Figure 11-20](ch11.xhtml#ch11fig20)
    illustrates the tag and valid bit checks in a two-way set associative cache.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在集合关联缓存中，内存地址的*索引*部分将地址映射到一组缓存行。当执行地址查找时，缓存会同时检查集合中的每一行。[图11-20](ch11.xhtml#ch11fig20)展示了二路集合关联缓存中的标签和有效位检查。
- en: 'If any of a set’s valid lines contains a tag that matches the address’s tag
    portion, the matching line completes the lookup. When the lookup narrows the search
    to just one cache line, it proceeds like a direct-mapped cache: the cache uses
    the address’s *offset* to send the desired bytes from the line’s cache block to
    the CPU’s arithmetic components.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个集合的有效行包含与地址的标签部分匹配的标签，则匹配的行完成查找。当查找将搜索范围缩小到仅一个缓存行时，它就像直接映射缓存一样进行：缓存使用地址的*偏移量*将所需的字节从该行的缓存块传送到CPU的算术组件。
- en: '![image](../images/11fig20.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/11fig20.jpg)'
- en: '*Figure 11-20: Valid bit verification and tag matching in a two-way set associative
    cache*'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '*图11-20：二路集合关联缓存中的有效位验证和标签匹配*'
- en: 'The additional flexibility of multiple cache lines in a set reduces conflicts,
    but it also introduces a new wrinkle: when loading a value into a cache (and when
    evicting data already resident in the cache), the cache must decide *which* of
    the line options to use.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在集合中多个缓存行的额外灵活性减少了冲突，但也引入了一个新问题：当将一个值加载到缓存中（以及当逐出已经驻留在缓存中的数据时），缓存必须决定*选择*哪个缓存行选项。
- en: 'To help solve this selection problem, caches turn to the idea of locality.
    Specifically, temporal locality suggests that recently used data is likely to
    be used again. Therefore, caches adopt the same strategy that the previous section
    used to manage our example bookcase: when deciding which line in a set to evict,
    choose the least recently used (LRU) line. LRU is known as a *cache replacement
    policy* because it governs the cache’s eviction mechanism.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个选择问题，缓存采用了局部性原则。具体来说，时间局部性表明，最近使用的数据很可能会再次使用。因此，缓存采用与前一节中管理我们示例书架相同的策略：在决定逐出集合中的哪一行时，选择最近最少使用（LRU）行。LRU被称为*缓存替换策略*，因为它决定了缓存的逐出机制。
- en: The LRU policy requires each set to store additional bits of metadata to identify
    which line of the set was used least recently. As the number of lines in a set
    increases, so does the number of bits required to encode the LRU status of the
    set. These extra metadata bits contribute to the “higher complexity” of set associative
    designs compared to simpler direct-mapped variants.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: LRU策略要求每个集合存储额外的元数据位来标识集合中最近最少使用的行。随着集合中行数的增加，编码集合LRU状态所需的位数也增加。与更简单的直接映射变体相比，这些额外的元数据位增加了集合关联设计的“更高复杂度”。
- en: '[Figure 11-21](ch11.xhtml#ch11fig21) illustrates a two-way set associative
    cache, meaning each set contains two lines. With just two lines, each set requires
    one LRU metadata bit to keep track of which line was least recently used. In the
    figure, an LRU value of zero indicates the leftmost line was least recently used,
    and a value of one means the rightmost line was least recently used.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11-21](ch11.xhtml#ch11fig21)展示了一个二路集合关联缓存，意味着每个集合包含两行。只有两行时，每个集合需要一个LRU元数据位来跟踪最近最少使用的行。在图中，LRU值为零表示最左边的行最近最少使用，值为一则表示最右边的行最近最少使用。'
- en: '![image](../images/11fig21.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/11fig21.jpg)'
- en: '*Figure 11-21: A two-way set associative cache in which each set stores one
    bit of LRU metadata to inform eviction decisions*'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '*图11-21：一个二路集合关联缓存，其中每个集合存储一个LRU元数据位来决定逐出策略*'
- en: '**Warning LRU BIT INTERPRETATION**'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '**警告：LRU位解释**'
- en: '[Figure 11-21](ch11.xhtml#ch11fig21)’s choice that zero means “left” and one
    means “right” is arbitrary. The interpretation of LRU bits varies across caches.
    If you’re asked to work with caches on an assignment, don’t assume the assignment
    is using the same LRU encoding scheme!'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11-21](ch11.xhtml#ch11fig21)中选择零表示“左”而一表示“右”是任意的。LRU位的解释在不同缓存中有所不同。如果你被要求处理缓存任务，不要假设任务使用的是相同的LRU编码方案！'
- en: Set Associative Cache Examples
  id: totrans-226
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 集合关联缓存示例
- en: 'Consider a CPU with the following characteristics:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑具有以下特点的CPU：
- en: 16-bit memory addresses.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 16位内存地址。
- en: A two-way set associative cache with 64 sets. Note that making a cache two-way
    set associative doubles its storage capacity (two lines per set), so this example
    halves the number of sets so that it stores the same number of lines as the earlier
    direct-mapped example.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有 64 个集合的二路组相联缓存。请注意，将缓存设置为二路组相联会使其存储容量翻倍（每个集合两行），因此这个示例减少了集合的数量，从而使它存储的行数与早期的直接映射示例相同。
- en: 32-byte cache blocks.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 32 字节缓存块。
- en: An LRU cache replacement policy that indicates whether the leftmost line of
    the set was least recently used (LRU = 0) or the rightmost line of the set was
    least recently used (LRU = 1).
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种 LRU 缓存替换策略，指示集合的最左边行是否为最近最少使用（LRU = 0），或者集合的最右边行是否为最近最少使用（LRU = 1）。
- en: Initially, the cache is empty (all lines invalid and LRU bits 0), as shown in
    [Figure 11-22](ch11.xhtml#ch11fig22).
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，缓存为空（所有行无效，LRU 位为 0），如 [图 11-22](ch11.xhtml#ch11fig22) 所示。
- en: '![image](../images/11fig22.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/11fig22.jpg)'
- en: '*Figure 11-22: An empty two-way set associative example cache*'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-22：一个空的二路组相联缓存示例*'
- en: 'Suppose that a program running on this CPU accesses the following memory locations
    (same as the direct-mapped example) (see [Figures 11-23](ch11.xhtml#ch11fig23)
    through [11-28](ch11.xhtml#ch11fig28)):'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 假设在此 CPU 上运行的程序访问以下内存位置（与直接映射示例相同）（参见 [图 11-23](ch11.xhtml#ch11fig23) 至 [11-28](ch11.xhtml#ch11fig28)）：
- en: 1\. Read from address 1010000001100100.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 从地址 1010000001100100 读取。
- en: 2\. Read from address 1010000001100111.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 从地址 1010000001100111 读取。
- en: 3\. Read from address 1001000000100000.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 从地址 1001000000100000 读取。
- en: 4\. Read from address 1111000001100101.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 从地址 1111000001100101 读取。
- en: 5\. Write to address 1111000001100000.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. 向地址 1111000001100000 写入。
- en: 6\. Write to address 1010000001100100.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 6\. 向地址 1010000001100100 写入。
- en: 'Begin by determining how to divide the memory addresses into their *offset*,
    *index*, and *tag* portions. Consider the address portions from low-order to high-order
    bits (right to left):'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 首先确定如何将内存地址分为 *偏移量*、*索引* 和 *标签* 部分。从低位到高位依次考虑地址的各个部分（从右到左）：
- en: '*Offset*: A 32-byte block size implies that the rightmost five bits of the
    address (log[2] 32 = 5) comprise the offset portion. Five bits allows the offset
    to uniquely identify any of the bytes in a block.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '*偏移量*：32 字节块大小意味着地址的最右边五位（log[2] 32 = 5）构成偏移量部分。五位可以唯一标识块中的任何字节。'
- en: '*Index*: A 64-set cache implies that the next six bits of the address (log[2]
    64 = 6) comprise the index portion. Six bits allows the index to uniquely identify
    each set in the cache.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '*索引*：一个 64 集合的缓存意味着地址的下六位（log[2] 64 = 6）构成索引部分。六位可以唯一标识缓存中的每个集合。'
- en: '*Tag*: The tag consists of any remaining bits of the address that don’t belong
    to the offset or index. Here, the address has five remaining bits left over for
    the tag (16 – (5 + 6) = 5).'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '*标签*：标签由地址中不属于偏移量或索引的剩余位组成。这里，地址剩下 5 位用于标签（16 - （5 + 6）= 5）。'
- en: '![image](../images/11fig23.jpg)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/11fig23.jpg)'
- en: '*Figure 11-23: Read from address 1010000001100100\. Both lines at index 000011
    (set 3) are invalid, so the request misses, and the cache loads data from main
    memory. The set’s LRU bit is 0, so the cache loads data into the left line and
    updates the LRU bit to 1.*'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-23：从地址 1010000001100100 读取。索引 000011（集合 3）处的两行都无效，因此请求未命中，缓存从主存中加载数据。该集合的
    LRU 位为 0，缓存将数据加载到左侧行，并更新 LRU 位为 1。*'
- en: '![image](../images/11fig24.jpg)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/11fig24.jpg)'
- en: '*Figure 11-24: Read from address 1010000001100111\. The left line at index
    000011 (set 3) holds a matching tag, so the request hits.*'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-24：从地址 1010000001100111 读取。索引 000011（集合 3）处的左侧行持有匹配的标签，因此请求命中。*'
- en: '![image](../images/11fig25.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/11fig25.jpg)'
- en: '*Figure 11-25: Read from address 1001000000100000\. Both lines at index 000001
    (set 1) are invalid, so the request misses, and the cache loads data from main
    memory. The set’s LRU bit is 0, so the cache loads data into the left line and
    updates the LRU bit to 1.*'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-25：从地址 1001000000100000 读取。索引 000001（集合 1）处的两行都无效，因此请求未命中，缓存从主存中加载数据。该集合的
    LRU 位为 0，缓存将数据加载到左侧行，并更新 LRU 位为 1。*'
- en: '![image](../images/11fig26.jpg)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/11fig26.jpg)'
- en: '*Figure 11-26: Read from address 1111000001100101\. At index 000011 (set 3),
    one line’s tag doesn’t match, and the other line is invalid, so the request misses.
    The set’s LRU bit is 1, so the cache loads data into the right line and updates
    the LRU bit to 0.*'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-26：从地址 1111000001100101 读取。在索引 000011（集合 3）处，一行的标签不匹配，另一行无效，因此请求未命中。该集合的
    LRU 位为 1，缓存将数据加载到右侧行，并更新 LRU 位为 0。*'
- en: '![image](../images/11fig27.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/11fig27.jpg)'
- en: '*Figure 11-27: Write to address 1111000001100000\. The right line at index
    000011 (set 3) is valid and holds a matching tag, so the request hits. Because
    this access is a write, the cache sets the line’s dirty bit to 1\. The LRU bit
    remains 0 to indicate that the left line remains least recently used.*'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '*图11-27：写入地址1111000001100000。索引000011（集合3）处的右行有效且具有匹配标签，因此请求命中。由于此访问是写入操作，缓存将该行的脏位设置为1。LRU位保持为0，表示左行仍然是最近最少使用的。*'
- en: '![image](../images/11fig28.jpg)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/11fig28.jpg)'
- en: '*Figure 11-28: Write to address 1010000001100100\. The left line at index 000011
    (set 3) is valid and holds a matching tag, so the request hits. Because this access
    is a write, the cache sets the line’s dirty bit to 1\. After accessing the left
    line, the cache sets the line’s LRU bit to 1.*'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '*图11-28：写入地址1010000001100100。索引000011（集合3）处的左行有效且具有匹配标签，因此请求命中。由于此访问是写入操作，缓存将该行的脏位设置为1。访问左行后，缓存将该行的LRU位设置为1。*'
- en: In this example, the same memory access sequence that produced two conflict
    misses with a direct-mapped cache suffers from no conflicts with a two-way set
    associative cache.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，相同的内存访问序列，在直接映射缓存中导致了两个冲突未命中，但在双路组相联缓存中则没有发生冲突。
- en: 11.5 Cache Analysis and Valgrind
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5 缓存分析与Valgrind
- en: Because caches significantly influence program performance, most systems provide
    profiling tools to measure a program’s use of the cache. One such tool is Valgrind’s
    `cachegrind` mode, which this section uses to evaluate cache performance.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 由于缓存对程序性能有显著影响，大多数系统提供了分析工具来测量程序对缓存的使用情况。一个这样的工具是Valgrind的`cachegrind`模式，本节将使用它来评估缓存性能。
- en: 'Consider the following program that generates a random *N* × *N* matrix:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下程序，它生成一个随机的 *N* × *N* 矩阵：
- en: '[PRE5]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Prior sections in this chapter introduced two functions for averaging every
    element of a matrix. They differ only in the way they index into the matrix:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 本章前面的章节介绍了两种对矩阵中每个元素进行平均化的函数。它们的区别仅在于访问矩阵的索引方式：
- en: Version 1
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 版本 1
- en: '[PRE6]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Version 2
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 版本 2
- en: '[PRE7]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This section uses cache profiling tools to quantify the differences between
    them.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 本节使用缓存分析工具来量化它们之间的差异。
- en: '11.5.1 A First Cut: Theoretical Analysis and Benchmarking'
  id: totrans-269
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.5.1 初步分析：理论分析与基准测试
- en: A theoretical analysis based on locality and the memory hierarchy suggests that
    the first version exhibits better spatial locality (on matrix `mat`) due to the
    fact that `mat` is stored in row-major order in memory (see the section “Two-Dimensional
    Array Memory Layout” on [page 86](ch02.xhtml#lev3_13)). The second solution has
    poor spatial locality because each element in the matrix is visited in column-major
    order. Recall that data is loaded into a cache in *blocks*. Traversing the matrix
    in column-major order will likely lead to more cache misses, resulting in poorer
    performance.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 基于局部性和内存层级的理论分析表明，第一种版本由于 `mat` 矩阵在内存中是以行主序（row-major）顺序存储的，因此在空间局部性上表现得更好（参见[第86页](ch02.xhtml#lev3_13)中的“二维数组内存布局”部分）。第二种解决方案由于每个元素都是按列主序（column-major）顺序访问，因此空间局部性较差。回忆一下，数据是按*块*加载到缓存中的。按列主序访问矩阵可能会导致更多的缓存未命中，从而导致性能较差。
- en: 'Let’s modify the main function to include calls to the `gettimeofday` function
    to accurately measure the difference in performance between the two versions:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们修改主函数，增加对 `gettimeofday` 函数的调用，以准确测量两个版本之间的性能差异：
- en: '[PRE8]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Compiling the code and running it yields the following result (note that the
    times will vary based on the machine on which it’s run):'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 编译代码并运行后，得到以下结果（请注意，时间会根据运行它的机器有所不同）：
- en: '[PRE9]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: That’s a big difference! In essence, the solution using row-major order is 4.61
    times faster than the second one!
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个很大的差异！本质上，使用行主序（row-major）顺序的解决方案比第二种方式快4.61倍！
- en: '11.5.2 Cache Analysis in the Real World: Cachegrind'
  id: totrans-276
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.5.2 真实世界中的缓存分析：Cachegrind
- en: Theoretically analyzing the two solutions and then running them verifies that
    the first version is faster than the second. However, it doesn’t confirm the details
    of the cache analysis. Fortunately, the Valgrind suite of tools can help. Earlier
    in the book, we discussed how Valgrind can help find memory leaks in a program
    (see “Debugging Memory with Valgrind” on [page 168](ch03.xhtml#lev1_22)). This
    section describes Cachegrind, Valgrind’s cache simulator. Cachegrind enables a
    programmer to study how a program or particular function affects the cache.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 理论分析这两种解决方案并运行它们验证了第一版比第二版更快。然而，它并没有确认缓存分析的细节。幸运的是，Valgrind工具套件可以提供帮助。在本书前面部分，我们讨论了Valgrind如何帮助查找程序中的内存泄漏（参见[第168页](ch03.xhtml#lev1_22)的“使用Valgrind调试内存”）。本节介绍了Cachegrind，Valgrind的缓存模拟器。Cachegrind使程序员能够研究程序或特定函数如何影响缓存。
- en: 'Cachegrind simulates how a program interacts with the computer’s cache hierarchy.
    In many cases, Cachegrind can autodetect the cache organization of a machine.
    In the cases that it cannot, Cachegrind still simulates the first level (L1) cache
    and the last level (LL) cache. It assumes the first level cache has two independent
    components: the instruction cache and the data cache. The reason for this is that
    the last level cache has the most important implications for runtime. L1 caches
    also have the lowest level of associativity, so it’s important to ensure that
    programs interact well with it. These assumptions match the structure of most
    modern machines.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: Cachegrind模拟程序与计算机缓存层次结构的交互。在许多情况下，Cachegrind可以自动检测机器的缓存组织结构。如果无法检测，Cachegrind仍会模拟第一级（L1）缓存和最后一级（LL）缓存。它假设第一级缓存有两个独立的组成部分：指令缓存和数据缓存。这样做的原因是最后一级缓存对运行时性能的影响最大。L1缓存的关联度也最低，因此确保程序与其良好交互非常重要。这些假设与大多数现代计算机的结构相符。
- en: 'Cachegrind collects and outputs the following information:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: Cachegrind收集并输出以下信息：
- en: Instruction cache reads (`Ir`)
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指令缓存读取（`Ir`）
- en: L1 instruction cache read misses (`I1mr`) and LL cache instruction read misses
    (`ILmr`)
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L1指令缓存读取未命中（`I1mr`）和LL缓存指令读取未命中（`ILmr`）
- en: Data cache reads (`Dr`)
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据缓存读取（`Dr`）
- en: D1 cache read misses (`D1mr`) and LL cache data misses (`DLmr`)
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: D1缓存读取未命中（`D1mr`）和LL缓存数据未命中（`DLmr`）
- en: Data cache writes (`Dw`)
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据缓存写入（`Dw`）
- en: D1 cache write misses (`D1mw`) and LL cache data write misses (`DLmw`)
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: D1缓存写入未命中（`D1mw`）和LL缓存数据写入未命中（`DLmw`）
- en: Note that D1 total access is computed by `D1` = `D1mr` + `D1mw` and LL total
    access is given by `ILmr` + `DLmr` + `DLmw`.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，D1总访问量通过`D1` = `D1mr` + `D1mw`计算，LL总访问量则由`ILmr` + `DLmr` + `DLmw`给出。
- en: 'Let’s see how well version 1 of the code operates under Cachegrind. To run
    it, execute Valgrind on the compiled code with the following command:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看版本1的代码在Cachegrind下的表现如何。要运行它，请使用以下命令执行已编译的代码，并在Valgrind上运行：
- en: '[PRE10]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In this invocation, Valgrind’s `cachegrind` tool acts as a wrapper around the
    `cachex` executable. Choosing a smaller matrix size for Cachegrind aids in the
    speed of execution. Cachegrind outputs information about the number of cache hits
    and misses in the overall program:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个调用中，Valgrind的`cachegrind`工具作为`cachex`可执行文件的包装器。为Cachegrind选择更小的矩阵大小有助于提高执行速度。Cachegrind输出有关整个程序缓存命中和未命中的信息：
- en: '[PRE11]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'However, this analysis is interested *specifically* in the hits and misses
    for the two versions of this averaging function. To view that information, use
    the Cachegrind tool `cg_annotate`. Running Cachegrind should have produced a file
    in the current working directory that looks similar to `cachegrind.out.n`, where
    `n` is some process ID number. To run `cg_annotate`, type in the following command
    (replacing `cachegrind.out.28657` with the name of the output file):'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这次分析特别关注这两个版本的平均函数的命中和未命中情况。要查看这些信息，可以使用Cachegrind工具`cg_annotate`。运行Cachegrind时应该会在当前工作目录下生成一个类似于`cachegrind.out.n`的文件，其中`n`是某个进程ID号。要运行`cg_annotate`，输入以下命令（将`cachegrind.out.28657`替换为输出文件的名称）：
- en: '[PRE12]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We’ve edited the output from this command slightly to focus on the two versions
    of the average function. This output shows that version 2 yields 1,062,996 data
    misses, compared to only `62688` misses in version 1\. Cachegrind provides solid
    proof that our analysis is correct!
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍微编辑了该命令的输出，专注于两个版本的平均函数。该输出显示，版本2产生了1,062,996次数据未命中，而版本1则只有`62688`次未命中。Cachegrind提供了有力的证据，证明我们的分析是正确的！
- en: '11.6 Looking Ahead: Caching on Multicore Processors'
  id: totrans-294
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.6 展望未来：多核处理器上的缓存
- en: So far our discussion of caching has focused on a single level of cache memory
    on a single-core processor. Modern processors, however, are multicore with several
    levels of cache memory. Typically, each core maintains its own private cache memory
    at the highest level(s) of the memory hierarchy and shares a single cache with
    all cores at lower levels. [Figure 11-29](ch11.xhtml#ch11fig29) shows an example
    of the memory hierarchy on a four-core processor in which each core contains a
    private level 1 (L1) cache, and the level 2 (L2) cache is shared by all four cores.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们对缓存的讨论集中在单核处理器上的单一级别的缓存内存。然而，现代处理器是多核的，且有多个级别的缓存内存。通常，每个核心在内存层次结构的最高级别维护自己的私有缓存内存，并在较低级别与所有核心共享一个单一的缓存。[图11-29](ch11.xhtml#ch11fig29)展示了一个四核处理器的内存层次结构示例，其中每个核心包含一个私有的一级（L1）缓存，而二级（L2）缓存由所有四个核心共享。
- en: '![image](../images/11fig29.jpg)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/11fig29.jpg)'
- en: '*Figure 11-29: An example memory hierarchy on a multicore processor. Each of
    the four cores has its own private L1 cache, and all four cores share a single
    L2 cache that they access through a shared bus. The multicore processor connects
    to RAM via the memory bus.*'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '*图11-29：一个多核处理器的内存层次结构示例。四个核心每个都有自己的私有L1缓存，所有四个核心通过共享总线访问单一的L2缓存。多核处理器通过内存总线连接到RAM。*'
- en: Recall that higher levels of the memory hierarchy are faster to access and smaller
    in capacity than lower levels of the memory hierarchy. Thus, an L1 cache is smaller
    and faster than an L2 cache, which in turn is smaller and faster than RAM. Also
    recall that cache memory stores a copy of a value from a lower level in the memory
    hierarchy; a value stored in a L1 cache is a copy of the same value stored in
    the L2 cache, which is a copy of the same value stored in RAM. As a result, higher
    levels of the memory hierarchy serve as caches for lower levels. Thus, for the
    example in [Figure 11-29](ch11.xhtml#ch11fig29), the L2 cache is a cache of RAM
    contents, and each core’s L1 cache is a cache of the L2 cache contents.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，内存层次结构的较高级别比较低级别的访问速度更快，但容量更小。因此，L1缓存比L2缓存小且更快，L2缓存又比RAM小且更快。还要记住，缓存内存存储的是来自内存层次结构较低级别的值的副本；存储在L1缓存中的值是存储在L2缓存中的同一个值的副本，而L2缓存中的值又是存储在RAM中的同一个值的副本。因此，内存层次结构的较高级别充当较低级别的缓存。因此，在[图11-29](ch11.xhtml#ch11fig29)中的例子中，L2缓存是RAM内容的缓存，每个核心的L1缓存是L2缓存内容的缓存。
- en: Each core in a multicore processor simultaneously executes an independent stream
    of instructions, often from separate programs. Providing each core a private L1
    cache allows the core to store copies of the data and instructions exclusively
    from the instruction stream it’s executing in its fastest cache memory. In other
    words, each core’s L1 cache stores a copy of only those blocks of memory that
    are from its execution stream as opposed to competing for space in a single L1
    cache shared by all cores. This design yields a higher hit rate in each core’s
    private L1 cache (in its fastest cache memory) than one in which all cores share
    a single L1 cache.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 多核处理器中的每个核心同时执行独立的指令流，通常来自不同的程序。为每个核心提供一个私有的L1缓存，允许核心将其执行的指令流中的数据和指令存储在它的最快缓存内存中。换句话说，每个核心的L1缓存只存储来自其执行流的内存块副本，而不是在所有核心共享的单一L1缓存中竞争空间。这种设计使得每个核心的私有L1缓存（即其最快缓存内存）比所有核心共享单一L1缓存时的命中率更高。
- en: Today’s processors often include more than two levels of cache. Three levels
    are common in desktop systems, with the highest level (L1) typically split into
    two separate L1 caches, one for program instructions and the other for program
    data. Lower-level caches are usually *unified caches*, meaning that they store
    both program data and instructions. Each core usually maintains a private L1 cache
    and shares a single L3 cache with all cores. The L2 cache layer, which sits between
    each core’s private L1 cache and the shared L3 cache, varies substantially in
    modern CPU designs. The L2 may be a private L2 cache, may be shared by all cores,
    or may be a hybrid organization with multiple L2 caches, each shared by a subset
    of cores.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 现代处理器通常包含超过两个级别的缓存。桌面系统中常见的是三级缓存，最高级别（L1）通常被分为两个独立的L1缓存，一个用于程序指令，另一个用于程序数据。低级缓存通常是*统一缓存*，即它们同时存储程序数据和指令。每个核心通常维持一个私有的L1缓存，并与所有核心共享一个单一的L3缓存。L2缓存层位于每个核心的私有L1缓存和共享的L3缓存之间，在现代CPU设计中差异很大。L2可能是一个私有的L2缓存，可能是所有核心共享的，或者可能是一个混合结构，其中包含多个L2缓存，每个L2缓存由一部分核心共享。
- en: PROCESSOR AND CACHE INFORMATION IN LINUX SYSTEMS
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: LINUX系统中的处理器和缓存信息
- en: 'If you’re curious about your CPU’s design, there are several ways to obtain
    information about a processor and its cache organization on your system. For example,
    the `lscpu` command displays information about the processor, including its number
    of cores and the levels and sizes of its caches:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对CPU的设计感兴趣，可以通过几种方式获取有关处理器及其缓存组织在系统上的信息。例如，`lscpu`命令显示有关处理器的信息，包括其核心数、缓存的级别和大小。
- en: '[PRE13]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This output shows that there are six total cores (the number of `Socket(s)`
    multiplied by the `Core(s) per socket`), and that each core is two-way hyper-
    threaded (`Thread(s) per core`) to make the six physical cores appear as 12 CPUs
    to the operating system (see ”Multicore and Hardware Multithreading” in [Chapter
    5](ch05.xhtml#ch05) for more information on hardware multithreading). Additionally,
    the output shows that there are three levels of cache (`L1`, `L2`, and `L3`),
    and that there are two separate L1 caches, one for caching data (`L1d`) and another
    for caching instructions (`L1i`).
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 此输出显示总共有六个核心（`Socket(s)`乘以`每个插槽的核心数`），每个核心都是双路超线程的（`每个核心的线程数`），使得这六个物理核心在操作系统中呈现为12个CPU（有关硬件多线程，请参阅[第5章](ch05.xhtml#ch05)中的”多核和硬件多线程“）。此外，输出显示有三级缓存（`L1`、`L2`和`L3`），并且有两个单独的L1缓存，一个用于缓存数据（`L1d`），另一个用于缓存指令（`L1i`）。
- en: In addition to `lscpu`, files in the `/proc` and `/sys` filesystems contain
    information about the processor. For example, the command `cat /proc/cpuinfo`
    outputs information about the processor, and the following command lists information
    about the caches for a specific processor core (note that these files are named
    in terms of a core’s hyperthreaded CPUs, and in this example `cpu0` and `cpu6`
    are the two hyperthreaded CPUs on core 0).
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`lscpu`命令外，`/proc`和`/sys`文件系统中的文件还包含有关处理器的信息。例如，命令`cat /proc/cpuinfo`会输出有关处理器的信息，以下命令列出特定处理器核心缓存的信息（请注意，这些文件以核心的超线程CPU命名，例如在此示例中，`cpu0`和`cpu6`是核心0上的两个超线程CPU）。
- en: '[PRE14]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This output indicates that core 0 has four caches (`index0` to `index3`). To
    see the details of each cache, examine the index directory’s `type`, `level`,
    and `shared` `_cpu_list` files:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 此输出表明核心0具有四个缓存（`index0`到`index3`）。要查看每个缓存的详细信息，请查看索引目录的`type`、`level`和`shared`
    `_cpu_list`文件。
- en: '[PRE15]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The `type` output indicates that core 0 has separate data and instruction caches
    as well as two other unified caches. Correlating the `level` output with the `type`
    output reveals that the data and instruction caches are both L1 caches, whereas
    the unified caches are L2 and L3 caches, respectively. The `shared_cpu_list` further
    shows that the L1 and L2 caches are private to core 0 (shared only by CPU `0`
    and `6`, the two hyperthread CPUs on core 0), and that the L3 cache is shared
    by all six cores (by all 12 hyperthreaded CPUs, `0-11`).
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '`type`输出表明核心0具有独立的数据和指令缓存以及其他两个统一缓存。将`level`输出与`type`输出相关联可知，数据和指令缓存都是L1缓存，而统一缓存分别是L2和L3缓存。`shared_cpu_list`进一步显示，L1和L2缓存是核心0专用的（仅由CPU
    `0`和`6`共享，即核心0上的两个超线程CPU），而L3缓存则由所有六个核心共享（所有12个超线程CPU，`0-11`）。'
- en: 11.6.1 Cache Coherency
  id: totrans-310
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.6.1 缓存一致性
- en: Because programs typically exhibit a high degree of locality of reference, it
    is advantageous for each core to have its own L1 cache to store copies of the
    data and instructions from the instruction stream it executes. However, multiple
    L1 caches can result in *cache coherency* problems. Problems with cache coherency
    arise when the value of a copy of a block of memory stored in one core’s L1 cache
    is different than the value of a copy of the same block stored in another core’s
    L1 cache. This situation occurs when one core writes to a block cached in its
    L1 cache that is also cached in other core’s L1 caches. Because a cache block
    contains a copy of memory contents, the system needs to maintain a coherent single
    value of the memory contents across all copies of the cached block. implement
    a *cache-coherence protocol* to ensure a coherent view of memory that can be cached
    and accessed by multiple cores. A cache coherency protocol ensures that any core
    accessing a memory location sees the most recently modified value of that memory
    location rather than seeing an older (stale) copy of the value that may be stored
    in its L1 cache.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 由于程序通常表现出高度的局部性引用特性，因此每个核心拥有自己的L1缓存，用于存储从其执行的指令流中获取的数据和指令是有优势的。然而，多个L1缓存可能会导致*缓存一致性*问题。当一个核心的L1缓存中存储的内存块副本的值与另一个核心L1缓存中存储的同一块内存的副本值不同时，就会出现缓存一致性问题。这种情况发生在一个核心向其L1缓存中已缓存的块写入数据，而该块同时也被其他核心的L1缓存缓存时。由于缓存块包含内存内容的副本，因此系统需要在所有缓存的块副本中维护一致的内存内容值。为了确保多个核心可以缓存并访问内存，必须实现一个*缓存一致性协议*。缓存一致性协议确保任何访问某内存位置的核心看到的是该内存位置的最新修改值，而不是看到可能存储在其L1缓存中的过时（陈旧）副本。
- en: 11.6.2 The MSI Protocol
  id: totrans-312
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.6.2 MSI协议
- en: 'There are many different cache coherency protocols. Here, we discuss the details
    of one example, the MSI protocol. The *MSI protocol* (Modified, Shared, Invalid)
    adds three flags (or bits) to each cache line. A flag’s value is either clear
    (0) or set (1). The values of the three flags encode the state of its data block
    with respect to cache coherency with other cached copies of the block, and their
    values trigger cache coherency actions on read or write accesses to the data block
    in the cache line. The three flags used by the MSI protocol are:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多不同的缓存一致性协议。在这里，我们讨论一个示例的详细信息——MSI协议。*MSI协议*（修改、共享、无效）为每个缓存行添加了三个标志（或位）。一个标志的值可以是清除（0）或设置（1）。这三个标志的值编码了其数据块相对于与其他缓存副本的一致性状态，并且它们的值会触发对数据块的读取或写入访问时的缓存一致性操作。MSI协议使用的三个标志是：
- en: The *M* flag that, if set, indicates the block has been modified, meaning that
    this core has written to its copy of the cached value.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*M*标志，若设置，表示该块已被修改，即该核心已经写入了其缓存值的副本。'
- en: The *S* flag that, if set, indicates that the block is unmodified and can be
    safely shared, meaning that multiple L1 caches may safely store a copy of the
    block and read from their copy.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*S*标志，若设置，表示该块未修改且可以安全共享，即多个L1缓存可以安全地存储该块的副本并从其副本读取。'
- en: The *I* flag that, if set, indicates if the cached block is invalid or contains
    stale data (is an older copy of the data that does not reflect the current value
    of the block of memory).
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*I*标志，若设置，表示缓存块无效或包含陈旧数据（是数据块的旧副本，不反映内存块的当前值）。'
- en: The MSI protocol is triggered on read and write accesses to cache entries.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: MSI协议在读取和写入访问缓存条目时触发。
- en: 'On a read access:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 对于读取访问：
- en: If the cache block is in the M or S state, then the cached value is used to
    satisfy the read (its copy’s value is the most current value of the block of memory).
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果缓存块处于M或S状态，则使用缓存的值来满足读取操作（其副本的值是内存块的最新值）。
- en: If the cache block is in the I state, then the cached copy is out of date with
    a newer version of the block, and the block’s new value needs to be loaded into
    the cache line before the read can be satisfied.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果缓存块处于I状态，则缓存的副本与块的新版本不同，必须将块的新值加载到缓存行中，才能满足读取操作。
- en: If another core’s L1 cache stores the new value (it stores the value with the
    M flag set indicating that it stores a modified copy of the value), that other
    core must first write its value back to the lower level (e.g., to the L2 cache).
    After performing the write-back, it clears the M flag with the cache line (its
    copy and the copy in the lower-level are now consistent) and sets the S bit to
    indicate that the block in this cache line is in a state that can be safely cached
    by other cores (the L1 block is consistent with its copy in the L2 cache and the
    core read the current value of the block from this L1 copy).
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The core that initiated the read access on an line with the I flag set can then
    load the new value of the block into its cache line. It clears the I flag indicating
    that the block is now valid and stores the new value of the block, sets the S
    flag indicating that the block can be safely shared (it stores the latest value
    and is consistent with other cached copies), and clears the M flag indicating
    that the L1 block’s value matches that of the copy stored in the L2 cache (a read
    does not modify the L1 cached copy of the memory).
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'On a write access:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: If the block is in the M state, then write to the cached copy of the block.
    No changes to the flags are needed (the block remains in the M state).
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the block is in the I or the S state, then notify other cores that the block
    is being written to (modified). Other L1 caches that have the block stored in
    the S state need to clear the S bit and set the I bit on their block (their copies
    of the block are now out of date with the copy that is being written to by the
    other core). If another L1 cache has the block in the M state, it will write its
    block back to the lower level and set its copy to I. The core writing will then
    load the new value of the block into its L1 cache, set the M flag (its copy will
    be modified by the write), and clear the I flags (its copy is now valid), and
    write to the cached block.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Figure 11-30](ch11.xhtml#ch11fig30) through [Figure 11-32](ch11.xhtml#ch11fig32)
    step through an example of the MSI protocol applied to ensure coherency of read
    and write accesses to a block of memory that is cached in two core’s private L1
    caches. In [Figure 11-30](ch11.xhtml#ch11fig30) our example starts with the shared
    data block copied into both core’s L1 cache with the S flag set, meaning that
    the L1 cached copies are the same as the value of the block in the L2 cache (all
    copies store the current value of the block, 6). At this point, both core 0 and
    core 1 can safely read from the copy stored in their L1 caches without triggering
    coherency actions (the S flag indicates that their shared copy is up to date).'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/11fig30.jpg)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-30: At the start, both cores have a copy of the block in their private
    L1 caches with the S flag set (in Shared mode)*'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: If core 0 next writes to the copy of the block stored in its L1 cache, its L1
    cache controller notifies the other L1 caches to invalidate their copy of the
    block. Core 1’s L1 cache controller then clears the S flag and sets the I flag
    on its copy, indicating that its copy of the block is stale. Core 0 writes to
    its copy of the block in its L1 cache (changing its value to 7 in our example)
    and sets the M flag and clears the S flag on the cache line to indicate that its
    copy has been modified and stores the current value of the block. At this point,
    the copy in the L2 cache and in core 1’s L1 cache are stale. The resulting cache
    state is shown in [Figure 11-31](ch11.xhtml#ch11fig31).
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 Core 0 接下来写入存储在其 L1 缓存中的该块副本，其 L1 缓存控制器会通知其他 L1 缓存使其副本无效。随后，Core 1 的 L1 缓存控制器清除
    S 标志，并设置 I 标志，表示其副本已过时。Core 0 向其 L1 缓存中的块副本写入数据（在我们的例子中将其值更改为 7），并设置 M 标志，清除 S
    标志，表示其副本已被修改并存储了当前的块值。此时，L2 缓存中的副本和 Core 1 的 L1 缓存中的副本都已经过时。结果缓存状态如[图 11-31](ch11.xhtml#ch11fig31)所示。
- en: '![image](../images/11fig31.jpg)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/11fig31.jpg)'
- en: '*Figure 11-31: The resulting state of the caches after Core 0 writes to its
    copy of the block*'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-31：Core 0 向其副本写入数据后的缓存状态*'
- en: At this point, core 0 can safely read from its copy of the cached block because
    its copy is in the M state, meaning that it stores the most recently written value
    of the block.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，Core 0 可以安全地从其缓存的块副本中读取数据，因为其副本处于 M 状态，表示它存储了该块的最新写入值。
- en: If core 1 next reads from the memory block, the I flag on its L1 cached copy
    indicates that its L1 copy of the block is stale and cannot be used to satisfy
    the read. Core 1’s L1 cache controller must first load the new value of the block
    into the L1 cache before the read can be satisfied. To achieve this, core 0’s
    L1 cache controller must first write its modified value of the block back to the
    L2 cache, so that core 1’s L1 cache can read the new value of the block into its
    L1 cache. The result of these actions, (shown in [Figure 11-32](ch11.xhtml#ch11fig32)),
    is that core 0 and core 1’s L1 cached copies of the block are now both stored
    in the S state, indicating that each core’s L1 copy is up to data and can be safely
    used to satisfy subsequent reads to the block.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 Core 1 接下来读取该内存块，则其 L1 缓存副本上的 I 标志表示该副本已过时，不能用来满足读取。Core 1 的 L1 缓存控制器必须先将该块的新值加载到
    L1 缓存中，才能满足读取。为此，Core 0 的 L1 缓存控制器必须首先将其修改后的块值写回到 L2 缓存中，这样 Core 1 的 L1 缓存才能读取该块的新值到其
    L1 缓存中。这些操作的结果（如[图 11-32](ch11.xhtml#ch11fig32)所示），是 Core 0 和 Core 1 的 L1 缓存副本都存储在
    S 状态，表示每个核心的 L1 副本已经是最新的，可以安全地用于满足对该块的后续读取。
- en: '![image](../images/11fig32.jpg)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/11fig32.jpg)'
- en: '*Figure 11-32: The resulting state of the caches after Core 1 next reads the
    block*'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-32：Core 1 接下来读取该块后的缓存状态*'
- en: 11.6.3 Implementing Cache Coherency Protocols
  id: totrans-336
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.6.3 实现缓存一致性协议
- en: To implement a cache coherency protocol, a processor needs some mechanism to
    identify when accesses to the other cores’ L1 cache contents require coherency
    state changes involving the other cores’ L1 cache contents. One way this mechanism
    is implemented is through *snooping* on a bus that is shared by all L1 caches.
    A snooping cache controller listens (or snoops) on the bus for reads or writes
    to blocks that it caches. Because every read and write request is in terms of
    a memory address, a snooping L1 cache controller can identify any read or write
    from another L1 cache for a block it stores, and can then respond appropriately
    based on the coherency protocol. For example, it can set the I flag on a cache
    line when it snoops a write to the same address by another L1 cache. This example
    is how a *write-invalidate protocol* would be implemented with snooping.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现缓存一致性协议，处理器需要一种机制来识别何时需要对其他核心的 L1 缓存内容进行一致性状态变更。一种实现该机制的方法是通过在所有 L1 缓存共享的总线上进行
    *监听*。监听缓存控制器会在总线上监听（或称为嗅探）读取或写入它缓存的块。因为每个读取和写入请求都是基于内存地址的，所以监听的 L1 缓存控制器可以识别来自另一个
    L1 缓存的对它所存储的块的任何读写操作，并可以根据一致性协议做出相应的反应。例如，当它嗅探到另一个 L1 缓存对相同地址的写操作时，可以在缓存行上设置 I
    标志。这就是如何通过嗅探实现 *写失效协议* 的例子。
- en: MSI and other similar protocols such as MESI and MOESI are write-invalidate
    protocols; that is, protocols that invalidate copies of cached entries on writes.
    Snooping can also be used by write-update cache coherency protocols, where the
    new value of a data is snooped from the bus and applied to update all copies stored
    in other L1 caches.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: Instead of snooping, a directory-based cache coherence mechanism can be used
    to trigger cache coherency protocols. This method scales better than snooping
    due to performance limitations of multiple cores sharing a single bus. However,
    directory-based mechanisms require more state to detect when memory blocks are
    shared, and are slower than snooping.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: 11.6.4 More About Multicore Caching
  id: totrans-340
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The benefits to performance of each core of a multicore processor having its
    own separate cache(s) at the highest levels of the memory hierarchy, which are
    used to store copies of only the program data and instructions that it executes,
    is worth the added extra complexity of the processor needing to implement a cache
    coherency protocol.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: Although cache coherency solves the memory coherency problem on multicore processors
    with separate L1 caches, there is another problem that can occur as a result of
    cache coherency protocols on multicore processors. This problem, called *false
    sharing*, may occur when multiple threads of a single multithreaded parallel program
    are running simultaneously across the multiple cores and are accessing memory
    locations that are near to those accessed by other threads. In section 14.5, we
    discuss the false sharing problem and some solutions to it.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: For more information and details about hardware caching on multicore processors,
    including different protocols and how they are implemented, refer to a computer
    architecture textbook.^([8](ch11.xhtml#fn11_8))
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: 11.7 Summary
  id: totrans-344
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This chapter explored the characteristics of computer storage devices and their
    trade-offs with respect to key measures like access latency, storage capacity,
    transfer latency, and cost. Because devices embody many disparate design and performance
    trade-offs, they naturally form a memory hierarchy, which arranges them according
    to their capacity and access time. At the top of the hierarchy, primary storage
    devices like CPU caches and main memory quickly provide data directly to the CPU,
    but their capacity is limited. Lower in the hierarchy, secondary storage devices
    like solid-state drives and hard disks offer dense bulk storage at the cost of
    performance.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: Because modern systems require both high capacity and good performance, system
    designers build computers with multiple forms of storage. Crucially, the system
    must manage which storage device holds any particular chunk of data. Systems aim
    to store data that’s being actively used in faster storage devices, and they relegate
    infrequently used data to slower storage devices.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: 'To determine which data is being used, systems rely on program data access
    patterns known as *locality*. Programs exhibit two important types of locality:
    *temporal locality*, whereby programs tend to access the same data repeatedly
    over time, and *spatial locality*, whereby programs tend to access data that is
    nearby other, previously accessed data.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: Locality serves as the basis for CPU caches, which store a small subset of main
    memory in fast storage directly on the CPU chip. When a program attempts to access
    main memory, the CPU first checks for the data in the cache; if it finds the data
    there, it can avoid the more costly trip to main memory.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: When a program issues a request to read or write memory, it provides the address
    of the memory location that it wants to access. CPU caches use three sections
    of the bits in a memory address to identify which subset of main memory a cache
    line stores. The middle *index* bits of an address map the address to a storage
    location in the cache, the high-order *tag* bits uniquely identify which subset
    of memory the cache location stores, and the low-order *offset* bits identify
    which bytes of stored data the program wants to access.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: Finally, this chapter concluded by demonstrating how the Cachegrind tool can
    enable cache performance profiling for a running program. Cachegrind simulates
    a program’s interaction with the cache hierarchy and collects statistics about
    a program’s use of the cache (e.g., the hit and miss rates).
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: Notes
  id: totrans-351
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[1.](ch11.xhtml#rfn11_1) *[https://www.youtube.com/watch?v=9eyFDBPk4Yw](https://www.youtube.com/watch?v=9eyFDBPk4Yw)*'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: '[2.](ch11.xhtml#rfn11_2) *[https://en.wikipedia.org/wiki/Punched_card](https://en.wikipedia.org/wiki/Punched_card)*'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '[3.](ch11.xhtml#rfn11_3) *[https://en.wikipedia.org/wiki/Magnetic_tape_data_storage](https://en.wikipedia.org/wiki/Magnetic_tape_data_storage)*'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: '[4.](ch11.xhtml#rfn11_4) *[https://en.wikipedia.org/wiki/Floppy_disk](https://en.wikipedia.org/wiki/Floppy_disk)*'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: '[5.](ch11.xhtml#rfn11_5) *[https://en.wikipedia.org/wiki/Optical_disc](https://en.wikipedia.org/wiki/Optical_disc)*'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: '[6.](ch11.xhtml#rfn11_6) *[https://en.wikipedia.org/wiki/Hard_disk_drive](https://en.wikipedia.org/wiki/Hard_disk_drive)*'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: '[7.](ch11.xhtml#rfn11_7) *[https://en.wikipedia.org/wiki/Flash_memory](https://en.wikipedia.org/wiki/Flash_memory)*'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: '[8.](ch11.xhtml#rfn11_8) One suggestion is “Computer Organization and Design:
    The Hardware and Software Interface,” by David A. Patterson and John L. Hennessy.'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
