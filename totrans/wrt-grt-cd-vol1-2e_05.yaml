- en: '**6'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MEMORY ORGANIZATION AND ACCESS**
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/comm1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This chapter describes the basic components of a computer system: the CPU,
    memory, I/O, and the bus that connects them. We’ll begin by discussing bus organization
    and memory organization. These two hardware components may have as large a performance
    impact on your software as the CPU’s speed. Understanding memory performance characteristics,
    data locality, and cache operation can help you design software that runs as fast
    as possible.'
  prefs: []
  type: TYPE_NORMAL
- en: '**6.1 The Basic System Components**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The basic operational design of a computer system is called its *architecture*.
    John von Neumann, a pioneer in computer design, is credited with the principal
    architecture in use today. For example, the 80x86 family uses the *[von Neumann
    architecture (VNA)](gloss01.xhtml#gloss01_260)*. A typical VNA has three major
    components: the *[central processing unit (CPU)](gloss01.xhtml#gloss01_46)*, *memory*,
    and *input/output (I/O)*, as shown in [Figure 6-1](ch06.xhtml#ch06fig01).'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/06fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6-1: Typical von Neumann machine*'
  prefs: []
  type: TYPE_NORMAL
- en: In VNA machines, like the 80x86 systems, all computations occur within the CPU.
    Data and machine instructions reside in memory until the CPU requires them, at
    which point the system transfers the data into the CPU. To the CPU, most I/O devices
    look like memory; the major difference between them is that I/O devices are generally
    located in the outside world, whereas memory is located within the same machine.
  prefs: []
  type: TYPE_NORMAL
- en: '***6.1.1 The System Bus***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The *[system bus](gloss01.xhtml#gloss01_243)* connects the various components
    of a VNA machine. A *bus* is a collection of wires on which electrical signals
    pass between system components. Most CPUs have three major buses: the *data* bus,
    the *address* bus, and the *control* bus. These buses vary from processor to processor,
    but each bus carries comparable information on most CPUs. For example, the data
    buses on the Pentium and 80386 have different implementations, but both variants
    carry data between the processor, I/O, and memory.'
  prefs: []
  type: TYPE_NORMAL
- en: '**6.1.1.1 The Data Bus**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: CPUs use the data bus to shuttle data between the various components in a computer
    system. The size of this bus varies widely among CPUs. Indeed, bus size (or *width*)
    is one of the main attributes that defines the “size” of the processor.
  prefs: []
  type: TYPE_NORMAL
- en: Most modern, general-purpose CPUs (such as those in PCs) employ a 32-bit-wide
    or, more commonly, 64-bit-wide data bus. Some processors use 8-bit or 16-bit data
    buses, and there may well be some CPUs with 128-bit data buses by the time you
    read this.
  prefs: []
  type: TYPE_NORMAL
- en: 'You’ll often hear the terms *8-*, *16-*, *32-*, or *64-bit processor*. Processor
    size is determined by whichever value is smaller: the number of data lines on
    the processor or the size of the largest general-purpose integer register. For
    example, older Intel 80x86 CPUs all have 64-bit buses but only 32-bit general-purpose
    integer registers, so they’re classified as 32-bit processors. The AMD (and newer
    Intel) x86-64 processors support 64-bit integer registers and a 64-bit bus, so
    they’re 64-bit processors.'
  prefs: []
  type: TYPE_NORMAL
- en: Although the 80x86 family members with 8-, 16-, 32-, and 64-bit data buses can
    process data blocks up to the bit width of the bus, they can also access smaller
    memory units of 8, 16, or 32 bits. Therefore, anything you can do with a small
    data bus can be done with a larger data bus as well; the larger data bus, however,
    may access memory faster and can access larger chunks of data in one memory operation.
    You’ll read about the exact nature of these memory accesses a little later in
    this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '**6.1.1.2 The Address Bus**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The data bus on an 80x86 family processor transfers information between a particular
    memory location or I/O device and the CPU. *Which* memory location or I/O device
    is where the address bus comes in. The system designer assigns each memory location
    and I/O device a unique memory address. When the software wants to access a particular
    memory location or I/O device, it places the corresponding address on the address
    bus. Circuitry within the device checks the address and, if it matches, transfers
    data. All other memory locations ignore the request on the address bus.
  prefs: []
  type: TYPE_NORMAL
- en: 'With a single address bus line, a processor can access exactly two unique addresses:
    0 and 1\. With *n* address lines, the processor can access 2^(*n*) unique addresses
    (because there are 2^(*n*) unique values in an *n*-bit binary number). The number
    of bits on the address bus determines the *maximum* number of addressable memory
    and I/O locations. Early 80x86 processors, for example, provided only 20 lines
    on the address bus. Therefore, they could access only up to 1,048,576 (or 2^(20))
    memory locations. Larger address buses can access more memory (see [Table 6-1](ch06.xhtml#ch06tab01)).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 6-1:** 80x86 Addressing Capabilities'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Processor** | **Address bus size** | **Maximum addressable memory** |'
  prefs: []
  type: TYPE_TB
- en: '| 8088, 8086, 80186, 80188 | 20 | 1,048,576 (1MB) |'
  prefs: []
  type: TYPE_TB
- en: '| 80286, 80386sx | 24 | 16,777,216 (16MB) |'
  prefs: []
  type: TYPE_TB
- en: '| 80386dx | 32 | 4,294,976,296 (4GB) |'
  prefs: []
  type: TYPE_TB
- en: '| 80486, Pentium | 32 | 4,294,976,296 (4GB) |'
  prefs: []
  type: TYPE_TB
- en: '| Pentium Pro, II, III, IV | 36 | 68,719,476,736 (64GB) |'
  prefs: []
  type: TYPE_TB
- en: '| Core, i3, i5, i7, i9 | ≥ 40 | ≥1,099,511,627,776 (≥1TB) |'
  prefs: []
  type: TYPE_TB
- en: Newer processors will support larger address buses. Many other processors (such
    as ARM and IA-64) already provide much larger addresses buses and, in fact, support
    addresses up to 64 bits in the software.
  prefs: []
  type: TYPE_NORMAL
- en: A 64-bit address range is truly infinite as far as memory is concerned. No one
    will ever put 2^(64) bytes of memory into a computer system and feel that they
    need more. Of course, people have made claims like this in the past. A few years
    ago, no one ever thought a computer would need 1GB of memory, yet computers with
    64GB of memory (or more) are very common today. However, 2^(64) is effectively
    infinity for one simple reason—it’s physically impossible to build that much memory
    based on estimates of the current size (about 2^(86) different elementary particles)
    of the universe. Unless you can attach 1 byte of memory to every elementary particle
    on the planet, you won’t even come close to approaching 2^(64) bytes of memory
    on a given computer system. Then again, maybe we really will use whole planets
    as computer systems one day, as Douglas Adams predicted in *The Hitchhiker’s Guide
    to the Galaxy*. Who knows?
  prefs: []
  type: TYPE_NORMAL
- en: While the newer 64-bit processors have an internal 64-bit address space, they
    rarely bring out 64 address lines on the chip. This is because pins are a precious
    commodity on large CPUs, and it doesn’t make sense to bring out extra address
    pins that will never be used. Currently, 40- to 52-bit address buses are the upper
    limit. In the distant future, this may expand a bit, but it’s hard to imagine
    the need for, or even possibility of, a physical 64-bit address bus.
  prefs: []
  type: TYPE_NORMAL
- en: On modern processors, CPU manufacturers are building memory controllers directly
    onto the CPU. Instead of having a traditional address and data bus to which you
    connect arbitrary memory devices, newer CPUs contain specialized buses intended
    to talk to very specific *dynamic random-access memory (DRAM)* modules. A typical
    CPU’s memory controller connects to only a certain number of DRAM modules; thus,
    the maximum DRAM you can easily connect to a CPU is a function of the memory control
    built into the CPU rather than the size of the external address bus. This is why
    some older laptops have a 16MB or 32MB maximum memory limitation even though they
    have 64-bit CPUs.^([1](footnotes.xhtml#fn6_1a))
  prefs: []
  type: TYPE_NORMAL
- en: '**6.1.1.3 The Control Bus**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The control bus is an eclectic collection of signals that control how the processor
    communicates with the rest of the system. To understand its importance, consider
    the data bus for a moment. The CPU uses the data bus to move data between itself
    and memory. The system uses two lines on the control bus, *read* and *write*,
    to determine the data flow direction (CPU to memory, or memory to CPU). So, when
    the CPU wants to write data to memory, it *asserts* (places a signal on) the write
    control line. When the CPU wants to read data from memory, it asserts the read
    control line.
  prefs: []
  type: TYPE_NORMAL
- en: Although the exact composition of the control bus varies among processors, some
    control lines—like the system clock lines, interrupt lines, status lines, and
    byte enable lines—are common to all processors. The byte enable lines appear on
    the control bus of some CPUs that support byte-addressable memory. These control
    lines allow 16-, 32-, and 64-bit processors to deal with smaller chunks of data
    by communicating the size of the accompanying data. Additional details appear
    in the sections “16-Bit Data Buses” on page [138](#page_138) and “32-Bit Data
    Buses” on [page 140](#page_140).
  prefs: []
  type: TYPE_NORMAL
- en: 'On the 80x86 family of processors, the control bus also contains a signal that
    helps distinguish between address spaces. The 80x86 family, unlike many other
    processors, provides two distinct address spaces: one for memory and one for I/O.
    However, it has only one physical address bus, shared between I/O and memory,
    so additional control lines decide which component the address is intended for.
    When these signals are active, the I/O devices use the address on the LO 16 bits
    of the address bus. When they’re inactive, the I/O devices ignore them, and the
    memory subsystem takes over at that point.'
  prefs: []
  type: TYPE_NORMAL
- en: '**6.2 Physical Organization of Memory**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A typical CPU addresses a maximum of 2^(*n*) different memory locations, where
    *n* is the number of bits on the address bus (most computer systems built around
    80x86 family CPUs do not include the maximum addressable amount of memory). But
    what exactly is a memory location? The 80x86, as an example, supports *[byte-addressable
    memory](gloss01.xhtml#gloss01_40)*. Therefore, the basic memory unit is a byte.
    With address buses containing 20, 24, 32, 36, or 40 address lines, the 80x86 processors
    can address 1MB, 16MB, 4GB, 64GB, or 1TB of memory, respectively. Some CPU families
    do not provide byte-addressable memory; instead, they commonly address memory
    only in double-word or even quad-word chunks. However, because of the vast amount
    of software that *assumes* memory is byte-addressable (such as all those C/C++
    programs out there), even CPUs that don’t support byte-addressable memory in hardware
    still use byte addresses and simulate byte addressing in software. We’ll return
    to this topic shortly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Think of memory as an array of bytes. The address of the first byte is 0 and
    the address of the last byte is 2^(*n*) – 1\. For a CPU with a 20-bit address
    bus, the following pseudo-Pascal array declaration is a good approximation of
    memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: To execute the equivalent of the Pascal statement `Memory [125] := 0;` the CPU
    places the value `0` on the data bus, places the address `125` on the address
    bus, and asserts the write line on the control bus, as shown in [Figure 6-2](ch06.xhtml#ch06fig02).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/06fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6-2: Memory write operation*'
  prefs: []
  type: TYPE_NORMAL
- en: To execute the equivalent of `CPU := Memory [125];` the CPU places the address
    `125` on the address bus, asserts the read line on the control bus, and then reads
    the resulting data from the data bus (see [Figure 6-3](ch06.xhtml#ch06fig03)).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/06fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6-3: Memory read operation*'
  prefs: []
  type: TYPE_NORMAL
- en: This discussion applies *only* when the processor is accessing a single byte
    in memory. What happens when it accesses a word or a double word? Because memory
    consists of an array of bytes, how can we possibly deal with values larger than
    8 bits?
  prefs: []
  type: TYPE_NORMAL
- en: Different computer systems have different solutions to this problem. The 80x86
    family stores the LO byte of a word at the address specified and the HO byte at
    the next location. Therefore, a word consumes two consecutive memory addresses
    (as you would expect, because a word consists of 2 bytes). Similarly, a double
    word consumes four consecutive memory locations.
  prefs: []
  type: TYPE_NORMAL
- en: The address for a word or a double word is the address of its LO byte. The remaining
    bytes follow this LO byte, with the HO byte appearing at the address of the word
    plus 1 or the address of the double word plus 3 (see [Figure 6-4](ch06.xhtml#ch06fig04)).
  prefs: []
  type: TYPE_NORMAL
- en: It is quite possible for byte, word, and double-word values to overlap in memory.
    For example, in [Figure 6-4](ch06.xhtml#ch06fig04), you could have a word variable
    beginning at address 193, a byte variable at address 194, and a double-word value
    beginning at address 192\. Bytes, words, and double words may begin at *any* valid
    address in memory. We’ll soon see, however, that starting larger objects at an
    arbitrary address is not a good idea.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/06fig04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6-4: Byte, word, and double-word storage in memory (on an 80x86)*'
  prefs: []
  type: TYPE_NORMAL
- en: '***6.2.1 8-Bit Data Buses***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A processor with an 8-bit bus (like the old 8088 CPU) can transfer 8 bits of
    data at a time. Because each memory address corresponds to an 8-bit byte, an 8-bit
    bus turns out to be the most convenient architecture (from the hardware perspective),
    as [Figure 6-5](ch06.xhtml#ch06fig05) shows.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/06fig05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6-5: An 8-bit CPU <–> memory interface*'
  prefs: []
  type: TYPE_NORMAL
- en: The term *byte-addressable memory array* means that the CPU can address memory
    in chunks as small as a single byte. It also means that this is the *smallest*
    unit of memory you can access at once with the processor. That is, if the processor
    wants to access a 4-bit value, it must read 8 bits and then ignore the extra 4
    bits.
  prefs: []
  type: TYPE_NORMAL
- en: Byte addressability *does not* imply that the CPU can access 8 bits starting
    at any arbitrary bit boundary. When you specify address 125 in memory, you get
    the entire 8 bits at that address—nothing less, nothing more. Addresses are integers;
    you cannot specify, for example, address 125.5 to fetch fewer than 8 bits or to
    fetch a byte straddling two byte addresses.
  prefs: []
  type: TYPE_NORMAL
- en: Although CPUs with an 8-bit data bus conveniently manipulate byte values, they
    can also manipulate word and double-word values. However, this requires multiple
    memory operations, because these processors can move only 8 bits of data at once.
    Loading a word requires two memory operations; loading a double word requires
    four memory operations.
  prefs: []
  type: TYPE_NORMAL
- en: '***6.2.2 16-Bit Data Buses***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Some CPUs (such as the 8086, the 80286, and variants of the ARM processor family)
    have a 16-bit data bus. This allows these processors to access twice as much memory
    in the same amount of time as their 8-bit counterparts. These processors organize
    memory into two *banks*: an “even” bank and an “odd” bank (see [Figure 6-6](ch06.xhtml#ch06fig06)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/06fig06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6-6: Byte addressing in word memory*'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 6-7](ch06.xhtml#ch06fig07) illustrates the data bus connection to the
    CPU. In this figure, the data bus lines D0 through D7 transfer the LO byte of
    the word, while bus lines D8 through D15 transfer the HO byte of the word.'
  prefs: []
  type: TYPE_NORMAL
- en: The 16-bit members of the 80x86 family can load a word from any arbitrary address.
    As mentioned earlier, the processor fetches the LO byte of the value from the
    address specified and the HO byte from the next consecutive address. However,
    this creates a subtle problem. What happens when you access a word that begins
    on an odd address? Suppose you want to read a word from location 125\. The LO
    byte of the word comes from location 125 and the HO byte of the word comes from
    location 126\. It turns out that there are actually *two* problems with this approach.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/06fig07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6-7: A 16-bit processor memory organization*'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in [Figure 6-7](ch06.xhtml#ch06fig07), data bus lines 8 through
    15 (the HO byte) connect to the odd bank, and data bus lines 0 through 7 (the
    LO byte) connect to the even bank. Accessing memory location 125 will transfer
    data to the CPU on lines D8 through D15 of the data bus, placing the data in the
    HO byte, yet we need this in the LO byte! Fortunately, the 80x86 CPUs automatically
    recognize and handle this situation.
  prefs: []
  type: TYPE_NORMAL
- en: The second problem is even more obscure. When accessing words, we’re really
    accessing two separate bytes, each of which has its own byte address. So, what
    address appears on the address bus? The 16-bit 80x86 CPUs always place even addresses
    on the bus. Bytes at even addresses always appear on data lines D0 through D7,
    and bytes at odd addresses always appear on data lines D8 through D15\. If you
    access a word at an even address, the CPU can bring in the entire 16-bit chunk
    in one memory operation. Likewise, if you access a single byte, the CPU activates
    the appropriate bank (using a byte-enable control line) and transfers that byte
    on the appropriate data lines for its address.
  prefs: []
  type: TYPE_NORMAL
- en: But what happens when the CPU accesses a word at an odd address, like the example
    given earlier? The CPU can’t place address 125 on the address bus and read the
    16 bits from memory. There are no odd addresses coming out of a 16-bit 80x86 CPU—they’re
    always even. Therefore, if you try to put 125 on the address bus, 124 is what
    will actually appear there. Were you to read the 16 bits at this address, you
    would get the word at addresses 124 (LO byte) and 125 (HO byte)—not what you’d
    expect. Accessing a word at an odd address requires two memory operations (just
    as with the 8-bit bus on the 8088/80188). First, the CPU must read the byte at
    address 125, and then the byte at address 126\. Second, it needs to swap the positions
    of these bytes internally because both entered the CPU on the wrong half of the
    data bus.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, the 16-bit 80x86 CPUs hide these details from you. Your programs
    can access words at *any* address and the CPU will properly access and swap (if
    necessary) the data in memory. However, because of the two operations it requires,
    accessing words at odd addresses on a 16-bit processor is slower than accessing
    words at even addresses. By carefully arranging how you use memory, you can improve
    the speed of your programs on these CPUs.
  prefs: []
  type: TYPE_NORMAL
- en: '***6.2.3 32-Bit Data Buses***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Accessing 32-bit quantities always takes at least two memory operations on the
    16-bit processors. To access a 32-bit quantity at an odd address, a 16-bit processor
    may require three memory operations.
  prefs: []
  type: TYPE_NORMAL
- en: The 80x86 processors with a 32-bit data bus, such as the Pentium and Core processors,
    use four banks of memory connected to the 32-bit data bus (see [Figure 6-8](ch06.xhtml#ch06fig08)).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/06fig08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6-8: 32-bit processor memory interface*'
  prefs: []
  type: TYPE_NORMAL
- en: With a 32-bit memory interface, the 80x86 CPU can access any single byte with
    one memory operation. With a 16-bit memory interface, the address placed on the
    address bus is always an even number; and with a 32-bit memory interface, it’s
    always some multiple of 4\. Using various byte-enable control lines, the CPU can
    select which of the 4 bytes at that address the software wants to access. As with
    the 16-bit processor, the CPU will automatically rearrange bytes as necessary.
  prefs: []
  type: TYPE_NORMAL
- en: A 32-bit CPU can also access a word at most memory addresses using a single
    memory operation, though word accesses at certain addresses will take two memory
    operations (see [Figure 6-9](ch06.xhtml#ch06fig09)). This is the same problem
    we encountered with the 16-bit processor attempting to retrieve a word with an
    odd address, except it occurs half as often—only when the address divided by 4
    leaves a remainder of 3.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/06fig09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6-9: Accessing a word on a 32-bit processor at (address mod 4) = 3*'
  prefs: []
  type: TYPE_NORMAL
- en: A 32-bit CPU can access a double word in a single memory operation *only if*
    the address of that value is evenly divisible by 4\. If not, the CPU may require
    two memory operations.
  prefs: []
  type: TYPE_NORMAL
- en: Once again, the 80x86 CPU handles all this automatically. However, there’s a
    performance benefit to proper data alignment. Generally, the LO byte of word values
    should always be placed at even addresses, and the LO byte of double-word values
    should always be placed at addresses that are evenly divisible by 4.
  prefs: []
  type: TYPE_NORMAL
- en: '***6.2.4 64-Bit Data Buses***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Pentium and later processors, like Intel i-Series, provide a 64-bit data
    bus and special cache memory that reduces the impact of nonaligned data access.
    Although there may still be a penalty for accessing data at an inappropriate address,
    modern x86 CPUs suffer from the problem less frequently than the earlier CPUs.
    We’ll look at the details in “Cache Memory” on page [151](#page_151).
  prefs: []
  type: TYPE_NORMAL
- en: '***6.2.5 Small Accesses on Non-80x86 Processors***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Although the 80x86 processor is not the only processor that will let you access
    a byte, word, or double-word object at an arbitrary byte address, most processors
    created in the past 30 years do *not* allow it. For example, the 68000 processor
    found in the original Apple Macintosh system would allow you to access a byte
    at any address, but raised an exception if you attempted to access a word at an
    odd address.^([2](footnotes.xhtml#fn6_2a)) Many processors require that you access
    an object at an address that is a multiple of the object’s size, or they’ll raise
    an exception.
  prefs: []
  type: TYPE_NORMAL
- en: Most RISC processors, including those found in modern smartphones and tablets
    (typically ARM processors), do not allow you to access byte and word objects at
    all. Most RISC CPUs require that all data accesses be the same size as the data
    bus (or general-purpose integer register size, whichever is smaller). This is
    generally a double-word (32-bit) or quad-word (64-bit) access. If you want to
    access bytes or words on such a machine, you have to treat them as packed fields
    and use the shift and mask techniques to extract or insert byte and word data
    in a double word. Although it’s nearly impossible to avoid byte accesses in software
    that does any character and string processing, if you expect your software to
    run efficiently on various modern RISC CPUs, you should avoid word data types
    (and the performance penalty for accessing them) in favor of double words.
  prefs: []
  type: TYPE_NORMAL
- en: '**6.3 Big-Endian vs. Little-Endian Organization**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Earlier, you read that the 80x86 CPU family stores the LO byte of a word or
    double-word value at a particular address in memory and the successive HO bytes
    at successively higher addresses. Now we’ll look in more depth at how different
    processors store multibyte objects in byte-addressable memory.
  prefs: []
  type: TYPE_NORMAL
- en: Almost every CPU whose “bit size” is some power of 2 (8, 16, 32, 64, and so
    on) numbers the bits and nibbles as shown in the previous chapters. There are
    some exceptions, but they are rare, and most of the time they represent a notational
    change, not a functional change (meaning you can safely ignore the difference).
    Once you start dealing with objects larger than 8 bits, however, things become
    more complicated. Different CPUs organize the bytes in a multibyte object differently.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the layout of the bytes in a double word on an 80x86 CPU (see [Figure
    6-10](ch06.xhtml#ch06fig10) ). The LO byte, which contributes the smallest component
    of a binary number, sits in bit positions 0 through 7 and appears at the lowest
    address in memory. It seems reasonable that the bits that contribute the least
    would be located at the lowest address in memory.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/06fig10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6-10: Byte layout in a double word on the 80x86 processor*'
  prefs: []
  type: TYPE_NORMAL
- en: This is not the only possible organization, however. Some CPUs reverse the memory
    addresses of all the bytes in a double word, using the organization shown in [Figure
    6-11](ch06.xhtml#ch06fig11).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/06fig11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6-11: Alternate byte layout in a double word*'
  prefs: []
  type: TYPE_NORMAL
- en: The original Apple Macintosh (68000 and PowerPC) and most non-80x86 Unix boxes
    use the data organization shown in [Figure 6-11](ch06.xhtml#ch06fig11). Even on
    80x86 systems, certain protocols (such as network transmissions) specify this
    data organization. Therefore, this isn’t some rare and esoteric convention; it’s
    quite common, and not something you can ignore if you work on PCs.
  prefs: []
  type: TYPE_NORMAL
- en: The byte organization that Intel uses is whimsically known as the *[little-endian
    byte organization](gloss01.xhtml#gloss01_137)*. The alternate form is known as
    *[big-endian byte organization](gloss01.xhtml#gloss01_27)*.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*These terms come from Jonathan Swift’s* Gulliver’s Travels; *the Lilliputians
    were arguing over whether one should open an egg by cracking it on the little
    end or the big end—a parody of the arguments the Catholics and Protestants were
    having over their respective doctrines when Swift was writing.*'
  prefs: []
  type: TYPE_NORMAL
- en: The time for arguing over which format is superior was back before there were
    several different CPUs created using different *[endianness](gloss01.xhtml#gloss01_88)*.
    Today, that argument is irrelevant. Regardless of which format is better or worse,
    we have to deal with the fact that different CPUs sport different endianness,
    and we have to take care when writing software if we want our programs to run
    on both types of processors.
  prefs: []
  type: TYPE_NORMAL
- en: 'We encounter the big-endian versus little-endian problem when we try to pass
    binary data between two computers. For example, the double-word binary representation
    of 256 on a little-endian machine has the following byte values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'If you assemble these 4 bytes on a little-endian machine, their layout takes
    this form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'On a big-endian machine, however, the layout takes the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This means that if you take a 32-bit value from one of these machines and attempt
    to use it on the other machine (with a different endianness), you won’t get correct
    results. For example, if you take a big-endian version of the value 256 and interpret
    it as little-endian, you’ll discover that it has a `1` in bit position 16, and
    a little-endian machine will think that the value is actually 65,536 (that is,
    `%1_0000_0000_0000_0000`).
  prefs: []
  type: TYPE_NORMAL
- en: When you’re exchanging data between two different machines, the best solution
    is to convert your values to some canonical form and then convert the canonical
    form back to the local format if the local and canonical formats are not the same.
    Exactly what constitutes a “canonical” format depends, usually, on the transmission
    medium. For example, when you are transmitting data across networks, the canonical
    form is usually big-endian because TCP/IP and some other network protocols use
    the big-endian format. When you’re transmitting data across the Universal Serial
    Bus (USB), the canonical format is little-endian. Of course, if you control the
    software on both ends, the choice of canonical form is arbitrary; still, you should
    attempt to use the appropriate form for the transmission medium to avoid confusion
    down the road.
  prefs: []
  type: TYPE_NORMAL
- en: 'To convert between the endian forms, you must do a *mirror-image swap* of the
    bytes in the object: first swap the bytes at opposite ends of the binary number,
    and then work your way toward the middle of the object, swapping pairs of bytes
    as you go along. For example, to convert between the big-endian and little-endian
    format within a double word, you’d first swap bytes 0 and 3, then you’d swap bytes
    1 and 2 (see [Figure 6-12](ch06.xhtml#ch06fig12)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/06fig12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6-12: Endian conversion in a double word*'
  prefs: []
  type: TYPE_NORMAL
- en: For word values, all you need to do is swap the HO and LO bytes to change the
    endianness. For quad-word values, you need to swap bytes 0 and 7, 1 and 6, 2 and
    5, and 3 and 4\. Because very little software deals with 128-bit integers, you
    probably won’t need to worry about long-word endianness conversion, but the concept
    is the same if you do.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the endianness conversion process is *reflexive*; that is, the same
    algorithm that converts big-endian to little-endian also converts little-endian
    to big-endian. If you run the algorithm twice, you wind up with the data in the
    original format.
  prefs: []
  type: TYPE_NORMAL
- en: Even if you’re not writing software that exchanges data between two computers,
    the issue of endianness may arise. Some programs assemble larger objects from
    discrete bytes by assigning those bytes to specific positions within the larger
    value. If the software puts the LO byte into bit positions 0 through 7 (little-endian
    format) on a big-endian machine, the program will not produce correct results.
    Therefore, if the software needs to run on different CPUs that have different
    byte organizations, it will have to determine the endianness of the machine it’s
    running on and adjust how it assembles larger objects from bytes accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate how to build larger objects from discrete bytes, we’ll start with
    a short example that demonstrates how you could assemble a 32-bit object from
    4 individual bytes. The most common way to do this is to create a *[discriminant
    union](gloss01.xhtml#gloss01_76)* structure that contains a 32-bit object and
    a 4-byte array.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Many languages, but not all, support the discriminant union data type. For
    example, in Pascal, you would instead use a case variant record. See your language
    reference manual for details.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Unions are similar to records or structures except the compiler allocates the
    storage for each field of the union at the same address in memory. Consider the
    following two declarations from the C programming language:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As [Figure 6-13](ch06.xhtml#ch06fig13) shows, the `RECORDvar` object consumes
    8 bytes in memory, and the fields do not share their memory with any other fields
    (that is, each field starts at a different offset from the base address of the
    record). The `UNIONvar` object, on the other hand, overlays all the fields in
    the union in the same memory locations. Therefore, writing a value to the `i`
    field of the union also overwrites the value of the `u` field as well as 2 bytes
    of the `r` field (whether they are the LO or HO bytes depends entirely on the
    endianness of the CPU).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/06fig13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6-13: Layout of a union versus a record (struct) in memory*'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the C programming language, you can use this behavior to access the individual
    bytes of a 32-bit object. Consider the following union declaration in C:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This creates the data type shown in [Figure 6-14](ch06.xhtml#ch06fig14) on a
    little-endian machine, and the structure shown in [Figure 6-15](ch06.xhtml#ch06fig15)
    on a big-endian machine.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/06fig14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6-14: A C union on a little-endian machine*'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/06fig15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6-15: A C union on a big-endian machine*'
  prefs: []
  type: TYPE_NORMAL
- en: 'To assemble a 32-bit object from 4 discrete bytes on a little-endian machine,
    you’d use code like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This code functions properly because C allocates the first byte of an array
    at the lowest address in memory (corresponding to bits 0..7 in the `theValue.bits32`
    object on a little-endian machine); the second byte of the array follows (bits
    8..15), then the third (bits 16..23), and finally the HO byte (occupying the highest
    address in memory, corresponding to bits 24..31).
  prefs: []
  type: TYPE_NORMAL
- en: 'However, on a big-endian machine, this code won’t work properly because `theValue.bytes[0]`
    corresponds to bits 24 through 31 of the 32-bit value rather than bits 0 through
    7\. To assemble this 32-bit value properly on a big-endian system, you’d need
    to use code like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'But how do you determine if your code is running on a little-endian or big-endian
    machine? This is actually a simple task. Consider the following C code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: On a big-endian machine, this code sequence will store the value `1` into bit
    16, producing a 32-bit value that is definitely not equal to 256, whereas on a
    little-endian machine this code will store the value `1` into bit 8, producing
    a 32-bit value equal to 256\. Therefore, you can test the `isLittleEndian` variable
    to determine whether the current machine is little-endian (`true`) or big-endian
    (`false`).
  prefs: []
  type: TYPE_NORMAL
- en: '**6.4 The System Clock**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Although modern computers are quite fast and getting faster all the time, they
    still require time to accomplish even the smallest tasks. On von Neumann machines,
    most operations are *serialized*, which means that the computer executes commands
    in a prescribed order.^([3](footnotes.xhtml#fn6_3a)) It wouldn’t do, in the following
    code sequence, to execute the Pascal statement `I := I * 5 + 2;` before the statement
    `I := J;` finishes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: These operations do not occur instantaneously. Moving a copy of `J` into `I`
    takes a certain amount of time. Likewise, multiplying `I` by 5 and then adding
    2 and storing the result back into `I` takes time.
  prefs: []
  type: TYPE_NORMAL
- en: To execute statements in the proper order, the processor relies on the *system
    clock*, which serves as the timing standard within the system. To understand why
    certain operations take longer than others, you must first understand how the
    system clock functions.
  prefs: []
  type: TYPE_NORMAL
- en: The system clock is an electrical signal on the control bus that alternates
    between 0 and 1 periodically (see [Figure 6-16](ch06.xhtml#ch06fig16)). All activity
    within the CPU is synchronized with the edges (rising or falling) of this clock
    signal.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/06fig16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6-16: The system clock*'
  prefs: []
  type: TYPE_NORMAL
- en: The rate at which the system clock alternates between 0 and 1 is the *system
    clock frequency*, and the time it takes for the system clock to switch from 0
    to 1 and back to 0 is the *clock period* or *clock cycle*. On most modern systems,
    the system clock frequency exceeds several billion cycles per second. A typical
    Pentium IV chip, circa 2004, runs at speeds of three billion cycles per second
    or faster. *Hertz (Hz)* is the unit corresponding to one cycle per second, so
    the aforementioned Pentium chip runs at between 3,000 and 4,000 million hertz,
    or 3,000 to 4,000 megahertz (MHz), or 3 to 4 gigahertz (GHz, or one billion cycles
    per second). Typical frequencies for 80x86 parts range from 5 MHz up to several
    gigahertz and beyond.
  prefs: []
  type: TYPE_NORMAL
- en: The clock period is the reciprocal of the clock frequency. For example, a 1
    MHz (MHz or one million cycles per second) clock would have a clock period of
    1 microsecond (one millionth of a second, µs^([4](footnotes.xhtml#fn6_4a))). A
    CPU running at 1 GHz would have a clock period of one nanosecond (ns), or one
    billionth of a second. Clock periods are usually expressed in microseconds or
    nanoseconds.
  prefs: []
  type: TYPE_NORMAL
- en: To ensure synchronization, most CPUs start an operation on either the *[falling
    edge](gloss01.xhtml#gloss01_93)* (when the clock goes from 1 to 0) or the *[rising
    edge](gloss01.xhtml#gloss01_217)* (when the clock goes from 0 to 1). The system
    clock spends most of its time at either 0 or 1 and very little time switching
    between the two. Therefore, a clock edge is the perfect synchronization point.
  prefs: []
  type: TYPE_NORMAL
- en: Because all CPU operations are synchronized with the clock, the CPU cannot perform
    tasks any faster than the clock runs. However, just because a CPU is running at
    some clock frequency doesn’t mean that it executes that many operations each second.
    Many operations take multiple clock cycles to complete, so the CPU often performs
    operations at a significantly slower rate.
  prefs: []
  type: TYPE_NORMAL
- en: '***6.4.1 Memory Access and the System Clock***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Memory access is an operation that is synchronized with the system clock; that
    is, memory access occurs no more than once every clock cycle. On some older processors,
    it takes several clock cycles to access a memory location. The *[memory access
    time](gloss01.xhtml#gloss01_149)* is the number of clock cycles between a memory
    request (read or write) and when the memory operation completes. This is an important
    value, because longer memory access times result in lower performance.
  prefs: []
  type: TYPE_NORMAL
- en: Modern CPUs are much faster than memory devices, so systems built around these
    CPUs often use a second clock, the *bus clock*, which is some fraction of the
    CPU speed. For example, typical processors in the 100 MHz to 4 GHz range can use
    1600 MHz, 800 MHz, 500 MHz, 400 MHz, 133 MHz, 100 MHz, or 66 MHz bus clocks (a
    given CPU generally supports several different bus speeds, and the exact range
    it supports depends upon that CPU).
  prefs: []
  type: TYPE_NORMAL
- en: When reading from memory, the memory access time is the time between when the
    CPU places an address on the address bus and the time when the CPU takes the data
    off the data bus. On typical 80x86 CPUs with a one-cycle memory access time, the
    timing of a read operation looks something like [Figure 6-17](ch06.xhtml#ch06fig17).
    The timing of writing data to memory is similar (see [Figure 6-18](ch06.xhtml#ch06fig18)).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/06fig17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6-17: A typical memory read cycle*'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/06fig18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6-18: A typical memory write cycle*'
  prefs: []
  type: TYPE_NORMAL
- en: The CPU doesn’t wait for memory. The access time is specified by the bus clock
    frequency. If the memory subsystem doesn’t work fast enough to keep up with the
    CPU’s expected access time, the CPU will read garbage data on a memory read operation
    and will not properly store the data on a memory write. This will surely cause
    the system to fail.
  prefs: []
  type: TYPE_NORMAL
- en: Memory devices have various ratings, but the two major ones are capacity and
    speed. Typical dynamic RAM (random access memory) devices have capacities of 16GB
    (or more) and speeds of 0.1 to 100 ns. A typical 4 GHz Intel system uses 1600
    MHz (1.6 GHz, or 0.625 ns) memory devices.
  prefs: []
  type: TYPE_NORMAL
- en: Now, I just said that the memory speed must match the bus speed or the system
    will fail. At 4 GHz the clock period is roughly 0.25 ns. So how can a system designer
    get away with using 0.625 ns memory? The answer is *[wait states](gloss01.xhtml#gloss01_261)*.
  prefs: []
  type: TYPE_NORMAL
- en: '***6.4.2 Wait States***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A wait state is an extra clock cycle that gives a device additional time to
    respond to the CPU. For example, a 100 MHz Pentium system has a 10 ns clock period,
    implying that you need 10 ns memory. In fact, you need even faster memory devices
    because in many computer systems there’s additional decoding and buffering logic
    between the CPU and memory, and this circuitry introduces its own delays. In [Figure
    6-19](ch06.xhtml#ch06fig19), you can see that buffering and decoding costs the
    system an additional 10 ns. If the CPU needs the data back in 10 ns, the memory
    must respond in 0 ns (which is impossible).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/06fig19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6-19: Decoding and buffer delays*'
  prefs: []
  type: TYPE_NORMAL
- en: If cost-effective memory won’t work with a fast processor, how do companies
    manage to sell fast PCs? One part of the answer is the wait state. For example,
    if you have a 100 MHz processor with a memory cycle time of 10 ns and you lose
    2 ns to buffering and decoding, you’ll need 8 ns memory. What if your system can
    only support 20 ns memory, though? By adding wait states to extend the memory
    cycle to 20 ns, you can solve this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Almost every general-purpose CPU in existence provides a pin (whose signal appears
    on the control bus) that allows you to insert wait states. If necessary, the memory
    address decoding circuitry asserts this signal to give the memory sufficient access
    time (see [Figure 6-20](ch06.xhtml#ch06fig20)).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/06fig20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6-20: Inserting a wait state into a memory read operation*'
  prefs: []
  type: TYPE_NORMAL
- en: From the system performance point of view, wait states are *not* a good thing.
    As long as the CPU is waiting for data from memory, it can’t operate on that data.
    Adding a wait state typically *doubles* (or worse, on some systems) the amount
    of time required to access memory. Running with a wait state on every memory access
    is almost like cutting the processor clock frequency in half. You’ll get less
    work done in the same amount of time.
  prefs: []
  type: TYPE_NORMAL
- en: However, we’re not doomed to slow execution because of added wait states. There
    are several tricks hardware designers can employ to achieve zero wait states *most*
    of the time. The most common is the use of *cache* (pronounced “cash”) memory.
  prefs: []
  type: TYPE_NORMAL
- en: '***6.4.3 Cache Memory***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A typical program tends to access the same memory locations repeatedly (known
    as *[temporal locality of reference](gloss01.xhtml#gloss01_245)*), and to access
    adjacent memory locations (*spatial locality of reference*). Both forms of locality
    occur in the following Pascal code segment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: There are two occurrences each of spatial and temporal locality of reference
    within this loop. Let’s consider the obvious ones first.
  prefs: []
  type: TYPE_NORMAL
- en: In this Pascal code, the program references the variable `i` several times.
    The `for` loop compares `i` against `10` to see if the loop is complete. It also
    increments `i` by 1 at the bottom of the loop. The assignment statement also uses
    `i` as an array index. This shows temporal locality of reference in action.
  prefs: []
  type: TYPE_NORMAL
- en: The loop itself zeros out the elements of array `A` by writing a `0` to the
    first location in `A`, then to the second location in `A`, and so on. Because
    Pascal stores the elements of `A` in consecutive memory locations, each loop iteration
    accesses adjacent memory locations. This shows spatial locality of reference.
  prefs: []
  type: TYPE_NORMAL
- en: What about the second occurrences of temporal and spatial locality? Machine
    instructions also reside in memory, and the CPU fetches these instructions sequentially
    from memory and executes them repeatedly, once for each loop iteration.
  prefs: []
  type: TYPE_NORMAL
- en: If you look at the execution profile of a typical program, you’ll probably discover
    that the program executes less than half the statements. Generally, a program
    might use only 10 to 20 percent of the memory allotted to it. At any given time,
    a 1MB program might access only 4KB to 8KB of data and code. So, if you paid an
    outrageous sum of money for expensive zero-wait-state RAM, you’d be using only
    a tiny fraction of it at any given time. Wouldn’t it be nice if you could buy
    a small amount of fast RAM and dynamically reassign its addresses as the program
    executes? This is exactly what cache memory does for you.
  prefs: []
  type: TYPE_NORMAL
- en: Cache memory is a small amount of very fast memory that sits between the CPU
    and main memory. Unlike in normal memory, the bytes within a cache do not have
    fixed addresses. Cache memory can dynamically reassign addresses, which allows
    the system to keep recently accessed values in the cache. Addresses that the CPU
    has never accessed, or hasn’t accessed in some time, remain in main (slow) memory.
    Because most memory accesses are to recently accessed variables (or to locations
    near a recently accessed location), the data generally appears in cache memory.
  prefs: []
  type: TYPE_NORMAL
- en: A *[cache hit](gloss01.xhtml#gloss01_41)* occurs whenever the CPU accesses memory
    and finds the data in the cache. In such a case, the CPU can usually access data
    with zero wait states. A *[cache miss](gloss01.xhtml#gloss01_43)* occurs if the
    data cannot be found in the cache. In that case, the CPU has to read the data
    from main memory, incurring a performance loss. To take advantage of temporal
    locality of reference, the CPU copies data into the cache whenever it accesses
    an address that’s not present in the cache. Because the system will likely access
    that address shortly, it can save wait states on future accesses by having that
    data in the cache.
  prefs: []
  type: TYPE_NORMAL
- en: Cache memory does not eliminate the need for wait states. Although a program
    may spend considerable time executing code in one area of memory, eventually it
    will call a procedure or wander off to some section of code outside cache memory.
    When that happens, the CPU has to go to main memory to fetch the data. Because
    main memory is slow, this will require the insertion of wait states. However,
    once the CPU accesses the data, it will be available in the cache for future use.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve discussed how cache memory handles the temporal aspects of memory access,
    but not the spatial aspects. Caching memory locations *when you access them* won’t
    speed up the program if you constantly access consecutive locations that you’ve
    never accessed before. To solve this problem, when a cache miss occurs, most caching
    systems will read several consecutive bytes of main memory (which engineers call
    a *cache line*). For example, 80x86 CPUs read between 16 and 64 bytes upon a cache
    miss. Most memory chips available today have special modes that let you quickly
    access several consecutive memory locations on the chip. The cache exploits this
    capability to reduce the average number of wait states needed to access sequential
    memory locations. Although reading 16 bytes on each cache miss is expensive if
    you access only a few bytes in the corresponding cache line, cache memory systems
    work quite well in the average case.
  prefs: []
  type: TYPE_NORMAL
- en: The ratio of cache hits to misses increases with the size (in bytes) of the
    cache memory subsystem. The 80486 CPU, for example, has 8,192 bytes of on-chip
    cache. Intel claims to get an 80 to 95 percent hit rate with this cache (meaning
    80 to 95 percent of the time the CPU finds the data in the cache). This sounds
    very impressive, but let’s play around with the numbers a little bit. Suppose
    we pick the 80 percent figure. This means that one out of every five memory accesses,
    on average, will not be in the cache. If you have a 50 MHz processor (20 ns period)
    and a 90 ns memory access time, four out of five memory accesses require only
    20 ns (one clock cycle) because they are in the cache, and the fifth will require
    about four wait states (20 ns for a normal memory access plus 80 additional ns,
    or four wait states, to get at least 90 ns). However, the cache always reads 16
    consecutive bytes (4 double words) from memory. Most 80486-era memory subsystems
    let you read consecutive addresses in about 40 ns after accessing the first location.
    Therefore, the 80486 will require an additional six clock cycles to read the remaining
    3 double words, for a total of 220 ns. This corresponds to 11 clock cycles (at
    20 ns each), which is one normal memory cycle plus 10 wait states.
  prefs: []
  type: TYPE_NORMAL
- en: Altogether, the system will require 15 clock cycles to access five memory locations,
    or 3 clock cycles per access, on average. That’s equivalent to two wait states
    added to every memory access. Doesn’t sound so impressive, does it? It gets even
    worse as you move up to faster processors and the difference in speed between
    the CPU and memory increases.
  prefs: []
  type: TYPE_NORMAL
- en: To improve the hit ratio, you can add more cache memory. Alas, you can’t pull
    an Intel i9 chip apart and solder more cache onto the chip. However, modern Intel
    CPUs have a significantly larger cache than the 80486 and operate with fewer average
    wait states. This improves the cache hit ratio. For example, increasing the hit
    ratio from 80 percent to 90 percent lets you access 10 memory locations in 20
    cycles. This reduces the average number of wait states per memory access to one
    wait state—a substantial improvement.
  prefs: []
  type: TYPE_NORMAL
- en: Another way to improve performance is to build a *two-level* (L2) caching system.
    Many Intel CPUs work in this fashion. The first level is the on-chip 8,192-byte
    cache. The next level, between the on-chip cache and main memory, is a secondary
    cache (see [Figure 6-21](ch06.xhtml#ch06fig21)). On newer processors, the first-
    and second-level caches generally appear in the same packaging as the CPU. This
    allows the CPU designers to build a higher-performance CPU/memory interface, allowing
    the CPU to move data between caches and the CPU (as well as main memory) much
    more rapidly.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/06fig21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6-21: A two-level caching system*'
  prefs: []
  type: TYPE_NORMAL
- en: A typical on-CPU secondary cache contains anywhere from 32,768 bytes to over
    2MB of memory.
  prefs: []
  type: TYPE_NORMAL
- en: Secondary cache generally does not operate at zero wait states. The circuitry
    to support that much fast memory would be *very* expensive, so most system designers
    use slower memory, which requires one or two wait states. This is still much faster
    than main memory. Combined with the existing on-chip L1 cache, you can get better
    performance from the system with a L2 caching system.
  prefs: []
  type: TYPE_NORMAL
- en: Today, many CPUs incorporate a *three-level (L3) cache*. Though the performance
    improvement afforded by an L3 cache is nowhere near what you get with an L1 or
    L2 cache subsystem, L3 cache subsystems can be quite large (usually several megabytes^([5](footnotes.xhtml#fn6_5a)))
    and work well for large systems with gigabytes of main memory. For programs that
    manipulate considerable data yet exhibit locality of reference, an L3 caching
    subsystem can be very effective.
  prefs: []
  type: TYPE_NORMAL
- en: '**6.5 CPU Memory Access**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most CPUs have two or three different ways to access memory. The most common
    *[memory addressing modes](gloss01.xhtml#gloss01_150)* modern CPUs support are
    *direct*, *indirect*, and *indexed*. A few CPUs (like the 80x86) support additional
    addressing modes like *scaled-index*, while some RISC CPUs support only indirect
    access to memory. Having additional memory addressing modes makes memory access
    more flexible. Sometimes a particular addressing mode will allow you to access
    data in a complex data structure with a single instruction, where otherwise two
    or more instructions would be required.
  prefs: []
  type: TYPE_NORMAL
- en: RISC processors can often take three to five instructions to do what a single
    80x86 instruction does. However, this does not mean that an 80x86 program will
    run three to five times faster. Don’t forget that access to memory is very slow,
    usually requiring wait states. Whereas the 80x86 frequently accesses memory, RISC
    processors rarely do. Therefore, that RISC processor can probably execute the
    first four instructions, which do not access memory at all, while the single 80x86
    instruction, which does access memory, is spinning on some wait states. In the
    fifth instruction the RISC CPU might access memory and incur wait states of its
    own. If both processors execute an average of one instruction per clock cycle
    and have to insert 30 wait states for a main memory access, we’re talking about
    31 clock cycles (80x86) versus 35 clock cycles (RISC), only about a 12 percent
    difference.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing an appropriate addressing mode often enables an application to compute
    the same result with fewer instructions and with fewer memory accesses, thus improving
    performance. Therefore, if you want to write fast and compact code, it’s important
    to understand how an application can use the different addressing modes a CPU
    provides.
  prefs: []
  type: TYPE_NORMAL
- en: '***6.5.1 The Direct Memory Addressing Mode***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The direct addressing mode encodes a variable’s memory address as part of the
    actual machine instruction that accesses the variable. On the 80x86, direct addresses
    are 32-bit values appended to the instruction’s encoding. Generally, a program
    uses the direct addressing mode to access global static variables. Here’s an example
    in HLA assembly language:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: When you’re accessing variables whose memory address is known prior to the program’s
    execution, the direct addressing mode is ideal. With a single instruction, you
    can reference the memory location associated with the variable. On those CPUs
    that don’t support a direct addressing mode, you may need an extra instruction
    (or more) to load a register with the variable’s memory address prior to accessing
    that variable.
  prefs: []
  type: TYPE_NORMAL
- en: '***6.5.2 The Indirect Addressing Mode***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The indirect addressing mode typically uses a register to hold a memory address
    (there are a few CPUs that use memory locations to hold the indirect address,
    but this form of indirect addressing is rare in modern CPUs).
  prefs: []
  type: TYPE_NORMAL
- en: There are a couple of advantages of the indirect addressing mode over the direct
    addressing mode. First, you can modify the value of an indirect address (the value
    being held in a register) at runtime. Second, encoding which register specifies
    the indirect address takes far fewer bits than encoding a 32-bit (or 64-bit) direct
    address, so the instructions are smaller. One disadvantage is that it may take
    one or more instructions to load a register with an address before you can access
    that address.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following HLA sequence uses an 80x86 indirect addressing mode (brackets
    around the register name denote the use of indirect addressing):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The indirect addressing mode is useful for many operations, such as accessing
    objects referenced by a pointer variable.
  prefs: []
  type: TYPE_NORMAL
- en: '***6.5.3 The Indexed Addressing Mode***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The indexed addressing mode combines the direct and indirect addressing modes.
    Specifically, the machine instructions using this addressing mode encode both
    an offset (direct address) and a register in the bits that make up the instruction.
    At runtime, the CPU computes the sum of these two address components to create
    an *[effective address](gloss01.xhtml#gloss01_87)*. This addressing mode is great
    for accessing array elements and for indirect access to objects like structures
    and records. Though the instruction encoding is usually larger than for the indirect
    addressing mode, the indexed addressing mode has the advantage that you can specify
    an address directly within an instruction without having to use a separate instruction
    to load the address into a register.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a typical example of an HLA sequence that uses an 80x86 indexed addressing
    mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The `byteArray[ebx]` instruction in this short program demonstrates the indexed
    addressing mode. The effective address is the address of the `byteArray` variable
    plus the current value in the EBX register.
  prefs: []
  type: TYPE_NORMAL
- en: 'To avoid wasting space encoding a 32-bit or 64-bit address into every instruction
    that uses an indexed addressing mode, many CPUs provide a shorter form that encodes
    an 8-bit or 16-bit offset as part of the instruction. When using this smaller
    form, the register provides the base address of the object in memory, and the
    offset provides a fixed displacement into that data structure in memory. This
    is useful, for example, for accessing fields of a record or structure in memory
    via a pointer to that structure. The earlier HLA example encodes the address of
    `byteArray` using a 4-byte address. Compare that with the following use of the
    indexed addressing mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This last instruction encodes the displacement value using a single byte (rather
    than 4 bytes); hence, the instruction is shorter and more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: '***6.5.4 The Scaled-Index Addressing Modes***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The scaled-index addressing mode, available on several CPUs, provides two facilities
    above and beyond the indexed addressing mode:'
  prefs: []
  type: TYPE_NORMAL
- en: The ability to use two registers (plus an offset) to compute the effective address
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to multiply one of those two registers’ values by a constant (typically
    1, 2, 4, or 8) prior to computing the effective address.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This addressing mode is especially useful for accessing elements of arrays whose
    element sizes match one of the scaling constants (see the discussion of arrays
    in [Chapter 7](ch07.xhtml#ch07) for the reasons).
  prefs: []
  type: TYPE_NORMAL
- en: 'The 80x86 provides a scaled-index addressing mode that takes one of several
    forms, as shown in the following HLA statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '**6.6 For More Information**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Hennessy, John L., and David A. Patterson. *Computer Architecture: A Quantitative
    Approach*. 5th ed. Waltham, MA: Elsevier, 2012.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hyde, Randall. *The Art of Assembly Language*. 2nd ed. San Francisco: No Starch
    Press, 2010.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Patterson, David A., and John L. Hennessy. *Computer Organization and Design:
    The Hardware/Software Interface*. 5th ed. Waltham, MA: Elsevier, 2014.'
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*[Chapter 11](ch11.xhtml#ch11) in this book provides additional information
    about cache memory and memory architecture.*'
  prefs: []
  type: TYPE_NORMAL
