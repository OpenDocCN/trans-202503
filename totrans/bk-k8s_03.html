<html><head></head><body>
<h2 class="h2" id="ch02"><span epub:type="pagebreak" id="page_19"/><span class="big">2</span><br/>PROCESS ISOLATION</h2>&#13;
<div class="image1"><img alt="image" src="../images/common01.jpg"/></div>&#13;
<p class="noindent">Containers build on a rich history of technologies designed to isolate one computer program from another while allowing many programs to share the same CPU, memory, storage, and network resources. Containers use fundamental capabilities of the Linux kernel, particularly namespaces, which create separate views of process identifiers, users, the filesystem, and network interfaces. Container runtimes use multiple types of namespaces to give each container an isolated view of the system.</p>&#13;
<p class="indent">In this chapter, we’ll consider some of the reasons for process isolation and look at how Linux has historically isolated processes. We’ll then examine how containers use namespaces to provide isolation. We’ll test this using a couple of different container runtimes. Finally, we will use Linux commands to create namespaces directly.</p>&#13;
<h3 class="h3" id="ch00lev1sec8"><span epub:type="pagebreak" id="page_20"/>Understanding Isolation</h3>&#13;
<p class="noindent">Before running some containers and inspecting their isolation, let’s look at the motivation for process isolation. We’ll also consider traditional process isolation in Linux and how that has led to the isolation capabilities that containers use.</p>&#13;
<h4 class="h4" id="ch00lev2sec17">Why Processes Need Isolation</h4>&#13;
<p class="noindent">The whole idea of a computer is that it is a general-purpose machine that can run many different kinds of programs. Ever since the beginning of computing, there has been a need to share a single computer between multiple programs. It started with people taking turns submitting programs on punch cards, but as computer multitasking became more sophisticated, people could start multiple programs, and the computer would make it seem as if they were all running on the same CPU at once.</p>&#13;
<p class="indent">Of course, as soon as something needs to be shared, there is a need to make sure it is shared fairly, and computer programs are no different. So although we think of a <em>process</em> as an independent program with its own time on the CPU and its own memory space, there are many ways that one process can cause trouble for another, including:</p>&#13;
<ul>&#13;
<li><p class="noindent">Using too much CPU, memory, storage, or network</p></li>&#13;
<li><p class="noindent">Overwriting the memory or files of another process</p></li>&#13;
<li><p class="noindent">Extracting secret information from another process</p></li>&#13;
<li><p class="noindent">Sending another process bad data to cause it to misbehave</p></li>&#13;
<li><p class="noindent">Flooding another process with requests so that it stops responding</p></li>&#13;
</ul>&#13;
<p class="indent">Bugs can cause processes to do these same things by accident, but a bigger concern is a security vulnerability that allows a bad actor to use one process to cause problems for another. It takes only one vulnerability to create major problems in a system, so we need ways to isolate processes that limit damage from both accidental and intentional behavior.</p>&#13;
<p class="indent">Physical isolation is best—<em>air-gapped</em> systems are regularly used to protect government-classified information and safety-critical systems—but this approach is also too expensive and inconvenient for many uses. Virtual machines can give the appearance of separation while sharing physical hardware, but a virtual machine has the overhead of running its own operating system, services, and virtual devices, making it slower to start and less scalable. The solution is to run regular processes, but use process isolation to reduce the risk of affecting other processes.</p>&#13;
<h4 class="h4" id="ch00lev2sec18">File Permissions and Change Root</h4>&#13;
<p class="noindent">Most of the effort in process isolation involves preventing one process from seeing things it shouldn’t. After all, if a process can’t even see another process, it will be far more difficult to cause trouble, either accidentally or on <span epub:type="pagebreak" id="page_21"/>purpose. The traditional ways that Linux has controlled what processes can see and do serve as the foundation for the ideas behind containers.</p>&#13;
<p class="indent">One of the most basic visibility controls is <em>filesystem permissions</em>. Linux associates an owner and group with each file and directory, and manages read, write, and execute permissions. This basic permission scheme works well to ensure that user files are kept private, that a process cannot overwrite the files of another process, and that only a privileged user like root can install new software or modify critical system configuration files.</p>&#13;
<p class="indent">Of course, this permission scheme relies on us ensuring that each process is run as the authentic user and that users are in the appropriate groups. Typically, each new service install creates a user just for running that service. Even better, this <em>service user</em> can be configured without a real login shell, which means that the user cannot be exploited to log in to the system. To make this clear, let’s look at an example.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>The example repository for this book is at</em> <a href="https://github.com/book-of-kubernetes/examples">https://github.com/book-of-kubernetes/examples</a>. <em>See “Running Examples” on <a href="ch00.xhtml#ch00lev1sec2">page xx</a> for details on getting set up.</em></p>&#13;
</div>&#13;
<p class="indent">The Linux <code>rsyslogd</code> service provides logging services, so it needs to write to files in <em>/var/log</em>, but it should not have permissions to read or write all of the files in that directory. File permissions are used to control this, as shown in this example:</p>&#13;
<pre>   root@host01:~# <span class="codestrong1">ps -ef | grep rsyslogd | grep -v grep</span>&#13;
<span class="ent">➊</span> syslog  698  1  0 Mar05 ?   00:00:04 /usr/sbin/rsyslogd -n -iNONE&#13;
   root@host01:~# <span class="codestrong1">su syslog</span>&#13;
<span class="ent">➋</span> This account is currently not available.&#13;
   root@host01:~# <span class="codestrong1">ls -l /var/log/auth.log</span>&#13;
<span class="ent">➌</span> -rw-r----- 1 syslog adm 18396 Mar  6 01:27 /var/log/auth.log&#13;
   root@host01:~# <span class="codestrong1">ls -ld /var/log/private</span>&#13;
<span class="ent">➍</span> drwx------ 2 root root 4096 Mar  5 21:04 /var/log/private</pre>&#13;
<p class="indent">The <em>syslog</em> user <span class="ent">➊</span> exists specifically to run <code>rsyslogd</code>, and that user is configured with no login shell for security reasons <span class="ent">➋</span>. Because <code>rsyslogd</code> needs to be able to write to <em>auth.log</em>, it’s given write permission, as shown in the file mode printout <span class="ent">➌</span>. Members of the admin (<em>adm</em>) group have read-only access to this file.</p>&#13;
<p class="indent">An initial <code>d</code> in the file mode <span class="ent">➍</span> indicates that this is a directory. The following <code>rwx</code> indicates that the root user has read, write, and execute permissions. The remaining dashes indicate that there are no rights for members of the <em>root</em> group or for other system users, so we can conclude that the <code>rsyslogd</code> process cannot see the contents of this directory.</p>&#13;
<p class="indent">Permission control is important, but it doesn’t fully satisfy our goal of process isolation. One reason is that it is not enough to protect us from <em>privilege escalation</em>, wherein a vulnerable process and a vulnerable system allow a bad actor to obtain root privileges. To help deal with this, some Linux <span epub:type="pagebreak" id="page_22"/>services go a step beyond by running in an isolated part of the filesystem. This approach is known as <code>chroot</code> for “change root.” Running in a <code>chroot</code> environment requires quite a bit of setup, as you can see in this example:</p>&#13;
<pre>   root@host01:~# <span class="codestrong1">mkdir /tmp/newroot</span>&#13;
   root@host01:~# <span class="codestrong1"><span class="ent">➊</span> cp --parents /bin/bash /bin/ls /tmp/newroot</span>&#13;
   root@host01:~# <span class="codestrong1">cp --parents /lib64/ld-linux-x86-64.so.2 \</span>&#13;
  <span class="ent">➋</span> <span class="codestrong1">$(ldd /bin/bash /bin/ls | grep '=&gt;' | awk '{print $3}') /tmp/newroot</span>&#13;
   ...&#13;
   root@host01:~# <span class="codestrong1"><span class="ent">➌</span> chroot /tmp/newroot /bin/bash</span>&#13;
   bash-5.0# <span class="codestrong1">ls -l /bin</span>&#13;
   total 1296&#13;
<span class="ent">➍</span> -rwxr-xr-x 1 0 0 1183448 Mar  6 02:15 bash&#13;
   -rwxr-xr-x 1 0 0  142144 Mar  6 02:15 ls&#13;
   bash-5.0# <span class="codestrong1">exit</span>&#13;
   exit</pre>&#13;
<p class="indent">First, we need to copy in all of the executables that we intend to run <span class="ent">➊</span>. We also need to copy in all of the shared libraries these executables use, which we specify with the <code>ldd | grep | awk</code> command <span class="ent">➋</span>. When both binaries and libraries are copied in, we can use the <code>chroot</code> command <span class="ent">➌</span> to move into our isolated environment. Only the files we copied in are visible <span class="ent">➍</span>.</p>&#13;
<h4 class="h4" id="ch00lev2sec19">Container Isolation</h4>&#13;
<p class="noindent">For experienced Linux system administrators, file permissions and change root are basic-level knowledge. However, those concepts also serve as the foundation for how containers work. Even though a running container appears like a completely separate system, with its own hostname, network, processes, and filesystem (as we saw in <a href="ch01.xhtml#ch01">Chapter 1</a>), it’s really a regular Linux process using isolation rather than a virtual machine.</p>&#13;
<p class="indent">A container has multiple kinds of isolation, including several essential kinds of isolation that we haven’t seen before:</p>&#13;
<ul>&#13;
<li><p class="noindent">Mounted filesystems</p></li>&#13;
<li><p class="noindent">Hostname and domain name</p></li>&#13;
<li><p class="noindent">Interprocess communication</p></li>&#13;
<li><p class="noindent">Process identifiers</p></li>&#13;
<li><p class="noindent">Network devices</p></li>&#13;
</ul>&#13;
<p class="indent">These separate kinds of isolation work together so that a process or collection of processes looks like a completely separate system. Although these processes still share the kernel and physical hardware, this isolation goes a long way toward ensuring that they cannot cause trouble for other processes, especially when we configure containers correctly to control the CPU, memory, storage, and network resources available to them.</p>&#13;
<h3 class="h3" id="ch00lev1sec9"><span epub:type="pagebreak" id="page_23"/>Container Platforms and Container Runtimes</h3>&#13;
<p class="noindent">Specifying all the binaries, libraries, and configuration files needed to run a process in an isolated filesystem would be laborious. Fortunately, as we saw in <a href="ch01.xhtml#ch01">Chapter 1</a>, <em>container images</em> come prepackaged with the needed executables and libraries. Using Docker, we were able to easily download and run NGINX in a container. Docker is an example of a <em>container platform</em>, providing not only the ability to run containers but also container storage, networking, and security.</p>&#13;
<p class="indent">Under the covers, modern versions of Docker are using <code>containerd</code> as the <em>container runtime</em>, also known as a <em>container engine</em>. A container runtime provides low-level functionality to run processes in containers.</p>&#13;
<p class="indent">To explore isolation further, let’s experiment with two different container runtimes to start containers from preexisting images and then inspect how processes in containers are isolated from the rest of the system.</p>&#13;
<h4 class="h4" id="ch00lev2sec20">Installing containerd</h4>&#13;
<p class="noindent">We’ll be using <code>containerd</code> in <a href="part02.xhtml#part02">Part II</a> in support of our Kubernetes clusters, so let’s begin by installing and interacting with this runtime directly. Interacting directly with <code>containerd</code> will also benefit our exploration of process isolation.</p>&#13;
<p class="indent">You can skip install commands by using the <em>extra</em> provisioning script provided with this chapter’s examples. See the README file for this chapter for instructions.</p>&#13;
<p class="indent">Even though <code>containerd</code> is available in the standard Ubuntu package repository, we’ll install it from the official Docker package registry so that we get the latest stable version. To do that, we need Apt to support HTTP/S, so let’s do that first:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">apt update</span>&#13;
...&#13;
root@host01:~# <span class="codestrong1">apt -y install apt-transport-https</span>&#13;
...</pre>&#13;
<p class="indent">Now let’s add the package registry and install:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">curl -fsSL https://download.docker.com/linux/ubuntu/gpg | \</span>&#13;
  <span class="codestrong1">gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg</span>&#13;
root@host01:~# <span class="codestrong1">echo "deb [arch=amd64" \</span>&#13;
  <span class="codestrong1">"signed-by=/usr/share/keyrings/docker-archive-keyring.gpg]" \</span>&#13;
  <span class="codestrong1">"https://download.docker.com/linux/ubuntu focal stable" &gt; \</span>&#13;
  <span class="codestrong1">/etc/apt/sources.list.d/docker.list</span>&#13;
root@host01:~# <span class="codestrong1">apt update &amp;&amp; apt install -y containerd.io</span>&#13;
...&#13;
root@host01:~# <span class="codestrong1">ctr images ls</span>&#13;
REF TYPE DIGEST SIZE PLATFORMS LABELS</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_24"/>The final command just ensures that the package installed correctly, that the service is running, and that the <code>ctr</code> command is working. We don’t see any images because we haven’t installed any yet.</p>&#13;
<p class="indent">Container runtimes are low-level libraries. They are typically not used directly but are used by a higher-level container platform or orchestration environment such as Docker or Kubernetes. This means that they put a lot of focus into a quality application programming interface (API) but not as much effort into user-facing tools we can use from the command line. Fortunately, command line tools are still needed for testing, and <code>containerd</code> provides the <code>ctr</code> tool that we’ll use for experimentation.</p>&#13;
<h4 class="h4" id="ch00lev2sec21">Using containerd</h4>&#13;
<p class="noindent">Our initial <code>containerd</code> command showed that no images have been downloaded yet. Let’s download a small image with which we can run a container. We will use <em>BusyBox</em>, a tiny container image that includes a shell and basic Linux utilities. To download the image, we use the <code>pull</code> command:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">ctr image pull docker.io/library/busybox:latest</span>&#13;
...&#13;
root@host01:~# <span class="codestrong1">ctr images ls</span>&#13;
REF                              ...&#13;
docker.io/library/busybox:latest ...</pre>&#13;
<p class="indent">Our list of images is no longer empty. Let’s run a container from that image:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">ctr run -t --rm docker.io/library/busybox:latest v1</span>&#13;
/ #</pre>&#13;
<p class="indent">This looks similar to using Docker. We use <code>-t</code> to create a TTY for this container, allowing us to interact with it, and we use <code>--rm</code> to tell <code>containerd</code> to delete the container when the main process stops. However, there are some important differences to note. When we used Docker in <a href="ch01.xhtml#ch01">Chapter 1</a>, we didn’t worry about pulling the image before running it, and we were able to use simpler names like <code>nginx</code> or <code>rockylinux:8</code>. The <code>ctr</code> tool requires us to specify <em>docker.io/library/busybox:latest</em>, the full path to the image, with registry hostname and tag included. Also, we are required to pull the image first because the runtime won’t do this for us automatically.</p>&#13;
<p class="indent">Now that we’re inside this container, we can see that it has an isolated network stack and process space:</p>&#13;
<pre>/ # <span class="codestrong1">ip a</span>&#13;
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue qlen 1000&#13;
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00&#13;
    inet 127.0.0.1/8 scope host lo&#13;
        valid_lft forever preferred_lft forever&#13;
    inet6 ::1/128 scope host &#13;
        valid_lft forever preferred_lft forever&#13;
/ # <span class="codestrong1">ps -ef</span>&#13;
PID   USER     TIME  COMMAND&#13;
    1 root      0:00 sh&#13;
    8 root      0:00 ps -ef&#13;
/ #</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_25"/>Inside the container, we see a loopback network interface. We also see our shell process and the <code>ps</code> command that we ran. As far as the processes in our container are concerned, we are running on a separate system with no other processes running or listening on the network.</p>&#13;
<div class="box5">&#13;
<p class="boxtitle-d"><strong>WHY NO BRIDGE INTERFACE?</strong></p>&#13;
<p class="noindents">If you’ve worked with Docker, you might be surprised to see that this container has only a loopback interface. Default networking on a container platform also provides an additional interface that is attached to a bridge. This allows containers to see one another and also allows containers to use the host interface to access external networks via Network Address Translation (NAT).</p>&#13;
<p class="noindents">In this case, we are talking directly to a lower-level container runtime. This container runtime handles managing images and running containers only. If we want a bridge interface and a connection to the internet, we’ll need to provide it ourselves (and we do exactly that in <a href="ch04.xhtml#ch04">Chapter 4</a>).</p>&#13;
</div>&#13;
<p class="indent">We’ve illustrated that we can talk to the <code>containerd</code> runtime to run a container, and that inside the container, we’re isolated from the rest of the system. How does that isolation work? To find out, let’s keep the container running and investigate it from the host system.</p>&#13;
<h4 class="h4" id="ch00lev2sec22">Introducing Linux Namespaces</h4>&#13;
<p class="noindent">Like other container runtimes, <code>containerd</code> uses a Linux kernel feature called <em>namespaces</em> to isolate the processes in the container. As mentioned earlier, most of the effort in process isolation is to ensure that a process can’t see things it shouldn’t. A process running in a namespace sees a limited view of a particular system resource.</p>&#13;
<p class="indent">Even though containerization seems like new technology, Linux namespaces have been available for many years. Over time, more types of namespaces were added. We can find out what namespaces are associated with our container using the <code>lsns</code> command, but first we need to know the process ID (PID) on the host for our container’s shell process. While leaving the container running, open another terminal tab or window. (See “Running Examples” on <a href="ch00.xhtml#ch00lev1sec2">page xx</a> for more information.) Then, use <code>ctr</code> to list running containers:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">ctr task ls</span>&#13;
TASK    PID      STATUS    &#13;
v1      18088    RUNNING</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_26"/>Let’s use <code>ps</code> to verify that we have the correct PID. When you run these commands yourself, be sure to use the PID that displays in your listing:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">ps -ef | grep <span class="codeitalic1">18088</span> | grep -v grep</span>&#13;
root       18088   18067  0 18:46 pts/0    00:00:00 sh&#13;
root@host01:~# <span class="codestrong1">ps -ef | grep <span class="codeitalic1">18067</span> | grep -v grep</span>&#13;
root       18067       1  0 18:46 ?        00:00:00 &#13;
  /usr/bin/containerd-shim-runc-v2 -namespace default -id v1 -address &#13;
  /run/containerd/containerd.sock&#13;
root       18088   18067  0 18:46 pts/0    00:00:00 sh</pre>&#13;
<p class="indent">As expected, the parent of this PID is <code>containerd</code>. Next let’s use <code>lsns</code> to list the namespaces that <code>containerd</code> has created to isolate this process:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">lsns | grep <span class="codeitalic1">18088</span></span>&#13;
4026532180 mnt         1 18088 root            sh&#13;
4026532181 uts         1 18088 root            sh&#13;
4026532182 ipc         1 18088 root            sh&#13;
4026532183 pid         1 18088 root            sh&#13;
4026532185 net         1 18088 root            sh</pre>&#13;
<p class="indent">Here, <code>containerd</code> is using five different types of namespaces in order to fully isolate the processes running in the <code>busybox</code> container:</p>&#13;
<div class="bqparan">&#13;
<p class="noindent5"><span class="codestrong">mnt</span> Mount points</p>&#13;
<p class="noindent5"><span class="codestrong">uts</span> Unix time sharing (hostname and network domain)</p>&#13;
<p class="noindent5"><span class="codestrong">ipc</span> Interprocess communication (for example, shared memory)</p>&#13;
<p class="noindent5"><span class="codestrong">pid</span> Process identifiers (and list of running processes)</p>&#13;
<p class="noindent5"><span class="codestrong">net</span> Network (including interfaces, routing table, and firewall)</p>&#13;
</div>&#13;
<p class="indent">Finally, we’ll close out the BusyBox container by running <code>exit</code> from within that container (first terminal window):</p>&#13;
<pre>/ # <span class="codestrong1">exit</span></pre>&#13;
<p class="indent">This command returns us to a regular shell prompt so that we can be ready for the next set of examples.</p>&#13;
<h4 class="h4" id="ch00lev2sec23">Containers and Namespaces in CRI-O</h4>&#13;
<p class="noindent">In addition to <code>containerd</code>, Kubernetes supports other container runtimes. Depending on which Kubernetes distribution you use, you might find that the container runtime is different. For example, Red Hat OpenShift uses <em>CRI-O</em>, an alternative container runtime. CRI-O is also used by the Podman, Buildah, and Skopeo suite of tools, which are the standard way to manage containers on Red Hat 8 and related systems.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_27"/>Let’s run the same container image using CRI-O to get a better picture of how container runtimes are different from one another but also to show how they use the same underlying Linux kernel capabilities for process isolation.</p>&#13;
<p class="indent">You can skip these install commands by using the <em>extra</em> provisioning script provided with this chapter’s examples. See the README file for this chapter for instructions.</p>&#13;
<p class="indent">The OpenSUSE Kubic project hosts repositories for CRI-O for various Linux distributions, including Ubuntu, so we will install from there. The exact URL is dependent on the version of CRI-O we want to install, and the URLs are long and challenging to type, so the automation installs a script to configure some useful environment variables. Before proceeding, we need to load that script:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">source /opt/crio-ver</span></pre>&#13;
<p class="indent">We can now use the environment variables to set up the CRI-O repositories and install CRI-O:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">echo "deb $REPO/$OS/ /" &gt; /etc/apt/sources.list.d/kubic.list</span>&#13;
root@host01:~# <span class="codestrong1">echo "deb $REPO:/cri-o:/$VERSION/$OS/ /" \</span>&#13;
  <span class="codestrong1">&gt; /etc/apt/sources.list.d/kubic.cri-o.list</span>&#13;
root@host01:~# <span class="codestrong1">curl -L $REPO/$OS/Release.key | apt-key add -</span>&#13;
...&#13;
OK&#13;
root@host01:~# <span class="codestrong1">apt update &amp;&amp; apt install -y cri-o cri-o-runc</span>&#13;
...&#13;
root@host01:~# <span class="codestrong1">systemctl enable crio &amp;&amp; systemctl start crio</span>&#13;
...&#13;
root@host01:~# <span class="codestrong1">curl -L -o /tmp/crictl.tar.gz $CRICTL_URL</span>&#13;
...&#13;
root@host01:~# <span class="codestrong1">tar -C /usr/local/bin -xvzf /tmp/crictl.tar.gz</span>&#13;
crictl&#13;
root@host01:~# <span class="codestrong1">rm -f /tmp/crictl.tar.gz</span></pre>&#13;
<p class="indent">We first add to the list of repositories for <code>apt</code> by adding files to <em>/etc/apt/sources.list.d</em>. We then use <code>apt</code> to install CRI-O packages. After CRI-O is installed, we use <code>systemd</code> to enable and start its service.</p>&#13;
<p class="indent">Unlike <code>containerd</code>, CRI-O does not ship with any command line tools that we can use for testing, so the last command installs <code>crictl</code>, which is part of the Kubernetes project and is designed for testing any container runtime compatible with the Container Runtime Interface (CRI) standard. CRI is the programming API that Kubernetes itself uses to communicate with container runtimes.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_28"/>Because <code>crictl</code> is compatible with any container runtime that supports CRI, it needs configuration to connect to CRI-O. CRI-O has installed a configuration file <em>/etc/crictl.yaml</em> to configure <code>crictl</code>:</p>&#13;
<p class="noindent6"><em>crictl.yaml</em></p>&#13;
<pre>runtime-endpoint: unix:///var/run/crio/crio.sock&#13;
image-endpoint: unix:///var/run/crio/crio.sock&#13;
...</pre>&#13;
<p class="indent">This configuration tells <code>crictl</code> to connect to CRI-O’s socket.</p>&#13;
<p class="indent">To create and run containers, the <code>crictl</code> command requires us to provide definition files in the JSON or YAML file format. The automated scripts for this chapter added two <code>crictl</code> definition files to <em>/opt</em>. The first file, shown in <a href="ch02.xhtml#ch02list1">Listing 2-1</a>, creates a Pod:</p>&#13;
<p class="noindent6"><em>pod.yaml</em></p>&#13;
<pre>---&#13;
metadata:&#13;
  name: busybox&#13;
  namespace: crio&#13;
linux:&#13;
  security_context:&#13;
    namespace_options:&#13;
      network: 2</pre>&#13;
<p class="caption" id="ch02list1"><em>Listing 2-1: CRI-O Pod definition</em></p>&#13;
<p class="indent">Similar to the Kubernetes Pod we saw in <a href="ch01.xhtml#ch01">Chapter 1</a>, the Pod is a group of one or more containers that run in the same isolated space. In our case, we need only one container in the Pod, and the second file, shown in <a href="ch02.xhtml#ch02list2">Listing 2-2</a>, defines the container process that CRI-O should start. We provide a name (<code>busybox</code>) and namespace (<code>crio</code>) to distinguish this Pod from any others. Otherwise, we need to provide only network configuration. CRI-O expects to use a Container Network Interface (CNI) plug-in to configure the network namespace. We cover CNI plug-ins in <a href="ch08.xhtml#ch08">Chapter 8</a>, so for now, we’ll use <code>network: 2</code> to tell CRI-O not to create a separate network namespace and instead use the host network:</p>&#13;
<p class="noindent6"><em>container.yaml</em></p>&#13;
<pre>---&#13;
metadata:&#13;
  name: busybox&#13;
image:&#13;
  image: docker.io/library/busybox:latest&#13;
args:&#13;
  - "/bin/sleep"&#13;
  - "36000"</pre>&#13;
<p class="caption" id="ch02list2"><em>Listing 2-2: CRI-O container definition</em></p>&#13;
<p class="indent">Again we are using BusyBox because its small size makes it fast and lightweight. However, because <code>crictl</code> will create this container in the background without a terminal, we need to specify <em>/bin/sleep</em> as the command to be run <span epub:type="pagebreak" id="page_29"/>inside the container; otherwise, the container will immediately terminate when the shell realizes that it doesn’t have a TTY.</p>&#13;
<p class="indent">Before we can run the container, we first need to pull the image:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">crictl pull docker.io/library/busybox:latest</span>&#13;
Image is up to date for docker.io/library/busybox@sha256:...</pre>&#13;
<p class="indent">Then, we provide the <em>pod.yaml</em> and <em>container.yaml</em> files to <code>crictl</code> to create and start our BusyBox container:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">cd /opt</span>&#13;
root@host01:~# <span class="codestrong1">POD_ID=$(crictl runp pod.yaml)</span>&#13;
root@host01:~# <span class="codestrong1">crictl pods</span>&#13;
POD ID              CREATED                  STATE ...&#13;
3bf297ace44b5       Less than a second ago   Ready ...&#13;
root@host01:~# <span class="codestrong1">CONTAINER_ID=$(crictl create $POD_ID container.yaml pod.yaml)</span>&#13;
root@host01:~# <span class="codestrong1">crictl start $CONTAINER_ID</span>&#13;
91394a7f37e3da3a557782ed6d6eb2cf8c23e5b3dd4e2febd415bba071d10734&#13;
root@host01:~# <span class="codestrong1">crictl ps</span>&#13;
CONTAINER           ... STATE&#13;
91394a7f37e3d       ... Running</pre>&#13;
<p class="indent">We capture the Pod’s unique identifier and the container in <code>POD_ID</code> and <code>CONTAINER_ID</code> variables, so we can use them here and upcoming commands.</p>&#13;
<p class="indent">Before looking at the Linux namespaces created by CRI-O, let’s look inside the <code>busybox</code> container by using the <code>crictl exec</code> command to start a new shell process inside it:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">crictl exec -ti $CONTAINER_ID /bin/sh</span>&#13;
/ # <span class="codestrong1">ip a</span>&#13;
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue qlen 1000&#13;
...&#13;
2: enp0s3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel qlen 1000&#13;
...&#13;
3: enp0s8: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel qlen 1000&#13;
...&#13;
/ # <span class="codestrong1">ps -ef</span>&#13;
PID   USER     TIME  COMMAND&#13;
    1 root      0:00 /pause&#13;
    7 root      0:00 /bin/sleep 36000&#13;
    13 root      0:00 /bin/sh&#13;
    20 root      0:00 ps -ef&#13;
/ # <span class="codestrong1">exit</span></pre>&#13;
<p class="indent">This BusyBox container running in CRI-O looks a little different from BusyBox running in <code>containerd</code>. First, because we configured our Pod with <code>network: 2</code>, the container can see the same network devices that a regular process would see. Second, we see a couple of additional processes. We look at the <code>pause</code> process with PID 1 when we discuss container runtimes under <span epub:type="pagebreak" id="page_30"/>Kubernetes in <a href="ch12.xhtml#ch12">Chapter 12</a>. The other extra process is <code>sleep</code>, which we created as the entry point for this container.</p>&#13;
<p class="indent">CRI-O is also using Linux namespaces for process isolation, as we can see from examining the container processes and listing namespaces:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">PID=$(crictl inspect $CONTAINER_ID | jq '.info.pid')</span>&#13;
root@host01:~# <span class="codestrong1">ps -ef | grep $PID | grep -v grep</span>&#13;
root       23906   23894  0 20:15 ?        00:00:00 /bin/sleep 36000&#13;
root@host01:/opt# <span class="codestrong1">ps -ef | grep <span class="codeitalic1">23894</span> | grep -v grep</span>&#13;
root       23894       1  0 20:15 ?        00:00:00 /usr/bin/conmon ...&#13;
root       23906   23894  0 20:15 ?        00:00:00 /bin/sleep 36000</pre>&#13;
<p class="indent">The <code>crictl inspect</code> command provides a wealth of information about the container, but for the moment, we need only the PID. Because <code>crictl</code> returns JSON-formatted output, we can use <code>jq</code> to extract the <code>pid</code> field from the <code>info</code> structure and save it to an environment variable called <code>PID</code>. Try running <span class="codestrong">crictl inspect $CONTAINER_ID</span> to see the full information.</p>&#13;
<p class="indent">Using the PID we discovered, we can see our <code>sleep</code> command. We then can use its parent PID to verify that it is managed by <code>conmon</code>, a CRI-O utility. Next, let’s see the namespaces that CRI-O has created. The allocation of namespaces to processes is more complex in CRI-O, so let’s just list all of the namespaces on our Linux system and pick out the ones related to the container:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">lsns</span>&#13;
        NS TYPE   NPROCS   PID USER            COMMAND&#13;
...&#13;
4026532183 uts         2 23867 root            /pause&#13;
4026532184 ipc         2 23867 root            /pause&#13;
4026532185 mnt         1 23867 root            /pause&#13;
4026532186 pid         2 23867 root            /pause&#13;
4026532187 mnt         1 23906 root            /bin/sleep 36000&#13;
...</pre>&#13;
<p class="indent">Here, we see only four types of namespaces. Because we told CRI-O to give the container access to the host’s network namespace, it didn’t need to create a <code>net</code> namespace. Also, with CRI-O, most namespaces are associated with the <code>pause</code> command (although some are shared by multiple processes, as we can see via the <code>NPROCS</code> column). There are two <code>mnt</code> namespaces because each separate container in a Pod gets a different set of mount points for reasons that we cover in <a href="ch05.xhtml#ch05">Chapter 5</a>.</p>&#13;
<h3 class="h3" id="ch00lev1sec10">Running Processes in Namespaces Directly</h3>&#13;
<p class="noindent">One of the trickier jobs when running a process in a container is handling the responsibility that comes with being PID 1. To better understand this, we won’t have our container runtime create a namespace for us. Instead, we’ll talk directly to the Linux kernel to run a process in a namespace <span epub:type="pagebreak" id="page_31"/>manually. We’ll use the command line, although container runtimes use the Linux kernel API, but the result will be the same.</p>&#13;
<p class="indent">Because namespaces are a Linux kernel feature, nothing else needs to be installed or configured. We just use the <code>unshare</code> command when launching the process:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">unshare -f -p --mount-proc -- /bin/sh -c /bin/bash</span></pre>&#13;
<p class="indent">The <code>unshare</code> command runs a program with different namespaces from the parent. By adding <code>-p</code>, we specify that a new PID namespace is needed. The option <code>--mount-proc</code> goes along with that, adding a new mount namespace and ensuring <em>/proc</em> is remounted correctly, so that the process sees the correct process information. Otherwise, the process would still be able to see information about other processes in the system. Finally, the content after <code>--</code> indicates the command to run.</p>&#13;
<p class="indent">Because this is an isolated process namespace, it cannot see a list of processes outside this namespace:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">ps -ef</span>&#13;
UID          PID    PPID  C STIME TTY          TIME CMD&#13;
root           1       0  0 22:21 pts/0    00:00:00 /bin/sh -c /bin/bash&#13;
root           2       1  0 22:21 pts/0    00:00:00 /bin/bash&#13;
root           9       2  0 22:22 pts/0    00:00:00 ps -ef</pre>&#13;
<p class="indent">Let’s get the ID of this namespace so that we can recognize it in a list:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">ls -l /proc/self/ns/pid</span>&#13;
lrwxrwxrwx 1 root root 0 Mar  6 22:22 /proc/self/ns/pid -&gt; 'pid:[4026532190]'</pre>&#13;
<p class="indent">Now, from another terminal window, list all of the namespaces and look for those related to our isolated shell:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">lsns</span>&#13;
        NS TYPE NPROCS PID   USER COMMAND&#13;
...&#13;
4026532189 mnt  3      12110 root unshare -f -p ...&#13;
4026532190 pid  2      12111 root /bin/sh -c /bin/bash&#13;
...&#13;
root@host01:~# <span class="codestrong1">exit</span></pre>&#13;
<p class="indent">We see a <code>pid</code> namespace matching what we saw. In addition, we see a <code>mnt</code> namespace. This namespace ensures that our shell sees the proper information in <em>/proc</em>.</p>&#13;
<p class="indent">Because the <code>pid</code> namespace is owned by the <code>sh</code> command, that command is PID 1 when we run <code>ps</code> within the namespace. This means that <code>sh</code> has the responsibility to manage its children properly (such as <code>bash</code>). For example, <code>sh</code> is responsible for passing signals to its children to ensure that they terminate correctly. It’s important to keep this in mind as it is a common problem when running containers that can result in zombie processes or other issues cleaning up a stopped container.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_32"/>Fortunately, <code>sh</code> handles its management duties well, as we can see by the fact that when we pass a <code>kill</code> signal to it, it passes that signal on to its children. Run this from the second terminal window, outside the namespace:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kill -9 12111</span></pre>&#13;
<p class="indent">Inside the first window you will see this output:</p>&#13;
<pre>root@host01:~# Killed</pre>&#13;
<p class="indent">This indicates that <code>bash</code> received the <code>kill</code> signal and terminated correctly.</p>&#13;
<h3 class="h3" id="ch00lev1sec11">Final Thoughts</h3>&#13;
<p class="noindent">Although containers create the appearance of a completely separate system, it’s done in a way that has nothing in common with virtual machines. Instead, the process is similar to traditional means of process isolation, such as user permissions and separate filesystems. Container runtimes use namespaces, which are built in to the Linux kernel and enable various types of process isolation. In this chapter, we examined how the <code>containerd</code> and CRI-O container runtimes use multiple types of Linux namespaces to give each container an independent view of other processes, network devices, and the filesystem. The use of namespaces prevents processes running in a container from seeing and interfering with other processes.</p>&#13;
<p class="indent">At the same time, processes in a container are still sharing the same CPU, memory, and network. A process that uses too many of those resources will prevent other processes from running properly. Namespaces can’t solve that problem, however. To prevent this issue, we’ll need to look at resource limiting—the topic of our next chapter.</p>&#13;
</body></html>