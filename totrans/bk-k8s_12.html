<html><head></head><body>
<h2 class="h2" id="ch10"><span epub:type="pagebreak" id="page_167"/><span class="big">10</span><br/>WHEN THINGS GO WRONG</h2>&#13;
<div class="image1"><img alt="image" src="../images/common01.jpg"/></div>&#13;
<p class="noindent">So far our installation and configuration of Kubernetes has gone as planned, and our controllers have had no problem creating Pods and starting containers. Of course, in the real world, it’s rarely that easy. Although showing everything that might go wrong with a complex application deployment isn’t possible, we can look at some of the most common problems. Most important, we can explore debugging tools that will help us diagnose any issue.</p>&#13;
<p class="indent">In this chapter, we’ll look at how to diagnose problems with application containers that we deploy on top of Kubernetes. We’ll work our way through the life cycle for scheduling and running containers, examining potential problems at each step as well as how to diagnose and fix them.</p>&#13;
<h3 class="h3" id="ch00lev1sec43">Scheduling</h3>&#13;
<p class="noindent">Scheduling is the first activity Kubernetes performs on a Pod and its containers. When a Pod is first created, the Kubernetes scheduler assigns it to a <span epub:type="pagebreak" id="page_168"/>node. Normally, this happens quickly and automatically, but some issues can prevent scheduling from happening successfully.</p>&#13;
<h4 class="h4" id="ch00lev2sec69">No Available Nodes</h4>&#13;
<p class="noindent">One possibility is that the scheduler simply doesn’t have any nodes available. This situation might occur because our cluster doesn’t have any nodes configured for regular application containers or because all nodes have failed.</p>&#13;
<p class="indent">To illustrate the case in which no nodes are available for assignment, let’s create a Pod with a <em>node selector</em>. A node selector specifies one or more node labels that are required for a Pod to be scheduled on that node. Node selectors are useful when some nodes in our cluster are different from others (for example, when some nodes have newer CPUs with support for more advanced instruction sets needed by some of our containers).</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>The example repository for this book is at</em> <a href="https://github.com/book-of-kubernetes/examples">https://github.com/book-of-kubernetes/examples</a>. <em>See “Running Examples” on <a href="ch00.xhtml#ch00lev1sec2">page xx</a> for details on getting set up.</em></p>&#13;
</div>&#13;
<p class="indent">We’ll begin with a Pod definition that has a node selector that doesn’t match any of our nodes:</p>&#13;
<p class="noindent6"><em>nginx-selector.yaml</em></p>&#13;
<pre>---&#13;
apiVersion: v1&#13;
kind: Pod&#13;
metadata:&#13;
  name: nginx&#13;
spec:&#13;
  containers:&#13;
  - name: nginx&#13;
    image: nginx&#13;
  nodeSelector:&#13;
 <span class="ent">➊</span> purpose: special</pre>&#13;
<p class="indent">The node selector <span class="ent">➊</span> tells Kubernetes to assign this Pod only to a node with a label called <code>purpose</code> whose value is equal to <code>special</code>. Even though none of our nodes currently match, we can still create this Pod:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/nginx-selector.yaml</span>&#13;
pod/nginx created</pre>&#13;
<p class="indent">However, Kubernetes is stuck trying to schedule the Pod, because it can’t find a matching node:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get pods -o wide</span>&#13;
NAME    READY   STATUS    RESTARTS   AGE    IP       NODE     ...&#13;
nginx   0/1     Pending   0          113s   &lt;none&gt;   &lt;none&gt;   ...</pre>&#13;
<p class="indent">We see a status of <code>Pending</code> and a node assignment of <code>&lt;none&gt;</code>. This is because Kubernetes has not yet scheduled this Pod onto a node.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_169"/>The <code>kubectl get</code> command is typically the first command we should run to see whether there are issues with a resource we’ve deployed to our cluster. If we have an issue, as we do in this case, the next step is to view the detailed status and event log using <code>kubectl describe</code>:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl describe pod nginx</span>&#13;
Name:         nginx&#13;
Namespace:    default&#13;
...&#13;
Status:       Pending&#13;
...&#13;
Node-Selectors:              purpose=special&#13;
&#13;
Events:&#13;
  Type     Reason            Age    From               Message&#13;
  ----     ------            ----   ----               -------&#13;
  Warning  FailedScheduling  4m36s  default-scheduler  0/3 nodes are &#13;
    available: 3 node(s) didn't match Pod's node affinity/selector.&#13;
  Warning  FailedScheduling  3m16s  default-scheduler  0/3 nodes are &#13;
    available: 3 node(s) didn't match Pod's node affinity/selector.</pre>&#13;
<p class="indent">The event log informs us as to exactly what the issue is: the Pod can’t be scheduled because none of the nodes matched the selector.</p>&#13;
<p class="indent">Let’s add the necessary label to one of our nodes:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get nodes</span>&#13;
NAME     STATUS   ROLES        ...&#13;
host01   Ready    control-plane...&#13;
host02   Ready    control-plane...&#13;
host03   Ready    control-plane...&#13;
root@host01:~# <span class="codestrong1">kubectl label nodes host02 purpose=special</span>&#13;
node/host02 labeled</pre>&#13;
<p class="indent">We first list the three nodes we have available and then apply the necessary label to one of them. As soon as we apply this label, Kubernetes can now schedule the Pod:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get pods -o wide</span>&#13;
NAME    READY   STATUS    RESTARTS   AGE   IP               NODE     ...&#13;
nginx   1/1     Running   0          10m   172.31.89.196   host02   ...&#13;
root@host01:~# <span class="codestrong1">kubectl describe pod nginx</span>&#13;
Name:         nginx&#13;
Namespace:    default&#13;
...&#13;
Events:&#13;
  Type     Reason            Age    From               Message&#13;
  ----     ------            ----   ----               -------&#13;
  Warning  FailedScheduling  10m    default-scheduler  0/3 nodes are &#13;
    available: 3 node(s) didn't match Pod's node affinity/selector.&#13;
<span epub:type="pagebreak" id="page_170"/>  Warning  FailedScheduling  9m17s  default-scheduler  0/3 nodes are &#13;
    available: 3 node(s) didn't match Pod's node affinity/selector.&#13;
  Normal   Scheduled         2m22s  default-scheduler  Successfully assigned &#13;
    default/nginx to host02&#13;
...</pre>&#13;
<p class="indent">As expected, the Pod was scheduled onto the node where we applied the label.</p>&#13;
<p class="indent">This example, like the others we’ll see in this chapter, illustrates debugging in Kubernetes. After we’ve created the resources that we need, we query the cluster state to make sure the actual deployment of those resources was successful. When we find issues, we can correct those issues and our resources will be started as desired without having to reinstall our application components.</p>&#13;
<p class="indent">Let’s clean up this NGINX Pod:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl delete -f /opt/nginx-selector.yaml</span>&#13;
pod "nginx" deleted</pre>&#13;
<p class="indent">Let’s also remove the label from the node. We remove the label by appending a minus sign to it to identify it:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl label nodes host02 purpose-</span>&#13;
node/host02 unlabeled</pre>&#13;
<p class="indent">We’ve covered one issue with the scheduler, but there’s still another we need to look at.</p>&#13;
<h4 class="h4" id="ch00lev2sec70">Insufficient Resources</h4>&#13;
<p class="noindent">When choosing a node to host a Pod, the scheduler also considers the resources that are available on each node and the resources the Pod requires. We explore resource limits in detail in <a href="ch14.xhtml#ch14">Chapter 14</a>; for now it’s enough to know that each container in a Pod can request the resources it needs, and the scheduler will ensure that it is scheduled onto a node that has those resources available. Of course, if there aren’t any nodes with enough room, the scheduler won’t be able to schedule the Pod. Instead the Pod will wait in a <code>Pending</code> state.</p>&#13;
<p class="indent">Let’s look at an example Pod definition to illustrate this:</p>&#13;
<p class="noindent6"><em>sleep-multiple.yaml</em></p>&#13;
<pre>---&#13;
apiVersion: v1&#13;
kind: Pod&#13;
metadata:&#13;
  name: sleep&#13;
spec:&#13;
  containers:&#13;
  - name: sleep&#13;
    image: busybox&#13;
<span epub:type="pagebreak" id="page_171"/>    command: &#13;
      - "/bin/sleep"&#13;
      - "3600"&#13;
    resources:&#13;
      requests:&#13;
        cpu: "2"&#13;
  - name: sleep2&#13;
    image: busybox&#13;
    command: &#13;
      - "/bin/sleep"&#13;
      - "3600"&#13;
    resources:&#13;
      requests:&#13;
        cpu: "2"</pre>&#13;
<p class="indent">In this YAML definition, we create two containers in the same Pod. Each container requests two CPUs. Because all of the containers in a Pod must be on the same host in order to share some Linux namespace types (especially the network namespace so that they can use <code>localhost</code> for communication), the scheduler needs to find a single node with four CPUs available. In our small cluster, that can’t happen, as we can see if we try to deploy the Pod:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/sleep-multiple.yaml</span>&#13;
pod/sleep created&#13;
root@host01:~# <span class="codestrong1">kubectl get pods -o wide</span>&#13;
NAME    READY   STATUS    RESTARTS   AGE   IP       NODE   ...&#13;
sleep   0/2     Pending   0          7s    &lt;none&gt;   &lt;none&gt; ...</pre>&#13;
<p class="indent">As before, <code>kubectl describe</code> gives us the event log that reveals the issue:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl describe pod sleep</span>&#13;
Name:         sleep&#13;
Namespace:    default&#13;
...&#13;
Events:&#13;
  Type     Reason            Age   From               Message&#13;
  ----     ------            ----  ----               -------&#13;
  Warning  FailedScheduling  71s   default-scheduler  0/3 nodes are &#13;
    available: 3 Insufficient cpu.</pre>&#13;
<p class="indent">Notice that it doesn’t matter how heavily loaded our nodes actually are:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl top node</span>&#13;
NAME     CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   &#13;
host01   429m         21%    1307Mi          69%&#13;
host02   396m         19%    1252Mi          66%&#13;
host03   458m         22%    1277Mi          67%</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_172"/>Nor does it matter how much CPU our containers will actually use. The scheduler allocates Pods purely based on what it requested; this way, we don’t suddenly overwhelm a CPU when load increases.</p>&#13;
<p class="indent">We can’t magically provide our nodes with more CPUs, so to get this Pod scheduled, we’re going to need to specify a lower CPU usage for our two containers. Let’s use a more sensible figure of 0.1 CPU:</p>&#13;
<p class="noindent6"><em>sleep-sensible.yaml</em></p>&#13;
<pre>---&#13;
apiVersion: v1&#13;
kind: Pod&#13;
metadata:&#13;
  name: sleep&#13;
spec:&#13;
  containers:&#13;
  - name: sleep&#13;
    image: busybox&#13;
    command: &#13;
      - "/bin/sleep"&#13;
      - "3600"&#13;
    resources:&#13;
      requests:&#13;
     <span class="ent">➊</span> cpu: "100m"&#13;
  - name: sleep2&#13;
    image: busybox&#13;
    command: &#13;
      - "/bin/sleep"&#13;
      - "3600"&#13;
    resources:&#13;
      requests:&#13;
        cpu: "100m"</pre>&#13;
<p class="indent">The value <code>100m</code> <span class="ent">➊</span> equates to “one hundred millicpu” or one-tenth (0.1) of a CPU.</p>&#13;
<p class="indent">Even though this is a separate file, it declares the same resource, so Kubernetes will treat it as an update. However, if we try to apply this as a change to the existing Pod, it will fail:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/sleep-sensible.yaml</span>&#13;
The Pod "sleep" is invalid: spec: Forbidden: pod updates may not change &#13;
  fields other than ...</pre>&#13;
<p class="indent">We are not allowed to change the resource request of an existing Pod, which makes sense given that a Pod is allocated to a node only once on creation, and a resource usage change might cause the node to be overly full.</p>&#13;
<p class="indent">If we were using a controller such as a Deployment, the controller could handle replacing the Pods for us. Because we created a Pod directly, we need to manually delete and then re-create it:</p>&#13;
<pre><span epub:type="pagebreak" id="page_173"/>root@host01:~# <span class="codestrong1">kubectl delete pod sleep</span>&#13;
pod "sleep" deleted&#13;
root@host01:~# <span class="codestrong1">kubectl apply -f /opt/sleep-sensible.yaml</span>&#13;
pod/sleep created</pre>&#13;
<p class="indent">Our new Pod has no trouble with node allocation:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get pods -o wide</span>&#13;
NAME    READY   STATUS    RESTARTS   AGE   IP               NODE  ...&#13;
sleep   2/2     Running   0          51s   172.31.89.199   host02 ...</pre>&#13;
<p class="indent">And if we run <code>kubectl describe</code> on the node, we can see how our new Pod has been allocated some of the node’s CPU:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl describe node</span> <span class="codestrong1"><span class="codeitalic1">host02</span></span>&#13;
Name:               host02&#13;
...&#13;
Capacity:&#13;
  cpu:                2&#13;
...&#13;
Non-terminated Pods:          (10 in total)&#13;
  Namespace  Name     CPU Requests  CPU Limits  ...&#13;
  ---------  ----     ------------  ----------  ...&#13;
...&#13;
  default    sleep <span class="ent">➊</span> 200m (10%)    0 (0%)      ... &#13;
...</pre>&#13;
<p class="indent">Be sure to use the correct node name for the node where your Pod was deployed. Because our Pod has two containers, each requesting <code>100m</code>, its total request is <code>200m</code> <span class="ent">➊</span>.</p>&#13;
<p class="indent">Let’s finish by cleaning up this Pod:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl delete pod sleep</span>&#13;
pod "sleep" deleted</pre>&#13;
<p class="indent">Other errors can prevent a Pod from being scheduled, but these are the most common. Most important, the commands we used here apply in all cases. First, use <code>kubectl get</code> to determine the Pod’s current status, followed by <code>kubectl describe</code> to view the event log. These two commands are always a good first step when something doesn’t seem to be working properly.</p>&#13;
<h3 class="h3" id="ch00lev1sec44">Pulling Images</h3>&#13;
<p class="noindent">After a Pod is scheduled onto a node, the local <code>kubelet</code> service interacts with the underlying container runtime to create an isolated environment and start containers. However, there’s still one application misconfiguration that can cause our Pod to become stuck in the <code>Pending</code> phase: inability to pull the container image.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_174"/>Three main issues can prevent the container runtime from pulling an image:</p>&#13;
<ul>&#13;
<li><p class="noindent">Failure to connect to the container image registry</p></li>&#13;
<li><p class="noindent">Authorization issue with the requested image</p></li>&#13;
<li><p class="noindent">Image is missing from the registry</p></li>&#13;
</ul>&#13;
<p class="indent">As we described in <a href="ch05.xhtml#ch05">Chapter 5</a>, an image registry is a web server. Often, the image registry is outside the cluster, and the nodes need to be able to connect to an external network or the internet to reach the registry. Additionally, most registries support publishing private images that require authentication and authorization to access. And, of course, if there is no image published under the name we specify, the container runtime is not going to be able to pull it from the registry.</p>&#13;
<p class="indent">All of these errors behave the same way in our Kubernetes cluster, with differences only in the message in the event log, so we’ll need to explore only one of them. We’ll look at what is probably the most common issue: a missing image caused by a typo in the image name.</p>&#13;
<p class="indent">Let’s try to create a Pod using this YAML file:</p>&#13;
<p class="noindent6"><em>nginx-typo.yaml</em></p>&#13;
<pre>---&#13;
apiVersion: v1&#13;
kind: Pod&#13;
metadata:&#13;
  name: nginx&#13;
spec:&#13;
  containers:&#13;
  - name: nginx&#13;
    image: nginz</pre>&#13;
<p class="indent">Because there is no image in Docker Hub called <code>nginz</code>, it won’t be possible to pull this image. Let’s explore what happens when we add this resource to the cluster:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/nginx-typo.yaml</span>&#13;
pod/nginx created&#13;
root@host01:~# <span class="codestrong1">kubectl get pods</span>&#13;
NAME    READY   STATUS             RESTARTS   AGE&#13;
nginx   0/1     ImagePullBackOff   0          20s</pre>&#13;
<p class="indent">Our Pod has status <code>ImagePullBackOff</code>, which immediately signals two things. First, this Pod is not yet getting to the point at which the containers are running, because it has not yet pulled the container images. Second, as with all errors, Kubernetes will continue attempting the action, but it will use a <em>back-off</em> algorithm to avoid overwhelming our cluster’s resources. <span epub:type="pagebreak" id="page_175"/>Pulling an image involves reaching out over the network to communicate with the image registry, and it would be rude and a waste of network bandwidth to flood the registry with many requests in a short amount of time. Moreover, the cause of the failure may be transient, so the cluster will keep trying in hopes that the problem will be resolved.</p>&#13;
<p class="indent">The fact that Kubernetes uses a back-off algorithm for retrying errors is important for debugging. In this case, we obviously are not going to publish an <code>nginz</code> image to Docker Hub to fix the problem. But for cases in which we do fix the issue by publishing an image, or by changing the permissions for the image, it’s important to know that Kubernetes will not pick up that change immediately, because the amount of delay between tries increases with each failure.</p>&#13;
<p class="indent">Let’s explore the event log so that we can see this back-off in action:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl describe pod nginx</span>&#13;
Name:         nginx&#13;
Namespace:    default&#13;
...&#13;
Status:     <span class="ent">➊</span> Pending &#13;
...&#13;
Events:&#13;
  Type     Reason     Age                 From               Message&#13;
  ----     ------     ----                ----               -------&#13;
  Normal   Scheduled  114s                default-scheduler  Successfully &#13;
    assigned default/nginx to host03&#13;
...&#13;
  Warning  Failed     25s (x4 over 112s)  kubelet            Failed to pull &#13;
    image "nginz": ... <span class="ent">➋</span> pull access denied, repository does not exist or may &#13;
    require authorization  ...&#13;
...&#13;
  Normal   BackOff    1s <span class="ent">➌</span> (x7 over 111s)   kubelet            ...</pre>&#13;
<p class="indent">As before, our Pod is stuck in a <code>Pending</code> status <span class="ent">➊</span>. In this case, however, the Pod has gotten past the scheduling activity and has moved on to pulling the image. For security reasons, the registry does not distinguish between a private image for which we don’t have permission to access and a missing image, so Kubernetes can tell us only that the issue is one or the other <span class="ent">➋</span>. Finally, we can see that Kubernetes has tried to pull the image seven times during the two minutes since we created this Pod <span class="ent">➌</span>, and it last tried to pull the image one second ago.</p>&#13;
<p class="indent">If we wait a few minutes and then run the same <code>kubectl describe</code> command again, focusing on the back-off behavior, we can see that a long amount of time elapses between tries:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl describe pod nginx</span>&#13;
Name:         nginx&#13;
Namespace:    default&#13;
...&#13;
<span epub:type="pagebreak" id="page_176"/>Events:&#13;
  Type     Reason     Age                   From               Message&#13;
  ----     ------     ----                  ----               -------&#13;
...&#13;
  Normal   BackOff    4m38s (x65 over 19m)  kubelet            ...</pre>&#13;
<p class="indent">Kubernetes has now tried to pull the image 65 times over the course of 19 minutes. However, the amount of delay has grown over time and has reached the maximum of five minutes between each attempt. This means that as we debug this issue, we will need to wait up to five minutes each time to see whether the problem has been resolved.</p>&#13;
<p class="indent">Let’s go ahead and fix the issue so that we can see this in action. We could fix the YAML file and run <code>kubectl apply</code> again, but we can also fix it using <code>kubectl set</code>:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl set image pod nginx nginx=nginx</span>&#13;
pod/nginx image updated&#13;
root@host01:~# <span class="codestrong1">kubectl get pods</span>&#13;
NAME    READY   STATUS             RESTARTS   AGE&#13;
nginx   0/1     ImagePullBackOff   0          28m</pre>&#13;
<p class="indent">The <code>kubectl set</code> command requires us to specify the resource type and name; in this case <code>pod nginx</code>. We then specify <code>nginx=nginx</code> to provide the name of the container to modify (because a Pod can have multiple containers) along with the new image.</p>&#13;
<p class="indent">We fixed the image name, but the Pod is still showing <code>ImagePullBackOff</code> because we must wait for the five-minute timer to elapse before Kubernetes tries again. Upon the next try, the pull is successful and the Pod starts running:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get pods</span>&#13;
NAME    READY   STATUS    RESTARTS   AGE&#13;
nginx   1/1     Running   0          32m</pre>&#13;
<p class="indent">Let’s clean up the Pod before moving on:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl delete pod nginx</span>&#13;
pod "nginx" deleted</pre>&#13;
<p class="indent">Again, we were able to solve this using <code>kubectl get</code> and <code>kubectl describe</code>. However, when we get to the point that the container is running, that won’t be sufficient.</p>&#13;
<h3 class="h3" id="ch00lev1sec45">Running Containers</h3>&#13;
<p class="noindent">After instructing the container runtime to pull any images needed, <code>kubelet</code> then tells the runtime to start the containers. For the rest of the examples in this chapter, we’ll assume that the container runtime is working as expected. At this point, then, the main problem we’ll run into is the case in which the <span epub:type="pagebreak" id="page_177"/>container does not start as expected. Let’s begin with a simpler example of debugging a container that fails to run, and then we’ll look at a more complex example.</p>&#13;
<h4 class="h4" id="ch00lev2sec71">Debugging Using Logs</h4>&#13;
<p class="noindent">For our simple example, we first need a Pod definition with a container that fails on startup. Here’s a Pod definition for PostgreSQL that will do what we want:</p>&#13;
<p class="noindent6"><em>postgres-misconfig.yaml</em></p>&#13;
<pre>---&#13;
apiVersion: v1&#13;
kind: Pod&#13;
metadata:&#13;
  name: postgres&#13;
spec:&#13;
  containers:&#13;
  - name: postgres&#13;
    image: postgres</pre>&#13;
<p class="indent">It might not seem like there are any issues with this definition, but PostgreSQL has some required configuration when running in a container.</p>&#13;
<p class="indent">We can create the Pod using <code>kubectl apply</code>:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/postgres-misconfig.yaml</span>&#13;
pod/postgres created</pre>&#13;
<p class="indent">After a minute or so to allow time to pull the image, we can check the status with <code>kubectl get</code>, and we’ll notice a status we haven’t seen before:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get pods</span>&#13;
NAME       READY   STATUS             RESTARTS     AGE&#13;
postgres   0/1     CrashLoopBackOff   1 (8s ago)   25s</pre>&#13;
<p class="indent">The <code>CrashLoopBackOff</code> status indicates that a container in the Pod has exited. As this is not a Kubernetes Job, it doesn’t expect the container to exit, so it’s considered a crash.</p>&#13;
<p class="indent">If you catch the Pod at the right time, you might see an <code>Error</code> status rather than <code>CrashLoopBackOff</code>. This is temporary: the Pod transitions through that status immediately after crashing.</p>&#13;
<p class="indent">Like the <code>ImagePullBackOff</code> status, a <code>CrashLoopBackOff</code> uses an algorithm to retry the failure, increasing the time between retries with every failure, to avoid overwhelming the cluster. We can see this back-off if we wait a few minutes and then print the status again:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get pods</span>&#13;
NAME       READY   STATUS             RESTARTS       AGE&#13;
postgres   0/1     CrashLoopBackOff   5 (117s ago)   5m3s</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_178"/>After five restarts, we’re already up to more than a minute of wait time between retries. The wait time will continue to increase until we reach five minutes, and then Kubernetes will continue to retry every five minutes thereafter indefinitely.</p>&#13;
<p class="indent">Let’s use <code>kubectl describe</code>, as usual, to try to get more information about this failure:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl describe pod postgres</span>&#13;
Name:         postgres&#13;
Namespace:    default&#13;
...&#13;
Containers:&#13;
  postgres:&#13;
...&#13;
    State:          Waiting&#13;
      Reason:       CrashLoopBackOff&#13;
    Last State:     Terminated&#13;
      Reason:       Error&#13;
      Exit Code:    1&#13;
...&#13;
Events:&#13;
  Type     Reason     Age                    From               Message&#13;
  ----     ------     ----                   ----               -------&#13;
...&#13;
  Warning  BackOff    3m13s (x24 over 8m1s)  kubelet            Back-off &#13;
    restarting failed container</pre>&#13;
<p class="indent">The <code>kubectl describe</code> command does give us one piece of useful information: the exit code for the container. However, that really just tells us there was an error of some kind; it isn’t enough to fully debug the failure. To establish why the container is failing, we’ll look at the container logs using the <code>kubectl logs</code> command:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl logs postgres</span>&#13;
Error: Database is uninitialized and superuser password is not specified.&#13;
  You must specify POSTGRES_PASSWORD to a non-empty value for the&#13;
  superuser. For example, "-e POSTGRES_PASSWORD=password" on "docker run".&#13;
...</pre>&#13;
<p class="indent">We can see the logs even though the container has already stopped, because the container runtime has captured them.</p>&#13;
<p class="indent">This message comes directly from PostgreSQL itself. Fortunately, it tells us exactly what the issue is: we are missing a required environment variable. We can quickly fix this with an update to the YAML resource file:</p>&#13;
<p class="noindent6"><em>postgres-fixed.yaml</em></p>&#13;
<pre>---&#13;
apiVersion: v1&#13;
kind: Pod&#13;
metadata:&#13;
<span epub:type="pagebreak" id="page_179"/>  name: postgres&#13;
spec:&#13;
  containers:&#13;
  - name: postgres&#13;
    image: postgres&#13;
 <span class="ent">➊</span> env:&#13;
    - name: POSTGRES_PASSWORD&#13;
      value: "supersecret"</pre>&#13;
<p class="indent">The <code>env</code> field <span class="ent">➊</span> adds a configuration to pass in the required environment variable. Of course, in a real system we would not put this directly in a YAML file in plaintext. We look at how to secure this kind of information in <a href="ch16.xhtml#ch16">Chapter 16</a>.</p>&#13;
<p class="indent">To apply this change, we first need to delete the Pod definition and then apply the new resource configuration to the cluster:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl delete pod postgres</span>&#13;
pod "postgres" deleted&#13;
root@host01:~# <span class="codestrong1">kubectl apply -f /opt/postgres-fixed.yaml</span>&#13;
pod/postgres created</pre>&#13;
<p class="indent">As before, if we were using a controller such as a Deployment, we could just update the Deployment, and it would handle deleting the old Pod and creating a new one for us.</p>&#13;
<p class="indent">Now that we’ve fixed the configuration, our PostgreSQL container starts as expected:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get pods</span>&#13;
NAME       READY   STATUS    RESTARTS   AGE&#13;
postgres   1/1     Running   0          77s</pre>&#13;
<p class="indent">Let’s clean up this Pod before we continue to our next example:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl delete pod postgres</span>&#13;
pod "postgres" deleted</pre>&#13;
<p class="indent">Most well-written applications will print log messages before terminating, but we need to be prepared for more difficult cases. Let’s look at one more example that includes two new debugging approaches.</p>&#13;
<h4 class="h4" id="ch00lev2sec72">Debugging Using Exec</h4>&#13;
<p class="noindent">For this example, we’ll need an application that behaves badly. We’ll use a C program that does some very naughty memory access. This program is packaged into an Alpine Linux container so that we can run it as a container in Kubernetes. Here’s the C source code:</p>&#13;
<p class="noindent6"><em>crasher.c</em></p>&#13;
<pre>int main() {&#13;
  char *s = "12";&#13;
  s[2] = '3';&#13;
<span epub:type="pagebreak" id="page_180"/>  return 0;&#13;
}</pre>&#13;
<p class="indent">The first line of code creates a pointer to a string that is two characters long; the second line then tries to write to the non-existent third character, causing the program to terminate immediately.</p>&#13;
<p class="indent">This C program can be compiled on any system by using <code>gcc</code> to create a <code>crasher</code> executable. If you build it on a host Linux system, use this <code>gcc</code> command:</p>&#13;
<pre>$ <span class="codestrong1">gcc -g -static -o crasher crasher.c</span></pre>&#13;
<p class="indent">The <code>-g</code> argument ensures that debugging symbols are available. We’ll use those in a moment. The <code>-static</code> argument is the most important; we want to package this as a standalone application inside an Alpine container image. If we are building on a different Linux distribution, such as Ubuntu, the standard libraries are based on a different toolchain, and dynamic linking will fail. For this reason, we want our executable to have all of its dependencies statically linked. Finally, we use <code>-o</code> to specify the output executable name and then provide the name of our C source file.</p>&#13;
<p class="indent">Alternatively, you can just use the container image that’s already been built and published to Docker Hub under the name <code>bookofkubernetes/crasher: stable</code>. This image is built and published automatically using GitHub Actions based on the code in the repository <em><a href="https://github.com/book-of-kubernetes/crasher">https://github.com/book-of-kubernetes/crasher</a></em>. Here’s the <em>Dockerfile</em> from that repository:</p>&#13;
<p class="noindent6"><em>Dockerfile</em></p>&#13;
<pre>FROM alpine AS builder&#13;
COPY ./crasher.c /&#13;
RUN apk --update add gcc musl-dev &amp;&amp; \&#13;
    gcc -g -o crasher crasher.c&#13;
&#13;
FROM alpine&#13;
COPY --from=builder /crasher /crasher&#13;
CMD [ "/crasher" ]</pre>&#13;
<p class="indent">This <em>Dockerfile</em> takes advantage of Docker’s multistage builds capability to reduce the final image size. To compile inside an Alpine container, we need <code>gcc</code> and the core C include files and libraries. However, these have the effect of making the container image significantly larger. We only need them at compile time, so we want to avoid having that extra content in the final image.</p>&#13;
<p class="indent">When we run this build using the <code>docker build</code> command that we saw in <a href="ch05.xhtml#ch05">Chapter 5</a>, Docker will create one container based on Alpine Linux, copy our source code into it, install the developer tools, and compile the application. Docker will then start over with a fresh Alpine Linux container and will copy the resulting executable from the first container. The final container image is captured from this second container, so we avoid adding the developer tools to the final image.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_181"/>Let’s run this image in our Kubernetes cluster. We’ll use a Deployment resource this time so that we can illustrate editing it to work around the crashing container:</p>&#13;
<p class="noindent6"><em>crasher-deploy.yaml</em></p>&#13;
<pre>---&#13;
kind: Deployment&#13;
apiVersion: apps/v1&#13;
metadata:&#13;
  name: crasher&#13;
spec:&#13;
  replicas: 1&#13;
  selector:&#13;
    matchLabels:&#13;
      app: crasher&#13;
  template:&#13;
    metadata:&#13;
      labels:&#13;
        app: crasher&#13;
    spec:&#13;
      containers:&#13;
      - name: crasher&#13;
        image: bookofkubernetes/crasher:stable</pre>&#13;
<p class="indent">This basic Deployment is very similar to what we saw when we introduced Deployments in <a href="ch07.xhtml#ch07">Chapter 7</a>. We specify the <code>image</code> field to match the location where the image is published.</p>&#13;
<p class="indent">We can add this Deployment to the cluster in the usual way:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/crasher-deploy.yaml</span>&#13;
deployment.apps/crasher created</pre>&#13;
<p class="indent">As soon as Kubernetes has had a chance to schedule the Pod and pull the image, it starts crashing, as expected:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get pods</span>&#13;
NAME                       READY   STATUS             RESTARTS      AGE&#13;
crasher-76cdd9f769-5blbn   0/1     CrashLoopBackOff   3 (24s ago)   73s</pre>&#13;
<p class="indent">As before, using <code>kubectl describe</code> tells us only the exit code of the container. There’s another way to get this exit code; we can use the JSON output format of <code>kubectl get</code> and the <code>jq</code> tool to capture just the exit code:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get pod <span class="codeitalic1">crasher-7978d9bcfb-wvx6q</span> -o json | \</span>&#13;
  <span class="codestrong1">jq '.status.containerStatuses[].lastState.terminated.exitCode'</span>&#13;
139</pre>&#13;
<p class="indent">Be sure to use the correct name for your Pod based on the output of <code>kubectl get pods</code>. The path to the specific field we need is based on how Kubernetes tracks this resource internally; with some practice it becomes easier <span epub:type="pagebreak" id="page_182"/>to craft a path to <code>jq</code> to capture a specific field, which is a very handy trick in scripting.</p>&#13;
<p class="indent">The exit code of 139 tells us that the container terminated with a segmentation fault. However, the logs are unhelpful in diagnosing the problem, because our program didn’t print anything before it crashed:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl logs</span> <span class="codestrong1"><span class="codeitalic1">crasher-76cdd9f769-5blbn</span></span>&#13;
[ no output ]</pre>&#13;
<p class="indent">We have quite a problem. The logs aren’t helpful, so the next step would be to use <code>kubectl exec</code> to get inside the container. However, the container stops immediately when our application crashes and is not around long enough for us to do any debugging work.</p>&#13;
<p class="indent">To fix this, we need a way to start this container without running the crashing program. We can do that by overriding the default command to have our container remain running. Because we built on an Alpine Linux image, the <code>sleep</code> command is available to us for this purpose.</p>&#13;
<p class="indent">We could edit our YAML file and update the Deployment that way, but we can also edit the Deployment directly using <code>kubectl edit</code>, which will bring up the current definition in an editor, and any changes we make will be saved to the cluster:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl edit deployment crasher</span></pre>&#13;
<p class="indent">This will bring up vi in an editor window with the Deployment resource in YAML format. The resource will include a lot more fields than we provided when we created it because Kubernetes will show us the status of the resource as well as some fields with default values.</p>&#13;
<p class="indent">If you don’t like vi, you can preface the <code>kubectl edit</code> command with <code>KUBE_EDITOR=nano</code> to use the Nano editor, instead.</p>&#13;
<p class="indent">Within the file, find these lines:</p>&#13;
<pre>    spec:&#13;
      containers:&#13;
      - image: bookofkubernetes/crasher:stable&#13;
        imagePullPolicy: IfNotPresent</pre>&#13;
<p class="indent">You will see the <code>imagePullPolicy</code> line even though it wasn’t in the YAML resource, as Kubernetes has added the default policy to the resource automatically. Add a new line between <code>image</code> and <code>imagePullPolicy</code> so that the result looks like this:</p>&#13;
<pre>    spec:&#13;
      containers:&#13;
      - image: bookofkubernetes/crasher:stable&#13;
        <span class="codestrong1">args: ["/bin/sleep", "infinity"]</span>&#13;
        imagePullPolicy: IfNotPresent</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_183"/>This added line overrides the default command for the container so that it runs <code>sleep</code> instead of running our crashing program. Save and exit the editor, and <code>kubectl</code> will pick up the new definition:</p>&#13;
<pre>deployment.apps/crasher edited</pre>&#13;
<p class="indent">After <code>kubectl</code> applies this change to the cluster, the Deployment must delete the old Pod and create a new one. This is done automatically, so the only difference we’ll notice is the automatically generated part of the Pod name. Of course, we’ll also see the Pod running:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get pods</span>&#13;
NAME                       READY   STATUS    RESTARTS   AGE&#13;
crasher-58d56fc5df-vghbt   1/1     Running   0          3m29s</pre>&#13;
<p class="indent">Our Pod is now running, but it’s only running <code>sleep</code>. We still need to debug our actual application. To do that, we can now get a shell prompt inside our container:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl exec -ti crasher-<span class="codeitalic1">58d56fc5df-vghbt</span></span> -- /bin/sh&#13;
/ #</pre>&#13;
<p class="indent">The Deployment replaced the Pod when we changed the definition, so the name has changed. As before, use the correct name for your Pod. At this point we can try out our crashing program manually:</p>&#13;
<pre>/ # <span class="codestrong1">/crasher</span>&#13;
Segmentation fault (core dumped)</pre>&#13;
<p class="indent">In many cases, the ability to run a program this way, playing with different environment variables and command line options, may be enough to find and fix the problem. Alternatively, we could try running the program with <code>strace</code>, which would tell us what system calls the program is trying to make and what files it is trying to open prior to crashing. In this case, we know that the program is crashing with a segmentation fault, meaning that the problem is likely a programming error, so our best approach is to connect a debugging tool to the application using port forwarding.</p>&#13;
<h4 class="h4" id="ch00lev2sec73">Debugging Using Port Forwarding</h4>&#13;
<p class="noindent">We’ll illustrate port forwarding using the text-based debugger <code>gdb</code>, but any debugger that can connect via a network port will work. First, we need to get our application created inside the container using a debugger that will listen on a network port and wait before it runs the code. To do that, we’ll need to install <code>gdb</code> inside our container. Because this is an Alpine container, we’ll use <code>apk</code>:</p>&#13;
<pre>/ # <span class="codestrong1">apk add gdb</span>&#13;
...&#13;
(13/13) Installing gdb (10.1-r0)&#13;
<span epub:type="pagebreak" id="page_184"/>Executing busybox-1.32.1-r3.trigger&#13;
OK: 63 MiB in 27 packages</pre>&#13;
<p class="indent">The version of <code>gdb</code> we installed includes <code>gdbserver</code>, which enables us to start a networked debug session.</p>&#13;
<p class="indent">Because <code>gdb</code> is a text-based debugger, we could obviously just start it directly to debug our application, but it is often nicer to use a debugger with a GUI, making it easier for us to step through source, set breakpoints, and watch variables. For this reason, I’m showing the process for connecting a debugger over the network.</p>&#13;
<p class="indent">Let’s start <code>gdbserver</code> and set it up to listen on port <code>2345</code>:</p>&#13;
<pre>/ # <span class="codestrong1">gdbserver localhost:2345 /crasher</span>&#13;
Process /crasher created; pid = 25&#13;
Listening on port 2345</pre>&#13;
<p class="indent">Note that we told <code>gdbserver</code> to listen to the <code>localhost</code> interface. We’ll still be able to connect to the debugger because we’ll have Kubernetes provide us with port forwarding with the <code>kubectl port-forward</code> command. This command causes <code>kubectl</code> to connect to the API server and request it to forward traffic to a specific port on a specific Pod. The advantage is that we can use this port forwarding capability from anywhere we can connect to the API server, even outside the cluster.</p>&#13;
<p class="indent">Using port forwarding specifically to run a remote debugger may not be an everyday occurrence for either a Kubernetes cluster administrator or the developer of a containerized application, but it’s a valuable skill to have when there’s no other way to find the bug. It’s also a great way to illustrate the power of port forwarding to reach a Pod.</p>&#13;
<p class="indent">Because we have our debugger running in our first terminal, we’ll need another terminal tab or window for the port forwarding, which can be done from any of the hosts in our cluster. Let’s use <code>host01</code>:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl port-forward <span class="codeitalic1">pods/crasher-58d56fc5df-vghbt</span> 2345:2345</span>&#13;
Forwarding from 127.0.0.1:2345 -&gt; 2345&#13;
Forwarding from [::1]:2345 -&gt; 2345</pre>&#13;
<p class="indent">This <code>kubectl</code> command starts listening on port <code>2345</code> and forwards all traffic through the API server to the Pod we specified. Because this command keeps running, we need yet another terminal window or tab for our final step, which is to run the debugger we’ll use to connect to our debug server running in the container. This must be done from the same host as our <code>kubectl port-forward</code> command because that program is listening only on local interfaces.</p>&#13;
<p class="indent">At this point, we could run any debugger that knows how to talk to the debug server. For simplicity, we’ll use <code>gdb</code> again. We’ll begin by changing to the <em>/opt</em> directory because our C source file is there:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">cd /opt</span></pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_185"/>Now we can kick off <code>gdb</code> and use it to connect to the debug server:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">gdb -q</span>&#13;
(gdb) <span class="codestrong1">target remote localhost:2345</span>&#13;
Remote debugging using localhost:2345&#13;
...&#13;
Reading /crasher from remote target...&#13;
Reading symbols from target:/crasher...&#13;
0x0000000000401bc0 in _start ()</pre>&#13;
<p class="indent">Our debug session connects successfully and is waiting for us to start the program, which we’ll do by using the <code>continue</code> command:</p>&#13;
<pre>(gdb) <span class="codestrong1">continue</span>&#13;
Continuing.&#13;
&#13;
Program received signal SIGSEGV, Segmentation fault.&#13;
main () at crasher.c:3&#13;
3         s[2] = '3';</pre>&#13;
<p class="indent">With the debugger, we’re able to see exactly which line of our source code is causing the segmentation fault, and now we can figure out how to fix it.</p>&#13;
<h3 class="h3" id="ch00lev1sec46">Final Thoughts</h3>&#13;
<p class="noindent">When we move our application components into container images and run them in a Kubernetes cluster, we gain substantial benefits in scalability and automated failover, but we introduce a number of new possibilities that can go wrong when getting our application running, and we introduce new challenges in debugging those problems. In this chapter, we’ve looked at how to use Kubernetes commands to systematically track our application startup and operation to determine what is preventing it from working correctly. With these commands, we can debug any kind of issue happening at the application level, even if an application component won’t start correctly in its containerized environment.</p>&#13;
<p class="indent">Now that we have a clear picture of running containers using Kubernetes, we can begin to look in depth into the capabilities of the cluster itself. As we do this, we’ll be sure to explore how each component works so as to have the tools needed to diagnose problems. We’ll start in the next chapter by looking in detail at the Kubernetes control plane.<span epub:type="pagebreak" id="page_186"/></p>&#13;
</body></html>