- en: '**19'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: FRACTIONAL NUMBERS**
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We have been using only integral values—integers and characters—in our programs
    so far. In this chapter, we’ll look at how computers represent fractional numbers.
    You’ll learn about two ways to represent fractional values: fixed point and floating
    point.'
  prefs: []
  type: TYPE_NORMAL
- en: I’ll start with fixed-point numbers, to show you how fractional values are represented
    in binary. As you will see, using some of the bits for the fractional part of
    a number reduces the number of bits left for the integer part, thus reducing the
    range of numbers we can represent. Including a fractional part only allows us
    to divide that range into smaller portions.
  prefs: []
  type: TYPE_NORMAL
- en: This limitation on the range will lead us to a discussion of floating-point
    numbers, which allow for a much larger range but introduce other limitations.
    I’ll show you the format and properties of floating-point representation and then
    discuss the most common floating-point binary standard, IEEE 754\. I’ll end the
    chapter with a brief look at how floating-point numbers are processed in the A64
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '**Fractional Values in Binary**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s start by looking at the mathematics of fractional values. Recall from
    [Chapter 2](ch02.xhtml) that a decimal integer, *N*, is expressed in binary as
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/pg438_Image_291.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where each *d[i]* = `0` or `1`.
  prefs: []
  type: TYPE_NORMAL
- en: We can extend this to include a fractional part, *F*, such that
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/pg438_Image_292.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'where each *d[i]* = `0` or `1`. Note the *binary point* between *d*[0] and
    *d*[–1] on the right-hand side of this equation. All the terms to the right of
    the binary point are inverse powers of two, so this portion of the number sums
    to a fractional value. Like the decimal point on the left-hand side, the binary
    point separates the fractional part from the integral part of the number. Here’s
    an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/pg438_Image_293.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Although any integer can be represented as a sum of powers of two, an exact
    representation of fractional values in binary is limited to sums of *inverse*
    powers of two. For example, consider an 8-bit representation of the fractional
    value 0.9\. From the equalities
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/pg438_Image_294.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/pg438_Image_295.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In fact,
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/pg438_Image_296.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where ![Image](../images/pg438_Image_297.jpg) means this bit pattern repeats
    indefinitely.
  prefs: []
  type: TYPE_NORMAL
- en: To round a fractional value to the nearest value, check the bits to the right
    of the rounding place. If the next bit to the right is `0`, drop all the bits
    to the right of the bit position where you’re rounding. If the next bit to the
    right is `1` and any of the bits following it are `1`, add `1` to the bit position
    where you’re rounding.
  prefs: []
  type: TYPE_NORMAL
- en: If the next bit to the right is `1` and all the bits following it are `0`, use
    the *ties-to-even* rule. If the bit you’re rounding to is `0`, simply drop all
    the bits to the right of your rounding place. If the bit you’re rounding to is
    `1`, add `1` to it and drop all the bits to the right of your rounding place.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s round 0.9 to 8 bits. Earlier, you saw that the ninth bit to the right
    of the binary point is `0`, so we drop all the bits to the right of the eighth
    bit position. Thus, we use
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/pg439_Image_298.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'which gives a rounding error as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/pg439_Image_299.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The AArch64 architecture supports other floating-point rounding algorithms.
    These are discussed in the *Arm Architecture Reference Manual for A-Profile Architecture*,
    available at *[https://developer.arm.com/documentation/ddi0487/latest](https://developer.arm.com/documentation/ddi0487/latest)*.
  prefs: []
  type: TYPE_NORMAL
- en: We typically write numbers in decimal, with a decimal point in a fixed location
    in the number to separate the fractional part from the integer part. Let’s see
    how this works in binary.
  prefs: []
  type: TYPE_NORMAL
- en: '**Fixed-Point Numbers**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A *fixed-point number* is essentially a scaled integer representation in which
    the scaling is shown by the location of the *radix point*, which separates the
    fractional part of a number from the integral part. We call it the *decimal point*
    in decimal numbers and the *binary point* in binary numbers. English-speaking
    countries commonly use a period; other regions typically use a comma.
  prefs: []
  type: TYPE_NORMAL
- en: For example, 1,234.5[10] represents 12,345[10] scaled by 1/10, and the binary
    10011010010.1[2] is 100110100101[2] scaled by a factor of 1/2\. When performing
    computations with fixed-point numbers, you need to be mindful of the location
    of the radix point.
  prefs: []
  type: TYPE_NORMAL
- en: In the first part of this section, we’ll look at scaling numbers with a fractional
    part that is an inverse power of two, in which case the fractional part can be
    represented exactly. Then, we’ll look at scaling fractional numbers in decimal
    to avoid the rounding errors described earlier.
  prefs: []
  type: TYPE_NORMAL
- en: '***When the Fractional Part Is a Sum of Inverse Powers of Two***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: I’ll start with a program that adds two measurements that are specified to the
    nearest sixteenth. An example would be measuring a length in inches. The fractional
    parts of inches are often specified in inverse powers of two (1/2, 1/4, 1/8, and
    so forth), which can be represented exactly in the binary system.
  prefs: []
  type: TYPE_NORMAL
- en: Our program uses lengths to the nearest sixteenth, so we’ll multiply each value
    by 16 to give us an integral number of sixteenths. The program will first read
    the integer part of a length from the keyboard and then read the number of sixteenths.
    [Listing 19-1](ch19.xhtml#ch19list1) shows how we scale the integer part of the
    number and then add in the fractional part as they’re read from the keyboard.
  prefs: []
  type: TYPE_NORMAL
- en: '*get_length.s*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 19-1: A function to read a number in inches and sixteenths of an inch
    from the keyboard*'
  prefs: []
  type: TYPE_NORMAL
- en: We allocate 32 bits for both the number of inches and the number of sixteenths
    of an inch, each to be read as integers from the keyboard. Notice that we’re using
    the `get_uint` function to read each unsigned `int` ❶. You were asked to write
    this function in “Your Turn” [exercise 16.9](ch16.xhtml#ch16exe9) on [page 358](ch16.xhtml#ch16you2).
  prefs: []
  type: TYPE_NORMAL
- en: We shift the integral part 4 bits to the left to multiply it by 16 ❷. After
    adding the fractional part, we have the total number of sixteenths in our value.
    For example, 5 9/16 would be stored as the integer 5 × 16 + 9 = 89.
  prefs: []
  type: TYPE_NORMAL
- en: The scaling leaves 28 bits for the integral part. This limits the range of our
    numbers to be 0 to 268,435,455 15/16\. This is 16 times less than the 0 to 4,294,967,295
    range of a 32-bit unsigned integer, but the resolution is to the nearest 1/16.
  prefs: []
  type: TYPE_NORMAL
- en: Our function to display these measurements, shown in [Listing 19-2](ch19.xhtml#ch19list2),
    displays both the integral and fractional parts.
  prefs: []
  type: TYPE_NORMAL
- en: '*display_length.s*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 19-2: A function to display a number to the nearest sixteenth*'
  prefs: []
  type: TYPE_NORMAL
- en: We shift the number 4 bits to the right so we can display the integral part
    as an integer ❷. Using a 4-bit mask ❶, we mask off the integral part and display
    the fractional part as another integer ❸. We add some text to show that this second
    integer is the fractional part ❹.
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 19-3](ch19.xhtml#ch19list3) shows a `main` function that adds two
    numbers to the nearest sixteenth.'
  prefs: []
  type: TYPE_NORMAL
- en: '*add_lengths.s*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 19-3: A program to add two lengths to the nearest sixteenth*'
  prefs: []
  type: TYPE_NORMAL
- en: If you look at the equation for representing fractional values in binary on
    [page 410](ch19.xhtml#page_410), you can probably convince yourself that the integer
    `add` instruction will work for the entire number, including the fractional part
    ❶.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s think about how we’ve handled the fractional part in our fixed-point format
    here. When we read the integer part from the keyboard, we shifted it four bit
    positions to the left to multiply by 16\. This left room to add the number of
    sixteenths of the fractional part to this `int`. We effectively created a 32-bit
    number with the binary point between the fifth and fourth bits (bits 4 and 3).
    This works because the fractional part is a sum of inverse powers of two.
  prefs: []
  type: TYPE_NORMAL
- en: This example works nicely with binary numbers, but we mostly use decimal numbers
    in computations. As you saw earlier in this chapter, most fractional decimal numbers
    can’t be converted to a finite number of bits and need to be rounded. In the next
    section, I’ll discuss how to avoid rounding errors when representing fractional
    decimal numbers in binary.
  prefs: []
  type: TYPE_NORMAL
- en: '***When the Fractional Part Is in Decimal***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: I’ll use a program that adds two US dollar values to the nearest cent as an
    example of using fractional values in decimal. As with the measurement adding
    program in [Listings 19-1](ch19.xhtml#ch19list1) to [19-3](ch19.xhtml#ch19list3),
    we’ll start with the function to read money values from the keyboard, `get_money`,
    shown in [Listing 19-4](ch19.xhtml#ch19list4).
  prefs: []
  type: TYPE_NORMAL
- en: '*get_money.s*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 19-4: A function to read dollars and cents from the keyboard*'
  prefs: []
  type: TYPE_NORMAL
- en: Our money values are specified to the nearest cent here, so we multiply dollars—the
    integer part—by 100 ❶. Then we add cents—the fractional part—to give our scaled
    `int` ❷.
  prefs: []
  type: TYPE_NORMAL
- en: When storing decimal fractions, the integer and fractional parts are not separated
    into bit fields as in our previous example. For example, $1.10 would be stored
    as 110 = `0x0000006e` and $2.10 as 210 = `0x000000d2`. Because we use 32-bit signed
    integers in this program, the range of a money value is –$21,473,836.48 ≤ *money*_*amount*
    ≤ +$21,473,836.47.
  prefs: []
  type: TYPE_NORMAL
- en: Displaying dollars and cents will require a different algorithm from displaying
    lengths in sixteenths, as shown in [Listing 19-5](ch19.xhtml#ch19list5).
  prefs: []
  type: TYPE_NORMAL
- en: '*display_money.s*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 19-5: A function to display dollars and cents*'
  prefs: []
  type: TYPE_NORMAL
- en: Shifting won’t allow us to divide by 100, so we use the signed divide instruction,
    `sdiv`, to get the dollars ❶. The remainder from this division will be the number
    of cents.
  prefs: []
  type: TYPE_NORMAL
- en: Our computation of the remainder will have the same sign as the integer part.
    The negative sign will show when we display the dollars, but we don’t want to
    repeat it for the cents, so we negate the value for cents before displaying it
    ❷. We check whether the number of cents is less than 10, and if so, we make the
    first digit to the right of the decimal point a 0 ❸.
  prefs: []
  type: TYPE_NORMAL
- en: We see a new instruction here, `cneg:`
  prefs: []
  type: TYPE_NORMAL
- en: cneg**—Conditional negate**
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '`cneg w`d`, w`s`,`cond loads the negated value of the 32-bit value in `w`s
    into `w`d ifcond is true. If it’s not true, `w`s is loaded into `w`d.'
  prefs: []
  type: TYPE_NORMAL
- en: '`cneg x`d`, x`s`,`cond loads the negated value of the 64-bit value in `x`s
    into `x`d ifcond is true. If it’s not true, `x`s is loaded into `x`d.'
  prefs: []
  type: TYPE_NORMAL
- en: The possible conditions, cond, can be any of the condition flags listed in [Table
    13-1](ch13.xhtml#ch13tab1) on [page 245](ch13.xhtml#page_245) except for `al`
    and `nv`.
  prefs: []
  type: TYPE_NORMAL
- en: Our `main` function for this program, shown in [Listing 19-6](ch19.xhtml#ch19list6),
    will get two dollar amounts entered from the keyboard, add them, and display their
    sum.
  prefs: []
  type: TYPE_NORMAL
- en: '*add_money.s*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 19-6: A program to add two dollar values*'
  prefs: []
  type: TYPE_NORMAL
- en: Our scaling of the integer part has converted dollars to cents, so a simple
    `add` instruction computes the sum for us ❶. Our `display_money` function will
    sort out the dollars and cents in this sum.
  prefs: []
  type: TYPE_NORMAL
- en: This solution works well for many numbers, but we commonly use scientific notation
    for writing very large and very small numbers. In the next sections, you’ll see
    how the scientific notation has led to another way to store fractional values.
  prefs: []
  type: TYPE_NORMAL
- en: '**YOUR TURN**'
  prefs: []
  type: TYPE_NORMAL
- en: 19.1   Enter the program in [Listings 19-1](ch19.xhtml#ch19list1) to [19-3](ch19.xhtml#ch19list3).
    Using the `gdb` debugger, examine the numbers stored in the `w19` and `w20` registers
    in `main`. Identify the integral and fractional parts.
  prefs: []
  type: TYPE_NORMAL
- en: 19.2   Enter the program in [Listings 19-4](ch19.xhtml#ch19list4) to [19-6](ch19.xhtml#ch19list6).
    Using the `gdb` debugger, examine the numbers stored in the `w19` and `w20` registers
    in `main`. Identify the integral and fractional parts.
  prefs: []
  type: TYPE_NORMAL
- en: 19.3   Enter the program in [Listings 19-4](ch19.xhtml#ch19list4) to [19-6](ch19.xhtml#ch19list6).
    Run the program, using $21,474,836.47 for one amount and $0.01 for the other.
    What total does the program give? Why?
  prefs: []
  type: TYPE_NORMAL
- en: 19.4   Write a program in assembly language that allows a user to enter a start
    time and the amount of time a task takes, then computes the finish time. Use a
    24-hour clock with resolution to the nearest second.
  prefs: []
  type: TYPE_NORMAL
- en: '**Floating-Point Numbers**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Floating-point numbers* allow for a much larger range than fixed-point numbers.
    However, it’s important to understand that floating-point numbers are not *real
    numbers*. Real numbers include the continuum of all numbers from –*∞* to +*∞*.
    You already know that we have a finite number of bits to work with in a computer,
    so there is a limit on the largest values that can be represented. But the problem
    is worse than simply a limit on the magnitude.'
  prefs: []
  type: TYPE_NORMAL
- en: As you will see in this section, floating-point numbers comprise a small subset
    of real numbers. There are significant gaps between adjacent floating-point numbers.
    These gaps can produce several types of errors, as detailed in “Floating-Point
    Arithmetic Errors” on [page 425](ch19.xhtml#ch19lev1sec5). To make matters worse,
    these errors can occur in intermediate results, where they are difficult to debug.
  prefs: []
  type: TYPE_NORMAL
- en: '***Floating-Point Representation***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Floating-point representation is based on scientific notation. In floating-point
    representation, we have a sign and two numbers to completely specify a value:
    a *significand* and an *exponent*. A decimal floating-point number is written
    as a significand times 10 raised to an exponent. For example, consider these two
    numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/pg448_Image_300.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the floating-point representation, the number is *normalized* such that
    only one digit appears to the left of the decimal point and the exponent of 10
    is adjusted accordingly. If we agree that each number is normalized and that we
    are working in base 10, then each floating-point number is completely specified
    by three items: the significand, exponent, and sign. In the previous two examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/pg448_Image_301.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The advantage of using floating-point representation is that, for a given number
    of digits, we can represent a larger range of values.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at how floating-point numbers are stored in a computer.
  prefs: []
  type: TYPE_NORMAL
- en: '***IEEE 754 Floating-Point Standard***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The most commonly used standard for storing floating-point numbers is IEEE 754
    (*[https://standards.ieee.org/standard/754-2019.html](https://standards.ieee.org/standard/754-2019.html)*).
    [Figure 19-1](ch19.xhtml#ch19fig1) shows the general pattern.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/pg448_Image_302.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 19-1: The general pattern for storing IEEE 754 floating-point numbers*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The A64 architecture supports four variants of this format for storing floating-point
    numbers: two 16-bit, one 32-bit, and one 64-bit. Of these, the 16-bit half-precision,
    32-bit single-precision, and 64-bit double-precision formats follow the IEEE 754
    standard. The *BF16* format (also called *BFloat16*) is the same as the IEEE 754
    single-precision format but with a truncated significand. This reduces memory
    storage requirements while preserving the dynamic range of the 32-bit format,
    but at the expense of precision. This trade-off is useful in some machine learning
    algorithms. The A64 architecture includes instructions to operate on BF16 data,
    but we won’t use them in this book. These formats are shown in [Figure 19-2](ch19.xhtml#ch19fig2).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/pg449_Image_303.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 19-2: The formats for (a) half-precision, (b) BF16, (c) single-precision,
    and (d) double-precision floating point*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Values in the formats shown in [Figure 19-2](ch19.xhtml#ch19fig2) represent
    a floating-point number, *N*, stored in the normalized form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/pg449_Image_304.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The first bit, *s*, is the sign bit, `0` for positive and `1` for negative.
    As in decimal scientific notation, the exponent is adjusted such that there is
    only one nonzero digit to the left of the binary point. In binary, though, this
    digit is always 1, giving 1.*f* as the significand. Since it’s always 1, the integer
    part (1) is not stored. It’s called the *hidden bit*. Only the fractional part
    of the significand, *f*, is stored.
  prefs: []
  type: TYPE_NORMAL
- en: The formats need to allow for negative exponents. Your first thought might be
    to use two’s complement. However, the IEEE standard was developed in the 1970s,
    when floating-point computations took a lot of CPU time. Many algorithms in programs
    depend upon only the comparison of two numbers, and the computer scientists of
    the day realized that a format that allowed integer comparison instructions would
    result in faster execution times. So, they decided to add an amount, called a
    *bias*, to the exponent before storing it, such that the most negative allowable
    exponent would be stored as 0\. The result, a *biased exponent*, can then be stored
    as an `unsigned int`. In [Figure 19-2](ch19.xhtml#ch19fig2), the bias is 15 for
    the half-precision IEEE format, 127 for the single-precision IEEE and BF16 formats,
    and 1,023 for the double-precision IEEE format.
  prefs: []
  type: TYPE_NORMAL
- en: 'The hidden bit scheme presents a problem: there is no way to represent 0\.
    To address this and other issues, the IEEE 754 standard has several special cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Zero value**   All the biased exponent bits and fraction bits are `0`, allowing
    for both –0 and +0\. This preserves the sign of a computation that converges to
    0.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Denormalized**   If the value to be represented is smaller than can be represented
    with all the biased exponent bits being `0`, meaning that *e* has the most negative
    value possible, the hidden bit is no longer assumed. In this case, the amount
    of bias is reduced by 1.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Infinity**   Infinity is represented by setting all the biased exponent bits
    to `1` and all the fraction bits to `0`. This allows the sign bit to designate
    both +*∞* and –*∞*, allowing us to still compare numbers that are out of range.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Not a number (NaN)**   If the biased exponent bits are all `1` but the fraction
    bits are not all `0`, this represents a value that is in error. This might be
    used to indicate that a floating-point variable doesn’t yet have a value. NaN
    should be treated as a program error.'
  prefs: []
  type: TYPE_NORMAL
- en: An example of an operation that gives infinity is dividing a nonzero value by
    0\. An example that produces NaN is an operation that has an undefined result,
    such as dividing 0 by 0.
  prefs: []
  type: TYPE_NORMAL
- en: Next, I’ll discuss the A64 hardware used to work with floating-point numbers.
  prefs: []
  type: TYPE_NORMAL
- en: '**Floating-Point Hardware**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Table 9-1](ch09.xhtml#ch9tab1) in [Chapter 9](ch09.xhtml) shows that the A64
    architecture includes a register file that has 32 128-bit registers for floating-point
    or vector computations, the *SIMD&FP* registers.'
  prefs: []
  type: TYPE_NORMAL
- en: The A64 architecture includes vector instructions that can operate on multiple
    data items in an SIMD&FP register simultaneously. This is a computing method called
    *single-instruction multiple-data (SIMD)*. Data items for these instructions can
    range from 8 to 64 bits, so a register can hold 2 to 16 data items. There are
    vector instructions for both integer and floating-point operations.
  prefs: []
  type: TYPE_NORMAL
- en: A vector instruction operates on each data item in a SIMD&FP register independently
    from all the other data items in the register. These instructions are useful for
    algorithms that do things like process arrays. One vector instruction can operate
    on several array elements in parallel, resulting in considerable speed gains.
    Such algorithms are common in multimedia and scientific applications.
  prefs: []
  type: TYPE_NORMAL
- en: The A64 architecture also includes scalar floating-point instructions that operate
    on a single floating-point data item in the low-order bits of the SIMD&FP registers.
  prefs: []
  type: TYPE_NORMAL
- en: Programming with SIMD instructions is beyond the scope of this book; we’ll consider
    only scalar floating-point computations here. [Figure 19-3](ch19.xhtml#ch19fig3)
    shows the names of the portions of the SIMD&FP registers used for the scalar floating-point
    instructions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/pg450_Image_305.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 19-3: The A64 floating-point register names*'
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 19-7](ch19.xhtml#ch19list7) shows how we can use these registers to
    perform floating-point arithmetic.'
  prefs: []
  type: TYPE_NORMAL
- en: '*add_floats.s*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 19-7: A program to add two floating-point numbers*'
  prefs: []
  type: TYPE_NORMAL
- en: We use the `scanf` function from the C standard library to read a floating-point
    number from the keyboard ❶. This will store the number in memory in the 32-bit
    IEEE 754 format. Thus, we don’t need a special instruction to load the number
    into a floating-point register; we can simply use an `ldr` instruction ❷.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to use the floating-point add instruction, `fadd`, to sum the numbers
    ❸. I won’t list all the floating-point instructions for performing arithmetic,
    but here are the four basic ones:'
  prefs: []
  type: TYPE_NORMAL
- en: fadd—Floating-point add (scalar)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '`fadd h`d`, h`s1`, h`s2 adds the half-precision floating-point numbers in `h`s1
    and `h`s2 and stores the result in `h`d.'
  prefs: []
  type: TYPE_NORMAL
- en: '`fadd s`d`, s`s1`, s`s2 adds the single-precision floating-point numbers in
    `s`s1 and `s`s2 and stores the result in `s`d.'
  prefs: []
  type: TYPE_NORMAL
- en: '`fadd d`d`, d`s1`, d`s2 adds the double-precision floating-point numbers in
    `d`s1 and `d`s2 and stores the result in `d`d.'
  prefs: []
  type: TYPE_NORMAL
- en: fsub—Floating-point subtract (scalar)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '`fsub h`d`, h`s1`, h`s2 subtracts the half-precision floating-point number
    in `h`s2 from the one in `h`s1 and stores the result in `h`d.'
  prefs: []
  type: TYPE_NORMAL
- en: '`fsub s`d`, s`s1`, s`s2 subtracts the single-precision floating-point number
    in `s`s2 from the one in `s`s1 and stores the result in `s`d.'
  prefs: []
  type: TYPE_NORMAL
- en: '`fsub d`d`, d`s1`, d`s2 subtracts the double-precision floating-point number
    in `d`s2 from the one in `d`s1 and stores the result in `d`d.'
  prefs: []
  type: TYPE_NORMAL
- en: fmul—Floating-point multiply (scalar)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '`fmul h`d`, h`s1`, h`s2 multiplies the half-precision floating-point numbers
    in `h`s1 and `h`s2 and stores the result in `h`d.'
  prefs: []
  type: TYPE_NORMAL
- en: '`fmul s`d`, s`s1`, s`s2 multiplies the single-precision floating-point numbers
    in `s`s1 and `s`s2 and stores the result in `s`d.'
  prefs: []
  type: TYPE_NORMAL
- en: '`fmul d`d`, d`s1`, d`s2 multiplies the double-precision floating-point numbers
    in `d`s1 and `d`s2 and stores the result in `d`d.'
  prefs: []
  type: TYPE_NORMAL
- en: fdiv—Floating-point divide (scalar)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '`fdiv h`d`, h`s1`, h`s2 divides the half-precision floating-point number in
    `h`s1 by the one in `h`s2 and stores the result in `h`d.'
  prefs: []
  type: TYPE_NORMAL
- en: '`fdiv s`d`, s`s1`, s`s2 divides the single-precision floating-point number
    in `s`s1 by the one in `s`s2 and stores the result in `s`d.'
  prefs: []
  type: TYPE_NORMAL
- en: '`fdiv d`d`, d`s1`, d`s2 divides the double-precision floating-point number
    in `d`s1 by the one in `d`s2 and stores the result in `d`d.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `printf` function requires that floating-point numbers be passed as `double`s,
    so we use the `fcvt` instruction to convert our `float` values to `double`s. The
    `fcvt` instruction converts from the floating-point format of the source register
    to the floating-point format of the destination register:'
  prefs: []
  type: TYPE_NORMAL
- en: fcvt—Floating-point convert precision (scalar)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '`fcvt s`d`, h`s converts half-precision in `h`s to single-precision in `s`d.'
  prefs: []
  type: TYPE_NORMAL
- en: '`fcvt d`d`, h`s converts half-precision in `h`s to double-precision in `d`d.'
  prefs: []
  type: TYPE_NORMAL
- en: '`fcvt h`d`, s`s converts single-precision in `s`s to half-precision in `h`d.'
  prefs: []
  type: TYPE_NORMAL
- en: '`fcvt d`d`, s`s converts single-precision in `s`s to double-precision in `d`d.'
  prefs: []
  type: TYPE_NORMAL
- en: '`fcvt h`d`, d`s converts double-precision in `d`s to half-precision in `h`d.'
  prefs: []
  type: TYPE_NORMAL
- en: '`fcvt s`d`, d`s converts double-precision in `d`s to single-precision in `s`d.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Although we’re not using comparisons in this program, here’s an example of
    a floating-point compare instruction:'
  prefs: []
  type: TYPE_NORMAL
- en: fcmp—Floating-point compare (scalar)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '`fcmp h`s1`, h`s2 compares the half-precision floating-point number in `h`s1
    with `h`s2 and sets the condition flags in the `nzcv` register accordingly.'
  prefs: []
  type: TYPE_NORMAL
- en: '`fcmp h`s`, 0.0` compares the half-precision floating-point number in `h`s
    with 0.0 and sets the condition flags in the `nzcv` register accordingly.'
  prefs: []
  type: TYPE_NORMAL
- en: '`fcmp s`s1`, s`s2 compares the single-precision floating-point number in `s`s1
    with `s`s2 and sets the condition flags in the `nzcv` register accordingly.'
  prefs: []
  type: TYPE_NORMAL
- en: '`fcmp s`s`, 0.0` compares the single-precision floating-point number in `s`s
    with 0.0 and sets the condition flags in the `nzcv` register accordingly.'
  prefs: []
  type: TYPE_NORMAL
- en: '`fcmp d`s1`, d`s2 compares the double-precision floating-point number in `d`s1
    with `d`s2 and sets the condition flags in the `nzcv` register accordingly.'
  prefs: []
  type: TYPE_NORMAL
- en: '`fcmp d`s`, 0.0` compares the double-precision floating-point number in `d`s
    with 0.0 and sets the condition flags in the `nzcv` register accordingly.'
  prefs: []
  type: TYPE_NORMAL
- en: Since the `fcmp` instruction sets the condition flags in the `nzcv` register,
    we can use the conditional branch instruction described in [Chapter 13](ch13.xhtml)
    with the conditions in [Table 13-1](ch13.xhtml#ch13tab1) to control program flow
    based on floating-point values.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlier, floating-point computations can lead to some subtle numerical
    errors in our programs. I’ll cover these in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: '**Floating-Point Arithmetic Errors**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It’s easy to think of floating-point numbers as real numbers, but they’re not.
    Most floating-point numbers are rounded approximations of the real numbers they
    represent. When using floating-point arithmetic, you need to be aware of the effects
    of rounding on your computations. If you don’t pay close attention to the rounding
    effects, you might not notice the errors that can creep into your computations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most of the arithmetic errors I’ll discuss here are also possible with fixed-point
    arithmetic. Probably the most common arithmetic error is *rounding error*, which
    can occur for two reasons: either the number of bits available for storage is
    limited or the fractional values cannot be precisely represented in all number
    bases.'
  prefs: []
  type: TYPE_NORMAL
- en: Both of these limitations also apply to fixed-point representation. As you saw
    earlier in this chapter, you can often scale fixed-point numbers to eliminate
    the problem with fractional values—but then the number of bits available for storage
    limits the range of the values.
  prefs: []
  type: TYPE_NORMAL
- en: Floating-point representation reduces the range problem by using an exponent
    to specify where the integer part begins. However, the significand of a floating-point
    number is a fraction, which means that most floating-point numbers do not have
    an exact representation in binary, leading to rounding errors.
  prefs: []
  type: TYPE_NORMAL
- en: One of the biggest problems with floating-point arithmetic is that the CPU instructions
    can shift the significand of a number, adjusting the exponent accordingly and
    causing bits to be lost and more rounding errors. With integer arithmetic, any
    shifting of bits is explicit in the program.
  prefs: []
  type: TYPE_NORMAL
- en: 'When computing with integers, you need to be aware of errors in the most significant
    places of the results: carry for unsigned integers and overflow for signed integers.
    With floating-point numbers, the radix point is adjusted to maintain the integrity
    of the most significant places. Most errors in floating-point arithmetic are the
    result of rounding in the low-order places that is needed to fit the value within
    the allocated number of bits. The errors in floating-point arithmetic are more
    subtle, but they can have important effects on the accuracy of our programs.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at the different types of errors that can arise in floating-point
    computations.
  prefs: []
  type: TYPE_NORMAL
- en: '***Rounding Error***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You saw at the beginning of this chapter that most decimal fractional values
    do not have exact equivalents in binary, leading to a rounded-off approximation
    being stored in memory. Running the `add_floats` program from [Listing 19-7](ch19.xhtml#ch19list7)
    illustrates this problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The numbers the program is using are not the ones I entered, and the `fadd`
    instruction didn’t add the program’s numbers correctly. Before you go back to
    look for the bugs in [Listing 19-7](ch19.xhtml#ch19list7), let’s bring in the
    debugger to see if we can figure out what’s happening:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'I set a breakpoint at the call to `printf` and then ran the program, entering
    the same numbers as earlier. Let’s look at the three values that are passed to
    `printf`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This display can be a bit confusing. For each floating-pointing register, the
    values in the first set of brackets are in hexadecimal. The first value (`f =`)
    shows the integer part of the number in hexadecimal. For example, the integer
    part of the value in `d0` is `0x7b` = 123[10], which is the integer part of the
    number I entered. The next two values (`u =` and `s =`) show the bit pattern of
    the entire number as it’s stored. We can use this bit pattern with the format
    in [Figure 19-2](ch19.xhtml#ch19fig2)(d) to figure out the floating-point number.
  prefs: []
  type: TYPE_NORMAL
- en: The values in the second set of brackets show the number in floating point (`f
    =`), as though the bits were interpreted as an unsigned integer (`u =`) and as
    a signed integer (`s =`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Don’t worry if you’re still confused by this display. I also find it a bit
    confusing. The important part is where the display shows the floating-point number
    that is actually stored in each register: 123.40000152587891 in `d0`, 567.79998779296875
    in `d1`, and 691.19998931884766 in `d2`. The `printf` function rounded each of
    these numbers to six decimal places when I ran the program. These values reflect
    the fact that most decimal fractional values do not have an exact binary equivalence.'
  prefs: []
  type: TYPE_NORMAL
- en: '***Absorption***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Absorption* results from adding or subtracting two numbers of widely different
    magnitude. The value of the smaller number gets lost in the computation. Let’s
    run our `add_floats` program under `gdb` to see how this occurs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'From the `gdb` display, we see that the values in the registers are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The CPU aligns the binary points of the numbers before performing the addition.
    The pattern in [Figure 19-2](ch19.xhtml#ch19fig2)(c) shows that the `fadd` instruction
    performed the following addition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/pg456_Image_306.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Since the significand in single-precision floating point is 24 bits (one is
    the hidden bit), the number in `s2` is rounded to `111111111111111111111111`,
    thus losing everything to the right of the binary point. The number in `s1` was
    absorbed by the much larger number in `s0`.
  prefs: []
  type: TYPE_NORMAL
- en: '***Cancellation***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Cancellation* can occur when subtracting two numbers that differ by a small
    amount. Since floating-point notation preserves the integrity of the high-order
    portions, the subtraction will give 0 in the high-order portion of the result.
    If either of the numbers has been rounded, its low-order portion is not exact,
    which means the result will be in error.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate, we can use our `add_floats` program to subtract by entering
    a negative number. Here’s an example using two close numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'From the `gdb` display, we see that the values in the registers are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The relative error in this subtraction is (0.125 – 0.1) / 0.1 = 0.25 = 25%.
    The second number has been rounded from –1,677,721.4 to –1,677,721.375, which
    led to the error in the arithmetic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at how these numbers are treated as `float`s:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/pg457_Image_307.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Subtraction has caused the high-order 20 bits in `s0` and `s1` to cancel, leaving
    only three bits of significance for `s2`. The rounding error in `s1` carries through
    to cause an error in `s2`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use two values that will not give a rounding error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, the three numbers are stored exactly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/pg457_Image_308.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The subtraction has still caused the high-order 20 bits of `s0` and `s1` to
    cancel and left only three bits of significance for `s3`, but `s3` is correct.
  prefs: []
  type: TYPE_NORMAL
- en: '*Catastrophic cancellation* occurs when at least one of the floating-point
    numbers has a rounding error that causes an error in the difference. If both numbers
    are stored exactly, we get *benign cancellation*. Both types of cancellation cause
    a loss of significance in the result.'
  prefs: []
  type: TYPE_NORMAL
- en: '***Associativity***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Probably the most insidious effects of floating-point errors are those that
    cause errors in intermediate results. They can show up in some sets of data but
    not in others. Errors in intermediate results can even cause floating-point addition
    not to be associative—that is, there are some values of the `float`s `x`, `y`,
    and `z` for which `(x + y) + z` is not equal to `x + (y + z)`.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s write a simple C program to test for associativity, as shown in [Listing
    19-8](ch19.xhtml#ch19list8).
  prefs: []
  type: TYPE_NORMAL
- en: '*three_floats.c*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 19-8: A program to show that floating-point arithmetic is not associative*'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll start with some simple numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'It appears that our program has a bug. Let’s use `gdb` to see if we can figure
    out what’s going on here. I set a breakpoint at the `sum1 += z` statement so we
    can view the contents of the five variables in this program, then I ran the program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s determine the addresses of the variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The variables are stored in five consecutive 32-bit words beginning with `z`
    at `0x7fffffef8c`. Let’s look at these five values, both in floating-point format
    and in hexadecimal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll work with the values in hexadecimal to determine what’s going on here.
    Using the IEEE 754 format for single-precision floating point in [Figure 19-2(c)](ch19.xhtml#ch19fig2),
    we get the following addition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/pg459_Image_309.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Since the format allows only 23 bits for the significand, the CPU will round
    off the sum to give the following number (remember the ties-to-even rule discussed
    on [page 410](ch19.xhtml#page_410)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/pg460_Image_310.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This is the number we saw stored in IEEE 754 format at the address of `sum1`
    (`0x7fffffef9c`) in our `gdb` display earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we’ll execute the current instruction, which adds `z` to `sum1`, and look
    at its new value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The CPU has performed the following addition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/pg460_Image_311.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The CPU then rounds off `sum1` to give a 23-bit significand:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/pg460_Image_312.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we’ll go though the same steps to compute `sum2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The numbers here are the result of the following addition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/pg460_Image_313.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Rounding off gives:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/pg460_Image_314.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The current statement adds `x`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This performs the addition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/pg461_Image_315.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The CPU then rounds off `sum2` to give a 23-bit significand (again, remember
    the ties-to-even rule):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/pg461_Image_316.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Continuing the program to the end gives:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The `printf` function has rounded off the display of `sum1` and `sum2` so they
    look equal, but looking inside the program with `gdb` shows that they are not
    equal. We conclude that the bug in our program is not in our logic but in our
    use of floating-point variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'The difference between the two orders of adding the three `float`s is very
    small:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/pg461_Image_317.jpg)'
  prefs: []
  type: TYPE_IMG
- en: However, this small difference could become significant if this is part of a
    computation that involves multiplying by large numbers.
  prefs: []
  type: TYPE_NORMAL
- en: The main lesson to learn from this example is that floating-point arithmetic
    is seldom precise.
  prefs: []
  type: TYPE_NORMAL
- en: '**YOUR TURN**'
  prefs: []
  type: TYPE_NORMAL
- en: 19.5   Modify the C program in [Listing 19-8](ch19.xhtml#ch19list8) to use `double`s.
    Does this make floating-point addition associative?
  prefs: []
  type: TYPE_NORMAL
- en: '**Comments About Numerical Accuracy**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Novice programmers often see floating-point numbers as real numbers and thus
    think they are more accurate than integers. It’s true that using integers carries
    its own set of problems: even adding two large integers can cause overflow. Multiplying
    integers is even more likely to produce a result that will overflow. And you need
    to take into account that integer division results in two values, the quotient
    and the remainder, instead of the one value that floating-point division gives
    us.'
  prefs: []
  type: TYPE_NORMAL
- en: 'But floating-point numbers are not real numbers. As you’ve seen in this chapter,
    floating-point representations extend the range of numerical values but have their
    own set of potential inaccuracies. Arithmetically accurate results require a thorough
    analysis of your algorithm. Here are some ideas to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: Try to scale the data such that integer arithmetic can be used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use `double`s instead of `float`s. This improves accuracy and may actually increase
    the speed of execution. Most C and C++ library routines take `double`s as arguments,
    so the compiler converts `float`s to `double`s when passing them as arguments,
    as in the call to `printf` in [Listing 19-7](ch19.xhtml#ch19list7).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try to arrange the order of computations so that similarly sized numbers are
    added or subtracted.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoid complex arithmetic statements that may obscure incorrect intermediate
    results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choose test data that stresses your algorithm. If your program processes fractional
    values, include data that does not have an exact binary equivalent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The good news is that with today’s 64-bit computers, the range of integers is
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/pg461_Image_317a.jpg)'
  prefs: []
  type: TYPE_IMG
- en: and there are libraries available in many programming languages that allow us
    to use arbitrary-precision arithmetic in our programs. You can find a list of
    these libraries at *[https://en.wikipedia.org/wiki/List_of_arbitrary-precision_arithmetic_software](https://en.wikipedia.org/wiki/List_of_arbitrary-precision_arithmetic_software)*.
  prefs: []
  type: TYPE_NORMAL
- en: This section has provided an overview of the primary causes of numerical errors
    when using floating-point numbers. For a more rigorous treatment of the topic,
    David Goldberg’s paper “What Every Computer Scientist Should Know About Floating-Point
    Arithmetic” (*ACM Computing Surveys*, Vol. 23, No. 1, March 1991) and *[https://en.wikipedia.org/wiki/Floating-point_arithmetic](https://en.wikipedia.org/wiki/Floating-point_arithmetic)*
    are good starting points. For an example of a programming technique to reduce
    rounding errors, you can read about the Kahan summation algorithm at *[https://en.wikipedia.org/wiki/Kahan_summation_algorithm](https://en.wikipedia.org/wiki/Kahan_summation_algorithm)*.
  prefs: []
  type: TYPE_NORMAL
- en: '**What You’ve Learned**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Binary representation of fractional values**   Fractional values in binary
    are equal to sums of inverse powers of two.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fixed point in binary**   The binary point is assumed to be in a specific
    position in the binary representation of the number.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Floating-point numbers are not real numbers**   The gap between adjacent
    floating-point numbers varies according to the exponent.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Floating-point is usually less accurate than fixed-point**   Rounding errors
    are commonly obscured by floating-point format normalization and can accumulate
    through multiple computations.'
  prefs: []
  type: TYPE_NORMAL
- en: '**IEEE 754**   The most common standard for representing floating-point values
    in a computer program. The integer part is always 1\. The exponent specifies the
    number of bits included in, or excluded from, the integer part.'
  prefs: []
  type: TYPE_NORMAL
- en: '**SIMD and floating-point hardware**   Floating-point instructions use a separate
    register file in the CPU.'
  prefs: []
  type: TYPE_NORMAL
- en: So far in this book, I have discussed programs that follow a step-by-step order
    of execution of instructions. But in some instances, an instruction cannot do
    anything meaningful with its operands—for example, when we divide by 0\. As you
    saw earlier in this chapter, this can trigger an exception to the intended order
    of program execution. We may also want to allow outside events, such as using
    the keyboard, to interrupt the ongoing program execution. After discussing input/output
    in [Chapter 20](ch20.xhtml), I’ll cover interrupts and exceptions in [Chapter
    21](ch21.xhtml).
  prefs: []
  type: TYPE_NORMAL
