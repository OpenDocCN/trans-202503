<html><head></head><body>
<span epub:type="pagebreak" id="page_117"/>&#13;
<h2 class="h2"><strong><span class="big">7</span><br/>COMPUTER HARDWARE</strong></h2>&#13;
<div class="image1"><img src="../images/common.jpg" alt="Image"/></div>&#13;
<p class="noindents">The preceding chapters covered the foundational elements of computing—binary, digital circuits, memory. Let’s now examine how these elements come together in a computer, a device that’s more than the sum of its parts. In this chapter, I first provide an overview of computer hardware. Then we dive deeper into three parts of a computer: main memory, the processor, and input/output.</p>&#13;
<h3 class="h3" id="lev1_44"><strong>Computer Hardware Overview</strong></h3>&#13;
<p class="noindent">Let’s begin with an overview of what makes a computer different from other electronic devices. Previously, we’ve seen how we can use logic circuits and memory devices to build circuits that perform useful tasks. The circuits we’ve built with logic gates have a set of features hard-wired into their design. If we want to add or modify a feature, we have to change the <span epub:type="pagebreak" id="page_118"/>physical design of our circuit. On a breadboard that’s possible, but for a device that has been manufactured and sent to customers, changing hardware isn’t usually an option. Defining the features of a device in hardware alone limits our ability to quickly innovate and improve a design.</p>&#13;
<p class="indent">The circuits we’ve built so far give us a glimpse into how computers work, but we’re missing one critical element of computers: <em>programmability</em>. That is, a computer must be able to perform new tasks <em>without</em> changing hardware. To accomplish such a feat, a computer must be able to accept a set of instructions (a <em>program</em>) and perform the actions specified in those instructions. It must therefore have hardware that can perform a variety of operations in the order specified in a program. Programmability is a key differentiator between a device that is a computer and one that is not. In this chapter we cover computer <em>hardware</em>, the physical elements of a computer. This is in contrast to <em>software</em>, the instructions that tell a computer what to do, which we’ll cover in the next chapter.</p>&#13;
<p class="indent">The ability to run software distinguishes a computer from a fixed-purpose device. That said, software still needs hardware, so what kind of hardware do we need to implement a general-purpose computer? First, we need memory. We’ve already covered single-bit memory devices such as latches and flip-flops; the type of memory used in a computer is a conceptual extension of those simple memory devices. The primary memory used in a computer is known as <em>main memory</em>, but often it’s referred to as just memory or <em>random access memory (RAM)</em>. It’s <em>volatile</em>, meaning it only retains data while powered. The “random access” part of RAM means that any arbitrary memory location can be accessed in roughly the same amount of time as any other location.</p>&#13;
<p class="indent">The second key component we need is a <em>central processing unit</em>, or <em>CPU</em>. Often simply called a <em>processor</em>, this component carries out the instructions specified in software. The CPU can directly access main memory. Most processors today are <em>microprocessors</em>, CPUs on a single integrated circuit. A processor built on a single integrated circuit has the benefits of lower cost, improved reliability, and increased performance. A CPU is a conceptual extension of the digital logic circuits we covered previously.</p>&#13;
<p class="indent">Although main memory and a CPU are the minimum hardware requirements for a computer, in practice most computing devices need to interact with the outside world, and they do so through input/output (I/O) devices. In this chapter, we cover main memory, the CPU, and I/O in more detail. These three elements are illustrated in <a href="ch07.xhtml#ch7fig1">Figure 7-1</a>.</p>&#13;
<div class="image" id="ch7fig1"><img src="../images/fig7-1.jpg" alt="image"/></div>&#13;
<p class="figcap"><em>Figure 7-1: The hardware elements of a computer</em></p>&#13;
<h3 class="h3" id="lev1_45"><strong><span epub:type="pagebreak" id="page_119"/>Main Memory</strong></h3>&#13;
<p class="noindent">While executing a program, a computer needs a place to store the program’s instructions and related data. For example, when a computer runs a word processor for editing documents, the computer needs a place to hold the program itself, the contents of the document, and the state of editing—what part of the document is visible, the location of the cursor, and so forth. All of this data is ultimately a series of bits that the CPU needs to be able to access. Main memory handles the task of storing these 1s and 0s.</p>&#13;
<p class="indent">Let’s explore how main memory works in a computer. There are two common types of computer memory: <em>static random access memory (SRAM)</em> and <em>dynamic random access memory (DRAM)</em>. In both types, the basic unit of memory storage is a <em>memory cell</em>, a circuit that can store a single bit. In SRAM, memory cells are a type of flip-flop. SRAM is static because its flip-flop memory cells retain their bit values while power is applied. On the other hand, DRAM memory cells are implemented using a transistor and a capacitor. The capacitor’s charge leaks over time, so data must be periodically rewritten to the cells. This refreshing of the memory cells is what makes DRAM dynamic. Today, DRAM is commonly used for main memory due to its relatively low price. SRAM is faster but more expensive, so it’s used in scenarios where speed is critical, such as in cache memory, which we’ll cover later. An example “stick” of RAM is shown in <a href="ch07.xhtml#ch7fig2">Figure 7-2</a>.</p>&#13;
<div class="image" id="ch7fig2"><img src="../images/fig7-2.jpg" alt="image"/></div>&#13;
<p class="figcap"><em>Figure 7-2: Random access memory</em></p>&#13;
<p class="indent">As a generalization, you can think of the internals of RAM as grids of memory cells. Each single-bit cell in a grid can be identified using two-dimensional coordinates, the location of that cell in its grid. Accessing a single bit at a time isn’t very efficient, so RAM accesses multiple grids of 1-bit memory cells in parallel, allowing for reads or writes of multiple bits at once—a whole byte, for example. The location of a set of bits in memory is known as a <em>memory address</em>, a numeric value that identifies a memory location. It’s common for memory to be <em>byte-addressable</em>, meaning a single memory address refers to 8 bits of data. The internal details of the arrangement of memory or the implementation of the memory cells aren’t required knowledge for a CPU (or a programmer!). The main thing to understand is that computers assign numeric addresses to bytes of memory, and the CPU can read or write to those addresses, as illustrated in <a href="ch07.xhtml#ch7fig3">Figure 7-3</a>.</p>&#13;
<span epub:type="pagebreak" id="page_120"/>&#13;
<div class="image" id="ch7fig3"><img src="../images/fig7-3.jpg" alt="image"/></div>&#13;
<p class="figcap"><em>Figure 7-3: A CPU reads a byte from a memory address.</em></p>&#13;
<p class="indent">Let’s consider a fictitious computer system that can address up to 64KB of memory. By today’s standards, that’s a tiny amount of memory for a computer, but it’s still useful for us as an example. Let’s also imagine that our fictitious computer’s memory is byte-addressable; each memory address represents a single byte. That means that we need one unique address for each byte of memory, and since 64KB is 64 × 1024 = 65,536 bytes, we need 65,536 unique addresses. Each address is just a number, and memory addresses usually start with 0, so our range of addresses is 0 to 65,535 (or 0xFFFF).</p>&#13;
<p class="indent">Since our fictitious 64KB computer is a digital device, memory addresses are ultimately represented in binary. How many bits do we need to represent a memory address on this system? The number of unique values that can be represented by a binary number with <em>n</em> bits is equal to 2<sup><em>n</em></sup>. So we want to know the value of <em>n</em> for 2<sup><em>n</em></sup> = 65,536. The inverse of raising 2 to a certain power is the base-2 logarithm. Therefore log<sub>2</sub>(2<sup><em>n</em></sup>) = <em>n</em> and log<sub>2</sub>(65,536) = 16. Stated another way, 2<sup>16</sup> = 65,536. Therefore, a 16-bit memory address is needed to address 65,536 bytes.</p>&#13;
<p class="indent">Or, more simply, since we already know that our highest numbered memory address is 0xFFFF, and we know that each hexadecimal symbol represents 4 bits, we can see that 16 bits are required (4 hex symbols × 4 bits per symbol). Again, our fictitious computer is able to address 65,536 bytes, and each byte is assigned a 16-bit memory address. <a href="ch07.xhtml#ch7tab1">Table 7-1</a> shows a 16-bit memory layout with some example data.</p>&#13;
<p class="tabcap" id="ch7tab1"><strong>Table 7-1:</strong> A 16-Bit Memory Address Layout, Skipping Middle Addresses, with Example Data</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:30%"/>&#13;
<col style="width:30%"/>&#13;
<col style="width:40%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr>&#13;
<th style="vertical-align: top;" class="borderb"><p class="tab"><strong>Memory address (as binary)</strong></p></th>&#13;
<th style="vertical-align: top;" class="borderb"><p class="tab"><strong>Memory address (as hex)</strong></p></th>&#13;
<th style="vertical-align: top;" class="borderb"><p class="tab"><strong>Example data</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="bg-g"><p class="tab"><code>0000000000000000</code></p></td>&#13;
<td style="vertical-align: top;" class="bg-g"><p class="tab"><code>0000</code></p></td>&#13;
<td style="vertical-align: top;" class="bg-g"><p class="tab"><code>23</code></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;"><p class="tab"><code>0000000000000001</code></p></td>&#13;
<td style="vertical-align: top;"><p class="tab"><code>0001</code></p></td>&#13;
<td style="vertical-align: top;"><p class="tab"><code>51</code></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="bg-g"><p class="tab"><code>0000000000000010</code></p></td>&#13;
<td style="vertical-align: top;" class="bg-g"><p class="tab"><code>0002</code></p></td>&#13;
<td style="vertical-align: top;" class="bg-g"><p class="tab"><code>4A</code></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;"><p class="tab"><code><span epub:type="pagebreak" id="page_121"/>----------------</code></p></td>&#13;
<td style="vertical-align: top;"><p class="tab"><code>----</code></p></td>&#13;
<td style="vertical-align: top;"><p class="tab"><code>--</code></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="bg-g"><p class="tab"><code>1111111111111101</code></p></td>&#13;
<td style="vertical-align: top;" class="bg-g"><p class="tab"><code>FFFD</code></p></td>&#13;
<td style="vertical-align: top;" class="bg-g"><p class="tab"><code>03</code></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;"><p class="tab"><code>1111111111111110</code></p></td>&#13;
<td style="vertical-align: top;"><p class="tab"><code>FFFE</code></p></td>&#13;
<td style="vertical-align: top;"><p class="tab"><code>94</code></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="bg-g"><p class="tab"><code>1111111111111111</code></p></td>&#13;
<td style="vertical-align: top;" class="bg-g"><p class="tab"><code>FFFF</code></p></td>&#13;
<td style="vertical-align: top;" class="bg-g"><p class="tab"><code>82</code></p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">Why does the number of bits matter? The number of bits used to represent a memory address is a key part of a computer system’s design. It limits the amount of memory that a computer can access, and it impacts how programs deal with memory at a low level.</p>&#13;
<p class="indent">Let’s now imagine that our fictitious computer has stored the ASCII string “Hello” starting at memory address 0x0002. Since ASCII characters each require 1 byte, storing “Hello” requires 5 bytes. When examining memory, it’s common to use hexadecimal to represent both memory addresses and the contents of those memory addresses. <a href="ch07.xhtml#ch7tab2">Table 7-2</a> provides a visual look at “Hello” stored in memory, starting at address 0x0002.</p>&#13;
<p class="tabcap" id="ch7tab2"><strong>Table 7-2:</strong> “Hello” Stored in Memory</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:30%"/>&#13;
<col style="width:30%"/>&#13;
<col style="width:40%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr>&#13;
<th style="vertical-align: top;" class="borderb"><p class="tab"><strong>Memory address</strong></p></th>&#13;
<th style="vertical-align: top;" class="borderb"><p class="tab"><strong>Data byte</strong></p></th>&#13;
<th style="vertical-align: top;" class="borderb"><p class="tab"><strong>Data as ASCII</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="bg-g"><p class="tab"><code>0000</code></p></td>&#13;
<td style="vertical-align: top;" class="bg-g"><p class="tab"><code>00</code></p></td>&#13;
<td style="vertical-align: top;" class="bg-g"/>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;"><p class="tab"><code>0001</code></p></td>&#13;
<td style="vertical-align: top;"><p class="tab"><code>00</code></p></td>&#13;
<td style="vertical-align: top;"/>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="bg-g"><p class="tab"><code>0002</code></p></td>&#13;
<td style="vertical-align: top;" class="bg-g"><p class="tab"><code>48</code></p></td>&#13;
<td style="vertical-align: top;" class="bg-g"><p class="tab"><code>H</code></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;"><p class="tab"><code>0003</code></p></td>&#13;
<td style="vertical-align: top;"><p class="tab"><code>65</code></p></td>&#13;
<td style="vertical-align: top;"><p class="tab"><code>e</code></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="bg-g"><p class="tab"><code>0004</code></p></td>&#13;
<td style="vertical-align: top;" class="bg-g"><p class="tab"><code>6C</code></p></td>&#13;
<td style="vertical-align: top;" class="bg-g"><p class="tab"><code>l</code></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;"><p class="tab"><code>0005</code></p></td>&#13;
<td style="vertical-align: top;"><p class="tab"><code>6C</code></p></td>&#13;
<td style="vertical-align: top;"><p class="tab"><code>l</code></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="bg-g"><p class="tab"><code>0006</code></p></td>&#13;
<td style="vertical-align: top;" class="bg-g"><p class="tab"><code>6F</code></p></td>&#13;
<td style="vertical-align: top;" class="bg-g"><p class="tab"><code>o</code></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;"><p class="tab"><code>0007</code></p></td>&#13;
<td style="vertical-align: top;"><p class="tab"><code>00</code></p></td>&#13;
<td style="vertical-align: top;"/>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="bg-g"><p class="tab"><code>----</code></p></td>&#13;
<td style="vertical-align: top;" class="bg-g"><p class="tab"><code>--</code></p></td>&#13;
<td style="vertical-align: top;" class="bg-g"/>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;"><p class="tab"><code>FFFF</code></p></td>&#13;
<td style="vertical-align: top;"><p class="tab"><code>00</code></p></td>&#13;
<td style="vertical-align: top;"/>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">Using this format makes it clear that each address only stores 1 byte, so storing all 5 ASCII characters requires addresses 0x0002 through 0x0006. Note that the table shows a value of 00 for other memory addresses, but in practice, it isn’t safe to assume that a random address will hold 0; it could be anything. That said, in some programming languages it’s standard practice to end a text string with a null terminator (a byte equal to 0), and in that case, we’d actually expect to see 00 at address 0x0007.</p>&#13;
<p class="indent">Applications that allow inspection of computer memory commonly represent the contents of memory in a format similar to what is shown in <a href="ch07.xhtml#ch7fig4">Figure 7-4</a>.</p>&#13;
<span epub:type="pagebreak" id="page_122"/>&#13;
<div class="image" id="ch7fig4"><img src="../images/fig7-4.jpg" alt="image"/></div>&#13;
<p class="figcap"><em>Figure 7-4: A typical view of memory bytes</em></p>&#13;
<p class="indent">The leftmost column in <a href="ch07.xhtml#ch7fig4">Figure 7-4</a> is a memory address in hexadecimal, and the following 16 values represent the bytes at that address and the 15 subsequent addresses. This approach is more compact than <a href="ch07.xhtml#ch7tab2">Table 7-2</a>, but it means that each address isn’t uniquely called out. In this figure, we again see the ASCII string “Hello” stored starting at address 0x0002.</p>&#13;
<p class="indent">Our hypothetical computer with 64KB of RAM is useful as an example, but modern computing devices tend to have a much larger amount of memory. As of 2020, smartphones commonly have at least 1GB of memory, and laptop computers usually have at least 4GB.</p>&#13;
<div class="sidebar">&#13;
<p class="exercise" id="ch7ex1"><strong>EXERCISE 7-1: CALCULATE THE REQUIRED NUMBER OF BITS</strong></p>&#13;
<p class="exercise-para">Using the techniques just described, determine the number of bits required for addressing 4GB of memory. You’ll want to look back at <a href="ch01.xhtml#ch1tab3">Table 1-3</a> for a reference on SI prefixes. Remember that each byte is assigned a unique address, which is just a number. The answer is in <a href="appa.xhtml">Appendix A</a>.</p>&#13;
</div>&#13;
<h3 class="h3" id="lev1_46"><strong>Central Processing Unit (CPU)</strong></h3>&#13;
<p class="noindent">Memory gives the computer a place to store data and program instructions, but it’s the CPU, or processor, that carries out those instructions. It’s the processor that allows a computer to have the flexibility to run programs that weren’t even conceived of at the time the processor was designed. A processor implements a set of instructions that programmers can then use to construct meaningful software. Each instruction is simple, but these basic instructions are the building blocks for all software.</p>&#13;
<p class="indent">Here are some examples of types of instructions that CPUs support:</p>&#13;
<p class="block"><strong>Memory access</strong>   read, write (to memory)</p>&#13;
<p class="block"><strong>Arithmetic</strong>   add, subtract, multiply, divide, increment</p>&#13;
<p class="block"><strong>Logic</strong>   AND, OR, NOT</p>&#13;
<p class="block"><strong>Program flow</strong>   jump (to a specific part of a program), call (a subroutine)</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_123"/>We’ll go into specific CPU instructions in <a href="ch08.xhtml">Chapter 8</a>, but for now, it’s important to understand that CPU instructions are just operations that the processor can perform. They are fairly simple (add two numbers, read from a memory address, perform a logical AND, and so forth). Programs consist of ordered sets of these instructions. To use a cooking analogy, the CPU is the cook, a program is a recipe, and each instruction in the program is a step of the recipe that the cook knows how to perform.</p>&#13;
<p class="indent">Program instructions reside in memory. The CPU reads these instructions so it can run the program. <a href="ch07.xhtml#ch7fig5">Figure 7-5</a> illustrates a simple program that’s read from memory by the CPU.</p>&#13;
<div class="image" id="ch7fig5"><img src="../images/fig7-5.jpg" alt="image"/></div>&#13;
<p class="figcap"><em>Figure 7-5: An example program is read from memory and runs on the CPU.</em></p>&#13;
<p class="indent">The example program in <a href="ch07.xhtml#ch7fig5">Figure 7-5</a> is written in <em>pseudocode</em>, a human-readable description of a program that’s not written in a real programming language. The steps in the program fall into the categories just described (memory access, arithmetic, logic, and program flow). In the first step, the program reads a number stored at a certain address in memory. The program then adds 3 to that number. It then performs a logical AND of two conditions. If the logical result is true, then the program does “this”; otherwise, it does “that.” Believe it or not, all programs are, in essence, simply various combinations of these types of fundamental operations.</p>&#13;
<h4 class="h4" id="lev2_15"><strong><em>Instruction Set Architectures</em></strong></h4>&#13;
<p class="noindent">Although all CPUs implement these types of instructions, the specific instructions available on different processors vary. Some instructions that exist for one type of CPU simply don’t exist on other types of CPUs. Even instructions that do exist on nearly all CPUs aren’t implemented in the same way. For example, the specific binary sequence used to mean “add two numbers” is not the same across processor types. A family of CPUs that use the same instructions are said to share an <em>instruction set architecture (ISA)</em>, or just <em>architecture</em>, a model of how a CPU works. Software that’s built for a certain ISA works on any CPU that implements that ISA. It’s possible for multiple processor models, even those from different manufacturers, to implement the same architecture. Such processors may work <span epub:type="pagebreak" id="page_124"/>very differently internally, but by adhering to the same ISA, they can run the same software. Today, there are two prevalent instruction set architectures: x86 and ARM.</p>&#13;
<p class="indent">The majority of desktop computers, laptops, and servers use x86 CPUs. The name comes from Intel Corporation’s naming convention for its processors (each ending in 86), beginning with the 8086 released in 1978, and continuing with the 80186, 80286, 80386, and 80486. After the 80486 (or more simply the 486), Intel began branding its CPUs with names such as Pentium and Celeron; these processors are still x86 CPUs despite the name change. Other companies besides Intel also produce x86 processors, notably Advanced Micro Devices, Inc. (AMD).</p>&#13;
<p class="indent">The term <em>x86</em> refers to a set of related architectures. Over time, new instructions have been added to the x86 architecture, but each generation has tried to retain backward compatibility. This generally means that software developed for an older x86 CPU runs on a newer x86 CPU, but software built for a newer x86 CPU that takes advantage of new x86 instructions won’t be able to run on older x86 CPUs that don’t understand the new instructions.</p>&#13;
<p class="indent">The x86 architecture includes three major generations of processors: 16-bit, 32-bit, and 64-bit. Let’s pause to examine what we mean when we say that a CPU is a 16-bit, 32-bit, or 64-bit processor. The number of bits associated with a processor, also known as its <em>bitness</em> or <em>word size</em>, refers to the number of bits it can deal with at a time. So a 32-bit CPU can operate on values that are 32 bits in length. More specifically, this means that the computer architecture has 32-bit registers, a 32-bit address bus, or a 32-bit data bus. Or all three may be 32-bit. We’ll cover more details on registers, data buses, and address buses later.</p>&#13;
<p class="indent">Going back to x86 and its generations of processors, the original 8086 processor, released in 1978, was a 16-bit processor. Encouraged by the success of the 8086, Intel continued producing compatible processors. Intel’s subsequent x86 processors were also 16-bit until the 80386 processor was released in 1985, bringing with it a new 32-bit version of the x86 architecture. This 32-bit version of x86 is sometimes called IA-32. Thanks to backward compatibility, modern x86 processors still fully support IA-32. An example x86 processor is shown in <a href="ch07.xhtml#ch7fig6">Figure 7-6</a>.</p>&#13;
<p class="indent">Interestingly, it was AMD, and not Intel, that brought x86 into the 64-bit era. In the late 1990s, Intel’s 64-bit focus was on a new CPU architecture called IA-64 or Itanium, which was <em>not</em> an x86 ISA, and ended up as a niche product for servers. With Intel focused on Itanium, AMD seized the opportunity to extend the x86 architecture. In 2003, AMD released the Opteron processor, the first 64-bit x86 CPU. AMD’s architecture was originally known as <em>AMD64</em>, and later Intel adopted this architecture and called its implementation <em>Intel 64</em>. The two implementations are mostly functionally identical, and today 64-bit x86 is generally referred to as <em>x64</em> or <em>x86-64</em>.</p>&#13;
<span epub:type="pagebreak" id="page_125"/>&#13;
<div class="image" id="ch7fig6"><img src="../images/fig7-6.jpg" alt="image"/></div>&#13;
<p class="figcap"><em>Figure 7-6: An Intel 486 SX, a 32-bit x86 processor</em></p>&#13;
<p class="indent">Although x86 rules the personal computer and server world, ARM processors command the realm of mobile devices like smartphones and tablets. Multiple companies manufacture ARM processors. A company called ARM Holdings develops the ARM architecture and licenses their designs to other companies to implement. It’s common for ARM CPUs to be used in <em>system-on-chip (SoC)</em> designs, where a single integrated circuit contains not only a CPU, but also memory and other hardware. The ARM architecture originated in the 1980s as a 32-bit ISA. A 64-bit version of the ARM architecture was introduced in 2011. ARM processors are favored in mobile devices due to their reduced power consumption and lower cost as compared to x86 processors. ARM processors can be used in PCs as well, but that market largely remains focused on x86, to retain backward compatibility with existing x86 PC software. However, in 2020, Apple announced their intention to move macOS computers from x86 to ARM CPUs.</p>&#13;
<h4 class="h4" id="lev2_16"><strong><em>CPU Internals</em></strong></h4>&#13;
<p class="noindent">Internally, a CPU consists of multiple components that work together to execute instructions. We’ll focus on three fundamental components: the processor registers, the arithmetic logic unit, and the control unit. <em>Processor registers</em> are locations within the CPU that hold data during processing. The <em>arithmetic logic unit (ALU)</em> performs logical and mathematical operations. The processor <em>control unit</em> directs the CPU, communicating with the processor registers, the ALU, and main memory. <a href="ch07.xhtml#ch7fig7">Figure 7-7</a> shows a simplified view of CPU architecture.</p>&#13;
<span epub:type="pagebreak" id="page_126"/>&#13;
<div class="image" id="ch7fig7"><img src="../images/fig7-7.jpg" alt="image"/></div>&#13;
<p class="figcap"><em>Figure 7-7: A greatly simplified view of CPU architecture</em></p>&#13;
<p class="indent">Let’s look at processor registers. Main memory holds data for an executing program. However, when a program needs to operate on a piece of data, the CPU needs a temporary place to store the data within the processor hardware. To accomplish this, CPUs have small internal storage locations known as processor registers, or just registers. Compared to accessing main memory, accessing registers is a very fast operation for a CPU, but registers can only hold very small amounts of data. We measure the size of an individual register in bits, not bytes, because registers are so small. As an example, a 32-bit CPU usually has registers that are 32 bits “wide,” meaning each register can hold 32 bits of data. The registers are implemented in a component known as the <em>register file</em> (not to be confused with a data file, such as a document or photo). The memory cells used in the register file are typically a type of SRAM.</p>&#13;
<p class="indent">The ALU handles logic and math operations within the CPU. We previously covered combinational logic circuits, circuits in which the output is a function of the input. A processor’s ALU is just a complex combinational logic circuit. The ALU’s inputs are values called <em>operands</em>, and a code indicating what operation to perform on those operands. The ALU outputs the result of the operation along with a status that provides more detail on execution of the operation.</p>&#13;
<p class="indent">The control unit acts as the coordinator of the CPU. It works on a repeating cycle: fetch an instruction from memory, decode it, and execute it. Since a running program is stored in memory, the control unit needs to know which memory address to read in order to fetch the next instruction. The control unit determines this by looking at a register known as the <em>program counter (PC)</em>, also known as the <em>instruction pointer</em> on x86. The program counter holds the memory address of the next instruction to execute. The control unit reads the instruction from the specified memory address, stores the instruction in a register called the <em>instruction register</em>, and updates <span epub:type="pagebreak" id="page_127"/>the program counter to point to the next instruction. The control unit then decodes the current instruction, making sense of the 1s and 0s that represent an instruction. Once decoded, the control unit executes the instruction, which may require coordinating with other components in the CPU. For example, execution of an addition operation requires the control unit to instruct the ALU to perform the needed math. Once an instruction has completed, the control unit repeats the cycle: fetch, decode, execute.</p>&#13;
<h4 class="h4" id="lev2_17"><strong><em>Clock, Cores, and Cache</em></strong></h4>&#13;
<p class="noindent">Since CPUs execute ordered sets of instructions, you may wonder what causes a CPU to progress from one instruction to the next. We previously demonstrated how a clock signal can be used to move a circuit from one state to another, such as in a counter circuit. The same principle applies here. A CPU takes an input clock signal, as illustrated in <a href="ch07.xhtml#ch7fig8">Figure 7-8</a>, and a clock pulse acts as a signal to the CPU to transition between states.</p>&#13;
<div class="image" id="ch7fig8"><img src="../images/fig7-8.jpg" alt="image"/></div>&#13;
<p class="figcap"><em>Figure 7-8: A clock provides an oscillating signal to the CPU.</em></p>&#13;
<p class="indent">It’s an oversimplification to think that a CPU executes exactly one instruction per clock cycle. Some instructions take multiple cycles to complete. Also, modern CPUs use an approach called <em>pipelining</em> to divide instructions into smaller steps so that portions of multiple instructions can be run in parallel by a single processor. For example, one instruction can be fetched while another is decoded and yet another is executed. Still, it can be helpful to think of each pulse of the clock as a signal to the CPU to move forward with executing a program. Modern CPUs have clock speeds measured in <em>gigahertz (GHz)</em>. For example, a 2GHz CPU has a clock that oscillates at 2 <em>billion</em> cycles per second!</p>&#13;
<p class="indent">Increasing the frequency of the clock allows a CPU to perform more instructions per second. Unfortunately, we can’t just run a CPU at an arbitrarily high clock rate. CPUs have a practical upper limit on their input clock frequency, and pushing a CPU beyond that limit leads to excessive heat generation. Also, the CPU’s logic gates may not be able to keep up, causing unexpected errors and crashes. For many years, the computer industry saw steady increases in the upper limit of clock rates for CPUs. This clock rate increase was largely due to regular improvements in manufacturing processes that led to increased transistor density, which allowed for CPUs with higher clock rates but roughly the same power consumption. In 1978, the Intel 8086 ran at 5MHz, and by 1999, the Intel Pentium III had a 500MHz clock, a 100x increase in only about 20 years! CPU clock rates <span epub:type="pagebreak" id="page_128"/>continued to increase rapidly until the 3GHz threshold was crossed in the early 2000s. Since then, despite continued growth in transistor count, physical limitations associated with diminutive transistor sizes have made significant increases to clock rate impractical.</p>&#13;
<p class="indent">With clock rates stagnant, the processor industry turned to a new approach for getting more work out of a CPU. Rather than focusing on increasing clock frequency, CPU design began to focus on execution of multiple instructions in parallel. The idea of a <em>multicore</em> CPU was introduced, a CPU with multiple processing units called <em>cores</em>. A <em>CPU core</em> is effectively an independent processor that resides alongside other independent processors in a single CPU package, as illustrated in <a href="ch07.xhtml#ch7fig9">Figure 7-9</a>.</p>&#13;
<div class="image" id="ch7fig9"><img src="../images/fig7-9.jpg" alt="image"/></div>&#13;
<p class="figcap"><em>Figure 7-9: A four-core CPU—each core has its own registers, ALU, and control unit</em></p>&#13;
<p class="indent">Note that multiple cores running in parallel is not the same as pipelining. The parallelism of multicore means that each core works on a different task, a separate set of instructions. In contrast, pipelining happens <em>within</em> each core, allowing portions of multiple instructions to be executed in parallel by a single core.</p>&#13;
<p class="indent">Each core added to a processor opens the door to a computer running additional instructions in parallel. That said, adding multiple cores to a computer’s CPU doesn’t mean all applications benefit immediately or equally. Software must be written to take advantage of parallel processing of instructions to get the maximum benefit of multicore hardware. However, even if individual programs aren’t designed with parallelism in mind, a computer system as a whole can benefit, since modern operating systems run multiple programs at once.</p>&#13;
<p class="indent">I’ve previously described how CPUs load data from main memory into registers for processing and then store that data back from registers to <span epub:type="pagebreak" id="page_129"/>memory for later use. It turns out that programs tend to access the same memory locations over and over. As you might expect, going back to main memory multiple times to access the same data is inefficient! To avoid this inefficiency, a small amount of memory resides within the CPU that holds a copy of data frequently accessed from main memory. This memory is known as a <em>CPU cache</em>.</p>&#13;
<p class="indent">The processor checks the cache to see if data it wishes to access is there. If so, the processor can speed things up by reading or writing to the cache rather than to main memory. When needed data is not in the cache, the processor can move that data into cache once it has been read from main memory. It’s common for processors to have multiple cache levels, often three. We refer to these cache levels as L1 cache, L2 cache, and L3 cache. A CPU first checks L1 for the needed data, then L2, then L3, before finally going to main memory, as illustrated in <a href="ch07.xhtml#ch7fig10">Figure 7-10</a>. L1 cache is the fastest to access, but it’s also the smallest. L2 is slower and larger, and L3 is slower and larger still. Remember that even with these progressively slower levels of cache, it is still slower to access main memory than any level of cache.</p>&#13;
<div class="image" id="ch7fig10"><img src="../images/fig7-10.jpg" alt="image"/></div>&#13;
<p class="figcap"><em>Figure 7-10: A single-core CPU with three levels of cache</em></p>&#13;
<p class="indent">In multicore CPUs, some caches are specific to each core, whereas others are shared among the cores. For example, each core may have its own L1 cache, whereas the L2 and L3 caches are shared, as shown in <a href="ch07.xhtml#ch7fig11">Figure 7-11</a>.</p>&#13;
<div class="image" id="ch7fig11"><img src="../images/fig7-11.jpg" alt="image"/></div>&#13;
<p class="figcap"><em>Figure 7-11: A two-core CPU with cache. Each core has its own L1 cache, whereas L2 and L3 caches are shared.</em></p>&#13;
<h3 class="h3" id="lev1_47"><strong><span epub:type="pagebreak" id="page_130"/>Beyond Memory and Processor</strong></h3>&#13;
<p class="noindent">I have outlined the two fundamental components required for a computer: memory and a processor. However, a device that consists of only memory and a processor has a couple of gaps that need to be filled if we want a useful device. The first gap is that both memory and CPUs are volatile; they lose state when power is removed. The second gap is that a computer with only memory and a processor has no way of interacting with the outside world. Let’s now see how secondary storage and I/O devices fill these gaps.</p>&#13;
<h4 class="h4" id="lev2_18"><strong><em>Secondary Storage</em></strong></h4>&#13;
<p class="noindent">If a computer only included memory and a processor, then every time that device was powered down, it would lose all its data! To emphasize that point, <em>data</em> here means not only a user’s files and settings, but also any installed applications, and even the operating system itself. This rather inconvenient computer would require someone to load the OS and any applications every time it was powered on. That might discourage users from ever turning it off. Believe it or not, computers in previous generations did work this way, but fortunately that isn’t the case today.</p>&#13;
<p class="indent">To address this problem, computers have secondary storage. <em>Secondary storage</em> is nonvolatile and therefore remembers data even when the system is powered down. Unlike RAM, secondary storage is not directly addressable by the CPU. Such storage is usually much cheaper per byte than RAM, allowing for a large capacity of storage as compared to main memory. However, secondary storage is also considerably slower than RAM; it isn’t a suitable replacement for main memory.</p>&#13;
<p class="indent">In modern computing devices, hard disk drives and solid-state drives are the most common secondary storage devices. A <em>hard disk drive (HDD)</em> stores data using magnetism on a rapidly spinning platter, whereas a <em>solid-state drive (SSD)</em> stores data using electrical charges in nonvolatile memory cells. Compared to HDDs, SSDs are faster, quieter, and more resistant to mechanical failure, since SSDs have no moving parts. <a href="ch07.xhtml#ch7fig12">Figure 7-12</a> is a photo of a couple of secondary storage devices.</p>&#13;
<p class="indent">With a secondary storage device in place, a computer can load data on demand. When a computer is powered on, the operating system loads from secondary storage into main memory; any applications that are set to run at startup also load. After startup, when an application is launched, program code loads from secondary storage into main memory. The same goes for any user data (documents, music, settings, and so on) stored locally; it must load from secondary storage into main memory before it can be used. In common usage, secondary storage is often referred to simply as storage, while primary storage/main memory is just called memory or RAM.</p>&#13;
<span epub:type="pagebreak" id="page_131"/>&#13;
<div class="image" id="ch7fig12"><img src="../images/fig7-12.jpg" alt="image"/></div>&#13;
<p class="figcap"><em>Figure 7-12: A 4GB hard disk drive from 1997 beside a modern 32GB microSD card, a type of solid-state storage</em></p>&#13;
<h4 class="h4" id="lev2_19"><strong><em>Input/Output</em></strong></h4>&#13;
<p class="noindent">Even with secondary storage in place, our hypothetical computer still has a problem. A computer consisting of a processor, memory, and storage doesn’t have any way of interacting with the outside world! This is where input/output devices come in. An <em>input/output (I/O) device</em> is a component that allows a computer to receive input from the outside world (keyboard, mouse), send data to the outside world (monitor, printer), or both (touchscreen). Human interaction with a computer requires going through I/O. Computer-to-computer interaction also requires going through I/O, often in the form of a computer network, such as the internet. Secondary storage devices are actually a type of I/O device. You may not think of accessing internal storage as I/O, but from the perspective of the CPU, reading or writing to storage is just another I/O operation. Reading from the storage device is input, while writing to the storage device is output. <a href="ch07.xhtml#ch7fig13">Figure 7-13</a> provides some examples of input and output.</p>&#13;
<div class="image" id="ch7fig13"><img src="../images/fig7-13.jpg" alt="image"/></div>&#13;
<p class="figcap"><em>Figure 7-13: Common types of input and output</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_132"/>So how does a CPU go about communicating with I/O devices? A computer can have a wide variety of I/O devices attached to it, and the CPU needs a standard way to communicate with any such device. To understand this, we need to first discuss <em>physical address space</em>, the range of hardware memory addresses available to a computer. Earlier in this chapter, in the section entitled “Main Memory” on <a href="ch07.xhtml#page_119">page 119</a>, we covered how bytes of memory are assigned an address. All memory addresses on a given computer system will be represented with a certain number of bits. That number of bits determines not only the size of each memory address, but also the range of addresses available for the computer hardware to use—the physical address space. Address space is often larger than the amount of RAM installed on a computer, leaving some physical memory addresses unused.</p>&#13;
<p class="indent">To give an example, in the case of a computer with a 32-bit physical address space, the physical address range is from 0x00000000 to 0xFFFFFFFF (the largest address that can be represented with a 32-bit number). That’s approximately 4 billion addresses, each representing a single byte, or 4GB of address space. Let’s say that this computer has 3GB of RAM, so 75 percent of the available physical memory addresses are assigned to bytes of RAM.</p>&#13;
<p class="indent">Now let’s return to the question of how CPUs communicate with I/O devices. Addresses in physical address space don’t always refer to bytes of memory; they can also refer to an I/O device. When physical address space is mapped to an I/O device, the CPU can communicate with that device just by reading or writing to its assigned memory address(es); this is called <em>memory-mapped I/O (MMIO)</em> and is illustrated in <a href="ch07.xhtml#ch7fig14">Figure 7-14</a>. When a computer treats the memory of I/O devices just like main memory, its CPU does not need any special instructions for I/O operations.</p>&#13;
<p class="indent">However, some CPU families, notably x86, do include special instructions for accessing I/O devices. When computers use this approach, rather than mapping I/O devices to a physical memory address, devices are assigned an <em>I/O port</em>. A port is like a memory address, but instead of referring to a location in memory, the port number refers to an I/O device. You can think of the set of I/O ports as just another address space, distinct from memory addresses. This means that port 0x378 does not refer to the same thing as physical memory address 0x378. Accessing I/O devices through a separate port address space is known as <em>port-mapped I/O (PMIO)</em>. Today’s x86 CPUs support both port-mapped and memory-mapped I/O.</p>&#13;
<p class="indent">I/O ports and memory-mapped I/O addresses generally refer to a device controller rather than directly to data stored on the device. For example, in the case of a hard disk drive, the bytes of the disk aren’t directly mapped into address space. Instead, a hard drive controller presents an interface, accessible through I/O ports or memory-mapped I/O addresses, that allows the CPU to request read or write operations to locations on the disk.</p>&#13;
<span epub:type="pagebreak" id="page_133"/>&#13;
<div class="image" id="ch7fig14"><img src="../images/fig7-14.jpg" alt="image"/></div>&#13;
<p class="figcap"><em>Figure 7-14: Memory-mapped I/O</em></p>&#13;
<div class="sidebar">&#13;
<p class="exercise"><strong>EXERCISE 7-2: GET TO KNOW THE HARDWARE DEVICES IN YOUR LIFE</strong></p>&#13;
<p class="exercise-para">Choose a couple of computing devices that you own or use—say a laptop, smartphone, or game console. Answer the following questions about each device. You may be able to find the answers by looking at the settings on the device itself, or you may have to do some research online.</p>&#13;
<ul>&#13;
<li class="noindent">What kind of CPU does the device have?</li>&#13;
<li class="noindent">Is the CPU 32-bit or 64-bit (or something else)?</li>&#13;
<li class="noindent">What’s the CPU clock frequency?</li>&#13;
<li class="noindent">Does the CPU have L1, L2, or L3 cache? If so, how much?</li>&#13;
<li class="noindent">Which instruction set architecture does the CPU use?</li>&#13;
<li class="noindent">How many cores does the CPU have?</li>&#13;
<li class="noindent">How much and what kind of main memory does the device have?</li>&#13;
<li class="noindent">How much and what kind of secondary storage does the device have?</li>&#13;
<li class="noindent">What I/O devices does the device have?</li>&#13;
</ul>&#13;
</div>&#13;
<h3 class="h3" id="lev1_48"><strong><span epub:type="pagebreak" id="page_134"/>Bus Communication</strong></h3>&#13;
<p class="noindent">At this point, we’ve covered the roles of memory, the CPU, and I/O devices in a computer. We’ve also touched on the CPU’s communication with both memory and I/O devices through memory address space. Let’s take a closer look at how the CPU communicates with memory and I/O devices.</p>&#13;
<p class="indent">A <em>bus</em> is a hardware communication system used by computer components. There are multiple bus implementations, but in the early days of computers, a bus was simply a set of parallel wires, each carrying an electrical signal. This allowed multiple bits of data to be transferred in parallel; the voltage on each wire represented a single bit. Today’s bus designs aren’t always that simple, but the intent is similar.</p>&#13;
<p class="indent">There are three common bus types used in communication between the CPU, memory, and I/O devices. An <em>address bus</em> acts as a selector for the memory address that the CPU wishes to access. For example, if a program wishes to write to address 0x2FE, the CPU writes 0x2FE to the address bus. The <em>data bus</em> transmits a value read from memory or a value to be written to memory. So if the CPU wishes to write the value 25 to memory, then 25 is written to the data bus. Or if the CPU is reading data from memory, the CPU reads the value from the data bus. Finally, a <em>control bus</em> manages the operations happening over the other two buses. As examples, the CPU uses the control bus to indicate that a write operation is about to happen, or the control bus can carry a signal indicating the status of an operation. <a href="ch07.xhtml#ch7fig15">Figure 7-15</a> illustrates how a CPU uses the address bus, data bus, and control bus to read memory.</p>&#13;
<div class="image" id="ch7fig15"><img src="../images/fig7-15.jpg" alt="image"/></div>&#13;
<p class="figcap"><em>Figure 7-15: The CPU requests a read of the value at address 3F4, and the value of 84 is returned.</em></p>&#13;
<p class="indent">In the example shown in <a href="ch07.xhtml#ch7fig15">Figure 7-15</a>, the CPU needs to read the value stored at memory address 000003F4. To do this, the CPU writes 000003F4 to the address bus. The CPU also sets a certain value on the control bus, indicating that it wishes to perform a read operation. These bus updates act as inputs to the memory controller (the circuit that manages interactions with main memory), telling it that the CPU wishes to read the value stored at address 000003F4 in main memory. In response, the memory controller retrieves the value stored at address 000003F4 (84 in this example) and writes it to the data bus, which the CPU can then read.</p>&#13;
<h3 class="h3" id="lev1_49"><strong><span epub:type="pagebreak" id="page_135"/>Summary</strong></h3>&#13;
<p class="noindent">In this chapter, we covered computer hardware: a central processing unit (CPU) to execute instructions, random access memory (RAM) that stores instructions and data while powered, and input/output (I/O) devices that interact with the outside world. You learned that memory is composed of single-bit memory cells, implemented with a type of flip-flop in SRAM, and with a transistor and capacitor in DRAM. We covered how memory addressing works, where each address refers to a byte of memory. You learned about CPU architectures, including x86 and ARM. We explored how CPUs work internally, looking at registers, the ALU, and the control unit. We covered secondary storage and other types of I/O, and finally, we looked at bus communication.</p>&#13;
<p class="indent">In the next chapter, we’ll move beyond hardware to the thing that makes computers unique among devices—software. We’ll examine the low-level instructions that processors execute, and we’ll see how those instructions can be combined to perform useful operations. You’ll have the opportunity to write software in assembly language and use a debugger to explore machine code.</p>&#13;
<span epub:type="pagebreak" id="page_136"/>&#13;
</body></html>