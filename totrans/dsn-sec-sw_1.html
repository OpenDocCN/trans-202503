<html><head></head><body>
<section>
<header>
<h1 class="part">
<span class="PartNumber"><span epub:type="pagebreak" title="1" id="Page_1"/>Part I</span><br/>
<span class="PartTitle">Concepts</span></h1>
</header>
</section>


<section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" title="3" id="Page_3"/>1</span><br/>
<span class="ChapterTitle">Foundations</span></h1>
</header>
<blockquote class="Epigraph" epub:type="epigraph">
<p class="Epigraph">Honesty is a foundation, and it’s usually a solid foundation. Even if I do get in trouble for what I said, it’s something that I can stand on.</p>
<p class="EpigraphSource">—Charlamagne tha God</p>
</blockquote>
<figure class="opener">
<img src="image_fi/book_art/chapterart.png" alt=""/>
</figure>
<p class="ChapterIntro">Software security is at once a logical practice and an art, one based on intuitive decision making. It requires an understanding of modern digital systems, but also a sensitivity to the humans interacting with, and affected by, those systems. If that sounds daunting, then you have a good sense of the fundamental challenge this book endeavors to explain. This perspective also sheds light on why software security has continued to challenge the field for so long, and why the solid progress made so far has taken so much effort, even if it has only chipped away at some of the problems. Yet there is very good news in this state of affairs, because it means that all of us can make a real difference by increasing our awareness of, and participation in, better security at every stage of the process.</p>
<p>We begin by considering what security exactly is. Given security’s subjective nature, it’s critical to think clearly about its foundations. This book <span epub:type="pagebreak" title="4" id="Page_4"/>represents my understanding of the best thinking out there, based on my own experience. Trust undergirds all of security, because nobody works in a vacuum, and modern digital systems are far too complicated to be built single-handedly from the silicon up; you have to trust others to provide everything (starting with the hardware, firmware, operating system, and compilers) that you don’t create yourself. Building on this base, next I present the six classic principles of security: the three components of classic information security and the three-part “Gold Standard” used to enforce it. Finally, the section on information privacy adds important human and societal factors necessary to consider as digital products and services become increasingly integrated into the most sensitive realms of modern life.</p>
<p>Though readers doubtlessly have good intuitions about what words such as <em>security</em>, <em>trust</em>, or <em>confidentiality</em> mean, in this book these words take on specific technical meanings worth teasing out carefully, so I suggest reading this chapter closely. As a challenge to more advanced readers, I invite you to attempt to write better descriptions yourself—no doubt it will be an educational exercise for everyone.</p>
<h2 id="h1-123456c01-0001">Understanding Security</h2>
<p class="BodyFirst">All organisms have natural instincts to chart a course away from danger, defend against attacks, and aim toward whatever sanctuary they can find. </p>
<p>It is important to appreciate just how remarkable our innate sense of physical security is, when it works. By contrast, we have few genuine signals to work with in the virtual world—and fake signals are easily fabricated. Before we approach security from a technical perspective, let’s consider a real-world story as an illustration of what humans are capable of. (As we’ll see later, in the digital domain we need a whole new set of skills.)</p>
<p>The following is a true story from an auto salesman. After conducting a customer test drive, the salesman and customer returned to the lot. The salesman got out of the car and continued to chat with the customer while walking around to the front of the car. “When I looked him in the eyes,” the salesman recounted, “That’s when I said, ‘Oh no. This guy’s gonna try and steal this car.’” Events accelerated: the customer-turned-thief put the car in gear and sped away while the salesman hung on for the ride of his life <em>on the hood of the car</em>. The perpetrator drove violently in an unsuccessful attempt to throw him from the vehicle. (Fortunately, the salesman sustained no major injuries and the criminal was soon arrested, convicted, and ordered to pay restitution.)</p>
<p>A subtle risk calculation took place when those men locked eyes. Within fractions of a second, the salesman had processed complex visual signals, derived from the customer’s facial expression and body language, distilling into a clear intention of a hostile action. Now imagine that the same salesman was the target of a <em>spear phishing</em> attack (a fraudulent email designed to fool a specific target, as opposed to a mass audience). In the digital realm, without the signals he detected when face-to-face with his attacker, he’d be much more easily tricked.</p>
<p><span epub:type="pagebreak" title="5" id="Page_5"/>When it comes to information security, computers, networks, and software, we need to think analytically to assess the risks we face if we want to have any hope of securing digital systems. And we must do this despite being unable to directly see, smell, or hear bits or code. Whenever you’re examining data online, you’re using software to display information in human-readable fonts, and typically, there’s a lot of code between you and the actual bits; in fact, it’s potentially a hall of mirrors. So you must trust your tools and trust that you really are examining the data you think you are.</p>
<p>Software security centers on the protection of digital assets against an array of threats, an effort largely driven by a basic set of security principles that the rest of this chapter will discuss. By analyzing a system from these first principles, we can learn how vulnerabilities slip into software, as well as how to proactively avoid and mitigate problems. These foundational principles, along with other design techniques covered in subsequent chapters, apply not only to software but also to designing and operating bicycle locks, bank vaults, or prisons. </p>
<p>The term <em>information security</em> refers specifically to the protection of data and how access is granted. <em>Software security</em> is a broader term that focuses on the design, implementation, and operation of software systems that are trustworthy, including the reliable enforcement of information security.  </p>
<h2 id="h1-123456c01-0002">Trust</h2>
<p class="BodyFirst">Trust is equally critical in the digital realm, yet too often taken for granted. Software security ultimately depends on trust, because you cannot control every part of a system, write all of your own software, or vet all suppliers of dependencies. Modern digital systems are so complex that not even the major tech giants can build a complete technology stack from scratch. From the silicon to the operating systems, networking, peripherals, and the numerous software layers that make it all work, the systems we rely on routinely are remarkable technical accomplishments of immense size and complexity. Since nobody can build these systems all by themselves, organizations rely on hardware and software products often chosen based on features or pricing—but it’s important to remember that each dependency also involves a <em>trust decision</em>. </p>
<p>Security demands that we examine these trust relationships closely, even though nobody has the time or resources to investigate and verify everything. Failing to trust enough means doing a lot of needless work to protect a system when no real threat is likely. On the other hand, trusting too freely could mean getting blindsided later. Put bluntly, when you fully trust an entity, they are free to fail without consequences. Trust can be violated in two fundamentally different ways: by malice (cheating, lying, subterfuge) and by incompetence (mistakes, misunderstandings, negligence).</p>
<p>The need to make critical decisions in the face of incomplete information is precisely what trust is best suited for. But our innate sense of trust relies on subtle sensory inputs wholly unsuited to the digital realm. The following discussion begins with the concept of trust itself, dissects what <span epub:type="pagebreak" title="6" id="Page_6"/>trust as we experience it is, and then shifts to trust as it relates to software. As you read along, try to find the common threads and connect how you think about software to your intuitions about trust. Tapping into your existing trust skills is a powerful technique that over time gives you a gut feel for software security that is more effective than any amount of technical analysis. </p>
<h3 id="h2-123456c01-0001">Feeling Trust</h3>
<p class="BodyFirst">The best way to understand trust is to pay attention while experiencing what relying on trust actually feels like. Here’s a thought experiment—or an exercise to try for real, with someone you <em>really trust</em>—that brings home exactly what trust means. Imagine walking along a busy thoroughfare with a friend, with traffic streaming by only a few feet away. Sighting a crosswalk up ahead, you explain that you would like them to guide you across the road, that you are relying on them to cross safely, and that you are closing your eyes and will obediently follow them. Holding hands, you and your friend proceed to the crosswalk, where they gently turn you to face the road, gesturing by touch that you should wait. Listening to the sounds of speeding cars, you know well that your friend (and now, guardian) is waiting until it is safe to cross, but your heartbeat has most likely also increased noticeably, and you may find yourself listening attentively for any sound of impending danger.</p>
<p>Now your friend unmistakably leads you forward, guiding you to step down from the curb. If you decide to step into the road with your eyes closed, what you are feeling is pure trust—or perhaps some degree of the lack thereof. Your mind keenly senses palpable risk, your senses strain to confirm safety directly, and something deep down is warning you not to do it. Your own internal security monitoring system has insufficient evidence and wants you to open your eyes before moving; what if your friend somehow misjudges the situation, or worse, is playing a deadly evil trick on you? Ultimately, it’s the trust you have invested in your friend that allows you to override those instincts and cross the road. </p>
<p>Raise your own awareness of digital trust decisions, and help others see how important their impact is on security. Ideally, when you select a component or choose a vendor for a critical service, you’ll be able to tap into the very same intuitions that guide trust decisions like in the exercise just described. </p>
<h3 id="h2-123456c01-0002">You Cannot See Bits</h3>
<p class="BodyFirst">All of this discussion is to emphasize the fact that when you think you are “looking directly at the data,” you are actually looking at a distant representation. In fact, you are looking at pixels on a screen that you believe represent the contents of certain bytes whose physical location you don’t know with any precision, and many millions of instructions were likely executed in order to map the data into the human-legible form on your display. Digital technology makes trust especially tricky, because it’s so abstract, lightning fast, and hidden from direct view. Whenever you examine data, remember that there is a lot of software and hardware between the actual data in memory and the <span epub:type="pagebreak" title="7" id="Page_7"/>pixels that form characters that we interpret as the data value. If something in there were maliciously misrepresenting the actual data, how would you possibly know? Ground truth about digital information is extremely difficult to observe directly.</p>
<p>Consider the lock icon in the address bar of a web browser indicating a secure connection to the website. The appearance or absence of these distinctive pixels communicates a single bit to the user: safe or unsafe. Behind the scenes, there is a lot of data and considerable computation, as will be detailed in <span class="xref" itemid="xref_target_Chapter 11">Chapter 11</span>, all rolling up into a binary yes/no security indication. Even an expert developer would face a Herculean task attempting to manually confirm the validity of just one instance. So all we can do is trust the software—and there is every reason that we should trust it. The point here is to recognize how deep and pervasive that trust is, not just take it for granted. </p>
<h3 id="h2-123456c01-0003">Competence and Imperfection</h3>
<p class="BodyFirst">Most attacks begin by exploiting a software flaw or misconfiguration that resulted from the honest, good faith efforts of programmers and IT staff, who happen to be human, and hence imperfect. Since licenses routinely disavow essentially all liability, all software is used on a <em>caveat emptor</em> basis. If, as is routinely claimed, “all software has bugs,” then a subset of those bugs will be exploitable, and eventually the attackers will find a few of those bugs and have an opportunity to use them maliciously. It’s relatively rare for software professionals to fall victim to an attack due to misplaced trust in malicious software, enabling a direct attack. </p>
<p>Fortunately, making big trust decisions about operating systems and programming languages is usually easy. Many large corporations have extensive track records of providing and supporting quality hardware and software products, and it’s quite reasonable to trust them. Trusting others with less of a track record might be riskier. While they likely have many skilled and motivated people working diligently, the industry’s lack of transparency makes the security of their products difficult to judge. Open source provides transparency, but depends on the degree of supervision the project owners provide as a hedge against contributors slipping in code that is buggy or even outright malicious. Remarkably, no software company even attempts to distinguish itself by promising higher levels of security or indemnification  in the event of an attack, so as customers we have no such options. Legal, regulatory, and business agreements all provide additional ways of mitigating the uncertainty around trust decisions.</p>
<p>Take trust decisions seriously, but recognize that nobody gets it right 100 percent of the time. The bad news is that these decisions will always be imperfect, because, as the US Securities and Exchange Commission warns us, “past performance does not guarantee future results.” The good news is that people are highly evolved to gauge trust—though it works best face-to-face, decidedly not via digital media—and in the vast majority of cases we do make the right trust decisions, provided we have accurate information and act with intention.  </p>
<h3 id="h2-123456c01-0004"><span epub:type="pagebreak" title="8" id="Page_8"/>Trust Is a Spectrum</h3>
<p class="BodyFirst">Trust is always granted in degrees, and trust assessments always have some uncertainty. At the far end of the spectrum, such as when undergoing major surgery, we may literally entrust our lives to medical professionals, willingly ceding not just control over our bodies but our very consciousness and ability to monitor the operation. In the worst case scenario, if they should fail us and we do not survive, we literally have no recourse whatsoever (legal rights of our estate aside). Everyday trust is much more limited: credit cards have limits to cap the bank’s potential loss on nonpayment, while cars have valet keys so we can limit access to the trunk. </p>
<p>Since trust is a spectrum, a “trust but verify” policy is a useful tool that bridges the gap between full trust and complete distrust. In software, you can achieve this through the combination of authorization and diligent auditing. Typically, this involves a combination of <em>automated auditing</em> (to accurately check a large volume of mostly repetitive activity logs) and <em>manual auditing</em> (spot checking, handling exceptional cases, and having a human in the loop to make final decisions). We’ll cover auditing in more detail later in this chapter.</p>
<h3 id="h2-123456c01-0005">Trust Decisions</h3>
<p class="BodyFirst">In software, you have a binary choice: to trust, or not to trust? While some systems do enforce a variety of permissions on applications, you still need to either allow or disallow each given permission. When in doubt, you can safely err on the side of distrusting, so long as at least one candidate solution reasonably gains your trust. If you are too demanding in your assessments, and no product can gain your trust, then you are stuck with the prospect of  building the component yourself.</p>
<p>Think of making trust decisions as cutting branches off a decision tree that otherwise would be effectively infinite. When you can trust a service or computer to be secure, that saves you the effort of doing deeper analysis. On the other hand, if you are reluctant to trust, then you need to build and secure more parts of the system, including all subcomponents. <a href="#figure1-1" id="figureanchor1-1">Figure 1-1</a> illustrates an example of making a trust decision. If there is no available cloud storage service you would fully trust to store your data, then you must operate the service yourself, and this entails further trust decisions: to use a trusted hosting service or do it yourself, and to use existing database software that you trust or write it yourself. Note that when you don’t trust a provider then more trust decisions are sure to follow since you cannot do everything.</p>
<p>For explicitly distrusted inputs—which should include virtually all inputs, especially anything from the public internet or any client—treat that data with suspicion and the highest levels of care (for more on this, see “Reluctance to Trust” on <span class="xref" itemid="xref_target_page 68">page 68</span> in <span class="xref" itemid="xref_target_Chapter 4">Chapter 4</span>). Even for trusted inputs, it can be risky to assume they are perfectly reliable. Consider opportunistically adding safety checks when it’s easy to do so, if only to reduce the fragility of the overall system and to prevent the propagation of errors in the event of an innocent bug.</p>
<span epub:type="pagebreak" title="9" id="Page_9"/><figure>
<img src="image_fi/501928c01/f01001.png" alt="f01001" class=""/>
<figcaption><p><a id="figure1-1">Figure 1-1</a>: An example of a decision tree with trust decisions</p></figcaption>
</figure>
<h3 id="h2-123456c01-0006">Implicitly Trusted Components</h3>
<p class="BodyFirst">Every software project relies on an extensive stack of technology that is <em>implicitly trusted</em>, including hardware, operating systems, development tools, libraries, and other dependencies that are impractical to vet, so we trust them based on the reputation of the vendor. Nonetheless, you should maintain some sense of what is implicitly trusted, and give these decisions due consideration, especially before greatly expanding the scope of implicit trust.</p>
<p><span epub:type="pagebreak" title="10" id="Page_10"/>There are no simple techniques for managing implicit trust, but here is an idea that can help: minimize the number of parties you trust. For example, if you are already committed to using Microsoft (or Apple, and so forth) operating systems, lean toward using their compilers, libraries, applications, and other products and services, so as to minimize your exposure. The reasoning is roughly that trusting additional companies increases the opportunities for any of these companies to let you down. Additionally, there is the practical aspect that one company’s line of products tend to be more compatible and better tested when used together.</p>
<h3 id="h2-123456c01-0007">Being Trustworthy</h3>
<p class="BodyFirst">Finally, don’t forget the flip side of making trust decisions, which is to <em>promote</em> trust when you offer products and services. Every software product must convince end users that it’s trustworthy. Often, just presenting a solid professional image is all it takes, but if the product is fulfilling critical functions, it’s crucial to give customers a solid basis for that trust. </p>
<p>Here are some suggestions of basic ways to enhance trust in your work:</p>
<ul>
<li>Transparency engenders trust. Working openly allows customers to assess the product.</li>
<li>Involving a third party builds trust through their independence (for example, using hired auditors).</li>
<li>Sometimes your product is the third party that integrates with other products. Trust grows because it’s difficult for two parties with an arm’s-length relationship to collude.</li>
<li>When problems do arise, be open to feedback, act decisively, and publicly disclose the results of any investigation and steps taken to prevent recurrences. </li>
<li>Specific features or design elements can make trust visible—for example, an archive solution that shows in real time how many backups have been saved and verified at distributed locations.</li>
</ul>
<p>Actions beget trust, while empty claims, if anything, erode trust for savvy customers. Provide tangible evidence of being trustworthy, ideally in a way that customers can potentially verify for themselves. Even though few will actually vet the quality of open source code, knowing that they could (and assuming others likely are doing so) is nearly as convincing.</p>
<h2 id="h1-123456c01-0003">Classic Principles</h2>
<p class="BodyFirst">The guiding principles of information security originated in the early days of computing, when computers were emerging from special locked, air-conditioned, raised-floor rooms and starting to be connected in networks. These traditional models are the “Newtonian physics” of modern information security: a good and simple guide for many applications, but not the be-all and end-all. For example, information privacy is one of the more <span epub:type="pagebreak" title="11" id="Page_11"/>nuanced considerations for modern data protection and stewardship that traditional information security principles do not cover. </p>
<p>The foundational principles group nicely into two sets of three. The first three principles, which I will call <em>C-I-A</em>, define data access requirements; the other three, in turn, concern how access is controlled and monitored. We call these the <em>Gold Standard</em>. The two sets of principles are interdependent, and only as a whole do they protect data assets. </p>
<p>Beyond the prevention of unauthorized data access lies the question of who or what components and systems should be entrusted with access. This is a harder question of trust, and ultimately beyond the scope of information security, even though confronting it is unavoidable in order to secure any digital system.</p>
<h3 id="h2-123456c01-0008">Information Security’s C-I-A</h3>
<p class="BodyFirst">We traditionally build software security on three basic principles of information security: <em>confidentiality</em>, <em>integrity</em>, and <em>availability</em>. Formulated around the fundamentals of data protection, the individual meanings of the three pillars are intuitive:</p>
<p class="ListHead"><b>Confidentiality</b></p>
<ol class="none">
<li>Allow only authorized data access—don’t leak information.</li>
</ol>
<p class="ListHead"><b>Integrity</b></p>
<ol class="none">
<li>Maintain data accurately—don’t allow unauthorized modification or deletion.</li>
</ol>
<p class="ListHead"><b>Availability</b></p>
<ol class="none">
<li>Preserve the availability of data—don’t allow significant delays or unauthorized shutdowns.</li>
</ol>
<p>Each of these brief definitions describes the goal and defenses against its subversion. In reviewing designs, it’s often helpful to think of ways one might undermine security, and work back to defensive measures. </p>
<p>All three components of C-I-A represent ideals, and it’s crucial to avoid insisting on perfection. For example, an analysis of even solidly encrypted network traffic could allow a determined eavesdropper to deduce something about the communications between two endpoints, like the volume of data exchanged. Technically, this exchange of data weakens the confidentiality of interaction between the endpoints; but for practical purposes, we can’t fix it without taking extreme measures, and usually the risk is minor enough to be safely ignored. (One way to conceal the fact of communication is for endpoints to always exchange a constant volume of data, adding dummy packets as needed when actual traffic is lower.) What activity corresponds to the traffic, and how might an adversary use that knowledge? The next chapter explains similar threat assessments in detail.</p>
<p>Notice that authorization is inherent in each component of C-I-A, which mandates only the right disclosures, modifications of data, or controls <span epub:type="pagebreak" title="12" id="Page_12"/>of availability. What constitutes “right” is an important detail, and an authorization policy needs to specify that, but it isn’t part of these fundamental data protection primitive concepts. That part of the story will be discussed in “The Gold Standard” starting on <span class="xref" itemid="xref_target_page 14">page 14</span>.</p>
<h4 id="h3-123456c01-0001">Confidentiality</h4>
<p class="BodyFirst">Maintaining confidentiality means disclosing private information in only an authorized manner. This sounds simple, but in practice it involves a number of complexities. </p>
<p>First, it’s important to carefully identify what information to consider private. Design documents should make this distinction clear. While what counts as sensitive might sometimes seem obvious, it’s actually surprising how people’s opinions vary, and without an explicit specification, we risk misunderstanding. The safest assumption is to treat all externally collected information as private by default, until declared otherwise by an explicit policy that explains how and why the designation can be relaxed. </p>
<p>Here are some oft-overlooked reasons to treat data as private:</p>
<ul>
<li>An end user might naturally expect their data to be private, unless informed otherwise, even if revealing it isn’t harmful.</li>
<li>People might enter sensitive information into a text field intended for a different use.</li>
<li>Information collection, handling, and storage might be subject to laws and regulations that many are unaware of. (For example, if Europeans browse your website, it may be subject to EU law, such as the General Data Protection Regulation.)</li>
</ul>
<p>When handling private information, determine what constitutes proper access. Deciding when and how to disclose information is ultimately a trust decision, and it’s worth not only spelling out the rules, but also explaining the subjective choices behind those rules.</p>
<p>Compromises of confidentiality happen on a spectrum. In a complete disclosure, attackers acquire an entire dataset, including metadata. At the lower end of the spectrum might be a minor disclosure of information, such as an internal error message or similar leak of no real consequence. As an example of a partial disclosure, consider the practice of assigning sequential numbers to new customers: a wily competitor can sign up as a new customer and get a new customer number from time to time, then compute the successive differences to learn the numbers of customers acquired during each interval. Any leakage of details about protected data is to some degree a confidentiality compromise. </p>
<p>It’s so easy to underestimate the potential value of minor disclosures. Attackers might put data to use in a completely different way than the developers originally intended, and combining tiny bits of information can provide more powerful insights than any of the individual parts on their own. Learning someone’s ZIP code might not tell you much, but if you also know their approximate age and that they’re an MD, you could perhaps <span epub:type="pagebreak" title="13" id="Page_13"/>combine this information to identify the individual in a sparsely populated area—a process known as <em>deanonymization</em> or <em>reidentification</em>. By analyzing a supposedly anonymized dataset published by Netflix, researchers were able to match numerous user accounts to IMDb accounts: it turns out that your favorite movies are an effective means of unique personal identification.</p>
<h4 id="h3-123456c01-0002">Integrity</h4>
<p class="BodyFirst">Integrity, used in an information security context, is simply the authenticity and accuracy of data, kept safe from unauthorized tampering or removal. In addition to protecting against unauthorized modification, an accurate record of the <em>provenance</em> of data—the original source, and any authorized changes made—can be an important, and stronger, assurance of integrity.</p>
<p>One classic defense against many tampering attacks is to preserve versions of critical data and record their provenance. Simply put, keep good backups. Incremental backups can be excellent mitigations because they’re simple and efficient to put in place and provide a series of snapshots that detail exactly what data changed, and when. However, the need for integrity goes far beyond the protection of data, and often includes ensuring the integrity of components, server logs, software source code and versions, and other forensic information necessary to determine the original source of tampering when problems occur. In addition to limited administrative access controls, secure digests (similar to checksums) and digital signatures are also strong integrity checks, as explained in <span class="xref" itemid="xref_target_Chapter 5">Chapter 5</span>. </p>
<p>Bear in mind that tampering can happen in many different ways, not necessarily by modifying data in storage. For instance, in a web application, tampering might happen on the client side, on the wire between the client and server, by tricking an authorized party into making a change, by modifying a script on the page, or in many other ways. </p>
<h4 id="h3-123456c01-0003">Availability</h4>
<p class="BodyFirst">Attacks on availability are a sad reality of the internet-connected world and can be among the most difficult to defend against. In the simplest cases, the attacker may just send an exceptionally heavy load of traffic to the server, overwhelming it with what looks like valid uses of the service. This principle implies that information is <em>temporarily</em> unavailable; while data that is permanently lost is also unavailable, this is generally considered to be fundamentally a compromise of integrity.</p>
<p>Anonymous denial-of-service (DoS) attacks, often for ransom, threaten any internet service, posing a difficult challenge. To best defend against these attacks, host on large-scale services with infrastructure that stands up to heavy loads, and maintain the flexibility to move infrastructure quickly in the event of problems. Nobody knows how common or costly DoS attacks really are, since many victims resolve these incidents privately. But without a doubt, you should create detailed plans in advance to prepare for such incidents.</p>
<p>Many other kinds of availability threats are possible as well. For a web server, a malformed request that triggers a bug, causing a crash or infinite loop, can devastate its service. Other attacks can also overload the <span epub:type="pagebreak" title="14" id="Page_14"/>storage, computation, or communication capacity of an application, or perhaps use patterns that break the effectiveness of caching, all of which pose serious issues. Unauthorized destruction of software, configuration, or data (even with backup, delays can result) also can adversely impact availability.</p>
<h3 id="h2-123456c01-0009">The Gold Standard</h3>
<p class="BodyFirst">If C-I-A is the goal of secure systems, the Gold Standard describes the means to that end. <em>Aurum</em> is Latin for gold, hence the chemical symbol “Au,” and it just so happens that the three important principles of security enforcement start with those same two letters:</p>
<p class="ListHead"><b>Authentication</b></p>
<ol class="none">
<li>High-assurance determination of the identity of a principal</li>
</ol>
<p class="ListHead"><b>Authorization</b></p>
<ol class="none">
<li>Reliably only allowing an action by an authenticated principal</li>
</ol>
<p class="ListHead"><b>Auditing</b></p>
<ol class="none">
<li>Maintaining a reliable record of actions by principals for inspection</li>
</ol>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p class="BodyFirst">	Jargon alert: because the words are so long and similar, you may encounter the handy abbreviations <em>authN</em> (for authentication) and <em>authZ</em> (for authorization) as short forms that plainly distinguish them.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>A <em>principal</em> is any reliably authenticated entity: a person, business or organization, government entity, application, service, device, or any other agent with the power to act.</p>
<p><em>Authentication</em> is the process of reliably establishing the validity of the principal’s credentials. Systems commonly allow registered users to authenticate by proving that they know the password associated with their user account, but authentication can be much broader. Credentials may be something the principal knows (a password) or possesses (a smart card), or something they are (biometric data); we’ll talk more about credentials in the next section.</p>
<p>Data access for authenticated principals is subject to <em>authorization</em> decisions, either allowing or denying their actions according to prescribed rules. For example, filesystems with access control settings may make certain files read-only for specific users. In a banking system, clerks may record transactions up to a certain amount, but might require a manager to approve larger transactions.</p>
<p>If a service keeps a secure log that accurately records what principals do, including any failed attempts at performing some action, the administrators can perform a subsequent <em>audit</em> to inspect how the system performed and ensure that all actions are proper. Accurate audit logs are an important component of strong security, because they provide a reliable report of actual events. Detailed logs provide a record of what happened, shedding light on exactly what transpired when an unusual or suspicious event takes place. For <span epub:type="pagebreak" title="15" id="Page_15"/>example, if you discover that an important file is gone, the log should ideally provide details of who deleted it and when, providing a starting point for further investigation. </p>
<p>The Gold Standard acts as the enforcement mechanism that protects C-I-A. We defined confidentiality and integrity as protection against <em>unauthorized</em> disclosure or tampering, and availability is also subject to control by an authorized administrator. The only way to truly enforce authorization decisions is if the principals using the system are properly authenticated. Auditing completes the picture by providing a reliable log of who did what and when, subject to regular review for irregularities, and holding the acting parties responsible.</p>
<p>Secure designs should always explicitly separate authentication from authorization, because combining them leads to confusion, and audit trails are clearer when these stages are cleanly divided. These two real-world examples illustrate why the separation is important:</p>
<ul>
<li>“Why did you let that guy into the vault?” “I have no idea, but he looked legit!”</li>
<li>“Why did you let that guy into the vault?” “His ID was valid for ‘Sam Smith’ and he had a written note from the branch manager.”</li>
</ul>
<p>The second response is much more complete than the first, which is of no help at all, other than proving that the guard is a nitwit. If the vault was compromised, the second response would give clear details to investigate: Did the branch manager have authority to grant vault access and write the note? If the guard retained a copy of the ID, then that information helps identify and find Sam Smith. By contrast, if the branch manager’s note had just said, “let the bearer into the vault”—authorization without authentication—investigators would have had little idea what happened or who the intruder was after security was breached. </p>
<h4 id="h3-123456c01-0004">Authentication</h4>
<p class="BodyFirst">An authentication process tests a principal’s claims of identity based on credentials that demonstrate they really are who they claim to be. Or the service might use a stronger form of credentials, such as a digital signature or a challenge, which proves that the principal possesses a private key associated with the identity, which is how browsers authenticate web servers via HTTPS. The digital signature is a better form of authentication because the principal can prove they know the secret without divulging it.</p>
<p>Evidence suitable for authentication falls into the following categories: </p>
<ul>
<li><em>Something you know</em>, like a password</li>
<li><em>Something you have</em>, like a secure token, or in the analog world some kind of certificate, passport, or signed document that is unforgeable</li>
<li><em>Something you are</em>—that is, biometrics (fingerprint, iris pattern, and such) </li>
<li><em>Somewhere you are</em>—your verified location, such as a connection to a private network in a secure facility </li>
</ul>
<p><span epub:type="pagebreak" title="16" id="Page_16"/>Many of these methods are quite fallible. Something you know can be revealed, something you have can be stolen or copied, your location can be manipulated in various ways, and even something you are can potentially be faked (and if it’s compromised, you can’t later change what you are). On top of those concerns, in today’s networked world, authentication almost always happens across a network, making the task more difficult than in-person authentication. On the web, for instance, the browser serves as a trust intermediary, locally authenticating and, only if successful, then passing along cryptographic credentials to the server. Systems commonly use multiple authentication factors to mitigate these concerns, and auditing these frequently is another important backstop. Two weak authentication factors are better than one (but not a lot better).</p>
<p>Before an organization can assign someone credentials, however, it has to address the gnarly question of how to determine a person’s true identity when they join a company, sign up for an account, or call the helpdesk to reinstate access after forgetting their password. </p>
<p>For example, when I joined Google, all of us new employees gathered on a Monday morning opposite several IT admin folks, who checked our passports or other ID against a new employee roster. Only then did they give us our badges and company-issued laptops and have us establish our login passwords. </p>
<p>By checking whether the credentials we provided (our IDs) correctly identified us as the people we purported to be, the IT team confirmed our identities. The security of this identification depended on the integrity of the government-issued IDs and supporting documents (for example, birth certificates) we provided. How accurately were those issued? How difficult would they be to forge, or obtain fraudulently? Ideally, a chain of association from registration at birth would remain intact throughout our lifetimes to uniquely identify each of us authentically. Securely identifying people is challenging largely because the most effective techniques reek of authoritarianism and are socially unacceptable, so to preserve some privacy and freedom, we opt for weaker methods in daily life. The issue of how to determine a person’s true identity is out of scope for this book, which will focus on the Gold Standard, not this harder problem of <em>identity management</em>.</p>
<p>Whenever feasible, rely on existing trustworthy authentication services, and do not reinvent the wheel unnecessarily. Even simple password authentication is quite difficult to do securely, and dealing securely with forgotten passwords is even harder. Generally speaking, the authentication process should examine credentials and provide either a pass or fail response. Avoid indicating partial success, since this could aid an attacker zeroing in on the credentials by trial and error. To mitigate the threat of brute-force guessing, a common strategy is to make authentication inherently computationally heavyweight, or to introduce increasing delay into the process (also see “Avoid Predictability” on <span class="xref" itemid="xref_target_page 61">page 61</span> in <span class="xref" itemid="xref_target_Chapter 4">Chapter 4</span>).</p>
<p>After authenticating the user, the system must find a way to securely bind the identity to the principal. Typically, an authentication module issues a token to the principal that they can use in lieu of full authentication for subsequent requests. The idea is that the principal, via an agent <span epub:type="pagebreak" title="17" id="Page_17"/>such as a web browser, presents the authentication token as shorthand assurance of who they claim to be, creating a <em>secure context</em> for future requests. This context binds the stored token for presentation with future requests on behalf of the authenticated principal. Websites often do this with a secure cookie associated with the browsing session, but there are many different techniques for other kinds of principals and interfaces. </p>
<p>The secure binding of an authenticated identity can be compromised in two fundamentally different ways. The obvious one is where an attacker usurps the victim’s identity. Alternatively, the authenticated principal may collude and try to give away their identity or even foist it off on someone else. An example of the latter case is the sharing of a paid streaming subscription. The web does not afford very good ways of defending against this because the binding is loose and depends on the cooperation of the principal.</p>
<h4 id="h3-123456c01-0005">Authorization</h4>
<p class="BodyFirst">A decision to allow or deny critical actions should be based on the identity of the principal as established by authentication. Systems implement authorization in business logic, an access control list, or some other formal access policy. </p>
<p>Anonymous authorization (that is, authorization without authentication) can be useful in rare circumstances; a real-world example might be possession of the key to a public locker in a busy station. Access restrictions based on time (for example, database access restricted to business hours) are another common example. </p>
<p>A single guard should enforce authorization on a given resource. Authorization code scattered throughout a codebase is a nightmare to maintain and audit. Instead, authorization should rely on a common framework that grants access uniformly. A well-structured design can help the developers get it right. Use one of the many standard authorization models rather than confusing ad hoc logic wherever possible.</p>
<p><em>Role-based access control (RBAC)</em> bridges the connection between authentication and authorization. RBAC grants access based on roles assigned to authenticated principals, simplifying access control with a uniform framework. For example, roles in a bank might include a clerk, manager, loan officer, security guard, financial auditor, and IT administrator. Instead of choosing access privileges for each person individually, RBAC designates one or more roles based on each person’s responsibilities to automatically and uniformly assign them associated privileges. In more advanced models, one person might have multiple roles and explicitly select which role they choose to apply for a given access.</p>
<p>Authorization mechanisms can be much more granular than the simple read/write access control that operating systems traditionally provide. By designing more robust authorization mechanisms, you can strengthen security by limiting access without losing useful functionality. These more advanced authorization models include <em>attribute-based access control (ABAC),</em> <em>policy-based access control (PBAC)</em>, and many more.</p>
<p><span epub:type="pagebreak" title="18" id="Page_18"/>Consider a simple bank teller example to see how fine-grained authorization might tighten up policy: </p>
<p class="ListHead"><b>Rate-limited</b></p>
<ol class="none">
<li>Tellers may do up to 20 transactions per hour, but more would be considered suspicious.</li>
</ol>
<p class="ListHead"><b>Time of day</b></p>
<ol class="none">
<li>Teller transactions must occur during business hours, when clocked in.</li>
</ol>
<p class="ListHead"><b>No self-service</b></p>
<ol class="none">
<li>Tellers are forbidden to do transactions with their personal accounts.</li>
</ol>
<p class="ListHead"><b>Multiple principals</b></p>
<ol class="none">
<li>Teller transactions over $10,000 require separate manager approval (eliminating the risk of one bad actor moving a lot of money at once).</li>
</ol>
<p>Finally, even read-only access may be too high a level for certain data, like passwords. Systems usually check login passwords by comparing digests, which avoids any possibility of leaking the actual plaintext password. The username and password go to a frontend server that computes the digest of the password and passes it to an authentication service, quickly destroying any trace of the plaintext password. The authentication service cannot read the plaintext password from the credentials database, but it can read the digest, which it compares to what the frontend server provided. In this way, it checks the credentials, but the authentication service never has access to any passwords, so even if compromised, the service cannot leak them. Unless the design of interfaces affords these alternatives, they will miss these opportunities to mitigate the possibility of data leakage. We’ll explore this further when we discuss the pattern of “Least Information” on page 57 in <span class="xref" itemid="xref_target_Chapter 4">Chapter 4</span>.</p>
<h4 id="h3-123456c01-0006">Auditing</h4>
<p class="BodyFirst">In order for an organization to audit system activity, the system must produce a reliable log of all events that are critical to maintaining security. These include authentication and authorization events, system startup and shutdown, software updates, administrative accesses, and so forth. Audit logs must also be tamper-resistant, and ideally even difficult for administrators to meddle with, to be considered fully reliable records. Auditing is a critical leg of the Gold Standard, because incidents do happen, and authentication and authorization policies can be flawed. Auditing can also provide necessary oversight to mitigate the risk of inside jobs in which authorized principals betray their trust. </p>
<p>If done properly, audit logs are essential for routine monitoring, measuring system activity level, detecting errors and suspicious activity, and, after an incident, determining when and how an attack actually happened and <span epub:type="pagebreak" title="19" id="Page_19"/>gauging the extent of the damage. Remember that completely protecting a digital system is not simply a matter of correctly enforcing policies; it’s about being a responsible steward of information assets. Auditing ensures that trusted principals acted properly within the broad range of their authority.</p>
<p>In May 2018, Twitter disclosed an embarrassing bug: they had discovered that a code change had inadvertently caused raw login passwords to appear in internal logs. It’s unlikely that this resulted in any abuse, but it certainly hurt customer confidence and should never have happened. Logs should record operational details but not store any actual private information so as to minimize the risk of disclosure, since many members of the technical staff may routinely view the logs. For a detailed treatment of this requirement, see the sample design document in Appendix A detailing a logging tool that addresses just this problem. </p>
<p>The system must also prevent anyone from tampering with the logs to conceal bad acts. If the attacker can modify logs, they’ll just clean out all traces of their activity. For especially sensitive logs at high risk, an independent system under different administrative and operational controls should manage audit logs in order to prevent the perpetrators of inside jobs from covering their own tracks. This is difficult to do completely, but the mere presence of independent oversight often serves as a powerful disincentive to any funny business, just as a modest fence and conspicuous video surveillance camera can be an effective deterrent to trespassing.</p>
<p>Furthermore, any attempt to circumvent the system would seem highly suspicious, and any false move would result in serious repercussions for the offender. Once caught, they would have a hard time repudiating their guilt. </p>
<p><em>Non-repudiability</em> is an important property of audit logs; if the log shows that a named administrator ran a certain command at a certain time and the system crashed immediately, it’s hard to point fingers at others. By contrast, if an organization allowed multiple administrators to share the same account (a terrible idea), it would have no way of definitively knowing who actually did anything, providing plausible deniability to all.</p>
<p>Ultimately, audit logs are useful only if you monitor them, analyze unusual events carefully, and follow up, taking appropriate actions when necessary. To this end, it’s important to log the right amount of detail by following the <em>Goldilocks principle</em>. Too much logging bloats the volume of data to oversee, and excessively noisy or disorganized logs make it difficult to glean useful information. On the other hand, sparse logging with insufficient detail might omit critical information, so finding the right balance is an ongoing challenge. </p>
<h3 id="h2-123456c01-0010">Privacy</h3>
<p class="BodyFirst">In addition to the foundations of information security—C-I-A and the Gold Standard—another fundamental topic I want to introduce is the related field of information privacy. The boundaries between security and privacy are difficult to clearly define, and they are at once closely related and quite different. In this book I would like to focus on the common points of <span epub:type="pagebreak" title="20" id="Page_20"/>intersection, not to attempt to unify them, but to incorporate both security and privacy into the process of building software.</p>
<p>To respect people’s digital information privacy, we must extend the principle of confidentiality by taking into account additional human factors, including:</p>
<ul>
<li>Customer expectations regarding information collection and use</li>
<li>Clear policies regarding appropriate information use and disclosure </li>
<li>Legal and regulatory issues relating to the collection and use of various classes of information</li>
<li>Political, cultural, and psychological aspects of processing personal information</li>
</ul>
<p>As software becomes more pervasive in modern life, people use it in more intimate ways involving sensitive areas of their lives, resulting in many complex issues. Past accidents and abuses have raised the visibility of the risks, and as society grapples with new challenges through political and legal means, handling private information properly has become challenging. </p>
<p>In the context of software security, this means:</p>
<ul>
<li>Considering the customer and stakeholder consequences of all data collection and sharing</li>
<li>Flagging all potential issues, and getting expert advice where necessary</li>
<li>Establishing and following clear policies and guidelines regarding private information use</li>
<li>Translating policy and guidance into software-enforced checks and balances</li>
<li>Maintaining accurate records of data acquisition, use, sharing, and deletion</li>
<li>Auditing data access authorizations and extraordinary access for compliance</li>
</ul>
<p>Privacy work tends to be less well-defined than the relatively cut-and-dried security work of maintaining proper control of systems and providing appropriate access. Also, we’re still working out privacy expectations and norms as society ventures deeper into a future with more data collection. Given these challenges, you would be wise to consider maximal transparency about data use, including keeping your policies simple enough to be understood by all, and to collect minimal data, especially personally identifiable information.</p>
<p>Collect information for a specific purpose only, and retain it only as long as it’s useful. Unless the design envisions an authorized use, avoid collection in the first place. Frivolously collecting data for use “someday” is risky, and almost never a good idea. When the last authorized use of some data becomes unnecessary, the best protection is secure deletion. For especially sensitive data, or for maximal privacy protection, make that <span epub:type="pagebreak" title="21" id="Page_21"/>even stronger: delete data when the potential risk of disclosure exceeds the potential value of retaining it. Retaining many years’ worth of emails might occasionally be handy for something, but probably not for any clear business need. Yet internal emails could represent a liability if leaked or disclosed, such as by power of subpoena. Rather than hang onto all that data indefinitely “just in case,” the best policy is usually to delete it.</p>
<p>A complete treatment of information privacy is outside the scope of this book, but privacy and security are tightly bound facets of the design of any system that collects data about people—and people interact with almost all digital systems, in one way or another. Strong privacy protection is only possible when security is solid, so these words are an appeal for awareness to consider and incorporate privacy considerations into software by design.</p>
<p>For all its complexity, one best practice for privacy is well known: the necessity of clearly communicating privacy expectations. In contrast to security, a privacy policy potentially affords a lot of leeway as to how much an information service does or does not want to leverage the use of customer data. “We will reuse and sell your data” is one extreme of the privacy spectrum, but “some days we may not protect your data” is not a viable stance on security. Privacy failures arise when user expectations are out of joint with actual privacy policy, or when there is a clear policy and it is somehow violated. The former problem stems from not proactively explaining data handling to the user. The latter happens when the policy is unclear, or ignored by responsible staff, or subverted in a security breakdown.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p class="BodyFirst">	See Appendix D for a cheat sheet summarizing the C-I-A and Gold Standard principles.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
</section>


<section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" title="23" id="Page_23"/>2</span><br/>
<span class="ChapterTitle">Threats</span></h1>
</header>
<blockquote class="Epigraph" epub:type="epigraph">
<p class="Epigraph">The threat is usually more terrifying than the thing itself.</p>
<p class="EpigraphSource">—Saul Alinsky</p>
</blockquote>
<figure class="opener">
<img src="image_fi/book_art/chapterart.png" alt=""/>
</figure>
<p class="ChapterIntro">Threats are omnipresent, but you can live with them if you manage them. Software is no different, except that you don’t have the benefit of millions of years of evolution to prepare yourself. That is why you need to adopt a software security mindset, which requires you to flip from the builder’s perspective to that of the attackers. Understanding the potential threats to a system is the essential starting point in order to bake solid defenses and mitigations into your software designs. But to perceive these threats in the first place, you’ll have to stop thinking about typical use cases and using the software as intended. Instead, you must simply see it for what it is: a bunch of code and components, with data flowing around and getting stored here and there. </p>
<p>For example, consider the paperclip: it’s cleverly designed to hold sheets of paper together, but if you bend a paperclip just right, it’s easily refashioned into a stiff wire. A security mindset discerns that you could insert <span epub:type="pagebreak" title="24" id="Page_24"/>this wire into the keyhole of a lock to manipulate the tumblers and open it without the key. </p>
<p>It’s worth emphasizing that threats include all manner of ways in which harm occurs. Adversarial attacks conducted with intention are an important focus of the discussion, but this does not mean that you should exclude other threats due to software bugs, human error, accidents, hardware failures, and so on.</p>
<p>Threat modeling provides a perspective with which to guide any decisions that impact security throughout the software development process. The following treatment focuses on concepts and principles, rather than any of the many specific methodologies for doing threat modeling. Early threat modeling as first practiced at Microsoft in the early 2000s proved effective, but it required extensive training, as well as a considerable investment of effort. Fortunately, you can do threat modeling in any number of ways, and once you understand the concepts, it’s easy to tailor your process to fit the time and effort available while still producing meaningful results.</p>
<p>Setting out to enumerate all the threats and identify all the points of vulnerability in a large software system is a daunting task. However, smart security work targets incrementally raising the bar, not shooting for perfection. Your first efforts may only find a fraction of all the potential issues, and only mitigate some of those: even so, that’s a substantial improvement. Such an effort may just possibly avert a major security incident—a real accomplishment. Unfortunately, you almost never know about foiled attacks, and that absence of feedback can feel disappointing. The more you flex your security mindset muscles, the better you’ll become at seeing threats.</p>
<p>Finally, it’s important to understand that threat modeling can provide new levels of understanding of the target system beyond the scope of security. Through the process of examining the software in new ways, you may gain insights that suggest various improvements, efficiencies, simplifications, and new features unrelated to security.</p>
<h2 id="h1-123456c01-0001">The Adversarial Perspective</h2>
<blockquote class="Quote">
<p class="QuotePara">Exploits are the closest thing to “magic spells” we experience in the real world: Construct the right incantation, gain remote control over device. </p>
<p class="QuoteSource">—Halvar Flake</p>
</blockquote>
<p class="BodyFirst">Human perpetrators are the ultimate threat; security incidents don’t just happen by themselves. Any concerted analysis of software security includes considering what hypothetical adversaries might try in order to anticipate and defend against potential attacks. Attackers are a motley group, from <em>script kiddies</em> (criminals without tech skills using automated malware) to sophisticated nation-state actors, and everything in between. To the extent you can think from an adversary’s perspective, that’s great, but don’t fool yourself into believing you can accurately predict their every move or spend too much time trying to get inside their heads, like a master sleuth outsmarting a wily foe. It’s helpful to understand the attacker’s mindset, but for our <span epub:type="pagebreak" title="25" id="Page_25"/>purposes of building secure software, the details of actual techniques they might use to probe, penetrate, and exfiltrate data are unimportant. </p>
<p>Consider what the obvious targets within a system might be (sometimes, what’s valuable to an adversary is less valuable to you, or vice versa) and ensure that those assets are robustly secured, but don’t waste time attempting to read the minds of hypothetical attackers. Rather than expend unnecessary effort, they’ll often focus on the weakest link to accomplish their goal (or they might be poking around aimlessly, which can be very hard to defend against since their actions will seem undirected and arbitrary). Bugs definitely attract attention because they suggest weakness, and attackers who stumble onto an apparent bug will try creative variations to see if they can really bust something. Errors or side effects that disclose details of the insides of the system (for example, detailed stack dumps) are prime fodder for attackers to jump on and run with.</p>
<p>Once attackers find a weakness, they’re likely to focus more effort on it, because some small flaws have a way of expanding to produce larger consequences under concerted attack (as we shall see in Chapter 8 in detail). Often, it’s possible to combine two tiny flaws that are of no concern individually to produce a major attack, so it’s wise to take all vulnerabilities seriously. Skilled attackers definitely know about threat modeling, though they are working without inside information (at least until they manage some degree of penetration).</p>
<p>Even though we can never really anticipate what our adversaries will spend time on, it does make sense to consider the motivation of hypothetical attackers as a measure of the likelihood of diligent attacks. Basically, this amounts to a famous criminal’s explanation of why he robbed banks: “Because that’s where the money is.” The point is, the greater the prospective gain from attacking a system, the higher the level of skill and resources you can expect potential attackers to apply. Speculative as this might be, the analysis is useful as a relative guide: powerful corporations and government, military, and financial institutions are big targets. Your cat photos are not.</p>
<p>In the end, as with all forms of violence, it’s always far easier to attack and cause harm than to defend. Attackers get to choose their point of entry, and with determination they can try as many exploits as they like, because they only need to succeed once. All this amounts to more reasons why it’s important to prioritize security work: the defenders need every advantage available.</p>
<h2 id="h1-123456c01-0002">The Four Questions</h2>
<p class="BodyFirst">Adam Shostack, who carried the threat modeling torch at Microsoft for years, boils the methodology down to Four Questions: </p>
<ul>
<li>What are we working on?</li>
<li>What can go wrong?</li>
<li>What are we going to do about it?</li>
<li>Did we do a good job?</li>
</ul>
<p><span epub:type="pagebreak" title="26" id="Page_26"/>The first question aims to establish the project’s context and scope. Answering it includes describing the project’s requirements and design, its components and their interactions, as well as considering operational issues and use cases. Next, at the core of the method, the second question attempts to anticipate potential problems, while the third question explores mitigations to those problems we identify. (We’ll look more closely at mitigations in <span class="xref" itemid="xref_target_Chapter 3">Chapter 3</span>, but first we will examine how they relate to threats.) Finally, the last question asks us to reflect on the entire process—what the software does, how it can go wrong, and how well we’ve mitigated the threats—in order to assess the risk reduction and confirm that the system will be sufficiently secure. Should unresolved issues remain, we go through the questions again to fill in the remaining gaps.</p>
<p>There is much more to threat modeling than this, but it’s surprising how far simply working from the Four Questions can take you. Armed with these concepts, in conjunction with the other ideas and techniques in this book, you can significantly raise the security bar for the systems you build and operate. </p>
<h2 id="h1-123456c01-0003">Threat Modeling</h2>
<p class="BodyFirst">“What could possibly go wrong?” </p>
<p>We often ask this question to make a cynical joke. But when asked unironically, it succinctly expresses the point of departure for threat modeling. Responding to this question requires us to identify and assess threats; we can then prioritize these and work on mitigations that reduce the risk of the important threats. </p>
<p>Let’s unpack that previous sentence. The following steps outline the basic threat modeling process: </p>
<ol class="decimal">
<li value="1">Work from a model of the system to ensure that we consider everything in scope.</li>
<li value="2">Identify <em>assets</em> (valuable data and resources) within the system that need protection.</li>
<li value="3">Scour the system model for potential threats, component by component, identifying <em>attack surface</em><em>s</em> (places where an attack could originate), <em>trust boundaries </em>(interfaces bridging more-trusted parts of the system with the less-trusted parts), and different types of threats.</li>
<li value="4">Analyze these potential threats, from the most concrete to the hypothetical.</li>
<li value="5">Rank the threats, working from the most to least critical.</li>
<li value="6">Propose mitigations to reduce risk for the most critical threats.</li>
<li value="7">Add mitigations, starting from the most impactful and easiest, and working up to the point of diminishing returns.</li>
<li value="8">Test the efficacy of the mitigations, starting with those for the most critical threats.</li>
</ol>
<p><span epub:type="pagebreak" title="27" id="Page_27"/>For complex systems, a complete inventory of all potential threats will be enormous, and a full analysis is almost certainly infeasible (just as enumerating every conceivable way of doing anything would never end if you got imaginative, which attackers often do). In practice, the first threat modeling pass should focus on the biggest and most likely threats to the high-value assets only. Once you’ve understood those threats and put first-line mitigations in place, you can evaluate the remaining risk by iteratively considering the remaining lesser threats that you’ve already identified. From that point, you can perform one or more additional threat modeling passes as needed, casting a wider net each time to include additional assets, deeper analysis, and more of the less likely or minor threats. The process stops when you’ve achieved a sufficiently thorough understanding of the most important threats, planned the necessary mitigations, and deemed the remaining known risk acceptable.</p>
<p>People intuitively do something akin to threat modeling in daily life, taking what we call common-sense precautions. To send a private message in a public place, most people type it instead of dictating it aloud to their phones. Using the language of threat modeling, we’d say the message content is the information asset, and disclosure is the threat. Speaking within earshot of others is the attack surface, and using a silent, alternative input method is a good mitigation. If a nosy stranger is watching, you could add an additional mitigation, like cupping the phone with your other hand to shield the screen from view. But while we do this sort of thing all the time quite naturally in the real world, applying these same techniques to complex software systems, where our familiar physical intuitions don’t apply, requires much more discipline.</p>
<h3 id="h2-123456c01-0001">Work from a Model</h3>
<p class="BodyFirst">You’ll need a rigorous approach in order to thoroughly identify threats. Traditionally, threat modeling uses data flow diagrams (DFDs) or Unified Modeling Language (UML) descriptions of the system, but you can use whatever model you like. Whatever high-level description of the system you choose, be it a DFD, UML, a design document, or an informal “whiteboard session,” the idea is to look at an abstraction of the system, so long as it has enough granularity to capture the detail you need for analysis.</p>
<p>More formalized approaches tend to be more rigorous and produce more accurate results, but at the cost of additional time and effort. Over the years, the security community has invented a number of alternative methodologies that offer different trade-offs, in no small part because the full-blown threat modeling method (involving formal models like DFDs) is so costly and effort-intensive. Today, you can use specialized software to help with the process. The best ones automate significant parts of the work, although interpreting the results and making risk assessments will always require human judgment. This book tells you all you need to know in order to threat model on your own, without special diagrams or tools, so long as you understand the system well enough to thoroughly answer the Four Questions. You can work toward more advanced forms from there as you like.</p>
<p><span epub:type="pagebreak" title="28" id="Page_28"/>Whatever model you work from, thoroughly cover the target system at the appropriate resolution. Choose the appropriate level of detail for the analysis by the Goldilocks principle: don’t attempt too much detail or the work will be endless, and don’t go too high-level or you’ll omit important details. Completing the process quickly with little to show for it is a sure sign of insufficient granularity, just as making little headway after hours of work indicates your model may be too granular.</p>
<p>Let’s consider what the right level of granularity would be for a generic web server. You’re handed a model consisting of a block diagram showing “the internet” on the left, connected to a “frontend server” in the center, with a third component, “database,” on the right. This isn’t helpful, because nearly every web application ever devised fits this model. All the assets are presumably in the database, but what exactly are they? There must be a trust boundary between the system and the internet, but is that the only one? Clearly, this model operates at too high a level. At the other extreme would be a model showing a detailed breakdown of every library, all the dependencies of the framework, and the relationships of components far below the level of the application you want to analyze.</p>
<p>The Goldilocks version would fall somewhere between these extremes. The data stored in the database (assets) would be clumped into categories, each of which you could treat as a whole: say, customer data, inventory data, and system logs. The server component would be broken into parts granular enough to reveal multiple processes, including what privilege each runs at, perhaps an internal cache on the host machine, and descriptions of the communication channels and network used to talk to the internet and the database.  </p>
<h3 id="h2-123456c01-0002">Identify Assets</h3>
<p class="BodyFirst">Working methodically through the model, identify assets and the potential threats to them. Assets are the entities in the system that you must protect. Most assets are data, but they could also include hardware, communication bandwidth, computational capacity, and physical resources, such as electricity. </p>
<p>Beginners at threat modeling naturally want to protect everything, which would be great in a perfect world. But in practice, you’ll need to prioritize your assets. For example, consider any web application: anyone on the internet can access it using browsers or other software that you have no control over, so it’s impossible to fully protect the client side. Also, you should always keep internal system logs private, but if the logs contain harmless details of no value to outsiders, it doesn’t make sense to invest much energy in protecting them. This doesn’t mean that you ignore such risks completely; just make sure that less important mitigations don’t take away effort needed elsewhere. For example, it literally takes a minute to protect non-sensitive logs by setting permissions so that only administrators can read the contents, so that’s effort well spent. </p>
<p>On the other hand, you could effectively treat data representing financial transactions as real money and prioritize it accordingly. Personal information is another increasingly sensitive category of asset, because knowledge of a <span epub:type="pagebreak" title="29" id="Page_29"/>person’s location or other identifying details can compromise their privacy or even put them at risk. </p>
<p>Also, I generally advise against attempting to perform complex risk-assessment calculations. For example, avoid attempting to assign dollar values for the purpose of risk ranking. To do this, you would have to somehow come up with probabilities for many unknowables. How many attackers will target you, and how hard will they try, and to do what? How often will they succeed, and to what degree? How much money is the customer database even worth? (Note that its value to the company and the amount an attacker could sell it for often differ, as might the value that users would assign to their own data.) How many hours of work and other expenses will a hypothetical security incident incur? </p>
<p>Instead, a simple way to prioritize assets that’s surprisingly effective is to rank them by “T-shirt sizes”—a simplification that I find useful, though it’s not a standard industry practice. Assign “Large” to major assets you definitely protect, “Medium” to valuable assets that are less critical, and “Small” to lesser ones of minor consequence (usually not even listed). High-value systems may have “Extra-Large” assets that deserve extraordinary levels of protection, such as bank account balances at a financial institution, or private encryption keys that anchor the security of communications. In this simple scheme, protection and mitigation efforts focus first on Large assets, and then <em>opportunistically</em> on Medium ones. Opportunistic protection consists of low-effort work that has little downside. But even if you can secure Small assets very opportunistically, defend all Large assets before spending any time on these. <span class="xref" itemid="xref_target_Chapter 13">Chapter 13</span> discusses ranking vulnerabilities in detail, and much of that is applicable to threat assessment as well.</p>
<p>The assets you choose to prioritize should probably include data such as customer resources, personal information, business documents, operational logs, and software internals, to name just a few possibilities. Prioritizing protection of data assets considers many factors, including information security (the C-I-A triad discussed in <span class="xref" itemid="xref_target_Chapter 1">Chapter 1</span>), because the harms of leaking, modification, and destruction of data may differ greatly. Information leaks, including partial disclosures of information (for example, the last four digits of a credit card number), are tricky to evaluate, because you must consider what an attacker could do with the information. Analysis becomes harder still when an attacker could join multiple shards of information into an approximation of the complete dataset. </p>
<p>If you lump assets together, you can simplify the analysis considerably, but beware of losing resolution in the process. For example, if you administer several of your databases together, grant access similarly, use them for data that originates from similar sources, and store them in the same location, treating them as one makes good sense. However, if any of these factors differs significantly, you would have sufficient reason to handle them separately. Make sure to consider these distinctions in your risk analysis, as well as for mitigation purposes.</p>
<p>Finally, always consider the value of assets from the perspectives of all parties involved. For instance, social media services manage all kinds of data: internal company plans, advertising data, and customer data. The <span epub:type="pagebreak" title="30" id="Page_30"/>value of each of these assets differs depending on if you are the company’s CEO, an advertiser, a customer, or perhaps an attacker seeking financial gain or pursuing a political agenda. In fact, even among customers you’ll likely find great differences in how they perceive the importance of privacy in their communications, or the value they place on their data. Good data stewardship principles suggest that your protection of customer and partner data should arguably exceed that of the company’s own proprietary data (and I have heard of company executives actually stating this as policy). </p>
<p>Not all companies take this approach. Facebook’s Beacon feature automatically posted the details of users’ purchases to their news feeds, then quickly shut down following an immediate outpouring of customer outrage and some lawsuits. While Beacon never endangered Facebook (except by damaging the brand’s reputation), it posed a real danger to customers. Threat modeling the consequences of information disclosure for customers would have quickly revealed that the unintended disclosure of purchases of Christmas or birthday presents, or worse, engagement rings, was likely to prove problematic.</p>
<h3 id="h2-123456c01-0003">Identify Attack Surfaces</h3>
<p class="BodyFirst">Pay special attention to attack surfaces, because these are the attacker’s first point of entry. You should consider any opportunity to minimize the attack surface a big win, because doing so shuts off a potential source of trouble entirely. Many attacks potentially fan out across the system, so stopping them early can be a great defense. This is why secure government buildings have checkpoints with metal detectors just inside the single public entrance. </p>
<p>Software design is typically much more complex than the design of a physical building, so identifying the entire attack surface is not so simple. Unless you can embed a system in a trusted, secure environment, having some attack surface is inevitable. The internet always provides a huge point of exposure, since literally anyone anywhere can anonymously connect through it. While it might be tempting to consider an <em>intranet</em> (a private network) as trusted, you probably shouldn’t, unless it has very high standards of both physical and IT security. At the very least, treat it as an attack surface with reduced risk. For devices or kiosk applications, consider the outside portion of the box, including screens and user interface buttons, an attack surface. </p>
<p>Note that attack surfaces exist outside the digital realm. Consider the kiosk, for example: a display in a public area could leak information via “shoulder surfing.” An attacker could also perform even subtler <em>side-channel attacks </em>to deduce information about the internal state of a system by monitoring its electromagnetic emissions, heat, power consumption, keyboard sounds, and so forth.</p>
<h3 id="h2-123456c01-0004">Identify Trust Boundaries</h3>
<p class="BodyFirst">Next, identify the system’s trust boundaries. Since trust and privilege are almost always paired, you can think in terms of privilege boundaries if that makes more sense. Human analogs of trust boundaries might be the <span epub:type="pagebreak" title="31" id="Page_31"/>interface between a manager (who is privy to more internal information) and an employee, or the door of your house, where you choose whom to let inside.</p>
<p>Consider a classic example of a trust boundary: an operating system’s kernel-userland interface. This architecture became popular in a time when mainframe computers were the norm and machines were often shared by many users. The system booted up the kernel, which isolated applications in different userland process instances (corresponding to different user accounts) from interfering with each other or crashing the whole system. Whenever userland code calls into the kernel, execution crosses a trust boundary. Trust boundaries are important, because the transition into higher-privilege execution is an opportunity for bigger trouble.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="box">
<h2>Trust Versus Privilege</h2>
<p class="BoxBodyFirst">In this book I’ll be talking about <em>high </em>and <em>low privilege</em> as well as <em>high</em> and <em>low trust</em>, and there is great potential for confusion since they are very closely related and difficult to separate cleanly. The inherent character of trust and privilege is such that they almost invariably correlate: where trust is high, privilege is also usually high, and vice versa. Beyond the scope of this book, it’s common for people to use these expressions (trust versus privilege) interchangeably, and generously interpreting them however makes best sense to you without insisting on correcting others is usually the best practice.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>The SSH secure shell daemon (<code>sshd(8)</code>) is a great example of secure design with trust boundaries. The SSH protocol allows authorized users to remotely log in to a host, then run a shell via a secure network channel over the internet. But the SSH daemon, which persistently listens for connections to initiate the protocol, requires very careful design because it crosses a trust boundary. The listener process typically needs superuser privileges, because when an authorized user presents valid credentials, it must be able to create processes for any user. Yet it must also listen to the public internet, exposing it to the world for attack. </p>
<p>To accept SSH login requests, the daemon must generate a secure channel for communication that’s impervious to snooping or tampering, then handle and validate sensitive credentials. Only then can it instantiate a shell process on the host computer with the right privileges. This entire process involves a lot of code, running with the highest level of privilege (so it can create a process for any user account), that must operate perfectly or risk deeply compromising the system. Incoming requests can come from anywhere on the internet and are initially indistinguishable from attacks, so it’s hard to imagine a more attractive target with higher stakes. </p>
<p>Given the large attack surface and the severity of any vulnerability, extensive efforts to mitigate risk are justified for the daemon process. <a href="#figure2-1" id="figureanchor2-1">Figure 2-1</a> shows a simplified view of how it is designed to protect this critical trust boundary. </p>
<span epub:type="pagebreak" title="32" id="Page_32"/><figure>
<img src="image_fi/501928c02/f02001.png" alt="f02001" class=""/>
<figcaption><p><a id="figure2-1">Figure 2-1</a>: How the design of the SSH daemon protects critical trust boundaries</p></figcaption>
</figure>
<p>Working from the top, each incoming connection forks a low-privilege child process, which listens on the socket and communicates with the parent (superuser) process. This child process also sets up the protocol’s complex secure-channel encryption and accepts login credentials that it passes to the privileged parent, which decides whether or not to trust the incoming request and grant it a shell. Forking a new child process for each request provides a strategic protection on the trust boundary; it isolates as much of the work as possible, and also minimizes the risk of unintentional side effects building up within the main daemon process. When a user successfully logs in, the daemon creates a new shell process with the privileges of the authenticated user account. When a login attempt fails to authenticate, the child process that handled the request terminates, so it can’t adversely affect the system in the future.</p>
<p>As with assets, you’ll decide when to lump together or split trust levels. In an operating system, the superuser is, of course, the highest level of trust, and some other administrative users may be close enough that you should consider them to be just as privileged. Authorized users typically rank next on the totem pole of trust. Some users may form a more trusted group with special privileges, but usually, there is no need to decide who you trust a little or more or less among them. Guest accounts typically rank lowest in trust, and you should probably emphasize protecting the system from them, rather than protecting their resources.</p>
<p>Web services need to resist malicious client users, so web frontend systems may validate incoming traffic and only forward well-formed requests for service, in effect straddling the trust boundary to the internet. Web servers often connect to more trusted databases and microservices behind a firewall. If money is involved (say, in a credit card processing service), a dedicated high-trust system should handle payments, ideally isolated in a fenced-off area of the datacenter. Authenticated users should be trusted to access their own account data, but you should treat them as very much untrusted beyond that, since anyone can typically create a login. Anonymous public web access represents an even lower trust level, and static public content could be served by machines unconnected to any private data services.</p>
<p>Always conduct transitions across trust boundaries through well-defined interfaces and protocols. You can think of these as analogous to checkpoints <span epub:type="pagebreak" title="33" id="Page_33"/>staffed by armed guards at international frontiers and ports of entry. Just as the border control agents ask for your passport (a form of authentication) and inspect your belongings (a form of input validation), you should treat the trust boundary as a rich opportunity to mitigate potential attacks. </p>
<p>The biggest risks usually hide in low-to-high trust transitions, like the SSH listener example, for obvious reasons. However, this doesn’t mean you should ignore high-to-low trust transitions. Any time your system passes data to a less-trusted component, it’s worth considering if you’re disclosing information, and if doing so might be a problem. For example, even low-privilege processes can read the hostname of the computer they are running in, so don’t name machines using sensitive information that might give attackers a hint if they attain a beachhead and get code running on the system. Additionally, whenever high-trust services work on behalf of low-trust requests, you risk a DoS attack if the userland requester manages to overtax the kernel. </p>
<h3 id="h2-123456c01-0005">Identify Threats</h3>
<p class="BodyFirst">Now we begin the work at the heart of threat modeling: identifying potential threats. Working from your model, pore over the parts of the system. The threats tend to cluster around assets and at trust boundaries, but could potentially lurk anywhere. </p>
<p>I recommend starting with a rough pass (say, from a 10,000-foot view of the system), then coming back later for a more thorough examination (at 1,000 feet) of the more fruitful or interesting parts. Keep an open mind, and be sure to include possibilities even if you cannot yet see exactly how to exploit them. </p>
<p>Identifying direct threats to your assets should be easy, as well as threats at trust boundaries, where attackers might easily trick trusted components into doing their bidding. Many examples of such threats in specific situations are given throughout this book. Yet you might also find threats that are indirect, perhaps because there is no asset immediately available to harm, or a trust boundary to cross. Don’t immediately disregard these without considering how such threats might work as part of a chain of events—think of them as bank shots in billiards, or stepping stones that form a path. In order to do damage, an attacker would have to combine multiple indirect threats; or perhaps, paired with bugs or poorly designed functionality, the indirect threats afford openings that give attackers a foot in the door. Even lesser threats might be worth mitigating, depending on how promising they look and how critical the asset at risk may be.</p>
<h4 id="h3-123456c01-0001">A Bank Vault Example</h4>
<p class="BodyFirst">So far, these concepts may still seem rather abstract, so let’s look at them in context by threat modeling an imaginary bank vault. While reading this walkthrough, focus on the concepts, and if you are paying attention, you should be able to expand on the points I raise (which, intentionally, are not exhaustive).  </p>
<p>Picture a bank office in your hometown. Say it’s an older building, with impressive Roman columns framing the heavy solid oak double doors in front. <span epub:type="pagebreak" title="34" id="Page_34"/>Built back when labor and materials were inexpensive, the thick, reinforced concrete walls appear impenetrable. For the purpose of this example, let’s focus solely on the large stock of gold stored in the secure vault at the heart of the bank building: this is the major asset we want to protect. We’ll use the building’s architectural drawings as the model, working from a floor plan with a 10-foot to 1-inch scale that provides an overview of the entire building’s layout. </p>
<p>The major trust boundary is clearly at the vault door, but there’s another one at the locked door to the employee-only area behind the counter, and a third at the bank’s front door that separates the customer lobby from the exterior. For simplicity, we’ll omit the back door from the model because it’s very securely locked at all times and only opened rarely, when guards are present. This leaves the front door and easily-accessible customer lobby areas as the only significant attack surfaces.</p>
<p>All of this sets the stage for the real work of finding potential threats. Obviously, having the gold stolen is the top threat, but that’s too vague to provide much insight into how to prevent it, so we continue looking for specifics. The attackers would need to gain unauthorized access to the vault in order to steal the gold. In order to do that, they’d need unauthorized access to the employee-only area where the vault is located. So far, we don’t know <em>how</em> such abstract threats could occur, but we can break them down and get more specific. Here are just a few potential threats: </p>
<ul>
<li>Observe the vault combination covertly.</li>
<li>Guess the vault combination.</li>
<li>Impersonate the bank’s president with makeup and a wig.</li>
</ul>
<p>Admittedly, these made-up threats are fairly silly, but notice how we developed them from a model, and how we transitioned from abstract threats to concrete ones. </p>
<p>In a more detailed second pass, we now use a model that includes full architectural drawings, the electrical and plumbing layout, and vault design specifications. Armed with more detail, we can imagine specific attacks more easily. Take the first threat we just listed: the attacker observing the vault combination. This could happen in several ways. Let’s look at three of them:</p>
<ul>
<li>An eagle-eyed robber loiters in the lobby to observe the opening of the vault.</li>
<li>The vault combination is on a sticky note, visible to a customer at the counter.</li>
<li>A confederate across the street can watch the vault combination dial through a scope.</li>
</ul>
<p>Naturally, just knowing the vault combination does not get the intruders any gold. An outsider learning the combination is a major threat, but it’s just one part of a complete attack that must include entering the employee-only area, entering the vault, and then escaping with the gold.</p>
<p><span epub:type="pagebreak" title="35" id="Page_35"/>Now we can prioritize the enumerated threats and propose mitigations. Here are some straightforward mitigations to each potential attack we’ve identified:</p>
<ul>
<li>Lobby loiterer: put an opaque screen in front of the vault.</li>
<li>Sticky-note leak: institute a policy prohibiting unsecured written copies.</li>
<li>Scope spy: install opaque, translucent glass windows.</li>
</ul>
<p>These are just a few of the many possible defensive mitigations. If these types of attacks had been considered during the building’s design, perhaps the layout could have eliminated some of these threats in the first place (for example, by ensuring there was no direct line of sight from any exterior window to the vault area, avoiding the need to retrofit opaque glass).</p>
<p>Real bank security and financial risk management are of course far more complex, but this simplified example shows how the threat modeling process works, including how it propels analysis forward. Gold in a vault is about as simple an asset as it gets, but now you should be wondering, how exactly does one examine a model of a complex software system to be able to see the threats it faces? </p>
<h4 id="h3-123456c01-0002">Categorizing Threats with STRIDE</h4>
<p class="BodyFirst">In the late 1990s, Microsoft Windows dominated the personal computing landscape. As PCs became essential tools for both businesses and homes, many believed the company’s sales would grow endlessly. But Microsoft had only begun to figure out how networking should work. The Internet (back then still usually spelled with a capital I) and this new thing called the World Wide Web were rapidly gaining popularity, and Microsoft’s Internet Explorer web browser had aggressively gained market share from the pioneering Netscape Navigator. Now the company faced this new problem of security: Who knew what can of worms connecting all the world’s computers might open up?</p>
<p>While a team of Microsoft testers worked creatively to find security flaws, the rest of the world appeared to be finding these flaws much faster. After a couple of years of reactive behavior, issuing patches for vulnerabilities that exposed customers over the network, the company formed a task force to get ahead of the curve. As part of this effort, I co-authored a paper with Praerit Garg that described a simple methodology to help developers see security flaws in their own products. Threat modeling based on the <em>STRIDE </em><em>threat taxonomy</em> drove a massive education effort across all the company’s product groups. More than 20 years later, researchers across the industry continue to use STRIDE and many independent derivatives to enumerate threats.  </p>
<p>STRIDE focuses the process of identifying threats by giving you a checklist of specific kinds of threats to consider: What can be <em>spoofed (S)</em>, <em>tampered (T)</em> with, or <em>repudiated (R)</em>? What <em>information (I) </em>can be disclosed? How could a <em>denial of service (D)</em> or <em>elevation of privilege (E)</em> happen? These categories are specific enough to focus your analysis, yet general enough that you can mentally flesh out details relevant to a particular design and dig in from there. </p>
<p><span epub:type="pagebreak" title="36" id="Page_36"/>Though members of the security community often refer to STRIDE as a threat modeling methodology, this is a misuse of the term (to my mind, at least, as the one who concocted the acronym). STRIDE is simply a taxonomy of threats to software. The acronym provides an easy and memorable mnemonic to ensure that you haven’t overlooked any category of threat. It’s not a complete threat modeling methodology, which would have to include the many other components we’ve already explored in this chapter.</p>
<p>To see how STRIDE works, let’s start with spoofing. Looking through the model, component by component, consider how secure operation depends on the identity of the user (or machine, or digital signature on code, and so on). What advantages might an attacker gain if they could spoof identity here? This thinking should give you lots of possible threads to pull on. By approaching each component in the context of the model from a threat perspective, you can more easily set aside thoughts of how it should work, and instead begin to perceive how it might be abused.</p>
<p>Here’s a great technique I’ve used successfully many times: start your threat modeling session by writing the six threat names on a whiteboard. To get rolling, brainstorm a few of these abstract threats before digging into the details. The term “brainstorm” can mean different things, but the idea here is to move quickly, covering a lot of area, without overthinking it too much or judging ideas yet (you can skip the duds later on). This warm-up routine primes you for what to look out for, and also helps you switch into the necessary mindset. Even if you’re familiar with these categories of threat, it’s worth going through them all, and a couple that are less familiar and more technical bear careful explanation. </p>
<p><a href="#table2-1" id="tableanchor2-1">Table 2-1</a> lists six security objectives, their corresponding threat categories, and several examples of threats in each category. The security objective and threat category are two sides of the same coin, and sometimes it’s easier to work from one or the other—on the defense (the objective) or the offense (the threat).</p>
<figure>
<figcaption class="TableTitle"><p><a id="table2-1">Table 2-1</a>: Summary of STRIDE Threat Categories</p></figcaption>
<table id="table-123456c01-0001" border="1">
<thead>
<tr>
<td><b>Objective</b></td>
<td><b>STRIDE threats</b></td>
<td><b>Examples</b></td>
</tr>
</thead>
<tbody>
<tr>
<td>Authenticity</td>
<td>Spoofing</td>
<td>Phishing, stolen password, impersonation, replay attack, <a href="https://www.cloudflare.com/learning/security/glossary/bgp-hijacking/" class="LinkURL">BGP hijacking</a></td>
</tr>
<tr>
<td>Integrity</td>
<td>Tampering</td>
<td>Unauthorized data modification and deletion, <a href="https://us-cert.cisa.gov/ncas/alerts/TA15-051A" class="LinkURL">Superfish ad injection</a></td>
</tr>
<tr>
<td>Non-repudiability</td>
<td>Repudiation</td>
<td>Plausible deniability, insufficient logging, destruction of logs</td>
</tr>
<tr>
<td>Confidentiality</td>
<td>Information disclosure</td>
<td>Data leak, side channel attack, weak encryption, residual cached data, <a href="https://meltdownattack.com/ " class="LinkURL">Spectre/Meltdown</a> </td>
</tr>
<tr>
<td>Availability</td>
<td>Denial of service</td>
<td>Simultaneous requests swamp a web server, ransomware, <a href="https://blog.cloudflare.com/memcrashed-major-amplification-attacks-from-port-11211/ " class="LinkURL">memcrashed</a></td>
</tr>
<tr>
<td>Authorization</td>
<td>Elevation of privilege</td>
<td>SQL injection, xkcd’s “<a href="https://xkcd.com/327/" class="LinkURL">Exploits of a Mom</a>”</td>
</tr>
</tbody>
</table>
</figure>
<p><span epub:type="pagebreak" title="37" id="Page_37"/>Half of the STRIDE menagerie are direct threats to the information security fundamentals you learned about in <span class="xref" itemid="xref_target_Chapter 1">Chapter 1</span>: information disclosure is the enemy of confidentiality, tampering is the enemy of integrity, and denial of service compromises availability. The other half of STRIDE targets the Gold Standard. Spoofing subverts authenticity by assuming a false identity. Elevation of privilege subverts proper authorization. That leaves repudiation as the threat to auditing, which may not be immediately obvious and so is worth a closer look. </p>
<p>According to the Gold Standard, we should maintain accurate records of critical actions taken within the system and then audit those actions. Repudiation occurs when someone credibly denies that they took some action. In my years working in software security, I have never seen anyone directly repudiate anything (nobody has ever yelled “did so!” and “did not!” at each other in front of me). But what does happen is, say, a database suddenly disappears, and nobody knows why, because nothing was logged, and the lost data is gone without a trace. The organization might suspect that an intrusion occurred. Or it could have been a rogue insider, or possibly a regrettable blunder by an administrator. But without any evidence, nobody knows. That’s a big problem, because if you cannot explain what happened after an incident, it’s very hard to prevent it from happening again. In the physical world, such perfect crimes are rare because activities such as robbing a bank involve physical presence, which inherently leaves all kinds of traces. Software is different; unless you provide a means to reliably collect evidence and log events, no fingerprints or muddy boot tracks remain as evidence.</p>
<p>Typically, we mitigate the threat of repudiation by running systems in which administrators and users understand they are responsible for their actions, because they know an accurate audit trail exists. This is also one more good reason to avoid having admin passwords written on a sticky note that everyone shares. If you do that, when trouble happens, everyone can credibly claim someone else must have done it. This applies even if you fully trust everyone, because accidents happen, and the more evidence you have available when trouble arises, the easier it is to recover and remediate.</p>
<h4 id="h3-123456c01-0003">STRIDE at the Movies</h4>
<p class="BodyFirst">Just for fun (and to solidify these concepts), consider the STRIDE threats applied to the plot of the film <em>Ocean’s Eleven</em>. This classic heist story nicely demonstrates threat modeling concepts, including the full complement of STRIDE categories, from the perspectives of both attacker and defender. Apologies for the simplification of the plot, which I’ve done for brevity and focus, as well as for spoilers.</p>
<p>Danny Ocean violates parole (an <em>elevation of privilege</em>), flies out to meet his old partner in crime, and heads for Vegas. He pitches an audacious heist to a wealthy casino insider, who fills him in on the casino’s operational details (<em>information disclosure</em>), then gathers his gang of ex-cons. They plan their operation using a full-scale replica vault built for practice. On the fateful night, Danny appears at the casino and is predictably apprehended by security, creating the perfect alibi (<em>repudiation </em>of guilt). Soon he slips away <span epub:type="pagebreak" title="38" id="Page_38"/>through an air duct, and through various intrigues he and his accomplices extract half the money from the vault (<em>tampering</em> with its integrity), exfiltrating their haul with a remote-control van.</p>
<p>Threatening to blow up the remaining millions in the vault (a very expensive <em>denial of service</em>), the gang negotiates to keep the money in the van. The casino owner refuses and calls in the SWAT team, and in the ensuing chaos the gang destroys the vault’s contents and gets away. After the smoke clears, the casino owner checks the vault, lamenting his total loss, then notices a minor detail that seems amiss. The owner confronts Danny—who is back in lockup, as if he had never left—and we learn that the SWAT team was, in fact, the gang (<em>spoofing</em> by impersonating the police), who walked out with the money hidden in their tactical equipment bags after the fake battle. The practice vault mock-up had provided video to make it only appear (<em>spoofing</em> of the location) that the real vault had been compromised, which didn’t actually happen until the casino granted full access to the fake SWAT team (an <em>elevation of privilege</em> for the gang). Danny and the gang make a clean getaway with the money—a happy ending for the perpetrators that might have turned out quite differently had the casino hired a threat modeling consultant!</p>
<h3 id="h2-123456c01-0006">Mitigate Threats</h3>
<p class="BodyFirst">At this stage, you should have a collection of potential threats. Now you need to assess and prioritize them to best guide an effective defense. Since threats are, at best, educated guesses about future events, all of your assessments will contain some degree of subjectivity. </p>
<p>What exactly does it mean to understand threats? There is no easy answer to this question, but it involves refining what we know, and maintaining a healthy skepticism to avoid falling into the trap of thinking that we have it all figured out. In practice, this means quickly scanning to collect a bunch of mostly abstract threats, then poking into each one a little further to learn more. Perhaps we will see one or two fairly clear-cut attacks, or parts of what could constitute an attack. We elaborate until we run up against a wall of diminishing returns.</p>
<p>At this point, we can deal with the threats we’ve identified in one of four ways:</p>
<ul>
<li><em>Mitigate</em> the risk by either redesigning or adding defenses to reduce its occurrence or lower the degree of harm to an acceptable level.</li>
<li><em>Remove</em> a threatened asset if it isn’t necessary, or, if removal isn’t possible, seek to reduce its exposure or limit optional features that increase the threat.</li>
<li><em>Transfer</em> the risk by offloading responsibility to a third party, usually in exchange for compensation. (Insurance, for example, is a common form of risk transfer, or the processing of sensitive data could be outsourced to a service with a duty to protect confidentiality.)</li>
<li><em>Accept</em> the risk, once it is well understood, as reasonable to incur. </li>
</ul>
<p><span epub:type="pagebreak" title="39" id="Page_39"/>Always attempt to mitigate any significant threats, but recognize that results are often mixed. In practice, the best possible solution isn’t always feasible, for many reasons: a major change might be too costly, or you may be stuck using an external dependency beyond your control. Other code might also depend on vulnerable functionality, such that a fix might break things. In these cases, mitigation means doing anything that reduces the threat. Any kind of edge for defense helps, even a small one. </p>
<p>Here are some examples of ways to do partial mitigation:</p>
<p class="ListHead"><b>Make harm less likely to occur </b></p>
<ol class="none">
<li>Make it so the attack only works a fraction of the time.</li>
</ol>
<p class="ListHead"><b>Make harm less severe </b></p>
<ol class="none">
<li>Make it so only a small part of the data can be destroyed.</li>
</ol>
<p class="ListHead"><b>Make it possible to undo the harm </b></p>
<ol class="none">
<li>Ensure that you can easily restore any lost data from a backup.</li>
</ol>
<p class="ListHead"><b>Make it obvious that harm occurred</b></p>
<ol class="none">
<li>Use tamper-evident packaging that makes it easy to detect a modified product, protecting consumers. (In software, good logging helps here.)</li>
</ol>
<p>Much of the remainder of the book is about mitigation: how to design software to minimize threats, and what strategies and secure software patterns are useful for devising mitigations of various sorts.</p>
<h2 id="h1-123456c01-0004">Privacy Considerations</h2>
<p class="BodyFirst">Privacy threats are just as real as security threats, and they require separate consideration in a full assessment of threats to a system, because they add a human element to the risk of information disclosure. In addition to possible regulatory and legal considerations, personal information handling may involve ethical concerns, and it’s important to honor stakeholder expectations.</p>
<p>If you’re collecting personal data of any kind, you should take privacy seriously as a baseline stance. Think of yourself as a steward of people’s private information. Strive to stay mindful of your users’ perspectives, including careful consideration of the wide range of privacy concerns they might have, and err on the side of care. It’s easy for builders of software to discount how sensitive personal data can be when they’re immersed in the logic of system building. What in code looks like yet another field in a database schema could be information that, if leaked, has real consequences for an actual person. As modern life increasingly goes digital, and mobile computing becomes ubiquitous, privacy will depend more and more on code, potentially in new ways that are difficult to imagine. All this is to say that you would be smart to stay well ahead of the curve by exercising extreme vigilance now. </p>
<p><span epub:type="pagebreak" title="40" id="Page_40"/>A few very general considerations for minimizing privacy threats include the following:</p>
<ul>
<li>Assess privacy by modeling scenarios of actual use cases, not thinking in the abstract.</li>
<li>Learn what privacy policies or legal requirements apply, and follow the terms rigorously.</li>
<li>Restrict the collection of data to only what is necessary. </li>
<li>Be sensitive to the possibility of seeming creepy.</li>
<li>Never collect or store private information without a clear intention for its use.</li>
<li>When information already collected is no longer used or useful, proactively delete it.</li>
<li>Minimize information sharing with third parties (which, if it occurs, should be well documented).</li>
<li>Minimize disclosure of sensitive information—ideally this should be done only on a need-to-know basis.</li>
<li>Be transparent, and help end users understand your data protection practices.</li>
</ul>
<h2 id="h1-123456c01-0005">Threat Modeling Everywhere</h2>
<p class="BodyFirst">The threat modeling process described here is a formalization of how we navigate in the world; we manage risk by balancing it against opportunities. In a dangerous environment, all living organisms make decisions based on these same basic principles. Once you start looking for it, you can find instances of threat modeling everywhere. </p>
<p>When expecting a visit from friends with a young child, we always take a few minutes to make special preparations. Alex, an active three-year-old, has an inquisitive mind, so we go through the house “child-proofing.” This is pure threat modeling, as we imagine the threats by categories—what could hurt Alex, what might get broken, what’s better kept out of view of a youngster—then look for assets that fit these patterns. Typical threats include a metal letter opener, which he could stick in a wall socket; a fragile antique vase that he might easily break; or perhaps a coffee table book of photography that contains images inappropriate for children. The attack surface is any place reachable by an active toddler. Mitigations generally consist of removing, reducing, or eliminating points of exposure or vulnerability: we could replace the fragile vase with a plastic one that contains just dried flowers, or move it up onto a mantlepiece. People with children know how difficult it is to anticipate what they might do. For instance, did we anticipate Alex might stack up enough books to climb up and reach a shelf that we thought was out of reach? This is what threat modeling looks like outside of software, and it illustrates why preemptive mitigation can be well worth the effort.</p>
<p><span epub:type="pagebreak" title="41" id="Page_41"/>Here are a few other examples of threat modeling you may have noticed in daily life:</p>
<ul>
<li>Stores design return policies specifically to mitigate abuses such as shoplifting and then returning the product for store credit, or wearing new apparel once and then returning it for a refund.</li>
<li>Website terms of use agreements attempt to prevent various ways that users might maliciously abuse the site.</li>
<li>Traffic safety laws, speed limits, driver licensing, and mandatory auto insurance requirements are all mitigation mechanisms to make driving safer. </li>
<li>Libraries design loan policies to mitigate theft, hoarding, and damage to the collection.</li>
</ul>
<p>You can probably think of lots of ways that you apply these techniques, too. For most of us, when we can draw on our physical intuitions about the world, threat modeling is remarkably easy to do. Once you recognize that software threat modeling works the same way as your already well-honed skills in other contexts, you can begin to apply your natural capabilities to software security analysis, and quickly raise your skills to the next level. </p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p class="BodyFirst">	See Appendix D for a cheat sheet summarizing the Four Questions and STRIDE as a handy reference for threat modeling.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
</section>


<section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" title="43" id="Page_43"/>3</span><br/>
<span class="ChapterTitle">Mitigation</span></h1>
</header>
<blockquote class="Epigraph" epub:type="epigraph">
<p class="Epigraph">Everything is possible to mitigate through art and diligence.</p>
<p class="EpigraphSource">—Gaius Plinius Caecilius Secundus (Pliny the Younger)</p>
</blockquote>
<figure class="opener">
<img src="image_fi/book_art/chapterart.png" alt=""/>
</figure>
<p class="ChapterIntro">This chapter focuses on the third of the Four Questions from <span class="xref" itemid="xref_target_Chapter 2">Chapter 2:</span> “What are we going to do about it?” Anticipating threats, then protecting against potential vulnerabilities, is how security thinking turns into effective action. This proactive response is called <em>mitigation</em>—reducing the severity, extent, or impact of problems—and as you saw in the previous chapter, it’s something we all do all the time. Bibs to catch the inevitable spills when feeding an infant, seat belts, speed limits, fire alarms, food safety practices, public health measures, and industrial safety regulations are just a few examples of mitigations. The common thread among these is that they take proactive measures to avoid, or lessen, anticipated harms in the face of risk. This is much of what we do to make software more secure. </p>
<p><span epub:type="pagebreak" title="44" id="Page_44"/>It’s important to bear in mind that mitigations reduce risk but don’t eliminate it. To be clear, if you can eliminate a risk somehow—say, by removing a legacy feature that is known to be insecure—by all means do that, but I would not call it a mitigation. Instead, mitigations focus on making attacks less likely, more difficult, or less harmful when they do occur. Even measures that make exploits more detectable are mitigations, analogous to tamper-evident packaging, if they lead to a faster response and remediation. Every small effort ratchets up the security of the system as a whole, and even modest wins can collectively add up to significantly better protection.</p>
<p>This chapter begins with a conceptual discussion of mitigation, and from there presents a number of general techniques. The focus here is on structural mitigations based on the perspective gained through threat modeling that can be useful for securing almost any system design. Subsequent chapters will build on these ideas to provide more detailed methods, drilling down into specific technologies and threats. </p>
<p> The rest of the chapter provides guidance for recurrent security challenges encountered in software design: instituting an access policy and access controls, designing interfaces, and protecting communications and storage. Together, these discussions form a playbook for addressing common security needs that will be fleshed out over the remainder of the book.</p>
<h2 id="h1-123456c01-0001">Addressing Threats</h2>
<p class="BodyFirst">Threat modeling reveals what can go wrong, and in doing so, focuses our security attention where it counts. But believing we can always eliminate vulnerabilities would be naive. Points of risk—critical events or decision thresholds—are great opportunities for mitigation. </p>
<p>As you learned in the previous chapter, you should always address the biggest threats first, limiting them as best you can. For systems that process sensitive personal information, as one example, the threat of unauthorized disclosure inevitably looms large. For this major risk, consider any or all of the following: minimizing access to the data, reducing the amount of information collected, actively deleting old data when no longer needed, auditing for early detection in the event of compromise, and taking measures to reduce an attacker’s ability to exfiltrate data. After securing the highest-priority risks, opportunistically mitigate lesser risks where it is easy to do so without adding much overhead or complexity to the design. </p>
<p>A good example of a smart mitigation is the best practice of checking the password submitted with each login attempt against a salted hash, instead of the actual password in plaintext. Protecting passwords is critical because disclosure threatens the fundamental authentication mechanism. Comparing hashes only requires slightly more work than comparing directly, yet it’s a big win as it eliminates the need to store plaintext passwords. This means that even if attackers somehow breach the system, they won’t learn actual passwords as easily. </p>
<p><span epub:type="pagebreak" title="45" id="Page_45"/>This example illustrates the idea of harm reduction but is quite specific to password checking. Now let’s consider mitigation strategies that are more widely applicable.</p>
<h2 id="h1-123456c01-0002">Structural Mitigation Strategies</h2>
<p class="BodyFirst">Mitigations often amount to common sense: reducing risk where there are opportunities to do so. Threat modeling helps us see potential vulnerabilities in terms of attack surfaces, trust boundaries, and assets (targets needing protection). <em>Structural mitigations</em> generally apply to these very features of the model, but their realization depends on the specifics of the design. The subsections that follow discuss techniques that should be widely applicable because they operate at the model level of abstraction.</p>
<h3 id="h2-123456c01-0001">Minimize Attack Surfaces</h3>
<p class="BodyFirst">Once you have identified the attack surfaces of a system, you know where exploits are most likely to originate, so anything you can do to harden the system’s “outer shell” will be a significant win. A good way to think about attack surface reduction is in terms of how much code and data are touched downstream of each point of entry. Systems that provide multiple interfaces to perform the same function may benefit from unifying these interfaces because that means less code that might contain vulnerabilities. Here are a few examples of this commonly used technique:</p>
<ul>
<li>In a client/server system, you can reduce the attack surface of the server by pushing functionality out to the client. Any operation that requires a server request represents an additional attack surface that a malformed request or forged credentials might be able to exploit. By contrast, if the necessary information and compute power exist on the client side, that reduces both the load on and the attack surface of the server.</li>
<li>Moving functionality from a publicly exposed API that anyone can invoke anonymously to an authenticated API can effectively reduce your attack surface. The added friction of account creation slows down attacks, and also helps trace attackers and enforce rate limiting.</li>
<li>Libraries and drivers that use kernel services can reduce the attack surface by minimizing interfaces to, and code within, the kernel. Not only are there fewer kernel transitions to attack that way, but userland code will be incapable of doing as much damage even if an attack is successful.</li>
<li>Deployment and operations offer many attack surface reduction opportunities. For an enterprise network, moving anything you can behind a firewall is an easy win.</li>
<li>A configuration setting that enables remote administration over the network is another good example: this feature may be convenient, but if it’s rarely used, consider disabling it and use wired access instead when necessary.</li>
</ul>
<p><span epub:type="pagebreak" title="46" id="Page_46"/>These are just some of the most common scenarios where attack surface reduction works. For particular systems, you might find much more creative customized opportunities. Keep thinking of ways to reduce external access, minimize functionality and interfaces, and protect any services that are needlessly exposed. The better you understand where and how a feature is actually used, the more of these mitigations you’ll be able to find.</p>
<h3 id="h2-123456c01-0002">Narrow Windows of Vulnerability</h3>
<p class="BodyFirst">This mitigation technique is similar to attack surface reduction, but instead of metaphorical surface area, it reduces the effective time interval in which a vulnerability can occur. Also based on common sense, this is why hunters only disengage the safety just before firing and reengage it soon after.</p>
<p>We usually apply this mitigation to trust boundaries, where low-trust data or requests interact with high-trust code. To best isolate the high-trust code, minimize the processing that it needs to do. For example, when possible, perform error checking ahead of invoking the high-trust code so it can do its work and exit quickly. </p>
<p><em><a href="https://docs.microsoft.com/en-us/dotnet/framework/misc/code-access-security" class="LinkURL">Code Access Security</a></em><em> </em><em>(CAS)</em>, a security model that is rarely used today, is a perfect illustration of this mitigation because it provides fine-grained control over code’s effective privileges. (Full disclosure: I was the program manager for security in .NET Framework version 1.0, which prominently featured CAS as a major security feature.) </p>
<p>The CAS runtime grants different permissions to different units of code based on trust. The following pseudocode example illustrates a common idiom for a generic <var>permission</var>, which could grant access to certain files, to the clipboard, and so on. In effect, CAS ensures that high-trust code inherits the lower privileges of the code invoking it, but when necessary, it can temporarily assert its higher privileges. Here’s how such an assertion of privilege works:</p>
<pre><code>Worker(parameters) {
  // When invoked from a low-trust caller, privileges are reduced.
  DoSetup();
  <var>permission</var>.Assert();
  // Following assertion, the designated permission can now be used.
  DoWorkRequiringPrivilege();
  CodeAccessPermission.RevertAssert();
  // Reverting the assertion undoes its effect.
  DoCleanup();
}</code></pre>
<p>The code in this example has powerful privileges, but it may be called by less-trusted code. When invoked by low-trust code, this code initially runs with the reduced privileges of the caller. Technically, the effective privileges are the intersection (that is, the minimum) of the privileges granted to the code, its caller, and its caller’s caller, and so on all the way up the stack. Some of what the <code>Worker</code> method does requires higher privileges than its callers may have, so after doing the setup, it asserts the necessary permission before invoking <code>DoWorkRequiringPrivilege</code>, which must also have <span epub:type="pagebreak" title="47" id="Page_47"/>that permission. Having done that portion of its work, it immediately drops the special permission by calling <code>RevertAssert</code>, before doing whatever is left that needs no special permissions and returning. In the CAS model, time window minimization provides for such assertions of privilege to be used when necessary and reverted as soon as they are no longer needed.</p>
<p>Consider this application of narrowing windows of vulnerability in a different way. Online banking offers convenience and speed, and mobile devices allow us to bank from anywhere. But storing your banking credentials in your phone is risky—you don’t want someone emptying out your bank account if you lose it, which is much more likely with a mobile device. A great mitigation that I would like to see implemented across the banking industry would be the ability to configure the privilege level you are comfortable with for each device. A cautious customer might restrict the mobile app to checking balances and a modest daily transaction dollar limit. The customer would then be able to bank by phone with confidence. Further useful limits might include windows of time, geolocation, domestic currency only, and so on. All of these mitigations help because they limit the worst-case scenario in the event of any kind of compromise.</p>
<h3 id="h2-123456c01-0003">Minimize Data Exposure</h3>
<p class="BodyFirst">Another structural mitigation to data disclosure risk is limiting the lifetime of sensitive data in memory. This is much like the preceding technique, but here you’re minimizing the duration for which sensitive data is accessible and potentially exposed instead of the duration for which code is running at high privilege. Recall that intraprocess access is hard to control, so the mere presence of data in memory puts it at risk. When the stakes are high, such as handling extremely sensitive data, you can think of it as “the meter is running.” For the most critical information—data such as private encryption keys, or authentication credentials such as passwords—it may be worth overwriting any in-memory copies as soon as they are no longer needed. This reduces the time during which a leak is conceivably possible through any means. As we shall see in <span class="xref" itemid="xref_target_Chapter 9">Chapter 9</span>, the Heartbleed vulnerability threatened security for much of the web, exposing all kinds of sensitive data lying around in memory. Limiting how long such data was retained probably would have been a useful mitigation (“stanching the blood flow,” if you will), even without foreknowledge of the exploit.</p>
<p>You can apply this technique to data storage design as well. When a user deletes their account in the system, that typically causes their data to be destroyed, but the system often offers a provision for a manual restore of the account in case of accidental or malicious closure. The easy way to implement this is to mark closed accounts as to-be-deleted but keep the data in place for, say, 30 days (after the manual restore period has passed) before the system finally deletes everything. To make this work, lots of code needs to check if the account is scheduled for deletion, lest it accidentally access the account data that the user directed to be destroyed. If a bulk mail job forgets to check, it could errantly send the user some notice that, to the user, would appear to be a violation of their intentions after they <span epub:type="pagebreak" title="48" id="Page_48"/>closed the account. This mitigation suggests a better option: after the user deletes the account, the system should push its contents to an offline backup and promptly delete the data. The rare case where a manual restore is needed can still be accomplished using the backup data, and now there is no way for a bug to possibly result in that kind of error.</p>
<p>Generally speaking, proactively wiping copies of data is an extreme measure that’s appropriate only for the most sensitive data, or important actions such as account closure. Some languages and libraries help do this automatically, and except where performance is a concern, a simple wrapper function can wipe the contents of memory clean before it is recycled.</p>
<h2 id="h1-123456c01-0003">Access Policy and Access Controls</h2>
<p class="BodyFirst">Standard operating system permissions provide very rudimentary file access controls. These control <em>read</em> (confidentiality) or <em>write</em> (integrity) access on an all-or-nothing basis for individual files based on the user and group ownership of a process. Given this functionality, it’s all too easy to think in the same limited terms when designing protections for assets and resources—but the right access policy might be more granular and depend on many other factors.  </p>
<p>First, consider how ill-suited traditional access controls are for many modern systems. Web services and microservices are designed to work on behalf of principals that usually do not correspond to the process owner. In this case, one process services all authenticated requests, requiring permission to access all client data all the time. This means that in the presence of a vulnerability, all client data is potentially at risk. </p>
<p>Defining an efficacious access policy is an important mitigation, as it closes the gap between what accesses should be allowed and what access controls the system happens to offer. Rather than start with the available operating system access controls, think through the needs of the various principals acting through the system and define an ideal access policy that expresses an accurate description of what constitutes proper access. A granular access policy potentially offers a wealth of options: you can cap the number of accesses per minute or hour or day, or enforce a maximum data volume, time-based limits corresponding to working hours, or variable access limits based on activity by peers or historical rates (to name a few obvious mechanisms). </p>
<p>Determining safe access limitations is hard work but worthwhile because it helps you understand the application’s security requirements. Even if the policy is not fully implemented in code, it will at least provide guidance for effective auditing. Given the right set of controls, you can start with lenient restrictions to gauge what real usage looks like and then, over time, narrow the policy as you learn how the system is used in practice.</p>
<p> For example, consider a hypothetical system that serves a team of customer service agents. Agents need access to the records of any customer who might contact them, but they only interact with a limited number of customers on a given day. A reasonable access policy might limit each agent <span epub:type="pagebreak" title="49" id="Page_49"/>to no more than 100 different customer records in one shift. With access to all records all the time, a dishonest agent could leak a copy of all customer data, whereas the limited policy greatly limits the worst-case daily damage. </p>
<p>Once you have a fine-grained access policy, you face the challenge of setting the right limits. This can be difficult when you must avoid impeding rightful use in extreme edge cases. In the customer service example, for instance, you might restrict agents to accessing the records of up to 100 customers per shift as a way of accommodating seasonal peak demand, even though, on most days, needing even 50 records would be unusual. Why? It would be impractical to adjust the policy configuration throughout the year, and you want to allow for leeway so the limit never impedes work. Also, defining a more specific and detailed policy based on fixed dates might not work well, as there could be unexpected surges in activity at any time. </p>
<p>But is there a way to narrow the gap between normal circumstances and the rare highest-demand case that the system should allow? One great tool to handle this tricky situation is a policy provision for self-declared exceptions to be used in extraordinary circumstances. Such an option allows individual agents to bump up their own limits for a short period of time by providing a rationale. With this kind of “relief valve” in place, the basic access policy can be tightly constrained. When needed, once agents hit the access limit, they can file a quick notice—stating, for example, “high call volume today, I’m working late to finish up”—and receive additional access authorization. Such notices can be audited, and if they become commonplace, management could bump the policy up with the knowledge that demand has legitimately grown and an understanding of why. Such flexible techniques enable you to create access policies with softer limits, rather than hard-and-fast restrictions that tend to be arbitrary.</p>
<h2 id="h1-123456c01-0004">Interfaces</h2>
<p class="BodyFirst">Software designs consist of components that correspond to functional parts of the system. You can visualize these designs as block diagrams, with lines representing the connections between the parts. These connections denote <em>interfaces</em>, which are a major focus of security analysis—not only because they reveal data and control flows, but also because they serve as well-defined chokepoints where you can add mitigations. In particular, where there is a trust boundary, the main security focus is on the flow of data and control from the lower- to the higher-trust component, so that is where defensive measures are often needed. </p>
<p>In large systems, there are typically interfaces between networks, between processes, and within processes. Network interfaces provide the strongest isolation because it’s virtually certain that any interactions between the endpoints will occur over the wire, but with the other kinds of interfaces it’s more complicated. Operating systems provide strong isolation at process boundaries, so interprocess communication interfaces are nearly as trustworthy as network interfaces. In both of these cases, it’s generally impossible <span epub:type="pagebreak" title="50" id="Page_50"/>to go around these channels and interact in some other way. The attack surface is cleanly constrained, and hence this is where most of the important trust boundaries are. As a consequence, interprocess communication and network interfaces are the major focal points of threat modeling. </p>
<p>Interfaces also exist within processes, where interaction is relatively unconstrained. Well-written software can still create meaningful security boundaries within a process, but these are only effective if all the code plays together well and stays within the lines. From the attacker’s perspective, intraprocess boundaries are much easier to penetrate. However, since attackers may only gain a limited degree of control via a given vulnerability, any protection you can provide is better than none. By analogy, think of a robber who only has a few seconds to act: even a weak precaution might be enough to prevent a loss.</p>
<p>Any large software design faces the delicate task of structuring components to minimize regions of highly privileged access, as well as restricting sensitive information flow in order to reduce security risk. To the extent that the design restricts information access to a minimal set of components that are well isolated, attackers will have a much harder time getting access to sensitive data. By contrast, in weaker designs, all kinds of data flow all over the place, resulting in greater exposure from a vulnerability anywhere within the component. The architecture of interfaces is a major factor that determines the success systems have at protecting assets.</p>
<h2 id="h1-123456c01-0005">Communication</h2>
<p class="BodyFirst">Modern networked systems are so common that standalone computers, detached from any network, have become rare exceptions. The cloud computing model, combined with mobile connectivity, makes network access ubiquitous. As a result, communication is fundamental to almost every software system in use today, be it through internet connections, private networks, or peripheral connections via Bluetooth, USB, and the like. </p>
<p>In order to protect these communications, the channel must be physically secured against wiretapping and snooping, or else the data must be encrypted to ensure its integrity and confidentiality. Reliance on physical security is typically fragile in the sense that if attackers bypass it, they usually gain access to the full data flow, and such incursions are difficult to detect. Modern processors are fast enough that the computational overhead of encryption is usually acceptable, so there is rarely a good reason not to encrypt communications. I cover basic encryption in <span class="xref" itemid="xref_target_Chapter 5">Chapter 5</span>, and HTTPS for the web specifically in <span class="xref" itemid="xref_target_Chapter 11">Chapter 11</span>.</p>
<p>Even the best encryption is not a magic bullet, though. One remaining threat is that encryption cannot conceal the <em>fact of communication</em>. In other words, if attackers can read the raw data in the channel, even if they’re unable to decipher its contents, they can still see that data is being sent and received on the wire, and roughly estimate the amount of data flow. Furthermore, if attackers can tamper with the communication channel, they might be able to delay or block the transmission entirely.</p>
<h2 id="h1-123456c01-0006"><span epub:type="pagebreak" title="51" id="Page_51"/>Storage</h2>
<p class="BodyFirst">The security of data storage is much like the security of communications, because storing data is analogous to sending it into the future, at which point you will retrieve it for some purpose. Viewed in this way, just as data that is being communicated is vulnerable on the wire, stored data is vulnerable at rest on the storage medium. Protecting data at rest from potential tampering or disclosure requires either physical security or encryption. Likewise, availability depends on the existence of backup copies or successful physical protection.</p>
<p>Storage is so ubiquitous in system designs that it’s easy to defer the details of data security for operations to deal with, but doing so misses good opportunities for proactively mitigating data loss in the design. For instance, data backup requirements are an important part of software designs, because the demands are by no means obvious, and there are many trade-offs. You could plan for redundant storage systems, designed to protect against data loss in the event of failure, but these can be expensive and incur performance costs. Your backups might be copies of the whole dataset, or they could be incremental, recording transactions that, cumulatively, can be used to rebuild an accurate copy. Either way, they should be reliably stored independently and with specific frequency, within acceptable limits of latency. Cloud architectures can provide redundant data replication in near real-time for perhaps the best continuous backup solution, but at a cost.</p>
<p>All data at rest, including backup copies, is at risk of exposure to unauthorized access, so you must physically secure or encrypt it for protection. The more backup copies you make, the greater the risk is of a leak due to having so many copies. Considering the potential extremes makes this point clear.  Photographs are precious memories and irreplaceable pieces of every family’s history, so keeping multiple backup copies is wise—if you don’t have any copies and the original files are lost, damaged, or corrupted, the loss could be devastating. To guard against this, you might send copies of your family photos to as many relatives as possible for safekeeping. But this has a downside too, as it raises the chances that one of them might have the data stolen (via malware, or perhaps a stolen laptop). This could also be catastrophic, as these are private memories, and it would be a violation of privacy to see all those photos publicly spread all over the web (and potentially a greater threat if it allowed strangers to identify children in a way that could lead to exploitation). This is a fundamental trade-off that requires you to weigh the risks of data loss against the risk of leaks—you cannot minimize both at once, but you can balance these concerns to a degree in a few ways. </p>
<p>As a compromise between these threats, you could send your relatives encrypted photos. (This means they would not be able to view them, of course.) However, now you are responsible for keeping the key that you chose not to entrust them with, and if you lose the key, the encrypted copies are worthless.  </p>
<p>Preserving photos also raises an important aspect of backing up data, which is the problem of media lifetime and obsolescence. Physical media <span epub:type="pagebreak" title="52" id="Page_52"/>(such as hard disks or DVDs) inevitably degrade over time, and support for legacy media fades away as new hardware evolves (this author recalls long ago personally moving data from dozens of floppy disks, which only antiquated computers can use, onto one USB memory stick, now copied to the cloud). Even if the media and devices still work, new software tends to drop support for older data formats. The choice of data format is thus important, with widely used open standards highly preferred, because proprietary formats must be reverse-engineered once they are officially retired. Over longer time spans, it might be necessary to convert file formats, as software standards evolve and application support for older formats becomes deprecated. </p>
<p>The examples mentioned throughout this chapter have been simplified for explanatory purposes, and while we’ve covered many techniques that can be used to mitigate identified threats, these are just the tip of the iceberg of possibilities. Adapt specific mitigations to the needs of each application, ideally by making them integral to the design. While this sounds simple, effective mitigations are challenging in practice because a panoply of threats must be considered in the context of each system, and you can only do so much. The next chapter presents major patterns with useful security properties, as well as anti-patterns to watch out for, that are useful in crafting these mitigations as part of secure design.</p>
</section>


<section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" title="53" id="Page_53"/>4</span><br/>
<span class="ChapterTitle">Patterns</span></h1>
</header>
<blockquote class="Epigraph" epub:type="epigraph">
<p class="Epigraph">Art is pattern informed by sensibility.</p>
<p class="EpigraphSource">—Herbert Read</p>
</blockquote>
<figure class="opener">
<img src="image_fi/book_art/chapterart.png" alt=""/>
</figure>
<p class="ChapterIntro">Architects have long used design patterns to envision new buildings, an approach just as useful for guiding software design. This chapter introduces many of the most useful patterns promoting secure design. Several of these patterns derive from ancient wisdom; the trick is knowing how to apply them to software and how they enhance security.</p>
<p>These patterns either mitigate or avoid various security vulnerabilities, forming an important toolbox to address potential threats. Many are simple, but others are harder to understand and best explained by example. Don’t underestimate the simpler ones, as they can be widely applicable and are among the most effective. Still, other concepts may be easier to grasp as anti-patterns describing what <em>not </em>to do. I present these patterns in groups based on shared characteristics that you can think of as sections of the toolbox (<a href="#figure4-1" id="figureanchor4-1">Figure 4-1</a>).</p>
<span epub:type="pagebreak" title="54" id="Page_54"/><figure>
<img src="image_fi/501928c04/f04001.png" alt="f04001" class=""/>
<figcaption><p><a id="figure4-1">Figure 4-1</a>: Groupings of secure software patterns this chapter covers</p></figcaption>
</figure>
<p>When and where to apply these patterns requires judgment. Let necessity and simplicity guide your design decisions. As powerful as these patterns are, don’t overdo it; just as you don’t need seven deadbolts and chains on your doors, you don’t need to apply every possible design pattern to fix a problem. Where several patterns are applicable, choose the best one or two, or maybe more for critical security demands. Overuse can be counterproductive because the diminishing returns of increased complexity and overhead quickly outweigh additional security gains.</p>
<h2 id="h1-123456c01-0001">Design Attributes</h2>
<p class="BodyFirst">The first group of patterns describe at a high level what secure design looks like: simple and transparent. These derive from the adages “keep it simple” and “you should have nothing to hide.” As basic and perhaps obvious as these patterns may be, they can be applied widely and are very powerful.</p>
<h3 id="h2-123456c01-0001">Economy of Design</h3>
<p class="BodyFirst">Designs should be as simple as possible. </p>
<p><em>Economy of Design</em> raises the security bar because simpler designs likely have fewer bugs, and thus fewer undetected vulnerabilities. Though developers claim that “all software has bugs,” we know that simple programs certainly can be bug-free. Prefer the simplest of competing designs for security mechanisms, and be wary of complicated designs that perform critical security functions.</p>
<p><span epub:type="pagebreak" title="55" id="Page_55"/> LEGO bricks are a great example of this pattern. Once the design and manufacture of the standard building element is perfected, it enables building a countless array of creative designs. A similar system composed of a number of less universally useful pieces would be more difficult to build with; any particular design would require a larger inventory of parts and involve other technical challenges.</p>
<p>You can find many examples of Economy of Design in the system architecture of large web services built to run in massive datacenters. For reliability at scale, these designs decompose functionality into smaller, self-contained components that collectively perform complicated operations. Often, a basic frontend terminates the HTTPS request, parsing and validating the incoming data into an internal data structure. That data structure gets sent on for processing by a number of subservices, which in turn use microservices to perform various functions. </p>
<p>In the case of an application such as web search, different machines may independently build different parts of the response in parallel, then yet another machine blends them into the complete response. It’s much easier to build many small services to do separate parts of the whole task—query parsing, spelling correction, text search, image search, results ranking, and page layout—than to do everything in one massive program.</p>
<p>Economy of Design is not an absolute mandate that everything must always be simple. Rather, it highlights the great advantages of simplicity, and says that you should only embrace complexity when it adds significant value. Consider the differences between the design of access control lists (ACLs) in *nix and Windows. The former is simple, specifying read/write/execute permissions by user or user group, or for everybody. The latter is much more involved, including an arbitrary number of both allow and deny access control entries as well as an inheritance feature; notably, evaluation is dependent on the ordering of entries within the list. (These simplified descriptions are to make a point about design, and are not intended as complete.) This pattern correctly shows that the simpler *nix permissions are easier to correctly enforce, and beyond that, it’s easier for users of the system to correctly understand how ACLs work and therefore to use them correctly. However, if the Windows ACL provides just the right protection for a given application and can be accurately configured, then it may be a fine solution.</p>
<p>The Economy of Design pattern does not say that the simpler option is unequivocally better, or that the more complex one is necessarily problematic. In this example, *nix ACLs are not inherently better, and Windows ACLs are not necessarily buggy. However, Windows ACLs do represent more of a learning curve for developers and users, and using their more complicated features can easily confuse people as well as invite unintended consequences. The key design choice here, which I will not weigh in on, is to what extent the ACL designs best fit the needs of users. Perhaps *nix ACLs are too simplistic and fail to meet real demands; on the other hand, perhaps Windows ACLs are overly feature-bound and cumbersome in typical use patterns. These are difficult questions we must each answer for our own purposes, but for which this design pattern provides insight.</p>
<h3 id="h2-123456c01-0002"><span epub:type="pagebreak" title="56" id="Page_56"/>Transparent Design</h3>
<p class="BodyFirst">Strong protection should never rely on secrecy. </p>
<p>Perhaps the most famous example of a design that failed to follow the pattern of <em>Transparent Design</em> is the Death Star in <em>Star Wars</em>, whose thermal exhaust port afforded a straight shot at the heart of the battle station. Had Darth Vader held his architects accountable to this principle as severely as he did Admiral Motti, the story would have turned out very differently. Revealing the design of a well-built system should have the effect of dissuading attackers by showing its invincibility. It shouldn’t make the task easier for them. The corresponding anti-pattern may be better known: we call it <em>Security by Obscurity</em>.</p>
<p>This pattern specifically warns against a <em>reliance</em> on the secrecy of a design. It doesn’t mean that publicly disclosing designs is mandatory, or that there is anything wrong with secret information. If full transparency about a design weakens it, you should fix the design, not rely on keeping it secret. This in no way applies to legitimately secret information, such as cryptographic keys or user identities, which actually would compromise security if leaked. That’s why the name of the pattern is Transparent <em>Design</em>, not Absolute Transparency. Full disclosure of the design of an encryption method—the key size, message format, cryptographic algorithms, and so forth—shouldn’t weaken security at all. The anti-pattern should be a big red flag: for instance, distrust any self-anointed “experts” who claim to invent amazing encryption algorithms that are so great that they cannot publish the details. Without exception, these are bogus.</p>
<p>The problem with Security by Obscurity is that while it may help forestall adversaries temporarily, it’s extremely fragile. For example, imagine that a design used an outdated cryptographic algorithm: if the attackers ever found out that the software was still using, say, DES (a legacy symmetric encryption algorithm from the 1970s), they could easily crack it within a day. Instead, do the work necessary to get to a solid security footing so that there is nothing to hide, whether or not the design details are public.</p>
<h2 id="h1-123456c01-0002">Exposure Minimization</h2>
<p class="BodyFirst">The largest group of patterns call for caution: think “err on the safe side.” These are expressions of basic risk/reward strategies where you play it safe unless there is an important reason to do otherwise.</p>
<h3 id="h2-123456c01-0003">Least Privilege</h3>
<p class="BodyFirst">It’s always safest to use just enough privilege for the job.</p>
<p>Never clean a loaded gun. Unplug power saws when changing blades. These commonplace safety practices are examples of the <em>Least Privilege</em> pattern, which aims to reduce the risk of making mistakes when performing a task. This pattern is the reason that administrators of important systems should not be randomly browsing the internet while logged in at work; if they visit a malicious website and get compromised, the attack could easily do serious harm.</p>
<p><span epub:type="pagebreak" title="57" id="Page_57"/>The *nix <code>sudo(1)</code> command performs exactly this purpose. User accounts with high privilege (known as <em>sudoers</em>) need to be careful not to inadvertently use their extraordinary power by accident or if compromised. To provide this protection, the user must prefix superuser commands with <code>sudo</code>, which may prompt the user for a password, in order to run them. Under this system, most commands (those that do not require <code>sudo</code>) will affect only the user’s own account, and cannot impact the entire system. This is akin to the “IN CASE OF EMERGENCY BREAK GLASS” cover on a fire alarm switch to prevent accidental activation, in that this forces an explicit step (corresponding to the <code>sudo</code> prefix) before activating the switch. With the glass cover, nobody can claim to have accidentally pulled the fire alarm, just as a competent administrator would never type <code>sudo</code> and a command that breaks the system all by accident. </p>
<p>This pattern is important for the simple reason that when vulnerabilities are exploited, it’s better for the attacker to have minimal privileges to use as leverage. Use all-powerful authorizations such as superuser privileges only when strictly necessary, and for the minimum possible duration. Even Superman practiced Least Privilege by only wearing his uniform when there was a job to do, and then, after saving the world, immediately changing back into his Clark Kent persona.</p>
<p>In practice, it does take more effort to selectively and sparingly use elevated privileges. Just as unplugging power tools to work on them requires more effort, discretion when using permissions requires discipline, but doing it right is always safer. In the case of an exploit, it means the difference between a minor incursion and total system compromise. Practicing Least Privilege can also mitigate damage done by bugs and human error.</p>
<p>Like all rules of thumb, use this pattern with a sense of balance to avoid overcomplication. Least Privilege does not mean the system should always grant literally the minimum level of authorization (for instance, creating code that, in order to write file X, is given write access to only that one file). You may wonder, why not always apply this excellent pattern to the max? In addition to maintaining a general sense of balance and recognizing diminishing returns for any mitigation, a big factor here is the granularity of the mechanism that controls authorization, and the cost incurred while adjusting privileges up and down. For instance, in a *nix process, permissions are conferred based on user and group ID access control lists. Beyond the flexibility of changing between effective and real IDs (which is what <code>sudo</code> does), there is no easy way to temporarily drop unneeded privileges without forking a process. Code should operate with lower ambient privileges where it can, using higher privileges in the necessary sections and transitioning at natural decision points.</p>
<h3 id="h2-123456c01-0004">Least Information</h3>
<p class="BodyFirst">It’s always safest to collect and access the minimum amount of private information needed for the job.</p>
<p>The <em>Least Information</em> pattern, the data privacy analog of Least Privilege, helps to minimize unintended disclosure risks. Avoid providing more private information than necessary when calling a subroutine, requesting a service, or responding to a request, and at every opportunity curtail unnecessary <span epub:type="pagebreak" title="58" id="Page_58"/>information flow. Implementing this pattern can be challenging in practice because software tends to pass data around in standard containers not optimized for purpose, so extra data often is included that isn’t really needed. </p>
<p>All too often, software fails this pattern because the design of interfaces evolves over time to serve a number of purposes, and it’s convenient to reuse the same parameters or data structure for consistency. As a result, data that isn’t strictly necessary gets sent along as extra baggage that seems harmless enough. The problem arises, of course, when this needless data flowing through the system creates additional opportunities for attack.</p>
<p>For example, imagine a large customer relationship management (CRM) system used by various workers in an enterprise. Different workers use the system for a wide variety of purposes, including sales, production, shipping, support, maintenance, R&amp;D, and accounting. Depending on their roles, each has a different authorization for access to subsets of this information. To practice Least Information, the applications in this enterprise should request only the minimum amount of data needed to perform a specific task. Consider a customer support representative responding to a phone call: if the system uses Caller ID to look up the customer record, the support person doesn’t need to know their phone number, just their purchase history. Contrast this with a more basic design that either allows or disallows the lookup of customer records that include all data fields. Ideally, even if the representative has more access, they should be able to request the minimum needed for a given task and work with that, thereby minimizing the risk of disclosure.</p>
<p>At the implementation level, Least Information design includes wiping locally cached information when no longer needed, or perhaps displaying a subset of available data on the screen until the user explicitly requests to see certain details. The common practice of displaying passwords as <code>********</code> uses this pattern to mitigate the risk of shoulder surfing.</p>
<p>It’s particularly important to apply this pattern at design time, as it can be extremely difficult to implement later on because both sides of the interface need to change together. If you design independent components suited to specific tasks that require different sets of data, you’re more likely to get this right. APIs handling sensitive data should provide flexibility to allow callers to specify subsets of data they need in order to minimize information exposure (<a href="#table4-1" id="tableanchor4-1">Table 4-1</a>).</p>
<figure>
<figcaption class="TableTitle"><p><a id="table4-1">Table 4-1</a>: How Least Information Changes API Design</p></figcaption>
<table id="table-123456c01-0001" border="1">
<thead>
<tr>
<td><b>Least Information non-compliant API</b></td>
<td><b>Least Information compliant API</b></td>
</tr>
</thead>
<tbody>
<tr>
<td><code>RequestCustomerData(id='12345')</code></td>
<td><code>RequestCustomerData(id='12345', items=['name', 'zip'])</code></td>
</tr>
<tr>
<td><code>{'id': '12345', 'name': 'Jane Doe', 'phone': '888-555-1212', 'zip': '01010', ...}</code></td>
<td><code>{'name': 'Jane Doe', 'zip': '01010'}</code></td>
</tr>
</tbody>
</table>
</figure>
<p>The <code>RequestCustomerData</code> API in the left column ignores the Least Information pattern because the caller has no option but to request the complete data record by ID. They don’t need the phone number, so there is <span epub:type="pagebreak" title="59" id="Page_59"/>no need to request it, and even ignoring it still expands the attack surface for an attacker trying to get it. The right column has a version of the same API that allows callers to specify what fields they need and delivers only those, which minimizes the flow of private information. </p>
<p>Considering the Secure by Default pattern as well, the default for the <code>items</code> parameter should be a minimal set of fields, provided that callers can request exactly what they need to minimize information flow. </p>
<h3 id="h2-123456c01-0005">Secure by Default</h3>
<p class="BodyFirst">Software should always be secure “out of the box.” </p>
<p>Design your software to be <em>Secure by Default</em>, including in its initial state, so that inaction by the operator does not represent a risk. This applies to the overall system configuration, as well as configuration options for components and API parameters. Databases or routers with default passwords notoriously violate this pattern, and to this day, this design flaw remains surprisingly widespread. </p>
<p>If you are serious about security, never configure an insecure state with the intention of making it secure later, because this creates an interval of vulnerability and is too often forgotten. If you must use equipment with a default password, for example, first configure it safely on a private network behind a firewall before deploying it in the network. A pioneer in this area, the state of California has mandated this pattern by law; its <a href="https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=201720180SB327" class="LinkURL">Senate Bill No. 327 (2018)</a> outlaws default passwords on connected devices.</p>
<p>Secure by Default applies to any setting or configuration that could have a detrimental security impact, not just to default passwords. Permissions should default to more restrictive settings; users should have to explicitly change them to less restrictive ones if needed, and only if it’s safe to do so. Disable all potentially dangerous options by default. Conversely, enable features that provide security protection by default so they are functioning from the start. And of course, keeping the software fully up-to-date is important; don’t start out with an old version (possibly one with known vulnerabilities) and hope that, at some point, it gets updated.</p>
<p>Ideally, you shouldn’t ever need to have insecure options. Carefully consider proposed configurable options, because it may be simple to provide an insecure option that will become a booby trap for others thereafter. Also remember that each new option increases the number of possible combinations, and the task of ensuring that all of those combinations of settings are actually useful and safe becomes more difficult as the number of options increases. Whenever you must provide unsafe configurations, make a point of proactively explaining the risk to the administrator.</p>
<p>Secure by Default applies much more broadly than to configuration options, though. Defaults for unspecified API parameters should be secure choices. A browser accepting a URL entered into the address bar without any protocol specified should assume the site uses HTTPS, and fall back to HTTP only if the former fails to connect. Two peers negotiating a new HTTPS connection should default to accepting the more secure cipher suite choices first.</p>
<h3 id="h2-123456c01-0006"><span epub:type="pagebreak" title="60" id="Page_60"/>Allowlists over Blocklists</h3>
<p class="BodyFirst">Prefer allowlists over blocklists when designing a security mechanism. <em>Allowlists</em> are enumerations of what’s safe, so they are inherently finite. By contrast, <em>blocklists</em> attempt to enumerate all that isn’t safe, and in doing so implicitly allow an infinite set of things you <em>hope</em> are safe. It’s clear which approach is riskier.</p>
<p>First, here’s a non-software example to make sure you understand what the allowlist versus blocklist alternative means, and why allowlists are always the way to go. During the early months of the COVID-19 stay-at-home emergency order, the governor of my state ordered the beaches closed with the following provisos, presented here in simplified form:</p>
<blockquote class="review">
<p class="Blockquote">No person shall sit, stand, lie down, lounge, sunbathe, or loiter on any beach except when “running, jogging, or walking on the beach, so long as social distancing requirements are maintained” (crossing the beach to surf is also allowed).</p></blockquote>
<p>The first clause is a blocklist, because it lists what activities are not allowed, and the second exception clause is an allowlist, because it grants permission to the activities listed. Due to legal issues, there may well be good reasons for this language, but from a strictly logical perspective, I think it leaves much to be desired. </p>
<p>First let’s consider the blocklist: I’m confident that there are other risky activities people could do at the beach that the first clause fails to prohibit. If the intention of the order was to keep people moving, it omitted many—kneeling, for example, as well as yoga and living statue performances. The problem with blocklists is that any omissions become flaws, so unless you can completely enumerate every possible bad case, it’s an insecure system. </p>
<p>Now consider the allowlist of allowable beach activities. While it, too, is incomplete—who would contest that skipping is also fine?—this won’t cause a big security problem. Perhaps a fraction of a percent of beach skippers will be unfairly punished, but the harm is minor, and more importantly, an incomplete enumeration doesn’t open up a hole that allows a risky activity. Additional safe items initially omitted can easily be added to the allowlist as needed.</p>
<p>More generally, think of a continuum, ranging from disallowed on the left, then shading to allowed on the right. Somewhere in the middle is a dividing line. The goal is to allow the good stuff on the right of the line while disallowing the bad on the left. Allowlists draw the line from the right side, then gradually move it to the left, including more parts of the continuum as the list of what to allow grows. If you omit something good from the allowlist, you’re still on the safe side of the elusive line that’s the true divide. You may never get to the precise point that allows all safe actions, at which point any addition to the list would be too much, but using this technique makes it easy to stay on the safe side. Contrast that to the blocklist approach: unless you enumerate everything to the left of the true divide, you’re allowing something you <span epub:type="pagebreak" title="61" id="Page_61"/>shouldn’t. The safest blocklist will be one that includes just about everything, and that’s likely to be overly restrictive, so it doesn’t work well either way.</p>
<p>Often, the use of an allowlist is so glaringly obvious we don’t notice it as a pattern. For example, a bank would reasonably authorize a small set of trusted managers to approve high-value transactions. Nobody would dream of maintaining a blocklist of all the employees <em>not</em> authorized, tacitly allowing any other employee such privilege. Yet sloppy coders might attempt to do input validation by checking that the value did not contain any of a list of invalid characters, and in the process easily forget about characters like NUL (ASCII 0) or perhaps DEL (ASCII 127).</p>
<p>Ironically, perhaps the biggest-selling consumer security product, antivirus software, attempts to block all known malware. Modern antivirus products are much more sophisticated than the old-school versions, which relied on comparing a digest against a database of known malware, but still, they all appear to work based on a blocklist to some extent. (A great example of Security by Obscurity, most commercial antivirus software is proprietary, so we can only speculate.) It makes sense that they’re stuck with blocklist techniques because they know how to collect examples of malware, and the prospect of somehow allowlisting all good software in the world before it’s released seems to be a nonstarter. My point isn’t about any particular product or an assessment of its worth, but about the design choice of protection by virtue of a blocklist, and why that’s inevitably risky.</p>
<h3 id="h2-123456c01-0007">Avoid Predictability</h3>
<p class="BodyFirst">Any data (or behavior) that is predictable cannot be kept private, since attackers can learn it by guessing.</p>
<p>Predictability of data in software design can lead to serious flaws because it can result in the leakage of information. For instance, consider the simple example of assigning new customer account IDs. When a new customer signs up on a website, the system needs a unique ID to designate the account. One obvious and easy way to do this is to name the first account 1, the second account 2, and so on. This works, but from the point of view of an attacker, what does it give away?</p>
<p>New account IDs now provide an attacker an easy way of learning the number of user accounts created so far. For example, if the attacker periodically creates a new, throwaway account, they have an accurate metric for how many customer accounts the website has at a given time—information that most businesses would be loathe to disclose to a competitor. Many other pitfalls are possible, depending on the specifics of the system. Another consequence of this poor design is that attackers can easily guess the account ID assigned to the next new account created, and armed with this knowledge, they might be able to interfere with the new account setup by claiming to be the new account and confusing the registration system.</p>
<p>The problem of predictability takes many guises, and different types of leakage can occur with different designs. For example, an account ID <span epub:type="pagebreak" title="62" id="Page_62"/>that includes several letters of the account holder’s name or ZIP code would needlessly leak clues about the account owner’s identity. Of course, this same problem applies to IDs for web pages, events, and more. The simplest mitigation against these issues is that if the purpose of an ID is to be a unique handle, you should make it just that—never a count of users, the email of the user, or based on other identifying information. </p>
<p>The easy way to avoid these problems is to use <em>securely random</em> IDs. Truly random values cannot be guessed, so they do not leak information. (Strictly speaking, the length of IDs leaks the maximum number of possible IDs, but this usually isn’t sensitive information.) A standard system facility, random number generators come in two flavors: pseudorandom number generators and secure random number generators. You should use the secure option, which is slower, unless you’re certain that predictability is harmless. See <span class="xref" itemid="xref_target_Chapter 5">Chapter 5</span> for more about secure random number generators.</p>
<h3 id="h2-123456c01-0008">Fail Securely</h3>
<p class="BodyFirst">If a problem occurs, be sure to end up in a secure state.</p>
<p>In the physical world, this pattern is common sense itself. An old-fashioned electric fuse is a great example: if too much current flows through it, the heat melts the metal, opening the circuit. The laws of physics make it impossible to fail in a way that maintains excessive current flow. This pattern perhaps may seem like the most obvious one, but software being what it is (we don’t have the laws of physics on our side), it’s easily disregarded.</p>
<p>Many software coding tasks that at first seem almost trivial often grow in complexity due to error handling. The normal program flow can be simple, but when a connection is broken, memory allocation fails, inputs are invalid, or any number of other potential problems arise, the code needs to proceed if possible, or back out gracefully if not. When writing code, you might feel as though you spend more time dealing with all these distractions than with the task at hand, and it’s easy to quickly dismiss error-handling code as unimportant, making this a common source of vulnerabilities. Attackers will intentionally trigger these error cases if they can, in hopes that there is a vulnerability they can exploit. </p>
<p>Error cases are often tedious to test thoroughly, especially when combinations of multiple errors can compound into new code paths, so this can be fertile ground for attack. Ensure that each error is either safely handled, or leads to full rejection of the request. For example, when someone uploads an image to a photo sharing service, immediately check that it is well formed (because malformed images are often used maliciously), and if not, then promptly remove the data from storage to prevent its further use.</p>
<h2 id="h1-123456c01-0003">Strong Enforcement</h2>
<p class="BodyFirst">These patterns concern how to ensure that code behaves by enforcing the rules thoroughly. Loopholes are the bane of any laws and regulations, so these patterns show how to avoid creating ways of gaming the system. Rather than <span epub:type="pagebreak" title="63" id="Page_63"/>write code and reason that you don’t think it will do something, it’s better to structurally design it so that forbidden operations cannot possibly occur.</p>
<h3 id="h2-123456c01-0009">Complete Mediation</h3>
<p class="BodyFirst">Protect all access paths, enforcing the same access, without exception.</p>
<p>An obscure term for an obvious idea, <em>Complete Mediation</em> means securely checking all accesses to a protected asset consistently. If there are multiple access methods to a resource, they must all be subject to the same authorization check, with no shortcuts that afford a free pass or looser policy. </p>
<p>For example, suppose a financial investment firm’s information system policy declares that regular employees cannot look up the tax IDs of customers without manager approval, so the system provides them with a reduced view of customer records omitting that field. Managers can access the full record, and in the rare instance that a non-manager has a legitimate need, they can ask a manager to look it up. Employees help customers in many ways, one of which is providing replacement tax documents if, for some reason, customers did not receive theirs in the mail. After confirming the customer’s identity, the employee requests a duplicate form (a PDF), which they print out and mail to the customer. The problem with this system is that the customer’s tax ID, which the employee should not have access to, appears on the tax form: that’s a failure of Complete Mediation. A dishonest employee could request any customer’s tax form, as if for a replacement, just to learn their tax ID, defeating the policy preventing disclosure to employees.</p>
<p>The best way to honor this pattern is, wherever possible, to have a single point where a particular security decision occurs. This is often known as a <em>guard</em> or, informally, a <em>bottleneck</em>. The idea is that all accesses to a given asset must go through one gate. Alternatively, if that is infeasible and multiple pathways need guards, then all checks for the same access should be functionally equivalent and ideally implemented as identical code. </p>
<p>In practice, this pattern can be challenging to accomplish consistently. There are different degrees of compliance, depending on the guards in place:</p>
<p class="ListHead"><b>High compliance</b></p>
<ol class="none">
<li>Resource access only allowed via one common routine (bottleneck guard)</li>
</ol>
<p class="ListHead"><b>Medium compliance</b></p>
<ol class="none">
<li>Resource access in various places, each guarded by an identical authorization check (common multiple guards)</li>
</ol>
<p class="ListHead"><b>Low compliance</b></p>
<ol class="none">
<li>Resource access in various places, variously guarded by inconsistent authorization checks (incomplete mediation)</li>
</ol>
<p><span epub:type="pagebreak" title="64" id="Page_64"/>A counter-example demonstrates why designs with simple authorization policies that concentrate authorization checks in a single bottleneck code path for a given resource are the best way to get this pattern right. A Reddit user recently reported a case of how easy it is to get it wrong: </p>
<blockquote class="review">
<p class="Blockquote">I saw that my 8-year-old sister was on her iPhone 6 on iOS 12.4.6 using YouTube past her screen time limit. Turns out, she discovered a bug with screen time in messages that allows the user to use apps that are available in the iMessage App Store.</p></blockquote>
<p class="BodyContinued">Apple designed iMessage to include its own apps, making it possible to invoke the YouTube app in multiple ways, but it didn’t implement the screen-time check on this alternate path to video watching—a classic failure of Complete Mediation. </p>
<p>Avoid having multiple paths for accessing the same resource, each with custom code that potentially works slightly differently, because any discrepancies could mean weaker guards on some paths than on others. Multiple guards would require implementing the same essential check multiple times, and would be more difficult to maintain because you’d need to make matching changes in several places. The use of multiple guards incurs more chances of making an error and more work to thoroughly test.</p>
<h3 id="h2-123456c01-0010">Least Common Mechanism</h3>
<p class="BodyFirst">Maintain isolation between independent processes by minimizing shared mechanisms. </p>
<p>To best appreciate what this means and how it helps, let’s consider an example. The kernel of a multiuser operating system manages system resources for processes running in different user contexts. The design of the kernel fundamentally ensures the isolation of processes unless they explicitly share a resource or a communication channel. Under the covers, the kernel maintains various data structures necessary to service requests from all user processes. This pattern points out that the common mechanism of these structures could inadvertently bridge processes, and therefore it’s best to minimize such opportunities. For example, if some functionality can be implemented in userland code, where the process boundary necessarily isolates it to the process, the functionality will be less likely to somehow bridge user processes. Here, the term <em>bridge</em> specifically means either leaking information, or allowing one process to influence another without authorization.</p>
<p>If that still feels abstract, consider this non-software analogy. You visit your accountant to review your tax return the day before the filing deadline. Piles of papers and folders cover the accountant’s desk like miniature skyscrapers. After shuffling through the chaotic mess, they pull out your paperwork and start the meeting. While waiting, you can see tax forms and bank statements with other people’s names and tax IDs in plain sight. Perhaps your accountant accidentally jots a quick note about your taxes in someone <span epub:type="pagebreak" title="65" id="Page_65"/>else’s file by mistake. This is exactly the kind of bridge between independent parties, created because the accountant uses the desktop as a common workspace, that the Least Common Mechanism strives to avoid. </p>
<p>Next year, you hire a different accountant, and when you meet with them, they pull your file out of a cabinet. They open it on their desk, which is neat, with no other clients’ paperwork in sight. That’s how to do Least Common Mechanism right, with minimal risk of mix-ups or nosy clients seeing other documents.</p>
<p>In the realm of software, apply this pattern by designing services that interface to independent processes or different users. Instead of a monolithic database with everyone’s data in it, can you provide each user with a separate database or otherwise scope access according to the context? There may be good reasons to put all the data in one place, but when you choose not to follow this pattern, be alert to the added risk and explicitly enforce the necessary separation. Web cookies are a great example of using this pattern because each client stores its own cookie data independently.</p>
<h2 id="h1-123456c01-0004">Redundancy</h2>
<p class="BodyFirst"><em>Redundancy</em> is a core strategy for safety in engineering that’s reflected in many common-sense practices, such as spare tires for cars. These patterns show how to apply it to make software more secure.</p>
<h3 id="h2-123456c01-0011">Defense in Depth</h3>
<p class="BodyFirst">Combining independent layers of protection makes for a stronger overall defense that is often synergistically far more effective than any single layer.</p>
<p>This powerful technique is one of the most important patterns we have for making inevitably bug-ridden software systems more secure than their components. Visualize a room that you want to convert to a darkroom by putting plywood over the window. You have plenty of plywood, but somebody has randomly drilled several small holes in every sheet. Nail up just one sheet, and numerous pinholes ruin the darkness. Nail a second sheet on top of that, and unless two holes just happen to align, you now have a completely dark room. A security checkpoint that utilizes both a metal detector and a pat-down is another example of this pattern.</p>
<p>In the realm of software design, deploy <em>Defense in Depth</em> by layering two or more independent protection mechanisms to enforce a particularly critical security decision. Like the holey plywood, there might be flaws in each of the implementations, but the likelihood that any given attack will penetrate both is minuscule, akin to having two plywood holes just happen to line up and let light through. Since two independent checks require double the effort and take twice as long, you should use this technique sparingly.</p>
<p>A great example of this technique that balances the effort and overhead against the benefit is the implementation of a <em>sandbox</em>, a container in which untrusted arbitrary code can run safely. (Modern web browsers run <a href="https://webassembly.org/" class="LinkURL"><span epub:type="pagebreak" title="66" id="Page_66"/>WebAssembly in a secure sandbox</a>.) Running untrusted code in your system could have disastrous consequences if anything goes wrong, justifying the overhead of multiple layers of protection (<a href="#figure4-2" id="figureanchor4-2">Figure 4-2</a>). </p>
<figure>
<img src="image_fi/501928c04/f04002.png" alt="f04002" class=""/>
<figcaption><p><a id="figure4-2">Figure 4-2</a>: An example of a sandbox as the Defense in Depth pattern</p></figcaption>
</figure>
<p>Code for sandbox execution first gets scanned by an analyzer (defense layer one), which examines it against a set of rules. If any violation occurs, the system rejects the code completely. For example, one rule might forbid the use of calls into the kernel; another rule might forbid the use of specific privileged machine instructions. If and only if the code passes the scanner, it then gets loaded into an interpreter that runs the code while also enforcing a number of restrictions intended to prevent the same kinds of overprivileged operations. For an attacker to break this system, they must first get past the scanner’s rule checking and also trick the interpreter into executing the forbidden operation. This example is especially effective because code scanning and interpretation are fundamentally different, so the chances of the same flaw appearing in both layers is low, especially if they’re developed independently. Even if there is a one-in-a-million chance <span epub:type="pagebreak" title="67" id="Page_67"/>that the scanner misses a particular attack technique, and the same goes for the interpreter, once they’re combined, the total system has about a one-in-a-trillion chance of actually failing. That’s the power of this pattern.</p>
<h3 id="h2-123456c01-0012">Separation of Privilege</h3>
<p class="BodyFirst">Two parties are more trustworthy than one.</p>
<p>Also known as <em>Separation of Duty</em>, the <em>Separation of Privilege</em> pattern refers to the indisputable fact that two locks are stronger than one when those locks have different keys entrusted to two different people. While it’s possible that those two people may be in cahoots, that rarely happens; plus, there are good ways to minimize that risk, and in any case it’s way better than relying entirely on one individual.</p>
<p>For example, safe deposit boxes are designed such that a bank maintains the security of the vault that contains all the boxes, and each box holder has a separate key that opens their box. Bankers cannot get into any of the boxes without brute-forcing them, such as by drilling the locks, yet no customer knows the combination that opens the vault. Only when a customer gains access from the bank and then uses their own key can their box be opened.</p>
<p>Apply this pattern when there are distinct overlapping responsibilities for a protected resource. Securing a datacenter is a classic case: the datacenter has a system administrator (or a team of them for a big operation) responsible for operating the machines with superuser access. In addition, security guards control physical access to the facility. These separate duties, paired with corresponding controls of the respective credentials and access keys, should belong to employees who report to different executives in the organization, making collusion less likely and preventing one boss from ordering an extraordinary action in violation of protocol. Specifically, the admins who work remotely shouldn’t have physical access to the machines in the datacenter, and the people physically in the datacenter shouldn’t know any of the access codes to log in to the machines, or the keys needed to decrypt any of the storage units. It would take two people colluding, one from each domain of control, to gain both physical and admin access in order to fully compromise security. In large organizations, different groups might be responsible for various datasets managed within the datacenter as an additional degree of separation.</p>
<p>The other use of this pattern, typically reserved for the most critical functions, is to split one responsibility into multiple duties to avoid any serious consequences as a result of a single actor’s mistake or malicious intent. As extra protection against a backup copy of data possibly leaking, you could encrypt it twice with different keys entrusted separately, so that it can be used only with the help of both parties. An extreme example, triggering a nuclear missile launch, requires two keys turned simultaneously in locks 10 feet apart, ensuring that no individual acting alone could possibly actuate it.</p>
<p>Secure your audit logs by Separation of Privilege, with one team responsible for the recording and reviewing of events and another for initiating the <span epub:type="pagebreak" title="68" id="Page_68"/>events. This means that the admins can audit user activity, but a separate group needs to audit the admins. Otherwise, a bad actor could block the recording of their own corrupt activity or tamper with the audit log to cover their tracks.</p>
<p>You can’t achieve Separation of Privilege within a single computer because an administrator with superuser rights has full control, but there are still many ways to approximate it to good effect. Implementing a design with multiple independent components can still be valuable as a mitigation, even though an administrator can ultimately defeat it, because it makes subversion more complicated; any attack will take longer and the attacker is more likely to make mistakes in the process, increasing their likelihood of being caught. Strong Separation of Privilege for administrators could be designed by forcing the admin to work via a special <code>ssh</code> gateway under separate control that logged their session in full detail and possibly imposed other restrictions.</p>
<p>Insider threats are difficult, or in some cases impossible, to eliminate, but that doesn’t mean mitigations are a waste of time. Simply knowing that somebody is watching is, in itself, a large deterrent. Such precautions are not just about distrust: honest staff should welcome any Separation of Privilege that adds accountability and reduces the risk posed by their own mistakes. Forcing a rogue insider to work hard to cleanly cover their tracks slows them down and raises the odds of their being caught red-handed. Fortunately, human beings have well-evolved trust systems for face-to-face encounters with coworkers, and as a result, insider duplicity is extremely rare in practice.</p>
<h2 id="h1-123456c01-0005">Trust and Responsibility</h2>
<p class="BodyFirst">Trust and responsibility are the glue that makes cooperation work. Software systems are increasingly interconnected and interdependent, so these patterns are important guideposts.</p>
<h3 id="h2-123456c01-0013">Reluctance to Trust</h3>
<p class="BodyFirst">Trust should be always be an explicit choice, informed by solid evidence.</p>
<p>This pattern acknowledges that trust is precious, and so urges skepticism. Before there was software, criminals exploited people’s natural inclination to trust others, dressing up as workmen to gain access, selling snake oil, or perpetrating an endless variety of other scams. <em>Reluctance to Trust</em> tells us not to assume that a person in a uniform is necessarily legit, and to consider that the caller who says they’re with the FBI may be a trickster. In software, this pattern applies to checking the authenticity of code before installing it, and requiring strong authentication before authorization.</p>
<p>The use of HTTP cookies is a great example of this pattern, as <span class="xref" itemid="xref_target_Chapter 11">Chapter 11</span> explains in detail. Web servers set cookies in their response to the client, expecting clients to send back those cookies with future requests. But since clients are under no actual obligation to comply, servers should always take cookies with a grain of salt, and it’s a huge risk to absolutely trust that clients will always faithfully perform this task. </p>
<p><span epub:type="pagebreak" title="69" id="Page_69"/>Reluctance to Trust is important even in the absence of malice. For example, in a critical system, it’s vital to ensure that all components are up to the same high standards of quality and security so as not to compromise the whole. Poor trust decisions, such using code from an anonymous developer (which might contain malware, or simply be buggy) for a critical function quickly undermines security. This pattern is straightforward and rational, yet can be challenging in practice because people are naturally trusting and it can feel paranoid to withhold trust.</p>
<h3 id="h2-123456c01-0014">Accept Security Responsibility</h3>
<p class="BodyFirst">All software professionals have a clear duty to take responsibility for security; they should reflect that attitude in the software they produce. </p>
<p>For example, a designer should include security requirements when vetting external components to incorporate into the system. And at the interface between two systems, both sides should explicitly take on certain responsibilities they will honor, as well as confirm any guarantees they depend on the caller to uphold. </p>
<p>The anti-pattern that you don’t want is to someday encounter a problem and have two developers say to each other, “I thought you were handling security, so I didn’t have to.” In a large system, both sides can easily find themselves pointing the finger at the other. Consider a situation where component A accepts untrusted input (for example, a web frontend server receiving an anonymous internet request) and passes it through, possibly with some processing or reformatting, to business logic in component B. Component A could take no security responsibility at all and blindly pass through all inputs, assuming B will handle the untrusted input safely with suitable validation and error checking. From component B’s perspective, it’s easy to assume that the frontend validates all requests and only passes safe requests on to B, so there is no need for B to worry about this. The right way to handle this situation is by explicit agreement; decide who validates requests and what guarantees to provide downstream, if any. For maximum safety, use Defense in Depth, where both components independently validate the input.</p>
<p>Consider another all-too-common case, where the responsibility gap occurs between the designer and user of the software. Recall the example of configuration settings from our discussion of the Secure by Default pattern, specifically when an insecure option is given. If the designer knows a configurable option to be less secure, they should carefully consider whether providing that option is truly necessary. That is, don’t just give users an option because it’s easy to do, or because “someone, someday, might want this.” That’s tantamount to setting a trap that someone will eventually fall into unwittingly. When valid reasons for a potentially risky configuration exist, first consider methods of changing the design to allow a safe way of solving the problem. Barring that, if the requirement is inherently unsafe, the designer should advise the user and protect them from configuring the option when unaware of the consequences. Not only is it important to <span epub:type="pagebreak" title="70" id="Page_70"/>document the risks and suggest possible mitigations to offset the vulnerability, but users should also receive clear feedback—ideally, something better than the responsibility-ditching “Are you sure? (Learn more: <em>&lt;link&gt;</em>)” dialog.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="box">
<h2>What’s Wrong with the “Are you Sure” Dialog?</h2>
<p class="BoxBodyFirst">This author personally considers “Are you sure?” dialogs and their ilk to almost always be a failure of design, and one that also often compromises security. I have yet to come across an example in which such a dialog is the best possible solution to the problem. When there are security consequences, this practice runs afoul of the Accept Security Responsibility pattern, in that the designer is foisting responsibility on to the user, who may well not be “sure” but has run out of options. To be clear, in these remarks I would not include normal confirmations, such as <code>rm(1)</code> command interactive prompts or other operations where it’s important to avoid accidental operation.</p>
<p>These dialogs can fall victim to the <em>dialog fatigue</em> phenomenon, in which people trying to get something done reflexively dismiss dialogs, almost universally considering them hindrances rather than help. As security conscious as I am, when presented with these dialogs I, too, wonder, “How else can I do what I want to do?” My choices are to either give up on what I want to do or proceed at my own considerable risk—and I can only guess at exactly what that risk is, since even if there is a “learn more” text provided, it never seems to provide a good solution. At this point, “Are you sure?” only signals to me that I’m about to do something I’ll potentially regret, without explaining exactly what might happen and implying there likely is no going back. </p>
<p>I’d like to see a new third option added to these dialogs—“No, I’m not sure but proceed anyway”—and have that logged as a severe error because the software has failed the user. For any situation where security is critical, scrutinize examples of this sort of responsibility offloading and treat them as significant bugs to be eventually resolved. Exactly how to eliminate these will depend on the particulars, but there are some general approaches to accepting responsibility. Be clear as to precisely what is about to happen and why. Keep the wording concise, but provide a link or equivalent reference to a complete explanation and good documentation. Avoid vague wording (“Are you sure you want to do this?”) and show exactly what the target of the action will be (don’t let the dialog box obscure important information). Never use double negatives or confusing phrasing (“Are you sure you want to go back?” where answering “No” selects the action). If possible, provide an undo option; a good pattern, seen more these days, is passively offering an undo following any major action. If there is no way to undo, then in the linked documentation, offer a workaround, or suggest backing up data beforehand if unsure. Let’s strive to reduce these Hobson’s choices in quantity, and ideally confine them to use by professional administrators who have the know-how to accept responsibility.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<h2 id="h1-123456c01-0006"><span epub:type="pagebreak" title="71" id="Page_71"/>Anti-Patterns</h2>
<blockquote class="Epigraph" epub:type="epigraph">
<p class="Epigraph">Learn to see in another’s calamity the ills which you should avoid.</p>
<p class="EpigraphSource">—Publilius Syrus</p>
</blockquote>
<p class="BodyFirst">Some skills are best learned by observing how a master works, but another important kind of learning comes from avoiding the past mistakes of others. Beginning chemists learn to always dilute acid by adding the acid to a container of water—never the reverse, because in the presence of a large amount of acid, the first drop of water reacts suddenly, producing a lot of heat that could instantly boil the water, expelling water and acid explosively. Nobody wants to learn this lesson by imitation, and in that spirit, I present here several anti-patterns best avoided in the interests of security. </p>
<p>The following short sections list a few software security anti-patterns. These patterns may generally carry security risks, so they are best avoided, but they are not actual vulnerabilities. In contrast to the named patterns covered in the previous sections, which are generally recognizable terms, some of these don’t have well-established names, so I have chosen descriptive monikers here for convenience.</p>
<h3 id="h2-123456c01-0015">Confused Deputy</h3>
<p class="BodyFirst">The <em>Confused Deputy</em> problem is a fundamental security challenge that is at the core of many software vulnerabilities. One could say that this is the mother of all anti-patterns. To explain the name and what it means, a short story is a good starting point. Suppose a judge issues a warrant, instructing their deputy to arrest Norman Bates. The deputy looks up Norman’s address, and arrests the man living there. The man insists there is a mistake, but the deputy has heard that excuse before. The plot twist of our story (which has nothing to do with <em>Psycho</em>) is that Norman anticipated getting caught and for years has used a false address. The deputy, confused by this subterfuge, used their arrest authority wrongly; you could say that Norman played them, managing to direct the deputy’s duly granted authority to his own malevolent purposes. (The despicable crime of swatting—falsely reporting an emergency to direct police forces against innocent victims—is a perfect example of the Confused Deputy problem, but I didn’t want to tell one of those sad stories in detail.)</p>
<p>Common examples of confused deputies include the kernel when called by userland code, or a web server when invoked from the internet. The callee is a <em>deputy</em> because the higher-privilege code is invoked to do things on behalf of the lower-privilege caller. This risk derives directly from the trust boundary crossing, which is why those are of such acute interest in threat modeling. In later chapters, numerous ways of confusing a deputy will be covered, including buffer overflows, poor input validation, and cross-site request forgery (CSRF) attacks, just to name a few. Unlike human deputies, who can rely on instinct, past experience, and other cues (including common sense), software is trivially tricked into doing things it wasn’t intended to, unless it’s designed and implemented with all necessary precautions fully anticipated.</p>
<h4 id="h3-123456c01-0001"><span epub:type="pagebreak" title="72" id="Page_72"/>Intention and Malice</h4>
<p class="BodyFirst">To recap from <span class="xref" itemid="xref_target_Chapter 1">Chapter 1</span>, for software to be trustworthy, there are two requirements: it must be built by people you can trust are both honest and competent to deliver a quality product. The difference between the two conditions is intention. The problem with arresting Norman Bates wasn’t that the deputy was crooked; it was failing to properly ID the arrestee. Of course, code doesn’t disobey or get lazy, but poorly-written code can easily work in ways other than how it was intended. While many gullible computer users and occasionally even technically adept software professionals do get tricked into trusting malicious software, many attacks work by exploiting a Confused Deputy in software that is duly trusted but happens to be flawed. </p>
<p>Often, Confused Deputy vulnerabilities arise when the context of the original request gets lost earlier in the code—for example, if the requester’s identity is no longer available. This sort of confusion is especially likely in common code shared by both high- and low-privilege invocations. <a href="#figure4-3" id="figureanchor4-3">Figure 4-3</a> shows what such an invocation looks like.</p>
<figure>
<img src="image_fi/501928c04/f04003.png" alt="f04003" class=""/>
<figcaption><p><a id="figure4-3">Figure 4-3</a>: An example of the Confused Deputy anti-pattern</p></figcaption>
</figure>
<p>The <code>Deputy</code> code in the center performs work for both low- and high-privilege code. When invoked from High on the right, it may do potentially dangerous operations in service of its trusted caller. Invocation from Low represents a trust boundary crossing, so <code>Deputy</code> should only do safe operations appropriate for low-privilege callers. Within the implementation, <code>Deputy</code> uses a subcomponent, <code>Utility</code>, to do its work. Code within <code>Utility</code> has no notion of high- and low-privilege callers, and hence is liable to mistakenly do potentially dangerous operations on behalf of <code>Deputy</code> that low-privilege callers should not be able to do. </p>
<h4 id="h3-123456c01-0002"><span epub:type="pagebreak" title="73" id="Page_73"/>Trustworthy Deputy</h4>
<p class="BodyFirst">Let’s break down how to be a trustworthy deputy, beginning with a consideration of where the danger lies. Recall that trust boundaries are where the potential for confusion begins, because the goal in attacking a Confused Deputy is to leverage its higher privilege. So long as the deputy understands the request and who is requesting it, and the appropriate authorization checks happen, everything should be fine. </p>
<p>Recall the previous example involving the <code>Deputy</code> code, where the problem occurred in the underlying <code>Utility</code> code that did not contend with the trust boundary when called from Low. In a sense, <code>Deputy</code> unwittingly made <code>Utility</code> a Confused Deputy. If <code>Utility</code> was not intended to defend against low-privilege callers, then either <code>Deputy</code> needs to thoroughly shield it from being tricked, or <code>Utility</code> may require modification to be aware of low-privilege invocations.</p>
<p>Another common Confused Deputy failing occurs in the actions taken on behalf of the request. <em>Data hiding</em> is a fundamental design pattern where the implementation hides the mechanisms it uses behind an abstraction, and the deputy works directly on the mechanism though the requester cannot. For example, the deputy might log information as a side effect of a request, but the requester has no access to the log. By causing the deputy to write the log, the requester is leveraging the deputy’s privilege, so it’s important to beware of unintended side effects. If the requester can present a malformed string to the deputy that flows into the log with the effect of damaging the data and making it illegible, that’s a Confused Deputy attack that effectively wipes the log. In this case, the defense begins by noting that a string from the requester can flow into the log and, considering the potential impact that might have, requiring input validation, for example.</p>
<p>The Code Access Security model, mentioned in <span class="xref" itemid="xref_target_Chapter 3">Chapter 3</span>, is designed specifically to prevent Confused Deputy vulnerabilities from arising. When low-privilege code calls high-privilege deputy code, the effective permissions are reduced accordingly. When the deputy needs its greater privileges, it must assert them explicitly, acknowledging that it is working at the behest of lower-privilege code.</p>
<p>In summary, at trust boundaries, handle lower-trust data and lower-privilege invocations with care so as not to become a Confused Deputy. Keep the context associated with requests throughout the process of performing the task so that authorization can be fully checked as needed. Beware that side effects do not allow requesters to exceed their authority. </p>
<h3 id="h2-123456c01-0016">Backflow of Trust</h3>
<p class="BodyFirst"><em>Backflow of Trust</em> is present whenever a lower-trust component controls a higher-trust component. An example of this is when a system administrator uses their personal computer to remotely administer an enterprise system. While the person is duly authorized and trusted, their home computer isn’t within the enterprise regime and shouldn’t be hosting sessions using admin rights. In essence, you can think of this as a structural Elevation of Privilege just waiting to happen.</p>
<p><span epub:type="pagebreak" title="74" id="Page_74"/>While nobody in their right mind would fall into this anti-pattern in real life, it’s surprisingly easy to miss in an information system. Remember that what counts here is not the trust you <em>give</em> components, but how much trust the components <em>merit</em>. Threat modeling can surface potential problems of this variety through an explicit look at trust boundaries.</p>
<h3 id="h2-123456c01-0017">Third-Party Hooks</h3>
<p class="BodyFirst">Another form of the Backflow of Trust anti-pattern is when hooks in a component within your system provide a third party undue access. Consider a critical business system that includes a proprietary component performing some specialized process within the system. Perhaps it uses advanced AI to predict future business trends, consuming confidential sales metrics and updating forecasts daily. The AI component is cutting-edge, and so the company that makes it must tend to it daily. To make it work like a turnkey system, it needs a direct tunnel through the firewall to access the administrative interface. </p>
<p>This also is a perverse trust relationship because this third party has direct access into the heart of the enterprise system, completely outside the purview of the administrators. If the AI provider were dishonest, or compromised, they could easily exfiltrate internal company data, or worse, and there would be no way of knowing. Note that a limited type of hook may not have this problem and would be acceptable. For example, if the hook implements an auto-update mechanism and is only capable of downloading and installing new versions of the software, it may be fine, given a suitable level of trust.</p>
<h3 id="h2-123456c01-0018">Unpatchable Components</h3>
<p class="BodyFirst">It’s almost invariably a matter of when, not if, someone will discover a vulnerability in any given popular component. Once such a vulnerability becomes public knowledge, unless it is completely disconnected from any attack surface, it needs patching promptly. Any component in a system that you cannot patch will eventually become a permanent liability.</p>
<p>Hardware components with preinstalled software are often unpatchable, but for all intents and purposes, so is any software whose publisher has ceased supporting it or gone out of business. In practice, there are many other categories of effectively unpatchable software: unsupported software provided in binary form only; code built with an obsolete compiler or other dependency; code retired by a management decision; code that becomes embroiled in a lawsuit; code lost to ransomware compromise; and, remarkably enough, code written in a language such as COBOL that is so old that, these days, experienced programmers are in short supply. Major operating system providers typically provide support and upgrades for a certain time period, after which the software becomes effectively unpatchable. Even software that is updatable may effectively be no better if the maker fails to provide timely releases. Don’t tempt fate by using anything you are not confident you can update quickly when needed.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p class="BodyFirst">	See Appendix D for a cheat sheet listing the secure design patterns and anti-patterns presented in this chapter.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
</section>


<section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" title="75" id="Page_75"/>5</span><br/>
<span class="ChapterTitle">Cryptography</span></h1>
</header>
<blockquote class="Epigraph" epub:type="epigraph">
<p class="Epigraph">Cryptography is typically bypassed, not penetrated.</p>
<p class="EpigraphSource">—Adi Shamir</p>
</blockquote>
<figure class="opener">
<img src="image_fi/book_art/chapterart.png" alt=""/>
</figure>
<p class="ChapterIntro">Back in high school, I nearly failed driver’s education. This was long ago, when public schools had funding to teach driving and when gasoline contained lead (nobody had threat modeled that brilliant idea). My first attempts at driving had not gone well. I specifically recall the day I first got behind the wheel of the Volkswagen Beetle, a manual transmission car, and the considerable trepidation on the stony face of the PE coach riding shotgun. I soon learned that pushing in the clutch while going downhill caused the car to speed up, not slow down as I’d intended. But from that mistake onward, something clicked, and suddenly I could drive. The coach expressed unguarded surprise, and relief, at this unlikely turn of events. With hindsight, I believe that my breakthrough was due to the hands-on feel of driving stick, which gave me a more direct connection to the vehicle, enabling me to drive by instinct for the first time.</p>
<p><span epub:type="pagebreak" title="76" id="Page_76"/>Just as driver’s ed teaches students how to drive a car safely, but not how to design or do major repairs, this chapter introduces the basic toolset of cryptography by discussing how to use it properly, without going into the nuts and bolts of how it works. To make crypto comprehensible to the less mathematically inclined, this chapter eschews the math, except in one instance, whose inclusion I couldn’t resist because it’s so clever. </p>
<p>This is an unconventional approach to the topic, but also an important one. Crypto tools are underutilized precisely because cryptography has come to be seen as the domain of experts with a high barrier of entry. Modern libraries provide cryptographic functionality, but developers need to know how to use these (and how to use them correctly) for them to be effective. I hope that this chapter serves as a springboard to provide useful intuitions about the potential uses of crypto. You should supplement this with further research as needed for your specific uses. </p>
<h2 id="h1-123456c01-0001">Crypto Tools</h2>
<p class="BodyFirst">At its core, much of modern crypto derives from pure mathematics, so when used properly, it really works. This doesn’t mean the algorithms are provably impenetrable, but that it will take major breakthroughs in mathematics to crack them.  </p>
<p>Crypto provides a rich array of security tools, but for them to be effective, you must use them thoughtfully. As this book repeatedly recommends, rely on high-quality libraries of code that provide complete solutions. It’s important to choose a library that provides an interface at the right level of abstraction, so you fully understand what it is doing. </p>
<p>The history of cryptography and the mathematics behind it are fascinating, but for the purposes of creating secure software, the modern toolbox consists of a modest collection of basic tools. The following list enumerates the basic crypto security functions and describes what each does, as well as what the security of each depends on:</p>
<ul>
<li><em>Random numbers</em> are useful as padding and nonces, but only if they are unpredictable.</li>
<li><em>Message digests</em> (or <em>hash functions</em>) serve as a fingerprint of data, but only if impervious to collisions.</li>
<li><em>Symmetric encryption</em> conceals data based on a secret key the parties share.</li>
<li><em>Asymmetric encryption </em>conceals data based on a secret the recipient knows.</li>
<li><em>Digital signatures</em> authenticate data based on a secret only the signer knows.</li>
<li><em>Digital certificates </em>authenticate signers based on trust in a root certificate.</li>
<li><em>Key exchange</em> allows two parties to establish a shared secret over an open channel, despite eavesdropping.</li>
</ul>
<p>The rest of this chapter will cover these tools and their uses in more detail.</p>
<h2 id="h1-123456c01-0002"><span epub:type="pagebreak" title="77" id="Page_77"/>Random Numbers</h2>
<p class="BodyFirst">Human minds struggle to grasp the concept of randomness. For security purposes, we can focus on <em>unpredictability</em> as the most important attribute of random numbers. As we shall see, these are critical in cases where we must prevent attackers from guessing correctly, in the same way that a predictable password would be weak. Applications for random numbers include authentication, hashing, encryption, and key generation, each of which depends on unpredictability. The following subsections describe the two classes of random numbers available to software, how they differ in predictability, and when to use which kind.</p>
<h3 id="h2-123456c01-0001">Pseudo-Random Numbers </h3>
<p class="BodyFirst"><em>Pseudo-random number generators (PRNGs) </em>use deterministic computations to produce what looks like an infinite sequence of random numbers. The outputs they generate can easily exceed our human capacity for pattern detection, but analysis and adversarial software may easily learn to mimic a PRNG, disqualifying these from use in security contexts because they are predictable. </p>
<p>However, since calculating pseudo-random numbers is very fast, they’re ideal for a broad range of non-security uses. If you want to run a Monte Carlo simulation or randomly assign variant web page designs for A/B testing, for example, a PRNG is the way to go, because even in the unlikely event that someone predicts the algorithm, there’s no real threat. </p>
<p>Taking a look at an example of a pseudo-random number may help solidify your understanding of why it is not truly random. Consider this digit sequence: </p>
<pre><code><code>94657640789512694683983525957098258226205224894077267194782684826</code></code></pre>
<p>Is this sequence random? There happen to be relatively few 1s and 3s, and disproportionally many 2s, but it wouldn’t be unreasonable to find these deviations from a flat distribution in a truly random number. Yet as random as this sequence appears, it’s easy to predict the next digits if you know the trick. And as the pattern of Transparent Design cautions us, it’s risky to assume we can keep our methods secret. In fact, if you entered this string of digits in a simple web search, you would learn that they are the digits of pi 200 decimals out, and that the next few digits will be <code>0147</code>. </p>
<p>As the decimals of an irrational number, the digits of pi have a statistically normal distribution and are, in a colloquial sense, entirely random. On the other hand, as an easily computed and well-known number, this sequence is completely predictable, and hence unsuitable for security purposes.</p>
<h3 id="h2-123456c01-0002">Cryptographically Secure Pseudo-Random Numbers</h3>
<p class="BodyFirst">Modern operating systems provide <em>cryptographically secure</em><em> pseudo-random number generator (CSPRNG)</em> functions to address the shortcomings of PRNGs when you need random bits for security. You may also see this written as <span epub:type="pagebreak" title="78" id="Page_78"/>CSRNG or CRNG; the important part is the “C,” which means it’s secure for crypto. The inclusion of “pseudo” is an admission that these, too, may fall short of perfect randomness, but experts have deemed them unpredictable enough to be secure for all practical purposes.</p>
<p>Use this kind of random number generator when security is at stake. In other words, if the hypothetical ability to predict the value of a supposedly random number weakens your security, use a CSPRNG. This applies to every security use of random numbers mentioned in this book.</p>
<p>Truly random data, by definition, isn’t generated by an algorithm, but comes from an unpredictable physical process. A Geiger counter could be such a <em>hardware random number generator (HRNG)</em>, also known as an <em>entropy source</em>, because the timing of radioactive decay events is random. HRNGs are built into many modern processors, or you can buy a hardware add-on. Software can also contribute entropy, usually by deriving it from the timing of events such as disk accesses, keyboard and mouse input events, and network transmissions that depend on complex interactions with external entities. </p>
<p>One major internet tech company uses an array of lava lamps to colorfully generate random inputs. But consider a threat model of this technique: because the company chooses to display these lava lamps in its corporate office, and in the reception area no less, potential attackers might be able to observe the state of this input and make an educated guess about the entropy source. In practice, however, the lava lamps merely add entropy to a (presumably) more conventional entropy source behind the scenes, mitigating the risk that this display will lead to an easy compromise of the company’s systems.</p>
<p>Entropy sources need time to produce randomness, and a CSPRNG will slow down to a crawl if you demand too many bits too fast. This is the cost of secure randomness, and why PRNGs have an important purpose as a reliably fast alternative. Use CSPRNGs sparingly unless you have a fast HRNG, and where throughput is an issue, test that it won’t become a bottleneck.</p>
<h2 id="h1-123456c01-0003">Message Authentication Codes</h2>
<p class="BodyFirst">A message <em>digest</em> (also called a <em>hash</em>) is a fixed-length value computed from a message using a one-way function. This means that each unique message will have a specific digest, and any tampering will result in a different digest value. Being one-way is important because it means the digest computation is irreversible, so it won’t be possible for an attacker to find a different message that happens to have the same digest result. If you know that the digest matches, then you know that the message content has not been tampered with.</p>
<p>If two different messages produce the same digest, we call this a <em>collision</em>. Since digests map large chunks of data to fixed-length values, collisions are inevitable because there are more possible messages than there are digest values. The defining feature of a good digest function is that collisions are extremely difficult to find. A <em>collision attack</em> succeeds if an attacker finds two different inputs that produce the same digest value. The most devastating <span epub:type="pagebreak" title="79" id="Page_79"/>kind of attack on a digest function is a <em>preimage attack</em>, where, given a specific digest value, the attacker can find an input that produces it. </p>
<p>Cryptographically secure digest algorithms are strong one-way functions that make collisions so unlikely that you can assume they never happen. This assumption is necessary to leverage the power of digests because it means that by comparing two digests for equality, you are essentially comparing the full messages. Think of this as comparing two fingerprints (which is also an informal term for a digest) to determine if they were made by the same finger.</p>
<p>If everyone used the same digest function for everything, then attackers could intensively study and analyze it, and they might eventually find a few collisions or other weaknesses. One way to guard against this is to use <em>keyed hash functions</em>, which take an extra secret key parameter that transforms the digest computation. In effect, a keyed hash function that takes a 256-bit key is a class of 2<sup>256</sup> different functions. These functions are also called <em>message authentication codes (MACs)</em>, because so long as the hash function key is secret, attackers cannot forge them. That is, by using a unique key, you get a customized digest function of your very own.</p>
<h3 id="h2-123456c01-0003">Using MACs to Prevent Tampering</h3>
<p class="BodyFirst">MACs are often used to prevent attackers from tampering with data. Suppose Alice wants to send a message to Bob over a public channel. The two of them have privately shared a certain secret key; they don’t care about eavesdropping, so they don’t need to encrypt their data, but fake messages would be a problem if undetected. Say the evil Mallory is able to tamper with communications on the wire, but she does not know the key. Alice uses the key to compute and send a MAC along with each message. When Bob receives a communication, he computes the MAC of the received message and compares it to the accompanying MAC that Alice sent; if they don’t match, he ignores it as bogus.</p>
<p>How secure is this arrangement at defending against the clever Mallory? First, let’s consider the obvious attacks:</p>
<ul>
<li>If Mallory tampers with the message, its MAC will not match the message digest (and Bob will ignore it).</li>
<li>If Mallory tampers with the MAC, it won’t match the message digest (and Bob will ignore it).</li>
<li>If Mallory concocts a brand-new message, she will have no way to compute the MAC (and Bob will ignore it).</li>
</ul>
<p>However, there is one more case that we need to protect against. Can you spot another opening for Mallory, and how you might defend against it?</p>
<h3 id="h2-123456c01-0004">Replay Attacks</h3>
<p class="BodyFirst">There is a remaining problem with the MAC communication scheme described previously, and it should give you an idea of how tricky using <span epub:type="pagebreak" title="80" id="Page_80"/>crypto tools against a determined attacker is. Suppose that Alice sends daily orders to Bob indicating how many widgets she wants delivered the next day. Mallory observes this traffic and collects message and MAC pairs that Alice sends: she orders three widgets the first day, then five the next. On the third day, Alice orders 10 widgets. At this point, Mallory gets an idea of how to tamper with Alice’s messages. Mallory intercepts Alice’s message and replaces it with a copy of the first day’s message (specifying three widgets), complete with the corresponding MAC that Alice has helpfully computed already and which Mallory recorded earlier.  Of course, this fools Bob.</p>
<p>This is a <em>replay attack</em>, and secure communications protocols need to address it. The problem isn’t that the cryptography is weak, it’s that it wasn’t used properly. In this case, the root problem is that authentic messages ordering three widgets are identical, which is fundamentally a predictability problem. </p>
<h3 id="h2-123456c01-0005">Secure MAC Communications</h3>
<p class="BodyFirst">There are a number of ways to fix Alice and Bob’s protocol and defeat replay attacks, and they all depend on ensuring that messages are always unique and unpredictable. A simple fix might be for Alice to include a timestamp in the message, with the understanding that Bob should ignore messages with old timestamps. Now if Mallory replays Monday’s order of three widgets on Wednesday, Bob will notice when he compares the timestamps and detect the fraud. However, if the messages are frequent or there’s a lot of network latency, then timestamps might not work well.</p>
<p>A more secure solution to the threat of replay attacks would be for Bob to send Alice a <em>nonce</em>—a random number for one-time use—before Alice sends each message. Then Alice can send back a message along with Bob’s nonce and a MAC of the message and nonce combined. This shuts down replay attacks because the nonce varies with every exchange. Mallory could intercept and change the nonce Bob sends, but Bob would notice if a different nonce came back.</p>
<p>Another problem with this simple example is that the messages are short, consisting of just a number of widgets. Setting aside the danger of replay attacks, very short messages are vulnerable to brute-force attacks. The time required to compute a keyed hash function is typically proportional to the message data length, and for just a few bits that computation is going to be fast. The faster Mallory can try different possible hash function keys, the easier it is to guess the right key to match the MAC of an authentic message. Knowing the key, Mallory can now impersonate Alice sending messages. </p>
<p>You can mitigate short message vulnerabilities by padding the messages with random bits until they reach a suitable minimum length. Computing the MACs for these longer messages takes time, but that’s good as it slows down Mallory’s brute-force attack to the point of being infeasible. In fact, it’s desirable for hash functions to be expensive computations for just this reason. This is a situation where it’s important for the padding to be random (as opposed to predictably pseudo-random) to make Mallory work as hard as possible.</p>
<h2 id="h1-123456c01-0004"><span epub:type="pagebreak" title="81" id="Page_81"/>Symmetric Encryption</h2>
<p class="BodyFirst">All encryption conceals messages by transforming the <em>plaintext</em>, or original message, into an unrecognizable form called the <em>ciphertext</em>. Symmetric encryption algorithms use a secret key to customize the message’s transformation for the private use of the communicants, who must agree on a key in advance. The decryption algorithm uses the same secret key to convert ciphertext back to plaintext. We call this reversible transformation <em>symmetric cryptography</em> because knowledge of the secret key allows you to both encrypt and decrypt.</p>
<p>This section introduces a couple of these symmetric encryption algorithms to illustrate their security properties, and explains some of the precautions necessary to use them safely.</p>
<h3 id="h2-123456c01-0006">One-Time Pad</h3>
<p class="BodyFirst">Cryptographers long ago discovered the ideal encryption algorithm, and even though, as we shall see, it is almost never actually used, it’s a great starting point for discussing encryption due to its utter simplicity. Known as the <em>one-time pad</em>, this algorithm requires the communicants to agree on a secret, random string of bits as the encryption key in advance. In order to encrypt a message, the sender exclusive-ors the message with the key, creating the ciphertext. The recipient then exclusive-ors the ciphertext with the same corresponding key bits to recover the plaintext message. Recall that in the exclusive-or (<span class="NSSymbol">⊕</span>) operation, if the key bit is a zero, then the corresponding message bit is unchanged; if the key bit is a one, then the message bit is inverted. <a href="#figure5-1" id="figureanchor5-1">Figure 5-1</a> graphically illustrates a simple example of one-time pad encryption and decryption.</p>
<figure>
<img src="image_fi/501928c05/f05001.png" alt="f05001" class=""/>
<figcaption><p><a id="figure5-1">Figure 5-1</a>: Alice and Bob using one-time pad encryption</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="82" id="Page_82"/>Subsequent messages are encrypted using bits further along in the secret key bit string. When the key is exhausted, the communicants need to somehow agree on a new secret key. There are good reasons it’s a <em>one-time</em> key, as I will explain shortly. Assuming that the key is random, the message bits either randomly invert or stay the same, so there is no way for attackers to discern the original message without knowing the key. Flipping half the bits randomly is the perfect disguise for a message, since either showing or inverting a large majority of the bits would partially reveal the plaintext. Impervious to attack by analysis as this may be, it’s easy to see why this method is rarely used: the key length limits the message length. </p>
<p>Let’s consider the prohibition against reusing one-time pad keys. Suppose that Alice and Bob use the same secret key <code>K</code> to encrypt two distinct plaintext messages, <code>M1</code> and <code>M2</code>. Mallory intercepts both ciphertexts: <code>M1 ⊕ K</code> and <code>M2 ⊕ K</code>. If Mallory exclusive-ors the two encrypted ciphertexts, the key cancels out, because when you exclusive-or any number with itself the result is zero (the ones invert to zeros, while the zeros are unchanged). The result is a weakly encrypted version of the two messages: </p>
<pre><code><code>(M1 ⊕ K) ⊕ (M2 ⊕ K) = (M1 ⊕ M2) ⊕ (K ⊕ K) = M1 ⊕ M2</code></code></pre>
<p>While this doesn’t directly disclose the plaintext, it begins to leak information. Having stripped away the key bits, analysis could reveal clues about patterns within the messages. For example, if either message contains a sequence of zero bits, then the corresponding bits of the other message will leak through. </p>
<p>The one-time key use limitation is a showstopper for most applications: Alice and Bob may not know how much data they want to encrypt in advance, making it infeasible to decide on how long the key will need to be.</p>
<h3 id="h2-123456c01-0007">Advanced Encryption Standard </h3>
<p class="BodyFirst">The <em>Advanced Encryption Standard</em><em> (AES)</em> is a frequently used modern symmetric encryption block cipher<em> </em>algorithm. In a <em>block cipher</em>, long messages are broken up into block-sized chunks, and shorter messages are padded with random bits to fill out the remainder of the block. AES encrypts 128-bit blocks of data using a secret key that is typically 256 bits long. Alice uses the same agreed-upon secret key to encrypt data that Bob uses to decrypt. </p>
<p>Let’s consider some possible weaknesses. If Alice sends identical message blocks to Bob over time, these will result in identical ciphertext, and clever Mallory will notice these repetitions. Even if Mallory can’t decipher the meaning of these messages, this represents a significant information leak that requires mitigation. The communication is also vulnerable to a replay attack because if Alice can resend the same ciphertext to convey the same plaintext message, then Mallory could do that, too.</p>
<p>Encrypting the same message in the same way is known as <em>electronic code book (ECB) mode</em>. Because of the vulnerability to replay attacks, this is usually a poor choice. To avoid this problem, you can use other modes that introduce <span epub:type="pagebreak" title="83" id="Page_83"/>feedback or other differences into subsequent blocks, so that the resulting ciphertext depends on the contents of preceding blocks or the position in the sequence. This ensures that even if the plaintext blocks are identical, the ciphertext results will be completely different. However, while chained encryption of data streams in blocks is advantageous, it does impose obligations on the communicants to maintain context of the ordering to encrypt and decrypt correctly. The choice of encryption modes thus often depends on the particular needs of the application.</p>
<h3 id="h2-123456c01-0008">Using Symmetric Cryptography</h3>
<p class="BodyFirst">Symmetric crypto is the workhorse for modern encryption because it’s fast and secure when applied properly. Encryption protects data communicated over an insecure channel, as well as data at rest in storage. When using symmetric crypto, it’s important to consider some fundamental limitations:</p>
<p class="ListHead"><b>Key establishment</b></p>
<ol class="none">
<li>Crypto algorithms depend on the prearrangement of secret keys, but do not specify how these keys should be established. </li>
</ol>
<p class="ListHead"><b>Key secrecy</b></p>
<ol class="none">
<li>The effectiveness of the encryption entirely depends on maintaining the secrecy of the keys while still having the keys available when needed.</li>
</ol>
<p class="ListHead"><b>Key size</b></p>
<ol class="none">
<li>Larger secret keys are stronger (with a one-time pad being the ideal in theory), but managing large keys becomes costly and unwieldy.</li>
</ol>
<p>Symmetric encryption inherently depends on shared secret keys, and unless Alice and Bob can meet directly for a trusted exchange, it’s challenging to set up. To address this limitation, <em>asymmetric encryption</em> offers some surprisingly useful new capabilities that fit the needs of an internet-connected world.</p>
<h2 id="h1-123456c01-0005">Asymmetric Encryption</h2>
<p class="BodyFirst">Asymmetric cryptography is a deeply counterintuitive form of encryption, and therein lies its power. With symmetric encryption Alice and Bob can both encrypt and decrypt messages using the same key, but with asymmetric encryption Bob can send secret messages to Alice that he is unable to decrypt. Thus, for Bob encryption is a one-way function, while only Alice knows the secret that enables her to invert the function (that is, to decrypt the message). </p>
<p>Asymmetric cryptography uses a pair of keys: a <em>public key</em> for encryption and a <em>private key</em> for decryption. I will describe how Bob, or anyone in the world for that matter, sends encrypted messages to Alice; for a two-way conversation, Alice would reply using the same process with Bob’s entirely separate key pair. The transformations made using the two keys are inverse functions, yet knowing only one of the keys does not help to figure out the <span epub:type="pagebreak" title="84" id="Page_84"/>other; so if you keep one key secret, then only you can perform that computation. As a result of this asymmetry, Alice can create a key pair and then publish one key for the world to see (her public key), enabling anyone to encrypt messages that only she can decrypt using her corresponding private key. This is revolutionary, because it grants Alice a unique capability based on knowing a secret. We shall see in the following pages all that this makes possible.</p>
<p>There are many asymmetric encryption algorithms, but the mathematical details of these are unimportant to understanding using them as crypto tools—what’s important is that you understand the security implications. We’ll focus on RSA, as it’s the least mathematically complicated progenitor.</p>
<h3 id="h2-123456c01-0009">The RSA Cryptosystem</h3>
<p class="BodyFirst">At MIT, I had the great fortune of working with two of the inventors of the RSA cryptosystem, and my bachelor’s thesis explored how asymmetric cryptography could improve security. The following simplified discussion follows the <a href="https://people.csail.mit.edu/rivest/Rsapaper.pdf" class="LinkURL">original RSA paper</a>, though (for various technical reasons that we don’t need to go into here) modern implementations are more involved.</p>
<p>The core idea of RSA is that it’s easy to multiply two large prime numbers together, but given that product, it’s infeasible to factor it into the constituent primes. To get started, choose a pair of random large prime numbers, which you will keep secret. Next, multiply the pair of primes together. From the result, which we’ll call N, you can compute a unique key pair. Each of these keys, together with N, allows you compute two functions D and E that are inverse functions. That is, for any positive integer <em>x </em>&lt; N, D(E(<em>x</em>)) is <em>x</em>, and E(D(<em>x</em>)) is also <em>x</em>. Finally, choose one of the keys of the key pair as your private key, and publicize to the world the other as the corresponding public key, along with N. So long as you keep the private key and the original two primes secret, only you can efficiently compute the function D.  </p>
<p>Here’s how Bob encrypts a message for Alice, and how she decrypts it. Here the functions E<sub>A</sub> and D<sub>A</sub> are based on Alice’s public and private keys, respectively, along with N:</p>
<ul>
<li>Bob encrypts a ciphertext C from message M for Alice using her public key: C = E<sub>A</sub>(M).</li>
<li>Alice decrypts message M from Bob’s ciphertext C using her private key: M = D<sub>A</sub>(C).</li>
</ul>
<p>Since the public key is not a secret, we assume that the attacker Mallory knows it, and this does raise a new concern particular to public key crypto. If an eavesdropper can guess a predictable message, they can encrypt various likely messages themselves using the public key and compare the results to the ciphertext transmitted on the wire. If they ever see matching ciphertext transmitted, they know the plaintext that produced it. Such a <em>chosen plaintext attack</em> is easily foiled by padding messages with a suitable number of random bits to make guessing impractical.</p>
<p>RSA was not the first published asymmetric cryptosystem, but it made a big splash because cracking it (that is, deducing someone’s private key from their public key) requires solving the well-known hard problem of factoring <span epub:type="pagebreak" title="85" id="Page_85"/>the product of large prime numbers. Since I was collaborating in a modest way with the inventors of RSA at the time of its public debut, I can offer a historical note that may be of interest about its significance then versus now. The algorithm was too compute-intensive for the computers of its day, so its use required expensive custom hardware. As a result, we envisioned it being used only by large financial institutions or military intelligence agencies. We knew about Moore’s law, which proposed that computational power increases exponentially over time—but nobody imagined then that 40 years later everyday people would routinely use connected mobile smartphones with processors capable of doing the necessary number crunching!</p>
<p>Today, RSA is being replaced by newer methods such as <em>elliptic curve algorithms</em>. These algorithms, which rely on different mathematics to achieve similar capabilities, offer more “bang for the buck,” producing strong encryption with less computation. Since asymmetric crypto is typically more computationally expensive than symmetric crypto, encryption is usually handled by choosing a random secret key, asymmetrically encrypting that, and then symmetrically encrypting the message itself.</p>
<h2 id="h1-123456c01-0006">Digital Signatures</h2>
<p class="BodyFirst">Public key cryptography can also be used to create digital signatures, giving the receiving party assurance of authenticity. Independent of message encryption, Alice’s signature assures Bob that a message is really from her. It also serves as evidence of the communication, should Alice deny having sent it. As you’ll recall from <span class="xref" itemid="xref_target_Chapter 2">Chapter 2</span>, authenticity and non-repudiability are two of the most important security properties for communication, after confidentiality.</p>
<p>Let’s walk through an example to illustrate exactly how this works. Alice creates digital signatures using the same key pair that makes public key encryption possible. Because only Alice knows the private key, only she can compute the signature function S<sub>A</sub>. Bob, or anyone with the public key (and N), can verify Alice’s signature by checking it using the function V<sub>A</sub>. In other words:</p>
<ul>
<li>Alice signs message M to produce a signature S = S<sub>A</sub>(M).</li>
<li>Bob verifies that the message M is from Alice by checking if M = V<sub>A</sub>(S).</li>
</ul>
<p>There are a few more details to explain so you fully understand how digital signatures work. Since verification only relies on the public key, Bob can prove to a third party that Alice signed a message without compromising Alice’s private key. Also, signing and encrypting messages are independent: you can do one, the other, or both as appropriate for the application. We won’t tackle the underlying math of RSA in this book, but you should know that the signature and decryption functions (both require the private key) are in fact the same computation, as are the verification and encryption functions (using the public key). To avoid confusion, it’s best to call them by different names according to their purpose.</p>
<p><span epub:type="pagebreak" title="86" id="Page_86"/><a href="#figure5-2" id="figureanchor5-2">Figure 5-2</a> summarizes the fundamental differences between symmetric encryption on the left, and asymmetric on the right. With symmetric encryption, signing isn’t possible because both communicants know the secret key. The security of asymmetric encryption depends on a private key known only to one communicant, so they alone can use it for signatures. Since verification only requires the public key, no secrets are disclosed in the process. </p>
<figure>
<img src="image_fi/501928c05/f05002.png" alt="f05002" class=""/>
<figcaption><p><a id="figure5-2">Figure 5-2</a>: A comparison of symmetric and asymmetric cryptography</p></figcaption>
</figure>
<p>Digital signatures are widely used to sign digital certificates (the subject of the next section), emails, application code, and legal documents, and to secure cryptocurrencies such as Bitcoin. By convention, digests of messages are signed as a convenience so that one signing operation covers an entire document. Now you can appreciate why a successful preimage attack on a digest function is very bad. If Mallory can concoct a payment agreement with the same message digest, Bob’s promissory note also serves as a valid signature for it. </p>
<h2 id="h1-123456c01-0007">Digital Certificates</h2>
<p class="BodyFirst">When I was first learning about the RSA algorithm from the inventors, we brainstormed at MIT about possible future applications. The defining <span epub:type="pagebreak" title="87" id="Page_87"/>advantage of public key crypto was the convenience it offered. It let you use one key for all of your correspondence, rather than managing separate keys for each correspondent, so long as you could announce your public key to the world for anyone to use. But how would one do that?</p>
<p>I came up with an answer in my thesis research and the idea has since been widely implemented. To promote the new phenomenon of digital public key crypto, we needed a new kind of organization, called a <em>certificate authority</em><em> (CA)</em>. To get started, a new CA would widely publish its public key. In time, operating systems and browsers would preinstall a trustworthy set of CA <em>root certificates</em>, which are self-signed with their respective public keys.</p>
<p>The CAs collect public keys from applicants, usually for a fee, and then publish a digital certificate for each that lists their name (such as “Alice”) and other details about them, along with their public key. The CA signs a digest of the digital certificate to ensure its authenticity. In theory, an important part of the CA’s service would involve reviewing the application to ensure that it really came from Alice, and people would choose to trust a CA only if it performed this reliably. In practice, it’s very hard to verify identities, especially over the internet, and this has proven problematic. </p>
<p>Once Alice has a digital certificate, she can send people a copy of it whenever she wants to communicate with them. If they trust the CA that issued it, then they have its public key and can validate the digital certificate signature that provides the public key that belongs to “Alice.” The digital certificate is basically a signed message from the CA stating that “Alice’s public key is X.” At that point, the recipient can immediately start encrypting messages for Alice, typically by first sending their own digital certificate in a signed message to assure Alice that her message got to the right person.</p>
<p>This simplified explanation of digital certificates focuses on how trusted CAs authenticate the association of a name with a public key. In practice, there is more to it; people do not always have unique names, names change, corporations in different states may have the same name, and so on. (<span class="xref" itemid="xref_target_Chapter 11">Chapter 11</span> digs into some of these complicating issues in the context of web security.) Today, digital certificates are used to bind keys to various identities, including web server domain names and email addresses, and for a number of specific purposes, such as code signing.</p>
<h2 id="h1-123456c01-0008">Key Exchange</h2>
<p class="BodyFirst">Whitfield Diffie and Martin Hellman developed a practical key exchange algorithm shortly before the invention of RSA. To understand the miracle of key exchange, imagine that Alice and Bob have somehow established a communication channel, but they have no prior arrangement of a secret key, or even a CA to trust as a source of public keys. Incredibly, key exchange allows them to establish a secret over an open channel while Mallory observes everything. The fact that this is possible is so counterintuitive that in this case I want to show the math so you can see for yourself how it works.</p>
<p><span epub:type="pagebreak" title="88" id="Page_88"/>Fortunately, the math is simple enough and, for small numbers, easy to compute. The only notation that might be unfamiliar to some readers is the suffix <em>(mod p)</em>, which means to divide by the integer <em>p</em> to yield the remainder of division. For example, 2<sup>7</sup> (mod 103) is 25, because 128 – 103 = 25. </p>
<p>This is the basis of the Diffie–Hellman key exchange algorithm:</p>
<ol class="decimal">
<li value="1">Alice and Bob openly agree on a prime number <em>p</em> and a random number <em>g (1 &lt; g &lt; p)</em>.</li>
<li value="2">Alice picks a random natural number <em>a (1 &lt; a &lt; p)</em>, and sends <em>g</em><sup><em>a</em></sup><em> (mod p)</em> to Bob.</li>
<li value="3">Bob picks a random natural number <em>b (1 &lt; b &lt; p)</em>, and sends <em>g</em><sup><em>b</em></sup><em> (mod p)</em> to Alice.</li>
<li value="4">Alice computes <em>S = (g</em><sup><em>b</em></sup><em>)</em><sup><em>a</em></sup><em> (mod p)</em> as their shared secret <em>S</em>.</li>
<li value="5">Bob computes <em>S = (g</em><sup><em>a</em></sup><em>)</em><sup><em>b</em></sup><em> (mod p)</em>, getting the same shared secret <em>S</em> as Alice.</li>
</ol>
<p><a href="#figure5-3" id="figureanchor5-3">Figure 5-3</a> illustrates a toy example using small numbers to show that this actually works. This example isn’t secure, because an exhaustive search of about 60 possibilities is easy to do. However, the same math works for big numbers, and at the scale of a few hundred digits, it’s wildly infeasible to do such an exhaustive search. </p>
<figure>
<img src="image_fi/501928c05/f05003.png" alt="f05003" class=""/>
<figcaption><p><a id="figure5-3">Figure 5-3</a>: Alice and Bob securely choosing a shared secret via key exchange</p></figcaption>
</figure>
<p>In this example, chosen to keep the numbers small, by coincidence Alice chooses 6, which happens to equal Bob’s result (<em>g</em><sup><em>b</em></sup>). That wouldn’t happen in practice, but of course the algorithm still works and only Alice would notice the coincidence. </p>
<p><span epub:type="pagebreak" title="89" id="Page_89"/>It’s important that both parties actually choose secure random numbers from a CSPRNG in order to prevent Mallory from possibly guessing their choices. For example, if Bob used a formula to compute his choice from <em>p</em> and <em>g</em>, Mallory might deduce that by observing many key exchanges and eventually mimic it, breaking the secrecy of the key exchange.</p>
<p>Key exchange is basically a magic trick that doesn’t require any deception. Alice and Bob walk in from the wings of the stage with Mallory standing right in the middle. Alice calls out numbers, Bob answers, and after two back-and-forth exchanges Mallory is still clueless. Alice and Bob write their shared secret numbers on large cards, and at a signal hold up their cards to reveal identical numbers representing the agreed secret.</p>
<p>Today, key exchange is critical to establishing a secure communication channel over the internet between any two endpoints. Most applications use elliptic curve key exchange because those algorithms are more performant, but the concept is much the same. Key exchange is particularly handy in setting up secure communication channels (such as with the TLS protocol) on the internet. The two endpoints first use a TCP channel—traffic that Mallory may be observing—then do key exchange to negotiate a secret with the as-yet-unconfirmed opposite communicant. Once they have a shared secret, encrypted communication enables a secure private channel. This is how any pair of communicants can bootstrap a secure channel without a prearranged secret.</p>
<h2 id="h1-123456c01-0009">Using Crypto</h2>
<p class="BodyFirst">This chapter explained the tools in the crypto toolbox at the “driver’s ed” level. Cryptographically secure random numbers add unpredictability to thwart attacks based on guessing. Digests are a secure way of distilling the uniqueness of data to a corresponding token for integrity checking. Encryption, available in both symmetric and asymmetric forms, protects confidentiality. Digital signatures are a way of authenticating messages. Digital certificates make it easy to share authentic public keys by leveraging trust in CAs. And key exchange rounds out the crypto toolbox, allowing remote parties to securely agree on a secret key via a public network connection. </p>
<p>The comic in <a href="#figure5-4" id="figureanchor5-4">Figure 5-4</a> illustrates the point made by the epigraph that opens this chapter: that well-built cryptography is so strong, the major threat is that it will be circumvented. Perhaps the most important takeaway from this chapter is that it’s crucial to use crypto correctly so you don’t inadvertently provide just such an opening for attack.</p>
<p>Crypto can help with many security challenges that arise in the design of your software, or which you identify by threat modeling. If your system must send data over the internet to a partner datacenter, encrypt it (for confidentiality) and digitally sign it (for integrity)—or you could do it the easy way with a TLS secure channel that authenticates the endpoints. Secure <span epub:type="pagebreak" title="90" id="Page_90"/>digests provide a nifty way to test for data equality, including as MACs, without you needing to store a complete copy of the data. Typically, you will use existing crypto services rather than building your own, and this chapter gives you an idea of when and how to use them, as well as some of the challenges involved in using the technology securely.</p>
<figure>
<img src="image_fi/501928c05/f05004.png" alt="f05004" class=""/>
<figcaption><p><a id="figure5-4">Figure 5-4</a>: Security versus the $5 wrench (courtesy of Randall Munroe, <a href="http://xkcd.com/538" class="LinkURL">xkcd.com/538</a>)</p></figcaption>
</figure>
<p>Financial account balances and credit card information are clear examples of data you absolutely must protect. This kind of sensitive data flows through a larger distributed system, and even with limited access to the facility, you don’t want someone to be able to physically plug in a network tap and siphon off sensitive data. One powerful mitigation would be to encrypt all incoming sensitive data immediately when it first hits the frontend web servers. Immediately encrypting credit card numbers with a public key enables you to pass around the encrypted data as opaque blobs while processing the transaction. Eventually, this data reaches the highly protected financial processing machine, which knows the private key and can decrypt the data and reconcile the transaction with the banking system. This approach allows most application code to safely pass along sensitive data for subsequent processing without risking disclosure itself.</p>
<p>Another common technique is storing symmetrically encrypted data and the secret key in separate locations. For example, consider an enterprise that wants to outsource long-term data storage for backup to a third party. They would hand over encrypted data for safekeeping while keeping the key in their own vault for use, should they need to restore from a backup. In terms of threats, the data storage service is being entrusted to protect integrity (because they could lose the data), but as long as the key is safe and the crypto was done right, there is no risk to confidentiality.</p>
<p>These are just a few common usages, and you will find many more ways to use these tools. (Cryptocurrency is one particularly clever application.) Modern operating systems and libraries provide mature implementations of a number of currently viable algorithms so you never have to even think about implementing the actual computations yourself. </p>
<p><span epub:type="pagebreak" title="91" id="Page_91"/>Encryption is not a panacea, however, and if attackers can observe the frequency and volume of encrypted data or other metadata, you may disclose some information to them. For example, consider a cloud-based security camera system that captures images when it detects motion in the house. When the family is away, there is no motion, and hence no transmission from the cameras. Even if the images were encrypted, an attacker able to monitor the home network could easily infer the family’s daily patterns and confirm when the house was unoccupied by the drop in camera traffic.</p>
<p>The security of cryptography rests on the known limits of mathematics and the state of the art of digital hardware technology, and both of these are inexorably progressing. Great fame awaits the mathematician who may someday find more efficient computational methods that undermine modern algorithms. Additionally, the prospect of a different kind of computing technology, such as quantum physics, is another potential threat. It is even possible that some powerful nation-state has already achieved such a breakthrough, and is currently using it discreetly, so as not to tip their hand. Like all mitigations, crypto inherently includes trade-offs and unknown risks, but it’s still a great set of tools well worth using. </p>
</section>
</body></html>