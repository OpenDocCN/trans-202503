<html><head></head><body><div id="sbo-rt-content"><h2 class="h2" id="ch06"><span epub:type="pagebreak" id="page_115"/><span class="big">6</span><br/>DISASSEMBLY AND BINARY ANALYSIS FUNDAMENTALS</h2>&#13;
<p class="noindent">Now that you know how binaries are structured and are familiar with basic binary analysis tools, it’s time to start disassembling some binaries! In this chapter, you’ll learn about the advantages and disadvantages of some of the major disassembly approaches and tools. I’ll also discuss some more advanced analysis techniques to analyze the control- and data-flow properties of disassembled code.</p>&#13;
<p class="indent">Note that this chapter is not a guide to reverse engineering; for that, I recommend Chris Eagle’s <em>The IDA Pro Book</em> (No Starch Press, 2011). The goal is to get familiar with the main algorithms behind disassembly and learn what disassemblers can and cannot do. This knowledge will help you better understand the more advanced techniques discussed in later chapters, as these techniques invariably rely on disassembly at their core. Throughout this chapter, I’ll use <span class="literal">objdump</span> and IDA Pro for most of the examples. In some of the examples, I’ll use pseudocode to simplify the discussion. <a href="appc.xhtml">Appendix C</a> contains a list of well-known disassemblers you can try if you want to use a disassembler other than IDA Pro or <span class="literal">objdump</span>.</p>&#13;
<h3 class="h3" id="ch06_1">6.1 <span epub:type="pagebreak" id="page_116"/>Static Disassembly</h3>&#13;
<p class="noindent">You can classify all binary analysis as either static analysis, dynamic analysis, or a combination of both. When people say “disassembly,” they usually mean <em>static disassembly</em>, which involves extracting the instructions from a binary without executing it. In contrast, <em>dynamic disassembly</em>, more commonly known as <em>execution tracing</em>, logs each executed instruction as the binary runs.</p>&#13;
<p class="indent">The goal of every static disassembler is to translate <em>all</em> code in a binary into a form that a human can read or a machine can process (for further analysis). To achieve this goal, static disassemblers need to perform the following steps:</p>&#13;
<ol>&#13;
<li class="noindent">Load a binary for processing, using a binary loader like the one implemented in <a href="ch04.xhtml#ch04">Chapter 4</a>.</li>&#13;
<li class="noindent">Find all the machine instructions in the binary.</li>&#13;
<li class="noindent">Disassemble these instructions into a human- or machine-readable form.</li>&#13;
</ol>&#13;
<p class="indent">Unfortunately, step 2 is often very difficult in practice, resulting in disassembly errors. There are two major approaches to static disassembly, each of which tries to avoid disassembly errors in its own way: <em>linear disassembly</em> and <em>recursive disassembly</em>. Unfortunately, neither approach is perfect in every case. Let’s discuss the trade-offs of these two static disassembly techniques. I’ll return to dynamic disassembly later in this chapter.</p>&#13;
<p class="indent"><a href="ch06.xhtml#ch06fig1">Figure 6-1</a> illustrates the basic principles of linear and recursive disassembly. It also highlights some types of disassembly errors that may occur with each approach.</p>&#13;
<div class="image"><a id="ch06fig1"/><img src="Images/f116-01.jpg" alt="image" width="522" height="444"/></div>&#13;
<p class="fig-caption"><em>Figure 6-1: Linear versus recursive disassembly. Arrows show the disassembly flow. Gray blocks show missed or corrupted code.</em></p>&#13;
<h4 class="h4" id="ch06_1_1"><span epub:type="pagebreak" id="page_117"/><em>6.1.1 Linear Disassembly</em></h4>&#13;
<p class="noindent">Let’s begin with linear disassembly, which is conceptually the simplest approach. It iterates through all code segments in a binary, decoding all bytes consecutively and parsing them into a list of instructions. Many simple disassemblers, including <span class="literal">objdump</span> from <a href="ch01.xhtml#ch01">Chapter 1</a>, use this approach.</p>&#13;
<p class="indent">The risk of using linear disassembly is that not all bytes may be instructions. For example, some compilers, such as Visual Studio, intersperse data such as jump tables with the code, without leaving any clues as to where exactly that data is. If disassemblers accidentally parse this <em>inline data</em> as code, they may encounter invalid opcodes. Even worse, the data bytes may coincidentally correspond to valid opcodes, leading the disassembler to output bogus instructions. This is especially likely on dense ISAs like x86, where most byte values represent a valid opcode.</p>&#13;
<p class="indent">In addition, on ISAs with variable-length opcodes, such as x86, inline data may even cause the disassembler to become desynchronized with respect to the true instruction stream. Though the disassembler will typically self-resynchronize, desynchronization can cause the first few real instructions following inline data to be missed, as shown in <a href="ch06.xhtml#ch06fig2">Figure 6-2</a>.</p>&#13;
<div class="image"><a id="ch06fig2"/><img src="Images/f117-01.jpg" alt="image" width="626" height="548"/></div>&#13;
<p class="fig-caption"><em>Figure 6-2: Disassembly desynchronization due to inline data interpreted as code. The instruction where the disassembly resynchronizes is shaded gray.</em></p>&#13;
<p class="indent">The figure illustrates <em>disassembler desynchronization</em> in part of a binary’s code section. You can see a number of inline data bytes (<span class="literal">0x8e 0x20 0x5c 0x00</span>), followed by some instructions (<span class="literal">push rbp</span>, <span class="literal">mov rbp,rsp</span>, and so on). The <span epub:type="pagebreak" id="page_118"/>correct decoding of all the bytes, as would be found by a perfectly synchronized disassembler, is shown on the left of the figure under “synchronized.” But a naive linear disassembler instead interprets the inline data as code, thus decoding the bytes as shown under “−4 bytes off.” As you can see, the inline data is decoded as a <span class="literal">mov fs,[rax]</span> instruction, followed by a <span class="literal">pop rsp</span> and an <span class="literal">add [rbp+0x48],dl</span>. This last instruction is especially nasty because it stretches beyond the inline data region and into the real instructions! In doing so, the <span class="literal">add</span> instruction “eats up” some of the real instruction bytes, causing the disassembler to miss the first two real instructions altogether. The disassembler encounters similar problems if it starts 3 bytes too early (“−3 bytes off”), which may happen if the disassembler tries to skip the inline data but fails to recognize all of it.</p>&#13;
<p class="indent">Fortunately, on x86, the disassembled instruction stream tends to automatically resynchronize itself after just a few instructions. But missing even a few instructions can still be bad news if you’re doing any kind of automated analysis or you want to modify the binary based on the disassembled code. As you’ll see in <a href="ch08.xhtml#ch08">Chapter 8</a>, malicious programs sometimes intentionally contain bytes designed to desynchronize disassemblers to hide the program’s true behavior.</p>&#13;
<p class="indent">In practice, linear disassemblers such as <span class="literal">objdump</span> are safe to use for disassembling ELF binaries compiled with recent versions of compilers such as <span class="literal">gcc</span> or LLVM’s <span class="literal">clang</span>. The x86 versions of these compilers don’t typically emit inline data. On the other hand, Visual Studio <em>does</em>, so it’s good to keep an eye out for disassembly errors when using <span class="literal">objdump</span> to look at PE binaries. The same is true when analyzing ELF binaries for architectures other than x86, such as ARM. And if you’re analyzing malicious code with a linear disassembler, well, all bets are off, as it may include obfuscations far worse than inline data!</p>&#13;
<h4 class="h4" id="ch06_1_2"><em>6.1.2 Recursive Disassembly</em></h4>&#13;
<p class="noindent">Unlike linear disassembly, recursive disassembly is sensitive to control flow. It starts from known entry points into the binary (such as the main entry point and exported function symbols) and from there recursively follows control flow (such as jumps and calls) to discover code. This allows recursive disassembly to work around data bytes in all but a handful of corner cases.<sup><a id="ch06fn_1a" href="footnote.xhtml#ch06fn_1">1</a></sup> The downside of this approach is that not all control flow is so easy to follow. For instance, it’s often difficult, if not impossible, to statically figure out the possible targets of indirect jumps or calls. As a result, the disassembler may miss blocks of code (or even entire functions, such as <em>f</em><sub>1</sub> and <em>f</em><sub>2</sub> in <a href="ch06.xhtml#ch06fig1">Figure 6-1</a>) targeted by indirect jumps or calls, unless it uses special (compiler-specific and error-prone) heuristics to resolve the control flow.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_119"/>Recursive disassembly is the de facto standard in many reverse-engineering applications, such as malware analysis. IDA Pro (shown in <a href="ch06.xhtml#ch06fig3">Figure 6-3</a>) is one of the most advanced and widely used recursive disassemblers. Short for <em>Interactive DisAssembler</em>, IDA Pro is meant to be used interactively and offers many features for code visualization, code exploration, scripting (in Python), and even decompilation<sup><a id="ch06fn_2a" href="footnote.xhtml#ch06fn_2">2</a></sup> that aren’t available in simple tools like <span class="literal">objdump</span>. Of course, there’s a price tag to match: at the time of writing, licenses for IDA Starter (a simplified edition of IDA Pro) start at $739, while full-fledged IDA Professional licenses go for $1,409 and up. But don’t worry—you don’t need to buy IDA Pro to use this book. This book focuses not on interactive reverse engineering but on creating your own automated binary analysis tools based on free frameworks.</p>&#13;
<div class="image"><a id="ch06fig3"/><img src="Images/f119-01.jpg" alt="image" width="672" height="493"/></div>&#13;
<p class="fig-caption"><em>Figure 6-3: IDA Pro’s graph view</em></p>&#13;
<p class="indent"><a href="ch06.xhtml#ch06fig4">Figure 6-4</a> illustrates some of the challenges that recursive disassemblers like IDA Pro face in practice. Specifically, the figure shows how a simple function from <span class="literal">opensshd</span> v7.1p2 is compiled by <span class="literal">gcc</span> v5.1.1 from C to x64 code.</p>&#13;
<div class="image"><a id="ch06fig4"/><img src="Images/f120-01.jpg" alt="image" width="645" height="1142"/></div>&#13;
<p class="fig-caption"><em>Figure 6-4: Example of a disassembled switch statement (from</em> <span class="literal">opensshd</span> <em>v7.1p2 compiled with</em> <span class="literal">gcc</span> <em>5.1.1 for x64, source edited for brevity). Interesting lines are shaded.</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_120"/><span epub:type="pagebreak" id="page_121"/>As you can see on the left side of the figure, which shows the C representation of the function, the function does nothing special. It uses a <span class="literal">for</span> loop to iterate over an array, applying a <span class="literal">switch</span> statement in each iteration to determine what to do with the current array element: skip uninteresting elements, return the index of an element that meets some criteria, or print an error and exit if something unexpected happens. Despite the simplicity of the C code, the compiled version of this function (shown on the right side of the figure) is far from trivial to disassemble correctly.</p>&#13;
<p class="indent">As you can see in <a href="ch06.xhtml#ch06fig4">Figure 6-4</a>, the x64 implementation of the switch statement is based on a <em>jump table</em>, a construct commonly emitted by modern compilers. This jump table implementation avoids the need for a complicated tangle of conditional jumps. Instead, the instruction at address <span class="literal">0x4438f9</span> uses the switch input value to compute (in <span class="literal">rax</span>) an index into a table, which stores at that index the address of the appropriate case block. This way, only the single indirect jump at address <span class="literal">0x443901</span> is required to transfer control to any case address the jump table defines.</p>&#13;
<p class="indent">While efficient, jump tables make recursive disassembly more difficult because they use <em>indirect control flow</em>. The lack of an explicit target address in the indirect jump makes it difficult for the disassembler to track the flow of instructions past this point. As a result, any instructions that the indirect jump may target remain undiscovered unless the disassembler implements specific (compiler-dependent) heuristics to discover and parse jump tables.<sup><a id="ch06fn_3a" href="footnote.xhtml#ch06fn_3">3</a></sup> For this example, this means a recursive disassembler that doesn’t implement switch-detection heuristics won’t discover the instructions at addresses <span class="literal">0x443903</span>–<span class="literal">0x443925</span> at all.</p>&#13;
<p class="indent">Things get even more complicated because there are multiple <span class="literal">ret</span> instructions in the switch, as well as calls to the <span class="literal">fatal</span> function, which throws an error and never returns. In general, it is not safe to assume that there are instructions following a <span class="literal">ret</span> instruction or nonreturning <span class="literal">call</span>; instead, these instructions may be followed by data or padding bytes not intended to be parsed as code. However, the converse assumption that these instructions are <em>not</em> followed by more code may lead the disassembler to miss instructions, leading to an incomplete disassembly.</p>&#13;
<p class="indent">These are just some of the challenges faced by recursive disassemblers; many more complex cases exist, especially in more complicated functions than the one shown in the example. As you can see, neither linear nor recursive disassembly is perfect. For benign x86 ELF binaries, linear disassembly is a good choice because it will yield both a complete and accurate disassembly: such binaries typically don’t contain inline data that will throw the disassembler off, and the linear approach won’t miss code because of unresolved indirect control flow. On the other hand, if inline data or <span epub:type="pagebreak" id="page_122"/>malicious code is involved, it’s probably a better idea to use a recursive disassembler that’s not as easily fooled into producing bogus output as a linear disassembler is.</p>&#13;
<p class="indent">In cases where disassembly correctness is paramount, even at the expense of completeness, you can use <em>dynamic disassembly</em>. Let’s look at how this approach differs from the static disassembly methods just discussed.</p>&#13;
<h3 class="h3" id="ch06_2">6.2 Dynamic Disassembly</h3>&#13;
<p class="noindent">In the previous sections, you saw the challenges that static disassemblers face, such as distinguishing data from code, resolving indirect calls, and so on. Dynamic analysis solves many of these problems because it has a rich set of runtime information at its disposal, such as concrete register and memory contents. When execution reaches a particular address, you can be absolutely sure there’s an instruction there, so dynamic disassembly doesn’t suffer from the inaccuracy problems involved in static disassembly. This allows dynamic disassemblers, also known as <em>execution tracers</em> or <em>instruction tracers</em>, to simply dump instructions (and possibly memory/register contents) as the program executes. The main downside of this approach is the <em>code coverage problem</em>: the fact that dynamic disassemblers don’t see all instructions but only those they execute. I’ll get back to the code coverage problem later in this section. First, let’s take a look at a concrete execution trace.</p>&#13;
<h4 class="h4" id="ch06_2_1"><em>6.2.1 Example: Tracing a Binary Execution with gdb</em></h4>&#13;
<p class="noindent">Surprisingly enough, there’s no widely accepted standard tool on Linux to do “fire-and-forget” execution tracing (unlike on Windows, where excellent tools such as OllyDbg are available<sup><a id="ch06fn_4a" href="footnote.xhtml#ch06fn_4">4</a></sup>). The easiest way using only standard tools is with a few <span class="literal">gdb</span> commands, as shown in <a href="ch06.xhtml#ch06list1">Listing 6-1</a>.</p>&#13;
<p class="listing1" id="ch06list1"><em>Listing 6-1: Dynamic disassembly with</em> <span class="codeitalic">gdb</span></p>&#13;
<p class="programs">   $ <span class="codestrong1">gdb /bin/ls</span><br/>   GNU gdb (Ubuntu 7.11.1-0ubuntu1~16.04) 7.11.1<br/>   ...<br/>   Reading symbols from /bin/ls...(no debugging symbols found)...done.<br/><span class="ent">➊</span> (gdb) <span class="codestrong1">info files</span><br/>   Symbols from "/bin/ls".<br/>   Local exec file:<br/>          `/bin/ls', file type elf64-x86-64.<br/><span class="ent">➋</span>        Entry point: 0x4049a0<br/>          0x0000000000400238 - 0x0000000000400254 is .interp<br/>          0x0000000000400254 - 0x0000000000400274 is .note.ABI-tag<br/>          0x0000000000400274 - 0x0000000000400298 is .note.gnu.build-id<br/>          0x0000000000400298 - 0x0000000000400358 is .gnu.hash<br/>          0x0000000000400358 - 0x0000000000401030 is .dynsym<br/>          0x0000000000401030 - 0x000000000040160c is .dynstr<br/>          0x000000000040160c - 0x000000000040171e is .gnu.version<br/>          0x0000000000401720 - 0x0000000000401790 is .gnu.version_r<br/>          0x0000000000401790 - 0x0000000000401838 is .rela.dyn<br/>          0x0000000000401838 - 0x00000000004022b8 is .rela.plt<br/>          0x00000000004022b8 - 0x00000000004022d2 is .init<br/>          0x00000000004022e0 - 0x00000000004029f0 is .plt<br/>          0x00000000004029f0 - 0x00000000004029f8 is .plt.got<br/>          0x0000000000402a00 - 0x0000000000413c89 is .text<br/>          0x0000000000413c8c - 0x0000000000413c95 is .fini<br/>          0x0000000000413ca0 - 0x000000000041a654 is .rodata<br/>          0x000000000041a654 - 0x000000000041ae60 is .eh_frame_hdr<br/>          0x000000000041ae60 - 0x000000000041dae4 is .eh_frame<br/>          0x000000000061de00 - 0x000000000061de08 is .init_array<br/>          0x000000000061de08 - 0x000000000061de10 is .fini_array<br/>          0x000000000061de10 - 0x000000000061de18 is .jcr<br/>          0x000000000061de18 - 0x000000000061dff8 is .dynamic<br/>          0x000000000061dff8 - 0x000000000061e000 is .got<br/>          0x000000000061e000 - 0x000000000061e398 is .got.plt<br/>          0x000000000061e3a0 - 0x000000000061e600 is .data<br/>          0x000000000061e600 - 0x000000000061f368 is .bss<br/><span class="ent">➌</span> (gdb) <span class="codestrong1">b *0x4049a0</span><br/>   Breakpoint 1 at 0x4049a0<br/><span class="ent">➍</span> (gdb) <span class="codestrong1">set pagination off</span><br/><span class="ent">➎</span> (gdb) <span class="codestrong1">set logging on</span><br/>   Copying output to gdb.txt.<br/>   (gdb) <span class="codestrong1">set logging redirect on</span><br/>   Redirecting output to gdb.txt.<br/><span class="ent">➏</span> (gdb) <span class="codestrong1">run</span><br/><span class="ent">➐</span> (gdb) <span class="codestrong1">display/i $pc</span><br/><span class="ent">➑</span> (gdb) <span class="codestrong1">while 1</span><br/><span class="ent">➑</span> &gt;<span class="codestrong1">si</span><br/>   &gt;<span class="codestrong1">end</span><br/>   chapter1 chapter2 chapter3 chapter4 chapter5<br/>   chapter6 chapter7 chapter8 chapter9 chapter10<br/>   chapter11 chapter12 chapter13 inc<br/>   (gdb)</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_123"/>This example loads <em>/bin/ls</em> into <span class="literal">gdb</span> and produces a trace of all instructions executed when listing the contents of the current directory. After starting <span class="literal">gdb</span>, you can list information on the files loaded into <span class="literal">gdb</span> (in this case, it’s just the executable <em>/bin/ls</em>) <span class="ent">➊</span>. This tells you the binary’s entry point address <span class="ent">➋</span> so that you can set a breakpoint there that will halt execution as soon as the binary starts running <span class="ent">➌</span>. You then disable pagination <span class="ent">➍</span> and configure <span class="literal">gdb</span> such that it logs to file instead of standard output <span class="ent">➎</span>. By default, the log file is called <em>gdb.txt</em>. Pagination means that <span class="literal">gdb</span> pauses after outputting a certain number of lines, allowing the user to read all the output <span epub:type="pagebreak" id="page_124"/>on the screen before moving on. It’s enabled by default. Since you’re logging to file, you don’t want these pauses, as you would have to constantly press a key to continue, which gets annoying quickly.</p>&#13;
<p class="indent">After setting everything up, you run the binary <span class="ent">➏</span>. It pauses immediately, as soon as the entry point is hit. This gives you a chance to tell <span class="literal">gdb</span> to log this first instruction to file <span class="ent">➐</span> and then enter a <span class="literal">while</span> loop <span class="ent">➑</span> that continuously executes a single instruction at a time <span class="ent">➒</span> (this is called <em>single stepping</em> ) until there are no more instructions left to execute. Each single-stepped instruction is automatically printed to the log file in the same format as before. Once the execution is complete, you get a log file containing all the executed instructions. As you might expect, the output is quite lengthy; even a simple run of a small program traverses tens or hundreds of thousands of instructions, as shown in <a href="ch06.xhtml#ch06list2">Listing 6-2</a>.</p>&#13;
<p class="listing1" id="ch06list2"><em>Listing 6-2: Output of dynamic disassembly with</em> <span class="codeitalic">gdb</span></p>&#13;
<p class="programs"><span class="ent">➊</span> $ <span class="codestrong1">wc -l gdb.txt</span><br/>   614390 gdb.txt<br/><span class="ent">➋</span> $ <span class="codestrong1">head -n 20 gdb.txt</span><br/>   Starting program: /bin/ls<br/>   [Thread debugging using libthread_db enabled]<br/>   Using host libthread_db library "/lib/x86_64-linux-gnu/libthread_db.so.1".<br/><br/>   Breakpoint 1, 0x00000000004049a0 in ?? ()<br/><span class="ent">➌</span> 1: x/i $pc<br/>   =&gt; 0x4049a0:          xor     %ebp,%ebp<br/>   0x00000000004049a2   in ?? ()<br/>   1: x/i $pc<br/>   =&gt; 0x4049a2:          mov     %rdx,%r9<br/>   0x00000000004049a5   in ?? ()<br/>   1: x/i $pc<br/>   =&gt; 0x4049a5:          pop     %rsi<br/>   0x00000000004049a6   in ?? ()<br/>   1: x/i $pc<br/>   =&gt; 0x4049a6:          mov     %rsp,%rdx<br/>   0x00000000004049a9   in ?? ()<br/>   1: x/i $pc<br/>   =&gt; 0x4049a9:          and     $0xfffffffffffffff0,%rsp<br/>   0x00000000004049ad   in ?? ()</p>&#13;
<p class="indent">Using <span class="literal">wc</span> to count the lines in the log file, you can see that the file contains 614,390 lines, far too many to list here <span class="ent">➊</span>. To give you an idea of what the output looks like, you can use <span class="literal">head</span> to take a look at the first 20 lines in the log <span class="ent">➋</span>. The actual execution trace starts at <span class="ent">➌</span>. For each executed instruction, <span class="literal">gdb</span> prints the command used to log the instruction, then the instruction itself, and finally some context on the instruction’s location (which is unknown since the binary is stripped). Using a <span class="literal">grep</span>, you can filter <span epub:type="pagebreak" id="page_125"/>out everything but the lines showing the executed instructions, since they’re all you’re interested in, yielding output as shown in <a href="ch06.xhtml#ch06list3">Listing 6-3</a>.</p>&#13;
<p class="listing1" id="ch06list3"><em>Listing 6-3: Filtered output of dynamic disassembly with</em> <span class="codeitalic">gdb</span></p>&#13;
<p class="programs">$ <span class="codestrong1">egrep '^=&gt; 0x[0-9a-f]+:' gdb.txt | head -n 20</span><br/>=&gt; 0x4049a0:        xor    %ebp,%ebp<br/>=&gt; 0x4049a2:        mov    %rdx,%r9<br/>=&gt; 0x4049a5:        pop    %rsi<br/>=&gt; 0x4049a6:        mov    %rsp,%rdx<br/>=&gt; 0x4049a9:        and    $0xfffffffffffffff0,%rsp<br/>=&gt; 0x4049ad:        push   %rax<br/>=&gt; 0x4049ae:        push   %rsp<br/>=&gt; 0x4049af:        mov    $0x413c50,%r8<br/>=&gt; 0x4049b6:        mov    $0x413be0,%rcx<br/>=&gt; 0x4049bd:        mov    $0x402a00,%rdi<br/>=&gt; 0x4049c4:        callq  0x402640 &lt;__libc_start_main@plt&gt;<br/>=&gt; 0x4022e0:        pushq  0x21bd22(%rip)         # 0x61e008<br/>=&gt; 0x4022e6:        jmpq   *0x21bd24(%rip)        # 0x61e010<br/>=&gt; 0x413be0:        push   %r15<br/>=&gt; 0x413be2:        push   %r14<br/>=&gt; 0x413be4:        mov    %edi,%r15d<br/>=&gt; 0x413be7:        push   %r13<br/>=&gt; 0x413be9:        push   %r12<br/>=&gt; 0x413beb:        lea    0x20a20e(%rip),%r12   # 0x61de00<br/>=&gt; 0x413bf2:        push   %rbp</p>&#13;
<p class="indent">As you can see, this is a lot more readable than the unfiltered <span class="literal">gdb</span> log.</p>&#13;
<h4 class="h4" id="ch06_2_2"><em>6.2.2 Code Coverage Strategies</em></h4>&#13;
<p class="noindent">The main disadvantage of all dynamic analysis, not just dynamic disassembly, is the code coverage problem: the analysis only ever sees the instructions that are actually executed during the analysis run. Thus, if any crucial information is hidden in other instructions, the analysis will never know about it. For instance, if you’re dynamically analyzing a program that contains a logic bomb (for instance, triggering malicious behavior at a certain time in the future), you’ll never find out until it’s too late. In contrast, a close inspection using static analysis might have revealed this. As another example, when dynamically testing software for bugs, you’ll never be sure that there isn’t a bug in some rarely executed code path that you failed to cover in your tests.</p>&#13;
<p class="indent">Many malware samples even try to actively hide from dynamic analysis tools or debuggers like <span class="literal">gdb</span>. Virtually all such tools produce some kind of detectable artifact in the environment; if nothing else, the analysis inevitably slows down execution, typically enough to be detectable. Malware detects these artifacts and hides its true behavior if it knows it’s being analyzed. To enable dynamic analysis on these samples, you must reverse engineer and then disable the malware’s anti-analysis checks (for instance, by overwriting <span epub:type="pagebreak" id="page_126"/>those code bytes with patched values). These anti-analysis tricks are the reason why, if possible, it’s usually a good idea to at least augment your dynamic malware analysis with static analysis methods.</p>&#13;
<p class="indent">Because it’s difficult and time-consuming to find the correct inputs to cover every possible program path, dynamic disassembly will almost never reveal all possible program behavior. There are several methods you can use to improve the coverage of dynamic analysis tools, though in general none of them achieves the level of completeness provided by static analysis. Let’s take a look at some of the methods used most often.</p>&#13;
<h3 class="h3">Test Suites</h3>&#13;
<p class="noindent">One of the easiest and most popular methods to increase code coverage is running the analyzed binary with known test inputs. Software developers often manually develop test suites for their programs, crafting inputs designed to cover as much of the program’s functionality as possible. Such test suites are perfect for dynamic analysis. To achieve good code coverage, simply run an analysis pass on the program with each of the test inputs. Of course, the downside of this approach is that a ready-made test suite isn’t always available, for instance, for proprietary software or malware.</p>&#13;
<p class="indent">The exact way to use test suites for code coverage differs per application, depending on how the application’s test suite is structured. Typically, there’s a special Makefile <span class="literal">test</span> target, which you can use to run the test suite by entering <span class="literal">make test</span> on the command line. Inside the Makefile, the <span class="literal">test</span> target is often structured something like <a href="ch06.xhtml#ch06list4">Listing 6-4</a>.</p>&#13;
<p class="listing1" id="ch06list4"><em>Listing 6-4: Structure of a Makefile</em> <span class="codeitalic">test</span> <em>target</em></p>&#13;
<p class="programs">PROGRAM := foo<br/><br/>test: test1 test2 test3 # ...<br/><br/>test1:<br/>        $(PROGRAM) &lt; input &gt; output<br/>        diff correct output<br/><br/># ...</p>&#13;
<p class="indent">The <span class="literal">PROGRAM</span> variable contains the name of the application that’s being tested, in this case <span class="literal">foo</span>. The <span class="literal">test</span> target depends on a number of test cases (<span class="literal">test1</span>, <span class="literal">test2</span>, and so on), each of which gets called when you run <span class="literal">make test</span>. Each test case consists of running <span class="literal">PROGRAM</span> on some input, recording the output, and then checking it against a correct output using <span class="literal">diff</span>.</p>&#13;
<p class="indent">There are many different (and more concise) ways of implementing this type of testing framework, but the key point is that you can run your dynamic analysis tool on each of the test cases by simply overriding the <span class="literal">PROGRAM</span> variable. For instance, say you want to run each of <span class="literal">foo</span>’s test cases with <span class="literal">gdb</span>. (In reality, instead of <span class="literal">gdb</span>, you’d more likely use a fully automated <span epub:type="pagebreak" id="page_127"/>dynamic analysis, which you’ll learn how to build in <a href="ch09.xhtml#ch09">Chapter 9</a>.) You could do this as follows:</p>&#13;
<p class="programs">make test PROGRAM="gdb foo"</p>&#13;
<p class="indent">Essentially, this redefines <span class="literal">PROGRAM</span> so that instead of just running <span class="literal">foo</span> with each test, you now run <span class="literal">foo</span> <em>inside</em> <em>gdb</em>. This way, <span class="literal">gdb</span> or whatever dynamic analysis tool you’re using runs <span class="literal">foo</span> with each of its test cases, allowing the dynamic analysis to cover all of <span class="literal">foo</span>’s code that’s covered by the test cases. In cases where there isn’t a <span class="literal">PROGRAM</span> variable to override, you’ll have to do a search and replace, but the idea remains the same.</p>&#13;
<h3 class="h3">Fuzzing</h3>&#13;
<p class="noindent">There are also tools, called <em>fuzzers</em>, that try to automatically generate inputs to cover new code paths in a given binary. Well-known fuzzers include AFL, Microsoft’s Project Springfield, and Google’s OSS-Fuzz. Broadly speaking, fuzzers fall into two categories based on the way they generate inputs.</p>&#13;
<ol>&#13;
<li class="noindent">Generation-based fuzzers: These generate inputs from scratch (possibly with knowledge of the expected input format).</li>&#13;
<li class="noindent">Mutation-based fuzzers: These fuzzers generate new inputs by mutating known valid inputs in some way, for instance, starting from an existing test suite.</li>&#13;
</ol>&#13;
<p class="indent">The success and performance of fuzzers depend greatly on the information available to the fuzzer. For instance, it helps if source information is available or if the program’s expected input format is known. If none of these things is known (and even if they all are known), fuzzing can require a lot of compute time and may not reach code hidden behind complex sequences of <span class="literal">if</span>/<span class="literal">else</span> conditions that the fuzzer fails to “guess.” Fuzzers are typically used to search programs for bugs, permuting inputs until a crash is detected.</p>&#13;
<p class="indent">Although I won’t go into details on fuzzing in this book, I encourage you to play around with one of the free tools available. Each fuzzer has its own usage method. A great choice for experimentation is AFL, which is free and comes with good online documentation.<sup><a id="ch06fn_5a" href="footnote.xhtml#ch06fn_5">5</a></sup> Additionally, in <a href="ch10.xhtml#ch10">Chapter 10</a> I’ll discuss how dynamic taint analysis can be used to augment fuzzing.</p>&#13;
<h3 class="h3">Symbolic Execution</h3>&#13;
<p class="noindent">Symbolic execution is an advanced technique that I discuss in detail in <a href="ch12.xhtml#ch12">Chapters 12</a> and <a href="ch13.xhtml#ch13">13</a>. It’s a broad technique with a multitude of applications, not just code coverage. Here, I’ll just give you a rough idea of how symbolic execution applies to code coverage, glossing over many details, so don’t worry if you can’t follow all of it yet.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_128"/>Normally, when you execute an application, you do so using concrete values for all variables. At each point in the execution, every CPU register and memory area contains some particular value, and these values change over time as the application’s computation proceeds. Symbolic execution is different.</p>&#13;
<p class="indent">In a nutshell, symbolic execution allows you to execute an application not with <em>concrete values</em> but with <em>symbolic values</em>. You can think of symbolic values as mathematical symbols. A symbolic execution is essentially an emulation of a program, where all or some of the variables (or register and memory states) are represented using such symbols.<sup><a id="ch06fn_6a" href="footnote.xhtml#ch06fn_6">6</a></sup> To get a clearer idea of what this means, consider the pseudocode program shown in <a href="ch06.xhtml#ch06list5">Listing 6-5</a>.</p>&#13;
<p class="listing1" id="ch06list5"><em>Listing 6-5: Pseudocode example to illustrate symbolic execution</em></p>&#13;
<p class="programs"><span class="ent">➊</span> x = int(argv[0])<br/>   y = int(argv[1])<br/><br/><span class="ent">➋</span> z = x + y<br/><span class="ent">➌</span> if(x &lt; 5)<br/>       foo(x, y, z)<br/><span class="ent">➍</span> else<br/>       bar(x, y, z)</p>&#13;
<p class="indent">The program starts by taking two command line arguments, converting them to numbers, and storing them in two variables called <span class="literal">x</span> and <span class="literal">y</span> <span class="ent">➊</span>. At the start of a symbolic execution, you might define the <span class="literal">x</span> variable to contain the symbolic value <em>α</em><sub>1</sub>, while <span class="literal">y</span> may be initialized to <em>α</em><sub>2</sub>. Both <em>α</em><sub>1</sub> and <em>α</em><sub>2</sub> are symbols that could represent any possible numerical value. Then, as the emulation proceeds, the program essentially computes formulas over these symbols. For instance, the operation <span class="literal">z = x + y</span> causes <span class="literal">z</span> to assume the symbolic expression <em>α</em><sub>1</sub> + <em>α</em><sub>2</sub> <span class="ent">➋</span>.</p>&#13;
<p class="indent">At the same time, the symbolic execution also computes <em>path constraints</em>, which are just restrictions on the concrete values that the symbols could take, given the branches that have been traversed so far. For instance, if the branch <span class="literal">if(x &lt; 5)</span> is taken, the symbolic execution adds a path constraint saying that <em>α</em><sub>1</sub> &lt; 5 <span class="ent">➌</span>. This constraint expresses the fact that if the <span class="literal">if</span> branch is taken, then <em>α</em><sub>1</sub> (the symbolic value in <span class="literal">x</span>) must always be less than 5. Otherwise, the branch wouldn’t have been taken. For each branch, the symbolic execution extends the list of path constraints accordingly.</p>&#13;
<p class="indent">How does all this apply to code coverage? The key point is that <em>given the list of path constraints, you can check whether there’s any concrete input that would satisfy all these constraints.</em> There are special programs, called <em>constraint solvers</em>, that check, given a list of constraints, whether there’s any way to satisfy these constraints. For instance, if the only constraint is <em>α</em><sub>1</sub> &lt; 5, the solver may yield the solution <em>α</em><sub>1</sub> = 4 ^ <em>α</em><sub>2</sub> = 0. Note that the path constraints don’t say anything about <em>α</em><sub>2</sub>, so it can take any value. This means that, at the start <span epub:type="pagebreak" id="page_129"/>of a concrete execution of the program, you can (via user input) set the value 4 for <span class="literal">x</span> and the value 0 for <span class="literal">y</span>, and the execution will then take the same series of branches taken in the symbolic execution. If there’s no solution, the solver will inform you.</p>&#13;
<p class="indent">Now, to increase code coverage, you can change the path constraints and ask the solver if there’s any way to satisfy the changed constraints. For instance, you could “flip” the constraint <em>α</em><sub>1</sub> &lt; 5 to instead say <em>α</em><sub>1</sub> ≥ <em>α</em><sub>5</sub> and ask the solver for a solution. The solver will then inform you of a possible solution, such as <em>α</em><sub>1</sub> = 5 ^ <em>α</em><sub>2</sub> = 0, which you can feed as input to a concrete execution of the program, thereby forcing that execution to take the <span class="literal">else</span> branch and thus increasing code coverage <span class="ent">➍</span>. If the solver informs you that there’s no possible solution, you know that there’s no way to “flip” the branch, and you should continue looking for new paths by changing other path constraints.</p>&#13;
<p class="indent">As you may have gathered from this discussion, symbolic execution (or even just its application to code coverage) is a complex subject. Even given the ability to “flip” path constraints, it’s still infeasible to cover all program paths since the number of possible paths increases exponentially with the number of branch instructions in a program. Moreover, solving a set of path constraints is computationally intensive; if you don’t take care, your symbolic execution approach can easily become unscalable. In practice, it takes a lot of care to apply symbolic execution in a scalable and effective way. I’ve only covered the gist of the ideas behind symbolic execution so far, but ideally it’s given you a taste of what to expect in <a href="ch12.xhtml#ch12">Chapters 12</a> and <a href="ch13.xhtml#ch13">13</a>.</p>&#13;
<h3 class="h3" id="ch06_3">6.3 Structuring Disassembled Code and Data</h3>&#13;
<p class="noindent">So far, I’ve shown you how static and dynamic disassemblers find instructions in a binary, but disassembly doesn’t end there. Large unstructured heaps of disassembled instructions are nearly impossible to analyze, so most disassemblers structure the disassembled code in some way that’s easier to analyze. In this section, I’ll discuss the common code and data structures that disassemblers recover and how they help binary analysis.</p>&#13;
<h4 class="h4" id="ch06_3_1"><em>6.3.1 Structuring Code</em></h4>&#13;
<p class="noindent">First, let’s take a look at the various ways of structuring disassembled code. Broadly speaking, the code structures I’ll show you make code easier to analyze in two ways.</p>&#13;
<ul>&#13;
<li><p>Compartmentalizing: By breaking the code into logically connected chunks, it becomes easier to analyze what each chunk does and how chunks of code relate to each other.</p></li>&#13;
<li><p>Revealing control flow: Some of the code structures I’ll discuss next explicitly represent not only the code itself but also the control transfers between blocks of code. These structures can be represented visually, making it much easier to quickly see how control flows through the code and to get a quick idea of what the code does.</p></li>&#13;
</ul>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_130"/>The following code structures are useful in both automated and manual analysis.</p>&#13;
<h3 class="h3">Functions</h3>&#13;
<p class="noindent">In most high-level programming languages (including C, C++, Java, Python, and so on), functions are the fundamental building blocks used to group logically connected pieces of code. As any programmer knows, programs that are well structured and properly divided into functions are much easier to understand than poorly structured programs with “spaghetti code.” For this reason, most disassemblers make some effort to recover the original program’s function structure and use it to group disassembled instructions by function. This is known as <em>function detection</em>. Not only does function detection make the code much easier to understand for human reverse engineers, but it also helps in automated analysis. For instance, in automated binary analysis, you may want to search for bugs at a per-function level or modify the code so that a particular security check happens at the start and end of each function.</p>&#13;
<p class="indent">For binaries with symbolic information, function detection is trivial; the symbol table specifies the set of functions, along with their names, start addresses, and sizes. Unfortunately, as you may recall from <a href="ch01.xhtml#ch01">Chapter 1</a>, many binaries are stripped of this information, which makes function detection far more challenging. Source-level functions have no real meaning at the binary level, so their boundaries may become blurred during compilation. The code belonging to a particular function might not even be arranged contiguously in the binary. Bits and pieces of the function might be scattered throughout the code section, and chunks of code may even be shared between functions (known as <em>overlapping code blocks</em>). In practice, most disassemblers make the assumption that functions are contiguous and don’t share code, which holds true in many but not all cases. This is especially not true if you’re analyzing things such as firmware or code for embedded systems.</p>&#13;
<p class="indent">The predominant strategy that disassemblers use for function detection is based on <em>function signatures</em>, which are patterns of instructions often used at the start or end of a function. This strategy is used in all well-known recursive disassemblers, including IDA Pro. Linear disassemblers like <span class="literal">objdump</span> typically don’t do function detection, except when symbols are available.</p>&#13;
<p class="indent">Typically, signature-based function detection algorithms start with a pass over the disassembled binary to locate functions that are directly addressed by a <span class="literal">call</span> instruction. These cases are easy for the disassembler to find; functions that are called only indirectly or tail-called are more of a challenge.<sup><a id="ch06fn_7a" href="footnote.xhtml#ch06fn_7">7</a></sup> To locate these challenging cases, signature-based function detectors consult a database of known function signatures.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_131"/>Function signature patterns include well-known <em>function prologues</em> (instructions used to set up the function’s stack frame) and <em>function epilogues</em> (used to tear down the stack frame). For instance, a typical pattern that many x86 compilers emit for unoptimized functions starts with the prologue <span class="literal">push ebp; mov ebp,esp</span> and ends with the epilogue <span class="literal">leave; ret</span>. Many function detectors scan the binary for such signatures and use them to recognize where functions start and end.</p>&#13;
<p class="indent">Although functions are an essential and useful way to structure disassembled code, you should always be wary of errors. In practice, function patterns vary depending on the platform, compiler, and optimization level used to create the binary. Optimized functions may not have well-known function prologues or epilogues at all, making them impossible to recognize using a signature-based approach. As a result, errors in function detection occur quite regularly. For example, it’s not rare for disassemblers to get 20 percent or more of the function start addresses wrong or even to report a function where there is none.</p>&#13;
<p class="indent">Recent research explores different methods for function detection, based not on signatures but on the structure of the code.<sup><a id="ch06fn_8a" href="footnote.xhtml#ch06fn_8">8</a></sup> While this approach is potentially more accurate than signature-based approaches, detection errors are still a fact of life. The approach has been integrated into Binary Ninja, and the research prototype tool can interoperate with IDA Pro, so you can give it a go if you want.</p>&#13;
<div class="box">&#13;
<h3 class="h3">Function Detection Using the .eh_frame Section</h3>&#13;
<p class="noindent">An interesting alternative approach to function detection for ELF binaries is based on the <span class="literal">.eh_frame</span> section, which you can use to circumvent the function detection problem entirely. The <span class="literal">.eh_frame</span> section contains information related to DWARF-based debugging features such as stack unwinding. This includes function boundary information that identifies all functions in the binary. The information is present even in stripped binaries, unless the binary was compiled with <span class="literal">gcc</span>’s <span class="literal">-fno-asynchronous-unwind-tables</span> flag. It’s used primarily for C++ exception handling but also for various other applications such as <span class="literal">backtrace()</span> and <span class="literal">gcc</span> intrinsics such as <span class="literal">__attribute__((__cleanup__(f)))</span> and <span class="literal">__builtin_return_address(n)</span>. Because of its many uses, <span class="literal">.eh_frame</span> is present by default not only in C++ binaries that use exception handling but in all binaries produced by <span class="literal">gcc</span>, including plain C binaries.</p>&#13;
<p class="indent">As far as I know, this method was first described by Ryan O’Neill (aka ElfMaster). On his website, he provides code to parse the <span class="literal">.eh_frame</span> section into a set of function addresses and sizes.<a id="ch06foot-a1"/><sup><a href="#ch06foot_a1">a</a></sup></p>&#13;
<p class="noindent"><a id="ch06foot_a1"/><a href="#ch06foot-a1">a</a>. <a href="http://www.bitlackeys.org/projects/eh_frame.tgz">http://www.bitlackeys.org/projects/eh_frame.tgz</a></p>&#13;
</div>&#13;
<h3 class="h3"><span epub:type="pagebreak" id="page_132"/>Control-Flow Graphs</h3>&#13;
<p class="noindent">Breaking the disassembled code into functions is one thing, but some functions are quite large, which means analyzing even one function can be a complex task. To organize the internals of each function, disassemblers and binary analysis frameworks use another code structure, called a <em>control-flow graph (CFG)</em>. CFGs are useful for automated analysis, as well as manual analysis. They also offer a convenient graphical representation of the code structure, which makes it easy to get a feel for a function’s structure at a glance. <a href="ch06.xhtml#ch06fig5">Figure 6-5</a> shows an example of the CFG of a function disassembled with IDA Pro.</p>&#13;
<div class="image"><a id="ch06fig5"/><img src="Images/f132-01.jpg" alt="image" width="614" height="850"/></div>&#13;
<p class="fig-caption"><em>Figure 6-5: A CFG as seen in IDA Pro</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_133"/>As you can see in the figure, CFGs represent the code inside a function as a set of code blocks, called <em>basic blocks</em>, connected by <em>branch edges</em>, shown here as arrows. A basic block is a sequence of instructions, where the first instruction is the only entry point (the only instruction targeted by any jump in the binary), and the last instruction is the only exit point (the only instruction in the sequence that may jump to another basic block). In other words, you’ll never see a basic block with an arrow connected to any instruction other than the first or last.</p>&#13;
<p class="indent">An edge in the CFG from a basic block <em>B</em> to another basic block <em>C</em> means that the last instruction in <em>B</em> may jump to the start of <em>C</em>. If <em>B</em> has only one outbound edge, that means it will definitely transfer control to the target of that edge. For instance, this is what you’ll see for an indirect jump or call instruction. On the other hand, if <em>B</em> ends in a conditional jump, then it will have two outbound edges, and which edge is taken at runtime depends on the outcome of the jump condition.</p>&#13;
<p class="indent">Call edges are not part of a CFG because they target code outside of the function. Instead, the CFG shows only the “fallthrough” edge that points to the instruction where control will return after the function call completes. There is another code structure, called a <em>call graph</em>, that is designed to represent the edges between call instructions and functions. I’ll discuss call graphs next.</p>&#13;
<p class="indent">In practice, disassemblers often omit indirect edges from the CFG because it’s difficult to resolve the potential targets of such edges statically. Disassemblers also sometimes define a global CFG rather than per-function CFGs. Such a global CFG is called an <em>interprocedural CFG (ICFG)</em> since it’s essentially the union of all per-function CFGs (<em>procedure</em> is another word for function). ICFGs avoid the need for error-prone function detection but don’t offer the compartmentalization benefits that per-function CFGs have.</p>&#13;
<h3 class="h3">Call Graphs</h3>&#13;
<p class="noindent"><em>Call graphs</em> are similar to CFGs, except they show the relationship between call sites and functions rather than basic blocks. In other words, CFGs show you how control may flow within a function, while call graphs show you which functions may call each other. Just as with CFGs, call graphs often omit indirect call edges because it’s infeasible to accurately figure out which functions may be called by a given indirect call site.</p>&#13;
<p class="indent">The left side of <a href="ch06.xhtml#ch06fig6">Figure 6-6</a> shows a set of functions (labeled <em>f</em><sub>1</sub> through <em>f</em><sub>4</sub>) and the call relationships between them. Each function consists of some basic blocks (the gray circles) and branch edges (the arrows). The corresponding call graph is on the right side of the figure. As you can see, the call graph contains a node for each function and has edges showing that function <em>f</em><sub>1</sub> can call both <em>f</em><sub>2</sub> and <em>f</em><sub>3</sub>, as well as an edge representing the call from <em>f</em><sub>3</sub> to <em>f</em><sub>1</sub>. Tail calls, which are really implemented as jump instructions, are shown as a regular call in the call graph. However, notice that the indirect call from <em>f</em><sub>2</sub> to <em>f</em><sub>4</sub> is <em>not</em> shown in the call graph.</p>&#13;
<div class="image"><span epub:type="pagebreak" id="page_134"/><a id="ch06fig6"/><img src="Images/f134-01.jpg" alt="image" width="686" height="497"/></div>&#13;
<p class="fig-caption"><em>Figure 6-6: CFGs and connections between functions (left) and the corresponding call graph (right)</em></p>&#13;
<p class="indent">IDA Pro can also display partial call graphs, which show only the potential callers of a particular function of your choice. For manual analysis, these are often more useful than complete call graphs because complete call graphs often contain too much information. <a href="ch06.xhtml#ch06fig7">Figure 6-7</a> shows an example of a partial call graph in IDA Pro that reveals the references to function <span class="literal">sub_404610</span>. As you can see, the graph shows from where the function is called; for instance, <span class="literal">sub_404610</span> is called by <span class="literal">sub_4e1bd0</span>, which is itself called by <span class="literal">sub_4e2fa0</span>.</p>&#13;
<p class="indent">In addition, the call graphs produced by IDA Pro show instructions that store the address of a function somewhere. For instance, at address <span class="literal">0x4e072c</span> in the <span class="literal">.text</span> section, there’s an instruction that stores the address of function <span class="literal">sub_4e2fa0</span> in memory. This is called “taking the address” of function <span class="literal">sub_4e2fa0</span>. Functions that have their address taken anywhere in the code are called <em>address-taken functions</em>.</p>&#13;
<p class="indent">It’s nice to know which functions are address-taken because this tells you they might be called indirectly, even if you don’t know exactly by which call site. If a function’s address is never taken and doesn’t appear in any data sections, you know it will never be called indirectly.<sup><a id="ch06fn_9a" href="footnote.xhtml#ch06fn_9">9</a></sup> That’s useful for some kinds of binary analysis or security applications, such as if you’re trying to secure the binary by restricting indirect calls to only legal targets.</p>&#13;
<div class="image"><span epub:type="pagebreak" id="page_135"/><a id="ch06fig7"/><img src="Images/f135-01.jpg" alt="image" width="692" height="326"/></div>&#13;
<p class="fig-caption"><em>Figure 6-7: A call graph of calls targeting function</em> <span class="literal">sub_404610</span><em>, as seen in IDA Pro</em></p>&#13;
<h3 class="h3">Object-Oriented Code</h3>&#13;
<p class="noindent">You’ll find that many binary analysis tools, including fully featured disassemblers like IDA Pro, are targeted at programs written in <em>procedural languages</em> like C. Because code is structured mainly through the use of functions in these languages, binary analysis tools and disassemblers provide features such as function detection to recover programs’ function structure, and they call graphs to examine the relationship between functions.</p>&#13;
<p class="indent">Object-oriented languages like C++ structure code using <em>classes</em> that group logically connected functions and data. They typically also offer complex exception-handling features that allow any instruction to throw an exception, which is then caught by a special block of code that handles the exception. Unfortunately, current binary analysis tools lack the ability to recover class hierarchies and exception-handling structures.</p>&#13;
<p class="indent">To make matters worse, C++ programs often contain lots of function pointers because of the way virtual methods are typically implemented. <em>Virtual methods</em> are class methods (functions) that are allowed to be overridden in a derived class. In a classic example, you might define a class called <span class="literal">Shape</span> that has a derived class called <span class="literal">Circle</span>. <span class="literal">Shape</span> defines a virtual method called <span class="literal">area</span> that computes the area of the shape, and <span class="literal">Circle</span> overrides that method with its own implementation appropriate to circles.</p>&#13;
<p class="indent">When compiling a C++ program, the compiler may not know whether a pointer will point to a base <span class="literal">Shape</span> object or a derived <span class="literal">Circle</span> object at runtime, so it cannot statically determine which implementation of the <span class="literal">area</span> method should be used at runtime. To solve this issue, compilers emit tables of function pointers, called <em>vtables</em>, that contain pointers to all the virtual functions of a particular class. Vtables are usually kept in read-only memory, and each polymorphic object has a pointer (called a <em>vptr</em>) to the vtable for the object’s type. To invoke a virtual method, the compiler emits code that follows the object’s vptr at runtime and indirectly calls the correct entry in its vtable. Unfortunately, all these indirect calls make the program’s control flow even more difficult to follow.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_136"/>The lack of support for object-oriented programs in binary analysis tools and disassemblers means that if you want to structure your analysis around the class hierarchy, you’re on your own. When reverse engineering a C<strong>++</strong> program manually, you can often piece together the functions and data structures belonging to different classes, but this requires significant effort. I won’t go into details on this subject here in order to keep our focus on (semi)automated binary analysis techniques. If you’re interested in learning how to manually reverse C++ code, I recommend Eldad Eilam’s book <em>Reversing: Secrets of Reverse Engineering</em> (Wiley, 2005).</p>&#13;
<p class="indent">In case of automated analysis, you can (as most binary analysis tools do) simply pretend classes don’t exist and treat object-oriented programs the same as procedural programs. In fact, this “solution” works adequately for many kinds of analysis and saves you from the pain of having to implement special C++ support unless really necessary.</p>&#13;
<h4 class="h4" id="ch06_3_2"><em>6.3.2 Structuring Data</em></h4>&#13;
<p class="noindent">As you saw, disassemblers automatically identify various types of code structures to help your binary analysis efforts. Unfortunately, the same cannot be said for data structures. Automatic data structure detection in stripped binaries is a notoriously difficult problem, and aside from some research work,<sup><a id="ch06fn_10a" href="footnote.xhtml#ch06fn_10">10</a></sup> disassemblers generally don’t even attempt it.</p>&#13;
<p class="indent">But there are some exceptions. For example, if a reference to a data object is passed to a well-known function, such as a library function, disassemblers like IDA Pro can automatically infer the data type based on the specification of the library function. <a href="ch06.xhtml#ch06fig8">Figure 6-8</a> shows an example.</p>&#13;
<p class="indent">Near the bottom of the basic block, there’s a call to the well-known <span class="literal">send</span> function used to send a message over a network. Since IDA Pro knows the parameters of the <span class="literal">send</span> function, it can label the parameter names (<span class="literal">flags</span>, <span class="literal">len</span>, <span class="literal">buf</span>, <span class="literal">s</span>) and infer the data types of the registers and memory objects used to load the parameters.</p>&#13;
<p class="indent">Additionally, primitive types can sometimes be inferred by the registers they’re kept in or the instructions used to manipulate the data. For instance, if you see a floating-point register or instruction being used, you know the data in question is a floating-point number. If you see a <span class="literal">lodsb</span> (<em>load string byte</em>) or <span class="literal">stosb</span> (<em>store string byte</em>) instruction, it’s likely manipulating a string.</p>&#13;
<p class="indent">For composite types such as <span class="literal">struct</span> types or arrays, all bets are off, and you’ll have to rely on your own analysis. As an example of why automatic identification of composite types is hard, take a look at how the following line of C code is compiled into machine code:</p>&#13;
<p class="programs">ccf-&gt;user = pwd-&gt;pw_uid;</p>&#13;
<div class="image"><span epub:type="pagebreak" id="page_137"/><a id="ch06fig8"/><img src="Images/f137-01.jpg" alt="image" width="436" height="488"/></div>&#13;
<p class="fig-caption"><em>Figure 6-8: IDA Pro automatically infers data types based on the use of the</em> <span class="literal">send</span> <em>function.</em></p>&#13;
<p class="indent">This is a line from the <span class="literal">nginx</span> v1.8.0 source, where an integer field from one <span class="literal">struct</span> is assigned to a field in another <span class="literal">struct</span>. When compiled with <span class="literal">gcc</span> v5.1 at optimization level <span class="literal">-O2</span>, this results in the following machine code:</p>&#13;
<p class="programs">mov eax,DWORD PTR [rax+0x10]<br/>mov DWORD PTR [rbx+0x60],eax</p>&#13;
<p class="indent">Now let’s take a look at the following line of C code, which copies an integer from a heap-allocated array called <span class="literal">b</span> into another array <span class="literal">a</span>:</p>&#13;
<p class="programs">a[24] = b[4];</p>&#13;
<p class="indent">Here’s the result of compiling that with <span class="literal">gcc</span> v5.1, again at optimization level <span class="literal">-O2</span>:</p>&#13;
<p class="programs">mov eax,DWORD PTR [rsi+0x10]<br/>mov DWORD PTR [rdi+0x60],eax</p>&#13;
<p class="indent">As you can see, the code pattern is exactly the same as for the <span class="literal">struct</span> assignment! This shows that there’s no way for any automated analysis to tell from a series of instructions like this whether they represent an array lookup, a <span class="literal">struct</span> access, or something else entirely. Problems like this make accurate detection of composite data types difficult, if not impossible in the <span epub:type="pagebreak" id="page_138"/>general case. Keep in mind that this example is quite simple; imagine reversing a program that contains an array of <span class="literal">struct</span> types, or nested <span class="literal">struct</span>s, and trying to figure out which instructions index which data structure! Clearly, that’s a complex task that requires an in-depth analysis of the code. Given the complexity of accurately recognizing nontrivial data types, you can see why disassemblers make no attempt at automated data structure detection.</p>&#13;
<p class="indent">To facilitate structuring data manually, IDA Pro allows you to define your own composite types (which you have to infer by reversing the code) and assign these to data items. Chris Eagle’s <em>The IDA Pro Book</em> (No Starch Press, 2011) is a great resource on manually reversing data structures with IDA Pro.</p>&#13;
<h4 class="h4" id="ch06_3_3"><em>6.3.3 Decompilation</em></h4>&#13;
<p class="noindent">As the name implies, <em>decompilers</em> are tools that attempt to “reverse the compilation process.” They typically start from disassembled code and translate it into a higher-level language, usually a form of C-like pseudocode. Decompilers are useful when reversing large programs because decompiled code is easier to read than lots of assembly instructions. But decompilers are limited to manual reversing because the decompilation process is too error-prone to serve as a reliable basis for any automated analysis. Although you won’t use decompilation in this book, let’s take a look at <a href="ch06.xhtml#ch06list6">Listing 6-6</a> to give you an idea of what decompiled code looks like.</p>&#13;
<p class="indent">The most widely used decompiler is Hex-Rays, a plugin that ships with IDA Pro.<sup><a id="ch06fn_11a" href="footnote.xhtml#ch06fn_11">11</a></sup> <a href="ch06.xhtml#ch06list6">Listing 6-6</a> shows the Hex-Rays output for the function shown earlier in <a href="ch06.xhtml#ch06fig5">Figure 6-5</a>.</p>&#13;
<p class="listing1" id="ch06list6"><em>Listing 6-6: A function decompiled with Hex-Rays</em></p>&#13;
<p class="programs"><span class="ent">➊</span> void **__usercall sub_4047D4&lt;eax&gt;(int a1&lt;ebp&gt;)<br/>   {<br/><span class="ent">➋</span>    int   v1; // eax@1<br/>      int   v2; // ebp@1<br/>      int   v3; // ecx@4<br/>      int   v5; // ST10_4@6<br/>      int   i; // [sp+0h] [bp-10h]@3<br/><br/><span class="ent">➌</span>    v2 = a1 + 12;<br/>      v1 = *(_DWORD *)(v2 - 524);<br/>      *(_DWORD *)(v2 - 540) = *(_DWORD *)(v2 - 520);<br/><span class="ent">➍</span>     if ( v1 == 1 )<br/>         goto LABEL_5;<br/>       if ( v1 != 2 )<br/>       {<br/><span class="ent">➎</span>      for ( i = v2 - 472; ; i = v2 - 472 )<br/>       {<br/>         *(_DWORD *)(v2 - 524) = 0;<br/><span class="ent">➏</span>       sub_7A5950(i);<br/>         v3 = *(_DWORD *)(v2 - 540);<br/>         *(_DWORD *)(v2 - 524) = -1;<br/>         sub_9DD410(v3);<br/>   LABEL_5:<br/>          ;<br/>        }<br/>     }<br/>     *(_DWORD *)(v2 - 472) = &amp;off_B98EC8;<br/>     *(_DWORD *)(v2 - 56) = off_B991E4;<br/>     *(_DWORD *)(v2 - 524) = 2;<br/>     sub_58CB80(v2 - 56);<br/>     *(_DWORD *)(v2 - 524) = 0;<br/>     sub_7A5950(v2 - 472);<br/>     v5 = *(_DWORD *)(v2 - 540);<br/>     *(_DWORD *)(v2 - 524) = -1;<br/>     sub_9DD410(v5);<br/><span class="ent">➐</span>   return &amp;off_AE1854;<br/>   }</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_139"/>As you can see in the listing, the decompiled code is a lot easier to read than raw assembly. The decompiler guesses the function’s signature <span class="ent">➊</span> and local variables <span class="ent">➋</span>. Moreover, instead of assembly mnemonics, arithmetic and logical operations are expressed more intuitively, using C’s normal operators <span class="ent">➌</span>. The decompiler also attempts to reconstruct control-flow constructs, such as <span class="literal">if</span>/<span class="literal">else</span> branches <span class="ent">➍</span>, loops <span class="ent">➎</span>, and function calls <span class="ent">➏</span>. There’s also a C-style return statement, making it easier to see what the end result of the function is <span class="ent">➐</span>.</p>&#13;
<p class="indent">Useful as all this is, keep in mind that decompilation is nothing more than a tool to help you understand what the program is doing. The decompiled code is nowhere close to the original C source, may fail explicitly, and suffers from any inaccuracies in the underlying disassembly as well as inaccuracies in the decompilation process itself. That’s why it’s generally not a good idea to layer more advanced analyses on top of decompilation.</p>&#13;
<h4 class="h4" id="ch06_3_4"><em>6.3.4 Intermediate Representations</em></h4>&#13;
<p class="noindent">Instruction sets like x86 and ARM contain many different instructions with complex semantics. For instance, on x86, even seemingly simple instructions like <span class="literal">add</span> have side effects, such as setting status flags in the <span class="literal">eflags</span> register. The sheer number of instructions and side effects makes it difficult to reason about binary programs in an automated way. For example, as you’ll see in <a href="ch10.xhtml#ch10">Chapters 10</a> through <a href="ch13.xhtml#ch13">13</a>, dynamic taint analysis and symbolic execution engines must implement explicit handlers that capture the data-flow <span epub:type="pagebreak" id="page_140"/>semantics of all the instructions they analyze. Accurately implementing all these handlers is a daunting task.</p>&#13;
<p class="indent"><em>Intermediate representations (IR)</em>, also known as <em>intermediate languages</em>, are designed to remove this burden. An IR is a simple language that serves as an abstraction from low-level machine languages like x86 and ARM. Popular IRs include <em>Reverse Engineering Intermediate Language (REIL)</em> and <em>VEX IR</em> (the IR used in the <em>valgrind</em> instrumentation framework<sup><a id="ch06fn_12a" href="footnote.xhtml#ch06fn_12">12</a></sup>). There’s even a tool called <em>McSema</em> that translates binaries into <em>LLVM bitcode</em> (also known as <em>LLVM IR</em>).<sup><a id="ch06fn_13a" href="footnote.xhtml#ch06fn_13">13</a></sup></p>&#13;
<p class="indent">The idea of IR languages is to automatically translate real machine code, such as x86 code, into an IR that captures all of the machine code’s semantics but is much simpler to analyze. For comparison, REIL contains only 17 different instructions, as opposed to x86’s hundreds of instructions. Moreover, languages like REIL, VEX and LLVM IR explicitly express all operations, with no obscure instruction side effects.</p>&#13;
<p class="indent">It’s still a lot of work to implement the translation step from low-level machine code to IR code, but once that work is done, it’s much easier to implement new binary analyses on top of the translated code. Instead of having to write instruction-specific handlers for every binary analysis, with IRs you only have to do that once to implement the translation step. Moreover, you can write translators for many ISAs, such as x86, ARM, and MIPS, and map them all onto the same IR. That way, any binary analysis tool that works on that IR automatically inherits support for all of the ISAs that the IR supports.</p>&#13;
<p class="indent">The trade-off of translating a complex instruction set like x86 into a simple language like REIL, VEX, or LLVM IR is that IR languages are far less concise. That’s an inherent result of expressing complex operations, including all side effects, with a limited number of simple instructions. This is generally not an issue for automated analyses, but it does tend to make intermediate representations hard to read for humans. To give you an idea of what an IR looks like, take a look at <a href="ch06.xhtml#ch06list7">Listing 6-7</a>, which shows how the x86-64 instruction <span class="literal">add rax,rdx</span> translates into VEX IR.<sup><a id="ch06fn_14a" href="footnote.xhtml#ch06fn_14">14</a></sup></p>&#13;
<p class="listing1" id="ch06list7"><em>Listing 6-7: Translation of the x86-64 instruction</em> <span class="codeitalic">add rax,rdx</span> <em>into VEX IR</em></p>&#13;
<p class="programs"><span class="ent">➊</span> IRSB {<br/><span class="ent">➋</span>    t0:Ity_I64 t1:Ity_I64 t2:Ity_I64 t3:Ity_I64<br/><span class="ent">➌</span>    00   |   ------ IMark(0x40339f, 3, 0) ------<br/><span class="ent">➍</span>    01   |   t2 = GET:I64(rax)<br/>      02   |   t1 = GET:I64(rdx)<br/><span class="ent">➎</span>    03   |   t0 = Add64(t2,t1)<br/><span class="ent">➏</span>    04   |   PUT(cc_op) = 0x0000000000000004<br/>      05   |   PUT(cc_dep1) = t2<br/>      06   |   PUT(cc_dep2) = t1 <br/><span class="ent">➐</span>    07   |   PUT(rax) = t0<br/><span class="ent">➑</span>    08   |   PUT(pc) = 0x00000000004033a2<br/>      09   |   t3 = GET:I64(pc) <br/><span class="ent">➒</span><span epub:type="pagebreak" id="page_141"/>   NEXT: PUT(rip) = t3; Ijk_Boring<br/>   }</p>&#13;
<p class="indent">As you can see, the single <span class="literal">add</span> instruction results in 10 VEX instructions, plus some metadata. First, there’s some metadata that says this is an <em>IR super block (IRSB)</em> <span class="ent">➊</span> corresponding to one machine instruction. The IRSB contains four temporary values labeled <span class="literal">t0</span>–<span class="literal">t3</span>, all of type <span class="literal">Ity_I64</span> (64-bit integer) <span class="ent">➋</span>. Then there’s an <em>IMark</em> <span class="ent">➌</span>, which is metadata stating the machine instruction’s address and length, among other things.</p>&#13;
<p class="indent">Next come the actual IR instructions modeling the <span class="literal">add</span>. First, there are two <span class="literal">GET</span> instructions that fetch 64-bit values from <span class="literal">rax</span> and <span class="literal">rdx</span> into temporary stores <span class="literal">t2</span> and <span class="literal">t1</span>, respectively <span class="ent">➍</span>. Note that, here, <span class="literal">rax</span> and <span class="literal">rdx</span> are just symbolic names for the parts of VEX’s state used to model these registers—the VEX instructions don’t fetch from the real <span class="literal">rax</span> or <span class="literal">rdx</span> registers but rather from VEX’s mirror state of those registers. To perform the actual addition, the IR uses VEX’s <span class="literal">Add64</span> instruction, adding the two 64-bit integers <span class="literal">t2</span> and <span class="literal">t1</span> and storing the result in <span class="literal">t0</span> <span class="ent">➎</span>.</p>&#13;
<p class="indent">After the addition, there are some <span class="literal">PUT</span> instructions that model the <span class="literal">add</span> instruction’s side effects, such as updating the x86 status flags <span class="ent">➏</span>. Then, another <span class="literal">PUT</span> stores the result of the addition into VEX’s state representing <span class="literal">rax</span> <span class="ent">➐</span>. Finally, the VEX IR models updating the program counter to the next instruction <span class="ent">➑</span>. The <span class="literal">Ijk_Boring</span> (<em>Jump Kind Boring</em> ) <span class="ent">➒</span> is a control-flow hint that says the <span class="literal">add</span> instruction doesn’t affect the control flow in any interesting way; since the <span class="literal">add</span> isn’t a branch of any kind, control just “falls through” to the next instruction in memory. In contrast, branch instructions can be marked with hints like <span class="literal">Ijk_Call</span> or <span class="literal">Ijk_Ret</span> to inform the analysis that a call or return is taking place, for example.</p>&#13;
<p class="indent">When implementing tools on top of an existing binary analysis framework, you typically won’t have to deal with IR. The framework will handle all IR-related stuff internally. However, it’s useful to know about IRs if you ever plan to implement your own binary analysis framework or modify an existing one.</p>&#13;
<h3 class="h3" id="ch06_4">6.4 Fundamental Analysis Methods</h3>&#13;
<p class="noindent">The disassembly techniques you’ve learned so far in this chapter are the foundation of binary analysis. Many of the advanced techniques discussed in later chapters, such as binary instrumentation and symbolic execution, are based on these basic disassembly methods. But before moving on to those techniques, there are a few “standard” analyses I’d like to cover because they’re widely applicable. Note that these aren’t stand-alone binary analysis techniques, but you can use them as ingredients of more advanced binary <span epub:type="pagebreak" id="page_142"/>analyses. Unless I note otherwise, these are all normally implemented as static analyses, though you can also modify them to work for dynamic execution traces.</p>&#13;
<h4 class="h4" id="ch06_4_1"><em>6.4.1 Binary Analysis Properties</em></h4>&#13;
<p class="noindent">First, let’s go over some of the different properties that any binary analysis approach can have. This will help to classify the different techniques I’ll cover here and in later chapters and help you understand their trade-offs.</p>&#13;
<h3 class="h3">Interprocedural and Intraprocedural Analysis</h3>&#13;
<p class="noindent">Recall that functions are one of the fundamental code structures that disassemblers attempt to recover because it’s more intuitive to analyze code at the function level. Another reason for using functions is scalability: some analyses are simply infeasible when applied to a complete program.</p>&#13;
<p class="indent">The number of possible paths through a program increases exponentially with the number of control transfers (such as jumps and calls) in the program. In a program with just 10 <span class="literal">if</span>/<span class="literal">else</span> branches, there are up to 2<sup>10</sup> = 1,024 possible paths through the code. In a program with a hundred such branches, there are up to 1.27 × 10<sup>30</sup> possible paths, and a thousand branches yield up to 1.07 × 10<sup>301</sup> paths! Many programs have far more branches than that, so it’s not computationally feasible to analyze every possible path through a nontrivial program.</p>&#13;
<p class="indent">That’s why computationally expensive binary analyses are often <em>intraprocedural</em>: they consider the code only within a single function at a time. Typically, an intraprocedural analysis will analyze the CFG of each function in turn. This is in contrast to <em>interprocedural</em> analysis, which considers an entire program as a whole, typically by linking all the function CFGs together via the call graph.</p>&#13;
<p class="indent">Because most functions contain only a few dozen control transfer instructions, complex analyses are computationally feasible at the function level. If you individually analyze 10 functions with 1,024 possible paths each, you analyze a total of 10 × 1,024 = 10,240 paths; that’s a lot better than the 1,024<sup>10</sup> ≈ 1.27 × 10<sup>30</sup> paths you’d have to analyze if you considered the whole program at once.</p>&#13;
<p class="indent">The downside of intraprocedural analysis is that it’s not complete. For instance, if your program contains a bug that’s triggered only after a very specific combination of function calls, an intraprocedural bug detection tool won’t find the bug. It will simply consider each function on its own and conclude there’s nothing wrong. In contrast, an interprocedural tool would find the bug but might take so long to do so that the results won’t matter anymore.</p>&#13;
<p class="indent">As another example, let’s consider how a compiler might decide to optimize the code shown in <a href="ch06.xhtml#ch06list8">Listing 6-8</a>, depending on whether it’s using intraprocedural or interprocedural optimization.</p>&#13;
<p class="listing1" id="ch06list8"><span epub:type="pagebreak" id="page_143"/><em>Listing 6-8: A program containing a dead function</em></p>&#13;
<p class="programs">   #include &lt;stdio.h&gt;<br/><br/>   static void<br/><span class="ent">➊</span> dead(int x)<br/>   {<br/><span class="ent">➋</span>    if(x == 5) {<br/>        printf("Never reached\n");<br/>     }<br/>   }<br/><br/>   int<br/>   main(int argc, char *argv[])<br/>   {<br/><span class="ent">➌</span>   dead(4);<br/>     return 0;<br/>   }</p>&#13;
<p class="indent">In this example, there’s a function called <span class="literal">dead</span> that takes a single integer parameter <span class="literal">x</span> and returns nothing <span class="ent">➊</span>. Inside the function, there is a branch that will print a message only if <span class="literal">x</span> is equal to 5 <span class="ent">➋</span>. As it happens, <span class="literal">dead</span> is invoked from only one location, with the constant value 4 as its argument <span class="ent">➌</span>. Thus, the branch at <span class="ent">➋</span> is never taken, and no message is ever printed.</p>&#13;
<p class="indent">Compilers use an optimization called <em>dead code elimination</em> to find instances of code that can never be reached in practice so that they can omit such useless code in the compiled binary. In this case, though, a purely intraprocedural dead code elimination pass would fail to eliminate the useless branch at <span class="ent">➋</span>. This is because when the pass is optimizing <span class="literal">dead</span>, it doesn’t know about any of the code in other functions, so it doesn’t know where and how <span class="literal">dead</span> is invoked. Similarly, when it’s optimizing <span class="literal">main</span>, it cannot look inside <span class="literal">dead</span> to notice that the specific argument passed to <span class="literal">dead</span> at <span class="ent">➌</span> results in <span class="literal">dead</span> doing nothing.</p>&#13;
<p class="indent">It takes an interprocedural analysis to conclude that <span class="literal">dead</span> is only ever called from <span class="literal">main</span> with the value 4 and that this means the branch at <span class="ent">➋</span> will never be taken. Thus, an intraprocedural dead code elimination pass will output the entire <span class="literal">dead</span> function (and its invocations) in the compiled binary, even though it serves no purpose, while an interprocedural pass will omit the entire useless function.</p>&#13;
<h3 class="h3">Flow-Sensitivity</h3>&#13;
<p class="noindent">A binary analysis can be either <em>flow-sensitive</em> or <em>flow-insensitive</em>.<sup><a id="ch06fn_15a" href="footnote.xhtml#ch06fn_15">15</a></sup> Flow-sensitivity means that the analysis takes the order of the instructions into <span epub:type="pagebreak" id="page_144"/>account. To make this clearer, take a look at the following example in pseudocode.</p>&#13;
<p class="programs">x = unsigned_int(argv[0]) #  <span class="ent">➊</span><span class="codeitalic1">x</span> ∊ [0,∞]<br/>x = x + 5                 #  <span class="ent">➋</span><span class="codeitalic1">x</span> ∊ [5,∞]<br/>x = x + 10                #  <span class="ent">➌</span><span class="codeitalic1">x</span> ∊ [15,∞]</p>&#13;
<p class="indent">The code takes an unsigned integer from user input and then performs some computation on it. For this example, let’s assume you’re interested in doing an analysis that tries to determine the potential values each variable can assume; this is called <em>value set analysis</em>. A flow-insensitive version of this analysis would simply determine that <span class="literal">x</span> may contain any value since it gets its value from user input. While it’s true in general that <span class="literal">x</span> could take on any value at some point in the program, this isn’t true for <em>all</em> points in the program. So, the information provided by the flow-insensitive analysis is not very precise, but the analysis is relatively cheap in terms of computational complexity.</p>&#13;
<p class="indent">A flow-sensitive version of the analysis would yield more precise results. In contrast to the flow-insensitive variant, it provides an estimate of <span class="literal">x</span>’s possible value set <em>at each point in the program</em>, taking into account the previous instructions. At <span class="ent">➊</span>, the analysis concludes that <span class="literal">x</span> can have any unsigned value since it’s taken from user input and there haven’t yet been any instructions to constrain the value of <span class="literal">x</span>. However, at <span class="ent">➋</span>, you can refine the estimate: since the value 5 is added to <span class="literal">x</span>, you know that from this point on, <span class="literal">x</span> can only have a value of at least 5. Similarly, after the instruction at <span class="ent">➌</span>, you know that <span class="literal">x</span> is at least equal to 15.</p>&#13;
<p class="indent">Of course, things aren’t quite so simple in real life, where you must deal with more complex constructs such as branches, loops, and (recursive) function calls instead of simple straight-line code. As a result, flow-sensitive analyses tend to be much more complex and also more computationally intensive than flow-insensitive analyses.</p>&#13;
<h3 class="h3">Context-Sensitivity</h3>&#13;
<p class="noindent">While flow-sensitivity considers the order of instructions, <em>context-sensitivity</em> takes the order of function invocations into account. Context-sensitivity is meaningful only for interprocedural analyses. A <em>context-insensitive</em> interprocedural analysis computes a single, global result. On the other hand, a <em>context-sensitive</em> analysis computes a separate result for each possible path through the call graph (in other words, for each possible order in which functions may appear on the call stack). Note that this implies that the accuracy of a context-sensitive analysis is bounded by the accuracy of the call graph. The <em>context</em> of the analysis is the state accrued while traversing the call graph. I’ll represent this state as a list of previously traversed functions, denoted as &lt; <em>f</em><sub>1</sub>, <em>f</em><sub>2</sub>, . . . , <em>f</em><sub>n</sub> &gt;.</p>&#13;
<p class="indent">In practice, the context is usually limited, because very large contexts make flow-sensitive analysis too computationally expensive. For instance, the analysis may only compute results for contexts of five (or any arbitrary number of) consecutive functions, instead of for complete paths of indefinite <span epub:type="pagebreak" id="page_145"/>length. As an example of the benefits of context-sensitive analysis, take a look at <a href="ch06.xhtml#ch06fig9">Figure 6-9</a>.</p>&#13;
<div class="image"><a id="ch06fig9"/><img src="Images/f145-01.jpg" alt="image" width="696" height="407"/></div>&#13;
<p class="fig-caption"><em>Figure 6-9: Context-sensitive versus context-insensitive indirect call analysis in</em> <span class="literal">opensshd</span></p>&#13;
<p class="indent">The figure shows how context-sensitivity affects the outcome of an indirect call analysis in <span class="literal">opensshd</span> v3.5. The goal of the analysis is to figure out the possible targets of an indirect call site in the <span class="literal">channel_handler</span> function (the line that reads <span class="literal">(*ftab[c-&gt;type])(c, readset, writeset);</span>). The indirect call site takes its target from a table of function pointers, which is passed in as an argument called <span class="literal">ftab</span> to <span class="literal">channel_handler</span>. The <span class="literal">channel_handler</span> function is called from two other functions: <span class="literal">channel_prepare_select</span> and <span class="literal">channel_after_select</span>. Each of these passes its own function pointer table as the <span class="literal">ftab</span> argument.</p>&#13;
<p class="indent">A context-insensitive indirect call analysis concludes that the indirect call in <span class="literal">channel_handler</span> could target any function pointer in either the <span class="literal">channel_pre</span> table (passed in from <span class="literal">channel_prepare_select</span>) or the <span class="literal">channel_post</span> table (passed in from <span class="literal">channel_after_select</span>). Effectively, it concludes that the set of possible targets is the union of all the possible sets in any path through the program <span class="ent">➊</span>.</p>&#13;
<p class="indent">In contrast, the context-sensitive analysis determines a different target set for each possible context of preceding calls. If <span class="literal">channel_handler</span> was invoked by <span class="literal">channel_prepare_select</span>, then the only valid targets are those in the <span class="literal">channel_pre</span> table that it passes to <span class="literal">channel_handler</span> <span class="ent">➋</span>. On the other hand, if <span class="literal">channel_handler</span> was called from <span class="literal">channel_after_select</span>, then only the targets in <span class="literal">channel_post</span> are possible <span class="ent">➌</span>. In this example, I’ve discussed only a context of length 1, but in general the context could be arbitrarily long (as long as the longest possible path through the call graph).</p>&#13;
<p class="indent">As with flow-sensitivity, the upside of context-sensitivity is increased precision, while the downside is the greater computational complexity. In addition, context-sensitive analyses must deal with the large amount of state that <span epub:type="pagebreak" id="page_146"/>must be kept to track all the different contexts. Moreover, if there are any recursive functions, the number of possible contexts is infinite, so special measures are needed to deal with these cases.<sup><a id="ch06fn_16a" href="footnote.xhtml#ch06fn_16">16</a></sup> Often, it may not be feasible to create a scalable context-sensitive version of an analysis without resorting to cost and benefit trade-offs such as limiting the context size.</p>&#13;
<h4 class="h4" id="ch06_4_2"><em>6.4.2 Control-Flow Analysis</em></h4>&#13;
<p class="noindent">The purpose of any binary analysis is to figure out information about a program’s control-flow properties, its data-flow properties, or both. A binary analysis that looks at control-flow properties is aptly called a <em>control-flow analysis</em>, while a data flow–oriented analysis is called a <em>data-flow analysis</em>. The distinction is based purely on whether the analysis focuses on control or data flow; it doesn’t say anything about whether the analysis is intraprocedural or interprocedural, flow-sensitive or insensitive, or context-sensitive or insensitive. Let’s start by looking at a common type of control-flow analysis, called <em>loop detection</em>. In the next section, you’ll see some common data-flow analyses.</p>&#13;
<h3 class="h3">Loop Detection</h3>&#13;
<p class="noindent">As the name implies, the purpose of loop detection is to find loops in the code. At the source level, keywords like <span class="literal">while</span> or <span class="literal">for</span> give you an easy way to spot loops. At the binary level, it’s a little harder, because loops are implemented using the same (conditional or unconditional) jump instructions used to implement <span class="literal">if</span>/<span class="literal">else</span> branches and switches.</p>&#13;
<p class="indent">The ability to find loops is useful for many reasons. For instance, from the compiler perspective, loops are interesting because much of a program’s execution time is spent inside loops (an often quoted number is 90 percent). That means that loops are an interesting target for optimization. From a security perspective, analyzing loops is useful because vulnerabilities such as buffer overflows tend to occur in loops.</p>&#13;
<p class="indent">Loop detection algorithms used in compilers use a different definition of a loop than what you might intuitively expect. These algorithms look for <em>natural loops</em>, which are loops that have certain well-formedness properties that make them easier to analyze and optimize. There are also algorithms that detect any <em>cycle</em> in a CFG, even those that don’t conform to the stricter definition of a natural loop. <a href="ch06.xhtml#ch06fig10">Figure 6-10</a> shows an example of a CFG containing a natural loop, as well as a cycle that isn’t a natural loop.</p>&#13;
<p class="indent">First, I’ll show you the typical algorithm used to detect natural loops. After that, it will be clearer to you why not every cycle fits that definition. To understand what a natural loop is, you’ll need to learn what a <em>dominance tree</em> is. The right side of <a href="ch06.xhtml#ch06fig10">Figure 6-10</a> shows an example of a dominance tree, which corresponds to the CFG shown on the left side of the figure.</p>&#13;
<div class="image"><span epub:type="pagebreak" id="page_147"/><a id="ch06fig10"/><img src="Images/f147-01.jpg" alt="image" width="661" height="309"/></div>&#13;
<p class="fig-caption"><em>Figure 6-10: A CFG and the corresponding dominance tree</em></p>&#13;
<p class="indent">A basic block <em>A</em> is said to <em>dominate</em> another basic block <em>B</em> if the only way to get to <em>B</em> from the entry point of the CFG is to go through <em>A</em> first. For instance, in <a href="ch06.xhtml#ch06fig10">Figure 6-10</a>, <em>BB</em><sub>3</sub> dominates <em>BB</em><sub>5</sub> but not <em>BB</em><sub>6</sub>, since <em>BB</em><sub>6</sub> can also be reached via <em>BB</em><sub>4</sub>. Instead, <em>BB</em><sub>6</sub> is dominated by <em>BB</em><sub>1</sub>, which is the last node that any path from the entry point to <em>BB</em><sub>6</sub> must flow through. The dominance tree encodes all the dominance relationships in the CFG.</p>&#13;
<p class="indent">Now, a natural loop is induced by a <em>back edge</em> from a basic block <em>B</em> to <em>A</em>, where <em>A</em> dominates <em>B</em>. The loop resulting from this back edge contains all basic blocks dominated by <em>A</em> from which there is a path to <em>B</em>. Conventionally, <em>B</em> itself is excluded from this set. Intuitively, this definition means that natural loops cannot be entered somewhere in the middle but only at a well-defined <em>header node</em>. This simplifies the analysis of natural loops.</p>&#13;
<p class="indent">For instance, in <a href="ch06.xhtml#ch06fig10">Figure 6-10</a>, there’s a natural loop spanning basic blocks <em>BB</em><sub>3</sub> and <em>BB</em><sub>5</sub> since there’s a back edge from <em>BB</em><sub>5</sub> to <em>BB</em><sub>3</sub> and <em>BB</em><sub>3</sub> dominates <em>BB</em><sub>5</sub>. In this case, <em>BB</em><sub>3</sub> is the header node of the loop, <em>BB</em><sub>5</sub> is the “loopback” node, and the loop “body” (which by definition doesn’t include the header and loopback nodes) doesn’t contain any nodes.</p>&#13;
<h3 class="h3">Cycle Detection</h3>&#13;
<p class="noindent">You may have noticed another back edge in the graph, leading from <em>BB</em><sub>7</sub> to <em>BB</em><sub>4</sub>. This back edge induces a cycle, but <em>not</em> a natural loop, since the loop can be entered “in the middle” at <em>BB</em><sub>6</sub> or <em>BB</em><sub>7</sub>. Because of this, <em>BB</em><sub>4</sub> doesn’t dominate <em>BB</em><sub>7</sub>, so the cycle does not meet the definition of a natural loop.</p>&#13;
<p class="indent">To find cycles like this, including any natural loops, you only need the CFG, not the dominance tree. Simply start a depth-first search (DFS) from the entry node of the CFG, then keep a stack where you push any basic block that the DFS traverses and “pop” it back off when the DFS backtracks. If the DFS ever hits a basic block that’s already on the stack, then you’ve found a cycle.</p>&#13;
<p class="indent">For instance, let’s assume you’re doing a DFS on the CFG shown in <a href="ch06.xhtml#ch06fig10">Figure 6-10</a>. The DFS starts at the entry point, <em>BB</em><sub>1</sub>. <a href="ch06.xhtml#ch06list9">Listing 6-9</a> shows how <span epub:type="pagebreak" id="page_148"/>the DFS state evolves and how the DFS detects both cycles in the CFG (for brevity, I don’t show how the DFS continues after finding both cycles).</p>&#13;
<p class="listing1" id="ch06list9"><em>Listing 6-9: Cycle detection using DFS</em></p>&#13;
<p class="programs">    0:   [BB<sub>1</sub>]<br/>    1:   [BB<sub>1</sub>,BB<sub>2</sub>]<br/>    2:   [BB<sub>1</sub>]<br/>    3:   [BB<sub>1</sub>,BB<sub>3</sub>]<br/>    4:   [BB<sub>1</sub>,BB<sub>3</sub>,BB<sub>5</sub>]<br/><span class="ent">➊</span>   5:   [BB<sub>1</sub>,BB<sub>3</sub>,BB<sub>5</sub>,BB<sub>3</sub>]                 *cycle found*<br/>    6:   [BB<sub>1</sub>,BB<sub>3</sub>,BB<sub>5</sub>]<br/>    7:   [BB<sub>1</sub>,BB<sub>3</sub>,BB<sub>5</sub>,BB<sub>7</sub>]<br/>    8:   [BB<sub>1</sub>,BB<sub>3</sub>,BB<sub>5</sub>,BB<sub>7</sub>,BB<sub>4</sub>]<br/>    9:   [BB<sub>1</sub>,BB<sub>3</sub>,BB<sub>5</sub>,BB<sub>7</sub>,BB<sub>4</sub>,BB<sub>6</sub>]<br/><span class="ent">➋</span>  10:  [BB<sub>1</sub>,BB<sub>3</sub>,BB<sub>5</sub>,BB<sub>7</sub>,BB<sub>4</sub>,BB<sub>6</sub>,BB<sub>7</sub>]      *cycle found*<br/>...</p>&#13;
<p class="indent">First, the DFS explores the leftmost branch of <em>BB</em><sub>1</sub> but quickly backtracks as it hits a dead end. It then enters the middle branch, leading from <em>BB</em><sub>1</sub> to <em>BB</em><sub>3</sub>, and continues its search through <em>BB</em><sub>5</sub>, after which it hits <em>BB</em><sub>3</sub> again, thereby finding the cycle encompassing <em>BB</em><sub>3</sub> and <em>BB</em><sub>5</sub> <span class="ent">➊</span>. It then backtracks to <em>BB</em><sub>5</sub> and continues its search down the path leading to <em>BB</em><sub>7</sub>, then <em>BB</em><sub>4</sub>, <em>BB</em><sub>6</sub>, until finally hitting <em>BB</em><sub>7</sub> again, finding the second cycle <span class="ent">➋</span>.</p>&#13;
<h4 class="h4" id="ch06_4_3"><em>6.4.3 Data-Flow Analysis</em></h4>&#13;
<p class="noindent">Now let’s take a look at some common data-flow analysis techniques: reaching definitions analysis, use-def chains, and program slicing.</p>&#13;
<h3 class="h3">Reaching Definitions Analysis</h3>&#13;
<p class="noindent"><em>Reaching definitions analysis</em> answers the question, “Which data definitions can reach this point in the program?” When I say a data definition can “reach” a point in the program, I mean that a value assigned to a variable (or, at a lower level, a register or memory location) can reach that point without the value being overwritten by another assignment in the meantime. Reaching definitions analysis is usually applied at the CFG level, though it can also be used interprocedurally.</p>&#13;
<p class="indent">The analysis starts by considering for each individual basic block which definitions the block <em>generates</em> and which it <em>kills</em>. This is usually expressed by computing a <em>gen</em> and <em>kill</em> set for each basic block. <a href="ch06.xhtml#ch06fig11">Figure 6-11</a> shows an example of a basic block’s <em>gen</em> and <em>kill</em> sets.</p>&#13;
<p class="indent">The <em>gen</em> set for <em>BB</em><sub>3</sub> contains the statements numbered 6 and 8 since those are data definitions in <em>BB</em><sub>3</sub> that survive until the end of the basic block. Statement 7 doesn’t survive since <span class="literal">z</span> is overwritten by statement 8. The <em>kill</em> set contains statements 1, 3, and 4 from <em>BB</em><sub>1</sub> and <em>BB</em><sub>2</sub> since those assignments are overwritten by other assignments in <em>BB</em><sub>3</sub>.</p>&#13;
<div class="image"><a id="ch06fig11"/><img src="Images/f149-01.jpg" alt="image" width="253" height="272"/></div>&#13;
<p class="fig-caption"><em>Figure 6-11: Example of</em> gen <em>and</em> kill <em>sets for a basic block</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_149"/>After computing each basic block’s <em>gen</em> and <em>kill</em> sets, you have a <em>local</em> solution that tells you which data definitions each basic block generates and kills. From that, you can compute a <em>global</em> solution that tells you which definitions (from anywhere in the CFG) can reach the start of a basic block and which can still be alive after the basic block. The global set of definitions that can reach a basic block <em>B</em> is expressed as a set <em>in</em>[<em>B</em>], defined as follows:</p>&#13;
<div class="image"><img src="Images/f149-02.jpg" alt="image" width="215" height="49"/></div>&#13;
<p class="indent">Intuitively, this means the set of definitions reaching <em>B</em> is the union of all sets of definitions leaving other basic blocks that precede <em>B</em>. The set of definitions leaving a basic block <em>B</em> is denoted as <em>out</em>[<em>B</em>] and defined as follows:</p>&#13;
<div class="image"><img src="Images/f149-03.jpg" alt="image" width="323" height="19"/></div>&#13;
<p class="indent">In other words, the definitions that leave <em>B</em> are those <em>B</em> either generates itself or that <em>B</em> receives from its predecessors (as part of its <em>in</em> set) and doesn’t kill. Note that there’s a mutual dependency between the definitions of the <em>in</em> and <em>out</em> sets: <em>in</em> is defined in terms of <em>out</em>, and vice versa. This means that in practice, it’s not enough for a reaching definitions analysis to compute the <em>in</em> and <em>out</em> sets for each basic block just once. Instead, the analysis must be iterative: in each iteration, it computes the sets for every basic block, and it continues iterating until there are no more changes in the sets. Once all of the <em>in</em> and <em>out</em> sets have reached a stable state, the analysis is complete.</p>&#13;
<p class="indent">Reaching definitions analysis forms the basis of many data-flow analyses. This includes <em>use-def analysis</em>, which I’ll discuss next.</p>&#13;
<h3 class="h3">Use-Def Chains</h3>&#13;
<p class="noindent"><em>Use-def chains</em> tell you, at each point in the program where a variable is used, where that variable may have been defined. For instance, in <a href="ch06.xhtml#ch06fig12">Figure 6-12</a>, the <span epub:type="pagebreak" id="page_150"/>use-def chain for <span class="literal">y</span> in <em>B</em><sub>2</sub> contains statements 2 and 7. This is because at that point in the CFG, <span class="literal">y</span> could have gotten its value from the original assignment at statement 2 or (after one iteration of the loop) at statement 7. Note that there’s no use-def chain for <span class="literal">z</span> in <em>B</em><sub>2</sub>, as <span class="literal">z</span> is only assigned in that basic block, not used.</p>&#13;
<div class="image"><a id="ch06fig12"/><img src="Images/f150-01.jpg" alt="image" width="438" height="368"/></div>&#13;
<p class="fig-caption"><em>Figure 6-12: Example of use-def chains</em></p>&#13;
<p class="indent">One instance where use-def chains come in handy is decompilation: they allow the decompiler to track where a value used in a conditional jump was compared. This way, the decompiler can take a <span class="literal">cmp x,5</span> and <span class="literal">je</span> (jump if equal) instruction and merge them into a higher-level expression like <span class="literal">if(x == 5)</span>. Use-def chains are also used in compiler optimizations such as <em>constant propagation</em>, which replaces a variable by a constant if that’s the only possible value at that point in the program. They’re also useful in a myriad of other binary analysis scenarios.</p>&#13;
<p class="indent">At first glance, computing use-def chains may seem complex. But given a reaching definitions analysis of the CFG, it’s quite straightforward to compute the use-def chain for a variable in a basic block using the <em>in</em> set to find the definitions of that variable that may reach the basic block. In addition to use-def chains, it’s also possible to compute def-use chains. In contrast to use-def chains, def-use chains tell you where in the program a given data definition may be used.</p>&#13;
<h3 class="h3">Program Slicing</h3>&#13;
<p class="noindent"><em>Slicing</em> is a data-flow analysis that aims to extract all instructions (or, for source-based analysis, lines of code) that contribute to the values of a chosen set of variables at a certain point in the program (called the <em>slicing criterion</em>). This is useful for debugging when you want to find out which parts of the code may be responsible for a bug, as well as when reverse engineering. Computing slices can get pretty complicated, and it’s still more of an active research topic than a production-ready technique. Still, it’s an interesting <span epub:type="pagebreak" id="page_151"/>technique, so it’s worth learning about. Here, I’ll just give you the general idea, but if you want to play around with slicing, I suggest taking a look at the angr reverse-engineering framework,<sup><a id="ch06fn_17a" href="footnote.xhtml#ch06fn_17">17</a></sup> which offers built-in slicing functionality. You’ll also see how to implement a practical slicing tool with symbolic execution in <a href="ch13.xhtml#ch13">Chapter 13</a>.</p>&#13;
<p class="indent">Slices are computed by tracking control and data flows to figure out which parts of the code are irrelevant to the slice and then deleting those parts. The final slice is whatever remains after deleting all the irrelevant code. As an example, let’s say you want to know which lines in <a href="ch06.xhtml#ch06list10">Listing 6-10</a> contribute to the value of <span class="literal">y</span> on line 14.</p>&#13;
<p class="listing1" id="ch06list10"><em>Listing 6-10: Using slicing to find the lines contributing to</em> <span class="codeitalic">y</span> <em>on line 14</em></p>&#13;
<p class="programs">1:  <span class="gray_mark">x = int(argv[0])</span><br/>2:  <span class="gray_mark">y = int(argv[1])</span><br/>3:<br/>4:  z = x +   y<br/>5:  <span class="gray_mark">while(x   &lt;  5) {</span><br/>6:    <span class="gray_mark">x = x   +  1</span><br/>7:    <span class="gray_mark">y = y   +  2</span><br/>8:    z = z   +  x<br/>9:    z = z   +  y<br/>10:   z = z  *   5<br/>11: }<br/>12:<br/>13: print(x)<br/>14: <span class="gray_mark">print(y)</span><br/>15: print(z)</p>&#13;
<p class="indent">The slice contains the lines shaded gray in this code. Note that all the assignments to <span class="literal">z</span> are completely irrelevant to the slice because they make no difference to the eventual value of <span class="literal">y</span>. What happens with <span class="literal">x</span> <em>is</em> relevant since it determines how often the loop on line 5 iterates, which in turn affects the value of <span class="literal">y</span>. If you compile a program with just the lines included in the slice, it will yield exactly the same output for the <span class="literal">print(y)</span> statement as the full program would.</p>&#13;
<p class="indent">Originally, slicing was proposed as a static analysis, but nowadays it’s often applied to dynamic execution traces instead. Dynamic slicing has the advantage that it tends to produce smaller (and therefore more readable) slices than static slicing does.</p>&#13;
<p class="indent">What you just saw is known as <em>backward slicing</em> since it searches backward for lines that affect the chosen slicing criterion. But there’s also <em>forward slicing</em>, which starts from a point in the program and then searches forward to determine which other parts of the code are somehow affected by the instruction and variable in the chosen slicing criterion. Among other things, this can predict which parts of the code will be impacted by a change to the code at the chosen point.</p>&#13;
<h3 class="h3" id="ch06_5"><span epub:type="pagebreak" id="page_152"/>6.5 Effects of Compiler Settings on Disassembly</h3>&#13;
<p class="noindent">Compilers optimize code to minimize its size or execution time. Unfortunately, optimized code is usually significantly harder to accurately disassemble (and therefore analyze) than unoptimized code.</p>&#13;
<p class="indent">Optimized code corresponds less closely to the original source, making it less intuitive to a human. For instance, when optimizing arithmetic code, compilers will go out of their way to avoid the very slow <span class="literal">mul</span> and <span class="literal">div</span> instructions and instead implement multiplications and divisions using a series of bitshift and add operations. These can be challenging to decipher when reverse engineering the code.</p>&#13;
<p class="indent">Also, compilers often merge small functions into the larger functions calling them, to avoid the cost of the <span class="literal">call</span> instruction; this merging is called <em>inlining</em>. Thus, not all functions you see in the source code are necessarily there in the binary, at least not as a separate function. In addition, common function optimizations such as tail calls and optimized calling conventions make function detection significantly less accurate.</p>&#13;
<p class="indent">At higher optimization levels, compilers often emit padding bytes between functions and basic blocks to align them at memory addresses where they can be most efficiently accessed. Interpreting these padding bytes as code can cause disassembly errors if the padding bytes aren’t valid instructions. Moreover, compilers may “unroll” loops to avoid the overhead of jumping to the next iteration. This hinders loop detection algorithms and decompilers, which try to find high-level constructs like <span class="literal">while</span> and <span class="literal">for</span> loops in the code.</p>&#13;
<p class="indent">Optimizations may also hinder data structure detection, not just code discovery. For instance, optimized code may use the same base register to index different arrays at the same time, making it difficult to recognize them as separate data structures.</p>&#13;
<p class="indent">Nowadays, <em>link-time optimization (LTO)</em> is gaining in popularity, which means that optimizations that were traditionally applied on a per-module basis can now be used on the whole program. This increases the optimization surface for many optimizations, making the effects even more profound.</p>&#13;
<p class="indent">When writing and testing your own binary analysis tools, always keep in mind that their accuracy may suffer from optimized binaries.</p>&#13;
<p class="indent">In addition to the previous optimizations, binaries are increasingly often compiled as <em>position-independent code (PIC)</em> to accommodate security features like <em>address-space layout randomization (ASLR)</em>, which need to be able to move code and data around without this breaking the binary.<sup><a id="ch06fn_18a" href="footnote.xhtml#ch06fn_18">18</a></sup> Binaries compiled with PIC are called <em>position-independent executables (PIEs)</em>. In contrast to position-dependent binaries, PIE binaries don’t use absolute addresses to reference code and data. Instead, they use references relative to the program counter. This also means that some common constructs, such as the PLT in ELF binaries, look different in PIE binaries than in non-PIE binaries. <span epub:type="pagebreak" id="page_153"/>Thus, binary analysis tools that aren’t built with PIC in mind may not work properly for such binaries.</p>&#13;
<h3 class="h3" id="ch06_6">6.6 Summary</h3>&#13;
<p class="noindent">You’re now familiar with the inner workings of disassemblers as well as the essential binary analysis techniques you’ll need to understand the rest of this book. Now you’re ready to move on to techniques that will allow you to not only disassemble binaries but also modify them. Let’s start with basic binary modification techniques in <a href="ch07.xhtml#ch07">Chapter 7</a>!</p>&#13;
<div class="box">&#13;
<p class="headbox" id="ch06_7">Exercises</p>&#13;
<p class="boxhead1">1. Confusing objdump</p>&#13;
<p class="noindent">Write a program that confuses <span class="literal">objdump</span> such that it interprets data as code, or vice versa. You’ll probably need to use some inline disassembly to achieve this (for instance, using <span class="literal">gcc</span>’s <span class="literal">asm</span> keyword).</p>&#13;
<p class="boxhead1">2. Confusing a Recursive Disassembler</p>&#13;
<p class="noindent">Write another program, this time so that it tricks your favorite recursive disassembler’s function detection algorithm. There are various ways to do this. For instance, you could create a tail-called function or a function that has a <span class="literal">switch</span> with multiple return cases. See how far you can go with confusing the disassembler!</p>&#13;
<p class="boxhead1">3. Improving Function Detection</p>&#13;
<p class="noindent">Write a plugin for your recursive disassembler of choice so that it can better detect functions such as those the disassembler missed in the previous exercise. You’ll need a recursive disassembler that you can write plugins for, such as IDA Pro, Hopper, or Medusa.<span epub:type="pagebreak" id="page_154"/></p>&#13;
</div>&#13;
</div></body></html>