<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><h2 class="h2" id="ch13"><span epub:type="pagebreak" id="page_79"/><strong><span class="big">13</span><br/>LARGE TRAINING SETS FOR VISION TRANSFORMERS</strong></h2>&#13;
<div class="image1"><img src="../images/common.jpg" alt="Image" width="252" height="252"/></div>&#13;
<p class="noindent">Why do vision transformers (ViTs) generally require larger training sets than convolutional neural networks (CNNs)?</p>&#13;
<p class="indent">Each machine learning algorithm and model encodes a particular set of assumptions or prior knowledge, commonly referred to as <em>inductive biases</em>, in its design. Some inductive biases are workarounds to make algorithms computationally more feasible, other inductive biases are based on domain knowledge, and some inductive biases are both.</p>&#13;
<p class="indent">CNNs and ViTs can be used for the same tasks, including image classification, object detection, and image segmentation. CNNs are mainly composed of convolutional layers, while ViTs consist primarily of multi-head attention blocks (discussed in <a href="ch08.xhtml">Chapter 8</a> in the context of transformers for natural language inputs).</p>&#13;
<p class="indent">CNNs have more inductive biases that are hardcoded as part of the algorithmic design, so they generally require less training data than ViTs. In a sense, ViTs are given more degrees of freedom and can or must learn certain inductive biases from the data (assuming that these biases are conducive to optimizing the training objective). However, everything that needs to be learned requires more training examples.</p>&#13;
<p class="indent">The following sections explain the main inductive biases encountered in CNNs and how ViTs work well without them.<span epub:type="pagebreak" id="page_80"/></p>&#13;
<h3 class="h3" id="ch00lev60"><strong>Inductive Biases in CNNs</strong></h3>&#13;
<p class="noindent">The following are the primary inductive biases that largely define how CNNs function:</p>&#13;
<p class="noindentin"><strong>Local connectivity</strong> In CNNs, each unit in a hidden layer is connected to only a subset of neurons in the previous layer. We can justify this restriction by assuming that neighboring pixels are more relevant to each other than pixels that are farther apart. As an intuitive example, consider how this assumption applies to the context of recognizing edges or contours in an image.</p>&#13;
<p class="noindentin"><strong>Weight sharing</strong> Via the convolutional layers, we use the same small set of weights (the kernels or filters) throughout the whole image. This reflects the assumption that the same filters are useful for detecting the same patterns in different parts of the image.</p>&#13;
<p class="noindentin"><strong>Hierarchical processing</strong> CNNs consist of multiple convolutional layers to extract features from the input image. As the network progresses from the input to the output layers, low-level features are successively combined to form increasingly complex features, ultimately leading to the recognition of more complex objects and shapes. Furthermore, the convolutional filters in these layers learn to detect specific patterns and features at different levels of abstraction.</p>&#13;
<p class="noindentin"><strong>Spatial invariance</strong> CNNs exhibit the mathematical property of spatial invariance, meaning the output of a model remains consistent even if the input signal is shifted to a different location within the spatial domain. This characteristic arises from the combination of local connectivity, weight sharing, and the hierarchical architecture mentioned earlier.</p>&#13;
<p class="indenta">The combination of local connectivity, weight sharing, and hierarchical processing in a CNN leads to spatial invariance, allowing the model to recognize the same pattern or feature regardless of its location in the input image.</p>&#13;
<p class="indent"><em>Translation invariance</em> is a specific case of spatial invariance in which the output remains the same after a shift or translation of the input signal in the spatial domain. In this context, the emphasis is solely on moving an object to a different location within an image without any rotations or alterations of its other attributes.</p>&#13;
<p class="indent">In reality, convolutional layers and networks are not truly translation-invariant; rather, they achieve a certain level of translation equivariance. What is the difference between translation invariance and equivariance? <em>Translation invariance</em> means that the output does not change with an input shift, while <em>translation equivariance</em> implies that the output shifts with the input in a corresponding manner. In other words, if we shift the input object to the right, the results will correspondingly shift to the right, as illustrated in <a href="ch13.xhtml#ch13fig1">Figure 13-1</a>.<span epub:type="pagebreak" id="page_81"/></p>&#13;
<div class="image"><img id="ch13fig1" src="../images/13fig01.jpg" alt="Image" width="572" height="318"/></div>&#13;
<p class="figcap"><em>Figure 13-1: Equivariance under different image translations</em></p>&#13;
<p class="indent">As <a href="ch13.xhtml#ch13fig1">Figure 13-1</a> shows, under translation invariance, we get the same output pattern regardless of the order in which we apply the operations: transformation followed by translation or translation followed by transformation.</p>&#13;
<p class="indent">As mentioned earlier, CNNs achieve translation equivariance through a combination of their local connectivity, weight sharing, and hierarchical processing properties. <a href="ch13.xhtml#ch13fig2">Figure 13-2</a> depicts a convolutional operation to illustrate the local connectivity and weight-sharing priors. This figure demonstrates the concept of translation equivariance in CNNs, in which a convolutional filter captures the input signal (the two dark blocks) irrespective of where it is located in the input.</p>&#13;
<div class="image"><img id="ch13fig2" src="../images/13fig02.jpg" alt="Image" width="591" height="465"/></div>&#13;
<p class="figcap"><em>Figure 13-2: Convolutional filters and translation equivariance</em></p>&#13;
<p class="indent"><a href="ch13.xhtml#ch13fig2">Figure 13-2</a> shows a 3<em>×</em>3 input image that consists of two nonzero pixel values in the upper-left corner (top portion of the figure) or upper-right corner (bottom portion of the figure). If we apply a 2<em>×</em>2 convolutional filter to these two input image scenarios, we can see that the output feature maps contain the same extracted pattern, which is on either the left (top of the figure) or the right (bottom of the figure), demonstrating the translation equivariance of the convolutional operation.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_82"/>For comparison, a fully connected network such as a multilayer perceptron lacks this spatial invariance or equivariance. To illustrate this point, picture a multilayer perceptron with one hidden layer. Each pixel in the input image is connected with each value in the resulting output. If we shift the input by one or more pixels, a different set of weights will be activated, as illustrated in <a href="ch13.xhtml#ch13fig3">Figure 13-3</a>.</p>&#13;
<div class="image"><img id="ch13fig3" src="../images/13fig03.jpg" alt="Image" width="463" height="494"/></div>&#13;
<p class="figcap"><em>Figure 13-3: Location-specific weights in fully connected layers</em></p>&#13;
<p class="indent">Like fully connected networks, ViT architecture (and transformer architecture in general) lacks the inductive bias for spatial invariance or equi-variance. For instance, the model produces different outputs if we place the same object in two different spatial locations within an image. This is not ideal, as the semantic meaning of an object (the concept that an object represents or conveys) remains the same based on its location. Consequently, it must learn these invariances directly from the data. To facilitate learning useful patterns present in CNNs requires pretraining over a larger dataset.</p>&#13;
<p class="indent">A common workaround for adding positional information in ViTs is to use relative positional embeddings (also known as <em>relative positional encodings</em>) that consider the relative distance between two tokens in the input sequence. However, while relative embeddings encode information that helps transformers keep track of the relative location of tokens, the transformer still needs to learn from the data whether and how far spatial information is relevant for the task at hand.</p>&#13;
<h3 class="h3" id="ch00lev61"><strong>ViTs Can Outperform CNNs</strong></h3>&#13;
<p class="noindent">The hardcoded assumptions via the inductive biases discussed in previous sections reduce the number of parameters in CNNs substantially compared to fully connected layers. On the other hand, ViTs tend to have larger numbers of parameters than CNNs, which require more training data. (Refer <span epub:type="pagebreak" id="page_83"/>to <a href="ch11.xhtml">Chapter 11</a> for a refresher on how to precisely calculate the number of parameters in fully connected and convolutional layers.)</p>&#13;
<p class="indent">ViTs may underperform compared to popular CNN architectures without extensive pretraining, but they can perform very well with a sufficiently large pretraining dataset. In contrast to language transformers, where unsupervised pretraining (such as self-supervised learning, discussed in <a href="ch02.xhtml">Chapter 2</a>) is a preferred choice, vision transformers are often pretrained using large, labeled datasets like ImageNet, which provides millions of labeled images for training, and regular supervised learning.</p>&#13;
<p class="indent">An example of ViTs surpassing the predictive performance of CNNs, given enough data, can be observed from initial research on the ViT architecture, as shown in the paper “An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.” This study compared ResNet, a type of convolutional network, with the original ViT design using different dataset sizes for pretraining. The findings also showed that the ViT model excelled over the convolutional approach only after being pretrained on a minimum of 100 million images.</p>&#13;
<h3 class="h3" id="ch00lev62"><strong>Inductive Biases in ViTs</strong></h3>&#13;
<p class="noindent">ViTs also possess some inductive biases. For example, vision transformers <em>patchify</em> the input image to process each input patch individually. Here, each patch can attend to all other patches so that the model learns relationships between far-apart patches in the input image, as illustrated in <a href="ch13.xhtml#ch13fig4">Figure 13-4</a>.</p>&#13;
<div class="image"><img id="ch13fig4" src="../images/13fig04.jpg" alt="Image" width="890" height="500"/></div>&#13;
<p class="figcap"><em>Figure 13-4: How a vision transformer operates on image patches</em></p>&#13;
<p class="indent">The patchify inductive bias allows ViTs to scale to larger image sizes without increasing the number of parameters in the model, which can be computationally expensive. By processing smaller patches individually, ViTs can efficiently capture spatial relationships between image regions while benefiting from the global context captured by the self-attention mechanism.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_84"/>This raises another question: how and what do ViTs learn from the training data? ViTs learn more uniform feature representations across all layers, with self-attention mechanisms enabling early aggregation of global information. In addition, the residual connections in ViTs strongly propagate features from lower to higher layers, in contrast to the more hierarchical structure of CNNs.</p>&#13;
<p class="indent">ViTs tend to focus more on global than local relationships because their self-attention mechanism allows the model to consider long-range dependencies between different parts of the input image. Consequently, the self-attention layers in ViTs are often considered low-pass filters that focus more on shapes and curvature.</p>&#13;
<p class="indent">In contrast, the convolutional layers in CNNs are often considered high-pass filters that focus more on texture. However, keep in mind that convolutional layers can act as both high-pass and low-pass filters, depending on the learned filters at each layer. High-pass filters detect an image’s edges, fine details, and texture, while low-pass filters capture more global, smooth features and shapes. CNNs achieve this by applying convolutional kernels of varying sizes and learning different filters at each layer.</p>&#13;
<h3 class="h3" id="ch00lev63"><strong>Recommendations</strong></h3>&#13;
<p class="noindent">ViTs have recently begun outperforming CNNs if enough data is available for pretraining. However, this doesn’t make CNNs obsolete, as methods such as the popular EfficientNetV2 CNN architecture are less memory and data hungry.</p>&#13;
<p class="indent">Moreover, recent ViT architectures don’t rely solely on large datasets, parameter numbers, and self-attention. Instead, they have taken inspiration from CNNs and added soft convolutional inductive biases or even complete convolutional layers to get the best of both worlds.</p>&#13;
<p class="indent">In short, vision transformer architectures without convolutional layers generally have fewer spatial and locality inductive biases than convolutional neural networks. Consequently, vision transformers need to learn data-related concepts such as local relationships among pixels. Thus, vision transformers require more training data to achieve good predictive performance and produce acceptable visual representations in generative modeling contexts.<span epub:type="pagebreak" id="page_85"/></p>&#13;
<h3 class="h3" id="ch00lev64"><strong>Exercises</strong></h3>&#13;
<p class="number1"><strong>13-1.</strong> Consider the patchification of the input images shown in <a href="ch13.xhtml#ch13fig4">Figure 13-4</a>. The size of the resulting patches controls a computational and predictive performance trade-off. The optimal patch size depends on the application and desired trade-off between computational cost and model performance. Do smaller patches typically result in higher or lower computational costs?</p>&#13;
<p class="number1"><strong>13-2.</strong> Following up on the previous question, do smaller patches typically lead to a higher or lower prediction accuracy?</p>&#13;
<h3 class="h3" id="ch00lev65"><strong>References</strong></h3>&#13;
<ul>&#13;
<li class="noindent">The paper proposing the original vision transformer model: Alexey Dosovitskiy et al., “An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale” (2020), <em><a href="https://arxiv.org/abs/2010.11929">https://arxiv.org/abs/2010.11929</a></em>.</li>&#13;
<li class="noindent">A workaround for adding positional information in ViTs is to use relative positional embeddings: Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani, “Self-Attention with Relative Position Representations” (2018), <em><a href="https://arxiv.org/abs/1803.02155">https://arxiv.org/abs/1803.02155</a></em>.</li>&#13;
<li class="noindent">Residual connections in ViTs strongly propagate features from lower to higher layers, in contrast to the more hierarchical structure of CNNs: Maithra Raghu et al., “Do Vision Transformers See Like Convolutional Neural Networks?” (2021), <em><a href="https://arxiv.org/abs/2108.08810">https://arxiv.org/abs/2108.08810</a></em>.</li>&#13;
<li class="noindent">A detailed research article covering the EfficientNetV2 CNN architecture: Mingxing Tan and Quoc V. Le, “EfficientNetV2: Smaller Models and Faster Training” (2021), <em><a href="https://arxiv.org/abs/2104.00298">https://arxiv.org/abs/2104.00298</a></em>.</li>&#13;
<li class="noindent">A ViT architecture that also incorporates convolutional layers: Stéphane d’Ascoli et al., “ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases” (2021), <em><a href="https://arxiv.org/abs/2103.10697">https://arxiv.org/abs/2103.10697</a></em>.</li>&#13;
<li class="noindent">Another example of a ViT using convolutional layers: Haiping Wu et al., “CvT: Introducing Convolutions to Vision Transformers” (2021), <em><a href="https://arxiv.org/abs/2103.15808">https://arxiv.org/abs/2103.15808</a></em>.<span epub:type="pagebreak" id="page_86"/></li></ul>&#13;
</div>
</div>
</body></html>