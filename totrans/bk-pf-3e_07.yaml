- en: Chapter 7. Traffic Shaping with Queues and Priorities
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第7章 使用队列和优先级进行流量整形
- en: '![Traffic Shaping with Queues and Priorities](httpatomoreillycomsourcenostarchimages2127149.png.jpg)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![使用队列和优先级进行流量整形](httpatomoreillycomsourcenostarchimages2127149.png.jpg)'
- en: 'In this chapter, we look at how to use traffic shaping to allocate bandwidth
    resources efficiently and according to a specified policy. If the term *traffic
    shaping* seems unfamiliar, rest assured it means what you think it means: that
    you’ll be altering the way your network allocates resources in order to satisfy
    the requirements of your users and their applications. With a proper understanding
    of your network traffic and the applications and users that generate it, you can,
    in fact, go quite a bit of distance toward “bigger, better, faster, more” just
    by optimizing your network for the traffic that’s actually supposed to pass there.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨如何使用流量整形来高效地分配带宽资源，并根据特定策略进行调整。如果“流量整形”这个术语对你来说有些陌生，请放心，它的意思正如你所理解的那样：你将修改网络分配资源的方式，以满足用户及其应用程序的需求。通过正确理解你的网络流量以及产生这些流量的应用程序和用户，你实际上可以通过优化网络来实现“更大、更好、更快、更强”的目标，尽管只针对网络中实际需要传输的流量进行优化。
- en: 'A small but powerful arsenal of traffic-shaping tools is at your disposal;
    all of them work by introducing nondefault behavior into your network setup to
    bend the realities of your network according to your wishes. Traffic shaping for
    PF contexts currently comes in two flavors: the once experimental *ALTQ* (short
    for *alternate queuing*) framework, now considered old-style after some 15 years
    of service, and the newer OpenBSD *priorities and queuing* system introduced in
    OpenBSD 5.5.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 有一套小巧而强大的流量整形工具可供你使用；所有这些工具通过在你的网络设置中引入非默认行为来调整网络现实，以符合你的需求。针对 PF 上下文的流量整形目前有两种方式：一种是曾经实验性的*ALTQ*（*alternate
    queuing*，即“替代队列”）框架，经过约 15 年的使用后，现在被视为旧式方法；另一种是 OpenBSD 5.5 引入的较新的 OpenBSD *优先级和队列*系统。
- en: In the first part of the chapter, we introduce traffic shaping by looking at
    the features of the new OpenBSD priority and queuing system. If you’re about to
    set up on OpenBSD 5.5 or newer, you can jump right in, starting with the next
    section, [Always-On Priority and Queues for Traffic Shaping](ch07.html#always-on_priority_and_queues_for_traffi
    "Always-On Priority and Queues for Traffic Shaping"). This is also where the main
    traffic-shaping concepts are introduced with examples.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的第一部分通过观察新的 OpenBSD 优先级和队列系统的特点来介绍流量整形。如果你准备在 OpenBSD 5.5 或更高版本上进行设置，可以直接进入下一部分，[始终开启的优先级和队列用于流量整形](ch07.html#always-on_priority_and_queues_for_traffi
    "始终开启的优先级和队列用于流量整形")。在这一部分，我们将介绍流量整形的主要概念，并通过示例进行说明。
- en: On OpenBSD 5.4 and earlier as well as other BSDs where the PF code wasn’t current
    with OpenBSD 5.5, traffic shaping was the domain of the ALTQ system. On OpenBSD,
    ALTQ was removed after one transitional release, leaving only the newer traffic-shaping
    system in place from OpenBSD 5.6 onward. If you’re interested in converting an
    existing ALTQ setup to the new system, you’ll most likely find [Transitioning
    from ALTQ to Priorities and Queues](ch07.html#transitioning_from_altq_to_priorities_an
    "Transitioning from ALTQ to Priorities and Queues") useful; this section highlights
    the differences between the older ALTQ system and the new system.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在 OpenBSD 5.4 及更早版本，以及其他一些 BSD 系统中，PF 代码尚未与 OpenBSD 5.5 保持同步，因此流量整形由 ALTQ 系统负责。在
    OpenBSD 中，ALTQ 在经过一次过渡发布后被移除，从 OpenBSD 5.6 开始，只有较新的流量整形系统得以保留。如果你有兴趣将现有的 ALTQ
    设置迁移到新系统，你可能会发现[从 ALTQ 到优先级和队列的过渡](ch07.html#transitioning_from_altq_to_priorities_an
    "从 ALTQ 到优先级和队列的过渡")部分很有用；这一节重点介绍了旧版 ALTQ 系统与新系统之间的差异。
- en: If you’re working with an operating system where the queues system introduced
    in OpenBSD 5.5 isn’t yet available, you’ll want to study the ALTQ traffic-shaping
    subsystem, which is described in [Directing Traffic with ALTQ](ch07.html#directing_traffic_with_altq
    "Directing Traffic with ALTQ"). If you’re learning traffic-shaping concepts and
    want to apply them to an ALTQ setup, please read the first part of this chapter
    before diving into ALTQ-specific configuration details.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用一个尚未支持 OpenBSD 5.5 引入的队列系统的操作系统，你需要研究 ALTQ 流量整形子系统，相关内容可参见[使用 ALTQ 引导流量](ch07.html#directing_traffic_with_altq
    "使用 ALTQ 引导流量")。如果你正在学习流量整形的概念并希望将其应用于 ALTQ 设置，请在深入 ALTQ 特定配置细节之前阅读本章的第一部分。
- en: Always-On Priority and Queues for Traffic Shaping
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 始终开启的优先级和队列用于流量整形
- en: Managing your bandwidth has a lot in common with balancing your checkbook or
    handling other resources that are either scarce or available in finite quantities.
    The resource is available in a constant supply with hard upper limits, and you
    need to allocate the resource with maximum *efficiency*, according to the *priorities*
    set out in your *policy* or *specification*.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 管理带宽与平衡支票簿或处理其他有限资源有很多相似之处。这些资源持续供应，且有严格的上限，你需要根据*政策*或*规格*中设定的*优先级*，以最大*效率*分配资源。
- en: OpenBSD 5.5 and newer offers several different options for managing your bandwidth
    resources via classification mechanisms in our PF rule sets. We’ll take a look
    at what you can do with pure *traffic prioritization* first and then move on to
    how to subdivide your bandwidth resources by allocating defined subsets of your
    traffic to *queues*.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: OpenBSD 5.5及更新版本通过PF规则集中的分类机制提供了几种不同的带宽资源管理选项。我们将首先看看如何仅使用*流量优先级*来管理流量，然后再讨论如何通过为你的流量分配定义的子集到*队列*来细分带宽资源。
- en: Note
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '*The always-on priorities were introduced as a teaser of sorts in OpenBSD 5.0\.
    After several years in development and testing, the new queuing system was finally
    committed in time to be included in OpenBSD 5.5, which was released on May 1,
    2014\. If you’re starting your traffic shaping from scratch on OpenBSD 5.5 or
    newer or you’re considering doing so, this section is the right place to start.
    If you’re upgrading from an earlier OpenBSD version or transitioning from another
    ALTQ system to a recent OpenBSD, you’ll most likely find the following section,
    [Transitioning from ALTQ to Priorities and Queues](ch07.html#transitioning_from_altq_to_priorities_an
    "Transitioning from ALTQ to Priorities and Queues"), useful.*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*始终启用的优先级是作为OpenBSD 5.0中的一种预告功能引入的。经过几年的开发和测试，新的队列系统最终在OpenBSD 5.5中正式提交，并于2014年5月1日发布。如果你从OpenBSD
    5.5或更新版本开始流量整形，或者打算这么做，本节是一个合适的起点。如果你从早期版本的OpenBSD升级或从其他ALTQ系统迁移到最新版本的OpenBSD，你很可能会发现接下来的部分，[从ALTQ到优先级和队列的过渡](ch07.html#transitioning_from_altq_to_priorities_an
    "从ALTQ到优先级和队列的过渡")，非常有用。*'
- en: Shaping by Setting Traffic Priorities
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过设置流量优先级进行整形
- en: 'If you’re mainly interested in pushing certain kinds of traffic ahead of others,
    you may be able to achieve what you want by simply setting priorities: assigning
    a higher priority to some items so that they receive attention before others.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你主要是希望将某些类型的流量推到其他流量之前，你可以通过简单地设置优先级来实现：为一些项目分配更高的优先级，使它们在其他流量之前获得处理。
- en: The prio Priority Scheme
  id: totrans-14
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: prio优先级方案
- en: Starting with OpenBSD 5.0, a priority scheme for classifying network traffic
    on a per-rule basis is available. The range of priorities is from 0 to 7, where
    0 is lowest priority. Items assigned priority 7 will skip ahead of everything
    else, and the default value 3 is automatically assigned for most kinds of traffic.
    The priority scheme, which you’ll most often hear referred to as `prio` after
    the PF syntax keyword, is always enabled, and you can tweak your traffic by setting
    priorities via your `match` or `pass` rules.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 从OpenBSD 5.0开始，可以基于每条规则对网络流量进行分类的优先级方案。优先级范围从0到7，其中0为最低优先级。被分配优先级7的项目会优先于其他所有流量，默认值3会自动分配给大多数类型的流量。该优先级方案通常称为`prio`，在PF语法关键字之后最常见，你可以通过`match`或`pass`规则设置优先级来调整你的流量。
- en: 'For example, to speed up your outgoing SSH traffic to the max, you could put
    a rule like this in your configuration:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，为了最大化你的出站SSH流量，你可以在配置中添加如下规则：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Then your SSH traffic would be served before anything else.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你的SSH流量将优先处理。
- en: You could then examine the rest of your rule set and decide what traffic is
    more or less important, what you would like always to reach its destination, and
    what parts of your traffic you feel matter less.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你可以检查其余的规则集，决定哪些流量更重要，哪些流量你希望始终能够到达目的地，哪些部分流量你认为不那么重要。
- en: 'To push your Web traffic ahead of everything else and bump up the priority
    for network time and name services, you could amend your configuration with rules
    like these:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将Web流量推到所有流量之前，并提升网络时间和名称服务的优先级，你可以在配置中添加类似这样的规则：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Or if you have a rule set that already includes rules that match criteria other
    than just the port, you could achieve much the same effect by writing your priority
    traffic shaping as `match` rules instead:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果你已经有一个包含符合除端口以外其他标准的规则集，你可以通过将优先级流量整形写成`match`规则来实现类似的效果：
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In some networks, time-sensitive traffic, like Voice over Internet Protocol
    (VoIP), may need special treatment. For VoIP, a priority setup like this may improve
    phone conversation quality:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些网络中，时间敏感型流量，如语音网络协议（VoIP），可能需要特殊处理。对于VoIP，像这样的优先级设置可能改善电话通话质量：
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'But do check your VoIP application’s documentation for information on what
    specific ports it uses. In any case, using `match` rules like these can have a
    positive effect on your configuration in other ways, too: You can use `match`
    rules like the ones in the examples here to separate filtering decisions—such
    as passing, blocking, or redirecting—from traffic-shaping decisions, and with
    that separation in place, you’re likely to end up with a more readable and maintainable
    configuration.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 但请检查你的VoIP应用程序的文档，了解它使用的具体端口。无论如何，像这样的`match`规则不仅能对你的配置产生积极影响：你可以像这里的例子那样使用`match`规则，将过滤决策——如通过、阻塞或重定向——与流量整形决策分开，并通过这种分离，你很可能会得到一个更易读和易维护的配置。
- en: It’s also worth noting that parts of the OpenBSD network stack set default priorities
    for certain types of traffic that the developers decided was essential to a functional
    network. If you don’t set any priorities, anything with `proto carp` and a few
    other management protocols and packet types will go by priority 6, and all types
    of traffic that don’t receive a specific classification with a `set prio` rule
    will have a default priority of 3.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 同样值得注意的是，OpenBSD网络栈的某些部分为开发者认为对网络功能至关重要的特定类型流量设置了默认优先级。如果你没有设置任何优先级，那么任何带有`proto
    carp`和其他一些管理协议及数据包类型的流量将按照优先级6处理，而所有没有通过`set prio`规则获得具体分类的流量，将有一个默认优先级3。
- en: The Two-Priority Speedup Trick
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 双优先级加速技巧
- en: In the examples just shown, we set different priorities for different types
    of traffic and managed to get specific types of traffic, such as VoIP and SSH,
    to move faster than others. But thanks to the design of TCP, which carries the
    bulk of your traffic, even a simple priority-shaping scheme has more to offer
    with only minor tweaks to the rule set.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在刚才展示的例子中，我们为不同类型的流量设置了不同的优先级，并成功地使特定类型的流量（如VoIP和SSH）比其他流量更快地传输。但得益于TCP的设计，尽管它承载了大部分流量，即使是一个简单的优先级整形方案，通过对规则集进行少量调整，也能提供更多的优化空间。
- en: As readers of RFCs and a few practitioners have discovered, the connection-oriented
    design of TCP means that for each packet sent, the sender will expect to receive
    an acknowledgment (ACK) packet back within a preset time or matching a defined
    “window” of sequence numbers. If the sender doesn’t receive the acknowledgment
    within the expected limit, she assumes the packet was lost in transit and arranges
    to resend the data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 正如RFC的读者和一些实践者所发现的那样，TCP的面向连接设计意味着，对于每个发送的数据包，发送方都期望在预设的时间内或在定义的“窗口”序列号范围内收到一个确认（ACK）数据包。如果发送方在预期的限制时间内没有收到确认，它会假设数据包在传输过程中丢失，并安排重新发送数据。
- en: One other important factor to consider is that by default, packets are handled
    in the order they arrive. This is known as *first in, first out (FIFO)*, and it
    means that the essentially dataless ACK packets will be waiting their turn in
    between the larger data packets. On a busy or congested link, which is exactly
    where traffic shaping becomes interesting, waiting for ACKs and performing retransmissions
    can eat measurably into effective bandwidth and slow down all transfers. In fact,
    concurrent transfers in both directions can slow each other significantly more
    than the value of their expected data sizes.^([[39](#ftn.ch07fn01)])
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要考虑的重要因素是，默认情况下，数据包按到达的顺序处理。这被称为*先进先出（FIFO）*，这意味着基本没有数据的ACK数据包会在较大的数据包之间等待它们的顺序。在繁忙或拥塞的链路上，这正是流量整形变得有趣的地方，等待ACK并执行重传会明显消耗有效带宽，减慢所有传输的速度。事实上，双向并发传输可能会相互显著减慢，超过它们预计数据大小的值。^([[39](#ftn.ch07fn01)])
- en: 'Fortunately, a simple and quite popular solution to this problem is at hand:
    You can use priorities to make sure those smaller packets skip ahead. If you assign
    two priorities in a `match` or `pass` rule, like this:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，这个问题有一个简单且相当流行的解决方案：你可以使用优先级确保那些较小的数据包提前通过。如果你在`match`或`pass`规则中分配两个优先级，例如这样：
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The first priority will be assigned to the regular traffic, while ACK packets
    and other packets with a low delay type of service (ToS) will be assigned the
    second priority and will be served faster than the regular packets.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: When a packet arrives, PF detects the ACK packets and puts them on the higher-priority
    queue. PF also inspects the ToS field on arriving packets. Packets that have the
    ToS set to low delay to indicate that the sender wants speedier delivery also
    get the high-priority treatment. When more than one priority is indicated, as
    in the preceding rule, PF assigns priority accordingly. Packets with other ToS
    values are processed in the order they arrive, but with ACK packets arriving faster,
    the sender spends less time waiting for ACKs and resending presumably lost data.
    The net result is that the available bandwidth is used more efficiently. (The
    `match` rule quoted here is the first one I wrote in order to get a feel for the
    new `prio` feature—on a test system, of course—soon after it was committed during
    the OpenBSD 5.0 development cycle. If you put that single `match` rule on top
    of an existing rule set, you’ll probably see that the link can take more traffic
    and more simultaneous connections before noticeable symptoms of congestion turn
    up.)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: See whether you can come up with a way to measure throughout before and after
    you introduce the two-priorities trick to your traffic shaping, and note the difference
    before you proceed to the more complex traffic-shaping options.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Queues for Bandwidth Allocation
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ve seen that traffic shaping using only priorities can be quite effective,
    but there will be times when a priorities-only scheme will fall short of your
    goals. One such scenario occurs when you’re faced with requirements that would
    be most usefully solved by assigning a higher priority, and perhaps a larger bandwidth
    share, to some kinds of traffic, such as email and other high-value services,
    and correspondingly less bandwidth to others. Another such scenario would be when
    you simply want to apportion your available bandwidth in different-sized chunks
    to specific services and perhaps set hard upper limits for some types of traffic,
    while at the same time wanting to ensure that all traffic that you care about
    gets at least its fair share of available bandwidth. In cases like these, you
    leave the pure-priority scheme behind, at least as the primary tool, and start
    doing actual traffic shaping using *queues*.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Unlike with the priority levels, which are always available and can be used
    without further preparations, in any rule, queues represent specific parts of
    your available bandwidth and can be used only after you’ve defined them in terms
    of available capacity. Queues are a kind of buffer for network packets. Queues
    are defined with a specific amount of bandwidth, or as a specific portion of available
    bandwidth, and you can allocate portions of each queue’s bandwidth share to subqueues,
    or queues within queues, which share the parent queue’s resources. The packets
    are held in a queue until they’re either dropped or sent according to the queue’s
    criteria and subject to the queue’s available bandwidth. Queues are attached to
    specific interfaces, and bandwidth is managed on a per-interface basis, with available
    bandwidth on a given interface subdivided into the queues you define.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 与优先级级别不同，优先级级别是始终可用的，可以在任何规则中使用而无需进一步准备，而队列代表你可用带宽的特定部分，只有在你根据可用容量定义它们之后，才可以在规则中使用。队列是网络数据包的缓冲区。队列被定义为具有特定带宽量，或作为可用带宽的特定部分，你可以将每个队列带宽份额的一部分分配给子队列，或将队列嵌套在其他队列中，共享父队列的资源。数据包会被保存在队列中，直到它们根据队列的标准被丢弃或发送，并受到队列可用带宽的限制。队列附加到特定接口，带宽是按接口管理的，每个接口的可用带宽会细分到你定义的队列中。
- en: 'The basic syntax for defining a queue follows this pattern:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 定义队列的基本语法遵循以下模式：
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The letters following the bandwidth number denote the unit of measurement:
    *`K`* denotes kilobits; *`M`* megabits; and *`G`* gigabits. When you write only
    the bandwidth number, it’s interpreted as the number of bits per second. It’s
    possible to tack on other options to this basic syntax, as we’ll see in later
    examples.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 带宽数字后面的字母表示测量单位：*`K`*表示千比特；*`M`*表示兆比特；*`G`*表示千兆比特。当你只写带宽数字时，它会被解释为每秒比特数。你还可以在这个基本语法上添加其他选项，正如我们在后面的示例中将看到的。
- en: Note
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '*Subqueue definitions name their parent queue, and one queue needs to be the
    default queue that receives any traffic not specifically assigned to other queues.*'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*子队列定义会命名它们的父队列，并且必须有一个队列作为默认队列，用于接收任何未特别分配到其他队列的流量。*'
- en: Once queue definitions are in place, you integrate traffic shaping into your
    rule set by rewriting your `pass` or `match` rules to assign traffic to a specific
    queue.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦队列定义完成，你可以通过重写`pass`或`match`规则，将流量分配到特定的队列中，从而将流量整形集成到规则集中。
- en: What’s your Total Usable Bandwidth?
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 你的总可用带宽是多少？
- en: Once we start working with defined parts of total bandwidth rather than priorities
    that somehow share the whole, determining the exact value of your total usable
    bandwidth becomes interesting. It can be difficult to determine actual usable
    bandwidth on a specific interface for queuing. If you don’t specify a total bandwidth,
    the total bandwidth available will be used to calculate the allocations, but some
    types of interfaces cannot reliably report the actual bandwidth value. One common
    example of this discrepancy is where your gateway’s external interface is a 100
    megabit (Mb) Ethernet interface, attached to a DSL line that offers only 8Mb download
    and 1Mb upload.^([[40](#ftn.ch07fn01a)]) The Ethernet interface will then confidently
    report 100Mb bandwidth, not the actual value of the Internet-facing connection.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们开始处理总带宽的定义部分，而不是依靠某种方式共享整个带宽的优先级，确定你总可用带宽的准确值就变得非常有趣。确定特定接口的实际可用带宽用于队列可能比较困难。如果你没有指定总带宽，那么将使用可用的总带宽来计算分配值，但某些类型的接口无法可靠地报告实际带宽值。一个常见的例子是，如果你的网关的外部接口是一个100兆比特（Mb）以太网接口，并连接到一条仅提供8Mb下载和1Mb上传速度的DSL线路。^([[40](#ftn.ch07fn01a)])
    这个以太网接口将自信地报告100Mb带宽，而不是面向互联网连接的实际值。
- en: For this reason, it usually makes sense to set the total bandwidth to a fixed
    value. Unfortunately, the value to use may not be exactly what your bandwidth
    supplier tells you is available because there will always be some overhead due
    to various technologies and implementations. For example, in typical TCP/IP over
    wired Ethernet, overhead can be as low as single-digit percentages, but TCP/IP
    over ATM has been known to have overhead of almost 20 percent. If your bandwidth
    supplier doesn’t provide the overhead information, you’ll need to make an educated
    guess at the starting value. In any case, remember that the total bandwidth available
    is never greater than the bandwidth of the weakest link in your network path.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Queues are supported only for outbound connections relative to the system doing
    the queuing. When planning your bandwidth management, consider the actual usable
    bandwidth to be equal to the weakest (lowest bandwidth) link in the connection’s
    path, even if your queues are set up on a different interface.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: The HFSC Algorithm
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Underlying any queue system you define using the queue system in OpenBSD 5.5
    and later is the *Hierarchical Fair Service Curve (HFSC)* algorithm. HFSC was
    designed to allocate resources fairly among queues in a hierarchy. One of its
    interesting features is that it imposes no limits until some part of the traffic
    reaches a volume that’s close to its preset limits. The algorithm starts shaping
    just before the traffic reaches a point where it deprives some other queue of
    its guaranteed minimum share.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*All sample configurations we present in this book assign traffic to queues
    in the outgoing direction because you can realistically control only traffic generated
    locally and, once limits are reached, any traffic-shaping system will eventually
    resort to dropping packets in order to make the endpoint back off. As we saw in
    the earlier examples, all well-behaved TCP stacks will respond to lost ACKs with
    slower packet rates.*'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know at least the basics of the theory behind the OpenBSD queue
    system, let’s see how queues work.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Splitting Your Bandwidth into Fixed-Size Chunks
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You’ll often find that certain traffic should receive a higher priority than
    other traffic. For example, you’ll often want important traffic, such as mail
    and other vital services, to have a baseline amount of bandwidth available at
    all times, while other services, such as peer-to-peer file sharing, shouldn’t
    be allowed to consume more than a certain amount. To address these kinds of issues,
    queues offer a wider range of options than the pure-priority scheme.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: The first queue example builds on the rule sets from earlier chapters. The scenario
    is that we have a small local network, and we want to let the users on the local
    network connect to a predefined set of services outside their own network while
    also letting users from outside the local network access a Web server and an FTP
    server somewhere on the local network.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Queue Definition
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the following example, all queues are set up with the root queue, called
    `main`, on the external, Internet-facing interface. This approach makes sense
    mainly because bandwidth is more likely to be limited on the external link than
    on the local network. In principle, however, allocating queues and running traffic
    shaping can be done on any network interface.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，所有队列都设置在根队列上，根队列叫做`main`，并且位于面向外部、连接互联网的接口上。这个方法主要是因为外部链路的带宽比本地网络更容易受到限制。然而，原则上，分配队列并进行流量整形可以在任何网络接口上进行。
- en: This setup includes a queue for a total bandwidth of 20Mb with six subqueues.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这个设置包括一个总带宽为20Mb的队列，并且有六个子队列。
- en: '[PRE6]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The subqueue `defq`, shown in the preceding example, has a bandwidth allocation
    of 3600K, or 18 percent of the bandwidth, and is designated as the default queue.
    This means any traffic that matches a `pass` rule but that isn’t explicitly assigned
    to some other queue ends up here.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 前面示例中显示的子队列`defq`具有3600K的带宽分配，即占总带宽的18%，并被指定为默认队列。这意味着任何匹配`pass`规则但没有明确分配到其他队列的流量都会进入此队列。
- en: The other queues follow more or less the same pattern, up to subqueue `ssh`,
    which itself has two subqueues (the two indented lines below it). Here, we see
    a variation on the trick of using two separate priorities to speed up ACK packets,
    and as we’ll see shortly, the rule that assigns traffic to the two SSH subqueues
    assigns different priorities. Bulk SSH transfers, typically SCP file transfers,
    are transmitted with a ToS indicating throughput, while interactive SSH traffic
    has the ToS flag set to low delay and skips ahead of the bulk transfers. The interactive
    traffic is likely to be less bandwidth consuming and gets a smaller share of the
    bandwidth, but it receives preferential treatment because of the higher-priority
    value assigned to it. This scheme also helps the speed of SCP file transfers because
    the ACK packets for the SCP transfers will be assigned a higher priority.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其他队列大致遵循相同的模式，直到子队列`ssh`，它本身有两个子队列（下方的两个缩进行）。在这里，我们看到了使用两个不同优先级加速ACK包的技巧的变化，正如我们稍后会看到的，分配流量到这两个SSH子队列的规则分配了不同的优先级。大容量SSH传输，通常是SCP文件传输，使用指示吞吐量的ToS，而交互式SSH流量则设置为低延迟的ToS标志，并且会优先于大容量传输。交互式流量可能消耗的带宽较少，因此获得较小的带宽份额，但由于它被分配了更高的优先级，因此得到优先处理。这个方案还帮助了SCP文件传输的速度，因为SCP传输的ACK包将被分配更高的优先级。
- en: Finally, we have the `icmp` queue, which is reserved for the remaining 400K,
    or 2 percent, of the bandwidth from the top level. This guarantees a minimum amount
    of bandwidth for ICMP traffic that we want to pass but that doesn’t match the
    criteria that would have it assigned to the other queues.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有`icmp`队列，它为剩余的400K带宽（即2%的带宽）保留。这确保了我们希望传递的ICMP流量能够获得最小的带宽保证，即使它不符合分配到其他队列的标准。
- en: Rule Set
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 规则集
- en: To tie the queues into the rule set, we use the `pass` rules to indicate which
    traffic is assigned to the queues and their criteria.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将队列与规则集绑定，我们使用`pass`规则来指示哪些流量被分配到哪些队列及其相应标准。
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The rules for `ssh`, `ftp`, `www`, `udp`, and `icmp` assign traffic to their
    respective queues, and we note again that the `ssh` queue’s subqueues are assigned
    traffic with two different priorities. The last catchall rule ➊ passes all other
    outgoing traffic from the local network, lumping it into the default `defq` queue.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`ssh`、`ftp`、`www`、`udp`和`icmp`的规则将流量分配到它们各自的队列，并且我们再次注意到，`ssh`队列的子队列分配了两种不同的优先级的流量。最后的兜底规则➊将所有其他来自本地网络的外发流量通过，并将其放入默认的`defq`队列。'
- en: You can always let a block of `match` rules do the queue assignment instead
    in order to make the configuration even more flexible. With match rules in place,
    you move the filtering decisions to block, pass, or even redirect to a set of
    rules elsewhere.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以让一组`match`规则来执行队列分配，从而使配置更加灵活。有了匹配规则，你可以将过滤决策移到其他地方的规则集来进行阻塞、通过，甚至重定向。
- en: '[PRE8]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note that with `match` rules performing the queue assignment, there’s no need
    for a final catchall to put the traffic that doesn’t match the other rules into
    the default queue. Any traffic that doesn’t match these rules and that’s allowed
    to pass will end up in the default queue.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，使用`match`规则进行队列分配时，无需一个最终的兜底规则将不匹配其他规则的流量放入默认队列。任何不匹配这些规则并且允许通过的流量都会进入默认队列。
- en: Upper and Lower Bounds with Bursts
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 上限和下限与突发流量
- en: Fixed bandwidth allocations are nice, but network admins with traffic-shaping
    ambitions tend to look for a little more flexibility once they’ve gotten their
    feet wet. Wouldn’t it be nice if there were a regime with flexible bandwidth allocation,
    offering guaranteed lower and upper bounds for bandwidth available to each queue
    and variable allocations over time—and one that starts shaping only when there’s
    an actual need to do so?
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 固定带宽分配是不错的选择，但对于那些有流量整形需求的网络管理员来说，在初步尝试后，他们往往希望有更多的灵活性。假如有一个带宽分配机制，既能提供每个队列的带宽下限和上限保证，又能随时间变化进行灵活分配，并且只有在真正需要时才开始进行流量整形，那该有多好呢？
- en: The good news is that the OpenBSD queues can do just that, courtesy of the underlying
    HFSC algorithm discussed earlier. HFSC makes it possible to set up queuing regimes
    with guaranteed minimum allocations and hard upper limits, and you can even have
    allocations that include `burst` values to let available capacity vary over time.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，OpenBSD队列正好能够做到这一点，得益于我们之前讨论过的HFSC算法。HFSC使得可以设置带有保证的最小分配和硬性上限的排队机制，甚至可以设置包含`burst`值的分配，让可用容量随时间变化。
- en: Queue Definition
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 队列定义
- en: 'Working from a typical gateway configuration like the ones we’ve altered incrementally
    over the earlier chapters, we insert this queue definition early in the *pf.conf*
    file:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们在前几章中逐步修改的典型网关配置出发，我们在*pf.conf*文件的开头插入这个队列定义：
- en: '[PRE9]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This definition has some characteristics that are markedly different from the
    previous one in [Introducing Queues for Bandwidth Allocation](ch07.html#introducing_queues_for_bandwidth_allocat
    "Introducing Queues for Bandwidth Allocation"). We start with this rather small
    hierarchy by splitting the top-level queue, `rootq`, into two. Next, we subdivide
    the `main` queue into several subqueues, all of which have a `min` value set—the
    guaranteed minimum bandwidth allocated to the queue. (The `max` value would set
    a hard upper limit on the queue’s allocation.) The `bandwidth` parameter also
    sets the allocation the queue will have available when it’s backlogged—that is,
    when it’s started to eat into its `qlimit`, or *queue limit*, allocation.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这个定义与[引入带宽分配队列](ch07.html#introducing_queues_for_bandwidth_allocat "引入带宽分配队列")中的定义有一些显著不同。我们从这个相对较小的层次结构开始，将顶层队列`rootq`分成两个。接下来，我们将`main`队列细分成几个子队列，所有子队列都有一个设置了的`min`值——即分配给队列的最小带宽保证。（`max`值会设置队列分配的硬上限。）`bandwidth`参数还设置了当队列出现积压时可用的带宽分配——即当队列开始消耗其`qlimit`（*队列限制*）分配时。
- en: 'The queue limit parameter works like this: In case of congestion, each queue
    by default has a pool of 50 slots, the queue limit, to keep packets around when
    they can’t be transmitted immediately. Here, the top-level queues, `main` and
    `spamd`, both have larger-than-default pools set by their `qlimit` setting: `100`
    for `main` and `300` for `spamd`. Cranking up these `qlimit` sizes means we’re
    a little less likely to drop packets when the traffic approaches the set limits,
    but it also means that when the traffic shaping kicks in, we’ll see increased
    latency for connections that end up in these larger pools.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 队列限制参数的工作原理如下：在发生拥塞时，每个队列默认有一个50个槽位的池（即队列限制），用来保存那些无法立即传输的数据包。在这里，顶层队列`main`和`spamd`都通过它们的`qlimit`设置，分别设定了大于默认值的池：`main`为100，`spamd`为300。增大这些`qlimit`值意味着我们在流量接近设置的限制时丢包的可能性较小，但也意味着当流量整形启动时，我们会看到进入这些较大池中的连接的延迟增加。
- en: Rule Set
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 规则集
- en: 'The next step is to tie the newly created queues into the rule set. If you
    have a filtering regime in place already, the tie-in is simple—just add a few
    `match` rules:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将新创建的队列与规则集关联起来。如果你已经有了过滤机制，那么关联就很简单——只需要添加几条`match`规则：
- en: '[PRE10]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Here, the `match` rules once again do the ACK packet speedup trick with the
    high- and low-priority queue assignment, just as we saw earlier in the pure-priority-based
    system. The only exception is when we assign traffic to our lowest-priority queue
    (with a slight modification to an existing `pass` rule), where we really don’t
    want any speedup.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`match`规则再次通过高优先级和低优先级队列分配来加速ACK包，就像我们在基于纯优先级的系统中看到的那样。唯一的例外是当我们将流量分配到最低优先级队列时（对现有的`pass`规则稍作修改），此时我们确实不希望加速。
- en: '[PRE11]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Assigning the `spamd` traffic to a minimal-sized queue with 0 priority here
    is intended to slow down the spammers on their way to our `spamd`. (See [Chapter 6](ch06.html
    "Chapter 6. Turning the Tables for Proactive Defense") for more on `spamd` and
    related matters.)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: 'With the queue assignment and priority setting in place, it should be clear
    that the queue hierarchy here uses two familiar tricks to make efficient use of
    available bandwidth. First, it uses a variation of the high- and low-priority
    mix demonstrated in the earlier pure-priority example. Second, we speed up almost
    all other traffic, especially the Web traffic, by allocating a small but guaranteed
    portion of bandwidth for name service lookups. For the `qdns` queue, we set the
    `burst` value with a time limit: After `3000` milliseconds, the allocation goes
    down to a minimum of `12K` to fit within the total `200K` quota. Short-lived `burst`
    values like this can be useful to speed connections that transfer most of their
    payload during the early phases.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 'It may not be immediately obvious from this example, but HFSC requires that
    traffic be assigned only to *leaf queues*, or queues without subqueues. That means
    it’s possible to assign traffic to `main`’s subqueues—`qpri`, `qdef`, `qweb`,
    and `qdns`—as well as `rootq`’s subqueue—`spamd`—as we just did with the `match`
    and `pass` rules, but not to `rootq` or `main` themselves. With all queue assignments
    in place, we can use `systat` queues to show the queues and their traffic:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The queues are shown indented to indicate their hierarchy, from root to leaf
    queues. The `main` queue and its subqueues—`qpri`, `qdef`, `qweb`, and `qdns`—are
    shown with their bandwidth allocations and number of bytes and packets passed.
    The `DROP_P` and `DROP_B` columns, which show the number of packets and bytes
    dropped, would appear if we had been forced to drop packets at this stage. `QLEN`
    is the number of packets waiting for processing, while the final two columns show
    live updates of packets and bytes per second.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: 'For a more detailed view, use `pfctl -vvsq` to show the queues and their traffic:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This view shows that the queues receive traffic roughly as expected with the
    site’s typical workload. Notice that only a few moments after the rule set has
    been reloaded, the `spamd` queue is already backed up more than halfway to its
    `qlimit` setting, which seems to indicate that the queues are reasonably dimensioned
    to actual traffic.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Pay attention to each queue’s dropped packets (`dropped pkts:`) counter. If
    the number of packets dropped is high or increasing, then that could mean that
    one of the bandwidth allocation parameters needs adjusting or that some other
    network problem needs to be investigated.*'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: The DMZ Network, Now with Traffic Shaping
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In [Chapter 5](ch05.html "Chapter 5. Bigger or Trickier Networks"), we set up
    a network with a single gateway and all externally visible services configured
    on a separate DMZ (demilitarized zone) network so that all traffic to the servers
    from both the Internet and the internal network had to pass through the gateway.
    That network schematic, illustrated in [Chapter 5](ch05.html "Chapter 5. Bigger
    or Trickier Networks"), is shown again in [Figure 7-1](ch07.html#network_with_dmz
    "Figure 7-1. Network with DMZ"). Using the rule set from [Chapter 5](ch05.html
    "Chapter 5. Bigger or Trickier Networks") as the starting point, we’ll add some
    queuing in order to optimize our network resources. The physical and logical layout
    of the network will not change.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '![Network with DMZ](httpatomoreillycomsourcenostarchimages2127159.png.jpg)Figure 7-1. Network
    with DMZ'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: The most likely bottleneck for this network is the bandwidth for the connection
    between the gateway’s external interface and the Internet. Although the bandwidth
    elsewhere in our setup isn’t infinite, of course, the available bandwidth on any
    interface in the local network is likely to be less limiting than the bandwidth
    actually available for communication with the outside world. In order to make
    services available with the best possible performance, we need to set up the queues
    so that the bandwidth available at the site is made available to the traffic we
    want to allow. The interface bandwidth on the DMZ interface is likely either 100Mb
    or 1Gb, while the *actual available bandwidth* for connections from outside the
    local network is considerably smaller. This consideration shows up in our queue
    definitions, where the actual bandwidth available for external traffic is the
    main limitation in the queue setup.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Notice that for each interface, there’s a root queue with a bandwidth limitation
    that determines the allocation for all queues attached to that interface. In order
    to use the new queuing infrastructure, we need to make some changes to the filtering
    rules, too.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Because any traffic not explicitly assigned to a specific queue is assigned
    to the default queue for the interface, be sure to tune your filtering rules as
    well as your queue definitions to the actual traffic in your network.*'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 'The main part of the filtering rules could end up looking like this after adding
    the queues:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Notice that only traffic that will pass either the DMZ or the external interface
    is assigned to queues. In this configuration, with no externally accessible services
    on the internal network, queuing on the internal interface wouldn’t make much
    sense because that’s likely the part of the network with the least restricted
    available bandwidth. Also, as in earlier examples, there’s a case to be made for
    separating the queue assignments from the filtering part of the rule set by making
    a block of `match` rules responsible for queue assignment.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Using Queues to Handle Unwanted Traffic
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, we’ve focused on queuing as a way to make sure specific kinds of traffic
    are let through as efficiently as possible. Now, we’ll look at two examples that
    present a slightly different way to identify and handle unwanted traffic using
    various queuing-related tricks to keep miscreants in line.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Overloading to a Tiny Queue
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In [Turning Away the Brutes](ch06.html#turning_away_the_brutes "Turning Away
    the Brutes"), we used a combination of state-tracking options and `overload` rules
    to fill a table of addresses for special treatment. The special treatment we demonstrated
    in [Chapter 6](ch06.html "Chapter 6. Turning the Tables for Proactive Defense")
    was to cut all connections, but it’s equally possible to assign `overload` traffic
    to a specific queue instead. For example, consider the rule from our first queue
    example, shown here.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'To create a variation of the overload table trick from [Chapter 6](ch06.html
    "Chapter 6. Turning the Tables for Proactive Defense"), add state-tracking options,
    like this:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Then, make one of the queues slightly smaller:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'And assign traffic from miscreants to the small-bandwidth queue with this rule:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: As a result, the traffic from the bruteforcers would pass, but with a hard upper
    limit of 512 bits per second. (It’s worth noting that tiny bandwidth allocations
    may be hard to enforce on high-speed links due to the network stack’s timer resolution.
    If the allocation is small enough relative to the capacity of the link, packets
    that exceed the stated per-second maximum allocation may be transferred anyway,
    before the bandwidth limit kicks in.) It might also be useful to supplement rules
    like these with table-entry expiry, as described in [Tidying Your Tables with
    pfctl](ch06.html#tidying_your_tables_with_pfctl "Tidying Your Tables with pfctl").
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Queue Assignments Based on Operating System Fingerprint
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[Chapter 6](ch06.html "Chapter 6. Turning the Tables for Proactive Defense")
    covered several ways to use `spamd` to cut down on spam. If running `spamd` isn’t
    an option in your environment, you can use a queue and rule set based on the knowledge
    that machines that send spam are likely to run a particular operating system.
    (Let’s call that operating system Windows.)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: PF has a fairly reliable operating system fingerprinting mechanism, which detects
    the operating system at the other end of a network connection based on characteristics
    of the initial SYN packets at connection setup. The following may be a simple
    substitute for `spamd` if you’ve determined that legitimate mail is highly unlikely
    to be delivered from systems that run that particular operating system.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Here, email traffic originating from hosts that run a particular operating system
    get no more than 512 bits per second of your bandwidth.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Transitioning from ALTQ to Priorities and Queues
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you already have configurations that use ALTQ for traffic shaping and you’re
    planning a switch to OpenBSD 5.5 or newer, this section contains some pointers
    for how to manage the transition. The main points are these:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '***The rules after transition are likely simpler.*** The OpenBSD 5.5 and newer
    traffic-shaping system has done away with the somewhat arcane ALTQ syntax with
    its selection of queuing algorithms, and it distinguishes clearly between queues
    and pure-priority shuffling. In most cases, your configuration becomes significantly
    more readable and maintainable after a conversion to the new traffic-shaping system.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '***For simple configurations, set prio is enough.*** The simplest queue discipline
    in ALTQ was `priq`, or priority queues. The most common simple use case was the
    two-priority speedup trick first illustrated by Daniel Hartmeier in the previously
    cited article. The basic two-priority configuration looks like this:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In OpenBSD 5.5 and newer, the equivalent effect can be achieved with no queue
    definitions. Instead, you assign two priorities in a `match` or `pass` rule, like
    this:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Here, the first priority will be assigned to regular traffic, while ACK and
    other packets with a low-delay ToS will be assigned the second priority and will
    be served faster than the regular packets. The effect is the same as in the ALTQ
    example we just quoted, with the exception of defined bandwidth limits and the
    somewhat dubious effect of traffic shaping on incoming traffic.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '***Priority queues can for the most part be replaced by set prio constructs.***
    For pure-priority differentiation, applying `set prio` on a per `pass` or `match`
    rule basis is simpler than defining queues and assigning traffic and affects only
    the packet priority. ALTQ allowed you to define CBQ or HFSC queues that also had
    a priority value as part of their definition. Under the new queuing system, assigning
    priority happens only in `match` or `pass` rules, but if your application calls
    for setting both priority and queue assignment in the same rule, the new syntax
    allows for that, too:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The effect is similar to the previous behavior shown in [Splitting Your Bandwidth
    into Fixed-Size Chunks](ch07.html#splitting_your_bandwidth_into_fixed-size "Splitting
    Your Bandwidth into Fixed-Size Chunks"), and this variant may be particularly
    helpful during transition.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '***Priorities are now always important. Keep in mind that the default is 3.***
    It’s important to be aware that traffic priorities are always enabled since OpenBSD
    5.0, and they need to be taken into consideration even when you’re not actively
    assigning priorities. In old-style configurations that employed the two-priority
    trick to speed up ACKs and by extension all traffic, the only thing that was important
    was that there were two different priorities in play. The low-delay packets would
    be assigned to the higher-priority queue, and the net effect would be that traffic
    would likely pass faster, with more efficient bandwidth use than with the default
    FIFO queue. Now the default priority is 3, and setting the priority for a queue
    to 0, as a few older examples do, will mean that the traffic assigned that priority
    will be considered ready to pass only when there’s no higher-priority traffic
    left to handle.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '***For actual bandwidth shaping, HFSC works behind the scenes.*** Once you’ve
    determined that your specification calls for slicing available bandwidth into
    chunks, the underlying algorithm is always HFSC. The variety of syntaxes for different
    types of queues is gone. HFSC was chosen for its flexibility as well as the fact
    that it starts actively shaping traffic only once the traffic approaches one of
    the limits set by your queuing configuration. In addition, it’s possible to create
    CBQ-like configurations by limiting the queue definitions to only bandwidth declarations.
    [Splitting Your Bandwidth into Fixed-Size Chunks](ch07.html#splitting_your_bandwidth_into_fixed-size
    "Splitting Your Bandwidth into Fixed-Size Chunks") (mentioned earlier) demonstrates
    a static configuration that implements CBQ as a subset of HFSC.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '***You can transition from ALTQ via the oldqueue mechanism.*** OpenBSD 5.5
    supports legacy ALTQ configurations with only one minor change to configurations:
    The `queue` keyword was needed as a reserved word for the new queuing system,
    so ALTQ queues need to be declared as `oldqueue` instead. Following that one change
    (a pure search and replace operation that you can even perform just before starting
    your operating system upgrade), the configuration will work as expected.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '***If your setup is sufficiently complicated, go back to specifications and
    reimplement.*** The examples in this chapter are somewhat stylized and rather
    simple. If you have running configurations that have been built up incrementally
    over several years and have reached a complexity level, orders of magnitude larger
    than those described here, the new syntax may present an opportunity to define
    what your setup is for and produce a specification that is fit to reimplement
    in a cleaner and more maintainable configuration.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Going the `oldqueue` route and tweaking from there will work to some degree,
    but it may be easier to make the transition via a clean reimplementation from
    revised specification in a test environment where you can test whether your accumulated
    assumptions hold up in a the context of the new traffic-shaping system. Whatever
    route you choose for your transition, you’re more or less certain to end up with
    a more readable and maintainable configuration after your switch to OpenBSD 5.5
    or newer.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Directing Traffic with ALTQ
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*ALTQ* is the very flexible legacy mechanism for network traffic shaping, which
    was integrated into PF on OpenBSD^([[41](#ftn.ch07fn02)]) in time for the OpenBSD
    3.3 release by Henning Brauer, who’s also the main developer of the priorities
    and queues system introduced in OpenBSD 5.5 (described in the previous sections
    of this chapter). OpenBSD 3.3 onward moved all ALTQ configuration into *pf.conf*
    to ease the integration of traffic shaping and filtering. PF ports to other BSDs
    were quick to adopt at least some optional ALTQ integration.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*OpenBSD 5.5 introduced a new queue system for traffic shaping with a radically
    different (and more readable) syntax that complements the always-on priority system
    introduced in OpenBSD 5.0\. The new system is intended to replace ALTQ entirely
    after one transitional release. The rest of this chapter is useful only if you’re
    interested in learning about how to set up or maintain an ALTQ-based system.*'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Basic ALTQ Concepts
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As the name suggests, ALTQ configurations are totally queue-centric. As in the
    more recent traffic-shaping system, ALTQ queues are defined in terms of bandwidth
    and attached to interfaces. Queues can be assigned priority, and in some contexts,
    they can have subqueues that receive a share of the parent queue’s bandwidth.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'The general syntax for ALTQ queues looks like this:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Note
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*On OpenBSD 5.5 and newer, ALTQ queues are denoted `oldqueue` instead of `queue`
    due to an irresolvable syntax conflict with the new queuing subsystem.*'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Once queue definitions are in place, you integrate traffic shaping into your
    rule set by rewriting your `pass` or `match` rules to assign traffic to a specific
    queue. Any traffic that you don’t explicitly assign to a specific queue gets lumped
    in with everything else in the default queue.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Queue Schedulers, aka Queue Disciplines
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the default networking setup, with no queuing, the TCP/IP stack and its filtering
    subsystem process the packets according to the FIFO discipline.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: ALTQ offers three queue-scheduler algorithms, or *disciplines*, that can alter
    this behavior slightly. The types are `priq`, `cbq`, and `hfsc`. Of these, `cbq`
    and `hfsc` queues can have several levels of subqueues. The `priq` queues are
    essentially flat, with only one queue level. Each of the disciplines has its own
    syntax specifics, and we’ll address those in the following sections.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: priq
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Priority-based queues* are defined purely in terms of priority within the
    total declared bandwidth. For `priq` queues, the allowed priority range is 0 through
    15, where a higher value earns preferential treatment. Packets that match the
    criteria for higher-priority queues are serviced before the ones matching lower-priority
    queues.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: cbq
  id: totrans-155
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Class-based queues* are defined as constant-sized bandwidth allocations, as
    a percentage of the total available or in units of kilobits, megabits, or gigabits
    per second. A `cbq` queue can be subdivided into queues that are also assigned
    priorities in the range 0 to 7, and again, a higher priority means preferential
    treatment.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: hfsc
  id: totrans-157
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `hfsc` discipline uses the HFSC algorithm to ensure a “fair” allocation
    of bandwidth among the queues in a hierarchy. HFSC comes with the possibility
    of setting up queuing regimes with guaranteed minimum allocations and hard upper
    limits. Allocations can even vary over time, and you can even have fine-grained
    priority with a 0 to 7 range.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Because both the algorithm and the corresponding setup with ALTQ are fairly
    complicated, with a number of tunable parameters, most ALTQ practitioners tend
    to stick with the simpler queue types. Yet the ones who claim to understand HFSC
    swear by it.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Setting Up ALTQ
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Enabling ALTQ may require some extra steps, depending on your choice of operating
    system.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: ALTQ on OpenBSD
  id: totrans-162
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: On OpenBSD 5.5, all supported queue disciplines are compiled into the GENERIC
    and GENERIC.MP kernels. Check that your OpenBSD version still supports ALTQ. If
    so, the only configuration you need to do involves editing your *pf.conf*.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: ALTQ on FreeBSD
  id: totrans-164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'On FreeBSD, make sure that your kernel has ALTQ and the ALTQ queue discipline
    options compiled in. The default FreeBSD GENERIC kernel doesn’t have ALTQ options
    enabled, as you may have noticed from the messages you saw when running the */etc/rc.d/pf*
    script to enable PF. The relevant options are as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The `ALTQ` option is needed to enable ALTQ in the kernel, but on SMP systems,
    you also need the `ALTQ_NOPCC` option. Depending on which types of queues you’ll
    be using, you’ll need to enable at least one of these: `ALTQ_CBQ`, `ALTQ_PRIQ`,
    or `ALTQ_HFSC`. Finally, you can enable the congestion-avoidance techniques *random
    early detection (RED)* and *RED In/Out* with the `ALTQ_RED` and `ALTQ_RIO` options,
    respectively. (See the *FreeBSD Handbook* for information on how to compile and
    install a custom kernel with these options.)'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: ALTQ on NetBSD
  id: totrans-168
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'ALTQ was integrated into the NetBSD 4.0 PF implementation and is supported
    in NetBSD 4.0 and later releases. NetBSD’s default GENERIC kernel configuration
    doesn’t include the ALTQ-related options, but the GENERIC configuration file comes
    with all relevant options commented out for easy inclusion. The main kernel options
    are these:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The `ALTQ` option is needed to enable ALTQ in the kernel. Depending on the
    types of queues you’ll be using, you must enable at least one of these: `ALTQ_CBQ`,
    `ALTQ_PRIQ`, or `ALTQ_HFSC`.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Using ALTQ requires you to compile PF into the kernel because the PF loadable
    module doesn’t support ALTQ functionality. (See the NetBSD PF documentation at
    *[http://www.netbsd.org/Documentation/network/pf.html](http://www.netbsd.org/Documentation/network/pf.html)*
    for the most up-to-date information.)
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Priority-Based Queues
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The basic concept behind priority-based queues (`priq`) is fairly straightforward.
    Within the total bandwidth allocated to the main queue, only traffic priority
    matters. You assign queues a priority value in the range 0 through 15, where a
    higher value means that the queue’s requests for traffic are serviced sooner.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Using ALTQ Priority Queues to Improve Performance
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Daniel Hartmeier discovered a simple yet effective way to improve the throughput
    for his home network by using ALTQ priority queues. Like many people, he had his
    home network on an asymmetric connection, with total usable bandwidth low enough
    that he wanted better bandwidth utilization. In addition, when the line was running
    at or near capacity, oddities started appearing. One symptom in particular seemed
    to suggest room for improvement: Incoming traffic (downloads, incoming mail, and
    such) slowed down disproportionately whenever outgoing traffic started—more than
    could be explained by measuring the raw amount of data transferred. It all came
    back to a basic feature of TCP.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: When a TCP packet is sent, the sender expects acknowledgment (in the form of
    an ACK packet) from the receiver and will wait a specified time for it to arrive.
    If the ACK doesn’t arrive within that time, the sender assumes that the packet
    hasn’t been received and resends it. And because in a default setup, packets are
    serviced sequentially by the interface as they arrive, ACK packets, with essentially
    no data payload, end up waiting in line while the larger data packets are transferred.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: If ACK packets could slip in between the larger data packets, the result would
    be more efficient use of available bandwidth. The simplest practical way to implement
    such a system with ALTQ is to set up two queues with different priorities and
    integrate them into the rule set. Here are the relevant parts of the rule set.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Here, the priority-based queue is set up on the external interface with two
    subordinate queues. The first subqueue, `q_pri`, has a high-priority value of
    7; the other subqueue, `q_def`, has a significantly lower-priority value of 1.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: This seemingly simple rule set works by exploiting how ALTQ treats queues with
    different priorities. Once a connection is set up, ALTQ inspects each packet’s
    ToS field. ACK packets have the ToS delay bit set to low, which indicates that
    the sender wanted the speediest delivery possible. When ALTQ sees a low-delay
    packet and queues of differing priorities are available, it assigns the packet
    to the higher-priority queue. This means that the ACK packets skip ahead of the
    lower-priority queue and are delivered more quickly, which in turn means that
    data packets are serviced more quickly. The net result is better performance than
    a pure FIFO configuration with the same hardware and available bandwidth. (Daniel
    Hartmeier’s article about this version of his setup, cited previously, contains
    a more detailed analysis.)
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Using a match Rule for Queue Assignment
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous example, the rule set was constructed the traditional way, with
    the queue assignment as part of the `pass` rules. However, this isn’t the only
    way to do queue assignment. When you use `match` rules (available in OpenBSD 4.6
    and later), it’s incredibly easy to retrofit this simple priority-queuing regime
    onto an existing rule set.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: If you worked through the examples in [Chapter 3](ch03.html "Chapter 3. Into
    the Real World") and [Chapter 4](ch04.html "Chapter 4. Wireless Networks Made
    Easy"), your rule set probably has a `match` rule that applies `nat-to` on your
    outgoing traffic. To introduce priority-based queuing to your rule set, you first
    add the queue definitions and make some minor adjustments to your outgoing `match`
    rule.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Start with the queue definition from the preceding example and adjust the total
    bandwidth to local conditions, as shown in here.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This gives the queues whatever bandwidth allocation you define with the `ext_bw`
    macro.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest and quickest way to integrate the queues into your rule set is
    to edit your outgoing `match` rule to read something like this:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Reload your rule set, and the priority-queuing regime is applied to all traffic
    that’s initiated from your local network.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: You can use the `systat` command to get a live view of how traffic is assigned
    to your queues.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This will give you a live display that looks something like this:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Looking at the numbers in the `PKTS` (packets) and `BYTES` columns, you see
    a clear indication that the queuing is working as intended.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: The `q_pri` queue has processed a rather large number of packets in relation
    to the amount of data, just as we expected. The ACK packets don’t take up a lot
    of space. On the other hand, the traffic assigned to the `q_def` queue has more
    data in each packet, and the numbers show essentially the reverse packet numbers–to–data
    size ratio as in to the `q_pri` queue.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*`systat` is a rather capable program on all BSDs, and the OpenBSD version
    offers several views that are relevant to PF and that aren’t found in the `systat`
    variants on the other systems as of this writing. We’ll be looking at `systat`
    again in the next chapter. In the meantime, read the man pages and play with the
    program. It’s a very useful tool for getting to know your system.*'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Class-Based Bandwidth Allocation for Small Networks
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Maximizing network performance generally feels nice. However, you may find that
    your network has other needs. For example, it might be important for some traffic—such
    as mail and other vital services—to have a baseline amount of bandwidth available
    at all times, while other services—peer-to-peer file sharing comes to mind—shouldn’t
    be allowed to consume more than a certain amount. To address these kinds of requirements
    or concerns, ALTQ offers the class-based queue (`cbq`) discipline with a slightly
    larger set of options.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate how to use `cbq`, we’ll build on the rule sets from previous chapters
    within a small local network. We want to let the users on the local network connect
    to a predefined set of services outside their own network and let users from outside
    the local network access a Web server and an FTP server somewhere on the local
    network.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Queue Definition
  id: totrans-202
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: All queues are set up on the external, Internet-facing interface. This approach
    makes sense mainly because bandwidth is more likely to be limited on the external
    link than on the local network. In principle, however, allocating queues and running
    traffic shaping can be done on any network interface. The example setup shown
    here includes a `cbq` queue for a total bandwidth of 2Mb with six subqueues.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The subqueue `main` has 18 percent of the bandwidth and is designated as the
    default queue. This means any traffic that matches a `pass` rule but isn’t explicitly
    assigned to some other queue ends up here. The `borrow` and `red` keywords mean
    that the queue may “borrow” bandwidth from its parent queue, while the system
    attempts to avoid congestion by applying the RED algorithm.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: The other queues follow more or less the same pattern up to the subqueue `ssh`,
    which itself has two subqueues with separate priorities. Here, we see a variation
    on the ACK priority example. Bulk SSH transfers, typically SCP file transfers,
    are transmitted with a ToS indicating throughput, while interactive SSH traffic
    has the ToS flag set to low delay and skips ahead of the bulk transfers. The interactive
    traffic is likely to be less bandwidth consuming and gets a smaller share of the
    bandwidth, but it receives preferential treatment because of the higher-priority
    value assigned to it. This scheme also helps the speed of SCP file transfers because
    the ACK packets for the SCP transfers will be assigned to the higher-priority
    subqueue.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we have the `icmp` queue, which is reserved for the remaining 2 percent
    of the bandwidth from the top level. This guarantees a minimum amount of bandwidth
    for ICMP traffic that we want to pass but that doesn’t match the criteria for
    being assigned to the other queues.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Rule Set
  id: totrans-208
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To make it all happen, we use these `pass` rules, which indicate which traffic
    is assigned to the queues and their criteria:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The rules for `ssh`, `ftp`, `www`, `udp`, and `icmp` assign traffic to their
    respective queues. The last catchall rule passes all other traffic from the local
    network, lumping it into the default `main` queue.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: A Basic HFSC Traffic Shaper
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The simple schedulers we have looked at so far can make for efficient setups,
    but network admins with traffic-shaping ambitions tend to look for a little more
    flexibility than can be found in the pure-priority-based queues or the simple
    class-based variety. The HFSC queuing algorithm (`hfsc` in *pf.conf* terminology)
    offers flexible bandwidth allocation, guaranteed lower and upper bounds for bandwidth
    available to each queue, and variable allocations over time, and it only starts
    shaping when there’s an actual need. However, the added flexibility comes at a
    price: The setup is a tad more complex than the other ALTQ types, and tuning your
    setup for an optimal result can be quite an interesting process.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Queue Definition
  id: totrans-214
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'First, working from the same configuration we altered slightly earlier, we
    insert this queue definition early in the *pf.conf* file:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The `hfsc` queue definitions take slightly different parameters than the simpler
    disciplines. We start off with this rather small hierarchy by splitting the top-level
    queue into two. At the next level, we subdivide the `main` queue into several
    subqueues, each with a defined priority. All the subqueues have a `realtime` value
    set—the guaranteed minimum bandwidth allocated to the queue. The optional `upperlimit`
    sets a hard upper limit on the queue’s allocation. The `linkshare` parameter sets
    the allocation the queue will have available when it’s backlogged—that is, when
    it’s started to eat into its `qlimit` allocation.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'In case of congestion, each queue by default has a pool of 50 slots, the queue
    limit (`qlimit`), to keep packets around when they can’t be transmitted immediately.
    In this example, the top-level queues `main` and `spamd` both have larger-than-default
    pools set by their `qlimit` setting: `100` for `main` and `300` for `spamd`. Cranking
    up queue sizes here means we’re a little less likely to drop packets when the
    traffic approaches the set limits, but it also means that when the traffic shaping
    kicks in, we’ll see increased latency for connections that end up in these larger
    than default pools.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 'The queue hierarchy here uses two familiar tricks to make efficient use of
    available bandwidth:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: It uses a variation of the high- and low-priority mix demonstrated in the earlier
    pure-priority example.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We speed up almost all other traffic (and most certainly the Web traffic that
    appears to be the main priority here) by allocating a small but guaranteed portion
    of bandwidth for name service lookups. For the `q_dns` queue, we set up the `realtime`
    value with a time limit—after `3000` milliseconds, the `realtime` allocation goes
    down to `12Kb`. This can be useful to speed connections that transfer most of
    their payload during the early phases.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rule Set
  id: totrans-222
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Next, we tie the newly created queues into the rule set. If you have a filtering
    regime in place already, which we’ll assume you do, the tie-in becomes amazingly
    simple, accomplished by adding a few `match` rules.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Here, the `match` rules once again do the ACK packet speedup trick with the
    high- and low-priority queue assignment, just as you saw earlier in the pure-priority-based
    system. The only exception is when we assign traffic to our lowest-priority queue,
    where we really don’t care to have any speedup at all.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: This rule is intended to slow down the spammers a little more on their way to
    our `spamd`. With a hierarchical queue system in place, `systat queues` shows
    the queues and their traffic as a hierarchy, too.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The root queue is shown as attached to the physical interface—as `nfe0` and
    `root_nfe0`, in this case. `main` and its subqueues—`q_pri`, `q_def`, `q_web`,
    and `q_dns`—are shown with their bandwidth allocations and number of bytes and
    packets passed. The `DROP_P` and `DROP_B` columns are where number of packets
    and bytes dropped, respectively, would appear if we had been forced to drop packets
    at this stage. The final two columns show live updates of packets per second and
    bytes per second.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Queuing for Servers in a DMZ
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In [Chapter 5](ch05.html "Chapter 5. Bigger or Trickier Networks"), we set up
    a network with a single gateway but with all externally visible services configured
    on a separate DMZ network. That way, all traffic to the servers from both the
    Internet and the internal network had to pass through the gateway (see [Figure 7-1](ch07.html#network_with_dmz
    "Figure 7-1. Network with DMZ")).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: With the rule set from [Chapter 5](ch05.html "Chapter 5. Bigger or Trickier
    Networks") as our starting point, we’ll add some queuing in order to optimize
    our network resources. The physical and logical layout of the network will not
    change. The most likely bottleneck for this network is the bandwidth for the connection
    between the gateway’s external interface and the Internet at large. The bandwidth
    elsewhere in our setup isn’t infinite, of course, but the available bandwidth
    on any interface in the local network is likely to be less of a limiting factor
    than the bandwidth actually available for communication with the outside world.
    For services to be available with the best possible performance, we need to set
    up the queues so the bandwidth available at the site is made available to the
    traffic we want to allow.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: In our example, it’s likely that the interface bandwidth on the DMZ interface
    is either 100Mb or 1Gb, while the *actual available bandwidth* for connections
    from outside the local network is considerably smaller. This consideration shows
    up in our queue definitions, where you clearly see that the bandwidth available
    for external traffic is the main limitation in the queue setup.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Notice that the `total_ext` bandwidth limitation determines the allocation for
    all queues where the bandwidth for external connections is available. In order
    to use the new queuing infrastructure, we need to make some changes to the filtering
    rules, too. Keep in mind that any traffic you don’t explicitly assign to a specific
    queue is assigned to the default queue for the interface. Thus, it’s important
    to tune your filtering rules as well as your queue definitions to the actual traffic
    in your network.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: 'With queue assignment, the main part of the filtering rules could end up looking
    like this:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Notice that only traffic that will pass either the DMZ interface or the external
    interface is assigned to queues. In this configuration, with no externally accessible
    services on the internal network, queuing on the internal interface wouldn’t make
    much sense because it’s likely the part of our network with the least restrictions
    on available bandwidth.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Using ALTQ to Handle Unwanted Traffic
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, we’ve focused on queuing as a method to make sure specific kinds of
    traffic are let through as efficiently as possible given the conditions that exist
    in and around your network. Now, we’ll look at two examples that present a slightly
    different approach to identify and handle unwanted traffic in order to demonstrate
    some queuing-related tricks you can use to keep miscreants in line.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Overloading to a Tiny Queue
  id: totrans-241
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Think back to [Turning Away the Brutes](ch06.html#turning_away_the_brutes "Turning
    Away the Brutes"), where we used a combination of state-tracking options and `overload`
    rules to fill up a table of addresses for special treatment. The special treatment
    we demonstrated in [Chapter 6](ch06.html "Chapter 6. Turning the Tables for Proactive
    Defense") was to cut all connections, but it’s equally possible to assign `overload`
    traffic to a specific queue instead.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Consider this rule from our class-based bandwidth example in [Class-Based Bandwidth
    Allocation for Small Networks](ch07.html#class-based_bandwidth_allocation_for_sma
    "Class-Based Bandwidth Allocation for Small Networks").
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We could add state-tracking options, as shown in here.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Then, we could make one of the queues slightly smaller.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Next, we could assign traffic from miscreants to the small-bandwidth queue with
    the following rule.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: It might also be useful to supplement rules like these with table-entry expiry,
    as described in [Tidying Your Tables with pfctl](ch06.html#tidying_your_tables_with_pfctl
    "Tidying Your Tables with pfctl").
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Queue Assignments Based on Operating System Fingerprint
  id: totrans-252
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[Chapter 6](ch06.html "Chapter 6. Turning the Tables for Proactive Defense")
    covered several ways to use `spamd` to cut down on spam. If running `spamd` isn’t
    an option in your environment, you can use a queue and rule set based on the common
    knowledge that machines that send spam are likely to run a particular operating
    system.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: PF has a fairly reliable operating system fingerprinting mechanism, which detects
    the operating system at the other end of a network connection based on characteristics
    of the initial SYN packets at connection setup. The following may be a simple
    substitute for `spamd` if you’ve determined that legitimate mail is highly unlikely
    to be delivered from systems that run that particular operating system.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Here, email traffic originating from hosts that run a particular operating system
    get no more than 1KB of your bandwidth, with no borrowing.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: 'Conclusion: Traffic Shaping for Fun, and Perhaps Even Profit'
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This chapter has dealt with traffic-shaping techniques that can make your traffic
    move faster, or at least make preferred traffic pass more efficiently and according
    to your specifications. By now you should have at least a basic understanding
    of traffic-shaping concepts and how they apply to the traffic-shaping tool set
    you’ll be using on your systems.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: I hope that the somewhat stylized (but functional) examples in this chapter
    have given you a taste of what’s possible with traffic shaping and that the material
    has inspired you to play with some of your own ideas of how you can use the traffic-shaping
    tools in your networks. If you pay attention to your network traffic and the underlying
    needs it expresses (see [Chapter 9](ch09.html "Chapter 9. Logging, Monitoring,
    and Statistics") and [Chapter 10](ch10.html "Chapter 10. Getting Your Setup Just
    Right") for more on studying network traffic in detail), you can use the traffic-shaping
    tools to improve the way your network serves its users. With a bit of luck, your
    users will appreciate your efforts and you may even enjoy the experience.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: ^([[39](#ch07fn01)]) Daniel Hartmeier, one of the original PF developers, wrote
    a nice article about this problem, which is available at *[http://www.benzedrine.cx/ackpri.html](http://www.benzedrine.cx/ackpri.html)*.
    Daniel’s explanations use the older ALTQ priority queues syntax but include data
    that clearly illustrates the effect of assigning two different priorities to help
    ACKs along.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: ^([[40](#ch07fn01a)]) This really dates the book, I know. In a few years, these
    numbers will seem quaint.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: ^([[41](#ch07fn02)]) The original research on ALTQ was presented in a paper
    for the USENIX 1999 conference. You can read Kenjiro Cho’s paper “Managing Traffic
    with ALTQ” online at *[http://www.usenix.org/publications/library/proceedings/usenix99/cho.html](http://www.usenix.org/publications/library/proceedings/usenix99/cho.html)*.
    The code turned up in OpenBSD soon after through the efforts of Cho and Chris
    Cappucio.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
