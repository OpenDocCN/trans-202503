- en: Chapter 7. Traffic Shaping with Queues and Priorities
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第7章 使用队列和优先级进行流量整形
- en: '![Traffic Shaping with Queues and Priorities](httpatomoreillycomsourcenostarchimages2127149.png.jpg)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![使用队列和优先级进行流量整形](httpatomoreillycomsourcenostarchimages2127149.png.jpg)'
- en: 'In this chapter, we look at how to use traffic shaping to allocate bandwidth
    resources efficiently and according to a specified policy. If the term *traffic
    shaping* seems unfamiliar, rest assured it means what you think it means: that
    you’ll be altering the way your network allocates resources in order to satisfy
    the requirements of your users and their applications. With a proper understanding
    of your network traffic and the applications and users that generate it, you can,
    in fact, go quite a bit of distance toward “bigger, better, faster, more” just
    by optimizing your network for the traffic that’s actually supposed to pass there.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨如何使用流量整形来高效地分配带宽资源，并根据特定策略进行调整。如果“流量整形”这个术语对你来说有些陌生，请放心，它的意思正如你所理解的那样：你将修改网络分配资源的方式，以满足用户及其应用程序的需求。通过正确理解你的网络流量以及产生这些流量的应用程序和用户，你实际上可以通过优化网络来实现“更大、更好、更快、更强”的目标，尽管只针对网络中实际需要传输的流量进行优化。
- en: 'A small but powerful arsenal of traffic-shaping tools is at your disposal;
    all of them work by introducing nondefault behavior into your network setup to
    bend the realities of your network according to your wishes. Traffic shaping for
    PF contexts currently comes in two flavors: the once experimental *ALTQ* (short
    for *alternate queuing*) framework, now considered old-style after some 15 years
    of service, and the newer OpenBSD *priorities and queuing* system introduced in
    OpenBSD 5.5.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 有一套小巧而强大的流量整形工具可供你使用；所有这些工具通过在你的网络设置中引入非默认行为来调整网络现实，以符合你的需求。针对 PF 上下文的流量整形目前有两种方式：一种是曾经实验性的*ALTQ*（*alternate
    queuing*，即“替代队列”）框架，经过约 15 年的使用后，现在被视为旧式方法；另一种是 OpenBSD 5.5 引入的较新的 OpenBSD *优先级和队列*系统。
- en: In the first part of the chapter, we introduce traffic shaping by looking at
    the features of the new OpenBSD priority and queuing system. If you’re about to
    set up on OpenBSD 5.5 or newer, you can jump right in, starting with the next
    section, [Always-On Priority and Queues for Traffic Shaping](ch07.html#always-on_priority_and_queues_for_traffi
    "Always-On Priority and Queues for Traffic Shaping"). This is also where the main
    traffic-shaping concepts are introduced with examples.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的第一部分通过观察新的 OpenBSD 优先级和队列系统的特点来介绍流量整形。如果你准备在 OpenBSD 5.5 或更高版本上进行设置，可以直接进入下一部分，[始终开启的优先级和队列用于流量整形](ch07.html#always-on_priority_and_queues_for_traffi
    "始终开启的优先级和队列用于流量整形")。在这一部分，我们将介绍流量整形的主要概念，并通过示例进行说明。
- en: On OpenBSD 5.4 and earlier as well as other BSDs where the PF code wasn’t current
    with OpenBSD 5.5, traffic shaping was the domain of the ALTQ system. On OpenBSD,
    ALTQ was removed after one transitional release, leaving only the newer traffic-shaping
    system in place from OpenBSD 5.6 onward. If you’re interested in converting an
    existing ALTQ setup to the new system, you’ll most likely find [Transitioning
    from ALTQ to Priorities and Queues](ch07.html#transitioning_from_altq_to_priorities_an
    "Transitioning from ALTQ to Priorities and Queues") useful; this section highlights
    the differences between the older ALTQ system and the new system.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在 OpenBSD 5.4 及更早版本，以及其他一些 BSD 系统中，PF 代码尚未与 OpenBSD 5.5 保持同步，因此流量整形由 ALTQ 系统负责。在
    OpenBSD 中，ALTQ 在经过一次过渡发布后被移除，从 OpenBSD 5.6 开始，只有较新的流量整形系统得以保留。如果你有兴趣将现有的 ALTQ
    设置迁移到新系统，你可能会发现[从 ALTQ 到优先级和队列的过渡](ch07.html#transitioning_from_altq_to_priorities_an
    "从 ALTQ 到优先级和队列的过渡")部分很有用；这一节重点介绍了旧版 ALTQ 系统与新系统之间的差异。
- en: If you’re working with an operating system where the queues system introduced
    in OpenBSD 5.5 isn’t yet available, you’ll want to study the ALTQ traffic-shaping
    subsystem, which is described in [Directing Traffic with ALTQ](ch07.html#directing_traffic_with_altq
    "Directing Traffic with ALTQ"). If you’re learning traffic-shaping concepts and
    want to apply them to an ALTQ setup, please read the first part of this chapter
    before diving into ALTQ-specific configuration details.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用一个尚未支持 OpenBSD 5.5 引入的队列系统的操作系统，你需要研究 ALTQ 流量整形子系统，相关内容可参见[使用 ALTQ 引导流量](ch07.html#directing_traffic_with_altq
    "使用 ALTQ 引导流量")。如果你正在学习流量整形的概念并希望将其应用于 ALTQ 设置，请在深入 ALTQ 特定配置细节之前阅读本章的第一部分。
- en: Always-On Priority and Queues for Traffic Shaping
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 始终开启的优先级和队列用于流量整形
- en: Managing your bandwidth has a lot in common with balancing your checkbook or
    handling other resources that are either scarce or available in finite quantities.
    The resource is available in a constant supply with hard upper limits, and you
    need to allocate the resource with maximum *efficiency*, according to the *priorities*
    set out in your *policy* or *specification*.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 管理带宽与平衡支票簿或处理其他有限资源有很多相似之处。这些资源持续供应，且有严格的上限，你需要根据*政策*或*规格*中设定的*优先级*，以最大*效率*分配资源。
- en: OpenBSD 5.5 and newer offers several different options for managing your bandwidth
    resources via classification mechanisms in our PF rule sets. We’ll take a look
    at what you can do with pure *traffic prioritization* first and then move on to
    how to subdivide your bandwidth resources by allocating defined subsets of your
    traffic to *queues*.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: OpenBSD 5.5及更新版本通过PF规则集中的分类机制提供了几种不同的带宽资源管理选项。我们将首先看看如何仅使用*流量优先级*来管理流量，然后再讨论如何通过为你的流量分配定义的子集到*队列*来细分带宽资源。
- en: Note
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '*The always-on priorities were introduced as a teaser of sorts in OpenBSD 5.0\.
    After several years in development and testing, the new queuing system was finally
    committed in time to be included in OpenBSD 5.5, which was released on May 1,
    2014\. If you’re starting your traffic shaping from scratch on OpenBSD 5.5 or
    newer or you’re considering doing so, this section is the right place to start.
    If you’re upgrading from an earlier OpenBSD version or transitioning from another
    ALTQ system to a recent OpenBSD, you’ll most likely find the following section,
    [Transitioning from ALTQ to Priorities and Queues](ch07.html#transitioning_from_altq_to_priorities_an
    "Transitioning from ALTQ to Priorities and Queues"), useful.*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*始终启用的优先级是作为OpenBSD 5.0中的一种预告功能引入的。经过几年的开发和测试，新的队列系统最终在OpenBSD 5.5中正式提交，并于2014年5月1日发布。如果你从OpenBSD
    5.5或更新版本开始流量整形，或者打算这么做，本节是一个合适的起点。如果你从早期版本的OpenBSD升级或从其他ALTQ系统迁移到最新版本的OpenBSD，你很可能会发现接下来的部分，[从ALTQ到优先级和队列的过渡](ch07.html#transitioning_from_altq_to_priorities_an
    "从ALTQ到优先级和队列的过渡")，非常有用。*'
- en: Shaping by Setting Traffic Priorities
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过设置流量优先级进行整形
- en: 'If you’re mainly interested in pushing certain kinds of traffic ahead of others,
    you may be able to achieve what you want by simply setting priorities: assigning
    a higher priority to some items so that they receive attention before others.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你主要是希望将某些类型的流量推到其他流量之前，你可以通过简单地设置优先级来实现：为一些项目分配更高的优先级，使它们在其他流量之前获得处理。
- en: The prio Priority Scheme
  id: totrans-14
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: prio优先级方案
- en: Starting with OpenBSD 5.0, a priority scheme for classifying network traffic
    on a per-rule basis is available. The range of priorities is from 0 to 7, where
    0 is lowest priority. Items assigned priority 7 will skip ahead of everything
    else, and the default value 3 is automatically assigned for most kinds of traffic.
    The priority scheme, which you’ll most often hear referred to as `prio` after
    the PF syntax keyword, is always enabled, and you can tweak your traffic by setting
    priorities via your `match` or `pass` rules.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 从OpenBSD 5.0开始，可以基于每条规则对网络流量进行分类的优先级方案。优先级范围从0到7，其中0为最低优先级。被分配优先级7的项目会优先于其他所有流量，默认值3会自动分配给大多数类型的流量。该优先级方案通常称为`prio`，在PF语法关键字之后最常见，你可以通过`match`或`pass`规则设置优先级来调整你的流量。
- en: 'For example, to speed up your outgoing SSH traffic to the max, you could put
    a rule like this in your configuration:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，为了最大化你的出站SSH流量，你可以在配置中添加如下规则：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Then your SSH traffic would be served before anything else.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你的SSH流量将优先处理。
- en: You could then examine the rest of your rule set and decide what traffic is
    more or less important, what you would like always to reach its destination, and
    what parts of your traffic you feel matter less.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你可以检查其余的规则集，决定哪些流量更重要，哪些流量你希望始终能够到达目的地，哪些部分流量你认为不那么重要。
- en: 'To push your Web traffic ahead of everything else and bump up the priority
    for network time and name services, you could amend your configuration with rules
    like these:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将Web流量推到所有流量之前，并提升网络时间和名称服务的优先级，你可以在配置中添加类似这样的规则：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Or if you have a rule set that already includes rules that match criteria other
    than just the port, you could achieve much the same effect by writing your priority
    traffic shaping as `match` rules instead:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果你已经有一个包含符合除端口以外其他标准的规则集，你可以通过将优先级流量整形写成`match`规则来实现类似的效果：
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In some networks, time-sensitive traffic, like Voice over Internet Protocol
    (VoIP), may need special treatment. For VoIP, a priority setup like this may improve
    phone conversation quality:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些网络中，时间敏感型流量，如语音网络协议（VoIP），可能需要特殊处理。对于VoIP，像这样的优先级设置可能改善电话通话质量：
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'But do check your VoIP application’s documentation for information on what
    specific ports it uses. In any case, using `match` rules like these can have a
    positive effect on your configuration in other ways, too: You can use `match`
    rules like the ones in the examples here to separate filtering decisions—such
    as passing, blocking, or redirecting—from traffic-shaping decisions, and with
    that separation in place, you’re likely to end up with a more readable and maintainable
    configuration.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 但请检查你的VoIP应用程序的文档，了解它使用的具体端口。无论如何，像这样的`match`规则不仅能对你的配置产生积极影响：你可以像这里的例子那样使用`match`规则，将过滤决策——如通过、阻塞或重定向——与流量整形决策分开，并通过这种分离，你很可能会得到一个更易读和易维护的配置。
- en: It’s also worth noting that parts of the OpenBSD network stack set default priorities
    for certain types of traffic that the developers decided was essential to a functional
    network. If you don’t set any priorities, anything with `proto carp` and a few
    other management protocols and packet types will go by priority 6, and all types
    of traffic that don’t receive a specific classification with a `set prio` rule
    will have a default priority of 3.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 同样值得注意的是，OpenBSD网络栈的某些部分为开发者认为对网络功能至关重要的特定类型流量设置了默认优先级。如果你没有设置任何优先级，那么任何带有`proto
    carp`和其他一些管理协议及数据包类型的流量将按照优先级6处理，而所有没有通过`set prio`规则获得具体分类的流量，将有一个默认优先级3。
- en: The Two-Priority Speedup Trick
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 双优先级加速技巧
- en: In the examples just shown, we set different priorities for different types
    of traffic and managed to get specific types of traffic, such as VoIP and SSH,
    to move faster than others. But thanks to the design of TCP, which carries the
    bulk of your traffic, even a simple priority-shaping scheme has more to offer
    with only minor tweaks to the rule set.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在刚才展示的例子中，我们为不同类型的流量设置了不同的优先级，并成功地使特定类型的流量（如VoIP和SSH）比其他流量更快地传输。但得益于TCP的设计，尽管它承载了大部分流量，即使是一个简单的优先级整形方案，通过对规则集进行少量调整，也能提供更多的优化空间。
- en: As readers of RFCs and a few practitioners have discovered, the connection-oriented
    design of TCP means that for each packet sent, the sender will expect to receive
    an acknowledgment (ACK) packet back within a preset time or matching a defined
    “window” of sequence numbers. If the sender doesn’t receive the acknowledgment
    within the expected limit, she assumes the packet was lost in transit and arranges
    to resend the data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 正如RFC的读者和一些实践者所发现的那样，TCP的面向连接设计意味着，对于每个发送的数据包，发送方都期望在预设的时间内或在定义的“窗口”序列号范围内收到一个确认（ACK）数据包。如果发送方在预期的限制时间内没有收到确认，它会假设数据包在传输过程中丢失，并安排重新发送数据。
- en: One other important factor to consider is that by default, packets are handled
    in the order they arrive. This is known as *first in, first out (FIFO)*, and it
    means that the essentially dataless ACK packets will be waiting their turn in
    between the larger data packets. On a busy or congested link, which is exactly
    where traffic shaping becomes interesting, waiting for ACKs and performing retransmissions
    can eat measurably into effective bandwidth and slow down all transfers. In fact,
    concurrent transfers in both directions can slow each other significantly more
    than the value of their expected data sizes.^([[39](#ftn.ch07fn01)])
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要考虑的重要因素是，默认情况下，数据包按到达的顺序处理。这被称为*先进先出（FIFO）*，这意味着基本没有数据的ACK数据包会在较大的数据包之间等待它们的顺序。在繁忙或拥塞的链路上，这正是流量整形变得有趣的地方，等待ACK并执行重传会明显消耗有效带宽，减慢所有传输的速度。事实上，双向并发传输可能会相互显著减慢，超过它们预计数据大小的值。^([[39](#ftn.ch07fn01)])
- en: 'Fortunately, a simple and quite popular solution to this problem is at hand:
    You can use priorities to make sure those smaller packets skip ahead. If you assign
    two priorities in a `match` or `pass` rule, like this:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，这个问题有一个简单且相当流行的解决方案：你可以使用优先级确保那些较小的数据包提前通过。如果你在`match`或`pass`规则中分配两个优先级，例如这样：
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The first priority will be assigned to the regular traffic, while ACK packets
    and other packets with a low delay type of service (ToS) will be assigned the
    second priority and will be served faster than the regular packets.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: When a packet arrives, PF detects the ACK packets and puts them on the higher-priority
    queue. PF also inspects the ToS field on arriving packets. Packets that have the
    ToS set to low delay to indicate that the sender wants speedier delivery also
    get the high-priority treatment. When more than one priority is indicated, as
    in the preceding rule, PF assigns priority accordingly. Packets with other ToS
    values are processed in the order they arrive, but with ACK packets arriving faster,
    the sender spends less time waiting for ACKs and resending presumably lost data.
    The net result is that the available bandwidth is used more efficiently. (The
    `match` rule quoted here is the first one I wrote in order to get a feel for the
    new `prio` feature—on a test system, of course—soon after it was committed during
    the OpenBSD 5.0 development cycle. If you put that single `match` rule on top
    of an existing rule set, you’ll probably see that the link can take more traffic
    and more simultaneous connections before noticeable symptoms of congestion turn
    up.)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: See whether you can come up with a way to measure throughout before and after
    you introduce the two-priorities trick to your traffic shaping, and note the difference
    before you proceed to the more complex traffic-shaping options.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Queues for Bandwidth Allocation
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ve seen that traffic shaping using only priorities can be quite effective,
    but there will be times when a priorities-only scheme will fall short of your
    goals. One such scenario occurs when you’re faced with requirements that would
    be most usefully solved by assigning a higher priority, and perhaps a larger bandwidth
    share, to some kinds of traffic, such as email and other high-value services,
    and correspondingly less bandwidth to others. Another such scenario would be when
    you simply want to apportion your available bandwidth in different-sized chunks
    to specific services and perhaps set hard upper limits for some types of traffic,
    while at the same time wanting to ensure that all traffic that you care about
    gets at least its fair share of available bandwidth. In cases like these, you
    leave the pure-priority scheme behind, at least as the primary tool, and start
    doing actual traffic shaping using *queues*.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Unlike with the priority levels, which are always available and can be used
    without further preparations, in any rule, queues represent specific parts of
    your available bandwidth and can be used only after you’ve defined them in terms
    of available capacity. Queues are a kind of buffer for network packets. Queues
    are defined with a specific amount of bandwidth, or as a specific portion of available
    bandwidth, and you can allocate portions of each queue’s bandwidth share to subqueues,
    or queues within queues, which share the parent queue’s resources. The packets
    are held in a queue until they’re either dropped or sent according to the queue’s
    criteria and subject to the queue’s available bandwidth. Queues are attached to
    specific interfaces, and bandwidth is managed on a per-interface basis, with available
    bandwidth on a given interface subdivided into the queues you define.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 与优先级级别不同，优先级级别是始终可用的，可以在任何规则中使用而无需进一步准备，而队列代表你可用带宽的特定部分，只有在你根据可用容量定义它们之后，才可以在规则中使用。队列是网络数据包的缓冲区。队列被定义为具有特定带宽量，或作为可用带宽的特定部分，你可以将每个队列带宽份额的一部分分配给子队列，或将队列嵌套在其他队列中，共享父队列的资源。数据包会被保存在队列中，直到它们根据队列的标准被丢弃或发送，并受到队列可用带宽的限制。队列附加到特定接口，带宽是按接口管理的，每个接口的可用带宽会细分到你定义的队列中。
- en: 'The basic syntax for defining a queue follows this pattern:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 定义队列的基本语法遵循以下模式：
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The letters following the bandwidth number denote the unit of measurement:
    *`K`* denotes kilobits; *`M`* megabits; and *`G`* gigabits. When you write only
    the bandwidth number, it’s interpreted as the number of bits per second. It’s
    possible to tack on other options to this basic syntax, as we’ll see in later
    examples.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 带宽数字后面的字母表示测量单位：*`K`*表示千比特；*`M`*表示兆比特；*`G`*表示千兆比特。当你只写带宽数字时，它会被解释为每秒比特数。你还可以在这个基本语法上添加其他选项，正如我们在后面的示例中将看到的。
- en: Note
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '*Subqueue definitions name their parent queue, and one queue needs to be the
    default queue that receives any traffic not specifically assigned to other queues.*'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*子队列定义会命名它们的父队列，并且必须有一个队列作为默认队列，用于接收任何未特别分配到其他队列的流量。*'
- en: Once queue definitions are in place, you integrate traffic shaping into your
    rule set by rewriting your `pass` or `match` rules to assign traffic to a specific
    queue.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦队列定义完成，你可以通过重写`pass`或`match`规则，将流量分配到特定的队列中，从而将流量整形集成到规则集中。
- en: What’s your Total Usable Bandwidth?
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 你的总可用带宽是多少？
- en: Once we start working with defined parts of total bandwidth rather than priorities
    that somehow share the whole, determining the exact value of your total usable
    bandwidth becomes interesting. It can be difficult to determine actual usable
    bandwidth on a specific interface for queuing. If you don’t specify a total bandwidth,
    the total bandwidth available will be used to calculate the allocations, but some
    types of interfaces cannot reliably report the actual bandwidth value. One common
    example of this discrepancy is where your gateway’s external interface is a 100
    megabit (Mb) Ethernet interface, attached to a DSL line that offers only 8Mb download
    and 1Mb upload.^([[40](#ftn.ch07fn01a)]) The Ethernet interface will then confidently
    report 100Mb bandwidth, not the actual value of the Internet-facing connection.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们开始处理总带宽的定义部分，而不是依靠某种方式共享整个带宽的优先级，确定你总可用带宽的准确值就变得非常有趣。确定特定接口的实际可用带宽用于队列可能比较困难。如果你没有指定总带宽，那么将使用可用的总带宽来计算分配值，但某些类型的接口无法可靠地报告实际带宽值。一个常见的例子是，如果你的网关的外部接口是一个100兆比特（Mb）以太网接口，并连接到一条仅提供8Mb下载和1Mb上传速度的DSL线路。^([[40](#ftn.ch07fn01a)])
    这个以太网接口将自信地报告100Mb带宽，而不是面向互联网连接的实际值。
- en: For this reason, it usually makes sense to set the total bandwidth to a fixed
    value. Unfortunately, the value to use may not be exactly what your bandwidth
    supplier tells you is available because there will always be some overhead due
    to various technologies and implementations. For example, in typical TCP/IP over
    wired Ethernet, overhead can be as low as single-digit percentages, but TCP/IP
    over ATM has been known to have overhead of almost 20 percent. If your bandwidth
    supplier doesn’t provide the overhead information, you’ll need to make an educated
    guess at the starting value. In any case, remember that the total bandwidth available
    is never greater than the bandwidth of the weakest link in your network path.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通常最好将总带宽设置为一个固定值。不幸的是，使用的值可能与带宽供应商告诉你的可用带宽值不完全相同，因为各种技术和实现总会有一些开销。例如，在典型的
    TCP/IP 有线以太网中，开销可以低至个位数的百分比，但在 ATM 上的 TCP/IP，开销已经接近 20%。如果你的带宽供应商没有提供开销信息，你需要对起始值做一个合理的猜测。无论如何，请记住，总带宽永远不会大于网络路径中最弱链路的带宽。
- en: Queues are supported only for outbound connections relative to the system doing
    the queuing. When planning your bandwidth management, consider the actual usable
    bandwidth to be equal to the weakest (lowest bandwidth) link in the connection’s
    path, even if your queues are set up on a different interface.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 队列仅支持相对于执行队列操作的系统的出站连接。在规划带宽管理时，考虑到实际可用带宽应该等于连接路径中最弱（带宽最小）的链路，即使你的队列设置在不同的接口上。
- en: The HFSC Algorithm
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: HFSC 算法
- en: Underlying any queue system you define using the queue system in OpenBSD 5.5
    and later is the *Hierarchical Fair Service Curve (HFSC)* algorithm. HFSC was
    designed to allocate resources fairly among queues in a hierarchy. One of its
    interesting features is that it imposes no limits until some part of the traffic
    reaches a volume that’s close to its preset limits. The algorithm starts shaping
    just before the traffic reaches a point where it deprives some other queue of
    its guaranteed minimum share.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在 OpenBSD 5.5 及以后版本中，任何你定义的队列系统的基础都是 *分层公平服务曲线（HFSC）* 算法。HFSC 的设计目的是在一个层级结构中公平地分配资源。它的一个有趣特性是，直到某部分流量接近预设限制时，它才开始整形流量。该算法会在流量接近剥夺其他队列的最小保证份额的点之前开始整形。
- en: Note
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '*All sample configurations we present in this book assign traffic to queues
    in the outgoing direction because you can realistically control only traffic generated
    locally and, once limits are reached, any traffic-shaping system will eventually
    resort to dropping packets in order to make the endpoint back off. As we saw in
    the earlier examples, all well-behaved TCP stacks will respond to lost ACKs with
    slower packet rates.*'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们在本书中展示的所有示例配置都将流量分配到出站队列中，因为你只能现实地控制本地生成的流量，并且一旦达到限制，任何流量整形系统最终都会通过丢包来迫使端点回退。正如我们在前面的示例中看到的，所有表现良好的
    TCP 堆栈会通过较慢的包速率响应丢失的 ACK。*'
- en: Now that you know at least the basics of the theory behind the OpenBSD queue
    system, let’s see how queues work.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你至少了解了 OpenBSD 队列系统背后的基本理论，让我们来看看队列是如何工作的。
- en: Splitting Your Bandwidth into Fixed-Size Chunks
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将带宽分成固定大小的块
- en: You’ll often find that certain traffic should receive a higher priority than
    other traffic. For example, you’ll often want important traffic, such as mail
    and other vital services, to have a baseline amount of bandwidth available at
    all times, while other services, such as peer-to-peer file sharing, shouldn’t
    be allowed to consume more than a certain amount. To address these kinds of issues,
    queues offer a wider range of options than the pure-priority scheme.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 你会经常发现某些流量应该比其他流量具有更高的优先级。例如，你可能希望重要的流量，如邮件和其他关键服务，始终保持一定的带宽，而其他服务，如点对点文件共享，则不应允许消耗超过某个量的带宽。为了解决这些问题，队列提供了比纯优先级方案更广泛的选择。
- en: The first queue example builds on the rule sets from earlier chapters. The scenario
    is that we have a small local network, and we want to let the users on the local
    network connect to a predefined set of services outside their own network while
    also letting users from outside the local network access a Web server and an FTP
    server somewhere on the local network.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个队列示例基于前面章节中的规则集。场景是我们有一个小型本地网络，我们希望让本地网络的用户连接到一个预定义的外部服务集，同时也允许外部网络的用户访问本地网络上的
    Web 服务器和 FTP 服务器。
- en: Queue Definition
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 队列定义
- en: In the following example, all queues are set up with the root queue, called
    `main`, on the external, Internet-facing interface. This approach makes sense
    mainly because bandwidth is more likely to be limited on the external link than
    on the local network. In principle, however, allocating queues and running traffic
    shaping can be done on any network interface.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，所有队列都设置在根队列上，根队列叫做`main`，并且位于面向外部、连接互联网的接口上。这个方法主要是因为外部链路的带宽比本地网络更容易受到限制。然而，原则上，分配队列并进行流量整形可以在任何网络接口上进行。
- en: This setup includes a queue for a total bandwidth of 20Mb with six subqueues.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这个设置包括一个总带宽为20Mb的队列，并且有六个子队列。
- en: '[PRE6]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The subqueue `defq`, shown in the preceding example, has a bandwidth allocation
    of 3600K, or 18 percent of the bandwidth, and is designated as the default queue.
    This means any traffic that matches a `pass` rule but that isn’t explicitly assigned
    to some other queue ends up here.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 前面示例中显示的子队列`defq`具有3600K的带宽分配，即占总带宽的18%，并被指定为默认队列。这意味着任何匹配`pass`规则但没有明确分配到其他队列的流量都会进入此队列。
- en: The other queues follow more or less the same pattern, up to subqueue `ssh`,
    which itself has two subqueues (the two indented lines below it). Here, we see
    a variation on the trick of using two separate priorities to speed up ACK packets,
    and as we’ll see shortly, the rule that assigns traffic to the two SSH subqueues
    assigns different priorities. Bulk SSH transfers, typically SCP file transfers,
    are transmitted with a ToS indicating throughput, while interactive SSH traffic
    has the ToS flag set to low delay and skips ahead of the bulk transfers. The interactive
    traffic is likely to be less bandwidth consuming and gets a smaller share of the
    bandwidth, but it receives preferential treatment because of the higher-priority
    value assigned to it. This scheme also helps the speed of SCP file transfers because
    the ACK packets for the SCP transfers will be assigned a higher priority.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其他队列大致遵循相同的模式，直到子队列`ssh`，它本身有两个子队列（下方的两个缩进行）。在这里，我们看到了使用两个不同优先级加速ACK包的技巧的变化，正如我们稍后会看到的，分配流量到这两个SSH子队列的规则分配了不同的优先级。大容量SSH传输，通常是SCP文件传输，使用指示吞吐量的ToS，而交互式SSH流量则设置为低延迟的ToS标志，并且会优先于大容量传输。交互式流量可能消耗的带宽较少，因此获得较小的带宽份额，但由于它被分配了更高的优先级，因此得到优先处理。这个方案还帮助了SCP文件传输的速度，因为SCP传输的ACK包将被分配更高的优先级。
- en: Finally, we have the `icmp` queue, which is reserved for the remaining 400K,
    or 2 percent, of the bandwidth from the top level. This guarantees a minimum amount
    of bandwidth for ICMP traffic that we want to pass but that doesn’t match the
    criteria that would have it assigned to the other queues.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有`icmp`队列，它为剩余的400K带宽（即2%的带宽）保留。这确保了我们希望传递的ICMP流量能够获得最小的带宽保证，即使它不符合分配到其他队列的标准。
- en: Rule Set
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 规则集
- en: To tie the queues into the rule set, we use the `pass` rules to indicate which
    traffic is assigned to the queues and their criteria.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将队列与规则集绑定，我们使用`pass`规则来指示哪些流量被分配到哪些队列及其相应标准。
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The rules for `ssh`, `ftp`, `www`, `udp`, and `icmp` assign traffic to their
    respective queues, and we note again that the `ssh` queue’s subqueues are assigned
    traffic with two different priorities. The last catchall rule ➊ passes all other
    outgoing traffic from the local network, lumping it into the default `defq` queue.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`ssh`、`ftp`、`www`、`udp`和`icmp`的规则将流量分配到它们各自的队列，并且我们再次注意到，`ssh`队列的子队列分配了两种不同的优先级的流量。最后的兜底规则➊将所有其他来自本地网络的外发流量通过，并将其放入默认的`defq`队列。'
- en: You can always let a block of `match` rules do the queue assignment instead
    in order to make the configuration even more flexible. With match rules in place,
    you move the filtering decisions to block, pass, or even redirect to a set of
    rules elsewhere.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以让一组`match`规则来执行队列分配，从而使配置更加灵活。有了匹配规则，你可以将过滤决策移到其他地方的规则集来进行阻塞、通过，甚至重定向。
- en: '[PRE8]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note that with `match` rules performing the queue assignment, there’s no need
    for a final catchall to put the traffic that doesn’t match the other rules into
    the default queue. Any traffic that doesn’t match these rules and that’s allowed
    to pass will end up in the default queue.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，使用`match`规则进行队列分配时，无需一个最终的兜底规则将不匹配其他规则的流量放入默认队列。任何不匹配这些规则并且允许通过的流量都会进入默认队列。
- en: Upper and Lower Bounds with Bursts
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 上限和下限与突发流量
- en: Fixed bandwidth allocations are nice, but network admins with traffic-shaping
    ambitions tend to look for a little more flexibility once they’ve gotten their
    feet wet. Wouldn’t it be nice if there were a regime with flexible bandwidth allocation,
    offering guaranteed lower and upper bounds for bandwidth available to each queue
    and variable allocations over time—and one that starts shaping only when there’s
    an actual need to do so?
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 固定带宽分配是不错的选择，但对于那些有流量整形需求的网络管理员来说，在初步尝试后，他们往往希望有更多的灵活性。假如有一个带宽分配机制，既能提供每个队列的带宽下限和上限保证，又能随时间变化进行灵活分配，并且只有在真正需要时才开始进行流量整形，那该有多好呢？
- en: The good news is that the OpenBSD queues can do just that, courtesy of the underlying
    HFSC algorithm discussed earlier. HFSC makes it possible to set up queuing regimes
    with guaranteed minimum allocations and hard upper limits, and you can even have
    allocations that include `burst` values to let available capacity vary over time.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，OpenBSD队列正好能够做到这一点，得益于我们之前讨论过的HFSC算法。HFSC使得可以设置带有保证的最小分配和硬性上限的排队机制，甚至可以设置包含`burst`值的分配，让可用容量随时间变化。
- en: Queue Definition
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 队列定义
- en: 'Working from a typical gateway configuration like the ones we’ve altered incrementally
    over the earlier chapters, we insert this queue definition early in the *pf.conf*
    file:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们在前几章中逐步修改的典型网关配置出发，我们在*pf.conf*文件的开头插入这个队列定义：
- en: '[PRE9]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This definition has some characteristics that are markedly different from the
    previous one in [Introducing Queues for Bandwidth Allocation](ch07.html#introducing_queues_for_bandwidth_allocat
    "Introducing Queues for Bandwidth Allocation"). We start with this rather small
    hierarchy by splitting the top-level queue, `rootq`, into two. Next, we subdivide
    the `main` queue into several subqueues, all of which have a `min` value set—the
    guaranteed minimum bandwidth allocated to the queue. (The `max` value would set
    a hard upper limit on the queue’s allocation.) The `bandwidth` parameter also
    sets the allocation the queue will have available when it’s backlogged—that is,
    when it’s started to eat into its `qlimit`, or *queue limit*, allocation.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这个定义与[引入带宽分配队列](ch07.html#introducing_queues_for_bandwidth_allocat "引入带宽分配队列")中的定义有一些显著不同。我们从这个相对较小的层次结构开始，将顶层队列`rootq`分成两个。接下来，我们将`main`队列细分成几个子队列，所有子队列都有一个设置了的`min`值——即分配给队列的最小带宽保证。（`max`值会设置队列分配的硬上限。）`bandwidth`参数还设置了当队列出现积压时可用的带宽分配——即当队列开始消耗其`qlimit`（*队列限制*）分配时。
- en: 'The queue limit parameter works like this: In case of congestion, each queue
    by default has a pool of 50 slots, the queue limit, to keep packets around when
    they can’t be transmitted immediately. Here, the top-level queues, `main` and
    `spamd`, both have larger-than-default pools set by their `qlimit` setting: `100`
    for `main` and `300` for `spamd`. Cranking up these `qlimit` sizes means we’re
    a little less likely to drop packets when the traffic approaches the set limits,
    but it also means that when the traffic shaping kicks in, we’ll see increased
    latency for connections that end up in these larger pools.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 队列限制参数的工作原理如下：在发生拥塞时，每个队列默认有一个50个槽位的池（即队列限制），用来保存那些无法立即传输的数据包。在这里，顶层队列`main`和`spamd`都通过它们的`qlimit`设置，分别设定了大于默认值的池：`main`为100，`spamd`为300。增大这些`qlimit`值意味着我们在流量接近设置的限制时丢包的可能性较小，但也意味着当流量整形启动时，我们会看到进入这些较大池中的连接的延迟增加。
- en: Rule Set
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 规则集
- en: 'The next step is to tie the newly created queues into the rule set. If you
    have a filtering regime in place already, the tie-in is simple—just add a few
    `match` rules:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将新创建的队列与规则集关联起来。如果你已经有了过滤机制，那么关联就很简单——只需要添加几条`match`规则：
- en: '[PRE10]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Here, the `match` rules once again do the ACK packet speedup trick with the
    high- and low-priority queue assignment, just as we saw earlier in the pure-priority-based
    system. The only exception is when we assign traffic to our lowest-priority queue
    (with a slight modification to an existing `pass` rule), where we really don’t
    want any speedup.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`match`规则再次通过高优先级和低优先级队列分配来加速ACK包，就像我们在基于纯优先级的系统中看到的那样。唯一的例外是当我们将流量分配到最低优先级队列时（对现有的`pass`规则稍作修改），此时我们确实不希望加速。
- en: '[PRE11]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Assigning the `spamd` traffic to a minimal-sized queue with 0 priority here
    is intended to slow down the spammers on their way to our `spamd`. (See [Chapter 6](ch06.html
    "Chapter 6. Turning the Tables for Proactive Defense") for more on `spamd` and
    related matters.)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 将`spamd`流量分配到一个最小大小的队列，并将其优先级设置为0，目的是在垃圾邮件发送者到达我们的`spamd`之前减缓他们的速度。（有关`spamd`及相关问题的更多信息，请参见[第6章](ch06.html
    "第6章. 主动防御策略的反击")。）
- en: 'With the queue assignment and priority setting in place, it should be clear
    that the queue hierarchy here uses two familiar tricks to make efficient use of
    available bandwidth. First, it uses a variation of the high- and low-priority
    mix demonstrated in the earlier pure-priority example. Second, we speed up almost
    all other traffic, especially the Web traffic, by allocating a small but guaranteed
    portion of bandwidth for name service lookups. For the `qdns` queue, we set the
    `burst` value with a time limit: After `3000` milliseconds, the allocation goes
    down to a minimum of `12K` to fit within the total `200K` quota. Short-lived `burst`
    values like this can be useful to speed connections that transfer most of their
    payload during the early phases.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在队列分配和优先级设置到位之后，应该能清楚地看到，队列层次结构使用了两种熟悉的技巧来高效利用可用带宽。首先，它使用了一种高优先级和低优先级混合的变种，这种方法在之前的纯优先级示例中已经展示过。其次，我们通过为名称服务查找分配一个小而有保证的带宽部分，来加速几乎所有其他流量，特别是Web流量。对于`qdns`队列，我们设置了带有时间限制的`burst`值：在`3000`毫秒后，分配将降低到最小值`12K`，以适应总`200K`配额。像这样的短时`burst`值对于加速在早期阶段传输大部分负载的连接非常有用。
- en: 'It may not be immediately obvious from this example, but HFSC requires that
    traffic be assigned only to *leaf queues*, or queues without subqueues. That means
    it’s possible to assign traffic to `main`’s subqueues—`qpri`, `qdef`, `qweb`,
    and `qdns`—as well as `rootq`’s subqueue—`spamd`—as we just did with the `match`
    and `pass` rules, but not to `rootq` or `main` themselves. With all queue assignments
    in place, we can use `systat` queues to show the queues and their traffic:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个示例中可能不太明显，但HFSC要求流量只能分配给*叶子队列*，即没有子队列的队列。这意味着，可以将流量分配给`main`的子队列——`qpri`、`qdef`、`qweb`和`qdns`——以及`rootq`的子队列——`spamd`——正如我们刚才通过`match`和`pass`规则所做的那样，但不能直接分配给`rootq`或`main`本身。所有队列分配到位后，我们可以使用`systat`队列命令来显示队列及其流量：
- en: '[PRE12]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The queues are shown indented to indicate their hierarchy, from root to leaf
    queues. The `main` queue and its subqueues—`qpri`, `qdef`, `qweb`, and `qdns`—are
    shown with their bandwidth allocations and number of bytes and packets passed.
    The `DROP_P` and `DROP_B` columns, which show the number of packets and bytes
    dropped, would appear if we had been forced to drop packets at this stage. `QLEN`
    is the number of packets waiting for processing, while the final two columns show
    live updates of packets and bytes per second.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 队列以缩进方式显示，表示它们的层级结构，从根队列到叶子队列。`main`队列及其子队列——`qpri`、`qdef`、`qweb`和`qdns`——显示了它们的带宽分配以及通过的字节和包数量。如果在此阶段我们被迫丢包，`DROP_P`和`DROP_B`列会显示丢弃的包和字节数。`QLEN`表示等待处理的包数量，而最后两列显示每秒包和字节的实时更新。
- en: 'For a more detailed view, use `pfctl -vvsq` to show the queues and their traffic:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看更详细的视图，请使用`pfctl -vvsq`显示队列及其流量：
- en: '[PRE13]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This view shows that the queues receive traffic roughly as expected with the
    site’s typical workload. Notice that only a few moments after the rule set has
    been reloaded, the `spamd` queue is already backed up more than halfway to its
    `qlimit` setting, which seems to indicate that the queues are reasonably dimensioned
    to actual traffic.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这个视图显示了队列接收流量的情况，基本与站点的典型工作负载一致。请注意，在规则集重新加载后不久，`spamd`队列就已经接近其`qlimit`设置的一半，似乎表明队列的维度设置与实际流量相符。
- en: Note
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '*Pay attention to each queue’s dropped packets (`dropped pkts:`) counter. If
    the number of packets dropped is high or increasing, then that could mean that
    one of the bandwidth allocation parameters needs adjusting or that some other
    network problem needs to be investigated.*'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*请注意每个队列的丢包计数器（`dropped pkts:`）。如果丢包数量较高或持续增加，那么可能意味着需要调整某个带宽分配参数，或者需要调查其他网络问题。*'
- en: The DMZ Network, Now with Traffic Shaping
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: DMZ 网络，现在启用了流量整形
- en: In [Chapter 5](ch05.html "Chapter 5. Bigger or Trickier Networks"), we set up
    a network with a single gateway and all externally visible services configured
    on a separate DMZ (demilitarized zone) network so that all traffic to the servers
    from both the Internet and the internal network had to pass through the gateway.
    That network schematic, illustrated in [Chapter 5](ch05.html "Chapter 5. Bigger
    or Trickier Networks"), is shown again in [Figure 7-1](ch07.html#network_with_dmz
    "Figure 7-1. Network with DMZ"). Using the rule set from [Chapter 5](ch05.html
    "Chapter 5. Bigger or Trickier Networks") as the starting point, we’ll add some
    queuing in order to optimize our network resources. The physical and logical layout
    of the network will not change.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](ch05.html "第5章. 更大或更复杂的网络")中，我们设置了一个拥有单一网关的网络，并将所有外部可见的服务配置在一个独立的 DMZ（非军事区）网络中，以便从互联网和内部网络到服务器的所有流量都必须通过该网关。该网络示意图在[第5章](ch05.html
    "第5章. 更大或更复杂的网络")中已展示，这里在[图7-1](ch07.html#network_with_dmz "图7-1. 带有 DMZ 的网络")中再次展示。以[第5章](ch05.html
    "第5章. 更大或更复杂的网络")中的规则集为起点，我们将添加一些队列，以优化我们的网络资源。网络的物理和逻辑布局不会改变。
- en: '![Network with DMZ](httpatomoreillycomsourcenostarchimages2127159.png.jpg)Figure 7-1. Network
    with DMZ'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '![带有 DMZ 的网络](httpatomoreillycomsourcenostarchimages2127159.png.jpg)图7-1. 带有
    DMZ 的网络'
- en: The most likely bottleneck for this network is the bandwidth for the connection
    between the gateway’s external interface and the Internet. Although the bandwidth
    elsewhere in our setup isn’t infinite, of course, the available bandwidth on any
    interface in the local network is likely to be less limiting than the bandwidth
    actually available for communication with the outside world. In order to make
    services available with the best possible performance, we need to set up the queues
    so that the bandwidth available at the site is made available to the traffic we
    want to allow. The interface bandwidth on the DMZ interface is likely either 100Mb
    or 1Gb, while the *actual available bandwidth* for connections from outside the
    local network is considerably smaller. This consideration shows up in our queue
    definitions, where the actual bandwidth available for external traffic is the
    main limitation in the queue setup.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 该网络最可能的瓶颈是网关外部接口与互联网之间的带宽。尽管我们设置中的其他地方的带宽当然不是无限的，但本地网络上任何接口的可用带宽通常会比与外部世界通信的实际带宽更不受限制。为了使服务以最佳性能可用，我们需要设置队列，以便站点上可用的带宽能够分配给我们希望允许的流量。DMZ
    接口上的接口带宽可能是 100Mb 或 1Gb，而*实际可用带宽*则大大小于从本地网络外部连接的带宽。这一考虑体现在我们的队列定义中，其中，外部流量的实际带宽可用性是队列设置中的主要限制因素。
- en: '[PRE14]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Notice that for each interface, there’s a root queue with a bandwidth limitation
    that determines the allocation for all queues attached to that interface. In order
    to use the new queuing infrastructure, we need to make some changes to the filtering
    rules, too.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，对于每个接口，都有一个根队列，其带宽限制决定了分配给该接口所有队列的带宽。为了使用新的队列基础设施，我们还需要对过滤规则进行一些修改。
- en: Note
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注
- en: '*Because any traffic not explicitly assigned to a specific queue is assigned
    to the default queue for the interface, be sure to tune your filtering rules as
    well as your queue definitions to the actual traffic in your network.*'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*由于任何未明确分配到特定队列的流量都会被分配到接口的默认队列，因此务必根据网络中的实际流量调整过滤规则和队列定义。*'
- en: 'The main part of the filtering rules could end up looking like this after adding
    the queues:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 添加队列后，过滤规则的主要部分可能如下所示：
- en: '[PRE15]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Notice that only traffic that will pass either the DMZ or the external interface
    is assigned to queues. In this configuration, with no externally accessible services
    on the internal network, queuing on the internal interface wouldn’t make much
    sense because that’s likely the part of the network with the least restricted
    available bandwidth. Also, as in earlier examples, there’s a case to be made for
    separating the queue assignments from the filtering part of the rule set by making
    a block of `match` rules responsible for queue assignment.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，只有通过 DMZ 或外部接口的流量才会被分配到队列。在这种配置下，如果内部网络没有外部可访问的服务，那么在内部接口上进行队列管理就没有太大意义，因为那可能是带宽限制最少的网络部分。此外，正如之前的示例所示，可以通过将`match`规则块专门用于队列分配，从而将队列分配与规则集的过滤部分分开。
- en: Using Queues to Handle Unwanted Traffic
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用队列处理不需要的流量
- en: So far, we’ve focused on queuing as a way to make sure specific kinds of traffic
    are let through as efficiently as possible. Now, we’ll look at two examples that
    present a slightly different way to identify and handle unwanted traffic using
    various queuing-related tricks to keep miscreants in line.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们专注于队列化作为确保特定类型流量高效通过的一种方式。现在，我们将查看两个示例，它们展示了一种稍微不同的方法，使用各种与队列相关的技巧来识别和处理不需要的流量，以确保不法分子被控制。
- en: Overloading to a Tiny Queue
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 超载到一个小队列
- en: In [Turning Away the Brutes](ch06.html#turning_away_the_brutes "Turning Away
    the Brutes"), we used a combination of state-tracking options and `overload` rules
    to fill a table of addresses for special treatment. The special treatment we demonstrated
    in [Chapter 6](ch06.html "Chapter 6. Turning the Tables for Proactive Defense")
    was to cut all connections, but it’s equally possible to assign `overload` traffic
    to a specific queue instead. For example, consider the rule from our first queue
    example, shown here.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在[《抵御暴力攻击》](ch06.html#turning_away_the_brutes "抵御暴力攻击")中，我们使用了状态跟踪选项和`overload`规则的组合，将特定地址填充到一个特殊处理的表格中。我们在[第六章](ch06.html
    "第六章。为主动防御翻盘")中演示的特殊处理方法是切断所有连接，但同样也可以将`overload`流量分配到特定队列。例如，考虑下面这个我们第一个队列示例中的规则。
- en: '[PRE16]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'To create a variation of the overload table trick from [Chapter 6](ch06.html
    "Chapter 6. Turning the Tables for Proactive Defense"), add state-tracking options,
    like this:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建[第六章](ch06.html "第六章。为主动防御翻盘")中超载表格技巧的变种，可以添加状态跟踪选项，像这样：
- en: '[PRE17]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Then, make one of the queues slightly smaller:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，将其中一个队列略微缩小：
- en: '[PRE18]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'And assign traffic from miscreants to the small-bandwidth queue with this rule:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 并通过这个规则将不法分子的流量分配到小带宽队列：
- en: '[PRE19]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: As a result, the traffic from the bruteforcers would pass, but with a hard upper
    limit of 512 bits per second. (It’s worth noting that tiny bandwidth allocations
    may be hard to enforce on high-speed links due to the network stack’s timer resolution.
    If the allocation is small enough relative to the capacity of the link, packets
    that exceed the stated per-second maximum allocation may be transferred anyway,
    before the bandwidth limit kicks in.) It might also be useful to supplement rules
    like these with table-entry expiry, as described in [Tidying Your Tables with
    pfctl](ch06.html#tidying_your_tables_with_pfctl "Tidying Your Tables with pfctl").
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，来自暴力破解者的流量将被通过，但每秒传输速率有一个严格的上限为512比特。（值得注意的是，由于网络栈的计时器精度问题，可能很难在高速链接上强制执行小带宽分配。如果分配的带宽相对于链路容量过小，超出每秒最大分配的包可能仍然会被传输，直到带宽限制生效。）此外，像这样的规则可能还需要补充表项过期设置，如在[使用pfctl整理表格](ch06.html#tidying_your_tables_with_pfctl
    "使用pfctl整理表格")中所描述的那样。
- en: Queue Assignments Based on Operating System Fingerprint
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于操作系统指纹的队列分配
- en: '[Chapter 6](ch06.html "Chapter 6. Turning the Tables for Proactive Defense")
    covered several ways to use `spamd` to cut down on spam. If running `spamd` isn’t
    an option in your environment, you can use a queue and rule set based on the knowledge
    that machines that send spam are likely to run a particular operating system.
    (Let’s call that operating system Windows.)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[第六章](ch06.html "第六章。为主动防御翻盘")介绍了几种使用`spamd`减少垃圾邮件的方法。如果在你的环境中无法运行`spamd`，你可以基于以下知识使用队列和规则集：发送垃圾邮件的机器很可能运行某个特定的操作系统。（我们将这个操作系统称为Windows。）'
- en: PF has a fairly reliable operating system fingerprinting mechanism, which detects
    the operating system at the other end of a network connection based on characteristics
    of the initial SYN packets at connection setup. The following may be a simple
    substitute for `spamd` if you’ve determined that legitimate mail is highly unlikely
    to be delivered from systems that run that particular operating system.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: PF具有一个相当可靠的操作系统指纹识别机制，可以根据连接建立时初始SYN数据包的特征来检测网络连接另一端的操作系统。如果你已经确定从运行该特定操作系统的系统中不太可能传送合法邮件，那么以下内容可以作为`spamd`的简单替代方案。
- en: '[PRE20]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Here, email traffic originating from hosts that run a particular operating system
    get no more than 512 bits per second of your bandwidth.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，来自运行特定操作系统的主机的电子邮件流量不会超过你带宽的512比特每秒。
- en: Transitioning from ALTQ to Priorities and Queues
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从ALTQ过渡到优先级和队列
- en: 'If you already have configurations that use ALTQ for traffic shaping and you’re
    planning a switch to OpenBSD 5.5 or newer, this section contains some pointers
    for how to manage the transition. The main points are these:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '***The rules after transition are likely simpler.*** The OpenBSD 5.5 and newer
    traffic-shaping system has done away with the somewhat arcane ALTQ syntax with
    its selection of queuing algorithms, and it distinguishes clearly between queues
    and pure-priority shuffling. In most cases, your configuration becomes significantly
    more readable and maintainable after a conversion to the new traffic-shaping system.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '***For simple configurations, set prio is enough.*** The simplest queue discipline
    in ALTQ was `priq`, or priority queues. The most common simple use case was the
    two-priority speedup trick first illustrated by Daniel Hartmeier in the previously
    cited article. The basic two-priority configuration looks like this:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In OpenBSD 5.5 and newer, the equivalent effect can be achieved with no queue
    definitions. Instead, you assign two priorities in a `match` or `pass` rule, like
    this:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Here, the first priority will be assigned to regular traffic, while ACK and
    other packets with a low-delay ToS will be assigned the second priority and will
    be served faster than the regular packets. The effect is the same as in the ALTQ
    example we just quoted, with the exception of defined bandwidth limits and the
    somewhat dubious effect of traffic shaping on incoming traffic.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '***Priority queues can for the most part be replaced by set prio constructs.***
    For pure-priority differentiation, applying `set prio` on a per `pass` or `match`
    rule basis is simpler than defining queues and assigning traffic and affects only
    the packet priority. ALTQ allowed you to define CBQ or HFSC queues that also had
    a priority value as part of their definition. Under the new queuing system, assigning
    priority happens only in `match` or `pass` rules, but if your application calls
    for setting both priority and queue assignment in the same rule, the new syntax
    allows for that, too:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The effect is similar to the previous behavior shown in [Splitting Your Bandwidth
    into Fixed-Size Chunks](ch07.html#splitting_your_bandwidth_into_fixed-size "Splitting
    Your Bandwidth into Fixed-Size Chunks"), and this variant may be particularly
    helpful during transition.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '***Priorities are now always important. Keep in mind that the default is 3.***
    It’s important to be aware that traffic priorities are always enabled since OpenBSD
    5.0, and they need to be taken into consideration even when you’re not actively
    assigning priorities. In old-style configurations that employed the two-priority
    trick to speed up ACKs and by extension all traffic, the only thing that was important
    was that there were two different priorities in play. The low-delay packets would
    be assigned to the higher-priority queue, and the net effect would be that traffic
    would likely pass faster, with more efficient bandwidth use than with the default
    FIFO queue. Now the default priority is 3, and setting the priority for a queue
    to 0, as a few older examples do, will mean that the traffic assigned that priority
    will be considered ready to pass only when there’s no higher-priority traffic
    left to handle.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '***优先级现在始终很重要。请记住，默认值是3。*** 需要注意的是，从OpenBSD 5.0开始，流量优先级始终是启用的，即使你没有主动分配优先级，也需要考虑优先级。在使用两级优先级技巧来加速ACK和所有流量的旧式配置中，唯一重要的是有两个不同的优先级。低延迟的数据包会被分配到高优先级队列中，最终的效果是流量可能通过得更快，带宽使用比默认的FIFO队列更有效。现在默认优先级是3，并且如果将队列的优先级设置为0（如一些旧示例所做的），那么只有在没有更高优先级的流量需要处理时，分配到该优先级的流量才会被视为准备通过。'
- en: '***For actual bandwidth shaping, HFSC works behind the scenes.*** Once you’ve
    determined that your specification calls for slicing available bandwidth into
    chunks, the underlying algorithm is always HFSC. The variety of syntaxes for different
    types of queues is gone. HFSC was chosen for its flexibility as well as the fact
    that it starts actively shaping traffic only once the traffic approaches one of
    the limits set by your queuing configuration. In addition, it’s possible to create
    CBQ-like configurations by limiting the queue definitions to only bandwidth declarations.
    [Splitting Your Bandwidth into Fixed-Size Chunks](ch07.html#splitting_your_bandwidth_into_fixed-size
    "Splitting Your Bandwidth into Fixed-Size Chunks") (mentioned earlier) demonstrates
    a static configuration that implements CBQ as a subset of HFSC.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '***对于实际的带宽塑形，HFSC在幕后工作。*** 一旦你确定你的规格要求将可用带宽分割成多个块，底层算法始终是HFSC。不同类型队列的语法已经不再使用。HFSC被选中是因为它的灵活性，以及它只有在流量接近你队列配置设置的某个限制时，才会主动开始塑形流量。此外，限制队列定义为仅包含带宽声明的配置，可以实现类似CBQ的配置。[将你的带宽划分为固定大小的块](ch07.html#splitting_your_bandwidth_into_fixed-size
    "将你的带宽划分为固定大小的块")（前文提到）展示了一种静态配置，作为HFSC的子集实现CBQ。'
- en: '***You can transition from ALTQ via the oldqueue mechanism.*** OpenBSD 5.5
    supports legacy ALTQ configurations with only one minor change to configurations:
    The `queue` keyword was needed as a reserved word for the new queuing system,
    so ALTQ queues need to be declared as `oldqueue` instead. Following that one change
    (a pure search and replace operation that you can even perform just before starting
    your operating system upgrade), the configuration will work as expected.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '***你可以通过旧队列机制从ALTQ过渡。*** OpenBSD 5.5支持传统的ALTQ配置，仅需要对配置进行一次小改动：`queue`关键字作为新队列系统的保留字，因此ALTQ队列需要声明为`oldqueue`。只要做出这个更改（这实际上是一个纯粹的搜索替换操作，甚至可以在操作系统升级前执行），配置就会按预期工作。'
- en: '***If your setup is sufficiently complicated, go back to specifications and
    reimplement.*** The examples in this chapter are somewhat stylized and rather
    simple. If you have running configurations that have been built up incrementally
    over several years and have reached a complexity level, orders of magnitude larger
    than those described here, the new syntax may present an opportunity to define
    what your setup is for and produce a specification that is fit to reimplement
    in a cleaner and more maintainable configuration.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '***如果你的设置足够复杂，请返回规格并重新实现。*** 本章中的示例有些风格化且相对简单。如果你有经过多次增量建设的现有配置，并且已经达到比这里描述的复杂度大几个数量级的水平，新语法可能提供了一个机会，帮助你定义你的设置用途，并创建一个更清晰、更易维护的配置规范进行重新实现。'
- en: Going the `oldqueue` route and tweaking from there will work to some degree,
    but it may be easier to make the transition via a clean reimplementation from
    revised specification in a test environment where you can test whether your accumulated
    assumptions hold up in a the context of the new traffic-shaping system. Whatever
    route you choose for your transition, you’re more or less certain to end up with
    a more readable and maintainable configuration after your switch to OpenBSD 5.5
    or newer.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 采用 `oldqueue` 路线并从那里进行调整会在一定程度上有效，但通过在测试环境中根据修订后的规范进行干净的重新实现可能更容易过渡，在这个环境中你可以测试你的假设是否在新的流量整形系统背景下成立。无论你选择哪种过渡方式，切换到
    OpenBSD 5.5 或更高版本后，你几乎可以确定最终会得到一个更加易读和可维护的配置。
- en: Directing Traffic with ALTQ
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 ALTQ 定向流量
- en: '*ALTQ* is the very flexible legacy mechanism for network traffic shaping, which
    was integrated into PF on OpenBSD^([[41](#ftn.ch07fn02)]) in time for the OpenBSD
    3.3 release by Henning Brauer, who’s also the main developer of the priorities
    and queues system introduced in OpenBSD 5.5 (described in the previous sections
    of this chapter). OpenBSD 3.3 onward moved all ALTQ configuration into *pf.conf*
    to ease the integration of traffic shaping and filtering. PF ports to other BSDs
    were quick to adopt at least some optional ALTQ integration.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '*ALTQ* 是一个非常灵活的旧有机制，用于网络流量整形，它在 OpenBSD^([[41](#ftn.ch07fn02)]) 3.3 版本时被 Henning
    Brauer 集成到 PF 中，他也是 OpenBSD 5.5 引入的优先级和队列系统（在本章前面的部分已描述）的主要开发者。从 OpenBSD 3.3 开始，所有
    ALTQ 配置都被移入 *pf.conf*，以便于流量整形和过滤的集成。PF 移植到其他 BSD 系统时，快速地采用了至少一些可选的 ALTQ 集成功能。'
- en: Note
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '*OpenBSD 5.5 introduced a new queue system for traffic shaping with a radically
    different (and more readable) syntax that complements the always-on priority system
    introduced in OpenBSD 5.0\. The new system is intended to replace ALTQ entirely
    after one transitional release. The rest of this chapter is useful only if you’re
    interested in learning about how to set up or maintain an ALTQ-based system.*'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '*OpenBSD 5.5 引入了一个全新的队列系统，用于流量整形，语法与之前有着根本性的不同（而且更易读），它补充了在 OpenBSD 5.0 中引入的始终开启的优先级系统。这个新系统旨在经过一次过渡发布后完全取代
    ALTQ。本章的其余部分只有在你有兴趣学习如何设置或维护基于 ALTQ 的系统时才有用。*'
- en: Basic ALTQ Concepts
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基本的 ALTQ 概念
- en: As the name suggests, ALTQ configurations are totally queue-centric. As in the
    more recent traffic-shaping system, ALTQ queues are defined in terms of bandwidth
    and attached to interfaces. Queues can be assigned priority, and in some contexts,
    they can have subqueues that receive a share of the parent queue’s bandwidth.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名，ALTQ 配置完全以队列为中心。与更新后的流量整形系统一样，ALTQ 队列是通过带宽来定义的，并附加到接口上。队列可以分配优先级，在某些情况下，它们可以有子队列，子队列可以接收父队列带宽的一部分。
- en: 'The general syntax for ALTQ queues looks like this:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ALTQ 队列的一般语法如下：
- en: '[PRE24]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Note
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '*On OpenBSD 5.5 and newer, ALTQ queues are denoted `oldqueue` instead of `queue`
    due to an irresolvable syntax conflict with the new queuing subsystem.*'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '*在 OpenBSD 5.5 及更新版本中，由于与新队列子系统之间无法解决的语法冲突，ALTQ 队列用 `oldqueue` 代替了 `queue`。*'
- en: Once queue definitions are in place, you integrate traffic shaping into your
    rule set by rewriting your `pass` or `match` rules to assign traffic to a specific
    queue. Any traffic that you don’t explicitly assign to a specific queue gets lumped
    in with everything else in the default queue.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦队列定义就位，你可以通过重写 `pass` 或 `match` 规则，将流量分配到特定队列，从而将流量整形集成到你的规则集中。任何未显式分配到特定队列的流量都会与其他所有流量一起放入默认队列。
- en: Queue Schedulers, aka Queue Disciplines
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 队列调度器，也称为队列纪律
- en: In the default networking setup, with no queuing, the TCP/IP stack and its filtering
    subsystem process the packets according to the FIFO discipline.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在默认的网络配置中，如果没有队列设置，TCP/IP 栈及其过滤子系统会按照 FIFO 纪律处理数据包。
- en: ALTQ offers three queue-scheduler algorithms, or *disciplines*, that can alter
    this behavior slightly. The types are `priq`, `cbq`, and `hfsc`. Of these, `cbq`
    and `hfsc` queues can have several levels of subqueues. The `priq` queues are
    essentially flat, with only one queue level. Each of the disciplines has its own
    syntax specifics, and we’ll address those in the following sections.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ALTQ 提供了三种队列调度算法，或称为 *纪律*，它们可以稍微改变这个行为。类型有 `priq`、`cbq` 和 `hfsc`。其中，`cbq` 和
    `hfsc` 队列可以有多个子队列。`priq` 队列本质上是扁平的，只有一个队列级别。每种纪律都有其独特的语法规则，我们将在接下来的部分中介绍这些规则。
- en: priq
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: priq
- en: '*Priority-based queues* are defined purely in terms of priority within the
    total declared bandwidth. For `priq` queues, the allowed priority range is 0 through
    15, where a higher value earns preferential treatment. Packets that match the
    criteria for higher-priority queues are serviced before the ones matching lower-priority
    queues.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '*基于优先级的队列*完全根据总带宽中声明的优先级来定义。对于`priq`队列，允许的优先级范围是 0 到 15，其中较高的值会获得优先处理。符合较高优先级队列标准的数据包会在符合较低优先级队列标准的数据包之前被处理。'
- en: cbq
  id: totrans-155
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: cbq
- en: '*Class-based queues* are defined as constant-sized bandwidth allocations, as
    a percentage of the total available or in units of kilobits, megabits, or gigabits
    per second. A `cbq` queue can be subdivided into queues that are also assigned
    priorities in the range 0 to 7, and again, a higher priority means preferential
    treatment.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '*基于类别的队列*定义为常量大小的带宽分配，作为总可用带宽的百分比，或以千比特、兆比特或千兆比特每秒为单位。`cbq` 队列可以细分为多个子队列，这些子队列也被分配了
    0 到 7 范围内的优先级，优先级较高意味着优先处理。'
- en: hfsc
  id: totrans-157
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: hfsc
- en: The `hfsc` discipline uses the HFSC algorithm to ensure a “fair” allocation
    of bandwidth among the queues in a hierarchy. HFSC comes with the possibility
    of setting up queuing regimes with guaranteed minimum allocations and hard upper
    limits. Allocations can even vary over time, and you can even have fine-grained
    priority with a 0 to 7 range.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`hfsc` 规则使用 HFSC 算法确保队列层次中的带宽分配“公平”。HFSC 允许设置具有最小保留分配和硬性上限的队列策略。分配甚至可以随时间变化，且你还可以拥有从
    0 到 7 范围的细粒度优先级。'
- en: Because both the algorithm and the corresponding setup with ALTQ are fairly
    complicated, with a number of tunable parameters, most ALTQ practitioners tend
    to stick with the simpler queue types. Yet the ones who claim to understand HFSC
    swear by it.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 因为算法和与 ALTQ 配置相关的设置都相当复杂，且有多个可调参数，大多数 ALTQ 用户倾向于使用更简单的队列类型。然而，那些声称理解 HFSC 的人都对它推崇备至。
- en: Setting Up ALTQ
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置 ALTQ
- en: Enabling ALTQ may require some extra steps, depending on your choice of operating
    system.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 启用 ALTQ 可能需要一些额外步骤，具体取决于你选择的操作系统。
- en: ALTQ on OpenBSD
  id: totrans-162
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: OpenBSD 上的 ALTQ
- en: On OpenBSD 5.5, all supported queue disciplines are compiled into the GENERIC
    and GENERIC.MP kernels. Check that your OpenBSD version still supports ALTQ. If
    so, the only configuration you need to do involves editing your *pf.conf*.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在 OpenBSD 5.5 上，所有支持的队列规则都已经编译到 GENERIC 和 GENERIC.MP 内核中。请检查你的 OpenBSD 版本是否仍然支持
    ALTQ。如果是，唯一需要配置的就是编辑你的 *pf.conf* 文件。
- en: ALTQ on FreeBSD
  id: totrans-164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: FreeBSD 上的 ALTQ
- en: 'On FreeBSD, make sure that your kernel has ALTQ and the ALTQ queue discipline
    options compiled in. The default FreeBSD GENERIC kernel doesn’t have ALTQ options
    enabled, as you may have noticed from the messages you saw when running the */etc/rc.d/pf*
    script to enable PF. The relevant options are as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在 FreeBSD 上，确保你的内核已经编译了 ALTQ 和 ALTQ 队列规则选项。默认的 FreeBSD GENERIC 内核没有启用 ALTQ 选项，正如你在运行
    */etc/rc.d/pf* 脚本启用 PF 时看到的消息所示。相关选项如下：
- en: '[PRE25]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The `ALTQ` option is needed to enable ALTQ in the kernel, but on SMP systems,
    you also need the `ALTQ_NOPCC` option. Depending on which types of queues you’ll
    be using, you’ll need to enable at least one of these: `ALTQ_CBQ`, `ALTQ_PRIQ`,
    or `ALTQ_HFSC`. Finally, you can enable the congestion-avoidance techniques *random
    early detection (RED)* and *RED In/Out* with the `ALTQ_RED` and `ALTQ_RIO` options,
    respectively. (See the *FreeBSD Handbook* for information on how to compile and
    install a custom kernel with these options.)'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '`ALTQ` 选项是启用 ALTQ 内核功能所必需的，但在 SMP 系统中，你还需要启用 `ALTQ_NOPCC` 选项。根据你将使用的队列类型，你至少需要启用以下其中一个选项：`ALTQ_CBQ`、`ALTQ_PRIQ`
    或 `ALTQ_HFSC`。最后，你可以通过启用 `ALTQ_RED` 和 `ALTQ_RIO` 选项来启用拥塞避免技术 *随机早期检测（RED）* 和 *RED
    进/出*。 （有关如何编译和安装带有这些选项的自定义内核的信息，请参阅 *FreeBSD 手册*。）'
- en: ALTQ on NetBSD
  id: totrans-168
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: NetBSD 上的 ALTQ
- en: 'ALTQ was integrated into the NetBSD 4.0 PF implementation and is supported
    in NetBSD 4.0 and later releases. NetBSD’s default GENERIC kernel configuration
    doesn’t include the ALTQ-related options, but the GENERIC configuration file comes
    with all relevant options commented out for easy inclusion. The main kernel options
    are these:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ALTQ 已集成到 NetBSD 4.0 PF 实现中，并支持 NetBSD 4.0 及之后的版本。NetBSD 的默认 GENERIC 内核配置不包括
    ALTQ 相关选项，但 GENERIC 配置文件中将所有相关选项注释掉，方便用户加入。主要的内核选项如下：
- en: '[PRE26]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The `ALTQ` option is needed to enable ALTQ in the kernel. Depending on the
    types of queues you’ll be using, you must enable at least one of these: `ALTQ_CBQ`,
    `ALTQ_PRIQ`, or `ALTQ_HFSC`.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '`ALTQ`选项用于启用内核中的ALTQ。根据你将使用的队列类型，必须至少启用以下之一：`ALTQ_CBQ`、`ALTQ_PRIQ`或`ALTQ_HFSC`。'
- en: Using ALTQ requires you to compile PF into the kernel because the PF loadable
    module doesn’t support ALTQ functionality. (See the NetBSD PF documentation at
    *[http://www.netbsd.org/Documentation/network/pf.html](http://www.netbsd.org/Documentation/network/pf.html)*
    for the most up-to-date information.)
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 使用ALTQ要求将PF编译进内核，因为PF加载模块不支持ALTQ功能。（有关最新信息，请参阅NetBSD PF文档，* [http://www.netbsd.org/Documentation/network/pf.html](http://www.netbsd.org/Documentation/network/pf.html)
    *）
- en: Priority-Based Queues
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于优先级的队列
- en: The basic concept behind priority-based queues (`priq`) is fairly straightforward.
    Within the total bandwidth allocated to the main queue, only traffic priority
    matters. You assign queues a priority value in the range 0 through 15, where a
    higher value means that the queue’s requests for traffic are serviced sooner.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 基于优先级的队列（`priq`）的基本概念相当简单。在分配给主队列的总带宽内，只有流量优先级才是重要的。你为队列分配一个介于0到15之间的优先级值，其中较高的值意味着该队列的流量请求会更早被处理。
- en: Using ALTQ Priority Queues to Improve Performance
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用ALTQ优先级队列来提高性能
- en: 'Daniel Hartmeier discovered a simple yet effective way to improve the throughput
    for his home network by using ALTQ priority queues. Like many people, he had his
    home network on an asymmetric connection, with total usable bandwidth low enough
    that he wanted better bandwidth utilization. In addition, when the line was running
    at or near capacity, oddities started appearing. One symptom in particular seemed
    to suggest room for improvement: Incoming traffic (downloads, incoming mail, and
    such) slowed down disproportionately whenever outgoing traffic started—more than
    could be explained by measuring the raw amount of data transferred. It all came
    back to a basic feature of TCP.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Daniel Hartmeier发现了一种简单而有效的方式，通过使用ALTQ优先级队列来改善家庭网络的吞吐量。像许多人一样，他的家庭网络是基于非对称连接，总带宽较低，以至于他希望能更好地利用带宽。此外，当线路接近或达到最大容量时，奇怪的现象开始出现。特别有一个症状似乎表明有改进的空间：每当出站流量开始时，进入的流量（下载、邮件等）会不成比例地变慢——这一现象无法仅通过测量传输的数据量来解释。所有这一切都回到了TCP的一个基本特性。
- en: When a TCP packet is sent, the sender expects acknowledgment (in the form of
    an ACK packet) from the receiver and will wait a specified time for it to arrive.
    If the ACK doesn’t arrive within that time, the sender assumes that the packet
    hasn’t been received and resends it. And because in a default setup, packets are
    serviced sequentially by the interface as they arrive, ACK packets, with essentially
    no data payload, end up waiting in line while the larger data packets are transferred.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个TCP数据包被发送时，发送方期望接收方的确认（ACK数据包），并会在指定时间内等待其到达。如果ACK在规定时间内没有到达，发送方假设数据包没有被接收并重新发送。由于在默认设置中，数据包是按到达顺序依次由接口处理的，因此ACK数据包几乎没有数据负载，结果会在较大的数据包传输时排队等待。
- en: If ACK packets could slip in between the larger data packets, the result would
    be more efficient use of available bandwidth. The simplest practical way to implement
    such a system with ALTQ is to set up two queues with different priorities and
    integrate them into the rule set. Here are the relevant parts of the rule set.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 如果ACK数据包能够在较大的数据包之间穿插，那么结果将是更高效地利用可用带宽。使用ALTQ实现这样一个系统的最简单实际方法是设置两个具有不同优先级的队列，并将它们集成到规则集中。以下是规则集的相关部分。
- en: '[PRE27]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Here, the priority-based queue is set up on the external interface with two
    subordinate queues. The first subqueue, `q_pri`, has a high-priority value of
    7; the other subqueue, `q_def`, has a significantly lower-priority value of 1.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，基于优先级的队列在外部接口上设置，并且有两个子队列。第一个子队列`q_pri`具有优先级值7；另一个子队列`q_def`的优先级值为1，明显较低。
- en: This seemingly simple rule set works by exploiting how ALTQ treats queues with
    different priorities. Once a connection is set up, ALTQ inspects each packet’s
    ToS field. ACK packets have the ToS delay bit set to low, which indicates that
    the sender wanted the speediest delivery possible. When ALTQ sees a low-delay
    packet and queues of differing priorities are available, it assigns the packet
    to the higher-priority queue. This means that the ACK packets skip ahead of the
    lower-priority queue and are delivered more quickly, which in turn means that
    data packets are serviced more quickly. The net result is better performance than
    a pure FIFO configuration with the same hardware and available bandwidth. (Daniel
    Hartmeier’s article about this version of his setup, cited previously, contains
    a more detailed analysis.)
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这个看似简单的规则集通过利用ALTQ如何处理不同优先级队列的方式来工作。一旦连接建立，ALTQ检查每个数据包的ToS字段。ACK数据包的ToS延迟位设置为低，这表示发送方希望尽可能快速地传输。当ALTQ看到低延迟的数据包且有不同优先级的队列可用时，它会将该数据包分配给优先级更高的队列。这意味着ACK数据包会跳过低优先级队列并更快地传输，从而使得数据包也能更快地得到服务。最终结果是，使用相同硬件和带宽的情况下，比纯FIFO配置的性能更好。（丹尼尔·哈特迈尔关于他这种配置的文章中有更详细的分析。）
- en: Using a match Rule for Queue Assignment
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用队列分配的匹配规则
- en: In the previous example, the rule set was constructed the traditional way, with
    the queue assignment as part of the `pass` rules. However, this isn’t the only
    way to do queue assignment. When you use `match` rules (available in OpenBSD 4.6
    and later), it’s incredibly easy to retrofit this simple priority-queuing regime
    onto an existing rule set.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的示例中，规则集是传统方式构建的，队列分配作为`pass`规则的一部分。然而，这并不是唯一的队列分配方式。当你使用`match`规则（在OpenBSD
    4.6及以后版本中可用）时，简单的优先级排队机制可以非常容易地应用到现有规则集上。
- en: If you worked through the examples in [Chapter 3](ch03.html "Chapter 3. Into
    the Real World") and [Chapter 4](ch04.html "Chapter 4. Wireless Networks Made
    Easy"), your rule set probably has a `match` rule that applies `nat-to` on your
    outgoing traffic. To introduce priority-based queuing to your rule set, you first
    add the queue definitions and make some minor adjustments to your outgoing `match`
    rule.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经完成了[第3章](ch03.html "第3章. 进入现实世界")和[第4章](ch04.html "第4章. 无线网络轻松入门")中的示例，那么你的规则集可能已经有一个`match`规则，用于处理出站流量的`nat-to`。要在规则集中引入基于优先级的排队，你需要首先添加队列定义，并对出站的`match`规则做一些小调整。
- en: Start with the queue definition from the preceding example and adjust the total
    bandwidth to local conditions, as shown in here.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面示例中的队列定义开始，根据本地情况调整总带宽，如下所示。
- en: '[PRE28]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This gives the queues whatever bandwidth allocation you define with the `ext_bw`
    macro.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这会根据你使用`ext_bw`宏定义的带宽分配队列。
- en: 'The simplest and quickest way to integrate the queues into your rule set is
    to edit your outgoing `match` rule to read something like this:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 将队列集成到规则集中的最简单快捷方式是编辑你的出站`match`规则，使其类似于这样：
- en: '[PRE29]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Reload your rule set, and the priority-queuing regime is applied to all traffic
    that’s initiated from your local network.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 重新加载你的规则集，优先级排队机制会应用到所有从本地网络发起的流量。
- en: You can use the `systat` command to get a live view of how traffic is assigned
    to your queues.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用`systat`命令实时查看流量如何分配到你的队列中。
- en: '[PRE30]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This will give you a live display that looks something like this:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这会给你一个实时显示，类似于以下内容：
- en: '[PRE31]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Looking at the numbers in the `PKTS` (packets) and `BYTES` columns, you see
    a clear indication that the queuing is working as intended.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 查看`PKTS`（数据包）和`BYTES`（字节）列中的数字，你会明显看到排队机制按预期工作。
- en: The `q_pri` queue has processed a rather large number of packets in relation
    to the amount of data, just as we expected. The ACK packets don’t take up a lot
    of space. On the other hand, the traffic assigned to the `q_def` queue has more
    data in each packet, and the numbers show essentially the reverse packet numbers–to–data
    size ratio as in to the `q_pri` queue.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`q_pri`队列处理了相当多的包与数据量的比率，正如我们预期的那样。ACK数据包占用的空间不大。另一方面，分配给`q_def`队列的流量在每个数据包中包含更多数据，数字显示了与`q_pri`队列相比的反向包与数据大小比率。'
- en: Note
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '*`systat` is a rather capable program on all BSDs, and the OpenBSD version
    offers several views that are relevant to PF and that aren’t found in the `systat`
    variants on the other systems as of this writing. We’ll be looking at `systat`
    again in the next chapter. In the meantime, read the man pages and play with the
    program. It’s a very useful tool for getting to know your system.*'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '*`systat` 是一个在所有 BSD 系统上都非常有用的程序，OpenBSD 版本提供了与 PF 相关的多个视图，而这些视图在其他系统的 `systat`
    版本中并没有，直到写这本书时仍然如此。我们将在下一章再次讨论 `systat`。在此期间，阅读手册并多尝试使用这个程序，它是了解系统的一个非常有用的工具。*'
- en: Class-Based Bandwidth Allocation for Small Networks
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 面向小型网络的基于类别的带宽分配
- en: Maximizing network performance generally feels nice. However, you may find that
    your network has other needs. For example, it might be important for some traffic—such
    as mail and other vital services—to have a baseline amount of bandwidth available
    at all times, while other services—peer-to-peer file sharing comes to mind—shouldn’t
    be allowed to consume more than a certain amount. To address these kinds of requirements
    or concerns, ALTQ offers the class-based queue (`cbq`) discipline with a slightly
    larger set of options.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 最大化网络性能通常感觉很好。然而，你可能会发现网络还有其他需求。例如，某些流量（如邮件和其他重要服务）可能需要始终保持一定的带宽基准，而其他服务（比如点对点文件共享）则不应允许消耗超过某个限制的带宽。为了应对这些需求或问题，ALTQ
    提供了带有更多选项的基于类别的队列（`cbq`）规则。
- en: To illustrate how to use `cbq`, we’ll build on the rule sets from previous chapters
    within a small local network. We want to let the users on the local network connect
    to a predefined set of services outside their own network and let users from outside
    the local network access a Web server and an FTP server somewhere on the local
    network.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明如何使用`cbq`，我们将在一个小型本地网络中基于前几章的规则集进行构建。我们希望允许本地网络中的用户连接到自己网络之外的预定义服务集，并允许外部网络的用户访问本地网络中的某个
    Web 服务器和 FTP 服务器。
- en: Queue Definition
  id: totrans-202
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 队列定义
- en: All queues are set up on the external, Internet-facing interface. This approach
    makes sense mainly because bandwidth is more likely to be limited on the external
    link than on the local network. In principle, however, allocating queues and running
    traffic shaping can be done on any network interface. The example setup shown
    here includes a `cbq` queue for a total bandwidth of 2Mb with six subqueues.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 所有队列都设置在面向外部的互联网接口上。这样做的主要原因是外部链路上的带宽往往比本地网络更容易受到限制。然而，原则上，队列的分配和流量整形可以在任何网络接口上进行。这里展示的示例设置包括一个总带宽为
    2Mb 的 `cbq` 队列，带有六个子队列。
- en: '[PRE32]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The subqueue `main` has 18 percent of the bandwidth and is designated as the
    default queue. This means any traffic that matches a `pass` rule but isn’t explicitly
    assigned to some other queue ends up here. The `borrow` and `red` keywords mean
    that the queue may “borrow” bandwidth from its parent queue, while the system
    attempts to avoid congestion by applying the RED algorithm.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 子队列 `main` 占有 18% 的带宽，并被指定为默认队列。这意味着任何符合 `pass` 规则但没有明确分配到其他队列的流量都会进入此队列。`borrow`
    和 `red` 关键字表示该队列可以从其父队列“借用”带宽，而系统会尝试通过应用 RED 算法来避免拥塞。
- en: The other queues follow more or less the same pattern up to the subqueue `ssh`,
    which itself has two subqueues with separate priorities. Here, we see a variation
    on the ACK priority example. Bulk SSH transfers, typically SCP file transfers,
    are transmitted with a ToS indicating throughput, while interactive SSH traffic
    has the ToS flag set to low delay and skips ahead of the bulk transfers. The interactive
    traffic is likely to be less bandwidth consuming and gets a smaller share of the
    bandwidth, but it receives preferential treatment because of the higher-priority
    value assigned to it. This scheme also helps the speed of SCP file transfers because
    the ACK packets for the SCP transfers will be assigned to the higher-priority
    subqueue.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 其他队列基本遵循相同的模式，直到子队列 `ssh`，该子队列本身有两个子队列，并具有独立的优先级。在这里，我们看到了一个基于 ACK 优先级的变种示例。大批量
    SSH 传输，通常是 SCP 文件传输，使用标明吞吐量的 ToS 进行传输，而交互式 SSH 流量则将 ToS 标志设置为低延迟，并优先于大批量传输。交互式流量通常消耗的带宽较少，因此得到较少的带宽份额，但由于赋予其更高的优先级，它会得到优待。这种方案也有助于提升
    SCP 文件传输的速度，因为 SCP 传输的 ACK 包将被分配到更高优先级的子队列。
- en: Finally, we have the `icmp` queue, which is reserved for the remaining 2 percent
    of the bandwidth from the top level. This guarantees a minimum amount of bandwidth
    for ICMP traffic that we want to pass but that doesn’t match the criteria for
    being assigned to the other queues.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有 `icmp` 队列，它被保留用于来自顶级的剩余 2% 带宽。这保证了一个最低的带宽分配给 ICMP 流量，确保其可以传递，但这些流量不符合其他队列的分配标准。
- en: Rule Set
  id: totrans-208
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 规则集
- en: 'To make it all happen, we use these `pass` rules, which indicate which traffic
    is assigned to the queues and their criteria:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这一切得以实现，我们使用这些 `pass` 规则，指示哪些流量被分配到相应的队列中，以及它们的分配标准：
- en: '[PRE33]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The rules for `ssh`, `ftp`, `www`, `udp`, and `icmp` assign traffic to their
    respective queues. The last catchall rule passes all other traffic from the local
    network, lumping it into the default `main` queue.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '`ssh`、`ftp`、`www`、`udp` 和 `icmp` 的规则将流量分配到各自的队列中。最后一个捕获规则将来自本地网络的所有其他流量传递到默认的
    `main` 队列。'
- en: A Basic HFSC Traffic Shaper
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个基础的 HFSC 流量整形器
- en: 'The simple schedulers we have looked at so far can make for efficient setups,
    but network admins with traffic-shaping ambitions tend to look for a little more
    flexibility than can be found in the pure-priority-based queues or the simple
    class-based variety. The HFSC queuing algorithm (`hfsc` in *pf.conf* terminology)
    offers flexible bandwidth allocation, guaranteed lower and upper bounds for bandwidth
    available to each queue, and variable allocations over time, and it only starts
    shaping when there’s an actual need. However, the added flexibility comes at a
    price: The setup is a tad more complex than the other ALTQ types, and tuning your
    setup for an optimal result can be quite an interesting process.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止我们所看到的简单调度器可以提供高效的配置，但有流量整形需求的网络管理员通常会寻找比纯优先级队列或简单的基于类队列更具灵活性的方案。HFSC 排队算法（在
    *pf.conf* 术语中为 `hfsc`）提供了灵活的带宽分配、为每个队列提供带宽的上下限保证以及随时间变化的动态分配，并且只有在确实需要时才开始进行流量整形。然而，增加的灵活性是有代价的：其设置比其他
    ALTQ 类型稍微复杂一些，调整配置以获得最佳结果可能是一个相当有趣的过程。
- en: Queue Definition
  id: totrans-214
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 队列定义
- en: 'First, working from the same configuration we altered slightly earlier, we
    insert this queue definition early in the *pf.conf* file:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在我们稍早更改过的相同配置基础上，我们将此队列定义提早放入 *pf.conf* 文件中：
- en: '[PRE34]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The `hfsc` queue definitions take slightly different parameters than the simpler
    disciplines. We start off with this rather small hierarchy by splitting the top-level
    queue into two. At the next level, we subdivide the `main` queue into several
    subqueues, each with a defined priority. All the subqueues have a `realtime` value
    set—the guaranteed minimum bandwidth allocated to the queue. The optional `upperlimit`
    sets a hard upper limit on the queue’s allocation. The `linkshare` parameter sets
    the allocation the queue will have available when it’s backlogged—that is, when
    it’s started to eat into its `qlimit` allocation.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '`hfsc` 队列定义使用的参数与简单调度器略有不同。我们从这个相对较小的层级开始，将顶级队列拆分成两个。在下一级，我们将 `main` 队列细分成几个子队列，每个子队列都有一个定义的优先级。所有子队列都设置了一个
    `realtime` 值——分配给队列的最低带宽保证。可选的 `upperlimit` 设置队列分配的硬性上限。`linkshare` 参数设置当队列积压时，队列可用的分配带宽——即当队列开始占用其
    `qlimit` 分配时。'
- en: 'In case of congestion, each queue by default has a pool of 50 slots, the queue
    limit (`qlimit`), to keep packets around when they can’t be transmitted immediately.
    In this example, the top-level queues `main` and `spamd` both have larger-than-default
    pools set by their `qlimit` setting: `100` for `main` and `300` for `spamd`. Cranking
    up queue sizes here means we’re a little less likely to drop packets when the
    traffic approaches the set limits, but it also means that when the traffic shaping
    kicks in, we’ll see increased latency for connections that end up in these larger
    than default pools.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在发生拥塞的情况下，每个队列默认有一个 50 个槽位的池（队列限制 `qlimit`），用于在数据包无法立即传输时将其保留。在此示例中，顶级队列 `main`
    和 `spamd` 都通过各自的 `qlimit` 设置设定了大于默认值的池：`main` 为 `100`，`spamd` 为 `300`。增加队列大小意味着当流量接近设置的限制时，我们丢包的可能性较小，但这也意味着当流量整形启动时，进入这些大于默认池的连接会出现更高的延迟。
- en: 'The queue hierarchy here uses two familiar tricks to make efficient use of
    available bandwidth:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 此队列层级使用了两种常见的技巧来有效利用可用带宽：
- en: It uses a variation of the high- and low-priority mix demonstrated in the earlier
    pure-priority example.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使用了先前纯优先级示例中展示的高优先级与低优先级混合的变种。
- en: We speed up almost all other traffic (and most certainly the Web traffic that
    appears to be the main priority here) by allocating a small but guaranteed portion
    of bandwidth for name service lookups. For the `q_dns` queue, we set up the `realtime`
    value with a time limit—after `3000` milliseconds, the `realtime` allocation goes
    down to `12Kb`. This can be useful to speed connections that transfer most of
    their payload during the early phases.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们通过为名称服务查找分配一个小而保证的带宽部分，加速几乎所有其他流量（特别是这里看似最优先的 Web 流量）。对于`q_dns`队列，我们设置了`realtime`值，并设定了时间限制——在`3000`毫秒后，`realtime`分配降至`12Kb`。这对于加速在初期阶段传输大部分负载的连接非常有用。
- en: Rule Set
  id: totrans-222
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 规则集
- en: Next, we tie the newly created queues into the rule set. If you have a filtering
    regime in place already, which we’ll assume you do, the tie-in becomes amazingly
    simple, accomplished by adding a few `match` rules.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将新创建的队列添加到规则集中。如果你已经有一个过滤机制（假设你已经有了），那么连接起来变得非常简单，只需要添加一些`match`规则。
- en: '[PRE35]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Here, the `match` rules once again do the ACK packet speedup trick with the
    high- and low-priority queue assignment, just as you saw earlier in the pure-priority-based
    system. The only exception is when we assign traffic to our lowest-priority queue,
    where we really don’t care to have any speedup at all.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`match`规则再次通过高优先级和低优先级队列分配的方式加速ACK数据包，正如你在纯优先级系统中看到的那样。唯一的例外是当我们将流量分配给最低优先级队列时，在这种情况下，我们真的不在乎是否加速。
- en: '[PRE36]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: This rule is intended to slow down the spammers a little more on their way to
    our `spamd`. With a hierarchical queue system in place, `systat queues` shows
    the queues and their traffic as a hierarchy, too.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这个规则的目的是让垃圾邮件发送者在到达我们的`spamd`时稍微慢一点。通过设置分层队列系统，`systat queues`也会将队列及其流量以层次结构的形式显示出来。
- en: '[PRE37]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The root queue is shown as attached to the physical interface—as `nfe0` and
    `root_nfe0`, in this case. `main` and its subqueues—`q_pri`, `q_def`, `q_web`,
    and `q_dns`—are shown with their bandwidth allocations and number of bytes and
    packets passed. The `DROP_P` and `DROP_B` columns are where number of packets
    and bytes dropped, respectively, would appear if we had been forced to drop packets
    at this stage. The final two columns show live updates of packets per second and
    bytes per second.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 根队列显示为附加到物理接口——在本例中为`nfe0`和`root_nfe0`。`main`及其子队列——`q_pri`、`q_def`、`q_web`和`q_dns`——显示了它们的带宽分配及通过的字节和数据包数量。`DROP_P`和`DROP_B`列会显示如果我们在此阶段被迫丢弃数据包时，丢弃的数据包数和字节数。最后两列显示了每秒数据包数和每秒字节数的实时更新。
- en: Queuing for Servers in a DMZ
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 DMZ 中排队等待服务器
- en: In [Chapter 5](ch05.html "Chapter 5. Bigger or Trickier Networks"), we set up
    a network with a single gateway but with all externally visible services configured
    on a separate DMZ network. That way, all traffic to the servers from both the
    Internet and the internal network had to pass through the gateway (see [Figure 7-1](ch07.html#network_with_dmz
    "Figure 7-1. Network with DMZ")).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](ch05.html "第5章 更大或更复杂的网络")中，我们配置了一个带有单一网关的网络，但所有对外可见的服务都配置在一个单独的 DMZ
    网络中。这样，从互联网和内部网络到服务器的所有流量都必须通过网关（见[图7-1](ch07.html#network_with_dmz "图7-1 DMZ
    网络")）。
- en: With the rule set from [Chapter 5](ch05.html "Chapter 5. Bigger or Trickier
    Networks") as our starting point, we’ll add some queuing in order to optimize
    our network resources. The physical and logical layout of the network will not
    change. The most likely bottleneck for this network is the bandwidth for the connection
    between the gateway’s external interface and the Internet at large. The bandwidth
    elsewhere in our setup isn’t infinite, of course, but the available bandwidth
    on any interface in the local network is likely to be less of a limiting factor
    than the bandwidth actually available for communication with the outside world.
    For services to be available with the best possible performance, we need to set
    up the queues so the bandwidth available at the site is made available to the
    traffic we want to allow.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 以[第5章](ch05.html "第5章 更大或更复杂的网络")中的规则集为起点，我们将添加一些排队策略，以优化我们的网络资源。网络的物理和逻辑布局不会改变。这个网络最可能的瓶颈是网关外部接口与互联网之间的连接带宽。虽然我们设置中的其他带宽并非无限，但任何本地网络接口上的可用带宽通常不会像与外部世界通信时实际可用的带宽那样成为限制因素。为了确保服务在最佳性能下可用，我们需要设置队列，使站点上的可用带宽分配给我们希望允许的流量。
- en: In our example, it’s likely that the interface bandwidth on the DMZ interface
    is either 100Mb or 1Gb, while the *actual available bandwidth* for connections
    from outside the local network is considerably smaller. This consideration shows
    up in our queue definitions, where you clearly see that the bandwidth available
    for external traffic is the main limitation in the queue setup.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，DMZ 接口的接口带宽可能是 100Mb 或 1Gb，而来自本地网络外部的连接的*实际可用带宽*要小得多。这个考虑因素在我们的队列定义中体现得非常明显，你可以清楚地看到可用于外部流量的带宽是队列设置中的主要限制。
- en: '[PRE38]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Notice that the `total_ext` bandwidth limitation determines the allocation for
    all queues where the bandwidth for external connections is available. In order
    to use the new queuing infrastructure, we need to make some changes to the filtering
    rules, too. Keep in mind that any traffic you don’t explicitly assign to a specific
    queue is assigned to the default queue for the interface. Thus, it’s important
    to tune your filtering rules as well as your queue definitions to the actual traffic
    in your network.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`total_ext`带宽限制决定了所有可以进行外部连接的队列的分配。为了使用新的队列基础设施，我们也需要对过滤规则进行一些修改。请记住，任何没有明确分配到特定队列的流量都会被分配到接口的默认队列。因此，调整过滤规则和队列定义，以适应网络中的实际流量是非常重要的。
- en: 'With queue assignment, the main part of the filtering rules could end up looking
    like this:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 使用队列分配，过滤规则的主要部分可能最终如下所示：
- en: '[PRE39]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Notice that only traffic that will pass either the DMZ interface or the external
    interface is assigned to queues. In this configuration, with no externally accessible
    services on the internal network, queuing on the internal interface wouldn’t make
    much sense because it’s likely the part of our network with the least restrictions
    on available bandwidth.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，只有通过 DMZ 接口或外部接口的流量才会被分配到队列中。在这种配置下，如果内部网络没有外部可访问的服务，那么在内部接口上进行排队就没有太大意义，因为它很可能是我们网络中对带宽限制最少的部分。
- en: Using ALTQ to Handle Unwanted Traffic
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 ALTQ 来处理不需要的流量
- en: So far, we’ve focused on queuing as a method to make sure specific kinds of
    traffic are let through as efficiently as possible given the conditions that exist
    in and around your network. Now, we’ll look at two examples that present a slightly
    different approach to identify and handle unwanted traffic in order to demonstrate
    some queuing-related tricks you can use to keep miscreants in line.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经专注于将队列作为一种方法，确保在现有网络条件下，以尽可能高效的方式通过特定类型的流量。接下来，我们将看两个示例，展示稍微不同的方法来识别和处理不需要的流量，并演示一些与队列相关的技巧，帮助你保持恶意流量的控制。
- en: Overloading to a Tiny Queue
  id: totrans-241
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 超载到一个小型队列
- en: Think back to [Turning Away the Brutes](ch06.html#turning_away_the_brutes "Turning
    Away the Brutes"), where we used a combination of state-tracking options and `overload`
    rules to fill up a table of addresses for special treatment. The special treatment
    we demonstrated in [Chapter 6](ch06.html "Chapter 6. Turning the Tables for Proactive
    Defense") was to cut all connections, but it’s equally possible to assign `overload`
    traffic to a specific queue instead.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下[拒绝野蛮人](ch06.html#turning_away_the_brutes "拒绝野蛮人")，我们使用了状态跟踪选项和`overload`规则的组合，填充了一个地址表以便进行特殊处理。我们在[第6章](ch06.html
    "第6章. 反击主动防御")中展示的特殊处理是切断所有连接，但同样也可以将`overload`流量分配到一个特定的队列中。
- en: Consider this rule from our class-based bandwidth example in [Class-Based Bandwidth
    Allocation for Small Networks](ch07.html#class-based_bandwidth_allocation_for_sma
    "Class-Based Bandwidth Allocation for Small Networks").
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考我们在[基于类别的带宽分配示例](ch07.html#class-based_bandwidth_allocation_for_sma "基于类别的带宽分配示例")中的规则。
- en: '[PRE40]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We could add state-tracking options, as shown in here.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以添加状态跟踪选项，如这里所示。
- en: '[PRE41]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Then, we could make one of the queues slightly smaller.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以将其中一个队列稍微设置得小一些。
- en: '[PRE42]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Next, we could assign traffic from miscreants to the small-bandwidth queue with
    the following rule.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以使用以下规则将恶意流量分配到这个小带宽队列。
- en: '[PRE43]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: It might also be useful to supplement rules like these with table-entry expiry,
    as described in [Tidying Your Tables with pfctl](ch06.html#tidying_your_tables_with_pfctl
    "Tidying Your Tables with pfctl").
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 补充类似这些规则的表项过期机制也可能很有用，正如在[使用 pfctl 整理表项](ch06.html#tidying_your_tables_with_pfctl
    "使用 pfctl 整理表项")中所描述的那样。
- en: Queue Assignments Based on Operating System Fingerprint
  id: totrans-252
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于操作系统指纹的队列分配
- en: '[Chapter 6](ch06.html "Chapter 6. Turning the Tables for Proactive Defense")
    covered several ways to use `spamd` to cut down on spam. If running `spamd` isn’t
    an option in your environment, you can use a queue and rule set based on the common
    knowledge that machines that send spam are likely to run a particular operating
    system.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '[第6章](ch06.html "第6章. 通过主动防御扭转局面")介绍了几种使用`spamd`减少垃圾邮件的方法。如果在你的环境中不能运行`spamd`，你可以使用基于常识的队列和规则集，因为发送垃圾邮件的机器很可能运行某些特定的操作系统。'
- en: PF has a fairly reliable operating system fingerprinting mechanism, which detects
    the operating system at the other end of a network connection based on characteristics
    of the initial SYN packets at connection setup. The following may be a simple
    substitute for `spamd` if you’ve determined that legitimate mail is highly unlikely
    to be delivered from systems that run that particular operating system.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: PF有一个相当可靠的操作系统指纹识别机制，可以根据连接建立时初始SYN包的特征，检测网络连接另一端的操作系统。如果你已经确定合法邮件不太可能来自运行该特定操作系统的系统，以下方法可能是`spamd`的简单替代方案。
- en: '[PRE44]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Here, email traffic originating from hosts that run a particular operating system
    get no more than 1KB of your bandwidth, with no borrowing.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，来自运行特定操作系统的主机的电子邮件流量不会超过1KB带宽，并且没有带宽借用。
- en: 'Conclusion: Traffic Shaping for Fun, and Perhaps Even Profit'
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论：流量整形的乐趣，甚至可能带来利润
- en: This chapter has dealt with traffic-shaping techniques that can make your traffic
    move faster, or at least make preferred traffic pass more efficiently and according
    to your specifications. By now you should have at least a basic understanding
    of traffic-shaping concepts and how they apply to the traffic-shaping tool set
    you’ll be using on your systems.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了可以让流量传输更快的流量整形技术，或者至少可以按照你的规格，使优先流量更高效地传输。到现在为止，你应该至少对流量整形的基本概念以及如何在系统上使用流量整形工具有了初步的了解。
- en: I hope that the somewhat stylized (but functional) examples in this chapter
    have given you a taste of what’s possible with traffic shaping and that the material
    has inspired you to play with some of your own ideas of how you can use the traffic-shaping
    tools in your networks. If you pay attention to your network traffic and the underlying
    needs it expresses (see [Chapter 9](ch09.html "Chapter 9. Logging, Monitoring,
    and Statistics") and [Chapter 10](ch10.html "Chapter 10. Getting Your Setup Just
    Right") for more on studying network traffic in detail), you can use the traffic-shaping
    tools to improve the way your network serves its users. With a bit of luck, your
    users will appreciate your efforts and you may even enjoy the experience.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望本章中那些稍显风格化（但功能性的）示例，能让你对流量整形的可能性感到一些兴趣，并激发你自己思考如何在网络中使用流量整形工具。如果你注意观察网络流量和它所表达的潜在需求（更多内容请参见[第9章](ch09.html
    "第9章. 日志记录、监控与统计")和[第10章](ch10.html "第10章. 让你的设置完美无缺")，了解如何详细研究网络流量），你可以利用流量整形工具来提升你的网络为用户提供服务的方式。幸运的话，用户会感谢你的努力，甚至你自己也可能会享受这个过程。
- en: '* * *'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ^([[39](#ch07fn01)]) Daniel Hartmeier, one of the original PF developers, wrote
    a nice article about this problem, which is available at *[http://www.benzedrine.cx/ackpri.html](http://www.benzedrine.cx/ackpri.html)*.
    Daniel’s explanations use the older ALTQ priority queues syntax but include data
    that clearly illustrates the effect of assigning two different priorities to help
    ACKs along.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: ^([[39](#ch07fn01)]) 丹尼尔·哈特迈尔（Daniel Hartmeier），PF的原始开发者之一，写了一篇关于这个问题的精彩文章，文章可以在
    *[http://www.benzedrine.cx/ackpri.html](http://www.benzedrine.cx/ackpri.html)*
    查阅。丹尼尔的解释使用了旧版ALTQ优先级队列语法，但包含的数据清楚地展示了为帮助ACK传递分配两个不同优先级的效果。
- en: ^([[40](#ch07fn01a)]) This really dates the book, I know. In a few years, these
    numbers will seem quaint.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: ^([[40](#ch07fn01a)]) 我知道这让书籍显得有些过时。几年后，这些数字看起来会显得有些古老。
- en: ^([[41](#ch07fn02)]) The original research on ALTQ was presented in a paper
    for the USENIX 1999 conference. You can read Kenjiro Cho’s paper “Managing Traffic
    with ALTQ” online at *[http://www.usenix.org/publications/library/proceedings/usenix99/cho.html](http://www.usenix.org/publications/library/proceedings/usenix99/cho.html)*.
    The code turned up in OpenBSD soon after through the efforts of Cho and Chris
    Cappucio.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: ^([[41](#ch07fn02)]) ALTQ的原始研究在USENIX 1999年会议上以论文形式发表。你可以在线阅读Kenjiro Cho的论文《使用ALTQ管理流量》，地址为
    *[http://www.usenix.org/publications/library/proceedings/usenix99/cho.html](http://www.usenix.org/publications/library/proceedings/usenix99/cho.html)*。该代码随后通过Cho和Chris
    Cappucio的努力出现在OpenBSD中。
