- en: Chapter 7. Traffic Shaping with Queues and Priorities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Traffic Shaping with Queues and Priorities](httpatomoreillycomsourcenostarchimages2127149.png.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In this chapter, we look at how to use traffic shaping to allocate bandwidth
    resources efficiently and according to a specified policy. If the term *traffic
    shaping* seems unfamiliar, rest assured it means what you think it means: that
    you’ll be altering the way your network allocates resources in order to satisfy
    the requirements of your users and their applications. With a proper understanding
    of your network traffic and the applications and users that generate it, you can,
    in fact, go quite a bit of distance toward “bigger, better, faster, more” just
    by optimizing your network for the traffic that’s actually supposed to pass there.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A small but powerful arsenal of traffic-shaping tools is at your disposal;
    all of them work by introducing nondefault behavior into your network setup to
    bend the realities of your network according to your wishes. Traffic shaping for
    PF contexts currently comes in two flavors: the once experimental *ALTQ* (short
    for *alternate queuing*) framework, now considered old-style after some 15 years
    of service, and the newer OpenBSD *priorities and queuing* system introduced in
    OpenBSD 5.5.'
  prefs: []
  type: TYPE_NORMAL
- en: In the first part of the chapter, we introduce traffic shaping by looking at
    the features of the new OpenBSD priority and queuing system. If you’re about to
    set up on OpenBSD 5.5 or newer, you can jump right in, starting with the next
    section, [Always-On Priority and Queues for Traffic Shaping](ch07.html#always-on_priority_and_queues_for_traffi
    "Always-On Priority and Queues for Traffic Shaping"). This is also where the main
    traffic-shaping concepts are introduced with examples.
  prefs: []
  type: TYPE_NORMAL
- en: On OpenBSD 5.4 and earlier as well as other BSDs where the PF code wasn’t current
    with OpenBSD 5.5, traffic shaping was the domain of the ALTQ system. On OpenBSD,
    ALTQ was removed after one transitional release, leaving only the newer traffic-shaping
    system in place from OpenBSD 5.6 onward. If you’re interested in converting an
    existing ALTQ setup to the new system, you’ll most likely find [Transitioning
    from ALTQ to Priorities and Queues](ch07.html#transitioning_from_altq_to_priorities_an
    "Transitioning from ALTQ to Priorities and Queues") useful; this section highlights
    the differences between the older ALTQ system and the new system.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re working with an operating system where the queues system introduced
    in OpenBSD 5.5 isn’t yet available, you’ll want to study the ALTQ traffic-shaping
    subsystem, which is described in [Directing Traffic with ALTQ](ch07.html#directing_traffic_with_altq
    "Directing Traffic with ALTQ"). If you’re learning traffic-shaping concepts and
    want to apply them to an ALTQ setup, please read the first part of this chapter
    before diving into ALTQ-specific configuration details.
  prefs: []
  type: TYPE_NORMAL
- en: Always-On Priority and Queues for Traffic Shaping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Managing your bandwidth has a lot in common with balancing your checkbook or
    handling other resources that are either scarce or available in finite quantities.
    The resource is available in a constant supply with hard upper limits, and you
    need to allocate the resource with maximum *efficiency*, according to the *priorities*
    set out in your *policy* or *specification*.
  prefs: []
  type: TYPE_NORMAL
- en: OpenBSD 5.5 and newer offers several different options for managing your bandwidth
    resources via classification mechanisms in our PF rule sets. We’ll take a look
    at what you can do with pure *traffic prioritization* first and then move on to
    how to subdivide your bandwidth resources by allocating defined subsets of your
    traffic to *queues*.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*The always-on priorities were introduced as a teaser of sorts in OpenBSD 5.0\.
    After several years in development and testing, the new queuing system was finally
    committed in time to be included in OpenBSD 5.5, which was released on May 1,
    2014\. If you’re starting your traffic shaping from scratch on OpenBSD 5.5 or
    newer or you’re considering doing so, this section is the right place to start.
    If you’re upgrading from an earlier OpenBSD version or transitioning from another
    ALTQ system to a recent OpenBSD, you’ll most likely find the following section,
    [Transitioning from ALTQ to Priorities and Queues](ch07.html#transitioning_from_altq_to_priorities_an
    "Transitioning from ALTQ to Priorities and Queues"), useful.*'
  prefs: []
  type: TYPE_NORMAL
- en: Shaping by Setting Traffic Priorities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you’re mainly interested in pushing certain kinds of traffic ahead of others,
    you may be able to achieve what you want by simply setting priorities: assigning
    a higher priority to some items so that they receive attention before others.'
  prefs: []
  type: TYPE_NORMAL
- en: The prio Priority Scheme
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Starting with OpenBSD 5.0, a priority scheme for classifying network traffic
    on a per-rule basis is available. The range of priorities is from 0 to 7, where
    0 is lowest priority. Items assigned priority 7 will skip ahead of everything
    else, and the default value 3 is automatically assigned for most kinds of traffic.
    The priority scheme, which you’ll most often hear referred to as `prio` after
    the PF syntax keyword, is always enabled, and you can tweak your traffic by setting
    priorities via your `match` or `pass` rules.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, to speed up your outgoing SSH traffic to the max, you could put
    a rule like this in your configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Then your SSH traffic would be served before anything else.
  prefs: []
  type: TYPE_NORMAL
- en: You could then examine the rest of your rule set and decide what traffic is
    more or less important, what you would like always to reach its destination, and
    what parts of your traffic you feel matter less.
  prefs: []
  type: TYPE_NORMAL
- en: 'To push your Web traffic ahead of everything else and bump up the priority
    for network time and name services, you could amend your configuration with rules
    like these:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Or if you have a rule set that already includes rules that match criteria other
    than just the port, you could achieve much the same effect by writing your priority
    traffic shaping as `match` rules instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In some networks, time-sensitive traffic, like Voice over Internet Protocol
    (VoIP), may need special treatment. For VoIP, a priority setup like this may improve
    phone conversation quality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'But do check your VoIP application’s documentation for information on what
    specific ports it uses. In any case, using `match` rules like these can have a
    positive effect on your configuration in other ways, too: You can use `match`
    rules like the ones in the examples here to separate filtering decisions—such
    as passing, blocking, or redirecting—from traffic-shaping decisions, and with
    that separation in place, you’re likely to end up with a more readable and maintainable
    configuration.'
  prefs: []
  type: TYPE_NORMAL
- en: It’s also worth noting that parts of the OpenBSD network stack set default priorities
    for certain types of traffic that the developers decided was essential to a functional
    network. If you don’t set any priorities, anything with `proto carp` and a few
    other management protocols and packet types will go by priority 6, and all types
    of traffic that don’t receive a specific classification with a `set prio` rule
    will have a default priority of 3.
  prefs: []
  type: TYPE_NORMAL
- en: The Two-Priority Speedup Trick
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the examples just shown, we set different priorities for different types
    of traffic and managed to get specific types of traffic, such as VoIP and SSH,
    to move faster than others. But thanks to the design of TCP, which carries the
    bulk of your traffic, even a simple priority-shaping scheme has more to offer
    with only minor tweaks to the rule set.
  prefs: []
  type: TYPE_NORMAL
- en: As readers of RFCs and a few practitioners have discovered, the connection-oriented
    design of TCP means that for each packet sent, the sender will expect to receive
    an acknowledgment (ACK) packet back within a preset time or matching a defined
    “window” of sequence numbers. If the sender doesn’t receive the acknowledgment
    within the expected limit, she assumes the packet was lost in transit and arranges
    to resend the data.
  prefs: []
  type: TYPE_NORMAL
- en: One other important factor to consider is that by default, packets are handled
    in the order they arrive. This is known as *first in, first out (FIFO)*, and it
    means that the essentially dataless ACK packets will be waiting their turn in
    between the larger data packets. On a busy or congested link, which is exactly
    where traffic shaping becomes interesting, waiting for ACKs and performing retransmissions
    can eat measurably into effective bandwidth and slow down all transfers. In fact,
    concurrent transfers in both directions can slow each other significantly more
    than the value of their expected data sizes.^([[39](#ftn.ch07fn01)])
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, a simple and quite popular solution to this problem is at hand:
    You can use priorities to make sure those smaller packets skip ahead. If you assign
    two priorities in a `match` or `pass` rule, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The first priority will be assigned to the regular traffic, while ACK packets
    and other packets with a low delay type of service (ToS) will be assigned the
    second priority and will be served faster than the regular packets.
  prefs: []
  type: TYPE_NORMAL
- en: When a packet arrives, PF detects the ACK packets and puts them on the higher-priority
    queue. PF also inspects the ToS field on arriving packets. Packets that have the
    ToS set to low delay to indicate that the sender wants speedier delivery also
    get the high-priority treatment. When more than one priority is indicated, as
    in the preceding rule, PF assigns priority accordingly. Packets with other ToS
    values are processed in the order they arrive, but with ACK packets arriving faster,
    the sender spends less time waiting for ACKs and resending presumably lost data.
    The net result is that the available bandwidth is used more efficiently. (The
    `match` rule quoted here is the first one I wrote in order to get a feel for the
    new `prio` feature—on a test system, of course—soon after it was committed during
    the OpenBSD 5.0 development cycle. If you put that single `match` rule on top
    of an existing rule set, you’ll probably see that the link can take more traffic
    and more simultaneous connections before noticeable symptoms of congestion turn
    up.)
  prefs: []
  type: TYPE_NORMAL
- en: See whether you can come up with a way to measure throughout before and after
    you introduce the two-priorities trick to your traffic shaping, and note the difference
    before you proceed to the more complex traffic-shaping options.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Queues for Bandwidth Allocation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ve seen that traffic shaping using only priorities can be quite effective,
    but there will be times when a priorities-only scheme will fall short of your
    goals. One such scenario occurs when you’re faced with requirements that would
    be most usefully solved by assigning a higher priority, and perhaps a larger bandwidth
    share, to some kinds of traffic, such as email and other high-value services,
    and correspondingly less bandwidth to others. Another such scenario would be when
    you simply want to apportion your available bandwidth in different-sized chunks
    to specific services and perhaps set hard upper limits for some types of traffic,
    while at the same time wanting to ensure that all traffic that you care about
    gets at least its fair share of available bandwidth. In cases like these, you
    leave the pure-priority scheme behind, at least as the primary tool, and start
    doing actual traffic shaping using *queues*.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike with the priority levels, which are always available and can be used
    without further preparations, in any rule, queues represent specific parts of
    your available bandwidth and can be used only after you’ve defined them in terms
    of available capacity. Queues are a kind of buffer for network packets. Queues
    are defined with a specific amount of bandwidth, or as a specific portion of available
    bandwidth, and you can allocate portions of each queue’s bandwidth share to subqueues,
    or queues within queues, which share the parent queue’s resources. The packets
    are held in a queue until they’re either dropped or sent according to the queue’s
    criteria and subject to the queue’s available bandwidth. Queues are attached to
    specific interfaces, and bandwidth is managed on a per-interface basis, with available
    bandwidth on a given interface subdivided into the queues you define.
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic syntax for defining a queue follows this pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The letters following the bandwidth number denote the unit of measurement:
    *`K`* denotes kilobits; *`M`* megabits; and *`G`* gigabits. When you write only
    the bandwidth number, it’s interpreted as the number of bits per second. It’s
    possible to tack on other options to this basic syntax, as we’ll see in later
    examples.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Subqueue definitions name their parent queue, and one queue needs to be the
    default queue that receives any traffic not specifically assigned to other queues.*'
  prefs: []
  type: TYPE_NORMAL
- en: Once queue definitions are in place, you integrate traffic shaping into your
    rule set by rewriting your `pass` or `match` rules to assign traffic to a specific
    queue.
  prefs: []
  type: TYPE_NORMAL
- en: What’s your Total Usable Bandwidth?
  prefs: []
  type: TYPE_NORMAL
- en: Once we start working with defined parts of total bandwidth rather than priorities
    that somehow share the whole, determining the exact value of your total usable
    bandwidth becomes interesting. It can be difficult to determine actual usable
    bandwidth on a specific interface for queuing. If you don’t specify a total bandwidth,
    the total bandwidth available will be used to calculate the allocations, but some
    types of interfaces cannot reliably report the actual bandwidth value. One common
    example of this discrepancy is where your gateway’s external interface is a 100
    megabit (Mb) Ethernet interface, attached to a DSL line that offers only 8Mb download
    and 1Mb upload.^([[40](#ftn.ch07fn01a)]) The Ethernet interface will then confidently
    report 100Mb bandwidth, not the actual value of the Internet-facing connection.
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, it usually makes sense to set the total bandwidth to a fixed
    value. Unfortunately, the value to use may not be exactly what your bandwidth
    supplier tells you is available because there will always be some overhead due
    to various technologies and implementations. For example, in typical TCP/IP over
    wired Ethernet, overhead can be as low as single-digit percentages, but TCP/IP
    over ATM has been known to have overhead of almost 20 percent. If your bandwidth
    supplier doesn’t provide the overhead information, you’ll need to make an educated
    guess at the starting value. In any case, remember that the total bandwidth available
    is never greater than the bandwidth of the weakest link in your network path.
  prefs: []
  type: TYPE_NORMAL
- en: Queues are supported only for outbound connections relative to the system doing
    the queuing. When planning your bandwidth management, consider the actual usable
    bandwidth to be equal to the weakest (lowest bandwidth) link in the connection’s
    path, even if your queues are set up on a different interface.
  prefs: []
  type: TYPE_NORMAL
- en: The HFSC Algorithm
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Underlying any queue system you define using the queue system in OpenBSD 5.5
    and later is the *Hierarchical Fair Service Curve (HFSC)* algorithm. HFSC was
    designed to allocate resources fairly among queues in a hierarchy. One of its
    interesting features is that it imposes no limits until some part of the traffic
    reaches a volume that’s close to its preset limits. The algorithm starts shaping
    just before the traffic reaches a point where it deprives some other queue of
    its guaranteed minimum share.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*All sample configurations we present in this book assign traffic to queues
    in the outgoing direction because you can realistically control only traffic generated
    locally and, once limits are reached, any traffic-shaping system will eventually
    resort to dropping packets in order to make the endpoint back off. As we saw in
    the earlier examples, all well-behaved TCP stacks will respond to lost ACKs with
    slower packet rates.*'
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know at least the basics of the theory behind the OpenBSD queue
    system, let’s see how queues work.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting Your Bandwidth into Fixed-Size Chunks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You’ll often find that certain traffic should receive a higher priority than
    other traffic. For example, you’ll often want important traffic, such as mail
    and other vital services, to have a baseline amount of bandwidth available at
    all times, while other services, such as peer-to-peer file sharing, shouldn’t
    be allowed to consume more than a certain amount. To address these kinds of issues,
    queues offer a wider range of options than the pure-priority scheme.
  prefs: []
  type: TYPE_NORMAL
- en: The first queue example builds on the rule sets from earlier chapters. The scenario
    is that we have a small local network, and we want to let the users on the local
    network connect to a predefined set of services outside their own network while
    also letting users from outside the local network access a Web server and an FTP
    server somewhere on the local network.
  prefs: []
  type: TYPE_NORMAL
- en: Queue Definition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the following example, all queues are set up with the root queue, called
    `main`, on the external, Internet-facing interface. This approach makes sense
    mainly because bandwidth is more likely to be limited on the external link than
    on the local network. In principle, however, allocating queues and running traffic
    shaping can be done on any network interface.
  prefs: []
  type: TYPE_NORMAL
- en: This setup includes a queue for a total bandwidth of 20Mb with six subqueues.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The subqueue `defq`, shown in the preceding example, has a bandwidth allocation
    of 3600K, or 18 percent of the bandwidth, and is designated as the default queue.
    This means any traffic that matches a `pass` rule but that isn’t explicitly assigned
    to some other queue ends up here.
  prefs: []
  type: TYPE_NORMAL
- en: The other queues follow more or less the same pattern, up to subqueue `ssh`,
    which itself has two subqueues (the two indented lines below it). Here, we see
    a variation on the trick of using two separate priorities to speed up ACK packets,
    and as we’ll see shortly, the rule that assigns traffic to the two SSH subqueues
    assigns different priorities. Bulk SSH transfers, typically SCP file transfers,
    are transmitted with a ToS indicating throughput, while interactive SSH traffic
    has the ToS flag set to low delay and skips ahead of the bulk transfers. The interactive
    traffic is likely to be less bandwidth consuming and gets a smaller share of the
    bandwidth, but it receives preferential treatment because of the higher-priority
    value assigned to it. This scheme also helps the speed of SCP file transfers because
    the ACK packets for the SCP transfers will be assigned a higher priority.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we have the `icmp` queue, which is reserved for the remaining 400K,
    or 2 percent, of the bandwidth from the top level. This guarantees a minimum amount
    of bandwidth for ICMP traffic that we want to pass but that doesn’t match the
    criteria that would have it assigned to the other queues.
  prefs: []
  type: TYPE_NORMAL
- en: Rule Set
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To tie the queues into the rule set, we use the `pass` rules to indicate which
    traffic is assigned to the queues and their criteria.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The rules for `ssh`, `ftp`, `www`, `udp`, and `icmp` assign traffic to their
    respective queues, and we note again that the `ssh` queue’s subqueues are assigned
    traffic with two different priorities. The last catchall rule ➊ passes all other
    outgoing traffic from the local network, lumping it into the default `defq` queue.
  prefs: []
  type: TYPE_NORMAL
- en: You can always let a block of `match` rules do the queue assignment instead
    in order to make the configuration even more flexible. With match rules in place,
    you move the filtering decisions to block, pass, or even redirect to a set of
    rules elsewhere.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Note that with `match` rules performing the queue assignment, there’s no need
    for a final catchall to put the traffic that doesn’t match the other rules into
    the default queue. Any traffic that doesn’t match these rules and that’s allowed
    to pass will end up in the default queue.
  prefs: []
  type: TYPE_NORMAL
- en: Upper and Lower Bounds with Bursts
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Fixed bandwidth allocations are nice, but network admins with traffic-shaping
    ambitions tend to look for a little more flexibility once they’ve gotten their
    feet wet. Wouldn’t it be nice if there were a regime with flexible bandwidth allocation,
    offering guaranteed lower and upper bounds for bandwidth available to each queue
    and variable allocations over time—and one that starts shaping only when there’s
    an actual need to do so?
  prefs: []
  type: TYPE_NORMAL
- en: The good news is that the OpenBSD queues can do just that, courtesy of the underlying
    HFSC algorithm discussed earlier. HFSC makes it possible to set up queuing regimes
    with guaranteed minimum allocations and hard upper limits, and you can even have
    allocations that include `burst` values to let available capacity vary over time.
  prefs: []
  type: TYPE_NORMAL
- en: Queue Definition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Working from a typical gateway configuration like the ones we’ve altered incrementally
    over the earlier chapters, we insert this queue definition early in the *pf.conf*
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This definition has some characteristics that are markedly different from the
    previous one in [Introducing Queues for Bandwidth Allocation](ch07.html#introducing_queues_for_bandwidth_allocat
    "Introducing Queues for Bandwidth Allocation"). We start with this rather small
    hierarchy by splitting the top-level queue, `rootq`, into two. Next, we subdivide
    the `main` queue into several subqueues, all of which have a `min` value set—the
    guaranteed minimum bandwidth allocated to the queue. (The `max` value would set
    a hard upper limit on the queue’s allocation.) The `bandwidth` parameter also
    sets the allocation the queue will have available when it’s backlogged—that is,
    when it’s started to eat into its `qlimit`, or *queue limit*, allocation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The queue limit parameter works like this: In case of congestion, each queue
    by default has a pool of 50 slots, the queue limit, to keep packets around when
    they can’t be transmitted immediately. Here, the top-level queues, `main` and
    `spamd`, both have larger-than-default pools set by their `qlimit` setting: `100`
    for `main` and `300` for `spamd`. Cranking up these `qlimit` sizes means we’re
    a little less likely to drop packets when the traffic approaches the set limits,
    but it also means that when the traffic shaping kicks in, we’ll see increased
    latency for connections that end up in these larger pools.'
  prefs: []
  type: TYPE_NORMAL
- en: Rule Set
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The next step is to tie the newly created queues into the rule set. If you
    have a filtering regime in place already, the tie-in is simple—just add a few
    `match` rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Here, the `match` rules once again do the ACK packet speedup trick with the
    high- and low-priority queue assignment, just as we saw earlier in the pure-priority-based
    system. The only exception is when we assign traffic to our lowest-priority queue
    (with a slight modification to an existing `pass` rule), where we really don’t
    want any speedup.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Assigning the `spamd` traffic to a minimal-sized queue with 0 priority here
    is intended to slow down the spammers on their way to our `spamd`. (See [Chapter 6](ch06.html
    "Chapter 6. Turning the Tables for Proactive Defense") for more on `spamd` and
    related matters.)
  prefs: []
  type: TYPE_NORMAL
- en: 'With the queue assignment and priority setting in place, it should be clear
    that the queue hierarchy here uses two familiar tricks to make efficient use of
    available bandwidth. First, it uses a variation of the high- and low-priority
    mix demonstrated in the earlier pure-priority example. Second, we speed up almost
    all other traffic, especially the Web traffic, by allocating a small but guaranteed
    portion of bandwidth for name service lookups. For the `qdns` queue, we set the
    `burst` value with a time limit: After `3000` milliseconds, the allocation goes
    down to a minimum of `12K` to fit within the total `200K` quota. Short-lived `burst`
    values like this can be useful to speed connections that transfer most of their
    payload during the early phases.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It may not be immediately obvious from this example, but HFSC requires that
    traffic be assigned only to *leaf queues*, or queues without subqueues. That means
    it’s possible to assign traffic to `main`’s subqueues—`qpri`, `qdef`, `qweb`,
    and `qdns`—as well as `rootq`’s subqueue—`spamd`—as we just did with the `match`
    and `pass` rules, but not to `rootq` or `main` themselves. With all queue assignments
    in place, we can use `systat` queues to show the queues and their traffic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The queues are shown indented to indicate their hierarchy, from root to leaf
    queues. The `main` queue and its subqueues—`qpri`, `qdef`, `qweb`, and `qdns`—are
    shown with their bandwidth allocations and number of bytes and packets passed.
    The `DROP_P` and `DROP_B` columns, which show the number of packets and bytes
    dropped, would appear if we had been forced to drop packets at this stage. `QLEN`
    is the number of packets waiting for processing, while the final two columns show
    live updates of packets and bytes per second.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a more detailed view, use `pfctl -vvsq` to show the queues and their traffic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This view shows that the queues receive traffic roughly as expected with the
    site’s typical workload. Notice that only a few moments after the rule set has
    been reloaded, the `spamd` queue is already backed up more than halfway to its
    `qlimit` setting, which seems to indicate that the queues are reasonably dimensioned
    to actual traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Pay attention to each queue’s dropped packets (`dropped pkts:`) counter. If
    the number of packets dropped is high or increasing, then that could mean that
    one of the bandwidth allocation parameters needs adjusting or that some other
    network problem needs to be investigated.*'
  prefs: []
  type: TYPE_NORMAL
- en: The DMZ Network, Now with Traffic Shaping
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In [Chapter 5](ch05.html "Chapter 5. Bigger or Trickier Networks"), we set up
    a network with a single gateway and all externally visible services configured
    on a separate DMZ (demilitarized zone) network so that all traffic to the servers
    from both the Internet and the internal network had to pass through the gateway.
    That network schematic, illustrated in [Chapter 5](ch05.html "Chapter 5. Bigger
    or Trickier Networks"), is shown again in [Figure 7-1](ch07.html#network_with_dmz
    "Figure 7-1. Network with DMZ"). Using the rule set from [Chapter 5](ch05.html
    "Chapter 5. Bigger or Trickier Networks") as the starting point, we’ll add some
    queuing in order to optimize our network resources. The physical and logical layout
    of the network will not change.
  prefs: []
  type: TYPE_NORMAL
- en: '![Network with DMZ](httpatomoreillycomsourcenostarchimages2127159.png.jpg)Figure 7-1. Network
    with DMZ'
  prefs: []
  type: TYPE_NORMAL
- en: The most likely bottleneck for this network is the bandwidth for the connection
    between the gateway’s external interface and the Internet. Although the bandwidth
    elsewhere in our setup isn’t infinite, of course, the available bandwidth on any
    interface in the local network is likely to be less limiting than the bandwidth
    actually available for communication with the outside world. In order to make
    services available with the best possible performance, we need to set up the queues
    so that the bandwidth available at the site is made available to the traffic we
    want to allow. The interface bandwidth on the DMZ interface is likely either 100Mb
    or 1Gb, while the *actual available bandwidth* for connections from outside the
    local network is considerably smaller. This consideration shows up in our queue
    definitions, where the actual bandwidth available for external traffic is the
    main limitation in the queue setup.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Notice that for each interface, there’s a root queue with a bandwidth limitation
    that determines the allocation for all queues attached to that interface. In order
    to use the new queuing infrastructure, we need to make some changes to the filtering
    rules, too.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Because any traffic not explicitly assigned to a specific queue is assigned
    to the default queue for the interface, be sure to tune your filtering rules as
    well as your queue definitions to the actual traffic in your network.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The main part of the filtering rules could end up looking like this after adding
    the queues:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Notice that only traffic that will pass either the DMZ or the external interface
    is assigned to queues. In this configuration, with no externally accessible services
    on the internal network, queuing on the internal interface wouldn’t make much
    sense because that’s likely the part of the network with the least restricted
    available bandwidth. Also, as in earlier examples, there’s a case to be made for
    separating the queue assignments from the filtering part of the rule set by making
    a block of `match` rules responsible for queue assignment.
  prefs: []
  type: TYPE_NORMAL
- en: Using Queues to Handle Unwanted Traffic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, we’ve focused on queuing as a way to make sure specific kinds of traffic
    are let through as efficiently as possible. Now, we’ll look at two examples that
    present a slightly different way to identify and handle unwanted traffic using
    various queuing-related tricks to keep miscreants in line.
  prefs: []
  type: TYPE_NORMAL
- en: Overloading to a Tiny Queue
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In [Turning Away the Brutes](ch06.html#turning_away_the_brutes "Turning Away
    the Brutes"), we used a combination of state-tracking options and `overload` rules
    to fill a table of addresses for special treatment. The special treatment we demonstrated
    in [Chapter 6](ch06.html "Chapter 6. Turning the Tables for Proactive Defense")
    was to cut all connections, but it’s equally possible to assign `overload` traffic
    to a specific queue instead. For example, consider the rule from our first queue
    example, shown here.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'To create a variation of the overload table trick from [Chapter 6](ch06.html
    "Chapter 6. Turning the Tables for Proactive Defense"), add state-tracking options,
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, make one of the queues slightly smaller:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'And assign traffic from miscreants to the small-bandwidth queue with this rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: As a result, the traffic from the bruteforcers would pass, but with a hard upper
    limit of 512 bits per second. (It’s worth noting that tiny bandwidth allocations
    may be hard to enforce on high-speed links due to the network stack’s timer resolution.
    If the allocation is small enough relative to the capacity of the link, packets
    that exceed the stated per-second maximum allocation may be transferred anyway,
    before the bandwidth limit kicks in.) It might also be useful to supplement rules
    like these with table-entry expiry, as described in [Tidying Your Tables with
    pfctl](ch06.html#tidying_your_tables_with_pfctl "Tidying Your Tables with pfctl").
  prefs: []
  type: TYPE_NORMAL
- en: Queue Assignments Based on Operating System Fingerprint
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[Chapter 6](ch06.html "Chapter 6. Turning the Tables for Proactive Defense")
    covered several ways to use `spamd` to cut down on spam. If running `spamd` isn’t
    an option in your environment, you can use a queue and rule set based on the knowledge
    that machines that send spam are likely to run a particular operating system.
    (Let’s call that operating system Windows.)'
  prefs: []
  type: TYPE_NORMAL
- en: PF has a fairly reliable operating system fingerprinting mechanism, which detects
    the operating system at the other end of a network connection based on characteristics
    of the initial SYN packets at connection setup. The following may be a simple
    substitute for `spamd` if you’ve determined that legitimate mail is highly unlikely
    to be delivered from systems that run that particular operating system.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Here, email traffic originating from hosts that run a particular operating system
    get no more than 512 bits per second of your bandwidth.
  prefs: []
  type: TYPE_NORMAL
- en: Transitioning from ALTQ to Priorities and Queues
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you already have configurations that use ALTQ for traffic shaping and you’re
    planning a switch to OpenBSD 5.5 or newer, this section contains some pointers
    for how to manage the transition. The main points are these:'
  prefs: []
  type: TYPE_NORMAL
- en: '***The rules after transition are likely simpler.*** The OpenBSD 5.5 and newer
    traffic-shaping system has done away with the somewhat arcane ALTQ syntax with
    its selection of queuing algorithms, and it distinguishes clearly between queues
    and pure-priority shuffling. In most cases, your configuration becomes significantly
    more readable and maintainable after a conversion to the new traffic-shaping system.'
  prefs: []
  type: TYPE_NORMAL
- en: '***For simple configurations, set prio is enough.*** The simplest queue discipline
    in ALTQ was `priq`, or priority queues. The most common simple use case was the
    two-priority speedup trick first illustrated by Daniel Hartmeier in the previously
    cited article. The basic two-priority configuration looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'In OpenBSD 5.5 and newer, the equivalent effect can be achieved with no queue
    definitions. Instead, you assign two priorities in a `match` or `pass` rule, like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Here, the first priority will be assigned to regular traffic, while ACK and
    other packets with a low-delay ToS will be assigned the second priority and will
    be served faster than the regular packets. The effect is the same as in the ALTQ
    example we just quoted, with the exception of defined bandwidth limits and the
    somewhat dubious effect of traffic shaping on incoming traffic.
  prefs: []
  type: TYPE_NORMAL
- en: '***Priority queues can for the most part be replaced by set prio constructs.***
    For pure-priority differentiation, applying `set prio` on a per `pass` or `match`
    rule basis is simpler than defining queues and assigning traffic and affects only
    the packet priority. ALTQ allowed you to define CBQ or HFSC queues that also had
    a priority value as part of their definition. Under the new queuing system, assigning
    priority happens only in `match` or `pass` rules, but if your application calls
    for setting both priority and queue assignment in the same rule, the new syntax
    allows for that, too:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The effect is similar to the previous behavior shown in [Splitting Your Bandwidth
    into Fixed-Size Chunks](ch07.html#splitting_your_bandwidth_into_fixed-size "Splitting
    Your Bandwidth into Fixed-Size Chunks"), and this variant may be particularly
    helpful during transition.
  prefs: []
  type: TYPE_NORMAL
- en: '***Priorities are now always important. Keep in mind that the default is 3.***
    It’s important to be aware that traffic priorities are always enabled since OpenBSD
    5.0, and they need to be taken into consideration even when you’re not actively
    assigning priorities. In old-style configurations that employed the two-priority
    trick to speed up ACKs and by extension all traffic, the only thing that was important
    was that there were two different priorities in play. The low-delay packets would
    be assigned to the higher-priority queue, and the net effect would be that traffic
    would likely pass faster, with more efficient bandwidth use than with the default
    FIFO queue. Now the default priority is 3, and setting the priority for a queue
    to 0, as a few older examples do, will mean that the traffic assigned that priority
    will be considered ready to pass only when there’s no higher-priority traffic
    left to handle.'
  prefs: []
  type: TYPE_NORMAL
- en: '***For actual bandwidth shaping, HFSC works behind the scenes.*** Once you’ve
    determined that your specification calls for slicing available bandwidth into
    chunks, the underlying algorithm is always HFSC. The variety of syntaxes for different
    types of queues is gone. HFSC was chosen for its flexibility as well as the fact
    that it starts actively shaping traffic only once the traffic approaches one of
    the limits set by your queuing configuration. In addition, it’s possible to create
    CBQ-like configurations by limiting the queue definitions to only bandwidth declarations.
    [Splitting Your Bandwidth into Fixed-Size Chunks](ch07.html#splitting_your_bandwidth_into_fixed-size
    "Splitting Your Bandwidth into Fixed-Size Chunks") (mentioned earlier) demonstrates
    a static configuration that implements CBQ as a subset of HFSC.'
  prefs: []
  type: TYPE_NORMAL
- en: '***You can transition from ALTQ via the oldqueue mechanism.*** OpenBSD 5.5
    supports legacy ALTQ configurations with only one minor change to configurations:
    The `queue` keyword was needed as a reserved word for the new queuing system,
    so ALTQ queues need to be declared as `oldqueue` instead. Following that one change
    (a pure search and replace operation that you can even perform just before starting
    your operating system upgrade), the configuration will work as expected.'
  prefs: []
  type: TYPE_NORMAL
- en: '***If your setup is sufficiently complicated, go back to specifications and
    reimplement.*** The examples in this chapter are somewhat stylized and rather
    simple. If you have running configurations that have been built up incrementally
    over several years and have reached a complexity level, orders of magnitude larger
    than those described here, the new syntax may present an opportunity to define
    what your setup is for and produce a specification that is fit to reimplement
    in a cleaner and more maintainable configuration.'
  prefs: []
  type: TYPE_NORMAL
- en: Going the `oldqueue` route and tweaking from there will work to some degree,
    but it may be easier to make the transition via a clean reimplementation from
    revised specification in a test environment where you can test whether your accumulated
    assumptions hold up in a the context of the new traffic-shaping system. Whatever
    route you choose for your transition, you’re more or less certain to end up with
    a more readable and maintainable configuration after your switch to OpenBSD 5.5
    or newer.
  prefs: []
  type: TYPE_NORMAL
- en: Directing Traffic with ALTQ
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*ALTQ* is the very flexible legacy mechanism for network traffic shaping, which
    was integrated into PF on OpenBSD^([[41](#ftn.ch07fn02)]) in time for the OpenBSD
    3.3 release by Henning Brauer, who’s also the main developer of the priorities
    and queues system introduced in OpenBSD 5.5 (described in the previous sections
    of this chapter). OpenBSD 3.3 onward moved all ALTQ configuration into *pf.conf*
    to ease the integration of traffic shaping and filtering. PF ports to other BSDs
    were quick to adopt at least some optional ALTQ integration.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*OpenBSD 5.5 introduced a new queue system for traffic shaping with a radically
    different (and more readable) syntax that complements the always-on priority system
    introduced in OpenBSD 5.0\. The new system is intended to replace ALTQ entirely
    after one transitional release. The rest of this chapter is useful only if you’re
    interested in learning about how to set up or maintain an ALTQ-based system.*'
  prefs: []
  type: TYPE_NORMAL
- en: Basic ALTQ Concepts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As the name suggests, ALTQ configurations are totally queue-centric. As in the
    more recent traffic-shaping system, ALTQ queues are defined in terms of bandwidth
    and attached to interfaces. Queues can be assigned priority, and in some contexts,
    they can have subqueues that receive a share of the parent queue’s bandwidth.
  prefs: []
  type: TYPE_NORMAL
- en: 'The general syntax for ALTQ queues looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*On OpenBSD 5.5 and newer, ALTQ queues are denoted `oldqueue` instead of `queue`
    due to an irresolvable syntax conflict with the new queuing subsystem.*'
  prefs: []
  type: TYPE_NORMAL
- en: Once queue definitions are in place, you integrate traffic shaping into your
    rule set by rewriting your `pass` or `match` rules to assign traffic to a specific
    queue. Any traffic that you don’t explicitly assign to a specific queue gets lumped
    in with everything else in the default queue.
  prefs: []
  type: TYPE_NORMAL
- en: Queue Schedulers, aka Queue Disciplines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the default networking setup, with no queuing, the TCP/IP stack and its filtering
    subsystem process the packets according to the FIFO discipline.
  prefs: []
  type: TYPE_NORMAL
- en: ALTQ offers three queue-scheduler algorithms, or *disciplines*, that can alter
    this behavior slightly. The types are `priq`, `cbq`, and `hfsc`. Of these, `cbq`
    and `hfsc` queues can have several levels of subqueues. The `priq` queues are
    essentially flat, with only one queue level. Each of the disciplines has its own
    syntax specifics, and we’ll address those in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: priq
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Priority-based queues* are defined purely in terms of priority within the
    total declared bandwidth. For `priq` queues, the allowed priority range is 0 through
    15, where a higher value earns preferential treatment. Packets that match the
    criteria for higher-priority queues are serviced before the ones matching lower-priority
    queues.'
  prefs: []
  type: TYPE_NORMAL
- en: cbq
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Class-based queues* are defined as constant-sized bandwidth allocations, as
    a percentage of the total available or in units of kilobits, megabits, or gigabits
    per second. A `cbq` queue can be subdivided into queues that are also assigned
    priorities in the range 0 to 7, and again, a higher priority means preferential
    treatment.'
  prefs: []
  type: TYPE_NORMAL
- en: hfsc
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `hfsc` discipline uses the HFSC algorithm to ensure a “fair” allocation
    of bandwidth among the queues in a hierarchy. HFSC comes with the possibility
    of setting up queuing regimes with guaranteed minimum allocations and hard upper
    limits. Allocations can even vary over time, and you can even have fine-grained
    priority with a 0 to 7 range.
  prefs: []
  type: TYPE_NORMAL
- en: Because both the algorithm and the corresponding setup with ALTQ are fairly
    complicated, with a number of tunable parameters, most ALTQ practitioners tend
    to stick with the simpler queue types. Yet the ones who claim to understand HFSC
    swear by it.
  prefs: []
  type: TYPE_NORMAL
- en: Setting Up ALTQ
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Enabling ALTQ may require some extra steps, depending on your choice of operating
    system.
  prefs: []
  type: TYPE_NORMAL
- en: ALTQ on OpenBSD
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: On OpenBSD 5.5, all supported queue disciplines are compiled into the GENERIC
    and GENERIC.MP kernels. Check that your OpenBSD version still supports ALTQ. If
    so, the only configuration you need to do involves editing your *pf.conf*.
  prefs: []
  type: TYPE_NORMAL
- en: ALTQ on FreeBSD
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'On FreeBSD, make sure that your kernel has ALTQ and the ALTQ queue discipline
    options compiled in. The default FreeBSD GENERIC kernel doesn’t have ALTQ options
    enabled, as you may have noticed from the messages you saw when running the */etc/rc.d/pf*
    script to enable PF. The relevant options are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The `ALTQ` option is needed to enable ALTQ in the kernel, but on SMP systems,
    you also need the `ALTQ_NOPCC` option. Depending on which types of queues you’ll
    be using, you’ll need to enable at least one of these: `ALTQ_CBQ`, `ALTQ_PRIQ`,
    or `ALTQ_HFSC`. Finally, you can enable the congestion-avoidance techniques *random
    early detection (RED)* and *RED In/Out* with the `ALTQ_RED` and `ALTQ_RIO` options,
    respectively. (See the *FreeBSD Handbook* for information on how to compile and
    install a custom kernel with these options.)'
  prefs: []
  type: TYPE_NORMAL
- en: ALTQ on NetBSD
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'ALTQ was integrated into the NetBSD 4.0 PF implementation and is supported
    in NetBSD 4.0 and later releases. NetBSD’s default GENERIC kernel configuration
    doesn’t include the ALTQ-related options, but the GENERIC configuration file comes
    with all relevant options commented out for easy inclusion. The main kernel options
    are these:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The `ALTQ` option is needed to enable ALTQ in the kernel. Depending on the
    types of queues you’ll be using, you must enable at least one of these: `ALTQ_CBQ`,
    `ALTQ_PRIQ`, or `ALTQ_HFSC`.'
  prefs: []
  type: TYPE_NORMAL
- en: Using ALTQ requires you to compile PF into the kernel because the PF loadable
    module doesn’t support ALTQ functionality. (See the NetBSD PF documentation at
    *[http://www.netbsd.org/Documentation/network/pf.html](http://www.netbsd.org/Documentation/network/pf.html)*
    for the most up-to-date information.)
  prefs: []
  type: TYPE_NORMAL
- en: Priority-Based Queues
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The basic concept behind priority-based queues (`priq`) is fairly straightforward.
    Within the total bandwidth allocated to the main queue, only traffic priority
    matters. You assign queues a priority value in the range 0 through 15, where a
    higher value means that the queue’s requests for traffic are serviced sooner.
  prefs: []
  type: TYPE_NORMAL
- en: Using ALTQ Priority Queues to Improve Performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Daniel Hartmeier discovered a simple yet effective way to improve the throughput
    for his home network by using ALTQ priority queues. Like many people, he had his
    home network on an asymmetric connection, with total usable bandwidth low enough
    that he wanted better bandwidth utilization. In addition, when the line was running
    at or near capacity, oddities started appearing. One symptom in particular seemed
    to suggest room for improvement: Incoming traffic (downloads, incoming mail, and
    such) slowed down disproportionately whenever outgoing traffic started—more than
    could be explained by measuring the raw amount of data transferred. It all came
    back to a basic feature of TCP.'
  prefs: []
  type: TYPE_NORMAL
- en: When a TCP packet is sent, the sender expects acknowledgment (in the form of
    an ACK packet) from the receiver and will wait a specified time for it to arrive.
    If the ACK doesn’t arrive within that time, the sender assumes that the packet
    hasn’t been received and resends it. And because in a default setup, packets are
    serviced sequentially by the interface as they arrive, ACK packets, with essentially
    no data payload, end up waiting in line while the larger data packets are transferred.
  prefs: []
  type: TYPE_NORMAL
- en: If ACK packets could slip in between the larger data packets, the result would
    be more efficient use of available bandwidth. The simplest practical way to implement
    such a system with ALTQ is to set up two queues with different priorities and
    integrate them into the rule set. Here are the relevant parts of the rule set.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Here, the priority-based queue is set up on the external interface with two
    subordinate queues. The first subqueue, `q_pri`, has a high-priority value of
    7; the other subqueue, `q_def`, has a significantly lower-priority value of 1.
  prefs: []
  type: TYPE_NORMAL
- en: This seemingly simple rule set works by exploiting how ALTQ treats queues with
    different priorities. Once a connection is set up, ALTQ inspects each packet’s
    ToS field. ACK packets have the ToS delay bit set to low, which indicates that
    the sender wanted the speediest delivery possible. When ALTQ sees a low-delay
    packet and queues of differing priorities are available, it assigns the packet
    to the higher-priority queue. This means that the ACK packets skip ahead of the
    lower-priority queue and are delivered more quickly, which in turn means that
    data packets are serviced more quickly. The net result is better performance than
    a pure FIFO configuration with the same hardware and available bandwidth. (Daniel
    Hartmeier’s article about this version of his setup, cited previously, contains
    a more detailed analysis.)
  prefs: []
  type: TYPE_NORMAL
- en: Using a match Rule for Queue Assignment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous example, the rule set was constructed the traditional way, with
    the queue assignment as part of the `pass` rules. However, this isn’t the only
    way to do queue assignment. When you use `match` rules (available in OpenBSD 4.6
    and later), it’s incredibly easy to retrofit this simple priority-queuing regime
    onto an existing rule set.
  prefs: []
  type: TYPE_NORMAL
- en: If you worked through the examples in [Chapter 3](ch03.html "Chapter 3. Into
    the Real World") and [Chapter 4](ch04.html "Chapter 4. Wireless Networks Made
    Easy"), your rule set probably has a `match` rule that applies `nat-to` on your
    outgoing traffic. To introduce priority-based queuing to your rule set, you first
    add the queue definitions and make some minor adjustments to your outgoing `match`
    rule.
  prefs: []
  type: TYPE_NORMAL
- en: Start with the queue definition from the preceding example and adjust the total
    bandwidth to local conditions, as shown in here.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: This gives the queues whatever bandwidth allocation you define with the `ext_bw`
    macro.
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest and quickest way to integrate the queues into your rule set is
    to edit your outgoing `match` rule to read something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Reload your rule set, and the priority-queuing regime is applied to all traffic
    that’s initiated from your local network.
  prefs: []
  type: TYPE_NORMAL
- en: You can use the `systat` command to get a live view of how traffic is assigned
    to your queues.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give you a live display that looks something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Looking at the numbers in the `PKTS` (packets) and `BYTES` columns, you see
    a clear indication that the queuing is working as intended.
  prefs: []
  type: TYPE_NORMAL
- en: The `q_pri` queue has processed a rather large number of packets in relation
    to the amount of data, just as we expected. The ACK packets don’t take up a lot
    of space. On the other hand, the traffic assigned to the `q_def` queue has more
    data in each packet, and the numbers show essentially the reverse packet numbers–to–data
    size ratio as in to the `q_pri` queue.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*`systat` is a rather capable program on all BSDs, and the OpenBSD version
    offers several views that are relevant to PF and that aren’t found in the `systat`
    variants on the other systems as of this writing. We’ll be looking at `systat`
    again in the next chapter. In the meantime, read the man pages and play with the
    program. It’s a very useful tool for getting to know your system.*'
  prefs: []
  type: TYPE_NORMAL
- en: Class-Based Bandwidth Allocation for Small Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Maximizing network performance generally feels nice. However, you may find that
    your network has other needs. For example, it might be important for some traffic—such
    as mail and other vital services—to have a baseline amount of bandwidth available
    at all times, while other services—peer-to-peer file sharing comes to mind—shouldn’t
    be allowed to consume more than a certain amount. To address these kinds of requirements
    or concerns, ALTQ offers the class-based queue (`cbq`) discipline with a slightly
    larger set of options.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate how to use `cbq`, we’ll build on the rule sets from previous chapters
    within a small local network. We want to let the users on the local network connect
    to a predefined set of services outside their own network and let users from outside
    the local network access a Web server and an FTP server somewhere on the local
    network.
  prefs: []
  type: TYPE_NORMAL
- en: Queue Definition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: All queues are set up on the external, Internet-facing interface. This approach
    makes sense mainly because bandwidth is more likely to be limited on the external
    link than on the local network. In principle, however, allocating queues and running
    traffic shaping can be done on any network interface. The example setup shown
    here includes a `cbq` queue for a total bandwidth of 2Mb with six subqueues.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The subqueue `main` has 18 percent of the bandwidth and is designated as the
    default queue. This means any traffic that matches a `pass` rule but isn’t explicitly
    assigned to some other queue ends up here. The `borrow` and `red` keywords mean
    that the queue may “borrow” bandwidth from its parent queue, while the system
    attempts to avoid congestion by applying the RED algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: The other queues follow more or less the same pattern up to the subqueue `ssh`,
    which itself has two subqueues with separate priorities. Here, we see a variation
    on the ACK priority example. Bulk SSH transfers, typically SCP file transfers,
    are transmitted with a ToS indicating throughput, while interactive SSH traffic
    has the ToS flag set to low delay and skips ahead of the bulk transfers. The interactive
    traffic is likely to be less bandwidth consuming and gets a smaller share of the
    bandwidth, but it receives preferential treatment because of the higher-priority
    value assigned to it. This scheme also helps the speed of SCP file transfers because
    the ACK packets for the SCP transfers will be assigned to the higher-priority
    subqueue.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we have the `icmp` queue, which is reserved for the remaining 2 percent
    of the bandwidth from the top level. This guarantees a minimum amount of bandwidth
    for ICMP traffic that we want to pass but that doesn’t match the criteria for
    being assigned to the other queues.
  prefs: []
  type: TYPE_NORMAL
- en: Rule Set
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To make it all happen, we use these `pass` rules, which indicate which traffic
    is assigned to the queues and their criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The rules for `ssh`, `ftp`, `www`, `udp`, and `icmp` assign traffic to their
    respective queues. The last catchall rule passes all other traffic from the local
    network, lumping it into the default `main` queue.
  prefs: []
  type: TYPE_NORMAL
- en: A Basic HFSC Traffic Shaper
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The simple schedulers we have looked at so far can make for efficient setups,
    but network admins with traffic-shaping ambitions tend to look for a little more
    flexibility than can be found in the pure-priority-based queues or the simple
    class-based variety. The HFSC queuing algorithm (`hfsc` in *pf.conf* terminology)
    offers flexible bandwidth allocation, guaranteed lower and upper bounds for bandwidth
    available to each queue, and variable allocations over time, and it only starts
    shaping when there’s an actual need. However, the added flexibility comes at a
    price: The setup is a tad more complex than the other ALTQ types, and tuning your
    setup for an optimal result can be quite an interesting process.'
  prefs: []
  type: TYPE_NORMAL
- en: Queue Definition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'First, working from the same configuration we altered slightly earlier, we
    insert this queue definition early in the *pf.conf* file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The `hfsc` queue definitions take slightly different parameters than the simpler
    disciplines. We start off with this rather small hierarchy by splitting the top-level
    queue into two. At the next level, we subdivide the `main` queue into several
    subqueues, each with a defined priority. All the subqueues have a `realtime` value
    set—the guaranteed minimum bandwidth allocated to the queue. The optional `upperlimit`
    sets a hard upper limit on the queue’s allocation. The `linkshare` parameter sets
    the allocation the queue will have available when it’s backlogged—that is, when
    it’s started to eat into its `qlimit` allocation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In case of congestion, each queue by default has a pool of 50 slots, the queue
    limit (`qlimit`), to keep packets around when they can’t be transmitted immediately.
    In this example, the top-level queues `main` and `spamd` both have larger-than-default
    pools set by their `qlimit` setting: `100` for `main` and `300` for `spamd`. Cranking
    up queue sizes here means we’re a little less likely to drop packets when the
    traffic approaches the set limits, but it also means that when the traffic shaping
    kicks in, we’ll see increased latency for connections that end up in these larger
    than default pools.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The queue hierarchy here uses two familiar tricks to make efficient use of
    available bandwidth:'
  prefs: []
  type: TYPE_NORMAL
- en: It uses a variation of the high- and low-priority mix demonstrated in the earlier
    pure-priority example.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We speed up almost all other traffic (and most certainly the Web traffic that
    appears to be the main priority here) by allocating a small but guaranteed portion
    of bandwidth for name service lookups. For the `q_dns` queue, we set up the `realtime`
    value with a time limit—after `3000` milliseconds, the `realtime` allocation goes
    down to `12Kb`. This can be useful to speed connections that transfer most of
    their payload during the early phases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rule Set
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Next, we tie the newly created queues into the rule set. If you have a filtering
    regime in place already, which we’ll assume you do, the tie-in becomes amazingly
    simple, accomplished by adding a few `match` rules.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Here, the `match` rules once again do the ACK packet speedup trick with the
    high- and low-priority queue assignment, just as you saw earlier in the pure-priority-based
    system. The only exception is when we assign traffic to our lowest-priority queue,
    where we really don’t care to have any speedup at all.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: This rule is intended to slow down the spammers a little more on their way to
    our `spamd`. With a hierarchical queue system in place, `systat queues` shows
    the queues and their traffic as a hierarchy, too.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The root queue is shown as attached to the physical interface—as `nfe0` and
    `root_nfe0`, in this case. `main` and its subqueues—`q_pri`, `q_def`, `q_web`,
    and `q_dns`—are shown with their bandwidth allocations and number of bytes and
    packets passed. The `DROP_P` and `DROP_B` columns are where number of packets
    and bytes dropped, respectively, would appear if we had been forced to drop packets
    at this stage. The final two columns show live updates of packets per second and
    bytes per second.
  prefs: []
  type: TYPE_NORMAL
- en: Queuing for Servers in a DMZ
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In [Chapter 5](ch05.html "Chapter 5. Bigger or Trickier Networks"), we set up
    a network with a single gateway but with all externally visible services configured
    on a separate DMZ network. That way, all traffic to the servers from both the
    Internet and the internal network had to pass through the gateway (see [Figure 7-1](ch07.html#network_with_dmz
    "Figure 7-1. Network with DMZ")).
  prefs: []
  type: TYPE_NORMAL
- en: With the rule set from [Chapter 5](ch05.html "Chapter 5. Bigger or Trickier
    Networks") as our starting point, we’ll add some queuing in order to optimize
    our network resources. The physical and logical layout of the network will not
    change. The most likely bottleneck for this network is the bandwidth for the connection
    between the gateway’s external interface and the Internet at large. The bandwidth
    elsewhere in our setup isn’t infinite, of course, but the available bandwidth
    on any interface in the local network is likely to be less of a limiting factor
    than the bandwidth actually available for communication with the outside world.
    For services to be available with the best possible performance, we need to set
    up the queues so the bandwidth available at the site is made available to the
    traffic we want to allow.
  prefs: []
  type: TYPE_NORMAL
- en: In our example, it’s likely that the interface bandwidth on the DMZ interface
    is either 100Mb or 1Gb, while the *actual available bandwidth* for connections
    from outside the local network is considerably smaller. This consideration shows
    up in our queue definitions, where you clearly see that the bandwidth available
    for external traffic is the main limitation in the queue setup.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the `total_ext` bandwidth limitation determines the allocation for
    all queues where the bandwidth for external connections is available. In order
    to use the new queuing infrastructure, we need to make some changes to the filtering
    rules, too. Keep in mind that any traffic you don’t explicitly assign to a specific
    queue is assigned to the default queue for the interface. Thus, it’s important
    to tune your filtering rules as well as your queue definitions to the actual traffic
    in your network.
  prefs: []
  type: TYPE_NORMAL
- en: 'With queue assignment, the main part of the filtering rules could end up looking
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Notice that only traffic that will pass either the DMZ interface or the external
    interface is assigned to queues. In this configuration, with no externally accessible
    services on the internal network, queuing on the internal interface wouldn’t make
    much sense because it’s likely the part of our network with the least restrictions
    on available bandwidth.
  prefs: []
  type: TYPE_NORMAL
- en: Using ALTQ to Handle Unwanted Traffic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, we’ve focused on queuing as a method to make sure specific kinds of
    traffic are let through as efficiently as possible given the conditions that exist
    in and around your network. Now, we’ll look at two examples that present a slightly
    different approach to identify and handle unwanted traffic in order to demonstrate
    some queuing-related tricks you can use to keep miscreants in line.
  prefs: []
  type: TYPE_NORMAL
- en: Overloading to a Tiny Queue
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Think back to [Turning Away the Brutes](ch06.html#turning_away_the_brutes "Turning
    Away the Brutes"), where we used a combination of state-tracking options and `overload`
    rules to fill up a table of addresses for special treatment. The special treatment
    we demonstrated in [Chapter 6](ch06.html "Chapter 6. Turning the Tables for Proactive
    Defense") was to cut all connections, but it’s equally possible to assign `overload`
    traffic to a specific queue instead.
  prefs: []
  type: TYPE_NORMAL
- en: Consider this rule from our class-based bandwidth example in [Class-Based Bandwidth
    Allocation for Small Networks](ch07.html#class-based_bandwidth_allocation_for_sma
    "Class-Based Bandwidth Allocation for Small Networks").
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: We could add state-tracking options, as shown in here.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Then, we could make one of the queues slightly smaller.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Next, we could assign traffic from miscreants to the small-bandwidth queue with
    the following rule.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: It might also be useful to supplement rules like these with table-entry expiry,
    as described in [Tidying Your Tables with pfctl](ch06.html#tidying_your_tables_with_pfctl
    "Tidying Your Tables with pfctl").
  prefs: []
  type: TYPE_NORMAL
- en: Queue Assignments Based on Operating System Fingerprint
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[Chapter 6](ch06.html "Chapter 6. Turning the Tables for Proactive Defense")
    covered several ways to use `spamd` to cut down on spam. If running `spamd` isn’t
    an option in your environment, you can use a queue and rule set based on the common
    knowledge that machines that send spam are likely to run a particular operating
    system.'
  prefs: []
  type: TYPE_NORMAL
- en: PF has a fairly reliable operating system fingerprinting mechanism, which detects
    the operating system at the other end of a network connection based on characteristics
    of the initial SYN packets at connection setup. The following may be a simple
    substitute for `spamd` if you’ve determined that legitimate mail is highly unlikely
    to be delivered from systems that run that particular operating system.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Here, email traffic originating from hosts that run a particular operating system
    get no more than 1KB of your bandwidth, with no borrowing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Conclusion: Traffic Shaping for Fun, and Perhaps Even Profit'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This chapter has dealt with traffic-shaping techniques that can make your traffic
    move faster, or at least make preferred traffic pass more efficiently and according
    to your specifications. By now you should have at least a basic understanding
    of traffic-shaping concepts and how they apply to the traffic-shaping tool set
    you’ll be using on your systems.
  prefs: []
  type: TYPE_NORMAL
- en: I hope that the somewhat stylized (but functional) examples in this chapter
    have given you a taste of what’s possible with traffic shaping and that the material
    has inspired you to play with some of your own ideas of how you can use the traffic-shaping
    tools in your networks. If you pay attention to your network traffic and the underlying
    needs it expresses (see [Chapter 9](ch09.html "Chapter 9. Logging, Monitoring,
    and Statistics") and [Chapter 10](ch10.html "Chapter 10. Getting Your Setup Just
    Right") for more on studying network traffic in detail), you can use the traffic-shaping
    tools to improve the way your network serves its users. With a bit of luck, your
    users will appreciate your efforts and you may even enjoy the experience.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: ^([[39](#ch07fn01)]) Daniel Hartmeier, one of the original PF developers, wrote
    a nice article about this problem, which is available at *[http://www.benzedrine.cx/ackpri.html](http://www.benzedrine.cx/ackpri.html)*.
    Daniel’s explanations use the older ALTQ priority queues syntax but include data
    that clearly illustrates the effect of assigning two different priorities to help
    ACKs along.
  prefs: []
  type: TYPE_NORMAL
- en: ^([[40](#ch07fn01a)]) This really dates the book, I know. In a few years, these
    numbers will seem quaint.
  prefs: []
  type: TYPE_NORMAL
- en: ^([[41](#ch07fn02)]) The original research on ALTQ was presented in a paper
    for the USENIX 1999 conference. You can read Kenjiro Cho’s paper “Managing Traffic
    with ALTQ” online at *[http://www.usenix.org/publications/library/proceedings/usenix99/cho.html](http://www.usenix.org/publications/library/proceedings/usenix99/cho.html)*.
    The code turned up in OpenBSD soon after through the efforts of Cho and Chris
    Cappucio.
  prefs: []
  type: TYPE_NORMAL
