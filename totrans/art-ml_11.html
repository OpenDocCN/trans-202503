<html><head></head><body>
<div id="sbo-rt-content" class="calibre1"><h2 class="h1" id="ch08"><span epub:type="pagebreak" id="page_123" class="calibre2"/><strong class="calibre3"><span class="big">8</span><br class="calibre18"/>PARAMETRIC METHODS</strong></h2>
<div class="imagec"><img alt="Image" src="../images/common.jpg" class="calibre14"/></div>
<p class="noindent">Recall the term <em class="calibre13">regression function</em>, first introduced in <a href="ch01.xhtml#ch01lev6" class="calibre12">Section 1.6</a> and denoted by <em class="calibre13">r</em>(<em class="calibre13">t</em>). It’s the mean <em class="calibre13">Y</em> in the subpopulation defined by the condition <em class="calibre13">X</em> = <em class="calibre13">t</em>. The example we gave then involved bike ridership data:</p>
<p class="blockquote">A regression function has as many arguments as we have features. Let’s take humidity as a second feature, for instance. To predict ridership for a day with temperature 28 and humidity 0.51, we would use the mean ridership in our dataset, among days in which temperature and humidity are approximately 28 and 0.51. In regression function notation, that’s <em class="calibre13">r</em>(28, 0.51).</p>
<p class="indent">Basically, ML methods all are techniques to estimate the regression function from sample data. With k-NN, we would estimate <em class="calibre13">r</em>(28, 0.51) in the bike ridership example by calculating the mean ridership among the days in the neighborhood of (28,0.51). With trees, we would plug (28,0.51) into our tree, follow the proper branches, and then calculate the mean ridership in the resulting leaf node, which acts like a neighborhood.</p>
<p class="indent">So far, we have not made any assumptions about the shape of the regression function graph. In this chapter, we will assume the shape is that of a straight line, or planes and so on in higher dimensions.</p>
<p class="indent"><span epub:type="pagebreak" id="page_124"/>The so-called <em class="calibre13">linear model</em> is quite old, a couple of centuries old, actually. It can work fairly well in “easy” prediction applications, and even in some “advanced” ones. Indeed, we will see in <a href="ch08.xhtml#ch08lev13" class="calibre12">Section 8.13</a> that a variant of the linear model can often outperform more sophisticated ML models.</p>
<p class="indent">The linear model should thus be in every analyst’s toolkit. But an even more compelling reason to know the linear model is that it forms the basis of some of the most popular and powerful ML algorithms, including the LASSO, support vector machines, and neural networks, which we will cover in the succeeding chapters of this book.</p>
<h3 class="h2" id="ch08lev1">8.1 Motivating Example: The Baseball Player Data</h3>
<p class="noindent">We’ll soon introduce the <span class="literal">qe*</span>-series function for linear models, <span class="literal">qeLin()</span>. But to understand what it does, let’s start with a simple setting in which we have only one feature and use it to motivate the concept of a linear model.</p>
<p class="indent">Recall the dataset <span class="literal">mlb</span> in <a href="ch01.xhtml#ch01lev8" class="calibre12">Section 1.8</a> that is included with <span class="literal">regtools</span>. Let’s restrict attention to just heights and weights of the players:</p>
<pre class="calibre16">&gt; <span class="codestrong">data(mlb)</span>
&gt; <span class="codestrong">hw &lt;- mlb[,2:3]</span></pre>
<p class="noindent">Here, <em class="calibre13">X</em> and <em class="calibre13">Y</em> will be height and weight, respectively.</p>
<h4 class="h3" id="ch08lev1sec1"><em class="calibre22"><strong class="calibre3">8.1.1 A Graph to Guide Our Intuition</strong></em></h4>
<p class="noindent">So, we are predicting weight from height. In the <em class="calibre13">r</em>() notation, that means that if we wish to predict the weight of a new player whose height is 71 inches, we need to estimate <em class="calibre13">r</em>(71). This is the mean weight of all players in the subpopulation of players having height 71.</p>
<p class="indent">We don’t know population values, as we only have a sample from the population. (As noted earlier, we consider our data to be a sample from the population of all players, past, present, and future.) How, then, can we estimate <em class="calibre13">r</em>(71)? The natural estimate is the analogous sample quantity, the mean weight of all height 71 players in our sample:</p>
<pre class="calibre16"># find indices of data rows having height 71
&gt; <span class="codestrong">ht71 &lt;- which(hw$Height == 71)</span>
# find the average weight in those rows
&gt; <span class="codestrong">mean(hw$Weight[ht71])</span>
[1] 190.3596</pre>
<p class="noindent">Recalling that the “hat” notation means “estimate of,” we have that <img alt="Image" class="middle1" src="../images/rcap1.jpg"/>(71) = 190.3596. With deft usage of R’s <span class="literal">tapply()</span> function, we can get all the estimated <em class="calibre13">r</em>() values:<span epub:type="pagebreak" id="page_125"/></p>
<pre class="calibre16">&gt; <span class="codestrong">meanWts &lt;- tapply(hw$Weight,hw$Height,mean)</span>
&gt; <span class="codestrong">meanWts</span>
      67       68       69       70       71       72       73       74
172.5000 173.8571 179.9474 183.0980 190.3596 192.5600 196.7716 202.4566
      75       76       77       78       79       80       81       82
208.7161 214.1386 216.7273 220.4444 218.0714 237.4000 245.0000 240.5000
      83
260.0000</pre>
<p class="noindent">This says, “Group the weight values by height, and find the mean weight in each group.” By the way, note that the heights are available as the names of the weight items:</p>
<pre class="calibre16">&gt; <span class="codestrong">meanWts['70']</span>
     70
183.098</pre>
<p class="indent">Let’s plot the estimated mean weights against height:</p>
<pre class="calibre16">&gt; <span class="codestrong">plot(names(meanWts),meanWts)</span></pre>
<p class="indent"><a href="ch08.xhtml#ch08fig01" class="calibre12">Figure 8-1</a> shows the result.</p>
<div class="image"><img alt="Image" id="ch08fig01" src="../images/ch08fig01.jpg" class="calibre38"/></div>
<p class="figcap"><em class="calibre13">Figure 8-1: Estimated regression function, weight vs. height</em></p>
<p class="noindent">Remarkably, the points seem to nearly lie on a straight line. This suggests a model for <em class="calibre13">r</em>(<em class="calibre13">t</em>),</p>
<div class="imagec" id="ch08equ01"><img alt="Image" src="../images/ch08equ01.jpg" class="calibre39"/></div>
<p class="noindent">for some unknown values of the slope <em class="calibre13">m</em> and intercept <em class="calibre13">b</em> that we will estimate from the data. We are assuming that the graph of <em class="calibre13">r</em>(<em class="calibre13">t</em>) is <em class="calibre13">some</em> straight line, <span epub:type="pagebreak" id="page_126"/>though we don’t know which one—that is, we don’t know <em class="calibre13">b</em> and <em class="calibre13">m</em>. This is the linear model.</p>
<p class="indent">Keep in mind <em class="calibre13">r</em>(<em class="calibre13">t</em>) is the <em class="calibre13">mean Y</em> for the subpopulation <em class="calibre13">X</em> = <em class="calibre13">t</em>, so we are modeling <em class="calibre13">mean Y</em> and not <em class="calibre13">Y</em> itself. We are not saying <a href="ch08.xhtml#ch08equ01" class="calibre12">Equation 8.1</a> gives us the weight of individual players, though we do use the equation as the basis of our predictions.</p>
<h4 class="h3" id="ch08lev1sec2"><em class="calibre22"><strong class="calibre3">8.1.2 View as Dimension Reduction</strong></em></h4>
<p class="noindent">If <a href="ch08.xhtml#ch08equ01" class="calibre12">Equation 8.1</a> is a valid model, we have greatly simplified our problem.</p>
<p class="indent">Ordinarily, we would need to estimate many different values of <em class="calibre13">r</em>(<em class="calibre13">t</em>), such as those for <em class="calibre13">t</em> equal to 68, 69, 70, 71, 72, 73, and so on, say, 15 or 20 of them. But with the above model, <em class="calibre13">we need to estimate only two numbers</em>, <em class="calibre13">m</em> and <em class="calibre13">b</em>. As such, this is a form of dimension reduction.</p>
<h3 class="h2" id="ch08lev2">8.2 The lm() Function</h3>
<p class="noindent">Assuming the linear model (again, we’ll address its validity shortly), we can use R’s <span class="literal">lm()</span> function to estimate <em class="calibre13">m</em> and <em class="calibre13">b</em>:</p>
<pre class="calibre16">&gt; <span class="codestrong">lmout &lt;- lm(Weight ~ .,data=hw)</span>
&gt; <span class="codestrong">lmout</span>
<br class="calibre1"/>
Call:
lm(formula = Weight ~ ., data = hw)
<br class="calibre1"/>
Coefficients:
(Intercept)       Height
   -151.133        4.783</pre>
<p class="noindent">So, <img alt="Image" class="middle5" src="../images/unch08equ01.jpg"/> and <img alt="Image" class="middle6" src="../images/unch08equ02.jpg"/></p>
<p class="indent">Let’s see what this call is saying:</p>
<pre class="calibre16">lm(Weight ~ .,data=hw)</pre>
<p class="noindent">Here we are requesting that R fit a linear model to our data frame <span class="literal">hw</span>, predicting weight. The dot (.) means “all other columns,” which, in this case, is just the height column.</p>
<p class="indent">To predict the weight of a new player of height 71, we would compute:</p>
<div class="imagec" id="ch08equ02"><img alt="Image" src="../images/ch08equ02.jpg" class="calibre40"/></div>
<p class="noindent">But hey, we should have the computer do this computation rather than do it by hand:</p>
<pre class="calibre16">&gt; <span class="codestrong">predict(lmout,data.frame(Height=71))</span>
       1
188.4833</pre>
<p class="noindent"><span epub:type="pagebreak" id="page_127"/>The slight discrepancy is due to a roundoff error in the computation by hand, where our data was given only to a few digits.</p>
<h3 class="h2" id="ch08lev3">8.3 Wrapper for lm() in the qe*-Series: qeLin()</h3>
<p class="noindent">The <span class="literal">lm()</span> function is so basic in R that everyone should see it at least once, so we used it in the last section. But for simplicity and uniformity, we will use its <span class="literal">qe*</span>-series wrapper, <span class="literal">qeLin()</span>.</p>
<p class="indent">Here’s how to do the above computations in <span class="literal">qeLin()</span>:</p>
<pre class="calibre16">&gt; <span class="codestrong">qelout &lt;- qeLin(hw,'Weight',holdout=NULL)</span>
&gt; <span class="codestrong">qelout$coef</span>
(Intercept)      Height
-151.133291    4.783332
&gt; <span class="codestrong">predict(qelout,data.frame(Height=71))</span>
       2
188.4833</pre>
<p class="indent">Most applications have more than just one feature. We cover the general case next.</p>
<h3 class="h2" id="ch08lev4">8.4 Use of Multiple Features</h3>
<p class="noindent">We can, and typically do, fit the model to more than one feature.</p>
<h4 class="h3" id="ch08lev4sec1"><em class="calibre22"><strong class="calibre3">8.4.1 Example: Baseball Player, Continued</strong></em></h4>
<p class="noindent">Say we add in age, so our linear model is:</p>
<p class="center" id="ch08equ03"><img alt="Image" src="../images/ch08equ03.jpg" class="calibre41"/></p>
<p class="noindent">For the purpose of terminology (used here and later), let’s write this as</p>
<p class="center" id="ch08equ04"><img alt="Image" src="../images/ch08equ04.jpg" class="calibre42"/></p>
<p class="noindent">where <em class="calibre13">D</em> is an artificial variable that is always equal to 1. Then we say that mean weight is a <em class="calibre13">linear combination</em> of the variables <em class="calibre13">D</em>, <em class="calibre13">height</em>, and <em class="calibre13">age</em>. This is just a term meaning that to get mean weight, we multiply each of the three variables <em class="calibre13">D</em>, <em class="calibre13">height</em>, and <em class="calibre13">age</em> by the corresponding coefficients <em class="calibre13">b</em>, <em class="calibre13">m</em><sub class="calibre27">1</sub>, and <em class="calibre13">m</em><sub class="calibre27">2</sub> and sum up the result.</p>
<p class="indent">We now are using columns 4, 5, and 6 of <span class="literal">mlb</span>, so we fit the model as follows, say, for age 28:</p>
<pre class="calibre16">&gt; <span class="codestrong">qelout &lt;- qeLin(mlb[,4:6],'Weight',holdout=NULL)</span>
&gt; <span class="codestrong">predict(qelout,data.frame(Height=71,Age=28))</span>
      <span class="codestrong">11</span>
187.4603</pre>
<h4 class="h3" id="ch08lev4sec2"><span epub:type="pagebreak" id="page_128" class="calibre2"/><em class="calibre22"><strong class="calibre3">8.4.2 Beta Notation</strong></em></h4>
<p class="noindent">Since the reader of this book will likely see other discussions, say, on the web, it should be mentioned that it’s traditional to use the Greek letter <em class="calibre13">β</em> for the coefficients. For instance, <a href="ch08.xhtml#ch08equ03" class="calibre12">Equation 8.3</a> would be written as:</p>
<p class="center" id="ch08equ05"><img alt="Image" src="../images/ch08equ05.jpg" class="calibre43"/></p>
<p class="noindent">We will estimate <em class="calibre13">β</em><sub class="calibre27">0</sub>, <em class="calibre13">β</em><sub class="calibre27">1</sub>, and <em class="calibre13">β</em><sub class="calibre27">2</sub> from our sample data, as seen in <a href="ch08.xhtml#ch08lev4sec4" class="calibre12">Section 8.4.4</a>. And, recalling that we use the hat notation for estimates, our estimated coefficients will be denoted by <img alt="Image" class="middle7" src="../images/unch08equ03.jpg"/> and <img alt="Image" class="middle8" src="../images/unch08equ04.jpg"/>.</p>
<h4 class="h3" id="ch08lev4sec3"><em class="calibre22"><strong class="calibre3">8.4.3 Example: Airbnb Data</strong></em></h4>
<p class="noindent">The short-term housing firm Airbnb makes available voluminous rental data. Here we look at some data from San Francisco.<sup class="calibre11"><a id="ch8fn1b" class="calibre12"/><a href="footnote.xhtml#ch8fn1" class="calibre12">1</a></sup> (The dataset used here, from February 1, 2019, appears to no longer be available.) It will not only provide another example of the linear model, but it also will illustrate some data-cleaning issues.</p>
<h5 class="h4" id="ch08lev4sec3sec1">8.4.3.1 Data Preparation</h5>
<p class="noindent">After downloading the data and reading it into R (details not shown), we had a data frame <span class="literal">Abb</span>, which still required a lot of attention.</p>
<p class="indent">Many of the features are textual, for example:</p>
<pre class="calibre16">&gt; <span class="codestrong">Abb[1,]$house_rules</span>
[1] "* No Pets - even visiting guests for a short time period. * No Smokers..."</pre>
<p class="noindent">We will treat the topic of text data later in this book but removed it for this example.</p>
<p class="indent">Another problem is that prices include dollar signs and commas, for example:</p>
<pre class="calibre16">&gt; <span class="codestrong">Abb[1,]$monthly_price</span>
[1] "$4,200.00"</pre>
<p class="noindent">Dealing with such issues tends to take up a remarkably large portion of a data scientist’s job. Here we wrote a function to convert a column <span class="literal">d</span> of such numbers to the proper form, using a couple of R’s character string manipulation facilities:</p>
<pre class="calibre16">convertFromDollars &lt;- function(d) {
   d &lt;- as.character(d)
   # replace dollar sign by ''
   d &lt;- sub('\\$','',d,fixed=F)
   # replace commas by ''
   d &lt;- gsub(',','',d)
   d &lt;- as.numeric(d)
   # some entries were ''; replace by NAs
   d[d == ''] &lt;- NA
   d
}</pre>
<p class="indent"><span epub:type="pagebreak" id="page_129"/>And, not surprisingly, this dataset seems to have its share of erroneous entries:</p>
<pre class="calibre16">&gt; <span class="codestrong">table(Abb$square_feet)</span>
<br class="calibre1"/>
   0    1    2   14  120  130  140  150  160  172  175  195  250  280  300  360
   2    3    2    1    1    1    3    2    1    1    1    1    2    2    4    2
 400  450  500  538  550  600  650  700  750  780  800  810  815  840  850  853
   1    2    8    1    1    4    1    3    5    1    4    1    1    1    1    1
 890  900  950 1000 1012 1019 1100 1200 1390 1400 1490 1500 1600 1660 1750 1800
   1    2    3    9    1    1    2    9    1    2    1    7    1    1    1    3
1850 1900 1996 2000 2100 2200 2250 2600 3000
   1    1    1    4    3    2    1    1    4</pre>
<p class="noindent">For instance, areas of 1 and 2 square feet are listed, obviously incorrect. We will not pursue this further here, but clearly we would have a lot more work to do if this were not merely an example for the book.</p>
<p class="indent">After data cleaning, the data frame looks like this:</p>
<pre class="calibre16">&gt; <span class="codestrong">head(Abb)</span>
  zipcode bathrooms bedrooms square_feet weekly_price monthly_price
1   94117       1.0        1          NA         1120          4200
2   94110       1.0        2          NA         1600          5500
3   94117       4.0        1          NA          485          1685
4   94117       4.0        1          NA          490          1685
5   94117       1.5        2          NA           NA            NA
6   94115       1.0        2          NA           NA            NA
  security_deposit guests_included minimum_nights maximum_nights
1              100               2              1             30
2               NA               2             30             60
3              200               1             32             60
4              200               1             32             90
5                0               2              7           1125
6                0               1              2            365
  review_scores_rating
1                   97
2                   98
3                   85
4                   93
5                   97
6                   90</pre>
<p class="indent">We are now ready to perform the analysis.<span epub:type="pagebreak" id="page_130"/></p>
<h4 class="h3" id="ch08lev4sec4"><em class="calibre22"><strong class="calibre3">8.4.4 Applying the Linear Model</strong></em></h4>
<p class="noindent">Here is the call, omitting the square footage and weekly price columns:</p>
<pre class="calibre16">&gt; <span class="codestrong">linout &lt;- qeLin(Abb[,-c(4,5)],'monthly_price',holdout=NULL)</span>
&gt; <span class="codestrong">linout$coef</span>
         (Intercept)         zipcode94103         zipcode94104
       -4.485690e+03        -4.441996e+02         6.364539e+02
        zipcode94105         zipcode94107         zipcode94108
        1.012009e+03        -2.846037e+02        -1.649897e+03
        zipcode94109         zipcode94110         zipcode94111
       -3.945963e+02        -1.113476e+03         1.619558e+03
        zipcode94112         zipcode94114         zipcode94115
       -2.304310e+03        -2.607913e+02        -3.881351e+02
        zipcode94116         zipcode94117         zipcode94118
       -1.959336e+03        -1.543353e+02        -1.362785e+03
        zipcode94121         zipcode94122         zipcode94123
       -1.315474e+03        -1.434050e+03         1.639610e+03
        zipcode94124         zipcode94127         zipcode94131
       -2.309765e+03        -2.127720e+03        -1.525655e+03
        zipcode94132         zipcode94133         zipcode94134
       -1.675761e+03         6.496800e+02        -1.370148e+03
        zipcode94158            bathrooms             bedrooms
       -2.509281e+03         2.025493e+02         1.540830e+03
    security_deposit      guests_included       minimum_nights
        3.462443e-01         3.663498e+02        -6.400597e-01
      maximum_nights review_scores_rating
       -2.371457e-04         6.613115e+01</pre>
<p class="noindent">As is common in R, the estimated coefficients are displayed here in <em class="calibre13">scientific notation</em>, in which, for instance, 1.605326<em class="calibre13">e</em> + 03 = 1.605326 × 10<sup class="calibre11">3</sup> = 1605.326. So, for instance, <img alt="Image" class="middle9" src="../images/unch08equ05.jpg"/> is about −4,486, <img alt="Image" class="middle10" src="../images/unch08equ06.jpg"/> is about −444, and so on.</p>
<p class="indent">Note that <span class="literal">lm()</span> (via its wrapper <span class="literal">qeLin()</span>) has converted the ZIP code feature, an R factor, to dummy variables. Recall that typically we have one fewer dummy than the number of categories—in this case, the number of ZIP codes. R leaves out the first one here, which is 94102.</p>
<p class="indent">Since our focus in this book is on prediction rather than causal interpretation, the estimated coefficients are of lesser interest. Furthermore, one must be very careful in interpreting coefficients. Nevertheless, some comments regarding the coefficients are in order, next.</p>
<h3 class="h2" id="ch08lev5">8.5 Dimension Reduction</h3>
<p class="noindent">Let’s discuss this fundamental ML topic in the context of linear models and the Airbnb example in the previous section.<span epub:type="pagebreak" id="page_131"/></p>
<h4 class="h3" id="ch08lev5sec1"><em class="calibre22"><strong class="calibre3">8.5.1 Which Features Are Important?</strong></em></h4>
<p class="noindent">As noted in <a href="ch03.xhtml#ch03lev1sec1" class="calibre12">Section 3.1.1</a>, there are more than 40,000 ZIP codes in the United States; this is typically far too many to use directly. In San Francisco, the number is manageable, but still we may wish to drop the ones that seem unimportant to our predictions.</p>
<p class="indent">On the other hand, as real estate agents say, “Location, location, location.” ZIP code should matter a lot, and the estimated coefficients at least seem to confirm this. For instance, according to the coefficient estimates given earlier, a property in ZIP code 94105, on average, commands a price premium of about $1,012, while one in 94107 will, on average, cost about $285 below market, holding all other variables fixed. But what are the terms <em class="calibre13">premium</em> and <em class="calibre13">cost less</em> relative to here? Since ZIP code 94102 was omitted, we see that 94105 costs about $1,012 more on average than 94102—the ZIP code term in <img alt="Image" class="middle1" src="../images/rcap1.jpg"/>(<em class="calibre13">t</em>) would be about 1 · 1012 for a property in that ZIP code, while it would be 0 for one in 94102, since there is no dummy variable for that ZIP code. Similarly, 94107 runs about $444 below 94102, and so on. In other words, 94102 becomes the baseline ZIP code.</p>
<p class="indent">But . . . note the phrasing above: “the estimated coefficients at least <em class="calibre13">seem</em> to confirm this.” After all, we are working with <em class="calibre13">estimates</em> of finite accuracy. This is a vital point to take into account, which we will do next.</p>
<p class="indent">On the other hand, the amount of <span class="literal">security_deposit</span> seems not to matter much at all, so we should consider dropping it from our analysis. Recall that having more features means less bias but more variance. Since the effect of a security deposit in prediction values seems small, dropping this feature should add very little bias. The same statement holds for the features <span class="literal">minimum_nights</span> and <span class="literal">maximum_nights</span>.</p>
<h4 class="h3" id="ch08lev5sec2"><em class="calibre22"><strong class="calibre3">8.5.2 Statistical Significance and Dimension Reduction</strong></em></h4>
<p class="noindent">In the previous section, we suggested several features to drop from our analysis. But we did so only on the basis of a “feeling.” It is natural to desire some magic formula that will determine which features to retain and which to remove. But alas, as has been explained in this book, no such magic formula exists. We have cited a few methods, such as cross-validation and PCA, that are commonly used, but again, these are not magic, foolproof solutions.</p>
<p class="indent">In this section, we look at the use of <em class="calibre13">statistical significance</em> for dimension reduction in parametric models. <em class="calibre13">We do not recommend it</em>, and it is less favored than in the past, but it is still popular among many analysts. Thus, it is imperative to cover the technique here.</p>
<p class="indent">First, we will need to introduce a new R generic function (<a href="ch01.xhtml#ch01lev5sec1" class="calibre12">Section 1.5.1</a>). In addition to <span class="literal">print()</span>, <span class="literal">plot()</span>, and <span class="literal">predict()</span>, another common generic function in R is <span class="literal">summary()</span>. It does what its name implies; that is, it provides a summary of the object.</p>
<p class="indent">Recall that a generic function is tailored to the class of the object at hand. What is the class of our object here, <span class="literal">linout</span>?<span epub:type="pagebreak" id="page_132"/></p>
<pre class="calibre16">&gt; <span class="codestrong">class(linout)</span>
[1] "qeLin" "lm"</pre>
<p class="noindent">So, if we make the call <span class="literal">summary(linout)</span>, the R interpreter will first check for a function <span class="literal">summary.qeLin()</span>. Since the <span class="literal">qeML</span> package has no such function, the interpreter will next look for <span class="literal">summary.lm()</span>, which does exist. Let’s see what the function gives us:</p>
<pre class="calibre16">&gt; <span class="codestrong">summary(linout)</span>
...
Coefficients:
                       Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)          -4.486e+03  1.478e+03  -3.034 0.002479 **
zipcode94103         -4.442e+02  5.177e+02  -0.858 0.391094
zipcode94104          6.365e+02  2.131e+03   0.299 0.765227
zipcode94105          1.012e+03  7.071e+02   1.431 0.152724
zipcode94107         -2.846e+02  4.906e+02  -0.580 0.561978
zipcode94108         -1.650e+03  6.354e+02  -2.597 0.009566 **
zipcode94109         -3.946e+02  4.955e+02  -0.796 0.426025
zipcode94110         -1.113e+03  4.280e+02  -2.601 0.009435 **
zipcode94111          1.620e+03  1.014e+03   1.598 0.110396
zipcode94112         -2.304e+03  4.761e+02  -4.840 1.52e-06 ***
zipcode94114         -2.608e+02  4.425e+02  -0.589 0.555770
zipcode94115         -3.881e+02  4.666e+02  -0.832 0.405719
zipcode94116         -1.959e+03  7.028e+02  -2.788 0.005412 **
zipcode94117         -1.543e+02  4.441e+02  -0.348 0.728269
zipcode94118         -1.363e+03  5.560e+02  -2.451 0.014434 *
zipcode94121         -1.315e+03  6.422e+02  -2.048 0.040819 *
zipcode94122         -1.434e+03  5.437e+02  -2.638 0.008493 **
zipcode94123          1.640e+03  5.507e+02   2.977 0.002985 **
zipcode94124         -2.310e+03  6.552e+02  -3.525 0.000444 ***
zipcode94127         -2.128e+03  6.051e+02  -3.516 0.000459 ***
zipcode94131         -1.526e+03  5.024e+02  -3.037 0.002459 **
zipcode94132         -1.676e+03  7.745e+02  -2.164 0.030746 *
zipcode94133          6.497e+02  5.402e+02   1.203 0.229402
zipcode94134         -1.370e+03  8.837e+02  -1.550 0.121376
zipcode94158         -2.509e+03  1.546e+03  -1.623 0.104905
bathrooms             2.025e+02  1.323e+02   1.531 0.125996
bedrooms              1.541e+03  1.071e+02  14.385  &lt; 2e-16 ***
security_deposit      3.462e-01  9.820e-02   3.526 0.000443 ***
guests_included       3.663e+02  5.897e+01   6.212 7.92e-10 ***
minimum_nights       -6.401e-01  2.670e+00  -0.240 0.810569
maximum_nights       -2.371e-04  2.132e-03  -0.111 0.911465
review_scores_rating  6.613e+01  1.432e+01   4.617 4.44e-06 ***
...</pre>
<p class="indent">One particular type of information computed here is standard errors, discussed next.<span epub:type="pagebreak" id="page_133"/></p>
<h5 class="h4" id="ch08lev5sec2sec1">8.5.2.1 Standard Errors</h5>
<p class="noindent">You can see above that a <em class="calibre13">standard error</em> is reported for each estimated coefficient <img alt="Image" class="middle11" src="../images/unch08equ07.jpg"/>. It’s the estimated standard deviation of <img alt="Image" class="middle11" src="../images/unch08equ07.jpg"/> over all possible samples for whatever population is being sampled. This gives us an idea as to how accurate <img alt="Image" class="middle11" src="../images/unch08equ07.jpg"/> is, using the following reasoning.</p>
<p class="indent">If the standard error is small, it says that if we had had a different set of sample data from the given population, <img alt="Image" class="middle11" src="../images/unch08equ07.jpg"/> probably would have come out to about the same value as what we got. In other words, we can treat <img alt="Image" class="middle11" src="../images/unch08equ07.jpg"/> as representative.</p>
<p class="indent">We can form an approximate 95 percent confidence interval (CI) for <em class="calibre13">β</em><sub class="calibre27"><em class="calibre13">i</em></sub> by adding and subtracting 1.96 times the standard error of <img alt="Image" class="middle11" src="../images/unch08equ07.jpg"/>.</p>
<p class="indent">For instance, consider the dummy variable <span class="literal">zipcode94134</span>. The estimated beta coefficient for this variable is −$1,370. This is relative to whichever ZIP code is the base, meaning the one for which there is no dummy variable. (Recall from <a href="ch01.xhtml#ch01lev4" class="calibre12">Section 1.4</a> that with a categorical feature, we have one fewer dummy than the number of categories.) As noted earlier, the omitted ZIP code is 94102. So, for a given security deposit, guest policy, and so on, this neighborhood is estimated to be more than $1,000 cheaper than the baseline. But look at the CI:</p>
<p class="center" id="ch08equ06"><img alt="Image" src="../images/ch08equ06.jpg" class="calibre44"/></p>
<p class="noindent">The CI suggests that this neighborhood actually could be hundreds of dollars more <em class="calibre13">expensive</em> than the base.</p>
<h5 class="h4" id="ch08lev5sec2sec2">8.5.2.2 Significance Tests</h5>
<p class="noindent">That last example suggests that the status of <span class="literal">zipcode94134</span> as a predictor of rent is inconclusive. We should thus seriously consider dropping it from our model. Remember the notion of the Bias-Variance Trade-off means that if a feature is not very helpful, then including it in our model may degrade our predictive ability.</p>
<p class="indent">But let’s consider another ZIP code, say, 94132. Here the CI is</p>
<p class="center" id="ch08equ07"><img alt="Image" src="../images/ch08equ07.jpg" class="calibre45"/></p>
<p class="noindent">which is entirely in negative territory. For this reason, it is flagged with an asterisk.</p>
<p class="indent">What do all those asterisks mean? Why are there double asterisks for some coefficients? Our focus in this book is not on statistics, but it is important for the reader to have at least an overview of the situation since it is common to use the asterisks as a guide for dimension reduction.</p>
<p class="indent">Roughly speaking, if the CI does not contain 0, the coefficient is flagged with an asterisk. It rates <em class="calibre13">two</em> asterisks if 0 is well outside the interval and three if the CI is far, far away from 0. A coefficient with one asterisk is termed <em class="calibre13">significant</em> (that is, significantly different from 0); one with two asterisks is called <em class="calibre13">highly significant</em>, and three asterisks wins a coefficient the accolade <em class="calibre13">very highly significant</em>.</p>
<p class="indent"><span epub:type="pagebreak" id="page_134"/>Well then, what constitutes “well outside the interval” and “far, far away from 0”? This is determined by the p-value. A p-value under 0.05 is significant, and it is highly or very highly significant if it is under 0.01 or 0.001, respectively.</p>
<p class="indent">The p-value is a certain probability whose convoluted definition we will skip. (Recall this term from <a href="ch05.xhtml#ch05lev5" class="calibre12">Section 5.5</a>.) Suffice it to say, under this approach to dimension reduction, one discards any feature with no asterisks, such as <span class="literal">zipcode94134</span>, and retains the others in the model. If one wants to exercise a little more caution, one might retain only the coefficients with at least two asterisks.</p>
<p class="indent">Today, many analysts, including myself, consider this approach to be flawed. Let’s see why. The short answer is that p-values are too dependent on the number of data points <em class="calibre13">n</em>. Actually, the standard error is inversely proportional to <img alt="Image" class="middle4" src="../images/unch08equ08.jpg"/>. This has quite an implication, as follows.</p>
<p class="indent">Suppose, hypothetically, that the estimated coefficient for, say, <span class="literal">zipcode94132</span> had been 1.4, with a standard error of 0.9. That would give us a confidence interval of:</p>
<p class="center" id="ch08equ08"><img alt="Image" src="../images/ch08equ08.jpg" class="calibre46"/></p>
<p class="noindent">This contains 0, hence no asterisks. And that’s probably a good thing, since this feature seems to have no real predictive power: being in that ZIP code makes an estimated difference in rent of only a dollar or so.</p>
<p class="indent">But what if we were fortunate to have 25 times as much data? Then <img alt="Images" class="middle4" src="../images/unch08equ08.jpg"/> would increase by a factor of 5, so the standard error would shrink by a factor of 5, coming out at approximately 0.18. It would change somewhat, as would the coefficient estimate 1.4, but in rough terms our CI would now be something like:</p>
<p class="center" id="ch08equ09"><img alt="Image" src="../images/ch08equ09.jpg" class="calibre47"/></p>
<p class="noindent">Ah, now it’s significant! Yay! But . . . the estimated coefficient would still be something like $1.40—less than $2! That variable can hardly help us predict rent. In other words, the so-called significant nature of that feature could really lead us astray.</p>
<p class="indent">Use of significance tests and p-values is frowned upon by many statisticians (including this author).<sup class="calibre11"><a id="ch8fn2b" class="calibre12"/><a href="footnote.xhtml#ch8fn2" class="calibre12">2</a></sup> The tests are especially unreliable in prediction applications. With large datasets, <em class="calibre13">every</em> feature will be declared “very highly significant” (three asterisks) regardless of whether the feature has substantial predictive power. A feature with a very small regression coefficient could be declared “significant,” in spite of being essentially useless as a predictor.<span epub:type="pagebreak" id="page_135"/></p>
<h5 class="h4" id="ch08lev5sec2sec3">8.5.2.3 Pitfall: NA Values and Impact on n</h5>
<p class="noindent">As shown above, this dataset also includes a number of NA values. We didn’t have to deal with this directly, since <span class="literal">lm()</span>, which <span class="literal">qeLin()</span> wraps, automatically restricts its computations to complete cases. Nevertheless, as noted in <a href="ch04.xhtml#ch04lev1" class="calibre12">Section 4.1</a>, if a dataset contains many NAs, this is yet another reason to seek dimension reduction, as doing so may increase the number of complete cases. This means less variance, which is very desirable. Some experimentation here reveals that removal of the most NA-prone features, beyond the ones we’ve already deleted, does not help increase data size in this particular case, but it is an important general principle.</p>
<h5 class="h4" id="ch08lev5sec2sec4">8.5.2.4 Pitfall: Difficulty of Forming Holdout Sets with Many-Level Categoricals</h5>
<p class="noindent">In our earlier Airbnb analysis, problems occur if we form a holdout set:</p>
<pre class="calibre16">&gt; <span class="codestrong">linout &lt;- qeLin(Abb[,-c(4,5)],'monthly_price')</span>
holdout set has  707 rows
Error in model.frame.default(Terms, newdata,
   na.action = na.action, xlev = object$xlevels) :
  factor zipcode has new levels 94014
&gt; <span class="codestrong">linout &lt;- qeLin(Abb[,-c(4,5)],'monthly_price')</span>
holdout set has  707 rows
Error in model.frame.default(Terms, newdata,
   na.action = na.action, xlev = object$xlevels) :
  factor zipcode has new levels 94014, 94106</pre>
<p class="indent">Of course, since the holdout set is chosen randomly, there may be a different result each time. But we see that in each of our two tries here, we had at least one error, “factor zipcode has new levels.” What is happening here?</p>
<p class="indent">The problem is that certain ZIP codes, such as 94014, appear in only a few data points. Apparently there were no 94014 cases in each training set here, so <span class="codeitalic1">lm()</span> <em class="calibre13">was “surprised” to see one in the holdout set.</em></p>
<p class="indent">The only solution would be to remove all cases with 94014 (and possibly others) from the data before running <span class="literal">qeLin()</span>.</p>
<h3 class="h2" id="ch08lev6">8.6 Least Squares and Residuals</h3>
<p class="noindent">Even though the computational details underlying <span class="literal">lm()</span> are beyond the scope of this book, it’s important to have a rough idea of what is involved, as similar computations will arise later in the book. This will bring in the notion of <em class="calibre13">least squares</em>. That, in turn, will lead to the idea of <em class="calibre13">residuals</em>, which are important in their own right.</p>
<p class="indent">For simplicity, let’s consider the context of <a href="ch08.xhtml#ch08lev2" class="calibre12">Section 8.2</a> here. The quantities <img alt="Image" class="middle12" src="../images/unch08equ09.jpg"/> and <img alt="Image" class="middle13" src="../images/unch08equ10.jpg"/> and so on are computed using the famous <em class="calibre13">ordinary least squares (OLS)</em> method, which works as follows.<span epub:type="pagebreak" id="page_136"/></p>
<p class="indent">Imagine that after we compute <img alt="Image" class="middle12" src="../images/unch08equ09.jpg"/> and <img alt="Image" class="middle13" src="../images/unch08equ10.jpg"/>, we go back and “predict” the weight of the first player in our sample data. As implied by the quotation marks, this would be silly; after all, we already know the weight of the first player, 180:</p>
<pre class="calibre16">&gt; <span class="codestrong">data(mlb)</span>
&gt; <span class="codestrong">mlb[1,]</span>
           Name Team Position Height Weight   Age
1 Adam_Donachie  BAL  Catcher     74    180 22.99
  PosCategory
1     Catcher</pre>
<p class="indent">But think through this exercise anyway. It will turn out to be the basis for how things work, both for linear models and all the ML methods in the remainder of the book.</p>
<p class="indent">Our predicted value would be <img alt="Image" class="middle14" src="../images/unch08equ11.jpg"/>. Thus our prediction error would be:</p>
<p class="center"><img alt="Image" src="../images/unch08equ12.jpg" class="calibre48"/></p>
<p class="noindent">This is the <em class="calibre13">residual</em> for that row in the dataset. (Recall that this was briefly mentioned in <a href="ch06.xhtml#ch06lev3sec2" class="calibre12">Section 6.3.2</a>.) We’ll square that error rather than using it in its raw form, as we will be summing errors and don’t want positive and negative ones to cancel. Now we “predict” all the other data points as well, and add up the squared errors:</p>
<p class="center" id="ch08equ10"><img alt="Image" src="../images/ch08equ10.jpg" class="calibre49"/></p>
<p class="indent">Now here is the point: the way <span class="literal">lm()</span> finds <img alt="Image" class="middle13" src="../images/unch08equ10.jpg"/> and <img alt="Image" class="middle12" src="../images/unch08equ09.jpg"/> is to set them to whatever values minimize the sum of squares (<a href="ch08.xhtml#ch08equ10" class="calibre12">Equation 8.10</a>). In other words, think of the expression as a function of two variables, <img alt="Image" class="middle13" src="../images/unch08equ10.jpg"/> and <img alt="Image" class="middle12" src="../images/unch08equ09.jpg"/>, and then minimize the expression with respect to those two variables. (Readers who know calculus may have spotted the fact that we set the two derivatives equal to 0 and solve for <img alt="Image" class="middle13" src="../images/unch08equ10.jpg"/> and <img alt="Image" class="middle12" src="../images/unch08equ09.jpg"/>.)</p>
<p class="indent">Since we are minimizing a sum of squares, the estimated coefficients are said to be the <em class="calibre13">least squares estimates</em>. (The word <em class="calibre13">ordinary</em> is often added, as ordinary least squares is distinct from some variants that we will not discuss here.)</p>
<h3 class="h2" id="ch08lev7">8.7 Diagnostics: Is the Linear Model Valid?</h3>
<p class="blockquotea"><em class="calibre13">All models are wrong, but some are useful.</em></p>
<p class="blockquoter">—George Box, famous early statistician</p>
<p class="noindenta1">The linearity assumption is pretty strong. When is it appropriate? Let’s take a closer look.<span epub:type="pagebreak" id="page_137"/></p>
<h4 class="h3" id="ch08lev7sec1"><em class="calibre22"><strong class="calibre3">8.7.1 Exactness?</strong></em></h4>
<p class="noindent">The reader may ask, “How can the linear model in <a href="ch08.xhtml#ch08equ01" class="calibre12">Equation 8.1</a> be valid?” Yes, the points in <a href="ch08.xhtml#ch08fig01" class="calibre12">Figure 8-1</a> look like they are kind of on a straight line, but not exactly so. There are two important answers:</p>
<ul class="calibre15">
<li class="noindent3">As the quote from George Box points out, no model is <em class="calibre13">exactly</em> correct. Commonly used physics models ignore things like air resistance and friction, and even models accounting for such things still don’t reflect all possible factors. A linear approximation to the regression function <em class="calibre13">r</em>(<em class="calibre13">t</em>) may do a fine job in prediction even if the model is not perfect.</li>
<li class="noindent3">Even if <a href="ch08.xhtml#ch08equ01" class="calibre12">Equation 8.1</a> were exactly correct, the points in <a href="ch08.xhtml#ch08fig01" class="calibre12">Figure 8-1</a> would not lie exactly on the line. Remember, <em class="calibre13">r</em>(71), for instance, is only the <em class="calibre13">mean</em> weight of all players of height 71. Most individual players of that height are heavier or lighter than that value, so their data points will not fall exactly on that line and, in fact, may be far from it in some cases. And the same point holds for the mean weights that we plotted in <a href="ch08.xhtml#ch08fig01" class="calibre12">Figure 8-1</a>; each of those means were based on only a few players.</li>
</ul>
<p class="indent">By the way, classical linear model methodology makes some assumptions beyond linearity, such as <em class="calibre13">Y</em> having a normal distribution in each subpopulation. But these are not relevant to our prediction context. (Actually, even for statistical inference, the normality assumption is not important in large samples.)</p>
<h4 class="h3" id="ch08lev7sec2"><em class="calibre22"><strong class="calibre3">8.7.2 Diagnostic Methods</strong></em></h4>
<p class="noindent">Over the years, analysts have developed a number of methods to check the validity of the linear model. Several are described in my book <em class="calibre13">Statistical Regression and Classification: From Linear Models to Machine Learning</em> (CRC Press, 2017).</p>
<p class="indent">Again, since we are interested in prediction rather than causal analysis, we will not cover this material here. As long as the outcome variable is an increasing or decreasing function of the features—for example, mean human weight is an increasing function of height—a linear model should do fairly well in prediction-oriented applications. With linear polynomial models (see <a href="ch08.xhtml#ch08lev11" class="calibre12">Section 8.11</a>), this can be refined.</p>
<h3 class="h2" id="ch08lev8">8.8 The R-Squared Value(s)</h3>
<p class="noindent">Recall that the estimated coefficients are calculated by minimizing the sum of squared differences between actual and predicted <em class="calibre13">Y</em> values (see <a href="ch08.xhtml#ch08lev6" class="calibre12">Section 8.6</a>). <em class="calibre13">R</em><sup class="calibre11">2</sup> is the squared correlation between actual and predicted <em class="calibre13">Y</em>.</p>
<p class="indent"><span epub:type="pagebreak" id="page_138"/>It can be shown that this can be interpreted as the proportion of variation in <em class="calibre13">Y</em> due to <em class="calibre13">X</em>. (As always, <em class="calibre13">X</em> refers collectively to all our features.) As such, we have 0 ≤ <em class="calibre13">R</em><sup class="calibre11">2</sup> ≤ 1, with a value of 1 meaning that <em class="calibre13">X</em> perfectly predicts <em class="calibre13">Y</em>. However, there is a big problem here, as we are predicting the same data that we used to estimate our prediction machine, the regression coefficients. If we are overfitting, then <em class="calibre13">R</em><sup class="calibre11">2</sup> will be overly optimistic.</p>
<p class="indent">This, of course, is the motivation for using holdout data. Thus <span class="literal">qeLin()</span> reports not only the standard <em class="calibre13">R</em><sup class="calibre11">2</sup> but also the <em class="calibre13">R</em><sup class="calibre11">2</sup> calculated on the holdout set (stored in the <span class="literal">holdoutR2</span> component of the <span class="literal">qeLin()</span> return value). The latter is more reliable. Furthermore, if there is a large discrepancy between the two, it suggests that we are overfitting.</p>
<p class="indent">Most linear regression software libraries also report the <em class="calibre13">adjusted R</em><sup class="calibre11">2</sup> value. The word <em class="calibre13">adjusted</em> here alludes to the fact that the formula attempts to correct for overfitting. The <span class="literal">qeLin()</span> reports this too, and again a large discrepancy between this value and the first <em class="calibre13">R</em><sup class="calibre11">2</sup> value suggests we are overfitting.</p>
<h3 class="h2" id="ch08lev9">8.9 Classification Applications: The Logistic Model</h3>
<p class="noindent">The linear model is designed for regression applications. What about classification? A generalization of the linear model, unsurprisingly called the <em class="calibre13">generalized linear model</em>, handles that. Here we will present one form of the model, the <em class="calibre13">logistic</em> model.</p>
<p class="indent">Recall the discussion at the beginning of <a href="ch02.xhtml" class="calibre12">Chapter 2</a>, which pointed out that in classification settings, where <em class="calibre13">Y</em> is either 1 or 0, the regression function becomes the probability of <em class="calibre13">Y</em> = 1 for the given subpopulation. If we fit a purely linear model with <span class="literal">lm()</span>, the estimated regression values may be outside the interval [0,1] and thus not represent a probability. We could, of course, truncate any value predicted by <span class="literal">lm()</span> to [0,1], but the <em class="calibre13">logistic</em> model provides a better approach.</p>
<p class="indent">The model takes its name from the logistic function <em class="calibre13">l</em>(<em class="calibre13">t</em>) = 1/(1 + <em class="calibre13">e</em><sup class="calibre11">−<em class="calibre13">t</em></sup>). Since that function takes values in (0,1), it is suitable for modeling a probability. We still use a linear form but run the form through the logistic function to squeeze it into (0,1) for probability modeling.</p>
<p class="indent">Say we wish to predict gender from height. Our model might be:</p>
<p class="center"><img alt="Image" src="../images/unch08equ13.jpg" class="calibre50"/></p>
<p class="noindent">Here <em class="calibre13">β</em><sub class="calibre27">0</sub> and <em class="calibre13">β</em><sub class="calibre27">1</sub> are, again, population values, which we estimate from our data.</p>
<p class="indent">The logistic and linear models are basically similar: in the linear model, <em class="calibre13">β</em><em class="calibre13"><sub class="calibre27">i</sub></em> is the impact that the <em class="calibre13">i</em><em class="calibre13"><sup class="calibre11">th</sup></em> feature has on mean <em class="calibre13">Y</em>, while in the logit case, <em class="calibre13">β</em><em class="calibre13"><sub class="calibre27">i</sub></em> is the impact that the <em class="calibre13">i</em><em class="calibre13"><sup class="calibre11">th</sup></em> feature has on the probability that <em class="calibre13">Y</em> = 1. (Some analysts view logit in terms of a linear model of the <em class="calibre13">log-odds ratio</em>, log(<em class="calibre13">P</em>(<em class="calibre13">Y</em> = 1| <em class="calibre13">X</em>) / [1 − <em class="calibre13">P</em>(<em class="calibre13">Y</em> = 1|<em class="calibre13">X</em>)]).)</p>
<p class="indent">The logistic model is often called the <em class="calibre13">logit</em> model for simplicity.<span epub:type="pagebreak" id="page_139"/></p>
<h4 class="h3" id="ch08lev9sec1"><em class="calibre22"><strong class="calibre3">8.9.1 The glm() and qeLogit() Functions</strong></em></h4>
<p class="noindent">In R, the standard function for the generalized linear model is <span class="literal">glm()</span>. In the logistic case, that function is wrapped by the <span class="literal">qeML</span> package function <span class="literal">qeLogit()</span>. The call form for the latter is:</p>
<pre class="calibre16">qeLogit(data,yName,
   holdout = floor(min(1000, 0.1 * nrow(data))),yesYVal = NULL)</pre>
<p class="noindent">The first three arguments are as in the other <span class="literal">qe*</span>-series functions. The last argument, <span class="literal">yesYVal</span>, is needed in the 2-class case. It specifies the value of <em class="calibre13">Y</em> that we wish to be coded as <em class="calibre13">Y</em> = 1.</p>
<h4 class="h3" id="ch08lev9sec2"><em class="calibre22"><strong class="calibre3">8.9.2 Example: Telco Churn Data</strong></em></h4>
<p class="noindent">In <a href="ch02.xhtml#ch02lev2" class="calibre12">Section 2.2</a>, we used k-NN to analyze some customer retention data. Let’s revisit that data, now using a logistic model. Recall that the <span class="literal">Churn</span> variable has values <span class="literal">'Yes'</span> and <span class="literal">'No'</span>.</p>
<pre class="calibre16"># data prep as before, not shown
&gt; <span class="codestrong">set.seed(9999)</span>
&gt; <span class="codestrong">glout &lt;- qeLogit(tc,'Churn',holdout=NULL,yesYVal='Yes')</span></pre>
<p class="noindent">Our model is:</p>
<p class="center"><img alt="Image" src="../images/unch08equ14.jpg" class="calibre51"/></p>
<p class="indent">Let’s predict a new case that is similar to, say, the 333rd one in our dataset but with a different gender:</p>
<pre class="calibre16">&gt; <span class="codestrong">names(tc)</span>
 [1] "gender"           "SeniorCitizen"    "Partner"          "Dependents"
 [5] "tenure"           "PhoneService"     "MultipleLines"    "InternetService"
 [9] "OnlineSecurity"   "OnlineBackup"     "DeviceProtection" "TechSupport"
[13] "StreamingTV"      "StreamingMovies"  "Contract"         "PaperlessBilling"
[17] "PaymentMethod"    "MonthlyCharges"   "TotalCharges"     "Churn"
&gt; <span class="codestrong">newx &lt;- tc[333,-20]</span>  # exclude Y
&gt; <span class="codestrong">newx</span>
    gender SeniorCitizen Partner Dependents tenure PhoneService MultipleLines
333 Male             0      No         No     46          Yes           Yes
    InternetService OnlineSecurity OnlineBackup DeviceProtection TechSupport
333     Fiber optic             No          Yes              Yes          No
    StreamingTV StreamingMovies Contract PaperlessBilling
333         Yes              No One year              Yes
              PaymentMethod MonthlyCharges TotalCharges
333 Credit card (automatic)           94.9      4422.95
&gt; <span class="codestrong">newx$gender &lt;- 'Female'</span>
&gt; <span class="codestrong">predict(glout,newx)</span>
$predClasses
[1] "No"
<br class="calibre1"/>
$probs
<br class="calibre1"/>
          [,1]
[1,] 0.2307227</pre>
<p class="noindent"><span epub:type="pagebreak" id="page_140"/>We guess that the customer will stay put—that is, not jump to another service provider—with a jump probability of only about 23 percent.</p>
<p class="indent">We also get a warning message:</p>
<pre class="calibre16">Warning messages:
1: In predict.lm(object, newdata, se.fit, scale = 1, type = if (type ==  :
  prediction from a rank-deficient fit may be misleading
2: In predict.lm(object, newdata, se.fit, scale = 1, type = if (type ==  :
  prediction from a rank-deficient fit may be misleading</pre>
<p class="noindent">This is a technical issue, occurring when the features are highly correlated. Here <span class="literal">glm()</span> has actually skipped over some of the features that are in essence redundant.</p>
<p class="indent">Sometimes <span class="literal">glm()</span> will give us a warning message like:</p>
<pre class="calibre16">glm.fit: fitted probabilities numerically 0 or 1 occurred</pre>
<p class="noindent">Again, this is a technical issue, which we will not pursue here. The reader may proceed as usual.</p>
<p class="indent">On the other hand, a warning that one cannot ignore is “failed to converge.” This will not happen with <span class="literal">lm()</span>, the R function wrapped by our <span class="literal">qeLinear()</span>, but it may occasionally occur with logit. This is usually remedied by performing some dimension reduction.</p>
<h4 class="h3" id="ch08lev9sec3"><em class="calibre22"><strong class="calibre3">8.9.3 Multiclass Case</strong></em></h4>
<p class="noindent">If there are more than two classes, we have two options. For concreteness, consider the vertebrae data from <a href="ch02.xhtml#ch02lev3" class="calibre12">Section 2.3</a>. There we had three classes, DH, NO, and SL. For now, just consider using <span class="literal">glm()</span> directly rather than its wrapper, <span class="literal">qeLogit()</span>.</p>
<div class="block3">
<p class="noindent1"><strong class="calibre5">One vs. All (OVA) method</strong> Here we run <span class="literal">glm()</span> once for each class. We first run logit with DH playing the role of <em class="calibre13">Y</em>. Then we do this with NO serving as <em class="calibre13">Y</em> and then finally the same for SL. That gives us three sets of coefficients—in fact, three return objects from <span class="literal">glm()</span>, say, <span class="literal">DHout</span>, <span class="literal">NOout</span>, and <span class="literal">SLout</span>. Then when predicting a new case <span class="literal">newx</span>, we run:</p>
<pre class="calibre16">predict(DHout,newx,type='response')
predict(NOout,newx,type='response')
predict(SLout,newx,type='response')</pre>
<p class="noindent"><span epub:type="pagebreak" id="page_141"/>This gives us three probabilities. We take as our prediction whichever class has the highest probability.</p>
<p class="noindent1z"><strong class="calibre5">All vs. All (AVA) method</strong> Here we run <span class="literal">glm()</span> once for each <em class="calibre13">pair</em> of classes. First, we restrict the data to just DH and NO, putting SL cases aside for the moment, and take <em class="calibre13">Y</em> as DH. Then we focus on just DH and SL, taking <em class="calibre13">Y</em> to be DH again. Finally, we put DH aside for the moment, running with NO and SL and taking <em class="calibre13">Y</em> to be NO. Again, that would give us three objects output from <span class="literal">glm()</span>.</p>
<p class="indent">Then we would call <span class="literal">predict()</span> three times on <span class="literal">newx</span>. Say the outcome with the first object is less than 0.5. That means between DH and NO, we would predict this new case to be NO—that is, NO “wins.” We do this on all three objects, and whichever class wins the most often is our predicted class.</p>
</div>
<p class="indent">The <span class="literal">qeLogit()</span> function uses the OVA approach. Since <span class="literal">qeLogit()</span> is a wrapper for <span class="literal">glm()</span>, we do not see the actions of the latter, and they are used only as intermediate internal computations. However, if desired, the results of the calls to <span class="literal">glm()</span> can be accessed in the <span class="literal">glOuts</span> component of the object returned by <span class="literal">qeLogit()</span>.</p>
<h4 class="h3" id="ch08lev9sec4"><em class="calibre22"><strong class="calibre3">8.9.4 Example: Fall Detection Data</strong></em></h4>
<p class="noindent">This dataset is included in <span class="literal">qeML</span>, originally from Kaggle.<sup class="calibre11"><a id="ch8fn3b" class="calibre12"/><a href="footnote.xhtml#ch8fn3" class="calibre12">3</a></sup> From the site:</p>
<p class="block">Falls are a serious public health problem and possibly life threatening for people in fall risk groups. We develop an automated fall detection system with wearable motion sensor units fitted to the subjects’ body at six different positions.</p>
<p class="noindent">There are six activity types, thus six classes, coded 0 (Standing), 1 (Walking), 2 (Sitting), 3 (Falling), 4 (Cramps), and 5 (Running). Let’s see how well we can predict the class:</p>
<pre class="calibre16">&gt; <span class="codestrong">data(falldetection)</span>
&gt; <span class="codestrong">fd &lt;- falldetection</span>
&gt; <span class="codestrong">head(fd)</span>
  ACTIVITY    TIME       SL      EEG BP  HR CIRCULATION
1        3 4722.92  4019.64 -1600.00 13  79         317
2        2 4059.12  2191.03 -1146.08 20  54         165
3        2 4773.56  2787.99 -1263.38 46  67         224
4        4 8271.27  9545.98 -2848.93 26 138         554
5        4 7102.16 14148.80 -2381.15 85 120         809
6        5 7015.24  7336.79 -1699.80 22  95         427
&gt; <span class="codestrong">fd$ACTIVITY &lt;- as.factor(fd$ACTIVITY)</span>  # was integer, need factor for qe*
&gt; <span class="codestrong">set.seed(9999)</span>
&gt; <span class="codestrong">fd$ACTIVITY &lt;- as.factor(fd$ACTIVITY)</span>
&gt; <span class="codestrong">fdout &lt;- qeLogit(fd,'ACTIVITY')</span>
&gt; <span class="codestrong">fdout$testAcc</span>
[1] 0.593
&gt; <span class="codestrong">fdout$baseAcc</span>
[1] 0.7186972
&gt; <span class="codestrong">table(fd$ACTIVITY)</span>
   0    1    2    3    4    5
4608  502 2502 3588 3494 1688</pre>
<p class="noindent"><span epub:type="pagebreak" id="page_142"/>We are correctly predicting only about 40 percent of the cases with our logit model, but that’s better than the 28 percent correct we’d get just guessing every case to be of Class 3, the most common class.</p>
<p class="indent">Let’s predict a hypothetical new case, say, like the first row in the data but with <span class="literal">BP</span> equal to 28:</p>
<pre class="calibre16">&gt; <span class="codestrong">newx &lt;- fd[1,-1]</span>
&gt; <span class="codestrong">newx</span>
     TIME      SL   EEG BP HR CIRCULATION
1 4722.92 4019.64 -1600 13 79         317
&gt; <span class="codestrong">newx$BP &lt;- 28</span>
&gt; <span class="codestrong">newx</span>
     TIME      SL   EEG BP HR CIRCULATION
1 4722.92 4019.64 -1600 28 79         317
&gt; <span class="codestrong">predict(fdout,newx)</span>
$predClasses
[1] "2"
<br class="calibre1"/>
$probs
             0         1         2         3         4          5
[1,] 0.2294015 0.1111428 0.2324076 0.1605359 0.1830733 0.08343888</pre>
<p class="noindent">Such a case would result in a prediction of Class 2, with a probability of about 23 percent.</p>
<h3 class="h2" id="ch08lev10">8.10 Bias and Variance in Linear/Generalized Linear Models</h3>
<p class="noindent">As discussed in <a href="ch03.xhtml" class="calibre12">Chapter 3</a>, the more features we use, the smaller the bias but the larger the variance. With parametric models, such as those in this chapter, the larger variance comes in the form of less-stable estimates of the coefficients, which, in turn, make later predictions less stable.</p>
<p class="indent">Once again, note that high variance for a coefficient estimate means that the value of that estimate will vary a lot from one sample to another. That large oscillation, in turn, means that the estimated coefficient vector will be less likely to be near the true population value.</p>
<p class="indent">Here we present a concrete example illustrating the fact that the variance increases with the complexity of the model.<span epub:type="pagebreak" id="page_143"/></p>
<h4 class="h3" id="ch08lev10sec1"><em class="calibre22"><strong class="calibre3">8.10.1 Example: Bike Sharing Data</strong></em></h4>
<p class="noindent">We can make that point about instability of predictions more concrete using the <span class="literal">regtools</span> function <span class="literal">stdErrPred()</span>. This function finds the standard error of a predicted value obtained from <span class="literal">lm()</span>. Recall from <a href="ch08.xhtml#ch08lev5sec2sec1" class="calibre12">Section 8.5.2.1</a> that the standard error of an estimator is the estimated standard deviation of that estimator. A larger standard error thus means more variability of the estimator from one sample to another.</p>
<p class="indent">We’ll fit two models, one with a smaller feature set and the other with a somewhat larger one, and then do the same prediction on each; just as an example, we’ll predict the third data point in the dataset. We will print out the two predictions and, most important, the standard error of the two predictions.</p>
<pre class="calibre16">&gt; <span class="codestrong">data(day1)</span>
&gt; <span class="codestrong">e1 &lt;- day1[,c(4,10,12,13,16)]</span>
&gt; <span class="codestrong">e2 &lt;- day1[,c(4,10,12,13,16,6,7)]</span>  # add holiday, weekday columns
&gt; <span class="codestrong">names(e1)</span>
[1] "yr"        "temp"      "hum"       "windspeed" "tot"
&gt; <span class="codestrong">names(e2)</span>
[1] "yr"        "temp"      "hum"       "windspeed" "tot"       "holiday"
[7] "weekday"
&gt; <span class="codestrong">set.seed(9999)</span>
&gt; <span class="codestrong">e1out &lt;- qeLin(e1,'tot')</span>
&gt; <span class="codestrong">e2out &lt;- qeLin(e2,'tot')</span>
&gt; <span class="codestrong">newx1 &lt;- e1[3,-5]</span>  # exclude tot
&gt; <span class="codestrong">newx2 &lt;- e2[3,-5]</span>  # exclude tot
&gt; <span class="codestrong">predict(e1out,newx1)</span>
      31
1818.779
&gt; <span class="codestrong">predict(e2out,newx2)</span>
       3
1689.054
&gt; <span class="codestrong">stdErrPred(e1out,newx1)</span>
[1] 97.77229
&gt; <span class="codestrong">stdErrPred(e2out,newx2)</span>
[1] 108.3989</pre>
<p class="noindent">So the prediction from the larger feature set has a larger standard error. The standard error is the standard deviation of an estimator—in this case, our estimate of prediction accuracy. So here we see the Bias-Variance Trade-off in action. The larger model, though more detailed and thus less biased, does have a larger variance.</p>
<p class="indent">Does that mean that we should use the smaller feature set? No. In order to see if we’ve hit the switchover point, we’d need to use cross-validation. But the reader should keep this concrete illustration of the trade-off in mind.<span epub:type="pagebreak" id="page_144"/></p>
<h3 class="h2" id="ch08lev11">8.11 Polynomial Models</h3>
<p class="noindent">Surprisingly, one can use linear regression methods to model nonlinear effects. We’ll see how in this section. Why is this important?</p>
<ul class="calibre15">
<li class="noindent3">A polynomial model can often match or even outperform many of the more glamorous ML models.</li>
<li class="noindent3">Polynomials will play an important role in our chapter on support vector machines (<a href="ch10.xhtml" class="calibre12">Chapter 10</a>), and even in our treatment of neural networks (<a href="ch11.xhtml" class="calibre12">Chapter 11</a>), where there is a surprising connection to polynomials.</li>
</ul>
<h4 class="h3" id="ch08lev11sec1"><em class="calibre22"><strong class="calibre3">8.11.1 Motivation</strong></em></h4>
<p class="noindent">We’ve used the example of programmer and engineer wages earlier in this book (see <a href="ch03.xhtml#ch03lev2sec3" class="calibre12">Section 3.2.3</a>). Consider the graph of wage income against age shown in <a href="ch08.xhtml#ch08fig02" class="calibre12">Figure 8-2</a>. There seems to be a steep rise in a worker’s 20s, then a long leveling off, with a hint even of a decline after age 55 or so. This is definitely not a linear relationship.</p>
<div class="image"><img alt="Image" id="ch08fig02" src="../images/ch08fig02.jpg" class="calibre52"/></div>
<p class="figcap"><em class="calibre13">Figure 8-2: Wage income vs. age</em></p>
<p class="indent">Or consider <a href="ch08.xhtml#ch08fig03" class="calibre12">Figure 8-3</a>, for the bike sharing data, graphing total ridership against temperature. The nonlinear relationship is even clearer here. (We seem to see two groups here, possibly for the registered and casual riders.) No surprise, of course—people don’t want to go bike riding if the weather is too cold or too hot—but again, the point is that a linear model would seem questionable.<span epub:type="pagebreak" id="page_145"/></p>
<div class="image"><img alt="Image" id="ch08fig03" src="../images/ch08fig03.jpg" class="calibre53"/></div>
<p class="figcap"><em class="calibre13">Figure 8-3: Ridership vs. temperature</em></p>
<p class="indent">Fortunately, these nonlinear effects actually <em class="calibre13">can</em> be accommodated with linear models.</p>
<h4 class="h3" id="ch08lev11sec2"><em class="calibre22"><strong class="calibre3">8.11.2 Modeling Nonlinearity with a Linear Model</strong></em></h4>
<p class="noindent">Starting simple, suppose in the bike rental data we wish to predict total ridership <span class="literal">tot</span>, using temperature <span class="literal">temp</span> as our sole feature, but want a quadratic model:</p>
<p class="center" id="ch08equ11"><img alt="Image" src="../images/ch08equ11.jpg" class="calibre54"/></p>
<p class="noindent">This is still a linear model! Sure, there is a squared term there for <span class="literal">temp</span>, so we say the model is nonlinear <em class="calibre13">in terms of</em> <span class="literal">temp</span>. But it is still linear in <span class="literal">b</span>, <span class="literal">c</span>, and <span class="literal">d</span>. We are modeling mean <span class="literal">tot</span> as a linear combination of three things: 1, <span class="literal">temp</span>, and <span class="literal">temp</span><sup class="calibre11">2</sup> (thinking of <em class="calibre13">b</em> as <em class="calibre13">b</em> × 1).</p>
<p class="indent">Then we could simply add a <span class="literal">temp</span><sup class="calibre11">2</sup> column and call <span class="literal">qeLin()</span>:</p>
<pre class="calibre16">&gt; <span class="codestrong">data(day1)</span>
&gt; <span class="codestrong">day1tottemp &lt;- day1[,c(10,16)]</span>  # just tot, temp
&gt; <span class="codestrong">head(day1tottemp)</span>
      temp  tot
1 8.175849  985
2 9.083466  801
3 1.229108 1349
4 1.400000 1562
5 2.666979 1600
6 1.604356 1606
&gt; <span class="codestrong">day1tottemp$tempSqr &lt;- day1tottemp$temp^2</span> 
&gt; <span class="codestrong">head(day1tottemp)</span>
      temp  tot   tempSqr
1 8.175849  985 66.844507
2 9.083466  801 82.509355
3 1.229108 1349  1.510706
4 1.400000 1562  1.960000
5 2.666979 1600  7.112777
6 1.604356 1606  2.573958
&gt; qeLin(day1tottemp,'tot')
...
Coefficients:
(Intercept)         temp      tempSqr
   1305.597      346.422       -6.815</pre>
<p class="noindent"><span epub:type="pagebreak" id="page_146"/>We see that <img alt="Image" class="middle15" src="../images/unch08equ15.jpg"/> and <img alt="Image" class="middle16" src="../images/unch08equ16.jpg"/>.</p>
<p class="indent">But this is inconvenient. Not only would we need to add that squared column manually, but we also would have to remember to add it later to new cases that we wish to predict.</p>
<p class="indent">Worse, we would need to add the <em class="calibre13">cross-product</em> terms. Say we are predicting total ridership from both temperature and humidity. In that case, <a href="ch08.xhtml#ch08equ11" class="calibre12">Equation 8.11</a> would include a product of these two features, becoming:</p>
<p class="center" id="ch08equ12"><img alt="Image" src="../images/ch08equ12.jpg" class="calibre55"/></p>
<p class="noindent">If we have a lot of features, adding these terms manually would become a real nuisance.</p>
<p class="indent">A more subtle problem concerns dummy variables. Since 0<sup class="calibre11">2</sup> = 0 and 1<sup class="calibre11">2</sup> = 1, we see that the square of any dummy is itself. So adding squared terms to our model for dummies would be redundant.</p>
<p class="indent">To avoid such issues, the <span class="literal">qeML</span> package has the <span class="literal">qePolyLin()</span> function, which takes care of these things for us automatically. Its basic call form is:</p>
<pre class="calibre16">qePolyLin(data, yName, deg = 2, maxInteractDeg = deg,
   holdout = floor(min(1000,0.1 * nrow(data))))</pre>
<p class="noindent">The argument <span class="literal">deg</span> is the degree of the polynomial, and <span class="literal">maxInteractDeg</span> is the maximum interaction degree term; for instance, <em class="calibre13">temp</em> × <em class="calibre13">hum</em> is considered to be of degree 2 in <a href="ch08.xhtml#ch08equ12" class="calibre12">Equation 8.12</a>. Of course, if we have just a single feature, there are no interaction terms.</p>
<p class="indent">It gives the same fit as the one we got above manually (of course). Let’s again predict <span class="literal">tot</span> from <span class="literal">temp</span>:</p>
<pre class="calibre16">&gt; <span class="codestrong">day1tottemp &lt;- day1[,c(10,16)]</span>
&gt; <span class="codestrong">qepout &lt;- qePolyLin(day1tottemp,'tot',deg=2,holdout=NULL)</span></pre>
<p class="noindent"><span epub:type="pagebreak" id="page_147"/>Let’s take a look at the resulting estimated coefficients:</p>
<pre class="calibre16">&gt; <span class="codestrong">names(qepout)</span>
 [1] "bh"             "deg"            "maxInteractDeg" "modelFormula"
 [5] "XtestFormula"   "retainedNames"  "standardize"    "x"
 [9] "y"              "classif"        "trainRow1"
&gt; <span class="codestrong">qepout$bh</span>
              [,1]
[1,] 1305.597268
[2,]  346.421623
[3,]   -6.815313</pre>
<p class="indent">Prediction is as usual, for example, for a 12-degree day:</p>
<pre class="calibre16">&gt; <span class="codestrong">predict(qepout,data.frame(temp=12))</span>
[1] 4481.252</pre>
<p class="indent">Let’s see if the quadratic model predicts better:</p>
<pre class="calibre16">&gt; <span class="codestrong">set.seed(9999)</span>
&gt; <span class="codestrong">qePolyLin(day1tottemp,'tot',deg=2,holdout=100)$testAcc</span>
[1] 1214.063
&gt; <span class="codestrong">set.seed(9999)</span>  # to get the same holdout set
&gt; <span class="codestrong">qeLin(day1tottemp,'tot',holdout=100)$testAcc</span>
[1] 1250.177</pre>
<p class="noindent">Yes, the MAPE value was smaller for the quadratic model, though as always, it must be added that we should use <span class="literal">replicMeans()</span> to be sure (see <a href="ch03.xhtml#ch03lev2sec2" class="calibre12">Section 3.2.2</a>).</p>
<h4 class="h3" id="ch08lev11sec3"><em class="calibre22"><strong class="calibre3">8.11.3 Polynomial Logistic Regression</strong></em></h4>
<p class="noindent">Recall that the logit model starts with a linear combination of the features and then runs it through the logistic function <em class="calibre13">l</em>(<em class="calibre13">t</em>) = 1/(1 + <em class="calibre13">e</em><sup class="calibre11">−<em class="calibre13">t</em></sup>) to squeeze it into [0,1]. That means we can add polynomial terms as in the linear model. The <span class="literal">regtools</span> function <span class="literal">qePolyLog()</span> does this.</p>
<h4 class="h3" id="ch08lev11sec4"><em class="calibre22"><strong class="calibre3">8.11.4 Example: Programmer and Engineer Wages</strong></em></h4>
<p class="noindent">Let’s predict occupation by applying nonpolynomial logit first:</p>
<pre class="calibre16">&gt; <span class="codestrong">data(pef)</span>
&gt; <span class="codestrong">set.seed(9999)</span>
&gt; <span class="codestrong">qeLogit(pef,'occ')$testAcc</span>
[1] 0.646</pre>
<p class="noindent"><span epub:type="pagebreak" id="page_148"/>About 35 percent right, which is not too bad, considering there are 6 classes. But maybe a quadratic model—that is, adding terms such as the squares of income and age—would improve things. Let’s see:</p>
<pre class="calibre16">&gt; <span class="codestrong">set.seed(9999)</span>
&gt; <span class="codestrong">qePolyLog(pef,'occ',2)$testAcc</span>
[1] 0.619</pre>
<p class="noindent">This is a slight improvement. But is it a sampling accident? We could use the <span class="literal">qeCompare()</span> function to compare different degrees while using many holdout sets to address sampling issues (see <a href="ch08.xhtml#ch08lev13" class="calibre12">Section 8.13</a>).</p>
<h3 class="h2" id="ch08lev12">8.12 Blending the Linear Model with Other Methods</h3>
<p class="noindent">Problems tend to occur with k-NN around the fringes of a dataset. As a simple concrete example, let’s again consider predicting human weight from height in the Major League Baseball Player data from <a href="ch01.xhtml#ch01lev8" class="calibre12">Section 1.8</a>.</p>
<p class="indent">Here’s a summary of the data:</p>
<pre class="calibre16">&gt; <span class="codestrong">table(mlb$Height)</span>
<br class="calibre1"/>
 67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83
  2   7  19  51  89 150 162 173 155 101  55  27  14   5   2   2   1</pre>
<p class="indent">Suppose we wish to predict the weight of a new player whose height is 68.2 and use k-NN. This height is on the low end of our training data, so most of the nearest neighbors will be taller than this. Taller people tend to be heavier, and the dataset neighbors of our new point will mostly be taller than this new player and thus likely heavier. The result will be that our prediction will be biased upward; we will tend to predict a larger weight than this player’s true value. Similarly, if our new case has height, say, 81.5, our prediction will be biased downward.</p>
<p class="indent">One remedy is to fit a linear model within the neighborhood. Say we are predicting a new case <span class="literal">x</span> and use <em class="calibre13">k</em> = 25 neighbors. Then, instead of averaging the weights of those 25 neighboring players, we invoke <span class="literal">lm()</span> on that neighborhood data. We then predict <span class="literal">x</span> from the output of <span class="literal">lm()</span>. The linearity of this process will result in more realistic predictions at the fringes of the data.</p>
<p class="indent">The point, then, is that instead of using the mean to smooth the data in a neighborhood, we could use <span class="literal">lm()</span>. Or, if we are worried about the effects of outliers, we might try <span class="literal">median()</span>.</p>
<p class="indent">There is an argument called <span class="literal">smoothingFtn</span> in the <span class="literal">regtools</span> function <span class="literal">kNN()</span> (which is wrapped by <span class="literal">qeKNN()</span>) that lets us specify some kind of smoothing other than the usual mean. The default is <span class="literal">smoothingFtn = mean</span>; to use median smoothing, we would specify <span class="literal">smoothingFtn = median</span>. For linear smoothing, we would use <span class="literal">smoothingFtn = loclin</span>.</p>
<p class="indent">Recall that decision trees (and thus random forests and tree-based boosting as well) form neighborhoods too. They thus are subject to the same problem—that is, bias in neighborhoods that are near the edges of the data. <span epub:type="pagebreak" id="page_149"/>Thus the same local-linear idea could be applied here. The CRAN package <span class="literal">grf</span> does this; it is wrapped by <span class="literal">qeRFgrf()</span>.</p>
<h3 class="h2" id="ch08lev13">8.13 The qeCompare() Function</h3>
<p class="noindent">In our experiment in <a href="ch08.xhtml#ch08lev11sec4" class="calibre12">Section 8.11.4</a>, a quadratic model did appear to help, with a slightly lower OME than that of the ordinary logit. An extensive investigation would involve <span class="literal">fineTuning()</span>, with cross-validation trials, and possibly exploring polynomial degrees other than 1 or 2.</p>
<p class="indent">But remember, “qe” stands for “quick and easy.” The <span class="literal">qeML</span> function <span class="literal">qeCompare()</span> can be used for quick comparisons between models. (Of course, for large datasets, it won’t be so quick.)</p>
<p class="indent">Let’s use it to compare ordinary logistic regression with a quadratic version for the vertebrae data. While we are at it, let’s compare to the other methods we’ve had in the book so far.</p>
<pre class="calibre16">&gt; <span class="codestrong">qeCompare(vert,'V7',c('qeLogit','qePolyLog','qeKNN','qeRF','qeGBoost'),100)</span>
      qeFtn    meanAcc
1   qeLogit 0.13677419
2 qePolyLog 0.09129032
3     qeKNN 0.23741935
4      qeRF 0.15741935
5  qeGBoost 0.16322581</pre>
<p class="noindent">What happened here?</p>
<ul class="calibre15">
<li class="noindent3">Here we generated 100 random holdout sets (of size 73, the default for this dataset). The same holdout set is used for all methods. (The <span class="literal">qeCompare()</span> function has an optional random-number <span class="literal">seed</span> argument, but we’ve taken the default value of 9999.)</li>
<li class="noindent3">We are using default hyperparameters for each of the functions. The <span class="literal">qeCompare()</span> function has an optional <span class="literal">opts</span> argument to set non-default values, such as <span class="literal">nTree</span> for <span class="literal">qeRF()</span>.</li>
<li class="noindent3">We found OME values for each method.</li>
</ul>
<p class="indent">The quadratic logit not only outperformed the ordinary logit, but it also turned out to be the best of the bunch! Yes, it outperformed the fancy ML methods. Of course, each method was run with the default hyperparameters, and things could change with other values.</p>
<h4 class="h3" id="ch08lev13sec1"><em class="calibre22"><strong class="calibre3">8.13.1 Need for Caution Regarding Polynomial Models</strong></em></h4>
<p class="noindent">The polynomial degree is a hyperparameter. In our example of predicting occupation in <a href="ch08.xhtml#ch08lev11sec4" class="calibre12">Section 8.11.4</a>, we might, say, try a cubic (degree-3) model</p>
<pre class="calibre16">qePolyLog(pef,'occ',3)</pre>
<p class="noindent">or even set degree to 4, 5, and so on.<span epub:type="pagebreak" id="page_150"/></p>
<p class="indent">However, with higher and higher degrees, we do need to watch for over-fitting, as the number of terms increases very rapidly with degree. Let’s illustrate that point by checking the number of <em class="calibre13">β</em> coefficients in each model for varying degree, as shown in <a href="ch08.xhtml#ch8tab1" class="calibre12">Table 8-1</a>. This requires some digging into the output objects. It would be a distraction to explain that here, but for the interested reader, this is in the component <span class="literal">$glmOuts[[1]]</span>. One must then exclude the intercept term.</p>
<p class="tabcap" id="ch8tab1"><strong class="calibre5">Table 8-1:</strong> Complexity of Polynomial Models</p>
<div class="bqparan">
<table class="all">
<colgroup class="calibre28">
<col class="calibre56"/>
<col class="calibre56"/>
</colgroup>
<thead class="calibre30">
<tr class="calibre31">
<th class="table-h"><p class="noindent"><strong class="calibre5">Degree</strong></p></th>
<th class="table-h"><p class="noindent"><strong class="calibre5">Coefficients</strong></p></th>
</tr>
</thead>
<tbody class="calibre32">
<tr class="calibre31">
<td class="gray"><p class="noindent">1</p></td>
<td class="gray"><p class="noindent">6</p></td></tr>
<tr class="calibre31">
<td class="calibre33"><p class="noindent">2</p></td>
<td class="calibre33"><p class="noindent">22</p></td>
</tr>
<tr class="calibre31">
<td class="gray"><p class="noindent">3</p></td>
<td class="gray"><p class="noindent">50</p></td>
</tr>
<tr class="calibre31">
<td class="calibre33"><p class="noindent">4</p></td>
<td class="calibre33"><p class="noindent">90</p></td>
</tr>
<tr class="calibre31">
<td class="gray"><p class="noindent">5</p></td>
<td class="gray"><p class="noindent">143</p></td>
</tr>
</tbody>
</table>
</div>
<p class="noindent">Remember, these are the values of <em class="calibre13">p</em>, our number of features, after we add in polynomial terms. Our sample size <em class="calibre13">n</em> remains at 20,090. At some point while increasing the degree, we will overfit.</p>
<p class="indent">If we follow the rough rule of thumb that we need to have <img alt="Image" class="middle3" src="../images/prootn.jpg"/>, this suggests a limit of something like <em class="calibre13">p</em> &lt; 141, corresponding to using at most a degree-4 model. But this is, after all, only a rule of thumb. It may be the case that, say, OME starts increasing after just a degree-2 model. The reader is urged to try polynomials of various degrees, on this dataset and others, noting the OME values that result.</p>
<p class="indent">We note the following:</p>
<ul class="calibre15">
<li class="noindent3">Polynomial models may be able to hold their own against, or even outperform, their more esoteric ML counterparts.</li>
<li class="noindent3">Use of polynomial models is attractive in that there is only one hyperparameter (the degree), and also because the calculation (in the case of <span class="literal">qePolyLin()</span>) is noniterative and thus has no convergence issues.</li>
<li class="noindent3">As with any ML method, one must always keep in mind the possibility of overfitting.</li>
</ul>
<p class="noindent">Unfortunately, many treatments of ML overlook polynomial models. But they can be quite powerful and should definitely be in the analyst’s toolkit.</p>
<h3 class="h2" id="ch08lev14">8.14 What’s Next</h3>
<p class="noindent">The linear model is the oldest form of ML. As we have seen, it still can be quite powerful, outperforming “modern” ML methods in some cases. But in some settings, it can be made even better by, oddly, “shrinking” <img alt="Image" class="middle17" src="../images/betacap.jpg"/> This is the topic of the next chapter.</p>
</div></body></html>