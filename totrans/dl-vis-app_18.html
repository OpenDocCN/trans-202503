<html><head></head><body><div id="sbo-rt-content"><section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" title="387" id="Page_387"/>15</span><br/>
<span class="ChapterTitle">Optimizers</span></h1>
</header>
<figure class="opener">
<img src="Images/chapterart.png" alt="" width="206" height="206"/>
</figure>
<p class="ChapterIntro">Training neural networks is frequently a time-consuming process. Anything that speeds it up is a welcome addition to our toolkit. This chapter is about a family of tools that are designed to speed up learning by improving the efficiency of gradient descent. The goals are to make gradient descent run faster and avoid some of the problems that can cause it to get stuck. These tools also automate some of the work of finding the best learning rate, including algorithms that can adjust that rate automatically over time. Collectively, these algorithms are called <em>optimizers</em>. Each optimizer has its strengths and weaknesses, so it’s worth becoming familiar with them so we can make good choices when training a neural network.</p>
<p>Let’s begin by drawing some pictures that enable us to visualize error and how it changes as we learn. These pictures will help us build some intuition for the algorithms yet to come. </p>
<h2 id="h1-500723c15-0001"><span epub:type="pagebreak" title="388" id="Page_388"/>Error as a 2D Curve</h2>
<p class="BodyFirst">It’s often helpful to think of the errors in our systems in terms of geometrical ideas. We frequently plot error as a 2D curve. </p>
<p>To get familiar with this 2D error, let’s consider the task of splitting two classes of samples represented as dots arranged on a line. Dots at negative values are in one class, and dots at zero and above are in the other, as shown in <a href="#figure15-1" id="figureanchor15-1">Figure 15-1</a>.</p>
<figure>
<img src="Images/F15001.png" alt="F15001" width="694" height="119"/>
<figcaption><p><a id="figure15-1">Figure 15-1</a>: Two classes of dots on a line. Dots to the left of 0 are in class 0, shown in blue, and the others are in class 1, shown in beige.</p></figcaption>
</figure>
<p>Let’s build a classifier for these samples. In this example, the boundary consists of just a single number. All samples to the left of that number are assigned to class 0, and all those to the right are assigned to class 1. If we imagine moving this dividing point along the line, we can count up the number of samples that are misclassified and call that our error. We can summarize the results as a plot, where the X axis shows us each potential splitting point, and the error associated with that point is plotted as a dot above it. <a href="#figure15-2" id="figureanchor15-2">Figure 15-2</a> shows the result.</p>
<figure>
<img src="Images/F15002.png" alt="F15002" width="694" height="509"/>
<figcaption><p><a id="figure15-2">Figure 15-2</a>: Plotting the error function for a simple classifier</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="389" id="Page_389"/>We can smooth out the error curve of <a href="#figure15-2">Figure 15-2</a> as shown in <a href="#figure15-3" id="figureanchor15-3">Figure 15-3</a>.</p>
<figure>
<img src="Images/F15003.png" alt="F15003" width="694" height="509"/>
<figcaption><p><a id="figure15-3">Figure 15-3</a>: A smoothed version of <a href="#figure15-2">Figure 15-2</a></p></figcaption>
</figure>
<p>For this particular set of random data, we see that the error is 0 when we’re at 0, or just a little to the left of it. This tells us that regardless of where we start, we want to end up with our divider just to the left of 0.</p>
<p>Our goal is to find a way to locate the smallest value of any error curve. When we can do that, we can apply the technique to all the weights of a neural network and thus reduce the whole network’s error.</p>
<h2 id="h1-500723c15-0002">Adjusting the Learning Rate</h2>
<p class="BodyFirst">When we teach a system using gradient descent, the critical parameter is the learning rate, usually written with the lowercase Greek letter <em>η</em> (eta). This is often a value in the range 0.01 to 0.0001. Larger values lead to faster learning, but they can lead us to miss valleys by jumping right over them. Smaller values of <em>η </em>(nearing 0, but always positive) lead to slower learning and can find narrow valleys, but they can also get stuck in gentle valleys even when there are much deeper ones nearby. <a href="#figure15-4" id="figureanchor15-4">Figure 15-4</a> recaps these phenomena graphically.</p>
<p>An important idea shared by many optimizers is that we can improve learning by changing the learning rate as we go. The general thinking is analogous to hunting for buried metal on a beach using a metal detector. We start by taking big steps as we walk across the beach, but when the detector goes off, we take smaller and smaller steps to pinpoint the metal object’s <span epub:type="pagebreak" title="390" id="Page_390"/>location. In the same way, we usually take big steps along the error curve early in the learning process while we’re hunting for a valley. As time goes on, we hope that we found that valley, and we can now take smaller and smaller steps as we approach its lowest point.</p>
<figure>
<img src="Images/F15004.png" alt="F15004" width="834" height="303"/>
<figcaption><p><a id="figure15-4">Figure 15-4</a>: The influence of the learning rate, <em>η</em>. (a) When <em>η</em> is too large, we can jump right over a deep valley and miss it. (b) When <em>η</em> is too small, we can slowly descend into a local minimum, and miss the deeper valley. </p></figcaption>
</figure>
<p>We can illustrate our optimizers with a simple error curve containing a single isolated valley with the shape of a negative Gaussian, shown in <a href="#figure15-5" id="figureanchor15-5">Figure 15-5</a>.</p>
<figure>
<img src="Images/F15005.png" alt="F15005" width="694" height="495"/>
<figcaption><p><a id="figure15-5">Figure 15-5</a>: Our error curve for looking at optimizers</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="391" id="Page_391"/>Some gradients for this error curve are shown in <a href="#figure15-6" id="figureanchor15-6">Figure 15-6</a> (we’re actually showing the negative gradients).</p>
<figure>
<img src="Images/F15006.png" alt="F15006" width="694" height="706"/>
<figcaption><p><a id="figure15-6">Figure 15-6</a>: Our error curve and its negative gradients (scaled down by a factor of 0.25) at some locations</p></figcaption>
</figure>
<p>The gradients in <a href="#figure15-6">Figure 15-6</a> have been scaled down to 25 percent of their actual length for clarity. We can see that for this curve, the gradient is negative for input values that are less than 0 and positive for input values that are greater than 0. When the input is 0, we’re at the very bottom of the bowl, so the gradient there is 0, drawn as just a single dot.</p>
<h3 id="h2-500723c15-0001">Constant-Sized Updates</h3>
<p class="BodyFirst">Let’s start our investigation of the effect of the learning rate by seeing what happens when we use a constant learning rate. In other words, we always scale the gradient by a value of <em>η </em>that stays fixed, or constant, during the whole training process.</p>
<p><span epub:type="pagebreak" title="392" id="Page_392"/><a href="#figure15-7" id="figureanchor15-7">Figure 15-7</a> shows the basic steps of updating with a fixed <em>η</em>. </p>
<figure>
<img src="Images/F15007.png" alt="F15007" width="657" height="322"/>
<figcaption><p><a id="figure15-7">Figure 15-7</a>: Finding the step for basic gradient descent</p></figcaption>
</figure>
<p>Suppose we’re looking at a particular weight in a neural network. Let’s pretend that the weight begins with a value of w1, and we updated it once, so it now has the value w2, shown in <a href="#figure15-7">Figure 15-7</a>(a). Its corresponding error is the point on the error curve directly above it, marked B. We want to update the weight again to a new and better value that we’ll call w3.</p>
<p>To update the weight, we find its gradient on the error surface at the point B, shown as the arrow labeled <em>g</em>. We scale the gradient by the learning rate <em>η</em> to get a new arrow labeled <em>ηg</em>. Because <em>η</em> is between 0 and 1, <em>ηg</em> is a new arrow that points in the same direction as <em>g</em> but is either the same size as <em>g</em> or smaller.</p>
<p>In <a href="#figure15-7">Figure 15-7</a>, the arrow we show for the gradient <em>g</em> is actually the <em>opposite</em>, or negative, of the gradient. The positive and negative gradients point in opposite directions along the same line, so people tend to refer to simply <em>the gradient</em> when the choice of positive or negative can be understood from context. We’ll follow that convention in this chapter. </p>
<p>To find w3, the new value of the weight, we add the scaled gradient to w2. In pictures, this means we place the tail of the arrow <em>ηg </em>at B, as in <a href="#figure15-7">Figure 15-7</a>(b). The horizontal position of the tip of that arrow is the new value of the weight, w3, and its value, directly above it on the error surface, is marked C. In this case, we stepped a bit too far and increased our error by a little.</p>
<p>Let’s look at this technique in practice using an error curve with a single valley. <a href="#figure15-8" id="figureanchor15-8">Figure 15-8</a> shows a starting point in the upper left. The gradient here is small, so we move to the right a small amount. The error at that new point is a little less than the error we started with.</p>
<p>For these figures, we’ve chosen <em>η </em>= 1/8, or 0.125. This is an unusually large value of <em>η</em> for constant-sized gradient descent, where we often use a value of 1/100 or less. We chose this large value because it makes for clearer pictures. Smaller values work in similar ways, just more slowly. We <span epub:type="pagebreak" title="393" id="Page_393"/>aren’t showing values on the axes for these graphs to avoid visual clutter, since we’re more interested in the nature of what happens rather than the numbers.</p>
<figure>
<img src="Images/F15008.png" alt="F15008" width="694" height="388"/>
<figcaption><p><a id="figure15-8">Figure 15-8</a>: Learning with a constant learning rate</p></figcaption>
</figure>
<p>Rather than move from our first point by the entire gradient, we’re moving only 1/8 of its length. This move takes us to a steeper part of the curve, where the gradient is larger, so the next update moves a little farther. Each step of learning is shown with a new color, which we use to draw the gradient from the previous location and then the new point. </p>
<p>We show a close-up of six steps in <a href="#figure15-9" id="figureanchor15-9">Figure 15-9</a>, starting after the first step in <a href="#figure15-8">Figure 15-8</a>. We also show the error for each point. </p>
<figure>
<img src="Images/F15009.png" alt="F15009" width="705" height="327"/>
<figcaption><p><a id="figure15-9">Figure 15-9</a>: Left: A close-up of the final image in <a href="#figure15-8">Figure 15-8</a>. Right: The error associated with each of these six points.</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="394" id="Page_394"/>Will this process ever reach the bottom of the bowl and get down to 0 error? <a href="#figure15-10" id="figureanchor15-10">Figure 15-10</a> shows the first 15 steps in this process.</p>
<figure>
<img src="Images/F15010.png" alt="F15010" width="706" height="319"/>
<figcaption><p><a id="figure15-10">Figure 15-10</a>: Left: The first 15 steps of learning with a constant learning rate. Right: The errors of these 15 points. </p></figcaption>
</figure>
<p>We get near the bottom and then head up the hill on the right side. But that’s okay because the gradient here points down and to the left, so we head back down the valley until we overshoot the bottom again, and end up somewhere on the left side, then we turn around and overshoot again and end up on the right side, and so on, back and forth. We’re <em>bouncing around</em> the bottom of the bowl.</p>
<p>It doesn’t look like we’ll ever get to 0. The problem is particularly bad in this symmetrical valley, as the error jumps back and forth between the left and right sides of the minimum. But this type of behavior happens a lot when we use a constant learning rate. The bouncing around is happening because when we’re near the bottom of a valley, we want to take small steps, but because our learning rate is a constant, we’re taking steps that are too big.</p>
<p>We might wonder if the bouncing problem of <a href="#figure15-10">Figure 15-10</a> was caused by too large a learning rate. <a href="#figure15-11" id="figureanchor15-11">Figure 15-11</a> shows how things go for the first 15 steps of some smaller values of <em>η.</em></p>
<span epub:type="pagebreak" title="395" id="Page_395"/><figure>
<img src="Images/F15011.png" alt="F15011" width="844" height="417"/>
<figcaption><p><a id="figure15-11">Figure 15-11</a>: Taking 15 steps with our small learning rates. Top row: Learning rates of 0.025 (left column), 0.05 (middle column), and 0.1 (right column). Bottom row: Errors for the points in the top row.</p></figcaption>
</figure>
<p>As we can see from <a href="#figure15-11">Figure 15-11</a>, taking smaller steps doesn’t solve the bouncing problem, though the bounces are smaller. On the other hand, increasing the learning rate makes the bouncing problem worse, as shown in <a href="#figure15-12" id="figureanchor15-12">Figure 15-12</a>.</p>
<figure>
<img src="Images/f15012.png" alt="f15012" width="851" height="414"/>
<figcaption><p><a id="figure15-12">Figure 15-12</a>: Top row: Learning rates of 0.5 (left column), 0.75 (middle column), and 1.0 (right column). Bottom row: Errors for the points in the top row.</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="396" id="Page_396"/>Larger learning rates can also cause us to jump out of a nice valley with a low minimum. In <a href="#figure15-13" id="figureanchor15-13">Figure 15-13</a>, starting at the green dot, we jump right over the rest of the valley we’re in (and would like to stay in) and into a new valley with a much larger minimum.</p>
<figure>
<img src="Images/F15013.png" alt="F15013" width="694" height="461"/>
<figcaption><p><a id="figure15-13">Figure 15-13</a>: A large step overshoots the valley and ends up in a different valley with a higher minimum.</p></figcaption>
</figure>
<p>Sometimes a big jump like this can help us move from a shallow valley to a deeper one, but for such a large learning rate, we’ll probably jump around valleys a lot, never finding a minimum. It seems like a challenge to find just one learning rate that moves at a reasonable speed but won’t overshoot valleys or get trapped bouncing around in the bottom. A nice alternative is to change the learning rate as we go.</p>
<h3 id="h2-500723c15-0002">Changing the Learning Rate over Time</h3>
<p class="BodyFirst">We can use a large value of <em>η</em> near the start of our learning so we don’t crawl along, and a small value near the end so we don’t end up bouncing around the bottom of a bowl.</p>
<p>An easy way to start big and gradually get smaller is to multiply the learning rate by some number that’s almost 1 after every update step. Let’s use 0.99 as a multiplier and suppose that the starting learning rate is 0.1. Then after the first step, it will be 0.1 × 0.99 = 0.099. On the next step, it would be 0.099 × 0.99 = 0.09801. <a href="#figure15-14" id="figureanchor15-14">Figure 15-14</a> shows what happens to <em>η</em> when we do this for many steps using a few different values for the multiplier.</p>
<p>The easiest way to write the equation of these curves involves using exponents, so this kind of curve is called an <em>exponential decay</em> curve. The value by which we multiply <em>η</em> on every step is called the <em>decay parameter</em>. This is usually a number very close to 1. </p>
<span epub:type="pagebreak" title="397" id="Page_397"/><figure>
<img src="Images/F15014.png" alt="F15014" width="704" height="506"/>
<figcaption><p><a id="figure15-14">Figure 15-14</a>: Starting with a learning rate of <em>η</em> = 1, the various curves show how the learning rate drops after multiplying it by a given value after each update.</p></figcaption>
</figure>
<p>Let’s apply this gradual reduction of the learning rate to gradient descent on our error curve. Once again, we start with a learning rate of 1/8. To make the effect of the decay parameter easily visible, let’s set it to the unusually low value of 0.8. This means each step will only be 80 percent as long as the step before it. <a href="#figure15-15" id="figureanchor15-15">Figure 15-15</a> shows the result for the first 15 steps.</p>
<figure>
<img src="Images/F15015.png" alt="F15015" width="706" height="325"/>
<figcaption><p><a id="figure15-15">Figure 15-15</a>: The first 15 steps using a shrinking learning rate</p></figcaption>
</figure>
<p>Let’s compare this with our “bouncing” result from using a constant step size. <a href="#figure15-16" id="figureanchor15-16">Figure 15-16</a> shows the results for the constant and shrinking step sizes together for 15 steps.</p>
<span epub:type="pagebreak" title="398" id="Page_398"/><figure>
<img src="Images/F15016.png" alt="F15016" width="701" height="444"/>
<figcaption><p><a id="figure15-16">Figure 15-16</a>: On the left is the constant step size from <a href="#figure15-10">Figure 15-10</a>, and on the right is the decaying step size from <a href="#figure15-15">Figure 15-15</a>. Notice how the shrinking learning rate helps us efficiently settle into the minimum of the valley.</p></figcaption>
</figure>
<p>The shrinking step size does a beautiful job of landing us in the bottom of the bowl and keeping us there. </p>
<h3 id="h2-500723c15-0003">Decay Schedules</h3>
<p class="BodyFirst">The decay technique is attractive, but it comes with some new challenges. First, we have to choose a value for the decay parameter. Second, we might not want to apply the decay after every update. To address these issues, we can try some other strategies for reducing the learning rate.</p>
<p>Any given approach to changing the learning rate over time is called a <em>decay schedule</em> (Bengio 2012).</p>
<p>Decay schedules are usually expressed in epochs, rather than samples. We train on all the samples in our training set, and only then consider changing the learning rate before we train on all the samples again.</p>
<p>The simplest decay schedule is to always apply decay to the learning rate after every epoch, as we just saw. <a href="#figure15-17" id="figureanchor15-17">Figure 15-17</a>(a) shows this schedule.</p>
<p>Another common scheduling method is to put off any decay at all for a while so our weights have a chance to get away from their starting random values and into something that might be close to finding a minimum. Then we apply whatever schedule we’ve picked. <a href="#figure15-17">Figure 15-17</a>(b) shows this <em>delayed exponential decay</em> approach, putting off the exponential decay schedule of <a href="#figure15-17">Figure 15-17</a>(a) for a few epochs. </p>
<p><span epub:type="pagebreak" title="399" id="Page_399"/>Another option is to apply the decay only every once in a while. The <em>interval decay</em> approach shown in <a href="#figure15-17">Figure 15-17</a>(c), also called <em>fixed-step decay</em>, reduces the learning rate after every fixed number of epochs, say every 4th or 10th. This way we don’t risk getting too small too fast.</p>
<figure>
<img src="Images/F15017.png" alt="F15017" width="694" height="677"/>
<figcaption><p><a id="figure15-17">Figure 15-17</a>: Decay schedules for reducing the size of the learning rate over time. (a) Exponential decay, where the learning rate is reduced after every epoch. (b) Delayed exponential decay. (c) Interval decay, where the learning rate is reduced after every fixed number of epochs (here, 4). (d) Error-based decay, where the learning rate is reduced when the error stops dropping.</p></figcaption>
</figure>
<p>Yet another option is to monitor the error of our network. As long as the error is going down, we stick with whatever learning rate we have now. When the network stops learning, we apply the decay so it can take smaller steps and hopefully work its way into a deeper part of the error landscape. This <em>error-based decay</em> is shown in <a href="#figure15-17">Figure 15-17</a>(d).</p>
<p>We can easily cook up a lot of alternatives, such as applying decay only when the error decreases by a certain amount or certain percentage, or perhaps updating the learning rate by just subtracting a small value from it rather than multiplying it by a number close to 1 (as long as we stop at some <span epub:type="pagebreak" title="400" id="Page_400"/>positive value—if the learning rate went to 0, the system would stop learning, and if the learning rate went negative, the system would increase the error, rather than decrease it).</p>
<p>We can even increase the learning rate over time if we want. The <em>bold driver</em> method looks at how the total loss is changing after each epoch (Orr 1999a; Orr 1999b). If the error is going down, then we <em>increase </em>the learning rate a little, say 1 percent to 5 percent. The thinking is that if things are going well, and the error is dropping, we can take big steps. But if the error has gone up by more than just a little, then we slash the learning rate, cutting it by half. This way we can stop any increases immediately, before they can carry us too far away from the decreasing error we were previously enjoying.</p>
<p>Learning rate schedules have the drawback that we have to pick their parameters in advance (Darken, Chang, and Moody 1992). We think of these parameters as <em>hyperparameters</em>, just like the learning rate itself. Most deep learning libraries offer routines that automatically search ranges of values for us to help us find the best values of one or more hyperparameters. </p>
<p>Generally speaking, simple strategies for adjusting the learning rate usually work well, and most machine-learning libraries let us pick one of them with little fuss (Karpathy 2016).</p>
<p>Some kind of learning rate reduction is a common feature in most machine learning systems. We want to learn quickly in the early stages, moving in big steps over the landscape, looking for the lowest minimum we can find. Then we reduce the learning rate, enabling us to gradually take smaller steps and land in the very lowest part of whatever valley we’ve found.</p>
<p>It’s natural to wonder if there’s a way to control the learning rate that doesn’t depend on a schedule that we set up before we start training. Surely, we can somehow detect when we’re near a minimum, or in a bowl, or bouncing around, and automatically adjust the learning rate in response.</p>
<p>An even more interesting question is to consider that maybe we don’t want to apply the same learning rate adjustments to all of our weights. It would be nice to be able to tune our updates so that each weight is learning at a rate that works best for it.</p>
<p>Let’s look at some variations on gradient descent that address those ideas.</p>
<h2 id="h1-500723c15-0003">Updating Strategies</h2>
<p class="BodyFirst">In the following sections, we compare the performance of three different ways to enhance gradient descent. In these examples, we use a small, but real, two-class classification problem.</p>
<p><a href="#figure15-18" id="figureanchor15-18">Figure 15-18</a> shows our familiar dataset of two fuzzy crescent moons. The classes for these points are shown by color. These 300 samples are our reference data for the rest of this chapter.</p>
<p>In order to compare different networks, we need to train them until the error has reached a minimum, or seems to have stopped improving. We can show the results of our training with plots that graph the error after each epoch. Because of the wide variation in algorithms, the number of epochs in these graphs vary over a large range.</p>
<span epub:type="pagebreak" title="401" id="Page_401"/><figure>
<img src="Images/F15018.png" alt="F15018" width="714" height="506"/>
<figcaption><p><a id="figure15-18">Figure 15-18</a>: The data we use for the rest of this chapter. The 300 points form two classes of 150 points each.</p></figcaption>
</figure>
<p>To classify our points, we’ll use a neural network with three fully connected hidden layers (of 12, 13, and 13 nodes), and an output layer of 2 nodes, giving us the probability for each of the two classes. We’ll use ReLU on each hidden layer, and softmax at the end. Whichever class has the larger probability at the output is taken as our network’s prediction. For consistency, when we need a constant learning rate, we use a value of <em>η</em>= 0.01. The network is shown in <a href="#figure15-19" id="figureanchor15-19">Figure 15-19</a>.</p>
<figure>
<img src="Images/F15019.png" alt="F15019" width="446" height="117"/>
<figcaption><p><a id="figure15-19">Figure 15-19</a>: Our network of four fully connected layers</p></figcaption>
</figure>
<h3 id="h2-500723c15-0004">Batch Gradient Descent</h3>
<p class="BodyFirst">Let’s begin by updating the weights just once per epoch, after we’ve evaluated all the samples. This is <em>batch gradient descent </em>(also called<em> epoch gradient descent</em>). In this approach, we run the entire training set through our system, accumulating the errors. Then we update all of the weights once using the combined information from all the samples. </p>
<p><a href="#figure15-20" id="figureanchor15-20">Figure 15-20</a> shows the error from a typical training run using batch gradient descent.</p>
<span epub:type="pagebreak" title="402" id="Page_402"/><figure>
<img src="Images/F15020.png" alt="F15020" width="706" height="514"/>
<figcaption><p><a id="figure15-20">Figure 15-20</a>: The error for a training run using batch gradient descent </p></figcaption>
</figure>
<p>The broad features are reassuring. The error drops quite a bit at the beginning, suggesting that the network is starting on a steep section of the error surface. Then the curve becomes much shallower. The error surface here might be a nearly flat region of a shallow saddle or a region that’s nearly a plateau but has just a bit of slope to it, because the error does continue to drop slowly. Eventually the algorithm finds another steep region and follows it all the way down to 0.</p>
<p>Batch gradient descent looks very smooth, but to get down to near 0 error for this network and data requires about 20,000 epochs, which can take a long time. Let’s get a closer look at what happens from one epoch to the next by zooming in on the first 400 epochs, shown in <a href="#figure15-21" id="figureanchor15-21">Figure 15-21</a>.</p>
<p>It seems that batch gradient descent really is moving smoothly. That makes sense, because it’s using the error from all the samples on each update.</p>
<p>Batch gradient descent usually produces a smooth error curve, but it has some issues in practice. If we have more samples than can fit in our computer’s memory, then the costs of <em>paging</em>, or retrieving data from slower storage media, can become substantial enough to make training impractically slow. This can be a problem in some real situations when we work with enormous datasets of millions of samples. It can take a great deal of time to read samples from slower memory (or even a hard drive) over and over. There are solutions to this problem, but they can involve a lot of work.</p>
<span epub:type="pagebreak" title="403" id="Page_403"/><figure>
<img src="Images/F15021.png" alt="F15021" width="708" height="515"/>
<figcaption><p><a id="figure15-21">Figure 15-21</a>: A close-up of the first 400 epochs of batch gradient descent shown in <a href="#figure15-20">Figure 15-20</a></p></figcaption>
</figure>
<p>Closely related to this memory issue is that we must keep all the samples around and available so that we can run through them over and over, once per epoch. We sometimes say that batch gradient descent is an <em>offline algorithm</em>, meaning that it works strictly from information that it has stored and has access to. We can imagine disconnecting the computer from all networks, and it could continue to learn from all of our training data.</p>
<h3 id="h2-500723c15-0005">Stochastic Gradient Descent </h3>
<p class="BodyFirst">Let’s go to the other extreme and update our weights after every sample. This is called <em>stochastic gradient descent</em>, or, more commonly, just <em>SGD</em>. Recall that the word <em>stochastic</em> is roughly a synonym for <em>random</em>. This word is used because we present the network with the training samples in a random order, so we can’t predict how the weights are going to change from one sample to the next.</p>
<p>Since we update after every sample, our dataset of 300 samples requires us to update the weights 300 times over the course of each epoch. This is going to cause the error to jump around a lot as each sample pulls the weights one way and then another. Since we’re only plotting the error on an epoch-by-epoch basis, we don’t see this small-scale wiggling. But we still see a lot of variation epoch by epoch. </p>
<p><span epub:type="pagebreak" title="404" id="Page_404"/><a href="#figure15-22" id="figureanchor15-22">Figure 15-22</a> shows the error of our network learning from this data using SGD.</p>
<figure>
<img src="Images/F15022.png" alt="F15022" width="703" height="513"/>
<figcaption><p><a id="figure15-22">Figure 15-22</a>: Stochastic gradient descent, or SGD</p></figcaption>
</figure>
<p>The graph has the same general shape as the one for batch gradient descent in <a href="#figure15-20">Figure 15-20</a>, which makes sense since both training runs use the same network and data. </p>
<p>The huge spike at around epoch 225 shows just how unpredictable SGD can be. Something in the sequencing of the samples and the way the network’s weights were updated caused the error to soar from nearly 0 to nearly 1. In other words, it went from finding the right class for almost every sample to being dead wrong on almost every sample, and then back to being right again (though this recovery took a few epochs, as shown by the small curve to the right of the spike). If we were watching the errors as learning progresses, we might be inclined to stop the training session at the spike. If we use an automatic algorithm to watch the error, it may also stop it there. Yet just a few epochs after that spike, the system has recovered and we’re back to nearly 0. The algorithm has definitely earned the word <em>stochastic</em> in its name.</p>
<p>We can see from the plot that SGD got down to about 0 error in just 400 epochs. We cut off <a href="#figure15-22">Figure 15-22</a> after that, since the curve stayed at 0 from then on. Compare this to the roughly 20,000 epochs required by batch gradient descent in <a href="#figure15-20">Figure 15-20</a>. This increase in efficiency over batch gradient descent is typical (Ruder 2017).</p>
<p><span epub:type="pagebreak" title="405" id="Page_405"/>But let’s compare apples to apples. How many times did each algorithm update the weights? Batch gradient descent updates the weights after each batch, so the 20,000 epochs means it did 20,000 updates. SGD does an update after every one of our 300 samples. So in 400 epochs it performed 300 × 400 = 120,000 updates, six times more than batch gradient descent. The moral is that the amount of time we actually spend waiting for results isn’t completely predicted by the number of epochs, since the time per epoch can vary considerably.</p>
<p>We call SGD an <em>online algorithm</em>, because it doesn’t require the samples to be stored or even to be consistent from one epoch to the next. It just handles each sample as it arrives and updates the network immediately.</p>
<p>SGD produces noisy results, as we can see in <a href="#figure15-22">Figure 15-22</a>. This is both good and bad. The upside of this is that SGD can jump from one region of the error surface to another as it searches for minima. But the downside is that SGD can overshoot a deep minimum and spend its time searching around inside of some valley with a larger error. Reducing the learning rate over time definitely helps with the jumping problem, but the progress is still typically noisy.</p>
<p>Noise in the error curve can be a problem because it makes it hard for us to know when the system is learning and when it starts overfitting. We can look at a sliding window of many epochs, but we may only know that we’ve overshot the minimum error long after it happened.</p>
<h3 id="h2-500723c15-0006">Mini-Batch Gradient Descent</h3>
<p class="BodyFirst">We can find a nice middle ground between the extremes of batch gradient descent, which updates once per epoch, and stochastic gradient descent, which updates after every sample. This compromise is called <em>mini-batch gradient descent</em>, or sometimes <em>mini-batch SGD</em>. Here, we update the weights after some fixed number of samples has been evaluated. This number is almost always considerably smaller than the batch size (the number of samples in the training set). We call this smaller number the <em>mini-batch size</em>, and a set of that many samples drawn from the training set is a <em>mini-batch</em>.</p>
<p>The mini-batch size is frequently a power of 2 between about 32 and 256, and often it is chosen to fully use the parallel capabilities of our GPU, if we have one. But that’s just for efficiency purposes. We can use any size of mini-batch that we like.</p>
<p><a href="#figure15-23" id="figureanchor15-23">Figure 15-23</a> shows the results of using a mini-batch of 32 samples.</p>
<p>This is indeed a nice blend of the two algorithms. The curve is smooth, like batch gradient descent, but not perfectly so. It drops down to 0 in about 5,000 epochs, between the 400 needed by SGD and the 20,000 of batch gradient descent. <a href="#figure15-24" id="figureanchor15-24">Figure 15-24</a> shows a close-up of the first 400 steps.</p>
<span epub:type="pagebreak" title="406" id="Page_406"/><figure>
<img src="Images/F15023.png" alt="F15023" width="716" height="520"/>
<figcaption><p><a id="figure15-23">Figure 15-23</a>: Mini-batch gradient descent  </p></figcaption>
</figure>
<figure>
<img src="Images/F15024.png" alt="F15024" width="718" height="523"/>
<figcaption><p><a id="figure15-24">Figure 15-24</a>: A close-up of the first 400 epochs of <a href="#figure15-23">Figure 15-23</a>, showing the deep plunge at the very beginning of training </p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="407" id="Page_407"/>How many updates did mini-batch SGD perform? We have 300 samples, and we used a mini-batch size of 32, so there are 10 mini-batches per epoch. (Ideally, we’d like the mini-batches to precisely divide the size of the input, but in practice, we can’t control the size of our datasets. This often leaves us with a partial mini-batch at the end.) So 10 updates per epoch, times 5,000 epochs, gives us 50,000 updates. This is also nicely between the 20,000 updates of batch gradient descent and the 120,000 updates of SGD.</p>
<p>Mini-batch gradient descent is less noisy than SGD, which makes it attractive for tracking the error. The algorithm can take advantage of huge efficiency gains by using the GPU for calculations, evaluating all the samples in a mini-batch in parallel. It’s faster than batch gradient descent, and more attractive in practice than SGD.</p>
<p>For all these reasons, mini-batch SGD is popular in practice, with “plain” SGD and batch gradient descent being used relatively infrequently. In fact, most of the time when the term <em>SGD</em> is used in the literature, or even just <em>gradient descent</em>, it’s understood that the authors mean mini-batch SGD (Ruder 2017). To make things a little more confusing, the term <em>batch</em> is often used instead of <em>mini-batch</em>. Because epoch-based gradient descent is used so rarely these days, references to batch gradient descent and batches almost always refer to mini-batch gradient descent and mini-batches. </p>
<h2 id="h1-500723c15-0004">Gradient Descent Variations</h2>
<p class="BodyFirst">Mini-batch gradient descent is an important algorithm, but it’s not perfect. Let’s review some of the challenges of mini-batch gradient descent, and a few ways to address them. Following convention, from here on we refer to mini-batch gradient descent as SGD. (The organization of this section is inspired by Ruder 2017.)</p>
<p>Our first challenge is to specify what value of the learning rate <em>η </em>we want to use, which is notoriously hard to pick ahead of time. As we’ve seen, a value that’s too small can result in long learning times and getting stuck in shallow local minima, but a value that’s too big can cause us to overshoot deep local minima and then get stuck bouncing around inside a minimum when we do find it. If we try to avoid the problem by using a decay schedule to change <em>η</em> over time, we still have to pick the starting value of <em>η</em> and then the schedule’s hyperparameters as well.</p>
<p>We also have to pick the size of the mini-batch. This is rarely an issue, because we usually choose whatever value produces calculations that are most closely matched to the structure of our GPU or other hardware.</p>
<p>Let’s consider some improvements. Right now, we’re updating all the weights with a one-update-rate-fits-all approach. Instead, we can find a unique learning rate for each weight in the system so we’re not just moving it in the best direction, but we’re moving it by the best amount. We’ll see examples of this in the following pages. </p>
<p>Another improvement begins with the recognition that sometimes when the error surface forms a saddle, the surface can be shallow in all directions, so locally, it’s almost (but not quite) a plateau. This can slow our progress to <span epub:type="pagebreak" title="408" id="Page_408"/>an excruciating crawl. Research has shown that deep learning systems often have plenty of saddles in their error landscapes (Dauphin et al. 2014). It would be nice if there was a way to get unstuck in these situations, or better yet, to avoid getting stuck in them in the first place. The same thing goes for plateaus: we’d like to avoid getting stuck in the flat regions where the gradient drops to 0. To do so, we want to avoid the regions where the gradient drops to 0, except of course for the minima we’re seeking.</p>
<p>Let’s look at some variations of gradient descent that address these issues.</p>
<h3 id="h2-500723c15-0007">Momentum</h3>
<p class="BodyFirst">Let’s consider two weights at the same time. We can plot their values on an XY plane, and above them show the error that results from training the system with those values for those weights. Let’s think of our error surface as a landscape. Now we can picture our task of minimizing error as following a drop of water that’s looking for the lowest point. </p>
<p><a href="#figure15-25" id="figureanchor15-25">Figure 15-25</a> repeats a figure from Chapter 5 that shows an example of this way of thinking about the training process.</p>
<figure>
<img src="Images/F15025.png" alt="F15025" width="450" height="423"/>
<figcaption><p><a id="figure15-25">Figure 15-25</a>: A drop of water rolling down an error surface. This is a repeat of a figure from Chapter 5.</p></figcaption>
</figure>
<p>Instead of water, let’s think of this as a little ball rolling down the error surface. We know from the physical world that a real ball rolling down a hill in this way has some <em>inertia</em>, which describes its resistance to a change in its motion. If it’s rolling along in a given direction at a certain speed, it will continue to move that way unless something interferes with it.</p>
<p><span epub:type="pagebreak" title="409" id="Page_409"/>A related idea is the ball’s <em>momentum</em>, which is a bit more abstract from a physical point of view. Although they’re distinct ideas, sometimes deep learning discussions casually refer to inertia as momentum, and the algorithm we’re about to look at uses that language. </p>
<p>This idea is what keeps the ball in <a href="#figure15-25">Figure 15-25</a> moving across the plateau after it has come down from the peak and passed into the saddle near the middle of the figure. If the ball’s motion was determined strictly by the gradient, when it hit the plateau near the middle of the figure, it would stop (or if it was a near-plateau, the ball would slow to a crawl). But the ball’s momentum (or more properly, its inertia) keeps it rolling onward.</p>
<p>Suppose we’re near the left side of <a href="#figure15-26" id="figureanchor15-26">Figure 15-26</a>. As we roll down the hill, we reach the plateau starting at around −0.5.</p>
<figure>
<img src="Images/F15026.png" alt="F15026" width="705" height="479"/>
<figcaption><p><a id="figure15-26">Figure 15-26</a>: An error curve with a plateau between a hill and valley</p></figcaption>
</figure>
<p>With regular gradient descent, we stop on the plateau since the gradient is zero, as shown in the left of <a href="#figure15-27" id="figureanchor15-27">Figure 15-27</a>. But if we include some momentum, as shown on the right, the ball keeps going for a while. It does slow down, but if we’re lucky, it continues to roll far enough to find the next valley.</p>
<p>The technique of <em>momentum gradient descent</em> (Qian 1999) is based on this idea. For each step, once we calculate how much we want each weight to change, we add in a small amount of its change from the previous<em> </em>step. If the change on a given step is 0, or nearly 0, but we had some larger change on the last step, we use some of that prior motion now, which pushes us along over the plateau.</p>
<span epub:type="pagebreak" title="410" id="Page_410"/><figure>
<img src="Images/F15027.png" alt="F15027" width="714" height="436"/>
<figcaption><p><a id="figure15-27">Figure 15-27</a>: Gradient descent on the error curve of <a href="#figure15-26">Figure 15-26</a>. Left: Gradient descent with decay. Right: Gradient descent with decay and momentum.</p></figcaption>
</figure>
<p><a href="#figure15-28" id="figureanchor15-28">Figure 15-28</a> shows the idea visually. </p>
<figure>
<img src="Images/F15028.png" alt="F15028" width="825" height="263"/>
<figcaption><p><a id="figure15-28">Figure 15-28</a>: Finding the step for gradient descent with momentum </p></figcaption>
</figure>
<p>We suppose that some weight had an error A. We updated that weight to value w2, with an error B. We now want to find the next value for the weight, w3, which will have error C. To find C, we find the change that we applied to point A. That is, we find the previous motion applied to A. This is the momentum, labeled <em>m</em>, shown in <a href="#figure15-28">Figure 15-28</a>(a).</p>
<p>We multiply the momentum, <em>m</em>, by a scaling factor usually referred to with the lowercase Greek letter <em>γ </em>(gamma). Sometimes this is called the <em>momentum scaling factor</em>, and it’s a value from 0 to 1. Multiplying <em>m</em> by this value gives us a new arrow <em>γm</em> that points in the same direction as <em>m</em> but <span epub:type="pagebreak" title="411" id="Page_411"/>is the same length or shorter. We then find the scaled gradient, <em>ηg</em>, at B, as we did before, shown in <a href="#figure15-28">Figure 15-28</a>(b). Now we have all we need. We add together the scaled momentum, <em>γm</em>, and the scaled gradient, <em>ηg</em>, to B, which we do graphically by placing the tail of <em>γm</em> at the head of <em>ηg</em>, as in <a href="#figure15-28">Figure 15-28</a>(c).</p>
<p>Let’s apply this rule and see how the weight and error change over time. <a href="#figure15-29" id="figureanchor15-29">Figure 15-29</a> shows our symmetrical valley from before, and sequential steps of training. In this figure, we use both an exponential decay schedule and momentum. This is just like our sequence from <a href="#figure15-15">Figure 15-15</a>, but now the change applied to each step also includes momentum, or a scaled version of the change from the previous step. We can see this by looking at the two lines that emerge from each point (one for the gradient, the other for the momentum). This total then becomes the new change.</p>
<figure>
<img src="Images/F15029.png" alt="F15029" width="694" height="383"/>
<figcaption><p><a id="figure15-29">Figure 15-29</a>: Learning with both an exponential decay schedule and momentum</p></figcaption>
</figure>
<p>On each step, we first find the gradient and multiply it by the current value of the learning rate <em>η</em>, as before. Then we find the previous change, scale it by <em>γ</em>, and add both of those changes to the current position of the weight. That combination gives us the change in this step.</p>
<p><a href="#figure15-30" id="figureanchor15-30">Figure 15-30</a> shows a close-up of the sixth step in the grid, along with the error at each point along the way.</p>
<p>An interesting thing happened here: when the ball reached the right side of the valley, it continued to roll up, even though the gradients pointed down. That’s just what we’d expect of a real ball. We can see it slowing down, and then eventually it comes back down the slope, overshooting the bottom, but by less than before, then slowing and coming back down again, and so on.</p>
<span epub:type="pagebreak" title="412" id="Page_412"/><figure>
<img src="Images/F15030.png" alt="F15030" width="714" height="320"/>
<figcaption><p><a id="figure15-30">Figure 15-30</a>: The final step in <a href="#figure15-29">Figure 15-29</a>, along with the error for each point</p></figcaption>
</figure>
<p>If we use too much momentum, our ball can fly right up the other side and out of the bowl altogether, but if we use too little momentum, our ball may not get across the plateaus it encounters along the way. <a href="#figure15-31" id="figureanchor15-31">Figure 15-31</a> shows our error curve from <a href="#figure15-26">Figure 15-26</a>. Here we used trial and error to find a value of <em>γ </em>to scale the momentum so that our ball gets through the plateau but can still settle into the minimum at the bottom of the bowl.</p>
<figure>
<img src="Images/F15031.png" alt="F15031" width="714" height="320"/>
<figcaption><p><a id="figure15-31">Figure 15-31</a>: Using enough momentum to cross a plateau, but not so much that the ball is unable to settle nicely into the bottom of the minimum</p></figcaption>
</figure>
<p>Finding the right amount of momentum to use is another task where we need to use our experience and intuition, along with trial and error, to help us understand the behavior of our specific network and the data we’re working with. We can also search for it using hyperparameter searching algorithms.</p>
<p>To put this all together, we find the gradient, scale it by the current learning rate <em>η</em>, add in the previous change scaled by <em>γ</em>, and that gives us our new position. If we set <em>γ </em>to 0, then we add in none of the last step, and we have “normal” (or “vanilla”) gradient descent. If <em>γ </em>is set to 1, then we add in the entirety of the last change. Often we use a value of around 0.9. In Figures 15-29 and <a href="#figure15-31">Figure 15-31</a>, we set gamma to 0.7 to better illustrate the process.</p>
<p><span epub:type="pagebreak" title="413" id="Page_413"/><a href="#figure15-32" id="figureanchor15-32">Figure 15-32</a> shows the result of 15 steps of learning with both learning rate decay and momentum. The ball starts on the left, rolls down and then far up the right side, then it rolls down again and rolls up the left side, and so on, climbing a little less each time.</p>
<figure>
<img src="Images/F15032.png" alt="F15032" width="710" height="318"/>
<figcaption><p><a id="figure15-32">Figure 15-32</a>: Learning with momentum and a decaying learning rate for 15 points</p></figcaption>
</figure>
<p>Momentum helps us get over flat plateaus and out of shallow places in saddles. It has the additional benefit of helping us zip down steep slopes, so even with a small learning rate, we can pick up some efficiency.</p>
<p><a href="#figure15-33" id="figureanchor15-33">Figure 15-33</a> shows the error for a training run on our dataset of <a href="#figure15-18">Figure 15-18</a> consisting of two crescent moons. </p>
<figure>
<img src="Images/F15033.png" alt="F15033" width="691" height="504"/>
<figcaption><p><a id="figure15-33">Figure 15-33</a>: Error curve for training with our two-crescent data using mini-batch gradient descent with momentum. We got to zero error in a little more than 600 epochs.</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="414" id="Page_414"/>Here we’re using mini-batch gradient descent (or SGD) with momentum. It’s noisier than the mini-batch curve of <a href="#figure15-23">Figure 15-23</a> because the momentum sometimes carries us past where we want to be, causing a spike in the error. The error when using mini-batch SGD alone in <a href="#figure15-23">Figure 15-23</a> for our data took about 5,000 epochs to reach about 0 error. With momentum, we get there in a little over 600 epochs. Not bad!</p>
<p>Momentum clearly helps us learn more quickly, which is a great thing. But momentum brings us a new problem: choosing the momentum value <em>γ</em>. As we mentioned, we can pick this value using experience and intuition or use a hyperparameter<em> </em>search for the value that gives us the best results. </p>
<h3 id="h2-500723c15-0008">Nesterov Momentum</h3>
<p class="BodyFirst">Momentum let us reach into the past for information to help us train. Now let’s reach into the future. The key idea is that instead of using only the gradient at the location where we currently are, we also use the gradient at the location where we expect that we’re <em>going to be</em>. Then we can use some of that “gradient from the future” to help us now.</p>
<p>Because we can’t really predict the future, we estimate<em> </em>where we’re going to be on the next step and use the gradient there. The thinking is that if the error surface is relatively smooth, and our estimate is pretty good, then the gradient we find at our estimated next position is close to the gradient where we’d actually end up if we just moved using standard gradient descent, with or without momentum.</p>
<p>Why is it useful to use the gradient from the future? Suppose we’re rolling down one side of a valley and approaching the bottom. On the next step, we overshoot the bottom and end up somewhere on the other wall. As we saw before, momentum carries us up that wall for a few steps, slowing as we lose momentum, until we turn around and come back down. But if we can predict<em> </em>that we’ll be on the far side, we can include some of the gradient at that point in our calculations now. So instead of moving so far to the right and up the hill, that leftward push from the future causes us to move by a little less distance, so we don’t overshoot so far and end up closer to the bottom of the valley.</p>
<p>In other words, if the next move we’re going to make is in the same direction as the last one, we take a larger step now. If the next move is going to move us backward, we take a smaller step.</p>
<p>Let’s break this down into steps so we don’t get mixed up between estimates and realities. <a href="#figure15-34" id="figureanchor15-34">Figure 15-34</a> shows the process. </p>
<p>As before, we imagine that we started with our weight at position A, and after the most recent update, we ended up at B, as shown in <a href="#figure15-34">Figure 15-34</a>(a). As with momentum, we find the change applied at point A to bring us to B (the arrow <em>m</em>) and we scale that by <em>γ</em>.</p>
<p>Now comes the new part, starting in <a href="#figure15-34">Figure 15-34</a>(b). Rather than finding the gradient at B, we first add the scaled momentum to B to get the “predicted” error P. This is our guess for where we will end up on the error <span epub:type="pagebreak" title="415" id="Page_415"/>surface after the next step. As shown <a href="#figure15-34">Figure 15-34</a>(c), we find the gradient <em>g </em>at point P and scale it as usual to get <em>ηg</em>. Now we find the new point C in <a href="#figure15-34">Figure 15-34</a>(d) by adding the scaled momentum <em>γm</em> and the scaled gradient <em>ηg</em> to B.</p>
<figure>
<img src="Images/F15034.png" alt="F15034" width="600" height="586"/>
<figcaption><p><a id="figure15-34">Figure 15-34</a>: Gradient descent with Nesterov momentum</p></figcaption>
</figure>
<p>Notice we’re not using the gradient at point B at all. We just combine a scaled version of the momentum that got us to B and a scaled version of the gradient at our predicted point P.</p>
<p>Notice too that the point C in <a href="#figure15-34">Figure 15-34</a>(d) is closer to the bottom of the bowl than point P, where we’d have ended up with normal momentum. By looking into the future and seeing that we’d be on the other side of the valley, we are able to use that left-pointing gradient to prevent rolling far up the far side.</p>
<p>In honor of the researcher who developed this method, it’s called <em>Nesterov momentum</em>, or the <em>Nesterov accelerated gradient</em> (Nesterov 1983). It’s basically a souped-up version of the momentum technique we saw earlier. Though we still have to pick a value for <em>γ,</em> we don’t have to pick any new parameters. This is a nice example of an algorithm that gives us increased performance without requiring more work on our end.</p>
<p><a href="#figure15-35" id="figureanchor15-35">Figure 15-35</a> shows the result of Nesterov momentum for 15 steps.</p>
<span epub:type="pagebreak" title="416" id="Page_416"/><figure>
<img src="Images/F15035.png" alt="F15035" width="711" height="312"/>
<figcaption><p><a id="figure15-35">Figure 15-35</a>: Running Nesterov momentum for 15 steps. It finds the bottom of the valley in about seven steps and then stays there.</p></figcaption>
</figure>
<p><a href="#figure15-36" id="figureanchor15-36">Figure 15-36</a> shows the error curve for our standard test case using Nesterov momentum. This uses the exact same model and parameters as the momentum-only results in <a href="#figure15-33">Figure 15-33</a>, but it’s both less noisy and more efficient, getting down to about 0 error at roughly epoch 425, rather than the roughly 600 required by regular momentum alone.</p>
<figure>
<img src="Images/f15036.png" alt="f15036" width="707" height="511"/>
<figcaption><p><a id="figure15-36">Figure 15-36</a>: Error for mini-batch SGD with Nesterov momentum. The system reaches zero error around epoch 600. The graph shows 1,000 epochs.</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="417" id="Page_417"/>Any time we use momentum it’s worth considering Nesterov momentum instead. It requires no additional parameters from us, but it usually learns more quickly and with less noise.</p>
<h3 id="h2-500723c15-0009">Adagrad</h3>
<p class="BodyFirst">We’ve seen two types of momentum that help push us through plateaus and reduce overshooting. We’ve been using the same learning rate when we update all the weights in our network. Earlier in this chapter, we mentioned the idea of using a learning rate <em>η </em>that’s tailored individually for each weight.</p>
<p>Several related algorithms use this idea. Their names all begin with <em>Ada</em>, standing for “adaptive.”</p>
<p>Let’s start with an algorithm called <em>Adagrad</em>, which is short for <em>adaptive gradient learning</em> (Duchi, Hazan, and Singer 2011). As the name implies, the algorithm adapts (or changes) the size of the gradient for each weight. In other words, Adagrad gives us a way to perform learning-rate decay on a weight-by-weight basis. For each weight, Adagrad takes the gradient that we use in that update step, squares it, and adds that into a running sum for that weight. Then the gradient is divided by a value derived from this sum, giving us the value that’s then used for the update.</p>
<p>Because each step’s gradient is squared before it’s added in, the value that’s added into the sum is always positive. As a result, this running sum gets larger and larger over time. To keep it from growing out of control, we divide each change by that growing sum, so the changes to each weight get smaller and smaller over time.</p>
<p>This sounds a lot like learning rate decay. As time goes on, the changes to the weights get smaller. The difference here is that the slowdown in learning is being computed uniquely for each weight based on its history.</p>
<p>Because Adagrad is effectively automatically computing a learning rate for every weight on the fly, the learning rate we use to kick things off isn’t as critical as it was for earlier algorithms. This is a huge benefit, since it frees us from the task of fine-tuning that error rate. We often set the learning rate <em>η</em> to a small value like 0.01 and let Adagrad handle things from there.</p>
<p><a href="#figure15-37" id="figureanchor15-37">Figure 15-37</a> shows the performance of Adagrad on our test data.</p>
<p>This has the same general shape as most of our other curves, but it takes a very long time to get to 0. Because the sum of the gradients gets larger over time, eventually we’ll find that dividing each new gradient by a value related to that sum gives us gradients that approach 0. The increasingly small updates are why the error curve for Adagrad descends so very slowly as it tries to get rid of that last remaining error.</p>
<p>We can fix that without too much work.</p>
<span epub:type="pagebreak" title="418" id="Page_418"/><figure>
<img src="Images/F15037.png" alt="F15037" width="707" height="511"/>
<figcaption><p><a id="figure15-37">Figure 15-37</a>: The performance of Adagrad on our test setup</p></figcaption>
</figure>
<h3 id="h2-500723c15-0010">Adadelta and RMSprop</h3>
<p class="BodyFirst">The problem with Adagrad is that the gradient we apply to each weight for its update step just keeps getting smaller and smaller. That’s because the running sum just gets larger and larger.</p>
<p>Instead of summing up all the squared gradients since the beginning of training, suppose we keep a <em>decaying sum</em> of these gradients. We can think of this as a running list of the most recent gradients for each weight. Each time we update the weights, we tack the new gradient onto the end of the list and drop the oldest one off the start. To find the value we use to divide the new gradient, we add up all the values in the list, but we first multiply them all by a number based on their position in the list. Recent values get multiplied by a large value, while the oldest ones get multiplied by a very small value. This way our running sum is most heavily determined by recent gradients, though it is influenced to a lesser degree by the older gradients (Ruder 2017).</p>
<p>In this way, the running sum of the gradients (and thus the value we divided new gradients by) can go up and down based on the gradients we’ve applied recently.</p>
<p>This algorithm is called <em>Adadelta</em> (Zeiler 2012). The name comes from “adaptive,” like Adagrad, and the <em>delta</em> refers to the Greek letter <em>δ </em>(delta), which mathematicians often use to refer to change. This algorithm adaptively changes how much the weights are updated on each step using each one’s weighted running sum.</p>
<p>Since Adadelta adjusts the learning rates on the weights individually, any weight that’s been on a steep slope for a while will slow down so it <span epub:type="pagebreak" title="419" id="Page_419"/>doesn’t go flying off, but when that weight is on a flatter section, it’s allowed to take bigger steps.</p>
<p>Like Adagrad, we often start the learning rate at a value around 0.01, and then let the algorithm adjust it from then on.</p>
<p><a href="#figure15-38" id="figureanchor15-38">Figure 15-38</a> shows the results of Adadelta on our test setup.</p>
<figure>
<img src="Images/F15038.png" alt="F15038" width="706" height="517"/>
<figcaption><p><a id="figure15-38">Figure 15-38</a>: The results of training with Adadelta on our test data</p></figcaption>
</figure>
<p>This compares favorably to Adagrad’s performance in <a href="#figure15-37">Figure 15-37</a>. It’s nice and smooth and reaches 0 at around epoch 2,500, much sooner than Adagrad’s 8,000 epochs.</p>
<p>Adadelta has the downside of requiring another parameter, which is unfortunately also called gamma (<em>γ</em>). It’s roughly related to the parameter <em>γ </em>used by the momentum algorithms, but they’re sufficiently different that it’s best to consider them distinct ideas that happen to have been given the same name. The value of <em>γ </em>here tells us how much we scale down the gradients in our history list over time. A large value of <em>γ</em> “remembers” values from farther back than smaller values and will let them contribute to the sum. A smaller value of <em>γ </em>just focuses on recent gradients. Often we set this <em>γ </em>to around 0.9.</p>
<p>There’s actually another parameter in Adadelta, named with the Greek letter <em>ε</em> (epsilon). This is a detail that’s used to keep the calculations numerically stable. Most libraries will set this to a default value that’s carefully selected by the programmers to make things work as well as possible, so it should never be changed unless there’s a specific need.</p>
<p>An algorithm that’s very similar to Adadelta, but that uses slightly different mathematics, is called <em>RMSprop</em> (Hinton, Srivastava, and Swersky 2015). The name comes from the fact that it uses a root-mean-squared <span epub:type="pagebreak" title="420" id="Page_420"/>operation, often abbreviated RMS, to determine the adjustment that is added (or <em>propagated</em>, hence the “prop” in the name) to the gradients.</p>
<p>RMSprop and Adadelta were invented around the same time, and work in similar ways. RMSprop also uses a parameter to control how much it “remembers,” and this parameter, too, is named <em>γ</em>. Again, a good starting value is around 0.9.</p>
<h3 id="h2-500723c15-0011">Adam</h3>
<p class="BodyFirst">The previous algorithms share the idea of saving a list of squared gradients with each weight. They then create a scaling factor by adding up the values in this list, perhaps after scaling them. The gradient at each update step is divided by this total. Adagrad gives all the elements in the list equal weight when it builds its scaling factor, while Adadelta and RMSprop treat older elements as less important, and thus they contribute less to the overall total.</p>
<p>Squaring the gradient before putting it into the list is useful mathematically, but when we square a number, the result is always positive. This means that we lose track of whether that gradient in our list was positive or negative, which is useful information to have. So, to avoid losing this information, we can keep a second list of the gradients without squaring them. Then we can use both lists to derive our scaling factor.</p>
<p>This is the approach of an algorithm called <em>adaptive moment estimation</em>, or more commonly <em>Adam</em> (Kingma and Ba 2015).</p>
<p><a href="#figure15-39" id="figureanchor15-39">Figure 15-39</a> shows how Adam performs.</p>
<figure>
<img src="Images/F15039.png" alt="F15039" width="707" height="500"/>
<figcaption><p><a id="figure15-39">Figure 15-39</a>: The Adam algorithm on our test set </p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="421" id="Page_421"/>The output is great. It’s only slightly noisy and hits about 0 error at around epoch 900, much sooner than Adagrad or Adadelta. The downside is that Adam has two parameters, which we must set at the start of learning. The parameters are named for the Greek letter <em>β </em>(beta) and are called “beta 1” and “beta 2,” written <em>β</em>1 and <em>β</em>2. The authors of the paper on Adam suggest setting <em>β</em>1 to 0.9, and <em>β</em>2 to 0.999, and these values indeed often work well.</p>
<h2 id="h1-500723c15-0005">Choosing an Optimizer</h2>
<p class="BodyFirst">This has not been a complete list of all the optimizers that have been proposed and studied. There are many others, with more coming all the time, and each has its own strengths and weaknesses. Our goal was to give an overview of some of the most popular techniques and to understand how they achieve their speedups.</p>
<p><a href="#figure15-40" id="figureanchor15-40">Figure 15-40</a> summarizes our two-moon results for SGD with Nesterov momentum and the three adaptive algorithms of Adagrad, Adadelta, and Adam.</p>
<figure>
<img src="Images/F15040.png" alt="F15040" width="707" height="517"/>
<figcaption><p><a id="figure15-40">Figure 15-40</a>: The loss, or error, over time for four of the algorithms just covered. This graph shows only the first 4,000 epochs.</p></figcaption>
</figure>
<p>In this simple test case, mini-batch SGD with Nesterov momentum is the clear winner, with Adam coming in a close second. In more complicated situations, the adaptive algorithms typically perform better.</p>
<p><span epub:type="pagebreak" title="422" id="Page_422"/>Across a wide variety of datasets and networks, the final three adaptive algorithms that we discussed (Adadelta, RMSprop, and Adam) often perform very similarly (Ruder 2017). Studies have found that Adam does a slightly better job than the others in some circumstances, so that’s usually a good place to start (Kingma and Ba 2015).</p>
<p>Why are there so many optimizers? Wouldn’t it be wise to find the best one and stick with that? It turns out that not only do we not know of a “best” optimizer, but there can’t be a best optimizer for all situations<em>. </em>No matter what optimizer we put forth as the “best,” we can prove that it’s always possible to find some situation in which another optimizer would be better. This result is famously known by its colorful name, the <em>No Free Lunch Theorem</em> (Wolpert 1996; Wolpert and Macready 1997). This guarantees us that no optimizer will always perform better than any other.</p>
<p>Note that the No Free Lunch Theorem doesn’t say that all optimizers are equal. As we’ve seen in our tests in this chapter, different optimizers do perform differently. The theorem only tells us that no one optimizer will <em>always</em> beat the others. </p>
<p>Though no one optimizer is the best choice for all possible training situations, we can find the best optimizer for any specific combination of network and data. Most deep learning libraries offer routines that carry out an automated search that can try out multiple optimizers and run through multiple parameter choices for each one. Whether we choose our optimizer and its values by ourselves or as the result of a search, we need to keep in mind that the best choices can vary from one network and set of data to the next. As soon as we make a big change to either, we should consider checking to see if a better optimizer would give us more efficient training. As a practical guide, many people start out with Adam, using its default parameters. </p>
<h2 id="h1-500723c15-0006">Regularization</h2>
<p class="BodyFirst">No matter what optimizer we choose, our network can suffer from overfitting. As we discussed in Chapter 9, overfitting is a natural result of training for too long. The problem is that the network learns the training data so well that it becomes tuned to just that data and performs poorly on new data once it’s released.</p>
<p>Techniques that delay the onset of overfitting are called <em>regularization</em> methods. They allow us to train for more epochs before overfitting has too great an impact, which means our networks have more training time in which to improve their performance.</p>
<h3 id="h2-500723c15-0012">Dropout</h3>
<p class="BodyFirst">A popular regularization method is called <em>dropout</em>. It is usually applied in a deep network in the form of a <em>dropout layer</em> (Srivastava et al. 2014). The dropout layer is called an <em>accessory layer </em>or a <em>supplemental layer</em>, because it doesn’t do any computation of its own. We call it a layer, and draw it as one, because it’s convenient conceptually, and lets us include dropout in <span epub:type="pagebreak" title="423" id="Page_423"/>drawings of networks. But we don’t consider it a real layer (hidden or otherwise), and we don’t count it when we describe how many layers make up a particular network. </p>
<p>Dropout is a placeholder that tells the network to run an algorithm on the previous layer. It’s also only active during training. When the network is deployed, dropout layers are disabled or removed.</p>
<p>The job of the dropout layer is to temporarily disconnect some of the neurons on the previous layer. We give it a parameter that describes the percentage<em> </em>of neurons that should be affected, and at the start of each batch, it randomly chooses that percentage of neurons on the preceding layer and temporarily disconnects their inputs and outputs from the network. Since they’re disconnected, these neurons don’t participate in any forward calculations, they’re not included in backprop, and the weights coming into them are not updated by the optimizer. When the batch is done and the rest of the weights have been updated, the chosen neurons and all of their connections are restored.</p>
<p>At the start of the next batch, the layer again chooses a new random set of neurons and temporarily removes those, repeating the process for each epoch. <a href="#figure15-41" id="figureanchor15-41">Figure 15-41</a> shows the idea graphically.</p>
<figure>
<img src="Images/F15041.png" alt="F15041" width="696" height="484"/>
<figcaption><p><a id="figure15-41">Figure 15-41</a>: Dropout. (a) 50 percent of the four neurons in the middle layer (in gray) are chosen to be disconnected before the batch is evaluated. (b) Our schematic for a single dropout layer is a diagonal slash. To the right, we indicate the proportion of neurons that are selected for disconnection. Since dropout applies to its preceding layer, in this example, we apply it to the middle of the three fully connected layers.</p></figcaption>
</figure>
<p>Dropout delays overfitting by preventing any neurons from overspecializing and dominating. Suppose that one neuron in a photo classification system gets highly specialized to detect the eyes of cats. That’s useful for <span epub:type="pagebreak" title="424" id="Page_424"/>recognizing picture of cats’ faces, but useless for all the other photographs the system might be asked to classify. If all the neurons in a network also specialize at finding just one or two features in the training data, then they can perform beautifully on that data because they spot the idiosyncratic details that they’re trained to locate. But the system as a whole will then perform badly when presented with new data that’s missing the precise cues those neurons became specialized for.</p>
<p>Dropout helps us avoid this kind of specialization. When a neuron is disconnected, the remaining neurons must adjust to pick up the slack. Thus, the specialized neuron is freed up to perform a more generally useful task, and we’ve delayed the onset of overfitting. Dropout helps us put off overfitting by spreading around the learning among all the neurons.</p>
<h3 id="h2-500723c15-0013">Batchnorm</h3>
<p class="BodyFirst">Another regularization technique is called <em>batch normalization</em>, often referred to simply as <em>batchnorm</em> (Ioffe and Szegedy 2015). Like dropout, batchnorm can be implemented as a layer without neurons. Unlike dropout, batchnorm actually does perform some computation, though there are no parameters for us to specify.</p>
<p>Batchnorm modifies the values that come out of a layer. This might seem strange, since the whole purpose of training is to get our neurons to produce output values that lead to good results. Why would we want to modify those outputs?</p>
<p>Recall that many of our activation functions, such as leaky ReLU and tanh, have their greatest effect near 0. To get the most benefit from those functions, we need the numbers flowing into them to be in a small range centered around 0. That’s what batchnorm does by scaling and shifting all the outputs of a layer together. Because batchnorm moves the neuron outputs into a small range near 0, we’re less prone to seeing any neuron learning one specific detail and producing a huge output that swamps all the other neurons, and thus we are able to delay the onset of overfitting. Batchnorm scales and shifts all the values coming out of the previous layer over the course of an entire mini-batch in just this way. It learns the parameters for this scaling and shifting along with the weights in the network so they take on the most useful values.</p>
<p>We apply batchnorm before the activation function so that the modified values will fall in the region of the activation function where they are affected the most. In practice, this means we place no activation function on the neurons going into batchnorm (or if we must specify a function, it’s the linear activation function, which has no effect). Those values go into batchnorm, and then they’re fed into the activation function we want to apply.</p>
<p>The process is illustrated in <a href="#figure15-42" id="figureanchor15-42">Figure 15-42</a>. Our icon for a regularization step like batchnorm is a black disc inside a circle, suggesting that the values in the circle are transformed into a smaller region. In later chapters, we’ll see other, similar regularization steps for which we’ll use the same icon. The text (or a nearby label) identifies which variety of regularization is applied.</p>
<span epub:type="pagebreak" title="425" id="Page_425"/><figure>
<img src="Images/F15042.png" alt="F15042" width="413" height="174"/>
<figcaption><p><a id="figure15-42">Figure 15-42</a>: Applying a batchnorm layer. Top: A neuron followed by a leaky ReLU activation function. Bottom: The same neuron with batchnorm. The activation function is replaced with the linear function, followed by batchnorm (represented by a circle with a black disc inside) and then the leaky ReLU. </p></figcaption>
</figure>
<p>Like dropout, batchnorm defers the onset of overfitting, allowing us to train longer.</p>
<h2 id="h1-500723c15-0007">Summary</h2>
<p class="BodyFirst">Optimization is the process of adjusting the weights so that our network learns. The core idea begins with the gradient for every weight. We follow that gradient to direct us to a lower point on the error surface, hence the name gradient descent. The most important value in this process is the learning rate. A common technique is to reduce the learning rate over time, according to a decay schedule.</p>
<p>We covered several efficient optimization techniques. We can adjust the weights after every epoch (batch gradient descent), after every sample (stochastic gradient descent, or SGD), or after mini-batches of samples (mini-batch gradient descent or mini-batch SGD). Mini-batch gradient descent is by far the most common technique, and the convention in the field is to refer to it simply as SGD. We can improve the efficiency of every type of gradient descent by using momentum. We can also improve learning by computing a custom, adaptive learning rate for every weight over time with an algorithm such as Adam. Lastly, to prevent overfitting, we can use a regularization technique such as dropout or batchnorm.</p>
<p>Deep networks that are made up of fully connected layers can do some amazing things. But if we create our layers by structuring the neurons in different ways and add a little bit of supporting computation, their power increases significantly. In the next few chapters, we’ll look at these new layers and how they can be used to classify, predict, and even generate images, sounds, and more. </p>
</section>
</div></body></html>