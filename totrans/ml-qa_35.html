<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><h2 class="h2" id="ch30"><span epub:type="pagebreak" id="page_193"/><strong><span class="big">30</span><br/>LIMITED LABELED DATA</strong></h2>&#13;
<div class="image1"><img src="../images/common.jpg" alt="Image" width="252" height="252"/></div>&#13;
<p class="noindent">Suppose we plot a learning curve (as shown in <a href="ch05.xhtml#ch5fig1">Figure 5-1</a> on <a href="ch05.xhtml#ch5fig1">page 24</a>, for example) and find the machine learning model overfits and could benefit from more training data. What are some different approaches for dealing with limited labeled data in supervised machine learning settings?</p>&#13;
<p class="indent">In lieu of collecting more data, there are several methods related to regular supervised learning that we can use to improve model performance in limited labeled data regimes.</p>&#13;
<h3 class="h3" id="ch00lev145"><strong>Improving Model Performance with Limited Labeled Data</strong></h3>&#13;
<p class="noindent">The following sections explore various machine learning paradigms that help in scenarios where training data is limited.</p>&#13;
<h4 class="h4" id="ch00levsec32"><em><strong>Labeling More Data</strong></em></h4>&#13;
<p class="noindent">Collecting additional training examples is often the best way to improve the performance of a model (a learning curve is a good diagnostic for this). However, this is often not feasible in practice, because acquiring high-quality <span epub:type="pagebreak" id="page_194"/>data can be costly, computational resources and storage might be insufficient, or the data may be hard to access.</p>&#13;
<h4 class="h4" id="ch00levsec33"><em><strong>Bootstrapping the Data</strong></em></h4>&#13;
<p class="noindent">Similar to the techniques for reducing overfitting discussed in <a href="ch05.xhtml">Chapter 5</a>, it can be helpful to “bootstrap” the data by generating modified (augmented) or artificial (synthetic) training examples to boost the performance of the predictive model. Of course, improving the quality of data can also lead to the improved predictive performance of a model, as discussed in <a href="ch21.xhtml">Chapter 21</a>.</p>&#13;
<h4 class="h4" id="ch00levsec34"><em><strong>Transfer Learning</strong></em></h4>&#13;
<p class="noindent">Transfer learning describes training a model on a general dataset (for example, ImageNet) and then fine-tuning the pretrained target dataset (for example, a dataset consisting of different bird species), as outlined in <a href="ch30.xhtml#ch30fig1">Figure 30-1</a>.</p>&#13;
<div class="image"><img id="ch30fig1" src="../images/30fig01.jpg" alt="Image" width="473" height="281"/></div>&#13;
<p class="figcap"><em>Figure 30-1: The process of transfer learning</em></p>&#13;
<p class="indent">Transfer learning is usually done in the context of deep learning, where model weights can be updated. This is in contrast to tree-based methods, since most decision tree algorithms are nonparametric models that do not support iterative training or parameter updates.</p>&#13;
<h4 class="h4" id="ch00levsec35"><em><strong>Self-Supervised Learning</strong></em></h4>&#13;
<p class="noindent">Similar to transfer learning, in self-supervised learning, the model is pre-trained on a different task before being fine-tuned to a target task for which only limited data exists. However, self-supervised learning usually relies on label information that can be directly and automatically extracted from unlabeled data. Hence, self-supervised learning is also often called <em>unsupervised pretraining</em>.</p>&#13;
<p class="indent">Common examples of self-supervised learning include the <em>next word</em> (used in GPT, for example) or <em>masked word</em> (used in BERT, for example) pre-training tasks in language modeling, covered in more detail in <a href="ch17.xhtml">Chapter 17</a>. Another intuitive example from computer vision includes <em>inpainting</em>: predicting the missing part of an image that was randomly removed, illustrated in <a href="ch30.xhtml#ch30fig2">Figure 30-2</a>.</p>&#13;
<div class="image"><img id="ch30fig2" src="../images/30fig02.jpg" alt="Image" width="540" height="345"/></div>&#13;
<p class="figcap"><em>Figure 30-2: Inpainting for self-supervised learning</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_195"/>For more detail on self-supervised learning, see <a href="ch02.xhtml">Chapter 2</a>.</p>&#13;
<h4 class="h4" id="ch00levsec36"><em><strong>Active Learning</strong></em></h4>&#13;
<p class="noindent">In active learning, illustrated in <a href="ch30.xhtml#ch30fig3">Figure 30-3</a>, we typically involve manual labelers or users for feedback during the learning process. However, instead of labeling the entire dataset up front, active learning includes a prioritization scheme for suggesting unlabeled data points for labeling to maximize the machine learning model’s performance.</p>&#13;
<div class="image"><img id="ch30fig3" src="../images/30fig03.jpg" alt="Image" width="554" height="286"/></div>&#13;
<p class="figcap"><em>Figure 30-3: In active learning, a model queries an oracle for labels.</em></p>&#13;
<p class="indent">The term <em>active learning</em> refers to the fact that the model actively selects data for labeling. For example, the simplest form of active learning selects data points with high prediction uncertainty for labeling by a human annotator (also referred to as an <em>oracle</em>).</p>&#13;
<h4 class="h4" id="ch00levsec37"><em><strong>Few-Shot Learning</strong></em></h4>&#13;
<p class="noindent">In a few-shot learning scenario, we often deal with extremely small datasets that include only a handful of examples per class. In research contexts, 1-shot (one example per class) and 5-shot (five examples per class) learning scenarios are very common. An extreme case of few-shot learning is zero-shot learning, where no labels are provided. Popular examples of zero-shot learning include GPT-3 and related language models, where the user has to <span epub:type="pagebreak" id="page_196"/>provide all the necessary information via the input prompt, as illustrated in <a href="ch30.xhtml#ch30fig4">Figure 30-4</a>.</p>&#13;
<div class="image"><img id="ch30fig4" src="../images/30fig04.jpg" alt="Image" width="907" height="553"/></div>&#13;
<p class="figcap"><em>Figure 30-4: Zero-shot classification with ChatGPT</em></p>&#13;
<p class="indent">For more detail on few-shot learning, see <a href="ch03.xhtml">Chapter 3</a>.</p>&#13;
<h4 class="h4" id="ch00levsec38"><em><strong>Meta-Learning</strong></em></h4>&#13;
<p class="noindent">Meta-learning involves developing methods that determine how machine learning algorithms can best learn from data. We can therefore think of meta-learning as “learning to learn.” The machine learning community has developed several approaches for meta-learning. Within the machine learning community, the term <em>meta-learning</em> doesn’t just represent multiple subcategories and approaches; it is also occasionally employed to describe related yet distinct processes, leading to nuances in its interpretation and application.</p>&#13;
<p class="indent">Meta-learning is one of the main subcategories of few-shot learning. Here, the focus is on learning a good feature extraction module, which converts support and query images into vector representations. These vector representations are optimized for determining the predicted class of the query example via comparisons with the training examples in the support set. (This form of meta-learning is illustrated in <a href="ch03.xhtml">Chapter 3</a> on <a href="ch03.xhtml#ch3fig2">page 17</a>.)</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_197"/>Another branch of meta-learning unrelated to the few-shot learning approach is focused on extracting metadata (also called <em>meta-features</em>) from datasets for supervised learning tasks, as illustrated in <a href="ch30.xhtml#ch30fig5">Figure 30-5</a>. The meta-features are descriptions of the dataset itself. For example, these can include the number of features and statistics of the different features (kurtosis, range, mean, and so on).</p>&#13;
<div class="image"><img id="ch30fig5" src="../images/30fig05.jpg" alt="Image" width="896" height="464"/></div>&#13;
<p class="figcap"><em>Figure 30-5: The meta-learning process involving the extraction of metadata</em></p>&#13;
<p class="indent">The extracted meta-features provide information for selecting a machine learning algorithm for the dataset at hand. Using this approach, we can narrow down the algorithm and hyperparameter search spaces, which helps reduce overfitting when the dataset is small.</p>&#13;
<h4 class="h4" id="ch00levsec39"><em><strong>Weakly Supervised Learning</strong></em></h4>&#13;
<p class="noindent">Weakly supervised learning, illustrated in <a href="ch30.xhtml#ch30fig6">Figure 30-6</a>, involves using an external label source to generate labels for an unlabeled dataset. Often, the labels created by a weakly supervised labeling function are more noisy or inaccurate than those produced by a human or domain expert, hence the term <em>weakly</em> supervised. We can develop or adopt a rule-based classifier to create the labels in weakly supervised learning; these rules usually cover only a subset of the unlabeled dataset.</p>&#13;
<div class="image"><img id="ch30fig6" src="../images/30fig06.jpg" alt="Image" width="772" height="608"/></div>&#13;
<p class="figcap"><em>Figure 30-6: Weakly supervised learning uses external labeling functions to train machine learning models.</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_198"/>Let’s return to the example of email spam classification from <a href="ch23.xhtml">Chapter 23</a> to illustrate a rule-based approach for data labeling. In weak supervision, we could design a rule-based classifier based on the keyword <em>SALE</em> in the email subject header line to identify a subset of spam emails. Note that while we may use this rule to label certain emails as spam positive, we should not apply this rule to label emails without <em>SALE</em> as non-spam. Instead, we should either leave those unlabeled or apply a different rule to them.</p>&#13;
<p class="indent">There is a subcategory of weakly supervised learning referred to as PU-learning. In <em>PU-learning</em>, which is short for <em>positive-unlabeled learning</em>, we label and learn only from positive examples.</p>&#13;
<h4 class="h4" id="ch00levsec40"><em><strong>Semi-Supervised Learning</strong></em></h4>&#13;
<p class="noindent">Semi-supervised learning is closely related to weakly supervised learning: it also involves creating labels for unlabeled instances in the dataset. The main difference between these two methods lies in <em>how</em> we create the labels. In weak supervision, we create labels using an external labeling function that is often noisy, inaccurate, or covers only a subset of the data. In semi-supervision, we do not use an external label function; instead, we leverage the structure of the data itself. We can, for example, label additional data points based on the density of neighboring labeled data points, as illustrated in <a href="ch30.xhtml#ch30fig7">Figure 30-7</a>.</p>&#13;
<div class="image"><img id="ch30fig7" src="../images/30fig07.jpg" alt="Image" width="704" height="226"/></div>&#13;
<p class="figcap"><em>Figure 30-7: Semi-supervised learning</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_199"/>While we can apply weak supervision to an entirely unlabeled dataset, semi-supervised learning requires at least a portion of the data to be labeled. In practice, it is possible first to apply weak supervision to label a subset of the data and then to use semi-supervised learning to label instances that were not captured by the labeling functions.</p>&#13;
<p class="indent">Thanks to their close relationship, semi-supervised learning is sometimes referred to as a subcategory of weakly supervised learning, and vice versa.</p>&#13;
<h4 class="h4" id="ch00levsec41"><em><strong>Self-Training</strong></em></h4>&#13;
<p class="noindent">Self-training falls somewhere between semi-supervised learning and weakly supervised learning. For this technique, we train a model to label the dataset or adopt an existing model to do the same. This model is also referred to as a <em>pseudo-labeler</em>.</p>&#13;
<p class="indent">Self-training does not guarantee accurate labels and is thus related to weakly supervised learning. Moreover, while we use or adopt a machine learning model for this pseudo-labeling, self-training is also related to semi-supervised learning.</p>&#13;
<p class="indent">An example of self-training is knowledge distillation, discussed in <a href="ch06.xhtml">Chapter 6</a>.</p>&#13;
<h4 class="h4" id="ch00levsec42"><em><strong>Multi-Task Learning</strong></em></h4>&#13;
<p class="noindent">Multi-task learning trains neural networks on multiple, ideally related tasks. For example, if we are training a classifier to detect spam emails, spam classification is the main task. In multi-task learning, we can add one or more related tasks for the model to solve, referred to as <em>auxiliary tasks</em>. For the spam email example, an auxiliary task could be classifying the email’s topic or language.</p>&#13;
<p class="indent">Typically, multi-task learning is implemented via multiple loss functions that have to be optimized simultaneously, with one loss function for each task. The auxiliary tasks serve as an inductive bias, guiding the model to prioritize hypotheses that can explain multiple tasks. This approach often results in models that perform better on unseen data.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_200"/>There are two subcategories of multi-task learning: multi-task learning with hard parameter sharing and multi-task learning with soft parameter sharing. <a href="ch30.xhtml#ch30fig8">Figure 30-8</a> illustrates the difference between these two methods.</p>&#13;
<div class="image"><img id="ch30fig8" src="../images/30fig08.jpg" alt="Image" width="1118" height="459"/></div>&#13;
<p class="figcap"><em>Figure 30-8: The two main types of multi-task learning</em></p>&#13;
<p class="indent">In <em>hard</em> parameter sharing, as shown in <a href="ch30.xhtml#ch30fig8">Figure 30-8</a>, only the output layers are task specific, while all the tasks share the same hidden layers and neural network backbone architecture. In contrast, <em>soft</em> parameter sharing uses separate neural networks for each task, but regularization techniques such as distance minimization between parameter layers are applied to encourage similarity among the networks.</p>&#13;
<h4 class="h4" id="ch00levsec43"><em><strong>Multimodal Learning</strong></em></h4>&#13;
<p class="noindent">While multi-task learning involves training a model with multiple tasks and loss functions, multimodal learning focuses on incorporating multiple types of input data.</p>&#13;
<p class="indent">Common examples of multimodal learning are architectures that take both image and text data as input (though multimodal learning is not restricted to only two modalities and can be used for any number of input modalities). Depending on the task, we may employ a matching loss that forces the embedding vectors between related images and text to be similar, as shown in <a href="ch30.xhtml#ch30fig9">Figure 30-9</a>. (See <a href="ch01.xhtml">Chapter 1</a> for more on embedding vectors.)</p>&#13;
<div class="image"><img id="ch30fig9" src="../images/30fig09.jpg" alt="Image" width="642" height="449"/></div>&#13;
<p class="figcap"><em>Figure 30-9: Multimodal learning with a matching loss</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_201"/><a href="ch30.xhtml#ch30fig9">Figure 30-9</a> shows image and text encoders as separate components. The image encoder can be a convolutional backbone or a vision transformer, and the language encoder can be a recurrent neural network or language transformer. However, it’s common nowadays to use a single transformer-based module that can simultaneously process image and text data. For example, the VideoBERT model has a joint module that processes both video and text for action classification and video captioning.</p>&#13;
<p class="indent">Optimizing a matching loss, as shown in <a href="ch30.xhtml#ch30fig9">Figure 30-9</a>, can be useful for learning embeddings that can be applied to various tasks, such as image classification or summarization. However, it is also possible to directly optimize the target loss, like classification or regression, as <a href="ch30.xhtml#ch30fig10">Figure 30-10</a> illustrates.</p>&#13;
<div class="image"><img id="ch30fig10" src="../images/30fig10.jpg" alt="Image" width="614" height="386"/></div>&#13;
<p class="figcap"><em>Figure 30-10: Multimodal learning for optimizing a supervised learning objective</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_202"/><a href="ch30.xhtml#ch30fig10">Figure 30-10</a> shows data being collected from two different sensors. One could be a thermometer and the other could be a video camera. The signal encoders convert the information into embeddings (sharing the same number of dimensions), which are then concatenated to form the input representation for the model.</p>&#13;
<p class="indent">Intuitively, models that combine data from different modalities generally perform better than unimodal models because they can leverage more information. Moreover, recent research suggests that the key to the success of multimodal learning is the improved quality of the latent space representation.</p>&#13;
<h4 class="h4" id="ch00levsec44"><em><strong>Inductive Biases</strong></em></h4>&#13;
<p class="noindent">Choosing models with stronger inductive biases can help lower data requirements by making assumptions about the structure of the data. For example, due to their inductive biases, convolutional networks require less data than vision transformers, as discussed in <a href="ch13.xhtml">Chapter 13</a>.</p>&#13;
<h3 class="h3" id="ch00lev146"><strong>Recommendations</strong></h3>&#13;
<p class="noindent">Of all these techniques for reducing data requirements, how should we decide which ones to use in a given situation?</p>&#13;
<p class="indent">Techniques like collecting more data, data augmentation, and feature engineering are compatible with all the methods discussed in this chapter. Multi-task learning and multimodal inputs can also be used with the learning strategies outlined here. If the model suffers from overfitting, we should also include techniques discussed in <a href="ch05.xhtml">Chapters 5</a> and <a href="ch06.xhtml">6</a>.</p>&#13;
<p class="indent">But how can we choose between active learning, few-shot learning, transfer learning, self-supervised learning, semi-supervised learning, and weakly supervised learning? Deciding which supervised learning technique(s) to try is highly context dependent. You can use the diagram in <a href="ch30.xhtml#ch30fig11">Figure 30-11</a> as a guide to choosing the best method for your particular project.</p>&#13;
<div class="image"><img id="ch30fig11" src="../images/30fig11.jpg" alt="Image" width="744" height="1143"/></div>&#13;
<p class="figcap"><em>Figure 30-11: Recommendations for choosing a supervised learning technique</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_203"/>Note that the dark boxes in <a href="ch30.xhtml#ch30fig11">Figure 30-11</a> are not terminal nodes but arc back to the second box, “Evaluate model performance”; additional arrows were omitted to avoid visual clutter.<span epub:type="pagebreak" id="page_204"/></p>&#13;
<h3 class="h3" id="ch00lev147"><strong>Exercises</strong></h3>&#13;
<p class="number1"><strong>30-1.</strong> Suppose we are given the task of constructing a machine learning model that utilizes images to detect manufacturing defects on the outer shells of tablet devices similar to iPads. We have access to millions of images of various computing devices, including smartphones, tablets, and computers, which are not labeled; thousands of labeled pictures of smart-phones depicting various types of damage; and hundreds of labeled images specifically related to the target task of detecting manufacturing defects on tablet devices. How could we approach this problem using self-supervised learning or transfer learning?</p>&#13;
<p class="number1"><strong>30-2.</strong> In active learning, selecting difficult examples for human inspection and labeling is often based on confidence scores. Neural networks can provide such scores by using the logistic sigmoid or softmax function in the output layer to calculate class-membership probabilities. However, it is widely recognized that deep neural networks exhibit overconfidence on out-of-distribution data, rendering their use in active learning ineffective. What are some other methods to obtain confidence scores using deep neural networks for active learning?</p>&#13;
<h3 class="h3" id="ch00lev148"><strong>References</strong></h3>&#13;
<ul>&#13;
<li class="noindent">While decision trees for incremental learning are not commonly implemented, algorithms for training decision trees in an iterative fashion do exist: <em><a href="https://en.wikipedia.org/wiki/Incremental_decision_tree">https://en.wikipedia.org/wiki/Incremental_decision_tree</a></em>.</li>&#13;
<li class="noindent">Models trained with multi-task learning often outperform models trained on a single task: Rich Caruana, “Multitask Learning” (1997), <em><a href="https://doi.org/10.1023%2FA%3A1007379606734">https://doi.org/10.1023%2FA%3A1007379606734</a></em>.</li>&#13;
<li class="noindent">A single transformer-based module that can simultaneously process image and text data: Chen Sun et al., “VideoBERT: A Joint Model for Video and Language Representation Learning” (2019), <em><a href="https://arxiv.org/abs/1904.01766">https://arxiv.org/abs/1904.01766</a></em>.</li>&#13;
<li class="noindent">The aforementioned research suggesting the key to the success of multimodal learning is the improved quality of the latent space representation: Yu Huang et al., “What Makes Multi-Modal Learning Better Than Single (Provably)” (2021), <em><a href="https://arxiv.org/abs/2106.04538">https://arxiv.org/abs/2106.04538</a></em>.</li>&#13;
<li class="noindent">For more information on active learning: Zhen et al., “A Comparative Survey of Deep Active Learning” (2022), <em><a href="https://arxiv.org/abs/2203.13450">https://arxiv.org/abs/2203.13450</a></em>.</li>&#13;
<li class="noindent">For a more detailed discussion on how out-of-distribution data can lead to overconfidence in deep neural networks: Anh Nguyen, Jason Yosinski, and Jeff Clune, “Deep Neural Networks Are Easily Fooled: High Confidence Predictions for Unrecognizable Images” (2014), <em><a href="https://arxiv.org/abs/1412.1897">https://arxiv.org/abs/1412.1897</a></em>.</li>&#13;
</ul>&#13;
</div>
</div>
</body></html>