- en: '**4'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: FLOATING-POINT REPRESENTATION**
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/comm1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Floating-point arithmetic is an approximation of real arithmetic that solves
    the major problem with integer data types—the inability to represent fractional
    values. However, the inaccuracies in this approximation can lead to serious defects
    in application software. In order to write great software that produces correct
    results when using floating-point arithmetic, programmers must be aware of the
    machine’s underlying numeric representation and exactly how floating-point arithmetic
    approximates real arithmetic.
  prefs: []
  type: TYPE_NORMAL
- en: '**4.1 Introduction to Floating-Point Arithmetic**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There is an infinite number of possible real values. Floating-point representation
    uses a finite number of bits and, therefore, can represent a finite number of
    different values. When a given floating-point format cannot exactly represent
    some real value, the closest value that the format *can* exactly represent is
    used. This section describes how the floating-point format works so you can better
    understand the drawbacks of these approximations.
  prefs: []
  type: TYPE_NORMAL
- en: Consider a couple of problems with integer and fixed-point formats. Integers
    cannot represent any fractional values, and they can represent only values in
    the range 0 through 2^(*n*) – 1 or –2^(*n*)^(–1) through 2^(*n*)^(–1) – 1\. Fixed-point
    formats represent fractional values, but at the expense of the range of integer
    values they can represent. This problem, which the floating-point format solves,
    is one of *[dynamic range](gloss01.xhtml#gloss01_85)*.
  prefs: []
  type: TYPE_NORMAL
- en: Consider a simple 16-bit unsigned fixed-point format that uses 8 bits for the
    fractional component and 8 bits for the integer component of the number. The integer
    component can represent values in the range 0 through 255, and the fractional
    component can represent the values 0 and fractions between 2^(–8) and 1 (with
    a resolution of about 2^(–8)). If in a string of calculations you need only 2
    bits to represent the fractional values 0.0, 0.25, 0.5, and 0.75, the extra 6
    bits in the fractional part of the number go to waste. Wouldn’t it be nice if
    we could utilize those bits in the integer portion of the number to extend its
    range from 0 through 255 to 0 through 16,383? Well, that’s the basic concept behind
    the floating-point representation.
  prefs: []
  type: TYPE_NORMAL
- en: In a floating-point value, the radix point (binary point) can float between
    digits in the number as needed. So, in a 16-bit binary number that needs only
    2 bits of precision for the fractional component, the binary point can float down
    between bits 1 and 2, leaving bits 2 through 15 for the integer portion. A floating-point
    format needs one additional field to specify the position of the radix point within
    the number, equivalent to the exponent in scientific notation.
  prefs: []
  type: TYPE_NORMAL
- en: Most floating-point formats use some number of bits to represent a mantissa
    and a smaller number of bits to represent an exponent*.* The *[mantissa](gloss01.xhtml#gloss01_145)*
    is a base value that usually falls within a limited range (for example, between
    0 and 1). The *[exponent](gloss01.xhtml#gloss01_92)* is a multiplier that, when
    applied to the mantissa, produces values outside this range. The big advantage
    of the mantissa/exponent configuration is that a floating-point format can represent
    values across a wide range. However, separating the number into these two parts
    means floating-point formats can represent only numbers with a specific number
    of *significant* digits. If the difference between the smallest and largest exponent
    is greater than the number of significant digits in the mantissa (and it usually
    is), then the floating-point format cannot exactly represent all the integers
    between the smallest and largest values in the floating-point representation.
  prefs: []
  type: TYPE_NORMAL
- en: To see the impact of limited-precision arithmetic, we’ll adopt a simplified
    *decimal* floating-point format for our examples. Our floating-point format will
    use a mantissa with three significant digits and a decimal exponent with two digits.
    The mantissa and exponents are both signed values, as shown in [Figure 4-1](ch04.xhtml#ch04fig01).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/04fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-1: Simple floating-point format*'
  prefs: []
  type: TYPE_NORMAL
- en: This particular floating-point representation can approximate all the values
    between 0.00 and 9.99 × 10^(99). However, this format cannot represent all (integer)
    values in this range (that would take 100 digits of precision!). A value like
    9,876,543,210 would be approximated with 9.88 × 10⁹ (or `9.88e+9` in programming
    language notation, which this book will generally use).
  prefs: []
  type: TYPE_NORMAL
- en: You cannot *exactly* represent as many different values with a floating-point
    format as with an integer format because the floating-point format encodes multiple
    representations (that is, different bit patterns) for the same value. In the simplified
    decimal floating-point format shown in [Figure 4-1](ch04.xhtml#ch04fig01), for
    example, `1.00e` `+` `1` and `0.10e + 2` are different representations of the
    same value. Because the number of different possible representations is finite,
    whenever a single value has two possible representations, that’s one less unique
    value the format can represent.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the floating-point format, a form of scientific notation, complicates
    arithmetic somewhat. When adding and subtracting two numbers in scientific notation,
    you must adjust the two values so that their exponents are the same. For example,
    when adding `1.23e1` and `4.56e0`, you could convert `4.56e0` to `0.456e1` and
    then add them. The result, `1.686e1`, does not fit into the three significant
    digits of our current format, so we must either *round* or *truncate* the result
    to three significant digits. Rounding generally produces the most accurate result,
    so let’s round the result to obtain `1.69e1`. The lack of *[precision](gloss01.xhtml#gloss01_202)*
    (the number of digits or bits maintained in a computation) affects the *[accuracy](gloss01.xhtml#gloss01_4)*
    (the correctness of the computation).
  prefs: []
  type: TYPE_NORMAL
- en: In the previous example, we were able to round the result because we maintained
    *four* significant digits *during* the calculation. If our floating-point calculation
    were limited to three significant digits during computation, we would have had
    to truncate (throw away) the last digit of the smaller number, obtaining `1.68e1`,
    which is even less correct. Therefore, to improve the accuracy, we use extra digits
    during the calculation. These extra digits are known as *guard digits* (or *guard
    bits* in the case of a binary format). They greatly enhance accuracy during a
    long chain of computations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The accuracy lost during a single computation usually isn’t bad. However, the
    error can accumulate over a sequence of floating-point operations and greatly
    affect the computation itself. For example, suppose we add `1.23e3` and `1.00e0`.
    Adjusting the numbers so their exponents are the same before the addition produces
    `1.23e3` `+` `0.001e3`. The sum of these two values, even after rounding, is `1.23e3`.
    This might seem perfectly reasonable to you: if we can maintain only three significant
    digits, adding in a small value shouldn’t affect the result. However, suppose
    we add `1.00e0` to `1.23e3` *10 times*. The first time we add `1.00e0` to `1.23e3`,
    we get `1.23e3`. Likewise, we get this same result the second, third, fourth .
    . . and tenth time. Had we added `1.00e0` to itself 10 times, then added the result
    (`1.00e1`) to `1.23e3`, we would obtain a different result, `1.24e3`. This is
    an important rule of limited-precision arithmetic:'
  prefs: []
  type: TYPE_NORMAL
- en: The order of evaluation can affect the accuracy of the result.
  prefs: []
  type: TYPE_NORMAL
- en: Adding or subtracting numbers with relative magnitudes (that is, the sizes of
    the exponents) that are similar produces better results. If you’re performing
    a chain calculation involving addition and subtraction, you should group the operations
    so that you can add or subtract values whose magnitudes are close to one another
    before adding or subtracting values whose magnitudes are not as close.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another problem with addition and subtraction is *[false precision](gloss01.xhtml#gloss01_94)*.
    Consider the computation `1.23e0` `-` `1.22e0`. This produces `0.01e0`. Although
    this is mathematically equivalent to `1.00e` `–` `2`, this latter form suggests
    that the last two digits (in the thousandths and ten-thousandths place) are both
    exactly 0\. Unfortunately, we only have a single significant digit after this
    computation, which is in the hundredths place, and some FPUs or floating-point
    software packages might actually insert random digits (or bits) into the LO positions.
    This brings up a second important rule:'
  prefs: []
  type: TYPE_NORMAL
- en: Whenever subtracting two numbers with the same signs or adding two numbers with
    different signs, the accuracy of the result may be less than the precision available
    in the floating-point format.
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiplication and division do not suffer from these problems, because you
    don’t have to adjust the exponents before the operation; all you need to do is
    add the exponents and multiply the mantissas (or subtract the exponents and divide
    the mantissas). By themselves, multiplication and division do not produce particularly
    poor results. However, they exacerbate any accuracy error that already exists
    in a value. For example, if you multiply `1.23e0` by 2, when you should be multiplying
    `1.24e0` by 2, the result is even less accurate than it was. This brings up a
    third important rule:'
  prefs: []
  type: TYPE_NORMAL
- en: When performing a chain of calculations involving addition, subtraction, multiplication,
    and division, perform the multiplication and division operations first.
  prefs: []
  type: TYPE_NORMAL
- en: 'Often, by applying normal algebraic transformations, you can arrange a calculation
    so the multiplication and division operations occur first. For example, suppose
    you want to compute the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*x* × (*y* + *z*)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Normally, you would add *y* and *z* together and multiply their sum by *x*.
    However, you’ll get a little more accuracy if you first transform the expression
    to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*x* × *y* + *x* × *z*'
  prefs: []
  type: TYPE_NORMAL
- en: Now you can compute the result by performing the multiplications first.^([1](footnotes.xhtml#fn4_1a))
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiplication and division have other problems as well. When you multiply
    two very large or very small numbers, *[overflow](gloss01.xhtml#gloss01_188)*
    or *[underflow](gloss01.xhtml#gloss01_249)* may occur. The same situation occurs
    when you divide a small number by a large number, or a large number by a small
    number. This brings up a fourth rule:'
  prefs: []
  type: TYPE_NORMAL
- en: When multiplying and dividing sets of numbers, try to multiply and divide numbers
    that have the same relative magnitudes.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing floating-point numbers is very dangerous. Given the inaccuracies inherent
    in any computation (including converting an input string to a floating-point value),
    you should *never* compare two floating-point values to see if they are equal.
    Different computations that produce the same (mathematical) result may differ
    in their least significant bits. For example, adding `1.31e0` and `1.69e0` should
    produce `3.00e0`. Likewise, adding `1.50e0` and `1.50e0` should produce `3.00e0`.
    However, were you to compare (`1.31e0` `+` `1.69e0`) to (`1.50e0` + `1.50e0`),
    you might find that these sums are *not* equal. Because two seemingly equivalent
    floating-point computations will not necessarily produce exactly equal results,
    a straight comparison for equality—which succeeds if and only if all bits (or
    digits) in the two operands are the same—may fail.
  prefs: []
  type: TYPE_NORMAL
- en: 'To test for equality between floating-point numbers, determine how much error
    (or tolerance) you’ll allow in a comparison, and then check to see if one value
    is within this error range of the other, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'More efficient is to use a statement of the form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The value for error should be slightly greater than the largest amount of error
    that will creep into your computations. The exact value depends upon the particular
    floating-point format you use and the magnitudes of the values you are comparing.
    So, the final rule is this:'
  prefs: []
  type: TYPE_NORMAL
- en: When comparing two floating-point numbers for equality, always compare the values
    to see if the difference between two values is less than some small error value.
  prefs: []
  type: TYPE_NORMAL
- en: Checking two floating-point numbers for equality is a very famous problem, one
    that almost every introductory programming text discusses. The same problems with
    comparing for less than or greater than, however, are not as well known. Suppose
    that a sequence of floating-point calculations produces a result that is accurate
    only to within ±error, even though the floating-point representation provides
    better accuracy than error suggests. If you compare such a result against some
    other calculation computed with less accumulated error, and those two values are
    very close to each other, then comparing them for less than or greater than may
    produce incorrect results.
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose that some chain of calculations in our simplified decimal
    representation produces 1.25, which is accurate only to ±0.05 (that is, the real
    value could be somewhere between 1.20 and 1.30), and a second chain of calculations
    produces 1.27, which is accurate to the full precision of our floating-point representation
    (that is, the actual value, before rounding, is somewhere between 1.265 and 1.275).
    Comparing the result of the first calculation (1.25) to the result of the second
    calculation (1.27) finds that the first result is less than the second. Unfortunately,
    given the inaccuracy of the first calculation, this might not be true—for example,
    if the correct result of the first computation is in the range 1.27 to 1.30 (exclusive).
  prefs: []
  type: TYPE_NORMAL
- en: About the only reasonable test is to see if the two values are within the error
    tolerance of each other. If so, treat the values as equal (neither is considered
    less than or greater than the other). If the values are not equal within the desired
    error tolerance, you can compare them to see if one value is less than or greater
    than the other. This is known as a *miserly approach*; that is, we try to find
    as few values that are less than or greater than as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'The other possibility is to use an *eager approach*, which attempts to make
    the result of the comparison `true` as often as possible. Given two values to
    compare and an error tolerance, here’s how you’d eagerly compare the two values
    for less than or greater than:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Don’t forget that calculations like (`B +` error) are subject to their own inaccuracies,
    depending on the relative magnitudes of the values `B` and error, and the inaccuracy
    of this calculation may affect the final result of the comparison.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Due to space limitations, this book merely touches on some major problems
    that can occur when you’re using floating-point values and why you can’t treat
    floating-point arithmetic like real arithmetic. For further details, consult a
    good text on numerical analysis or even scientific computing. If you’re going
    to be working with floating-point arithmetic,* in any language, *take some time
    to study the effects of limited-precision arithmetic on your computations.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**4.2 IEEE Floating-Point Formats**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When Intel planned to introduce a floating-point unit (FPU) for its original
    8086 microprocessor, the company was smart enough to realize that the electrical
    engineers and solid-state physicists who design chips probably didn’t have the
    necessary numerical analysis background to design a good floating-point representation.
    So, Intel went out and hired the best numerical analyst it could find to design
    a floating-point format for its 8087 FPU. That person then hired two other experts
    in the field, and the three of them (Kahan, Coonen, and Stone) designed the *KCS
    Floating-Point Standard*. They did such a good job that the IEEE organization
    used this format as the basis for the IEEE Std 754 floating-point format.
  prefs: []
  type: TYPE_NORMAL
- en: 'To handle a wide range of performance and accuracy requirements, Intel actually
    introduced *three* floating-point formats: single precision, double precision,
    and extended precision. The single- and double-precision formats corresponded
    to C’s `float` and `double` types or FORTRAN’s `real` and `double precision` types.
    Extended precision contains 16 extra bits that long chains of computations can
    use as guard bits before rounding down to a double-precision value when storing
    the result.'
  prefs: []
  type: TYPE_NORMAL
- en: '***4.2.1 Single-Precision Floating-Point Format***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The single-precision format uses a 24-bit mantissa and an 8-bit exponent. The
    mantissa represents a value between 1.0 and just less than 2.0\. The HO bit of
    the mantissa is always `1` and represents a value just to the left of the binary
    point. The remaining 23 mantissa bits appear to the right of the binary point
    and represent the value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The mantissa is always greater than or equal to 1 because of the implied `1`
    bit. Even if the other mantissa bits are all `0`, the implied `1` bit always gives
    us the value `1`. Each position to the right of the binary point represents a
    value (`0` or `1`) times a successive negative power of 2, but even if we had
    an almost infinite number of `1` bits after the binary point, they still would
    not add up to 2\. So, the mantissa can represent values in the range 1.0 to just
    less than 2.0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some examples would probably be useful here. Consider the decimal value 1.7997\.
    Here are the steps we could go through to compute the binary mantissa for this
    value:'
  prefs: []
  type: TYPE_NORMAL
- en: Subtract 2⁰ from 1.7997 to produce 0.7997 and `%1.00000000000000000000000`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Subtract 2^(–1) (¹/[2]) from 0.7997 to produce 0.2997 and `%1.10000000000000000000000`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Subtract 2^(–2) (¹/[4]) from 0.2997 to produce 0.0497 and `%1.11000000000000000000000`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Subtract 2^(–5) (¹/[32]) from 0.0497 to produce 0.0185 and `%1.11001000000000000000000`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Subtract 2^(–6) (¹/[64]) from 0.0185 to produce 0.00284 and `%1.11001100000000000000000`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Subtract 2^(–9) (¹/[512]) from 0.00284 to produce 0.000871 and `%1.11001100100000000000000`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Subtract 2^(-10) (¹/[1,024]) from 0.000871 to (approximately) produce 0 and
    `%1.11001100110000000000000`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Although there is an infinite number of values between 1 and 2, we can represent
    only 8 million (2^(23) ) of them because we use a 23-bit mantissa (the 24th bit
    is always `1`), and therefore have only 23 bits of precision.
  prefs: []
  type: TYPE_NORMAL
- en: The mantissa uses a *one’s complement* format rather than two’s complement.
    This means that the 24-bit value of the mantissa is simply an unsigned binary
    number, and the sign bit, in bit position 31, determines whether that value is
    positive or negative. One’s complement has the unusual property that there are
    two representations for `0` (with the sign bit set or clear). Generally, this
    is important only to the person designing the floating-point software or hardware
    system. We’ll assume that the value `0` always has the sign bit clear.
  prefs: []
  type: TYPE_NORMAL
- en: The single-precision floating-point format is shown in [Figure 4-2](ch04.xhtml#ch04fig02).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/04fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-2: Single-precision (32-bit) floating-point format*'
  prefs: []
  type: TYPE_NORMAL
- en: We represent values outside the range of the mantissa by raising 2 to the power
    specified by the exponent and then multiplying the result by the mantissa. The
    exponent is 8 bits and uses an [*excess-127* format](gloss01.xhtml#gloss01_89)
    (sometimes called *bias-127 exponents*). In excess-127 format, the exponent 2⁰
    is represented by the value 127 (`$7f`). To convert an exponent to excess-127
    format, add 127 to the exponent value. For example, the single-precision representation
    for 1.0 is `$3f800000`. The mantissa is 1.0 (including the implied bit) and the
    exponent is 2⁰, encoded as 127 (`$7f`). The representation for 2.0 is `$40000000`,
    with the exponent 2¹ encoded as 128 (`$80`).
  prefs: []
  type: TYPE_NORMAL
- en: The excess-127 exponent makes it easy to compare two floating-point numbers
    for less than or greater than as though they were unsigned integers, as long as
    we handle the sign bit (bit 31) separately. If the signs of the two values are
    not equal, then the positive value (the one with bit 31 set to `0`) is greater
    than the value that has the HO bit set to `1`.^([2](footnotes.xhtml#fn4_2a)) If
    the sign bits are both `0`, we use a straight unsigned binary comparison. If the
    signs are both `1`, we do an unsigned comparison but invert the result (that is,
    we treat less than as greater than and vice versa). On some CPUs, where a 32-bit
    unsigned comparison is much faster than a 32-bit floating-point comparison, it’s
    probably worthwhile to do the comparison using integer arithmetic rather than
    floating-point arithmetic.
  prefs: []
  type: TYPE_NORMAL
- en: A 24-bit mantissa provides approximately 6½ decimal digits of precision (one-half
    digit of precision means that the first six digits can be in the range 0..9, but
    the seventh digit can only be in the range 0 through *x* where *x* < 9 and is
    generally close to 5). With an 8-bit excess-127 exponent, the dynamic range of
    single-precision floating-point numbers is approximately 2^(±128) or about 10^(±38).
  prefs: []
  type: TYPE_NORMAL
- en: Although single-precision floating-point numbers are perfectly suitable for
    many applications, the dynamic range is unsuitable for many financial, scientific,
    and other applications. Furthermore, during long chains of computations, the limited
    accuracy may introduce significant error. For serious calculations, we need a
    floating-point format with more precision.
  prefs: []
  type: TYPE_NORMAL
- en: '***4.2.2 Double-Precision Floating-Point Format***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The double-precision format helps overcome the problems of the single-precision
    floating-point. Using twice the space, the double-precision format has an 11-bit
    excess-1,023 exponent, a 53-bit mantissa (including an implied HO bit of `1`),
    and a sign bit. This provides a dynamic range of about 10^(±308) and 15 to 16+
    digits of precision, which is sufficient for most applications. Double-precision
    floating-point values take the form shown in [Figure 4-3](ch04.xhtml#ch04fig03).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/04fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-3: Double-precision (64-bit) floating-point format*'
  prefs: []
  type: TYPE_NORMAL
- en: '***4.2.3 Extended-Precision Floating-Point Format***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To ensure accuracy during long chains of computations involving double-precision
    floating-point numbers, Intel designed the extended-precision format. The extended-precision
    format uses 80 bits: a 64-bit mantissa, a 15-bit excess-16,383 exponent, and a
    1-bit sign. The mantissa does not have an implied HO bit that is always `1`. The
    format for the extended-precision floating-point value appears in [Figure 4-4](ch04.xhtml#ch04fig04).'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/04fig04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-4: Extended-precision (80-bit) floating-point format*'
  prefs: []
  type: TYPE_NORMAL
- en: On the 80x86 FPUs, all computations use the extended-precision form. Whenever
    you load a single- or double-precision value, the FPU automatically converts it
    to an extended-precision value. Likewise, when you store a single- or double-precision
    value to memory, the FPU automatically rounds the value down to the appropriate
    size before storing it. The extended-precision format guarantees the inclusion
    of a large number of guard bits in 32- and 64-bit computations, which helps ensure
    (but not guarantee) that you’ll get full 32- or 64-bit accuracy in your computations.
    Some error will inevitably creep into the LO bits because the FPUs provide no
    guard bits for 80-bit computations (the FPU uses only 64 mantissa bits during
    80-bit computations). While you can’t assume that you’ll get an accurate 80-bit
    computation, you can usually do better than 64 bits when using the extended-precision
    format.
  prefs: []
  type: TYPE_NORMAL
- en: Non-Intel CPUs that support floating-point arithmetic generally provide only
    the 32-bit and 64-bit formats. Therefore, calculations on those CPUs may produce
    less accurate results than the equivalent string of calculations on the 80x86
    using 80-bit calculations. Also note that modern x86-64 CPUs have additional floating-point
    hardware as part of the SSE extensions; however, those SSE extensions support
    only 64- and 32-bit floating-point calculations.
  prefs: []
  type: TYPE_NORMAL
- en: '***4.2.4 Quad-Precision Floating-Point Format***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The original 80-bit extended-precision floating-point format was a stopgap measure.
    From a “types should be consistent” point of view, the proper extension to the
    64-bit floating-point format should have been a 128-bit floating-point format.
    Alas, when Intel was working on floating-point formats in the late 1970s, a quad-precision
    (128-bit) floating-point format was too expensive to implement in hardware, so
    the 80-bit extended-precision format became the interim compromise. Today, a few
    CPUs (such as IBM’s POWER9 and later-version ARMs) are capable of quad-precision
    floating-point arithmetic.
  prefs: []
  type: TYPE_NORMAL
- en: The IEEE Std 754 quad-precision floating-point format uses a single sign bit,
    a 15-bit excess-16,383 biased exponent, and a 112-bit (with implied 113th bit)
    mantissa (see [Figure 4-5](ch04.xhtml#ch04fig05)). This provides 36 decimal digits
    of precision and exponents in the approximate range 10^(±4932).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/04fig05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-5: Extended-precision (80-bit) floating-point format*'
  prefs: []
  type: TYPE_NORMAL
- en: '**4.3 Normalization and Denormalized Values**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To maintain maximum precision during floating-point computations, most computations
    use *normalized* values. A normalized floating-point value is one whose HO mantissa
    bit contains `1`. A floating-point computation will be more accurate if it involves
    only normalized values because the mantissa has that many fewer bits of precision
    available for computation if several HO bits of the mantissa are all `0`.
  prefs: []
  type: TYPE_NORMAL
- en: You can normalize almost any unnormalized value by shifting the mantissa bits
    to the left and decrementing the exponent until a `1` appears in the mantissa’s
    HO bit.^([3](footnotes.xhtml#fn4_3a)) Remember, the exponent is a binary exponent.
    Each time you increment the exponent, you multiply the floating-point value by
    2\. Likewise, whenever you decrement the exponent, you divide the floating-point
    value by 2\. By the same token, shifting the mantissa to the left one bit position
    multiplies the floating-point value by 2, and shifting it to the right divides
    the floating-point value by 2\. Therefore, shifting the mantissa to the left one
    position *and* decrementing the exponent does not change the value of the floating-point
    number (this is why, as you saw earlier, there are multiple representations for
    certain numbers in the floating-point format).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of an unnormalized value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Shift the mantissa to the left one position and decrement the exponent to normalize
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: There are two important cases in which a floating-point number cannot be normalized.
    First, 0 cannot be normalized because the floating-point representation contains
    all `0` bits in the exponent and mantissa fields. This, however, is not a problem,
    because we can exactly represent 0 with a single `0` bit, and extra bits of precision
    are unnecessary.
  prefs: []
  type: TYPE_NORMAL
- en: We also cannot normalize a floating-point number when we have some HO bits in
    the mantissa that are `0` but the biased exponent^([4](footnotes.xhtml#fn4_4a))
    is also `0` (and we can’t decrement it to normalize the mantissa). Rather than
    prohibiting certain small values whose HO mantissa bits and biased exponent are
    `0` (the most negative exponent possible), the IEEE standard permits special *denormalized*
    values in these cases.^([5](footnotes.xhtml#fn4_5a)) Although the use of denormalized
    values enables IEEE floating-point computations to produce better results than
    if underflow occurred, denormalized values offer fewer bits of precision.
  prefs: []
  type: TYPE_NORMAL
- en: '**4.4 Rounding**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'During a calculation, floating-point arithmetic functions may produce a result
    with greater precision than the floating-point format supports (the *guard bits*
    in the calculation maintain this extra precision). When the calculation is complete
    and the code needs to store the result back into a floating-point variable, something
    must be done about those extra bits of precision. How the system uses guard bits
    to affect the remaining bits is known as *rounding*, and how rounding is done
    can affect the accuracy of the computation. Traditionally, floating-point software
    and hardware use one of four different ways to round values: truncation, rounding
    up, rounding down, or rounding to nearest.'
  prefs: []
  type: TYPE_NORMAL
- en: Truncation is easy, but it generates the least accurate results in a chain of
    computations. Few modern floating-point systems use truncation except as a means
    for converting floating-point values to integers (truncation is the standard conversion
    for coercing a floating-point value to an integer).
  prefs: []
  type: TYPE_NORMAL
- en: Rounding up leaves the value alone if the guard bits are all `0`, but if the
    current mantissa does not exactly fit into the destination bits, then rounding
    up sets the mantissa to the smallest possible larger value in the floating-point
    format. Like truncation, this is not a normal rounding mode. It is, however, useful
    for implementing functions like `ceil()`, which rounds a floating-point value
    to the smallest possible larger integer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rounding down is just like rounding up, except it rounds the result to the
    largest possible smaller value. This may sound like truncation, but there’s a
    subtle difference: truncation always rounds toward 0\. For positive numbers, truncation
    and rounding down do the same thing. For negative values, truncation simply uses
    the existing bits in the mantissa, whereas rounding down will add a `1` bit to
    the LO position if the result was negative. This is also not a normal rounding
    mode, but it’s useful for implementing functions like `floor()`, which rounds
    a floating-point value to the largest possible smaller integer.'
  prefs: []
  type: TYPE_NORMAL
- en: Rounding to nearest is the most intuitive way to process the guard bits. If
    the value of the guard bits is less than half the value of the mantissa’s LO bit,
    then rounding to nearest truncates the result to the largest possible smaller
    value (ignoring the sign). If the guard bits represent some value that is greater
    than half of the value of the LO mantissa bit, then rounding to nearest rounds
    the mantissa to the smallest possible greater value (ignoring the sign). If the
    guard bits represent a value that is exactly half the value of the mantissa’s
    LO bit, then the IEEE floating-point standard says that half the time it should
    round up and half the time it should round down. You do this by rounding the mantissa
    to the value that has a `0` in the LO bit position. That is, if the current mantissa
    already has a `0` in its LO bit, you use the current mantissa; if the current
    mantissa has a `1` in its LO bit, then you add 1 to round it up to the smallest
    possible larger value with a `0` in the LO bit. This scheme, mandated by the IEEE
    floating-point standard, produces the best possible result when loss of precision
    occurs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples of rounding, using 24-bit mantissas, with 4 guard bits
    (that is, these examples round 28-bit numbers to 24-bit numbers using the rounding
    to nearest algorithm):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**4.5 Special Floating-Point Values**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The IEEE floating-point format provides a special encoding for several special
    values. In this section, we’ll look these special values, their purpose and meaning,
    and their representation in the floating-point format.
  prefs: []
  type: TYPE_NORMAL
- en: Under normal circumstances, the exponent bits of a floating-point number do
    not contain all `0`s or all `1`s. An exponent containing all `1` or `0` bits indicates
    a special value.
  prefs: []
  type: TYPE_NORMAL
- en: If the exponent contains all `1`s and the mantissa is nonzero (discounting the
    implied bit), then the HO bit of the mantissa (again discounting the implied bit)
    determines whether the value represents a *quiet not-a-number* (QNaN) or a *signaling
    not-a-number* (SNaN) (see [Table 4-1](ch04.xhtml#ch04tab01)). These not-a-number
    (NaN) results tell the system that some serious miscalculation has taken place
    and that the result of the calculation is completely undefined. QNaNs represent
    *indeterminate* results, while SNaNs specify that an *invalid* operation has taken
    place. Any calculation involving a NaN produces a NaN result, regardless of the
    values of any other operand(s). Note that the sign bit is irrelevant for NaNs.
    The binary representations of NaNs are shown in [Table 4-1](ch04.xhtml#ch04tab01).
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 4-1:** Binary Representations for NaN'
  prefs: []
  type: TYPE_NORMAL
- en: '| **NaN** | **FP format** | **Value** |'
  prefs: []
  type: TYPE_TB
- en: '| SNaN | 32 bits | `%s_11111111_0`xxxx...xx(The value of `s` is irrelevant—at
    least one of the x bits must be nonzero.) |'
  prefs: []
  type: TYPE_TB
- en: '| SNaN | 64 bits | `%s_1111111111_0`xxxxx`...`x(The value of `s` is irrelevant—at
    least one of the x bits must be nonzero.) |'
  prefs: []
  type: TYPE_TB
- en: '| SNaN | 80 bits | `%``s_1111111111_0`xxxxx...x(The value of `s` is irrelevant—at
    least one of the x bits must be nonzero.) |'
  prefs: []
  type: TYPE_TB
- en: '| QNaN | 32 bits | `%s_11111111_1`xxxx...xx(The value of `s` is irrelevant.)
    |'
  prefs: []
  type: TYPE_TB
- en: '| QNaN | 64 bits | `%s_1111111111_1`xxxxx...x(The value of `s` is irrelevant.)
    |'
  prefs: []
  type: TYPE_TB
- en: '| QNaN | 80 bits | `%s_1111111111_1`xxxxx...x(The value of `s` is irrelevant.)
    |'
  prefs: []
  type: TYPE_TB
- en: Two other special values are represented when the exponent contains all 1 bits,
    and the mantissa contains all 0s. In such a case, the sign bit determines whether
    the result is the representation for +*infinity* or –*infinity*. Whenever a calculation
    involves infinity as one of the operands, the result will be one of the (well-defined)
    values found in [Table 4-2](ch04.xhtml#ch04tab02).
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 4-2:** Operations Involving Infinity'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Operation** | **Result** |'
  prefs: []
  type: TYPE_TB
- en: '| `n / ±infinity` | `0` |'
  prefs: []
  type: TYPE_TB
- en: '| `±infinity × ±infinity` | `±infinity` |'
  prefs: []
  type: TYPE_TB
- en: '| `±nonzero / 0` | `±infinity` |'
  prefs: []
  type: TYPE_TB
- en: '| `infinity + infinity` | `infinity` |'
  prefs: []
  type: TYPE_TB
- en: '| `n + infinity` | `infinity` |'
  prefs: []
  type: TYPE_TB
- en: '| `n - infinity` | `-infinity` |'
  prefs: []
  type: TYPE_TB
- en: '| `±0 / ±0` | `NaN` |'
  prefs: []
  type: TYPE_TB
- en: '| `infinity - infinity` | `NaN` |'
  prefs: []
  type: TYPE_TB
- en: '| `±infinity / ±infinity` | `NaN` |'
  prefs: []
  type: TYPE_TB
- en: '| `±infinity × 0` | `NaN` |'
  prefs: []
  type: TYPE_TB
- en: Finally, if the exponent bits are all `0`, the sign bit indicates which of the
    two special values, –0 or +0, the floating-point number represents. Because the
    floating-point format uses a one’s complement notation, there are two separate
    representations for 0\. Note that with respect to comparisons, arithmetic, and
    other operations, +0 is equal to –0.
  prefs: []
  type: TYPE_NORMAL
- en: Using Multiple Representations of Zero
  prefs: []
  type: TYPE_NORMAL
- en: The IEEE floating-point format supports both +0 and –0 (depending on the value
    of the sign bit), which are treated as equivalent by arithmetic calculations and
    comparisons—the sign bit is ignored. Software operating on floating-point values
    that represent 0 can use the sign bit as a flag to indicate different things.
    For example, you could use the sign bit to indicate that the value is exactly
    0 (with the sign bit clear) or to indicate that it is nonzero but too small to
    represent with the current format (with the sign bit set). Intel recommends using
    the sign bit to indicate that 0 was produced via underflow of a negative value
    (with the sign bit set) or underflow of a positive number (with the sign bit clear).
    Presumably, their FPUs set the sign bit according to their recommendations when
    the FPUs produce a `0` result.
  prefs: []
  type: TYPE_NORMAL
- en: '**4.6 Floating-Point Exceptions**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The IEEE floating-point standard defines certain degenerate conditions under
    which the floating-point processor (or software-implemented floating-point code)
    should notify the application software. These exceptional conditions include the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Invalid operation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Division by zero
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Denormalized operand
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Numeric overflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Numeric underflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inexact result
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of these, inexact result is the least serious, because most floating-point calculations
    will produce an inexact result. A denormalized operand also isn’t too serious
    (though this exception indicates that your calculation may be less accurate as
    a result of less available precision). The other exceptions indicate a more serious
    problem, and you shouldn’t ignore them.
  prefs: []
  type: TYPE_NORMAL
- en: How the computer system notifies your application of these exceptions depends
    on the CPU/FPU, operating system, and programming language, so we can’t really
    go into how you might handle these exceptions. Generally, though, you can use
    the exception handling facilities in your programming language to trap these conditions
    as they occur. Note that most computer systems won’t notify you when one of the
    exceptional conditions exists unless you explicitly set up a notification.
  prefs: []
  type: TYPE_NORMAL
- en: '**4.7 Floating-Point Operations**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although most modern CPUs support an FPU that does floating-point arithmetic
    in hardware, it’s worthwhile to develop a set of software floating-point arithmetic
    routines to get a solid feel for what’s involved. Generally, you’d use assembly
    language to write the math functions because speed is a primary design goal for
    a floating-point package. However, because here we’re writing a floating-point
    package simply to get a clearer picture of the process, we’ll opt for code that
    is easy to write, read, and understand.
  prefs: []
  type: TYPE_NORMAL
- en: As it turns out, floating-point addition and subtraction are easy to do in a
    high-level language like C/C++ or Pascal, so we’ll implement these functions in
    these languages. Floating-point multiplication and division are easier to do in
    assembly language than in a high-level language, so we’ll write those routines
    using High-Level Assembly (HLA).
  prefs: []
  type: TYPE_NORMAL
- en: '***4.7.1 Floating-Point Representation***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This section will use the IEEE 32-bit single-precision floating-point format
    (shown earlier in [Figure 4-2](ch04.xhtml#ch04fig02)), which uses a one’s complement
    representation for signed values. This means that the sign bit (bit 31) contains
    a `1` if the number is negative and a `0` if the number is positive. The exponent
    is an 8-bit excess-127 exponent sitting in bits 23 through 30, and the mantissa
    is a 24-bit value with an implied HO bit of `1`. Because of the implied HO bit,
    this format does not support denormalized values.
  prefs: []
  type: TYPE_NORMAL
- en: '***4.7.2 Floating-Point Addition and Subtraction***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Addition and subtraction use essentially the same code. After all, computing
    `X` `-` `Y` is equivalent to computing `X + (-` `Y)`. If we can add a negative
    number to some other value, then we can also perform subtraction by first negating
    some number and then adding it to another value. And because the IEEE floating-point
    format uses the one’s complement representation, negating a value is trivial—we
    just invert the sign bit.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because we’re using the standard IEEE 32-bit single-precision floating-point
    format, we could theoretically get away with using the C/C++ `float` data type
    (assuming the underlying C/C++ compiler also uses this format, as most do on modern
    machines). However, you’ll soon see that when doing floating-point calculations
    in software, we need to manipulate various fields within the floating-point format
    as bit strings and integer values. Therefore, it’s more convenient to use a 32-bit
    `unsigned` integer type to hold the bit representation for our floating-point
    values. To avoid confusing our real values with actual integer values in a program,
    we’ll define the following `real` data type, which assumes that `unsigned` `long`s
    are 32-bit values in your implementation of C/C++ (this section assumes the `uint32_t`
    type achieves that, which is something like `typedef unsigned long uint32_t`),
    and declare all our real variables using this type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: One advantage of using the same floating-point format that C/C++ uses for float
    values is that we can assign floating-point literal constants to our `real` variables,
    and we can perform other floating-point operations such as input and output using
    existing library routines. However, one potential problem is that C/C++ will attempt
    to automatically convert between integer and floating-point formats if we use
    a `real` variable in a floating-point expression (remember, as far as C/C++ is
    concerned, `real` is just an `unsigned long` integer value). This means that we
    need to tell the compiler to treat the bit patterns found in our `real` variables
    as though they were `float` objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple type coercion like `(float)` realVariable won’t work. The C/C++ compiler
    will emit code to convert the integer it believes realVariable contains into the
    equivalent floating-point value. However, we want the C/C++ compiler to treat
    the bit pattern it finds in realVariable as a `float` without doing any conversion.
    The following C/C++ macro is a sneaky way to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This macro requires a single parameter that must be a `real` variable. The result
    is a variable that the compiler believes is a `float` variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have our `float` variable, we’ll develop two C/C++ functions to
    compute floating-point addition and subtraction: `fpadd()` and `fpsub()`. These
    two functions each take three parameters: the left and right operands of the operator
    and a pointer to a destination where these functions will store their result.
    The prototypes for these functions are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The `fpsub()` function negates the right operand and calls the `fpadd()` function.
    Here’s the code for the `fpsub()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `fpadd()` function is where all the real work is done. To make `fpadd()`
    a little easier to understand and maintain, we’ll decompose it into several different
    functions that help with various tasks. In an actual software floating-point library
    routine, you wouldn’t do this decomposition, because the extra subroutine calls
    would be a little slower; however, we’re developing `fpadd()` for educational
    purposes, and besides, if you need high-performance floating-point addition, you’ll
    probably use a hardware FPU rather than a software implementation.
  prefs: []
  type: TYPE_NORMAL
- en: The IEEE floating-point formats are good examples of packed data types. As you’ve
    seen in previous chapters, packed data types are great for reducing storage requirements
    for a data type, but not so much when you need to use the packed fields in actual
    calculations. Therefore, one of the first things our floating-point functions
    will do is unpack the sign, exponent, and mantissa fields from the floating-point
    representation.
  prefs: []
  type: TYPE_NORMAL
- en: The first unpacking function, `extractSign()`, extracts the sign bit (bit 31)
    from our packed floating-point representation and returns the value `0` (for positive
    numbers) or `1` (for negative numbers).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This code could have also extracted the sign bit using this (possibly more
    efficient) expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: However, shifting bit 31 down to bit 0 is, arguably, easier to understand.
  prefs: []
  type: TYPE_NORMAL
- en: The next utility function, `extractExponent()`, unpacks the exponent from bits
    23 through 30 in the packed real format. It does this by shifting the real value
    to the right by 23 bits, masking out the sign bit, and converting the excess-127
    exponent to a two’s complement format (by subtracting 127).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Next is the `extractMantissa()` function, which extracts the mantissa from the
    real value. To extract the mantissa, we must mask out the exponent and sign bits
    and then insert the implied HO bit of `1`. The only catch is that we must return
    `0` if the entire value is `0`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'As you learned earlier, whenever adding or subtracting two values using scientific
    notation (which the IEEE floating-point format uses), you must first adjust the
    two values so that they have the same exponent. For example, to add the two decimal
    (base-10) numbers `1.2345e3` and `8.7654e1`, we must first adjust one or the other
    so that their exponents are the same. We can reduce the exponent of the first
    number by shifting the decimal point to the right. For example, the following
    values are all equivalent to `1.2345e3`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Likewise, we can increase the value of an exponent by shifting the decimal
    point to the left. The following values are all equal to `8.7654e1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: For floating-point addition and subtraction involving binary numbers, we can
    make the binary exponents the same by shifting the mantissa one position to the
    left and decrementing the exponent, or by shifting the mantissa one position to
    the right and incrementing the exponent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Shifting the mantissa bits to the right means that we reduce the precision
    of our number (because the bits wind up going off the LO end of the mantissa).
    To maintain as much accuracy as possible in our calculations, we shouldn’t truncate
    the bits we shift out of the mantissa, but rather round the result to the nearest
    value we can represent with the remaining mantissa bits. These are the IEEE rules
    for rounding, in order:'
  prefs: []
  type: TYPE_NORMAL
- en: Truncate the result if the last bit shifted out was a `0`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Increment the mantissa by 1 if the last bit shifted out was a `1` and there
    was at least one bit set to `1` in all the other bits that were shifted out.^([6](footnotes.xhtml#fn4_6a))
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the last bit we shifted out was a `1`, and all the other bits were `0`s,
    then round the resulting mantissa up by 1 if the mantissa’s LO bit contains a
    `1`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Shifting the mantissa and rounding it is a relatively complex operation, and
    it will occur a couple of times in the floating-point addition code. Therefore,
    it’s another candidate for a utility function. Here’s the C/C++ code that implements
    this function, `shiftAndRound()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The “trick” in this code is that it uses a couple of lookup tables, `masks`
    and `HOmasks`, to extract those bits that the mantissa will use from the shift
    right operation. The `masks` table entries contain `1` bits (set bits) in the
    positions that will be lost during the shift. The `HOmasks` table entries contain
    a single set bit in the position specified by the index into the table; that is,
    the entry at index 0 contains a `1` in bit position 0, the entry at index 1 contains
    a `1` in bit position 1, and so on. This code selects an entry from each of these
    tables based on the number of mantissa bits it needs to shift to the right.
  prefs: []
  type: TYPE_NORMAL
- en: If the original mantissa value, logically ANDed with the appropriate entry in
    `masks`, is greater than the corresponding entry in `HOmasks`, then the `shiftAndRound()`
    function rounds the shifted mantissa to the next greater value. If the ANDed mantissa
    value is equal to the corresponding `HOmasks` element, this code rounds the shifted
    mantissa value according to its LO bit (note that the expression `(*valToShift
    & 1)` produces `1` if the mantissa’s LO bit is `1`, and it produces `0` otherwise).
    Finally, if the ANDed mantissa value is less than the entry from the `HOmasks`
    table, then this code doesn’t have to do anything because the mantissa is already
    rounded down.
  prefs: []
  type: TYPE_NORMAL
- en: Once we’ve adjusted one of the values so that the exponents of both operands
    are the same, the next step in the addition algorithm is to compare the signs
    of the values. If the signs of the two operands are the same, we add their mantissas
    (using a standard integer add operation). If the signs differ, we have to subtract,
    rather than add, the mantissas. Because floating-point values use one’s complement
    representation, and standard integer arithmetic uses two’s complement, we cannot
    simply subtract the negative value from the positive value. Instead, we have to
    subtract the smaller value from the larger value and determine the sign of the
    result based on the signs and magnitudes of the original operands. [Table 4-3](ch04.xhtml#ch04tab03)
    describes how to accomplish this.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 4-3:** Dealing with Operands That Have Different Signs'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Left sign** | **Right sign** | **Left mantissa > right mantissa?** | **Compute
    mantissa as** | **Result****sign is** |'
  prefs: []
  type: TYPE_TB
- en: '| – | + | Yes | LeftMantissa `-` RightMantissa | – |'
  prefs: []
  type: TYPE_TB
- en: '| + | – | Yes | LeftMantissa `-` RightMantissa | + |'
  prefs: []
  type: TYPE_TB
- en: '| – | + | No | RightMantissa `-` LeftMantissa | + |'
  prefs: []
  type: TYPE_TB
- en: '| + | – | No | RightMantissa `-` LeftMantissa | – |'
  prefs: []
  type: TYPE_TB
- en: 'Whenever you’re adding or subtracting two 24-bit numbers, it’s possible to
    produce a result that requires 25 bits (in fact, this is common when you’re dealing
    with normalized values). Immediately after an addition or subtraction, the floating-point
    code has to check the result to see if overflow has occurred. If so, it needs
    to shift the mantissa right by 1 bit, round the result, and then increment the
    exponent. After completing this step, all that remains is to pack the resulting
    sign, exponent, and mantissa fields into the 32-bit IEEE floating-point format.
    The following `packFP()` function is responsible for packing the `sign`, `exponent`,
    and `mantissa` fields into the 32-bit floating-point format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Note that this function works for normalized values, denormalized values, and
    zero, but does not work for NaNs and infinities.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the utility routines out of the way, take a look at the `fpadd()` function,
    which adds two floating-point values, producing a 32-bit real result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'To conclude this discussion of the software implementation of the `fpadd()`
    and `fsub()` functions, here’s a C `main()` function demonstrating their use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the output produced by compiling with Microsoft Visual C++ (and defining
    `uint32_t` as an `unsigned long`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '***4.7.3 Floating-Point Multiplication and Division***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Most software floating-point libraries are actually written in hand-optimized
    assembly language, not in a high-level language (HLL). As the previous section
    shows, it’s possible to write floating-point routines in an HLL and, particularly
    in the case of single-precision floating-point addition and subtraction, you could
    write the code efficiently. Given the right library routines, you could also write
    the floating-point multiplication and division routines in an HLL. However, because
    their implementation is actually easier in assembly language, this section presents
    an HLA implementation of the single-precision floating-point multiplication and
    division algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'The HLA code in this section implements two functions, `fpmul()` and `fpdiv()`,
    that have the following prototypes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Beyond the fact that this code is written in assembly language rather than C,
    it differs in two main ways from the code in the previous section. First, it uses
    the built-in `real32` data type rather than creating a new data type for the real
    values, because we can easily coerce any 32-bit memory object to `real32` or `dword`
    in assembly language. Second, these prototypes support only two parameters; there
    is no destination parameter. These functions simply return the `real32` result
    in the EAX register.^([7](footnotes.xhtml#fn4_7a))
  prefs: []
  type: TYPE_NORMAL
- en: '**4.7.3.1 Floating-Point Multiplication**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Whenever you multiply two values in scientific notation, you compute the result
    sign, exponent, and mantissa as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The result sign is the exclusive-OR of the operand signs. That is, the result
    is positive if both operand signs were the same, and the result sign is negative
    if the operand signs were different.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The result exponent is the sum of the operands’ exponents.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The result mantissa is the integer (fixed-point) product of the two operand
    mantissas.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are a few additional rules that affect the floating-point multiplication
    algorithm that are a direct result of the IEEE floating-point format:'
  prefs: []
  type: TYPE_NORMAL
- en: If either, or both, of the operands are `0`, the result is `0` (this is a special
    case because the representation for `0` is special).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If either operand is infinity, the result is infinity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If either operand is a NaN, the result is that same NaN.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `fpmul()` procedure begins by checking if either of the operands is `0`.
    If so, the function immediately returns `0.0` to the caller. Next, the `fpmul()`
    code checks for NaN or infinity values in the `left` and `right` operands. If
    it finds one of these values, it returns that same value to the caller.
  prefs: []
  type: TYPE_NORMAL
- en: 'If both of the `fpmul()` operands are reasonable floating-point values, then
    the `fpmul()` code extracts the sign, exponent, and mantissa fields of the packed
    floating-point value. Actually, *extract* isn’t the correct term here; *isolate*
    is a better description. Here’s the code that isolates the sign bits of the two
    operands and computes the result sign:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: This code exclusive-ORs the two operands and then masks out bits 0 through 30,
    leaving only the result sign value in bit 31 of the EBX register. This procedure
    doesn’t bother moving the sign bit down to bit 0 (as you’d normally do when unpacking
    data), because it would just have to move this bit back to bit 31 when it repacks
    the floating-point value later.
  prefs: []
  type: TYPE_NORMAL
- en: 'To process the exponent, `fpmul()` isolates bits 23 through 30 and operates
    on the exponent in place. When multiplying two values using scientific notation,
    you must add the values of the exponents together. However, you must subtract
    127 from the exponent’s sum, since adding excess-127 exponents ends up adding
    the bias twice. The following code isolates the exponent bits, adjusts for the
    extra bias, and adds the exponents together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: First, notice that this code subtracts 126 rather than 127\. The reason is that
    later we’ll need to double the result of the multiplication of the mantissas.
    Subtracting 126 rather than 127 does this multiplication by 2 implicitly (saving
    an instruction later on).
  prefs: []
  type: TYPE_NORMAL
- en: If the sum of the exponents with `add(eax, ecx)` in the preceding code is too
    large to fit into 8 bits, there will be a carry out of bit 30 into bit 31 of ECX,
    which will set the 80x86 overflow flag. If overflow occurs on a multiplication,
    our code will return `infinity` as the result.
  prefs: []
  type: TYPE_NORMAL
- en: If overflow does not occur, then the `fpmul()` procedure needs to set the implied
    HO bit of the two mantissa values. The following code handles this chore, strips
    out all the exponent and sign bits from the mantissas, and left-justifies the
    mantissa bits up against bit position 31 in EAX and EDX.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the mantissas are shifted to bit 31 in EAX and EDX, we multiply using
    the 80x86 `mul()` instruction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: This instruction computes the 64-bit product of EAX and EDX, leaving the result
    in EDX:EAX (the HO double word is in EDX, and the LO double word is in EAX). Because
    the product of any two *n*-bit integers could require as many as 2×*n* bits, the
    `mul()` instruction computes EDX:EAX = EAX×EDX. Left-justifying the mantissas
    in EAX and EDX before doing the multiplication ensures the mantissa of the product
    winds up in bits 7 through 30 of EDX. We actually need them in bit positions 8
    through 31 of EDX—that’s why earlier this code subtracted only 126, rather than
    127, when adjusting for the excess-127 value (this multiplies the result by 2,
    which is equivalent to shifting the bits left one position). As these numbers
    were normalized prior to the multiplication, bit 30 of EDX will contain a `1`
    after the multiplication unless the result is `0`. The 32-bit IEEE real format
    does not support denormalized values, so we don’t have to worry about this case
    when using 32-bit floating-point values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because the mantissas are 24 bits each, the product of the mantissas could
    have as many as 48 significant bits. Our result mantissa can hold only 24 bits,
    so we need to round the value to produce a 24-bit result (using the IEEE rounding
    algorithm — see “Rounding” on [page 71](#page_71)). Here’s the code that rounds
    the value in EDX to 24 significant bits (in positions 8..31):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The number may need to be renormalized after rounding. If the mantissa contains
    all `1` bits and needs to be rounded up, this will produce an overflow out of
    the HO bit of the mantissa. The `rcr()` and `inc()` instructions at the end of
    this code sequence put the overflow bit back into the mantissa if overflow occurs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The only thing left to do after this is pack the destination sign, exponent,
    and mantissa into the 32-bit EAX register. The following code does this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The only tricky thing in this code is the use of the `lea()` (load effective
    address) instruction to compute the sum of EDX (the mantissa) and ECX (the exponent)
    and move the result to EAX all with a single instruction.
  prefs: []
  type: TYPE_NORMAL
- en: '**4.7.3.2 Floating-Point Division**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Floating-point division is a little bit more involved than multiplication because
    the IEEE floating-point standard says many things about degenerate conditions
    that can occur during division. We’re not going to discuss all the code that handles
    those conditions here. Instead, see the discussion of the conditions for `fpmul()`
    earlier, and check out the complete code listing for `fdiv()` later in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assuming we have reasonable numbers to divide, the division algorithm first
    computes the result sign using the same algorithm (and code) as for multiplying.
    When dividing two values using scientific notation, we have to subtract their
    exponents. In contrast to the multiplication algorithm, here it’s more convenient
    to truly unpack the exponents for the two division operands and convert them from
    excess-127 to two’s complement form. Here’s the code that does this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The 80x86 `div()` instruction absolutely, positively requires the quotient
    to fit into 32 bits. If this condition is not true, the CPU may abort the operation
    with a divide exception. As long as the HO bit of the divisor contains a `1` and
    the HO 2 bits of the dividend contain `%01`, we won’t get a division error. Here’s
    the code that prepares the operands prior to the division operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to actually do the division. As noted earlier, in order to
    prevent a division error, we have to shift the dividend 1 bit to the right (to
    set the HO 2 bits to `%01`), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Once the `div()` instruction executes, the quotient is sitting in the HO 24
    bits of EAX, and the remainder is in AL:EDX. We now need to normalize and round
    the result. Rounding is a little easier because AL:EDX contains the remainder
    after the division; if we need to round down, it will contain a value less than
    `$80:0000_0000` (that is, the 80x86 AL register contains `$80` and EDX contains
    `0`); if we need to round up, it will contain a value greater than `$80:0000_`;
    and if we need to round to the nearest value, it will contain exactly `$80:0000_0000`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the code that does this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The last step in `fpdiv` is to add the bias back into the exponent (and verify
    that overflow doesn’t occur) and then pack the quotient’s sign, exponent, and
    mantissa fields into the 32-bit floating-point format. Here’s the code that does
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Whew! This has been a lot of code. However, going through all of it just to
    see how floating-point operations work has hopefully given you an appreciation
    of exactly what an FPU does for you.
  prefs: []
  type: TYPE_NORMAL
- en: '**4.8 For More Information**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Hyde, Randall. *The Art of Assembly Language*. 2nd ed. San Francisco: No Starch
    Press, 2010.'
  prefs: []
  type: TYPE_NORMAL
- en: '———. “Webster: The Place on the Internet to Learn Assembly.” *[http://plantation-productions.com/Webster/index.html](http://plantation-productions.com/Webster/index.html)*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Knuth, Donald E. *The Art of Computer Programming, Volume 2: Seminumerical
    Algorithms*. 3rd ed. Boston: Addison-Wesley, 1998.'
  prefs: []
  type: TYPE_NORMAL
