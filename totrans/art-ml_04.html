<html><head></head><body>
<div id="sbo-rt-content" class="calibre1"><h2 class="h1" id="ch03"><span epub:type="pagebreak" id="page_51" class="calibre2"/><strong class="calibre3"><span class="big">3</span><br class="calibre18"/>BIAS, VARIANCE, OVERFITTING, AND CROSS-VALIDATION</strong></h2>
<div class="imagec"><img alt="Image" src="../images/common.jpg" class="calibre14"/></div>
<p class="noindent">We now look in detail at a vital topic touched on in <a href="ch01.xhtml#ch01lev7" class="calibre12">Sections 1.7</a>, <a href="ch01.xhtml#ch01lev12sec4" class="calibre12">1.12.4</a>, and <a href="ch02.xhtml#ch02lev2sec5" class="calibre12">2.2.5</a>—overfitting. In this chapter, we’ll explain what bias and variance really mean in ML contexts and how they affect overfitting. We’ll then cover a popular approach to avoiding overfitting known as <em class="calibre13">cross-validation</em>.</p>
<p class="indent">The problem of overfitting exemplifies the point made in the title of this book: ML is an art, not a science. There is no formulaic solution to various problems, especially overfitting. Professor Yaser Abu-Mostafa of Caltech, a prominent ML figure, once summed it up: “The ability to avoid overfitting is what separates professionals from amateurs in ML.”<sup class="calibre11"><a id="ch3fn1b" class="calibre12"/><a href="footnote.xhtml#ch3fn1" class="calibre12">1</a></sup> And my Google query on “overfitting” yielded 6,560,000 results!</p>
<p class="indent">Don’t be intimidated. The professor is correct, but avoiding overfitting is not difficult, provided one has a good understanding of bias and variance. One uses this understanding and gains skill through experience.</p>
<h3 class="h2" id="ch03lev1"><span epub:type="pagebreak" id="page_52" class="calibre2"/>3.1 Overfitting and Underfitting</h3>
<p class="noindent">So, what is all the fuss about overfitting? We’ve given a hint here and there in earlier chapters. Now let’s go into the topic in depth.</p>
<p class="indent">Recall our discussion in <a href="ch01.xhtml#ch01lev7" class="calibre12">Section 1.7</a> of the Bias-Variance Trade-off involved in choosing hyperparameter values, specifically the value <em class="calibre13">k</em> in k-NN. Once again, let’s take the bike sharing data (<a href="ch01.xhtml#ch01lev1" class="calibre12">Section 1.1</a>) as our motivating example. As before, say we wish to predict ridership, such as for a day in which the temperature is 28 degrees. We will look at the days in our data with temperatures nearest to 28. Our predicted value will be the average ridership among those days.</p>
<p class="indent">Say we take <em class="calibre13">k</em> = 5. Even those outside the technology world might intuitively feel that a value of 5 for <em class="calibre13">k</em> is “too small a sample.” There is too much variability in ridership from one set of 5 days to another, even if their temperatures are near 28. If we had a sample from a different set of 731 days than the one we have, we’d have a different set of 5 closest days to 28, with a different average ridership. With <em class="calibre13">k</em> = 50, a lot of high and low ridership values would largely cancel out during the averaging process, but not so with just <em class="calibre13">k</em> = 5. This argues for choosing a larger value than 5 for <em class="calibre13">k</em>. This is a <em class="calibre13">variance</em> issue: choosing too small a value for <em class="calibre13">k</em> brings us too much sampling variability.</p>
<p class="indent">On the other hand, if we use, say, the <em class="calibre13">k</em> = 25 days with temperatures closest to 28, we risk getting some days whose temperatures are rather far from 28. Say, for instance, the 25th-closest day had a temperature of 35. People do not want to ride bikes in such hot weather. If we include too many hot days in our prediction for the 28-degree day, we will have a tendency to underpredict the true ridership. In such a situation, <em class="calibre13">k</em> = 25 may be too large. That’s a <em class="calibre13">bias</em> issue: choosing too large a value of <em class="calibre13">k</em> may induce a systemic tendency to underpredict or overpredict.</p>
<div class="note">
<p class="notet"><strong class="calibre3"><span class="notes">NOTE</span></strong></p>
<p class="notep"><em class="calibre13">We’ll repeatedly mention variance and bias in this chapter and in later ones. It’s important to keep in mind what quantity’s variance and bias is under discussion: predicted values. Say we are predicting ridership for a 28-degree day. The larger the value of</em> k <em class="calibre13">we use, the lesser the variability in our predicted value, but the greater the bias of that value.</em></p>
</div>
<p class="indent">Variance and bias are at odds with each other. For a given dataset, we can reduce one only at the expense of the other. This trade-off is central to choosing the values of hyperparameters, as well as choosing which features to use. Using too small a <em class="calibre13">k</em>—trying to reduce bias below what is possible on this data—is called <em class="calibre13">overfitting</em>. Using too large a <em class="calibre13">k</em>—an overly conservative one—is called <em class="calibre13">underfitting</em>. We hope to choose our hyperparameter values in the “sweet spot,” neither overfitting nor underfitting.</p>
<h4 class="h3" id="ch03lev1sec1"><span epub:type="pagebreak" id="page_53" class="calibre2"/><em class="calibre22"><strong class="calibre3">3.1.1 Intuition Regarding the Number of Features and Overfitting</strong></em></h4>
<p class="noindent">A similar statement holds for features: using too large a value for <em class="calibre13">p</em> (that is, the number of features) results in overfitting, while using too small a value gives us underfitting. Here is the intuition behind this.</p>
<p class="indent">Recall <code>mlb</code>, the dataset on Major League Baseball players (in <a href="ch01.xhtml#ch01lev8" class="calibre12">Section 1.8</a>). We might predict weight from height and age. But what if we were to omit height from our feature set? That would induce a bias. Roughly speaking, we’d be tacitly assuming everyone is of middling height, which would result in our tending to overpredict the weight of shorter players while underpredicting that of the taller ones.</p>
<p class="indent">On the other hand, it turns out that the more predictors we use (in general, not just for this data), the higher the variance of our predicted values. To see this, say we are conducting a marketing study, predicting purchases of winter parkas, and wish to account for geography of customers. There are about 42,000 ZIP codes (US postal codes). Say we use ZIP code as one of our features in predicting purchases. We would then have 42,000 dummy variables and would have other features such as age, gender, and income, or <em class="calibre13">p</em> &gt; 42000. If our data consists of, say, 100,000 customers, we would have on average only 2 or 3 data points per ZIP code. Again, even nontechies would point out that this is far too small a sample, causing variance to rise. In other words, having too large a value of <em class="calibre13">p</em> increases variance. Once again, we see a tension between variance and bias.</p>
<h4 class="h3" id="ch03lev1sec2"><em class="calibre22"><strong class="calibre3">3.1.2 Relation to Overall Dataset Size</strong></em></h4>
<p class="noindent">But there’s more. In choosing a “good” value of <em class="calibre13">k</em> or <em class="calibre13">p</em>, we need to take into consideration <em class="calibre13">n</em>, the number of data points we have. Recall that in the bike sharing example, we had <em class="calibre13">n</em> = 731 (that is, only 731 days’ worth of data). Is that large enough to make good predictions? Why should that number matter? Actually, it relates directly to the Bias-Variance Trade-off. Here’s why.</p>
<p class="indent">In our bike sharing example above, we worried that with <em class="calibre13">k</em> = 25 nearest neighbors, we might have some days among those 25 whose temperatures are rather far from 28. But if we had, say, 2,000 days instead of 731, the 25th-closest might still be pretty close to 28. In other words:</p>
<p class="block">The larger <em class="calibre13">n</em> is, the larger we can make <em class="calibre13">k</em> while still avoiding overly large bias.</p>
<p class="noindent">Similarly, consider the ZIP code issue mentioned above. With 100,000 customers, we would have on average only 2 or 3 data points per ZIP code. But what if our dataset consisted of 50 million customers? Then it may be useful to include the dummies for ZIP codes, as we may have a sufficient number of customers from most ZIP codes. Remember, <em class="calibre13">p</em> denotes the number of features, and this counts each dummy variable separately. Thus, inclusion of ZIP codes in our feature set would increase <em class="calibre13">p</em> by about 42,000.</p>
<p class="indent"><span epub:type="pagebreak" id="page_54"/>In other words:</p>
<p class="block">The larger <em class="calibre13">n</em> is, the larger the value we can use for <em class="calibre13">p</em>—that is, the more features we can use while still avoiding overly large variance.</p>
<h4 class="h3" id="ch03lev1sec3"><em class="calibre22"><strong class="calibre3">3.1.3 Well Then, What Are the Best Values of k and p?</strong></em></h4>
<p class="noindent">Mind you, this still doesn’t tell us how to set a good “Goldilocks” value of <em class="calibre13">k</em>—not too small and not too large. The same holds for choosing <em class="calibre13">p</em> (that is, choosing the number of features to use); in fact, it’s an even more challenging problem, as it is a question of not only <em class="calibre13">how many</em> features to use but also <em class="calibre13">which ones</em>.</p>
<p class="indent">As we have stated so many times:</p>
<p class="block">This is a fact of life in machine learning. For most issues, there are no neat, magic-formula answers. Again, ML is an art, not a science. However, holdout methods are used in practice, and they generally work pretty well, especially as the analyst gains experience.</p>
<p class="noindent">We’ll present holdout methods in full detail later in this chapter.</p>
<p class="indent">Also, a rough rule of thumb, suggested by some mathematical theory, is to follow this limitation:</p>
<div class="imagec" id="ch03equ01"><img alt="Image" src="../images/ch03equ01.jpg" class="calibre24"/></div>
<p class="noindent">That is, the number of nearest neighbors should be less than the square root of the number of data points.</p>
<p class="indent">What about choosing <em class="calibre13">p</em>? As noted, a feature set is not “large” or “small” on its own. Instead, its size <em class="calibre13">p</em> must be viewed relative to the number of data points <em class="calibre13">n</em>. Overfitting can arise by using too many features for a given dataset size. In classical statistics, a rough—though in my experience, conservative— rule of thumb has been to follow another “square root of <em class="calibre13">n</em>” limitation:</p>
<div class="imagec" id="ch03equ02"><img alt="Image" src="../images/ch03equ02.jpg" class="calibre25"/></div>
<p class="noindent">That is, the number of features should be less than the square root of the number of data points. Under this criterion, if our data frame has, say, 1,000 rows, it can support about 30 features. This is not a bad rough guide and is supported by theoretical results for parametric models.</p>
<p class="indent">However, in modern statistics and ML, it is now common to have—or at least start with—a value of <em class="calibre13">p</em> much larger than <em class="calibre13">n</em>. We will see this with certain methods used later in the book. We’ll stick with <img alt="Image" class="middle3" src="../images/prootn.jpg"/> as a reasonable starting point. If our data satisfies that rule, we can feel safe. But if <em class="calibre13">p</em> is larger, we should not automatically consider it to be overly large.<span epub:type="pagebreak" id="page_55"/></p>
<h3 class="h2" id="ch03lev2">3.2 Cross-Validation</h3>
<p class="noindent">The most common approach to choosing the value of hyperparameters or choosing feature sets is to minimize MAPE (numeric- <em class="calibre13">Y</em> case) or the overall misclassification error (OME, classification case). For k-NN and a numeric- <em class="calibre13">Y</em> setting, we may find MAPE for each of a range of candidate values of <em class="calibre13">k</em> and then choose the one producing minimal MAPE.</p>
<p class="indent">In deciding what value of <em class="calibre13">k</em> to use, we need to assess the predictive ability of various values of that hyperparameter. But in doing so, we need to make sure we are using a “fresh” dataset to predict. This motivates splitting the data into two sets: a training set and a holdout, or test, set.</p>
<p class="indent">However, holdout sets are chosen randomly. This induces additional randomness, on top of the sampling variation we already have. We saw an example of this in <a href="ch01.xhtml#ch01lev12sec3" class="calibre12">Section 1.12.3</a>. So, in choosing <em class="calibre13">k</em> in k-NN, for instance, one holdout set may indicate <em class="calibre13">k</em> = 5 as best, while another would favor <em class="calibre13">k</em> = 12. To be thorough, we should not rely on a single holdout set. This leads to the method of <em class="calibre13">K-fold cross-validation</em>, where we generate many holdout sets, averaging MAPE, OME, or other criterion over all those sets. Note that <em class="calibre13">k</em>, the number of neighbors, is different from <em class="calibre13">K</em>, the number of <em class="calibre13">folds</em>, or possible holdout sets.</p>
<h4 class="h3" id="ch03lev2sec1"><em class="calibre22"><strong class="calibre3">3.2.1 K-Fold Cross-Validation</strong></em></h4>
<p class="noindent">To see how K-fold cross-validation works, consider the “leaving one out” method, in which we set a holdout set size of 1. Say we wish to evaluate the predictive ability of <em class="calibre13">k</em> = 5. For each of our <em class="calibre13">n</em> data points, we would take the holdout set to be that point and take the remaining <em class="calibre13">n</em> − 1 points as our training set; we then predict the holdout point. This gives us <em class="calibre13">n</em> predictions, and we calculate MAPE as the average absolute prediction error among those <em class="calibre13">n</em> predictions. In other words, we would proceed as in the following pseudocode for data frame <code>d</code>:</p>
<pre class="calibre16">set sumMape = 0
for i = 1,2,...,n
   set training set = d[-i,]
   set test set = d[i,]
   apply k-NN to training set, with k = 5
   predict the test set
   sumMape = sumMape + abs(predicted Y - actual Y)
MAPE = sumMape / n</pre>
<p class="noindent">We’d call this <em class="calibre13">n</em>-fold cross-validation. Alternatively, we could take our holdout sets to have size 2, say, by partitioning the set 1,2, . . . ,<em class="calibre13">n</em> into non-overlapping adjacent pairs. Now there are <em class="calibre13">n</em>/2 possible holdout sets (folds). For each fold, we apply k-NN to the remaining data and then predict the data in that fold. MAPE is then the average over the <em class="calibre13">n</em>/2 folds.</p>
<p class="indent"><span epub:type="pagebreak" id="page_56"/>One might expect that <em class="calibre13">K</em> = <em class="calibre13">n</em> is best, since then MAPE will be based on the most trials. On the other hand, each trial will be based on predicting just 1 data point, which is presumably less accurate. There also may be computational and theoretical issues that we won’t go into here. How should we then choose <em class="calibre13">K</em>?</p>
<p class="indent">Note that <em class="calibre13">K</em> is not a hyperparameter, as it is not a trait of k-NN. It is simply a matter of how to estimate MAPE reliably. But yes, it’s one more thing to think about. Many analysts recommend using a value of 5 or 10.</p>
<p class="indent">Another approach is as follows, say, for holdout sets of size 2. We simply choose many random holdout sets, as many as we have time for, as in the following pseudocode:</p>
<pre class="calibre16">set sumMape = 0
for m = 1,2,...,r
   set test set = a random pair of data points (not necessarily adjacent)
   set training set = the remaining n-2 points
   apply k-NN to training set, with a candidate k
   predict the test set
   sumMape = sumMape + mean(abs(predicted Ys - actual Ys))
MAPE = sumMape / r</pre>
<p class="indent">Here, <em class="calibre13">r</em> is the number of holdout sets. The larger value we choose for <em class="calibre13">r</em>, the more accurate MAPE will be. It just depends on how much computation time we wish to expend. (The plurals, such as <code>predicted Ys</code>, allude to the fact that any holdout set has two Y values to predict.)</p>
<h4 class="h3" id="ch03lev2sec2"><em class="calibre22"><strong class="calibre3">3.2.2 Using the replicMeans() Function</strong></em></h4>
<p class="noindent">We can use the <code>regtools</code> function <code>replicMeans()</code> to implement K-means cross-validation. The function name is short for “replicate an action and then take the mean of the results.”</p>
<p class="indent">For instance, say we have some data frame <code>d</code> in which we are predicting a column <code>y</code>. Consider the effect of the following call:</p>
<pre class="calibre16">cmd &lt;- "qeKNN(d,'y')$testAcc"
crossvalOutput &lt;- replicMeans(10,cmd)</pre>
<p class="noindent">This says to run <code>cmd</code> 10 times and return the mean of the result. Since the command is to run <code>qeKNN()</code>, 10 runs will use 10 different holdout sets, yielding 10 different values of <code>testAcc</code>. The end result will be that the function returns the average of those 10 values, which is exactly what we want.</p>
<h4 class="h3" id="ch03lev2sec3"><em class="calibre22"><strong class="calibre3">3.2.3 Example: Programmer and Engineer Data</strong></em></h4>
<p class="noindent">Here we will introduce a new dataset, <code>pef</code>, to be used at several points in the book, and illustrate cross-validation on this data.</p>
<p class="indent">The <code>pef</code> dataset is included in the <code>regtools</code> package, which in turn is included in <code>qeML</code>. It is drawn from the 2000 US census, showing data on programmers and engineers. Here is a glimpse:<span epub:type="pagebreak" id="page_57"/></p>
<pre class="calibre16">&gt; <span class="codestrong">data(pef)</span>
&gt; <span class="codestrong">head(pef)</span>
       age     educ occ sex wageinc wkswrkd
1 50.30082 zzzOther 102   2   75000      52
2 41.10139 zzzOther 101   1   12300      20
3 24.67374 zzzOther 102   2   15400      52
4 50.19951 zzzOther 100   1       0      52
5 51.18112 zzzOther 100   2     160       1
6 57.70413 zzzOther 100   1       0       0
&gt; <span class="codestrong">dim(pef)</span>
[1] 20090     6</pre>
<p class="noindent">So, data on a bit more than 20,000 workers is stored here.</p>
<p class="indent">The education variable here needs some explanation. The census has codes for education of various levels, down to even none at all. But for this dataset, there won’t be many (if any) workers with, say, just a sixth-grade education. For that reason, the <code>educ</code> column here has been simplified to just three levels: master’s (code 14), PhD (16), and “other” (coded as <code>zzzOther</code> by the software, <code>regtools::toSubFactor()</code>). Most of the “other” workers have a bachelor’s degree, but even those with less have been lumped into this level.</p>
<p class="indent">Why do this? The <code>qe*</code>-series functions convert any feature that is an R factor to dummy variables, and for some such functions, the output is displayed in terms of the dummies. So, consolidation as above compactifies output. Even running <code>head()</code> would give very wide output if all education levels were included and dummy variables were displayed.</p>
<p class="indent">Second, simplification of this nature may, in general, be needed to avoid overfitting—remember, each dummy variable counts separately in the feature count <em class="calibre13">p</em>—even though in this dataset we are well within the “<img alt="Images" src="../images/prootn.jpg" class="calibre26"/>” rule of thumb.</p>
<p class="indent">For detailed information on this dataset, such as the various occupation codes, type <span class="codestrong1">?pef</span> at the R prompt.</p>
<h5 class="h4" id="ch03lev2sec3sec1">3.2.3.1 Improved Estimation of MAPE</h5>
<p class="noindent">Suppose we wish to predict <code>wageinc</code>, wage income, in this <code>pef</code> dataset. Let’s take a first cut at it:</p>
<pre class="calibre16">&gt; <span class="codestrong">z &lt;- qeKNN(pef,'wageinc',k=10)</span>
holdout set has  1000 rows
&gt; <span class="codestrong">z$testAcc</span>
[1] 25296.21</pre>
<p class="noindent">On average, our predictions are off by about $25,300. This is a rather large number, but as emphasized in <a href="ch02.xhtml#ch02lev4" class="calibre12">Section 2.4</a>, we must always gauge prediction accuracy of a feature set compared to predicting <em class="calibre13">without</em> the features:</p>
<pre class="calibre16">&gt; <span class="codestrong">z$baseAcc</span>
[1] 32081.36</pre>
<p class="noindent"><span epub:type="pagebreak" id="page_58"/>So, just predicting everyone to have the overall mean income would give us a much larger MAPE.</p>
<p class="indent">At any rate, our point here concerns not this particular dataset but the general accuracy of MAPE if the latter is based on just a single holdout set. We really need to look at multiple holdout sets using cross-validation. Let’s do that using <code>replicMeans()</code>.</p>
<pre class="calibre16">&gt; <span class="codestrong">cmd &lt;- "qeKNN(pef,'wageinc')$testAcc"</span>
&gt; <span class="codestrong">replicMeans(10,cmd)</span>
holdout set has  1000 rows
holdout set has  1000 rows
holdout set has  1000 rows
holdout set has  1000 rows
holdout set has  1000 rows
holdout set has  1000 rows
holdout set has  1000 rows
holdout set has  1000 rows
holdout set has  1000 rows
holdout set has  1000 rows
[1] 25633.51
attr(,"stderr")
[1] 412.1483</pre>
<p class="indent">So, the indicated <code>qeKNN()</code> call was run 10 times, yielding 10 holdout sets, having an average value of about $25,633 for accuracy on the test set. This is somewhat larger than the $25,296 figure we had obtained earlier based on just one holdout set. Thus, we should treat this new figure as more reliable.</p>
<p class="indent">That $412 number is the <em class="calibre13">standard error</em>. Multiplying it by 1.96 gives us the margin of error. If we feel that is too large, we can call <code>replicMeans()</code> with, say, 100 replications (that is, 100 holdout sets).</p>
<p class="indent">We could then try other values of <em class="calibre13">k</em>, running <code>replicMeans()</code> for each one as above and then finally choosing the value that gives the best MAPE or OME. If we have more than a few such values, it would be easier to use the <code>qeML</code> function <code>qeFT()</code>, which will be presented in <a href="ch07.xhtml" class="calibre12">Chapter 7</a>.</p>
<h4 class="h3" id="ch03lev2sec4"><em class="calibre22"><strong class="calibre3">3.2.4 Triple Cross-Validation</strong></em></h4>
<p class="noindent">Suppose we split our data into training and test sets, and then fit many different combinations of hyperparameters, choosing the combination that does best on the test set. Again we run into the problem of potential p-hacking, meaning that the accuracy rates reported in the test set may be overly optimistic.</p>
<p class="indent">One common solution is to partition the data into three subsets rather than two, with the intermediate one being termed the <em class="calibre13">validation set</em>. We fit the various combinations of hyperparameters to the training set and evaluate them on the validation set. After choosing the best combination, we then evaluate (only) that combination on the test set to obtain an accuracy estimate untainted by p-hacking.<span epub:type="pagebreak" id="page_59"/></p>
<h3 class="h2" id="ch03lev3">3.3 Conclusions</h3>
<p class="noindent">In summary, the main concepts in this brief but vital chapter are:</p>
<ul class="calibre15">
<li class="noindent3">In choosing a hyperparameter such as k-NN’s <em class="calibre13">k</em>, and in choosing a feature set, variance and bias are at odds with each other. For a fixed dataset, a small <em class="calibre13">k</em> or large <em class="calibre13">p</em> increases variance while reducing bias, and vice versa.</li>
<li class="noindent3">With a larger <em class="calibre13">n</em>, we can afford to take a larger value of <em class="calibre13">k</em> or <em class="calibre13">p</em>.</li>
<li class="noindent3">Unfortunately, there is no hard-and-fast formula for the “Goldilocks” values of <em class="calibre13">k</em> and <em class="calibre13">p</em>. But there are some very rough rules of thumb, and careful use of holdout sets and cross-validation will serve us pretty well. As one gains experience, one also becomes more skilled at this.</li>
</ul>
<p class="indent">Again, use of holdout sets is the main remedy, including using multiple holdout sets if there is concern about accuracy of MAPE or OME on a single set.<span epub:type="pagebreak" id="page_60"/></p>
</div></body></html>