- en: '**8'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: THE SUCCESS OF TRANSFORMERS**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: What are the main factors that have contributed to the success of transformers?
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: In recent years, transformers have emerged as the most successful neural network
    architecture, particularly for various natural language processing tasks. In fact,
    transformers are now on the cusp of becoming state of the art for computer vision
    tasks as well. The success of transformers can be attributed to several key factors,
    including their attention mechanisms, ability to be parallelized easily, unsupervised
    pretraining, and high parameter counts.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '**The Attention Mechanism**'
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The self-attention mechanism found in transformers is one of the key design
    components that make transformer-based LLMs so successful. However, transformers
    are not the first architecture to utilize attention mechanisms.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Attention mechanisms were first developed in the context of image recognition
    back in 2010, before being adopted to aid the translation of long sentences in
    recurrent neural networks. ([Chapter 16](ch16.xhtml) compares the attention mechanisms
    found in recurrent neural networks and transformers in greater detail.)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: The aforementioned attention mechanism is inspired by human vision, focusing
    on specific parts of an image (foveal glimpses) at a time to process information
    hierarchically and sequentially. In contrast, the fundamental mechanism underlying
    transformers is a self-attention mechanism used for sequence-to-sequence tasks,
    such as machine translation and text generation. It allows each token in a sequence
    to attend to all other tokens, thus providing context-aware representations of
    each token.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: What makes attention mechanisms so unique and useful? For the following illustration,
    suppose we are using an encoder network on a fixed-length representation of the
    input sequence or image—this can be a fully connected, convolutional, or attention-based
    encoder.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: In a transformer, the encoder uses self-attention mechanisms to compute the
    importance of each input token relative to other tokens in the sequence, allowing
    the model to focus on relevant parts of the input sequence. Conceptually, attention
    mechanisms allow the transformers to attend to different parts of a sequence or
    image. On the surface, this sounds very similar to a fully connected layer where
    each input element is connected via a weight with the input element in the next
    layer. In attention mechanisms, the computation of the attention weights involves
    comparing each input element to all others. The attention weights obtained by
    this approach are dynamic and input dependent. In contrast, the weights of a convolutional
    or fully connected layer are fixed after training, as illustrated in [Figure 8-1](ch08.xhtml#ch8fig1).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/08fig01.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
- en: '*Figure 8-1: The conceptual difference between model weights in fully connected
    layers (top) and attention scores (bottom)*'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: As the top part of [Figure 8-1](ch08.xhtml#ch8fig1) shows, once trained, the
    weights of fully connected layers remain fixed regardless of the input. In contrast,
    as shown at the bottom, self-attention weights change depending on the inputs,
    even after a transformer is trained.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图8-1](ch08.xhtml#ch8fig1)顶部所示，一旦训练完成，完全连接层的权重将保持固定，无论输入如何。相比之下，正如底部所示，自注意力机制的权重会根据输入的不同而变化，即便变换器已经训练完成。
- en: Attention mechanisms allow a neural network to selectively weigh the importance
    of different input features, so the model can focus on the most relevant parts
    of the input for a given task. This provides a contextual understanding of each
    word or image token, allowing for more nuanced interpretations, which is one of
    the aspects that can make transformers work so well.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制使神经网络能够有选择性地权衡不同输入特征的重要性，从而使模型能够专注于给定任务中最相关的输入部分。这提供了对每个单词或图像标记的上下文理解，从而允许更细致的解读，这是变换器如此高效的原因之一。
- en: '**Pretraining via Self-Supervised Learning**'
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**通过自监督学习进行预训练**'
- en: Pretraining transformers via self-supervised learning on large, unlabeled datasets
    is another key factor in the success of transformers. During pre-training, the
    transformer model is trained to predict missing words in a sentence or the next
    sentence in a document, for example. By learning to predict these missing words
    or the next sentence, the model is forced to learn general representations of
    language that can be fine-tuned for a wide range of downstream tasks.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在大规模未标记数据集上使用自监督学习进行预训练是变换器成功的另一个关键因素。例如，在预训练过程中，变换器模型会训练去预测句子中缺失的单词或文档中的下一句话。通过学习预测这些缺失的单词或下一句话，模型被迫学习语言的通用表示，这些表示可以针对各种下游任务进行微调。
- en: While unsupervised pretraining has been highly effective for natural language
    processing tasks, its effectiveness for computer vision tasks is still an active
    area of research. (Refer to [Chapter 2](ch02.xhtml) for a more detailed discussion
    of self-supervised learning.)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然无监督预训练在自然语言处理任务中非常有效，但其在计算机视觉任务中的有效性仍然是一个活跃的研究领域。（有关自监督学习的更详细讨论，请参见[第2章](ch02.xhtml)。）
- en: '**Large Numbers of Parameters**'
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**大量的参数**'
- en: One noteworthy characteristic of transformers is their large model sizes. For
    example, the popular 2020 GPT-3 model consists of 175 billion trainable parameters,
    while other transformers, such as switch transformers, have trillions of parameters.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器的一个显著特点是其大规模的模型。举例来说，2020年流行的GPT-3模型包含了1750亿个可训练参数，而其他变换器，如switch transformers，拥有万亿级的参数。
- en: The scale and number of trainable parameters of transformers are essential factors
    in their modeling performance, particularly for large-scale natural language processing
    tasks. For instance, linear scaling laws suggest that the training loss decreases
    proportionally with an increase in model size, so a doubling of the model size
    can halve the training loss.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器的规模和可训练参数数量是其建模性能的关键因素，尤其是对于大规模自然语言处理任务。例如，线性缩放定律表明，随着模型规模的增加，训练损失按比例减少，因此模型规模加倍可以将训练损失减半。
- en: This, in turn, can lead to better performance on the downstream target task.
    However, it is essential to scale the model size and the number of training tokens
    equally. This means the number of training tokens should be doubled for every
    doubling of model size.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这反过来可以提高下游目标任务的表现。然而，必须同时扩大模型的规模和训练标记的数量。这意味着每当模型规模加倍时，训练标记的数量也应加倍。
- en: Since labeled data is limited, utilizing large amounts of data during un-supervised
    pretraining is vital.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 由于标注数据有限，因此在无监督预训练过程中利用大量数据至关重要。
- en: To summarize, large model sizes and large datasets are critical factors in transformers’
    success. Additionally, using self-supervised learning, the ability to pretrain
    transformers is closely tied to using large model sizes and large datasets. This
    combination has been critical in enabling the success of transformers in a wide
    range of natural language processing tasks.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，大规模模型和大规模数据集是变换器（transformers）成功的关键因素。此外，使用自监督学习，预训练变换器的能力与使用大规模模型和大规模数据集密切相关。这种结合对于变换器在广泛的自然语言处理任务中取得成功至关重要。
- en: '**Easy Parallelization**'
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**易于并行化**'
- en: Training large models on large datasets requires vast computational resources,
    and it’s key that the computations can be parallelized to utilize these resources.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, transformers are easy to parallelize since they take a fixed-length
    sequence of word or image tokens as input. For instance, the self-attention mechanism
    used in most transformer architectures involves computing the weighted sum between
    a pair of input elements. Furthermore, these pair-wise token comparisons can be
    computed independently, as illustrated in [Figure 8-2](ch08.xhtml#ch8fig2), making
    the self-attention mechanism relatively easy to parallelize across different GPU
    cores.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/08fig02.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
- en: '*Figure 8-2: A simplified self-attention mechanism without weight parameters*'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the individual weight matrices used in the self-attention mechanism
    (not shown in [Figure 8-2](ch08.xhtml#ch8fig2)) can be distributed across different
    machines for distributed and parallel computing.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercises**'
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**8-1.** As discussed in this chapter, self-attention is easily parallelizable,
    yet transformers are considered computationally expensive due to self-attention.
    How can we explain this contradiction?'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '**8-2.** Since self-attention scores represent importance weights for the various
    input elements, can we consider self-attention to be a form of feature selection?'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '**References**'
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An example of an attention mechanism in the context of image recognition: Hugo
    Larochelle and Geoffrey Hinton, “Learning to Combine Foveal Glimpses with a Third-Order
    Boltzmann Machine” (2010), *[https://dl.acm.org/doi/10.5555/2997189.2997328](https://dl.acm.org/doi/10.5555/2997189.2997328)*.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The paper introducing the self-attention mechanism with the original transformer
    architecture: Ashish Vaswani et al., “Attention Is All You Need” (2017), *[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)*.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Transformers can have trillions of parameters: William Fedus, Barret Zoph,
    and Noam Shazeer, “Switch Transformers: Scaling to Trillion Parameter Models with
    Simple and Efficient Sparsity” (2021), *[https://arxiv.org/abs/2101.03961](https://arxiv.org/abs/2101.03961)*.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Linear scaling laws suggest that training loss decreases proportionally with
    an increase in model size: Jared Kaplan et al., “Scaling Laws for Neural Language
    Models” (2020), *[https://arxiv.org/abs/2001.08361](https://arxiv.org/abs/2001.08361)*.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Research suggests that in transformer-based language models, the training tokens
    should be doubled for every doubling of model size: Jordan Hoffmann et al., “Training
    Compute-Optimal Large Language Models” (2022), *[https://arxiv.org/abs/2203.15556](https://arxiv.org/abs/2203.15556)*.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For more about the weights used in self-attention and cross-attention mechanisms,
    check out my blog post: “Understanding and Coding the Self-Attention Mechanism
    of Large Language Models from Scratch” at *[https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html)*.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 想了解更多关于自注意力和交叉注意力机制中使用的权重，查看我的博客文章：“从零开始理解和编码大语言模型的自注意力机制”，链接为 *[https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html)*。
