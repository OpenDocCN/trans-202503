- en: '**12'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SOFTWARE TEST DOCUMENTATION**
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/com.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This chapter covers software test documentation, focusing primarily on the Software
    Test Case (STC) and Software Test Procedure (STP) documents. As has been the case
    for the previous chapters, this discussion is based on IEEE Standards, specifically
    the IEEE Standard for Software and System Test Documentation (IEEE Std 829-2008,
    hereafter *Std 829*^([1](ch19_footnote.xhtml#ch12fn1))).
  prefs: []
  type: TYPE_NORMAL
- en: 12.1 The Software Test Documents in Std 829
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Std 829 actually describes many additional documents above and beyond the STC
    and STP, including:'
  prefs: []
  type: TYPE_NORMAL
- en: Master Test Plan (MTP)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Level* Test Plan (LTP)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Level* Test Design (LTD)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Level* Test Case (LTC)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Level* Test Procedure (LTPr)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Level* Test Log (LTL)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anomaly Report (AR)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Level* Interim Test Status Report (LITSR)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Level* Test Report (LTR)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Master Test Report (MTR)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that these are not actual document names—the word *level* is a placeholder
    for the scope or extent of software testing being documented. The scope could
    be at the level of *components* or *component integration*, apply to the entire
    *system*, or focus on *acceptance*. For example, *Level* Test Plan could refer
    to a Component (or Unit) Test Plan, Component Integration (or simply Integration)
    Test Plan, System (or System Integration) Test Plan, or an Acceptance Test Plan.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Test levels are explained further in “Software Development Testing Levels”
    on [page 265](ch12.xhtml#page_265).*'
  prefs: []
  type: TYPE_NORMAL
- en: 'In all, Std 829 defines 31 different document types, but these are the main
    ones. The majority of these documents exist to support software management activities.
    Because this is a book on personal software engineering rather than software project
    management, this chapter won’t go into detail on most of them. Instead, we’ll
    concentrate on those *level* test documents that pertain to actual software testing—specifically,
    the *Level* Test Case, *Level* Test Procedure, *Level* Test Log, and Anomaly Report
    document types. We will cover all four *level* classifications—component, component
    integration, system, and acceptance—though the latter two are the main test documents
    used in this chapter. The differences between the *level* test documents are relatively
    minor, so this chapter applies the umbrella names mentioned earlier: Software
    Test Cases and Software Test Procedures. Keep in mind, however, that while these
    are common software engineering terms, Std 829 refers only to the *level* test
    documents.'
  prefs: []
  type: TYPE_NORMAL
- en: '***12.1.1 Process Support***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Although this chapter focuses on software testing, Std 829 describes the testing
    process in far more general terms. In particular, the testing process also handles
    the verification and validation of each document step in the development process.
    Specifically, this means that the testing process tests the documentation as well
    as the actual software.
  prefs: []
  type: TYPE_NORMAL
- en: For the SyRS and SRS, the verification step ensures that the requirements actually
    satisfy customer needs (and *only* satisfy customer needs, without gold plating).
    For the SDD, the verification step ensures that the SDD covers all the requirements.
    For the STC, the verification step ensures that each requirement has one or more
    test cases that test the requirement. For the STP, the verification ensures that
    the set of test procedures fully covers all the test cases.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to documentation, Std 829 discusses test procedures for verifying
    acquisitions (such as purchases of third-party libraries and computing hardware),
    administering RFPs (Requests for Proposals), and many other activities. These
    testing activities are very important. As noted previously, though, these are
    largely management activities rather than software development activities, so
    they’re mentioned only briefly here.
  prefs: []
  type: TYPE_NORMAL
- en: Std 829 states that testing needs to support the processes of management, acquisition,
    supply, development, operation, and maintenance. This chapter will concentrate
    on the development and operation processes (and, to a limited extent, the maintenance
    processes, which are largely an iteration of the development and operation processes).
    For more details on the other processes, see Std 829, IEEE/EIA Std 12207.0-1996
    [B21], and ISO-IEC-IEEE-29148-2011.
  prefs: []
  type: TYPE_NORMAL
- en: Note that Std 829 allows you to combine and omit some of the testing documents.
    This means that you could have only a single document and still conform to Std
    829\. In reality, the final number of documents you create depends on the size
    of the project (large projects will require more documentation) and the turnaround
    you expect (fast projects will have fewer documents).
  prefs: []
  type: TYPE_NORMAL
- en: '***12.1.2 Integrity Levels and Risk Assessment***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Std 829 defines four *integrity levels* that describe the importance or sensitivity
    to risk for a piece of software:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Catastrophic (level 4)** This level means that the software must execute
    properly, or something disastrous could occur (such as death, irreparable harm
    to the system, environmental damage, or a huge financial loss). There are no workarounds
    for catastrophic system failures. An example is a braking failure in a software-controlled
    self-driving vehicle.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Critical (level 3)** This level means that software must execute properly,
    or there could be serious problems including permanent injury, major performance
    degradation, environmental damage, or financial loss. A partial workaround may
    be possible for a critical system failure. An example is the transmission-controlling
    software in the self-driving vehicle being unable to shift out of second gear.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Marginal (level 2)** This level means that the software must execute properly,
    or there may be (minor) incorrect results produced and some functionality lost.
    Workarounds to solve the problem are possible. Continuing with the self-driving-vehicle
    example, a software failure that prevents the infotainment center from operating
    is a marginal problem.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Negligible (level 1)** This level means that the software must execute properly,
    or else some minor functionality might not exist in the system (or the software
    might not be as “polished” as it should be). Negligible issues generally don’t
    require a workaround and can be safely ignored until an update comes along. An
    example is a spelling mistake on the touchscreen of the infotainment center in
    the self-driving vehicle.'
  prefs: []
  type: TYPE_NORMAL
- en: The higher the level, the greater the importance of the testing process; that
    is, level 4 (catastrophic) items demand higher-quality and more intensive testing
    than level 1 (negligible) items. Integrity levels, then, become the basis for
    determining the number, quality, and depth of test cases you create. For a feature
    in the program that could have catastrophic results in the event of a failure,
    you want a fair number of test cases that exercise that feature with considerable
    depth. For features that have negligible potential consequences, you might not
    have any test cases or only very shallow tests (such as a cursory review).^([2](ch19_footnote.xhtml#ch12fn2))
  prefs: []
  type: TYPE_NORMAL
- en: '*Risk assessment* is an attempt to determine where in your system failures
    are likely to occur, their expected frequency, and the associated costs. While
    risk assessment is predictive by its very nature (which means it won’t be perfect),
    you can often identify those parts of the program that are more likely to exhibit
    problems (such as complex sections of code, code produced by less experienced
    engineers, code from questionable sources like open source libraries found on
    the internet, and code using poorly understood algorithms). If you can categorize
    the likelihood of a problem as *likely*, *probable*, *occasional*, or *unlikely*,
    you can help identify the code that warrants more stringent testing (and, conversely,
    code that requires minimal testing).'
  prefs: []
  type: TYPE_NORMAL
- en: You can combine the integrity level and risk assessment levels in a matrix to
    produce a risk assessment scheme, as shown in [Table 12-1](ch12.xhtml#ch12tab1).
    In this example, a value of 4 denotes extreme importance, and a value of 1 indicates
    little importance.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 12-1:** Risk Assessment Scheme'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Consequence** | **Likelihood** |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | Likely | Probable | Occasional | Unlikely |'
  prefs: []
  type: TYPE_TB
- en: '| Catastrophic | 4 | 4 | 3.5 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| Critical | 4 | 3.5 | 3 | 2.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Marginal | 3 | 2.5 | 1.5 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Negligible | 2 | 1.5 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: Std 829 does not mandate using an integrity level or risk assessment scheme
    in your test documentation, though it does consider this to be best practice.
    If you do use an integrity level, Std 829 does not require that you use the IEEE-recommended
    scheme (you could, for example, use a finer-grained integrity level with values
    from 1 to 10). However, if you “roll your own” integrity level, the IEEE recommends
    that you document a mapping from your integrity levels to those suggested by the
    IEEE so that readers can easily compare them.
  prefs: []
  type: TYPE_NORMAL
- en: '***12.1.3 Software Development Testing Levels***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In addition—and in contrast—to the integrity levels just described, the IEEE
    defines four testing levels, each of which generally describes the scope or extent
    of software testing being documented:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Component (also known as** ***unit*****)****^([3](ch19_footnote.xhtml#ch12fn3))**
    This level deals with subroutines, functions, modules, and subprograms at the
    lowest code level. *Unit testing*, for example, consists of testing individual
    functions and other small program units independent of the rest of the program.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Component integration (also known as simply** ***integration*****)** This
    level is the point at which you begin combining individual units together to form
    a larger portion of the system, though not necessarily the whole system. Integration
    testing, for example, occurs when you combine (pretested) units to see if they
    play well together (that is, pass appropriate parameters, return appropriate function
    results, and so on).'
  prefs: []
  type: TYPE_NORMAL
- en: '**System (also known as** ***system integration*****)** This level of testing
    is the ultimate form of integration testing—you’ve integrated all the program
    units together and formed the complete system. Unit testing, integration testing,
    and system integration testing are typically tests the developers perform before
    releasing a complete system outside the development group.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Acceptance (variants include** ***factory acceptance*** **and** ***site acceptance*****)**
    *Acceptance testing* *(AT)* is post-development. As its name implies, it refers
    to how the customer determines whether the system is acceptable. Depending on
    the system, there may be a couple of acceptance testing variants. *Factory acceptance
    testing (FAT)* occurs on systems prior to leaving the manufacturer (typically
    on the factory floor, hence the name). Even if a product is pure software, it
    can have a factory acceptance test where the customer’s representatives come to
    test the software under the watchful eye of the software development team. This
    allows the team to make quick changes to the system if the customer discovers
    minor errors during the FAT.'
  prefs: []
  type: TYPE_NORMAL
- en: A *site acceptance test* *(SAT)* is performed at the customer’s site after the
    system is installed. For hardware-based systems, this ensures that the hardware
    is installed properly and the software is functioning as intended. For pure software
    systems, the SAT provides a final check (after a possible AT or FAT) that the
    software is usable by the system’s end users.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.2 Test Plans**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A software test plan is a document that describes the scope, organization, and
    activities associated with the testing process. This is largely a managerial overview
    of how the testing will take place, the resources testing will require, schedules,
    necessary tools, and objectives. This chapter won’t consider test plans in detail,
    as they are beyond the scope of this book; however, the following sections will
    present outlines provided in IEEE Std 829-2008 as a reference. For more details
    on these test plans, consult Std 829.
  prefs: []
  type: TYPE_NORMAL
- en: '***12.2.1 Master Test Plan***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The *Master Test Plan (MTP)* is an organization-wide top-level management document
    that tracks the testing process across a whole project (or set of projects). Software
    engineers are rarely involved directly with the MTP, which is largely an umbrella
    document that the QA (Quality Assurance) department uses to track quality aspects
    of a project. A project manager or project lead might be aware of the MTP—and
    might contribute to it during schedule and resource development—but the development
    team rarely sees the MTP except in passing.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following outline comes from Section 8 of IEEE Std 829-2008 (and uses the
    IEEE section numbers):'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 Document Identifier
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 Scope
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 References
  prefs: []
  type: TYPE_NORMAL
- en: 1.4 System Overview and Key Features
  prefs: []
  type: TYPE_NORMAL
- en: 1.5 Test Overview
  prefs: []
  type: TYPE_NORMAL
- en: 1.5.1 Organization
  prefs: []
  type: TYPE_NORMAL
- en: 1.5.2 Master Test Schedule
  prefs: []
  type: TYPE_NORMAL
- en: 1.5.3 Integrity Level Schema
  prefs: []
  type: TYPE_NORMAL
- en: 1.5.4 Resources Summary
  prefs: []
  type: TYPE_NORMAL
- en: 1.5.5 Responsibilities
  prefs: []
  type: TYPE_NORMAL
- en: 1.5.6 Tools, Techniques, Methods, and Metrics
  prefs: []
  type: TYPE_NORMAL
- en: 2 Details of the Master Test Plan
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Test Processes Including Definition of Test Levels
  prefs: []
  type: TYPE_NORMAL
- en: '2.1.1 Process: Management'
  prefs: []
  type: TYPE_NORMAL
- en: '2.1.1.1 Activity: Management of Test Effort'
  prefs: []
  type: TYPE_NORMAL
- en: '2.1.2 Process: Acquisition'
  prefs: []
  type: TYPE_NORMAL
- en: '2.1.2.1 Activity: Acquisition Support Test'
  prefs: []
  type: TYPE_NORMAL
- en: '2.1.3 Process: Supply'
  prefs: []
  type: TYPE_NORMAL
- en: '2.1.3.1 Activity: Planning Test'
  prefs: []
  type: TYPE_NORMAL
- en: '2.1.4 Process: Development'
  prefs: []
  type: TYPE_NORMAL
- en: '2.1.4.1 Activity: Concept'
  prefs: []
  type: TYPE_NORMAL
- en: '2.1.4.2 Activity: Requirements'
  prefs: []
  type: TYPE_NORMAL
- en: '2.1.4.3 Activity: Design'
  prefs: []
  type: TYPE_NORMAL
- en: '2.1.4.4 Activity: Implementation'
  prefs: []
  type: TYPE_NORMAL
- en: '2.1.4.5 Activity: Test'
  prefs: []
  type: TYPE_NORMAL
- en: '2.1.4.6 Activity: Install/Checkout'
  prefs: []
  type: TYPE_NORMAL
- en: '2.1.5 Process: Operation'
  prefs: []
  type: TYPE_NORMAL
- en: '2.1.5.1 Activity: Operational Test'
  prefs: []
  type: TYPE_NORMAL
- en: '2.1.6 Process: Maintenance'
  prefs: []
  type: TYPE_NORMAL
- en: '2.1.6.1 Activity: Maintenance Test'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Test Documentation Requirements
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Test Administration Requirements
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Test Reporting Requirements
  prefs: []
  type: TYPE_NORMAL
- en: 3 General
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Glossary
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Document Change Procedures and History
  prefs: []
  type: TYPE_NORMAL
- en: Many of these sections contain information common to IEEE documents (for example,
    see the SRS and SDD samples in previous chapters). As the MTP is beyond the scope
    of this chapter, please consult Std 829 for specific descriptions of each section
    in this outline.
  prefs: []
  type: TYPE_NORMAL
- en: '***12.2.2* Level *Test Plan***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A Level *Test Plan (LTP)* refers to a set of test plans based on the development
    state. As this chapter noted earlier, each document in the set generally describes
    the scope or extent of software test being documented: Component Test Plan (aka
    Unit Test Plan, or UTP), Component Integration Test Plan (aka Integration Test
    Plan, or ITP), System Test Plan (aka System Integration Test Plan, or SITP), and
    Acceptance Test Plan (ATP; may include a Factory Acceptance Test Plan [FATP] or
    Site Acceptance Test Plan [SATP]).^([4](ch19_footnote.xhtml#ch12fn4))'
  prefs: []
  type: TYPE_NORMAL
- en: LTPs are also managerial/QA documents, but the development team (even to the
    level of individual software engineers) often has input on their creation and
    use, because these documents reference detailed features of the software design.
    These test plans are not guiding documents—that is, a software engineer wouldn’t
    necessarily reference these documents while actually testing the software—but
    they can’t be created without development team feedback. Like the MTP, LTPs provide
    a road map for the creation of the test case and test procedure documents (of
    primary interest to the development and testing teams) and outline how to perform
    the tests. LTPs provide a good high-level view of the testing process, especially
    for external organizations interested in its quality.^([5](ch19_footnote.xhtml#ch12fn5))
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the LTP outline from Std 829:'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 Document Identifier
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 Scope
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 References
  prefs: []
  type: TYPE_NORMAL
- en: 1.4 Level in the Overall Sequence
  prefs: []
  type: TYPE_NORMAL
- en: 1.5 Test Classes and Overall Test Conditions
  prefs: []
  type: TYPE_NORMAL
- en: 2 Details for This Level of Test Plan
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Test Items and Their Identifiers
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Test Traceability Matrix
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Features to Be Tested
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Features Not to Be Tested
  prefs: []
  type: TYPE_NORMAL
- en: 2.5 Approach
  prefs: []
  type: TYPE_NORMAL
- en: 2.6 Item Pass/Fail Criteria
  prefs: []
  type: TYPE_NORMAL
- en: 2.7 Suspension Criteria and Resumption Requirements
  prefs: []
  type: TYPE_NORMAL
- en: 2.8 Test Deliverables
  prefs: []
  type: TYPE_NORMAL
- en: 3 Test Management
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Planned Activities and Tasks; Test Progression
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Environmental/Infrastructure
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Responsibilities and Authority
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Interfaces Among the Parties Involved
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Resources and Their Allocation
  prefs: []
  type: TYPE_NORMAL
- en: 3.6 Training
  prefs: []
  type: TYPE_NORMAL
- en: 3.7 Schedules, Estimates, and Costs
  prefs: []
  type: TYPE_NORMAL
- en: 3.8 Risk(s) and Contingency(s)
  prefs: []
  type: TYPE_NORMAL
- en: 4 General
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Quality Assurance Procedures
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Metrics
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Test Coverage
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Glossary
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Document Change Procedures and History
  prefs: []
  type: TYPE_NORMAL
- en: You might notice that there is considerable overlap between the LTPs and the
    MTP. Std 829 states that if you are replicating information in a test plan that
    exists elsewhere, you can simply provide a reference to the containing document
    rather than duplicating the information in your LTP (or MTP). For example, you’re
    likely to have an overall Reverse Traceability Matrix (RTM) that includes traceability
    for all the tests. Rather than replicating that traceability information in section
    2.2 of an LTP, you would simply reference the RTM document that contains this
    information.
  prefs: []
  type: TYPE_NORMAL
- en: '***12.2.3* Level *Test Design Documentation***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The Level *Test Design (LTD)* documentation, as its name suggests, describes
    the design of the tests. Once again, there are four types of LTD documents, each
    generally describing the scope or extent of software testing being documented:
    Component Test Design (aka Unit Test Design, or UTD), Component Integration Test
    Design (aka Integration Test Design, or ITD), System Test Design (aka System Integration
    Test Design, or SITD), and Acceptance Test Design (ATD; this may include a Factory
    Acceptance Test Design [FATD] or a Site Acceptance Test Design [SATD]).'
  prefs: []
  type: TYPE_NORMAL
- en: The main purpose of the LTD is to collect common information in one place that
    would be replicated throughout the test procedures. That means that this document
    could very easily be merged with your test procedures document (at the cost of
    some repetition in that document). This book will take that approach, merging
    pertinent items from the test design directly into the test cases and test procedures
    documents.^([6](ch19_footnote.xhtml#ch12fn6)) For that reason this section will
    present the IEEE recommended outline without additional commentary and save the
    details for the STC and STP documents.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 Document Identifier
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 Scope
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 References
  prefs: []
  type: TYPE_NORMAL
- en: 2 Details of the Level Test Design
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Features to Be Tested
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Approach Refinements
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Test Identification
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Feature Pass/Fail Criteria
  prefs: []
  type: TYPE_NORMAL
- en: 2.5 Test Deliverables
  prefs: []
  type: TYPE_NORMAL
- en: 3 General
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Glossary
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Document Change Procedures and History
  prefs: []
  type: TYPE_NORMAL
- en: '**12.3 Software Review List Documentation**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When you build the RTM starting with your requirements, one of the columns
    you usually create is the test/verification type column. Typically, a software
    requirement will have one of two associated verification types: *T* (for *test*)
    and *R* (for *review*).^([7](ch19_footnote.xhtml#ch12fn7)) Requirements marked
    *T* will have associated test cases and test procedures (see “[Updating the Traceability
    Matrix with Requirement Information](ch10.xhtml#lev-10.9)” on [page 222](ch10.xhtml#page_222)
    for details on creating test cases). Items marked *R* will need to be reviewed.
    This section describes how to create a Software Review List (SRL) document to
    track the review of the system (usually the source code) to verify those requirements.'
  prefs: []
  type: TYPE_NORMAL
- en: The SRL is relatively straightforward. The core of the document is simply a
    list of items, each of which you check off after you review it and are confident
    that the software properly supports the associated requirement.
  prefs: []
  type: TYPE_NORMAL
- en: 'In theory, you could create *level* review list documentation at four separate
    levels: component, component integration, system, and acceptance (as is the case
    for other Std 829 *level* documents). In reality, however, a single SRL that is
    suitable for both system (integration) and acceptance use will suffice.'
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*The SRL document is not a part of Std 829 (or any other IEEE standards document,
    for that matter). Std 829 certainly allows you to use this document as part of
    your verification package, but the format presented in this section is not from
    the IEEE.*'
  prefs: []
  type: TYPE_NORMAL
- en: '***12.3.1 Sample SRL Outline***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Although the SRL is not a standard IEEE document, the following outline for
    it is somewhat similar to the SRS, STC, and STP recommended formats from the IEEE:'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction (once per document)
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 Document Identifier
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 Document Change Procedures and History
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 Scope
  prefs: []
  type: TYPE_NORMAL
- en: 1.4 Intended Audience
  prefs: []
  type: TYPE_NORMAL
- en: 1.5 Definitions, Acronyms, and Abbreviations
  prefs: []
  type: TYPE_NORMAL
- en: 1.6 References
  prefs: []
  type: TYPE_NORMAL
- en: 1.7 Notation for Description
  prefs: []
  type: TYPE_NORMAL
- en: 2 General System Description
  prefs: []
  type: TYPE_NORMAL
- en: 3 Checklist (one per review item)
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Review Identifier (Tag)
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Discussion of Item to Review
  prefs: []
  type: TYPE_NORMAL
- en: '***12.3.2 Sample SRL***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This sample SRL continues to use the DAQ DIP switch project from the previous
    chapters. Specifically, this SRL is based on the requirements from “(Selected)
    DAQ Software Requirements (from SRS)” on [page 219](ch10.xhtml#page_219) and the
    verification types detailed in “Requirements to Be Verified by Review” on [page
    223](ch10.xhtml#page_223).
  prefs: []
  type: TYPE_NORMAL
- en: '**1 Introduction**'
  prefs: []
  type: TYPE_NORMAL
- en: This Software Review List provides a software review checklist for those DAQ
    system requirements that are to be verified by review.
  prefs: []
  type: TYPE_NORMAL
- en: '**1.1 Document Identifier**'
  prefs: []
  type: TYPE_NORMAL
- en: DAQ_SRL v1.0
  prefs: []
  type: TYPE_NORMAL
- en: '**1.2 Document Change Procedures and History**'
  prefs: []
  type: TYPE_NORMAL
- en: All revisions should be noted here, by date and version number.
  prefs: []
  type: TYPE_NORMAL
- en: Mar 23, 2018—Version 1.0
  prefs: []
  type: TYPE_NORMAL
- en: '**1.3 Scope**'
  prefs: []
  type: TYPE_NORMAL
- en: This SRL deals with those requirements in the DAQ DIP switch initialization
    project for which creating a formal test procedure would be difficult (or otherwise
    economically unviable) but whose correctness can be easily verified by reviewing
    the source code and the build system for the source code.
  prefs: []
  type: TYPE_NORMAL
- en: '**1.4 Intended Audience**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The *normal* audience for an SRL:'
  prefs: []
  type: TYPE_NORMAL
- en: This document is intended primarily for those individuals who will be testing/reviewing
    the DAQ DIP switch project. Project management and the development team may also
    wish to review this document.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *real* audience for this SRL:'
  prefs: []
  type: TYPE_NORMAL
- en: This SRL is intended for readers of *Write Great Code, Volume 3*. It provides
    an example SRL that can serve as a template for SRLs they may need to create.
  prefs: []
  type: TYPE_NORMAL
- en: '**1.5 Definitions, Acronyms, and Abbreviations**'
  prefs: []
  type: TYPE_NORMAL
- en: 'DAQ: Data acquisition system'
  prefs: []
  type: TYPE_NORMAL
- en: 'DIP: Dual inline package'
  prefs: []
  type: TYPE_NORMAL
- en: 'SDD: Software Design Document'
  prefs: []
  type: TYPE_NORMAL
- en: 'SRL: Software Review List'
  prefs: []
  type: TYPE_NORMAL
- en: 'SRS: Software Requirements Specification'
  prefs: []
  type: TYPE_NORMAL
- en: '**1.6 References**'
  prefs: []
  type: TYPE_NORMAL
- en: 'SDD: IEEE Std 1016-2009'
  prefs: []
  type: TYPE_NORMAL
- en: 'SRS: IEEE Std 830-1998'
  prefs: []
  type: TYPE_NORMAL
- en: 'STC/STP: IEEE Std 829-2008'
  prefs: []
  type: TYPE_NORMAL
- en: '**1.7 Notation for Description**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Review identifiers (*tags*) in this document shall take the form:'
  prefs: []
  type: TYPE_NORMAL
- en: DAQ_SR_*xxx_yyy_zzz*
  prefs: []
  type: TYPE_NORMAL
- en: where *xxx_yyy* is a string of (possibly decimal) numbers taken from the corresponding
    requirement (for example, DAQ_SRS_*xxx_yyy*) and *zzz* is a (possibly decimal)
    numeric sequence that creates a unique identifier out of the whole sequence. Note
    that *zzz* values in SRL tags are usually numbered from 000 or 001 and usually
    increment by 1 for each additional review item sharing the same *xxx_yyy* string.
  prefs: []
  type: TYPE_NORMAL
- en: '**2 General System Description**'
  prefs: []
  type: TYPE_NORMAL
- en: The purpose behind the DAQ DIP switch system is to initialize the DAQ system
    upon power-up. The DAQ DIP switch system is a small subset of the larger Plantation
    Productions DAQ system that is useful as an example within this book.
  prefs: []
  type: TYPE_NORMAL
- en: '**3 Checklist**'
  prefs: []
  type: TYPE_NORMAL
- en: Check off each of the following items as it is verified during the review process.
  prefs: []
  type: TYPE_NORMAL
- en: '**3.1 DAQ_SR_700_000_000**'
  prefs: []
  type: TYPE_NORMAL
- en: Verify code is written for a Netburner MOD54415 evaluation board.
  prefs: []
  type: TYPE_NORMAL
- en: '**3.2 DAQ_SR_700_000.01_000.1**'
  prefs: []
  type: TYPE_NORMAL
- en: Verify code is written for μC/OS.
  prefs: []
  type: TYPE_NORMAL
- en: '**3.3 DAQ_SR_702_001_000**'
  prefs: []
  type: TYPE_NORMAL
- en: Verify that software creates a separate task to handle serial port command processing.
  prefs: []
  type: TYPE_NORMAL
- en: '**3.4 DAQ_SR_702_002_000**'
  prefs: []
  type: TYPE_NORMAL
- en: Verify that serial task priority is lower than USB and Ethernet task priorities
    (note that the higher the priority number, the lower the priority).
  prefs: []
  type: TYPE_NORMAL
- en: '**3.5 DAQ_SR_703_001_000**'
  prefs: []
  type: TYPE_NORMAL
- en: Same as DAQ_SRS_702_001, but doesn’t start an RS-232 task if DIP switch 1 is
    in the OFF position.
  prefs: []
  type: TYPE_NORMAL
- en: '**3.6 DAQ_SR_705_001_000**'
  prefs: []
  type: TYPE_NORMAL
- en: Verify that software creates a separate task to handle USB port command processing.
  prefs: []
  type: TYPE_NORMAL
- en: '**3.7 DAQ_SR_705_002_000**'
  prefs: []
  type: TYPE_NORMAL
- en: Verify that a USB task has a higher priority than the Ethernet and serial protocol
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '**3.8 DAQ_SR_706_001_000**'
  prefs: []
  type: TYPE_NORMAL
- en: Verify that software does not start the USB task if DIP switch 2 is in the OFF
    position.
  prefs: []
  type: TYPE_NORMAL
- en: '**3.9 DAQ_SR_716_001_000**'
  prefs: []
  type: TYPE_NORMAL
- en: Verify that the Ethernet listening task is started only if Ethernet communications
    are enabled.
  prefs: []
  type: TYPE_NORMAL
- en: '**3.10 DAQ_SR_716_002_000**'
  prefs: []
  type: TYPE_NORMAL
- en: Verify that the Ethernet listening task has a priority lower than the USB task
    but higher than the serial task.
  prefs: []
  type: TYPE_NORMAL
- en: '**3.11 DAQ_SR_719_000_000**'
  prefs: []
  type: TYPE_NORMAL
- en: Verify that software sets the unit test mode value to ON based on the DIP switch
    7 setting.
  prefs: []
  type: TYPE_NORMAL
- en: '**3.12 DAQ_SR_720_000_000**'
  prefs: []
  type: TYPE_NORMAL
- en: Verify that software sets the unit test mode value to OFF based on the DIP switch
    7 setting.
  prefs: []
  type: TYPE_NORMAL
- en: '**3.13 DAQ_SR_723_000_000**'
  prefs: []
  type: TYPE_NORMAL
- en: Verify that the software provides a function to read the DIP switches.
  prefs: []
  type: TYPE_NORMAL
- en: '**3.14 DAQ_SR_723_000.01_000**'
  prefs: []
  type: TYPE_NORMAL
- en: Verify that the system uses the DIP switch reading to initialize RS-232 (serial),
    USB, Ethernet, unit test mode, and debug mode on startup.
  prefs: []
  type: TYPE_NORMAL
- en: '**3.15 DAQ_SR_723_000.02_000**'
  prefs: []
  type: TYPE_NORMAL
- en: Verify that the startup code stores the DIP switch reading for later use by
    the software.
  prefs: []
  type: TYPE_NORMAL
- en: '**3.16 DAQ_SR_725_000_000**'
  prefs: []
  type: TYPE_NORMAL
- en: Verify that the command processor responds to a command when a complete line
    of text is received from the USB, RS-232, and Ethernet ports.
  prefs: []
  type: TYPE_NORMAL
- en: '**3.17 DAQ_SR_738_001_000**'
  prefs: []
  type: TYPE_NORMAL
- en: Verify that the system starts a new process (task) to handle command processing
    for each new Ethernet connection.
  prefs: []
  type: TYPE_NORMAL
- en: '**3.18 DAQ_SR_738_002_000**'
  prefs: []
  type: TYPE_NORMAL
- en: Verify that the Ethernet command processing tasks have a priority between the
    Ethernet listening task and the USB command task.
  prefs: []
  type: TYPE_NORMAL
- en: '***12.3.3 Adding SRL Items to the Traceability Matrix***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Once you’ve created an SRL, you’ll want to add all the *SR* tags to the RTM
    so you can trace the reviewed items back to the requirements, as well as to everything
    else in the RTM. To do so, just locate the requirement associated with each review
    item tag (this is trivial if you’re using the tag numbering this chapter recommends;
    the SRS tag number is incorporated into the SRL tag number) and add the SRL tag
    to the appropriate column in the same row of the RTM containing the requirement.
  prefs: []
  type: TYPE_NORMAL
- en: When you’ve got both SRL and STC documents, there’s really no need to create
    separate columns in the RTM for both types, as they are mutually exclusive and
    the tag will differentiate them. (See “[A Sample Software Requirements Specification](ch10.xhtml#lev-10.4.5)”
    on [page 203](ch10.xhtml#page_203) for some additional commentary on this.)
  prefs: []
  type: TYPE_NORMAL
- en: '**12.4 Software Test Case Documentation**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For each item in the RTM whose requirement verification type is *T*, you’ll
    need to create a software test case. The *Software Test Case (STC)* document is
    where you’ll put the actual test cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'As with all the 829 Std *level* documents, there are four levels in the *Level*
    Test Case documentation. The term *Software Test Case* generically refers to any
    one of these. As this chapter noted earlier, this is actually a set of test cases,
    where each document in the set type generally describes the scope or extent of
    software testing being documented: Component Test Cases (aka Unit Test Cases,
    or UTC), Component Integration Test Cases (aka Integration Test Cases, or ITC),
    System Test Cases (aka System Integration Test Cases, or SITC), and Acceptance
    Test Cases (ATC; may include Factory Acceptance Test Cases [FATC] and Site Acceptance
    Test Cases [SATC]).^([8](ch19_footnote.xhtml#ch12fn8))'
  prefs: []
  type: TYPE_NORMAL
- en: 'The STC document lists all the individual test cases (tests) for a project.
    Here is the Std 829 outline for the *Level* Test Case documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction (once per document)
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 Document Identifier
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 Scope
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 References
  prefs: []
  type: TYPE_NORMAL
- en: 1.4 Context
  prefs: []
  type: TYPE_NORMAL
- en: 1.5 Notation for Description
  prefs: []
  type: TYPE_NORMAL
- en: 2 Details (once per test case)
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Test Case Identifier
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Objective
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Inputs
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Outcome(s)
  prefs: []
  type: TYPE_NORMAL
- en: 2.5 Environmental Needs
  prefs: []
  type: TYPE_NORMAL
- en: 2.6 Special Procedural Requirements
  prefs: []
  type: TYPE_NORMAL
- en: 2.7 Intercase Dependencies
  prefs: []
  type: TYPE_NORMAL
- en: 3 Global (once per document)
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Glossary
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Document Change Procedures and History
  prefs: []
  type: TYPE_NORMAL
- en: In common practice, the Unit Test Cases and the Integration Test Cases are often
    combined into the same document (the differentiation between the two usually occurs
    at the level of test procedures). You will typically develop UTCs and ITCs from
    your source code and from the SDD (see [Figure 12-1](ch12.xhtml#ch12fig1), which
    is an extension of [Figure 9-1](ch09.xhtml#ch9fig1)).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/fig12-1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-1: Unit and Integration Test Case sources*'
  prefs: []
  type: TYPE_NORMAL
- en: Often, the UTC and ITC (and test procedure) documents exist as software rather
    than as natural-language documents. Using an *automated test procedure*, a piece
    of software that runs all the unit and integration tests, is a software engineering
    best practice. By doing so, you can dramatically reduce the time it takes to run
    tests as well as the errors introduced in manually performed test procedures.^([9](ch19_footnote.xhtml#ch12fn9))
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, it isn’t possible to create automated tests for every test case,
    so you’ll usually have a UTC/ITC document covering (at least) the test cases you
    must perform manually.
  prefs: []
  type: TYPE_NORMAL
- en: Many organizations—particularly those that embrace Agile development models
    and test-driven development (TDD)—forgo formal UTC and ITC documents. Informally
    written procedures and automated test procedures are far more common in these
    situations because the cost of creating and (especially) maintaining the documentation
    quickly gets out of hand. As long as the development team can provide *some* documentation
    that they are performing a fixed set of unit/integration tests (that is, they’re
    not doing ad hoc, “by the seat of the pants” tests that could differ on every
    test run), larger organizations tend to leave them be.
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of whether it’s formal, informal, or automated, having a repeatable
    test procedure is key. *Regression tests*, which check to see if anything has
    broken, or regressed, since you’ve made changes to the code, require a repeatable
    testing process. Therefore, you need some kind of test case to ensure repeatability.
  prefs: []
  type: TYPE_NORMAL
- en: For unit/integration testing, the test data you generate will be a combination
    of black-box-generated test data and white-box-generated test data. *Black-box
    test data* generally comes from the system requirements (SyRS and SRS); you consider
    only the functionality of the system (which the requirements provide) when you
    create its input test data. When you generate *white-box test data*, on the other
    hand, you analyze the software’s source code. For example, ensuring that you execute
    every statement in the program at least once during testing—that is, achieving
    complete code coverage—requires careful analysis of the source code and, therefore,
    is a white-box test-data-generation technique.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Write Great Code, Volume 6: Testing, Debugging, and Quality Assurance *will
    consider the techniques for generating white-box and black-box test data in greater
    detail.*'
  prefs: []
  type: TYPE_NORMAL
- en: Once you get to the level of a system integration test or (even more importantly)
    an acceptance test, formal documentation for your test cases becomes mandatory.
    If you’re creating a custom system for a customer, or your software is subject
    to regulatory or legal restrictions (such as life-threatening environments in
    an autonomous vehicle), you’ll likely have to convince some overseer organization
    that you’ve put in your best effort during testing and prove that the system meets
    its requirements. This is where it’s essential to have formal documentation like
    that recommended by Std 829.^([10](ch19_footnote.xhtml#ch12fn10)) For this reason,
    most SITC and (most certainly) ATC documents derive their cases directly from
    the requirements (see [Figure 12-2](ch12.xhtml#ch12fig2)). So, with this motivation
    in hand, let’s return to the discussion of the *Level* Test Case document (see
    the outline at the beginning of this section).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/fig12-2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-2: SITC and ATC derivation*'
  prefs: []
  type: TYPE_NORMAL
- en: More often than not, the (F)ATC document is simply a subset of the SITC document.
    (If you have FATC documentation and SATC documentation, the site variant is often
    a subset of the FATC document.) The SITC document will contain test cases for
    every requirement. In the ATC documents, system architects may merge or eliminate
    test cases that are nearly or entirely redundant, or are of little interest to
    customers and end users.
  prefs: []
  type: TYPE_NORMAL
- en: '***12.4.1 Introduction in the STC Document***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The introductory section of an STC (or any *Level* Test Case) document should
    include the following information.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.4.1.1 Document Identifier**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The document identifier should be some unique name/number and should include
    the issuing date, author identification, status (for example, draft or final),
    approval signatures, and possibly a version number. A single ID name/number is
    imperative so you can reference the test case documentation in other documents
    (such as the STP and RTM).
  prefs: []
  type: TYPE_NORMAL
- en: '**12.4.1.2 Scope**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This section summarizes the software system and features to test.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.4.1.3 References**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This section should provide a list of all reference documents, internal and
    external, associated with the STC. Internal references would normally include
    documents such as the SyRS, SRS, SDD, RTM, and (if it exists) the MTP. External
    references would include standards like IEEE Std 829-2008 and any regulatory or
    legal documents that might apply.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.4.1.4 Context**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In this section you provide any context for the test cases that doesn’t appear
    in any other documentation. Examples might include naming automated test-generation
    software or internet-based tools used to generate or evaluate test cases.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.4.1.5 Notation for Description**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This section should describe the tags (identifiers) you’ll apply to the test
    cases. For example, this chapter uses tags of the form *proj*_STC_*xxx*_*yyy*_*zzz*,
    so this section of the STC would explain what this means and how to generate STC
    tags.
  prefs: []
  type: TYPE_NORMAL
- en: '***12.4.2 Details***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You repeat the subsections contained herein for each test case in the STC.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.4.2.1 Test Case Identifier**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The test case identifier is the tag associated with this particular test case.
    For example, this book uses tags of the form *DAQ_STC_002_000_001* where *DAQ*
    is the project ID (for the DAQ DIP switch project), *002_000* is from the SRS
    requirement tag, and *001* is a test-case-specific value to make this tag unique
    among all the others. The Swimming Pool Monitor (SPM) project from previous chapters
    might use tags like *POOL_STC_002_001* within the STC. Std 829 doesn’t require
    the use of this tag format, only that all test case tags be unique.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.4.2.2 Objective**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This is a brief description of the focus or goal of this particular test case.
    (Note that a set of test cases can have the same objective, in which case this
    field could simply reference the objectives in a different test case.) This field
    is a good place to put risk assessment and integrity level information, if relevant.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.4.2.3 Input(s)**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This section lists all inputs and their relationships (in terms of timing, ordering,
    and the like) that a tester needs in order to perform this test case. Some inputs
    might be exact, and some may be approximate, in which case you must provide tolerances
    for the input data. If the input set is large, this section might simply reference
    an input file, database, or some other input stream that will provide the test
    data.^([11](ch19_footnote.xhtml#ch12fn11))
  prefs: []
  type: TYPE_NORMAL
- en: '**12.4.2.4 Outcome(s)**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This section lists all expected output data values and behaviors such as response
    time, timing relationships, and order of output data. The test case should provide
    exact output data values if possible; if you can provide only approximate data
    values, the test case must also supply tolerances. If an output stream is large,
    then this section can reference externally supplied files or databases.
  prefs: []
  type: TYPE_NORMAL
- en: If the test is successful by virtue of the fact that it runs without crashing—that
    is, self-validating—then this section is unnecessary in the test case.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.4.2.5 Environmental Needs**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This section describes any preexisting software or data such as a known database
    that is needed for the test. It could also describe any internet sites referenced
    by their URLs that must be active in order to execute the test case. This could
    also include any special power requirements, such as requiring a UPS to be fully
    charged before testing power failures, or it could include other conditions such
    as the swimming pool being filled with water before running tests on the SPM system.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.4.2.5.1 Hardware Environmental Needs**'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This section lists any hardware needed to run the test and specifies its configuration
    settings. It could also specify any special hardware such as a test fixture for
    the test operation. For example, a test fixture for the SPM might be a five-gallon
    bucket filled with water and a hose connected to the water feed valve that is
    part of the SPM.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.4.2.5.2 Software Environmental Needs**'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This section lists all software (and its versions/configurations) that would
    be needed to run the test. This could include operating systems/device drivers,
    dynamically linked libraries, simulators, code scaffolding (as in code drivers),^([12](ch19_footnote.xhtml#ch12fn12))
    and test tools.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.4.2.5.3 Other Environmental Needs**'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This is a catch-all section that lets you add information such as configuration
    specifics or anything else you feel the need to document. For example, for tests
    at a specific date or time, you’d need to consider Daylight Saving Time changes
    where a daily report may have 23 or 25 hours to report on, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.4.2.6 Special Procedural Requirements**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This section lists any exceptional conditions or constraints on the test case.
    This could also include any special preconditions or postconditions. For example,
    one precondition on the SPM when testing to see if the software properly responds
    to a low pool condition is that the water level is below all three low-pool sensors.
    This should also list any postconditions, such as the bucket must not have overfilled.
    If you’re using an automated test procedure, this is a good place to specify the
    particular tool to use and how to employ it for the test.
  prefs: []
  type: TYPE_NORMAL
- en: Note that this section should not duplicate steps that appear in the test procedure.
    Instead, it should provide guidance for properly writing the steps in the test
    procedure that will perform this test case.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.4.2.7 Intercase Dependencies**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This section should list (by tag identifier) any test cases that must be executed
    immediately prior to the current one, so that appropriate system state conditions
    are in place before the current test is executed. Std 829 suggests that by sequencing
    the test cases in the order in which they must execute, you can reduce the need
    to state intercase dependencies. (Obviously, such dependencies should be clearly
    documented.) In general, however, you shouldn’t rely on such implicit dependency
    organization and should explicitly document any dependencies. In the STP, though,
    you *can* rely on the ordering of test steps. Having already clearly delineated
    the execution order in the STC will help reduce errors when you create the STP.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.4.2.8 Pass/Fail Criteria**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In Std 829, the IEEE recommends putting the pass/fail criteria in the *Level*
    Test Design documentation; they are not part of the Std 829 STC. However, it’s
    not a bad idea, especially in cases where you don’t have an LTD in your documentation
    set, to include pass/fail criteria for each test case.
  prefs: []
  type: TYPE_NORMAL
- en: Note that if the pass/fail criterion is simply “All system outputs must match
    that specified by the Outcome(s) section,” then you can probably dispense with
    this section, but it wouldn’t hurt to explicitly state this default condition
    in the introduction section.
  prefs: []
  type: TYPE_NORMAL
- en: '***12.4.3 General***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This section provides a brief introduction and discussion of the Glossary and
    Document Change Procedures and History sections.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.4.3.1 Glossary**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The Glossary section provides an alphabetical list of all terms used in the
    STC. It should include all acronyms along with their definitions. Although Std
    829 lists the glossary at the end of the outline, it usually appears near the
    beginning of the document, close to the References section.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.4.3.2 Document Change Procedures and History**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'This section describes the process for creating, implementing, and approving
    changes to the STC. This could be nothing more than a reference to a Configuration
    Management Plan document that describes the document change procedures for all
    project documents or for all documents within an organization. The change history
    should contain a chronological list of the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: Document ID (each revision should have a unique ID, which can simply be a date
    affixed to the document ID)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Version number (which you should number sequentially, starting with the first
    approved version of the STC)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A description of the changes made to the STC for the current version
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authorship and role
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Often, the change history appears in the STC near the beginning of the document,
    or just after the cover page and near the document identifier.
  prefs: []
  type: TYPE_NORMAL
- en: '***12.4.4 A Sample Software Test Case Document***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Continuing with the theme of the past couple of chapters, this chapter will
    provide a sample STC for the Plantation Productions DAQ system DIP switch initialization
    design. This STC will serve as an acceptance test (pure functional test cases)
    built exclusively from the project SRS (see “(Selected) DAQ Software Requirements
    (from SRS)” on [page 219](ch10.xhtml#page_219)). The test cases appearing in this
    sample STC are all the requirements from this project SRS that have not been included
    in “Requirements to Be Verified by Review” on [page 223](ch10.xhtml#page_223)
    where the “verify by review” requirements are listed. Note, however, that for
    editorial/space reasons, this example will not provide test cases for every “verify
    by review” test requirement in that project SRS.^([13](ch19_footnote.xhtml#ch12fn13))
  prefs: []
  type: TYPE_NORMAL
- en: '| **Term** | **Definition** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| DAQ | Data acquisition system |'
  prefs: []
  type: TYPE_TB
- en: '| SBC | Single-board computer |'
  prefs: []
  type: TYPE_TB
- en: '| Software Design Description (SDD) | Documentation of the design of the software
    system (IEEE Std 1016-2009)—that is, this document. |'
  prefs: []
  type: TYPE_TB
- en: '| Software Requirements Specification (SRS) | Documentation of the essential
    requirements (functions, performance, design constraints, and attributes) of the
    software and its external interfaces (IEEE Std 610.12-1990). |'
  prefs: []
  type: TYPE_TB
- en: '| System Requirements Specification (SyRS) | A structured collection of information
    that embodies the requirements of the system (IEEE Std 1233-1998). A specification
    that documents the requirements to establish a design basis and the conceptual
    design for a system or subsystem. |'
  prefs: []
  type: TYPE_TB
- en: '| Software Test Cases (STC) | Documentation that describes test cases (inputs
    and outcomes) to verify correct operation of the software based on various design
    concerns/requirements (IEEE Std 829-2009). |'
  prefs: []
  type: TYPE_TB
- en: '| Software Test Procedures (STP) | Documentation that describes the step-by-step
    procedure to execute a set of test cases to verify correct operation of the software
    based on various design concerns/requirements (IEEE Std 829-2009). |'
  prefs: []
  type: TYPE_TB
- en: '**1 Introduction**'
  prefs: []
  type: TYPE_NORMAL
- en: Software Test Cases for DAQ DIP Switch Project
  prefs: []
  type: TYPE_NORMAL
- en: '**1.1 Document Identifier (and Change History)**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mar 22, 2018: DAQ_STC v1.0; Author: Randall Hyde'
  prefs: []
  type: TYPE_NORMAL
- en: '**1.2 Scope**'
  prefs: []
  type: TYPE_NORMAL
- en: This document describes only the DIP switch test cases in the DAQ system (for
    space/editorial reasons). For the full software design description, please see
    *[http://www.plantation-productions.com/Electronics/DAQ/DAQ.html](http://www.plantation-productions.com/Electronics/DAQ/DAQ.html)*.
  prefs: []
  type: TYPE_NORMAL
- en: '**1.3 Glossary, Acronyms, and Abbreviations**'
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*This is a very simple and short example to keep the book’s page count down.
    Please don’t use this as boilerplate; you should diligently pick out terms and
    abbreviations that your document uses and list them in this section.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**1.4 References**'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Reference** | **Discussion** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| DAQ STC | An example of a full STC for the Plantation Productions DAQ system
    can be found at *[http://www.plantation-productions.com/Electronics/DAQ/DAQ.html](http://www.plantation-productions.com/Electronics/DAQ/DAQ.html)*.
    |'
  prefs: []
  type: TYPE_TB
- en: '| IEEE Std 830-1998 | SRS documentation standard |'
  prefs: []
  type: TYPE_TB
- en: '| IEEE Std 829-2008 | STP documentation standard |'
  prefs: []
  type: TYPE_TB
- en: '| IEEE Std 1012-1998 | Software verification and validation standard |'
  prefs: []
  type: TYPE_TB
- en: '| IEEE Std 1016-2009 | SDD documentation standard |'
  prefs: []
  type: TYPE_TB
- en: '| IEEE Std 1233-1998 | SyRS documentation standard |'
  prefs: []
  type: TYPE_TB
- en: '**1.5 Context**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The DAQ system of Plantation Productions, Inc., fulfilled a need for a well-documented
    digital data acquisition and control system that engineers could design into safety-critical
    systems such as nuclear research reactors. Although there are many COTS systems^([14](ch19_footnote.xhtml#ch12fn14))
    that could be used, they suffer from a couple of major drawbacks, including: they
    are usually proprietary, thus being difficult to modify or repair after purchase;
    they are often obsolete within 5 to 10 years without a way to repair or replace
    them; and they rarely have full support documentation (for example, SRS, SDD,
    STC, and STP) that an engineer can use to validate and verify the system.'
  prefs: []
  type: TYPE_NORMAL
- en: The DAQ system overcomes this problem by providing an open hardware and open
    source set of designs with full design documentation that is validated and verified
    for safety systems.
  prefs: []
  type: TYPE_NORMAL
- en: Although originally designed for a nuclear research reactor, the DAQ system
    is useful anywhere you need an Ethernet-based control system supporting digital
    (TTL-level) I/O, optically isolated digital inputs, mechanical or solid-state
    relay digital outputs, (isolated and conditioned) analog inputs (for example,
    ±10v and 4-20mA), and (conditioned) analog outputs (±10v).
  prefs: []
  type: TYPE_NORMAL
- en: '**1.6 Notation for Description**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Test case identifiers (*tags*) in this document shall take the form:'
  prefs: []
  type: TYPE_NORMAL
- en: DAQ_STC_*xxx*_*yyy*_*zzz*
  prefs: []
  type: TYPE_NORMAL
- en: where *xxx_yyy* is a string of (possibly decimal) numbers taken from the corresponding
    requirement (for example, DAQ_SRS_*xxx*_*yyy*) and *zzz* is a (possibly decimal)
    numeric sequence that creates a unique identifier out of the whole sequence. Note
    that *zzz* values in STC tags are usually numbered from 000 or 001 and usually
    increment by 1 for each additional test case item sharing the same *xxx_yyy* string.
  prefs: []
  type: TYPE_NORMAL
- en: '**2 Details (Test Cases)**'
  prefs: []
  type: TYPE_NORMAL
- en: '**2.1 DAQ_STC_701_000_000**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Objective: Test command acceptance across RS-232.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. DIP switch 1 set to ON position.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Type `help` command on serial terminal.
  prefs: []
  type: TYPE_NORMAL
- en: 'Outcome:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Screen displays `help` message.
  prefs: []
  type: TYPE_NORMAL
- en: 'Environmental Needs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hardware** Functioning (booted) DAQ system, PC with RS-232 port connected
    to DAQ'
  prefs: []
  type: TYPE_NORMAL
- en: '**Software** Latest version of DAQ firmware installed'
  prefs: []
  type: TYPE_NORMAL
- en: '**External** Serial terminal emulator program running on PC'
  prefs: []
  type: TYPE_NORMAL
- en: 'Special Procedural Requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[None]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Intercase Dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[None]'
  prefs: []
  type: TYPE_NORMAL
- en: '**2.2 DAQ_STC_702_000_000**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Objective: Test command acceptance with DIP switch 1 ON.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. DIP switch 1 set to ON position.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Type `help` command on serial terminal.
  prefs: []
  type: TYPE_NORMAL
- en: 'Outcome:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Screen displays `help` message.
  prefs: []
  type: TYPE_NORMAL
- en: 'Environmental Needs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hardware** Functioning (booted) DAQ system, PC with RS-232 port connected
    to DAQ'
  prefs: []
  type: TYPE_NORMAL
- en: '**Software** Latest version of DAQ firmware installed'
  prefs: []
  type: TYPE_NORMAL
- en: '**External** Serial terminal emulator program running on PC'
  prefs: []
  type: TYPE_NORMAL
- en: 'Special Procedural Requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[None]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Intercase Dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: Same test as DAQ_STC_701_000_000
  prefs: []
  type: TYPE_NORMAL
- en: '**2.3 DAQ_STC_703_000_000**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Objective: Test command rejection with DIP switch 1 OFF.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. DIP switch 1 set to OFF position.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Type `help` command on serial terminal.
  prefs: []
  type: TYPE_NORMAL
- en: 'Outcome:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. System ignores command, no response on terminal program.
  prefs: []
  type: TYPE_NORMAL
- en: 'Environmental Needs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hardware** Functioning (booted) DAQ system, PC with RS-232 port connected
    to DAQ'
  prefs: []
  type: TYPE_NORMAL
- en: '**Software** Latest version of DAQ firmware installed'
  prefs: []
  type: TYPE_NORMAL
- en: '**External** Serial terminal emulator program running on PC'
  prefs: []
  type: TYPE_NORMAL
- en: 'Special Procedural Requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[None]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Intercase Dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[None]'
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*For space/editorial reasons, this sample has deleted several test cases at
    this point because they are very similar in content to the previous test cases.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**2.4 DAQ_STC_709_000_000**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Objective: Test Ethernet address with both DIP switches 5 and 6 OFF.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. DIP switch 3 set to ON position (4 = don’t care).
  prefs: []
  type: TYPE_NORMAL
- en: 2\. DIP switch 5 set to OFF position.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. DIP switch 6 set to OFF position
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Using an Ethernet terminal program, attempt connection to IP address 192.168.2.70,
    port 20560 (0x5050).
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Issue `help` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Outcome:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Ethernet terminal connects to DAQ system.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Terminal program display DAQ `help` message.
  prefs: []
  type: TYPE_NORMAL
- en: 'Environmental Needs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hardware** Functioning (booted) DAQ system, PC with Ethernet port connected
    to DAQ'
  prefs: []
  type: TYPE_NORMAL
- en: '**Software** Latest version of DAQ firmware installed'
  prefs: []
  type: TYPE_NORMAL
- en: '**External** Ethernet terminal emulator program running on PC'
  prefs: []
  type: TYPE_NORMAL
- en: 'Special Procedural Requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[None]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Intercase Dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: Cases DAQ_STC_708_000_000 to DAQ_STC_718_001_000 are closely related and should
    be performed together.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*For space/editorial reasons, this sample has deleted several test cases at
    this point because they are very similar in content to the previous test cases.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**2.6 DAQ_STC_710_000_000**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Objective: Test Ethernet address with DIP switches 5 ON and 6 OFF.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. DIP switch 3 set to ON position (4 = don’t care).
  prefs: []
  type: TYPE_NORMAL
- en: 2\. DIP switch 5 set to ON position.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. DIP switch 6 set to OFF position.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Using an Ethernet terminal program, attempt connection to IP address 192.168.2.71,
    port 20560 (0x5050).
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Issue `help` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Outcome:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Ethernet terminal connects to DAQ system.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Terminal program displays DAQ `help` message.
  prefs: []
  type: TYPE_NORMAL
- en: 'Environmental Needs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hardware** Functioning (booted) DAQ system, PC with Ethernet port connected
    to DAQ'
  prefs: []
  type: TYPE_NORMAL
- en: '**Software** Latest version of DAQ firmware installed'
  prefs: []
  type: TYPE_NORMAL
- en: '**External** Ethernet terminal emulator program running on PC'
  prefs: []
  type: TYPE_NORMAL
- en: 'Special Procedural Requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[None]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Intercase Dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: Cases DAQ_STC_708_000_000 to DAQ_STC_718_001_000 are closely related and should
    be performed together.
  prefs: []
  type: TYPE_NORMAL
- en: '**2.7 DAQ_STC_711_000_000**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Objective: Test Ethernet address with DIP switch 5 OFF and 6 ON.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. DIP switch 3 set to ON position (4 = don’t care).
  prefs: []
  type: TYPE_NORMAL
- en: 2\. DIP switch 5 set to OFF position.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. DIP switch 6 set to ON position.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Using an Ethernet terminal program, attempt connection to IP address 192.168.2.72,
    port 20560 (0x5050).
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Issue `help` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Outcome:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Ethernet terminal connects to DAQ system.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Terminal program displays DAQ `help` message.
  prefs: []
  type: TYPE_NORMAL
- en: 'Environmental Needs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hardware** Functioning (booted) DAQ system, PC with Ethernet port connected
    to DAQ'
  prefs: []
  type: TYPE_NORMAL
- en: '**Software** Latest version of DAQ firmware installed'
  prefs: []
  type: TYPE_NORMAL
- en: '**External** Ethernet terminal emulator program running on PC'
  prefs: []
  type: TYPE_NORMAL
- en: 'Special Procedural Requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[None]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Intercase Dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: Cases DAQ_STC_708_000_000 to DAQ_STC_718_001_000 are closely related and should
    be performed together.
  prefs: []
  type: TYPE_NORMAL
- en: '**2.8 DAQ_STC_712_000_000**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Objective: Test Ethernet address with both DIP switches 5 and 6 ON.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. DIP switch 3 set to ON position (4 = don’t care).
  prefs: []
  type: TYPE_NORMAL
- en: 2\. DIP switch 5 set to ON position.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. DIP switch 6 set to ON position.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Using an Ethernet terminal program, attempt connection to IP address 192.168.2.73,
    port 20560 (0x5050).
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Issue `help` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Outcome:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Ethernet terminal connects to DAQ system.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Terminal program displays DAQ `help` message.
  prefs: []
  type: TYPE_NORMAL
- en: 'Environmental Needs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hardware** Functioning (booted) DAQ system, PC with Ethernet port connected
    to DAQ'
  prefs: []
  type: TYPE_NORMAL
- en: '**Software** Latest version of DAQ firmware installed'
  prefs: []
  type: TYPE_NORMAL
- en: '**External** Ethernet terminal emulator program running on PC'
  prefs: []
  type: TYPE_NORMAL
- en: 'Special Procedural Requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[None]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Intercase Dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: Cases DAQ_STC_708_000_000 to DAQ_STC_718_001_000 are closely related and should
    be performed together.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*For space/editorial reasons, this sample has deleted several test cases at
    this point because they are very similar in content to the previous test cases.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**2.9 DAQ_STC_726_000_000**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Objective: Test command acceptance from RS-232 port.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. DIP switch 1 set to ON position.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Type `help` command on serial terminal.
  prefs: []
  type: TYPE_NORMAL
- en: 'Outcome:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Screen displays `help` message.
  prefs: []
  type: TYPE_NORMAL
- en: 'Environmental Needs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hardware** Functioning (booted) DAQ system, PC with RS-232 port connected
    to DAQ'
  prefs: []
  type: TYPE_NORMAL
- en: '**Software** Latest version of DAQ firmware installed'
  prefs: []
  type: TYPE_NORMAL
- en: '**External** Serial terminal emulator program running on PC'
  prefs: []
  type: TYPE_NORMAL
- en: 'Special Procedural Requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[None]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Intercase Dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: Same test as DAQ_STC_701_000_000
  prefs: []
  type: TYPE_NORMAL
- en: '**3 Test Case Document Change Procedure**'
  prefs: []
  type: TYPE_NORMAL
- en: When making any modifications to this STC, the author of the change must make
    a new entry in section 1.1 of this STC document, listing (at a minimum) the date,
    document ID (DAQ_STC), version number, and authorship.
  prefs: []
  type: TYPE_NORMAL
- en: '***12.4.5 Updating the RTM with STC Information***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Due to software review and software test case (and analysis/other) verification
    methods being mutually exclusive, you need only a single column in the RTM to
    associate the tags for these objects with other items in the RTM. In the RTM of
    the official DAQ system (which has only test cases and software review items),
    the label for this column is simply *Software Test/Review Cases*. When you add
    both DAQ_SR_*xxx_yyy_zzz* and DAQ_STC_*xxx_yyy_zzz* items to this column, there
    is never any ambiguity as the tag clearly identifies which verification type you’re
    using. Of course, this assumes that you’re using the tag identifier format that
    this chapter suggests. You could use your own tag format that also differentiates
    review and test case items in the tag name.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re using this chapter’s STC tag format, locating the row in the RTM where
    you want to place the test case tag is very easy. Just locate the requirement
    with the tag DAQ_SRS_*xxx_yyy* and add the STC tag to the appropriate column in
    the same row. If you’re using a different tag format that doesn’t include requirement
    traceability directly in the tag name, you’ll have to determine the association
    manually (hopefully it’s contained within the test case).
  prefs: []
  type: TYPE_NORMAL
- en: '**12.5 Software Test Procedure Documentation**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The *Software Test Procedure (STP)* specifies the steps for executing a collection
    of test cases, which, in turn, evaluate the quality of the software system. In
    one respect, the STP is an optional document; after all, if you execute all the
    test cases (in an appropriate order), you will fully test all the test cases.
    The purpose behind an STP is to streamline the testing process. More often than
    not, test cases overlap. Although they test different requirements, it may turn
    out that the inputs for multiple test cases are identical. In some cases, even
    the outcomes are identical. By merging such test cases into a single procedure,
    you can run a single test sequence that handles all test cases.
  prefs: []
  type: TYPE_NORMAL
- en: Another reason for merging test cases into a single STP is the convenience of
    a common setup. Many test cases require (possibly elaborate) setup to ensure certain
    environmental conditions prior to execution. More often than not, multiple test
    cases require the same setup prior to their execution. By merging those test cases
    into a single procedure, you can perform the setup once for the entire set rather
    than repeating it for each and every test case.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, some test cases may have dependencies that require other test cases
    to execute prior to their execution. By putting these test cases in a test procedure,
    you can ensure that the test operation satisfies the dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Std 829 defines a set of Level *Test Procedures (LTPr)*. As with all of the
    *level* test documents in Std 829 there are four variants of the LTPr, each variant
    being a document generally describing the scope or extent of software testing
    being documented: Component Test Procedures (aka Unit Test Procedures, or UTP),
    Component Integration Test Procedures (aka Integration Test Procedures, or ITP),
    System Test Procedures (aka System Integration Test Procedures, or SITP), and
    Acceptance Test Procedures (ATP; may include Factory Acceptance Test Procedures
    [FATP] or Site Acceptance Test Procedures [SATP]).^([15](ch19_footnote.xhtml#ch12fn15))'
  prefs: []
  type: TYPE_NORMAL
- en: UTPs and ITPs are often automated test procedures or less formal documents,
    similar to their test case document counterparts; see “[Software Test Case Documentation](ch12.xhtml#lev-12.4)”
    on [page 274](ch12.xhtml#page_274) for an in-depth discussion.
  prefs: []
  type: TYPE_NORMAL
- en: If you look back at [Figures 12-1](ch12.xhtml#ch12fig1) and [12-2](ch12.xhtml#ch12fig2),
    you can see that the STP (and all LTPrs) are derived directly from the STC (LTC)
    documentation. [Figure 12-1](ch12.xhtml#ch12fig1) applies to UTPs and ITPs. [Figure
    12-2](ch12.xhtml#ch12fig2) applies to SITPs and ATPs (noting that ATPs derive
    from test cases that come strictly from SyRS/SRS requirements, not from SDD elements).
  prefs: []
  type: TYPE_NORMAL
- en: As is true for test case documentation, ATPs are usually a subset of the SITPs
    to the customer or end user. Likewise, if there are FATP and SATP documents, the
    SATP is often a subset of the FATP, with further refinement to end-user requirements.^([16](ch19_footnote.xhtml#ch12fn16))
  prefs: []
  type: TYPE_NORMAL
- en: '***12.5.1 The IEEE Std 829-2009 Software Test Procedure***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The outline for the Std 829 STP is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 Document Identifier
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 Scope
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 References
  prefs: []
  type: TYPE_NORMAL
- en: 1.4 Relationship to Other Documents
  prefs: []
  type: TYPE_NORMAL
- en: 2 Details
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Inputs, Outputs, and Special Requirements
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Ordered Description of the Steps to Be Taken to Execute the Test Cases
  prefs: []
  type: TYPE_NORMAL
- en: 3 General
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Glossary
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Document Change Procedures and History
  prefs: []
  type: TYPE_NORMAL
- en: '*12.5.2 Extended Outline for Software Test Procedure*'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As is typical for IEEE standards, you’re allowed to augment this outline (adding,
    deleting, moving, and editing items, with appropriate justification). This flexibility
    is important in this particular case because there are a couple of things missing
    from this outline.
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, the introduction is missing Notation for Descriptions, which
    appears in the STC outline (“Software Test Case Documentation” on [page 274](ch12.xhtml#page_274)).^([17](ch19_footnote.xhtml#ch12fn17))
    Perhaps the authors of Std 829 were expecting very few test procedures to appear
    in Section 2 (“Details”) of the document. In practice, however, it’s common to
    have a large number of test procedures. There are some very good reasons for breaking
    a single large test procedure into a series of smaller ones:'
  prefs: []
  type: TYPE_NORMAL
- en: Testing can take place in parallel. By assigning (independent) test procedures
    to multiple test teams, you can complete the testing faster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Certain tests may tie up resources (for example, test equipment such as oscilloscopes,
    logic analyzers, test fixtures, and signal generators). By breaking up a large
    test procedure into smaller test procedures, you may be able to limit the time
    a testing team needs access to certain resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s nice to be able to complete a test procedure within a single working day
    (or even between breaks in the day) so testers don’t lose focus when performing
    tests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Organizing test procedures by their related activities (and by required setup
    prior to those activities) can streamline test procedures, reducing steps and
    making them more efficient to run.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many organizations require a testing team to rerun a test procedure from the
    beginning (a regression test) if any part of that test fails. Breaking a test
    procedure into smaller pieces makes rerunning test procedures far less expensive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To be able to trace these test procedures back to the STC, to the SRS, and to
    other documentation in the RTM, you’re going to need test procedure identifiers
    (tags). Therefore, you should have a section to describe the notation you’re using
    for these tags.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, the second thing missing from the IEEE outline is an entry for the
    test procedure identification in the *Details* section. To make traceability easier,
    it would also be nice to have a section in each test procedure where you list
    the associated test cases it covers. Finally, for my own purposes, I like to include
    the following information with each test procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: Brief description
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tag/identification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Purpose
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Traceability (test cases covered)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pass/fail criteria (as this may change with each procedure)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any special requirements (for example, environmental) required to run this test
    procedure; this could include input/output files that must exist, among other
    things
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All setup required prior to running the test procedure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Software version number when executing the test procedure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Procedure steps to execute the test procedure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Incorporating these items produces the following extended outline for an arbitrary
    STP suitable for an SIT, AT, FAT, or SAT:'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Table of Contents
  prefs: []
  type: TYPE_NORMAL
- en: 2 Introduction
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Document Identifier and Change History (moved)
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Scope
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Glossary, Acronyms, and Abbreviations (moved)
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 References
  prefs: []
  type: TYPE_NORMAL
- en: 2.5 Notation for Descriptions
  prefs: []
  type: TYPE_NORMAL
- en: 2.6 Relationship to Other Documents (removed)
  prefs: []
  type: TYPE_NORMAL
- en: 2.7 Instructions for Running the Tests (added)
  prefs: []
  type: TYPE_NORMAL
- en: 3 Test Procedures (name changed from *Details*)
  prefs: []
  type: TYPE_NORMAL
- en: '3.1 Brief Description (simple phrase), Procedure #1'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1 Procedure Identification (Tag)
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2 Purpose
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.3 List of Test Cases Covered by This Procedure
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.4 Special Requirements
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.5 Setup Required Prior to Running Procedure
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.6 Software Version Number for This Execution
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.7 Detailed Steps to Run the Procedure
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.8 Sign-off on Test Procedure
  prefs: []
  type: TYPE_NORMAL
- en: '3.2 Brief Description (simple phrase), Procedure #2'
  prefs: []
  type: TYPE_NORMAL
- en: (Same subsections as previous section)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: . . .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '3.*n* Brief Description (simple phrase), Procedure #*n*'
  prefs: []
  type: TYPE_NORMAL
- en: (Same subsections as previous sections)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4 General
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Document Change Procedures
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Attachments and Appendixes
  prefs: []
  type: TYPE_NORMAL
- en: 5 Index
  prefs: []
  type: TYPE_NORMAL
- en: '***12.5.3 Introduction in the STP Document***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The following subsections describe the components of the STP introduction.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.5.3.1 Document Identifier and Change History**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The document identifier should be some (organization-wide) unique name; this
    will typically include some project designation such as *DAQ_STP*, a creation/modification
    date, a version number, and authorship. A list of these identifiers (one for each
    revision to the document) would form the change history.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.5.3.2 Scope**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The scope here has largely the same definition as that used for the STC (see
    “[Software Test Case Documentation](ch12.xhtml#lev-12.4)” on [page 274](ch12.xhtml#page_274)).
    Std 829 suggests describing the scope of the STP based on its focus and relationship
    to the STC and other test documentation. More often than not, you can get away
    with a simple reference to the Scope section in the STC.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.5.3.3 References**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As usual, provide a link to any external documents (such as the STC) that are
    relevant to the STP. Std 829 also suggests including links to the individual test
    cases covered by this procedure. That, however, would be meaningful only if the
    STP contained just a few test procedures. In this revised format, the STP will
    attach the test case links to the individual test procedures in Section 3 (“Test
    Procedures”). If you have a very large system consisting of multiple, independent
    applications, you will probably have separate STPs for each of those applications.
    You would want to provide links to those other STPs in this section of the STP
    document.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.5.3.4 Notation for Descriptions**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As in the STC, you would describe your STP tag format here. This book recommends
    using STP tags of the form *proj*_STP_*xxx*, where *proj* is some project-specific
    ID (such as *DAQ* or *POOL*) and *xxx* is some unique (possibly decimal) numeric
    sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Note that there is a many-to-one relationship from STC test cases to STP test
    procedures. Therefore, you cannot easily embed traceability information into the
    STP tags (there’s a similar situation with SDD tags; see “[SDD Traceability and
    Tags](ch11.xhtml#lev-11.4)” on [page 245](ch11.xhtml#page_245)). This is why it’s
    important to include the related STC tags with each test procedure, to facilitate
    traceability back to the corresponding test cases.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.5.3.5 Relationship to Other Documents**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In the modified variant of the STP, I’ve removed this section. Std 829 suggests
    using it to describe the relationship of this STP to other test procedure documents—specifically,
    which test procedures must be performed before or after other test procedures.
    However, in the modified form all test procedures appear in the same document.
    Therefore, a description of the relationship between tests should accompany each
    individual test procedure. (This information appears in the “Special Requirements”
    section.)
  prefs: []
  type: TYPE_NORMAL
- en: 'This is one reason for including this section in the modified form of the STP:
    very large systems may contain multiple (and relatively independent) software
    applications. There would probably be separate STP documents for each of these
    applications. This section of the modified STP could describe the relationship
    of this STP to those others, including the order in which tests must execute these
    STPs.'
  prefs: []
  type: TYPE_NORMAL
- en: '**12.5.3.6 Instructions for Running Tests**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This section should contain generic instructions to whomever will be running
    the tests. Usually the people running the tests are not the software developers.^([18](ch19_footnote.xhtml#ch12fn18))
    This section can provide insights into the software to be tested for those who
    have not lived with it on a daily basis from its inception.
  prefs: []
  type: TYPE_NORMAL
- en: One important piece of information that should appear here is what to do if
    a test procedure fails. Should the tester attempt to continue that test procedure
    (if possible) in hopes of finding additional problems? Should the tester immediately
    suspend testing operations until the development team resolves the issue? If a
    test has been suspended, what is the process for resuming the test? For example,
    most QA teams require, at the very least, rerunning the test procedure from the
    beginning.^([19](ch19_footnote.xhtml#ch12fn19)) Some QA teams may also require
    a meeting with development to determine a set of regression tests to run before
    resuming the test procedure from the point of failure.
  prefs: []
  type: TYPE_NORMAL
- en: This section should also discuss how to log any problems/anomalies that occur
    during testing and to describe how to bring the system back into a stable state
    or shut it down should a critical or catastrophic event occur.
  prefs: []
  type: TYPE_NORMAL
- en: This is also where you’ll describe how to log successful runs of a test procedure.
    A tester will usually log the date and time they begin a test, provide the name
    of the test engineer, and specify which test procedure they are executing. At
    the successful conclusion of a test, most test procedures require signatures by
    the test engineer, a possible QA or customer representative, and possibly other
    managerial or project-related personnel. This section should describe the process
    for obtaining these signatures and signing off on successful runs of a test procedure.
  prefs: []
  type: TYPE_NORMAL
- en: '***12.5.4 Test Procedures***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This section of the document repeats for each individual test procedure for
    the system under test. This is a modification of the Std 829 STP, which describes
    only a single (or maybe a few) test procedures in the document. Presumably, there
    would be multiple STP documents if your system requires a large number of test
    procedures.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.5.4.1 Brief Description (for Test Procedure #1)**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'This is the title of the test procedure. It should be a short phrase, such
    as *DIP Switch #1 Test*, that provides a quick and perhaps informal procedure
    identification.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Procedure Identification**'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This is the unique identifier (tag) for this test procedure. Other documentation
    (such as the RTM) will reference this test procedure using its tag.
  prefs: []
  type: TYPE_NORMAL
- en: '**Purpose**'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This is an expanded description of this test procedure: why it exists, what
    it tests, and where it sits in the big picture.'
  prefs: []
  type: TYPE_NORMAL
- en: '**List of Test Cases Covered by This Procedure**'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This section provides reverse traceability back to the STC document. It is simply
    a list of all the test cases that this test procedure covers. Note that this set
    of test cases should be mutually exclusive of the sets found in other test procedures—no
    test case tag should ever appear in more than one test procedure. You want to
    preserve the many-to-one relationship from test cases to test procedures. This
    will help keep the RTM clean, meaning that you won’t have to attach multiple test
    procedures to the same row in the RTM.
  prefs: []
  type: TYPE_NORMAL
- en: Now, it is quite possible that multiple test procedures will provide inputs
    (and verify corresponding outcomes) that test the same test case. This isn’t a
    problem; just pick one procedure that will take credit for covering that test
    case and assign the test case to that procedure. When someone is tracing through
    the requirements and verifying that the test procedures test a particular requirement,
    they’re not going to care if the test procedures test that requirement multiple
    times; they’ll be interested only in determining that the requirement has been
    tested at least once somewhere in the test procedures.
  prefs: []
  type: TYPE_NORMAL
- en: If you have a choice of test procedures with which to associate a given test
    case, it’s best to include that test case in a test procedure that also handles
    related test cases. Of course, in general, this type of association, whereby related
    test cases are put into the same test procedure, happens automatically. That’s
    because you don’t arbitrarily create test procedures and then assign test cases
    to them. Instead, you pick a set of (related) test cases and use them to generate
    a test procedure.
  prefs: []
  type: TYPE_NORMAL
- en: '**Special Requirements**'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This section identifies anything external you’ll need for the test procedure
    in order to successfully execute the test. This includes databases, input files,
    existing directory paths, online resources (such as web pages), dynamically linked
    libraries and other third-party tools, and automated test procedures.
  prefs: []
  type: TYPE_NORMAL
- en: '**Setup Required Prior to Running Procedure**'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This section describes any processes or procedures to execute before you can
    run the test procedure. For example, a test procedure for autonomous vehicle software
    might require an operator to drive the vehicle to a specified starting point on
    a test track before starting the test. Other examples might be ensuring an internet
    or server connection is available. With the SPM, an example of setup could include
    ensuring that the test fixture (five-gallon bucket of water) is filled to some
    specified level.
  prefs: []
  type: TYPE_NORMAL
- en: '**Software Version Number for This Execution**'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This is a “fill in the blank” field for the test procedure. It does not mandate
    a software version for running the test; rather, the tester enters the current
    software version number prior to the test’s execution. Note that this field has
    to be filled in for each test procedure. You cannot simply write this value down
    once for the whole STP. The reason is quite simple: during testing you may encounter
    defects that require you to suspend the test. Once the development team corrects
    those defects, the testing can resume, usually from the beginning of the test
    procedure. Because different procedures in an STP could have been run on different
    versions of the software, you need to identify which version of the software you’re
    using when running each procedure.^([20](ch19_footnote.xhtml#ch12fn20))'
  prefs: []
  type: TYPE_NORMAL
- en: '**Detailed Steps Required to Run This Procedure**'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This section contains steps that are necessary to execute the test procedure.
    There are two types of steps in a test procedure: actions and verifications. An
    *action* is a statement of work to be done, such as providing some input to the
    system. A *verification* involves checking some outcome/output and confirming
    that the system is operating correctly.'
  prefs: []
  type: TYPE_NORMAL
- en: You must number all procedure steps sequentially—typically starting from 1,
    though you could also use section numbers like 3.2.1 through 3.2.40 for a test
    procedure that has 40 steps. At the very least, each verification step should
    be preceded by three or so underline characters (___) or a box symbol (see [Figure
    12-3](ch12.xhtml#ch12fig3)) so that the tester can physically check off the step
    once they have successfully completed it. Some people prefer putting the checkbox
    on every item (that is, both actions and verifications) in the test procedure
    to ensure that the tester marks off each step as they complete it. Perhaps there
    should be lines on the actions and checkboxes on the verifications. However, this
    adds considerable menial work to the process, so consider carefully whether it’s
    important enough to do.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/fig12-3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-3: Using a checkbox on a verify statement*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the detailed steps should include information (in appropriate positions)
    such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Any actions needed to start the procedure (obviously, these should appear in
    the first few steps of the procedure)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A discussion of how to make measurements or observe outputs (don’t assume the
    tester is as familiar with the software as the developers are)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to shut down the system at the conclusion of the test procedure to leave
    the system in a stable state (if this is necessary, it will obviously appear in
    the last steps of the procedure)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sign-off
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the end of the test procedure there should be blank lines for the tester,
    observers, customer representatives, and possibly management personnel to sign
    off on the successful conclusion of the test procedure. A signature and date are
    the minimum information that should appear here. Each organization may mandate
    which signatures are necessary. At the very least (such as in a one-person shop),
    whoever executes the test procedure should sign and date it to affirm that it
    was run.
  prefs: []
  type: TYPE_NORMAL
- en: '***12.5.5 General***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The last section of an STP is a generic catch-all section where you can place
    information that doesn’t fit anywhere else.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.5.5.1 Document Change Procedures**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Many organizations have set policies for changing test procedure documents.
    They could, for example, require customer approval before making official changes
    to an ATP. This section outlines the rules and necessary approval procedures and
    processes for making changes to the STP.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.5.5.2 Attachments and Appendixes**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: It’s often useful to attach large tables, images, and other documentation directly
    to the LTP so that it is always available to a reader, as opposed to providing
    a link to a document that the reader cannot access.
  prefs: []
  type: TYPE_NORMAL
- en: '***12.5.6 Index***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If desired, you can add an index at the end of the STP.
  prefs: []
  type: TYPE_NORMAL
- en: '***12.5.7 A Sample STP***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This section presents a shortened (for space/editorial purposes) example of
    an STP for the DAQ DIP switch project.
  prefs: []
  type: TYPE_NORMAL
- en: '**1 Table of Contents**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Omitted for space reasons]'
  prefs: []
  type: TYPE_NORMAL
- en: '**2 Introduction**'
  prefs: []
  type: TYPE_NORMAL
- en: '**2.1 Document Identifier**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mar 22, 2018: DAQ_LTP, Version 1.0 Randall Hyde'
  prefs: []
  type: TYPE_NORMAL
- en: '**2.2 Scope**'
  prefs: []
  type: TYPE_NORMAL
- en: This document describes some of the DIP switch test procedures in the DAQ system
    (shortened for space/editorial reasons).
  prefs: []
  type: TYPE_NORMAL
- en: '**2.3 Glossary, Acronyms, and Abbreviations**'
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*This is a very simple and short example to keep this book smaller. Please
    don’t use this as boilerplate; you should diligently pick out terms and abbreviations
    your document uses and list them in this section.*'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Term** | **Definition** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| DAQ | Data acquisition system |'
  prefs: []
  type: TYPE_TB
- en: '| SBC | Single-board computer |'
  prefs: []
  type: TYPE_TB
- en: '| Software Design Description (SDD) | Documentation of the design of the software
    system (IEEE Std 1016-2009)—that is, this document. |'
  prefs: []
  type: TYPE_TB
- en: '| Software Requirements Specification (SRS) | Documentation of the essential
    requirements (functions, performance, design constraints, and attributes) of the
    software and its external interfaces (IEEE Std 610.12-1990). |'
  prefs: []
  type: TYPE_TB
- en: '| System Requirements Specification (SyRS) | A structured collection of information
    that embodies the requirements of the system (IEEE Std 1233-1998). A specification
    that documents the requirements to establish a design basis and the conceptual
    design for a system or subsystem. |'
  prefs: []
  type: TYPE_TB
- en: '| Software Test Cases (STC) | Documentation that describes test cases (inputs
    and outcomes) to verify correct operation of the software based on various design
    concerns/requirements (IEEE Std 829-2009). |'
  prefs: []
  type: TYPE_TB
- en: '| Software Test Procedures (STP) | Documentation that describes the step-by-step
    procedure to execute a set of test cases to verify correct operation of the software
    based on various design concerns/requirements (IEEE Std 829-2009). |'
  prefs: []
  type: TYPE_TB
- en: '**2.4 References**'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Reference** | **Discussion** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| DAQ STC | See “[A Sample Software Test Case Document](ch12.xhtml#lev-12.4.4)”
    on [page 281](ch12.xhtml#page_281). |'
  prefs: []
  type: TYPE_TB
- en: '| DAQ STP | An example of a full STP for the Plantation Productions DAQ system
    can be found at *[http://www.plantation-productions.com/Electronics/DAQ/DAQ.html](http://www.plantation-productions.com/Electronics/DAQ/DAQ.html)*.
    |'
  prefs: []
  type: TYPE_TB
- en: '| IEEE Std 830-1998 | SRS documentation standard |'
  prefs: []
  type: TYPE_TB
- en: '| IEEE Std 829-2008 | STP documentation standard |'
  prefs: []
  type: TYPE_TB
- en: '| IEEE Std 1012-1998 | Software verification and validation standard |'
  prefs: []
  type: TYPE_TB
- en: '| IEEE Std 1016-2009 | SDD documentation standard |'
  prefs: []
  type: TYPE_TB
- en: '| IEEE Std 1233-1998 | SyRS documentation standard |'
  prefs: []
  type: TYPE_TB
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*An additional reference that might make sense (not included here because it
    doesn’t exist for this simple project) is a link to any associated documentation
    for the DAQ system, such as programming manuals or schematics.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**2.5 Notation for Descriptions**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Test procedure identifiers (*tags*) in this document shall take the form:'
  prefs: []
  type: TYPE_NORMAL
- en: DAQ_STP_*xxx*
  prefs: []
  type: TYPE_NORMAL
- en: where *xxx* is a (possibly dotted decimal) numeric sequence that creates a unique
    identifier out of the whole sequence. Note that *xxx* values for STP tags are
    usually numbered from 000 or 001 and usually increment by 1 for each additional
    test case item sharing the same *xxx* string.
  prefs: []
  type: TYPE_NORMAL
- en: '**2.6 Instructions for Running the Tests**'
  prefs: []
  type: TYPE_NORMAL
- en: Execute each test procedure exactly as stated. If tester encounters an error
    or omission in the procedure, tester should redline (with red ink, which tester
    should use only for redlines) the procedure with the correct information and justify
    the redline in the test log (with date/timestamp and signature). All redlines
    within the test procedure(s) must be initialized by all signatories at the end
    of the test procedure.
  prefs: []
  type: TYPE_NORMAL
- en: If tester discovers a defect in the software itself (that is, not simply a defect
    in the test procedure), the tester shall note the anomaly in a test log and create
    an Anomaly Report for the defect. If the defect is marginal or negligible in nature,
    the tester may continue with the test procedure, if possible, and attempt to find
    any other defects in the system on the same test procedure run. If the defect
    is critical or catastrophic in nature, or the defect is such that it is impossible
    to continue the test procedure, the tester shall immediately suspend the test
    and shut off power to the system. Once the defect is corrected, tester must restart
    the test procedure from the beginning of the procedure.
  prefs: []
  type: TYPE_NORMAL
- en: A test procedure succeeds if and only if the tester completes all steps without
    any failures.
  prefs: []
  type: TYPE_NORMAL
- en: '**3 Test Procedures**'
  prefs: []
  type: TYPE_NORMAL
- en: '**3.1 RS-232 (Serial Port) Operation**'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1 DAQ_STP_001
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2 Purpose
  prefs: []
  type: TYPE_NORMAL
- en: This test procedure tests the proper operation of DAQ commands sourced from
    the RS-232 port.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.3 Test Cases
  prefs: []
  type: TYPE_NORMAL
- en: DAQ_STC_701_000_000
  prefs: []
  type: TYPE_NORMAL
- en: DAQ_STC_702_000_000
  prefs: []
  type: TYPE_NORMAL
- en: DAQ_STC_703_000_000
  prefs: []
  type: TYPE_NORMAL
- en: DAQ_STC_726_000_000
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.4 Special Requirements
  prefs: []
  type: TYPE_NORMAL
- en: This test procedure requires a serial terminal emulator program running on a
    PC (for example, the *MTTY.exe* program that comes as part of the Netburner SDK;
    you could even use Hyperterm if you are masochistic). There should be a NULL modem
    cable between the PC’s serial port and the COM1 port on the Netburner.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.5 Setup Required Prior to Running
  prefs: []
  type: TYPE_NORMAL
- en: Netburner powered up and running application software. Serial terminal program
    should be properly connected to the serial port on the PC that is wired to the
    Netburner.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.6 Software Version Number
  prefs: []
  type: TYPE_NORMAL
- en: 'Version number: ____________'
  prefs: []
  type: TYPE_NORMAL
- en: 'Date: ____________'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.7 Detailed Steps
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Set DIP switch 1 to the ON position.
  prefs: []
  type: TYPE_NORMAL
- en: '2\. Reset the Netburner and wait several seconds for it to finish rebooting.
    Note: Rebooting Netburner may produce information on the serial terminal. You
    can ignore this.'
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Press ENTER on the line by itself into the terminal emulator.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. ______ Verify that the DAQ system responds with a newline without any other
    output
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Type `help`, then press ENTER on a line by itself.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. ______ Verify that the DAQ software responds with a help message (contents
    unimportant as long as it is obviously a help response).
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Set DIP switch 1 to the OFF position.
  prefs: []
  type: TYPE_NORMAL
- en: '8\. Reset the Netburner and wait several seconds for it to finish rebooting.
    Note: Rebooting Netburner may produce information on the serial terminal. You
    can ignore this.'
  prefs: []
  type: TYPE_NORMAL
- en: 9\. Type the help command into the serial terminal.
  prefs: []
  type: TYPE_NORMAL
- en: 10\. ______ Verify that the DAQ system ignores the help command.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.8 Sign-off on Test Procedure
  prefs: []
  type: TYPE_NORMAL
- en: 'Tester: _________________ Date: _________'
  prefs: []
  type: TYPE_NORMAL
- en: 'QA: _________________ Date: _________'
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*In a full STP document, there would probably be additional test procedures
    here; the following test procedure ignores that possibility and continues tag
    numbering with DAQ_STP_002.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**3.2 Ethernet Address Selection**'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 DAQ_STP_002
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Purpose
  prefs: []
  type: TYPE_NORMAL
- en: This test procedure tests the initialization of the Ethernet IP address based
    on DIP switches 5 and 6.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.3 Test Cases
  prefs: []
  type: TYPE_NORMAL
- en: DAQ_STC_709_000_000
  prefs: []
  type: TYPE_NORMAL
- en: DAQ_STC_710_000_000
  prefs: []
  type: TYPE_NORMAL
- en: DAQ_STC_711_000_000
  prefs: []
  type: TYPE_NORMAL
- en: DAQ_STC_712_000_000
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.4 Special Requirements
  prefs: []
  type: TYPE_NORMAL
- en: This test procedure requires an Ethernet terminal emulator program running on
    a PC (*Hercules.exe* has been a good choice in the past). There should be an Ethernet
    (crossover or through a hub) cable between the PC’s Ethernet port and the Ethernet
    port on the Netburner.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.5 Setup Required Prior to Running
  prefs: []
  type: TYPE_NORMAL
- en: Netburner powered up and running application software. DIP switch 3 in the ON
    position. DIP switch 4 in the OFF position.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.6 Software Version Number
  prefs: []
  type: TYPE_NORMAL
- en: 'Version number: _________'
  prefs: []
  type: TYPE_NORMAL
- en: 'Date: _________'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.7 Detailed Steps
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Set DIP switches 5 and 6 to the OFF position.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Reset the Netburner and wait several seconds for it to finish rebooting.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. From the Ethernet terminal program, attempt to connect to the Netburner
    at IP address 192.168.2.70, port 20560 (0x5050).
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Verify that the connection was successful.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Enter a `help` command and press the ENTER key.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. ______ Verify that the DAQ system responds with an appropriate help message.
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Set DIP switch 5 to the ON position and 6 to the OFF position.
  prefs: []
  type: TYPE_NORMAL
- en: 8\. Reset the Netburner and wait several seconds for it to finish rebooting.
  prefs: []
  type: TYPE_NORMAL
- en: 9\. From the Ethernet terminal program, attempt to connect to the Netburner
    at IP address 192.168.2.71, port 20560 (0x5050).
  prefs: []
  type: TYPE_NORMAL
- en: 10\. ______ Verify that the connection was successful.
  prefs: []
  type: TYPE_NORMAL
- en: 11\. Enter a `help` command and press the ENTER key.
  prefs: []
  type: TYPE_NORMAL
- en: 12\. ______ Verify that the DAQ system responds with an appropriate help message.
  prefs: []
  type: TYPE_NORMAL
- en: 13\. Set DIP switch 5 to the OFF position and 6 to the ON position.
  prefs: []
  type: TYPE_NORMAL
- en: 14\. Reset the Netburner and wait several seconds for it to finish rebooting.
  prefs: []
  type: TYPE_NORMAL
- en: 15\. From the Ethernet terminal program, attempt to connect to the Netburner
    at IP address 192.168.2.72, port 20560 (0x5050).
  prefs: []
  type: TYPE_NORMAL
- en: 16\. ______ Verify that the connection was successful.
  prefs: []
  type: TYPE_NORMAL
- en: 17\. Enter a `help` command and press the ENTER key.
  prefs: []
  type: TYPE_NORMAL
- en: 18\. ______ Verify that the DAQ system responds with an appropriate help message.
  prefs: []
  type: TYPE_NORMAL
- en: 19\. Set DIP switches 5 and 6 to the ON position.
  prefs: []
  type: TYPE_NORMAL
- en: 20\. Reset the Netburner and wait several seconds for it to finish rebooting.
  prefs: []
  type: TYPE_NORMAL
- en: 21\. From the Ethernet terminal program, attempt to connect to the Netburner
    at IP address 192.168.2.73, port 20560 (0x5050).
  prefs: []
  type: TYPE_NORMAL
- en: 22\. ______ Verify that the connection was successful.
  prefs: []
  type: TYPE_NORMAL
- en: 23\. Enter a `help` command and press the ENTER key.
  prefs: []
  type: TYPE_NORMAL
- en: 24\. ______ Verify that the DAQ system responds with an appropriate help message.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.8 Sign-off on Test Procedure
  prefs: []
  type: TYPE_NORMAL
- en: 'Tester: _________________ Date: _________'
  prefs: []
  type: TYPE_NORMAL
- en: 'QA: _________________ Date: _________'
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*In a full STP document, there would probably be additional test procedures
    here.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**4 General**'
  prefs: []
  type: TYPE_NORMAL
- en: '**4.1 Document Change Procedures**'
  prefs: []
  type: TYPE_NORMAL
- en: Whenever making changes to this document, add a new line to Section 2.1 listing,
    at a minimum, the date, project name (*DAQ_STP*), version number, and authorship.
  prefs: []
  type: TYPE_NORMAL
- en: '**4.2 Attachments and Appendixes**'
  prefs: []
  type: TYPE_NORMAL
- en: '[In the interests of space, none are provided here; in a real STP, putting
    the schematic of the DAQ system would be a good idea.]'
  prefs: []
  type: TYPE_NORMAL
- en: '**5 Index**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Omitted for space reasons.]'
  prefs: []
  type: TYPE_NORMAL
- en: '***12.5.8 Updating the RTM with STP Information***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Because STP tags are very similar in nature to SDD tags, it should come as no
    surprise that the process for adding STP tags to the RTM is quite similar to that
    for adding SDD tags (see “[Updating the Traceability Matrix with Design Information](ch11.xhtml#lev-11.7)”
    on [page 259](ch11.xhtml#page_259)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The STP adds a single column to the RTM: the STP tag column. Unfortunately,
    the STP tag does not directly embed any traceability information, so you’ll have
    to extract that information from the STP to determine where to place STP tags
    in the RTM.'
  prefs: []
  type: TYPE_NORMAL
- en: As you may recall from “List of Test Cases Covered by This Procedure” on [page
    294](ch12.xhtml#page_294), each test procedure in an STP must include the list
    of test cases it covers. Though Std 829 does not require this, I strongly suggest
    that you include this section. If you’ve done that, you’ve already created the
    reverse traceability back to the requirements, which makes it easy to fill in
    the STP tags in the RTM. To do so, just locate each test case tag (listed in the
    current test procedure) and copy the test procedure’s STP tag into the STP tag
    column in the RTM (on the same row as the corresponding test case). Of course,
    because there are multiple test cases associated with a single test procedure,
    you’ll also have several copies of the same STP tag spread throughout the RTM
    (one per associated test case).
  prefs: []
  type: TYPE_NORMAL
- en: Should you ever want to easily trace your STP tags back to all the requirements
    in the RTM, particularly without having to look up the list in the STP, simply
    sort the RTM by the STP tag column. This will collect all the requirements (and
    everything else linked to that STP tag) into a contiguous group in the matrix
    and make it easy to identify everything associated with that tag.
  prefs: []
  type: TYPE_NORMAL
- en: If you choose some other method of specifying test cases in the test procedure
    that doesn’t involve incorporating the STC tags within the test procedures, then
    determining the placement of the STP tags in the RTM becomes a manual—and often
    laborious—process. That’s why I strongly recommend including STC tag numbers in
    a test procedure when you first create it.
  prefs: []
  type: TYPE_NORMAL
- en: 12.6 *Level* Test Logs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although each test procedure contains a signature section where the tester (and
    any other desired personnel) can sign off on a successful test completion, a separate
    test log is needed to handle anomalies that occur during testing or to simply
    hold comments and concerns that the tester may have while running the test procedure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perhaps the most important job of this Level *Test Log (LTL)* is to present
    a chronological view of the testing process. Whenever a tester begins running
    a test procedure, they should first log an entry stating the date, time, test
    procedure they are executing, and their name. Throughout the test’s execution,
    the tester can add entries to the test log (as necessary) indicating:'
  prefs: []
  type: TYPE_NORMAL
- en: Start of a test procedure (date/time)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: End of a test procedure (date/time)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anomalies/defects found (and whether the test was continued or suspended)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Redlines/changes needed to the test procedure because of errors found in the
    procedure itself (for example, the test procedure could list an incorrect outcome;
    if the tester can show that the program output was correct even if it differs
    from the test procedure, they would redline the test procedure and add an appropriate
    justification to the test log)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concerns about outcomes the program produces that the tester finds questionable
    (perhaps the test procedure doesn’t list any outcome, or the test procedure’s
    outcomes are questionable)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Personnel changes (for example, if a tester changes in the middle of a test
    due to a break, shift change, or different experience needed)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any break period during the test procedure (for example, lunch break or end
    of the workday)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Technically, all you need for a test log is a sheet of (preferably lined) paper.
    More often than not, STP creators add several sheets of lined paper to the end
    of the STP specifically for this test log. Some organizations simply maintain
    the test log electronically using a word processor or text editor (or even a specially
    written application). Of course, Std 829 outlines a formal recommendation for
    test logs:'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 Document Identifier
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 Scope
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 References
  prefs: []
  type: TYPE_NORMAL
- en: 2 Details
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Description
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Activity and Event Entries
  prefs: []
  type: TYPE_NORMAL
- en: 3 General
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Glossary
  prefs: []
  type: TYPE_NORMAL
- en: '*12.6.1 Introduction in the* Level *Test Logs Document*'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In addition to introducing the subsections that follow, this section might also
    identify the organization that created the document and the current status.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.6.1.1 Document Identifier**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A unique identifier for this document; as with all Std 829 documents this should
    include, at the very least, the date, some descriptive name, a version number,
    and authorship. A change history (of the outline/format, not the specific log)
    might appear here as well.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.6.1.2 Scope**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The Scope section summarizes the system and features that the associated test
    procedure tested. Generally, this would be a reference to the test procedure’s
    Scope section unless there was something special about this particular test run.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.6.1.3 References**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: At the very least, this section should refer to the STP (and in particular,
    the specific test) document for which this test log was created.
  prefs: []
  type: TYPE_NORMAL
- en: '***12.6.2 Details***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This section introduces the following subsections and is what most people would
    consider the actual “test log.”
  prefs: []
  type: TYPE_NORMAL
- en: '**12.6.2.1 Description**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'This section (only one occurrence per test log) describes items that will apply
    to all test log entries. This could include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Identification of the test subject (for example, by version number)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identification of any changes made to the test procedure (for example, redlines)
    prior to this test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Date and time of the start of the test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Date and time of the stop of the test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Name of the tester running the test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explanation for why testing was halted (if this should happen)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**12.6.2.2 Activities and Event Entries**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'This section of the test log records each event during the execution of the
    test procedure. This section (containing multiple entries) typically documents
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Description of the test procedure execution (procedure ID/tag)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All personnel observing/involved in the test run—including testers, support
    personnel, and observers—and the role of each participant
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The result of each test procedure execution (pass, fail, commentary)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A record of any deviations from the test procedure (for example, redlines)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A record of any defects or anomalies discovered during the test procedure (along
    with a reference to an associated Anomaly Report if one is generated)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***12.6.3 Glossary***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This section of the LTL documentation contains the usual glossary associated
    with all Std 829 documents.
  prefs: []
  type: TYPE_NORMAL
- en: '***12.6.4 A Few Comments on Test Logs***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To be honest, the Std 829 outline is way too much effort for such a simple task.
    There are a few tips for managing the effort involved in this document.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.6.4.1 Overhead Management**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Almost all of the effort that would go into creating an Std 829 LTL outline-compliant
    document can be eliminated by simply attaching the test log directly to the end
    of the STP. The test log then inherits all the preface information from the STP,
    so all you need to document is the information that appears at the very beginning
    of “*Level* Test Logs” on [page 303](ch12.xhtml#page_303).
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that LTLs have four variants, as typical for all Std 829 level documents:
    Component Test Logs (aka Unit Test Logs), Component Integration Test Logs (aka
    Integration Test Logs), System Test Logs (aka System Integration Test Logs), and
    Acceptance Test Logs (possibly including Factory Acceptance Test Logs or Site
    Acceptance Test Logs).^([21](ch19_footnote.xhtml#ch12fn21))'
  prefs: []
  type: TYPE_NORMAL
- en: In reality, it’s rare for there to be much in the way of Component or Component
    Integration Test Logs. Most frequently, the corresponding test procedures are
    automated tests. Even when they’re not, the development team usually runs these
    tests and immediately corrects any defects they find. Because these tests run
    frequently (often multiple times per day, particularly in teams using Agile-based
    methodologies), the overhead with documenting these test runs is far too much.
  prefs: []
  type: TYPE_NORMAL
- en: System Test Logs and Acceptance Test Logs are the variants of the LTL that testers
    (independent of the development team) run, and hence the ones that require the
    creation of actual test logs.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.6.4.2 Recordkeeping**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The test logs are different from the other Std 829 documents in a very fundamental
    sense. Most Std 829 documents are static documents; about the only thing you do
    with them is fill in details like software version numbers and check off verification
    steps. The basic structure of the document doesn’t change if you run the procedure
    over and over again. Ultimately, there is no reason to keep any old copies of
    the test procedure around (like runs of the test procedure that failed in the
    middle of execution). All you really need to show the customer is the last run
    of the test procedure where you successfully executed all steps and passed the
    entire procedure.
  prefs: []
  type: TYPE_NORMAL
- en: The test logs, unlike the other documents you’ve seen in this chapter thus far,
    are *dynamic* documents. They will differ radically from test run to test run
    (even if nothing else changes, at least all the dates and timestamps will change).
    Furthermore, a test log isn’t a boilerplate document where you simply fill in
    a few blanks and check off some checkboxes. It’s essentially a blank slate that
    you create while actually running the test. If there are failures, or redlines,
    or commentary, the test log maintains the history of these events. Therefore,
    it is important to keep all your test logs, even the ones that recorded failed
    tests. It is highly improbable that any system will be perfect; there will be
    mistakes and defects you discover during testing. The test logs provide proof
    that you’ve found, corrected, and retested for these defects.
  prefs: []
  type: TYPE_NORMAL
- en: If you throw away all the old test logs that document all the defects discovered
    along the way and present only perfect test logs, any reasonable customer is going
    to question what you’re hiding. Mistakes and defects are a normal part of the
    process. If you don’t show that you’ve found and corrected these mistakes, your
    customers will assume that you haven’t tested the system well enough to find the
    defects or that you’ve faked the test logs. Keep the old test logs! This proves
    you’ve done your QA due diligence for your product.
  prefs: []
  type: TYPE_NORMAL
- en: You could argue that keeping old test procedures to show redlines or interruptions
    in the test process is also important. However, any redline or interruption that
    appears on a test procedure document had better show up in the corresponding test
    log, so you don’t need to keep old test procedures that you’ve actually rerun.
  prefs: []
  type: TYPE_NORMAL
- en: Note that this does not imply that all test procedures you’ve run should be
    perfect. If you have properly documented and justified redlines on a test procedure,
    yet the test execution ran successfully to its conclusion, there is no need to
    *rewrite* the test procedure and refill all the checkboxes to include a clean
    test procedure in your final documentation. If it was successful, even with redlines,
    leave it alone.^([22](ch19_footnote.xhtml#ch12fn22)) Redlines don’t indicate a
    failure of the software system; they are a defect, of course, but in the test
    procedure itself rather than the software. The goal of the test procedure is to
    test the software, not the test procedure. If minor changes to the test procedure
    are all you have, redline them and move on.
  prefs: []
  type: TYPE_NORMAL
- en: 'In many organizations, as I’ve said before, if any verification step in a test
    procedure fails, then after any defects are corrected, the entire procedure must
    be run from the beginning (a full regression test). For some test procedures or
    in some organizations, there may be a process in place to temporarily suspend
    a test procedure, update the software, and then resume the test procedure upon
    resolving the defect. In such cases, you can treat the verification failure step
    as though it were a redline: document the original failure in the test log, document
    the fact that the development team repaired the defect, and then document the
    correct operation of the software (at the failed verification step) with the new
    version of the software.^([23](ch19_footnote.xhtml#ch12fn23))'
  prefs: []
  type: TYPE_NORMAL
- en: '**12.6.4.3 Paper vs. Electronic Logs**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Some people prefer creating electronic test logs; some organizations or customers
    demand paper test logs (filled in with pens, not pencils). The problem with electronic
    logs (especially if you create them using a word processor rather than an application
    program specifically designed to log test procedure runs) is that they are easily
    faked. Of course, no great programmer would ever fake a test log. However, there
    are less-than-great programmers in this world who have faked a test log. Unfortunately,
    the actions of those few have sullied the reputations of all software engineers.
    Therefore, it’s best to create test logs that are not easily faked, which often
    means using paper.
  prefs: []
  type: TYPE_NORMAL
- en: Someone *could* fake paper logs; however, it’s a lot more work and usually more
    obvious. Ultimately, customers are probably going to want hard copies of the test
    logs; when they want them in electronic form, they’ll probably want scanned images
    of the hardcopy logs. They will be expecting you to maintain those paper logs
    in storage for legal reasons.
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps the best solution is to use a software application specifically designed
    for creating test logs, one that automatically logs the entries to a database
    (making it a bit more difficult to fake the data). For the customer, you would
    print a report from the database to provide a hardcopy (or generate a PDF report
    if they wanted an electronic copy).
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of how testers generate the original test log, most organizations
    will require them to eventually create a paper test log, and then the testers,
    observers, and other personnel associated with the test run will have to sign
    and date it to certify that the information is correct and accurate. This is a
    legal document at this point; someone attempting to fake any data could land in
    serious legal jeopardy.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.6.4.4 Inclusion in the RTM**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Normally, test logs don’t appear in the traceability matrix. However, there
    is no reason you couldn’t include them there. There is a one-to-many relationship
    between test procedures (and, therefore, STPs) and test logs. Thus, if you assign
    a unique identifier (tag) to each test report, you can add that identifier to
    an appropriate column in the RTM.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because test logs have a many-to-one relationship to test procedures, it wouldn’t
    be a bad idea to model the tag ID on the others that this book presents. For example,
    use something such as: proj_TL_*xxx*_*yyy* where *xxx* comes from the test procedure
    tag (for example, *005* from *DAQ_STP_005*) and *yyy* is a (possibly decimal)
    numeric sequence that creates a unique tag for the test log.'
  prefs: []
  type: TYPE_NORMAL
- en: '**12.7 Anomaly Reports**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When a tester, a development team member, a customer, or anyone else using the
    system discovers a software defect, the proper way to document it is with an *Anomaly
    Report (AR)*, also known as a *Bug Report* or *Defect Report*. All too often an
    AR consists of someone telling a programmer, “Hey, I found a problem in your code.”
    The programmer then runs off to their machine to correct the problem and there’s
    no documentation to track the anomaly. This is very unfortunate, because tracking
    defects in a system is very important to maintaining the quality of that system.
  prefs: []
  type: TYPE_NORMAL
- en: 'The AR is the formal way to track system defects. Among other things, it captures
    the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: Date and time of defect occurrence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The person who discovered the defect (or at least, who recorded the defect report
    in response to some user’s complaint)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A description of the defect
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A procedure for reproducing the defect in the system (assuming the issue is
    deterministic and is easy enough to reproduce)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The impact the defect has on the system (for example, catastrophic, critical,
    marginal, negligible)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The importance of the defect to end users (economic and social impact) so management
    can assign a priority to correcting it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any possible workarounds to the defect (so users can continue using the system
    while the development team works on correcting the defect)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A discussion of what it might take to correct the defect (including recommendations
    and conclusions concerning the defect)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Current status of the anomaly (for example, “new anomaly,” “development team
    is working on correction,” “in testing,” “corrected in software version *xxx.xxx*”)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Naturally, Std 829 has a suggested outline for Anomaly Reports. However, most
    organizations use defect-tracking software to record defects or anomalies. If
    you aren’t willing to spend the money on a commercial product, there are many
    open source products freely available, such as Bugzilla. Most of these products
    use a database organization that is reasonably compatible with the recommendations
    from Std 829:'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 Document Identifier
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 Scope
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 References
  prefs: []
  type: TYPE_NORMAL
- en: 2 Details
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Summary
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Date Anomaly Discovered
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Context
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Description of Anomaly
  prefs: []
  type: TYPE_NORMAL
- en: 2.5 Impact
  prefs: []
  type: TYPE_NORMAL
- en: 2.6 Originator’s Assessment of Urgency (see IEEE 1044-1993 [B13])
  prefs: []
  type: TYPE_NORMAL
- en: 2.7 Description of Corrective Action
  prefs: []
  type: TYPE_NORMAL
- en: 2.8 Status of the Anomaly
  prefs: []
  type: TYPE_NORMAL
- en: 2.9 Conclusions and Recommendations
  prefs: []
  type: TYPE_NORMAL
- en: 3 General
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Document Change Procedures and History
  prefs: []
  type: TYPE_NORMAL
- en: '***12.7.1 Introduction in the Anomaly Reports Document***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The following subsections describe the components of the AR introduction.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.7.1.1 Document Identifier**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This is a unique name that other reports can reference (such as test logs and
    test reports).
  prefs: []
  type: TYPE_NORMAL
- en: '**12.7.1.2 Scope**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The Scope section gives a brief description of anything that doesn’t appear
    elsewhere in the AR.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.7.1.3 References**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: References include links to other relevant documents, such as test logs and
    test procedures.
  prefs: []
  type: TYPE_NORMAL
- en: '***12.7.2 Details***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This section introduces the subsections that follow.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.7.2.1 Summary**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Here you give a brief description of the anomaly.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.7.2.2 Date Anomaly Discovered**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: List the date (and time, if possible/appropriate) when the anomaly was discovered.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.7.2.3 Context**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Software version and installation/configuration information goes in the Context
    section. This section should also refer to relevant test procedures and test logs,
    if appropriate, which should help to identify this anomaly. If no such test procedure
    exists for this anomaly, consider suggesting an addition to some test procedure
    that would catch it.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.7.2.4 Description of Anomaly**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Provide an in-depth description of the defect including (if possible) how to
    reproduce it. The description might include the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: Inputs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Actual results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outcome(s) (particularly, the outcomes that vary from the test procedure)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Procedure step of failure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Was the defect repeatable?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any tests executed immediately prior to failure than might have affected results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tester(s)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Observer(s)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**12.7.2.5 Impact**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Describe the impact this defect will have on system users. Describe any possible
    workarounds, such as changing the documentation or modifying the use of the system.
    If possible, estimate cost and time to repair this defect and the risk associated
    with leaving it in place. Estimate the risk associated with fixing it, which could
    impact other system features.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.7.2.6 Originator’s Assessment of Urgency**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: State the level of urgency for a speedy repair. The integrity levels and risk
    assessment scale from “Integrity Levels and Risk Assessment” on [page 263](ch12.xhtml#page_263)
    are probably a good minimum mechanism for stating the urgency of repair.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.7.2.7 Description of Corrective Action**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This section describes the time needed to determine the reason for the defect;
    an estimate of the time, cost, and risk associated with repairing it; and an estimate
    of the effort required to retest the system. Include any necessary regression
    tests to ensure that nothing else is broken by the fix.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.7.2.8 Status of the Anomaly**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: List the status of the current defect. Std 829 recommends statuses such as “open,”
    “approved for resolution,” “assigned for resolution,” “fixed,” and “tested with
    the fix confirmed.”
  prefs: []
  type: TYPE_NORMAL
- en: '**12.7.2.9 Conclusions and Recommendations**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This section should provide commentary as to why the defect occurred and recommend
    possible changes to the development process to prevent similar defects in the
    future. This section might also suggest additional requirements, test cases, and
    (modifications to) test procedures to catch the anomaly in the future; this is
    particularly important if testing discovered the anomaly by accident rather than
    by running specific test procedure steps to catch this particular defect.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.7.2.10 General**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This is the usual end-of-document section in Std 829 documents providing a change
    history (to the AR format, not to a specific AR) and change procedures. Std 829
    does not recommend a glossary.
  prefs: []
  type: TYPE_NORMAL
- en: '***12.7.3 A Few Comments on Anomaly Reports***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It is worthwhile to bear the following points in mind when dealing with Anomaly
    Reports.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.7.3.1 ARs Don’t Go in the RTM**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The purpose of the traceability matrix is to be able to trace requirements of
    designs and tests to ensure that the system successfully meets all requirements.
    While one could argue that test logs belong in the RTM, most people don’t bother
    to put them there because they normally attach test logs directly to the completed
    test procedures.
  prefs: []
  type: TYPE_NORMAL
- en: Anomalies, on the other hand, aren’t something whose existence you’re trying
    to prove; indeed, in a perfect world you’re trying to *disprove* the existence
    of anomalies. This doesn’t mean you discard ARs. Just as with test logs, it’s
    very important to keep all the old ARs around—they provide valuable proof that
    you’ve done your due diligence when testing the system. More importantly, you
    want to keep ARs for regression purposes. Sometimes long after a defect has been
    discovered and corrected, it finds its way into the system again. Having a historical
    record of ARs makes it possible to go back and examine the original cause and
    its solution.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.7.3.2 Electronic vs. Paper ARs**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As this chapter noted earlier, most organizations use a defect-tracking system
    to capture and track ARs. Although Std 829 doesn’t specifically suggest or require
    paper documents (indeed, Std 829 points out that you can use software to track
    anomalies), the outline form tends to suggest a hardcopy format. But given that
    most organizations use defect-tracking software, why bother with hardcopy ARs?
    The main reason is portability in the “you can carry it with you” sense. While
    using the defect-tracking system makes a lot of sense for system integration,
    factory acceptance tests, and other tests done at the development site where there
    is easy access to the tracker, in some cases it may not be available or accessible
    at an installation during a site acceptance test.^([24](ch19_footnote.xhtml#ch12fn24))
    In such situations, creating ARs on paper and then entering them into the defect-tracking
    system when possible is probably the best approach.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.8 Test Reports**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When testing is completed, a test report summarizes the results. As for many
    of the other test documents, Std 829 describes a wide variety of test reports
    you can produce. Std 829 defines *Level* Interim Test Status Reports (LITSR),
    *Level* Test Reports (LTR), and Master Test Reports (MTR). Of course, you can
    substitute *Component*, *Component Integration*, *System*, and *Acceptance* in
    place of *Level* (with the usual common names as well).
  prefs: []
  type: TYPE_NORMAL
- en: A very large organization might need to produce interim test reports so management
    can figure out what’s going on in an equally large system. For more information
    on LITSRs, refer to IEEE Std 829-2008; they are, quite frankly, documentation
    for documentation’s sake for most projects, but large governmental contracts might
    explicitly require them.
  prefs: []
  type: TYPE_NORMAL
- en: '*Level* and Master Test Reports vary according to the size of the project.
    Most small to medium-sized systems with (typically) a single software application
    and, therefore, a single STP, will have a single test report, if any at all.'
  prefs: []
  type: TYPE_NORMAL
- en: Once a system grows to the size that it contains several major software applications,
    there will usually be a test report for each major application and then an MTR
    as a summary of the results from the individual test reports. The MTR, then, provides
    an *executive-level review* of all the tests.
  prefs: []
  type: TYPE_NORMAL
- en: '***12.8.1 Brief Mention of the Master Test Report***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As the MTR is generally not a document that individual developers will deal
    with, this section will simply present the Std 829-suggested outline without further
    comment and then concentrate on LTRs.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 Document Identifier
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 Scope
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 References
  prefs: []
  type: TYPE_NORMAL
- en: 2 Details of the Master Test Report
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Overview of All Aggregate Test Results
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Rationale for Decisions
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Conclusions and Recommendations
  prefs: []
  type: TYPE_NORMAL
- en: 3 General
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Glossary
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Document Change Procedures and History
  prefs: []
  type: TYPE_NORMAL
- en: For more information on the MTR, see IEEE Std 829-2008.
  prefs: []
  type: TYPE_NORMAL
- en: '*12.8.2* Level *Test Reports*'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Although you could have component/unit test reports and component integration
    test reports, most organizations leave unit and integration testing to the development
    department, as upper management generally doesn’t care about the low-level details.
    Thus, the most common Level *Test Reports (LTRs)* you’ll see will be System (Integration)
    Test Reports and Acceptance Test Reports, typically Factory Acceptance Test Reports
    and Site Acceptance Test Reports. Std 829 outlines LTRs as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 Document Identifier
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 Scope
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 References
  prefs: []
  type: TYPE_NORMAL
- en: 2 Details
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Overview of Test Results
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Detailed Test Results
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Rationale for Decisions
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Conclusions and Recommendations
  prefs: []
  type: TYPE_NORMAL
- en: 3 General
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Glossary
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Document Change Procedures and History
  prefs: []
  type: TYPE_NORMAL
- en: Sections 1 (“Introduction”) and 3 (“General”) are the same as for most other
    Std 829 test documents in this chapter. The core of the test report is in Section
    2 (“Details”). The following subsections describe its contents.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.8.2.1 Overview of the Test Results**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This section is a summary of the test activities. It would briefly describe
    the features covered by the tests, testing environment, software/hardware version
    numbers, and any other general information about the test. The overview should
    also mention if there was anything special about the testing environment that
    would yield different results if the test were conducted in a different environment,
    like a factory.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.8.2.2 Detailed Test Result**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Summarize all the results in this section. List all anomalies discovered and
    their resolution. If the resolution to a defect has been deferred, be sure to
    provide justification and discuss the impact that defect will have on the system.
  prefs: []
  type: TYPE_NORMAL
- en: If there were any deviations from the test procedure, explain and justify those
    deviations. Describe any changes (redlines) to the test procedures.
  prefs: []
  type: TYPE_NORMAL
- en: This section should also provide a confidence level in the testing process.
    For example, if the testing process focuses on code coverage, this section should
    describe the estimated percentage of code coverage that the testing processing
    achieved.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.8.2.3 Rationale for Decisions**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: If the team had to make any decisions during the testing process such as deviations
    from test procedures or failure to correct known anomalies, this section should
    provide the rationale for those decisions. This section might also justify any
    conclusions reached (in the next section).
  prefs: []
  type: TYPE_NORMAL
- en: '**12.8.2.4 Conclusions and Recommendations**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This section should state any conclusions emanating from the test processing.
    This section should discuss the product’s fitness for release/production use,
    and recommend possibilities such as disabling certain, possibly known, anomalous
    features to allow early release of the system. This section could also recommend
    stalling the release pending further development and possible debugging.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.9 Do You Really Need All of This?**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: IEEE Std 829-2008 describes a huge volume of documentation. Do you really need
    to create all this documentation for the next “killer app” you’re developing by
    yourself in your home office? Of course not. Except for the largest (government-sponsored)
    applications, the vast majority of the documentation described in Std 829 is complete
    overkill. For normal projects, you’ll probably want to have the STC, SRL, and
    STP documents.^([25](ch19_footnote.xhtml#ch12fn25)) Test logs will simply be an
    appendix to the STP. Anomaly Reports would be entries in your defect-tracking
    system (from which you can produce hardcopy reports).
  prefs: []
  type: TYPE_NORMAL
- en: You can also reduce the size of your STC and STP documents by using automated
    testing. You probably can’t eliminate all manual tests, but you can get rid of
    many of them.
  prefs: []
  type: TYPE_NORMAL
- en: Test reports are easy enough to eliminate in smaller projects. The test log
    at the end of the STP will likely serve as a reasonable alternative unless you
    have multiple levels of management demanding full documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Agile development methodologies might seem like a good alternative for reducing
    the cost of all this documentation. However, keep in mind that developing, validating,
    verifying, and maintaining all those automated test procedures also has an associated—and
    often equivalent—cost.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.10 For More Information**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Dingeldein, Tirena. “5 Best Free and Open Source Bug Tracking Software for Cutting
    IT Costs.” September 6, 2019\. *[https://blog.capterra.com/top-free-bug-tracking-software/](https://blog.capterra.com/top-free-bug-tracking-software/)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'IEEE. “IEEE Std 829-2008: IEEE Standard for Software and System Test Documentation.”
    July 18, 2008\. *[http://standards.ieee.org/findstds/standard/829-2008.html](http://standards.ieee.org/findstds/standard/829-2008.html)*.
    This is expensive ($160 US when I last checked), but this is the gold standard.
    It’s more readable than the SDD standard, but still heavy reading.'
  prefs: []
  type: TYPE_NORMAL
- en: Peham, Thomas. “7 Excellent Open Source Bug Tracking Tools Unveiled by Usersnap.”
    May 8, 2016\. *[https://usersnap.com/blog/open-source-bug-tracking/](https://usersnap.com/blog/open-source-bug-tracking/)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Plantation Productions, Inc. “Open Source/Open Hardware: Digital Data Acquisition
    & Control System.” n.d. *[http://www.plantation-productions.com/Electronics/DAQ/DAQ.html](http://www.plantation-productions.com/Electronics/DAQ/DAQ.html)*.
    This is where you’ll find the DAQ Data Acquisition Software Review, Software Test
    Case, Software Test Procedures, and Reverse Traceability Matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Software Testing Help. “15 Best Bug Tracking Software: Top Defect/Issue Tracking
    Tools of 2019.” November 14, 2019\. *[http://www.softwaretestinghelp.com/popular-bug-tracking-software/](http://www.softwaretestinghelp.com/popular-bug-tracking-software/)*.'
  prefs: []
  type: TYPE_NORMAL
- en: Wikipedia. “Bug Tracking System.” Last modified April 4, 2020\. [*https://en.wikipedia.org/wiki/Bug_tracking_system*](https://en.wikipedia.org/wiki/Bug_tracking_system).
  prefs: []
  type: TYPE_NORMAL
