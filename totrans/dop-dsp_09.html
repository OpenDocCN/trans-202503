<html><head></head><body>
<section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" title="77" id="Page_77"/>7</span><br/>
<span class="ChapterTitle">Orchestrating with Kubernetes</span></h1>
</header>
<figure class="opener">
<img src="image_fi/book_art/chapterart.png" alt=""/>
</figure>
<p class="ChapterIntro">A container makes applications portable and consistent, but it’s only one piece of a modern application stack. Imagine needing to manage thousands of containers on different hosts, network ports, and shared volumes. What if one container stopped? How could you scale for load? How could you force containers to run on different hosts for availability? Container <em>orchestration</em> solves all these issues and more. <em>Kubernetes</em>, or <em>K8s</em>, is the open-source orchestration system many companies use to manage their containers. Kubernetes comes preloaded with some useful patterns (such as networking, role-based access control, and versioned APIs), but it’s meant to be the foundational framework on which to build your unique infrastructure and tools. Kubernetes is the standard in container orchestration. You can think of it as a low-level piece of your infrastructure, just like Linux. </p>
<p><span epub:type="pagebreak" title="78" id="Page_78"/>In this chapter, you’ll learn some basic Kubernetes resources and concepts concerning container orchestration. To put orchestration into practice, you’ll deploy the telnet-server container image from <span class="xref" itemid="xref_target_Chapter 6">Chapter 6</span> inside your Kubernetes cluster using the <code>kubectl</code> command line client. </p>
<h2 id="h1-502482c07-0001">Kubernetes from 30,000 Feet</h2>
<p class="BodyFirst">Kubernetes (which means <em>helmsman</em> in Greek) evolved from its predecessors, Borg and Omega, at Google. It was open-sourced in 2014 and has received great community support and many enhancements since then.</p>
<p>A Kubernetes cluster consists of one or more control plane nodes and one or more worker nodes. A <em>node</em> can be anything from a cloud VM to a bare-metal racked server to a Raspberry Pi. The <em>control plane nodes</em> handle things like the Kubernetes API calls, the cluster state, and the scheduling of containers. The core services (such as the API, etcd, and the scheduler) run on the control plane. The <em>worker nodes</em> run the containers and resources that are scheduled by the control plane. See <a href="#figure7-1" id="figureanchor7-1">Figure 7-1</a> for more details.</p>

<figure>
<img src="image_fi/502482c07/f07001.png" class="" alt="Diagram showing the command line interface, connected to the control plane node 1, which has the API server, scheduler, controller, and etcd layers. The control plane connects to the worker node 1. Inside the worker node are pods 1 and 2, docker, and the kubelet and kube-proxy layers."/>
<figcaption><p><a id="figure7-1">Figure 7-1</a>: The basic building blocks of a Kubernetes cluster</p></figcaption>
</figure>

<p>Networking and scheduling are the most complex issues you’ll encounter when orchestrating containers. When networking containers, you must consider all the ports and access they need. Containers can communicate with each other, both inside and outside the cluster. This happens with microservices internal communication or when running a public-facing web server. When scheduling containers, you must take into account the current system resources and any special placement strategies. You can tune a worker node for a specific use case, like high connections, and then create rules to ensure that the applications that need that feature end up on that specific worker node. This is called <em>node affinity</em>. As a container orchestrator, you also need to restrict user authentication and authorizations. You can use an approach like role-based access control, which allows containers <span epub:type="pagebreak" title="79" id="Page_79"/>to run in a safe and controlled manner. These approaches represent just a small part of the complex glue and wiring you’ll need. It takes a whole framework to successfully deploy and manage containers.</p>
<h2 id="h1-502482c07-0002">Kubernetes Workload Resources</h2>
<p class="BodyFirst">A <em>resource</em> is a type of object that encapsulates state and intent. To make this concept a little clearer, let’s consider an automobile analogy. If a workload running on Kubernetes were a car, the resources would describe the parts of the car. For example, you could set your car to have two seats and four doors. You would not have to understand how to make a seat or a door. You would just need to know that Kubernetes will maintain the given count for both (no more, no less). Kubernetes resources are defined in a file called a <em>manifest</em>. Throughout this chapter, we will use the terms <em>resource</em> and <em>object</em> interchangeably. </p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">NOTE</span></h2>
<p>	To learn more about resources and other concepts, visit <a href="https://kubernetes.io/docs/concepts/" class="LinkURL">https://kubernetes.io/docs/concepts/</a>.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>Let’s look at the most commonly used Kubernetes resources in a modern application stack.</p>
<h3 id="h2-502482c07-0001">Pods</h3>
<p class="BodyFirst"><em>Pods</em> are the smallest building blocks in Kubernetes, and they form the foundation for everything interesting you’ll do with containers. A Pod is made up of one or more containers that share network and storage resources. Each container can connect to the other containers, and all containers can share a directory between them by a mounted volume. You won’t deploy Pods directly; instead, they’ll be incorporated into a higher-level abstraction layer like a ReplicaSet.</p>
<h3 id="h2-502482c07-0002">ReplicaSet</h3>
<p class="BodyFirst">A <em>ReplicaSet</em> resource is used to maintain a fixed number of identical Pods. If a Pod is killed or deleted, the ReplicaSet will create another Pod to take its place. You’ll only want to use a ReplicaSet if you need to create a custom orchestration behavior. Typically, you should reach for a Deployment to manage your application instead.</p>
<h3 id="h2-502482c07-0003">Deployments</h3>
<p class="BodyFirst">A <em>Deployment</em> is a resource that manages Pods and ReplicaSets. It is the most widely used resource for governing applications. A Deployment’s main job is to maintain the state that is configured in its manifest. For example, you can define the number of Pods (which are called <em>replicas</em> in this context) along with the strategy for deploying new Pods. The Deployment resource controls a Pod’s lifecycle—from creation, to updates, to scaling, <span epub:type="pagebreak" title="80" id="Page_80"/>to deletion. You can also roll back to earlier versions of a Deployment if needed. Anytime your application needs to be long lived and fault tolerant, a Deployment should be your first choice.</p>
<h3 id="h2-502482c07-0004">StatefulSets</h3>
<p class="BodyFirst">A <em>StatefulSet </em>is a resource for managing stateful applications, such as PostgreSQL, ElasticSearch, and etcd. Similar to a Deployment, it can manage the state of Pods defined in a manifest. However, it also adds features like managing unique Pod names, managing Pod creation, and ordering termination. Each Pod in a StatefulSet has its own state and data bound to it. If you are adding a stateful application to your cluster, choose a StatefulSet over a Deployment.</p>
<h3 id="h2-502482c07-0005">Services</h3>
<p class="BodyFirst"><em>Services </em>allow you to expose applications running in a Pod or group of Pods within the Kubernetes cluster or over the internet. You can choose from the following basic Service types:</p>
<ol class="none">
<li><code class="bold">ClusterIP</code>  This is the default type when you create a Service. It is assigned an internal routable IP address that proxies connections to one or more Pods. You can access a <code>ClusterIP</code> only from within the Kubernetes cluster.</li>
<li><code class="bold">Headless</code>  This does not create a single-service IP address. It is not load balanced.</li>
<li><code class="bold">NodePort</code>  This exposes the Service on the node’s IP addresses and port.</li>
<li><code class="bold">LoadBalancer</code>  This exposes the Service externally. It does this either by using a cloud provider’s component, like AWS’s Elastic Load Balancing (ELB), or a bare-metal solution, like MetalLB.</li>
<li><code class="bold">ExternalName</code>  This maps a Service to the contents of the <code>externalName</code> field to a <code>CNAME</code> record with its value.</li>
</ol>
<p>You’ll use <code>ClusterIP</code> and <code>LoadBalancer</code> the most. Note that only the <code>LoadBalancer</code> and <code>NodePort</code> Services can expose a Service outside the Kubernetes cluster. </p>
<h3 id="h2-502482c07-0006">Volumes</h3>
<p class="BodyFirst">A <em>Volume</em> is basically a directory, or a file, that all containers in a Pod can access, with some caveats. Volumes provide a way for containers to share and store data between them. If a container in a Pod is killed, the Volume and its data will survive; if the entire Pod is killed, the Volume and its contents will be removed. Thus, if you need storage that is not linked to a Pod’s lifecycle, use a <em>Persistent Volume (PV)</em> for your application. A PV is a resource in a cluster just like a node. Pods can use the PV resource, but the PV does not terminate when the Pod does. If your Kubernetes cluster is running in AWS, you can use <em>Amazon Elastic Block Storage (Amazon EBS)</em> as your PV. This makes Pod catastrophes easier to survive.</p>
<h3 id="h2-502482c07-0007"><span epub:type="pagebreak" title="81" id="Page_81"/>Secrets</h3>
<p class="BodyFirst"><em>Secrets</em> are convenient resources for safely and reliably sharing sensitive information (such as passwords, tokens, SSH keys, and API keys) with Pods. You can access Secrets either via environment variables or as a Volume mount inside a Pod. Secrets are stored in a RAM-backed filesystem on the Kubernetes nodes until a Pod requests them. When not used by a Pod, they are stored in memory, instead of in a file on disk. However, be careful because the Secrets manifest expects the data to be in Base64 encoding, which is not a form of encryption. </p>
<p>With Secrets, sensitive information is kept separate from the application. This is because such information is more likely to be exposed in the continuous integration/continuous deployment process than if it’s living in a resource. You still need to keep your Secret manifests safe by using RBAC to restrict broad access to the Secrets API. You can also store the sensitive data encrypted in the Secret and have another process to decrypt it on the Pod once it is mounted or needed. Another option is to encrypt the manifests locally before adding them to version control. No matter which method you choose, make sure you have a secure plan for storing sensitive information in Secrets.</p>
<h3 id="h2-502482c07-0008">ConfigMaps</h3>
<p class="BodyFirst"><em>ConfigMaps</em> allow you to mount nonsensitive configuration files inside a container. A Pod’s containers can access the ConfigMap from an environment variable, from command line arguments, or as a file in a Volume mount. If your application has a configuration file, putting it into a ConfigMap manifest provides two main benefits. First, you can update or deploy a new manifest file without having to redeploy your whole application. Second, if you have an application that watches for changes in a configuration file, then when it gets updated, your application will be able to reload the configuration without having to restart.</p>
<h3 id="h2-502482c07-0009">Namespaces</h3>
<p class="BodyFirst">The <em>Namespace</em> resource allows you to divide a Kubernetes cluster into several smaller virtual clusters. When a Namespace is set, it provides a logical separation of resources, even though those resources can reside on the same nodes. If you don’t specify a Namespace when creating a resource, it will inherit the Namespace cleverly named <em>default</em>. If your team has many users and a lot of projects spread among them, you might split those teams or applications into separate Namespaces. This makes it easy to apply secure permissions or other constraints to only those resources.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">NOTE</span></h2>
<p> 	This is <em>not</em> the same namespace you learned about in Chapter 6. That is a Linux kernel feature.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<h2 id="h1-502482c07-0003"><span epub:type="pagebreak" title="82" id="Page_82"/>Deploying the Sample telnet-server Application</h2>
<p class="BodyFirst">To start exploring Kubernetes, you’ll create a Deployment and two Services for the telnet-server application. I have chosen a Deployment to provide fault tolerance for your application. The two Services will expose the telnet-server application port and the application metrics port. By the end of this section, you’ll have a Kubernetes Deployment with two Pods (replicas) running the telnet-server application that can be accessed from your local host.</p>
<h3 id="h2-502482c07-0010">Interacting with Kubernetes</h3>
<p class="BodyFirst">Before you can deploy your telnet-server application, you’ll need to make sure you can connect to your Kubernetes cluster. The most direct way to interact with the cluster is to use the <code>kubectl</code> command line application, which you can get in two ways. The first way is to download the standalone binary from <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/" class="LinkURL">https://kubernetes.io/docs/tasks/tools/install-kubectl/</a> for your specific OS. The second way, which you’ll use here, is to leverage minikube’s built-in support for <code>kubectl</code>. Minikube will fetch the <code>kubectl</code> binary for you the first time you invoke the <code>minikube kubectl</code> command (if it’s not already installed). </p>
<p>When using <code>minikube kubectl</code>, most commands will require double dashes (<code>--</code>) between <code>minikube kubectl</code> and subcommands. The standalone version of <code>kubectl</code>, however, does not need dashes between the commands. If you already have <code>kubectl </code>installed or want to use the standalone version, drop the <code>minikube</code> prefix and the double dashes from all the examples that follow.</p>
<p>Let’s start out with a simple command so minikube can download the <code>kubectl</code> binary and test access to the cluster. Use the <code class="bold">cluster-info</code> subcommand  for this example to verify that the cluster is up and running:</p>
<pre><code>$ <b>minikube kubectl cluster-info</b>
Kubernetes master is running at https://192.168.99.109:8443
<var>--snip--</var></code></pre>
<p>You’ll want to see similar output that indicates you can connect to the Kubernetes cluster. If there were an issue with talking to the cluster, you might see an error like <code>"The control plane node must be running for this command"</code>. If that happens, enter the <code class="bold">minikube status</code> command to make sure minikube is still up and running.</p>
<h3 id="h2-502482c07-0011">Reviewing the Manifests</h3>
<p class="BodyFirst">Now that you have access to the cluster, review the provided manifests for the Deployment and Services. Kubernetes manifests are files designed to describe the desired state for applications and services. They manage resources like Deployments, Pods, and Secrets. These files can either be in JSON or YAML; we use the YAML format for this book, purely out of preference. The manifest files should be kept under source control. You’ll usually find the files co-residing with the application they describe. </p>
<p><span epub:type="pagebreak" title="83" id="Page_83"/>I have provided the manifest files to create the telnet-server Deployment and two Services. The files are located in the repository at <a href="https://github.com/bradleyd/devops_for_the_desperate/" class="LinkURL">https://github.com/bradleyd/devops_for_the_desperate/</a>. Navigate to the <em>telnet-server/</em> directory and list the files in the <em>kubernetes/</em> subdirectory. There, you should find two files. The first file, <em>deployment.yaml</em>, creates a Kubernetes Deployment with two Pods of the telnet-server container image. The second file, <em>service.yaml</em>, creates two separate Services. The first Service creates a <code>LoadBalancer</code> so you can connect to the telnet-server from outside the Kubernetes cluster. The other Service creates a <code>ClusterIP</code>, which exposes the metrics endpoint from within the cluster. Don’t worry about the metrics port for this chapter—we’ll use it in <span class="xref" itemid="xref_target_Chapter 9 ">Chapter 9 </span>when discussing monitoring and metrics.</p>
<p>These manifest files can be quite verbose, so we’ll focus on the basic structure each file contains. To describe a complex object, you’ll need multiple fields, subfields, and values to define how a resource behaves. Because of this, it can be difficult to write a manifest from scratch. Among all these fields and values, there is a subset of required fields called <em>top-level fields</em>. These are common across all manifest files. Understanding top-level fields makes it easier to remember and parse a manifest file. The four top-level fields are as follows:</p>
<ol class="none">
<li><code class="bold">apiVersion</code>  This is a Kubernetes API version and group, like apps/v1. Kubernetes uses versioned APIs and groups to deliver different versions of features and support for resources.</li>
<li><code class="bold">kind</code>  This is the type of resource you want to create, such as a Deployment.</li>
<li><code class="bold">metadata</code>  This is where you set things like names, annotations, and labels.</li>
<li><code class="bold">spec</code>  This is where you set the desired behavior for the resource (kind).</li>
</ol>
<p>Each of these top-level fields contains multiple subfields. The subfields contain information such as name, replica count, template, and container image. For example, <code>metadata</code> has <code>name</code> and <code>labels</code> subfields. The formats for the fields can be different for each Kubernetes resource. I won’t describe every field, but I’ll often use the <code>labels</code> subfield. <em>Labels</em> provide a way for the user to tag a resource with identifiable key values. For example, you could add a label to all resources that are in the <code>production environment</code>.</p>
<pre><code><var>--snip--</var>
metadata: 
  labels:
    environment: production
<var>--snip--</var></code></pre>
<p>You can use these <code>labels </code>to narrow down search results and group similar applications together, as with a frontend website and its backend database counterpart. You’ll use <code>labels</code> later, when you invoke the <code>minikube kubectl</code> command.</p>
<p><span epub:type="pagebreak" title="84" id="Page_84"/>Listing all the different field structures in a manifest file would take up a lot of real estate. Instead, you can explore the documentation in two different places. The Kubernetes documentation at <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/" class="LinkURL">https://kubernetes.io/docs/concepts/overview/working-with-objects/</a> describes all the resources and provides examples. The second place to explore, which is my favorite, is the <code>explain</code> subcommand for <code>kubectl</code>. The <code>explain</code> subcommand describes the fields associated with each resource type. You can use the dot (<code>.</code>) notation as a type field separator when searching for nested fields. For example, to learn more about a Deployment’s <code>metadata</code> <code>labels</code> subfield, enter the following in a terminal:</p>
<pre><code>$ <b>minikube kubectl -- explain deployment.metadata.labels</b>
KIND:     Deployment
VERSION:  apps/v1

FIELD:    labels &lt;map[string]string&gt;

DESCRIPTION:
     Map of string keys and values that can be used to organize and categorize
     (scope and select) objects. May match selectors of replication
     controllers and services. More info:
     http://kubernetes.io/docs/user-guide/labels</code></pre>
<p>Notice how this example first searches for the resource type, then its top-level field, and then the subfield.</p>
<h4 id="h3-502482c07-0001">Examining the telnet-server Deployment</h4>
<p class="BodyFirst">Now that you have an understanding of the building blocks of a manifest file, let’s apply what you’ve learned to the telnet-server Deployment manifest. I’ve broken the <em>deployment.yaml</em> file into sections to make it easier to dissect. The first section at the top of the file has the <code>apiVersion</code>, <code>kind</code>, and <code>metadata</code> fields:</p>
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:	
  name: telnet-server
  labels:
    app: telnet-server
<var>--snip--</var></code></pre>
<p>The type (<code>kind</code>) is <code>Deployment</code>, which uses the Kubernetes API group <code>apps</code> and API version <code>v1</code>. Under the <code>metadata</code> field, the Deployment <code>name</code> is set to <code>telnet-server</code>, and the <code>labels</code> are set to <code>app: telnet-server</code>. You’ll use this label when searching for the telnet-server Deployment later on.</p>
<p>The next section of the file contains the parent <code>spec</code> field that describes the behavior and specification of the Deployment. The <code>spec</code> field contains a lot of subfields and values:</p>
<pre><code><span epub:type="pagebreak" title="85" id="Page_85"/><var>--snip--</var>
spec:
  replicas: 2
  selector:
    matchLabels:
      app: telnet-server
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
<var>--snip--</var></code></pre>
<p>First, <code>spec</code> describes the <code>replicas</code> count for the Deployment; it’s set to <code>2</code> to reflect the number of Pods you want to run. Inside the <code>selector</code> field, <code>matchLabels</code> locates the Pods that this Deployment will affect. The key value used in <code>matchLabels</code> must match the Pod’s template labels (more on this later). </p>
<p>The <code>strategy</code> field describes how to replace the current running Pods with new ones during a rollout. This example uses a <code>RollingUpdate</code>, which will replace one Pod at a time as it goes. This is the default strategy for a Deployment. The other option for strategy, <code>Recreate</code>, kills the current running Pods before creating the new ones. </p>
<p>The <code>maxSurge</code> and <code>maxUnavailable</code> keys control the number of Pods created and terminated. Here, it’s set to bring up an extra Pod during a rollout, which temporarily brings the Pod count to <code>replicas</code> + 1 (or three, in this case). Once the new Pod is up and running, one of the old Pods will be terminated. Then, the process repeats until all the new Pods are running and the old Pods are terminated. These settings will ensure that there is always a Pod to serve traffic during a Deployment. See <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy/" class="LinkURL">https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy/</a> for more information about strategy.</p>
<p>The next part of the <code>spec</code> section is the <code>template</code> field. This field (along with its subfields) describes the Pods that this Deployment will create. The major subfields in this section are <code>metadata</code> and <code>spec</code>:</p>
<pre><code><var>--snip--</var>
template:
    metadata:
      labels:
        app: telnet-server
    spec:
      containers:
      - image: <b>dftd/telnet-server:v1</b>
        imagePullPolicy: IfNotPresent
        name: telnet-server
        resources:
          requests:
            cpu: 1m
            memory: 1Mi
          limits:
<span epub:type="pagebreak" title="86" id="Page_86"/>            cpu: 500m
            memory: 100Mi
        ports:
        - containerPort: <b>2323</b>
          name: telnet
        - containerPort: <b>9000</b>
          name: metrics</code></pre>
<p>Here, the <code>app: telnet-server</code> key value is added for each Pod in the Deployment, using the <code>labels</code> subfield under <code>template</code> and <code>metadata</code>. The <code>app: telnet-server</code> label matches the key and value you used earlier in the <code>spec</code> <code>selector:</code> field. (You’ll use this label again when searching for the Pods later.)</p>
<p>The <code>containers</code> field sets the container image for the first container in the Pod. In this case, it’s set to the <em>dftd/telnet-server:v1</em> image you built in <span class="xref" itemid="xref_target_Chapter 6">Chapter 6</span>. This container <code>name</code> is <code>telnet-server</code>, just like the Deployment. Using the same <code>name </code>isn’t a requirement; the <code>name</code> could be any string you choose so long as it is unique among the containers in the Pod. </p>
<p>The next subfield under <code>containers</code> is <code>resources</code>, which controls CPU and memory for a container. You can define <code>requests</code> and <code>limits</code> for each container individually. The <code>requests</code><code> </code>are used for Kubernetes scheduling (orchestration) and overall application health. If a container needs a minimum of 2GB of memory and one CPU to start, you don’t want Kubernetes to schedule that Pod (container) on a node that has only 1GB of memory or no CPUs available. <code>Requests</code> are the minimum resources your application needs. <code>Limits</code>, on the other hand, control the maximum CPU and memory a container can use on a node. You don’t want a container to use all the memory or CPU on a node while starving any other containers running on it. In this example, the <code>CPU</code> limit is set to <code>500m</code> (millicpu), or half of a CPU. This unit can also be expressed as a decimal, like 0.5. In Kubernetes, one CPU is equivalent to one CPU core. The <code>memory</code> limit is set to <code>100Mi</code>, or 104,857,600 bytes. In Kubernetes, <code>memory</code> is expressed in bytes, but you can use more familiar units like M, Mi, G, and Gi. When these <code>limits</code> are set and the telnet-server container consumes more than <code>100Mi</code> of <code>memory</code>, Kubernetes will terminate it. However, if the <code>CPU</code> limit (<code>500m</code>) is surpassed, Kubernetes won’t just kill the container. It will throttle, or limit, the CPU request time for that container. For more details on how Kubernetes quantifies resources, see <a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/" class="LinkURL">https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/</a>. </p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p>	Setting <code>requests</code> and <code>limits</code> for each resource you deploy in a k8s cluster will save you a lot of investigation time when things can’t be scheduled or get killed. It will also save you money, since you’ll use your nodes more efficiently.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>The container <code>ports</code> field sets the exposed ports you want to announce. This example exposes ports <code>2323</code> (<code>telnet</code>) and <code>9000</code> (<code>metrics</code>). These port definitions are for informational purposes only and have no bearing on whether a container can receive traffic. They simply let the user and Kubernetes know on what ports you expect the container to be listening.</p>
<h4 id="h3-502482c07-0002"><span epub:type="pagebreak" title="87" id="Page_87"/>Examining the telnet-server Service</h4>
<p class="BodyFirst">The next manifest to examine is the <code>Service</code> resource. The <em>service.yaml</em> file creates two separate Services: one to expose the telnet-server and the other to expose the application metrics. We’ll look at only the <code>telnet</code> Service and specific fields here since the <code>metric</code> Service is almost identical:</p>
<pre><code>apiVersion: v1
kind: Service
metadata:
  labels:
    app: telnet-server
  name: telnet-server
spec:
  ports:
  - port: 2323
    name: telnet
    protocol: TCP
    targetPort: 2323
  selector:
    app: telnet-server
  type: LoadBalancer
<var>--snip--</var></code></pre>
<p>A <code>Service</code><code> </code>resource is set in the <code>kind</code> field, which is different from the Deployment manifest shown earlier. The Service <code>name</code> can be anything, but it must be unique within a Kubernetes Namespace. I’ve kept the <code>names</code> consistent with the rest of the resources here, for ease of use. I’ve also used the same <code>app: telnet-server</code> label to make finding things uniform and simple. </p>
<p>The <code>ports</code> field tells the <code>Service</code> which port to expose and how to connect it to the Pods. This exposes port <code>2323</code> (<code>telnet</code>) and forwards any traffic to port <code>2323</code> on the Pod. </p>
<p>Just as with the <code>selector</code> field for a Deployment, a <code>Service</code> uses a <code>selector</code> field to find the Pods to forward traffic to. This instance uses the familiar Pod label <code>app: telnet-server</code> as the match for the <code>selector</code>, which means any Pods with the label <code>app: telnet-server</code> will receive traffic from this <code>Service</code>. If there is more than one Pod, like in the Deployment, the traffic will be sent to all the Pods in a round-robin manner. Since the goal of the <code>telnet-server</code> application is to be exposed outside the cluster, it’s set as a <code>LoadBalancer</code>.</p>
<h3 id="h2-502482c07-0012">Creating a Deployment and Services</h3>
<p class="BodyFirst">It is time to create the Deployment and Services. To turn the sample application into a Kubernetes Deployment, you’ll use the <code>minikube</code> <code>kubectl</code> command line tool and the manifest files you just reviewed (<a href="https://github.com/bradleyd/devops_for_the_desperate/" class="LinkURL">https://github.com/bradleyd/devops_for_the_desperate/</a>).</p>
<p>To create and update resources, you can pass <code>minikube</code> <code>kubectl</code> two subcommands: <code>create</code> and <code>apply</code>. The <code>create</code> subcommand is <em>imperative</em>, which means it makes the resource reassemble the manifest file. It also throws an error if the resource already exists. The <code>apply</code> subcommand is <em>declarative</em>, which means it creates the resource if it does not exist and updates it if it <span epub:type="pagebreak" title="88" id="Page_88"/>does. For this scenario, you’ll use the <code>apply</code> command with an <code>-f </code>flag to instruct <code>kubectl</code> to run the operation against all the files in the <em>kubernetes/</em> directory. The <code>-f</code> flag can take filenames in lieu of directories as well.</p>
<p>From within the <em>telnet-server/</em> directory, enter the following command to create the <code>Deployment</code> and two <code>Services</code>:</p>
<pre><code>$ <b>minikube</b> <b>kubectl -- apply -f kubernetes/</b>
deployment.apps/telnet-server created
service/telnet-server-metrics created
service/telnet-server created</code></pre>
<p>The output should show that all three resources have been <code>created</code>. Be sure to investigate any errors if they arise from this command. Common errors you might see are usually due to syntax errors or typos in the YAML file.</p>
<h3 id="h2-502482c07-0013">Viewing the Deployment and Services</h3>
<p class="BodyFirst">Once the telnet-server Deployment and Services are created, you need to know how to find them. Kubernetes provides multiple ways to view any object’s status. The easiest method is to use the <code>minikube kubectl -- get &lt;</code><var>resource</var><code>&gt; &lt;</code><var>name</var><code>&gt;</code> command. </p>
<p>You can start by fetching the Deployment status by its name and then explore the Services. Enter the following to get the <code>Deployment</code> status for the <code>telnet-server</code>:</p>
<pre><code>$ <b>minikube kubectl -- get deployments.apps telnet-server </b>
NAME            READY   UP-TO-DATE   AVAILABLE   AGE
telnet-server   2/2     2            2           7s</code></pre>
<p>The output should show that the <code>telnet-server</code> Deployment has two replicas (Pods) running (<code>2/2 READY</code>) and that they have been running for seven seconds (<code>7s AGE</code>). This should match the number of replicas set in the Deployment manifest. The <code>UP-TO-DATE</code> and <code>AVAILABLE</code> columns show how many Pods were updated to get to the desired number (<code>2</code>) and how many are available (<code>2</code>) to users, respectively. In this case, Kubernetes believes the Deployment is up and running and fully available.</p>
<p>You can also run the <code>minikube kubectl get pods</code> command to find out whether a Deployment is ready for traffic. Because you could have hundreds of Pods, you want to narrow down your results with the <code>-l</code> label filter flag. Enter the following to show only the <code>telnet-server</code> Pods:</p>
<pre><code>$ <b>minikube kubectl -- get pods -l app=telnet-server</b>
NAME                            READY   STATUS    RESTARTS   AGE
telnet-server-775769766-2bmd5   1/1     Running   0          4m34s
telnet-server-775769766-k9kx9   1/1     Running   0          4m34s</code></pre>
<p>This command lists any Pods that have the label <code>app: telnet-server</code> set; it’s the same label set in the <em>deployment.yaml</em> file under the <code>spec.template.metadata.labels</code> field. The output shows two <code>telnet-server</code> Pods ready for traffic. You know this because the <code>READY</code> column shows <code>1/1</code> containers running and your <span epub:type="pagebreak" title="89" id="Page_89"/>Deployment has only one container (<code>telnet-server</code>). If you had a Pod with multiple containers, you would want the number of running containers over the number of total containers to be the same. </p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">NOTE</span></h2>
<p>	The <code>kubectl</code> <code>get </code><var>&lt;resource&gt;</var> command is one you will use most often when interacting with Kubernetes.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>Now, use the same command as above but substitute <code>services</code> for the <code>pods</code> resource to display the two Services:</p>
<pre><code>$ <b>minikube kubectl -- get services -l app=telnet-server</b>
NAME                    TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
telnet-server           LoadBalancer   10.105.187.105   &lt;pending&gt;     2323:30557/TCP   10m
telnet-server-metrics   ClusterIP      10.96.53.191     &lt;none&gt;        9000/TCP         10m </code></pre>
<p>Since you used the same label (<code>app: telnet-server</code>) to organize your application, you can use the <code>-l</code> flag to find your match. The output shows that two Services were created about 10 minutes ago. One Service type is a <code>LoadBalancer</code>, and the other is a <code>ClusterIP</code>. The <code>LoadBalancer</code> is for exposing the <code>telnet-server</code> application. Don’t be alarmed if your <code>EXTERNAL-IP</code> status is <code>&lt;pending&gt;</code>. Because you are running on minikube, no real <code>LoadBalancer</code> piece is included.</p>
<p>The <code>ClusterIP</code> Service allows the application metrics to be scraped from within the cluster. In this example, internal applications can reach the metrics endpoint by using either the <code>telnet-server-metrics</code> canonical name or the IP <code>10.96.53.191</code>. Using the canonical name is recommended.</p>
<h2 id="h1-502482c07-0004">Testing the Deployment and Services</h2>
<p class="BodyFirst">Now that the telnet-server Deployment and Services are running, you’ll want to test connectivity and availability. You want to be able to access the telnet-server application, like you did in <span class="xref" itemid="xref_target_Chapter 6,">Chapter 6,</span> with the telnet client. After that, you’ll test the Deployment’s resiliency by killing a telnet-server Pod and watching it recover. Finally, you’ll learn how to <em>scale</em>, meaning change the number of replicas that the Deployment has up and down from the command line, in the case of a change in load.</p>
<h3 id="h2-502482c07-0014">Accessing the Telnet Server</h3>
<p class="BodyFirst">You’ll use the <code>minikube tunnel</code> command to expose your <code>LoadBalancer</code> Service outside the Kubernetes cluster. This command will provide you with an IP address that you can use to connect, using the <code>telnet</code><code> client</code> command again. The <code>tunnel</code> subcommand runs in the foreground, so it should be run in a terminal that won’t get closed. The command also requires <em>root</em> privileges. If you do not have <em>root</em> privileges on your local machine, use the <code>minikube service</code> command instead. Visit <a href="https://minikube.sigs.k8s.io/docs/commands/service/" class="LinkURL">https://minikube.sigs.k8s.io/docs/commands/service/</a> for more details.  </p>
<p><span epub:type="pagebreak" title="90" id="Page_90"/>In a terminal, enter the following to create the network <code>tunnel</code> to the <code>telnet-server</code> Service:</p>
<pre><code>$ <b>minikube tunnel</b>
Password:
Status:
	machine: minikube
	pid: 42612
	route: 10.96.0.0/12 -&gt; 192.168.99.103
	minikube: Running
	services: [telnet-server]
    errors:
		minikube: no errors
		router: no errors
		loadbalancer emulator: no errors</code></pre>
<p>After entering your password, the command outputs a <code>route</code>, the <code>services</code> exposed, and any present <code>errors</code>. Make sure you leave this running while you try to connect to the <code>telnet-server</code>. Once the <code>tunnel</code> is closed, all the connections will drop. Since there are <code>no errors</code> to report, the <code>tunnel</code> should be operational at this point. Don’t do it now, but when you want to close the <code>tunnel</code>, press <span class="KeyCaps">CTRL-C</span> to shut it down. </p>
<p>Now, with the tunnel up, you need to get the new external IP address for the <code>LoadBalancer </code>Service. As a shortcut, pass the Service name to <code class="bold">get services</code> <code class="bold">telnet-server</code> (in this case) to view only the Service you are interested in:</p>
<pre><code>$ <b>minikube kubectl -- get services telnet-server</b>
NAME            TYPE           CLUSTER-IP       EXTERNAL-IP      PORT(S)          AGE
telnet-server   LoadBalancer   10.105.187.105   10.105.187.105   2323:30557/TCP   15m </code></pre>
<p>The <code>EXTERNAL-IP</code> column should now be populated with an IP address instead of <code>&lt;pending&gt;</code>. Here, the <code>telnet-server</code> application IP address is set to <code>10.105.187.105</code>, and the external <code>PORT</code> is set to <code>2323</code>. Your <code>EXTERNAL-IP</code> may be different from mine, so just use the IP from this column.</p>
<p>In another terminal that is not running the tunnel, use the <code>telnet client</code> command again (<code>telnet</code> <code>10.105.187.105</code>) with the new IP address to access the <code>telnet-server</code>, as shown in <a href="#figure7-2" id="figureanchor7-2">Figure 7-2</a>.</p>
<p>As you can see, the telnet-server responded with the ASCII art logo. Press <span class="KeyCaps">Q</span> to quit, since you are just testing connectivity. The <code>tunnel</code> command made it possible to hit the Service using an assigned IP like it was a public-facing application. If this were on a cloud provider like AWS, the IP would be accessible to anyone on the internet. Feel free to kill the <code>tunnel </code>command in the other terminal, but you’ll use it again in future chapters.</p>
<span epub:type="pagebreak" title="91" id="Page_91"/><figure>
<img src="image_fi/502482c07/f07002.png" class="" alt="Screenshot showing the DFTD banner in green ASCII art in a terminal window with a black background"/>
<figcaption><p><a id="figure7-2">Figure 7-2</a>: Testing telnet access to telnet-server</p></figcaption>
</figure>
<h3 id="h2-502482c07-0015">Troubleshooting Tips</h3>
<p class="BodyFirst">If you cannot connect to the telnet-server like in <a href="#figure7-2">Figure 7-2</a>, check that the Pods are still running and that they are reporting that <code>1/1</code> containers are <code>READY</code>. If the <code>READY</code> column shows <code>0/1</code> instead and the <code>STATUS</code> column has an error like <code>ImagePullBackOff</code><code> </code>or<code> ErrImagePull</code>, then the Pod could not find the telnet-server image you built in <span class="xref" itemid="xref_target_Chapter 6">Chapter 6</span>. Make sure the image is built and available when you list the Docker images. </p>
<p>If the <code>READY</code> and <code>STATUS</code> columns are correct, the next step is to make sure your Service is wired up to your Pods. One way to check this connection is with the <code>kubectl get endpoints</code> command, which will tell you if the Service can find the Pods you specified in the Service <code>spec.selector</code> field located in the <em>service.yaml</em> file:</p>
<pre><code>$ <b>minikube kubectl -- get endpoints -l app=telnet-server</b>
NAME                    ENDPOINTS                         AGE
telnet-server           172.17.0.3:2323,172.17.0.5:2323   20m
telnet-server-metrics   172.17.0.3:9000,172.17.0.5:9000   20m </code></pre>
<p>The <code>ENDPOINTS</code> column shows the internal Pod IP addresses with ports. Since you have two Pods, there are two IP addresses separated by a comma for each Service. If the Service can’t locate the Pods, the <code>ENDPOINTS</code> column will be set to <code>&lt;none&gt;</code>. If your <code>ENDPOINTS</code> column has <code>&lt;none&gt;</code>, check that the <code>spec.selector</code> field in your Service matches what is in the <code>spec.template.metadata.labels</code> field in the <em>deployment.yaml</em> file. I have preset it to the label <code>app: telnet-server</code> in the example. Having mismatched labels between a Service and a resource is a common mistake; it will happen to you at least once. </p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p>	Visit <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/" class="LinkURL">https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/</a> for more debugging tips and possible solutions.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<h3 id="h2-502482c07-0016"><span epub:type="pagebreak" title="92" id="Page_92"/>Killing a Pod</h3>
<p class="BodyFirst">Another great feature of Deployments is recovery. Failure is going to happen, so embrace it! A Deployment will get you back up and to full strength in no time. Remember, a Deployment’s main purpose is to keep the desired number of Pods running. To test this, you’ll delete one of the <code>telnet-server</code> Pods and then watch the Deployment respawn another in its place. First, you’ll need to fetch one of the <code>telnet-server</code> Pods’ names and delete it.</p>
<p>Enter the following to get the <code>telnet-server</code> Pods again:</p>
<pre><code>$ <b>minikube kubectl -- get pods -l app=telnet-server</b>
NAME                            READY   STATUS    RESTARTS   AGE
telnet-server-775769766-2bmd5   1/1     Running   0          25m
telnet-server-775769766-k9kx9   1/1     Running   0          25m</code></pre>
<p>It really doesn’t matter which Pod you delete, so just choose the first one on the list, which is <code>telnet-server-775769766-2bmd5</code> on my cluster. (Your Pod names will be different, as they are autogenerated.) </p>
<p>Now, enter the following command to <code>delete</code> the selected Pod:</p>
<pre><code>$ <b>minikube kubectl -- delete pod </b><var class="bold">&lt;telnet-server-775769766-2bmd5&gt;</var>
pod "telnet-server-775769766-2bmd5" deleted</code></pre>
<p>The command might appear to hang for a few seconds, but it will eventually finish when the Pod has terminated.</p>
<p>If you list the Pods again, you’ll see two Pods are still running, but now the <code>telnet-server-775769766-2bmd5</code> is gone and has been replaced with a new Pod:</p>
<pre><code>$ <b>minikube kubectl -- get pods -l app=telnet-server</b>
NAME                            READY   STATUS    RESTARTS   AGE
telnet-server-775769766-k9kx9   1/1     Running   0          25m
telnet-server-775769766-rdg5w   1/1     Running   0          1m16s</code></pre>
<p class="BodyContinued">This new Pod, named <code>telnet-server-775769766-rdg5w</code>, is more than a minute old, is <code>Running</code>, and is ready to accept connections.</p>
<h3 id="h2-502482c07-0017">Scaling</h3>
<p class="BodyFirst">Let’s pretend the telnet-server application really resonates with the nostalgic over-35 crowd and becomes a runaway success. The two telnet-servers will no longer be adequate for handling the increased traffic, so you’ll need to scale up your replicas to a count greater than two. You can do this in two ways. The first way is to edit the <em>deployment.yaml</em> manifest file and apply the changes to the cluster using the <code>minikube apply</code> command. The second way is to use the <code>minikube kubectl scale</code> command. I’ll demonstrate this example using the <code>minikube kubectl scale</code> command, since you already learned how to apply manifest files earlier in this chapter.</p>
<p>You are going to increase the Deployment replica count by one, bringing the total number of Pods to three. (In a real production environment, <span epub:type="pagebreak" title="93" id="Page_93"/>you would base the replica count number off some key metrics instead of a finger in the wind.) Enter the following command to scale up the <code>telnet-server</code> Deployment:</p>
<pre><code>$ <b>minikube kubectl -- scale deployment telnet-server --replicas=3</b>
deployment.apps/telnet-server scaled</code></pre>
<p class="BodyContinued">The <code>scale deployment</code> command takes a <code>--replicas</code> flag to set the number of Pod replicas. The output shows the <code>telnet-server</code> Deployment has <code>scaled</code>, but let’s verify this. </p>
<p>Enter the following command to verify that the replica count has changed for your Deployment:</p>
<pre><code>$ <b>minikube kubectl -- get deployments.apps telnet-server</b>
NAME            READY   UP-TO-DATE   AVAILABLE   AGE
telnet-server   3/3     3            3           17m</code></pre>
<p class="BodyContinued">Here, you get the Deployment resource information for <code>telnet-server</code>. The Deployment has three out of three <code>(3/3)</code> replicas <code>READY</code>, up from the two it had earlier. </p>
<p>The <code>scale</code> command changes the replica count in real time on the cluster. This can be dangerous. If a colleague pushes out a new version of the <code>telnet-server</code> application right after you scaled from the command line, the replica state will not match. This is because when he or she runs the <code>minikube kubectl -- apply -f kubernetes/deployment.yaml</code> command, the Deployment replica count would go back to two, since that’s what’s stated in the <em>deployment.yaml</em> manifest file. </p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">note</span></h2>
<p>	Changing the replica count or editing any resources live via the command line is more trouble than it’s worth, as doing so often causes a split brain between the running state and the saved state in your manifests. To save yourself time debugging and avoid causing your customers pain, always opt for infrastructure changes that are tracked and versioned in source control instead of quick-and-dirty live changes. </p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<h3 id="h2-502482c07-0018">Logs</h3>
<p class="BodyFirst">The last piece of orchestration to test is accessing the telnet-server application logs. Fortunately, Kubernetes makes this simple with the <code>kubectl</code> <code>logs</code> subcommand. You want to grab the logs for all three of your telnet-server Pods. One way to do this is to execute the <code>logs</code> command for each of the three Pods and view the results. Enter the following command to view one of the Pods logs (remember, your Pod names will be different from mine):</p>
<pre><code>$ <b>minikube kubectl -- logs </b><var class="bold">&lt;telnet-server-775769766-rdg5w&gt;</var>
<var>--snip--</var></code></pre>
<p>This works fine if you do not have many Pods or if you know which Pod an event happened on. If not, a better option is to grab all the Pods logs <span epub:type="pagebreak" title="94" id="Page_94"/>at the same time and mark each log line with the Pod name from which it came. Enter the following command to fetch all the logs for each Pod:</p>
<pre><code>$ <b>minikube kubectl -- logs -l app=telnet-server --all-containers=true --prefix=true</b>
[pod/telnet-server-775769766-k9kx9/telnet-server] telnet-server: 2022/02/03 21:07:30 telnet-server listening on [::]:2323
[pod/telnet-server-775769766-k9kx9/telnet-server] telnet-server: 2022/02/03 21:07:30 Metrics endpoint listening on :9000
<var>--snip--</var></code></pre>
<p>Quite a few flags are used in this command; let’s break each one down:</p>
<ul>
<li>To fetch only Pods with this label: <code>-l app=telnet-server</code></li>
<li>When you have multiple Pods and want to see all the logs: <code>--all-containers=true</code></li>
<li>Each log line with the Pod name from which the log came: <code>--prefix=true</code></li>
</ul>
<p>The output should show at least six log lines—two start-up log lines for each Pod (3) and whatever other logs may have shown up from connecting earlier with the <code>telnet</code> command. The log output is not important now, as you just need to make sure you can access the logs for your application.</p>
<h2 id="h1-502482c07-0005">Summary</h2>
<p class="BodyFirst">In this chapter, you learned how to run the telnet-server container image inside a Kubernetes cluster. You successfully orchestrated your application by using a Kubernetes Deployment resource that you exposed to your local host via a Kubernetes Service. Finally, you explored how to create, query, and view your resources and logs with the <code>minikube kubectl</code> command. In the next chapter, you’ll learn to automate the deployment of telnet-server by implementing a simple delivery pipeline inside Kubernetes.</p>
</section>
</body></html>