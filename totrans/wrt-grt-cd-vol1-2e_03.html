<html><head></head><body>
<h2 class="h2" id="ch04"><span epub:type="pagebreak" id="page_61"/><strong><span class="big">4</span><br/>FLOATING-POINT REPRESENTATION</strong></h2>&#13;
<div class="image1"><img alt="Image" src="../images/comm1.jpg"/></div>&#13;
<p class="noindents">Floating-point arithmetic is an approximation of real arithmetic that solves the major problem with integer data types—the inability to represent fractional values. However, the inaccuracies in this approximation can lead to serious defects in application software. In order to write great software that produces correct results when using floating-point arithmetic, programmers must be aware of the machine’s underlying numeric representation and exactly how floating-point arithmetic approximates real arithmetic.</p>&#13;
<h3 class="h3" id="sec4_1"><strong>4.1 Introduction to Floating-Point Arithmetic</strong></h3>&#13;
<p class="noindent">There is an infinite number of possible real values. Floating-point representation uses a finite number of bits and, therefore, can represent a finite number of different values. When a given floating-point format cannot exactly represent some real value, the closest value that the format <em>can</em> exactly represent is <span epub:type="pagebreak" id="page_62"/>used. This section describes how the floating-point format works so you can better understand the drawbacks of these approximations.</p>&#13;
<p class="indent">Consider a couple of problems with integer and fixed-point formats. Integers cannot represent any fractional values, and they can represent only values in the range 0 through 2<sup><em>n</em></sup> – 1 or –2<sup><em>n</em></sup><sup>–1</sup> through 2<sup><em>n</em></sup><sup>–1</sup> – 1. Fixed-point formats represent fractional values, but at the expense of the range of integer values they can represent. This problem, which the floating-point format solves, is one of <em><a href="gloss01.xhtml#gloss01_85">dynamic range</a></em>.</p>&#13;
<p class="indent">Consider a simple 16-bit unsigned fixed-point format that uses 8 bits for the fractional component and 8 bits for the integer component of the number. The integer component can represent values in the range 0 through 255, and the fractional component can represent the values 0 and fractions between 2<sup>–8</sup> and 1 (with a resolution of about 2<sup>–8</sup>). If in a string of calculations you need only 2 bits to represent the fractional values 0.0, 0.25, 0.5, and 0.75, the extra 6 bits in the fractional part of the number go to waste. Wouldn’t it be nice if we could utilize those bits in the integer portion of the number to extend its range from 0 through 255 to 0 through 16,383? Well, that’s the basic concept behind the floating-point representation.</p>&#13;
<p class="indent">In a floating-point value, the radix point (binary point) can float between digits in the number as needed. So, in a 16-bit binary number that needs only 2 bits of precision for the fractional component, the binary point can float down between bits 1 and 2, leaving bits 2 through 15 for the integer portion. A floating-point format needs one additional field to specify the position of the radix point within the number, equivalent to the exponent in scientific notation.</p>&#13;
<p class="indent">Most floating-point formats use some number of bits to represent a mantissa and a smaller number of bits to represent an exponent<em>.</em> The <em><a href="gloss01.xhtml#gloss01_145">mantissa</a></em> is a base value that usually falls within a limited range (for example, between 0 and 1). The <em><a href="gloss01.xhtml#gloss01_92">exponent</a></em> is a multiplier that, when applied to the mantissa, produces values outside this range. The big advantage of the mantissa/exponent configuration is that a floating-point format can represent values across a wide range. However, separating the number into these two parts means floating-point formats can represent only numbers with a specific number of <em>significant</em> digits. If the difference between the smallest and largest exponent is greater than the number of significant digits in the mantissa (and it usually is), then the floating-point format cannot exactly represent all the integers between the smallest and largest values in the floating-point representation.</p>&#13;
<p class="indent">To see the impact of limited-precision arithmetic, we’ll adopt a simplified <em>decimal</em> floating-point format for our examples. Our floating-point format will use a mantissa with three significant digits and a decimal exponent with two digits. The mantissa and exponents are both signed values, as shown in <a href="ch04.xhtml#ch04fig01">Figure 4-1</a>.</p>&#13;
<div class="image"><img alt="image" src="../images/04fig01.jpg"/></div>&#13;
<p class="figcap"><a id="ch04fig01"/><em>Figure 4-1: Simple floating-point format</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_63"/>This particular floating-point representation can approximate all the values between 0.00 and 9.99 × 10<sup>99</sup>. However, this format cannot represent all (integer) values in this range (that would take 100 digits of precision!). A value like 9,876,543,210 would be approximated with 9.88 × 10<sup>9</sup> (or <span class="literal">9.88e+9</span> in programming language notation, which this book will generally use).</p>&#13;
<p class="indent">You cannot <em>exactly</em> represent as many different values with a floating-point format as with an integer format because the floating-point format encodes multiple representations (that is, different bit patterns) for the same value. In the simplified decimal floating-point format shown in <a href="ch04.xhtml#ch04fig01">Figure 4-1</a>, for example, <span class="literal">1.00e</span> <span class="literal">+</span> <span class="literal">1</span> and <span class="literal">0.10e + 2</span> are different representations of the same value. Because the number of different possible representations is finite, whenever a single value has two possible representations, that’s one less unique value the format can represent.</p>&#13;
<p class="indent">Furthermore, the floating-point format, a form of scientific notation, complicates arithmetic somewhat. When adding and subtracting two numbers in scientific notation, you must adjust the two values so that their exponents are the same. For example, when adding <span class="literal">1.23e1</span> and <span class="literal">4.56e0</span>, you could convert <span class="literal">4.56e0</span> to <span class="literal">0.456e1</span> and then add them. The result, <span class="literal">1.686e1</span>, does not fit into the three significant digits of our current format, so we must either <em>round</em> or <em>truncate</em> the result to three significant digits. Rounding generally produces the most accurate result, so let’s round the result to obtain <span class="literal">1.69e1</span>. The lack of <em><a href="gloss01.xhtml#gloss01_202">precision</a></em> (the number of digits or bits maintained in a computation) affects the <em><a href="gloss01.xhtml#gloss01_4">accuracy</a></em> (the correctness of the computation).</p>&#13;
<p class="indent">In the previous example, we were able to round the result because we maintained <em>four</em> significant digits <em>during</em> the calculation. If our floating-point calculation were limited to three significant digits during computation, we would have had to truncate (throw away) the last digit of the smaller number, obtaining <span class="literal">1.68e1</span>, which is even less correct. Therefore, to improve the accuracy, we use extra digits during the calculation. These extra digits are known as <em>guard digits</em> (or <em>guard bits</em> in the case of a binary format). They greatly enhance accuracy during a long chain of computations.</p>&#13;
<p class="indent">The accuracy lost during a single computation usually isn’t bad. However, the error can accumulate over a sequence of floating-point operations and greatly affect the computation itself. For example, suppose we add <span class="literal">1.23e3</span> and <span class="literal">1.00e0</span>. Adjusting the numbers so their exponents are the same before the addition produces <span class="literal">1.23e3</span> <span class="literal">+</span> <span class="literal">0.001e3</span>. The sum of these two values, even after rounding, is <span class="literal">1.23e3</span>. This might seem perfectly reasonable to you: if we can maintain only three significant digits, adding in a small value shouldn’t affect the result. However, suppose we add <span class="literal">1.00e0</span> to <span class="literal">1.23e3</span> <em>10 times</em>. The first time we add <span class="literal">1.00e0</span> to <span class="literal">1.23e3</span>, we get <span class="literal">1.23e3</span>. Likewise, we get this same result the second, third, fourth . . . and tenth time. Had we added <span class="literal">1.00e0</span> to itself 10 times, then added the result (<span class="literal">1.00e1</span>) to <span class="literal">1.23e3</span>, we would obtain a different result, <span class="literal">1.24e3</span>. This is an important rule of limited-precision arithmetic:</p>&#13;
<p class="block-quote">The order of evaluation can affect the accuracy of the result.</p>&#13;
<p class="indent">Adding or subtracting numbers with relative magnitudes (that is, the sizes of the exponents) that are similar produces better results. If you’re performing a chain calculation involving addition and subtraction, you <span epub:type="pagebreak" id="page_64"/>should group the operations so that you can add or subtract values whose magnitudes are close to one another before adding or subtracting values whose magnitudes are not as close.</p>&#13;
<p class="indent">Another problem with addition and subtraction is <em><a href="gloss01.xhtml#gloss01_94">false precision</a></em>. Consider the computation <span class="literal">1.23e0</span> <span class="literal">-</span> <span class="literal">1.22e0</span>. This produces <span class="literal">0.01e0</span>. Although this is mathematically equivalent to <span class="literal">1.00e</span> <span class="literal">–</span> <span class="literal">2</span>, this latter form suggests that the last two digits (in the thousandths and ten-thousandths place) are both exactly 0. Unfortunately, we only have a single significant digit after this computation, which is in the hundredths place, and some FPUs or floating-point software packages might actually insert random digits (or bits) into the LO positions. This brings up a second important rule:</p>&#13;
<p class="block-quote">Whenever subtracting two numbers with the same signs or adding two numbers with different signs, the accuracy of the result may be less than the precision available in the floating-point format.</p>&#13;
<p class="indent">Multiplication and division do not suffer from these problems, because you don’t have to adjust the exponents before the operation; all you need to do is add the exponents and multiply the mantissas (or subtract the exponents and divide the mantissas). By themselves, multiplication and division do not produce particularly poor results. However, they exacerbate any accuracy error that already exists in a value. For example, if you multiply <span class="literal">1.23e0</span> by 2, when you should be multiplying <span class="literal">1.24e0</span> by 2, the result is even less accurate than it was. This brings up a third important rule:</p>&#13;
<p class="block-quote">When performing a chain of calculations involving addition, subtraction, multiplication, and division, perform the multiplication and division operations first.</p>&#13;
<p class="indent">Often, by applying normal algebraic transformations, you can arrange a calculation so the multiplication and division operations occur first. For example, suppose you want to compute the following:</p>&#13;
<p class="equation"><em>x</em> × (<em>y</em> + <em>z</em>)</p>&#13;
<p class="indent">Normally, you would add <em>y</em> and <em>z</em> together and multiply their sum by <em>x</em>. However, you’ll get a little more accuracy if you first transform the expression to the following:</p>&#13;
<p class="equation"><em>x</em> × <em>y</em> + <em>x</em> × <em>z</em></p>&#13;
<p class="indent">Now you can compute the result by performing the multiplications first.<sup><a href="footnotes.xhtml#fn4_1a" id="fn4_1">1</a></sup></p>&#13;
<p class="indent">Multiplication and division have other problems as well. When you multiply two very large or very small numbers, <em><a href="gloss01.xhtml#gloss01_188">overflow</a></em> or <em><a href="gloss01.xhtml#gloss01_249">underflow</a></em> may occur. The same situation occurs when you divide a small number by a large number, or a large number by a small number. This brings up a fourth rule:</p>&#13;
<p class="block-quote">When multiplying and dividing sets of numbers, try to multiply and divide numbers that have the same relative magnitudes.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_65"/>Comparing floating-point numbers is very dangerous. Given the inaccuracies inherent in any computation (including converting an input string to a floating-point value), you should <em>never</em> compare two floating-point values to see if they are equal. Different computations that produce the same (mathematical) result may differ in their least significant bits. For example, adding <span class="literal">1.31e0</span> and <span class="literal">1.69e0</span> should produce <span class="literal">3.00e0</span>. Likewise, adding <span class="literal">1.50e0</span> and <span class="literal">1.50e0</span> should produce <span class="literal">3.00e0</span>. However, were you to compare (<span class="literal">1.31e0</span> <span class="literal">+</span> <span class="literal">1.69e0</span>) to (<span class="literal">1.50e0</span> + <span class="literal">1.50e0</span>), you might find that these sums are <em>not</em> equal. Because two seemingly equivalent floating-point computations will not necessarily produce exactly equal results, a straight comparison for equality—which succeeds if and only if all bits (or digits) in the two operands are the same—may fail.</p>&#13;
<p class="indent">To test for equality between floating-point numbers, determine how much error (or tolerance) you’ll allow in a comparison, and then check to see if one value is within this error range of the other, like so:</p>&#13;
<p class="programs">if(  (<span class="EmpItalic1">Value1</span> &gt;= (<span class="EmpItalic1">Value2</span> – <span class="EmpItalic1">error</span>))  and  (<span class="EmpItalic1">Value1</span> &lt;= (<span class="EmpItalic1">Value2</span> + <span class="EmpItalic1">error</span>)) then . . .</p>&#13;
<p class="indent">More efficient is to use a statement of the form:</p>&#13;
<p class="programs">if( abs(<span class="EmpItalic1">Value1</span> – <span class="EmpItalic1">Value2</span>) &lt;= <span class="EmpItalic1">error</span> ) then . . .</p>&#13;
<p class="indent">The value for <span class="EmpItalic">error</span> should be slightly greater than the largest amount of error that will creep into your computations. The exact value depends upon the particular floating-point format you use and the magnitudes of the values you are comparing. So, the final rule is this:</p>&#13;
<p class="block-quote">When comparing two floating-point numbers for equality, always compare the values to see if the difference between two values is less than some small error value.</p>&#13;
<p class="indent">Checking two floating-point numbers for equality is a very famous problem, one that almost every introductory programming text discusses. The same problems with comparing for less than or greater than, however, are not as well known. Suppose that a sequence of floating-point calculations produces a result that is accurate only to within ±<span class="EmpItalic">error</span>, even though the floating-point representation provides better accuracy than <span class="EmpItalic">error</span> suggests. If you compare such a result against some other calculation computed with less accumulated error, and those two values are very close to each other, then comparing them for less than or greater than may produce incorrect results.</p>&#13;
<p class="indent">For example, suppose that some chain of calculations in our simplified decimal representation produces 1.25, which is accurate only to ±0.05 (that is, the real value could be somewhere between 1.20 and 1.30), and a second chain of calculations produces 1.27, which is accurate to the full precision of our floating-point representation (that is, the actual value, before rounding, is somewhere between 1.265 and 1.275). Comparing the result of the first calculation (1.25) to the result of the second calculation (1.27) finds that the first result is less than the second. Unfortunately, given the inaccuracy of the <span epub:type="pagebreak" id="page_66"/>first calculation, this might not be true—for example, if the correct result of the first computation is in the range 1.27 to 1.30 (exclusive).</p>&#13;
<p class="indent">About the only reasonable test is to see if the two values are within the error tolerance of each other. If so, treat the values as equal (neither is considered less than or greater than the other). If the values are not equal within the desired error tolerance, you can compare them to see if one value is less than or greater than the other. This is known as a <em>miserly approach</em>; that is, we try to find as few values that are less than or greater than as possible.</p>&#13;
<p class="indent">The other possibility is to use an <em>eager approach</em>, which attempts to make the result of the comparison <span class="literal">true</span> as often as possible. Given two values to compare and an error tolerance, here’s how you’d eagerly compare the two values for less than or greater than:</p>&#13;
<p class="programs">if( A &lt; (B + <span class="EmpItalic1">error</span>) ) then Eager_A_lessthan_B;<br/>&#13;
if( A &gt; (B – <span class="EmpItalic1">error</span>) ) then Eager_A_greaterthan_B;</p>&#13;
<p class="indent">Don’t forget that calculations like (<span class="literal">B +</span> <span class="EmpItalic">error</span>) are subject to their own inaccuracies, depending on the relative magnitudes of the values <span class="literal">B</span> and <span class="EmpItalic">error</span>, and the inaccuracy of this calculation may affect the final result of the comparison.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>Due to space limitations, this book merely touches on some major problems that can occur when you’re using floating-point values and why you can’t treat floating-point arithmetic like real arithmetic. For further details, consult a good text on numerical analysis or even scientific computing. If you’re going to be working with floating-point arithmetic,</em> in any language, <em>take some time to study the effects of limited-precision arithmetic on your computations.</em></p>&#13;
</div>&#13;
<h3 class="h3" id="sec4_2"><strong>4.2 IEEE Floating-Point Formats</strong></h3>&#13;
<p class="noindent">When Intel planned to introduce a floating-point unit (FPU) for its original 8086 microprocessor, the company was smart enough to realize that the electrical engineers and solid-state physicists who design chips probably didn’t have the necessary numerical analysis background to design a good floating-point representation. So, Intel went out and hired the best numerical analyst it could find to design a floating-point format for its 8087 FPU. That person then hired two other experts in the field, and the three of them (Kahan, Coonen, and Stone) designed the <em>KCS Floating-Point Standard</em>. They did such a good job that the IEEE organization used this format as the basis for the IEEE Std 754 floating-point format.</p>&#13;
<p class="indent">To handle a wide range of performance and accuracy requirements, Intel actually introduced <em>three</em> floating-point formats: single precision, double precision, and extended precision. The single- and double-precision formats corresponded to C’s <span class="literal">float</span> and <span class="literal">double</span> types or FORTRAN’s <span class="literal">real</span> and <span class="literal">double precision</span> types. Extended precision contains 16 extra bits that long chains of computations can use as guard bits before rounding down to a double-precision value when storing the result.</p>&#13;
<h4 class="h4" id="sec4_2_1"><span epub:type="pagebreak" id="page_67"/><strong><em>4.2.1 Single-Precision Floating-Point Format</em></strong></h4>&#13;
<p class="noindent">The single-precision format uses a 24-bit mantissa and an 8-bit exponent. The mantissa represents a value between 1.0 and just less than 2.0. The HO bit of the mantissa is always <span class="literal">1</span> and represents a value just to the left of the binary point. The remaining 23 mantissa bits appear to the right of the binary point and represent the value:</p>&#13;
<p class="programs">1.<span class="EmpItalic1">mmmmmmm</span> <span class="EmpItalic1">mmmmmmmm</span> <span class="EmpItalic1">mmmmmmmm</span></p>&#13;
<p class="indent">The mantissa is always greater than or equal to 1 because of the implied <span class="literal">1</span> bit. Even if the other mantissa bits are all <span class="literal">0</span>, the implied <span class="literal">1</span> bit always gives us the value <span class="literal">1</span>. Each position to the right of the binary point represents a value (<span class="literal">0</span> or <span class="literal">1</span>) times a successive negative power of 2, but even if we had an almost infinite number of <span class="literal">1</span> bits after the binary point, they still would not add up to 2. So, the mantissa can represent values in the range 1.0 to just less than 2.0.</p>&#13;
<p class="indent">Some examples would probably be useful here. Consider the decimal value 1.7997. Here are the steps we could go through to compute the binary mantissa for this value:</p>&#13;
<ol>&#13;
<li class="noindent">Subtract 2<sup>0</sup> from 1.7997 to produce 0.7997 and <span class="literal">%1.00000000000000000000000</span>.</li>&#13;
<li class="noindent">Subtract 2<sup>–1</sup> (<sup>1</sup>/<sub>2</sub>) from 0.7997 to produce 0.2997 and <span class="literal">%1.10000000000000000000000</span>.</li>&#13;
<li class="noindent">Subtract 2<sup>–2</sup> (<sup>1</sup>/<sub>4</sub>) from 0.2997 to produce 0.0497 and <span class="literal">%1.11000000000000000000000</span>.</li>&#13;
<li class="noindent">Subtract 2<sup>–5</sup> (<sup>1</sup>/<sub>32</sub>) from 0.0497 to produce 0.0185 and <span class="literal">%1.11001000000000000000000</span>.</li>&#13;
<li class="noindent">Subtract 2<sup>–6</sup> (<sup>1</sup>/<sub>64</sub>) from 0.0185 to produce 0.00284 and <span class="literal">%1.11001100000000000000000</span>.</li>&#13;
<li class="noindent">Subtract 2<sup>–9</sup> (<sup>1</sup>/<sub>512</sub>) from 0.00284 to produce 0.000871 and <span class="literal">%1.11001100100000000000000</span>.</li>&#13;
<li class="noindent">Subtract 2<sup>-10</sup> (<sup>1</sup>/<sub>1,024</sub>) from 0.000871 to (approximately) produce 0 and <span class="literal">%1.11001100110000000000000</span>.</li></ol>&#13;
<p class="indent">Although there is an infinite number of values between 1 and 2, we can represent only 8 million (2<sup>23</sup> ) of them because we use a 23-bit mantissa (the 24th bit is always <span class="literal">1</span>), and therefore have only 23 bits of precision.</p>&#13;
<p class="indent">The mantissa uses a <em>one’s complement</em> format rather than two’s complement. This means that the 24-bit value of the mantissa is simply an unsigned binary number, and the sign bit, in bit position 31, determines whether that value is positive or negative. One’s complement has the unusual property that there are two representations for <span class="literal">0</span> (with the sign bit set or clear). Generally, this is important only to the person designing the floating-point software or hardware system. We’ll assume that the value <span class="literal">0</span> always has the sign bit clear.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_68"/>The single-precision floating-point format is shown in <a href="ch04.xhtml#ch04fig02">Figure 4-2</a>.</p>&#13;
<div class="image"><img alt="image" src="../images/04fig02.jpg"/></div>&#13;
<p class="figcap"><a id="ch04fig02"/><em>Figure 4-2: Single-precision (32-bit) floating-point format</em></p>&#13;
<p class="indent">We represent values outside the range of the mantissa by raising 2 to the power specified by the exponent and then multiplying the result by the mantissa. The exponent is 8 bits and uses an <a href="gloss01.xhtml#gloss01_89"><em>excess-127</em> format</a> (sometimes called <em>bias-127 exponents</em>). In excess-127 format, the exponent 2<sup>0</sup> is represented by the value 127 (<span class="literal">$7f</span>). To convert an exponent to excess-127 format, add 127 to the exponent value. For example, the single-precision representation for 1.0 is <span class="literal">$3f800000</span>. The mantissa is 1.0 (including the implied bit) and the exponent is 2<sup>0</sup>, encoded as 127 (<span class="literal">$7f</span>). The representation for 2.0 is <span class="literal">$40000000</span>, with the exponent 2<sup>1</sup> encoded as 128 (<span class="literal">$80</span>).</p>&#13;
<p class="indent">The excess-127 exponent makes it easy to compare two floating-point numbers for less than or greater than as though they were unsigned integers, as long as we handle the sign bit (bit 31) separately. If the signs of the two values are not equal, then the positive value (the one with bit 31 set to <span class="literal">0</span>) is greater than the value that has the HO bit set to <span class="literal">1</span>.<sup><a href="footnotes.xhtml#fn4_2a" id="fn4_2">2</a></sup> If the sign bits are both <span class="literal">0</span>, we use a straight unsigned binary comparison. If the signs are both <span class="literal">1</span>, we do an unsigned comparison but invert the result (that is, we treat less than as greater than and vice versa). On some CPUs, where a 32-bit unsigned comparison is much faster than a 32-bit floating-point comparison, it’s probably worthwhile to do the comparison using integer arithmetic rather than floating-point arithmetic.</p>&#13;
<p class="indent">A 24-bit mantissa provides approximately 6½ decimal digits of precision (one-half digit of precision means that the first six digits can be in the range 0..9, but the seventh digit can only be in the range 0 through <em>x</em> where <em>x</em> &lt; 9 and is generally close to 5). With an 8-bit excess-127 exponent, the dynamic range of single-precision floating-point numbers is approximately 2<sup>±128</sup> or about 10<sup>±38</sup>.</p>&#13;
<p class="indent">Although single-precision floating-point numbers are perfectly suitable for many applications, the dynamic range is unsuitable for many financial, scientific, and other applications. Furthermore, during long chains of computations, the limited accuracy may introduce significant error. For serious calculations, we need a floating-point format with more precision.</p>&#13;
<h4 class="h4" id="sec4_2_2"><span epub:type="pagebreak" id="page_69"/><strong><em>4.2.2 Double-Precision Floating-Point Format</em></strong></h4>&#13;
<p class="noindent">The double-precision format helps overcome the problems of the single-precision floating-point. Using twice the space, the double-precision format has an 11-bit excess-1,023 exponent, a 53-bit mantissa (including an implied HO bit of <span class="literal">1</span>), and a sign bit. This provides a dynamic range of about 10<sup>±308</sup> and 15 to 16+ digits of precision, which is sufficient for most applications. Double-precision floating-point values take the form shown in <a href="ch04.xhtml#ch04fig03">Figure 4-3</a>.</p>&#13;
<div class="image"><img alt="image" src="../images/04fig03.jpg"/></div>&#13;
<p class="figcap"><a id="ch04fig03"/><em>Figure 4-3: Double-precision (64-bit) floating-point format</em></p>&#13;
<h4 class="h4" id="sec4_2_3"><strong><em>4.2.3 Extended-Precision Floating-Point Format</em></strong></h4>&#13;
<p class="noindent">To ensure accuracy during long chains of computations involving double-precision floating-point numbers, Intel designed the extended-precision format. The extended-precision format uses 80 bits: a 64-bit mantissa, a 15-bit excess-16,383 exponent, and a 1-bit sign. The mantissa does not have an implied HO bit that is always <span class="literal">1</span>. The format for the extended-precision floating-point value appears in <a href="ch04.xhtml#ch04fig04">Figure 4-4</a>.</p>&#13;
<div class="image"><img alt="image" src="../images/04fig04.jpg"/></div>&#13;
<p class="figcap"><a id="ch04fig04"/><em>Figure 4-4: Extended-precision (80-bit) floating-point format</em></p>&#13;
<p class="indent">On the 80x86 FPUs, all computations use the extended-precision form. Whenever you load a single- or double-precision value, the FPU automatically converts it to an extended-precision value. Likewise, when you store a single- or double-precision value to memory, the FPU automatically rounds the value down to the appropriate size before storing it. The extended-precision format guarantees the inclusion of a large number of guard bits in 32- and 64-bit computations, which helps ensure (but not guarantee) that you’ll get full 32- or 64-bit accuracy in your computations. Some error will inevitably creep into the LO bits because the FPUs provide no guard bits for 80-bit computations (the FPU uses only 64 mantissa bits during 80-bit computations). While you can’t assume that you’ll get an accurate 80-bit computation, you can usually do better than 64 bits when using the extended-precision format.</p>&#13;
<p class="indent">Non-Intel CPUs that support floating-point arithmetic generally provide only the 32-bit and 64-bit formats. Therefore, calculations on those CPUs may produce less accurate results than the equivalent string of calculations on the 80x86 using 80-bit calculations. Also note that modern x86-64 CPUs have additional floating-point hardware as part of the SSE <span epub:type="pagebreak" id="page_70"/>extensions; however, those SSE extensions support only 64- and 32-bit floating-point calculations.</p>&#13;
<h4 class="h4" id="sec4_2_4"><strong><em>4.2.4 Quad-Precision Floating-Point Format</em></strong></h4>&#13;
<p class="noindent">The original 80-bit extended-precision floating-point format was a stopgap measure. From a “types should be consistent” point of view, the proper extension to the 64-bit floating-point format should have been a 128-bit floating-point format. Alas, when Intel was working on floating-point formats in the late 1970s, a quad-precision (128-bit) floating-point format was too expensive to implement in hardware, so the 80-bit extended-precision format became the interim compromise. Today, a few CPUs (such as IBM’s POWER9 and later-version ARMs) are capable of quad-precision floating-point arithmetic.</p>&#13;
<p class="indent">The IEEE Std 754 quad-precision floating-point format uses a single sign bit, a 15-bit excess-16,383 biased exponent, and a 112-bit (with implied 113th bit) mantissa (see <a href="ch04.xhtml#ch04fig05">Figure 4-5</a>). This provides 36 decimal digits of precision and exponents in the approximate range 10<sup>±4932</sup>.</p>&#13;
<div class="image"><img alt="image" src="../images/04fig05.jpg"/></div>&#13;
<p class="figcap"><a id="ch04fig05"/><em>Figure 4-5: Extended-precision (80-bit) floating-point format</em></p>&#13;
<h3 class="h3" id="sec4_3"><strong>4.3 Normalization and Denormalized Values</strong></h3>&#13;
<p class="noindent">To maintain maximum precision during floating-point computations, most computations use <em>normalized</em> values. A normalized floating-point value is one whose HO mantissa bit contains <span class="literal">1</span>. A floating-point computation will be more accurate if it involves only normalized values because the mantissa has that many fewer bits of precision available for computation if several HO bits of the mantissa are all <span class="literal">0</span>.</p>&#13;
<p class="indent">You can normalize almost any unnormalized value by shifting the mantissa bits to the left and decrementing the exponent until a <span class="literal">1</span> appears in the mantissa’s HO bit.<sup><a href="footnotes.xhtml#fn4_3a" id="fn4_3">3</a></sup> Remember, the exponent is a binary exponent. Each time you increment the exponent, you multiply the floating-point value by 2. Likewise, whenever you decrement the exponent, you divide the floating-point value by 2. By the same token, shifting the mantissa to the left one bit position multiplies the floating-point value by 2, and shifting it to the right divides the floating-point value by 2. Therefore, shifting the mantissa to the left one position <em>and</em> decrementing the exponent does not change the value <span epub:type="pagebreak" id="page_71"/>of the floating-point number (this is why, as you saw earlier, there are multiple representations for certain numbers in the floating-point format).</p>&#13;
<p class="indent">Here’s an example of an unnormalized value:</p>&#13;
<p class="programs">0.100000 × 2<sup>1</sup></p>&#13;
<p class="indent">Shift the mantissa to the left one position and decrement the exponent to normalize it:</p>&#13;
<p class="programs">1.000000 × 2<sup>0</sup></p>&#13;
<p class="indent">There are two important cases in which a floating-point number cannot be normalized. First, 0 cannot be normalized because the floating-point representation contains all <span class="literal">0</span> bits in the exponent and mantissa fields. This, however, is not a problem, because we can exactly represent 0 with a single <span class="literal">0</span> bit, and extra bits of precision are unnecessary.</p>&#13;
<p class="indent">We also cannot normalize a floating-point number when we have some HO bits in the mantissa that are <span class="literal">0</span> but the biased exponent<sup><a href="footnotes.xhtml#fn4_4a" id="fn4_4">4</a></sup> is also <span class="literal">0</span> (and we can’t decrement it to normalize the mantissa). Rather than prohibiting certain small values whose HO mantissa bits and biased exponent are <span class="literal">0</span> (the most negative exponent possible), the IEEE standard permits special <em>denormalized</em> values in these cases.<sup><a href="footnotes.xhtml#fn4_5a" id="fn4_5">5</a></sup> Although the use of denormalized values enables IEEE floating-point computations to produce better results than if underflow occurred, denormalized values offer fewer bits of precision.</p>&#13;
<h3 class="h3" id="sec4_4"><strong>4.4 Rounding</strong></h3>&#13;
<p class="noindent">During a calculation, floating-point arithmetic functions may produce a result with greater precision than the floating-point format supports (the <em>guard bits</em> in the calculation maintain this extra precision). When the calculation is complete and the code needs to store the result back into a floating-point variable, something must be done about those extra bits of precision. How the system uses guard bits to affect the remaining bits is known as <em>rounding</em>, and how rounding is done can affect the accuracy of the computation. Traditionally, floating-point software and hardware use one of four different ways to round values: truncation, rounding up, rounding down, or rounding to nearest.</p>&#13;
<p class="indent">Truncation is easy, but it generates the least accurate results in a chain of computations. Few modern floating-point systems use truncation except as a means for converting floating-point values to integers (truncation is the standard conversion for coercing a floating-point value to an integer).</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_72"/>Rounding up leaves the value alone if the guard bits are all <span class="literal">0</span>, but if the current mantissa does not exactly fit into the destination bits, then rounding up sets the mantissa to the smallest possible larger value in the floating-point format. Like truncation, this is not a normal rounding mode. It is, however, useful for implementing functions like <span class="literal">ceil()</span>, which rounds a floating-point value to the smallest possible larger integer.</p>&#13;
<p class="indent">Rounding down is just like rounding up, except it rounds the result to the largest possible smaller value. This may sound like truncation, but there’s a subtle difference: truncation always rounds toward 0. For positive numbers, truncation and rounding down do the same thing. For negative values, truncation simply uses the existing bits in the mantissa, whereas rounding down will add a <span class="literal">1</span> bit to the LO position if the result was negative. This is also not a normal rounding mode, but it’s useful for implementing functions like <span class="literal">floor()</span>, which rounds a floating-point value to the largest possible smaller integer.</p>&#13;
<p class="indent">Rounding to nearest is the most intuitive way to process the guard bits. If the value of the guard bits is less than half the value of the mantissa’s LO bit, then rounding to nearest truncates the result to the largest possible smaller value (ignoring the sign). If the guard bits represent some value that is greater than half of the value of the LO mantissa bit, then rounding to nearest rounds the mantissa to the smallest possible greater value (ignoring the sign). If the guard bits represent a value that is exactly half the value of the mantissa’s LO bit, then the IEEE floating-point standard says that half the time it should round up and half the time it should round down. You do this by rounding the mantissa to the value that has a <span class="literal">0</span> in the LO bit position. That is, if the current mantissa already has a <span class="literal">0</span> in its LO bit, you use the current mantissa; if the current mantissa has a <span class="literal">1</span> in its LO bit, then you add 1 to round it up to the smallest possible larger value with a <span class="literal">0</span> in the LO bit. This scheme, mandated by the IEEE floating-point standard, produces the best possible result when loss of precision occurs.</p>&#13;
<p class="indent">Here are some examples of rounding, using 24-bit mantissas, with 4 guard bits (that is, these examples round 28-bit numbers to 24-bit numbers using the rounding to nearest algorithm):</p>&#13;
<p class="programs">1.000_0100_1010_0100_1001_0101_<span class="EmpItalic1">0001</span> -&gt; 1.000_0100_1010_0100_1001_0101<br/>&#13;
1.000_0100_1010_0100_1001_0101_<span class="EmpItalic1">1100</span> -&gt; 1.000_0100_1010_0100_1001_0110<br/>&#13;
1.000_0100_1010_0100_1001_0101_<span class="EmpItalic1">1000</span> -&gt; 1.000_0100_1010_0100_1001_0110<br/><br/>&#13;
1.000_0100_1010_0100_1001_0100_<span class="EmpItalic1">0001</span> -&gt; 1.000_0100_1010_0100_1001_0100<br/>&#13;
1.000_0100_1010_0100_1001_0100_<span class="EmpItalic1">1100</span> -&gt; 1.000_0100_1010_0100_1001_0101<br/>&#13;
1.000_0100_1010_0100_1001_0100_<span class="EmpItalic1">1000</span> -&gt; 1.000_0100_1010_0100_1001_0100</p>&#13;
<h3 class="h3" id="sec4_5"><span epub:type="pagebreak" id="page_73"/><strong>4.5 Special Floating-Point Values</strong></h3>&#13;
<p class="noindent">The IEEE floating-point format provides a special encoding for several special values. In this section, we’ll look these special values, their purpose and meaning, and their representation in the floating-point format.</p>&#13;
<p class="indent">Under normal circumstances, the exponent bits of a floating-point number do not contain all <span class="literal">0</span>s or all <span class="literal">1</span>s. An exponent containing all <span class="literal">1</span> or <span class="literal">0</span> bits indicates a special value.</p>&#13;
<p class="indent">If the exponent contains all <span class="literal">1</span>s and the mantissa is nonzero (discounting the implied bit), then the HO bit of the mantissa (again discounting the implied bit) determines whether the value represents a <em>quiet not-a-number</em> (QNaN) or a <em>signaling not-a-number</em> (SNaN) (see <a href="ch04.xhtml#ch04tab01">Table 4-1</a>). These not-a-number (NaN) results tell the system that some serious miscalculation has taken place and that the result of the calculation is completely undefined. QNaNs represent <em>indeterminate</em> results, while SNaNs specify that an <em>invalid</em> operation has taken place. Any calculation involving a NaN produces a NaN result, regardless of the values of any other operand(s). Note that the sign bit is irrelevant for NaNs. The binary representations of NaNs are shown in <a href="ch04.xhtml#ch04tab01">Table 4-1</a>.</p>&#13;
<p class="tabcap"><a id="ch04tab01"/><strong>Table 4-1:</strong> Binary Representations for NaN</p>&#13;
<table class="topbot-d">&#13;
<colgroup>&#13;
<col style="width:30%"/>&#13;
<col style="width:30%"/>&#13;
<col style="width:40%"/>&#13;
</colgroup>&#13;
<tbody>&#13;
<tr>&#13;
<td class="table-h" style="vertical-align: top;"><p class="tab_th"><strong>NaN</strong></p></td>&#13;
<td class="table-h" style="vertical-align: top;"><p class="tab_th"><strong>FP format</strong></p></td>&#13;
<td class="table-h" style="vertical-align: top;"><p class="tab_th"><strong>Value</strong></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba">SNaN</p></td>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba">32 bits</p></td>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba"><span class="literal">%s_11111111_0</span><span class="EmpItalic">xxxx...xx</span></p>&#13;
<p class="taba">(The value of <span class="literal">s</span> is irrelevant—at least one of the <span class="EmpItalic">x</span> bits must be nonzero.)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">SNaN</p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">64 bits</p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba"><span class="literal">%s_1111111111_0</span><span class="EmpItalic">xxxxx</span><span class="literal">...</span><span class="EmpItalic">x</span></p>&#13;
<p class="taba">(The value of <span class="literal">s</span> is irrelevant—at least one of the <span class="EmpItalic">x</span> bits must be nonzero.)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba">SNaN</p></td>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba">80 bits</p></td>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba"><span class="literal">%</span><span class="literal">s_1111111111_0</span><span class="EmpItalic">xxxxx...x</span></p>&#13;
<p class="taba">(The value of <span class="literal">s</span> is irrelevant—at least one of the <span class="EmpItalic">x</span> bits must be nonzero.)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">QNaN</p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">32 bits</p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba"><span class="literal">%s_11111111_1</span><span class="EmpItalic">xxxx...xx</span></p>&#13;
<p class="taba">(The value of <span class="literal">s</span> is irrelevant.)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba">QNaN</p></td>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba">64 bits</p></td>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba"><span class="literal">%s_1111111111_1</span><span class="EmpItalic">xxxxx...x</span></p>&#13;
<p class="taba">(The value of <span class="literal">s</span> is irrelevant.)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">QNaN</p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">80 bits</p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba"><span class="literal">%s_1111111111_1</span><span class="EmpItalic">xxxxx...x</span></p>&#13;
<p class="taba">(The value of <span class="literal">s</span> is irrelevant.)</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">Two other special values are represented when the exponent contains all 1 bits, and the mantissa contains all 0s. In such a case, the sign bit determines whether the result is the representation for +<em>infinity</em> or –<em>infinity</em>. Whenever a calculation involves infinity as one of the operands, the result will be one of the (well-defined) values found in <a href="ch04.xhtml#ch04tab02">Table 4-2</a>.</p>&#13;
<p class="tabcap"><span epub:type="pagebreak" id="page_74"/><a id="ch04tab02"/><strong>Table 4-2:</strong> Operations Involving Infinity</p>&#13;
<table class="topbot-d">&#13;
<colgroup>&#13;
<col style="width:50%"/>&#13;
<col style="width:50%"/>&#13;
</colgroup>&#13;
<tbody>&#13;
<tr>&#13;
<td class="table-h" style="vertical-align: top;"><p class="tab_th"><strong>Operation</strong></p></td>&#13;
<td class="table-h" style="vertical-align: top;"><p class="tab_th"><strong>Result</strong></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba"><span class="literal">n / ±infinity</span></p></td>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba"><span class="literal">0</span></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba"><span class="literal">±infinity × ±infinity</span></p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba"><span class="literal">±infinity</span></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba"><span class="literal">±nonzero / 0</span></p></td>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba"><span class="literal">±infinity</span></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba"><span class="literal">infinity + infinity</span></p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba"><span class="literal">infinity</span></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba"><span class="literal">n + infinity</span></p></td>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba"><span class="literal">infinity</span></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba"><span class="literal">n - infinity</span></p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba"><span class="literal">-infinity</span></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba"><span class="literal">±0 / ±0</span></p></td>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba"><span class="literal">NaN</span></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba"><span class="literal">infinity - infinity</span></p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba"><span class="literal">NaN</span></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba"><span class="literal">±infinity / ±infinity</span></p></td>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba"><span class="literal">NaN</span></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba"><span class="literal">±infinity × 0</span></p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba"><span class="literal">NaN</span></p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">Finally, if the exponent bits are all <span class="literal">0</span>, the sign bit indicates which of the two special values, –0 or +0, the floating-point number represents. Because the floating-point format uses a one’s complement notation, there are two separate representations for 0. Note that with respect to comparisons, arithmetic, and other operations, +0 is equal to –0.</p>&#13;
<div class="box5">&#13;
<p class="boxtitle-c">Using Multiple Representations of Zero</p>&#13;
<p class="noindent">The IEEE floating-point format supports both +0 and –0 (depending on the value of the sign bit), which are treated as equivalent by arithmetic calculations and comparisons—the sign bit is ignored. Software operating on floating-point values that represent 0 can use the sign bit as a flag to indicate different things. For example, you could use the sign bit to indicate that the value is exactly 0 (with the sign bit clear) or to indicate that it is nonzero but too small to represent with the current format (with the sign bit set). Intel recommends using the sign bit to indicate that 0 was produced via underflow of a negative value (with the sign bit set) or underflow of a positive number (with the sign bit clear). Presumably, their FPUs set the sign bit according to their recommendations when the FPUs produce a <span class="literal">0</span> result.</p>&#13;
</div>&#13;
<h3 class="h3" id="sec4_6"><strong>4.6 Floating-Point Exceptions</strong></h3>&#13;
<p class="noindent">The IEEE floating-point standard defines certain degenerate conditions under which the floating-point processor (or software-implemented floating-point code) should notify the application software. These exceptional conditions include the following:</p>&#13;
<ul>&#13;
<li class="noindent">Invalid operation</li>&#13;
<li class="noindent">Division by zero</li>&#13;
<li class="noindent"><span epub:type="pagebreak" id="page_75"/>Denormalized operand</li>&#13;
<li class="noindent">Numeric overflow</li>&#13;
<li class="noindent">Numeric underflow</li>&#13;
<li class="noindent">Inexact result</li>&#13;
</ul>&#13;
<p class="indent">Of these, inexact result is the least serious, because most floating-point calculations will produce an inexact result. A denormalized operand also isn’t too serious (though this exception indicates that your calculation may be less accurate as a result of less available precision). The other exceptions indicate a more serious problem, and you shouldn’t ignore them.</p>&#13;
<p class="indent">How the computer system notifies your application of these exceptions depends on the CPU/FPU, operating system, and programming language, so we can’t really go into how you might handle these exceptions. Generally, though, you can use the exception handling facilities in your programming language to trap these conditions as they occur. Note that most computer systems won’t notify you when one of the exceptional conditions exists unless you explicitly set up a notification.</p>&#13;
<h3 class="h3" id="sec4_7"><strong>4.7 Floating-Point Operations</strong></h3>&#13;
<p class="noindent">Although most modern CPUs support an FPU that does floating-point arithmetic in hardware, it’s worthwhile to develop a set of software floating-point arithmetic routines to get a solid feel for what’s involved. Generally, you’d use assembly language to write the math functions because speed is a primary design goal for a floating-point package. However, because here we’re writing a floating-point package simply to get a clearer picture of the process, we’ll opt for code that is easy to write, read, and understand.</p>&#13;
<p class="indent">As it turns out, floating-point addition and subtraction are easy to do in a high-level language like C/C++ or Pascal, so we’ll implement these functions in these languages. Floating-point multiplication and division are easier to do in assembly language than in a high-level language, so we’ll write those routines using High-Level Assembly (HLA).</p>&#13;
<h4 class="h4" id="sec4_7_1"><strong><em>4.7.1 Floating-Point Representation</em></strong></h4>&#13;
<p class="noindent">This section will use the IEEE 32-bit single-precision floating-point format (shown earlier in <a href="ch04.xhtml#ch04fig02">Figure 4-2</a>), which uses a one’s complement representation for signed values. This means that the sign bit (bit 31) contains a <span class="literal">1</span> if the number is negative and a <span class="literal">0</span> if the number is positive. The exponent is an 8-bit excess-127 exponent sitting in bits 23 through 30, and the mantissa is a 24-bit value with an implied HO bit of <span class="literal">1</span>. Because of the implied HO bit, this format does not support denormalized values.</p>&#13;
<h4 class="h4" id="sec4_7_2"><strong><em>4.7.2 Floating-Point Addition and Subtraction</em></strong></h4>&#13;
<p class="noindent">Addition and subtraction use essentially the same code. After all, computing <span class="literal">X</span> <span class="literal">-</span> <span class="literal">Y</span> is equivalent to computing <span class="literal">X + (-</span> <span class="literal">Y)</span>. If we can add a negative number to some other value, then we can also perform subtraction by first <span epub:type="pagebreak" id="page_76"/>negating some number and then adding it to another value. And because the IEEE floating-point format uses the one’s complement representation, negating a value is trivial—we just invert the sign bit.</p>&#13;
<p class="indent">Because we’re using the standard IEEE 32-bit single-precision floating-point format, we could theoretically get away with using the C/C++ <span class="literal">float</span> data type (assuming the underlying C/C++ compiler also uses this format, as most do on modern machines). However, you’ll soon see that when doing floating-point calculations in software, we need to manipulate various fields within the floating-point format as bit strings and integer values. Therefore, it’s more convenient to use a 32-bit <span class="literal">unsigned</span> integer type to hold the bit representation for our floating-point values. To avoid confusing our real values with actual integer values in a program, we’ll define the following <span class="literal">real</span> data type, which assumes that <span class="literal">unsigned</span> <span class="literal">long</span>s are 32-bit values in your implementation of C/C++ (this section assumes the <span class="literal">uint32_t</span> type achieves that, which is something like <span class="literal">typedef unsigned long uint32_t</span>), and declare all our real variables using this type:</p>&#13;
<p class="programs">typedef uint32_t  real;</p>&#13;
<p class="indent">One advantage of using the same floating-point format that C/C++ uses for float values is that we can assign floating-point literal constants to our <span class="literal">real</span> variables, and we can perform other floating-point operations such as input and output using existing library routines. However, one potential problem is that C/C++ will attempt to automatically convert between integer and floating-point formats if we use a <span class="literal">real</span> variable in a floating-point expression (remember, as far as C/C++ is concerned, <span class="literal">real</span> is just an <span class="literal">unsigned long</span> integer value). This means that we need to tell the compiler to treat the bit patterns found in our <span class="literal">real</span> variables as though they were <span class="literal">float</span> objects.</p>&#13;
<p class="indent">A simple type coercion like <span class="literal">(float)</span> <span class="EmpItalic">realVariable</span> won’t work. The C/C++ compiler will emit code to convert the integer it believes <span class="EmpItalic">realVariable</span> contains into the equivalent floating-point value. However, we want the C/C++ compiler to treat the bit pattern it finds in <span class="EmpItalic">realVariable</span> as a <span class="literal">float</span> without doing any conversion. The following C/C++ macro is a sneaky way to do this:</p>&#13;
<p class="programs">#define asreal(x) (*((float *) &amp;x))</p>&#13;
<p class="indent">This macro requires a single parameter that must be a <span class="literal">real</span> variable. The result is a variable that the compiler believes is a <span class="literal">float</span> variable.</p>&#13;
<p class="indent">Now that we have our <span class="literal">float</span> variable, we’ll develop two C/C++ functions to compute floating-point addition and subtraction: <span class="literal">fpadd()</span> and <span class="literal">fpsub()</span>. These two functions each take three parameters: the left and right operands of the operator and a pointer to a destination where these functions will store their result. The prototypes for these functions are the following:</p>&#13;
<p class="programs">void fpadd( real left, real right, real *dest );<br/>&#13;
void fpsub( real left, real right, real *dest );</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_77"/>The <span class="literal">fpsub()</span> function negates the right operand and calls the <span class="literal">fpadd()</span> function. Here’s the code for the <span class="literal">fpsub()</span> function:</p>&#13;
<p class="programs">void fpsub( real left, real right, real *dest )<br/>&#13;
{<br/>&#13;
    right = right ^ 0x80000000;   // Invert the sign bit of the right operand.<br/>&#13;
    fpadd( left, right, dest );   // Let fpadd do the real work.<br/>&#13;
}</p>&#13;
<p class="indent">The <span class="literal">fpadd()</span> function is where all the real work is done. To make <span class="literal">fpadd()</span> a little easier to understand and maintain, we’ll decompose it into several different functions that help with various tasks. In an actual software floating-point library routine, you wouldn’t do this decomposition, because the extra subroutine calls would be a little slower; however, we’re developing <span class="literal">fpadd()</span> for educational purposes, and besides, if you need high-performance floating-point addition, you’ll probably use a hardware FPU rather than a software implementation.</p>&#13;
<p class="indent">The IEEE floating-point formats are good examples of packed data types. As you’ve seen in previous chapters, packed data types are great for reducing storage requirements for a data type, but not so much when you need to use the packed fields in actual calculations. Therefore, one of the first things our floating-point functions will do is unpack the sign, exponent, and mantissa fields from the floating-point representation.</p>&#13;
<p class="indent">The first unpacking function, <span class="literal">extractSign()</span>, extracts the sign bit (bit 31) from our packed floating-point representation and returns the value <span class="literal">0</span> (for positive numbers) or <span class="literal">1</span> (for negative numbers).</p>&#13;
<p class="programs">inline int extractSign( real from )<br/>&#13;
{<br/>&#13;
    return( from &gt;&gt; 31);<br/>&#13;
}</p>&#13;
<p class="indent">This code could have also extracted the sign bit using this (possibly more efficient) expression:</p>&#13;
<p class="programs">(from &amp; 0x80000000) != 0</p>&#13;
<p class="indent">However, shifting bit 31 down to bit 0 is, arguably, easier to understand.</p>&#13;
<p class="indent">The next utility function, <span class="literal">extractExponent()</span>, unpacks the exponent from bits 23 through 30 in the packed real format. It does this by shifting the real value to the right by 23 bits, masking out the sign bit, and converting the excess-127 exponent to a two’s complement format (by subtracting 127).</p>&#13;
<p class="programs">inline int extractExponent( real from )<br/>&#13;
{<br/>&#13;
    return ((from &gt;&gt; 23) &amp; 0xff) - 127;<br/>&#13;
}</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_78"/>Next is the <span class="literal">extractMantissa()</span> function, which extracts the mantissa from the real value. To extract the mantissa, we must mask out the exponent and sign bits and then insert the implied HO bit of <span class="literal">1</span>. The only catch is that we must return <span class="literal">0</span> if the entire value is <span class="literal">0</span>.</p>&#13;
<p class="programs">inline int extractMantissa( real from )<br/>&#13;
{<br/>&#13;
    if( (from &amp; 0x7fffffff) == 0 ) return 0;<br/>&#13;
    return ((from &amp; 0x7FFFFF) | 0x800000 );<br/>&#13;
}</p>&#13;
<p class="indent">As you learned earlier, whenever adding or subtracting two values using scientific notation (which the IEEE floating-point format uses), you must first adjust the two values so that they have the same exponent. For example, to add the two decimal (base-10) numbers <span class="literal">1.2345e3</span> and <span class="literal">8.7654e1</span>, we must first adjust one or the other so that their exponents are the same. We can reduce the exponent of the first number by shifting the decimal point to the right. For example, the following values are all equivalent to <span class="literal">1.2345e3</span>:</p>&#13;
<p class="programs">12.345e2 123.45e1 1234.5 12345e-1</p>&#13;
<p class="indent">Likewise, we can increase the value of an exponent by shifting the decimal point to the left. The following values are all equal to <span class="literal">8.7654e1</span>:</p>&#13;
<p class="programs">0.87654e2 0.087654e3 0.0087654e4</p>&#13;
<p class="indent">For floating-point addition and subtraction involving binary numbers, we can make the binary exponents the same by shifting the mantissa one position to the left and decrementing the exponent, or by shifting the mantissa one position to the right and incrementing the exponent.</p>&#13;
<p class="indent">Shifting the mantissa bits to the right means that we reduce the precision of our number (because the bits wind up going off the LO end of the mantissa). To maintain as much accuracy as possible in our calculations, we shouldn’t truncate the bits we shift out of the mantissa, but rather round the result to the nearest value we can represent with the remaining mantissa bits. These are the IEEE rules for rounding, in order:</p>&#13;
<ol>&#13;
<li class="noindent">Truncate the result if the last bit shifted out was a <span class="literal">0</span>.</li>&#13;
<li class="noindent">Increment the mantissa by 1 if the last bit shifted out was a <span class="literal">1</span> and there was at least one bit set to <span class="literal">1</span> in all the other bits that were shifted out.<sup><a href="footnotes.xhtml#fn4_6a" id="fn4_6">6</a></sup></li>&#13;
<li class="noindent">If the last bit we shifted out was a <span class="literal">1</span>, and all the other bits were <span class="literal">0</span>s, then round the resulting mantissa up by 1 if the mantissa’s LO bit contains a <span class="literal">1</span>.</li></ol>&#13;
<p class="indent">Shifting the mantissa and rounding it is a relatively complex operation, and it will occur a couple of times in the floating-point addition code. <span epub:type="pagebreak" id="page_79"/>Therefore, it’s another candidate for a utility function. Here’s the C/C++ code that implements this function, <span class="literal">shiftAndRound()</span>:</p>&#13;
<p class="programs">void shiftAndRound( uint32_t *valToShift, int bitsToShift )<br/>&#13;
{<br/>&#13;
    // Masks is used to mask out bits to check for a "sticky" bit.<br/>&#13;
    static unsigned masks[24] =<br/>&#13;
    {<br/>&#13;
        0, 1, 3, 7, 0xf, 0x1f, 0x3f, 0x7f, <br/>&#13;
        0xff, 0x1ff, 0x3ff, 0x7ff, 0xfff, 0x1fff, 0x3fff, 0x7fff,<br/>&#13;
        0xffff, 0x1ffff, 0x3ffff, 0x7ffff, 0xfffff, 0x1fffff, 0x3fffff,<br/>&#13;
        0x7fffff<br/>&#13;
    };<br/>&#13;
<br/>&#13;
    // HOmasks: Masks out the HO bit of the value masked by the masks entry.<br/>&#13;
    static unsigned HOmasks[24] =<br/>&#13;
    {<br/>&#13;
        0, <br/>&#13;
        1, 2, 4, 0x8, 0x10, 0x20, 0x40, 0x80, <br/>&#13;
        0x100, 0x200, 0x400, 0x800, 0x1000, 0x2000, 0x4000, 0x8000, <br/>&#13;
        0x10000, 0x20000, 0x40000, 0x80000, 0x100000, 0x200000, 0x400000<br/>&#13;
    };<br/>&#13;
<br/>&#13;
    // shiftedOut: Holds the value that will be shifted out of a mantissa<br/>&#13;
    // during the denormalization operation (used to round a denormalized<br/>&#13;
    // value).<br/>&#13;
    int shiftedOut;<br/>&#13;
<br/>&#13;
    assert( bitsToShift &lt;= 23 );<br/>&#13;
<br/>&#13;
    // Okay, first grab the bits we're going to shift out (so we can determine<br/>&#13;
    // how to round this value after the shift).<br/>&#13;
    shiftedOut = *valToShift &amp; masks[ bitsToShift ];<br/>&#13;
<br/>&#13;
    // Shift the value to the right the specified number of bits.<br/>&#13;
    // Note: bit 31 is always 0, so it doesn't matter if the C<br/>&#13;
    // compiler does a logical shift right or an arithmetic shift right.<br/>&#13;
    *valToShift = *valToShift &gt;&gt; bitsToShift;<br/>&#13;
<br/>&#13;
    // If necessary, round the value:<br/>&#13;
<br/>&#13;
    if(  shiftedOut &gt; HOmasks[ bitsToShift ] )<br/>&#13;
    {<br/>&#13;
        // If the bits we shifted out are greater than 1/2 the LO bit, then<br/>&#13;
        // round the value up by 1.<br/>&#13;
<br/>&#13;
        *valToShift = *valToShift + 1;<br/>&#13;
    }<br/>&#13;
    else if( shiftedOut == HOmasks[ bitsToShift ] )<br/>&#13;
    {<br/>&#13;
        // If the bits we shifted out are exactly 1/2 of the LO bit's value,<br/>&#13;
        // then round the value to the nearest number whose LO bit is 0.<br/>&#13;
<br/>&#13;
        *valToShift = *valToShift + (*valToShift &amp; 1);<br/>&#13;
<span epub:type="pagebreak" id="page_80"/>    }<br/>&#13;
    // else<br/>&#13;
    // We round the value down to the previous value. The current<br/>&#13;
    // value is already truncated (rounded down), so we don't have to do<br/>&#13;
    // anything.<br/>&#13;
}</p>&#13;
<p class="indent">The “trick” in this code is that it uses a couple of lookup tables, <span class="literal">masks</span> and <span class="literal">HOmasks</span>, to extract those bits that the mantissa will use from the shift right operation. The <span class="literal">masks</span> table entries contain <span class="literal">1</span> bits (set bits) in the positions that will be lost during the shift. The <span class="literal">HOmasks</span> table entries contain a single set bit in the position specified by the index into the table; that is, the entry at index 0 contains a <span class="literal">1</span> in bit position 0, the entry at index 1 contains a <span class="literal">1</span> in bit position 1, and so on. This code selects an entry from each of these tables based on the number of mantissa bits it needs to shift to the right.</p>&#13;
<p class="indent">If the original mantissa value, logically ANDed with the appropriate entry in <span class="literal">masks</span>, is greater than the corresponding entry in <span class="literal">HOmasks</span>, then the <span class="literal">shiftAndRound()</span> function rounds the shifted mantissa to the next greater value. If the ANDed mantissa value is equal to the corresponding <span class="literal">HOmasks</span> element, this code rounds the shifted mantissa value according to its LO bit (note that the expression <span class="literal">(*valToShift &amp; 1)</span> produces <span class="literal">1</span> if the mantissa’s LO bit is <span class="literal">1</span>, and it produces <span class="literal">0</span> otherwise). Finally, if the ANDed mantissa value is less than the entry from the <span class="literal">HOmasks</span> table, then this code doesn’t have to do anything because the mantissa is already rounded down.</p>&#13;
<p class="indent">Once we’ve adjusted one of the values so that the exponents of both operands are the same, the next step in the addition algorithm is to compare the signs of the values. If the signs of the two operands are the same, we add their mantissas (using a standard integer add operation). If the signs differ, we have to subtract, rather than add, the mantissas. Because floating-point values use one’s complement representation, and standard integer arithmetic uses two’s complement, we cannot simply subtract the negative value from the positive value. Instead, we have to subtract the smaller value from the larger value and determine the sign of the result based on the signs and magnitudes of the original operands. <a href="ch04.xhtml#ch04tab03">Table 4-3</a> describes how to accomplish this.</p>&#13;
<p class="tabcap"><a id="ch04tab03"/><strong>Table 4-3:</strong> Dealing with Operands That Have Different Signs</p>&#13;
<table class="topbot-d">&#13;
<colgroup>&#13;
<col style="width:20%"/>&#13;
<col style="width:20%"/>&#13;
<col style="width:20%"/>&#13;
<col style="width:20%"/>&#13;
<col style="width:20%"/>&#13;
</colgroup>&#13;
<tbody>&#13;
<tr>&#13;
<td class="table-h" style="vertical-align: top;"><p class="tab_th"><strong>Left sign</strong></p></td>&#13;
<td class="table-h" style="vertical-align: top;"><p class="tab_th"><strong>Right sign</strong></p></td>&#13;
<td class="table-h" style="vertical-align: top;"><p class="tab_th"><strong>Left mantissa &gt; right mantissa?</strong></p></td>&#13;
<td class="table-h" style="vertical-align: top;"><p class="tab_th"><strong>Compute mantissa as</strong></p></td>&#13;
<td class="table-h" style="vertical-align: top;"><p class="tab_th"><strong>Result</strong></p>&#13;
<p class="tab_th"><strong>sign is</strong></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba">–</p></td>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba">+</p></td>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba">Yes</p></td>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba"><span class="EmpItalic">LeftMantissa</span> <span class="literal">-</span> <span class="EmpItalic">RightMantissa</span></p></td>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba">–</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">+</p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">–</p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">Yes</p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba"><span class="EmpItalic">LeftMantissa</span> <span class="literal">-</span> <span class="EmpItalic">RightMantissa</span></p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">+</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba">–</p></td>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba">+</p></td>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba">No</p></td>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba"><span class="EmpItalic">RightMantissa</span> <span class="literal">-</span> <span class="EmpItalic">LeftMantissa</span></p></td>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba">+</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">+</p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">–</p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">No</p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba"><span class="EmpItalic">RightMantissa</span> <span class="literal">-</span> <span class="EmpItalic">LeftMantissa</span></p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">–</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">Whenever you’re adding or subtracting two 24-bit numbers, it’s possible to produce a result that requires 25 bits (in fact, this is common when you’re dealing with normalized values). Immediately after an addition or <span epub:type="pagebreak" id="page_81"/>subtraction, the floating-point code has to check the result to see if overflow has occurred. If so, it needs to shift the mantissa right by 1 bit, round the result, and then increment the exponent. After completing this step, all that remains is to pack the resulting sign, exponent, and mantissa fields into the 32-bit IEEE floating-point format. The following <span class="literal">packFP()</span> function is responsible for packing the <span class="literal">sign</span>, <span class="literal">exponent</span>, and <span class="literal">mantissa</span> fields into the 32-bit floating-point format:</p>&#13;
<p class="programs">inline real packFP( int sign, int exponent, int mantissa )<br/>&#13;
{<br/>&#13;
   return <br/>&#13;
        (real)<br/>&#13;
        ( <br/>&#13;
                (sign &lt;&lt; 31) <br/>&#13;
            |   ((exponent + 127) &lt;&lt; 23)  <br/>&#13;
            |   (mantissa &amp; 0x7fffff)<br/>&#13;
        );<br/>&#13;
}</p>&#13;
<p class="indent">Note that this function works for normalized values, denormalized values, and zero, but does not work for NaNs and infinities.</p>&#13;
<p class="indent">With the utility routines out of the way, take a look at the <span class="literal">fpadd()</span> function, which adds two floating-point values, producing a 32-bit real result:</p>&#13;
<p class="programs">void fpadd( real left, real right, real *dest )<br/>&#13;
{   <br/>&#13;
    // The following variables hold the fields associated with the <br/>&#13;
    // left operand:<br/>&#13;
    int             Lexponent;<br/>&#13;
    uint32_t        Lmantissa;<br/>&#13;
    int             Lsign;<br/>&#13;
<br/>&#13;
    // The following variables hold the fields associated with the <br/>&#13;
    // right operand:<br/>&#13;
    int             Rexponent;<br/>&#13;
    uint32_t        Rmantissa;<br/>&#13;
    int             Rsign;<br/>&#13;
<br/>&#13;
    // The following variables hold the separate fields of the result:<br/>&#13;
    int             Dexponent;<br/>&#13;
    uint32_t        Dmantissa;<br/>&#13;
    int             Dsign;<br/>&#13;
<br/>&#13;
    // Extract the fields so that they're easy to work with:<br/>&#13;
    Lexponent = extractExponent( left );<br/>&#13;
    Lmantissa = extractMantissa( left );<br/>&#13;
    Lsign     = extractSign( left );<br/>&#13;
<br/>&#13;
    Rexponent = extractExponent( right );<br/>&#13;
    Rmantissa = extractMantissa( right );<br/>&#13;
    Rsign     = extractSign( right );<br/>&#13;
<br/>&#13;
<br/>&#13;
<span epub:type="pagebreak" id="page_82"/>    // Code to handle special operands (infinity and NaNs):<br/>&#13;
<br/>&#13;
    if( Lexponent == 127 )<br/>&#13;
    {<br/>&#13;
        if( Lmantissa == 0 )<br/>&#13;
        {<br/>&#13;
            // If the left operand is infinity, then the result<br/>&#13;
            // depends upon the value of the right operand.<br/>&#13;
            <br/>&#13;
            if( Rexponent == 127 )<br/>&#13;
            {<br/>&#13;
                // If the exponent is all 1 bits (127 after unbiasing)<br/>&#13;
                // then the mantissa determines if we have an infinity value<br/>&#13;
                // (zero mantissa), a QNaN (mantissa = 0x800000), or a SNaN<br/>&#13;
                // (nonzero mantissa not equal to 0x800000). <br/>&#13;
                <br/>&#13;
                if( Rmantissa == 0 )  // Do we have infinity?<br/>&#13;
                {<br/>&#13;
                    // infinity + infinity = infinity<br/>&#13;
                    // -infinity - infinity = -infinity <br/>&#13;
                    // -infinity + infinity = NaN<br/>&#13;
                    // infinity - infinity = NaN<br/>&#13;
                    <br/>&#13;
                    if( Lsign == Rsign )<br/>&#13;
                    {<br/>&#13;
                        *dest = right;<br/>&#13;
                    }<br/>&#13;
                    else<br/>&#13;
                    {<br/>&#13;
                        *dest = 0x7fC00000;  // +QNaN<br/>&#13;
                    }<br/>&#13;
                }<br/>&#13;
                else  // Rmantissa is nonzero, so it's a NaN<br/>&#13;
                {<br/>&#13;
                    *dest = right;  // Right is a NaN, propagate it.<br/>&#13;
                }<br/>&#13;
            }<br/>&#13;
            <br/>&#13;
        }<br/>&#13;
        else // Lmantissa is nonzero, Lexponent is all 1s.<br/>&#13;
        {<br/>&#13;
            // If the left operand is some NaN, then the result will<br/>&#13;
            // also be the same NaN.<br/>&#13;
            <br/>&#13;
            *dest = left;<br/>&#13;
        }<br/>&#13;
<br/>&#13;
        // We've already calculated the result, so just return.<br/>&#13;
        return;<br/>&#13;
<br/>&#13;
    }<br/>&#13;
    else if( Rexponent == 127 )<br/>&#13;
    {<br/>&#13;
        // Two case: right is either a NaN (in which case we need to<br/>&#13;
        // propagate the NaN regardless of left's value) or it is<br/>&#13;
<span epub:type="pagebreak" id="page_83"/>        // +/– infinity. Because left is a "normal" number, we'll also<br/>&#13;
        // wind up propagating the infinity because any normal number<br/>&#13;
        // plus infinity is infinity.<br/>&#13;
<br/>&#13;
        *dest = right;  // Right is a NaN, so propagate it.<br/>&#13;
        return;<br/>&#13;
    }<br/>&#13;
<br/>&#13;
    // Okay, we've got two actual floating-point values. Let's add them <br/>&#13;
    // together. First, we have to "denormalize" one of the operands if<br/>&#13;
    // their exponents aren't the same (when adding or subtracting values,<br/>&#13;
    // the exponents must be the same).<br/>&#13;
    //<br/>&#13;
    // Algorithm: choose the value with the smaller exponent. Shift its <br/>&#13;
    // mantissa to the right the number of bits specified by the difference <br/>&#13;
    // between the two exponents.<br/>&#13;
<br/>&#13;
    Dexponent = Rexponent;<br/>&#13;
    if( Rexponent &gt; Lexponent )<br/>&#13;
    {<br/>&#13;
        shiftAndRound( &amp;Lmantissa, (Rexponent - Lexponent));<br/>&#13;
    }<br/>&#13;
    else if( Rexponent &lt; Lexponent )<br/>&#13;
    {<br/>&#13;
        shiftAndRound( &amp;Rmantissa, (Lexponent - Rexponent));<br/>&#13;
        Dexponent = Lexponent;<br/>&#13;
    }<br/>&#13;
<br/>&#13;
    // Okay, add the mantissas. There is one catch: if the signs are opposite<br/>&#13;
    // then we've actually got to subtract one value from the other (because<br/>&#13;
    // the FP format is one's complement, we'll subtract the larger mantissa<br/>&#13;
    // from the smaller and set the destination sign according to a<br/>&#13;
    // combination of the original sign values and the largest mantissa).<br/><br/>&#13;
    if( Rsign ^ Lsign )<br/>&#13;
    {<br/>&#13;
        // Signs are different, so we must subtract one value from the other.<br/>&#13;
<br/>&#13;
        if( Lmantissa &gt; Rmantissa )<br/>&#13;
        {<br/>&#13;
            // The left value is greater, so the result inherits the<br/>&#13;
            // sign of the left operand.<br/>&#13;
<br/>&#13;
            Dmantissa = Lmantissa - Rmantissa;<br/>&#13;
            Dsign = Lsign;<br/>&#13;
        }<br/>&#13;
        else<br/>&#13;
        {<br/>&#13;
            // The right value is greater, so the result inherits the<br/>&#13;
            // sign of the right operand.<br/>&#13;
<br/>&#13;
            Dmantissa = Rmantissa - Lmantissa;<br/>&#13;
            Dsign = Rsign;<br/>&#13;
        }<br/>&#13;
<br/>&#13;
<span epub:type="pagebreak" id="page_84"/>    }<br/>&#13;
    else<br/>&#13;
    {<br/>&#13;
        // Signs are the same, so add the values:<br/>&#13;
<br/>&#13;
        Dsign = Lsign;<br/>&#13;
        Dmantissa = Lmantissa + Rmantissa;<br/>&#13;
    }<br/>&#13;
<br/>&#13;
    // Normalize the result here.<br/>&#13;
    //<br/>&#13;
    // Note that during addition/subtraction, overflow of 1 bit is possible.<br/>&#13;
    // Deal with that possibility here (if overflow occurred, shift the <br/>&#13;
    // mantissa to the right one position and adjust for this by incrementing <br/>&#13;
    // the exponent). Note that this code returns infinity if overflow occurs <br/>&#13;
    // when incrementing the exponent (infinity is a value with an exponent <br/>&#13;
    // of $FF);<br/><br/>&#13;
   if( Dmantissa &gt;= 0x1000000 )<br/>&#13;
    {<br/>&#13;
        // Never more than 1 extra bit when doing addition/subtraction.<br/>&#13;
        // Note that by virtue of the floating-point format we're using,<br/>&#13;
        // the maximum value we can produce via addition or subtraction is<br/>&#13;
        // a mantissa value of 0x1fffffe. Therefore, when we round this<br/>&#13;
        // value it will not produce an overflow into the 25th bit.<br/>&#13;
<br/>&#13;
        shiftAndRound( &amp;Dmantissa, 1 ); // Move result into 24 bits.<br/>&#13;
        ++Dexponent;                    // Shift operation did a div by 2,<br/>&#13;
                                        // this counteracts the effect of<br/>&#13;
                                        // the shift (incrementing exponent<br/>&#13;
                                        // multiplies the value by 2).<br/>&#13;
    }<br/>&#13;
    else<br/>&#13;
    {<br/>&#13;
        // If the HO bit is clear, normalize the result<br/>&#13;
        // by shifting bits up and simultaneously decrementing<br/>&#13;
        // the exponent. We will treat 0 as a special case<br/>&#13;
        // because it's a common enough result.<br/>&#13;
<br/>&#13;
        if( Dmantissa != 0 )<br/>&#13;
        {<br/>&#13;
            <br/>&#13;
            // The while loop multiplies the mantissa by 2 (via a shift <br/>&#13;
            // left) and then divides the whole number by 2 (by <br/>&#13;
            // decrementing the exponent. This continues until the HO bit of <br/>&#13;
            // Dmantissa is set or the exponent becomes -127 (0 in the <br/>&#13;
            // biased-127 form). If Dexponent drops down to -128, then we've <br/>&#13;
            // got a denormalized number and we can stop.<br/>&#13;
                                                         <br/>&#13;
            while( (Dmantissa &lt; 0x800000) &amp;&amp; (Dexponent &gt; -127 ))<br/>&#13;
            {<br/>&#13;
                Dmantissa = Dmantissa &lt;&lt; 1;<br/>&#13;
                --Dexponent;<br/>&#13;
            }<br/>&#13;
            <br/>&#13;
<span epub:type="pagebreak" id="page_85"/>        }<br/>&#13;
        else<br/>&#13;
        {<br/>&#13;
            // If the mantissa went to 0, clear everything else, too.<br/>&#13;
            <br/>&#13;
            Dsign = 0;<br/>&#13;
            Dexponent = 0;<br/>&#13;
        }<br/>&#13;
    }<br/>&#13;
<br/>&#13;
    // Reconstruct the result and store it away:<br/><br/>&#13;
    *dest = packFP( Dsign, Dexponent, Dmantissa );<br/>&#13;
<br/>&#13;
}</p>&#13;
<p class="indent">To conclude this discussion of the software implementation of the <span class="literal">fpadd()</span> and <span class="literal">fsub()</span> functions, here’s a C <span class="literal">main()</span> function demonstrating their use:</p>&#13;
<p class="programs">// A simple main program that does some trivial tests on fpadd and fpsub.<br/><br/>&#13;
int main( int argc, char **argv )<br/>&#13;
{<br/>&#13;
    real l, r, d;<br/>&#13;
<br/>&#13;
    asreal(l) = 1.0;<br/><br/>&#13;
    asreal(r) = 2.0;<br/>&#13;
<br/>&#13;
    fpadd( l, r, &amp;d );<br/>&#13;
    printf( "dest = %x\n", d );<br/>&#13;
    printf( "dest = %12E\n", asreal( d ));<br/>&#13;
<br/>&#13;
    l = d;<br/>&#13;
    asreal(r) = 4.0;<br/>&#13;
    fpsub( l, r, &amp;d );<br/>&#13;
    printf( "dest2 = %x\n", d );<br/>&#13;
    printf( "dest2 = %12E\n", asreal( d ));<br/>&#13;
}</p>&#13;
<p class="indent">Here’s the output produced by compiling with Microsoft Visual C++ (and defining <span class="literal">uint32_t</span> as an <span class="literal">unsigned long</span>):</p>&#13;
<p class="programs">l = 3f800000<br/>&#13;
l = 1.000000E+00<br/>&#13;
r = 40000000<br/>&#13;
r = 2.000000E+00<br/>&#13;
dest = 40400000<br/>&#13;
dest = 3.000000E+00<br/>&#13;
dest2 = bf800000<br/>&#13;
dest2 = -1.000000E+00</p>&#13;
<h4 class="h4" id="sec4_7_3"><span epub:type="pagebreak" id="page_86"/><strong><em>4.7.3 Floating-Point Multiplication and Division</em></strong></h4>&#13;
<p class="noindent">Most software floating-point libraries are actually written in hand-optimized assembly language, not in a high-level language (HLL). As the previous section shows, it’s possible to write floating-point routines in an HLL and, particularly in the case of single-precision floating-point addition and subtraction, you could write the code efficiently. Given the right library routines, you could also write the floating-point multiplication and division routines in an HLL. However, because their implementation is actually easier in assembly language, this section presents an HLA implementation of the single-precision floating-point multiplication and division algorithms.</p>&#13;
<p class="indent">The HLA code in this section implements two functions, <span class="literal">fpmul()</span> and <span class="literal">fpdiv()</span>, that have the following prototypes:</p>&#13;
<p class="programs">procedure fpmul( left:real32; right:real32 );  @returns( "eax" );<br/>&#13;
procedure fpdiv( left:real32; right:real32 );  @returns( "eax" );</p>&#13;
<p class="indent">Beyond the fact that this code is written in assembly language rather than C, it differs in two main ways from the code in the previous section. First, it uses the built-in <span class="literal">real32</span> data type rather than creating a new data type for the real values, because we can easily coerce any 32-bit memory object to <span class="literal">real32</span> or <span class="literal">dword</span> in assembly language. Second, these prototypes support only two parameters; there is no destination parameter. These functions simply return the <span class="literal">real32</span> result in the EAX register.<sup><a href="footnotes.xhtml#fn4_7a" id="fn4_7">7</a></sup></p>&#13;
<h5 class="h5" id="sec4_7_3_1"><strong>4.7.3.1 Floating-Point Multiplication</strong></h5>&#13;
<p class="noindent">Whenever you multiply two values in scientific notation, you compute the result sign, exponent, and mantissa as follows:</p>&#13;
<ul>&#13;
<li class="noindent">The result sign is the exclusive-OR of the operand signs. That is, the result is positive if both operand signs were the same, and the result sign is negative if the operand signs were different.</li>&#13;
<li class="noindent">The result exponent is the sum of the operands’ exponents.</li>&#13;
<li class="noindent">The result mantissa is the integer (fixed-point) product of the two operand mantissas.</li>&#13;
</ul>&#13;
<p class="indent">There are a few additional rules that affect the floating-point multiplication algorithm that are a direct result of the IEEE floating-point format:</p>&#13;
<ul>&#13;
<li class="noindent">If either, or both, of the operands are <span class="literal">0</span>, the result is <span class="literal">0</span> (this is a special case because the representation for <span class="literal">0</span> is special).</li>&#13;
<li class="noindent"><span epub:type="pagebreak" id="page_87"/>If either operand is infinity, the result is infinity.</li>&#13;
<li class="noindent">If either operand is a NaN, the result is that same NaN.</li>&#13;
</ul>&#13;
<p class="indent">The <span class="literal">fpmul()</span> procedure begins by checking if either of the operands is <span class="literal">0</span>. If so, the function immediately returns <span class="literal">0.0</span> to the caller. Next, the <span class="literal">fpmul()</span> code checks for NaN or infinity values in the <span class="literal">left</span> and <span class="literal">right</span> operands. If it finds one of these values, it returns that same value to the caller.</p>&#13;
<p class="indent">If both of the <span class="literal">fpmul()</span> operands are reasonable floating-point values, then the <span class="literal">fpmul()</span> code extracts the sign, exponent, and mantissa fields of the packed floating-point value. Actually, <em>extract</em> isn’t the correct term here; <em>isolate</em> is a better description. Here’s the code that isolates the sign bits of the two operands and computes the result sign:</p>&#13;
<p class="programs">mov( (type dword left), ebx );  // Result sign is the XOR of the<br/>&#13;
xor( (type dword right), ebx ); // operand signs.<br/>&#13;
and( $8000_0000, ebx );         // Keep only the sign bit.</p>&#13;
<p class="indent">This code exclusive-ORs the two operands and then masks out bits 0 through 30, leaving only the result sign value in bit 31 of the EBX register. This procedure doesn’t bother moving the sign bit down to bit 0 (as you’d normally do when unpacking data), because it would just have to move this bit back to bit 31 when it repacks the floating-point value later.</p>&#13;
<p class="indent">To process the exponent, <span class="literal">fpmul()</span> isolates bits 23 through 30 and operates on the exponent in place. When multiplying two values using scientific notation, you must add the values of the exponents together. However, you must subtract 127 from the exponent’s sum, since adding excess-127 exponents ends up adding the bias twice. The following code isolates the exponent bits, adjusts for the extra bias, and adds the exponents together:</p>&#13;
<p class="programs">mov( (type dword left), ecx );  // Exponent goes into bits 23..30<br/>&#13;
and( $7f80_0000, ecx );         // of ECX; mask these bits.<br/>&#13;
sub( 126 &lt;&lt; 23, ecx );          // Eliminate the bias of 127 and multiply by 2<br/><br/>&#13;
mov( (type dword right), eax );<br/>&#13;
and( $7f80_0000, eax );<br/><br/>&#13;
// For multiplication, we need to add the exponents:<br/>&#13;
<br/>&#13;
add( eax, ecx );                // Exponent value is now in bits<br/>&#13;
                                // 23..30 of ECX.</p>&#13;
<p class="indent">First, notice that this code subtracts 126 rather than 127. The reason is that later we’ll need to double the result of the multiplication of the mantissas. Subtracting 126 rather than 127 does this multiplication by 2 implicitly (saving an instruction later on).</p>&#13;
<p class="indent">If the sum of the exponents with <span class="literal">add(eax, ecx)</span> in the preceding code is too large to fit into 8 bits, there will be a carry out of bit 30 into bit 31 of ECX, which will set the 80x86 overflow flag. If overflow occurs on a multiplication, our code will return <span class="literal">infinity</span> as the result.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_88"/>If overflow does not occur, then the <span class="literal">fpmul()</span> procedure needs to set the implied HO bit of the two mantissa values. The following code handles this chore, strips out all the exponent and sign bits from the mantissas, and left-justifies the mantissa bits up against bit position 31 in EAX and EDX.</p>&#13;
<p class="programs">mov( (type dword left), eax );<br/>&#13;
mov( (type dword right), edx );<br/><br/>&#13;
// If we don't have a 0 value, then set the implied HO bit of the mantissa:<br/><br/>&#13;
if( eax &lt;&gt; 0 ) then<br/><br/>&#13;
    or( $80_0000, eax );  // Set the implied bit to 1.<br/><br/>&#13;
endif;<br/>&#13;
shl( 8, eax );  // Moves mantissa to bits 8..31 and removes sign/exp.<br/><br/>&#13;
// Repeat this for the right operand.<br/><br/>&#13;
if( edx &lt;&gt; 0 ) then<br/><br/>&#13;
    or( $80_0000, edx );<br/><br/>&#13;
endif;<br/>&#13;
shl( 8, edx );</p>&#13;
<p class="indent">Once the mantissas are shifted to bit 31 in EAX and EDX, we multiply using the 80x86 <span class="literal">mul()</span> instruction:</p>&#13;
<p class="programs">mul( edx );</p>&#13;
<p class="indent">This instruction computes the 64-bit product of EAX and EDX, leaving the result in EDX:EAX (the HO double word is in EDX, and the LO double word is in EAX). Because the product of any two <em>n</em>-bit integers could require as many as 2×<em>n</em> bits, the <span class="literal">mul()</span> instruction computes EDX:EAX = EAX×EDX. Left-justifying the mantissas in EAX and EDX before doing the multiplication ensures the mantissa of the product winds up in bits 7 through 30 of EDX. We actually need them in bit positions 8 through 31 of EDX—that’s why earlier this code subtracted only 126, rather than 127, when adjusting for the excess-127 value (this multiplies the result by 2, which is equivalent to shifting the bits left one position). As these numbers were normalized prior to the multiplication, bit 30 of EDX will contain a <span class="literal">1</span> after the multiplication unless the result is <span class="literal">0</span>. The 32-bit IEEE real format does not support denormalized values, so we don’t have to worry about this case when using 32-bit floating-point values.</p>&#13;
<p class="indent">Because the mantissas are 24 bits each, the product of the mantissas could have as many as 48 significant bits. Our result mantissa can hold only 24 bits, so we need to round the value to produce a 24-bit result (using the <span epub:type="pagebreak" id="page_89"/>IEEE rounding algorithm — see “Rounding” on <a href="#page_71">page 71</a>). Here’s the code that rounds the value in EDX to 24 significant bits (in positions 8..31):</p>&#13;
<p class="programs">test( $80, edx );  // Clears zero flag if bit 7 of EDX = 1.<br/>&#13;
if( @nz ) then<br/><br/>&#13;
    add( $FFFF_FFFF, eax );  // Sets carry if EAX &lt;&gt; 0.<br/>&#13;
    adc( $7f, dl );          // Sets carry if DL:EAX &gt; $80_0000_0000.<br/>&#13;
    if( @c ) then<br/><br/>&#13;
        // If DL:EAX &gt; $80_0000_0000 then round the mantissa<br/>&#13;
        // up by adding 1 to bit position 8:<br/><br/>&#13;
        add( 1 &lt;&lt; 8, edx );<br/><br/>&#13;
    else // DL:EAX = $80_0000_0000<br/><br/>&#13;
        // We need to round to the value that has a 0<br/>&#13;
        // in bit position 0 of the mantissa (bit #8 of EDX):<br/><br/>&#13;
        test( 8, edx );  // Clears zero flag if bit #8 contains a 1.<br/>&#13;
        if( @nz ) then<br/><br/>&#13;
            add( 1 &lt;&lt; 8, edx );  // Adds a 1 starting at bit position 8.<br/><br/>&#13;
            // If there was an overflow, renormalize:<br/><br/>&#13;
            if( @c ) then<br/><br/>&#13;
                rcr( 1, edx );  // Shift overflow (in carry) back into EDX.<br/>&#13;
                inc( ecx );     // Shift did a divide by 2. Fix that.<br/><br/>&#13;
        endif;<br/><br/>&#13;
        endif;<br/><br/>&#13;
    endif;<br/><br/>&#13;
endif;</p>&#13;
<p class="indent">The number may need to be renormalized after rounding. If the mantissa contains all <span class="literal">1</span> bits and needs to be rounded up, this will produce an overflow out of the HO bit of the mantissa. The <span class="literal">rcr()</span> and <span class="literal">inc()</span> instructions at the end of this code sequence put the overflow bit back into the mantissa if overflow occurs.</p>&#13;
<p class="indent">The only thing left to do after this is pack the destination sign, exponent, and mantissa into the 32-bit EAX register. The following code does this:</p>&#13;
<p class="programs">shr( 8, edx );          // Move mantissa into bits 0..23.<br/>&#13;
and( $7f_ffff, edx );   // Clear the implied bit.<br/>&#13;
lea( eax, [edx+ecx] );  // Merge mantissa and exponent into EAX.<br/>&#13;
or( ebx, eax );         // Merge in the sign.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_90"/>The only tricky thing in this code is the use of the <span class="literal">lea()</span> (load effective address) instruction to compute the sum of EDX (the mantissa) and ECX (the exponent) and move the result to EAX all with a single instruction.</p>&#13;
<h5 class="h5" id="sec4_7_3_2"><strong>4.7.3.2 Floating-Point Division</strong></h5>&#13;
<p class="noindent">Floating-point division is a little bit more involved than multiplication because the IEEE floating-point standard says many things about degenerate conditions that can occur during division. We’re not going to discuss all the code that handles those conditions here. Instead, see the discussion of the conditions for <span class="literal">fpmul()</span> earlier, and check out the complete code listing for <span class="literal">fdiv()</span> later in this section.</p>&#13;
<p class="indent">Assuming we have reasonable numbers to divide, the division algorithm first computes the result sign using the same algorithm (and code) as for multiplying. When dividing two values using scientific notation, we have to subtract their exponents. In contrast to the multiplication algorithm, here it’s more convenient to truly unpack the exponents for the two division operands and convert them from excess-127 to two’s complement form. Here’s the code that does this:</p>&#13;
<p class="programs">mov( (type dword left), ecx );  // Exponent comes from bits 23..30.<br/>&#13;
shr( 23, ecx );<br/>&#13;
and( $ff, ecx );                // Mask out the sign bit (in bit 8).<br/>&#13;
<br/>&#13;
mov( (type dword right), eax );<br/>&#13;
shr( 23, eax );<br/>&#13;
and( $ff, eax );<br/>&#13;
<br/>&#13;
// Eliminate the bias from the exponents:<br/>&#13;
<br/>&#13;
sub( 127, ecx );<br/>&#13;
sub( 127, eax );<br/>&#13;
<br/>&#13;
// For division, we need to subtract the exponents:<br/>&#13;
<br/>&#13;
sub( eax, ecx );                // Leaves result exponent in ECX.</p>&#13;
<p class="indent">The 80x86 <span class="literal">div()</span> instruction absolutely, positively requires the quotient to fit into 32 bits. If this condition is not true, the CPU may abort the operation with a divide exception. As long as the HO bit of the divisor contains a <span class="literal">1</span> and the HO 2 bits of the dividend contain <span class="literal">%01</span>, we won’t get a division error. Here’s the code that prepares the operands prior to the division operation:</p>&#13;
<p class="programs">mov (type dword left), edx );<br/>&#13;
if( edx &lt;&gt; 0 ) then<br/>&#13;
<br/>&#13;
    or( $80_0000, edx );   // Set the implied bit to 1 in the left operand.<br/>&#13;
    shl( 8, edx );<br/>&#13;
<br/>&#13;
endif;<br/>&#13;
mov( (type dword right), edi );<br/>&#13;
<span epub:type="pagebreak" id="page_91"/>if( edi &lt;&gt; 0 ) then<br/>&#13;
<br/>&#13;
    or( $80_0000, edi );        // Set the implied bit to 1 in the right operand.<br/>&#13;
    shl( 8, edi );<br/>&#13;
<br/>&#13;
else<br/>&#13;
<br/>&#13;
    // Division by zero error, here.<br/><br/>&#13;
endif;</p>&#13;
<p class="indent">The next step is to actually do the division. As noted earlier, in order to prevent a division error, we have to shift the dividend 1 bit to the right (to set the HO 2 bits to <span class="literal">%01</span>), as follows:</p>&#13;
<p class="programs">xor( eax, eax );    // EAX := 0;<br/>&#13;
shr( 1, edx );      // Shift EDX:EAX to the right 1 bit to<br/>&#13;
rcr( 1, eax );      // prevent a division error.<br/>&#13;
div( edi );         // Compute EAX = EDX:EAX / EDI.</p>&#13;
<p class="indent">Once the <span class="literal">div()</span> instruction executes, the quotient is sitting in the HO 24 bits of EAX, and the remainder is in AL:EDX. We now need to normalize and round the result. Rounding is a little easier because AL:EDX contains the remainder after the division; if we need to round down, it will contain a value less than <span class="literal">$80:0000_0000</span> (that is, the 80x86 AL register contains <span class="literal">$80</span> and EDX contains <span class="literal">0</span>); if we need to round up, it will contain a value greater than <span class="literal">$80:0000_</span>; and if we need to round to the nearest value, it will contain exactly <span class="literal">$80:0000_0000</span>.</p>&#13;
<p class="indent">Here’s the code that does this:</p>&#13;
<p class="programs">test( $80, al );    // See if the bit just below the LO bit of the<br/>&#13;
if( @nz ) then      // mantissa contains a 0 or 1.<br/>&#13;
<br/>&#13;
    // Okay, the bit just below the LO bit of our mantissa contains a 1.<br/>&#13;
    // If all other bits below the mantissa and this bit contain 0s,<br/>&#13;
    // we have to round to the nearest mantissa value whose LO bit is 0.<br/>&#13;
<br/>&#13;
    test( $7f, al );             // Clears zero flag if bits 0..6 &lt;&gt; 0.<br/>&#13;
    if( @nz || edx &lt;&gt; 0 ) then   // If bits 0..6 in AL are 0 and EDX<br/>&#13;
                                 // is 0.<br/>&#13;
<br/>&#13;
        // We need to round up:<br/>&#13;
<br/>&#13;
        add( $100, eax );  // Mantissa starts in bit #8 );<br/>&#13;
        if( @c ) then      // Carry set if mantissa overflows.<br/>&#13;
<br/>&#13;
            // If there was an overflow, renormalize.<br/>&#13;
<br/>&#13;
            rcr( 1, eax );<br/>&#13;
            inc( ecx );<br/>&#13;
<br/>&#13;
        endif;<br/><br/>&#13;
<span epub:type="pagebreak" id="page_92"/>    else<br/><br/>&#13;
        // The bits below the mantissa are exactly 1/2 the value<br/>&#13;
        // of the LO mantissa bit. So we need to round to the value<br/>&#13;
        // that has a LO mantissa bit of 0:<br/>&#13;
<br/>&#13;
        test( $100, eax );<br/>&#13;
        if( @nz ) then<br/>&#13;
<br/>&#13;
            add( $100, eax );<br/>&#13;
            if( @c ) then<br/>&#13;
<br/>&#13;
                // If there was an overflow, renormalize.<br/>&#13;
<br/>&#13;
                rcr( 1, eax );  // Put overflow bit back into EAX.<br/>&#13;
                inc( ecx );     // Adjust exponent accordingly.<br/>&#13;
<br/>&#13;
            endif;<br/>&#13;
<br/>&#13;
        endif;<br/>&#13;
<br/>&#13;
    endif;<br/>&#13;
<br/>&#13;
endif;</p>&#13;
<p class="indent">The last step in <span class="literal">fpdiv</span> is to add the bias back into the exponent (and verify that overflow doesn’t occur) and then pack the quotient’s sign, exponent, and mantissa fields into the 32-bit floating-point format. Here’s the code that does this:</p>&#13;
<p class="programs">if( (type int32 ecx) &gt; 127 ) then<br/>&#13;
<br/>&#13;
    mov($ff-127, ecx );    // Set exponent value for infinity<br/>&#13;
    xor( eax, eax );       // because we just had overflow.<br/>&#13;
<br/>&#13;
elseif( (type int32 ecx) &lt; -128 ) then<br/><br/>&#13;
    mov( -127, ecx );      // Return 0 for underflow (note that<br/>&#13;
    xor( eax, eax );       // next we add 127 to ECX).<br/>&#13;
<br/>&#13;
endif;                                      <br/>&#13;
add( 127, ecx );           // Add the bias back in.<br/>&#13;
shl( 23, ecx );            // Move the exponent to bits 23..30.<br/>&#13;
<br/>&#13;
// Okay, assemble the final real32 value:<br/>&#13;
<br/>&#13;
shr( 8, eax );             // Move mantissa into bits 0..23.<br/>&#13;
and( $7f_ffff, eax );      // Clear the implied bit.<br/>&#13;
or( ecx, eax );            // Merge mantissa and exponent into EAX.<br/>&#13;
or( ebx, eax );            // Merge in the sign.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_93"/>Whew! This has been a lot of code. However, going through all of it just to see how floating-point operations work has hopefully given you an appreciation of exactly what an FPU does for you.</p>&#13;
<h3 class="h3" id="sec4_8"><strong>4.8 For More Information</strong></h3>&#13;
<p class="ref">Hyde, Randall. <em>The Art of Assembly Language</em>. 2nd ed. San Francisco: No Starch Press, 2010.</p>&#13;
<p class="ref">———. “Webster: The Place on the Internet to Learn Assembly.” <em><a href="http://plantation-productions.com/Webster/index.html">http://plantation-productions.com/Webster/index.html</a></em>.</p>&#13;
<p class="ref">Knuth, Donald E. <em>The Art of Computer Programming, Volume 2: Seminumerical Algorithms</em>. 3rd ed. Boston: Addison-Wesley, 1998.<span epub:type="pagebreak" id="page_94"/></p>&#13;
</body></html>