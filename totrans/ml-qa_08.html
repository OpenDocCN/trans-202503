<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><h2 class="h2" id="ch07"><span epub:type="pagebreak" id="page_37"/><strong><span class="big">7</span><br/>MULTI-GPU TRAINING PARADIGMS</strong></h2>&#13;
<div class="image1"><img src="../images/common.jpg" alt="Image" width="252" height="252"/></div>&#13;
<p class="noindent">What are the different multi-GPU training paradigms, and what are their respective advantages and disadvantages?</p>&#13;
<p class="indent">Multi-GPU training paradigms can be categorized into two groups: dividing data for parallel processing with multiple GPUs and dividing the model among multiple GPUs to handle memory constraints when the model size surpasses that of a single GPU. Data parallelism falls into the first category, while model parallelism and tensor parallelism fall into the second category. Techniques like pipeline parallelism borrow ideas from both categories. In addition, current software implementations such as DeepSpeed, Colossal AI, and others blend multiple approaches into a hybrid technique.</p>&#13;
<p class="indent">This chapter introduces several training paradigms and provides advice on which to use in practice.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>This chapter primarily uses the term</em> GPUs <em>to describe the hardware utilized for parallel processing. However, the same concepts and techniques discussed can be applied to other specialized hardware devices, such as tensor processing units (TPUs) or other accelerators, depending on the specific architecture and requirements of the system.</em></p>&#13;
</div>&#13;
<h3 class="h3" id="ch00lev30"><strong>The Training Paradigms</strong></h3>&#13;
<p class="noindent">The following sections discuss the model parallelism, data parallelism, tensor parallelism, and sequence parallelism multi-GPU training paradigms.<span epub:type="pagebreak" id="page_38"/></p>&#13;
<h4 class="h4" id="ch00levsec8"><em><strong>Model Parallelism</strong></em></h4>&#13;
<p class="noindent"><em>Model parallelism</em>, or <em>inter-op parallelism</em>, is a technique in which different sections of a large model are placed on different GPUs and are computed sequentially, with intermediate results passed between the devices. This allows for the training and execution of models that might not fit entirely on a single device, but it can require intricate coordination to manage the dependencies between different parts of the model.</p>&#13;
<p class="indent">Model parallelism is perhaps the most intuitive form of parallelization across devices. For example, for a simple neural network that consists of only two layers—a hidden layer and an output layer—we can keep one layer on one GPU and the other layer on another GPU. Of course, this can scale to an arbitrary number of layers and GPUs.</p>&#13;
<p class="indent">This is a good strategy for dealing with limited GPU memory where the complete network does not fit into one GPU. However, there are more efficient ways of using multiple GPUs, such as tensor parallelism, because the chain-like structure (layer 1 on GPU 1 <em>→</em> layer 2 on GPU 2 <em>→</em> . . .) in model parallelism introduces a bottleneck. In other words, a major disadvantage of model parallelism is that the GPUs have to wait for each other. They cannot efficiently work in parallel, as they depend on one other’s outputs.</p>&#13;
<h4 class="h4" id="ch00levsec9"><em><strong>Data Parallelism</strong></em></h4>&#13;
<p class="noindent"><em>Data parallelism</em> has been the default mode for multi-GPU training for several years. Here, we divide a minibatch into smaller microbatches. Each GPU then processes a microbatch separately to compute the loss and loss gradients for the model weights. After the individual devices process the microbatches, the gradients are combined to compute the weight update for the next round.</p>&#13;
<p class="indent">An advantage of data parallelism over model parallelism is that the GPUs can run in parallel. Each GPU processes a portion of the training minibatch, that is, a microbatch. However, a caveat is that each GPU requires a full copy of the model. This is obviously not feasible if we have large models that don’t fit into the GPU’s VRAM.</p>&#13;
<h4 class="h4" id="ch00levsec10"><em><strong>Tensor Parallelism</strong></em></h4>&#13;
<p class="noindent"><em>Tensor parallelism</em>, or <em>intra-op parallelism</em>, is a more efficient form of model parallelism. Here, the weight and activation matrices are spread across the devices instead of distributing whole layers across devices: the individual matrices are split, so we split an individual matrix multiplication across GPUs.</p>&#13;
<p class="indent">We can implement tensor parallelism using basic principles of linear algebra; we can split a matrix multiplication across two GPUs in a row- or column-wise fashion, as illustrated in <a href="ch07.xhtml#ch7fig1">Figure 7-1</a> for two GPUs. (This concept can be extended to an arbitrary number of GPUs.)<span epub:type="pagebreak" id="page_39"/></p>&#13;
<div class="image"><img id="ch7fig1" src="../images/07fig01.jpg" alt="Image" width="1071" height="574"/></div>&#13;
<p class="figcap"><em>Figure 7-1: Tensor parallelism for distributing matrix multiplication across different devices</em></p>&#13;
<p class="indent">Like model parallelism, tensor parallelism allows us to work around memory limitations. At the same time, it also lets us execute operations in parallel, similar to data parallelism.</p>&#13;
<p class="indent">A small weakness of tensor parallelism is that it can result in high communication overhead between the multiple GPUs across which the matrices are split or sharded. For instance, tensor parallelism requires frequent synchronization of the model parameters across devices, which can slow down the overall training process.</p>&#13;
<p class="indent"><a href="ch07.xhtml#ch7fig2">Figure 7-2</a> compares model, data, and tensor parallelism.</p>&#13;
<div class="image"><img id="ch7fig2" src="../images/07fig02.jpg" alt="Image" width="869" height="551"/></div>&#13;
<p class="figcap"><em>Figure 7-2: A comparison of model, data, and tensor parallelism</em><span epub:type="pagebreak" id="page_40"/></p>&#13;
<p class="indent">In model parallelism, we put different layers onto different GPUs to work around GPU memory limitations. In data parallelism, we split a batch across GPUs to train copies of the model in parallel, averaging gradients for the weight update afterward. In tensor parallelism, we split matrices (inputs and weights) across different GPUs for parallel processing when models are too large to fit into GPU memory.</p>&#13;
<h4 class="h4" id="ch00levsec11"><em><strong>Pipeline Parallelism</strong></em></h4>&#13;
<p class="noindent">In <em>pipeline parallelism</em>, activations are passed during the forward pass, as in model parallelism. The twist is that the gradients of the input tensor are passed backward to prevent the devices from being idle. In a sense, pipeline parallelism is a sophisticated hybrid version of data and model parallelism.</p>&#13;
<p class="indent">We can think of pipeline parallelism as a form of model parallelism that tries to minimize the sequential computation bottleneck, enhancing the parallelism between the individual layers sitting on different devices. However, pipeline parallelism also borrows ideas from data parallelism, such as splitting minibatches further into microbatches.</p>&#13;
<p class="indent">Pipeline parallelism is definitely an improvement over model parallelism, though it is not perfect and there will be idle bubbles. A further disadvantage of pipeline parallelism is that it may require significant effort to design and implement the pipeline stages and associated communication patterns. Additionally, the performance gains it generates may not be as substantial as those from other parallelization techniques, such as pure data parallelism, especially for small models or in cases where the communication overhead is high.</p>&#13;
<p class="indent">For modern architectures that are too large to fit into GPU memory, it is more common nowadays to use a blend of data parallelism and tensor parallelism techniques instead of pipeline parallelism.</p>&#13;
<h4 class="h4" id="ch00levsec12"><em><strong>Sequence Parallelism</strong></em></h4>&#13;
<p class="noindent"><em>Sequence parallelism</em> aims to address computational bottlenecks when working with long sequences using transformer-based LLMs. More specifically, one shortcoming of transformers is that the self-attention mechanism (the original scaled-dot product attention) scales quadratically with the input sequence length. There are, of course, more efficient alternatives to the original attention mechanism that scale linearly.</p>&#13;
<p class="indent">However, these efficient self-attention mechanisms are less popular, and most people still prefer the original scaled-dot product attention mechanism as of this writing. Sequence parallelism, illustrated in <a href="ch07.xhtml#ch7fig3">Figure 7-3</a>, splits the input sequence into smaller chunks to be distributed across GPUs, which aims to reduce computation memory constraints of self-attention mechanisms.<span epub:type="pagebreak" id="page_41"/></p>&#13;
<div class="image"><img id="ch7fig3" src="../images/07fig03.jpg" alt="Image" width="571" height="426"/></div>&#13;
<p class="figcap"><em>Figure 7-3: Sequence parallelism divides long inputs among GPUs.</em></p>&#13;
<p class="indent">How does sequence parallelism relate to the multi-GPU techniques discussed earlier? Sequence parallelism deals specifically with sequential data, tensor parallelism deals with the model’s internal structure, and data parallelism deals with how the training data is divided. Theoretically, since each of these parallelism strategies addresses a different aspect of the computational challenge, they can thus be combined in various ways to optimize the training or inference process. Sequence parallelism is not as well studied as other parallelization techniques, however.</p>&#13;
<p class="indent">While sequence parallelism appears useful in practice, it also introduces additional communication overheads similar to the aforementioned parallelism techniques. Like data parallelism, it requires us to duplicate the model and make sure it fits into the device memory. Another of its disadvantages (depending on the implementation) for multi-GPU training of transformers is that breaking up the input sequence into smaller subsequences can decrease the model’s accuracy (mainly when the model is applied to longer sequences).</p>&#13;
<h3 class="h3" id="ch00lev31"><strong>Recommendations</strong></h3>&#13;
<p class="noindent">Practical recommendations depend on the context. If we train small models that fit onto a single GPU, then data parallelism strategies may be the most efficient. Performance gains from pipeline parallelism may not be as significant as those from other parallelization techniques, such as data parallelism, especially for small models or in cases where the communication overhead is high.</p>&#13;
<p class="indent">If models are too large to fit into the memory of a single GPU, we need to explore model or tensor parallelism. Tensor parallelism is naturally more <span epub:type="pagebreak" id="page_42"/>efficient; the GPUs can work in parallel since there is no sequential dependency as in model parallelism.</p>&#13;
<p class="indent">Modern multi-GPU strategies also typically combine data parallelism and tensor parallelism.</p>&#13;
<h3 class="h3" id="ch00lev32"><strong>Exercises</strong></h3>&#13;
<p class="number"><strong>7-1.</strong> Suppose we are implementing our own version of tensor parallelism, which works great when we train our model with a standard stochastic gradient descent optimizer. However, when we try the Adam optimizer by Diederik P. Kingma and Jimmy Ba, we encounter an out-of-memory device. What problem might explain this issue?</p>&#13;
<p class="number"><strong>7-2.</strong> Suppose we don’t have access to a GPU and are considering using data parallelism on the CPU. Is this a good idea?</p>&#13;
<h3 class="h3" id="ch00lev33"><strong>References</strong></h3>&#13;
<ul>&#13;
<li class="noindent">The original paper on the Adam optimizer: Diederik P. Kingma and Jimmy Ba, “Adam: A Method for Stochastic Optimization” (2014), <em><a href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</a></em>.</li>&#13;
<li class="noindent">For more on DeepSpeed and Colossal-AI for multi-GPU training: <em><a href="https://github.com/microsoft/DeepSpeed">https://github.com/microsoft/DeepSpeed</a></em> and <em><a href="https://github.com/hpcaitech/ColossalAI">https://github.com/hpcaitech/ColossalAI</a></em>.</li>&#13;
<li class="noindent">Pipeline parallelism tutorials and research by the DeepSpeed team: <em><a href="https://www.deepspeed.ai/tutorials/pipeline">https://www.deepspeed.ai/tutorials/pipeline</a></em> and Yanping Huang et al., “GPipe: Efficient Training of Giant Neural Networks Using Pipeline Parallelism” (2018), <em><a href="https://arxiv.org/abs/1811.06965">https://arxiv.org/abs/1811.06965</a></em>.</li>&#13;
<li class="noindent">The paper proposing sequence parallelism for transformer-based language models: Shenggui Li et al., “Sequence Parallelism: Long Sequence Training from [a] System[s] Perspective” (2022), <em><a href="https://arxiv.org/abs/2105.13120">https://arxiv.org/abs/2105.13120</a></em>.</li>&#13;
<li class="noindent">The scaled-dot product attention mechanism was proposed with the original transformer architecture: Ashish Vaswani et al., “Attention Is All You Need” (2017), <em><a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a></em>.</li>&#13;
<li class="noindent">A survey covering alternatives to the original self-attention mechanism that scale linearly: Yi Tay et al., “Efficient Transformers: A Survey” (2020), <em><a href="https://arxiv.org/abs/2009.06732">https://arxiv.org/abs/2009.06732</a></em>.</li>&#13;
<li class="noindent">A survey covering additional techniques to improve the training efficiency of transformers: Bohan Zhuang et al., “A Survey on Efficient Training of Transformers” (2023), <em><a href="https://arxiv.org/abs/2302.01107">https://arxiv.org/abs/2302.01107</a></em>.</li>&#13;
<li class="noindent">Modern multi-GPU strategies typically combine data parallelism and tensor parallelism. Popular examples include DeepSpeed stages 2 and 3, described in this tutorial on the zero redundancy optimizer: <em><a href="https://www.deepspeed.ai/tutorials/zero/">https://www.deepspeed.ai/tutorials/zero/</a></em>.</li>&#13;
</ul>&#13;
</div>
</div>
</body></html>