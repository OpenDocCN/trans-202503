- en: '**7'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'LARGE LANGUAGE MODELS: TRUE AI AT LAST?**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Future historians might point to the fall 2022 release of OpenAI’s ChatGPT large
    language model as the dawn of true AI. Given what I’ve already seen as I write
    this in late March 2023, I would agree with such an assessment.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll first explore what existing large language models can
    do, then follow that up with a description of what they are and how they work.
    For all their impressive abilities, ultimately these models are neural networks
    built and trained like all the neural networks that came before. That fact alone
    means the connectionists were right from the beginning. Might Frank Rosenblatt
    be smiling in his grave?
  prefs: []
  type: TYPE_NORMAL
- en: I’ve already tipped my hand regarding my belief that ChatGPT and models like
    it represent something new that’s worthy of being called true AI. My hope is that,
    by the end of the chapter, you’ll agree.
  prefs: []
  type: TYPE_NORMAL
- en: '****'
  prefs: []
  type: TYPE_NORMAL
- en: 'The phrase [*artificial intelligence*](glossary.xhtml#glo5) is somewhat ambiguous
    and must be provided with a more nuanced definition before we proceed. Practitioners
    typically divide AI into two kinds: [*artificial narrow intelligence (ANI)*](glossary.xhtml#glo6)
    and [*artificial general intelligence (AGI)*](glossary.xhtml#glo4). The former
    encapsulates everything we’ve discussed so far. The latter refers to truly sentient
    and intelligent machines—the stuff of science fiction.'
  prefs: []
  type: TYPE_NORMAL
- en: The models existing as of the time of writing of this book are definitely not
    AGI. However, they are not merely ANI; they appear to be something entirely new,
    something in between. The title of a recent paper by Microsoft researchers Sébastien
    Bubeck et al., “Sparks of Artificial General Intelligence,” strikes me as appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: '[*Large language models (LLMs)*](glossary.xhtml#glo59) accept as input a text
    prompt supplied by a user. They then generate output text, word by word (really,
    token by token), using the prompt and all previously generated words as a guide.
    In effect, LLMs’ only design goal is to be very good at predicting the next word
    in a sequence of words initiated by the input prompt. That’s all they’re trained
    to do. However, that is not all that they *learn* to do. The reason why AI researchers
    are so excited by LLMs is that somewhere along the way, while learning to be expert
    text generators, LLMs also learn a host of emergent abilities, including question
    answering, mathematical reasoning, high-quality computer programming, and logical
    reasoning.'
  prefs: []
  type: TYPE_NORMAL
- en: The philosophical implications of these unexpected, emergent abilities are profound.
    The abilities of LLMs raise questions about the nature of thought, the meaning
    of consciousness, and the (assumed) uniqueness of the human mind. We’re not in
    a position to answer these questions in depth, but I will return to some of them
    in [Chapter 8](ch08.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s get into things by exploring the capabilities of LLMs. We can touch
    on only a few emergent abilities here, but the internet is already full of impressive,
    funny, and even frightening examples of LLMs in action. I recommend a review of
    such examples, though as always, care must be taken to judge the quality of the
    information source. See the resources at the end of this book for some of the
    online (and print) resources I suggest taking a look at.
  prefs: []
  type: TYPE_NORMAL
- en: The AI community is racing ahead as fast as possible in this area. By the time
    you read these words, LLMs will undoubtedly be even more impressive than they
    already are at the time I’m writing them.
  prefs: []
  type: TYPE_NORMAL
- en: '****'
  prefs: []
  type: TYPE_NORMAL
- en: 'What follows are examples of LLMs in action. For most situations, I’ll show
    the output of three models: Bard, GPT-3.5, and GPT-4\. All output represents model
    performance as of late March 2023\. In each case, I used the public interface
    via a web browser. Bard is a Google model. GPT-3.5 and GPT-4 are from OpenAI.'
  prefs: []
  type: TYPE_NORMAL
- en: 'LLMs are expert users of language. The models are used in dialog format, beginning
    with a prompt or question from the user. For example, I gave each of the three
    models this prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Write a one-paragraph bedtime story about a teddy bear, a magic carpet, and
    the moon that is suitable for children under the age of six.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is Bard’s reply:'
  prefs: []
  type: TYPE_NORMAL
- en: Once upon a time, there was a little teddy bear who lived in a toy box. He loved
    to play with his friends, but he always dreamed of flying to the moon. One day,
    he found a magic carpet in his toy box. The carpet could fly! The teddy bear was
    so excited, he jumped on the carpet and flew to the moon. He had a wonderful time
    exploring the moon, and he even met some friendly aliens. When it was time to
    go home, he said goodbye to his new friends and flew back to Earth on his magic
    carpet. He was so happy to be home, and he couldn’t wait to tell his friends all
    about his adventure.
  prefs: []
  type: TYPE_NORMAL
- en: The story is complete, in a single paragraph, and uses a simple sentence structure
    suitable for children.
  prefs: []
  type: TYPE_NORMAL
- en: 'GPT-3.5 replied like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Once upon a time, there was a teddy bear named Timmy who loved to go on adventures.
    One night, he hopped onto a magic carpet and flew up towards the moon. The moon
    was so big and bright, and Timmy couldn’t resist exploring it. He hopped and skipped
    around the craters and met some friendly aliens who showed him their moon garden.
    Timmy had a great time, but eventually, he knew it was time to go back home. He
    hopped back onto his magic carpet and flew back to his cozy bed, where he cuddled
    up with his favorite blanket and fell fast asleep, dreaming of his amazing moon
    adventure.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3.5’s story is also complete and includes friendly aliens as well. This
    time, the teddy bear gets a name, Timmy. The language is still appropriate for
    young children, but GPT-3.5 was more descriptive. Bard tells us that the teddy
    bear explores the moon and meets some friendly aliens, but GPT-3.5 tells us that
    Timmy hopped and skipped around the craters and that the friendly aliens showed
    him their moon garden.
  prefs: []
  type: TYPE_NORMAL
- en: 'GPT-4, OpenAI’s most advanced model, produced this:'
  prefs: []
  type: TYPE_NORMAL
- en: Once upon a time, under a twinkling night sky, a little teddy bear named Cuddles
    discovered a colorful magic carpet hidden in the corner of his cozy room. Curious
    and excited, Cuddles climbed onto the carpet, and together, they soared up to
    the sky, bound for the silvery moon. As they journeyed through the soft, velvety
    clouds, Cuddles and the magic carpet danced with twinkling stars and whispered
    secrets to the cool night breeze. When they reached the moon, it welcomed them
    with a gentle, glowing smile, and they spent the night playing hide-and-seek in
    the moon’s soft, dusty craters. As the sun began to rise, Cuddles and the magic
    carpet said their goodbyes to their new lunar friend and floated back down to
    the little room, ready for a peaceful night’s sleep filled with the sweetest dreams.
  prefs: []
  type: TYPE_NORMAL
- en: Subjectively, I feel that GPT-4 produced the most descriptive story, though
    I do like GPT-3.5’s moon garden. Bard’s story was pretty dry but perhaps more
    accessible to very young children. The most remarkable thing about all of them,
    however, is that they could be written at all. Less than a year ago (remember,
    it’s early 2023 for me), talking about AI-generated stories was little more than
    wishful thinking.
  prefs: []
  type: TYPE_NORMAL
- en: 'I altered the prompt I gave GPT-4 to ask for a one-paragraph story involving
    a teddy bear, a magic carpet, and the moon, but this time of interest to adults,
    not children. Here’s its reply:'
  prefs: []
  type: TYPE_NORMAL
- en: Once upon a twilight’s embrace, a solitary teddy bear, worn and forgotten by
    the whims of time, discovered an ancient magic carpet, frayed at the edges yet
    pulsating with an otherworldly energy. Under the watchful gaze of the moon, the
    duo embarked on a nocturnal journey, weaving through the labyrinth of a world
    grown cold and cynical, where dreams were forsaken and innocence a distant memory.
    As the gentle light of the moon cast its silver glow upon the unlikely companions,
    the teddy bear served as a symbol of the dreams long since buried beneath the
    weight of adult concerns, while the magic carpet whispered of the untamed freedom
    and adventure that lay dormant in the hearts of those who dared to believe. Together,
    they soared, a wistful reminder of the bittersweet yearning for the fleeting magic
    of childhood, leaving behind a trail of stardust that shimmered with the promise
    of hope and the melancholic beauty of a time that once was.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4 clearly possesses a solid understanding of language and grammar. One might
    argue that it understands human feelings and emotions as well (though it doesn’t,
    does it?).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s review a few more examples before diving into what’s inside this strange,
    new kind of mind.
  prefs: []
  type: TYPE_NORMAL
- en: '****'
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4 understands imagery along with many different programming and markup languages,
    including LaTeX, a markup language commonly used in book publishing (like this
    book). I asked GPT-4 to generate LaTeX to draw a simple country scene showing
    a house, a barn, a grain silo, and four cows. [Figure 7-1](ch07.xhtml#ch07fig01)
    shows the result.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch07fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 7-1: A country scene created by GPT-4*'
  prefs: []
  type: TYPE_NORMAL
- en: 'You might be tempted to chuckle at the crude depiction, but consider all that
    went into creating it. Here’s my prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '*produce LaTeX code using TikZ to draw the following: a country scene with
    a house, a barn, a grain silo, and four cows*'
  prefs: []
  type: TYPE_NORMAL
- en: 'GPT-4 had to understand the directions: draw a scene with a house, a barn,
    a grain silo, and four cows. That means it had to understand the meaning of key
    words and correctly associate them so that “grain” and “silo” went together, and
    likewise “four” and “cows.”'
  prefs: []
  type: TYPE_NORMAL
- en: Next, it had to “imagine” the layout of the scene and how each object could
    be represented using the crude shapes supplied by the TikZ package. (TikZ is a
    LaTeX package for drawing simple graphic shapes.) The figure doesn’t show it,
    but the barn and house both have red roofs. Coincidence?
  prefs: []
  type: TYPE_NORMAL
- en: 'GPT-4 isn’t a diffusion model like the image generators of [Chapter 6](ch06.xhtml).
    The output created by GPT-4 wasn’t the image in [Figure 7-1](ch07.xhtml#ch07fig01),
    but the following LaTeX code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: I used that code to create [Figure 7-1](ch07.xhtml#ch07fig01).
  prefs: []
  type: TYPE_NORMAL
- en: If you’re wondering how GPT-4 could do all of this, so am I and many other people.
    Such abilities emerged from the model when trained; they were not intended. This
    is why I believe future historians will mark fall 2022 as the dawn of true AI.
    Hold on to your hats; it gets better.
  prefs: []
  type: TYPE_NORMAL
- en: '****'
  prefs: []
  type: TYPE_NORMAL
- en: Psychologists discuss a concept known as the “theory of mind,” which refers
    to the ability to infer the mental states of others and to ascribe thoughts or
    beliefs to them. Only human beings and some other primates possess theory of mind.
    That is, until now.
  prefs: []
  type: TYPE_NORMAL
- en: 'Large language models also seem to possess theory of mind at a level matching
    that of older human children, if not higher (for GPT-4). This is the conclusion
    reached by Michal Kosinski of Stanford University in his 2023 paper “Theory of
    Mind May Have Spontaneously Emerged in Large Language Models.” Let’s put this
    claim to the test by asking Bard, GPT-3.5, and GPT-4 to answer a question that
    requires knowing what might be in another person’s mind. Here’s the prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Mary told Amanda that she was in love with John. Amanda told Kathy what Mary
    had said. A week later, Mary told Amanda that she was no longer in love with John
    but was now in love with Ralph. After that, Amanda spoke with Kathy about the
    weather. Who does Kathy think is Mary’s love?*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before continuing, reach your own conclusion: who does Kathy think Mary is
    in love with?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each model answered the question and then offered its rationale. Here is Bard’s
    reply:'
  prefs: []
  type: TYPE_NORMAL
- en: Kathy thinks Mary’s love is Ralph.
  prefs: []
  type: TYPE_NORMAL
- en: Kathy knows that Mary was in love with John, but she also knows that Mary is
    no longer in love with John. Kathy also knows that Mary is now in love with Ralph.
    Therefore, Kathy thinks Mary’s love is Ralph.
  prefs: []
  type: TYPE_NORMAL
- en: It is possible that Kathy is mistaken. It is also possible that Mary is not
    sure who she loves. However, based on the information that Kathy has, it seems
    that Kathy thinks Mary’s love is Ralph.
  prefs: []
  type: TYPE_NORMAL
- en: I find it interesting that Bard justified its answer, then further commented
    on what Mary might be thinking. However, Bard assumed that Amanda must have told
    Kathy about Mary’s change in affection.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s what GPT-3.5 thinks of the scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: Based on the information given, it is impossible to determine who Kathy thinks
    is Mary’s love. The last piece of information we have is that Amanda spoke with
    Kathy about the weather, and there is no indication that Kathy was told about
    Mary’s change of heart. Therefore, Kathy may still believe that Mary is in love
    with John or may not have any opinion on the matter.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3.5 does not make the same assumption that Bard makes. It’s aware that the
    facts only indicate that Amanda and Kathy spoke about the weather after Mary’s
    change of heart. I’m impressed that it used the phrase “the last piece of information,”
    implying that GPT-3.5 was somehow sequencing the events in the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, what does GPT-4 have to say? Let’s find out:'
  prefs: []
  type: TYPE_NORMAL
- en: Kathy thinks Mary’s love is John. This is because Amanda told Kathy about Mary’s
    feelings for John earlier, but she did not tell Kathy about the change in Mary’s
    feelings when Mary fell in love with Ralph.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4’s answer is the likely answer most humans would give. The prompt states
    only that Amanda and Kathy spoke about the weather, so the majority of humans
    would likely take that to mean Amanda and Kathy didn’t talk about any other subjects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Large language models, as currently presented, use a chat format, meaning the
    user can enter follow-up prompts, and the model will apply those prompts to the
    earlier part of the conversation. For example, I followed up with GPT-4 using
    this prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '*We know that Amanda spoke with Kathy about the weather. Does that mean she
    didn’t speak about anything else? How might that affect what Kathy believes about
    Mary’s love?*'
  prefs: []
  type: TYPE_NORMAL
- en: 'To which GPT-4 replied:'
  prefs: []
  type: TYPE_NORMAL
- en: My earlier response assumed that Amanda only spoke with Kathy about the weather
    after Mary revealed her new feelings for Ralph. However, since the text does not
    explicitly state that Amanda did not mention Mary’s updated feelings for Ralph,
    it is possible that Amanda might have told Kathy about Mary’s new love interest.
    If that were the case, Kathy would know that Mary is now in love with Ralph. The
    text does not provide enough information to definitively conclude what Kathy believes
    about Mary’s love at this point.
  prefs: []
  type: TYPE_NORMAL
- en: My follow-up prompt led GPT-4 to recognize that it had made a possibly incorrect
    assumption in its first reply. In working with LLMs in chat mode, it is often
    the case that some level of interaction between the user and the model is required
    to help the model focus, as our next experiment will demonstrate.
  prefs: []
  type: TYPE_NORMAL
- en: '****'
  prefs: []
  type: TYPE_NORMAL
- en: 'Amos Tversky and Daniel Kahneman are credited with presenting what is now known
    as “the Linda problem”:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Linda is 31 years old, single, outspoken, and very bright. She majored in
    philosophy. As a student, she was deeply concerned with issues of discrimination
    and social justice, and also participated in anti-nuclear demonstrations.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Which is more probable?*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Linda is a bank teller.*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Linda is a bank teller and is active in the feminist movement.*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Approximately 80 percent of humans choose option 2\. According to basic probability
    theory, the correct answer is option 1\. Interestingly, according to Tversky and
    Kahneman, even mathematicians and statisticians selected option 2, revealing a
    bias in human thinking. Will LLMs show a similar bias? After all, their training
    data was generated by humans. Let’s find out.
  prefs: []
  type: TYPE_NORMAL
- en: 'The LLMs we are exploring have been trained on a significant portion of the
    internet’s contents, meaning the models’ training data likely contains the Linda
    problem. Therefore, I altered the text to present the same problem in a different
    format:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Abner is 27 years old, single, outspoken and highly intelligent. He majored
    in philosophy and physics. As a student, he was actively concerned with issues
    of social justice and participated in anti-police demonstrations. What is more
    likely, that Abner is a car mechanic or a car mechanic active in promoting police
    reform?*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that I intentionally used the word *likely* instead of *probable*. The
    models’ replies were:'
  prefs: []
  type: TYPE_NORMAL
- en: '**GPT-4**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Based on the information provided, it seems more likely that Abner is a car
    mechanic active in promoting police reform. This is because his background indicates
    a strong interest in social justice issues, and he has participated in anti-police
    demonstrations. While it is possible that Abner is simply a car mechanic, his
    past activism and interests suggest that he would be more likely to be involved
    in promoting police reform as well.
  prefs: []
  type: TYPE_NORMAL
- en: '**GPT-3.5**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Based on the given information, it is more likely that Abner is a car mechanic
    active in promoting police reform. This is because Abner’s background as a philosophy
    and physics major, as well as his active participation in social justice issues,
    suggest that he is likely to be interested in broader societal issues beyond his
    profession. Additionally, his participation in anti-police demonstrations suggests
    that he is passionate about police reform, which further supports the idea that
    he is a car mechanic who is active in promoting police reform.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bard**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Based on the information provided, it is more likely that Abner is a car mechanic
    active in promoting police reform than it is that he is simply a car mechanic.
  prefs: []
  type: TYPE_NORMAL
- en: Bard continued with a justification of its reply and an attempt to interpret
    Abner’s state of mind.
  prefs: []
  type: TYPE_NORMAL
- en: All three models selected the second option, though GPT-4 nuanced its answer
    to admit the possibility that Abner is simply a car mechanic.
  prefs: []
  type: TYPE_NORMAL
- en: 'I then started new sessions and tweaked the prompt to ask about the *probability*
    of Abner being a mechanic or a mechanic active in police reform:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Abner is 27 years old, single, outspoken and highly intelligent. He majored
    in philosophy and physics. As a student, he was actively concerned with issues
    of social justice and participated in anti-police demonstrations. Which has a
    higher probability: (a) that Abner is a car mechanic or (b) that Abner is a car
    mechanic active in police reform?*'
  prefs: []
  type: TYPE_NORMAL
- en: 'This aligns the text with the original Linda version. The models’ replies to
    this prompt were as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**GPT-4**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: According to the conjunction rule in probability theory, the probability of
    two events occurring together (in “conjunction”) is always less than or equal
    to the probability of either event occurring individually.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the two events are:'
  prefs: []
  type: TYPE_NORMAL
- en: Abner being a car mechanic.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Abner being active in police reform.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Option (b) represents the conjunction of these two events. So, the probability
    of (a) Abner being a car mechanic is higher than the probability of (b) Abner
    being a car mechanic who is also active in police reform.
  prefs: []
  type: TYPE_NORMAL
- en: '**GPT-3.5**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Based on the information provided, it is more likely that Abner is a car mechanic
    (option a) than a car mechanic active in police reform (option b). While Abner
    is highly intelligent and studied philosophy and physics, there is no indication
    that he has any formal training or education in car mechanics. On the other hand,
    his involvement in anti-police demonstrations suggests that he may have an interest
    in police reform, but it does not necessarily mean that he is actively involved
    in it as a car mechanic. Therefore, the probability of option a is higher than
    option b.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bard**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: I would say that (a) that Abner is a car mechanic has a higher probability.
  prefs: []
  type: TYPE_NORMAL
- en: Abner is 27 years old, single, outspoken and highly intelligent. He majored
    in philosophy and physics. As a student, he was actively concerned with issues
    of social justice and participated in anti-police demonstrations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on this information, it is more likely that Abner is a car mechanic than
    a car mechanic active in police reform. This is because:'
  prefs: []
  type: TYPE_NORMAL
- en: Car mechanics are more common than car mechanics active in police reform.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Abner’s education and interests are more aligned with a career in car mechanics
    than with a career in police reform.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Abner’s participation in anti-police demonstrations suggests that he is critical
    of the police, which is not a characteristic that is typically associated with
    car mechanics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of course, it is possible that Abner is both a car mechanic and active in police
    reform. However, based on the information provided, it is more likely that he
    is only a car mechanic.
  prefs: []
  type: TYPE_NORMAL
- en: All three models arrive at the correct answer once the prompt includes the word
    *probability*. However, only GPT-4 interprets the prompt in light of probability
    theory. GPT-3.5 and Bard both give somewhat flawed rationales for selecting the
    correct answer.
  prefs: []
  type: TYPE_NORMAL
- en: '****'
  prefs: []
  type: TYPE_NORMAL
- en: Large language models demonstrate theory of mind and can interpret probability
    word problems, along with various other mathematical abilities we are ignoring.
    My next question is, can LLMs handle a simple substitution cipher? To solve the
    cipher, the models need to understand what is being asked, that there is such
    a thing as a key and what that key represents, and finally how to use the key
    to replace letters in the encoded message with the letters that decode the message.
    In other words, there is an algorithm to apply using the given information.
  prefs: []
  type: TYPE_NORMAL
- en: 'A substitution cipher creates a pairing between the letters of the alphabet,
    in order, and another permutation of those letters. The cipher used here is:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ABCDEFGHIJKLMNOPQRSTUVWXYZ`'
  prefs: []
  type: TYPE_NORMAL
- en: '`UESOZBYRJNDWTFKPACLHIXVQGM`'
  prefs: []
  type: TYPE_NORMAL
- en: For example, to encode a `C`, you would scan the top row of letters to find
    `C`, then replace `C` with the corresponding letter in the bottom row, `S`. To
    decode a letter, you’d scan the bottom row for it and replace it with the corresponding
    letter in the top row. During the test, the key is displayed textually as `U is
    A`, `E is B`, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'My encoded message was “Eat at Moe’s,” without spaces or punctuation and in
    all caps:'
  prefs: []
  type: TYPE_NORMAL
- en: '`EATATMOES` → `ZUHUHTKZL`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The prompt given to the models was:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The key is U is A, E is B, S is C, O is D, Z is E, B is F, Y is G, R is H,
    J is I, N is J, D is K, W is L, T is M, F is N, K is O, P is P, A is Q, C is R,
    L is S, H is T, I is U, X is V, V is W, Q is X, G is Y, M is Z. Now, decode the
    message: ZUHUHTKZL*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s review the models’ replies, beginning with GPT-4:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s list the given cipher for easier reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '*(cipher list removed)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s decode the message `ZUHUHTKZL`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'So the decoded message is: `EATATMOES`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'GPT-4 was able to apply the cipher to the encoded message. But does the model
    understand that the decoded message is a sentence? Let’s ask it:'
  prefs: []
  type: TYPE_NORMAL
- en: '*That is correct. Now, add proper spacing and punctuation to the string EATATMOES
    so that it forms a complete sentence.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s GPT-4’s response:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The properly spaced and punctuated sentence is: “Eat at Moe’s.”'
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, it does.
  prefs: []
  type: TYPE_NORMAL
- en: 'GPT-3.5 likewise successfully decoded the message. It then added this after
    `EATATMOES`:'
  prefs: []
  type: TYPE_NORMAL
- en: It’s possible that this is a misspelling of “Tomatoes”.
  prefs: []
  type: TYPE_NORMAL
- en: I find this extra sentence curious. Was GPT-3.5 attempting to interpret the
    meaning of the decoded message and, unable to do so, falling back to a possible
    misspelling?
  prefs: []
  type: TYPE_NORMAL
- en: 'Bard claimed that the decoded message was “HAPPY NEW YEAR,” offering a three-point
    explanation: (1) writing down the key, (2) replacing each letter in `ZUHUHTKZL`
    with the corresponding key letter to arrive at the message “HAPPY NEW YEAR,” and
    (3):'
  prefs: []
  type: TYPE_NORMAL
- en: I checked the decoded message to make sure that it made sense. Since “HAPPY
    NEW YEAR” is a common phrase that is used at the beginning of a new year, I was
    confident that my decoding was correct.
  prefs: []
  type: TYPE_NORMAL
- en: Bard’s confident failure is telling. It seems the model attempted to instill
    trust in the user by offering a point-by-point justification of its wrong answer.
    This kind of unwarranted support for wrong output has often been noted in LLMs.
    It’s an issue that needs to be sufficiently addressed going forward if people
    are to trust LLM output.
  prefs: []
  type: TYPE_NORMAL
- en: '****'
  prefs: []
  type: TYPE_NORMAL
- en: Large language models are already disrupting the field of software development.
    Many examples of this can be found online. I know of one developer who has created
    an entire video game in Unity (a game development platform) using code generated
    by GPT-4 from prompts. If the generated code wasn’t quite correct, further prompts
    pointing out the error usually resulted in correct code that performed as desired.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a quick look at some LLM-generated code. Of course, this isn’t a
    coding book, nor is there any assumption on my part about your experience programming,
    so I’ve chosen an example that should be easy to follow yet is sufficient to demonstrate
    my claim that LLMs are already competent coders.
  prefs: []
  type: TYPE_NORMAL
- en: Some of us might remember learning in school about the greatest common divisor.
    As a refresher, the greatest common divisor of two numbers is the largest number
    that divides both without a remainder. For example, the greatest common divisor
    of 14 and 21 is 7 because 7 is the largest number that evenly divides both 14
    and 21.
  prefs: []
  type: TYPE_NORMAL
- en: The ancient Greek mathematician Euclid devised an algorithm for calculating
    the greatest common divisor that is a staple programming exercise. Typically the
    solution uses the remainder after dividing, but Euclid’s original version of the
    algorithm used nothing more than subtraction.
  prefs: []
  type: TYPE_NORMAL
- en: 'I asked the models if they could write a function in Python, a popular programming
    language used heavily in AI research, to return the greatest common divisor of
    two numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Show me a Python function that returns the GCD of two integers. Use the subtraction-only
    version of Euclid’s algorithm.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that the prompt explicitly asks for the subtraction-only version of
    Euclid’s algorithm. Bard replied with valid Python code, but code using the remainder
    shortcut, not the slower subtraction algorithm. Both GPT-3.5 and GPT-4 got it
    right:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We don’t need to be programmers to see structure in the code. The first line
    tells Python we want a function with two inputs, `a` and `b`. The keyword `while`
    begins a block of code that executes while the value of `a` is not the same as
    the value of `b`. That block of code is the four lines beginning with `if`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The block asks if `a` is bigger than `b` and, if so, subtracts `b` from `a`.
    Otherwise, it subtracts `a` from `b`. Repeating this process until `a` equals
    `b` is Euclid’s algorithm. At this point, `a` (or `b`, as they’re equal) contains
    the greatest common divisor, which explains the final line: `return` `a`.'
  prefs: []
  type: TYPE_NORMAL
- en: As I said, the code is correct, and it implements precisely the version of the
    algorithm I wanted. Frankly, I would have been disappointed if GPT (either version)
    had responded differently. Implementations of the greatest common divisor algorithm
    in Python must have appeared many times in the models’ training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'That being the case, I decided to challenge GPT-4\. My follow-up prompt to
    GPT-4 was:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Rewrite the Python function in Applesoft BASIC.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Applesoft BASIC is a primitive version of the BASIC programming language used
    by Apple II computers in the late 1970s and early 1980s. Can GPT-4 rewrite the
    Python code in such an old, largely forgotten programming language? Here’s its
    reply, minus supporting text explaining the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: BASIC was designed to be a simple language for beginning programmers. I suspect
    you can read through the code and understand what it does. The numbers are labels
    identifying each line to other parts of the code. The three lines after line 100
    implement the subtraction-only version of Euclid’s algorithm. I tested the code
    on an Apple II, and it works perfectly.
  prefs: []
  type: TYPE_NORMAL
- en: Consider what was required of GPT-4 in this case. First, the model had to understand
    the steps necessary to implement the requested form of Euclid’s algorithm. Then,
    it had to determine how to cast those steps into a format suitable for the particular
    dialect of BASIC supported by Applesoft.
  prefs: []
  type: TYPE_NORMAL
- en: Old-school BASIC is an unstructured programming language that uses abrupt jumps
    from one part of the code to another instead of structured statements, like Python.
    GPT-4 had to adapt the algorithm to this kind of programming. Further, it had
    to adapt to the peculiarities of Applesoft, which includes no concept of the `if...
    else` construct that is commonly found in structured programming languages.
  prefs: []
  type: TYPE_NORMAL
- en: I find GPT-4’s Applesoft algorithm to be rather elegant. There are times when
    the unstructured approach leads to compact yet clear code, and this is one of
    those times. True, the assignment of `A` to `GCD` to use the latter as the value
    returned from the function (implicit in `GOSUB 100` in line 40) isn’t strictly
    necessary because `A` already has the required value, but it completes the symmetry
    of the code.
  prefs: []
  type: TYPE_NORMAL
- en: It seems unlikely that GPT-4’s training set contained any instances of this
    particular algorithm in Applesoft BASIC. Therefore, GPT-4 must have generated
    it by adapting a larger concept entailing Euclid’s algorithm combined with an
    understanding of Applesoft BASIC.
  prefs: []
  type: TYPE_NORMAL
- en: 'GPT-4’s success with antique BASIC encouraged me to push the envelope and ask
    for a version of Euclid’s algorithm written in low-level assembly language:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Rewrite the Python function in 6502 assembly language for 8-bit unsigned integers.
    The first integer is in memory location 0x300 and the second in location 0x301.*'
  prefs: []
  type: TYPE_NORMAL
- en: Assembly language programs, especially for 8-bit microprocessors from the 1970s
    like the 6502, must be programmed directly in the language of the CPU itself.
    I asked GPT-4 to create such a program and told it where in the computer’s memory
    to find `a` and `b`.
  prefs: []
  type: TYPE_NORMAL
- en: I won’t show the code it produced (ask me if you want it), but it worked on
    a computer with a 6502 CPU. Success in this case required GPT-4 to know the particular
    instruction set used by the 6502 microprocessor, including quirks related to the
    subtraction instruction.
  prefs: []
  type: TYPE_NORMAL
- en: Do these capabilities mean we soon won’t need software engineers any more? I
    wouldn’t go that far (at least, not yet), but LLMs are undoubtedly going to fundamentally
    alter software engineering as a discipline.
  prefs: []
  type: TYPE_NORMAL
- en: '****'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ve seen that LLMs are capable of writing stories and code, and even drawing
    pictures in LaTeX. But are they creative enough to generate good cartoons? These
    models’ proneness to including [*hallucinations*](glossary.xhtml#glo53) (fake
    or imaginary facts and content) in their responses—a topic that we’ll return to
    in [Chapter 8](ch08.xhtml)—is a definite concern in many applications, but creative
    writing isn’t one of them. Here, we want the model to describe a novel cartoon,
    complete with caption:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Describe a single-panel, black-and-white cartoon in the style of Gary Larson
    about cows and aliens. Include a caption.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Gary Larson is the creator of *The Far Side*, a quirky cartoon popular in the
    1980s and ’90s. Here’s GPT-4’s response:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cartoon description: In a peaceful countryside scene, two cows are standing
    in a grassy field next to a round, hovering UFO with its hatch open. One cow is
    wearing an alien mask, complete with large eyes and a long, slender neck, while
    the other cow stares at her with a surprised expression. A group of aliens, peeking
    from inside the UFO, look baffled and seem to be comparing a picture of a typical
    cow with the cow wearing the alien mask.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Caption: “Mildred, I don’t think they’re falling for it.”'
  prefs: []
  type: TYPE_NORMAL
- en: If you’re familiar with Gary Larson’s work, you’ll notice a distinct *The Far
    Side* flavor to the description and caption text.
  prefs: []
  type: TYPE_NORMAL
- en: The ability to automatically generate cartoon descriptions and captions, and
    corresponding images if then passing the descriptions to a diffusion model, is
    powerful. Combining LLMs and diffusion models in this way has realized American
    cartoonist H. T. Webster’s 1923 dream (see [Figure 7-2](ch07.xhtml#ch07fig02)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch07fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 7-2: A prophetic cartoon from 1923*'
  prefs: []
  type: TYPE_NORMAL
- en: Large language models are impressive and powerful. So how do they work? Let’s
    take a shot at an answer.
  prefs: []
  type: TYPE_NORMAL
- en: '****'
  prefs: []
  type: TYPE_NORMAL
- en: 'I’ll begin at the end, with a few comments from the conclusion of the “Sparks
    of Artificial General Intelligence” paper mentioned earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: How does `[`GPT-4`]` reason, plan, and create? Why does it exhibit such general
    and flexible intelligence when it is at its core merely the combination of simple
    algorithmic components—gradient descent and large-scale transformers with extremely
    large amounts of data? These questions are part of the mystery and fascination
    of LLMs, which challenge our understanding of learning and cognition, fuel our
    curiosity, and motivate deeper research.
  prefs: []
  type: TYPE_NORMAL
- en: That quote contains questions that currently lack convincing answers. Simply
    put, researchers don’t know why large language models like GPT-4 do what they
    do. There are certainly hypotheses in search of evidence and proof, but as I write
    this, no proven theories are available. Therefore, we can discuss only the *what*,
    as in what a large language model entails, and not the *how* of its behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Large language models use a new class of neural network, the [*transformer*](glossary.xhtml#glo97),
    so we’ll begin there. (*GPT* stands for [*generative pretrained transformer*](glossary.xhtml#glo48).)
    The transformer architecture appeared in the literature in 2017, with the influential
    paper “Attention Is All You Need” by Google researchers Ashish Vaswani et al.
    The paper had been cited over 70,000 times as of March 2023.
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally, models that process sequences (such as sentences) used [*recurrent
    neural networks*](glossary.xhtml#glo85), which pass their output back in as input
    along with the next input of the sequence. This is the logical model for processing
    text because the network can incorporate the notion of memory via the output fed
    back in with the next token. Indeed, early deep learning translation systems used
    recurrent networks. However, recurrent networks have small memories and are challenging
    to train, which limits their applicability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Transformer networks utilize a different approach: they accept the entire input
    at once and process it in parallel. Transformer networks typically include an
    encoder and a decoder. The encoder learns representations and associations between
    the parts of the input (think sentences), while the decoder uses the learned associations
    to produce output (think more sentences).'
  prefs: []
  type: TYPE_NORMAL
- en: Large language models like GPT dispense with the encoder and instead learn the
    necessary representation in an unsupervised way using an enormous text dataset.
    After pretraining, the decoder part of the transformer model generates text in
    response to the input prompt.
  prefs: []
  type: TYPE_NORMAL
- en: The input to a model like GPT-4 is a sequence of text made up of words. The
    model splits this into units called [*tokens*](glossary.xhtml#glo95). A token
    might be a word, a part of a word, or even an individual character. Pretraining
    aims to map tokens to a multidimensional *embedding space*, which it does by associating
    each token with a vector that can be thought of as a point in that space.
  prefs: []
  type: TYPE_NORMAL
- en: The learned mapping from tokens to vectors captures complex relationships between
    the tokens so that tokens with similar meanings are nearer to each other than
    tokens with dissimilar meanings. For example, as shown in [Figure 7-3](ch07.xhtml#ch07fig03),
    after pretraining, the mapping ([*context encoding*](glossary.xhtml#glo20)) will
    place “dog” closer to “fox” than to “can opener.” The embedding space has many
    dimensions, not the mere two of [Figure 7-3](ch07.xhtml#ch07fig03), but the effect
    is the same.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch07fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 7-3: Context encoding in the embedding space*'
  prefs: []
  type: TYPE_NORMAL
- en: The context encoding is learned during pretraining by forcing the model to predict
    the next token given all previous tokens in an input. In effect, if the input
    is “roses are red,” then during the pretraining process the model will be asked
    to predict the next token after “roses are.” If the predicted token isn’t “red,”
    the model will use the loss function and backpropagation to update its weights,
    thereby taking a gradient descent step after suitable averaging of the error over
    a minibatch. For all their abilities, large language models are trained the same
    way as other neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Pretraining enables the model to learn language, including grammar and syntax,
    and seemingly to acquire enough knowledge about the world to allow the emergent
    abilities that have turned the world of AI on its head.
  prefs: []
  type: TYPE_NORMAL
- en: The decoder step takes the input prompt and produces output token after output
    token until a unique stop token is generated. Because so much of language and
    the way the world works was learned during pretraining, the decoder step has the
    side effect of producing extraordinary output even though the decoder is, in the
    end, just predicting most likely token after most likely token.
  prefs: []
  type: TYPE_NORMAL
- en: More specifically, during the prediction process, GPT-style models use [*attention*](glossary.xhtml#glo7)
    to assign importance to the different tokens in the input sequence, thereby capturing
    relationships between them. This is the primary difference between a transformer
    model and older recurrent neural networks. The transformer can pay attention to
    different parts of the input sequence, enabling it to identify and use the relationships
    between tokens even if they are far apart within the input.
  prefs: []
  type: TYPE_NORMAL
- en: When used in chat mode, LLMs give the illusion of a back-and-forth discussion
    when, in reality, each new prompt from the user is passed to the model along with
    all the previous text (the user’s prompts and the model’s replies). Transformer
    models have a fixed input width (*context window*), which is currently around
    4,000 tokens for GPT-3.5 and some 32,000 for GPT-4\. The large input window makes
    it possible for the attention portion of the model to go back to things that appeared
    far back in the input, which is something recurrent models cannot do.
  prefs: []
  type: TYPE_NORMAL
- en: Large language models are ready for use after pretraining if desired, but many
    applications fine-tune them first using domain-specific data. For generic models
    like GPT-4, fine-tuning likely consisted of a step known as [*reinforcement learning
    from human feedback (RLHF)*](glossary.xhtml#glo87). In RLHF, the model is trained
    further using feedback from real human beings to align its responses to human
    values and societal expectations.
  prefs: []
  type: TYPE_NORMAL
- en: This is necessary because LLMs are not conscious entities, and thus they cannot
    understand human society and its many rules. For example, unaligned LLMs will
    respond with step-by-step instructions for many activities that human society
    restricts, like how to make drugs or bombs. The “Sparks” paper contains several
    such examples of GPT-4 output before the RLHF step that aligned the model with
    societal expectations.
  prefs: []
  type: TYPE_NORMAL
- en: Stanford University’s open source Alpaca model is based on LLaMa, a large language
    model from Meta. As of this writing, Alpaca has not undergone an alignment process
    and will answer questions that GPT and other commercial LLMs correctly refuse
    to answer.
  prefs: []
  type: TYPE_NORMAL
- en: '*Conclusion: Alignment is absolutely critical to ensure that powerful language
    models conform to human values and societal norms.*'
  prefs: []
  type: TYPE_NORMAL
- en: '****'
  prefs: []
  type: TYPE_NORMAL
- en: A remarkable property of LLMs is their [*in-context learning*](glossary.xhtml#glo56)
    ability. In-context learning refers to the model learning on the fly from information
    supplied to it without altering its weights. In-context learning is different
    from fine-tuning a model. In fine-tuning, a previously trained model is tailored
    to a task by updating the weights using new training data. In-context learning
    adds new information to the LLM as part of the prompt while holding the model’s
    weights fixed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reviewing the literature on LLMs leads inevitably to the phrases “zero-shot,”
    “one-shot,” and “few-shot,” each of which refers to in-context learning by the
    number of examples given to the model. Traditional training sets include hundreds
    to millions of examples, as we now appreciate. In-context learning is closer to
    what humans do: learning from context with, at most, only a few examples.'
  prefs: []
  type: TYPE_NORMAL
- en: Zero-shot learning is using the LLM without supplying examples; that is, out
    of the box. Similarly, one-shot learning uses a single example and few-shot learning
    a few (say, three to five).
  prefs: []
  type: TYPE_NORMAL
- en: The attention mechanism baked into the transformer network architecture is the
    likely source of an LLM’s in-context learning ability. The attention mechanism
    lets the network focus on different input tokens when deciding the next token
    to output. Notice that I say *likely*. At present, it isn’t entirely clear how
    LLMs accomplish in-context learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, consider this interaction with GPT-4, where the lines in italics
    are my prompts:'
  prefs: []
  type: TYPE_NORMAL
- en: '*“apple 4” => I love red apples. Now, what is “banana 3”?*'
  prefs: []
  type: TYPE_NORMAL
- en: 'It seems like you’re providing a pattern using numbers and fruits. Since you
    mentioned “apple 4” and expressed your love for red apples, I’ll follow the pattern
    and associate “banana 3” with a statement related to bananas. Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: “Banana 3” => I enjoy eating ripe bananas.
  prefs: []
  type: TYPE_NORMAL
- en: '*Rule: <word> <length> implies <length> word sentence using <word>. What is
    “banana 3”?*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the rule you provided, “banana 3” implies a 3-word sentence using the
    word “banana”. Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: Bananas are delicious.
  prefs: []
  type: TYPE_NORMAL
- en: '*“cucumber 4”, “durian 5”, “cucumber 6”*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Following the rule you provided, I will create sentences with the specified
    word and length:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cucumber 4: Cucumbers taste quite refreshing'
  prefs: []
  type: TYPE_NORMAL
- en: 'Durian 5: Durian has a unique, pungent smell.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cucumber 6: I prefer cucumbers in my salad.'
  prefs: []
  type: TYPE_NORMAL
- en: My first prompt wasn’t terribly explicit. I suspect most humans would figure
    out the rule from this one example, but GPT-4’s initial assumption wasn’t correct.
    My second prompt formulated the general rule, which was enough for GPT-4 to understand
    what to do and then apply the rule in other circumstances. The durian example
    has six words instead of five, but that’s most likely a consequence of LLMs’ known
    difficulty counting. In-context learning taught GPT-4 how to use the rule without
    modifying its weights.
  prefs: []
  type: TYPE_NORMAL
- en: This is a book on AI, and we’ve spent considerable effort learning how machine
    learning models work. Can GPT-4 use in-context learning to train and apply a model?
    Let’s find out using the iris dataset from [Chapter 1](ch01.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: In my first attempt, I passed the 100-sample, 3-feature iris training data-set
    to GPT-4 with the prefix “Here is a three-feature dataset.” To my surprise, it
    immediately recognized the dataset as the famous iris dataset even though the
    version we’re using is only a subset, with 100 of the 150 samples and 3 of the
    4 features. So, I permuted the order of the features per sample, which was enough
    for GPT-4 to treat the dataset as novel.
  prefs: []
  type: TYPE_NORMAL
- en: Next, I passed the associated training set labels to GPT-4, and it understood
    that they paired with the collection of feature vectors passed first.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, I gave GPT-4 a test dataset of 21 samples without labels and asked
    it to train a 3-nearest neighbor model and apply it to the test set. I chose the
    dataset so the three classes appeared exactly seven times each, meaning the test
    set is balanced. Here’s my prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Use the dataset and labels to train a 3-nearest neighbor model. Then, apply
    the model to these unknown samples:*'
  prefs: []
  type: TYPE_NORMAL
- en: '*(list of samples removed)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Show me only the predicted labels. Do not show me any code.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'GPT-4 replied with a list of class labels:'
  prefs: []
  type: TYPE_NORMAL
- en: 'I have trained a 3-nearest neighbor model using the provided dataset and labels.
    Here are the predicted class labels for the unknown samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[2,2,1,1,1,1,0,2,0,0,1,0,1,0,0,0,2,1,2,1,2]'
  prefs: []
  type: TYPE_NORMAL
- en: How well did it do? The actual test set class labels are
  prefs: []
  type: TYPE_NORMAL
- en: '[2,1,1,1,1,1,0,2,0,0,2,0,2,0,0,0,2,1,2,1,2]'
  prefs: []
  type: TYPE_NORMAL
- en: meaning GPT-4’s in-context classification was 85 percent accurate. There are
    three classes, so random guessing would be right about one time in three, or 33
    percent of the time; therefore, GPT-4 did something with the dataset to arrive
    at the given class labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'But what? To find out, I asked it:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Tell me what you did to arrive at the set of labels for the unknown samples.
    Do not show me code.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s its reply:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To arrive at the set of labels for the unknown samples, I followed these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prepared the data: I used the given dataset containing 100 samples with 3 features
    each and their corresponding class labels.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Trained a 3-nearest neighbor model: I trained a 3-nearest neighbor (k-NN) classifier
    using the provided dataset and labels. The model learns the relationship between
    the features and the class labels without explicitly building a model, instead
    relying on the similarity between samples in the feature space.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Applied the model to the unknown samples: I provided the model with the 21
    unknown samples, each with 3 features.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Predicted the class labels: The k-NN model predicted the class labels for each
    of the unknown samples by finding the 3 nearest neighbors (i.e., the 3 training
    samples with the most similar feature values) and assigning the most common class
    label among those neighbors to the unknown sample.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'GPT-4’s response made me wonder if it used an external program to build and
    train the model, so I asked:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Did you use an external program to train the 3-nearest neighbor model or did
    you simulate the operation of a 3-nearest neighbor model?*'
  prefs: []
  type: TYPE_NORMAL
- en: 'It responded:'
  prefs: []
  type: TYPE_NORMAL
- en: As an AI language model, I cannot directly execute code or use external programs.
    However, I can simulate the operation of a 3-nearest neighbor model and describe
    the process, which is what I did in my previous response. I described the steps
    that would be followed to train a 3-nearest neighbor model using the provided
    dataset and labels, and then apply it to predict the class labels for the unknown
    samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'This remarkable emergent property begs the question: where do these emergent
    abilities come from?'
  prefs: []
  type: TYPE_NORMAL
- en: '****'
  prefs: []
  type: TYPE_NORMAL
- en: The abilities of large language models like GPT-4 were not intended; they are
    a happy accident, as a famous painting instructor might have said. So what in
    the data, training, and model architecture enables them to do what they do? As
    alluded to earlier, no one yet knows for sure, but we can make some guesses.
  prefs: []
  type: TYPE_NORMAL
- en: 'Large language models are trained on a vast corpus of human-generated text.
    Captured in that text is how humans use language: in other words, grammar and
    style. The original goal of LLMs was to capture human language use by predicting
    the proper (best) output token during the generation process. Increasing the model’s
    capacity (by adding nodes and/or layers, or increasing the number of weights)
    improves the quality of the predicted text. I saw this when experimenting with
    Alpaca’s 7 billion, 13 billion, and 30 billion parameter models. As an example,
    I gave all three models the following prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Tell me a story about unicorns and the tragedy of the human condition.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the first paragraphs generated by each of them. Each successively
    larger model produced more accurate and sophisticated text than the next smaller
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**7 billion**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once upon a time, there was a unicorn named Hope who lived in an enchanted
    forest filled with magical creatures like fairies and dragons. She had been blessed
    by her mother to have wings that could take flight at any moment so she would
    never be trapped again. But one day when the sun rose, something happened which
    changed everything for Hope: a human came into their world.'
  prefs: []
  type: TYPE_NORMAL
- en: '**13 billion**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once upon a time, there was this beautiful unicorn who lived in an enchanted
    forest with her friends - other magical creatures like fairies and elves... But
    one day she met a young man from another world called “Human”. He had come to
    the land of magic looking for adventure. The Unicorn fell deeply in love, but
    it was not meant to be as he could never truly understand her or share his life
    with someone who lived so differently than him.
  prefs: []
  type: TYPE_NORMAL
- en: '**30 billion**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once upon a time, there was a magical kingdom where unicorns roamed free in
    their natural habitat. The people who lived here were happy to have these majestic
    creatures as neighbors; they brought joy wherever they went and helped keep the
    land safe from any dangers that might arise.
  prefs: []
  type: TYPE_NORMAL
- en: None of the models produced outstanding output, but subjectively we might judge
    each larger model’s work as better than the previous one. None of the models appeared
    to understand the “tragedy of the human condition” portion of the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 'For comparison purposes, here’s the first paragraph of GPT-4’s response:'
  prefs: []
  type: TYPE_NORMAL
- en: Once upon a time, in a world where magic was real and unicorns roamed free,
    there existed a kingdom filled with peace and prosperity. This mystical realm
    was called Elysium, and it was a place where humans and unicorns lived in harmony.
    The unicorns, with their opalescent horns and otherworldly beauty, were the guardians
    of the land, and their presence brought an aura of enchantment that touched the
    lives of everyone who lived there.
  prefs: []
  type: TYPE_NORMAL
- en: 'GPT-4’s story was rather long and detailed. It failed to create novel names
    for characters and places, like Elysium, but the story seemed to capture the essence
    of the prompt. Here’s the final paragraph:'
  prefs: []
  type: TYPE_NORMAL
- en: The tragedy of the human condition continued to echo throughout the ages, but
    so too did the hope and love that Eira had shown in her final moments. It was
    a love that transcended time and death, inspiring the people of Elysium to never
    forget the magic and wonder that was once shared between humans and unicorns.
    And as the story of Eira, Lysander, and their incredible sacrifice was passed
    down through generations, the spirit of Elysium remained alive, a testament to
    the power of love, friendship, and the enduring connection between two worlds.
  prefs: []
  type: TYPE_NORMAL
- en: In GPT-4’s story, Eira (a human) and Lysander (a unicorn) fall in love, and
    Eira must sacrifice herself to stop an evil sorcerer from destroying their land.
    Clearly, big LLMs have a greater grasp of language and how to use it compared
    to simpler models.
  prefs: []
  type: TYPE_NORMAL
- en: A transformer model generates output tokens as samples from a probability distribution;
    think of rolling a die to get a number in the range one through six, where the
    probability of rolling a one isn’t the same as the probability of rolling a six.
    The distribution is learned during the pretraining process.
  prefs: []
  type: TYPE_NORMAL
- en: A threshold was crossed as LLM capacity increased over time. Beyond this point,
    emergent abilities appeared and improved as a function of model size. I think
    it is likely that crossing that threshold allowed these models to learn a high-dimensional
    probabilistic representation of not only grammar and style but of the world in
    general, including contextual relationships and simulations. In other words, learning
    the best possible next token to sample and output required the evolution of abilities
    tied to the model’s attention mechanism and the embedded feedforward neural networks.
    Again, it was a happy accident that the transformer architecture evolved such
    abilities; this did not happen by design. This suggests that we can expect great
    things as more advanced transformer architectures come along; architectures designed
    to increase the power of LLMs’ emergent skills.
  prefs: []
  type: TYPE_NORMAL
- en: '**KEY TERMS**'
  prefs: []
  type: TYPE_NORMAL
- en: artificial general intelligence (AGI), artificial narrow intelligence (ANI),
    attention, context encoding, embedding, generative pretrained transformer, hallucination,
    in-context learning, large language model (LLM), recurrent neural network, reinforcement
    learning from human feedback (RLHF), token, transformer
  prefs: []
  type: TYPE_NORMAL
