- en: '**1'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'AND AWAY WE GO: AN AI OVERVIEW**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[*Artificial intelligence*](glossary.xhtml#glo5) attempts to coax a machine,
    typically a computer, to behave in ways humans judge to be intelligent. The phrase
    was coined in the 1950s by prominent computer scientist John McCarthy (1927–2011).'
  prefs: []
  type: TYPE_NORMAL
- en: This chapter aims to clarify what AI is and its relationship to [*machine learning*](glossary.xhtml#glo64)
    and [*deep learning*](glossary.xhtml#glo29), two terms you may have heard in recent
    years. We’ll dive in with an example of machine learning in action. Think of this
    chapter as an overview of AI as a whole. Later chapters will build on and review
    the concepts introduced here.
  prefs: []
  type: TYPE_NORMAL
- en: '****'
  prefs: []
  type: TYPE_NORMAL
- en: Computers are *programmed* to carry out a particular task by giving them a sequence
    of instructions, a *program*, which embodies an [*algorithm*](glossary.xhtml#glo2),
    or the recipe that the program causes the computer to execute.
  prefs: []
  type: TYPE_NORMAL
- en: The word [*algorithm*](glossary.xhtml#glo2) is cast about often these days,
    though it isn’t new; it’s a corruption of *Al-Khwarizmi*, referring to ninth-century
    Persian mathematician Muhammad ibn Musa al-Khwarizmi, whose primary gift to the
    world was the mathematics we call *algebra*.
  prefs: []
  type: TYPE_NORMAL
- en: '****'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin with a story.
  prefs: []
  type: TYPE_NORMAL
- en: Tonya owns a successful hot sauce factory. The hot sauce recipe is Tonya’s own,
    and she guards it carefully. It’s literally her secret sauce, and only she understands
    the process of making it.
  prefs: []
  type: TYPE_NORMAL
- en: Tonya employs one worker for each step of the hot sauce–making process. These
    are human workers, but Tonya treats them as if they were machines because she’s
    worried they’ll steal her hot sauce recipe—and because Tonya is a bit of a monster.
    In truth, the workers don’t mind much because she pays them well, and they laugh
    at her behind her back.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tonya’s recipe is an algorithm; it’s the set of steps that must be followed
    to create the hot sauce. The collection of instructions Tonya uses to tell her
    workers how to make the hot sauce is a program. The program embodies the algorithm
    in a way that the workers (the machine) can follow step by step. Tonya has programmed
    her workers to implement her algorithm to create hot sauce. The sequence looks
    something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch01fig00.jpg)'
  prefs: []
  type: TYPE_IMG
- en: There are a few things to note about this scenario. First, Tonya is definitely
    a monster for treating human beings as machines. Second, at no point in the process
    of making hot sauce does any worker need to understand why they do what they do.
    Third, the programmer (Tonya) knows why the machine (the workers) does what it
    does, even if the machine doesn’t.
  prefs: []
  type: TYPE_NORMAL
- en: '****'
  prefs: []
  type: TYPE_NORMAL
- en: What I’ve just described is how we’ve controlled virtually all computers, going
    back to the first conceptual machines envisioned by Alan Turing in the 1930s and
    even earlier to the 19th-century Analytical Engine of Charles Babbage. A human
    conceives an algorithm, then translates that algorithm into a sequence of steps
    (a program). The machine executes the program, thereby implementing the algorithm.
    The machine doesn’t understand what it’s doing; it’s simply performing a series
    of primitive instructions.
  prefs: []
  type: TYPE_NORMAL
- en: The genius of Babbage and Turing lay in the realization that there could be
    a general-purpose machine capable of executing arbitrary algorithms via programs.
    However, I would argue that it was Ada Lovelace, a friend of Babbage’s often regarded
    as the world’s first programmer, who initially understood the far-reaching possibilities
    of what we now call a computer. We’ll talk more about Turing, Babbage, and Lovelace
    in [Chapter 2](ch02.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*In Lovelace’s day, a “computer” was not a machine but a human being who calculated
    by hand. Hence, Babbage’s Engine was a mechanical computer.*'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a moment to explore the relationship between the terms *AI*, [*machine
    learning*](glossary.xhtml#glo64), and [*deep learning*](glossary.xhtml#glo29).
    On the one hand, all three have become synonymous as referring to modern AI. This
    is wrong, but convenient. [Figure 1-1](ch01.xhtml#ch01fig01) shows the proper
    relationship between the terms.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch01fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 1-1: The relationship between artificial intelligence, machine learning,
    and deep learning*'
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning is a subfield of machine learning, which is a subfield of artificial
    intelligence. This relationship implies that AI involves concepts that are neither
    machine learning nor deep learning. We’ll call those concepts *old-school AI*,
    which includes the algorithms and approaches developed from the 1950s onward.
    Old-school AI is not what people currently mean when discussing AI. Going forward,
    we’ll entirely (and unfairly) ignore this portion of the AI universe.
  prefs: []
  type: TYPE_NORMAL
- en: '[*Machine learning*](glossary.xhtml#glo64) builds models from data. For us,
    a [*model*](glossary.xhtml#glo69) is an abstract notion of something that accepts
    inputs and generates outputs, where the inputs and outputs are related in some
    meaningful way. The primary goal of machine learning is to condition a model using
    *known* data so that the model produces meaningful output when given *unknown*
    data. That’s about as clear as muddy water, but bear with me; the mud will settle
    in time.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Deep learning*](glossary.xhtml#glo29) uses large models of the kind previously
    too big to make useful. More muddy water, but I’m going to argue that there’s
    no strict definition of deep learning other than that it involves neural networks
    with many layers. [Chapter 4](ch04.xhtml) will clarify.'
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we’ll be sloppy but in accord with popular usage, even by experts,
    and take “deep learning” to mean large neural networks (yet to be formally defined),
    “machine learning” to mean models conditioned by data, and “AI” to be a catchall
    for both machine learning and deep learning—remembering that there is more to
    AI than what we discuss here.
  prefs: []
  type: TYPE_NORMAL
- en: Data is everything in AI. I can’t emphasize this enough. Models are blank slates
    that data must condition to make them suitable for a task. If the data is bad,
    the model is bad. Throughout the book, we’ll return to this notion of “good” and
    “bad” data.
  prefs: []
  type: TYPE_NORMAL
- en: For now, let’s focus on what a model is, how it’s made useful by conditioning,
    and how it’s used after conditioning. All this talk of conditioning and using
    sounds dark and sinister, if not altogether evil, but, I assure you, it’s not,
    even though we have ways of making the model talk.
  prefs: []
  type: TYPE_NORMAL
- en: '****'
  prefs: []
  type: TYPE_NORMAL
- en: A machine learning model is a black box that accepts an input, usually a collection
    of numbers, and produces an output, typically a label like “dog” or “cat,” or
    a continuous value like the probability of being a “dog” or the value of a house
    with the characteristics given to the model (size, number of bathrooms, ZIP code,
    and so on).
  prefs: []
  type: TYPE_NORMAL
- en: The model has [*parameters*](glossary.xhtml#glo80), which control the model’s
    output. Conditioning a model, known as [*training*](glossary.xhtml#glo96), seeks
    to set the model’s parameters in such a way that they produce the correct output
    for a given input.
  prefs: []
  type: TYPE_NORMAL
- en: 'Training implies that we have a collection of inputs, and the outputs the model
    should produce when given those inputs. At first blush, this seems a bit silly;
    why do we want the model to give us an output we already have? The answer is that
    we will, at some future point, have inputs for which we don’t already have the
    output. This is the entire point of making the model: to use it with unknown inputs
    and to believe the model when it gives us an output.'
  prefs: []
  type: TYPE_NORMAL
- en: Training uses the collection of known inputs and outputs to adjust the model’s
    parameters to minimize mistakes. If we can do that, we begin to believe the model’s
    outputs when given new, unknown inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Training a model is fundamentally different from programming. In programming,
    we implement the algorithm we want by instructing the computer step by step. In
    training, we use data to teach the model to adjust its parameters to produce correct
    output. There is no programming because, most of the time, we have no idea what
    the algorithm should be. We only know or believe a relationship exists between
    the inputs and the desired outputs. We hope a model can approximate that relationship
    well enough to be useful.
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth remembering the sage words of British statistician George Box, who
    said that all models are wrong, but some are useful. At the time, he was referring
    to other kinds of mathematical models, but the wisdom applies to machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we understand why the field is called machine learning: we teach the machine
    (model) by giving it data. We don’t program the machine; we instruct it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, then, is the machine learning algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Gather a training dataset consisting of a collection of inputs to the model
    and the outputs we expect from the model for those inputs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the type of model we want to train.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model by presenting the training inputs and adjusting the model’s
    parameters when it gets the outputs wrong.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat step 3 until we are satisfied with the model’s performance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the now-trained model to produce outputs for new, unknown inputs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Most of machine learning follows this algorithm. Since we’re using known *labeled
    data* to train the model, this approach is called *supervised learning*: we supervise
    the model while it learns to produce correct output. In a sense, we punish the
    model until it gets it right. This is a dark enterprise, after all.'
  prefs: []
  type: TYPE_NORMAL
- en: We’re ready for an example, but let’s first summarize the story so far. We want
    a system where, for an unknown input, we get a meaningful output. To make the
    system, we train a machine learning model using a collection of inputs and their
    known outputs. Training conditions the model by modifying its parameters to minimize
    the mistakes it makes on the training data. When we’re satisfied with the model’s
    performance, we use the model with unknown inputs because we now believe the model
    when it gives us an output (at least, most of the time).
  prefs: []
  type: TYPE_NORMAL
- en: Our first example comes from a famous dataset consisting of measurements of
    the parts of iris flowers. This dataset is from the 1930s, indicating how long
    people have contemplated what we now call machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal is a model that, for an input collection of measurements, outputs
    the specific species of iris flower. The full dataset has four measurements for
    three iris species. We’ll keep it simple and use two measurements and two species:
    petal length and width in centimeters (cm) for *I. setosa* versus *I. versicolor*.
    Therefore, we want the model to accept two measurements as input and give us an
    output we can interpret as *I. setosa* or *I. versicolor*. *Binary models* like
    this decide between two possible outputs and are common in AI. If the model decides
    between more than two categories, it’s a *multiclass* model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have 100 samples in our dataset: 100 pairs of petal measurements, and the
    corresponding iris flower types. We’ll call *I. setosa* class 0 and *I. versicolor*
    class 1, where *class* labels the input categories.'
  prefs: []
  type: TYPE_NORMAL
- en: Models often want numeric class labels, which tells us that models don’t know
    what their inputs and outputs mean; they only make associations between sets of
    inputs and outputs. Models don’t “think” using any commonly accepted definition
    of the word. (The models of [Chapter 7](ch07.xhtml) might beg to differ, but more
    on that then.)
  prefs: []
  type: TYPE_NORMAL
- en: '****'
  prefs: []
  type: TYPE_NORMAL
- en: Here we must pause to introduce some critical terminology. I know, not what
    you want to read, but it’s essential to all that follows. Artificial intelligence
    makes frequent use of vectors and matrices (singular “matrix”). A *vector* is
    a string of numbers treated as a single entity. For example, the four measurements
    of each iris flower mean we can represent the flower as a string of four numbers,
    say, (4.5, 2.3, 1.3, 0.3). The flower described by this vector has a sepal length
    of 4.5 cm, sepal width of 2.3 cm, petal length of 1.3 cm, and petal width of 0.3
    cm. By grouping these measurements together, we can refer to them as a single
    entity.
  prefs: []
  type: TYPE_NORMAL
- en: 'The number of elements in a vector determines its dimensionality; for example,
    the iris dataset uses four-dimensional vectors, the four measurements of the flower.
    AI often works with inputs that have hundreds or even thousands of dimensions.
    If the input is an image, every pixel of that image is one dimension, meaning
    a small 28-pixel-square image becomes an input vector of 28×28, or 784 dimensions.
    The concept is the same in 3 dimensions or 33,000 dimensions: it remains a string
    of numbers treated as a single entity. But an image has rows and columns, making
    it a two-dimensional array of numbers, not a string. Two-dimensional arrays of
    numbers are *matrices*. In machine learning, we often represent datasets as matrices,
    where the rows are vectors representing the elements of the dataset, like an iris
    flower, and the columns are the measurements. For example, the first five flowers
    in the iris dataset form the following matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/math10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Each row is a flower. Notice that the first row matches the vector example.
    The remaining rows list the measurements for other flowers.
  prefs: []
  type: TYPE_NORMAL
- en: 'While you’re reading, keep these thoughts in the back of your mind:'
  prefs: []
  type: TYPE_NORMAL
- en: Vectors are strings of numbers often representing measurements in a dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matrices are two-dimensional arrays of numbers often representing datasets (stacks
    of vectors).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we continue our exploration of AI, the differences between vectors and matrices
    will come into focus. Now, let’s return to our story.
  prefs: []
  type: TYPE_NORMAL
- en: '****'
  prefs: []
  type: TYPE_NORMAL
- en: The inputs to a model are its [*features*](glossary.xhtml#glo42). Our iris flower
    dataset has two features, the petal’s length and width, which are grouped into
    [*feature vectors*](glossary.xhtml#glo43) (or *samples*). A single feature vector
    serves as the model’s input. A binary model’s output is typically a number relating
    to the model’s belief that the input belongs to class 1\. For our example, we’ll
    give the model a feature vector consisting of two features and expect an output
    that lets us decide whether we should call the input *I. versicolor*. If not,
    we declare the input to be *I. setosa* because we *assume* that inputs will always
    be one or the other.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning etiquette states that we should test our model; otherwise,
    how will we know it’s working? You might think it’s working when it gets all the
    training samples right, but experience has taught practitioners this isn’t always
    the case. The proper way to test a model is to keep some of the labeled training
    data to use after training. The model’s performance on this held-out test dataset
    better indicates how well the model has learned. We’ll use 80 labeled samples
    for training and keep 20 of them for testing, making sure that both the training
    and test sets contain an approximately even mix of both classes (flower types).
    This is also essential in practice, as far as possible. If we never show the model
    examples of a particular class of input, how can it learn to distinguish that
    class from others?
  prefs: []
  type: TYPE_NORMAL
- en: 'Using a held-out test set to judge the performance of a model isn’t just etiquette.
    It addresses a foundational issue in machine learning: generalization. Some machine
    learning models follow a process quite similar to a widely used approach known
    as *optimization*. Scientists and engineers use optimization to fit measured data
    to known functions; machine learning models also use optimization to condition
    their parameters, but the goal is different. Fitting data to a function, like
    a line, seeks to create the best possible *fit*, or the line that best explains
    the measured data. In machine learning, we instead want a model that learns the
    general characteristics of the training data to *generalize* to new data. That’s
    why we evaluate the model with the held-out test set. To the model, the test set
    contains new, unseen data it didn’t use to modify its parameters. The model’s
    performance on the test set is a clue to its generalization abilities.'
  prefs: []
  type: TYPE_NORMAL
- en: Our example has two input features, meaning the feature vectors are two-dimensional.
    Since we have two dimensions, we can opt to make a plot of the training dataset.
    (If we have two or three features in a feature vector, we can plot the feature
    vectors. However, most feature vectors have hundreds to thousands of features.
    I don’t know about you, but I can’t visualize a thousand-dimensional space.)
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 1-2](ch01.xhtml#ch01fig02) displays the two-dimensional iris training
    data; the *x*-axis is petal length, and the *y*-axis is petal width. The circles
    correspond to instances of *I. setosa* and the squares *I. versicolor*. Each circle
    or square represents a single training sample, the petal length and width for
    a specific flower. To place each point, find the petal length on the *x*-axis
    and the petal width on the *y*-axis. Then, move up from the *x*-axis and to the
    right from the *y*-axis. Where your fingers meet is the point representing that
    flower. If the flower is *I. setosa*, make the point a circle; otherwise, make
    it a square.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch01fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 1-2: The iris training data*'
  prefs: []
  type: TYPE_NORMAL
- en: The plot in [Figure 1-2](ch01.xhtml#ch01fig02) shows the *feature space* of
    the training set. In this case, we can visualize the training set directly, because
    we only have two features. When that’s not possible, all is not lost. Advanced
    algorithms exist that allow us to make plots like [Figure 1-2](ch01.xhtml#ch01fig02)
    where the points in two or three dimensions reflect the distribution of the samples
    in the much higher-dimensional space. Here, the word *space* means much the same
    as it does in everyday parlance.
  prefs: []
  type: TYPE_NORMAL
- en: Look carefully at [Figure 1-2](ch01.xhtml#ch01fig02). Does anything jump out
    at you? Are the different classes mixed or well separated? Every circle inhabits
    the lower-left corner of the plot, while all of the squares are in the upper right.
    There is no overlap between the classes, meaning they are entirely separate in
    the feature space.
  prefs: []
  type: TYPE_NORMAL
- en: How can we use this fact to make a [*classifier*](glossary.xhtml#glo15), a model
    that classifies iris flowers? (While [*model*](glossary.xhtml#glo69) is the more
    general term, as not all models place their inputs into categories, when they
    do, use the term *classifier*.)
  prefs: []
  type: TYPE_NORMAL
- en: We have many model types to choose from for our classifier, including [*decision
    trees*](glossary.xhtml#glo28), which generate a series of yes/no questions related
    to the features used to decide the class label to output for a given input. When
    the questions are laid out visually, they form a structure reminiscent of an upside-down
    tree. Think of a decision tree as a computer-generated version of the game *20
    Questions*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Even though we have two features, petal length and petal width, we can classify
    new iris flowers by asking a single question: is the petal length less than 2.5
    cm? If the answer is “yes,” then return class 0, *I. setosa*; otherwise, return
    class 1, *I. versicolor*. To classify the training data correctly, we need only
    the answer to this simple question.'
  prefs: []
  type: TYPE_NORMAL
- en: Did you catch what I did just now? I said that the question correctly classifies
    all the *training* data. What about the 20 test samples we didn’t use? Is our
    single-question classifier sufficient to give each of them the correct label?
    In practice, that’s what we want to know, and that is what we would report as
    the classifier’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 1-3](ch01.xhtml#ch01fig03) shows the training data again, along with
    the test data we didn’t use to make our single-question classifier. The solid
    circles and squares represent the test data.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch01fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 1-3: The iris training data with the held-out test data (solid)*'
  prefs: []
  type: TYPE_NORMAL
- en: None of the test data violates our rule; we still get correct class labels by
    asking if the petal length is less than 2.5 cm. Therefore, our model is perfect;
    it makes no mistakes. Congratulations, you just created your first machine learning
    model!
  prefs: []
  type: TYPE_NORMAL
- en: We should be happy, but not too happy. Let’s repeat this exercise, replacing
    *I. setosa* with the remaining iris species, *I. virginica*. This leads to [Figure
    1-4](ch01.xhtml#ch01fig04), where the triangles are instances of *I. virginica*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch01fig04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 1-4: The new iris training data*'
  prefs: []
  type: TYPE_NORMAL
- en: Hmm, things are not as clear-cut now. The obvious gap between the classes is
    gone, and they overlap.
  prefs: []
  type: TYPE_NORMAL
- en: I trained a decision tree using this new iris dataset. As before, there were
    80 samples for training and 20 held back for testing. This time, the model wasn’t
    perfect. It correctly labeled 18 of the 20 samples, for an accuracy of 9 out of
    10, or 90 percent. This roughly means that when this model assigns a flower to
    a particular class, there is a 90 percent chance it’s correct. The previous sentence,
    to be rigorous, needs careful clarification, but for now, you get the idea—machine
    learning models are not always perfect; they (quite frequently) make mistakes.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 1-5](ch01.xhtml#ch01fig05) shows the learned decision tree. Begin at
    the top, which is the root, and answer the question in that box. If the answer
    is “yes,” move to the box on the left; otherwise, move to the right. Keep answering
    and moving in this way until you arrive at a leaf: a box with no arrows. The label
    in this box is assigned to the input.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch01fig05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 1-5: The decision tree for* I. virginica *versus* I. versicolor'
  prefs: []
  type: TYPE_NORMAL
- en: The first decision tree classifier was trivial, as the answer to a single question
    was sufficient to decide class membership. The second decision tree classifier
    is more common. Most machine learning models are not particularly simple. Though
    their operation is comprehensible, understanding why they act as they do is an
    entirely different matter. Decision trees are among the few model types that readily
    explain themselves. For any input, the path traversed from root to leaf in [Figure
    1-5](ch01.xhtml#ch01fig05) explains in detail why the input received a particular
    label. The neural networks behind modern AI are not so transparent.
  prefs: []
  type: TYPE_NORMAL
- en: '****'
  prefs: []
  type: TYPE_NORMAL
- en: For a model to perform well “in the wild,” meaning when used in the real world,
    the data used to train the model must cover the entire range of inputs that the
    model might encounter. For example, say we want a model to identify pictures of
    dogs, and our training set contains images of only dogs and parrots. While the
    model performs well on our held-out test set, which also includes pictures of
    dogs and parrots, what might happen when we deploy the model and it comes across
    a picture of a wolf? Intuitively, we might expect the model to say “it’s a dog,”
    just as a small child might before they learn what a wolf is. This is precisely
    what most machine learning models would do.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate this, let’s try an experiment. A popular dataset used by all AI
    researchers consists of tens of thousands of small images containing handwritten
    digits, 0 through 9\. It goes by the uninspiring name of MNIST (Modified NIST)
    because it was derived in the late 1990s from a dataset constructed by the National
    Institute of Standards and Technology (NIST), the division of the United States
    Department of Commerce tasked with implementing all manner of standards for just
    about everything in the commercial and industrial realm.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 1-6](ch01.xhtml#ch01fig06) presents some typical MNIST digit samples.
    Our goal is to build a neural network that learns to identify the digits 0, 1,
    3, and 9\. We can train neural networks without knowing how they work because
    of powerful, open source toolkits like scikit-learn that are available to everyone.
    On the one hand, this democratizes AI; on the other, a little knowledge is often
    a dangerous thing. Models may appear to be good when they’re flawed in reality,
    and lack of knowledge about how the models work might prevent us from realizing
    that fact before it’s too late.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch01fig06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 1-6: Sample MNIST digits*'
  prefs: []
  type: TYPE_NORMAL
- en: After the classifier is trained, we’ll throw it a few curveballs by handing
    it images of fours and sevens—inputs the AI never saw during training. What might
    the model do with such inputs?
  prefs: []
  type: TYPE_NORMAL
- en: I trained the digits model using an open source toolkit. For now, all we need
    to know about the dataset is that the input feature vectors are unraveled digit
    images; the first row of pixels is followed by the second row, then the third
    row, and so on, until the entire image is unraveled into one long vector, a string
    of numbers. The digit images are 28×28 pixels, making the feature vector 784 numbers
    long. We’re asking the neural network to learn about things in a 784-dimensional
    space, rather than the simple 2-dimensional space we used previously, but machine
    learning is up to the challenge.
  prefs: []
  type: TYPE_NORMAL
- en: The training set used to condition the neural network model contained 24,745
    samples, roughly 6,000 of each digit type (0, 1, 3, and 9). This is likely enough
    to fairly represent the types of digits the model might encounter when used, but
    we need to try it to know. AI is a largely empirical science.
  prefs: []
  type: TYPE_NORMAL
- en: The held-out test set, also containing the digits 0, 1, 3, and 9, had 4,134
    samples (about 1,000 for each digit).
  prefs: []
  type: TYPE_NORMAL
- en: We’ll use a [*confusion matrix*](glossary.xhtml#glo19), a two-dimensional table
    of numbers, to evaluate the model. Confusion matrices are the most common way
    to evaluate a model because they show how it behaves on the test data.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the confusion matrix for our digit classifier is shown in [Table
    1-1](ch01.xhtml#ch01tab1).
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 1-1:** The Digit Classifier Confusion Matrix'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **0** | **1** | **3** | **9** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **0** | 978 | 0 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | 2 | 1,128 | 3 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| **3** | 5 | 0 | 997 | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| **9** | 5 | 1 | 8 | 995 |'
  prefs: []
  type: TYPE_TB
- en: The matrix rows represent the true labels for the samples given to the model.
    The columns are the model’s responses. The values in the table are counts, the
    number of times each possible combination of input class and model-assigned label
    happened.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the first row represents the zeros in the test set. Of those 980
    inputs, the model returned a label of zero for 978 of them, but it said the input
    was a three once and a nine another time. Therefore, when zero was the input,
    the model’s output was correct 978 out of 980 times. That’s encouraging.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, when the input was a one, the model returned the correct label 1,128
    times. It was right 997 times for threes and 995 times for nines. When a classifier
    is good, the numbers along the diagonal of the confusion matrix from upper left
    to lower right are high, and there are almost no numbers off that diagonal. Off-diagonal
    numbers are errors made by the model.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the digits model is 99 percent accurate. We have a solid, well-performing
    model—that is, if we can ensure that all inputs to the model are indeed a 0, 1,
    3, or 9\. But what if they aren’t?
  prefs: []
  type: TYPE_NORMAL
- en: 'I handed the model 982 fours. The model replied like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **0** | **1** | **3** | **9** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 48 | 9 | 8 | 917 |'
  prefs: []
  type: TYPE_TB
- en: In other words, the model returned a label of 9 for 917 of the 982 fours, a
    label of 1 for 48 fours, and labels of 1 or 3 for the rest. How about sevens?
  prefs: []
  type: TYPE_NORMAL
- en: '| **0** | **1** | **3** | **9** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 19 | 20 | 227 | 762 |'
  prefs: []
  type: TYPE_TB
- en: The model still favored calling sevens nines, but it often called them threes
    as well. Neural networks are loath to give up their secrets when explaining their
    actions, but in this case, of the 227 sevens labeled as threes, 47 of them were
    European-style sevens with a slash. A random sample of 227 sevens from the entire
    dataset turned up only 24 European-style sevens. The comparison isn’t rigorous
    mathematically, but it hints that sevens with a slash are often close enough to
    a three to fool the model.
  prefs: []
  type: TYPE_NORMAL
- en: The model was never taught to recognize fours or sevens, so it did the next
    best thing and placed them in a nearby category. Depending on how they’re written,
    people might sometimes confuse fours and sevens for nines, for example. The model
    is making the kind of mistakes people make, which is interesting—but, more significantly,
    the model is poor because it wasn’t trained on the full range of inputs it might
    encounter. It has no way of saying “I don’t know,” and getting a model to reliably
    say this can be tricky.
  prefs: []
  type: TYPE_NORMAL
- en: This is a simple exercise, but the implications are profound. Instead of digits,
    what if the model was looking for cancer in medical images but was never trained
    to recognize an important category of lesion or all the forms that lesion might
    take? A properly constructed and comprehensive dataset might mean the difference
    between life and death.
  prefs: []
  type: TYPE_NORMAL
- en: '****'
  prefs: []
  type: TYPE_NORMAL
- en: We can also think about the digits example in terms of interpolation and extrapolation.
    *Interpolation* approximates within the range of known data, and *extrapolation*
    goes beyond known data.
  prefs: []
  type: TYPE_NORMAL
- en: For the digits example, interpolation might refer to encountering a tilted zero
    in the wild when none of the zeros in the training set were particularly tilted.
    The model must interpolate, in a sense, to respond correctly. Extrapolation is
    more like classifying a zero with a slash through it, which is something unseen
    during training time. To better understand these terms, let’s model the world
    population from 1950 through 2020.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we’ll fit a line to the data from 1950 through 1970\. Fitting a line
    is a form of curve fitting; think of it as machine learning’s less sophisticated
    cousin. To fit a line, find two numbers: the slope and the intercept. The slope
    tells us how steep the line is. If the slope is positive, the line is increasing
    as we move from left to right along the *x*-axis of a graph. A negative slope
    means the line decreases as we move along the *x*-axis. The intercept is where
    the line intersects the *y*-axis; that is, the value when the input is zero.'
  prefs: []
  type: TYPE_NORMAL
- en: To fit a line, we use an algorithm to find the slope and intercept that best
    characterize the data (here, world population from 1950 through 1970). [Figure
    1-7](ch01.xhtml#ch01fig07) shows a plot of the line and the actual populations
    by year, denoted by plus signs. The line passes through or near to most of the
    plus signs, so the fit is reasonable. Notice that the population is in billions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch01fig07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 1-7: World population from 1950 through 1970*'
  prefs: []
  type: TYPE_NORMAL
- en: Once we have the line, we can use the slope and intercept to estimate the population
    for any year. Estimating for years between 1950 and 1970 is interpolating, because
    we used data from that range of years to create the line. If we estimate populations
    for years before 1950 or after 1970, we are extrapolating. [Table 1-2](ch01.xhtml#ch01tab2)
    shows our results when interpolating.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 1-2:** Interpolating the Population Between 1950 and 1970'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Year** | **Interpolated** | **Actual** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1954 | 2.71 | 2.72 |'
  prefs: []
  type: TYPE_TB
- en: '| 1960 | 3.06 | 3.03 |'
  prefs: []
  type: TYPE_TB
- en: '| 1966 | 3.41 | 3.41 |'
  prefs: []
  type: TYPE_TB
- en: The interpolated population values are quite close to the actual population
    values, meaning the model (here the line fit to the data) is doing well. Now,
    let’s extrapolate to dates outside the fit range, as shown in [Table 1-3](ch01.xhtml#ch01tab3).
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 1-3:** Extrapolating the Population After 1970'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Year** | **Extrapolated** | **Actual** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1995 | 5.10 | 5.74 |'
  prefs: []
  type: TYPE_TB
- en: '| 2010 | 5.98 | 6.96 |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | 6.56 | 7.79 |'
  prefs: []
  type: TYPE_TB
- en: The difference between the extrapolated population values and the actual population
    is increasing with each year. The model isn’t doing well. Plotting the full range
    from 1950 through 2020 reveals the problem; see [Figure 1-8](ch01.xhtml#ch01fig08).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch01fig08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 1-8: World population from 1950 through 2020*'
  prefs: []
  type: TYPE_NORMAL
- en: As time goes by, the fit line becomes increasingly wrong because the data is
    not linear after all. That is, the rate of growth is not constant and doesn’t
    follow a straight line.
  prefs: []
  type: TYPE_NORMAL
- en: When extrapolating, we might have reason to believe that the data will continue
    to fit the line; if that’s a valid assumption, then the line will continue to
    be a good fit. However, in the real world, we usually have no such assurance.
    So, as a slogan, we might say interpolation good, extrapolation bad.
  prefs: []
  type: TYPE_NORMAL
- en: Fitting a line to some data is an example of *curve fitting*. What is true for
    curve fitting is also true for AI. The handwritten digits model did well when
    given inputs close to the data it was trained to recognize. The digits in the
    test data were all instances of 0, 1, 3, and 9, so the test data was like the
    training data. The two datasets are from the same *distribution*, and the same
    data-generating process created both. We can therefore claim that the model was,
    in a way, interpolating in those cases. However, when we forced the model to make
    decisions about fours and sevens, we were extrapolating by having the model make
    decisions about data it never saw during training.
  prefs: []
  type: TYPE_NORMAL
- en: 'It bears repeating: interpolation good, extrapolation bad. Bad datasets lead
    to bad models; good datasets lead to good models, which behave badly when forced
    to extrapolate. And, for good measure: all models are wrong, but some are useful.'
  prefs: []
  type: TYPE_NORMAL
- en: '****'
  prefs: []
  type: TYPE_NORMAL
- en: Along the same lines of Hilaire Belloc’s 1907 book *Cautionary Tales for Children*—an
    amusing and somewhat horrifying look at foolish things children do that could
    lead to an unfortunate end—let’s examine some cautionary tales that AI practitioners
    should be aware of when training, testing, and, most of all, deploying models.
  prefs: []
  type: TYPE_NORMAL
- en: In 2016, I attended a conference talk where the presenter demonstrated research
    into understanding why a neural network chooses the way it does. This is not yet
    a solved problem, but progress has been made. In this case, the research marked
    parts of the input images that influenced the model’s decision.
  prefs: []
  type: TYPE_NORMAL
- en: The speaker displayed pictures of huskies and wolves and discussed his classifier
    for differentiating between the two. He showed how well it performed on a test
    set and asked the audience of machine learning researchers if this was a good
    model. Many people said yes, but with hesitation because they expected a trap.
    They were right to be hesitant. The speaker then marked the images to show the
    parts that the neural network focused on when making its decisions. The model
    wasn’t paying attention to the dogs or the wolves. Instead, the model noticed
    that all the wolf training images had snow in the background, while none of the
    dog images contained snow. The model learned nothing about dogs and wolves but
    only about snow and no snow. Careless acceptance of the model’s behavior wouldn’t
    have revealed that fact, and the model might have been deployed only to fail in
    the wild.
  prefs: []
  type: TYPE_NORMAL
- en: A similar tale is told of a very early machine learning system from the 1950s
    or 1960s. This one is likely apocryphal, though I have read a paper from that
    period that might be the origin of the urban legend. In this case, the images
    were bird’s-eye views of forests. Some images contained a tank, while others did
    not.
  prefs: []
  type: TYPE_NORMAL
- en: A model trained to detect tanks seemed to work well on the training data but
    failed miserably when set loose in the wild. It was eventually realized that one
    set of training images had been taken on a sunny day and the other on a cloudy
    day. The model had learned nothing that its creators assumed it had.
  prefs: []
  type: TYPE_NORMAL
- en: More recent examples of this phenomenon exist with more advanced machine learning
    models. Some have even fooled experts into believing the model had learned something
    fundamental about language or the like when, instead, it had learned extremely
    subtle correlations in the training data that no human could (easily) detect.
  prefs: []
  type: TYPE_NORMAL
- en: The word *correlation* has a strict mathematical meaning, but we capture its
    essence with the phrase “correlation does not imply causation.” Correlation is
    when two things are linked so that the occurrence of one implies the occurrence
    of the other, often in a particular order. More concretely, correlation measures
    how strongly a change in one thing is associated with a change in another. If
    both increase, they are positively correlated. If one increases while the other
    decreases, they are negatively correlated.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, a rooster crows, and the sun comes up. The two events are time-dependent:
    the rooster first, then the sun. This correlation does not imply causation, as
    the rooster crowing doesn’t cause the sun to rise, but if such a correlation is
    observed often enough, the human mind begins to see one as causing the other,
    even when there is no real evidence of this. Why humans act this way isn’t hard
    to understand. Evolution favored early humans who made such associations because,
    sometimes, the associations led to behavior beneficial for survival.'
  prefs: []
  type: TYPE_NORMAL
- en: “Correlation does not imply causation” also applies to AI. The aforementioned
    models learned to detect things in the training data that correlated with the
    intended targets (dogs, wolves, tanks) but didn’t learn about the targets themselves.
    Savvy machine learning practitioners are always on the lookout for such spurious
    correlations. Using a large and highly diverse dataset for training and testing
    can defend against this effect, though this isn’t always possible in practice.
  prefs: []
  type: TYPE_NORMAL
- en: We must ask whether our models have learned what we assume they have. And, as
    we saw with the MNIST digits, we must ensure that our models have seen all the
    kinds of inputs they will encounter in the wild—they should interpolate, not extrapolate.
  prefs: []
  type: TYPE_NORMAL
- en: This matters more than it might initially appear. Google learned this lesson
    in 2015 when it deployed a feature for Google Photos, wherein the model was insufficiently
    trained on human faces and made incorrect and inappropriate associations. Bias,
    in both the generic and social senses, is a real issue in AI.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s perform another experiment with MNIST digits. This time, the model has
    a seemingly simple decision to make: is the input digit a nine? The model is the
    same neural network used previously. If trained on a dataset where every image
    is either a nine or any other digit except four or seven (that is, no fours or
    sevens are in the training data), then the model is 99 percent accurate, as the
    confusion matrix shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Not 9** | **9** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Not 9** |   9,754 | 23 |'
  prefs: []
  type: TYPE_TB
- en: '|         **9** |   38 | 1,362 |'
  prefs: []
  type: TYPE_TB
- en: The confusion matrix tells us that the model correctly labeled 9,754 out of
    9,777 test images that were not a nine. The model’s label was also correct for
    1,362 of the 1,400 nines. While the model performs well on the test set, the set
    does not contain fours or sevens.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the confusion matrix is small because the model has only two
    classes: nine or not nine. In other words, this is a binary model.'
  prefs: []
  type: TYPE_NORMAL
- en: The 23 in the upper-right corner of the matrix represents 23 times when the
    input wasn’t a nine, but the model said it was. For a binary model, class 1 is
    usually considered the class of interest, or the positive class. Therefore, these
    23 inputs represent [*false positives*](glossary.xhtml#glo41), because the model
    said “it’s a nine” when it wasn’t. Similarly, the 38 samples at the lower left
    are [*false negatives*](glossary.xhtml#glo40) because the model said “it’s not
    a nine” when the input actually was. We want models with no false positives or
    negatives, but sometimes it’s more important to minimize one than the other.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if a model is to detect breast cancer in mammograms, a false positive
    represents a case where the model says, “it might be cancer,” even though it isn’t.
    That’s scary to hear, but further testing will show that the model was wrong.
    However, a false negative represents a missed cancer. We might tolerate a model
    with more false positives if it also has virtually no false negatives, as a false
    positive is less catastrophic than a false negative. We’re beginning to appreciate
    how important it is to fully train, characterize, test, and *understand* our machine
    learning models.
  prefs: []
  type: TYPE_NORMAL
- en: '****'
  prefs: []
  type: TYPE_NORMAL
- en: 'All right, back to our experiment. The “is it a nine” classifier, like our
    earlier MNIST model, knows nothing about fours or sevens. When shown fours and
    sevens, the MNIST model typically called them nines. Will this model do the same?
    Here’s what I received when I gave the model fours and sevens:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Not 9** | **9** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Not 9** |   5,014 | 9,103 |'
  prefs: []
  type: TYPE_TB
- en: The model marked 9,103 of the 14,117 fours and sevens as nines. That’s slightly
    more than 65 percent, or roughly two out of every three. This mimics the case
    where we present the model with inputs of a type it was never trained to detect.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s help the model by adding fours and sevens to the training set. Hopefully,
    providing examples that say, “It looks like a nine, but it isn’t,” formally known
    as *hard negatives*, will improve the model. I made 3 percent of the training
    data fours and sevens. The overall model was just as accurate as before, 99 percent,
    and here’s what happened when I gave it fours and sevens it had never seen before:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Not 9** | **9** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Not 9** |   9,385 | 3,321 |'
  prefs: []
  type: TYPE_TB
- en: That’s better. Instead of calling two-thirds of four or seven inputs a nine,
    the model labeled only one in four as a nine. Even a few examples of things that
    look like the positive class but aren’t can help. If I boost the proportion of
    fours and sevens in the training set to 18 percent, the model misclassifies fours
    and sevens less than 1 percent of the time. Because models learn from data, we
    *must* use datasets that are as complete as possible so our models interpolate
    and do not extrapolate.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*To be completely accurate, recent research shows that modern deep learning
    models are almost always extrapolating, but the more similar the inputs are to
    the data on which the model was trained, the better the performance, so I feel
    justified in using the analogy.*'
  prefs: []
  type: TYPE_NORMAL
- en: Everyone who seeks to understand, let alone work with, AI must take the warnings
    about the quality of the data used to train AI models to heart. A 2021 research
    article published in the journal *Nature Machine Intelligence* by Michael Roberts
    et al., “Common Pitfalls and Recommendations for Using Machine Learning to Detect
    and Prognosticate for COVID-19 Using Chest Radiographs and CT Scans,” is a sobering
    example. The authors assessed the performance of machine learning models designed
    to detect COVID-19 in chest X-rays and CT scans, reducing the initial candidate
    pool of over 2,000 studies (models) to 62 for rigorous testing. In the end, the
    authors declared *none* of the models fit for clinical use because of flaws in
    construction, bias in the datasets, or both.
  prefs: []
  type: TYPE_NORMAL
- en: Results like these have led to the creation of [*explainable AI*](glossary.xhtml#glo39),
    a subfield that seeks to give models the ability to explain themselves.
  prefs: []
  type: TYPE_NORMAL
- en: Look at your data and try to understand, as far as humanly possible, what your
    model is doing and *why*.
  prefs: []
  type: TYPE_NORMAL
- en: '****'
  prefs: []
  type: TYPE_NORMAL
- en: This chapter’s title, “And Away We Go,” was comedian Jackie Gleason’s tagline.
    It’s often good to dive into a subject to get an overview before coming back to
    understand things at a deeper level. In other words, we rush in to get a feel
    for the topic before exploring more methodically.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll find the many new terms and concepts introduced in this chapter in the
    glossary at the end of the book. My goal isn’t for you to understand them all
    now, let alone retain them, but to plant seeds so that the next time you encounter
    one of these terms or concepts, you’ll be more likely to think, “Ah, I know that
    one.” Later chapters reinforce them, and you’ll learn the important ones via repeated
    exposure.
  prefs: []
  type: TYPE_NORMAL
- en: There are two categories of takeaways from this chapter. The first has to do
    with what AI is and its essential pieces. The second is about building intuition
    about what AI offers and how we should respond.
  prefs: []
  type: TYPE_NORMAL
- en: 'AI involves models, as yet nebulous entities we can condition with data to
    perform some desired task. There are many types of AI models, and this chapter
    introduced two: decision trees and neural networks. I won’t say much more about
    decision trees, but neural networks occupy most of the remainder of the book.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Models are often best thought of as functions, like the mathematical functions
    you may remember from school or the functions that form the core of most computer
    programs. Both can be considered black boxes, where something goes in (the input)
    and something comes out (the output). In AI, the input is a feature vector, a
    collection of whatever is appropriate for the task at hand. In this chapter, we
    used two feature vectors: measurements of a flower and images of a handwritten
    digit.'
  prefs: []
  type: TYPE_NORMAL
- en: Training conditions the model by altering its parameters to make it as accurate
    as possible. It’s necessary to exercise caution when training most models to learn
    the general features of the data and not spurious correlations or the minute details
    of the training set (a concept known as [*overfitting*](glossary.xhtml#glo79),
    which we’ll discuss in [Chapter 4](ch04.xhtml)).
  prefs: []
  type: TYPE_NORMAL
- en: Proper development of machine learning models means we must have a test set,
    a collection of known input and output pairs that we do not use when training.
    We use this set after training to evaluate the model. If the dataset is constructed
    correctly, the test set provides an idea of how well we can expect the model to
    perform in the wild.
  prefs: []
  type: TYPE_NORMAL
- en: The second takeaway relates to what AI offers and how we should respond to it.
    While AI is powerful, it doesn’t think as we do (though the models of [Chapter
    7](ch07.xhtml) might disagree). AI lives and dies by data and is only as good
    as the data we feed to it. If the dataset is biased, the AI is biased. If the
    dataset neglects to include examples of the types of inputs it will encounter
    when used, the AI will fail to handle such inputs properly.
  prefs: []
  type: TYPE_NORMAL
- en: The chapter’s examples warn us to be careful when assuming AI operates as intended.
    Did the model learn what we wanted it to learn? Was it influenced by correlations
    in the data that we didn’t notice or, worse still, that we are too limited to
    discern? Think back to the huskies versus wolves example.
  prefs: []
  type: TYPE_NORMAL
- en: Because AI is only as good as the data fed to it, it’s on us to make datasets
    fair and unbiased and to understand what the AI has truly learned without assumptions.
  prefs: []
  type: TYPE_NORMAL
- en: AI first appeared in the 1950s, so why is it now suddenly everywhere we look?
    The next chapter answers this question.
  prefs: []
  type: TYPE_NORMAL
- en: '**KEY TERMS**'
  prefs: []
  type: TYPE_NORMAL
- en: algorithm, artificial intelligence, classifier, class label, confusion matrix,
    dataset, decision tree, deep learning, explainable AI, feature, feature vector,
    machine learning, model, neural network, parameters, testing, training
  prefs: []
  type: TYPE_NORMAL
