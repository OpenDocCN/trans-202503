<html><head></head><body><div id="sbo-rt-content"><section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" title="211" id="Page_211"/>12</span><br/>
<span class="ChapterTitle">Rust Without the Standard Library</span></h1>
</header>
<figure class="opener">
<img src="Images/chapterart.png" alt="" width="206" height="206"/>
</figure>
<p class="ChapterIntro">Rust is intended to be a language for systems programming, but it isn’t always clear what that really means. At the very least, a systems programming language is usually expected to allow the programmer to write programs that do not rely on the operating system and can run directly on the hardware, whether that is a thousand-core supercomputer or an embedded device with a single-core ARM processor with a clock speed of 72MHz and 256KiB of memory.</p>
<p>In this chapter, we’ll take a look at how you can use Rust in unorthodox environments, such as those without an operating system, or those that don’t even have the ability to dynamically allocate memory! Much of our discussion will focus on the <code>#![no_std]</code> attribute, but we’ll also investigate <span epub:type="pagebreak" title="212" id="Page_212"/>Rust’s <code>alloc</code> module, the Rust runtime (yes, Rust does technically have a runtime), and some of the tricks you have to play to write up a Rust binary for use in such an environment.</p>
<h2 id="h1--0001">Opting Out of the Standard Library</h2>
<p class="BodyFirst">As a language, Rust consists of multiple independent pieces. First there’s the compiler, which dictates the grammar of the Rust language and implements type checking, borrow checking, and the final conversion into machine-runnable code. Then there’s the standard library, <code>std</code>, which implements all the useful common functionality that most programs need—things like file and network access, a notion of time, facilities for printing and reading user input, and so on. But <code>std</code> itself is also a composite, building on top of two other, more fundamental libraries called <code>core</code> and <code>alloc</code>. In fact, many of the types and functions in <code>std</code> are just re-exports from those two libraries.</p>
<p>The <code>core</code> library sits at the bottom of the standard library pyramid and contains any functionality that depends on nothing but the Rust language itself and the hardware the resulting program is running on—things like sorting algorithms, marker types, fundamental types such as <code>Option</code> and <code>Result</code>, low-level operations such as atomic memory access methods, and compiler hints. The <code>core</code> library works as if the operating system does not exist, so there is no standard input, no filesystem, and no network. Similarly, there is no memory allocator, so types like <code>Box</code>, <code>Vec</code>, and <code>HashMap</code> are nowhere to be seen.</p>
<p>Above <code>core</code> sits <code>alloc</code>, which holds all the functionality that depends on dynamic memory allocation, such as collections, smart pointers, and dynamically allocated strings (<code>String</code>). We’ll get back to <code>alloc</code> in the next section.</p>
<p>Most of the time, because <code>std</code> re-exports everything in <code>core</code> and <code>alloc</code>, developers do not need to know about the differences among the three libraries. This means that even though <code>Option</code> technically lives in <code>core::option::Option</code>, you can access it through <code>std::option::Option</code>.</p>
<p>However, in an unorthodox environment, such as on an embedded device where there is no operating system, the distinction is crucial. While it’s fine to use an <code>Iterator</code> or to sort a list of numbers, an embedded device may simply have no meaningful way to access a file (as that requires a filesystem) or print to the terminal (as that requires a terminal)—so there’s no <code>File</code> or <code>println!</code>. Furthermore, the device may have so little memory that dynamic memory allocation is a luxury you can’t afford, and thus anything that allocates memory on the fly is a no-go—say goodbye to <code>Box</code> and <code>Vec</code>.</p>
<p>Rather than force developers to carefully avoid those basic constructs in such environments, Rust provides a way to opt out of anything but the core functionality of the language: the <code>#![no_std]</code> attribute. This is a crate-level attribute (<code>#!</code>) that switches the prelude (see the box on <span class="xref" itemid="xref_target_page 213">page 213</span>) for the crate from <code>std::prelude</code> to <code>core::prelude</code> so that you don’t accidentally depend on anything outside of <code>core</code> that might not work in your target environment.</p>
<p><span epub:type="pagebreak" title="213" id="Page_213"/>However, that is <em>all</em> the <code>#![no_std]</code> attribute does—it does not prevent you from bringing in the standard library explicitly with <code>extern std</code>. This may be surprising, as it means a crate marked <code>#![no_std]</code> may in fact not be compatible with a target environment that does not support <code>std</code>, but this design decision was intentional: it allows you to mark your crate as being <code>no_std</code>-compatible but to still use features from the standard library when certain features are enabled. For example, many crates have a feature named <code>std</code> that, when enabled, gives access to more sophisticated APIs and integrations with types that live in <code>std</code>. This allows crate authors to both supply the core implementation for constrained use cases and add bells and whistles for consumers on more standard platforms.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p>	Since features should be additive, prefer an <code>std</code>-enabling feature to an <code>std</code>-disabling one. Otherwise, if <em>any</em> crate in a consumer’s dependency graph enables the no-<code>std</code> feature, <em>all</em> consumers will be given access only to the bare-bones API without <code>std</code> support, which may then mean that APIs they depend on aren’t available, causing them to no longer compile.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="box">
<h2>The Prelude</h2>
<p class="BoxBodyFirst">Have you ever wondered why there are some types and traits—like <code>Box</code>, <code>Iterator</code>, <code>Option</code>, and <code>Clone</code>—that are available in every Rust file without you needing to <code>use</code> them? Or why you don’t need to <code>use</code> any of the macros in the standard library (like <code>vec![]</code>)? The reason is that every Rust module automatically imports the Rust standard prelude<em> </em>with<em> </em>an implicit <code>use std::prelude::rust_2021::*</code> (or similar for other editions), which brings all the exports from the crate’s chosen edition’s prelude into scope. The prelude modules themselves aren’t special beyond this auto-inclusion—they are merely collections of <code>pub use</code> statements for key types, traits, and macros that the Rust developers expect to be commonly used.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<h2 id="h1--0002">Dynamic Memory Allocation</h2>
<p class="BodyFirst">As we discussed in <span class="xref" itemid="xref_target_Chapter 1">Chapter 1</span>, a machine has many different regions of memory, and each one serves a distinct purpose. There’s static memory for your program code and static variables, there’s the stack for function-local variables and function arguments, and there’s the heap for, well, everything else. The heap supports allocating variably sized regions of memory at runtime, and those allocations stick around for however long you want them to. This makes heap memory extremely versatile, and as a result, you find it used everywhere. <code>Vec</code>, <code>String</code>, <code>Arc</code> and <code>Rc</code>, and the collection types are all implemented in heap memory, which allows them to grow and shrink over time and to be returned from functions without the borrow checker complaining.</p>
<p><span epub:type="pagebreak" title="214" id="Page_214"/>Behind the scenes, the heap is really just a huge chunk of contiguous memory that is managed by an <em>allocator</em>. It’s the allocator that provides the illusion of distinct allocations in the heap, ensuring that those allocations do not overlap and that regions of memory that are no longer in use are reused. By default Rust uses the system allocator, which is generally the one dictated by the standard C library. This works well for most use cases, but if necessary, you can override which allocator Rust will use through the <code>GlobalAlloc</code> trait combined with the <code>#[global_allocator]</code> attribute, which requires an implementation of an <code>alloc</code> method for allocating a new segment of memory and <code>dealloc</code> for returning a past allocation to the allocator to reuse.</p>
<p>In environments without an operating system, the standard C library is also generally not available, and so neither is the standard system allocator. For that reason, <code>#![no_std]</code> also excludes all types that rely on dynamic memory allocation. But since it’s entirely possible to implement a memory allocator without access to a full-blown operating system, Rust allows you to opt back into just the part of the Rust standard library that requires an allocator without opting into all of <code>std</code> through the <code>alloc</code> crate. The <code>alloc</code> crate comes with the standard Rust toolchain (just like <code>core</code> and <code>std</code>) and contains most of your favorite heap-allocation types, like <code>Box</code>, <code>Arc</code>, <code>String</code>, <code>Vec</code>, and <code>BTreeMap</code>. <code>HashMap</code> is not among them, since it relies on random number generation for its key hashing, which is an operating system facility. To use types from <code>alloc</code> in a <code>no_std</code> context, all you have to do is replace any imports of those types that previously had <code>use std::</code> with <code>use alloc::</code> instead. Do keep in mind, though, that depending on <code>alloc</code> means your <code>#![no_std]</code> crate will no longer be usable by any program that disallows dynamic memory allocation, either because it doesn’t have an allocator or because it has too little memory to permit dynamic memory allocation in the first place.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p>	Some programming domains, like the Linux kernel, may allow dynamic memory allocation only if out-of-memory errors are handled gracefully (that is, without panicking). For such use cases, you’ll want to provide <code>try_</code> versions of any methods you expose that might allocate. The <code>try_</code> methods should use fallible methods of any inner types (like the currently unstable <code>Box::try_new</code> or <code>Vec::try_reserve</code>) rather than ones that just panic (like <code>Box::new</code> or <code>Vec::reserve</code>) and propagate those errors out to the caller, who can then handle them appropriately.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>It might strike you as odd that it’s possible to write nontrivial crates that use <em>only</em> <code>core</code>. After all, they can’t use collections, the <code>String</code> type, the network, or the filesystem, and they don’t even have a notion of time! The trick to <code>core</code>-only crates is to utilize the stack and static allocations. For example, for a heapless vector, you allocate enough memory up front—either in static memory or in a function’s stack frame—for the largest number of elements you expect the vector to be able to hold, and then augment it with a <code>usize</code> that tracks how many elements it currently holds. To push to the vector, you write to the next element in the (statically sized) array and increment a variable that tracks the number of elements. If the vector’s length ever reaches the static size, the next push fails. <a href="#listing12-1" id="listinganchor12-1">Listing 12-1</a> gives an example of such a heapless vector type implemented using <code>const</code> generics.</p>
<pre><code><span epub:type="pagebreak" title="215" id="Page_215"/>struct ArrayVec&lt;T, const N: usize&gt; {
    values: [Option&lt;T&gt;; N],
    len: usize,
}
impl&lt;T, const N: usize&gt; ArrayVec&lt;T, N&gt; {
    fn try_push(&amp;mut self, t: T) -&gt; Result&lt;(), T&gt; {
        if self.len == N {
            return Err(t);
        }
        self.values[self.len] = Some(t);
        self.len += 1;
        return Ok(());
    }
}</code></pre>
<p class="CodeListingCaption"><a id="listing12-1">Listing 12-1</a>: A heapless vector type</p>
<p>We make <code>ArrayVec</code> generic over both the type of its elements, <code>T</code>, and the maximum number of elements, <code>N</code>, and then represent the vector as an array of <code>N</code> <em>optional</em> <code>T</code>s. This structure always stores <code>N</code> <code>Option&lt;T&gt;</code>, so it has a size known at compile time and can be stored on the stack, but it can still act like a vector by using runtime information to inform how we access the array.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p>	We could have implemented <code>ArrayVec</code> using <code>[MaybeUninit&lt;T&gt;; N]</code> to avoid the overhead of the <code>Option</code>, but that would require using unsafe code, which isn’t warranted for this example.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<h2 id="h1--0003">The Rust Runtime</h2>
<p class="BodyFirst">You may have heard the claim that Rust doesn’t have a runtime. While that’s true at a high level—it doesn’t have a garbage collector, an interpreter, or a built-in user-level scheduler—it’s not really true in the strictest sense. Specifically, Rust does have some special code that runs before your <code>main</code> function and in response to certain special conditions in your code, which really is a form of bare-bones runtime.</p>
<h3 id="h2--0001">The Panic Handler</h3>
<p class="BodyFirst">The first bit of such special code is Rust’s <em>panic handler</em>. When Rust code panics by invoking <code>panic!</code> or <code>panic_any</code>, the panic handler dictates what happens next. When the Rust runtime is available—as is the case on most targets that supply <code>std</code>—the panic handler first invokes the <em>panic hook</em> set via <code>std::panic::set_hook</code>, which prints a message and optionally a backtrace to standard error by default. It then either unwinds the current thread’s stack or aborts the process, depending on the panic setting chosen for current compilation (either through Cargo configuration or arguments passed directly to <code>rustc</code>).</p>
<p>However, not all targets provide a panic handler. For example, most embedded targets do not, as there isn’t necessarily a single implementation that makes sense across all the uses for such a target. For targets that don’t <span epub:type="pagebreak" title="216" id="Page_216"/>supply a panic handler, Rust still needs to know what to do when a panic occurs. To that end, we can use the <code>#[panic_handler]</code> attribute to decorate a single function in the program with the signature <code>fn(&amp;PanicInfo) -&gt; !</code>. That function is called whenever the program invokes a panic, and it is passed information about the panic in the form of a <code>core::panic::PanicInfo</code>. What the function does with that information is entirely unspecified, but it can never return (as indicated by the <code>!</code> return type). This is important, since the Rust compiler assumes that no code that follows a panic is run.</p>
<p>There are many valid ways for a panic handler to avoid returning. The standard panic handler unwinds the thread’s stack and then terminates the thread, but a panic handler can also halt the thread using <code>loop {}</code>, abort the program, or do anything else that makes sense for the target platform, even as far as resetting the device.</p>
<h3 id="h2--0002">Program Initialization</h3>
<p class="BodyFirst">Contrary to popular belief, the <code>main</code> function is not the first thing that runs in a Rust program. Instead, the <code>main</code> symbol in a Rust binary actually points to a function in the standard library called <code>lang_start</code>. That function performs the (fairly minimal) setup for the Rust runtime, including stashing the program’s command-line arguments in a place where <code>std::env::args</code> can get to them, setting the name of the main thread, handling panics in the <code>main</code> function, flushing standard output on program exit, and setting up signal handlers. The <code>lang_start</code> function in turn calls the <code>main</code> function defined in your crate, which then doesn’t need to think about how, for example, Windows and Linux differ in how command-line arguments are passed in.</p>
<p>This arrangement works well on platforms where all of that setup is sensible and supported, but it presents a problem on embedded platforms where main memory may not even be accessible when the program starts. On such platforms, you’ll generally want to opt out of the Rust initialization code entirely using the <code>#![no_main]</code> crate-level attribute. This attribute completely omits <code>lang_start</code>, meaning you as the developer must figure out how the program should be started, such as by declaring a function with <code>#[export_name = "main"]</code> that matches the expected launch sequence for the target platform.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p>	On platforms that truly run no code before they jump to the defined start symbol, like most embedded devices, the initial values of static variables may not even match what’s specified in the source code. In such cases, your initialization function will need to explicitly initialize the various static memory segments with the initial data values specified in your program binary.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<h3 id="h2--0003">The Out-of-Memory Handler</h3>
<p class="BodyFirst">If you write a program that wishes to use <code>alloc</code> but is built for a platform that does not supply an allocator, you must dictate which allocator to use using the <code>#[global_allocator]</code> attribute mentioned earlier in the chapter. But you also have to specify what happens if that global allocator fails <span epub:type="pagebreak" title="217" id="Page_217"/>to allocate memory. Specifically, you need to define an <em>out-of-memory handler </em>to say what should happen if an infallible operation like <code>Vec::push</code> needs to allocate more memory, but the allocator cannot supply it.</p>
<p>The default behavior of the out-of-memory handler on <code>std</code>-enabled platforms is to print an error message to standard error and then abort the process. However, on a platform that, for example, doesn’t have standard error, that obviously won’t work. At the time of writing, on such platforms your program must explicitly define an out-of-memory handler using the unstable attribute <code>#[lang = "oom"]</code>. Keep in mind that the handler should almost certainly prevent future execution, as otherwise the code that tried to allocate will continue executing without knowing that it did not receive the memory it asked for!</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p>	By the time you read this, the out-of-memory handler may already have been stabilized under a permanent name (<code>#[alloc_error_handler]</code>, most likely). Work is also underway to give the default <code>std</code> out-of-memory handler the same kind of “hook” functionality as Rust’s panic handler, so that code can change the out-of-memory behavior on the fly through a method like <code>set_alloc_error_hook</code>.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<h2 id="h1--0004">Low-Level Memory Accesses</h2>
<p class="BodyFirst">In <span class="xref" itemid="xref_target_Chapter 10">Chapter 10</span>, we discussed the fact that the compiler is given a fair amount of leeway in how it turns your program statements into machine instructions, and that the CPU is allowed some wiggle room to execute instructions out of order. Normally, the shortcuts and optimizations that the compiler and CPU can take advantage of are invisible to the semantics of the program—you can’t generally tell whether, say, two reads have been reordered relative to each other or whether two reads from the same memory location actually result in two CPU load instructions. This is by design. The language and hardware designers carefully specified what semantics programmers commonly expect from their code when it runs so that your code generally does what you expect it to.</p>
<p>However, <code>no_std</code> programming sometimes takes you beyond the usual border of “invisible optimizations.” In particular, you’ll often communicate with hardware devices through <em>memory mapping</em>, where the internal state of the device is made available in carefully chosen regions in memory. For example, while your computer starts up, the memory address range <code>0xA0000</code>–<code>0xBFFFF</code> maps to a crude graphics rendering pipeline; writes to individual bytes in that range will change particular pixels (or blocks, depending on the mode) on the screen.</p>
<p>When you’re interacting with device-mapped memory, the device may implement custom behavior for each memory access to that region of memory, so the assumptions your CPU and compiler make about regular memory loads and stores may no longer hold. For instance, it is common for hardware devices to have memory-mapped registers that are modified <span epub:type="pagebreak" title="218" id="Page_218"/>when they’re read, meaning the reads have side effects. In such cases, the compiler can’t safely elide a memory store operation if you read the same memory address twice in a row!</p>
<p>A similar issue arises when program execution is suddenly diverted in ways that aren’t represented in the code and thus that the compiler cannot expect. Execution might be diverted if there is no underlying operating system to handle processor exceptions or interrupts, or if a process receives a signal that interrupts execution. In those cases, the execution of the active segment of code is stopped, and the CPU starts executing instructions in the event handler for whatever event triggered the diversion instead. Normally, since the compiler can anticipate all possible executions, it arranges its optimizations so that executions cannot observe when operations have been performed out of order or optimized away. However, since the compiler can’t predict these exceptional jumps, it also cannot plan for them to be oblivious to its optimizations, so these event handlers might actually observe instructions that have run in a different order than those in the original program code.</p>
<p>To deal with these exceptional situations, Rust provides <em>volatile</em> memory operations that cannot be elided or reordered with respect to other volatile operations. These operations take the form of <code>std::ptr::read_volatile</code> and <code>std::ptr::write_volatile</code>. Volatile operations are exactly the right fit for accessing memory-mapped hardware resources: they map directly to memory access operations with no compiler trickery, and the guarantee that volatile operations aren’t reordered relative to one another ensures that hardware operations with possible side effects don’t happen out of order even when they would normally look interchangeable (such as a load of one address and a store to a different address). The no-reordering guarantee also helps the exceptional execution situation, as long as any code that touches memory accessed in an exceptional context uses only volatile memory operations.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p>	There is also a <code>std::sync::atomic::compiler_fence</code> function that prevents the compiler from reordering non-volatile memory accesses. You’ll very rarely need a compiler fence, but its documentation is an interesting read.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="box">
<h2>Including Assembly Code</h2>
<p class="BoxBodyFirst">These days, you rarely need to drop down to writing assembly code to accomplish any given task. But for low-level hardware programming where you need to initialize CPUs at boot or issue strange instructions to manipulate memory mappings, assembly code is still sometimes required. At the time of writing, there is an RFC and a mostly complete implementation of inline assembly syntax on nightly Rust, but nothing has been stabilized yet, so I won’t discuss the syntax in this book.</p>
<p><span epub:type="pagebreak" title="219" id="Page_219"/>It’s still possible to write assembly on stable Rust—you just need to get a little creative. Specifically, remember build scripts from <span class="xref" itemid="xref_target_Chapter 11">Chapter 11</span>? Well, Cargo build scripts can emit certain special directives to standard output to augment Cargo’s standard build process, including <code>cargo:rustc-link-lib=static=</code><var>xyz</var> to link the static library file <em>libxyz.a</em> into the final binary, and <code>cargo:rustc-link-search:</code><var>/some/path</var> to add <em>/some/path</em> to the search path for link objects. Using those, we can add a <em>build.rs</em> to the project that compiles a standalone assembly file (<em>.s</em>) to an object file (<em>.o</em>) using the target platform’s compiler and then repackages it into a static archive (<em>.a</em>) using the appropriate archiving tool (usually <code>ar</code>). The project then emits those two Cargo directives, pointing at where it placed the static archive—probably in <code>OUT_DIR</code>—and we’re off to the races! If the target platform doesn’t change, you can even include the precompiled <em>.a</em> when publishing your crate so that consumers don’t need to rebuild it.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<h2 id="h1--0005">Misuse-Resistant Hardware Abstraction</h2>
<p class="BodyFirst">Rust’s type system excels at encapsulating unsafe, hairy, and otherwise unpleasant code behind safe, ergonomic interfaces. Nowhere is that more important than in the infamously complex world of low-level systems programming, littered with magic hardware-defined values pulled from obscure manuals and mysterious undocumented assembly instruction incantations to get devices into just the right state. And all that in a space where a runtime error might crash more than just a user program!</p>
<p>In <code>no_std</code> programs, it is immensely important to use the type system to make illegal states impossible to represent, as we discussed in <span class="xref" itemid="xref_target_Chapter 3">Chapter 3</span>. If certain combinations of register values cannot occur at the same time, then create a single type whose type parameters indicate the current state of the relevant registers, and implement only legal transitions on it, like we did for the rocket example in <a href="c03.xhtml#listing3-2" id="listinganchor3-2">Listing 3-2</a>.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p>	Make sure to also review the advice from <span class="xref" itemid="xref_target_Chapter 3">Chapter 3</span> on API design—all of that applies in the context of <code>no_std</code> programs as well!</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>For example, consider a pair of registers where at most one register should be “on” at any given point in time. <a href="#listing12-2" id="listinganchor12-2">Listing 12-2</a> shows how you can represent that in a (single-threaded) program in a way makes it impossible to write code that violates that invariant.</p>
<pre><code><span class="LiteralGray">// raw register address -- private submodule</span>
mod registers;
pub struct On;
pub struct Off;
pub struct Pair&lt;R1, R2&gt;(PhantomData&lt;(R1, R2)&gt;);
impl Pair&lt;Off, Off&gt; {
    pub fn get() -&gt; Option&lt;Self&gt; {
<span epub:type="pagebreak" title="220" id="Page_220"/>        static mut PAIR_TAKEN: bool = false;
        if unsafe { PAIR_TAKEN } {
            None
        } else {
<span class="LiteralGray">            // Ensure initial state is correct.</span>
            registers::off("r1");
            registers::off("r2");
            unsafe { PAIR_TAKEN = true };
            Some(Pair(PhantomData))
        }
    }

    pub fn first_on(self) -&gt; Pair&lt;On, Off&gt; {
        registers::set_on("r1");
        Pair(PhantomData)
    }
<span class="LiteralGray">    // .. and inverse for -&gt; Pair&lt;Off, On&gt;</span>
}
impl Pair&lt;On, Off&gt; {
    pub fn off(self) -&gt; Pair&lt;Off, Off&gt; {
        registers::set_off("r1");
        Pair(PhantomData)
    }
}
<span class="LiteralGray">// .. and inverse for Pair&lt;Off, On&gt;</span></code></pre>
<p class="CodeListingCaption"><a id="listing12-2">Listing 12-2</a>: Statically ensuring correct operation</p>
<p>There are a few noteworthy patterns in this code. The first is that we ensure only a single instance of <code>Pair</code> ever exists by checking a private static Boolean in its only constructor and making all methods consume <code>self</code>. We then ensure that the initial state is valid and that only valid state transitions are possible to express, and therefore the invariant must hold globally.</p>
<p>The second noteworthy pattern in <a href="#listing12-2">Listing 12-2</a> is that we use <code>PhantomData</code> to take advantage of zero-sized types and represent runtime information statically. That is, at any given point in the code the types tell us what the runtime state <em>must</em> be, and therefore we don’t need to track or check any state related to the registers at runtime. There’s no need to check that <code>r2</code> isn’t already on when we’re asked to enable <code>r1</code>, since the types prevent writing a program in which that is the case.</p>
<h2 id="h1--0006">Cross-Compilation</h2>
<p class="BodyFirst">Usually, you’ll write <code>no_std</code> programs on a computer with a full-fledged operating system running and all the niceties of modern hardware, but ultimately run it on a dinky hardware device with 93/4 bits of RAM and a sock for a CPU. That calls for <em>cross-compilation</em>—you need to compile the code in your development environment, but compile it <em>for </em>the sock. That’s not the only context in which cross-compilation is important, though. For example, it’s increasingly common to have one build pipeline produce binary <span epub:type="pagebreak" title="221" id="Page_221"/>artifacts for all consumer platforms rather than trying to have a build pipeline for every platform your consumers may be using, and that means using cross-compilation.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p>	If you’re actually compiling for something sock-like with limited memory, or even something as fancy as a potato, you may want to set the<code> opt-level</code> Cargo configuration to <code>"s"</code> to optimize for smaller binary sizes.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>Cross-compiling involves two platforms: the <em>host</em> platform and the <em>target</em> platform. The host platform is the one doing the compiling, and the target platform is the one that will eventually run the output of the compilation. We specify platforms as <em>target triples</em>, which take the form <var>machine-vendor-os</var>. The <var>machine</var> part dictates the machine architecture the code will run on, such as <code>x86_64</code>, <code>armv7</code>, or <code>wasm32</code>, and tells the compiler what instruction set to use for the emitted machine code. The <var>vendor</var> part generally takes the value of <code>pc</code> on Windows, <code>apple</code> on macOS and iOS, and <code>unknown</code> everywhere else, and doesn’t affect compilation in any meaningful way; it’s mostly irrelevant and can even be left out. The <var>os</var> part tells the compiler what format to use for the final binary artifacts, so a value of <code>linux</code> dictates Linux <em>.so</em> files, <code>windows</code> dictates Windows <em>.dll</em> files, and so on.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p>	By default, Cargo assumes that the target platform is the same as the host platform, which is why you generally never have to tell Cargo to, say, compile for Linux when you’re already on Linux. Sometimes you may want to use <code>--target</code> even if the CPU and OS of the target are the same, though, such as to target the <code>musl</code> implementation of <code>libc</code>.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>To tell Cargo to cross-compile, you simply pass it the <code>--target &lt;</code><var>target triple</var><code>&gt;</code> argument with your triple of choice. Cargo will then take care of forwarding that information to the Rust compiler so that it generates binary artifacts that will work on the given target platform. Cargo will also take care to use the appropriate version of the standard library for that platform—after all, the standard library contains a lot of conditional compilation directives (using <code>#[cfg(...)]</code>) so that the right system calls get invoked and the right architecture-specific implementations are used, so we can’t use the standard library for the host platform on the target.</p>
<p>The target platform also dictates what components of the standard library are  available. For example, while <code>x86_64-unknown-linux-gnu</code> includes the full <code>std</code> library, something like <code>thumbv7m-none-eabi</code> does no, and doesn’t even define an allocator, so if you use <code>alloc</code> without defining one explicitly, you’ll get a build error. This comes in handy for testing that code you write <em>actually </em>doesn’t require <code>std</code> (recall that even with <code>#![no_std]</code> you can still have <code>use std::</code>, since <code>no_std</code> opts out of only the <code>std</code> prelude). If you have your continuous integration pipeline build your crate with <code>--target thumbv7m-none-eabi</code>, any attempt to access components from anything but <code>core</code> will trigger a build failure. Crucially, this will also check that your crate doesn’t accidentally bring in dependencies that themselves use items from <code>std</code> (or <code>alloc</code>).</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="box">
<h2><span epub:type="pagebreak" title="222" id="Page_222"/>Platform Support</h2>
<p class="BoxBodyFirst">The standard Rust installer, Rustup, doesn’t install the standard library for all the target triples that Rust supports by default. That would be a waste of space and bandwidth. Instead, you have to use the command <code>rustup target add</code> to install the appropriate standard library versions for additional targets. If no version of the standard library exists for your target platform, you’ll have to compile it from source yourself by adding the <code>rust-src</code> Rustup component and using Cargo’s (currently unstable) <code>build-std</code> feature to also build <code>std</code> (and/or <code>core</code> and <code>alloc</code>) when building any crate.</p>
<p>If your target is not supported by the Rust compiler—that is, if <code>rustc</code> doesn’t even know about your target triple—you’ll have to go one step further and teach <code>rustc</code> about the properties of the triple using a custom target specification. How you do that is both currently unstable and beyond the scope of this book, but a search for “custom target specification json” is a good place to start.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<h2 id="h1--0007">Summary</h2>
<p class="BodyFirst">In this chapter, we’ve covered what lies beneath the standard library—or, more precisely, beneath <code>std</code>. We’ve gone over what you get with <code>core</code>, how you can extend your non-<code>std</code> reach with <code>alloc</code>, and what the (tiny) Rust runtime adds to your programs to make <code>fn main</code> work. We’ve also taken a look at how you can interact with device-mapped memory and otherwise handle the unorthodox execution patterns that can happen at the very lowest level of hardware programming, and how to safely encapsulate at least some of the oddities of hardware in the Rust type system. Next, we’ll move from the very small to the very large by discussing how to navigate, understand, and maybe even contribute to the larger Rust ecosystem.</p>
</section>
</div></body></html>