- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: 'Final Project: Analyzing Text Data'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 期末项目：分析文本数据
- en: '![](image_fi/book_art/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/book_art/chapterart.png)'
- en: In this chapter, we’ll put together some of the tools developed in previous
    chapters by working on a project related to linguistics and psychology. Many important
    data projects today deal with text data, from text matching to chatbots to customer
    sentiment analysis to authorship discernment and linguistic analysis. We’ll look
    at a small dataset of linguistic features derived from different creative writing
    samples to see how language usage varies over genres; some genres encourage a
    different writing process that reflects a different author mindset, such as writing
    a personal essay versus writing a spontaneous haiku.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将通过一个与语言学和心理学相关的项目，将之前章节中开发的一些工具结合起来。如今，许多重要的数据项目都涉及文本数据，从文本匹配到聊天机器人，再到客户情感分析、作者身份辨识和语言学分析。我们将查看一个小型的语言学特征数据集，这些特征来自不同的创意写作样本，看看不同文体之间的语言使用如何变化；某些文体鼓励不同的写作过程，这反映出不同的作者思维方式，例如写个人散文和写即兴俳句的不同。
- en: Specifically, we’ll look at cluster overlap using *k*-NN and our distance metric
    of choice, then visualize the feature space by reducing the dimensionality of
    the dataset, then use dgLARS to create a cross-validated model distinguishing
    poetry types by features, and finally examine a predictive model based on language
    embedding. By completing a whole project, we’ll see how these tools can fit together
    to derive insight from data.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将使用*k*-NN和我们选择的距离度量来查看聚类重叠情况，然后通过降低数据集的维度来可视化特征空间，再使用dgLARS创建一个交叉验证模型，通过特征区分诗歌类型，最后基于语言嵌入检查一个预测模型。通过完成一个完整的项目，我们将看到这些工具如何结合在一起，从数据中得出洞察。
- en: Building a Natural Language Processing Pipeline
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建自然语言处理管道
- en: In [Chapter 1](c01.xhtml), we briefly discussed the importance of text data
    and how natural language processing (NLP) pipelines can transform text data into
    model features that fit well with supervised learning methods. This is the approach
    we’ll take with our upcoming project. We’ll use Python to transform the text data
    into features and then use R to analyze those features.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](c01.xhtml)中，我们简要讨论了文本数据的重要性以及自然语言处理（NLP）管道如何将文本数据转化为适合监督学习方法的模型特征。这就是我们即将进行的项目的方法。我们将使用Python将文本数据转化为特征，然后使用R来分析这些特征。
- en: The first step is to *parse* the data; we have to break the blocks of text into
    more manageable chunks—either sentences within a paragraph or words and punctuation
    within a sentence. This allows us to analyze small pieces of text and combine
    the results into a sentence, a document, or even a set of documents. For instance,
    in this chapter, you might want to parse each section, then each paragraph, and
    then each word in each paragraph to understand how the language usage varies between
    introductory material and application examples.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是*解析*数据；我们需要将文本块拆分成更易处理的部分——可以是段落中的句子，也可以是句子中的单词和标点符号。这样我们就能分析小块文本，并将结果结合成一个句子、一个文档，甚至一组文档。例如，在本章中，你可能想要解析每个部分，然后是每个段落，最后是每个段落中的每个单词，以理解语言使用在引言材料和应用示例之间的变化。
- en: 'Sometimes, you’ll want to take out punctuation, certain types of filler words,
    or additions to root words. *Root words* exclude endings like *ing*, which change
    the tense of a verb or turn one part of speech into another. Consider the differences
    between *dribble*, *dribbled*, and *dribbling* with respect to a basketball practice.
    If the point guard is dribbling the ball, the action is occurring now. If they’ve
    already dribbled for an entire game, they are probably tired and have put the
    ball back on the rack. However, the point guard’s action, whether past or present,
    is the same. It doesn’t matter much if they’re doing it now or did it yesterday.
    Stemming and lemmatizing are two approaches that break words into root words:
    *stemming* does this by reducing words to their roots regardless of whether the
    root is still a word; *lemmatizing* reduces words to roots in a way that ensures
    the root is still a word.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，你可能需要去除标点符号、某些类型的填充词或词根的附加部分。*词根*排除了像*ing*这样的词尾，它会改变动词的时态或将某一词性转换为另一词性。考虑一下关于篮球练习中*dribble*、*dribbled*和*dribbling*的区别。如果控球后卫正在运球，动作正在发生。如果他们已经整个比赛都在运球，他们可能很累，并已经将球放回架子上。然而，控球后卫的动作，不论是过去还是现在，都是一样的。无论他们是现在做还是昨天做，都不重要。词干提取和词形还原是两种将词语分解为词根的方法：*词干提取*通过将词语简化为其词根来实现，不管词根是否仍然是一个有效的单词；*词形还原*则以确保词根仍然是一个单词的方式简化词语。
- en: Once text is parsed to the extent necessary for your specific application, you
    can start analyzing each piece. *Sentiment analysis* aims to understand the emotions
    behind the words and phrases of a given piece of text. For instance, “terrible
    product!!! never buy” has a fairly negative tone compared to “some users may not
    like how the product smells.” Sentiment analysis quantifies emotions within the
    text by tallying up totals for each emotion within the text, such that each receives
    a score and can be rolled into a final score, if preferred.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦文本解析达到你特定应用所需的程度，你就可以开始分析每一部分内容。*情感分析*旨在理解给定文本中的单词和短语背后的情感。例如，“糟糕的产品！！！永远不要买”与“有些用户可能不喜欢这款产品的味道”相比，语气明显更为负面。情感分析通过统计文本中每种情感的总数，量化文本中的情感，使每种情感得到一个分数，并可以将这些分数汇总为最终分数（如果需要的话）。
- en: Once parsing is done, we can apply *named entity recognition*, which matches
    words to lists of important people, places, or terms of interest in a field. For
    instance, when processing medical notes related to patient discharge and outcome,
    you might want to match words to a list of diagnoses.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦解析完成，我们可以应用*命名实体识别*，该方法将单词与重要人物、地点或领域相关的术语列表进行匹配。例如，在处理与患者出院和结果相关的医学记录时，你可能希望将单词与诊断列表进行匹配。
- en: In other applications, it may be important to tag parts of speech, including
    pronouns, verbs, prepositions, adjectives, and more. Some people might use certain
    parts of speech at higher rates than others, and understanding these patterns
    can give insight into the text source’s personality, temporary state of mind,
    or even truthfulness. For some NLP applications, it’s possible to load these factors
    onto a given outcome or set of outcomes to create entirely new metrics from text
    data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他应用中，标注词性（包括代词、动词、介词、形容词等）可能非常重要。有些人可能使用某些词性比其他词性更多，理解这些模式可以揭示文本来源的个性、暂时的心态，甚至是诚实程度。对于某些NLP应用，可以将这些因素加载到给定的结果或一组结果中，从文本数据中创建全新的度量标准。
- en: Each of these analysis types can be integrated into a relational database as
    additional features for downstream models. For instance, as we’ll see later in
    the chapter, we can tag parts of speech, normalize them by the length of that
    particular document, and feed those features into models. You can also vectorize
    the words that exist in the document or set of documents to count frequencies
    of each word that exists in the set of documents for a given document. This often
    precedes deep learning models and visualization methods within NLP applications.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 每种分析类型都可以作为附加特征集成到关系型数据库中，用于下游模型。例如，正如我们在本章后面将看到的，我们可以标注词性，按特定文档的长度进行标准化，然后将这些特征输入到模型中。你还可以将文档或一组文档中的单词向量化，以计算每个单词在给定文档的文档集中的频率。这通常是深度学习模型和可视化方法在NLP应用中之前的步骤。
- en: Again, because R has limited NLP capabilities compared to Python, we’ve done
    this first step—parsing the text data into features—in Python and provided the
    resulting data in the files for the book. We’ll then do the analysis using R.
    We used Python’s NLTK toolkit to parse the text data, and while these steps are
    beyond the scope of this book, we have included the scripts in our Python downloadable
    repository ([https://nostarch.com/download/ShapeofData_PythonCode.zip](https://nostarch.com/download/ShapeofData_PythonCode.zip)),
    and we encourage you to take the raw data provided and see if you can build a
    similar NLP pipeline.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，由于R相比于Python的NLP能力有限，我们已经在Python中完成了第一步——将文本数据解析成特征——并将结果文件提供给这本书。接下来，我们将使用R进行分析。我们使用了Python的NLTK工具包来解析文本数据，虽然这些步骤超出了本书的范畴，但我们已将脚本包含在我们的Python下载库中（[https://nostarch.com/download/ShapeofData_PythonCode.zip](https://nostarch.com/download/ShapeofData_PythonCode.zip)），并鼓励你下载原始数据，看看你是否能够构建类似的NLP管道。
- en: 'The Project: Analyzing Language in Poetry'
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 项目：诗歌语言分析
- en: 'Modern poetry includes many types of poems with different structures, literary
    devices, and subject matters. For instance, formal-verse poems, such as sonnets
    or villanelles, have a defined number of stressed and total syllables in each
    line and a defined rhyme scheme. These types of poems often make heavy use of
    other literary devices, such as allusion or conceit (reference to other works
    as a juxtaposition of ideas), alliteration or assonance (repetition of a certain
    sound), or meter (patterns of stressed syllables within a line of poetry). This
    is an example of formal verse (a sonnet):'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现代诗歌包含了许多不同结构、文学手法和主题的诗篇。例如，形式诗歌，如十四行诗或维拉内尔诗歌，在每行中有规定的重音音节数和总音节数，并且有明确的押韵模式。这些类型的诗歌通常广泛使用其他文学手法，如典故或寓言（通过对比其他作品的思想），头韵或元音韵（某种声音的重复），或韵律（诗句中重音音节的模式）。这是一个形式诗歌的例子（十四行诗）：
- en: Ever After
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 从此以后
- en: ''
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Her glass slipper turns to his M-16,
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 她的玻璃鞋变成了他的M-16，
- en: ''
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: her elegant dress to faded fatigues.
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 她那优雅的裙子变成了褪色的军装。
- en: ''
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: He’s a shell of the man from their intrigues,
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 他已不再是当初那个卷入他们阴谋中的人，
- en: ''
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: served five months patrol away from his queen.
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 他在远离王后的地方巡逻了五个月。
- en: ''
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Cinders to palace, her dreams now rubble,
  id: totrans-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 灰烬化作宫殿，她的梦如今成了废墟，
- en: ''
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: she watches her carriage morph to Abrams tank,
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 她看着她的马车变成了艾布拉姆斯坦克，
- en: ''
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: as if her fairy tale were some cruel prank.
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 仿佛她的童话是一个残酷的恶作剧。
- en: ''
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: He shouts, “Hurry, men! March on the double!”
  id: totrans-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 他喊道：“快点，伙计们！加速前进！”
- en: ''
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Her clock strikes twelve, his tank an IED,
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 她的时钟敲响十二点，他的坦克成了简易爆炸装置，
- en: ''
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: and widower’s daughter is left widow.
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 寡妇的女儿最终也成了寡妇。
- en: ''
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: She has but memories, now as shadows,
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 现在她只剩下回忆，变成了阴影，
- en: ''
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: to comfort her dark days of misery.
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 以安慰她那黑暗的痛苦日子。
- en: ''
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Her ever after has no tomorrow,
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 她的“从此以后”没有明天，
- en: ''
  id: totrans-43
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: leaving Cinderella grief and sorrow.
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 留下灰姑娘的悲伤与忧愁。
- en: 'In contrast, free-verse poetry doesn’t have a defined rhyme scheme for the
    end of each line, may have varying line lengths (or consistently long, short,
    or medium lines), and tends to use literary devices such as meter or rhyme for
    emphasis of a particular piece of the poem. This is an example of a free-verse
    poem with short line lengths:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，自由诗不要求每行的结尾有固定的押韵模式，行长可以不同（或者保持一致的长短），并且通常使用如韵律或押韵等文学手法来强调诗中的某些部分。这是一个行长较短的自由诗例子：
- en: Anya
  id: totrans-46
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 安雅
- en: ''
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Gaunt,
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 瘦削，
- en: ''
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: made-up,
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 化妆过的，
- en: ''
  id: totrans-51
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: wobbling in heels
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在高跟鞋中晃动
- en: ''
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: too big for tiny feet,
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 太大了，穿不下她的小脚，
- en: ''
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: heels
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 高跟鞋
- en: ''
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: that sparkle and clack
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 那闪耀的光芒与撞击声
- en: ''
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: against cold concrete
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 对着冰冷的混凝土
- en: ''
  id: totrans-61
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: as wind
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如同风
- en: ''
  id: totrans-63
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: whips her teased hair
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 抽动她挑弄过的头发
- en: ''
  id: totrans-65
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: like a lasso
  id: totrans-66
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 像套索一样
- en: ''
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: roping a steer
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 驯牛的绳索
- en: ''
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: as snow
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如雪花般飘落
- en: ''
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: binds to tight jeans
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 紧紧束缚着牛仔裤
- en: ''
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: like the shackles
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如同枷锁
- en: ''
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: she wore on the ship to her Shangri La
  id: totrans-76
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 她穿着它，走上了通往香格里拉的船
- en: ''
  id: totrans-77
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: as streetlight
  id: totrans-78
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如街灯般
- en: ''
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: catches a gleam in her eyes,
  id: totrans-80
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 她眼中闪烁着光芒，
- en: ''
  id: totrans-81
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: a glimpse as she stares into
  id: totrans-82
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 她凝视着
- en: ''
  id: totrans-83
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: tonight
  id: totrans-84
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 今晚
- en: ''
  id: totrans-85
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: her fifteenth birthday.
  id: totrans-86
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 她的十五岁生日。
- en: 'Some poems don’t fit neatly into free verse or formal verse, such as prose
    poetry (where line breaks don’t exist) or haibun (a Japanese form that incorporates
    prose poetry with different types of haiku). Modern haiku and its related forms
    juxtapose two images or thoughts with a turn, such as a dash, that connects the
    two images or thoughts in a moment of insight. Typically, modern haiku doesn’t
    conform to the Japanese syllable count requirements, but it usually includes some
    reference to season and nature (or human nature in the case of the related form,
    senryu). Haibun knits a story together through the poem title, the haiku, and
    the prose pieces. Here’s one example of a modern haiku:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 有些诗歌不太适合自由诗或形式诗，比如散文诗（其中不存在换行）或俳文（一种将散文诗与不同类型的俳句结合的日本形式）。现代俳句及其相关形式通过一个连接两个图像或思想的瞬间的破折号等转折来对比两个图像或思想。通常，现代俳句不符合日语的音节要求，但通常包含对季节和自然（或人性，对于相关形式
    senryu）的某种参考。俳文通过诗歌标题、俳句和散文片段将一个故事编织在一起。以下是一个现代俳句的例子：
- en: shooting stars—
  id: totrans-88
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 流星——
- en: ''
  id: totrans-89
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: the dash between
  id: totrans-90
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 破折号之间
- en: ''
  id: totrans-91
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: born and died
  id: totrans-92
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 出生和死亡
- en: From prior research, we know that different authors can be identified by their
    preferred word choices and their unique usage of different parts of speech (this
    is a core feature of antiplagiarism software); we also know that language usage
    varies by the author’s state of mind. However, it’s unknown if the same author
    constructs poetry differently depending on the type of poem they are writing.
    Because haiku and haibun originated as a spark of insight juxtaposing ideas, it’s
    possible that the different genesis of the poem influences the use of language
    and grammar within the poem.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 从先前的研究中，我们知道不同的作者可以通过他们偏好的词汇选择和对不同词类的独特使用来识别（这是反抄袭软件的核心特征）；我们也知道语言使用会因作者的心境而异。然而，目前尚不清楚同一作者是否会根据他们所写的诗歌类型而构建不同的诗歌。由于俳句和俳文起源于对想法进行对比的灵感，不同类型的诗歌可能会影响诗歌中语言和语法的使用。
- en: Let’s dive into the dataset a bit before we start visualizing it. We have eight
    haiku, eight haibun, eight free-verse poems, and eight formal-verse poems in the
    dataset. We’ll group them into haiku-based poems and other poems to simplify the
    analyses by how poems are typically generated (free association versus crafting).
    The features we’ll consider are punctuation fraction, noun fraction, verb fraction,
    personal pronoun fraction, adjective fraction, adverb fraction, and preposition/conjunction
    fraction. Given the construction of each poem type, it’s likely some of these
    factors will vary. With a larger sample of poems, you could use other parts of
    speech or break categories, such as verbs, into their individual components.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始可视化之前，让我们先深入了解一下数据集。数据集中包含八首俳句、八首俳文、八首自由诗和八首形式诗。我们将它们分为基于俳句的诗歌和其他诗歌，以简化按照诗歌通常生成的方式进行分析（自由联想与精心创作）。我们将考虑的特征包括标点符号比例、名词比例、动词比例、人称代词比例、形容词比例、副词比例和介词/连词比例。考虑到每种诗歌类型的构造，这些因素中的一些可能会有所不同。有了更多的诗歌样本，您可以使用其他词类或将动词等类别分解为其各个组成部分。
- en: We’ll be completing these steps in Python, so we’ll overview only the steps
    used rather than dive into code. You can find the processed data in the files
    for this book.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在 Python 中完成这些步骤，因此我们只会概述使用的步骤，而不会深入到代码中。您可以在本书的文件中找到处理过的数据。
- en: Tokenizing Text Data
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标记化文本数据
- en: The first step in processing the poems for analysis is *tokenizing* the text
    data, meaning we need to parse our poems into individual words and punctuation
    marks. The Treebank tokenizer uses regular expressions to parse words in sentences,
    handle contractions and other combinations of words and punctuation, and splice
    quotes. These are important steps for parsing poem text data, as punctuation is
    interspersed with words and phrases at relatively high rates. Haiku, in particular,
    tends to use punctuation to create a cut in the poem to link two different images
    or ideas.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 对诗歌进行分析的第一步是*标记化*文本数据，这意味着我们需要将诗歌解析为单词和标点符号。 Treebank 标记器使用正则表达式来解析句子中的单词，处理缩略词和其他单词和标点的组合，并拼接引号。这些是解析诗歌文本数据的重要步骤，因为标点符号与单词和短语交错出现的频率相对较高。特别是俳句倾向于使用标点符号来在诗歌中创建一个切口，连接两个不同的图像或想法。
- en: Because the Treebank tokenizer often splits contractions and other words that
    connect with punctuation, it’s useful to the regex tokenizer to count the number
    of actual words that exist in the text and parse them into words that can contain
    punctuation. Given how short some poems are, we want to make sure we aren’t inflating
    word counts. The regex tokenizer results give us an accurate word count to normalize
    parts of speech or punctuation proportions.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Treebank分词器经常将缩写和其他与标点连接的词拆分，因此使用正则表达式分词器来统计文本中实际存在的单词数量并将它们解析成包含标点的单词是很有用的。考虑到一些诗歌的篇幅较短，我们希望确保不会夸大单词的统计数量。正则表达式分词器的结果为我们提供了一个准确的单词计数，从而可以规范化词性或标点符号的比例。
- en: After obtaining the lengths of tokenizer results, we can subtract the number
    of words from the number of tokens to derive a length of punctuation in the text.
    This allows us to compare fractions of words and fractions of punctuation for
    different poem types, which likely varies by poem type (and by poem author, according
    to prior research on linguistic differences in text passages by author).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在获得分词器结果的长度后，我们可以通过从令牌的数量中减去单词的数量来推导出文本中的标点符号长度。这使我们能够比较不同诗歌类型中单词和标点符号的比例，这可能会根据诗歌类型（以及根据先前关于不同作者文本片段语言差异的研究）而有所不同。
- en: Tagging Parts of Speech
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词性标注
- en: To tag relevant parts of speech, we’ll use the *averaged perceptron tagger*,
    a supervised learning algorithm that tends to have pretty good accuracy across
    text types and has been pretrained for the NLTK package. While it’s a bit slow
    on large volumes of text, our text samples are fairly small, allowing the application
    to tag words without much processing power required. It’s possible to scale NLTK’s
    averaged perceptron tagger application to very large datasets using the big data
    technologies that we’ll consider in the next chapter.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为了标注相关的词性，我们将使用*平均感知机标注器*，这是一种监督学习算法，通常在各种文本类型中具有相当高的准确性，并且已经为NLTK包进行了预训练。虽然它在处理大量文本时稍显缓慢，但我们的文本样本相对较小，允许应用程序在不需要大量计算资源的情况下标注单词。通过使用大数据技术，接下来的章节中我们将讨论如何将NLTK的平均感知机标注器应用于非常大的数据集。
- en: We’ll parse out nouns, verbs, personal pronouns, adverbs, adjectives, and prepositions
    and conjunctions and count the numbers of each category that exist in each text
    sample.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将解析名词、动词、个人代词、副词、形容词、介词和连词，并统计每个文本样本中每个类别的数量。
- en: Nouns include singular, plural, common, and proper noun combinations. Verbs
    include all type and tense combinations. Personal pronouns include pronouns and
    possessive pronouns. Adverbs and adjectives include comparative and superlative
    forms of adverbs and adjectives. Prepositions and coordinating conjunctions are
    also tagged and counted.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 名词包括单数、复数、普通名词和专有名词的组合。动词包括所有类型和时态的组合。个人代词包括代词和所有格代词。副词和形容词包括副词和形容词的比较级和最高级形式。介词和并列连词也被标注和统计。
- en: Some other tagged parts of speech exist in the averaged perceptron tagger, and
    other taggers may include further divisions of parts of speech. If you want to
    explore how parts of speech can be used to distinguish text types, text authorship,
    or demographics of the text author, you may want to use another tagger or disaggregate
    nouns, verbs, and so on, from their individual components. However, this will
    result in more columns within your dataset, so we recommend you collect more samples
    if you’re doing that type of nuanced analysis of text attributes and parts-of-speech
    analysis.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 平均感知机标注器中还存在一些其他标注的词性，其他标注器可能会包括更细致的词性划分。如果你希望探索如何利用词性来区分文本类型、文本作者或作者的群体特征，你可能需要使用其他标注器，或将名词、动词等从它们的各个成分中分解出来。然而，这将导致数据集中出现更多的列，因此如果你进行这种细致的文本属性和词性分析，建议收集更多样本。
- en: Normalizing Vectors
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 向量规范化
- en: Because our text samples include some short samples and some long samples according
    to poem type, we’ll want to standardize the parts-of-speech counts before we work
    with the data. Our approach includes normalization of punctuation by token count
    (punctuation and word count totals) and normalization of parts-of-speech count
    by word count, summarized in this chapter’s files. This should give good enough
    features to demonstrate poem type differences and still allow for good dimensionality
    reduction results to visualize our dataset.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: For a more nuanced approach that parses out types of verbs, nouns, and so on,
    you could derive fractions of part of speech category or break down fractions
    within parts of speech to engineer more detailed features for your analysis. If
    you’re familiar with Python, we encourage you to play around with the NLP pipeline
    and customize your analyses for more insight into poem-type linguistic differences.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: For now, let’s move onto the analysis in R.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the Poem Dataset in R
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ll start by loading the processed poem dataset and exploring the features
    we’ve derived using the code in [Listing 9-1](#listing9-1).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Listing 9-1: A script that reads in the processed poem data'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Adverbs tend to be the least-represented features, accounting for only 0–10.5
    percent of words used in any given poem. Nouns tend to be the most frequent words
    appearing in this set of poems, accounting for 12.5–53.5 percent of words in a
    given poem. Personal pronouns are rare, with more than a quarter of the poems
    not containing a personal pronoun (likely due to the haiku, which tend not to
    use them). Punctuation usage varies quite a bit, from no representation in a haiku
    to nearly half of a free-verse poem being composed of punctuation (a list poem
    of medical diagnoses at a hospital). Given this variation, it’s likely we have
    good features to use in our analyses.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s set the seed for our analyses and visualize the parts-of-speech features
    with t-SNE, using the shape of each visualized point as a designation of the poem
    type; add the following code to [Listing 9-1](#listing9-1):'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](image_fi/503083c09/f09001.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9-1: A t-SNE plot of poem features by poem type, with type represented
    by either circles or triangles (`perplexity=5`)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: From the plot in [Figure 9-1](#figure9-1), we can see that our poems tend to
    separate out into clusters where most points are of the same type. This means
    a kernel-based model or nearest neighbor model is probably sufficient for classifying
    poem type by features.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Given some separation of points by features, it’s likely we can apply algorithms
    to cluster our data and use supervised learning to understand which differences
    exist between haiku-type poems and other poems.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s divide our sample into training and test fractions and then apply a Euclidean-distance-based
    *k*-NN algorithm to classify poem types based on a poem’s five nearest neighbors
    by adding to our code so far:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Since our t-SNE plot ([Figure 9-1](#figure9-1)) suggests some separation and
    points generally near poems of a similar type, it’s likely that this model has
    worked well. Let’s examine the predicted and true labels for our test set under
    the *k*-NN model through this addition to our code:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的t-SNE图（[图9-1](#figure9-1)）表明一些分离和通常靠近相似类型诗歌的点，这个模型很可能工作良好。让我们通过这个代码的补充来检查我们测试集的预测和真实标签在*k*-NN模型下：
- en: '[PRE3]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: It looks like this model correctly classifies all test poems, suggesting a high-quality
    model that can separate types of poems based on features.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来这个模型正确分类了所有的测试诗歌，表明这是一个能够根据特征区分诗歌类型的高质量模型。
- en: 'We’ll examine these potential type differences by feature further using a 10-fold
    cross-validated dgLARS model in an additional step to our code:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将进一步使用一个10折交叉验证的dgLARS模型来检查这些潜在的类型差异，作为我们代码的一个额外步骤：
- en: '[PRE4]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Because we are working with a small dataset, it’s possible that one or more
    of your folds will have issues, giving a slightly different model than the results
    shown here:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用的是一个小数据集，可能会出现一个或多个折叠存在问题，导致模型与这里显示的结果略有不同：
- en: '[PRE5]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Your model should show differences in punctuation fraction and adverb fraction.
    Free-verse and formal-verse poems have higher rates of punctuation usage. Given
    that these types of poems are more likely to use full sentences rather than connected
    phrases, the differences in punctuation usage are consistent with expected differences.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 您的模型应该显示标点符号分数和副词分数的差异。自由诗和形式诗的标点使用率更高。鉴于这些类型的诗更有可能使用完整句子而不是连接短语，标点使用的差异与预期的差异一致。
- en: The differences in adverb fractions aren’t as expected. However, adverb usage
    is linked to many different transient and fixed personality traits. It’s possible
    that haiku taps into a different transient mood or trait, creating style differences
    reflected in adverb usage. Subject matter may also influence this difference.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 副词分数的差异并不如预期。然而，副词的使用与许多不同的瞬时和固定人格特征相关联。可能是俳句涉及到不同的瞬时情绪或特质，从而在副词使用中反映出风格上的差异。主题内容也可能影响这种差异。
- en: Importantly, this analysis shows that parts-of-speech and punctuation patterns
    vary within samples of writing by the same author in the same type of writing
    (poetry). Given that prior research suggests that some of these input features
    can be used to identify the likely author of a text, it may be prudent to rethink
    authorship prediction based on parts-of-speech analysis. Different subject matters,
    different types of writing, and different life stages of the author may influence
    word choice, sentence structure, and usage of punctuation.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，这个分析显示了同一作者在相同类型的写作样本中，词性和标点模式会有所变化（诗歌）。鉴于先前的研究表明，一些这些输入特征可以用来识别文本的可能作者，因此重新考虑基于词性分析的作者预测可能是明智的。不同的主题、不同类型的写作以及作者的不同生活阶段可能会影响词汇选择、句子结构和标点符号的使用。
- en: 'To see how the poems group together within each poem type, let’s visualize
    the persistence diagrams for each poem type through adding to our code:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看到每种诗歌类型内的诗歌是如何分组的，让我们通过添加到我们的代码中来可视化每种诗歌类型的持久图：
- en: '[PRE6]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[Figure 9-2](#figure9-2) shows the persistence diagrams for haiku and non-haiku
    samples, highlighting some differences in poem clustering within type:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9-2](#figure9-2) 显示了俳句和非俳句样本的持久图，突出了类型内诗歌聚类中的一些差异：'
- en: '![](image_fi/503083c09/f09002.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c09/f09002.png)'
- en: 'Figure 9-2: Persistence diagrams by poem type'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图9-2：按诗歌类型分类的持久图
- en: As we can see in [Figure 9-2](#figure9-2), non-haiku poems tend to cluster more
    consistently, while haiku poems spread out without a lot of separation involving
    multiple poems grouped together. This suggests that haiku features vary from poem
    to poem, while non-haiku poems show more consistency in features. The spontaneity
    of the composition may result in a wider variety of poem structures found with
    haiku poems. Data related to spontaneousness of poem creation and time spent crafting
    the poem may shed light on conscious language usage differences between poems
    (such as haiku) that arise from a single moment and those that are created with
    more intent behind their creation.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[图9-2](#figure9-2)中看到的，非俳句诗歌的聚类更为一致，而俳句诗歌则呈现出分散的状态，多个诗歌组合在一起时没有太多分隔。这表明，俳句的特征在每首诗歌之间有所不同，而非俳句诗歌的特征则显示出更多的一致性。创作的自发性可能导致俳句诗歌的诗歌结构更加多样化。与诗歌创作的自发性和创作过程中所花费的时间相关的数据，可能会揭示出单次创作时产生的诗歌（例如俳句）与那些有更多创作意图的诗歌之间，在语言使用上的差异。
- en: Using Topology-Based NLP Tools
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用基于拓扑的自然语言处理工具
- en: NLP as a field has evolved over the past years, and some common tools in NLP
    leverage topology to solve important problems. Early embedding tools tended to
    tally word frequencies within and across documents of interest to parse text data
    into numeric data that works well in machine learning algorithms. However, words
    often don’t contain all the semantic information needed to make sense of a sentence
    or paragraph or entire document. Negatives, such as *no* or *not*, modify actions
    or actors within a sentence. For instance, “she did let him in the house” is a
    very different statement semantically than “she did not let him in the house.”
    Depending on what you’re trying to predict or classify, simple mappings from individual
    words to a matrix of numbers don’t work well.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）作为一个领域，在过去几年中已经发展了很多，NLP中一些常用工具通过拓扑来解决重要问题。早期的嵌入工具倾向于统计文档中或跨文档的词频，将文本数据解析为适用于机器学习算法的数值数据。然而，单词通常并不包含足够的语义信息来理解一个句子、段落或整篇文档。否定词，如*no*或*not*，会修饰句子中的动作或行为者。例如，“她确实让他进了房子”在语义上与“她确实没有让他进房子”是完全不同的。根据你试图预测或分类的内容，从单个单词到数字矩阵的简单映射并不适用。
- en: In addition, a document or collection of documents might have 30,000+ words
    that occur at least once. Most common words will occur often with little value
    added by their presence. Important words might occur only once or twice. This
    leads us into the problem of dimensionality again when we try to sift through
    the data for important trends.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一份文档或文档集可能包含超过30,000个至少出现一次的单词。最常见的单词通常会频繁出现，但它们的存在并不会增加多少价值。重要的单词可能只会出现一两次。这又引出了维度问题，当我们试图从数据中筛选出重要的趋势时。
- en: Fortunately, recent years have seen some great developments in low-dimensional
    embeddings via topological mapping with special types of neural networks. *Pretrained
    transformer models* are neural networks that can pass information forward and
    backward through their hidden layers to obtain optimal topological mappings for
    text data. Pretrained transformer models learn low-dimensional embeddings of text
    data from massive training sets in the language or languages of interest. *BERT*
    (Bidirectional Encoder Representations from Transformers) and its sentence-based
    embedding cousin, *SBERT*, are two of the most common open source pretrained transformer
    models used to embed text data into lower-dimensional, dense matrices for use
    in machine learning tasks. BERT models can be extended to languages that are not
    currently supported, such as embeddings of text in Hausa or Lingala or Rushani;
    this has the potential to accelerate language translation services and preserve
    endangered languages. *GPT-3*, trained on a similar premise as BERT, has created
    accurate translations and chatbots that can parse meaning from input text rather
    than match keywords or eat up computing resources trying to process high-dimensional
    matrices within machine learning algorithms.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Using Python, we’ve built a BERT model on our poem set based on their serious
    tone or humorous tone to show how BERT embeddings can fit with our supervised
    learning tools in this book. (Note that the order of poems has changed from the
    original set with the data munging to get BERT embeddings and pass them back to
    a *.csv* file.) You can consult Python’s transformer package to learn more about
    how BERT models are imported and leveraged in text embedding or refer to the Python
    scripts for this chapter ([https://nostarch.com/download/ShapeofData_PythonCode.zip](https://nostarch.com/download/ShapeofData_PythonCode.zip)).
    However, we’ll stick to importing the results into R and visualizing the data
    in a smaller dimension, as we did with our poem linguistic features earlier in
    the chapter. Let’s create a t-SNE embedding and plot it with [Listing 9-2](#listing9-2).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Listing 9-2: A script that loads BERT data for serious and humorous poems,
    embeds the data with t-SNE, and plots the results'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 9-3](#figure9-3) shows the embedding, which demonstrates that poems
    separate into clusters by poem type. This mirrors our haiku versus non-haiku poem
    results in [Figure 9-1](#figure9-1), where we saw features separating out by type
    in the t-SNE embedding. Again, this suggests that a machine learning classifier
    should work well with our dataset.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c09/f09003.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9-3: A plot of t-SNE embedding results, with serious or humorous types
    of poems denoted by circles and triangles, respectively (`perplexity=5`)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we know a machine learning model may do well at classifying this data,
    let’s fit a Lasso model with homotopy continuation to handle the small sample
    size by adding [Listing 9-3](#listing9-3):'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Listing 9-3: A script that loads BERT data for serious and humorous poems,
    splits the data into training and test sets, builds a Lasso model with homotopy
    continuation fitting, and shows prediction and actual values for test data'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: The dataset has 384 components from the embedded BERT model and 25 poems. Your
    version of R may split the data and fit the model differently, but in our version
    of R, the test data has three serious poems followed by a humorous one followed
    by a serious one. The model predicts a serious poem, a serious one, a serious
    one, a humorous one, and a serious one (giving a model accuracy of 100 percent).
    Given how small this training set is compared to the 384 predictors fed into our
    model, this is great performance. Note that the selected features have little
    meaning semantically, as they are simply embeddings. Coupling topology-based methods
    into pipelines to process and model small datasets can provide decent prediction
    where other models will fail entirely.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we applied several of the methods overviewed in the book on
    a linguistics question involving a dataset of features derived from NLP-processed
    poems; we also embedded our data and created a model to predict tone differences
    in our poem set.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: For the first problem, we reduced the dimensionality of the dataset to visualize
    group differences, applied two supervised learning models to understand classification
    accuracy and important features distinguishing the poetry types, and visualized
    topological features that exist in both sets of poems. This showed us that language
    usage, particularly punctuation, varies across poem types.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: We then looked at context-aware embeddings and predicting the tone of our poem
    set using a topology-based embedding method and a topology-based classification
    model, which showed that we could get fairly accurate prediction building a model
    from 384 embedded components and a training set of 21 poems.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll wrap up the book by looking at ways to scale topological
    data analysis algorithms with distributed and quantum computing approaches.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
