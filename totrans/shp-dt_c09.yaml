- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Final Project: Analyzing Text Data'
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/book_art/chapterart.png)'
  prefs: []
  type: TYPE_IMG
- en: In this chapter, we’ll put together some of the tools developed in previous
    chapters by working on a project related to linguistics and psychology. Many important
    data projects today deal with text data, from text matching to chatbots to customer
    sentiment analysis to authorship discernment and linguistic analysis. We’ll look
    at a small dataset of linguistic features derived from different creative writing
    samples to see how language usage varies over genres; some genres encourage a
    different writing process that reflects a different author mindset, such as writing
    a personal essay versus writing a spontaneous haiku.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, we’ll look at cluster overlap using *k*-NN and our distance metric
    of choice, then visualize the feature space by reducing the dimensionality of
    the dataset, then use dgLARS to create a cross-validated model distinguishing
    poetry types by features, and finally examine a predictive model based on language
    embedding. By completing a whole project, we’ll see how these tools can fit together
    to derive insight from data.
  prefs: []
  type: TYPE_NORMAL
- en: Building a Natural Language Processing Pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [Chapter 1](c01.xhtml), we briefly discussed the importance of text data
    and how natural language processing (NLP) pipelines can transform text data into
    model features that fit well with supervised learning methods. This is the approach
    we’ll take with our upcoming project. We’ll use Python to transform the text data
    into features and then use R to analyze those features.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to *parse* the data; we have to break the blocks of text into
    more manageable chunks—either sentences within a paragraph or words and punctuation
    within a sentence. This allows us to analyze small pieces of text and combine
    the results into a sentence, a document, or even a set of documents. For instance,
    in this chapter, you might want to parse each section, then each paragraph, and
    then each word in each paragraph to understand how the language usage varies between
    introductory material and application examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes, you’ll want to take out punctuation, certain types of filler words,
    or additions to root words. *Root words* exclude endings like *ing*, which change
    the tense of a verb or turn one part of speech into another. Consider the differences
    between *dribble*, *dribbled*, and *dribbling* with respect to a basketball practice.
    If the point guard is dribbling the ball, the action is occurring now. If they’ve
    already dribbled for an entire game, they are probably tired and have put the
    ball back on the rack. However, the point guard’s action, whether past or present,
    is the same. It doesn’t matter much if they’re doing it now or did it yesterday.
    Stemming and lemmatizing are two approaches that break words into root words:
    *stemming* does this by reducing words to their roots regardless of whether the
    root is still a word; *lemmatizing* reduces words to roots in a way that ensures
    the root is still a word.'
  prefs: []
  type: TYPE_NORMAL
- en: Once text is parsed to the extent necessary for your specific application, you
    can start analyzing each piece. *Sentiment analysis* aims to understand the emotions
    behind the words and phrases of a given piece of text. For instance, “terrible
    product!!! never buy” has a fairly negative tone compared to “some users may not
    like how the product smells.” Sentiment analysis quantifies emotions within the
    text by tallying up totals for each emotion within the text, such that each receives
    a score and can be rolled into a final score, if preferred.
  prefs: []
  type: TYPE_NORMAL
- en: Once parsing is done, we can apply *named entity recognition*, which matches
    words to lists of important people, places, or terms of interest in a field. For
    instance, when processing medical notes related to patient discharge and outcome,
    you might want to match words to a list of diagnoses.
  prefs: []
  type: TYPE_NORMAL
- en: In other applications, it may be important to tag parts of speech, including
    pronouns, verbs, prepositions, adjectives, and more. Some people might use certain
    parts of speech at higher rates than others, and understanding these patterns
    can give insight into the text source’s personality, temporary state of mind,
    or even truthfulness. For some NLP applications, it’s possible to load these factors
    onto a given outcome or set of outcomes to create entirely new metrics from text
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Each of these analysis types can be integrated into a relational database as
    additional features for downstream models. For instance, as we’ll see later in
    the chapter, we can tag parts of speech, normalize them by the length of that
    particular document, and feed those features into models. You can also vectorize
    the words that exist in the document or set of documents to count frequencies
    of each word that exists in the set of documents for a given document. This often
    precedes deep learning models and visualization methods within NLP applications.
  prefs: []
  type: TYPE_NORMAL
- en: Again, because R has limited NLP capabilities compared to Python, we’ve done
    this first step—parsing the text data into features—in Python and provided the
    resulting data in the files for the book. We’ll then do the analysis using R.
    We used Python’s NLTK toolkit to parse the text data, and while these steps are
    beyond the scope of this book, we have included the scripts in our Python downloadable
    repository ([https://nostarch.com/download/ShapeofData_PythonCode.zip](https://nostarch.com/download/ShapeofData_PythonCode.zip)),
    and we encourage you to take the raw data provided and see if you can build a
    similar NLP pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Project: Analyzing Language in Poetry'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Modern poetry includes many types of poems with different structures, literary
    devices, and subject matters. For instance, formal-verse poems, such as sonnets
    or villanelles, have a defined number of stressed and total syllables in each
    line and a defined rhyme scheme. These types of poems often make heavy use of
    other literary devices, such as allusion or conceit (reference to other works
    as a juxtaposition of ideas), alliteration or assonance (repetition of a certain
    sound), or meter (patterns of stressed syllables within a line of poetry). This
    is an example of formal verse (a sonnet):'
  prefs: []
  type: TYPE_NORMAL
- en: Ever After
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Her glass slipper turns to his M-16,
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: her elegant dress to faded fatigues.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: He’s a shell of the man from their intrigues,
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: served five months patrol away from his queen.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Cinders to palace, her dreams now rubble,
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: she watches her carriage morph to Abrams tank,
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: as if her fairy tale were some cruel prank.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: He shouts, “Hurry, men! March on the double!”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Her clock strikes twelve, his tank an IED,
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: and widower’s daughter is left widow.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: She has but memories, now as shadows,
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: to comfort her dark days of misery.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Her ever after has no tomorrow,
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: leaving Cinderella grief and sorrow.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In contrast, free-verse poetry doesn’t have a defined rhyme scheme for the
    end of each line, may have varying line lengths (or consistently long, short,
    or medium lines), and tends to use literary devices such as meter or rhyme for
    emphasis of a particular piece of the poem. This is an example of a free-verse
    poem with short line lengths:'
  prefs: []
  type: TYPE_NORMAL
- en: Anya
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Gaunt,
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: made-up,
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: wobbling in heels
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: too big for tiny feet,
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: heels
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: that sparkle and clack
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: against cold concrete
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: as wind
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: whips her teased hair
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: like a lasso
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: roping a steer
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: as snow
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: binds to tight jeans
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: like the shackles
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: she wore on the ship to her Shangri La
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: as streetlight
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: catches a gleam in her eyes,
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: a glimpse as she stares into
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: tonight
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: her fifteenth birthday.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Some poems don’t fit neatly into free verse or formal verse, such as prose
    poetry (where line breaks don’t exist) or haibun (a Japanese form that incorporates
    prose poetry with different types of haiku). Modern haiku and its related forms
    juxtapose two images or thoughts with a turn, such as a dash, that connects the
    two images or thoughts in a moment of insight. Typically, modern haiku doesn’t
    conform to the Japanese syllable count requirements, but it usually includes some
    reference to season and nature (or human nature in the case of the related form,
    senryu). Haibun knits a story together through the poem title, the haiku, and
    the prose pieces. Here’s one example of a modern haiku:'
  prefs: []
  type: TYPE_NORMAL
- en: shooting stars—
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: the dash between
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: born and died
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: From prior research, we know that different authors can be identified by their
    preferred word choices and their unique usage of different parts of speech (this
    is a core feature of antiplagiarism software); we also know that language usage
    varies by the author’s state of mind. However, it’s unknown if the same author
    constructs poetry differently depending on the type of poem they are writing.
    Because haiku and haibun originated as a spark of insight juxtaposing ideas, it’s
    possible that the different genesis of the poem influences the use of language
    and grammar within the poem.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dive into the dataset a bit before we start visualizing it. We have eight
    haiku, eight haibun, eight free-verse poems, and eight formal-verse poems in the
    dataset. We’ll group them into haiku-based poems and other poems to simplify the
    analyses by how poems are typically generated (free association versus crafting).
    The features we’ll consider are punctuation fraction, noun fraction, verb fraction,
    personal pronoun fraction, adjective fraction, adverb fraction, and preposition/conjunction
    fraction. Given the construction of each poem type, it’s likely some of these
    factors will vary. With a larger sample of poems, you could use other parts of
    speech or break categories, such as verbs, into their individual components.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll be completing these steps in Python, so we’ll overview only the steps
    used rather than dive into code. You can find the processed data in the files
    for this book.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizing Text Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first step in processing the poems for analysis is *tokenizing* the text
    data, meaning we need to parse our poems into individual words and punctuation
    marks. The Treebank tokenizer uses regular expressions to parse words in sentences,
    handle contractions and other combinations of words and punctuation, and splice
    quotes. These are important steps for parsing poem text data, as punctuation is
    interspersed with words and phrases at relatively high rates. Haiku, in particular,
    tends to use punctuation to create a cut in the poem to link two different images
    or ideas.
  prefs: []
  type: TYPE_NORMAL
- en: Because the Treebank tokenizer often splits contractions and other words that
    connect with punctuation, it’s useful to the regex tokenizer to count the number
    of actual words that exist in the text and parse them into words that can contain
    punctuation. Given how short some poems are, we want to make sure we aren’t inflating
    word counts. The regex tokenizer results give us an accurate word count to normalize
    parts of speech or punctuation proportions.
  prefs: []
  type: TYPE_NORMAL
- en: After obtaining the lengths of tokenizer results, we can subtract the number
    of words from the number of tokens to derive a length of punctuation in the text.
    This allows us to compare fractions of words and fractions of punctuation for
    different poem types, which likely varies by poem type (and by poem author, according
    to prior research on linguistic differences in text passages by author).
  prefs: []
  type: TYPE_NORMAL
- en: Tagging Parts of Speech
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To tag relevant parts of speech, we’ll use the *averaged perceptron tagger*,
    a supervised learning algorithm that tends to have pretty good accuracy across
    text types and has been pretrained for the NLTK package. While it’s a bit slow
    on large volumes of text, our text samples are fairly small, allowing the application
    to tag words without much processing power required. It’s possible to scale NLTK’s
    averaged perceptron tagger application to very large datasets using the big data
    technologies that we’ll consider in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll parse out nouns, verbs, personal pronouns, adverbs, adjectives, and prepositions
    and conjunctions and count the numbers of each category that exist in each text
    sample.
  prefs: []
  type: TYPE_NORMAL
- en: Nouns include singular, plural, common, and proper noun combinations. Verbs
    include all type and tense combinations. Personal pronouns include pronouns and
    possessive pronouns. Adverbs and adjectives include comparative and superlative
    forms of adverbs and adjectives. Prepositions and coordinating conjunctions are
    also tagged and counted.
  prefs: []
  type: TYPE_NORMAL
- en: Some other tagged parts of speech exist in the averaged perceptron tagger, and
    other taggers may include further divisions of parts of speech. If you want to
    explore how parts of speech can be used to distinguish text types, text authorship,
    or demographics of the text author, you may want to use another tagger or disaggregate
    nouns, verbs, and so on, from their individual components. However, this will
    result in more columns within your dataset, so we recommend you collect more samples
    if you’re doing that type of nuanced analysis of text attributes and parts-of-speech
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Normalizing Vectors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Because our text samples include some short samples and some long samples according
    to poem type, we’ll want to standardize the parts-of-speech counts before we work
    with the data. Our approach includes normalization of punctuation by token count
    (punctuation and word count totals) and normalization of parts-of-speech count
    by word count, summarized in this chapter’s files. This should give good enough
    features to demonstrate poem type differences and still allow for good dimensionality
    reduction results to visualize our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: For a more nuanced approach that parses out types of verbs, nouns, and so on,
    you could derive fractions of part of speech category or break down fractions
    within parts of speech to engineer more detailed features for your analysis. If
    you’re familiar with Python, we encourage you to play around with the NLP pipeline
    and customize your analyses for more insight into poem-type linguistic differences.
  prefs: []
  type: TYPE_NORMAL
- en: For now, let’s move onto the analysis in R.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the Poem Dataset in R
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ll start by loading the processed poem dataset and exploring the features
    we’ve derived using the code in [Listing 9-1](#listing9-1).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 9-1: A script that reads in the processed poem data'
  prefs: []
  type: TYPE_NORMAL
- en: Adverbs tend to be the least-represented features, accounting for only 0–10.5
    percent of words used in any given poem. Nouns tend to be the most frequent words
    appearing in this set of poems, accounting for 12.5–53.5 percent of words in a
    given poem. Personal pronouns are rare, with more than a quarter of the poems
    not containing a personal pronoun (likely due to the haiku, which tend not to
    use them). Punctuation usage varies quite a bit, from no representation in a haiku
    to nearly half of a free-verse poem being composed of punctuation (a list poem
    of medical diagnoses at a hospital). Given this variation, it’s likely we have
    good features to use in our analyses.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s set the seed for our analyses and visualize the parts-of-speech features
    with t-SNE, using the shape of each visualized point as a designation of the poem
    type; add the following code to [Listing 9-1](#listing9-1):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](image_fi/503083c09/f09001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9-1: A t-SNE plot of poem features by poem type, with type represented
    by either circles or triangles (`perplexity=5`)'
  prefs: []
  type: TYPE_NORMAL
- en: From the plot in [Figure 9-1](#figure9-1), we can see that our poems tend to
    separate out into clusters where most points are of the same type. This means
    a kernel-based model or nearest neighbor model is probably sufficient for classifying
    poem type by features.
  prefs: []
  type: TYPE_NORMAL
- en: Given some separation of points by features, it’s likely we can apply algorithms
    to cluster our data and use supervised learning to understand which differences
    exist between haiku-type poems and other poems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s divide our sample into training and test fractions and then apply a Euclidean-distance-based
    *k*-NN algorithm to classify poem types based on a poem’s five nearest neighbors
    by adding to our code so far:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Since our t-SNE plot ([Figure 9-1](#figure9-1)) suggests some separation and
    points generally near poems of a similar type, it’s likely that this model has
    worked well. Let’s examine the predicted and true labels for our test set under
    the *k*-NN model through this addition to our code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: It looks like this model correctly classifies all test poems, suggesting a high-quality
    model that can separate types of poems based on features.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll examine these potential type differences by feature further using a 10-fold
    cross-validated dgLARS model in an additional step to our code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Because we are working with a small dataset, it’s possible that one or more
    of your folds will have issues, giving a slightly different model than the results
    shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Your model should show differences in punctuation fraction and adverb fraction.
    Free-verse and formal-verse poems have higher rates of punctuation usage. Given
    that these types of poems are more likely to use full sentences rather than connected
    phrases, the differences in punctuation usage are consistent with expected differences.
  prefs: []
  type: TYPE_NORMAL
- en: The differences in adverb fractions aren’t as expected. However, adverb usage
    is linked to many different transient and fixed personality traits. It’s possible
    that haiku taps into a different transient mood or trait, creating style differences
    reflected in adverb usage. Subject matter may also influence this difference.
  prefs: []
  type: TYPE_NORMAL
- en: Importantly, this analysis shows that parts-of-speech and punctuation patterns
    vary within samples of writing by the same author in the same type of writing
    (poetry). Given that prior research suggests that some of these input features
    can be used to identify the likely author of a text, it may be prudent to rethink
    authorship prediction based on parts-of-speech analysis. Different subject matters,
    different types of writing, and different life stages of the author may influence
    word choice, sentence structure, and usage of punctuation.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see how the poems group together within each poem type, let’s visualize
    the persistence diagrams for each poem type through adding to our code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 9-2](#figure9-2) shows the persistence diagrams for haiku and non-haiku
    samples, highlighting some differences in poem clustering within type:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c09/f09002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9-2: Persistence diagrams by poem type'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in [Figure 9-2](#figure9-2), non-haiku poems tend to cluster more
    consistently, while haiku poems spread out without a lot of separation involving
    multiple poems grouped together. This suggests that haiku features vary from poem
    to poem, while non-haiku poems show more consistency in features. The spontaneity
    of the composition may result in a wider variety of poem structures found with
    haiku poems. Data related to spontaneousness of poem creation and time spent crafting
    the poem may shed light on conscious language usage differences between poems
    (such as haiku) that arise from a single moment and those that are created with
    more intent behind their creation.
  prefs: []
  type: TYPE_NORMAL
- en: Using Topology-Based NLP Tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: NLP as a field has evolved over the past years, and some common tools in NLP
    leverage topology to solve important problems. Early embedding tools tended to
    tally word frequencies within and across documents of interest to parse text data
    into numeric data that works well in machine learning algorithms. However, words
    often don’t contain all the semantic information needed to make sense of a sentence
    or paragraph or entire document. Negatives, such as *no* or *not*, modify actions
    or actors within a sentence. For instance, “she did let him in the house” is a
    very different statement semantically than “she did not let him in the house.”
    Depending on what you’re trying to predict or classify, simple mappings from individual
    words to a matrix of numbers don’t work well.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, a document or collection of documents might have 30,000+ words
    that occur at least once. Most common words will occur often with little value
    added by their presence. Important words might occur only once or twice. This
    leads us into the problem of dimensionality again when we try to sift through
    the data for important trends.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, recent years have seen some great developments in low-dimensional
    embeddings via topological mapping with special types of neural networks. *Pretrained
    transformer models* are neural networks that can pass information forward and
    backward through their hidden layers to obtain optimal topological mappings for
    text data. Pretrained transformer models learn low-dimensional embeddings of text
    data from massive training sets in the language or languages of interest. *BERT*
    (Bidirectional Encoder Representations from Transformers) and its sentence-based
    embedding cousin, *SBERT*, are two of the most common open source pretrained transformer
    models used to embed text data into lower-dimensional, dense matrices for use
    in machine learning tasks. BERT models can be extended to languages that are not
    currently supported, such as embeddings of text in Hausa or Lingala or Rushani;
    this has the potential to accelerate language translation services and preserve
    endangered languages. *GPT-3*, trained on a similar premise as BERT, has created
    accurate translations and chatbots that can parse meaning from input text rather
    than match keywords or eat up computing resources trying to process high-dimensional
    matrices within machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Using Python, we’ve built a BERT model on our poem set based on their serious
    tone or humorous tone to show how BERT embeddings can fit with our supervised
    learning tools in this book. (Note that the order of poems has changed from the
    original set with the data munging to get BERT embeddings and pass them back to
    a *.csv* file.) You can consult Python’s transformer package to learn more about
    how BERT models are imported and leveraged in text embedding or refer to the Python
    scripts for this chapter ([https://nostarch.com/download/ShapeofData_PythonCode.zip](https://nostarch.com/download/ShapeofData_PythonCode.zip)).
    However, we’ll stick to importing the results into R and visualizing the data
    in a smaller dimension, as we did with our poem linguistic features earlier in
    the chapter. Let’s create a t-SNE embedding and plot it with [Listing 9-2](#listing9-2).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 9-2: A script that loads BERT data for serious and humorous poems,
    embeds the data with t-SNE, and plots the results'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 9-3](#figure9-3) shows the embedding, which demonstrates that poems
    separate into clusters by poem type. This mirrors our haiku versus non-haiku poem
    results in [Figure 9-1](#figure9-1), where we saw features separating out by type
    in the t-SNE embedding. Again, this suggests that a machine learning classifier
    should work well with our dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c09/f09003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9-3: A plot of t-SNE embedding results, with serious or humorous types
    of poems denoted by circles and triangles, respectively (`perplexity=5`)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we know a machine learning model may do well at classifying this data,
    let’s fit a Lasso model with homotopy continuation to handle the small sample
    size by adding [Listing 9-3](#listing9-3):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 9-3: A script that loads BERT data for serious and humorous poems,
    splits the data into training and test sets, builds a Lasso model with homotopy
    continuation fitting, and shows prediction and actual values for test data'
  prefs: []
  type: TYPE_NORMAL
- en: The dataset has 384 components from the embedded BERT model and 25 poems. Your
    version of R may split the data and fit the model differently, but in our version
    of R, the test data has three serious poems followed by a humorous one followed
    by a serious one. The model predicts a serious poem, a serious one, a serious
    one, a humorous one, and a serious one (giving a model accuracy of 100 percent).
    Given how small this training set is compared to the 384 predictors fed into our
    model, this is great performance. Note that the selected features have little
    meaning semantically, as they are simply embeddings. Coupling topology-based methods
    into pipelines to process and model small datasets can provide decent prediction
    where other models will fail entirely.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we applied several of the methods overviewed in the book on
    a linguistics question involving a dataset of features derived from NLP-processed
    poems; we also embedded our data and created a model to predict tone differences
    in our poem set.
  prefs: []
  type: TYPE_NORMAL
- en: For the first problem, we reduced the dimensionality of the dataset to visualize
    group differences, applied two supervised learning models to understand classification
    accuracy and important features distinguishing the poetry types, and visualized
    topological features that exist in both sets of poems. This showed us that language
    usage, particularly punctuation, varies across poem types.
  prefs: []
  type: TYPE_NORMAL
- en: We then looked at context-aware embeddings and predicting the tone of our poem
    set using a topology-based embedding method and a topology-based classification
    model, which showed that we could get fairly accurate prediction building a model
    from 384 embedded components and a training set of 21 poems.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll wrap up the book by looking at ways to scale topological
    data analysis algorithms with distributed and quantum computing approaches.
  prefs: []
  type: TYPE_NORMAL
