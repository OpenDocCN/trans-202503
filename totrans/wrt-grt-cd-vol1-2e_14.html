<html><head></head><body>
<h2 class="h2" id="ch15"><span epub:type="pagebreak" id="page_413"/><strong><span class="big">15</span><br/>MISCELLANEOUS INPUT AND OUTPUT DEVICES</strong></h2>&#13;
<div class="image1"><img alt="Image" src="../images/comm1.jpg"/></div>&#13;
<p class="noindents">Although mass storage devices are, arguably, the most common peripheral in modern computer systems, there are many other widely used devices, such as communication ports (serial and parallel), keyboards and mice, and sound cards. These peripherals will be the focus of this chapter.</p>&#13;
<h3 class="h3" id="sec15_1"><strong>15.1 Exploring Specific PC Peripheral Devices</strong></h3>&#13;
<p class="noindent">In some respects, it’s dangerous to discuss real devices on modern PCs because the traditional (“legacy”) devices have all but disappeared from PC designs. As manufacturers introduce new PCs, they are removing many of the legacy, easy-to-program peripherals like parallel and serial ports, and replacing them with complex peripherals like USB and Thunderbolt. Although a detailed discussion on programming these newer peripheral <span epub:type="pagebreak" id="page_414"/>devices is beyond the scope of this book, you need to understand their behavior in order to write great code that accesses them.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>Because of the nature of the peripheral devices discussed in the rest of this chapter, the information presented applies only to IBM-compatible PCs. There simply isn’t enough space in this book to cover how particular I/O devices behave on different systems. Other systems support similar I/O devices, but their hardware interfaces may differ from what’s described here. Nevertheless, the general principles still apply.</em></p>&#13;
</div>&#13;
<h4 class="h4" id="sec15_1_1"><strong><em>15.1.1 The Keyboard</em></strong></h4>&#13;
<p class="noindent">The original IBM PC’s keyboard is a computer system in its own right. Buried inside the keyboard’s case is an 8042 microcontroller chip that constantly scans the switches on the keyboard to see if any keys are being pressed. This processing occurs in parallel with the normal activities of the PC, and even though the PC’s 80x86 is busy with other things, the keyboard never misses a keystroke.</p>&#13;
<p class="indent">A typical keystroke starts with the user pressing a key on the keyboard. This closes an electrical contact in a switch, which the keyboard’s microcontroller can sense. Unfortunately, mechanical switches do not always close perfectly clean. Often, the contacts bounce off one another several times before coming to rest with a solid connection. To a microcontroller chip that is reading the switch constantly, these bouncing contacts look like a very quick series of keypresses and releases. If the microcontroller registers these as multiple keystrokes, it can result in a phenomenon known as <em>keybounce</em>, a problem common to many cheap and old keyboards. Even on the most expensive and newest keyboards, keybounce can be a problem if you look at the switch a million times a second, because mechanical switches simply cannot settle down that quickly. A typical inexpensive key will settle down within 5 milliseconds, so if the keyboard-scanning software polls the key less often than this, the controller will effectively miss the keybounce. The practice of limiting how often the keyboard is scanned in order to eliminate keybounce is known as <em>debouncing</em>. Typical keyboard controllers scan the keyboard once every 10 to 25 milliseconds; any less than this may produce bouncy keys, and any more may result in lost keystrokes (by very fast typists).</p>&#13;
<p class="indent">The keyboard controller must not generate a new key code sequence every time it scans the keyboard and finds a key held down. The user may hold a key down for many tens or hundreds of milliseconds before releasing it, and we don’t want this to register as multiple keystrokes. Instead, the keyboard controller should generate a single key code value when the key goes from the up position to the down position (a <em>down key</em> operation). In addition, modern keyboards provide an <em>autorepeat</em> capability that engages once the user has held down a key for a given time period (usually about half a second), and it treats the held key as a sequence of keystrokes as long as the user continues to hold the key down. However, even these autorepeat keystrokes are regulated to allow only about 10 keystrokes per second rather than the number of times per second the keyboard controller scans all the switches on the keyboard.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_415"/>Upon detecting a down keystroke, the microcontroller sends a keyboard <em>scan code</em> to the PC. The scan code is <em>not</em> related to the ASCII code for that key; it is an arbitrary value IBM chose when the PC’s keyboard was first developed. The PC keyboard actually generates <em>two</em> scan codes for every key pressed. It generates a <em>down code</em> when a key is pressed down and an <em>up code</em> when the key is released. If the user holds the key down long enough for the autorepeat operation to begin, the keyboard controller sends a sequence of down codes until the key is released, at which point the keyboard controller sends a single up code.</p>&#13;
<p class="indent">The 8042 microcontroller chip transmits these scan codes to the PC, where they are processed by an <em>interrupt service routine (ISR)</em> for the keyboard. Having separate up and down codes is important because certain keys (like <small>SHIFT</small>, <small>CTRL</small>, and <small>ALT</small>) are meaningful only when held down. By generating up codes for all the keys, the keyboard ensures that the keyboard ISR knows which keys are pressed while the user is holding down one of these <em>modifier</em> keys. Exactly what the system does with these scan codes depends on the OS, but usually the OS’s keyboard device driver will translate the scan code sequence into an appropriate ASCII code or some other notation that applications can work with.</p>&#13;
<p class="indent">Today, almost all PC keyboards interface via the USB port, and they probably use a more modern microcontroller than the 8042 found in the original IBM PC keyboard, but otherwise their behavior is exactly the same.</p>&#13;
<h4 class="h4" id="sec15_1_2"><strong><em>15.1.2 The Standard PC Parallel Port</em></strong></h4>&#13;
<p class="noindent">The original IBM PC design provided support for three parallel printer ports (which IBM designated <em>LPT1:</em>, <em>LPT2:</em>, and <em>LPT3:</em>). With laser and inkjet printers still a few years in the future, IBM probably envisioned machines that could support a standard dot matrix printer, a daisy wheel printer, and maybe some other auxiliary type of printer for different purposes. IBM almost certainly didn’t anticipate the widespread use of parallel ports, or it probably would have designed them differently. At their prime, the PC’s parallel port controlled keyboards, disk drives, tape drives, SCSI adapters, Ethernet and other network adapters, joystick adapters, auxiliary keypad devices, other miscellaneous devices, and, oh yes, printers.</p>&#13;
<p class="indent">Today, the parallel port is largely absent in systems because of connector size and performance problems. Nevertheless, it remains an interesting device. It’s one of the few interfaces that hobbyists can use to connect the PC to simple devices they’ve built themselves. Therefore, learning to program the parallel port is a task many hardware enthusiasts have taken upon themselves.</p>&#13;
<p class="indent">In a unidirectional parallel communication system, there are two distinguished sites: the transmitting site and the receiving site. The transmitting site places its data on the data lines and informs the receiving site that data is available; the receiving site then reads the data lines and informs the transmitting site that it has taken the data. Note how the two sites synchronize their access to the data lines—the receiving site does not read the data lines until the transmitting site tells it to, and the transmitting site does not place a new value on the data lines until the receiving site removes the data <span epub:type="pagebreak" id="page_416"/>and tells the transmitting site that it has the data. In other words, this form of parallel communication between the printer and computer system relies on handshaking to coordinate the data transfer.</p>&#13;
<p class="indent">The PC’s parallel port implements handshaking using three control signals in addition to the eight data lines. The transmitting site uses the <em>strobe</em> (or data strobe) line to tell the receiving site that data is available. The receiving site uses the <em>acknowledge</em> line to tell the transmitting site that it has taken the data. A third handshaking line, <em>busy</em>, tells the transmitting site that the receiving site is busy so it should not attempt to send data yet. The busy signal differs from the acknowledge signal in that acknowledge tells the system that the receiving site has accepted the data <em>and processed it</em>, whereas busy communicates only that the receiving site can’t accept any new data yet—it does not imply that the last transmission has been processed (or even received).</p>&#13;
<p class="indent">In a typical data transmission session, the transmitting site:</p>&#13;
<ol>&#13;
<li class="noindent">Checks the busy line to see if the receiving site is busy. If the busy line is active, the transmitter waits in a loop until the busy line becomes inactive.</li>&#13;
<li class="noindent">Places its data on the data lines.</li>&#13;
<li class="noindent">Activates the strobe line.</li>&#13;
<li class="noindent">Waits in a loop for the acknowledge line to become active.</li>&#13;
<li class="noindent">Sets the strobe inactive.</li>&#13;
<li class="noindent">Waits in a loop for the receiving site to set the acknowledge line inactive, indicating that it recognizes that the strobe line is now inactive.</li>&#13;
<li class="noindent">Repeats steps 1 through 6 for each byte it must transmit.</li></ol>&#13;
<p class="indent">Meanwhile, the receiving site:</p>&#13;
<ol>&#13;
<li class="noindent">Sets the busy line inactive when it is ready to accept data.</li>&#13;
<li class="noindent">Waits in a loop until the strobe line becomes active.</li>&#13;
<li class="noindent">Reads the data from the data lines.</li>&#13;
<li class="noindent">Activates the acknowledge line.</li>&#13;
<li class="noindent">Waits in a loop until the strobe line goes inactive.</li>&#13;
<li class="noindent">Sets the busy line active (optional).</li>&#13;
<li class="noindent">Sets the acknowledge line inactive.</li>&#13;
<li class="noindent">Processes the data.</li>&#13;
<li class="noindent">Sets the busy line inactive (optional).</li>&#13;
<li class="noindent">Repeats steps 2 through 9 for each additional byte it receives.</li></ol>&#13;
<p class="indent">By carefully following these steps, the receiving and transmitting sites coordinate their actions so that the transmitting site doesn’t attempt to put several bytes on the data lines before the receiving site consumes them, and so the receiving site doesn’t attempt to read data that the transmitting site has not sent.</p>&#13;
<h4 class="h4" id="sec15_1_3"><span epub:type="pagebreak" id="page_417"/><strong><em>15.1.3 Serial Ports</em></strong></h4>&#13;
<p class="noindent">The RS-232 serial communication standard is probably the most popular serial communication scheme in the world. Although it suffers from many drawbacks (speed being the primary one), it is widely used, and there are thousands of devices you can connect to a PC using an RS-232 serial interface. Though many devices still use this standard, it is rapidly being eclipsed by USB (and today you can handle most RS-232 interfacing requirements by plugging a USB-to-RS232 cable into your PC).</p>&#13;
<p class="indent">The original PC system design supports concurrent use of up to four RS-232 compatible devices connected through the <em>COM1:</em>, <em>COM2:</em>, <em>COM3:</em>, and <em>COM4:</em> ports. To connect additional serial devices, you can buy interface cards that let you add 16 or more serial ports to the PC.</p>&#13;
<p class="indent">In the early days of the PC, DOS programmers had to directly access the 8250 serial communication controller (SCC) to implement RS-232 communications in their applications. A typical serial communications program would have a serial port ISR that read incoming data from the SCC and wrote outgoing data to the chip, as well as code to initialize the chip and to buffer incoming and outgoing data.</p>&#13;
<p class="indent">Fortunately, today’s application programmers rarely program the SCC directly. Instead, OSes such as Windows and Linux provide sophisticated serial communications device drivers that application programmers can call. These drivers provide a consistent feature set that all applications can use, and this reduces the learning curve needed to provide serial communication functionality. Another advantage to the OS device driver approach is that it removes the dependency on the 8250 SCC. Applications that use an OS device driver will automatically work with different SCCs. In contrast, an application that programs the 8250 directly won’t work on a system that uses a USB-to-RS232 converter cable. However, if the manufacturer of that converter cable provides an appropriate device driver for an OS, applications that do serial communications via that OS will automatically work with the USB/serial device.</p>&#13;
<p class="indent">An in-depth examination of RS-232 serial communications is beyond the scope of this book. For more information on this topic, consult your OS programmer’s guide or pick up one of the many excellent texts devoted specifically to this subject.</p>&#13;
<h3 class="h3" id="sec15_2"><strong>15.2 Mice, Trackpads, and Other Pointing Devices</strong></h3>&#13;
<p class="noindent">Along with disk drives, keyboards, and display devices, pointing devices are probably the most common peripherals you’ll find on modern PCs. Pointing devices are among the least complex peripheral devices, providing a very simple data stream to the computer. They come in two categories: those that return the relative position of the pointer and those that return the absolute position of the pointing device. The <em>relative position</em> is the change in position since the last time the system read the device; the <em>absolute position</em> is some set of coordinate values within a fixed coordinate system. Mice, trackpads, and <span epub:type="pagebreak" id="page_418"/>trackballs return relative coordinates; touch screens, light pens, pressure-sensitive tablets, and joysticks return absolute coordinates.</p>&#13;
<p class="indent">Generally, it’s easy to translate an absolute coordinate system to a relative one, but problematic to do the reverse. Converting a relative coordinate system to an absolute one requires a constant reference point that may become meaningless if, for example, someone lifts a mouse off the surface and sets it down elsewhere. Fortunately, most windowing systems work with relative coordinate values from pointing devices, so the limitations of pointing devices that return relative coordinates are not a problem.</p>&#13;
<p class="indent">Early mice were typically optomechanical devices that rotated two encoding wheels oriented along the x- and y-axes of the mouse body. Usually, both wheels were encoded to send 2-bit pulses whenever they moved a certain distance. One bit told the system that the wheel had moved a certain distance, and the other bit told the system which direction the wheel had moved.<sup><a href="footnotes.xhtml#fn15_1a" id="fn15_1">1</a></sup> By constantly tracking the 4 bits (2 bits for each axis) from the mouse, the computer system could determine the mouse’s distance and direction traveled, and keep a very accurate record of the mouse’s position in between application requests for that position.</p>&#13;
<p class="indent">One problem with having the CPU track each mouse movement is that, when moved quickly, mice can generate a constant and high-speed stream of data. If the system is busy with other computations, it might miss some of the incoming mouse data and therefore lose track of the mouse’s position. Furthermore, the host’s CPU time is better spent on application computations than tracking the mouse position.</p>&#13;
<p class="indent">As a result, mouse manufacturers decided early on to incorporate a simple microcontroller in the mouse package, to keep track of the physical mouse movements and respond to system requests for mouse coordinate updates, or at the very least generate interrupts on a periodic basis when the mouse position changes. Most modern mice connect to the system via the USB and respond with positional updates to system requests that occur about every 8 milliseconds.</p>&#13;
<p class="indent">Because of the wide acceptance of the mouse as a GUI pointing device, computer manufacturers have created many other devices that serve the same purpose but are more portable—mice aren’t the most convenient pointing devices to attach to a laptop computer on the road, for example. Trackballs, strain gauges (the little “stick” between the <em>G</em> and <em>H</em> keys on many laptops), trackpads, trackpoints, and touch screens are all examples of devices that manufacturers have attached to laptop computers, tablets, and PDAs to create more portable pointing devices. Though these devices vary in their convenience to the end user, to the OS they can all look like a mouse. So, from a software perspective, there’s little difference between them.</p>&#13;
<p class="indent">In modern OSes, the application rarely interfaces with a pointing device directly. Instead, the OS tracks the mouse position and updates cursors and other mouse effects in the system, then notifies the application when some sort of pointing device event (such as a button press) occurs. <span epub:type="pagebreak" id="page_419"/>In response to a query from an application, the OS returns the position of the system cursor and the state of the buttons on the pointing device.</p>&#13;
<h3 class="h3" id="sec15_3"><strong>15.3 Joysticks and Game Controllers</strong></h3>&#13;
<p class="noindent">The analog game adapter created for the IBM PC allowed users to connect up to four resistive potentiometers and four digital switch connections to the PC. The design of the PC’s game adapter was obviously influenced by the analog input capabilities of the Apple II computer, the most popular computer available at the time the PC was developed. IBM’s analog input design, like Apple’s, was designed to be dirt-cheap. Accuracy and performance were not a concern at all. In fact, you can purchase the electronic parts to build your own version of the game adapter, at retail, for less than $3.</p>&#13;
<p class="indent">Due to the inherent inefficiencies of reading the original electronics of the IBM PC game controller, most modern game controllers contain the analog electronics that convert physical position into a digital value directly inside the controller, and then interface to the system via USB. Microsoft Windows and other modern OSes provide a special game-controller device-driver interface and APIs that allow applications to determine what facilities the game controller has, and also send the data to those applications in a standardized form. This allows game-controller manufacturers to provide many special features that were not possible with the original PC game-controller interface. Modern applications read game-controller data just as though they were reading data from a file or some other character-oriented device like a keyboard. This vastly simplifies the programming of such devices while improving overall system performance.</p>&#13;
<p class="indent">Some “old-school” game programmers feel that calling APIs is inherently inefficient and that great code always controls the hardware directly. This thinking is a bit outdated, for a few reasons. First, most modern OSes don’t allow applications direct access to hardware even if the programmer wants it. Second, software that talks directly to the hardware won’t work with as wide a variety of devices as software that lets the OS handle the hardware. Finally, most OS device drivers can probably be written more efficiently by the manufacturer’s or OS developer’s programming team than by an individual.</p>&#13;
<p class="indent">Because newer game controllers are no longer constrained by the design of the original IBM PC game-controller card, they provide a wide range of capabilities. Refer to the relevant game controller and OS documentation for information on how to program the API for a specific device.</p>&#13;
<h3 class="h3" id="sec15_4"><strong>15.4 Sound Cards</strong></h3>&#13;
<p class="noindent">The original IBM PC included a built-in speaker that the CPU could program (using an onboard timer chip) to produce a single-frequency tone. Producing a wide range of sound effects was possible, but it required programming a single bit connected directly to the speaker. This process consumed nearly all the available CPU time. Within a couple of years of the PC’s arrival, various manufacturers like Creative Labs created a special <span epub:type="pagebreak" id="page_420"/>interface board—a sound card—that provided higher-quality PC audio output and didn’t consume anywhere near that amount of CPU resources.</p>&#13;
<p class="indent">The first sound cards to appear for the PC didn’t follow any standards because none existed at the time. Creative Labs’ Sound Blaster card became the de facto standard because it had reasonable capabilities and sold in very high volumes. At the time, there was no such thing as a device driver for sound cards, so most applications were programming the registers directly on the sound card. Initially, so many applications were written for the Sound Blaster card that anyone wanting to use most audio applications also had to purchase it. Other sound card manufacturers quickly copied the Sound Blaster design, and all of them were subsequently stuck with it, because any new features they added wouldn’t be supported by the available audio software.</p>&#13;
<p class="indent">Sound card technology stagnated until Microsoft introduced multimedia support into Windows. The original audio cards were capable of mediocre music synthesis, suitable only for cheesy sound effects for video games. Some boards supported 8-bit telephone-quality audio sampling, but the audio was definitely not high fidelity. Once Windows provided a standardized, device-independent interface for audio, the sound card manufacturers began producing high-quality sound cards for the PC.</p>&#13;
<p class="indent">Immediately, “CD-quality” cards appeared that were capable of recording and playing back audio at 44.1 KHz and 16 bits. Higher-quality sound cards began adding <em>wavetable</em> synthesis hardware that produced realistic synthesis of musical instruments. Synthesizer manufacturers like Roland and Yamaha produced sound cards with the same electronics found in their high-end synthesizers. Today, professional recording studios use PC-based digital audio recording systems to record original music with 24-bit resolution at 96 (or even 192) KHz, arguably producing better results than all but the finest analog recording systems. Of course, such systems cost many thousands of dollars. They’re definitely not your typical sound card that retails for under $100.</p>&#13;
<h4 class="h4" id="sec15_4_1"><strong><em>15.4.1 How Audio Interface Peripherals Produce Sound</em></strong></h4>&#13;
<p class="noindent">Modern audio interface peripherals<sup><a href="footnotes.xhtml#fn15_2a" id="fn15_2">2</a></sup> generally produce sound in one of three different ways: analog (FM synthesis), digital wavetable synthesis, or digital playback. The first two schemes produce musical tones and are the basis for most computer-based synthesizers, while the third is used to play back audio that was digitally recorded.</p>&#13;
<p class="indent">The FM synthesis scheme is an older, lower-cost, music synthesis mechanism that creates musical tones by controlling various oscillators and other sound-producing circuits on the sound card. The sound produced by such devices is usually very low quality, reminiscent of early video games; there’s no mistaking it for an actual musical instrument. While some very low-end <span epub:type="pagebreak" id="page_421"/>sound cards still use FM synthesis as their main sound-producing mechanism, few modern audio peripherals use it for anything other than producing intentionally “synthetic” sounds.</p>&#13;
<p class="indent">Modern sound cards that provide musical synthesis capabilities tend to use wavetable synthesis: the audio manufacturer typically records and digitizes several notes from an actual musical instrument, and then programs these digital recordings into read-only memory (ROM), which they assemble into the audio interface circuit. When an application requests that the audio interface play some note on a given musical instrument, the audio hardware plays back the recording from ROM, producing a very realistic sound.</p>&#13;
<p class="indent">However, wavetable synthesis is not simply a digital playback scheme. To record over 100 different instruments, each with a several octave range, would require a prohibitively expensive amount of ROM storage. Therefore, most manufacturers of such devices use software embedded on the audio interface card to raise or lower, by some integral number of octaves, a small number of digitized waveforms stored in ROM. This allows manufacturers to record and store only a single octave (12 notes) for each instrument. Some synthesizers use software to convert only a single recorded note into any other note, to reduce costs, but the more notes the manufacturer records, the better the quality of the resulting sound. Some of the higher-end audio boards record several octaves on complex musical instruments (like a piano) but only a few notes on some lesser-used, less complex sound-producing objects, like sound effects for gunshots, explosions, and crowd noise.</p>&#13;
<p class="indent">Finally, pure digital playback is used for two purposes: playing back arbitrary audio recordings and performing very high-end musical synthesis, known as <em>sampling</em>. A sampling synthesizer is, effectively, a RAM-based version of a wavetable synthesizer. Rather than storing digitized instruments in ROM, a sampling synthesizer stores them in system RAM. Whenever an application wants to play a given note from a musical instrument, the system fetches the recording for that note from system RAM and sends it to the audio circuitry for playback. Like wavetable synthesis methods, a sampling synthesizer can convert digitized notes up and down octaves, but because the system doesn’t have the cost-per-byte constraints associated with ROM, the audio manufacturer can usually record a wider range of samples from real-world musical instruments. Generally, sampling synthesizers provide a microphone input so you can create your own samples. This allows you, for example, to play a song by recording a barking dog and generating a couple octaves of “dog bark” notes on the synthesizer. Third parties often sell “sound fonts” containing high-quality samples of popular musical instruments.</p>&#13;
<p class="indent">The other use for pure digital playback is as a digital audio recorder. Almost every modern sound card has an audio input that will theoretically record “CD-quality” sound in stereo.<sup><a href="footnotes.xhtml#fn15_3a" id="fn15_3">3</a></sup> This allows the user to record an <span epub:type="pagebreak" id="page_422"/>analog signal and play it back verbatim, like a tape recorder. With sufficient outboard gear, it’s even possible to make your own musical recordings and burn your own music CDs, though to do so you’d want something a little bit fancier than a typical Sound Blaster card—something at least as advanced as the DigiDesign ProTools HDX or M-Audio system.</p>&#13;
<h4 class="h4" id="sec15_4_2"><strong><em>15.4.2 The Audio and MIDI File Formats</em></strong></h4>&#13;
<p class="noindent">There are two standard mechanisms for playing back sound in a modern PC: audio file playback and MIDI file playback.</p>&#13;
<p class="indent">Audio files contain digitized samples of the sound to play back. While there are many different audio file formats (for example, WAV and AIF), the basic idea is the same—the file contains some header information that specifies the recording format (such as 16-bit 44.1 KHz, or 8-bit 22 KHz) and the number of samples, followed by the actual sound samples. Some of the simpler file formats allow you to dump the data directly to a typical sound card after proper initialization of the card; other formats may require a minor data translation before the sound card can process the data. In either case, the audio file format is essentially a hardware-independent version of the data you’d normally feed to a generic sound card.</p>&#13;
<p class="indent">One problem with sound files is that they can grow rather large. One minute of stereo CD-quality audio requires just less than 10MB of storage. A typical 3- to 4-minute song requires between 20MB and 45MB. Not only does such a file take up an inordinate amount of RAM, but it consumes a fair amount of storage in the software’s distribution file as well. If you’re playing back a unique audio sequence that you’ve recorded, you have no choice but to use this space to hold the sequence. However, if you’re playing back an audio sequence that consists of a series of repeated sounds, you can use the same technique that sampling synthesizers use and store only one instance of each sound, and then use some sort of index value to indicate which sound you want to play. This can dramatically reduce the size of a music file.</p>&#13;
<p class="indent">This is exactly the idea behind the <em>Musical Instrument Digital Interface (MIDI)</em> file format. MIDI is a standard protocol for controlling music synthesis and other equipment. If you want to play back music that doesn’t contain vocals or other nonmusical elements, MIDI can be very efficient.</p>&#13;
<p class="indent">Rather than holding audio samples, a MIDI file simply specifies the musical notes to play, when to play them, how long to play them, which instrument to play them on, and so on. Because it takes only a few bytes to specify all this information, a MIDI file can represent an entire song very compactly. High-quality MIDI files generally range from about 20KB to 100KB for a typical 3- to 4-minute song. Contrast this with the 20MB to 45MB for an audio file of the same length. Most sound cards today are capable of playing back <em>General MIDI (GM)</em> files using an on-board wavetable synthesizer or FM synthesis. Most synthesizer manufacturers use the GM standard to control their equipment, so its use is very widespread and GM files are easy to obtain.</p>&#13;
<p class="indent">One problem with MIDI is that the quality of the playback is dependent upon the quality of the end user’s sound card. Some of the more expensive <span epub:type="pagebreak" id="page_423"/>audio boards do a very good job of playing back MIDI files, but some of the lower-cost boards—including, unfortunately, a large number of systems that have the audio interface built into the motherboard—produce cartoonish-sounding playback.</p>&#13;
<p class="indent">Therefore, you need to carefully consider using MIDI in your applications. On the one hand, MIDI offers the advantages of smaller files and faster processing. On the other hand, on some systems the audio quality will be quite low, making your application sound bad. You have to balance the pros and cons of these approaches for your particular application.</p>&#13;
<p class="indent">Because most modern sound cards are capable of playing back CD-quality recordings, you might wonder why the manufacturers don’t collect a bunch of samples and simulate one of these sampling synthesizers. Well, they do. Roland, for example, provides the Virtual Sound Canvas program, which simulates its hardware Sound Canvas module in software. These virtual synthesizers produce very high-quality output, but consume a large percentage of the CPU’s capability, thus leaving less power for your applications. If your applications don’t need the full power of the CPU, these virtual synthesizers provide a very high-quality, low-cost solution.</p>&#13;
<p class="indent">If you know your target audience will have a synthesizer, another solution is to connect an outboard synthesizer module to your PC via a MIDI interface port and send the MIDI data to a synthesizer to play. This is an acceptable solution for a specialized application with a limited customer base, since few people outside of musicians would own a synthesizer.</p>&#13;
<h4 class="h4" id="sec15_4_3"><strong><em>15.4.3 Programming Audio Devices</em></strong></h4>&#13;
<p class="noindent">One of the best aspects of audio in modern applications is that there’s been a tremendous amount of standardization. File formats and audio hardware interfaces are very easy to use in modern applications. As with most other peripherals, few modern programs control audio hardware directly, because OSes like Windows and Linux provide device drivers that handle it for you. Producing sound in a typical Windows application requires little more than reading data from a file that contains the sound information and writing that data to another file used by the device driver, which interfaces with the actual audio hardware.</p>&#13;
<p class="indent">One other issue to consider when writing audio-based software is the availability of multimedia extensions in the CPU you’re using. The Pentium and later 80x86 CPUs provide the MMX, SSE, and AVX instruction sets. Other CPU families provide comparable instruction set extensions (such as the AltiVec instructions on the PowerPC or NEON on ARM). Although the OS probably uses these extended instructions in the device driver, you can employ them in your own applications as well. Unfortunately, that usually involves assembly language programming, because few high-level languages provide efficient access to them. Therefore, if you’re going to be doing high-performance multimedia programming, assembly language is something you’ll probably want to learn. See <em>The Art of Assembly Language</em> for additional details on the Pentium’s SSE/AVX instruction set.</p>&#13;
<h3 class="h3" id="sec15_5"><span epub:type="pagebreak" id="page_424"/><strong>15.5 For More Information</strong></h3>&#13;
<p class="ref">Axelson, Jan. <em>Parallel Port Complete: Programming, Interfacing, &amp; Using the PC’s Parallel Printer Port</em>. Madison, WI: Lakeview Publishing, 2000.</p>&#13;
<p class="ref">———. <em>Serial Port Complete: Programming and Circuits for RS-232 and RS-485 Links and Networks</em>. Madison, WI: Lakeview Publishing, 2000.</p>&#13;
<p class="ref">Hyde, Randall. <em>The Art of Assembly Language</em>. 2nd ed. San Francisco: No Starch Press, 2010.</p>&#13;
</body></html>