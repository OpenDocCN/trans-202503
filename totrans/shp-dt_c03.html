<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" epub:prefix="index: http://www.index.com/" lang="en" xml:lang="en">
<head>
<title>Chapter 3: Network Analysis</title>
<link href="NSTemplate_v1.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:ea5eeba6-9dea-4463-bfe4-b91d6c6b5861" name="Adept.expected.resource"/>
</head>
<body epub:type="bodymatter chapter">
<section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" id="Page_55" title="55"/><a class="XrefDestination" id="3"/><span class="XrefDestination" id="xref-503083c03-001"/>3</span><br/>
<span class="ChapterTitle"><a class="XrefDestination" id="NetworkAnalysis"/><span class="XrefDestination" id="xref-503083c03-002"/>Network Analysis</span></h1>
</header>
<figure class="opener">
<img alt="" src="image_fi/book_art/chapterart.png"/>
</figure>
<p class="ChapterIntro">In <span class="xref" itemid="xref_target_Chapter 2"><a href="c02.xhtml">Chapter 2</a></span>, we considered a few network geometry metrics; in this chapter, we’ll use them. First, we’ll explain how vertex metrics allow you to do supervised learning within a network, that is, predicting values associated to vertices and predicting new edges in the network; we’ll also look at how vertex metrics enable you to cluster vertices in a network. We’ll then discuss a few clustering algorithms that operate directly within the geometry of the network. Next, we’ll explain how to use global network metrics to do machine learning and statistical analyses on datasets that consist of collections of networks. We’ll then explore a network variant of the susceptible, infected, and recovered (SIR) model from epidemiology. With this model, we can see how entities (from diseases to misinformation) spread through networks and how network geometry influences this spread. Finally, we’ll examine how we can use vertex metrics to devise targeted strategies for disrupting this spread.</p>
<h2 id="h1-503083c03-0001"><span epub:type="pagebreak" id="Page_56" title="56"/><a class="XrefDestination" id="UnderstandingSupervisedLearninginaNetwork"/><span class="XrefDestination" id="xref-503083c03-003"/>Using Network Data for Supervised Learning</h2>
<p class="BodyFirst">Network data is often accompanied by a traditional structured dataset where the rows are identified with the vertices in the network. For instance, you might have a social network dataset that consists of a list of individuals (the vertices), the friendships between them (edges in the network), and one or more numerical or categorical columns providing additional non-network information about each individual—such as age, gender, or salary. We might want to consider this as a supervised learning problem, where we train a machine learning algorithm to predict one of the data columns. The way to do this is to use vertex metrics as independent variables (along with any of the other data columns), which lets the algorithm incorporate the network role of each vertex when making predictions. Let’s try this.</p>
<h3 id="h2-503083c03-0001"><a class="XrefDestination" id="PredictingDiaryMentions"/><span class="XrefDestination" id="xref-503083c03-004"/>Making Predictions with Social Media Network Metrics</h3>
<p class="BodyFirst">Let’s return to Farrelly’s social network, which we analyzed in the previous chapter to illustrate the different measures of vertex centrality. How might bridges between different parts of the network or hubs tightly connecting a few medical school friends influence how often Farrelly mentioned those individuals in her diary during the first term of medical school? Let’s take a look at how network metrics relate to diary mentions in this social network.</p>
<p>We’ll choose a few vertex centrality metrics and attach Farrelly’s diary data as a dependent variable in the set. Let’s load the data and examine the distribution of our dependent variable with <a href="#listing3-1" id="listinganchor3-1">Listing 3-1</a>.</p>
<pre><code>#import dataset
g&lt;-read.csv("SocialNetworkModel.csv")

#create a histogram of the data
hist(g$Diary.Entries,main="Diary Entry Histogram",xlab="Diary Entries")</code></pre>
<p class="CodeListingCaption"><a id="listing3-1">Listing 3-1</a>: A script that imports the relevant <em>.csv</em> file for further analysis</p>
<p><a href="#figure3-1" id="figureanchor3-1">Figure 3-1</a> shows a histogram of the dependent variable’s distribution (Poisson).</p>
<figure>
<img alt="" class="" src="image_fi/503083c03/f03001.png"/>
<figcaption><p><a id="figure3-1">Figure 3-1</a>: A histogram of diary entry data for Farrelly’s social network</p></figcaption>
</figure>
<p><span epub:type="pagebreak" id="Page_57" title="57"/>Generally, count variables, such as those conforming to the Poisson distribution, include a lot of zero and near-zero values, along with some large values. We can see from <a href="#figure3-1">Figure 3-1</a> that this outcome is Poisson distributed. Poisson-distributed variables can pose issues to machine learning algorithms, as they involve a lot of zero values and some outliers, such as the diary entries involving V10. This suggests that a generalized linear model (Poisson regression) is probably more appropriate of a supervised learning model than other machine learning algorithms. It looks like most individuals in the network receive few (if any) mentions over the term. However, a few outliers exist, including Farrelly and her closest friends within the network (V3, V10, and V14). Let’s dive deeper to see how centrality measures predict diary mentions. In <a href="#listing3-2" id="listinganchor3-2">Listing 3-2</a>, we sample Farrelly’s social network metrics of interest and use them as independent variables in our Poisson regression model.</p>
<pre><code>#create a training sample from Farrelly's social network metrics
n&lt;-dim(g)[1]
set.seed(10)
train.index=sample(1:n,15)
train&lt;-g[train.index,]

#build a Poisson regression model
gl&lt;-glm(Diary.Entries~.,data=train,family="poisson")

#examine performance with a model summary and Chi-squared test
summary(gl)

1-pchisq(summary(gl)$deviance,summary(gl)$df[2])</code></pre>
<p class="CodeListingCaption"><a id="listing3-2">Listing 3-2</a>: A script that computes a Poisson regression and analyzes its results</p>
<p>Your results may vary depending on your R version’s seeding, but in the samples we modeled with this dataset and other seeds (9 of 10 random splits), the summary functions show that betweenness centrality seems to have large coefficient values in the model and be the most consistent predictor of diary entry mentions across subsets of Farrelly’s social network data modeled with our regression function. The Chi-squared test values in our samples ranged from <em>p</em> &lt; 0.01 to <em>p</em> = 0.25. When we examine the plots associated with the linear regression, we can see that most of our sample fits the regression equation well. <a href="#figure3-2" id="figureanchor3-2">Figure 3-2</a> shows two of the plots generated by <a href="#listing3-2">Listing 3-2</a> (including V3, V7, and V13 as outliers).</p>
<p>The small sample size likely contributes to the variation between samples, but overall, we have a good predictive model. Indeed, this reflects Farrelly’s own intuition that the bridges of her network tended to coordinate memorable events and activities that brought together various pieces of her network that term. We’ll return to the instability of regression models on small sample sizes with outliers in <span class="xref" itemid="xref_target_Chapter 6"><a href="c06.xhtml">Chapter 6</a></span> and <span class="xref" itemid="xref_target_Chapter 8"><a href="c08.xhtml">Chapter 8</a></span>, along with more stable models you can use for these situations to get consistent model results.</p>
<span epub:type="pagebreak" id="Page_58" title="58"/><figure>
<img alt="" class="" src="image_fi/503083c03/f03002.png"/>
<figcaption><p><a id="figure3-2">Figure 3-2</a>: Residual and quantile plots of the Poisson regression run in <a href="#listing3-2">Listing 3-2</a></p></figcaption>
</figure>
<p>Bigger networks with dependent variables more closely tied to one’s social network (such as strength of political views, workout habits, and so on) tend to work better in this sort of analysis. Within biological networks, centrality measures might be used to predict disease severity, likelihood of response to a drug undergoing clinical trials, or disease risk at six months after a social-network-based behavioral health intervention. Analyzing the geometry of the network often produces useful independent variables for predicting some quality associated with the vertices of the network.</p>
<h3 id="h2-503083c03-0002"><a class="XrefDestination" id="PredictingLinks"/><span class="XrefDestination" id="xref-503083c03-005"/>Predicting Network Links in Social Media</h3>
<p class="BodyFirst">Another important form of supervised learning in a network is <em>link prediction</em>, in which potential new edges are inferred from a network’s structure or metadata. One way to predict links is to use prior growth patterns of a network to predict which edges are most likely to appear next. This has many real-world applications, some of the most notable being in social media. Whenever Facebook or another platform suggests a person for you to friend, an algorithm has run a link prediction on its network of users behind the scenes and given the missing edge between you and this potential friend a high score. There are many sophisticated methods for performing link prediction, but a common general strategy is to translate the problem to a traditional Euclidean supervised learning task. Let’s explore this conceptually using Farrelly’s social network as an example.</p>
<p>Imagine this network evolving over time, with a binary indicator denoting edges that formed since the last time period. Wouldn’t it be neat if we could predict edge formation over a time period based on what the network looked like geometrically in the past time period? Or if we could use vertex labels (such as class schedule or volunteer days) to predict edge formation in the next time period?</p>
<p>For the independent variables, we can use any collection of features associated with the two vertices. These can be intrinsic network-based features or extrinsic features such as user demographics in a social network. The network-based predictors come in two flavors. We can use vertex <span epub:type="pagebreak" id="Page_59" title="59"/>metrics by choosing a function to aggregate the two vertex scores in each pair to a single number (common choices for this include sum, max, mean, and absolute value of difference). For instance, we could compute the PageRank score for the two vertices in each vertex pair and then take the average of these two scores to assign the vertex pair.</p>
<p>The other flavor of network-based features uses some measure of the network relationship between the two vertices; the most natural choice here is simply the network distance between the two vertices, though you can try other options such as the number of shortest paths between the two vertices or the average time a random walk takes to get from one to the other. All these network-based predictors should be computed for the current version of the network rather than the earlier snapshot, and these network-based predictors can be combined with any collection of non-network features. (In practice, most non-network features are attached to individual vertices rather than pairs, so once again you’ll have to aggregate them to get a single score for each vertex pair.)</p>
<p>Once the independent predictors are computed for a time period of interest and an indicator variable exists for that time period, a supervised classifier can be used to predict edge formation over the time periods of interest. The higher the likelihood score for edge formation, the more likely that relationship will exist by the next time period. In Farrelly’s social network, betweenness centrality would likely be the main network-based predictor, as well as social activity data or diary entries from the first term of medical school. Everyone in her original social network was connected to her (and most to each other) by the end of that term.</p>
<h2 id="h1-503083c03-0002"><a class="XrefDestination" id="UnderstandingUnsupervisedLearninginaNetwork"/><span class="XrefDestination" id="xref-503083c03-006"/>Using Network Data for Unsupervised Learning</h2>
<p class="BodyFirst">Just as we can use the vertex metrics from <span class="xref" itemid="xref_target_Chapter 2"><a href="c02.xhtml">Chapter 2</a></span> as predictors in a supervised learning task, we can also use them as features in unsupervised learning tasks. In the case of clustering, this will partition the vertices into sets of those with similar functions in the network (hubs, bridges, and so on). Clustering vertices is known in the network sciences as <em>community mining</em>, so when using vertex metrics for this purpose, we obtain communities defined by the structural role they play in the network.</p>
<h3 id="h2-503083c03-0003"><a class="XrefDestination" id="ClusteringUsingVertexMetrics"/><span class="XrefDestination" id="xref-503083c03-007"/>Applying Clustering to the Social Media Dataset</h3>
<p class="BodyFirst">In <a href="#listing3-3" id="listinganchor3-3">Listing 3-3</a>, we apply k-means clustering to Farrelly’s social network dataset that was our main running example in <span class="xref" itemid="xref_target_Chapter 2"><a href="c02.xhtml">Chapter 2</a></span>.</p>
<pre><code>#rescale the matrix of vertex metrics and apply k-means with k=3
clust&lt;-kmeans(scale(vertdata),3)

#plot the network with clusters represented by vertex color and label
plot(g,vertex.size=6,vertex.color=clust$cluster,vertex.label=clust$cluster)</code></pre>
<p class="CodeListingCaption"><a id="listing3-3">Listing 3-3</a>: A script that uses k-means on the vertex metrics to cluster the vertices from the network in <a href="#figure3-1">Figure 3-1</a> into <em>k</em> = 3 groups</p>
<p><span epub:type="pagebreak" id="Page_60" title="60"/>Using k-means with <em>k</em> = 3 and PageRank, degree, hub centrality, betweenness, and transitivity as our features (which we first rescale), we get clusters with the means and sizes in <a href="#table3-1" id="tableanchor3-1">Table 3-1</a>, where they were computed from the original prescaled values.</p>
<figure>
<figcaption class="TableTitle"><p><a id="table3-1">Table 3-1</a>: Cluster Means and Sizes for k-Means Clustering (with <em>k</em> = 3) Run on a Handful of Vertex Metrics for Farrelly’s Social Network</p></figcaption>
<table border="1" id="table-503083c03-0001">
<thead>
<tr>
<td><b>Cluster</b></td>
<td><b>PageRank</b></td>
<td><b>Degree</b></td>
<td><b>Hub score</b></td>
<td><b>Betweenness</b></td>
<td><b>Transitivity</b></td>
<td><b>Cluster size</b></td>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>0.05</td>
<td>3.14</td>
<td>0.54</td>
<td>3.21</td>
<td>0.80</td>
<td>7</td>
</tr>
<tr>
<td>2</td>
<td>0.11</td>
<td>8.00</td>
<td>1.00</td>
<td>100.50</td>
<td>0.25</td>
<td>1</td>
</tr>
<tr>
<td>3</td>
<td>0.05</td>
<td>2.50</td>
<td>0.08</td>
<td>25.75</td>
<td>0.097</td>
<td>12</td>
</tr>
</tbody>
</table>
</figure>
<p>Since k-means involves a random initiation, you might get different clustering results each time you try this. For this particular clustering, we see that one vertex has been assigned its own cluster (Farrelly’s vertex, V7); all of this vertex’s scores other than transitivity are exceptionally high, so it is an outlier in many metrics. All the remaining vertices are split between the other two clusters, which seem mostly distinguished by the fact that one cluster has higher hub and transitivity scores, while the other cluster has higher betweenness. Let’s plot this network with the vertices labeled by cluster (doing so is an easy adaptation of the code in <a href="#listing3-3">Listing 3-3</a>); see <a href="#figure3-3" id="figureanchor3-3">Figure 3-3</a>.</p>
<figure>
<img alt="" class="" src="image_fi/503083c03/f03003.png"/>
<figcaption><p><a id="figure3-3">Figure 3-3</a>: Farrelly’s social network colored and labeled by cluster for the k-means clusters summarized in <a href="#table3-1">Table 3-1</a></p></figcaption>
</figure>
<p>We see that Farrelly is indeed her own cluster, and rather remarkably, cluster 1 is almost precisely the remaining medical school individuals, while cluster 2 is the veterans group individuals. Interestingly, one medical school person has been placed in cluster 3, because they are isolated (have a transitivity score of zero) and are not part of the main hub of medical school individuals.</p>
<h3 id="h2-503083c03-0004"><span epub:type="pagebreak" id="Page_61" title="61"/><a class="XrefDestination" id="ClusteringUsingtheGeometryoftheNetwork"/><span class="XrefDestination" id="xref-503083c03-008"/>Community Mining in a Network</h3>
<p class="BodyFirst">So far, we’ve used the geometry of the network to extract features and then run traditional Euclidean machine learning clustering algorithms on them. Another approach to clustering vertices in a network (called <em>community mining</em>) is to rely directly on the geometry of the network. We’ll briefly walk through several ways to do this.</p>
<h4 id="h3-503083c03-0001"><a class="XrefDestination" id="UsingtheWalktrapAlgorithm"/><span class="XrefDestination" id="xref-503083c03-009"/>Exploring Networks with Random Walks</h4>
<p class="BodyFirst">The <em>walktrap algorithm</em> uses random walks to explore the network and find communities in which the random walk gets “trapped.” If a random walk frequently stays within a certain set of vertices, then that set is a good candidate for a cluster. For instance, in Farrelly’s social network, a random walk starting among the medical school individuals has a high likelihood of staying with them for many steps because the only way out is across the bridge from Farrelly’s vertex. Indeed, to get out, the random walk would have to be at Farrelly’s vertex, and we would then choose that bridge as the next step (which happens with only one out of eight probability from that vertex because that vertex has eight edges attached to it). Similarly, a random walk starting among the veterans group individuals has a high likelihood of staying among them. In this way, the walktrap algorithm is good at finding communities that are separated by bridges. One downside with this approach is that it is computationally intensive to explore a large network this way.</p>
<h4 id="h3-503083c03-0002"><a class="XrefDestination" id="UnderstandingModularityandGreedyClustering"/><span class="XrefDestination" id="xref-503083c03-010"/>Evaluating a Cluster’s Quality Outcome</h4>
<p class="BodyFirst">In traditional Euclidean clustering, a typical way to evaluate the quality of a clustering outcome is to compare the distances within each cluster to the distances between the different clusters. There is a widely used network variant of this idea that applies to vertex clustering: the <em>modularity</em> of a vertex clustering is the probability that a randomly chosen edge is attached to two vertices in the same cluster minus the probability that this would occur if the edges in the network were randomly distributed. Intuitively, this compares the number of intracommunity edges to the number of intercommunity edges. It is sort of like a “lift” measure, because it compares how much better (in the sense of edges staying within clusters) the clustering is than a randomized benchmark. One of the important small-world properties of the Watts–Strogatz networks introduced in the preceding chapter is that they tend to be highly modular (clusters with high modularity scores).</p>
<p>Modularity also enables you to view vertex clustering as an optimization problem: we find the cluster division that maximizes the modularity score. It’s not practical to try all possible divisions into clusters, so various algorithms have been introduced that attempt to search for high-modularity clusterings without being guaranteed to find the global optimum. Two of the popular approaches for this are <em>greedy algorithms</em>, where greedy means that at each step they go in the direction that most increases the modularity, rather than taking suboptimal steps in the short run in the hopes that they lead to greater values in the long run.</p>
<p><span epub:type="pagebreak" id="Page_62" title="62"/><em>Louvain clustering</em> is one such greedy algorithm. It starts by treating each vertex as its own cluster and then iteratively merges neighboring clusters whenever doing so increases the modularity. Once this iterative local optimization process terminates, the algorithm creates a new, smaller network by merging all the vertices that have been assigned to the same cluster. This yields a network in which each vertex is its own cluster, so the same iterative local optimization process can be run again on this smaller graph. This algorithm is quite fast in practice and tends to perform well, but it often struggles to find smaller communities within large networks. There is a faster greedy algorithm that is often used, conveniently called <em>fast greedy clustering</em>, but it tends not to reach as high modularity scores.</p>
<h4 id="h3-503083c03-0003"><a class="XrefDestination" id="UnderstandingSpinglassClustering"/><span class="XrefDestination" id="xref-503083c03-011"/>Understanding Spinglass Clustering</h4>
<p class="BodyFirst">Another approach to clustering doesn’t involve random walks or local optimization. Rather, it draws from statistical mechanics, a branch of physics dealing with particle interactions. <em>Spinglass algorithms</em> are based on magnetic couplings within a system of particles (positive and negative charges); they seek to optimize how the charges are aligned across the system. This can be applied to vertex clustering, called <em>spinglass clustering</em>. The basic idea is to define an energy associated to clusterings and then try to minimize this energy. This energy minimization process is usually done by <em>simulated annealing</em>, which is an algorithmic approach to optimization that also has roots in statistical mechanics. In simulated annealing, rather than always moving in the direction that decreases the energy the most (as would be done in a greedy algorithm that runs the risk of getting stuck in local optima), there is a temperature parameter that determines the probability of moving instead in a “wrong” direction. As the algorithm proceeds, the temperature is steadily lowered. This helps the algorithm explore large portions of the energy landscape early on before settling down and honing in on a particular solution. It mimics the cooling process in metallurgy in which metal purifies.</p>
<h4 id="h3-503083c03-0004"><a class="XrefDestination" id="RunningOurClusteringAlgorithms"/><span class="XrefDestination" id="xref-503083c03-012"/>Running the Clustering Algorithms on a Social Network</h4>
<p class="BodyFirst">Let’s try running these four vertex clustering algorithms on Farrelly’s social network. <a href="#listing3-4" id="listinganchor3-4">Listing 3-4</a> does this and then plots the results and computes the modularity scores (to run this, make sure you’ve already loaded this network data as in <a href="c02.xhtml#listing2-4" id="listinganchor2-4">Listing 2-4</a>).</p>
<pre><code>#run walktrap, louvain, fast greedy, and spinglass clustering algorithms
cw&lt;-cluster_walktrap(g_social)
modularity(cw) #0.505
plot(cw,g_social,vertex.size=15,vertex.label.cex=0.6,main="Walktrap")

lo&lt;-cluster_louvain(g_social)
modularity(lo) #0.476
plot(lo,g_social,vertex.size=15,vertex.label.cex=0.6,main="Louvain")

fg&lt;-cluster_fast_greedy(g_social)
<span epub:type="pagebreak" id="Page_63" title="63"/>modularity(fg) #0.467
plot(fg,g_social,vertex.size=15,vertex.label.cex=0.6,main="Fast Greedy")

sg&lt;-cluster_spinglass(g_social)
modularity(sg) #0.505
plot(sg,g_social,vertex.size=15,vertex.label.cex=0.6,main="Spinglass")</code></pre>
<p class="CodeListingCaption"><a id="listing3-4">Listing 3-4</a>: A script that runs the four vertex clustering algorithms discussed earlier on Farrelly’s social network, plots the results, and computes the modularity score for each</p>
<p><a href="#figure3-4" id="figureanchor3-4">Figure 3-4</a> shows the resulting plots.</p>
<figure>
<img alt="" class="" src="image_fi/503083c03/f03004_m.png"/>
<figcaption><p><a id="figure3-4">Figure 3-4</a>: Clustering on Farrelly’s social network provided by four different algorithms</p></figcaption>
</figure>
<p><span epub:type="pagebreak" id="Page_64" title="64"/>Note that for all these functions the number of clusters was not specified by the user as it is for k-means. These algorithms determine the number of clusters as part of their search for optimality. In this example, walktrap and spinglass found the same three clusters, which are the medical school individuals (including Farrelly’s vertex V7) and a division of the veterans group individuals into two parts. This three-way clustering yields the highest modularity score among the solutions found by these algorithms.</p>
<p>Louvain found the next best score, and the result is similar to the previous one except that it splits the veterans group in a slightly different way (reassigning two vertices from one cluster to the other). With a modularity score just slightly below this, the fast greedy algorithm ended up with only two clusters (the medical school community with Farrelly in it and the veterans group community). Evidently, the greediness in this algorithm prevented it from finding that a higher modularity could be achieved by splitting the veterans group community. That said, this two-cluster solution found by the fast greedy algorithm describes the original context of the data, in which Farrelly combined her two separate communities.</p>
<p>If you are interested, you can explore a few other vertex clustering algorithms implemented in igraph. For instance, the function <code>cluster_edge_betweenness()</code> uses the betweenness metric not as a feature but in a more direct way. Vertices with a high betweenness score are considered to be <em>bridges</em>, and the communities this function uncovers are the ones that are separated by these bridges. Another interesting approach is provided by the function <code>cluster_infomap()</code>, which uses information theory to find communities in which information flows readily; this can also be interpreted in terms of the behavior of random walks on the network.</p>
<p>So far, we’ve discussed supervised and unsupervised learning among the vertices within a single network. Let’s now consider situations in which we are comparing the networks themselves.</p>
<h2 id="h1-503083c03-0003"><a class="XrefDestination" id="ComparingNetworks"/><span class="XrefDestination" id="xref-503083c03-013"/>Comparing Networks</h2>
<p class="BodyFirst">Sometimes a network isn’t your entire dataset. It’s just a single instance in a dataset comprising many networks. For instance, detecting bot accounts on social media platforms usually involves supervised classification in which the friend or follower networks of real users are compared to those of fake users. In the Pennsylvania gerrymandering case mentioned in <span class="xref" itemid="xref_target_Chapter 2"><a href="c02.xhtml">Chapter 2</a></span>, districting maps were converted to networks, and the old map was shown to be a dubious outlier in the distribution of networks. Neuroscience provides another important example where one needs to compare networks. Indeed, it’s common to translate functional magnetic resonance imaging (fMRI) and positron emission tomography (PET) data into a network structure in which the vertices represent different regions of the brain and in which edges are based on activity patterns (sequential activation of an area, for <span epub:type="pagebreak" id="Page_65" title="65"/>instance, or coactivation of multiple regions during one task). One often needs to compare two different groups of patients—either healthy patients against a group of patients with a particular neurological or psychological disorder or two different disease groups. Translated to network data science, this means we’re looking at a two-class dataset of networks to see if there are statistically significant differences between the two classes (to understand structural differences). We might also want to train a supervised classifier to predict the class based on the network structure.</p>
<p>To generate some synthetic data, let’s create 100 networks of each of the types described at the end of <span class="xref" itemid="xref_target_Chapter 2"><a href="c02.xhtml">Chapter 2</a></span>: Erdös–Renyi, scale-free, and Watts–Strogatz. In <a href="#listing3-5" id="listinganchor3-5">Listing 3-5</a>, we do this and plot a histogram of the network diameter to see how it varies within and across the different types of networks.</p>
<pre><code>#initiate vectors/lists
n&lt;-100
er&lt;-list()
sf&lt;-list()
ws&lt;-list()
er_d&lt;-rep(NA,n)
sf_d&lt;-rep(NA,n)
ws_d&lt;-rep(NA,n)

#loop to create and store random graphs and compute their diameters
for (i in 1:n){
  er[[i]]&lt;-sample_gnp(100,0.02)
  sf[[i]]&lt;-sample_pa(100,power=2.5,directed=F)
  ws[[i]]&lt;-sample_smallworld(1,100,1,0.1)
  er_d[i]&lt;-diameter(er[[i]])
  sf_d[i]&lt;-diameter(sf[[i]])
  ws_d[i]&lt;-diameter(ws[[i]])
}

#plot combined histogram
hist(er_d,col=rgb(0,0,1,0.2),xlim=c(0,max(max(ws_d),max(ws_d),max(ws_d))),ylim=c(0,40),xlab="Diameter",main="")
hist(sf_d, col=rgb(0,0,1,0.5), add=T)
hist(ws_d, col=rgb(0,0,1,0.8), add=T)
box()</code></pre>
<p class="CodeListingCaption"><a id="listing3-5">Listing 3-5</a>: A script that generates 300 networks, evenly split among three different types; computes their network diameter; and then plots the histograms for each: Erdös–Renyi, scale-free, and Watts–Strogatz</p>
<p>The parameters are chosen here so that all random networks have the same number of vertices (chosen arbitrarily to be 100) and approximately the same edge density (around 2 percent); this ensures that the network structure does differentiate the three groups, rather than something simpler like the number of vertices or edges. <a href="#figure3-5" id="figureanchor3-5">Figure 3-5</a> shows the resulting histogram plot.</p>
<span epub:type="pagebreak" id="Page_66" title="66"/><figure>
<img alt="" class="" src="image_fi/503083c03/f03005.png"/>
<figcaption><p><a id="figure3-5">Figure 3-5</a>: Histograms of network diameter for three different types of random networks: Erdös–Renyi (light gray), scale-free (medium gray), and Watts–Strogatz (dark gray)</p></figcaption>
</figure>
<p>We see that the three histograms are disjoint. Erdös–Renyi networks have moderate diameters. Scale-free networks have small diameter values. Watts–Strogatz networks have large diameters. If interested, you might try modifying <a href="#listing3-5">Listing 3-5</a> to compute some of the other global network metrics discussed in <span class="xref" itemid="xref_target_Chapter 2"><a href="c02.xhtml">Chapter 2</a></span> (such as efficiency, transitivity, and spectral radius) to see how they behave for the different types of random graph structures.</p>
<p>There are many machine learning tasks that you can do now on datasets of networks using the tools developed so far. For classification (such as labeling social media accounts as bot versus real based on their friend network) or regression (such as predicting the journal ranking of academic publications based on their citation networks), you can compute a collection of global network metrics to then use as features for a traditional supervised learning algorithm. Similarly, to cluster a collection of networks into different types (for example, grouping individuals according to their fMRI brain network structure), you can compute global network metrics and feed them into a traditional clustering algorithm. You can also do statistics, such as outlier detection and confidence interval estimation. Indeed, by representing each network with its vector of global network metric values, you “structure” your network data and open the door to all the statistical and machine learning methods that we traditionally rely upon in data science.</p>
<h2 id="h1-503083c03-0004"><a class="XrefDestination" id="AnalyzingSpreadThroughNetworks"/><span class="XrefDestination" id="xref-503083c03-014"/>Analyzing Spread Through Networks</h2>
<p class="BodyFirst">Another important topic in network analysis is the spread (or <em>propagation</em>) of various entities through a network. There are many real-world instances of this, including infectious diseases in contact networks and viral content <span epub:type="pagebreak" id="Page_67" title="67"/>in social media networks. Understanding the geometry of a network can help predict the way that entities spread on the network, and we can leverage this insight to change the network geometry so that we impact spread.</p>
<h3 id="h2-503083c03-0005"><a class="XrefDestination" id="TrackingDiseaseSpreadBetweenTowns"/><span class="XrefDestination" id="xref-503083c03-015"/>Tracking Disease Spread Between Towns</h3>
<p class="BodyFirst">Let’s return to the weighted network of four towns from the previous chapter’s <a href="c02.xhtml#figure2-3" id="figureanchor2-3">Figure 2-3</a>. We’ll take the adjacency matrix for it from <a href="c02.xhtml#listing2-3" id="listinganchor2-3">Listing 2-3</a> and use it to create a weighted network whose edge weights are the inverses of the original distance; this turns distances into proximity scores, where shorter roads have larger edge weights than longer roads. <a href="#listing3-6" id="listinganchor3-6">Listing 3-6</a> (which relies on first running the script in <a href="c02.xhtml#listing2-3">Listing 2-3</a>) does this and plots the result.</p>
<pre><code>#invert the nonzero entries in the towns adjacency matrix from last chapter
townprox&lt;-apply(towns,MARGIN=c(1,2),function(x) 1/x)
townprox[which(townprox == Inf)]&lt;-0

#create weighted network from this new adjacency matrix
g_townprox&lt;-graph_from_adjacency_matrix(townprox,mode="undirected",weighted=T)

#plot network with edges labeled by weights
plot(g_townprox,edge.label=round(E(g_townprox)$weight,3),vertex.color=2,vertex.size=15,vertex.label.cex=0.8)</code></pre>
<p class="CodeListingCaption"><a id="listing3-6">Listing 3-6</a>: A script that creates and plots the four towns network from <span class="xref" itemid="xref_target_Chapter 2"><a href="c02.xhtml">Chapter 2</a></span> but with the edge weights given by the inverses of the road lengths</p>
<p><a href="#figure3-6" id="figureanchor3-6">Figure 3-6</a> shows the resulting plot.</p>
<figure>
<img alt="" class="" src="image_fi/503083c03/f03006.png"/>
<figcaption><p><a id="figure3-6">Figure 3-6</a>: Four towns and the proximity scores (inverse distance) of the roads between them</p></figcaption>
</figure>
<p><span epub:type="pagebreak" id="Page_68" title="68"/>Let’s consider a simple epidemiological model for the spread of a transmissible disease across this network in which the probability of the disease spreading from an infected town to each of its neighboring towns is given by the edge proximity scores. If the disease starts in town V1, then it has a 25 percent chance of spreading to V4. If it does this, it then has a 16.7 percent chance of further spreading from V4 to V3. But multiplying these two probabilities (which yields about 4.2 percent) does not give the probability that the disease spreads from V1 to V3; it gives only the probability that it does so along the transmission route V1→V4→V3. Another potential transmission route is V1→V4→V2→V3, which has a 1 percent chance of occurring.</p>
<p>For a larger network, computing all the conditional probabilities based on potential transmission routes given by paths in the network will clearly be too cumbersome to do by hand, so we need these calculations to be automated. Moreover, this epidemiological model is too simple to be of much practical use; we discussed it here just to give a sense of how the structure of a weighted network might influence the spread of various entities (disease, information, and so on) across its vertices, as well as to motivate the more sophisticated epidemiological model that we’ll be turning to next.</p>
<p>An <em>SIR model</em>, or <em>susceptible-infected-resistant</em> model (alternatively, a <em>susceptible-infected-recovered</em> model), is a model that projects the spread of a disease among a population by assuming each individual can be in one of three disease states: susceptible (can become infected), infected (has the disease and can transmit it), or recovered or resistant (immune to the disease). Many variations of this model exist, including models that are susceptible-infected-susceptible, models that are susceptible-infected-recovered-susceptible, models including partial immunity from vaccines, models where individuals are born or die during the epidemic, and partitioned or geographic models where populations mix at different rates. Underlying all these models are systems of partial differential equations with parameters related to population mixing (such as contact rates or times) and disease characteristics (such as the number of new infections expected for a single infectious individual). Often these differential equations are too difficult to solve explicitly, so instead we run computer simulations to quantify the range and likelihood of different possible outcomes.</p>
<p>To take into account the social interactions between individuals within a population, SIR models have been adapted to networks. This provides more detailed predictions of how a disease might spread, and it also helps people find ways of mitigating this spread: we can run the model to see what impact deleting a vertex or edge, or restructuring the network in other ways, will have on the spread of the disease.</p>
<p>Many of the network geometry concepts from <span class="xref" itemid="xref_target_Chapter 2"><a href="c02.xhtml">Chapter 2</a></span> play a role here. Hubs are high transmission zones that might need to be shut down or reduced in size, bridges and vertices with high betweenness scores suggest targeted ways of cutting off the main transmission routes of the disease, and vertices with high centrality scores might indicate the individuals most important to vaccinate or quarantine as quickly as possible. Moreover, SIR models on networks and the computer simulation techniques used to explore them have applications far beyond epidemiology because they give a powerful empirical method for studying the complex relationship <span epub:type="pagebreak" id="Page_69" title="69"/>between network structure and network propagation more generally. For instance, the spread of misinformation on social media is a problem that has attracted a lot of attention recently and driven a need to better understand how network structure influences social media virality—and SIR-type models have proven to be valuable tools in this realm.</p>
<h3 id="h2-503083c03-0006"><a class="XrefDestination" id="TrackingDiseaseSpreadBetweenWindsurfers"/><span class="XrefDestination" id="xref-503083c03-016"/>Tracking Disease Spread Between Windsurfers</h3>
<p class="BodyFirst">Let’s jump right in with an example. <a href="#listing3-7" id="listinganchor3-7">Listing 3-7</a> loads a popular network dataset, the KONECT Windsurfer Network. This is a weighted network representing 43 Southern California windsurfers and their level of interactions during the fall of 1986. Almost all the nondiagonal entries of the adjacency matrix are nonzero—meaning almost every possible edge exists in this network—so it’s really the weights that matter. This makes it difficult to visualize the network, so let’s create two less dense versions of the network—one with all the edges whose weights are not in the top quartile removed and one with those below the median removed. (This is a simple form of filtering weighted networks, a concept we’ll return to in more depth in <span class="xref" itemid="xref_target_Chapter 4"><a href="c04.xhtml">Chapter 4</a></span>.) <a href="#listing3-7">Listing 3-7</a> does this and plots the results.</p>
<pre><code>#load dataset, compute quartiles, and convert to weighted network
wind&lt;-as.matrix(read.csv("beachdata.csv",header=F))
q&lt;-quantile(wind,prob=c(.25,.5,.75))
g_wind&lt;-graph_from_adjacency_matrix(wind,mode="undirected",weighted=T)

#new networks, keeping only edges with weight in top one and two quartiles
wind_top&lt;-wind
wind_top[which(wind &lt; q[3])]&lt;-0
g_wind_top&lt;-graph_from_adjacency_matrix(wind_top,mode="undirected",weighted=T)
wind_mid&lt;-wind
wind_mid[which(wind &lt; q[2])]&lt;-0
g_wind_mid&lt;-graph_from_adjacency_matrix(wind_mid,mode="undirected",weighted=T)

#plot these two thinned-out networks with weights^2 as edge thickness
#(squaring the weights is just to increase the visual distinction) plot(g_wind_top,vertex.size=10,vertex.label.cex=0.4,vertex.color=2,edge.width=E(g_wind_top)$weight^2)
plot(g_wind_mid,vertex.size=10,vertex.label.cex=0.4,vertex.color=2,edge.width=E(g_wind_mid)$weight^2)</code></pre>
<p class="CodeListingCaption"><a id="listing3-7">Listing 3-7</a>: A script that loads the KONECT Windsurfer Network dataset and creates two less dense versions of it, by removing edges whose weights are not in the top one or two quartiles, and then plots the result</p>
<p>Using the <code>edge_density()</code> function, we find that the original network has a density of 99.3 percent, the top-quartile network has a density of 25.8 percent, and the above-median network has a density of 51.4 percent. The plots in <a href="#figure3-7" id="figureanchor3-7">Figure 3-7</a> show these two thinned-out versions of the windsurfer network. The edge thicknesses represent the edge weights, but to increase the visual distinction among them, we set the thickness to the square of the edge weight.</p>
<span epub:type="pagebreak" id="Page_70" title="70"/><figure>
<img alt="" src="image_fi/503083c03/f03007_m.png"/>
<figcaption><p><a id="figure3-7">Figure 3-7</a>: Two thinned-out versions of the windsurfer network, in which all edges whose weights are not in the top quartile (left) or top two quartiles (right) have been removed</p></figcaption>
</figure>
<p>Running an SIR simulation in R is easy. Using igraph’s <code>sir()</code> function, you can just specify the network, the infection rate (called <em>beta</em>), and the recovery rate (called <em>gamma</em>), and then (optionally) specify the number of simulation trials to conduct (the default value is 100). The infection rate determines the probability at each time step that a susceptible vertex becomes infected by an infected neighbor (higher rates for more contagious diseases); having two infected neighbors doubles the odds of getting infected. The recovery rate determines the probability distribution for the duration of infections; higher recovery rates mean higher probability at each time step that infected vertices move on to the recovered state. Higher recovery rates indicate shorter-duration infections.</p>
<p>When plotting the result of <code>sir()</code>, you’ll see the number of actively infected individuals in the network together with the median value across the trials and estimated confidence intervals as a function of time. Applying the function <code>median()</code> to the output of <code>sir()</code> provides three time series: the median number of susceptible individuals, the median number of infected individuals, and the median number of recovered individuals. Let’s try this. In <a href="#listing3-8" id="listinganchor3-8">Listing 3-8</a> we simulate a disease on the full dataset with an infection rate of 3 and a recovery rate of 2.</p>
<pre><code>#SIR simulations on the original windsurfer network
sim&lt;-sir(g_wind,beta=3,gamma=2)

#plot the result
plot(sim,main="Number of Infected Over Time, Including Confidence Intervals")

<span epub:type="pagebreak" id="Page_71" title="71"/>#display the median number of infected individuals for each time bucket
median(sim)$NI</code></pre>
<p class="CodeListingCaption"><a id="listing3-8">Listing 3-8</a>: A script that runs 100 simulation trials of an SIR model on the KONECT Windsurfer Network dataset with an infection rate of <code>beta=3</code> and a recovery rate of <code>gamma=2</code> and then plots the results and displays the median number of infected individuals across time</p>
<p><a href="#figure3-8" id="figureanchor3-8">Figure 3-8</a> shows the resulting plot.</p>
<figure>
<img alt="" class="" src="image_fi/503083c03/f03008.png"/>
<figcaption><p><a id="figure3-8">Figure 3-8</a>: Plot of SIR simulations (100 trials) on the original KONECT Windsurfer Network dataset, showing the number of infected individuals over time (with mean and confidence intervals) for a disease with infection rate 3 and recovery rate 2</p></figcaption>
</figure>
<p>At its peak, the median is 28 actively infected individuals—which is 65 percent of the entire network. This is a highly infectious disease spreading through a densely connected network. Running the same code as in <a href="#listing3-4">Listing 3-4</a> but lowering the infection rate to <code>beta=1</code> and raising the recovery rate to <code>gamma=10</code> yields the plot in <a href="#figure3-9" id="figureanchor3-9">Figure 3-9</a>.</p>
<figure>
<img alt="" class="" src="image_fi/503083c03/f03009.png"/>
<figcaption><p><a id="figure3-9">Figure 3-9</a>: SIR simulations on the original KONECT Windsurfer Network dataset (with confidence levels), now with infection rate 1 and recovery rate 10</p></figcaption>
</figure>
<p>As expected, this new epidemic simulation shows fewer infected individuals over a smaller period of time. Now the median number of active <span epub:type="pagebreak" id="Page_72" title="72"/>infections peaks at 17, and the time period of this epidemic is only one-quarter what it was for the previous parameters. When using SIR models to study real-world epidemics, epidemiologists look up the infection and recovery rate parameters in the scientific literature if they are already known, and if they’re not already known, they can be estimated from data on how the disease has spread so far. Usually, such estimates will involve some degree of uncertainty, so we can run SIR simulations on a range of parameters to see the range of possible outcomes.</p>
<h3 id="h2-503083c03-0007"><a class="XrefDestination" id="LesseningDiseaseSpread"/><span class="XrefDestination" id="xref-503083c03-017"/>Disrupting Communication and Disease Spread</h3>
<p class="BodyFirst">One of the interesting proposed uses of vertex Forman–Ricci curvature is to rank vertices for removal to disrupt communication and disease spread on a network. In a communications network, disrupting communication may involve targeting a specific cell tower or isolating an individual import to the network. In 2020, we saw how isolating COVID-infected or COVID-exposed individuals by social distancing and quarantines helped stop the spread of COVID in large cities. Recall that vertex 7 in the author’s network had a large Forman–Ricci curvature. Let’s run an SIR model on the author’s network with and without vertex 7 included to compare the results:</p>
<pre><code>#run and plot SIR epidemic on full author's network
sim1&lt;-sir(g_social,beta=3,gamma=2)
plot(sim1,main="Epidemic on Full Author's Network")

#remove vertex 7 from the author's network and rerun SIR epidemic

g2&lt;-delete_vertices(g_social,v=7)
sim2&lt;-sir(g2,beta=3,gamma=2)
plot(sim2,main="Epidemic on Author's Network with Vertex 7 Removed")</code></pre>
<p>This script runs epidemics on the original network and the modified network, with vertex 7 removed, to compare the severity of the simulated epidemic. This should yield an initial plot similar to <a href="#figure3-10" id="figureanchor3-10">Figure 3-10</a>, with the epidemic propagating through the whole network.</p>
<figure>
<img alt="" class="" src="image_fi/503083c03/f03010.png"/>
<figcaption><p><a id="figure3-10">Figure 3-10</a>: An SIR epidemic on the author’s full network</p></figcaption>
</figure>
<p><span epub:type="pagebreak" id="Page_73" title="73"/><a href="#figure3-10">Figure 3-10</a> shows an SIR epidemic resulting in five time periods of infection spread, with a median number of infected at 5. Some simulations suggest the possibility of up to 12 infections at once within the first 2 time periods. This is a pretty severe epidemic, forecast to impact more than 25 percent of the population at a time before the infection takes out the whole susceptible population.</p>
<p>Let’s examine what happens when vertex 7 is removed, shown in <a href="#figure3-11" id="figureanchor3-11">Figure 3-11</a>.</p>
<figure>
<img alt="" class="" src="image_fi/503083c03/f03011.png"/>
<figcaption><p><a id="figure3-11">Figure 3-11</a>: An SIR epidemic on the author’s edited network (with vertex 7 removed)</p></figcaption>
</figure>
<p><a href="#figure3-11">Figure 3-11</a> shows a less severe epidemic over a shorter time frame. There are only four periods in which infection occurs, and the median number infected at the height of the epidemic is only 2, with a maximum estimate of 8. While the epidemic still impacts the population, it is confined to fewer individuals and quickly over. By the end of the second time period, most models suggest the epidemic has ended.</p>
<p>Many applications that let us target vertices to disrupt a network exist. Not only can we mitigate potential epidemics by removing pieces of a network, but we can also disrupt enemy communication within a terrorist cell or hostile government by taking out targets with highly negative Forman–Ricci curvature or disrupt disease processes by taking out proteins or genes within the backbone of the biological network. Changes in Forman–Ricci curvature as a network evolves also contribute to network-related analytics capabilities.</p>
<p><em>Forman–Ricci flow</em> is a geometric flow (differential equation) related to changes in curvature over time on a network, analogous to heat dissipating across a network from a defined starting point. Tracking changes in curvature can identify areas of change within a network. Forman–Ricci flow provides a way to quantify regions of growth or shrinkage of connections within networks, such as the rapid expansion of terrorist cell membership, accumulation of mutations in a cancer gene network, or increased disease spread risk in an epidemic. For instance, the increased large party or event activity in some regions during COVID quarantines resulted in an increased spread of COVID within those parts of an area’s social network. Forman–Ricci flow on image datasets also provides a way to map medical image data <span epub:type="pagebreak" id="Page_74" title="74"/>from a raw source file onto a standard surface, such as a plane or a sphere or even a frying pan, such that results can be pooled and compared within and across patient groups.</p>
<h2 id="h1-503083c03-0005"><a class="XrefDestination" id="Summary"/><span class="XrefDestination" id="xref-503083c03-018"/>Summary</h2>
<p class="BodyFirst">In this chapter, we first saw how the vertex metrics covered in <span class="xref" itemid="xref_target_Chapter 2"><a href="c02.xhtml">Chapter 2</a></span> can serve as predictors for supervised learning within a network (including link prediction) and as features for vertex clustering (that is, community mining). This approach to machine learning translates a network back to structured datasets and applies Euclidean machine learning algorithms. We then looked into a handful of community mining algorithms that operate directly within the network. Next, we moved from analyzing data within a network to analyzing datasets where each data point is itself a network. Similar to our use of vertex metrics in the previous setting, here we used global network metrics as predictors or features to do machine learning and statistical analyses on this kind of network data. Finally, in the last section of this chapter, we explored the SIR disease spread model from epidemiology as it applies to networks. The emphasis here is on the intersection of network geometry and network spread; in particular, we discuss some targeted strategies for disrupting epidemic spread that are rooted in network geometry.</p>
</section>
</body>
</html>