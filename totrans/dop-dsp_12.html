<html><head></head><body>
<section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" title="109" id="Page_109"/>9</span><br/>
<span class="ChapterTitle">Observability</span></h1>
</header>
<figure class="opener">
<img src="image_fi/book_art/chapterart.png" alt=""/>
</figure>
<p class="ChapterIntro"><em>Observability</em> is an attribute of a system, rather than something you do. It is a system’s ability to be monitored, tracked, and analyzed. Any application worthy of production should be observable. Your main goal in observing a system is to discern what it is doing, internally. You do this by analyzing system outputs like metrics, traces, and logs. <em>Metrics</em> usually consist of data over time that provide key insights into an application’s health and/or performance. <em>Traces</em> track a request as it traverses different services, to provide a holistic view. <em>Logs</em> provide a historical audit trail of errors or events that can be used for troubleshooting. Once you collect this data, you need to monitor it and alert someone when there is unexpected behavior.</p>
<p>It is not necessary to analyze metrics, traces, and logs from every application or piece of architecture. For example, tracing is key when you are running distributed microservices because it can shed light on the individual state of a given service and its interactions with other services. Your decisions about what, how, and how much to observe really will hinge on <span epub:type="pagebreak" title="110" id="Page_110"/>the level of architectural complexity you are dealing with. Since your application and infrastructure are relatively uncomplicated, you’ll observe your telnet-server application with metrics, monitoring, and alerting.</p>
<p>In this chapter, you’ll first install a monitoring stack inside the Kubernetes cluster you created in <span class="xref" itemid="xref_target_Chapter 7">Chapter 7</span>. Then, you’ll investigate common metric patterns you can use as a starting point for any service or application you may encounter. Finally, you’ll configure the monitoring stack to send an email notification when an alert is triggered. By the end of this chapter, you’ll have a solid understanding of how to install, monitor, and send notifications for any application inside Kubernetes.</p>
<h2 id="h1-502482c09-0001">Monitoring Overview</h2>
<p class="BodyFirst"><em>Monitoring</em> is any action that entails recording, analyzing, and alerting on predefined metrics to understand the current state of a system. To measure a system’s state, you need applications to publish metrics that can tell a story about what the system is doing at any given time. By setting thresholds around metrics, you can create a baseline of what you expect the application’s behavior to be. For example, a web application is expected to respond with an HTTP 200 in most cases. When the application’s baseline is not in a range you expect, you’ll need to alert someone so they can bring the application back into line. Systems will fail, but robust monitoring and alerting can be the bridge to user satisfaction and on-call shifts that end with you getting a good night’s sleep.</p>
<p>An observable system should do its best to answer two main questions: “What?” and “Why?” “What?” asks about a symptom of an application or service during a specific time frame, while “Why?” asks for the reasons behind the symptom. You can usually get the answer to “What?” by monitoring symptoms, while you can get the answer to “Why?” by other means, like logs and traces. Correlating the symptom with the cause can be the hardest part of monitoring and observability. This means your application’s resiliency is only as good as the data the application outputs. A common phrase to describe this concept is “Garbage in, garbage out.” If the metrics exported from an application are not targeted or relevant to how the user interacts with the service, detecting and diagnosing issues will be more difficult. Because of this, it’s more important to measure the application’s <em>critical path</em>, or its most-used parts, than every possible use case.</p>
<p>For instance, say you go to your doctor because you woke up with nausea and stomach cramps. The doctor asks you some basic questions and takes your temperature, heart rate, and blood pressure. While your temperature is a bit elevated, everything else falls within the normal range. After reviewing all the data, the doctor makes a judgment call about why you feel bad. Odds are, the doctor will be able to correctly diagnose your ailment (or at least find more clues about it to follow up on). </p>
<p>This process of medical diagnosis is the same process you’ll follow when diagnosing application issues. You’ll measure the symptoms and try to explain them with a diagnosis or a hypothesis. If you have enough relevant data points, it will be easier for you to correlate the symptoms with a cause. <span epub:type="pagebreak" title="111" id="Page_111"/>In the example above, if the doctor asked what you had eaten recently (another solid data point), they might have correlated your nausea and cramps with your unwise choice to eat gas station sushi at 3 <span class="KeyCaps">AM</span>. </p>
<p>Finally, always consider the “What?” and “Why?” when designing metrics and monitoring solutions for your applications. Avoid metrics or alerts that do not provide value to your stakeholders. Engineers who get bombarded by nonactionable alerts tend to get tired and ignore them. </p>
<h2 id="h1-502482c09-0002">Monitoring the Sample Application</h2>
<p class="BodyFirst">You’ll begin by monitoring the metrics that this book’s example telnet-server publishes. The telnet-server application has an HTTP endpoint that serves up metrics about the application. The metrics you’re interested in gathering for the application focus on user experiences, like connection errors and traffic. The stack for your telnet-server application will consist of three main monitoring applications and a traffic simulation application. You’ll use these applications to monitor, alert, and visualize the metrics instrumented by telnet-server. </p>
<p>The monitoring applications are Prometheus, Alertmanager, and Grafana. They are commonly used in the Kubernetes ecosystem. <em>Prometheus</em> is a metric collection application that queries metric data with its powerful built-in query language. It can set alerts for those metrics as well. If a collected metric crosses a set threshold, Prometheus sends an alert to <em>Alertmanager</em>, which takes the alerts from Prometheus and decides where to route them based on some criteria that are user configurable. The routes are usually notifications. <em>Grafana</em> provides an easy-to-use interface to create and view dashboards and graphs from the data Prometheus provides. The traffic simulator, bbs-warrior, simulates the traffic an end user of the telnet-server application might generate. This lets you test your monitoring system, application metrics, and alerts. <a href="#figure9-1" id="figureanchor9-1">Figure 9-1</a> shows an overview of the example stack.</p>

<figure>
<img src="image_fi/502482c09/f09001.png" class="" alt="Diagram showing the flow between Grafana, the telnet-server application, bbs-warrior, Prometheus, and Alertmanager to an email message"/>
<figcaption><p><a id="figure9-1">Figure 9-1</a>: Overview of our monitoring stack</p></figcaption>
</figure>


<h3 id="h2-502482c09-0001"><span epub:type="pagebreak" title="112" id="Page_112"/>Installing the Monitoring Stack</h3>
<p class="BodyFirst">To install these applications, you’ll use the provided Kubernetes manifest files. The manifest files for the monitoring stack and traffic simulator are in the repository (<a href="https://github.com/bradleyd/devops_for_the_desperate/" class="LinkURL">https://github.com/bradleyd/devops_for_the_desperate/</a>), under the <em>monitoring</em> directory. Within that directory are four subdirectories: <em>alertmanager</em>, <em>bbs-warrior</em>, <em>grafana</em>, and <em>prometheus</em>. These make up the example monitoring stack. You’ll install Prometheus, Alertmanager, and Grafana in a new Kubernetes Namespace called <code>monitoring</code> by applying all the manifests in each of these directories.</p>
<p>In a terminal, enter the following command to install the monitoring stack and bbs-warrior:</p>
<pre><code>$ <b>minikube kubectl -- apply -R -f monitoring/</b>
namespace/monitoring created
serviceaccount/alertmanager created
configmap/alertmanager-config created
deployment.apps/alertmanager created
service/alertmanager-service created
cronjob.batch/bbs-warrior created
configmap/grafana-dashboard-pods created
configmap/grafana-dashboard-telnet-server created
configmap/grafana-dashboards created
configmap/grafana-datasources created
deployment.apps/grafana created
service/grafana-service created
clusterrolebinding.rbac.authorization.k8s.io/kube-state-metrics created
clusterrole.rbac.authorization.k8s.io/kube-state-metrics created
deployment.apps/kube-state-metrics created
serviceaccount/kube-state-metrics created
service/kube-state-metrics created
clusterrole.rbac.authorization.k8s.io/prometheus created
clusterrolebinding.rbac.authorization.k8s.io/prometheus created
configmap/prometheus-server-conf created
deployment.apps/prometheus created
service/prometheus-service created</code></pre>
<p>The output shows that all the manifests for your monitoring stack and bbs-warrior were run without errors. The <code>-R</code> flag makes the <code>kubectl</code> command recursively go through all the application directories and their subdirectories under the <em>monitoring</em> directory. Without this flag, <code>kubectl</code> will skip any nested subdirectories, like <em>grafana/dashboards/</em>. Prometheus, Grafana, Alertmanager, and bbs-warrior should be up and running in a few moments.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p>	You may have noticed that the <em>namespace</em> file in the <em>monitoring</em> directory has a <em>00_</em> prefix. This prefix ensures that when <code>kubectl</code> applies the manifests, the <em>namespace</em> file will be evaluated first. All the monitoring applications are installed in a separate Namespace called <code>monitoring</code>. Those applications reference the <code>monitoring</code> Namespace, and if it isn’t created first, they will not install. Using a <em>00_</em> prefix is a simple way to force ordering in a directory of files. If you needed a file to be the second one evaluated, you would use a <em>01_</em> prefix.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<h3 id="h2-502482c09-0002"><span epub:type="pagebreak" title="113" id="Page_113"/>Verifying the Installation</h3>
<p class="BodyFirst">If the monitoring stack installation was successful on your Kubernetes cluster, you should be able to access Grafana’s, Alertmanager’s, and Prometheus’s web interfaces on your browser. In the provided Kubernetes manifest files, I have set the Kubernetes Service types for Prometheus, Grafana, and Alertmanager to <code>NodePort</code>. A Kubernetes <em>NodePort</em> Service allows you to connect to an application outside the Kubernetes cluster, so you should be able to access each application on the minikube IP address and a dynamic port. You should also be able to confirm that the bbs-warrior traffic simulator was installed and is running periodically.</p>
<h4 id="h3-502482c09-0001">Grafana</h4>
<p class="BodyFirst">In a terminal, enter the following command to open Grafana:</p>
<pre><code>$ <b>minikube -n monitoring service grafana-service</b>
|------------|-----------------|-------------|-----------------------------|
| NAMESPACE  |      NAME       | TARGET PORT |             URL             |
|------------|-----------------|-------------|-----------------------------|
| monitoring | grafana-service |        3000 | http://192.168.99.105:31517 |
|------------|-----------------|-------------|-----------------------------|
Opening service monitoring/grafana-service in default browser...</code></pre>
<p>Grafana lives in the <code>monitoring</code> Namespace, so this command uses the <code>-n</code> (Namespace) flag to show the <code>minikube service</code> command where to locate the Service. If you omit the <code>-n</code> flag, minikube will error, as there’s no Service named <code>grafana-service</code> in the default Namespace. You should now see Grafana open in your web browser, with the telnet-server dashboard loaded as the first page. If you don’t see the telnet-server dashboard, check the terminal where you ran the <code>minikube service</code> command for any errors. (You’ll need access to Grafana to follow along with the rest of this chapter.) We’ll discuss the graphs on the Grafana dashboard later; for now, you should ensure that Grafana is installed correctly and that you can open it in your browser.</p>
<h4 id="h3-502482c09-0002">Alertmanager</h4>
<p class="BodyFirst">In a terminal, enter the same command you used to open Grafana in your browser, but replace the Service name with <code class="bold">alertmanager-service</code>, like this:</p>
<pre><code>$ <b>minikube -n monitoring service alertmanager-service</b>
<var>--snip--</var></code></pre>
<p>The Alertmanager application should now be open in your browser. This page has a few navigation links, like Alerts, Silences, Status, and Help. The Alerts page displays current alerts and any metadata, like timestamps and severity associated with an alert. The Silences page shows any alerts that have been silenced. You can mute or silence an alert for a specific amount of time, which is helpful if an alert is being triggered and you don’t want <span epub:type="pagebreak" title="114" id="Page_114"/>to keep getting paged for it. The Status page shows information about Alertmanager, like its version, ready status, and current configuration. Alertmanager is configured via the <em>configmap.yaml</em> file in the <em>alertmanager/</em> directory. (You’ll edit this file later to enable notifications.) Finally, the Help page is a link to Alertmanager’s documentation. </p>
<h4 id="h3-502482c09-0003">Prometheus</h4>
<p class="BodyFirst">In your terminal, enter the same command you just entered, but replace <code>grafana-service</code> with <code class="bold">prometheus-service</code> to open Prometheus: </p>
<pre><code>$ <b>minikube -n monitoring service prometheus-service</b>
<var>--snip--</var></code></pre>
<p>Prometheus should open in your browser with a few links at the top of the page: Alerts, Graph, Status, and Help. The Alerts page displays all known alerts and their current state. The Graph page is the default page that allows you to run queries against the metric database. The Status page contains information about Prometheus’s health and configuration file. Prometheus, like Alertmanager, is controlled by the <em>configmap.yaml</em> file in the <em>prometheus</em> directory. This file controls what endpoints Prometheus scrapes for metrics, and it contains the alert rules for specific metrics. (We’ll explore the alert rules later.) The Help page is a link to Prometheus’s official documentation. For now, you are just confirming that Prometheus is running. Leave Prometheus open, as you’ll need it in the next section.</p>
<h4 id="h3-502482c09-0004">bbs-warrior</h4>
<p class="BodyFirst">The bbs-warrior application is a Kubernetes CronJob that runs every minute and creates a random number of connections and errors to the telnet-server application. It also sends a random number of BBS commands (like <code>date</code> and <code>help</code>) to the telnet-server, to mimic typical user activity. About a minute after you install bbs-warrior, it should start generating random traffic. This simulation should last only a few seconds. </p>
<p>To make sure bbs-warrior is active and installed correctly in your Kubernetes cluster, enter the following command in a terminal: </p>
<pre><code>$ <b>minikube kubectl -- get cronjobs.batch -l app=bbs-warrior</b>
NAME                   SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
bbs-warrior            */1 * * * *   False     0        25s             60s </code></pre>
<p>The <code>-l</code> (label) flag narrows down the results when searching for CronJobs. The output shows that the CronJob was installed over a minute ago (<code>60s</code>, under the <code>AGE</code> column) and that it last ran <code>25</code> seconds ago (under the <code>LAST</code> <code>SCHEDULE</code> column). If it were actively running, the <code>ACTIVE</code> column would be set to <code>1</code> rather than <code>0</code>. </p>
<p>You now know that the CronJob ran, but you should make sure it completed successfully. To do that, you’ll list the Pod with the label <code>bbs-warrior</code> <span epub:type="pagebreak" title="115" id="Page_115"/>in the default Namespace and look for <code>Completed </code>in the <code>STATUS</code> column. In the same terminal you used above, enter the following command:</p>
<pre><code>$ <b>minikube kubectl -- get pods -l app=bbs-warrior</b>
NAME                           READY   STATUS              RESTARTS   AGE
bbs-warrior-1600646880-chkbw   0/1     Completed           0          60s</code></pre>
<p>The output shows that the <code>bbs-warrior</code> CronJob completed successfully about <code>60</code> seconds ago. If the CronJob has a status different from <code>Completed</code>, check the Pod’s logs for errors like you did in <span class="xref" itemid="xref_target_Chapter 7">Chapter 7</span>.</p>
<h2 id="h1-502482c09-0003">Metrics</h2>
<p class="BodyFirst">You’ve installed and verified your monitoring stack, so now, you should focus on what you are monitoring for your telnet-server. Since you want to tailor your metrics to user happiness, you should use a common pattern to align all your applications. This is always a good approach when instrumenting your services, because allowing applications to do their own unique version of metrics makes triaging (and thus, on-call shifts) very difficult.</p>
<p>For this example, you’ll explore a common metric pattern called <em>Golden</em> <em>Signals.</em> This provides a subset of metrics to track, like errors and traffic, plus a common language for you and your peers to use to discuss what healthy looks like. </p>
<h3 id="h2-502482c09-0003">Golden Signals</h3>
<p class="BodyFirst">The Golden Signals (a term first coined by Google) are four metrics that help us understand the health of a microservice. The Golden Signals are latency, traffic, errors, and saturation. <em>Latency</em> is the time it takes for a service to process a request. <em>Traffic</em> is how many requests an application is receiving. <em>Errors</em> refers to the number of errors an application is reporting (such as a web server reporting 500s). <em>Saturation</em> is how full a service is. For a saturation signal, you could measure CPU usage to determine how much headroom is left on the system before the application or host becomes slow or unresponsive. You will use this pattern often when measuring applications. If you are ever in a situation where you don’t know what to monitor, start with the Golden Signals. They’ll provide ample information about your application’s health.</p>
<p>A <em>microservice</em> typically is an application loosely coupled to other services in your platform. It is designed to focus only on one or two aspects of your overall domain. In this chapter, the telnet-server application will serve as the microservice whose health you will measure.</p>
<h3 id="h2-502482c09-0004">Adjusting the Monitoring Pattern</h3>
<p class="BodyFirst">Chances are your application will not fit neatly into a predefined monitoring pattern like the Golden Signals. Use your best judgment about what matters. For example, I decided not to track latency when instrumenting <span epub:type="pagebreak" title="116" id="Page_116"/>the telnet-server application even though the pattern lists it. Users of such an application typically wouldn’t connect, run a command, and then quit. You could track latency of the commands, or you could add tracing for each command workflow. However, that would be overkill for this sample application and beyond the scope of this book. Your commands are for demonstration purposes only, so focusing on traffic, errors, and saturation signals will provide an overall idea of the application’s health from a user’s point of view.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="box">
<h2>Other Metric Patterns</h2>
<p class="BoxBodyFirst">Two other common metric patterns are RED and USE. The <em>RED</em> (rate, error, and duration) method (<a href="https://www.weave.works/blog/the-red-method-key-metrics-for-microservices-architecture/" class="LinkURL">https://www.weave.works/blog/the-red-method-key-metrics-for-microservices-architecture/</a>) was created by Tom Wilkie of Grafana Labs. Like Golden Signals, RED was designed to help monitor microservices. However, RED focuses more on the application’s health than on underlying system resources like CPU or memory. The<em> rate</em> is the number of requests per second a service is receiving. <em>Error</em> is the number of failed requests per second (such as connection failures that a client experiences) that the service encounters. <em>Duration</em> is the amount of time it takes to serve a request, or how long it takes to return the data requested from your service to the client.</p>
<p>The <em>USE</em> (utilization, saturation, and errors) method was developed by Brendan Gregg (<a href="https://www.brendangregg.com/usemethod.html" class="LinkURL">https://www.brendangregg.com/usemethod.html</a>) for quickly discovering performance issues based on underlying resources rather than the microservices that run on them. <em>Utilization</em> is the average time the resource (for example, a disk drive at 85 percent usage) is busy doing work. <em>Saturation</em> can be thought of as extra work the system could not get to, such as happens with a busy host that is queueing up connections to serve traffic. <em>Errors</em> are the number of errors (such as network collisions or disk IO errors) a system is having.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<h3 id="h2-502482c09-0005">The telnet-server Dashboard</h3>
<p class="BodyFirst">Let’s review the traffic, saturation, and error signals on your Grafana dashboard. In the browser where you first opened Grafana, the telnet-server dashboard has three graphs for the Golden Signals and two collapsed graph rows titled System and Application (see <a href="#figure9-2" id="figureanchor9-2">Figure 9-2</a>). You’ll focus on the Golden Signals graphs, which are as follows: Connections per second, Saturation, and Errors per second. </p>
<p>The first graph, Connections per second (in the top left), provides the traffic Golden Signal. In this case, you measure how many connections per second you are receiving in a two-minute time frame. The telnet-server application increases a metric counter each time a connection is made, providing a good idea of how many people connect to the application. Many connections could pose an issue with performance or reliability. In this example, the x-axis shoots up over 4.0 connections per second for both <span epub:type="pagebreak" title="117" id="Page_117"/>telnet-server Pods. Your graphs will show different results from mine since bbs-warrior generates the traffic randomly; the goal is to make sure the graphs are being populated. </p>
<figure>
<img src="image_fi/502482c09/f09002.png" class="" alt="Screenshot of three graphs showing Connections per second (for the system and application, in the top left, going up and down), Saturation (which is flat at 0s, in the top right), and Errors per second (going up and down, in the bottom left)"/>
<figcaption><p><a id="figure9-2">Figure 9-2</a>: The telnet-server Grafana dashboard</p></figcaption>
</figure>
<p>The Saturation graph (top right) represents the saturation Golden Signal. For saturation, you measure the amount of time Kubernetes throttles your telnet-server container’s CPU. You set a CPU resource limit of 500 millicpu for the telnet-server container in <span class="xref" itemid="xref_target_Chapter 7">Chapter 7</span>. Therefore, if the telnet-server container uses more than the maximum limit, Kubernetes will throttle it, possibly making the telnet-server slow to respond to commands or connections. This throttle could potentially cause poor performance or a service interruption. In the Saturation graph shown in <a href="#figure9-2">Figure 9-2</a>, the x-axis is flat at 0 microseconds for both Pods. The line of the graph will rise if CPU throttling occurs. </p>
<p>The Errors per second (bottom left) graph maps to the error Golden Signal. For this metric, you track the connection errors per second that you receive in a two-minute time frame. These errors are incremented when a client fails to connect properly or if the connection is killed unexpectedly. A high error rate could indicate a code or infrastructure issue that you need to address. In the graph shown in <a href="#figure9-2">Figure 9-2</a>, the errors-per-second rate is spiking to 0.4 for both pods.</p>
<p>The collapsed two rows at the bottom of this dashboard contain some miscellaneous graphs not covered in this chapter, but you should explore them on your own. The telnet-server dashboard System row contains two graphs: one for memory, and one for CPU usage by the telnet-server Pods. The Application row contains four graphs: Total Connections, Active Connections, Connection Error Total, and Unknown Command Total.</p>
<p>The telnet-server dashboard is in the <em>grafana/dashboards/telnet-server.yaml</em> file. This file is a Kubernetes ConfigMap resource that contains the JSON configuration that Grafana requires to create the dashboard and graphs. </p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p> 	Keep your dashboards under version control to make it easier to reproduce and make changes.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<h3 id="h2-502482c09-0006"><span epub:type="pagebreak" title="118" id="Page_118"/>PromQL: A Primer</h3>
<p class="BodyFirst"><em>PromQL</em> is a query language built into the Prometheus application. You use it to query and manipulate metric data. Think of PromQL as a distant cousin of SQL. It has some built-in functions (like <code>average</code> and <code>sum</code>) to make querying data easier plus conditional logic (like <code>&gt;</code> or <code>=</code>). We won’t explore this query language in depth here except to show how I queried telnet-server’s Golden Signal metrics to populate your graphs and alerts. </p>
<p>For example, here is the query you enter to generate the Errors-per-second graph:</p>
<pre><code><b>rate(telnet_server_connection_errors_total{job="kubernetes-pods"}[2m])</b></code></pre>
<p>The name of the metric is <code>telnet_server_connection_errors_total</code>. This metric measures the total amount of <code>connection errors</code> a user may encounter. The query uses Prometheus’ <code>rate()</code> function, which calculates the per-second connection error average over a specified time interval. You limit the time frame for which this query fetches data to two minutes using square brackets <code>[2m]</code>. The result will show the two running telnet-server Pods you installed in <span class="xref" itemid="xref_target_Chapter 7">Chapter 7</span>. The curly brackets (<code>{}</code>) allow you to refine the query, using labels as matchers. Here, you specify that you want data only for the <code>telnet_server_connection_errors</code> metric with the <code>job="kubernetes-pods" </code>label. </p>
<p>When creating an alert rule in Prometheus, you can enter the same query as above to drive the alert. However, this time, you should wrap the results from the <code>rate()</code> function in a <code>sum</code><code>()</code> function. You’ll do this because you want to know the overall error rate for both Pods. The alert rule should look like the following:</p>
<pre><code><b>sum(rate(telnet_server_connection_errors_total{job="kubernetes-pods"}[2m]))</b> <b>&gt; 2</b></code></pre>
<p>At the end of the query, you add greater-than (<code>&gt;</code>) conditional logic with a number: <code>2</code>. This basically means that if the error rate is greater than two per second, this alert query evaluates to true. (Later in this chapter, we’ll discuss what happens when alert rules are true.) </p>
<p>If you want to review or tinker with any of these metrics, see the Graph page in the Prometheus web interface. <a href="#figure9-3" id="figureanchor9-3">Figure 9-3</a> shows the <code>telnet_server_connection_errors_total</code> query being run.</p>

<figure>
<img src="image_fi/502482c09/f09003Keyline.png" class="" alt="Screenshot showing a query in the Prometheus dashboard"/>
<figcaption><p><a id="figure9-3">Figure 9-3</a>: Running a query in Prometheus’s web interface</p></figcaption>
</figure>

<p>The query returns connection error data for both Pods. To learn more about PromQL, visit <a href="https://prometheus.io/docs/prometheus/latest/querying/basics/" class="LinkURL">https://prometheus.io/docs/prometheus/latest/querying/basics/</a> for more examples and information<em>.</em></p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<span epub:type="pagebreak" title="119" id="Page_119"/><h2><span class="NoteHead">Note</span></h2>
<p><em>	</em>A good rule of thumb when calculating per-second averages (rates) is to set the sample time window to at least two times the Prometheus scrape interval. In your case, Prometheus fetches the telnet-server metrics endpoint every 30 seconds, so your time interval should not be less than one minute.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<h2 id="h1-502482c09-0004">Alerts</h2>
<p class="BodyFirst">Metrics and graphs only constitute half of a monitoring solution. When your application decides to take a stroll off a cliff (and it will), someone or something needs to know about it. If a Pod dies in a Deployment, Kubernetes just replaces it with a new one. But if the Pod keeps restarting, someone needs to address it, and that’s where the alerts and notifications come into play. </p>
<p>What constitutes a good alert? Besides an alert for each of your application’s Golden Signals, you may need an alert around a key metric to monitor. When this does happen, keep in mind a couple of guidelines to follow when creating alerts:</p>
<ol class="none">
<li><span class="RunInHead">Do not set thresholds too low.</span>  Setting alert thresholds too low can cause the alerts to repeatedly fire and then clear if you have spiky metrics. This behavior is known as <em>flapping</em>, and it can be quite normal. The system should not issue alerts for flapping metrics every few minutes, because on-call engineers get stressed out when they repeatedly get a notification and then find the alarm has already cleared.</li>
<li><span class="RunInHead">Avoid creating alerts that are not actionable.</span>  Don’t create alerts for a service when nothing can be done to remedy it. I call these alerts <em>status alerts</em>. Nothing is more frustrating to an on-call engineer than being woken up in the middle of the night only to babysit an alert that requires no action. </li>
</ol>
<p>For this book, I have provided three alerts called <code>HighErrorRatePerSecond</code>, <code>HighConnectionRatePerSecond</code>, and <code>HighCPUThrottleRate</code> (more on these later). These alerts are located in the <em>prometheus.rules</em> section inside Prometheus’s configuration file (<em>configmap.yaml</em>). Prometheus uses alert rules to decide whether a metric is in an undesired state. An <em>alert rule</em> has information like the alert name, PromQL query, threshold, and labels. For your example, I have gone against my own alert-creation advice and set the provided thresholds extremely low, allowing bbs-warrior to trigger the alerts easily. Nothing beats a live example when learning about real-time metrics and alerts!</p>
<h3 id="h2-502482c09-0007">Reviewing Golden Signal Alerts in Prometheus</h3>
<p class="BodyFirst">You can view alerts in either Prometheus’s or Alertmanager’s web interfaces. The difference is that Alertmanger displays only alerts that are being triggered, whereas Prometheus will show all alerts, whether they are firing or not. You want to view all the alerts, so you’ll use Prometheus for this example. However, you should visit Alertmanager’s interface as well when an alert is being triggered. </p>
<p><span epub:type="pagebreak" title="120" id="Page_120"/>In the browser where Prometheus was originally opened, click the <b>Alerts</b> link in the top-left navigation bar. You should see the three provided telnet-server Golden Signals alerts: <code>HighErrorRatePerSecond</code>, <code>HighConnectionRatePerSecond</code>, and <code>HighCPUThrottleRate</code>. These alerts were created when you installed Prometheus earlier. The Alerts page should look like <a href="#figure9-4" id="figureanchor9-4">Figure 9-4</a>.</p>

<figure>
<img src="image_fi/502482c09/f09004Keyline.png" class="" alt="Screenshot showing the alerts in the Prometheus dashboard. The green alert at the top for HighErrorRatePerSecond shows 0 active. The red alert for HighConnectionRatePerSecond shows 1 active, and beneath that is the code breakdown with more information. At the bottom is the green alert for HighCPUThrottleRate, which shows 0 active."/>
<figcaption><p><a id="figure9-4">Figure 9-4</a>: Prometheus alerts for telnet-server</p></figcaption>
</figure>

<p>Each alert will be in one of three states: Pending (yellow), Inactive (green), or Firing (red). In <a href="#figure9-4">Figure 9-4</a>, the <code>HighConnectionRatePerSecond</code> alert is Firing. The other two alerts, <code>HighCPUThrottleRate</code> and <code>HighErrorRatePerSecond</code>, are Inactive since they are not being triggered. Your Alerts page will be different from mine because of bbs-warrior’s randomness. If your page doesn’t show any alerts in a Firing state, wait a few minutes until more traffic is generated. Then refresh the browser page. In all my testing for this chapter, I always had at least one alert transition to a Firing state. </p>
<p>The <code>HighErrorRatePerSecond</code> alert is concerned with the number of connection errors received per second. If the rate of connection errors in a two-minute window is greater than 2, the alert is in a Firing state. On my local Kubernetes setup, the alert is currently in the Inactive state.</p>
<p>The next alert, <code>HighConnectionRatePerSecond</code>, detects whether the connection rate is greater than 2 per second in a two-minute time frame. Currently, this alert is in the Firing state. <a href="#figure9-4">Figure 9-4</a> shows that the current value for my connection rate is more than 9.1 connections per second, which is well beyond the set threshold of 2. I have expanded the alert in the browser to show the provided metadata in a key-value layout that an alert provides. In the labels section for all three alerts, I have set a label called <code>severity</code> with a value of <code>Critical</code> so it’s easier to distinguish between noncritical alerts and ones that need immediate attention. You’ll use this label to route important alerts in Alertmanager later. The annotations section includes a description, a summary, and a link to a <em>runbook</em>, which is a blueprint that provides unfamiliar engineers with the what, why, and how for a service. Having this information is crucial when sending out an alert notification, because it gives the person on call an idea of what to look for when troubleshooting. </p>
<p><span epub:type="pagebreak" title="121" id="Page_121"/>The last alert, <code>HighCPUThrottleRate</code>, detects high CPU saturation. If the CPU is being throttled by Kubernetes for more than 300 microseconds in a two-minute window, you’ll transition to a Firing state. This alert is currently inactive, but normally, I’d suggest a minimum five-minute window when tracking CPU throttling. This is because smaller time windows can make you more susceptible to alerting on a temporarily spiky workload. </p>
<h3 id="h2-502482c09-0008">Routing and Notifications</h3>
<p class="BodyFirst">You’ve verified that the metrics and alerts are visible and active, so now, you should set up Alertmanager to send out email notifications. Once an alert is in the Firing state, Prometheus sends it to Alertmanager for routing and notification. Notifications can be sent via text messages, push notifications, or email. Alertmanager calls these notification methods <em>receivers</em>. Routing is used to match on alerts and send them to a specific receiver. A common pattern is to route alerts based on specific labels. Alert labels are set in the Prometheus <em>configmap.yaml</em> file. You’ll use this pattern later, when you enable notifications. </p>
<p>The provided Alertmanager configuration is located in the <em>alertmanager/configmap.yaml</em> file. It is set up to match on all alerts with a <code>severity</code> label set to <code>Critical</code> and route them to a <em>none receiver</em>, which is basically a black hole that won’t notify anyone when there’s an alert. This means that to see whether an alert is being triggered, you would need to visit the web page on either Alertmanager or Prometheus. This setup isn’t ideal, as refreshing the web browser every few minutes would become tedious, so you’ll route any alert to the <code>email</code> receiver if the alert has a <code>severity</code> label set to <code>Critical</code>. If you’re following along, this step is completely optional, but it shows you how to configure receivers in Alertmanager. </p>
<h4 id="h3-502482c09-0005">Enabling Email Notifications</h4>
<p class="BodyFirst">To route an alert to the <code>email</code> receiver, you need to edit Alertmanager’s configuration. I have stubbed out a template for the <code>email</code> receiver and <code>route</code> block in the <em>configmap.yaml</em> file. The email example is based on a Gmail account, but you can alter it to accommodate any email provider. See <a href="https://www.prometheus.io/docs/alerting/latest/configuration/#email_config/" class="LinkURL">https://www.prometheus.io/docs/alerting/latest/configuration/#email_config/</a> for more details.</p>
<p>Open Alertmanager’s <em>configmap.yaml</em> file in your favorite editor; it should look like this:</p>
<pre><code><var>--snip--</var>
    global: null
    receivers:
  <span class="CodeAnnotationCode" aria-label="annotation1">1</span> #- name: email
    #  email_configs:
    #  - send_resolved: true
    #    to: &lt;<var>GMAIL_USERNAME@gmail.com</var>&gt;
    #    from: &lt;<var>GMAIL_USERNAME@gmail.com</var>&gt;
    #    smarthost: smtp.gmail.com:587
    #    auth_username: &lt;<var>GMAIL_USERNAME@gmail.com</var>&gt;
<span epub:type="pagebreak" title="122" id="Page_122"/>    #    auth_identity: &lt;<var>GMAIL_USERNAME@gmail.com</var>&gt;
    #    auth_password: &lt;<var>GMAIL_PASSWORD</var>&gt;
  <span class="CodeAnnotationCode" aria-label="annotation2">2</span> - name: none
    route:
      group_by:
        - job
      group_interval: 5m
      group_wait: 10s
    <span class="CodeAnnotationCode" aria-label="annotation3">3</span> receiver: none
      repeat_interval: 3h
      routes:
      <span class="CodeAnnotationCode" aria-label="annotation4">4</span> - receiver: none
          match:
            severity: "Critical"</code></pre>
<p>Here, you have two receivers named <code>email</code> <span class="CodeAnnotation" aria-label="annotation1">1</span> and <code>none</code> <span class="CodeAnnotation" aria-label="annotation2">2</span>. The <code>none</code> receiver won’t send alerts anywhere, but when uncommented, the <code>email</code> receiver will send alerts to a Gmail account. Uncomment the <code>email </code>receiver lines and then replace with an email account you can use for testing.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">NOTE</span></h2>
<p>	If you are using Gmail and have 2FA enabled, you’ll need to set up an app-specific password credential as the generic username. Password authentication will not work. See <a href="https://support.google.com/accounts/answer/185833/" class="LinkURL">https://support.google.com/accounts/answer/185833/</a> for more details.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>After configuring your email settings, change the <code>receiver</code> <span class="CodeAnnotation" aria-label="annotation3">3</span> under the <code>routes</code> section to <code class="bold">email</code>. This configures Alertmanager to route any alert to the <code>email</code> receiver if the alert has a <code>severity</code> label set to <code>Critical</code>. The receiver line <span class="CodeAnnotation" aria-label="annotation4">4</span> should now look like this:</p>
<pre><code>- receiver: <b>email</b></code></pre>
<p>You’ll still have your default or catch-all receiver <span class="CodeAnnotation" aria-label="annotation3">3</span> set to <code>none</code>, so any alert that does not match your <code>severity</code> label rule will be sent there. Save this file, as you are done modifying it. </p>
<h4 id="h3-502482c09-0006">Applying Alertmanager’s Configuration Changes</h4>
<p class="BodyFirst">Next, you’ll update Alertmanager’s ConfigMap inside the Kubernetes cluster. Since the local file contains changes that don’t exist on the cluster, enter the following in a terminal:</p>
<pre><code>$ <b>minikube kubectl -- apply -f monitoring/alertmanager/configmap.yaml</b>
configmap/alertmanager-config configured</code></pre>
<p>The next step is to tell Kubernetes to restart the Alertmanager Deployment so it can pick up the new configuration changes. In the same terminal, enter the following command to restart Alertmanager:</p>
<pre><code>$ <b>minikube kubectl -- -n monitoring rollout restart deployment alertmanager</b>
deployment.apps/alertmanager restarted</code></pre>
<p><span epub:type="pagebreak" title="123" id="Page_123"/>The Alertmanager Pod should restart after a few moments. If you have any alerts in the Firing state, you should start receiving email in your inbox. Depending on Alertmanager and your email provider, the notifications may take some time to appear.</p>
<p>If you do not receive any notification emails, check for a couple of common issues. First, make sure the <em>configmap.yaml</em> file does not have any typos or indentation errors. It is very easy to misalign a YAML file. Second, make sure the email settings you entered match what is required by your email provider. Look in Alertmanager’s logs to find these and other common issues. Enter the following <code>kubectl</code> command to view the logs for any errors:</p>
<pre><code>$<b> minikube kubectl -- -n monitoring logs -l app=alertmanager</b></code></pre>
<p>If you need to disable the notifications for any reason, set the routes receiver back to <code>none</code> in the <em>configmap.yaml</em> file, apply the manifest changes, and restart.</p>
<p>You now have alerts and notifications configured for telnet-server’s Golden Signals.</p>
<h2 id="h1-502482c09-0005">Summary</h2>
<p class="BodyFirst">Metrics and alerts are foundational pieces when monitoring an application. They provide insight into the health and performance of your service. In this chapter, you learned about the Golden Signals monitoring pattern and how to install a modern monitoring stack inside a Kubernetes cluster using Prometheus, Alertmanager, and Grafana. Finally, you learned how to configure Alertmanager to send email notifications for critical alerts.</p>
<p>In the next chapter, we’ll shift gears and discuss common troubleshooting scenarios you will find on a host or network, plus the tools you can use to diagnose them.</p>
</section>
</body></html>