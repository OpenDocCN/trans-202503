- en: '**6**'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**6**'
- en: '**UNDERSTANDING MACHINE LEARNING–BASED MALWARE DETECTORS**'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**理解基于机器学习的恶意软件检测器**'
- en: '![image](../images/common01.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/common01.jpg)'
- en: With the open source machine learning tools available today, you can build custom,
    machine learning–based malware detection tools, whether as your primary detection
    tool or to supplement commercial solutions, with relatively little effort.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 借助现今开源的机器学习工具，你可以构建自定义的基于机器学习的恶意软件检测工具，无论是作为主要检测工具，还是作为补充商业解决方案的工具，而且所需的工作量相对较少。
- en: But why build your own machine learning tools when commercial antivirus solutions
    are already available? When you have access to examples of particular threats,
    such as malware used by a certain group of attackers targeting your network, building
    your own machine learning–based detection technologies can allow you to catch
    new examples of these threats.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，为什么要构建自己的机器学习工具，而市面上已有商业杀毒解决方案呢？当你能够获得特定威胁的样本时，比如某个攻击团伙针对你网络使用的恶意软件，构建自己的基于机器学习的检测技术可以帮助你捕捉到这些威胁的新样本。
- en: In contrast, commercial antivirus engines might miss these threats unless they
    already include signatures for them. Commercial tools are also “closed books”—that
    is, we don’t necessarily know how they work and we have limited ability to tune
    them. When we build our own detection methods, we know how they work and can tune
    them to our liking to reduce false positives or false negatives. This is helpful
    because in some applications you might be willing to tolerate more false positives
    in exchange for fewer false negatives (for example, when you’re searching your
    network for suspicious files so that you can hand-inspect them to determine if
    they are malicious), and in other applications you might be willing to tolerate
    more false negatives in exchange for fewer false positives (for example, if your
    application blocks programs from executing if it determines they are malicious,
    meaning that false positives are disruptive to users).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，商业杀毒引擎可能无法识别这些威胁，除非它们已经包含相关的签名。商业工具也是“黑箱”——也就是说，我们不一定知道它们是如何工作的，并且我们调节它们的能力有限。当我们构建自己的检测方法时，我们知道它们的工作原理，并且可以根据需要进行调整，以减少误报或漏报。这很有帮助，因为在某些应用中，你可能愿意接受更多的误报，以换取更少的漏报（例如，当你在网络中查找可疑文件，以便人工检查它们是否恶意时），而在其他应用中，你可能愿意接受更多的漏报，以换取更少的误报（例如，如果你的应用在判断程序恶意时阻止其执行，那么误报会对用户造成干扰）。
- en: In this chapter, you learn the process of developing your own detection tools
    at a high level. I start by explaining the big ideas behind machine learning,
    including feature spaces, decision boundaries, training data, underfitting, and
    overfitting. Then I focus on four foundational approaches—logistic regression,
    k-nearest neighbors, decision trees, and random forest—and how these can be applied
    to perform detection.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习如何从高层次开发自己的检测工具。我首先会解释机器学习背后的大致思想，包括特征空间、决策边界、训练数据、欠拟合和过拟合。然后，我会重点讲解四种基础方法——逻辑回归、k-最近邻、决策树和随机森林——以及如何应用这些方法进行检测。
- en: You’ll then use what you learned in this chapter to learn how to evaluate the
    accuracy of machine learning systems in [Chapter 7](ch07.xhtml#ch07) and implement
    machine learning systems in Python in [Chapter 8](ch08.xhtml#ch08). Let’s get
    started.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你将使用本章学到的知识，学习如何在[第七章](ch07.xhtml#ch07)中评估机器学习系统的准确性，并在[第八章](ch08.xhtml#ch08)中用Python实现机器学习系统。让我们开始吧。
- en: '**Steps for Building a Machine Learning–Based Detector**'
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**构建基于机器学习的检测器步骤**'
- en: There is a fundamental difference between machine learning and other kinds of
    computer algorithms. Whereas traditional algorithms tell the computer what to
    do, machine-learning systems learn how to solve a problem by example. For instance,
    rather than simply pulling from a set of preconfigured rules, machine learning
    security detection systems can be trained to determine whether a file is bad or
    good by learning from examples of good and bad files.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习与其他类型的计算机算法有一个根本性的区别。传统算法告诉计算机该做什么，而机器学习系统通过示例学习如何解决问题。例如，机器学习安全检测系统不是简单地从一组预配置的规则中提取，而是通过学习良好和恶意文件的示例来判断一个文件是好是坏。
- en: The promise of machine learning systems for computer security is that they automate
    the work of creating signatures, and they have the potential to perform more accurately
    than signature-based approaches to malware detection, especially on new, previously
    unseen malware.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统在计算机安全中的承诺是自动化创建签名的工作，并且它们有潜力在恶意软件检测上比基于签名的方法更准确，特别是对于新的、之前未见过的恶意软件。
- en: 'Essentially, the workflow we follow to build any machine learning–based detector,
    including a decision tree, boils down to these steps:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，我们构建任何基于机器学习的检测器（包括决策树）的工作流程都可以归结为以下步骤：
- en: '**Collect** examples of malware and benignware. We will use these examples
    (called *training examples*) to train the machine learning system to recognize
    malware.'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**收集**恶意软件和良性软件的示例。我们将使用这些示例（称为*训练示例*）来训练机器学习系统以识别恶意软件。'
- en: '**Extract** features from each training example to represent the example as
    an array of numbers. This step also includes research to design good features
    that will help your machine learning system make accurate inferences.'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**提取**每个训练示例的特征，将示例表示为一个数字数组。这一步还包括研究设计好的特征，帮助你的机器学习系统做出准确的推断。'
- en: '**Train** the machine learning system to recognize malware using the features
    we have extracted.'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**训练**机器学习系统以识别恶意软件，使用我们提取的特征。'
- en: '**Test** the approach on some data not included in our training examples to
    see how well our detection system works.'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**测试**该方法在一些未包含在我们训练示例中的数据上，以查看我们的检测系统效果如何。'
- en: Let’s discuss each of these steps in more detail in the following sections.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在接下来的章节中详细讨论这些步骤。
- en: '***Gathering Training Examples***'
  id: totrans-17
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***收集训练示例***'
- en: Machine learning detectors live or die by the training data provided to them.
    Your malware detector’s ability to recognize suspicious binaries depends heavily
    on the quantity and quality of training examples you provide. Be prepared to spend
    much of your time gathering training examples when building machine learning–based
    detectors, because the more examples you feed your system, the more accurate it’s
    likely to be.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习检测器的生死取决于提供给它们的训练数据。你的恶意软件检测器识别可疑二进制文件的能力在很大程度上依赖于你提供的训练示例的数量和质量。构建基于机器学习的检测器时，准备好花费大量时间收集训练示例，因为你给系统提供的示例越多，它的准确性就越高。
- en: The quality of your training examples is also important. The malware and benignware
    you collect should mirror the kind of malware and benignware you expect your detector
    to see when you ask it to decide whether new files are malicious or benign.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 训练示例的质量也非常重要。你收集的恶意软件和良性软件应当能反映你期望检测器在判断新文件是否为恶意或良性时可能会遇到的恶意软件和良性软件。
- en: For example, if you want to detect malware from a specific threat actor group,
    you must collect as much malware as possible from that group for use in training
    your system. If your goal is to detect a broad class of malware (such as ransomware),
    it’s essential to collect as many representative samples of this class as possible.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你想检测来自特定威胁演员组的恶意软件，你必须收集尽可能多的该组恶意软件用于训练你的系统。如果你的目标是检测广泛类别的恶意软件（如勒索软件），那么收集尽可能多的此类别的代表性样本至关重要。
- en: By the same token, the benign training examples you feed your system should
    mirror the kinds of benign files you will ask your detector to analyze once you
    deploy it. For example, if you are working on detecting malware on a university
    network, you should train your system with a broad sampling of the benignware
    that students and university employees use, in order to avoid false positives.
    These benign examples would include computer games, document editors, custom software
    written by the university IT department, and other types of nonmalicious programs.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 同理，你为系统提供的良性训练示例应当与部署后让检测器分析的良性文件类型相吻合。例如，如果你正在进行大学网络中的恶意软件检测，你应该用学生和大学员工使用的各种良性软件进行训练，以避免假阳性。这些良性示例包括计算机游戏、文档编辑器、大学IT部门编写的自定义软件以及其他类型的非恶意程序。
- en: To give a real-world example, at my current day job, we built a detector that
    detects malicious Office documents. We spent about half the time on this project
    gathering training data, and this included collecting benign documents generated
    by more than a thousand of my company’s employees. Using these examples to train
    our system significantly reduced our false positive rate.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 举个现实生活中的例子，在我目前的工作中，我们建立了一个检测恶意Office文档的检测器。我们在这个项目上花了大约一半的时间收集训练数据，包括收集了超过一千名公司员工生成的良性文档。通过使用这些例子来训练我们的系统，显著减少了我们的误报率。
- en: '***Extracting Features***'
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***提取特征***'
- en: 'To classify files as good or bad, we train machine learning systems by showing
    them features of software binaries; these are file attributes that will help the
    system distinguish between good and bad files. For example, here are some features
    we might use to determine whether a file is good or bad:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将文件分类为好文件或坏文件，我们通过展示软件二进制文件的特征来训练机器学习系统；这些文件属性将帮助系统区分好文件和坏文件。例如，以下是我们可能用来判断文件好坏的特征：
- en: Whether it’s digitally signed
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否经过数字签名
- en: The presence of malformed headers
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 格式错误的头部的存在
- en: The presence of encrypted data
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否包含加密数据
- en: Whether it has been seen on more than 100 network workstations
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否已在超过100台网络工作站上出现
- en: To obtain these features, we need to extract them from files. For example, we
    might write code to determine whether a file is digitally signed, has malformed
    headers, contains encrypted data, and so on. Often, in security data science,
    we use a huge number of features in our machine learning detectors. For example,
    we might create a feature for every library call in the Win32 API, such that a
    binary would have that feature if it had the corresponding API call. We’ll revisit
    feature extraction in [Chapter 8](ch08.xhtml#ch08), where we discuss more advanced
    feature extraction concepts as well as how to use them to implement machine learning
    systems in Python.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得这些特征，我们需要从文件中提取它们。例如，我们可能编写代码来判断一个文件是否经过数字签名、是否有格式错误的头部、是否包含加密数据等等。在安全数据科学中，我们经常在机器学习检测器中使用大量特征。例如，我们可能会为每个Win32
    API的库调用创建一个特征，若二进制文件调用了对应的API，那么它就会拥有该特征。在[第8章](ch08.xhtml#ch08)中，我们将重新讨论特征提取，在那里我们会讲解更高级的特征提取概念，并介绍如何使用它们在Python中实现机器学习系统。
- en: '***Designing Good Features***'
  id: totrans-30
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***设计良好的特征***'
- en: Our goal should be to select features that yield the most accurate results.
    This section provides some general rules to follow.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标应当是选择能够产生最准确结果的特征。本节提供了一些应遵循的通用规则。
- en: First, when selecting features, choose ones that represent your best guess as
    to what might help a machine learning system distinguish bad files from good files.
    For example, the feature “contains encrypted data” might be a good marker for
    malware because we know that malware often contains encrypted data, and we’re
    guessing that benignware will contain encrypted data more rarely. The beauty of
    machine learning is that if this hypothesis is wrong, and benignware contains
    encrypted data just as often as malware does, the system will more or less ignore
    this feature. If our hypothesis is right, the system will learn to use the “contains
    encrypted data” feature to detect malware.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在选择特征时，选择那些代表你最好的猜测，认为它们可能帮助机器学习系统区分坏文件和好文件的特征。例如，“包含加密数据”这一特征可能是恶意软件的一个良好标记，因为我们知道恶意软件通常包含加密数据，而我们猜测良性软件更少包含加密数据。机器学习的美妙之处在于，如果这个假设是错误的，即良性软件和恶意软件一样频繁地包含加密数据，系统将或多或少忽略这个特征。如果我们的假设是对的，系统将学习利用“包含加密数据”这一特征来检测恶意软件。
- en: Second, don’t use so many features that your set of features becomes too large
    relative to the number of training examples for your detection system. This is
    what the machine learning experts call “the curse of dimensionality.” For example,
    if you have a thousand features and only a thousand training examples, chances
    are you don’t have enough training examples to teach your machine learning system
    what each feature actually says about a given binary. Statistics tells us that
    it’s better to give your system a few features relative to the number of training
    examples you have available and let it form well-founded beliefs about which features
    truly indicate malware.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，不要使用过多的特征，以至于你的特征集相对于训练样本的数量过大。这就是机器学习专家所说的“维度诅咒”。例如，如果你有一千个特征，而只有一千个训练样本，那么很可能你没有足够的训练样本来教会你的机器学习系统每个特征到底能告诉你关于某个二进制文件的信息。统计学告诉我们，相对于你拥有的训练样本数量，给系统提供少量特征会更好，这样系统可以形成关于哪些特征真正能指示恶意软件的有根据的判断。
- en: Finally, make sure your features represent a range of hypotheses about what
    constitutes malware or benignware. For example, you may choose to build features
    related to encryption, such as whether a file uses encryption-related API calls
    or a public key infrastructure (PKI), but make sure to also use features unrelated
    to encryption to hedge your bets. That way, if your system fails to detect malware
    based on one type of feature, it might still detect it using other features.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，确保你的特征能够代表关于什么构成恶意软件或良性软件的一系列假设。例如，你可能选择构建与加密相关的特征，如文件是否使用了加密相关的 API 调用或公钥基础设施（PKI），但也要确保使用与加密无关的特征，以降低风险。这样，如果你的系统基于某种特征未能检测到恶意软件，它仍然可以通过其他特征来检测。
- en: '***Training Machine Learning Systems***'
  id: totrans-35
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***训练机器学习系统***'
- en: After you’ve extracted features from your training binaries, it’s time to train
    your machine learning system. What this looks like algorithmically depends completely
    on the machine learning approach you’re using. For example, training a decision
    tree approach (which we discuss shortly) involves a different learning algorithm
    than training a logistic regression approach (which we also discuss).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在你从训练二进制文件中提取特征后，接下来就是训练你的机器学习系统了。算法上具体的训练过程完全取决于你使用的机器学习方法。例如，训练决策树方法（我们稍后会讨论）涉及的学习算法与训练逻辑回归方法（我们也会讨论）是不同的。
- en: Fortunately, all machine learning detectors provide the same basic interface.
    You provide them with training data that contains features from sample binaries,
    as well as corresponding labels that tell the algorithm which binaries are malware
    and which are benignware. Then the algorithms learn to determine whether or not
    new, previously unseen binaries are malicious or benign. We cover training in
    more detail later in this chapter.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，所有机器学习检测器都提供相同的基本接口。你需要为它们提供包含样本二进制文件特征的训练数据，以及相应的标签，告诉算法哪些二进制文件是恶意的，哪些是良性软件。然后，算法会学习判断新的、以前未见过的二进制文件是恶意的还是良性的。我们将在本章后面更详细地介绍训练过程。
- en: '**NOTE**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*In this book, we focus on a class of machine learning algorithms known as*
    supervised machine learning algorithms*. To train models using these algorithms,
    we tell them which examples are malicious and which are benign. Another class
    of machine learning algorithms,* unsupervised algorithms*, does not require us
    to know which examples are malicious or benign in our training set. These algorithms
    are much less effective at detecting malicious software and malicious behavior,
    and we will not cover them in this book.*'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*在本书中，我们专注于一类被称为*监督机器学习算法*的机器学习算法。要使用这些算法训练模型，我们会告诉它们哪些示例是恶意的，哪些是良性的。另一类机器学习算法，*无监督算法*，则不需要我们知道训练集中哪些示例是恶意的，哪些是良性的。这些算法在检测恶意软件和恶意行为方面的效果要差得多，我们在本书中不会涵盖这些算法。*'
- en: '***Testing Machine Learning Systems***'
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***测试机器学习系统***'
- en: 'Once you’ve trained your machine learning system, you need to check how accurate
    it is. You do this by running the trained system on data that you didn’t train
    it on and seeing how well it determines whether or not the binaries are malicious
    or benign. In security, we typically train our systems on binaries that we gathered
    up to some point in time, and then we test on binaries that we saw *after* that
    point in time, to measure how well our systems will detect new malware, and to
    measure how well our systems will avoid producing false positives on new benignware.
    Most machine learning research involves thousands of iterations that go something
    like this: we create a machine learning system, test it, and then tweak it, train
    it again, and test it again, until we’re satisfied with the results. I’ll cover
    testing machine learning systems in detail in [Chapter 8](ch08.xhtml#ch08).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你训练好你的机器学习系统，你需要检查它的准确性。你可以通过在系统没有接受过训练的数据上运行该系统，并观察它判断二进制文件是恶意还是良性的效果来做到这一点。在安全领域，我们通常会使用我们收集到的一些二进制文件来训练系统，然后测试系统在*此后*看到的二进制文件，以衡量我们的系统检测新恶意软件的效果，并衡量系统在处理新良性文件时避免产生误报的能力。大多数机器学习研究涉及数千次的迭代，过程大致如下：我们创建一个机器学习系统，测试它，然后调整它，重新训练，再次测试，直到我们对结果满意为止。我将在[第8章](ch08.xhtml#ch08)中详细介绍机器学习系统的测试。
- en: Let’s now discuss how a variety of machine learning algorithms work. This is
    the hard part of the chapter, but also the most rewarding if you take the time
    to understand it. In this discussion, I talk about the unifying ideas that underlie
    these algorithms and then move on to each algorithm in detail.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们讨论各种机器学习算法是如何工作的。这是本章的难点，但如果你花时间去理解它，它也是最有收获的部分。在这个讨论中，我将讲述这些算法背后的统一思想，然后详细介绍每种算法。
- en: '**Understanding Feature Spaces and Decision Boundaries**'
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**理解特征空间和决策边界**'
- en: 'Two simple geometric ideas can help you understand all machine learning–based
    detection algorithms: the idea of a geometrical feature space and the idea of
    a decision boundary. A *feature space* is the geometrical space defined by the
    features you’ve selected, and a *decision boundary* is a geometrical structure
    running through this space such that binaries on one side of this boundary are
    defined as malware, and binaries on the other side of the boundary are defined
    as benignware. When we use a machine learning algorithm to classify files as malicious
    or benign, we extract features so that we can place the samples in the feature
    space, and then we check which side of the decision boundary the samples are on
    to determine whether the files are malware or benignware.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 两个简单的几何概念可以帮助你理解所有基于机器学习的检测算法：几何特征空间的概念和决策边界的概念。*特征空间*是由你选择的特征定义的几何空间，*决策边界*是在该空间中划定的几何结构，使得在该边界一侧的二进制文件被定义为恶意软件，而在边界另一侧的二进制文件被定义为良性文件。当我们使用机器学习算法将文件分类为恶意或良性时，我们提取特征以便将样本放入特征空间中，然后检查样本处于决策边界的哪一侧，以确定这些文件是恶意软件还是良性文件。
- en: This geometrical way of understanding feature spaces and decision boundaries
    is accurate for systems that operate on feature spaces of one, two, or three dimensions
    (features), but it also holds for feature spaces with millions of dimensions,
    even though it’s impossible to visualize or conceive of million-dimensional spaces.
    We’ll stick to examples with two dimensions in this chapter to make them easy
    to visualize, but just remember that real-world security machine learning systems
    pretty much always use hundreds, thousands, or millions of dimensions, and the
    basic concepts we discuss in a two-dimensional context hold for real-world systems
    that have more than two dimensions.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这种几何方式理解特征空间和决策边界对于在一维、二维或三维（特征）的特征空间中操作的系统是准确的，但它同样适用于具有数百万维的特征空间，尽管我们无法想象或可视化百万维空间。在本章中，我们将使用二维的示例以便于可视化，但请记住，现实世界中的安全机器学习系统几乎总是使用数百、数千甚至数百万维的特征空间，而我们在二维背景下讨论的基本概念同样适用于拥有超过二维的实际系统。
- en: 'Let’s create a toy malware detection problem to clarify the idea of a decision
    boundary in a feature space. Suppose we have a training dataset consisting of
    malware and benignware samples. Now suppose we extract the following two features
    from each binary: the percentage of the file that appears to be compressed, and
    the number of suspicious functions each binary imports. We can visualize our training
    dataset as shown in [Figure 6-1](ch06.xhtml#ch06fig1) (bear in mind I created
    the data in the plot artificially, for example purposes).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个玩具恶意软件检测问题，以便澄清特征空间中决策边界的概念。假设我们有一个训练数据集，其中包含恶意软件和良性软件样本。现在假设我们从每个二进制文件中提取以下两个特征：文件中看似被压缩的部分的百分比，以及每个二进制文件导入的可疑函数的数量。我们可以如[图
    6-1](ch06.xhtml#ch06fig1)所示可视化我们的训练数据集（请注意，我在图中人工创建了数据，供示例使用）。
- en: '![image](../images/f0094-01.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0094-01.jpg)'
- en: '*Figure 6-1: A plot of a sample dataset we’ll use in this chapter, where gray
    dots are benignware and black dots are malware*'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6-1：我们将在本章中使用的样本数据集的图示，其中灰色点为良性软件，黑色点为恶意软件*'
- en: 'The two-dimensional space shown in [Figure 6-1](ch06.xhtml#ch06fig1), which
    is defined by our two features, is the feature space for our sample dataset. You
    can see a clear pattern in which the black dots (the malware) are generally in
    the upper-right part of the space. In general, these have more suspicious imported
    function calls and more compressed data than the benignware, which mostly inhabits
    the lower-left part of the plot. Suppose, after viewing this plot, you were asked
    to create a malware detection system based solely on the two features we’re using
    here. It seems clear that, based on the data, you can formulate the following
    rule: if a binary has both a lot of compressed data and a lot of suspicious imported
    function calls, it’s malware, and if it has neither a lot of suspicious imported
    calls nor much compressed data, it’s benignware.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6-1](ch06.xhtml#ch06fig1)中显示的二维空间是由我们的两个特征定义的，它是我们样本数据集的特征空间。你可以看到一个明显的模式，黑色的点（恶意软件）通常位于空间的右上部分。一般来说，这些点的导入函数调用比良性软件更多，压缩数据也更多，而良性软件主要分布在图表的左下部分。假设在查看完这个图表后，你被要求仅根据我们在这里使用的两个特征来创建一个恶意软件检测系统。基于数据，似乎很明显，你可以制定如下规则：如果一个二进制文件同时具有大量压缩数据和大量可疑导入函数调用，它是恶意软件；如果既没有大量可疑导入调用，也没有太多压缩数据，它就是良性软件。'
- en: In geometrical terms, we can visualize this rule as a diagonal line that separates
    the malware samples from the benignware samples in the feature space, such that
    binaries with sufficient compressed data and imported function calls (defined
    as malware) are above the line, and the rest of the binaries (defined as benignware)
    are below the line. [Figure 6-2](ch06.xhtml#ch06fig2) shows such a line, which
    we call a decision boundary.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 从几何学角度来看，我们可以将这个规则可视化为一条对角线，将恶意软件样本与良性软件样本在特征空间中分开，使得具有足够压缩数据和导入函数调用的二进制文件（定义为恶意软件）位于线的上方，而其余的二进制文件（定义为良性软件）位于线的下方。[图
    6-2](ch06.xhtml#ch06fig2)展示了这样一条线，我们称之为决策边界。
- en: '![image](../images/f0095-01.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0095-01.jpg)'
- en: '*Figure 6-2: A decision boundary drawn through our sample dataset, which defines
    a rule for detecting malware*'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6-2：通过我们样本数据集绘制的决策边界，用于定义恶意软件检测规则*'
- en: As you can see from the line, *most* of the black (malware) dots are on one
    side of the boundary, and *most* of the gray (benignware) samples are on the other
    side of the decision boundary. Note that it’s impossible to draw a line that separates
    *all* of the samples from one another, because the black and gray clouds in this
    dataset overlap one another. But from looking at this example, it appears we’ve
    drawn a line that will correctly classify new malware samples and benignware samples
    in most cases, assuming they follow the pattern seen in the data in this image.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 从这条线可以看出，*大多数*黑色（恶意软件）点位于边界的一侧，*大多数*灰色（良性软件）样本位于决策边界的另一侧。请注意，无法绘制一条将*所有*样本完全分开的线，因为这个数据集中的黑色和灰色云团是重叠的。但是通过查看这个例子，我们可以看出我们绘制的这条线在大多数情况下会正确分类新的恶意软件样本和良性软件样本，前提是它们遵循图中数据所显示的模式。
- en: In [Figure 6-2](ch06.xhtml#ch06fig2), we manually drew a decision boundary through
    our data. But what if we want a more exact decision boundary and want to do it
    in an automated way? This is exactly what machine learning does. In other words,
    all machine learning detection algorithms look at data and use an automated process
    to determine the ideal decision boundary, such that there’s the greatest chance
    of correctly performing detection on new, previously unseen data.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 6-2](ch06.xhtml#ch06fig2)中，我们手动绘制了一个决策边界。那么如果我们想要一个更精确的决策边界，并希望以自动化的方式进行处理呢？这正是机器学习所做的事情。换句话说，所有的机器学习检测算法都会查看数据，并使用自动化过程来确定理想的决策边界，以便最大限度地提高正确检测新数据、之前未见数据的机会。
- en: Let’s look at the way a real-world, commonly used machine learning algorithm
    identifies a decision boundary within the sample data shown in [Figure 6-3](ch06.xhtml#ch06fig3).
    This example uses an algorithm called logistic regression.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下一个常见的机器学习算法如何识别在[图 6-3](ch06.xhtml#ch06fig3)中显示的样本数据中的决策边界。这个示例使用了一种名为逻辑回归的算法。
- en: '![image](../images/f0096-01.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0096-01.jpg)'
- en: '*Figure 6-3: The decision boundary automatically created by training a logistic
    regression model*'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6-3：通过训练逻辑回归模型自动创建的决策边界*'
- en: Notice that we’re using the same sample data we used in the previous plots,
    where gray dots are benignware and black dots are malware. The line running through
    the center of the plot is the decision boundary that the logistic regression algorithm
    *learns* by looking at the data. On the right side of the line, the logistic regression
    algorithm assigns a greater than 50 percent probability that binaries are malware,
    and on the left side of the line, it assigns a less than 50 percent probability
    that a binary is malware.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用的是之前图表中相同的样本数据，其中灰点表示良性软件，黑点表示恶意软件。图中通过中心的线是逻辑回归算法通过观察数据*学习*到的决策边界。在线的右侧，逻辑回归算法将二进制文件是恶意软件的概率评估为大于
    50%，而在线的左侧，则将二进制文件是恶意软件的概率评估为小于 50%。
- en: Now note the shaded regions of the plot. The dark gray shaded region is the
    region where the logistic regression model is highly confident that files are
    malware. Any new file the logistic regression model sees that has features that
    land in this region should have a high probability of being malware. As we get
    closer and closer to the decision boundary, the model has less and less confidence
    about whether or not binaries are malware or benignware. Logistic regression allows
    us to easily move the line up into the darker region or down into the lighter
    region, depending on how aggressive we want to be about detecting malware. For
    example, if we move it down, we’ll catch more malware, but get more false positives.
    If we move it up, we’ll catch less malware, but get fewer false positives.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在请注意图表中的阴影区域。深灰色阴影区域是逻辑回归模型对文件是恶意软件非常有信心的区域。任何逻辑回归模型看到的新文件，只要其特征落在该区域内，应该具有较高的恶意软件概率。随着我们越来越接近决策边界，模型对文件是否是恶意软件或良性软件的信心越来越低。逻辑回归允许我们根据想要多积极地检测恶意软件，轻松地将决策边界向深色区域移动或向浅色区域移动。例如，如果我们将它向下移动，我们会捕捉到更多的恶意软件，但会增加误报。如果我们将它向上移动，我们会捕捉到更少的恶意软件，但会减少误报。
- en: I want to emphasize that logistic regression, and all other machine learning
    algorithms, can operate in arbitrarily high dimensional feature spaces. [Figure
    6-4](ch06.xhtml#ch06fig4) illustrates how logistic regression works in a slightly
    higher dimensional feature space.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我想强调的是，逻辑回归和所有其他机器学习算法都可以在任意高维的特征空间中运行。[图 6-4](ch06.xhtml#ch06fig4)展示了逻辑回归在稍高维度的特征空间中的工作原理。
- en: In this higher-dimensional space, the decision boundary is not a line, but a
    *plane* separating the points in the 3D volume. If we were to move to four or
    more dimensions, logistic regression would create a *hyperplane*, which is an
    *n*-dimensional plane-like structure that separates the malware from benignware
    points in this high dimensional space.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个高维空间中，决策边界不再是直线，而是一个*平面*，它将三维空间中的点分开。如果我们转到四维或更多维度，逻辑回归将创建一个*超平面*，它是一个*n*维的平面状结构，将高维空间中的恶意软件点和良性软件点分开。
- en: '![image](../images/f0097-01.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0097-01.jpg)'
- en: '*Figure 6-4: A planar decision boundary through a hypothetical three dimensional
    feature space created by logistic regression*'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6-4：逻辑回归创建的通过假设的三维特征空间的平面决策边界*'
- en: Because logistic regression is a relatively simple machine learning algorithm,
    it can only create simple geometrical decision boundaries such as lines, planes,
    and higher dimensional planes. Other machine learning algorithms can create decision
    boundaries that are more complex. Consider, for example, the decision boundary
    shown in [Figure 6-5](ch06.xhtml#ch06fig5), given by the k-nearest neighbors algorithm
    (which I discuss in detail shortly).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 由于逻辑回归是一个相对简单的机器学习算法，它只能创建简单的几何决策边界，如直线、平面和更高维度的平面。其他机器学习算法可以创建更复杂的决策边界。举个例子，考虑[图
    6-5](ch06.xhtml#ch06fig5)所示的决策边界，它是由 k 最近邻算法（我稍后会详细讨论）给出的。
- en: '![image](../images/f0097-02.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0097-02.jpg)'
- en: '*Figure 6-5: A decision boundary created by the k-nearest neighbors algorithm*'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6-5：由 k 最近邻算法创建的决策边界*'
- en: 'As you can see, this decision boundary isn’t a plane: it’s a highly irregular
    structure. Also note that some machine learning algorithms can generate disjointed
    decision boundaries, which define some regions of the feature space as malicious
    and some regions as benign, even if those regions are not contiguous. [Figure
    6-6](ch06.xhtml#ch06fig6) shows a decision boundary with this irregular structure,
    using a different sample dataset with a more complex pattern of malware and benignware
    in our sample feature space.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这个决策边界不是平面，而是一个高度不规则的结构。同时需要注意的是，一些机器学习算法可以生成不相交的决策边界，这些边界将特征空间中的某些区域定义为恶意区，而某些区域定义为良性区，即使这些区域不是连续的。[图
    6-6](ch06.xhtml#ch06fig6)展示了一个具有这种不规则结构的决策边界，使用了一个具有更复杂恶意软件和良性软件模式的不同样本数据集。
- en: '![image](../images/f0098-01.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0098-01.jpg)'
- en: '*Figure 6-6: A disjoint decision boundary created by the k-nearest neighbors
    algorithm*'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6-6：由 k 最近邻算法创建的不相交决策边界*'
- en: Even though the decision boundary is noncontiguous, it’s still common machine
    learning parlance to call these disjoint decision boundaries simply “decision
    boundaries.” You can use different machine learning algorithms to express different
    types of decision boundaries, and this difference in expressivity is why we might
    pick one machine learning algorithm over another for a given project.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管决策边界是不连续的，但在机器学习术语中，通常仍将这些不相交的决策边界称为“决策边界”。你可以使用不同的机器学习算法来表达不同类型的决策边界，这种表达能力的差异也是我们在某些项目中选择一种机器学习算法而非另一种的原因。
- en: Now that we’ve explored core machine learning concepts like feature spaces and
    decision boundaries, let’s discuss what machine learning practitioners call overfitting
    and underfitting next.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探讨了核心的机器学习概念，如特征空间和决策边界，接下来让我们讨论机器学习从业者所称的过拟合与欠拟合。
- en: '**What Makes Models Good or Bad: Overfitting and Underfitting**'
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**模型好坏的决定因素：过拟合与欠拟合**'
- en: I can’t overemphasize the importance of overfitting and underfitting in machine
    learning. Avoiding both cases is what defines a good machine learning algorithm.
    Good, accurate detection models in machine learning capture the general trend
    in what the training data says about what distinguishes malware from benignware,
    without getting distracted by the outliers or the exceptions that prove the rule.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我不能过于强调过拟合与欠拟合在机器学习中的重要性。避免这两种情况是定义一个好的机器学习算法的标准。优秀、准确的检测模型能够捕捉到训练数据中关于区分恶意软件与良性软件的总体趋势，而不会被异常值或那些证明规则的例外所干扰。
- en: Underfit models ignore outliers but fail to capture the general trend, resulting
    in poor accuracy on new, previously unseen binaries. Overfit models get distracted
    by outliers in ways that don’t reflect the general trend, and they yield poor
    accuracy on previously unseen binaries. Building machine learning malware detection
    models is all about capturing the general trend that distinguishes the malicious
    from the benign.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 欠拟合的模型忽略了异常值，但未能捕捉到总体趋势，导致在新数据和以前未见过的二进制文件上准确性较差。过拟合的模型被异常值干扰，方式与总体趋势无关，因此在以前未见过的二进制文件上也会产生较差的准确性。构建机器学习恶意软件检测模型的核心在于捕捉区分恶意与良性的软件的总体趋势。
- en: Let’s use the examples of underfit, well fit, and overfit models in [Figures
    6-7](ch06.xhtml#ch06fig7), [6-8](ch06.xhtml#ch06fig8), and [6-9](ch06.xhtml#ch06fig9)
    to illustrate these terms. [Figure 6-7](ch06.xhtml#ch06fig7) shows an underfit
    model.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过[图 6-7](ch06.xhtml#ch06fig7)、[6-8](ch06.xhtml#ch06fig8)和[6-9](ch06.xhtml#ch06fig9)中的欠拟合、拟合良好和过拟合模型的例子来说明这些术语。[图
    6-7](ch06.xhtml#ch06fig7)展示了一个欠拟合的模型。
- en: '![image](../images/f0099-01.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0099-01.jpg)'
- en: '*Figure 6-7: An underfit machine learning model*'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6-7：欠拟合的机器学习模型*'
- en: Here, you can see the black dots (malware) cluster in the upper-right region
    of the plot, and the gray dots (benignware) cluster in the lower left. However,
    our machine learning model simply slices the dots down the middle, crudely separating
    the data without capturing the diagonal trend. Because the model doesn’t capture
    the general trend, we say that it is underfit.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以看到黑色的点（恶意软件）聚集在图的右上方区域，而灰色的点（良性软件）聚集在左下方。然而，我们的机器学习模型仅仅将这些点从中间划分，粗略地分隔数据，而没有捕捉到斜向的趋势。由于模型没有捕捉到整体趋势，我们称之为欠拟合。
- en: 'Also note that there are only two shades of certainty that the model gives
    in all of the regions of the plot: either the shade is dark gray or it’s white.
    In other words, the model is either absolutely certain that points in the feature
    space are malicious or absolutely certain they’re benign. This inability to express
    certainty correctly is also a reason this model is underfit.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 还需要注意的是，模型在图的所有区域中仅给出两种确定性：要么阴影是深灰色，要么是白色。换句话说，模型要么完全确定特征空间中的点是恶意的，要么完全确定它们是良性的。无法正确表达确定性也是该模型欠拟合的一个原因。
- en: Let’s contrast the underfit model in [Figure 6-7](ch06.xhtml#ch06fig7) with
    the well-fit model in [Figure 6-8](ch06.xhtml#ch06fig8).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对比[图6-7](ch06.xhtml#ch06fig7)中的欠拟合模型与[图6-8](ch06.xhtml#ch06fig8)中的良拟合模型。
- en: '![image](../images/f0100-01.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0100-01.jpg)'
- en: '*Figure 6-8: A well-fit machine learning model*'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6-8：良拟合的机器学习模型*'
- en: In this case, the model not only captures the general trend in the data but
    also creates a reasonable model of certainty with respect to its estimate of which
    regions of the feature space are definitely malicious, definitely benign, or are
    in a gray area.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，模型不仅捕捉到了数据的整体趋势，还根据其对特征空间中哪些区域是明确恶意、明确良性或处于灰色区域的估计，创建了一个合理的确定性模型。
- en: 'Note the decision line running from the top to the bottom of this plot. The
    model has a simple theory about what divides the malware from the benignware:
    a vertical line with a diagonal notch in the middle of the plot. Also note the
    shaded regions in the plot, which tells us that the model is only certain that
    data in the upper-right part of the plot are malware, and only certain that binaries
    in the lower-left corner of the plot are benignware.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，从图的顶部到底部的决策线。模型有一个简单的理论来区分恶意软件和良性软件：在图的中间有一条垂直线和一个对角切口。同时请注意图中的阴影区域，它告诉我们模型仅确定图右上方的区域是恶意软件，并且仅确定图左下角的二进制文件是良性软件。
- en: Finally, let’s contrast the overfit model shown next in [Figure 6-9](ch06.xhtml#ch06fig9)
    to the underfit model you saw in [Figure 6-7](ch06.xhtml#ch06fig7) as well as
    the well-fit model in [Figure 6-8](ch06.xhtml#ch06fig8).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们对比下图中的过拟合模型[图6-9](ch06.xhtml#ch06fig9)与[图6-7](ch06.xhtml#ch06fig7)中看到的欠拟合模型，以及[图6-8](ch06.xhtml#ch06fig8)中的良拟合模型。
- en: The overfit model in [Figure 6-9](ch06.xhtml#ch06fig9) fails to capture the
    general trend in the data. Instead, it obsesses over the exceptions in the data,
    including the handful of black dots (malware training examples) that occur in
    the cluster of gray dots (benign training examples) and draws decision boundaries
    around them. Similarly, it focuses on the handful of benignware examples that
    occur in the malware cluster, drawing boundaries around those as well.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6-9](ch06.xhtml#ch06fig9)中的过拟合模型未能捕捉到数据的整体趋势。相反，它过度关注数据中的例外情况，包括发生在灰色点（良性训练样本）聚集中的少量黑点（恶意软件训练样本），并围绕它们画出决策边界。同样，它也关注在恶意软件聚集中的少数良性软件样本，并围绕它们画出边界。'
- en: This means that when we see new, previously unseen binaries that happen to have
    features that place them close to these outliers, the machine learning model will
    think they are malware when they are almost definitely benignware, and vice versa.
    In practice, this means that this model won’t be as accurate as it could be.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着当我们看到新的、以前未见过的二进制文件，并且它们恰好具有将它们置于这些异常值附近的特征时，机器学习模型会认为它们是恶意软件，而它们几乎肯定是良性软件，反之亦然。实际上，这意味着该模型的准确性不会达到它应有的水平。
- en: '![image](../images/f0101-01.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0101-01.jpg)'
- en: '*Figure 6-9: An overfit machine learning model*'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6-9：过拟合的机器学习模型*'
- en: '**Major Types of Machine Learning Algorithms**'
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**主要类型的机器学习算法**'
- en: 'So far I’ve discussed machine learning in very general terms, touching on two
    machine learning methods: logistic regression and k-nearest neighbors. In the
    remainder of this chapter, we delve deeper and discuss logistic regression, k-nearest
    neighbors, decision trees, and random forest algorithms in more detail. We use
    these algorithms quite often in the security data science community. These algorithms
    are complex, but the ideas behind them are intuitive and straightforward.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我已经用非常概括的方式讨论了机器学习，涉及了两种机器学习方法：逻辑回归和k近邻。在本章的剩余部分，我们将更深入地探讨逻辑回归、k近邻、决策树和随机森林算法。我们在安全数据科学社区中经常使用这些算法。这些算法比较复杂，但它们背后的理念是直观且简单的。
- en: First, let’s look at the sample datasets we use to explore the strengths and
    weaknesses of each algorithm, shown in [Figure 6-10](ch06.xhtml#ch06fig10).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们来看一下我们用来探索每种算法优缺点的样本数据集，见[图 6-10](ch06.xhtml#ch06fig10)。
- en: I created these datasets for example purposes. On the left, we have our simple
    dataset, which I’ve already used in [Figures 6-7](ch06.xhtml#ch06fig7), [6-8](ch06.xhtml#ch06fig8),
    and [6-9](ch06.xhtml#ch06fig9). In this case, we can separate the black training
    examples (malware) from the gray training examples (benignware) using a simple
    geometric structure such as a line.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我创建了这些数据集作为示例。在左侧，我们有我们的简单数据集，我已经在[图 6-7](ch06.xhtml#ch06fig7)、[6-8](ch06.xhtml#ch06fig8)和[6-9](ch06.xhtml#ch06fig9)中使用过。在这种情况下，我们可以用简单的几何结构（如一条线）将黑色训练示例（恶意软件）与灰色训练示例（良性软件）分开。
- en: 'The dataset on the right, which I’ve already shown in [Figure 6-6](ch06.xhtml#ch06fig6),
    is complex because we can’t separate the malware from the benignware using a simple
    line. But there is still a clear pattern to the data: we just have to use more
    complex methods to create a decision boundary. Let’s see how different algorithms
    perform with these two sample datasets.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧的数据集，我在[图 6-6](ch06.xhtml#ch06fig6)中已经展示过，它之所以复杂，是因为我们无法仅用一条简单的线将恶意软件与良性软件区分开。但数据中仍然存在明显的模式：我们只需要使用更复杂的方法来创建决策边界。让我们看看不同算法在这两个样本数据集上的表现如何。
- en: '![image](../images/f0102-01.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0102-01.jpg)'
- en: '*Figure 6-10: The two sample datasets we use in this chapter, with black dots
    representing malware and gray dots representing benignware*'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6-10：我们在本章中使用的两个样本数据集，黑点表示恶意软件，灰点表示良性软件*'
- en: '***Logistic Regression***'
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***逻辑回归***'
- en: As you learned previously, logistic regression is a machine learning algorithm
    that creates a line, plane, or hyperplane (depending on how many features you
    provide) that geometrically separates your training malware from your training
    benignware. When you use the trained model to detect new malware, logistic regression
    checks whether a previously unseen binary is on the malware side or the benignware
    side of the boundary to determine whether it’s malicious or benign.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如你之前所学，逻辑回归是一种机器学习算法，它创建一条线、一个平面或一个超平面（取决于你提供的特征数量），在几何上将训练数据中的恶意软件与良性软件分开。当你使用训练好的模型来检测新的恶意软件时，逻辑回归会检查一个先前未见过的二进制文件是否位于恶意软件的一侧还是良性软件的一侧，从而判断它是恶意的还是良性的。
- en: A limitation of logistic regression is that if your data can’t be separated
    simply using a line or hyperplane, logistic regression is not the right solution.
    Whether or not you can use logistic regression for your problem depends on your
    data and your features. For example, if your problem has lots of individual features
    that on their own are strong indicators of maliciousness (or “benignness”), then
    logistic regression might be a winning approach. If your data is such that you
    need to use complex relationships between features to decide that a file is malware,
    then another approach, like k-nearest neighbors, decision trees, or random forest,
    might make more sense.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归的一个局限性是，如果你的数据无法通过简单的一条线或超平面分开，那么逻辑回归就不是正确的解决方案。你是否可以使用逻辑回归来解决你的问题，取决于你的数据和特征。例如，如果你的问题有许多单独的特征，而这些特征本身就是恶意性（或“良性”）的强指示器，那么逻辑回归可能是一个有效的方法。如果你的数据需要使用特征之间的复杂关系来判断文件是否是恶意软件，那么其他方法，比如k近邻、决策树或随机森林，可能会更合适。
- en: To illustrate the strengths and weaknesses of logistic regression, let’s look
    at the performance of logistic regression on our two sample datasets, as shown
    in [Figure 6-11](ch06.xhtml#ch06fig11). We see that logistic regression yields
    a very effective separation of the malware and benignware in our simple dataset
    (on the left). In contrast, the performance of logistic regression on our complex
    dataset (on the right) is not effective. In this case, the logistic regression
    algorithm gets confused, because it can only express a linear decision boundary.
    You can see both binary types on both sides of the line, and the shaded gray confidence
    bands don’t really make any sense relative to the data. For this more complex
    dataset, we’d need to use an algorithm capable of expressing more geometric structures.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明逻辑回归的优缺点，我们来看看逻辑回归在我们的两个示例数据集上的表现，如[图6-11](ch06.xhtml#ch06fig11)所示。我们看到，逻辑回归在我们的简单数据集（左侧）上能有效地区分恶意软件和良性软件。相比之下，逻辑回归在我们复杂数据集（右侧）上的表现则不够理想。在这种情况下，逻辑回归算法变得困惑，因为它只能表示线性决策边界。你可以看到，线上两侧都有这两种二元类型，而阴影部分的灰色置信带与数据相比并没有多大意义。对于这个更复杂的数据集，我们需要使用一种能够表达更多几何结构的算法。
- en: '![image](../images/f0103-01.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0103-01.jpg)'
- en: '*Figure 6-11: A decision boundary drawn through our sample datasets using logistic
    regression*'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6-11：使用逻辑回归绘制的决策边界，基于我们的示例数据集*'
- en: '**The Math Behind Logistic Regression**'
  id: totrans-103
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**逻辑回归背后的数学原理**'
- en: Let’s now look at the math behind how logistic regression detects malware samples.
    [Listing 6-1](ch06.xhtml#ch06list1) shows Pythonic pseudocode for computing the
    probability that a binary is malware using logistic regression.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看逻辑回归如何检测恶意软件样本背后的数学原理。[清单6-1](ch06.xhtml#ch06list1)展示了用于计算一个二进制文件是否是恶意软件的逻辑回归的Python伪代码。
- en: '[PRE0]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*Listing 6-1: Pseudocode using logistic regression to calculate probability*'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单6-1：使用逻辑回归计算概率的伪代码*'
- en: Let’s step through the code to understand what this means. We first define the
    `logistic_regression` function ➊ and its parameters. Its parameters are the features
    of the binary (`compressed_data` and `suspicious_calls`) that represent the amount
    of compressed data and the number of suspicious calls it makes, respectively,
    and the parameter `learned_parameters` stands for the elements of the logistic
    regression function that were learned by training the logistic regression model
    on training data. I discuss how the parameters were learned later in this chapter;
    for now, just accept that they were derived from the training data.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步分析代码，理解这意味着什么。我们首先定义`logistic_regression`函数➊及其参数。其参数是代表二进制特征（`compressed_data`和`suspicious_calls`）的特征，分别表示压缩数据的量和它所发出的可疑调用次数，`learned_parameters`参数表示通过在训练数据上训练逻辑回归模型所学到的逻辑回归函数元素。我将在本章稍后讨论这些参数是如何被学习的；现在，先接受它们是从训练数据中得出的。
- en: Then, we take the `compressed_data` feature ➋ and multiply it by the `compressed_data_weight`
    parameter. This weight scales the feature up or down, depending on how indicative
    of malware the logistic regression function thinks this feature is. Note that
    the weight can also be negative, which indicates that the logistic regression
    model thinks that the feature is an indicator of a file being benign.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们取`compressed_data`特征➋并将其乘以`compressed_data_weight`参数。这个权重根据逻辑回归函数判断该特征对恶意软件的指示性来调整特征的大小。请注意，权重也可以是负数，这表示逻辑回归模型认为该特征是文件为良性指示的标志。
- en: On the line below that, we perform the same step for the `suspicious_calls`
    parameter. Then, we add these two weighted features together ➌, plus add in a
    parameter called the `bias` parameter (also learned from training data). In sum,
    we take the `compressed_data` feature, scaled by how indicative of maliciousness
    we believe it to be, add the `suspicious_calls` feature, also scaled by how indicative
    of maliciousness we believe it to be, and add the `bias` parameter, which indicates
    how suspicious the logistic regression model thinks we should be of files in general.
    The result of these additions and multiplications is a `score` indicating how
    likely it is that a given file is malicious.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在下方的那一行，我们对`suspicious_calls`参数执行相同的步骤。然后，我们将这两个加权特征相加 ➌，再加上一个称为`bias`的参数（也是从训练数据中学习到的）。总的来说，我们取`compressed_data`特征，按其与恶意性的相关性进行缩放，添加`suspicious_calls`特征，也按其与恶意性的相关性进行缩放，再加上`bias`参数，后者表示逻辑回归模型认为我们应该对文件的可疑性有多大的警觉性。通过这些加法和乘法操作，我们得到一个`score`，它表示一个给定文件是恶意的可能性。
- en: Finally, we use `logistic_function` ➍ to convert our suspiciousness score into
    a probability. [Figure 6-12](ch06.xhtml#ch06fig12) visualizes how this function
    works.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用`logistic_function` ➍将我们的可疑性得分转换为概率。[图6-12](ch06.xhtml#ch06fig12)直观地展示了这个函数是如何工作的。
- en: '![image](../images/f0104-01.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0104-01.jpg)'
- en: '*Figure 6-12: A plot of the logistic function used in logistic regression*'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6-12：逻辑回归中使用的逻辑函数的图示*'
- en: Here, the logistic function takes a score (shown on the x-axis) and translates
    it into a value that’s bounded between 0 and 1 (a probability).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，逻辑函数接受一个分数（显示在x轴上），并将其转换为一个介于0和1之间的值（即概率）。
- en: '**How the Math Works**'
  id: totrans-114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**数学原理如何运作**'
- en: 'Let’s return to the decision boundaries you saw in [Figure 6-11](ch06.xhtml#ch06fig11)
    to see how this math works in practice. Recall how we compute our probability:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到[图6-11](ch06.xhtml#ch06fig11)中看到的决策边界，看看这个数学是如何在实践中运作的。回想一下我们是如何计算我们的概率的：
- en: '[PRE1]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: For example, if we were to plot the resulting probabilities at every point in
    the feature spaces shown in [Figure 6-11](ch06.xhtml#ch06fig11) using the same
    feature weights and `bias` parameter, we’d wind up with the shaded regions shown
    in the same figure, which shows where the model “thinks” malicious and benign
    samples lie, and with how much confidence.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们在[图6-11](ch06.xhtml#ch06fig11)中展示的特征空间的每个点上绘制结果概率，使用相同的特征权重和`bias`参数，我们最终会得到该图中显示的阴影区域，这些区域展示了模型“认为”恶意样本和良性样本的位置以及它的置信度。
- en: If we were then to set a threshold of 0.5 (recall that at a probability of greater
    than 50 percent, files are defined as malicious), the line in [Figure 6-11](ch06.xhtml#ch06fig11)
    would appear as our decision boundary. I encourage you to experiment with my sample
    code, plug in some feature weights and a bias term, and try it yourself.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们设置一个阈值为0.5（记住，当概率超过50%时，文件被定义为恶意的），那么在[图6-11](ch06.xhtml#ch06fig11)中，线条就会作为我们的决策边界。我鼓励你实验我的示例代码，输入一些特征权重和偏差项，亲自试试。
- en: '**NOTE**'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*Logistic regression doesn’t constrain us to using only two features. In reality,
    we usually use scores or hundreds or even thousands of features with logistic
    regression. But the math doesn’t change: we just compute our probability as follows
    for any number of features:*'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '*逻辑回归并不局限于仅使用两个特征。实际上，我们通常使用逻辑回归来处理数十、数百甚至数千个特征。但数学原理并没有改变：我们只需按照以下方式计算任何数量特征的概率：*'
- en: '[PRE2]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: So how exactly does logistic regression learn to place the decision boundary
    in the right place based on the training data? It uses an iterative, calculus-based
    approach called *gradient descent*. We won’t get into the details of this approach
    in this book, but the basic idea is that the line, plane, or hyperplane (depending
    on the number of features you’re using) is iteratively adjusted such that it maximizes
    the probability that the logistic regression model will get the answer right when
    asked if a data point in the training set is either a malware sample or a benignware
    sample.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，逻辑回归是如何根据训练数据将决策边界放置在正确的位置的呢？它使用一种基于微积分的迭代方法，称为*梯度下降*。我们在本书中不会深入探讨这种方法的细节，但基本的思路是：无论使用的是线、平面还是超平面（这取决于你使用的特征数量），它都会通过迭代调整，最大化逻辑回归模型在训练集中的数据点是否为恶意样本或良性样本时给出正确答案的概率。
- en: You can train logistic regression models to bias the logistic regression learning
    algorithm toward coming up with simpler or more complex theories about what constitutes
    malware and benignware. These training methods are beyond the scope of this book,
    but if you’re interested in learning about these helpful methods, I encourage
    you to Google “logistic regression and regularization” and read explanations of
    them online.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以训练逻辑回归模型，调整逻辑回归学习算法，以便得出关于什么构成恶意软件和良性软件的更简单或更复杂的理论。这些训练方法超出了本书的范围，但如果你有兴趣了解这些有用的方法，我鼓励你在谷歌上搜索“逻辑回归与正则化”，并在线阅读相关解释。
- en: '**When to Use Logistic Regression**'
  id: totrans-124
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**何时使用逻辑回归**'
- en: Logistic regression has distinct advantages and disadvantages relative to other
    machine learning algorithms. An advantage of logistic regression is that one can
    easily interpret what a logistic regression model thinks constitutes benignware
    and malware. For example, we can understand a given logistic regression model
    by looking at its feature weights. Features that have high weight are those the
    model interprets as malicious. Features with negative weight are those the model
    believes are benignware. Logistic regression is a fairly simple approach, and
    when the data you’re working with contains clear indicators of maliciousness,
    it can work well. But when the data is more complex, logistic regression often
    fails.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归相对于其他机器学习算法有明显的优缺点。逻辑回归的一个优点是可以轻松解释逻辑回归模型认为构成良性软件和恶意软件的标准。例如，我们可以通过查看模型的特征权重来理解一个给定的逻辑回归模型。权重较高的特征是模型认为恶意的特征，负权重的特征则是模型认为良性的特征。逻辑回归是一种相对简单的方法，当你所处理的数据中包含明确的恶意指示时，它可以很好地工作。但当数据更复杂时，逻辑回归往往会失败。
- en: 'Now let’s explore another simple machine learning approach that can express
    much more complex decision boundaries: k-nearest neighbors.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们探讨另一种简单的机器学习方法，它可以表示更复杂的决策边界：k-近邻算法。
- en: '***K-Nearest Neighbors***'
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***K-近邻算法***'
- en: K-nearest neighbors is a machine learning algorithm based on the idea that if
    a binary in the feature space is close to other binaries that are malicious, then
    it’s malicious, and if its features place it close to binaries that are benign,
    it must be benign. More precisely, if the majority of the *k* closest binaries
    to an unknown binary are malicious, the file is malicious. Note that *k* represents
    the number of nearby neighbors that we pick and define ourselves, depending on
    how many neighbors we think should be involved in determining whether a sample
    is benign or malicious.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: k-近邻算法是一种基于这样一个思想的机器学习算法：如果一个二进制文件在特征空间中接近其他恶意的二进制文件，那么它就是恶意的；如果它的特征使得它接近良性二进制文件，那么它必须是良性的。更准确地说，如果与一个未知二进制文件最接近的*k*个二进制文件中的大多数是恶意的，那么该文件就是恶意的。请注意，*k*表示我们自己定义的邻居数量，取决于我们认为在确定样本是否是恶意或良性时，应该考虑多少个邻居。
- en: In the real world, this makes intuitive sense. For example, if you have a dataset
    of weights and heights of both basketball players and table tennis players, chances
    are that the basketball players’ weights and heights are likely closer to one
    another than they are to the measurements of table tennis players. Similarly,
    in a security setting, malware will often have similar features to other malware,
    and benignware will often have similar features to other benignware.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中，这个方法是直观的。例如，如果你有一个包含篮球运动员和乒乓球运动员的体重和身高数据集，篮球运动员的体重和身高很可能彼此更接近，而不是和乒乓球运动员的数据接近。类似地，在安全领域，恶意软件通常与其他恶意软件具有相似的特征，而良性软件则通常与其他良性软件有相似的特征。
- en: 'We can translate this idea into a k-nearest neighbors algorithm to compute
    whether a binary is malicious or benign using the following steps:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这个思想转化为k-近邻算法，使用以下步骤计算二进制文件是否是恶意的或良性的：
- en: Extract the binary’s features and find the *k* samples that are closest to it
    in the feature space.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取二进制文件的特征，并在特征空间中找到与其最接近的*k*个样本。
- en: Divide the number of malware samples that are close to the sample by *k* to
    get the percentage of nearest neighbors that are malicious.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将接近样本的恶意软件样本数量除以*k*，以获得最近邻中恶意的比例。
- en: If enough of the samples are malicious, define the sample as malicious.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果足够多的样本是恶意的，则将该样本定义为恶意的。
- en: '[Figure 6-13](ch06.xhtml#ch06fig13) shows how k-nearest neighbors algorithm
    works at a high level.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6-13](ch06.xhtml#ch06fig13)展示了k-近邻算法的高层次工作原理。'
- en: '![image](../images/f0106-01.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0106-01.jpg)'
- en: '*Figure 6-13: An illustration of the way k-nearest neighbors can be used to
    detect previously unseen malware*'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6-13：k-最近邻用于检测之前未见过的恶意软件的示意图*'
- en: We see a set of malware training examples in the upper left and a set of benignware
    examples in the lower right. We also see a new, unknown binary that is connected
    to its three nearest neighbors. In this case, we’ve set *k* to 3, meaning we’re
    looking at the three nearest neighbors to unknown binaries. Because all three
    of the nearest neighbors are malicious, we’d classify this new binary as malicious.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到左上角是一组恶意软件的训练样本，右下角是一组良性软件的样本。我们还看到一个新的未知二进制文件，它与其三个最近的邻居相连。在这种情况下，我们将 *k*
    设置为 3，这意味着我们正在查看未知二进制文件的三个最近邻居。由于这三个最近邻居都是恶意的，我们会将这个新二进制文件分类为恶意的。
- en: '**The Math Behind K-Nearest Neighbors**'
  id: totrans-138
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**K-最近邻背后的数学原理**'
- en: Let’s now discuss the math that allows us to compute the distance between new,
    unknown binaries’ features and the samples in the training set. We use a *distance
    function* to do this, which tells us the distance between our new example and
    the examples in the training set. The most common distance function is *Euclidean
    distance*, which is the length of the shortest path between two points in our
    feature space. [Listing 6-2](ch06.xhtml#ch06list2) shows pseudocode for Euclidean
    distance in our sample two-dimensional feature space.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们讨论一下使我们能够计算新未知二进制特征与训练集中的样本之间距离的数学原理。我们使用一个*距离函数*来实现这一点，它告诉我们新样本与训练集样本之间的距离。最常见的距离函数是*欧几里得距离*，即我们特征空间中两点之间最短路径的长度。[列表
    6-2](ch06.xhtml#ch06list2) 显示了我们样本二维特征空间中欧几里得距离的伪代码。
- en: '![image](../images/f0107-01.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0107-01.jpg)'
- en: '*Listing 6-2: Pseudocode for writing the* euclidean_distance *function*'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 6-2：编写* euclidean_distance *函数的伪代码*'
- en: Let’s walk through how the math in this code works. [Listing 6-2](ch06.xhtml#ch06list2)
    takes a pair of samples and computes the distance between them based on the differences
    between their features. First, the caller passes in the features of the binaries
    ➊, where `compression1` is the compression feature of the first example, `suspicious_calls1`
    is the `suspicious_calls` feature of the first example, `compression2` is the
    compression feature of the second example, and `suspicious_calls2` is the suspicious
    calls feature of the second example.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一起来看看这段代码中的数学是如何工作的。[列表 6-2](ch06.xhtml#ch06list2) 通过计算一对样本之间基于特征差异的距离来工作。首先，调用者传入二进制特征
    ➊，其中 `compression1` 是第一个样本的压缩特征，`suspicious_calls1` 是第一个样本的 `suspicious_calls`
    特征，`compression2` 是第二个样本的压缩特征，`suspicious_calls2` 是第二个样本的可疑呼叫特征。
- en: Then we compute the squared difference between the compression features of each
    sample ➋, and we compute the squared difference between the suspicious calls feature
    of each sample ➌. We won’t cover the reason we use squared distance, but note
    that the resulting difference is always positive. Finally, we compute the square
    root of the two differences, which is the Euclidean distance between the two feature
    vectors, and return it to the caller ➍. Although there are other ways to compute
    distances between examples, Euclidean distance is the most commonly used with
    the k-nearest neighbors algorithm, and it works well for security data science
    problems.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们计算每个样本的压缩特征之间的平方差 ➋，并计算每个样本的可疑呼叫特征之间的平方差 ➌。我们不会在此讨论为什么使用平方距离，但需要注意的是，结果差异始终为正数。最后，我们计算这两个差异的平方根，它就是两个特征向量之间的欧几里得距离，并将其返回给调用者
    ➍。虽然也有其他计算样本之间距离的方法，但欧几里得距离是 k-最近邻算法中最常用的，它在安全数据科学问题中表现良好。
- en: '**Choosing the Number of Neighbors That Vote**'
  id: totrans-144
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**选择投票的邻居数量**'
- en: Let’s now look at the kinds of decision boundaries and probabilities that a
    k-nearest neighbors algorithm produces for the sample datasets we’re using in
    this chapter. In [Figure 6-14](ch06.xhtml#ch06fig14), I set *k* to 5, thus allowing
    five closest neighbors to “vote.”
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看 k-最近邻算法在本章中使用的样本数据集上产生的决策边界和概率。在[图 6-14](ch06.xhtml#ch06fig14)中，我将 *k*
    设置为 5，因此允许 5 个最接近的邻居进行“投票”。
- en: '![image](../images/f0108-01.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0108-01.jpg)'
- en: '*Figure 6-14: The decision boundaries created by* k-*nearest neighbors when*
    k *is set to 5*'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6-14：当* k *设置为 5 时，k-最近邻创建的决策边界*'
- en: But in [Figure 6-15](ch06.xhtml#ch06fig15), I set *k* to 50, allowing the 50
    closest neighbors to “vote.”
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 但是在[图 6-15](ch06.xhtml#ch06fig15)中，我将 *k* 设置为 50，允许 50 个最接近的邻居进行“投票”。
- en: '![image](../images/f0108-02.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0108-02.jpg)'
- en: '*Figure 6-15: The decision boundaries created by k-nearest neighbors when*
    k *is set to 50*'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6-15：当k设置为50时，k近邻算法创建的决策边界*'
- en: Note the dramatic difference between the models depending on the number of neighbors
    that vote. The model in [Figure 6-14](ch06.xhtml#ch06fig14) shows a gnarly, complex
    decision boundary for both datasets, which is overfit in the sense that it draws
    local decision boundaries around outliers, but underfit because it fails to capture
    the simple, general trends. In contrast, the model in [Figure 6-15](ch06.xhtml#ch06fig15)
    is well-fit to both datasets, because it doesn’t get distracted by outliers and
    cleanly identifies general trends.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，模型在不同邻居投票数的情况下差异明显。[图6-14](ch06.xhtml#ch06fig14)中的模型展示了一个复杂、曲折的决策边界，适用于两个数据集，这个模型是过拟合的，因为它在异常值周围画出了局部的决策边界，但又是欠拟合的，因为它未能捕捉到简单的、普遍的趋势。相比之下，[图6-15](ch06.xhtml#ch06fig15)中的模型非常适合这两个数据集，因为它没有被异常值干扰，能够清晰地识别出一般趋势。
- en: As you can see, k-nearest neighbors can produce a much more complex decision
    boundary than logistic regression. We can control the complexity of this boundary
    to guard against both over- and underfitting by changing *k*, the number of neighbors
    that get to vote on whether a sample is malicious or benign. Whereas the logistic
    regression model in [Figure 6-11](ch06.xhtml#ch06fig11) got it completely wrong,
    k-nearest neighbors does well at separating the malware from the benignware, especially
    when we let 50 neighbors vote. Because k-nearest neighbors is not constrained
    by a linear structure and is simply looking at the nearest neighbors of each point
    to make a decision, it can create decision boundaries with arbitrary shapes, thus
    modeling complex datasets much more effectively.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，k近邻算法能够生成比逻辑回归更复杂的决策边界。我们可以通过调整*k*（即投票判断样本是恶意还是良性的邻居数目）来控制决策边界的复杂度，从而防止过拟合和欠拟合。而[图6-11](ch06.xhtml#ch06fig11)中的逻辑回归模型完全判断错误，k近邻算法则能很好地区分恶意软件和良性软件，尤其是当我们让50个邻居参与投票时。由于k近邻算法不受线性结构的限制，它只是通过查看每个点的最近邻居来做出决策，因此它能够创建具有任意形状的决策边界，从而更有效地建模复杂的数据集。
- en: '**When to Use K-Nearest Neighbors**'
  id: totrans-153
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**何时使用K近邻算法**'
- en: K-nearest neighbors is a good algorithm to consider when you have data where
    features don’t map cleanly onto the concept of suspiciousness, but closeness to
    malicious samples is a strong indicator of maliciousness. For example, if you’re
    trying to classify malware into families that share code, k-nearest neighbors
    might be a good algorithm to try, because you want to classify a malware sample
    into a family if its features are similar to known members of a given family.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: K近邻算法是一个值得考虑的好算法，尤其是当你的数据中，特征无法直接映射到可疑性概念时，但接近恶意样本却能强烈指示恶意性。例如，如果你试图将恶意软件按代码相似度划分为不同的家族，k近邻算法可能是一个不错的选择，因为你希望将一个恶意软件样本分类到某个家族中，如果它的特征与该家族中的已知成员相似的话。
- en: Another reason to use k-nearest neighbors is that it provides clear explanations
    of *why* it has made a given classification decision. In other words, it’s easy
    to identify and compare similarities between samples and an unknown sample to
    figure out why the algorithm has classified it as malware or benignware.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 使用k近邻算法的另一个原因是，它能够清楚地解释*为什么*它做出了某个分类决策。换句话说，它很容易识别和比较样本与未知样本之间的相似性，从而弄清楚算法为什么将其分类为恶意软件或良性软件。
- en: '***Decision Trees***'
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***决策树***'
- en: Decision trees are another frequently used machine learning method for solving
    detection problems. Decision trees automatically generate a series of questions
    through a training process to decide whether or not a given binary is malware,
    similar to the game Twenty Questions. [Figure 6-16](ch06.xhtml#ch06fig16) shows
    a decision tree that I automatically generated by training it on the simple dataset
    we’ve been using in this chapter. Let’s follow the flow of the logic in the tree.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是另一种常用于解决检测问题的机器学习方法。决策树通过训练过程自动生成一系列问题，以决定某个二进制文件是否为恶意软件，类似于“二十个问题”游戏。[图6-16](ch06.xhtml#ch06fig16)展示了我通过对本章使用的简单数据集进行训练，自动生成的决策树。让我们按照树中逻辑的流程来分析。
- en: '![image](../images/f0109-01.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0109-01.jpg)'
- en: '*Figure 6-16: A decision tree learned for our simple dataset example*'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6-16：为我们的简单数据集示例学习到的决策树*'
- en: 'The decision tree flow starts when we input the features we’ve extracted from
    a new, previously unseen binary into the tree. Then the tree defines the series
    of questions to ask of this binary’s features. The box at the top of the tree,
    which we call a *node*, asks the first question: is the number of suspicious calls
    in the tree less than or equal to 40.111? Note that the decision tree uses a floating
    point number here because we’ve normalized the number of suspicious calls in each
    binary to a range between 0 and 100\. If the answer is “yes,” we ask another question:
    is the percentage of compressed data in the file less than or equal to 37.254?
    If the answer is “yes,” we proceed to the next question: is the number of suspicious
    calls in the binary less than or equal to 33.836? If the answer is “yes,” we reach
    the end of the decision tree. At this point, the probability that the binary is
    malware is 0 percent.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的流程开始时，我们将从一个新的、之前未见过的二进制文件中提取的特征输入到树中。然后，树定义出一系列问题来询问这个二进制文件的特征。树顶端的框，我们称之为*节点*，提出第一个问题：树中的可疑调用次数是否小于或等于40.111？请注意，决策树在这里使用浮动小数点数字，因为我们已将每个二进制文件中的可疑调用次数归一化到0到100之间的范围。如果答案是“是”，我们接着问另一个问题：文件中压缩数据的百分比是否小于或等于37.254？如果答案是“是”，我们继续问下一个问题：二进制文件中的可疑调用次数是否小于或等于33.836？如果答案是“是”，我们就到达了决策树的终点。此时，二进制文件是恶意软件的概率为0%。
- en: '[Figure 6-17](ch06.xhtml#ch06fig17) shows a geometrical interpretation of this
    decision tree.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6-17](ch06.xhtml#ch06fig17)展示了这个决策树的几何解释。'
- en: '![image](../images/f0110-01.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0110-01.jpg)'
- en: '*Figure 6-17: The decision boundary created by a decision tree for our simple
    dataset example*'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6-17：决策树为我们的简单数据集示例创建的决策边界*'
- en: Here, the shaded regions indicate where the decision tree thinks samples are
    malicious. The lighter regions indicate where the decision tree thinks samples
    are benign. The probabilities assigned by the series of questions and answers
    in [Figure 6-16](ch06.xhtml#ch06fig16) should correspond with those in the shaded
    regions in [Figure 6-17](ch06.xhtml#ch06fig17).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，阴影区域表示决策树认为样本是恶意的地方。较亮的区域表示决策树认为样本是良性的地方。由[图6-16](ch06.xhtml#ch06fig16)中的一系列问题和答案所分配的概率应该与[图6-17](ch06.xhtml#ch06fig17)中的阴影区域相对应。
- en: '**Choosing a Good Root Node**'
  id: totrans-165
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**选择一个好的根节点**'
- en: So how do we use a machine learning algorithm to generate a decision tree like
    this from training data? The basic idea is that the decision tree starts with
    an initial question called a *root node*. The best root node is the one for which
    we get a “yes” answer for *most if not all* samples of one type, and a “no” answer
    for *most if not all* samples of the other type. For example, in [Figure 6-16](ch06.xhtml#ch06fig16),
    the root node question asks whether a previously unseen binary has 40.111 or fewer
    calls. (Note that the number of calls per binary here is normalized to a 0 to
    100 scale, making floating point values valid.) As you can see from the vertical
    line in [Figure 6-17](ch06.xhtml#ch06fig17), most of the benign data has less
    than this number, while most of the malware data has more than this number of
    suspicious calls, making this a good initial question to ask.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们如何使用机器学习算法从训练数据中生成像这样的决策树呢？基本的想法是，决策树从一个叫做*根节点*的初始问题开始。最好的根节点是那种对于一种类型的大部分（如果不是全部）样本，能得到“是”的答案，对于另一种类型的大部分（如果不是全部）样本，能得到“否”的答案。例如，在[图6-16](ch06.xhtml#ch06fig16)中，根节点问题询问一个之前未见过的二进制文件是否有40.111次或更少的调用。（请注意，这里每个二进制文件的调用次数已归一化为0到100的范围，使得浮动小数点值是有效的。）从[图6-17](ch06.xhtml#ch06fig17)中的垂直线可以看到，大部分良性数据的调用次数少于这个数字，而大部分恶意数据的调用次数则超过这个数目，这使得这个问题成为一个很好的初始问题。
- en: '**Picking Follow-Up Questions**'
  id: totrans-167
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**选择后续问题**'
- en: 'After choosing a root node, pick the next questions using a method similar
    to the one we used to pick the root node. For example, the root node allowed us
    to split the samples into two groups: one group that has less than or equal to
    40.111 suspicious calls (negative feature space) and another that has more than
    40.111 suspicious calls (positive feature space). To choose the next question,
    we just need questions that will further distinguish the samples in each area
    of the feature space into malicious and benign training examples.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 选择了根节点之后，可以使用类似选择根节点时的方法来选择下一个问题。例如，根节点让我们将样本分成了两组：一组可疑调用次数小于或等于40.111（负特征空间），另一组可疑调用次数大于40.111（正特征空间）。为了选择下一个问题，我们只需要提出那些能够进一步区分每个特征空间区域内的恶意和良性训练样本的问题。
- en: 'We can see this in the way the decision tree is structured in [Figure 6-16](ch06.xhtml#ch06fig16)
    and [6-17](ch06.xhtml#ch06fig17). For example, [Figure 6-16](ch06.xhtml#ch06fig16)
    shows that after we ask an initial “root” question about the number of suspicious
    calls binaries make, we ask questions about how much compressed data binaries
    have. [Figure 6-17](ch06.xhtml#ch06fig17) shows why we do this based on the data:
    after we ask our first question about suspicious function calls, we have a crude
    decision boundary that separates most malware from most benignware in the plot.
    How can we refine the decision boundary further by asking follow-up questions?
    It’s clear visually that the next best question to ask, which will refine our
    decision boundary, will be about the amount of compressed data in the binaries.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从[图6-16](ch06.xhtml#ch06fig16)和[6-17](ch06.xhtml#ch06fig17)中看到这一点。例如，[图6-16](ch06.xhtml#ch06fig16)显示，在我们提出有关可疑调用二进制文件数量的初始“根”问题后，我们会询问关于二进制文件中压缩数据量的问题。[图6-17](ch06.xhtml#ch06fig17)显示了我们为什么这么做的原因：在我们提出关于可疑函数调用的第一个问题之后，图中有一个粗略的决策边界，将大多数恶意软件与大多数良性软件分开。我们如何通过提出后续问题来进一步细化决策边界呢？从图中可以清楚地看到，接下来最好的问题是关于二进制文件中压缩数据量的问题，它将进一步细化我们的决策边界。
- en: '**When to Stop Asking Questions**'
  id: totrans-170
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**何时停止提问**'
- en: At some point in our decision tree creation process, we need to decide when
    the decision tree should stop asking questions and simply determine whether a
    binary file is benign or malicious based on our certainty about our answer. One
    way is to simply limit the number of questions our decision tree can ask, or to
    limit its *depth* (the maximum number of questions we can ask of any binary).
    Another is to allow the decision tree to keep growing until we’re absolutely certain
    about whether or not every example in our training set is malware or benignware
    based on the structure of the tree.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们创建决策树的过程中，某个时刻我们需要决定决策树何时停止提问，并仅根据我们对答案的确信程度来判断一个二进制文件是良性还是恶意的。一种方法是简单地限制决策树可以提出的问题数量，或者限制其*深度*（即我们可以对任何二进制文件提出的最大问题数）。另一种方法是允许决策树继续生长，直到我们完全确定训练集中每个示例是恶意软件还是良性软件。
- en: The advantage of constraining the size of the tree is that if the tree is simpler,
    we have a greater chance of getting the answer right (think of Occam’s razor—the
    simpler the theory, the better). In other words, there’s less chance that the
    decision tree will overfit the training data if we keep it small.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 限制树的大小的优势在于，如果树更简单，我们就有更大的机会得出正确的答案（想想奥卡姆剃刀——越简单的理论越好）。换句话说，如果我们保持树的简洁，决策树就更不容易过拟合训练数据。
- en: Conversely, allowing the tree to grow to maximum size can be useful if we are
    *underfitting* the training data. For example, allowing the tree to grow further
    will increase the complexity of the decision boundary, which we’ll want to do
    if we’re underfitting. In general, machine learning practitioners usually try
    multiple depths, or allow for maximum depth on previously unseen binaries, repeating
    this process until they get the most accurate results.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，如果我们*欠拟合*训练数据，允许树长到最大尺寸可能会很有用。例如，允许树进一步生长会增加决策边界的复杂度，而如果我们欠拟合的话，这正是我们想要的。一般来说，机器学习从业者通常会尝试多个深度，或者允许树在以前未见过的二进制文件上达到最大深度，并重复这个过程，直到获得最准确的结果。
- en: '**Using Pseudocode to Explore Decision Tree Generation Algorithms**'
  id: totrans-174
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**使用伪代码探索决策树生成算法**'
- en: Now let’s examine an automated decision tree generation algorithm. You learned
    that the basic idea behind this algorithm is to create the root node in the tree
    by finding the question that best increases our certainty about whether the training
    examples are malicious or benign, and then to find subsequent questions that will
    further increase our certainty. The algorithm should stop asking questions and
    make a decision once its certainty about the training examples has surpassed some
    threshold we set in advance.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看看一个自动化的决策树生成算法。你已经了解，这个算法背后的基本思想是通过找到最能增加我们对训练样本是否为恶意或良性的确信度的问题来创建树的根节点，然后再找到后续的问题，以进一步增加我们的确信度。一旦算法对训练样本的确信度超过了我们事先设定的某个阈值，它就应该停止提问并做出决策。
- en: Programmatically, we can do this recursively. The Python-like pseudocode in
    [Listing 6-3](ch06.xhtml#ch06list3) shows the complete process for building a
    decision tree in simplified form.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在程序上，我们可以递归地执行这一过程。[清单6-3](ch06.xhtml#ch06list3)中的类Python伪代码以简化形式展示了构建决策树的完整过程。
- en: '[PRE3]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '*Listing 6-3: Pseudocode for building a decision tree algorithm*'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 6-3：构建决策树算法的伪代码*'
- en: The pseudocode recursively adds questions to a decision tree, beginning with
    the root node and working its way down until the algorithm feels confident that
    the decision tree can provide a highly certain answer about whether a new file
    is benign or malicious.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 伪代码递归地向决策树添加问题，从根节点开始，一直到算法确信决策树可以提供一个高度确定的答案，来判断一个新文件是良性还是恶意。
- en: When we start building the tree, we use `pick_best_question()` to pick our root
    node ➊ (for now, don’t worry about how this function works). Then, we look at
    how much uncertainty we now have about the training samples for which the answer
    is “yes” to this initial question ➋. This will help us to decide if we need to
    keep asking questions about these samples or if we can stop, and predict whether
    the samples are malicious or benign. We do the same for the samples for which
    we answered “no” for the initial question ➌.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始构建树时，我们使用`pick_best_question()`来选择我们的根节点 ➊（暂时不必担心这个函数如何工作）。然后，我们查看对于那些我们对初始问题回答“是”的训练样本，现在我们有多少不确定性
    ➋。这将帮助我们决定是否需要继续对这些样本提问，或者我们是否可以停止，并预测这些样本是恶意的还是良性的。我们对那些我们对初始问题回答“否”的样本做同样的事情
    ➌。
- en: Next, we check if the uncertainty we have about the samples for which we answered
    “yes” (`uncertainty_yes`) is sufficiently low to decide whether they are malicious
    or benign ➍. If we can determine whether they’re malicious or benign at this point,
    we don’t ask any additional questions. But if we can’t, we call `add_question()`
    again, using `yes_samples`, or the number of samples for which we answered “yes,”
    as our input. This is a classic example of *recursion*, which is a function that
    calls itself. We’re using recursion to repeat the same process we performed for
    the root node with a subset of training examples. The next `if` statement does
    the same thing for our “no” examples ➎. Finally, we call our decision tree building
    function on our training examples ➏.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们检查对于那些我们回答“是”（`uncertainty_yes`）的样本，其不确定性是否足够低，以决定它们是恶意的还是良性的 ➍。如果我们此时能确定它们是恶意的还是良性的，我们就不再提出其他问题。但是如果不能，我们会再次调用`add_question()`，使用`yes_samples`（即我们回答“是”的样本数量）作为输入。这是*递归*的经典例子，递归是一种调用自身的函数。我们使用递归对训练样本的子集执行与根节点相同的过程。接下来的`if`语句对我们的“否”样本执行相同的操作
    ➎。最后，我们对我们的训练样本调用决策树构建函数 ➏。
- en: How exactly `pick_best_question()` works involves math that is beyond the scope
    of this book, but the idea is simple. To pick the best question at any point in
    the decision tree building process, we look at the training examples about which
    we’re still uncertain, enumerate all the questions we could ask about them, and
    then pick the one that best reduces our uncertainty about whether the examples
    are malware or benignware. We measure this reduction in uncertainty using a statistical
    measurement called *information gain*. This simple method for picking the best
    question works surprisingly well.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '`pick_best_question()`的具体工作原理涉及一些超出本书范围的数学内容，但其思路很简单。为了在决策树构建过程中任何时刻选择最佳问题，我们查看我们仍然不确定的训练样本，列举出可以对它们提出的所有问题，然后选择最能减少我们对这些样本是恶意软件还是良性软件的不确定性的问题。我们使用一种叫做*信息增益*的统计度量来衡量这种不确定性的减少。这种选择最佳问题的简单方法效果出奇地好。'
- en: '**NOTE**'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*This is a simplified example of how real-world, decision tree–generating,
    machine learning algorithms work. I’ve left out the math required to calculate
    how much a given question increases our certainty about whether or not a file
    is bad.*'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '*这是一个简化的示例，展示了真实世界中决策树生成机器学习算法的工作方式。我省略了计算给定问题如何增加我们对文件是否为恶意的确定性的数学部分。*'
- en: Let’s now look at the behavior of decision trees on the two sample datasets
    we’ve been using in this chapter. [Figure 6-18](ch06.xhtml#ch06fig18) shows the
    decision boundary learned by a decision tree detector.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看在本章中使用的两个示例数据集上决策树的表现。[图 6-18](ch06.xhtml#ch06fig18)展示了决策树检测器学习到的决策边界。
- en: '![image](../images/f0113-01.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0113-01.jpg)'
- en: '*Figure 6-18: Decision boundaries for our sample datasets produced by a decision
    tree approach*'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6-18：我们示例数据集通过决策树方法生成的决策边界*'
- en: In this case, instead of setting a maximum depth for the trees, we allow them
    to grow to the point where there are no false positives or false negatives relative
    to the training data so that every training sample is correctly classified.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们不是为树设置最大深度，而是允许它们生长到没有假阳性或假阴性，相对于训练数据而言，每个训练样本都能被正确分类的位置。
- en: Notice that decision trees can only draw horizontal and vertical lines in the
    feature space, even when it seems clear and obvious that a curved or diagonal
    line might be more appropriate. This is because decision trees only allow us to
    express simple conditions on individual features (such as greater than or equal
    to and less than or equal to), which always leads to horizontal or vertical lines.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，决策树只能在特征空间中绘制水平和垂直线，即使当看起来曲线或对角线可能更合适时也是如此。这是因为决策树仅允许我们在单个特征上表达简单的条件（例如，大于或等于、小于或等于），这总是导致水平或垂直线。
- en: You can also see that although the decision trees in these examples succeed
    in separating the benignware from the malware, the decision boundaries look highly
    irregular and have strange artifacts. For example, the malware region extends
    into the benignware region in strange ways, and vice versa. On the positive side,
    the decision tree does far better than logistic regression at creating a decision
    boundary for the complex dataset.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以看到，尽管这些示例中的决策树成功地将良性软件与恶意软件分开，但决策边界看起来非常不规则，并且存在奇怪的伪影。例如，恶意软件区域以奇怪的方式扩展到良性软件区域，反之亦然。从积极的一面来看，决策树在为复杂数据集创建决策边界方面远远优于逻辑回归。
- en: Let’s now compare the decision trees in [Figure 6-18](ch06.xhtml#ch06fig18)
    to the decision tree models in [Figure 6-19](ch06.xhtml#ch06fig19).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将[图 6-18](ch06.xhtml#ch06fig18)中的决策树与[图 6-19](ch06.xhtml#ch06fig19)中的决策树模型进行比较。
- en: '![image](../images/f0114-01.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0114-01.jpg)'
- en: '*Figure 6-19: Decision boundaries for our sample datasets produced by a limited-depth
    decision tree*'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6-19：由有限深度决策树生成的示例数据集的决策边界*'
- en: The decision trees in [Figure 6-19](ch06.xhtml#ch06fig19) use the same decision
    tree generation algorithm used for [Figure 6-18](ch06.xhtml#ch06fig18), except
    I limit the tree depth to five nodes. This means that for any given binary, I
    can ask a maximum of five questions of its features.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6-19](ch06.xhtml#ch06fig19)中的决策树使用了与[图 6-18](ch06.xhtml#ch06fig18)中相同的决策树生成算法，唯一的区别是我将树的深度限制为五个节点。这意味着，对于任何给定的二叉树，我最多可以问五个关于其特征的问题。'
- en: The result is dramatic. Whereas the decision tree models shown in [Figure 6-18](ch06.xhtml#ch06fig18)
    are clearly overfit, focusing on outliers and drawing overly complex boundaries
    that fail to capture the general trend, the decision trees in [Figure 6-19](ch06.xhtml#ch06fig19)
    fit the data much more elegantly, identifying a general pattern in both datasets
    without focusing on outliers (with one exception, the skinnier decision region
    in the upper-right area of the simple dataset). As you can see, picking a good
    maximum decision tree depth can have a big effect on your decision tree–based
    machine learning detector.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 结果非常显著。虽然[图 6-18](ch06.xhtml#ch06fig18)中显示的决策树模型明显是过拟合的，专注于异常值并绘制出过于复杂的边界，未能捕捉到总体趋势，[图
    6-19](ch06.xhtml#ch06fig19)中的决策树则更加优雅地拟合了数据，在两个数据集中识别出了一个普遍的模式，而没有专注于异常值（唯一的例外是简单数据集右上方较窄的决策区域）。正如你所看到的，选择一个合适的最大决策树深度对你的基于决策树的机器学习检测器有很大影响。
- en: '**When to Use Decision Trees**'
  id: totrans-196
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**何时使用决策树**'
- en: Because decision trees are expressive and simple, they can learn both simple
    and highly irregular boundaries based on simple yes-or-no questions. We can also
    set the maximum depth to control how simple or complex their theories of what
    constitutes malware versus benignware should be.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 由于决策树既具有表现力又简单，它们可以通过简单的是或否问题来学习简单的以及高度不规则的边界。我们还可以设置最大深度，以控制它们对恶意软件与良性软件的理解应该有多简单或多复杂。
- en: Unfortunately, the downside to decision trees is that they often simply do not
    result in very accurate models. The reason for this is complex, but it’s related
    to the fact that decision trees express jagged decision boundaries, which don’t
    fit the training data in ways that generalize to previously unseen examples very
    well.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，决策树的缺点是它们通常无法生成非常准确的模型。造成这种情况的原因很复杂，但与决策树表达不规则决策边界的事实有关，这些边界不能很好地拟合训练数据，也不能很好地推广到先前未见过的示例。
- en: Similarly, decision trees don’t usually learn accurate probabilities around
    their decision boundaries. We can see this by inspecting the shaded regions around
    the decision boundary in [Figure 6-19](ch06.xhtml#ch06fig19). The decay is not
    natural or gradual and doesn’t happen in the regions it should—areas where the
    malware and benignware examples overlap.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，决策树通常不会在其决策边界周围学习到准确的概率。我们可以通过检查[图 6-19](ch06.xhtml#ch06fig19)中决策边界周围的阴影区域来看到这一点。衰减并不自然或渐进，且并没有在它应该发生的区域——恶意软件和良性软件样本重叠的地方发生。
- en: Next, I discuss the random forest approach, which combines multiple decision
    trees to yield far better results.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我将讨论随机森林方法，它结合了多棵决策树以获得更好的结果。
- en: '***Random Forest***'
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***随机森林***'
- en: Although the security community relies heavily on decision trees for malware
    detection, they almost never use them individually. Instead, hundreds or thousands
    of decision trees are used in concert to make detections through an approach called
    *random forest*. Instead of training one decision tree, we train many, usually
    a hundred or more, but we train each decision tree differently so that it has
    a different perspective on the data. Finally, to decide whether a new binary is
    malicious or benign, we allow the decision trees to vote. The probability that
    a binary is malware is the number of positive votes divided by the total number
    of trees.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管安全社区在恶意软件检测中大量依赖决策树，但他们几乎从不单独使用它们。相反，成百上千棵决策树会联合起来，通过一种叫做*随机森林*的方法进行检测。我们不是训练一棵决策树，而是训练许多，通常是一百棵或更多，但我们以不同的方式训练每棵决策树，使其对数据有不同的看法。最后，为了决定一个新的二进制文件是恶意还是良性，我们让这些决策树进行投票。一个二进制文件是恶意软件的概率是正投票数除以总树数。
- en: Of course, if all the decision trees are identical, they would all vote the
    same way, and the random forest would simply replicate the results of the individual
    decision trees. To address this, we want the decision trees to have different
    perspectives on what constitutes malware and benignware, and we use two methods,
    which I discuss next, to induce this diversity into our collection of decision
    trees. By inducing diversity, we generate a “wisdom of crowds” dynamic in our
    model, which typically results in a more accurate model.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，如果所有的决策树完全相同，它们将会做出相同的投票，随机森林将简单地复制个别决策树的结果。为了解决这个问题，我们希望决策树对什么构成恶意软件和良性软件有不同的看法，我们使用接下来要讨论的两种方法，将这种多样性引入我们的决策树集合中。通过引入多样性，我们在模型中产生了“集体智慧”的动态，通常会导致一个更准确的模型。
- en: 'We use the following steps to generate a random forest algorithm:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下步骤来生成随机森林算法：
- en: 'Training: for every tree out of the number we plan to generate (typically 100
    or more)'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练：对于我们计划生成的每棵树（通常是 100 棵或更多）
- en: Randomly sample some training examples from our training set.
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从我们的训练集中随机抽取一些训练样本。
- en: Build a decision tree from the random sample.
  id: totrans-207
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从随机样本中建立一棵决策树。
- en: For each tree that we build, each time we consider “asking a question,” consider
    asking a question of only a handful of features, and disregard the other features.
  id: totrans-208
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于我们建立的每棵树，每次我们考虑“提问”时，只考虑向少数特征提问，忽略其他特征。
- en: Detection on a previously unseen binary
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对一个以前未见过的二进制文件进行检测
- en: Run detection for each individual tree on the binary.
  id: totrans-210
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对每棵树在二进制文件上进行检测。
- en: Decide whether or not the binary is malware based on the number of trees that
    voted “yes.”
  id: totrans-211
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据投票“是”的树的数量决定二进制文件是否为恶意软件。
- en: To understand this in more detail, let’s examine the results generated by the
    random forest approach on our two sample datasets, as shown in [Figure 6-20](ch06.xhtml#ch06fig20).
    These results were generated using 100 decision trees.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更详细地理解这一点，让我们查看使用随机森林方法在我们两个示例数据集上生成的结果，如[图 6-20](ch06.xhtml#ch06fig20)所示。这些结果是使用
    100 棵决策树生成的。
- en: '![image](../images/f0116-01.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0116-01.jpg)'
- en: '*Figure 6-20: Decision boundaries created using the random forest approach*'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6-20：使用随机森林方法创建的决策边界*'
- en: In contrast to the individual decision tree results shown in [Figures 6-18](ch06.xhtml#ch06fig18)
    and [6-19](ch06.xhtml#ch06fig19), random forest can express much smoother and
    more intuitive decision boundaries for both simple and complex datasets than individual
    decision trees. Indeed, the random forest model fits the training dataset very
    cleanly, with no jagged edges; the model seems to have learned good theories about
    what constitutes “malicious versus benign” for both datasets.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 与[图6-18](ch06.xhtml#ch06fig18)和[图6-19](ch06.xhtml#ch06fig19)中显示的单个决策树结果相比，随机森林能够为简单和复杂的数据集表达更平滑、更直观的决策边界。实际上，随机森林模型对训练数据集的拟合非常干净，没有任何锯齿边缘；该模型似乎已经学到了关于“恶意与良性”构成的良好理论，适用于这两个数据集。
- en: Additionally, the shaded regions are intuitive. For example, the further you
    get from benign or malicious examples, the less certainty random forest has about
    whether examples are malicious or benign. This bodes well for random forest’s
    performance on previously unseen binaries. In fact, as you’ll see in the next
    chapter, random forest is the best performing model on previously unseen binaries
    of all the approaches discussed in this chapter.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，阴影区域是直观的。例如，离良性或恶意样本越远，随机森林对于样本是恶意还是良性的判断越不确定。这对随机森林在未见过的二进制文件上的表现预示着积极的前景。事实上，正如你将在下一章看到的，随机森林是所有本章讨论的方法中在未见过的二进制文件上表现最好的模型。
- en: To understand why random forest draws such clean decision boundaries compared
    to individual decision trees, let’s think about what the 100 decision trees are
    doing. Each tree sees only about two-thirds of the training data, and only gets
    to consider a randomly selected feature whenever it makes a decision about what
    question to ask. This means that behind the scenes, we have 100 diverse decision
    boundaries that get *averaged* to create the final decision boundaries in the
    examples (and the shaded regions). This “wisdom of crowds” dynamic creates an
    aggregate opinion that can identify the trends in the data in a much more sophisticated
    way than individual decision trees can.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解为什么随机森林相比单个决策树能够画出如此干净的决策边界，让我们思考一下这100棵决策树在做什么。每棵树只看到大约三分之二的训练数据，并且每当它做出关于要问什么问题的决策时，只会考虑一个随机选择的特征。这意味着在幕后，我们有100个不同的决策边界，这些边界被*平均*以创建示例中的最终决策边界（以及阴影区域）。这种“集体智慧”的动态创造了一个能够识别数据趋势的汇聚意见，比单个决策树能够以更复杂的方式识别数据趋势。
- en: '**Summary**'
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**总结**'
- en: 'In this chapter, you got a high-level introduction to machine learning–based
    malware detection as well as four major approaches to machine learning: logistic
    regression, k-nearest neighbors, decision trees, and random forests. Machine learning–based
    detection systems can automate the work of writing detection signatures, and they
    often perform better in practice than custom written signatures.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你获得了基于机器学习的恶意软件检测的高层次介绍，以及机器学习的四种主要方法：逻辑回归、k近邻、决策树和随机森林。基于机器学习的检测系统可以自动化编写检测签名的工作，并且在实践中，它们通常比手写签名表现得更好。
- en: In the following chapters, I’ll show you how these approaches perform on real-world
    malware detection problems. Specifically, you’ll learn how to use open source,
    machine learning software to build machine learning detectors to accurately classify
    files as either malicious or benign, and how to use basic statistics to evaluate
    the performance of your detectors on previously unseen binaries.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我将向你展示这些方法在实际恶意软件检测问题中的表现。具体而言，你将学习如何使用开源的机器学习软件构建机器学习检测器，以准确地将文件分类为恶意或良性，并且如何使用基本统计学来评估你的检测器在之前未见过的二进制文件上的表现。
