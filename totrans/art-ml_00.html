<html><head></head><body>
<div id="sbo-rt-content" class="calibre1"><h2 class="h2fd" id="intro"><span epub:type="pagebreak" id="page_xix" class="calibre2"/>INTRODUCTION</h2>
<div class="imagec"><img alt="Image" src="../images/common.jpg" class="calibre14"/></div>
<p class="noindents">Machine learning! With such a science fiction-ish name, one might expect it to be technology that is strictly reserved for highly erudite specialists. Not true.</p>
<p class="indent">Actually, machine learning (ML) can easily be explained in commonsense terms, and anyone with a good grasp of charts, graphs, and the slope of a line should be able to both understand and productively <em class="calibre13">use</em> ML. Of course, as the saying goes, “The devil is in the details,” and one must work one’s way through those details. But ML is not rocket science, in spite of it being such a powerful tool.</p>
<h3 class="h2" id="ch00lev1">0.1 What Is ML?</h3>
<p class="noindent">ML is all about prediction. Does a patient have a certain disease? Will a customer switch from her current cell phone service to another? What is actually being said in this rather garbled audio recording? Is that bright spot observed by a satellite a forest fire or just a reflection?</p>
<p class="indent">We predict an <em class="calibre13">outcome</em> from one or more <em class="calibre13">features</em>. In the disease diagnosis example, the outcome is having the disease or not, and the features may be blood tests, family history, and so on.</p>
<p class="indent">All ML methods involve a simple idea: similarity. In the cell phone service example, how do we predict the outcome for a certain customer? We <span epub:type="pagebreak" id="page_xx"/>look at past customers and select the ones who are most similar in features (size of bill, lateness record, yearly income, and so on) to our current customer. If most of those similar customers bolted, we predict the same for the current one. Of course, we are not guaranteed that outcome, but it is our best guess.</p>
<h3 class="h2" id="ch00lev2">0.2 The Role of Math in ML Theory and Practice</h3>
<p class="noindent">Many ML methods are based on elegant mathematical theory, with support vector machines (SVMs) being a notable example. However, knowledge of this theory has very little use in terms of being able to apply SVM well in actual applications.</p>
<p class="indent">To be sure, a good <em class="calibre13">intuitive</em> understanding of how ML methods work is essential to effective use of ML in practice. This book strives to develop in the reader a keen understanding of the intuition, <em class="calibre13">without using advanced mathematics</em>. Indeed, there are very few equations in this book.</p>
<h3 class="h2" id="ch00lev3">0.3 Why Another ML Book?</h3>
<p class="noindent">There are many great ML books out there, of course, but none really <em class="calibre13">empower</em> the reader to use ML effectively in real-world problems. In many cases, the problem is that the books are too theoretical, but I am equally concerned that the applied books tend to be “cookbooks” (too “recipe-oriented”) that treat the subject in a Step 1, Step 2, Step 3 manner. Their focus is on the syntax and semantics of ML software, with the result that while the reader may know the software well, the reader is not positioned to <em class="calibre13">use</em> ML well.</p>
<p class="indent">I wrote this book because:</p>
<ul class="calibre15">
<li class="noindenta">There is a need for a book that <em class="calibre13">uses</em> the R language but is not <em class="calibre13">about</em> R. This is a book on ML that happens to use R for examples and not a book about the use of R in ML.</li>
<li class="noindenta">There is a need for an ML book that recognizes that <em class="calibre13">ML is an art, not a science.</em> (Hence the title of this book.)</li>
<li class="noindenta">There is a need for an ML book that avoids advanced math but addresses the point that, in order to use ML effectively, <em class="calibre13">one does need to understand the concepts well—the why and how of ML methods.</em> Most “applied” ML books do too little in explaining these things.</li>
</ul>
<p class="calibre10">All three of these bullets go back to the “anti-cookbook” theme. My goal is, then, this:</p>
<p class="block">I would like those who use ML to not only know the definition of random forests but also be ready to cogently explain how the various hyperparameters in random forests may affect overfitting. MLers also should be able to give a clear account of the problems of “p-hacking” in feature engineering.</p>
<p class="block"><span epub:type="pagebreak" id="page_xxi"/>We will <em class="calibre13">empower</em> the reader with strong, <em class="calibre13">practical</em>, real-world knowledge of ML methods—their strengths and weaknesses, what makes them work and fail, what to watch out for. We will do so without much formal math and will definitely take a hands-on approach, using prominent software packages on real datasets. But we will do so in a savvy manner. We will be “informed consumers.”</p>
<h3 class="h2" id="ch00lev4">0.4 Recurring Special Sections</h3>
<p class="noindent">There are special recurring themes and sections throughout this book:</p>
<p class="noindent1"><strong class="calibre5">Bias vs. Variance</strong></p>
<p class="noindent2">Numerous passages explain in concrete terms—no superstition!—how these two central notions play out for each specific ML method.</p>
<p class="noindent1"><strong class="calibre5">Pitfalls</strong></p>
<p class="noindent2">Numerous sections with the “Pitfall” title warn the reader of potential problems and show how to avoid them.</p>
<h3 class="h2" id="ch00lev5">0.5 Background Needed</h3>
<p class="noindent">What kind of background will the reader need to use this book profitably?</p>
<ul class="calibre15">
<li class="noindenta">No prior exposure to ML or statistics is assumed.</li>
<li class="noindenta">As to math in general, the book is mostly devoid of formal equations. As long as the reader is comfortable with basic graphs, such as histograms and scatterplots, and simple algebra notions, such as the slope of a line, that is quite sufficient.</li>
<li class="noindenta">The book does assume some prior background in R coding, such as familiarity with vectors, factors, data frames, and functions. The R command line (&gt; prompt, Console in RStudio) is used throughout. Readers without a background in R, or those wishing to have a review, may find my <code>fasteR</code> tutorial useful: <a href="https://github.com/matloff/fasteR" class="calibre12"><em class="calibre13">https://github.com/matloff/fasteR</em></a>.</li>
<li class="noindenta">Make sure R and the <code>qeML</code> package are installed on your computer. For the package, the preferred installation source is GitHub, as it will always have the most up-to-date version of the package. You’ll need the <code>devtools</code> package; if you don’t already have it, type:
<pre class="calibre16"><span class="codestrong">install.packages('devtools')</span></pre>
<p class="calibre10">Then, to install <code>qeML</code>, type:</p>
<pre class="calibre16"><span class="codestrong">install_github('https://github.com/matloff/qeML')</span></pre>
<p class="calibre10">The <code>qeML</code> package will also be on the CRAN R code repository but updated less frequently.</p>
</li></ul>
<h3 class="h2" id="ch00lev6"><span epub:type="pagebreak" id="page_xxii" class="calibre2"/><strong class="calibre3">0.6 The qe*-Series Software</strong></h3>
<p class="calibre10">Most of the software used here will come from popular R packages:</p>
<ul class="calibre15">
<li class="noindenta"><code>e1071</code></li>
<li class="noindenta"><code>gbm</code></li>
<li class="noindenta"><code>glmnet</code></li>
<li class="noindenta"><code>keras</code></li>
<li class="noindenta"><code>randomForest</code></li></ul>
<p class="indent">Readers can use these packages directly if they wish. But in order to keep things simple and convenient for readers, we usually will be using wrappers for the functions in those packages, which are available in my package, <code>qeML</code>. This is a big help in two ways:</p>
<ol class="calibre17">
<li class="noindenta">The wrappers provide a uniform interface.</li>
<li class="noindenta">That uniform interface is also <strong class="calibre5"><em class="calibre13">simple</em></strong>.</li>
</ol>
<p class="indent">For instance, consider <code>day1</code>, a bike rental dataset used at various points in this book. We wish to predict <code>tot</code>, total ridership. Here’s how we would do that using random forests, an ML topic covered in this book:</p>
<pre class="calibre16">qeRF(day1,'tot')</pre>
<p class="calibre10">For support vector machines, another major topic, the call would be</p>
<pre class="calibre16">qeSVM(day1,'tot')</pre>
<p class="calibre10">and so on. Couldn’t be simpler! No preparatory code, say, to define a model; just call one of the <code>qe</code> functions and go! The prefix <code>qe</code>- stands for “quick and easy.” One can also specify method-specific parameters, which we will do, but still, it will be quite simple.</p>
<p class="indent">For very advanced usage, this book shows how to use those packages directly.</p>
<h3 class="h2" id="ch00lev7">0.7 The Book’s Grand Plan</h3>
<p class="noindent">Here is the path we’ll take. The first three chapters introduce general concepts that recur throughout the book, as well as specific machine learning methods. The rough description of ML above—predict on the basis of similar cases—is most easily developed using an ML method known as <em class="calibre13">k-nearest neighbors (k-NN)</em>. <a href="part1.xhtml" class="calibre12">Part I</a> of the book will play two roles. First, it will cover k-NN in detail. Second, it will introduce the reader to general concepts that apply to all ML methods, such as choice of <em class="calibre13">hyperparameters</em>. In k-NN, the number of similar cases, usually denoted <em class="calibre13">k</em>, is the hyperparameter. For k-NN, what is the “Goldilocks” value of <em class="calibre13">k</em>—not too small and not too large? Again, choice of hyperparameters is key in most ML methods, and it will be introduced via k-NN.</p>
<p class="indent"><span epub:type="pagebreak" id="page_xxiii"/><a href="part2.xhtml" class="calibre12">Part II</a> will then present a natural extension of k-NN, <em class="calibre13">tree-based methods</em>, specifically <em class="calibre13">random forests</em> and <em class="calibre13">gradient boosting</em>. These methods work in a flowchart-like manner, asking questions about features one at a time. In the disease diagnosis example given before, the first question might be, Is the patient over age 50? The next might be something like, Is the patient’s body mass index below 20.2? In the end, this process partitions the patients into small groups in which the members are similar to each other, so it’s like k-NN. But the groups do take different forms from k-NN, and tree methods often outperform k-NN in prediction accuracy and are considered a major ML tool.</p>
<p class="indent"><a href="part3.xhtml" class="calibre12">Part III</a> discusses methods based on linear relationships. Readers who have some background in linear regression analysis will recognize some of this, though again, no such background is assumed. This part closes with a discussion of the <em class="calibre13">LASSO</em> and <em class="calibre13">ridge regression</em>, which have the tantalizing property of deliberately shrinking down some classical linear regression estimates.</p>
<p class="indent"><a href="part4.xhtml" class="calibre12">Part IV</a> involves methods based on separating lines and planes. Consider again the cell phone service example. Say we plot the data for the old customers who left the service using the color blue in our graph. Then on the same graph, we plot those who remained loyal in red. Can we find a straight line that separates most of the blue points from most of the red points? If so, we will predict the action of the new customer by checking which side of the line his case falls on. This description not only fits <em class="calibre13">SVM</em> but also fits, in a sense, the most famous ML method, <em class="calibre13">neural networks</em>, which we cover as well.</p>
<p class="indent">Finally, <a href="part5.xhtml" class="calibre12">Part V</a> introduces several specific types of ML applications, such as <em class="calibre13">image classification</em>.</p>
<p class="indent">It’s often said that no one ML method works best in all applications. True, but hopefully this book’s structure will impart a good understanding of similarities and differences between the methods, appreciating where each fits in the grand scheme of things.</p>
<p class="indent">There is a website for the book at <a href="http://heather.cs.ucdavis.edu/artofml" class="calibre12"><em class="calibre13">http://heather.cs.ucdavis.edu/artofml</em></a>, which contains code, errata, new examples, and more.</p>
<h3 class="h2" id="ch00lev8">0.8 One More Point</h3>
<p class="noindent">In reading this book, keep in mind that <em class="calibre13">the prose is just as important as the code.</em> Avoid the temptation to focus only on the code and graphs. A page that is all prose—no math, no graphs, and no code—may be one of the most important pages in the book. It is there that you will learn the all-important <em class="calibre13">why</em> of ML, such as why choice of hyperparameters is so vital. The prose is crucial to your goal of becoming adept at ML with the most insight and predictive power!</p>
<p class="indent">Keep in mind that those dazzling ML successes you’ve heard about come only after careful, lengthy tuning and thought on the analyst’s part, requiring real insight. This book aims to develop that insight. Formal math <span epub:type="pagebreak" id="page_xxiv"/>is minimized here, but note that this means the math will give way to prose that describes many key issues.</p>
<p class="indent">So, let’s get started. Happy ML-ing!</p>
</div></body></html>