- en: '1'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: WHY CONTAINERS MATTER
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/common01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: It’s a great time to be a software developer. Creating a brand-new application
    and making it available to millions of people has never been easier. Modern programming
    languages, open source libraries, and application platforms make it possible to
    write a small amount of code and end up with lots of functionality. However, although
    it’s easy to get started and create a new application quickly, the best application
    developers are those who move beyond treating the application platform as a “black
    box” and really understand how it works. Creating a reliable, resilient, and scalable
    application requires more than just knowing how to create a Deployment in the
    browser or on the command line.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll look at application architecture in a scalable, cloud
    native world. We will show why containers are the preferred way to package and
    deploy application components, and how container orchestration addresses key needs
    for containerized applications. We’ll finish with an example application deployed
    to Kubernetes to give you an introductory glimpse into the power of these technologies.
  prefs: []
  type: TYPE_NORMAL
- en: Modern Application Architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The main theme of modern software applications is *scale*. We live in a world
    of applications with millions of simultaneous users. What is remarkable is the
    ability of these applications to achieve not only this scale but also a level
    of stability such that an outage makes headlines and serves as fodder for weeks
    or months of technical analysis.
  prefs: []
  type: TYPE_NORMAL
- en: With so many modern applications running at large scale, it can be easy to forget
    that a lot of hard work goes into architecting, building, deploying, and maintaining
    applications of this caliber, whether the scale they’re designed for is thousands,
    millions, or billions of users. Our job in this chapter is to identify what we
    need from our application platform to run a scalable, reliable application, and
    to see how containerization and Kubernetes meet those requirements. We’ll start
    by looking at three key attributes of modern application architecture. Then we’ll
    move on to looking at three key benefits these attributes bring.
  prefs: []
  type: TYPE_NORMAL
- en: 'Attribute: Cloud Native'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are lots of ways to define *cloud native* technologies (and a good place
    to start is the Cloud Native Computing Foundation at *[https://cncf.io](https://cncf.io)*).
    I like to start with an idea of what “the cloud” is and what it enables so that
    we can understand what kind of architecture can make best use of it.
  prefs: []
  type: TYPE_NORMAL
- en: At its heart, the cloud is an abstraction. We talked about abstractions in the
    introduction, so you know that abstractions are essential to computing, but we
    also need a deep understanding of our abstractions to use them properly. In the
    case of the cloud, the provider is abstracting away the real physical processors,
    memory, storage, and networking, allowing cloud users to simply declare a need
    for these resources and have them provisioned on demand. To have a “cloud native”
    application, then, we need an application that can take advantage of that abstraction.
    As much as possible, the application shouldn’t be tied to a specific host or a
    specific network layout, because we don’t want to constrain our flexibility in
    how application components are divided among hosts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Attribute: Modular'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Modularity* is nothing new to application architecture. The goal has always
    been *high cohesion*, where everything within a module relates to a single purpose,
    and *low coupling*, where modules are organized to minimize intermodule communication.
    However, even though modularity remains a key design goal, the definition of what
    makes a module is different. Rather than just treat modularity as a way of organizing
    the code, modern application architecture today prefers to carry modularity into
    the runtime, providing each module with a separate operating system process and
    discouraging the use of a shared filesystem or shared memory for communication.
    Because modules are separate processes, communication between modules is standard
    network (socket) communication.'
  prefs: []
  type: TYPE_NORMAL
- en: This approach seems wasteful of hardware resources. It is more compact and faster
    to share memory than it is to copy data over a socket. But there are two good
    reasons to prefer separate processes. First, modern hardware is fast and getting
    faster, and it would be a form of premature optimization to imagine that sockets
    are not fast enough for our application. Second, no matter how large a server
    we have, there is going to be a limit to how many processes we can fit on it,
    so a shared memory model ultimately limits our ability to grow.
  prefs: []
  type: TYPE_NORMAL
- en: 'Attribute: Microservice-Based'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Modern application architecture is based on modules in the form of separate
    processes—and these individual modules tend to be very small. In theory, a cloud
    can provide us with virtual servers that are as powerful as we need; however,
    in practice, using a few powerful servers is more expensive and less flexible
    than many small servers. If our modules are small enough, they can be deployed
    to cheap commodity servers, which means that we can leverage our cloud provider’s
    hardware to best advantage. Although there is no single answer as to how small
    a module needs to be in order to be a *microservice*, “small enough that we can
    be flexible regarding where it is deployed” is a good first rule.
  prefs: []
  type: TYPE_NORMAL
- en: A microservice architecture also has practical advantages for organizing teams.
    Ever since Fred Brooks wrote *The Mythical Man-Month*, architects have understood
    that organizing people is one of the biggest challenges to developing large, complex
    systems. Building a system from many small pieces reduces the complexity of testing
    but also makes it possible to organize a large team of people without everyone
    getting in everyone else’s way.
  prefs: []
  type: TYPE_NORMAL
- en: '**WHAT ABOUT APPLICATION SERVERS?**'
  prefs: []
  type: TYPE_NORMAL
- en: The idea of modular services has a long history, and one popular way to implement
    it was building modules to run in an application server, such as a Java Enterprise
    environment. Why not then just continue to follow that pattern for applications?
  prefs: []
  type: TYPE_NORMAL
- en: Although application servers were successful for many uses, they don’t have
    the same degree of isolation that a microservice architecture has. As a result,
    there are more issues with interdependency, leading to more complex testing and
    reduced team independence. Additionally, the typical model of having a single
    application server per host, with many applications deployed to it and sharing
    the same process space, is much less flexible than the containerized approaches
    you will see in this book.
  prefs: []
  type: TYPE_NORMAL
- en: This is not to say that you should immediately throw away your application server
    architecture to use containers. There are lots of benefits to containerization
    for any architecture. But as you adopt a containerized architecture, over time
    it will make sense for you to move your code toward a true microservice architecture
    to take best advantage of what containers and Kubernetes offer.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve looked at three key attributes of modern architecture. Now, let’s look
    at three key benefits that result.
  prefs: []
  type: TYPE_NORMAL
- en: 'Benefit: Scalability'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s begin by envisioning the simplest application possible. We create a single
    executable that runs on a single machine and interacts with only a single user
    at a time. Now, suppose that we want to grow this application so that it can interact
    with thousands or millions of users at once. Obviously, no matter how powerful
    a server we use, eventually some computing resource will become a bottleneck.
    It doesn’t matter whether the bottleneck is processing, or memory, or storage,
    or network bandwidth; the moment we hit that bottleneck, our application cannot
    handle any additional users without hurting performance for others.
  prefs: []
  type: TYPE_NORMAL
- en: The only possible way to solve this issue is to stop sharing the resource that
    caused the bottleneck. This means that we need to find a way to distribute our
    application across multiple servers. But if we’re really scaling up, we can’t
    stop there. We need to distribute across multiple networks as well, or we’ll hit
    the limit of what one network switch can do. And eventually, we will even need
    to distribute geographically, or we’ll saturate the broader network.
  prefs: []
  type: TYPE_NORMAL
- en: To build applications with no limit to scalability, we need an architecture
    that can run additional application instances at will. And because an application
    is only as slow as its slowest component, we need to find a way to scale *everything*,
    including our data stores. It’s obvious that the only way to do this effectively
    is to create our application from many independent pieces that are not tied to
    specific hardware. In other words, cloud native microservices.
  prefs: []
  type: TYPE_NORMAL
- en: 'Benefit: Reliability'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s go back to our simplest possible application. In addition to scalability
    limits, it has another flaw. It runs on one server, and if that server fails,
    the entire application fails. Our application is lacking reliability. As before,
    the only possible way to solve this issue is to stop sharing the resource that
    could potentially fail. Fortunately, when we start distributing our application
    across many servers, we have the opportunity to avoid a single point of failure
    in the hardware that would bring down our application. And as an application is
    only as reliable as its least reliable component, we need to find a way to distribute
    everything, including storage and networks. Again, we need cloud native microservices
    that are flexible about where they are run and about how many instances are running
    at once.
  prefs: []
  type: TYPE_NORMAL
- en: 'Benefit: Resilience'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There is a third, subtler advantage to cloud native microservice architecture.
    This time, imagine an application that runs on a single server, but it can easily
    be installed as a single package on as many servers as we like. Each instance
    can serve a new user. In theory, this application would have good scalability,
    given that we can always install it on another server. And overall, the application
    could be said to be reliable because a failure of a single server is going to
    affect only that one user, whereas the others can keep running as normal.
  prefs: []
  type: TYPE_NORMAL
- en: What is missing from this approach is the concept of resilience, or the ability
    of an application to respond meaningfully to failure. A truly resilient application
    can handle a hardware or software failure somewhere in the application without
    an end user noticing at all. And although separate, unrelated instances of this
    application keep running when one instance fails, we can’t really say that the
    application exhibits resilience, at least not from the perspective of the unlucky
    user with the failed system.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, if we construct our application out of separate microservices,
    each of which has the ability to communicate over a network with other microservices
    on any server, the loss of a single server might cost us several microservice
    instances, but end users can be moved to other instances on other servers transparently,
    such that they don’t even notice the failure.
  prefs: []
  type: TYPE_NORMAL
- en: Why Containers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: I’ve made modern application architecture with its fancy cloud native microservices
    sound pretty appealing. Engineering is full of trade-offs, however, so experienced
    engineers will suspect that there must be some pretty significant trade-offs,
    and, of course, there are.
  prefs: []
  type: TYPE_NORMAL
- en: It’s *very difficult* to build an application from lots of small pieces. Organizing
    teams around microservices so that they can work independently from one another
    might be great, but when it comes time to put those together into a working application,
    the sheer number of pieces means worrying about how to package them up, how to
    deliver them to the runtime environment, how to configure them, how to provide
    them with (potentially conflicting) dependencies, how to update them, and how
    to monitor them to make sure they are working.
  prefs: []
  type: TYPE_NORMAL
- en: This problem only grows worse when we consider the need to run multiple instances
    of each microservice. Now, we need a microservice to be able to find a working
    instance of another microservice, balancing the load across all of the working
    instances. We need that load balancing to reconfigure itself immediately if we
    have a hardware or software failure. We need to fail over seamlessly and retry
    failed work in order to hide that failure from the end user. And we need to monitor
    not just each individual service, but how all of them are working together to
    get the job done. After all, our users don’t care if 99 percent of our microservices
    are working correctly if the 1 percent failure prevents them from using our application.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have lots of problems to solve if we want to build an application out of
    many individual microservices, and we do not want each of our microservice teams
    working those problems, or they would never have time to write code! We need a
    common way to manage the packaging, deployment, configuration, and maintenance
    of our microservices. Let’s look at two categories of required attributes: those
    that apply to a single microservice, and those that apply to multiple microservices
    working together.'
  prefs: []
  type: TYPE_NORMAL
- en: Requirements for Containers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For a single microservice, we need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Packaging** Bundle the application for delivery, which needs to include dependencies
    so that the package is portable and we avoid conflicts between microservices.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Versioning** Uniquely identify a version. We need to update microservices
    over time, and we need to know what version is running.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Isolation** Keep microservices from interfering with one another. This allows
    us to be flexible about what microservices are deployed together.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fast startup** Start new instances rapidly. We need this to scale and respond
    to failures.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Low overhead** Minimize required resources to run a microservice in order
    to avoid limits on how small a microservice can be.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Containers* are designed to address exactly these needs. Containers provide
    isolation together with low overhead and fast startup. And, as we’ll see in [Chapter
    5](ch05.xhtml#ch05), a container runs from a container image, which provides a
    way to package an application with its dependencies and to uniquely identify the
    version of that package.'
  prefs: []
  type: TYPE_NORMAL
- en: Requirements for Orchestration
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For multiple microservices working together, we need:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Clustering** Provide processing, memory, and storage for containers across
    multiple servers.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discovery** Provide a way for one microservice to find another. Our microservices
    might run anywhere on the cluster, and they might move around.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Configuration** Separate configuration from runtime, allowing us to reconfigure
    our application without rebuilding and redeploying our microservices.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Access control** Manage authorization to create containers. This ensures
    that the right containers run, and the wrong ones don’t.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Load balancing** Spread requests among working instances in order to avoid
    the need for end users or other microservices to track all microservice instances
    and balance the load themselves.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Monitoring** Identify failed microservice instances. Load balancing won’t
    work well if traffic is going to failed instances.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Resilience** Automatically recover from failures. If we don’t have this ability,
    a chain of failures could kill our application.'
  prefs: []
  type: TYPE_NORMAL
- en: These requirements come into play only when we are running containers on multiple
    servers. It’s a different problem from just packaging up and running a single
    container. To address these needs, we require a *container orchestration* environment.
    A container orchestration environment such as Kubernetes allows us to treat multiple
    servers as a single set of resources to run containers, dynamically allocating
    containers to available servers and providing distributed communication and storage.
  prefs: []
  type: TYPE_NORMAL
- en: Running Containers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By now, hopefully you’re excited by the possibilities of building an application
    using containerized microservices and Kubernetes. Let’s walk through the basics
    so that you can see what these ideas look like in practice, providing a foundation
    for the deeper dive into container technology that you’ll find in the rest of
    this book.
  prefs: []
  type: TYPE_NORMAL
- en: What Containers Look Like
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In [Chapter 2](ch02.xhtml#ch02), we’ll look at the difference between a container
    platform and a container runtime, and we’ll run containers using multiple container
    runtimes. For now, let’s begin with a simple example running in the most popular
    container platform, *Docker*. Our goal is to learn the basic Docker commands,
    which align to universal container concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Running a Container
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The first command is `run`, which creates a container and runs a command inside
    it. We will tell Docker the name of the container image to use. We discuss container
    images more in [Chapter 5](ch05.xhtml#ch05); for now, it’s enough to know that
    it provides a unique name and version so that Docker knows exactly what to run.
    Let’s get started using the example for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*The example repository for this book is at* [https://github.com/book-of-kubernetes/examples](https://github.com/book-of-kubernetes/examples).
    *See “Running Examples” on [page xx](ch00.xhtml#ch00lev1sec2) for details on getting
    set up.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'A key idea for this section is that containers look like a completely separate
    system. To illustrate this, before we run a container, let’s look at the host
    system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The first command looks at a file called */etc/os-release*, which has information
    about the installed Linux distribution. In this case, our example virtual machine
    is running Ubuntu. That matches the output of the next command, in which we see
    an Ubuntu-based Linux kernel. Finally, we list network interfaces and see an IP
    address of `192.168.61.11`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The example setup steps automatically installed Docker, so we have it ready
    to go. First, let’s download and start a Rocky Linux container with a single command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We use `-ti` in our `docker run` command to tell Docker that we need an interactive
    terminal to run commands. The only other parameter to `docker run` is the container
    image, `rockylinux:8`, which specifies the name `rockylinux` and the version `8`.
    Because we don’t provide a command to run, the default `bash` command for that
    container image is used.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have a shell prompt inside the container, we can run a few commands
    and then use `exit` to leave the shell and stop the container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run commands within our container, it looks like we are running in
    a Rocky Linux system. Compared to the host system, there are multiple differences:'
  prefs: []
  type: TYPE_NORMAL
- en: A different hostname in the shell prompt ➊ (`18f20e2d7e49` for mine, though
    yours will be different)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different filesystem contents ➋, including basic files like */etc/os-release*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The use of `yum` ➌ to install packages, and the need to install packages even
    for basic commands
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A limited set of running processes, with no base system services and our bash
    shell ➍ as process ID (PID) 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different network devices ➎, including a different MAC address and IP address
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strangely, however, when we run `uname -v`, we see the exact same Ubuntu Linux
    kernel ➏ as when we were on the host. Clearly, a container is not a wholly separate
    system as we might otherwise believe.
  prefs: []
  type: TYPE_NORMAL
- en: Images and Volume Mounts
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'At first glance, a container looks like a mix between a regular process and
    a virtual machine. And the way we interact with Docker only deepens that impression.
    Let’s illustrate that by running an Alpine Linux container. We’ll start by “pulling”
    the container image, which feels a lot like downloading a virtual machine image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we’ll run a container from the image. We’ll use a *volume mount* to see
    files from the host, a common task with a virtual machine. However, we’ll also
    tell Docker to specify an environment variable, which is the kind of thing we
    would do when running a regular process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We can print the contents of */etc/os-release* inside the container, as before
    with Rocky Linux:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'However, this time we can also print the host’s */etc/os-release* file because
    the host filesystem is mounted at */host*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'And finally, within the container we also have access to the environment variable
    we passed in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This mix of ideas from virtual machines and regular processes sometimes leads
    new container users to ask questions like, “Why can’t I SSH into my container?”
    A major goal of the next few chapters is to make clear what containers really
    are.
  prefs: []
  type: TYPE_NORMAL
- en: What Containers Really Are
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Despite what a container looks like, with its own hostname, filesystem, process
    space, and networking, a container is not a virtual machine. It does not have
    a separate kernel, so it cannot have separate kernel modules or device drivers.
    A container can have multiple processes, but they must be started explicitly by
    the first process (PID 1). So a container will not have an SSH server in it by
    default, and most containers do not have any system services running.
  prefs: []
  type: TYPE_NORMAL
- en: In the next several chapters, we’ll look at how a container manages to look
    like a separate system while being a group of processes. For now, let’s try one
    more Docker example to see what a container looks like from the host system.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we’ll download and run NGINX with a single command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This example illustrates a couple of additional useful Docker commands. And
    again, we are mixing ideas from virtual machines and regular processes. By using
    the `-d` flag, we tell Docker to run this container in *daemon mode* (in the background),
    which is the kind of thing we would do for a regular process. Using `-p 8080:80`,
    however, brings in another concept from virtual machines, as it instructs Docker
    to forward port 8080 on the host to port 80 in the container, letting us connect
    to NGINX from the host even though the container has its own network interfaces.
  prefs: []
  type: TYPE_NORMAL
- en: 'NGINX is now running in the background in a Docker container. To see it, run
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Because of the port forwarding, we can connect to it from our host system using
    `curl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: With this example, we’re starting to see how containerization meets some of
    the needs we identified earlier in this chapter. Because NGINX is packaged into
    a container image, we can download and run it with a single command, with no concern
    for any conflict with anything else that might be installed on our host.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s run one more command to explore our NGINX server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: If NGINX were running in a virtual machine, we would not see it in a `ps` listing
    on the host system. Clearly, NGINX in a container is running as a regular process.
    At the same time, we didn’t need to install NGINX onto our host system to get
    it working. In other words, we are getting the benefits of a virtual machine approach
    without the overhead of a virtual machine.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Containers to Kubernetes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To have load balancing and resilience in our containerized applications, we
    need a container orchestration framework like Kubernetes. Our example system also
    has a Kubernetes cluster automatically installed, with a web application and database
    deployed to it. As a preparation for our deep dive into Kubernetes in [Part II](part02.xhtml#part02),
    let’s look at that application.
  prefs: []
  type: TYPE_NORMAL
- en: There are many different options for installing and configuring a Kubernetes
    cluster, with distributions available from many companies. We discuss multiple
    options for Kubernetes distributions in [Chapter 6](ch06.xhtml#ch06). For this
    chapter, we’ll use a lightweight distribution called “K3s” from a company called
    Rancher.
  prefs: []
  type: TYPE_NORMAL
- en: To use a container orchestration environment like Kubernetes, we have to give
    up some control over our containers. Rather than executing commands directly to
    run containers, we’ll tell Kubernetes what containers we want it to run, and it
    will decide where to run each container. Kubernetes will then monitor our containers
    for us and handle automatic restart, failover, updates to new versions, and even
    autoscaling based on load. This style of configuration is called *declarative*.
  prefs: []
  type: TYPE_NORMAL
- en: Talking to the Kubernetes Cluster
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A Kubernetes cluster has an API server that we can use to get status and change
    the cluster configuration. We interact with the API server using the `kubectl`
    client application. K3s comes with its own embedded `kubectl` command that we’ll
    use. Let’s begin by getting some basic information about the Kubernetes cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we’re working with a single-node Kubernetes cluster. Of course,
    this would not meet our needs for high availability. Most Kubernetes distributions,
    including K3s, support a multinode, highly available cluster, and we will look
    at how that works in detail in [Part II](part02.xhtml#part02).
  prefs: []
  type: TYPE_NORMAL
- en: Application Overview
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Our example application provides a “to-do” list with a web interface, persistent
    storage, and tracking of item state. It will take several minutes for this to
    be running in Kubernetes, even after the automated scripts are finished. After
    it’s running, we can access it in a browser and should see something like [Figure
    1-1](ch01.xhtml#ch01fig1).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0015-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 1-1: An example application in Kubernetes*'
  prefs: []
  type: TYPE_NORMAL
- en: 'This application is divided into two types of containers, one for each application
    component. A Node.js application serves files to the browser and provides a REST
    API. The Node.js application communicates with a PostgreSQL database. The Node.js
    component is stateless, so it is easy to scale up to as many instances as we need
    based on the number of users. In this case, our application’s Deployment asked
    Kubernetes for three Node.js containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The command `get pods` tells Kubernetes to list *Pods*. A Pod is a group of
    one or more containers that Kubernetes treats as a single unit for scheduling
    and monitoring. We look at Pods more closely throughout [Part II](part02.xhtml#part02).
  prefs: []
  type: TYPE_NORMAL
- en: Here, we have one Pod whose name starts with `todo-db`, which is our PostgreSQL
    database. The other three Pods, with names starting with `todo`, are the Node.js
    containers. (We’ll explain later why the names have random characters after them;
    you can ignore that for now.)
  prefs: []
  type: TYPE_NORMAL
- en: According to Kubernetes, our application component containers are running, so
    we should be able to access our application in a browser. How you do this depends
    on whether you are running in AWS or Vagrant; the example setup scripts will print
    out what URL you should use in your browser. If you visit that URL, you should
    see something like [Figure 1-1](ch01.xhtml#ch01fig1).
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes Features
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If our only goal were to run four containers, we could have done that just using
    the Docker commands described earlier. Kubernetes is providing a lot more functionality,
    though. Let’s take a quick tour of the most important features.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to running our containers, Kubernetes is also monitoring them.
    Because we asked for three instances, Kubernetes will work to keep three instances
    running. Let’s destroy one and watch Kubernetes automatically recover:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: To run this command, you will need to copy and paste the full name of one of
    your three Pods. The name will be a little different from mine. When you delete
    a Pod, you should see that Kubernetes immediately creates a new one. (You can
    identify which one is brand new by the `AGE` field.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Next let’s explore how Kubernetes can automatically scale our application.
    Later, we’ll see how to make Kubernetes do this automatically, but for now, we
    will do it manually. Suppose that we decide we need five Pods instead of three.
    We can do this with one command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We tell Kubernetes to scale the *Deployment* that manages our Pods. For now,
    you can think of the Deployment as the “owner” of the Pods; it monitors them and
    controls how many there are. Here, two extra Pods are immediately created. We
    just scaled up our application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we close, let’s look at one more critically important Kubernetes feature.
    When you load the application in your web browser, Kubernetes is sending your
    browser’s request to one of the available Pods. Each time you reload, the request
    might be routed to a different Pod because Kubernetes is automatically balancing
    the application’s load. To make this happen, when we deploy our application to
    Kubernetes, the application configuration includes a *Service*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: A Service has its own IP address and routes traffic to one or more endpoints.
    In this case, because we scaled up to five Pods, the Service is balancing traffic
    across all five endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: Final Thoughts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Modern applications achieve scalability and reliability through an architecture
    based on microservices that can be deployed independently and dynamically to available
    hardware, including cloud resources. By using containers and container orchestration
    to run our microservices, we achieve a common approach for packaging, scaling,
    monitoring, and maintaining microservices, enabling our development teams to focus
    on the hard work of actually building the application.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we saw how containerization can create the appearance of a
    separate system while really being a regular process run in an isolated way. We
    also saw how we can use Kubernetes to deploy an entire application as a set of
    containers, with scalability and self-healing. Of course, Kubernetes has a lot
    more important features than what we’ve mentioned here, enough that it will take
    the whole book for us to cover them all! With this brief overview, I hope you
    are excited to dive more deeply into containers and Kubernetes in order to understand
    how to build applications that perform well and are reliable.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll come back to Kubernetes in [Part II](part02.xhtml#part02) of this book.
    For now, let’s look closely at how containers create the illusion of a separate
    system. We’ll start by looking at process isolation using Linux namespaces.
  prefs: []
  type: TYPE_NORMAL
