<html><head></head><body>
<div id="sbo-rt-content" class="calibre1"><h2 class="h1" id="ch11"><span epub:type="pagebreak" id="page_185" class="calibre2"/><strong class="calibre3"><span class="big">11</span><br class="calibre18"/>LINEAR MODELS ON STEROIDS: NEURAL NETWORKS</strong></h2>
<div class="imagec"><img alt="Image" src="../images/common.jpg" class="calibre14"/></div>
<p class="noindent">The method of neural networks (NNs) is probably the best-known ML technology among the general public. The science fiction−sounding name is catchy—even more so with the advent of the term <em class="calibre13">deep learning</em>—and NNs have become the favorite approach to image classification in applications that also intrigue the general public, such as facial recognition.</p>
<p class="indent">Yet NNs are probably the most challenging ML technology to use well, with problems such as:</p>
<ul class="calibre15">
<li class="noindent3">“Black box” operation, where it’s not clear what’s going on inside</li>
<li class="noindent3">Numerous hyperparameters to tune</li>
<li class="noindent3">Tendency toward overfitting</li>
<li class="noindent3">Possibly lengthy computation time, with some large-data cases running for hours or even days when large amounts of RAM may be needed</li>
<li class="noindent3">Convergence issues</li>
</ul>
<p class="indent">Let’s see what all the fuss is about.<span epub:type="pagebreak" id="page_186"/></p>
<h3 class="h2" id="ch11lev1">11.1 Overview</h3>
<p class="noindent">The term <em class="calibre13">neural network</em> alludes to an ML method that is inspired by the biology of human thought. In a two-class classification problem, for instance, the predictor variables serve as inputs to a <em class="calibre13">neuron</em>, outputting 1 or 0, with 1 meaning that the neuron <em class="calibre13">fires</em>—and we decide class 1. NNs consist of several <em class="calibre13">hidden layers</em> in which the outputs of one layer of neurons are fed into the next layer and so on, until the process reaches the final output layer. This, too, has been given biological interpretation. The terms <em class="calibre13">node</em> and <em class="calibre13">units</em> are synonymous with <em class="calibre13">neurons</em>.</p>
<p class="indent">The method was later generalized, using <em class="calibre13">activation functions</em> with outputs other than just 1 and 0 and allowing backward feedback from later layers to earlier ones. This led development of the field somewhat away from the biological motivation, and some questioned the biological interpretation anyway, but NNs have a strong appeal for many in the machine learning community. Indeed, well-publicized large projects using <em class="calibre13">deep learning</em> have revitalized interest in NNs.</p>
<p class="indent"><a href="ch11.xhtml#ch11fig01" class="calibre12">Figure 11-1</a>, generated by the <code>neuralnet</code> package on our vertebrae data, illustrates how the method works. (We will not be using that package, but it does produce nice displays.)</p>
<div class="image"><img alt="Image" id="ch11fig01" src="../images/ch11fig01.jpg" class="calibre97"/></div>
<p class="figcap"><em class="calibre13">Figure 11-1: Vertebrae NN with one hidden layer</em><span epub:type="pagebreak" id="page_187"/></p>
<p class="indent">Here’s the overview:</p>
<ul class="calibre15">
<li class="noindent3">A neural network consists of a number of <em class="calibre13">layers</em> (three in this case).</li>
<li class="noindent3">In pictures describing a particular network, there is an input layer on the far left (the vertebrae measurements here) and an output layer on the far right, which, in this case, is giving the class predictions.</li>
<li class="noindent3">There are one or more <em class="calibre13">hidden</em> layers in between, with one in this case.</li>
<li class="noindent3">The outputs of one layer are fed as inputs into the next layer.</li>
<li class="noindent3">The output is typically a single number, for regression problems, and <em class="calibre13">c</em> numbers, for <em class="calibre13">c</em>-class classification problems.</li>
<li class="noindent3">The inputs into a layer are fed through what amounts to a linear model. The outputs of a layer are fed through an <em class="calibre13">activation function</em>, which is analogous to kernel functions in SVM, to accommodate nonlinear relations. In <a href="ch11.xhtml#ch11fig01" class="calibre12">Figure 11-1</a>, the activation function used was our old friend the logit, <em class="calibre13">a</em>(<em class="calibre13">t</em>) = 1/[1 + exp (−<em class="calibre13">t</em>)] (though in an entirely different context; we are <em class="calibre13">not</em> performing logistic regression).</li>
</ul>
<p class="indent">How does all this play out in <a href="ch11.xhtml#ch11fig01" class="calibre12">Figure 11-1</a>? Let’s look at some of the numbers in the diagram. For example, the input to the first circle, center column is 1.0841 · 1 + 0.84311 V1 + 0.49439 V2 + . . . , which is a linear combination of the features. A different linear combination is fed into the second circle. The output of each circle is fed into the next layer.</p>
<p class="indent">How are the coefficients (<em class="calibre13">weights</em>) in these linear combinations computed? We forgo a detailed mathematical answer here, but in essence, we minimize the sum of squared prediction errors in the regression case. In the classification case, we choose the weights to minimize the overall misclassification rate or a variant thereof.</p>
<h3 class="h2" id="ch11lev2">11.2 Working on Top of a Complex Infrastructure</h3>
<p class="noindent">Before we begin, a few words are in order regarding <code>qeNeural()</code>, our <code>qe*</code>-series function for building neural networks.</p>
<p class="indent">As we’ve noted throughout the book, the <code>qe*</code>-series functions are mainly wrappers—that is, convenient wrappers to other functions. This is done so that the series can provide a uniform, quick-and-easy user interface to a variety of ML algorithms.</p>
<p class="indent">Our function <code>qeSVM()</code>, for instance, wraps the <code>svm()</code> function in the package <code>e1071</code>. What about <code>qeNeural()</code>? There is quite a tale here! What happens (approximately) is:</p>
<ul class="calibre15">
<li class="noindent3">The function <code>qeNeural()</code> wraps the <code>regtools()</code> function <code>krsFit()</code>.</li>
<li class="noindent3">The function <code>krsFit()</code> wraps a number of functions in the R <code>keras</code> package for NNs.</li>
<li class="noindent3">The R <code>keras</code> package wraps the R <code>tensorflow</code> package.</li>
<li class="noindent3"><span epub:type="pagebreak" id="page_188"/>The R <code>tensorflow</code> package wraps the Python package of the same name.</li>
<li class="noindent3">And much of <code>tensorflow</code> is actually written in the C language.</li>
</ul>
<p class="noindent">And much of this, in turn, depends on the function <code>reticulate()</code> from the package of the same name. Its role is to translate between R and Python.</p>
<p class="indent">Setting this up, then, can be a bit delicate. See the RStudio site for help with your particular platform (for instance, <a href="https://tensorflow.rstudio.com/tutorials/quickstart/beginner.xhtml" class="calibre12"><em class="calibre13">https://tensorflow.rstudio.com/tutorials/quickstart/beginner.xhtml</em></a>). The R interfaces in the above list, as well as <code>reticulate</code>, were developed by RStudio.</p>
<p class="indent">It’s important to keep these points in mind about the “bilingual” nature of the software. For example, one implication is that even if you call <code>set.seed()</code> before your NNs run, you still will notice some variation from one run to the next. This will mystify you if you don’t know that Python has its own random number generator!</p>
<h3 class="h2" id="ch11lev3">11.3 Example: Vertebrae Data</h3>
<p class="noindent">Say we wish to fit a model and then do prediction. As before, we’ll specify no holdout set so that as much data as possible is used in the prediction:</p>
<pre class="calibre16">&gt; z &lt;- vert[1,-7]  # exclude "Y", which we are predicting
&gt; <span class="codestrong">nnout &lt;- qeNeural(vert,'V7',holdout=NULL)</span>
Epoch 1/30
2/2 [==============================] - 0s 62ms/step - loss: 1.0794 - accuracy: 0
.4274 - val_loss: 1.2847 - val_accuracy: 0.0000e+00
Epoch 2/30
2/2 [==============================] - 0s 19ms/step - loss: 0.9832 - accuracy: 0
.6048 - val_loss: 1.3886 - val_accuracy: 0.0000e+00
...</pre>
<p class="noindent">The fitting process is iterative, and a report is given on each iteration or <em class="calibre13">epoch</em>. The number of epochs is a hyperparameter. This and the other hyperparameters will be discussed in the next section.</p>
<p class="indent">As an example of prediction, consider a patient similar to the first one in our data, but with V2 being 18 rather than 22.55. What would be our predicted class?</p>
<pre class="calibre16">&gt; <span class="codestrong">z$V2 &lt;- 18</span>
&gt; <span class="codestrong">predict(nnout,z)</span>
$predClasses
[1] "DH"</pre>
<p class="noindent">We predict class DH.<span epub:type="pagebreak" id="page_189"/></p>
<h3 class="h2" id="ch11lev4">11.4 Neural Network Hyperparameters</h3>
<p class="noindent">NN libraries are notorious for having tons of hyperparameters. Our <code>qeNeural()</code> function has been designed to avoid this, having only a few hyperparameters. The call form is:</p>
<pre class="calibre16">qeNeural(data,yName,
   hidden=c(100,100),
   nEpoch=30,
   acts=rep("relu", length(hidden)),
   learnRate=0.001,
   conv = NULL, xShape = NULL,
   holdout=floor(min(1000,0.1*nrow(data)))
)</pre>
<p class="noindent">Here is what the NN-specific arguments represent:</p>
<div class="block3">
<p class="noindent"><span class="codestrong1">hidden</span>   Specifies the number of hidden layers and number of units per layer (this need not be constant across layers). The default means two hidden layers with 100 units each. If a number in this vector is fractional, it indicates <em class="calibre13">dropout</em>, which will be discussed below.</p>
<p class="noindent"><span class="codestrong1">nEpoch</span>   Specifies the number of epochs.</p>
<p class="noindent"><span class="codestrong1">acts</span>   Specifies the activation functions, with one for each hidden layer.</p>
<p class="noindent"><span class="codestrong1">learnRate</span>   Very similar to what we saw in gradient boosting (see <a href="ch06.xhtml#ch06lev3sec8" class="calibre12">Section 6.3.8</a>).</p>
<p class="noindent"><span class="codestrong1">conv, xShape</span>   Arguments used in image classification settings, which will be discussed in <a href="ch12.xhtml" class="calibre12">Chapter 12</a>.</p>
</div>
<p class="noindent">The analyst can use the <code>keras</code> package directly for more detailed control. These arguments address one or both of these aims:</p>
<ul class="calibre15">
<li class="noindent3">Controlling the Bias-Variance Trade-off: <code>hidden</code>, <code>nEpoch</code></li>
<li class="noindent3">Dealing with convergence issues: <code>nEpoch</code>, <code>acts</code>, <code>learnRate</code></li>
</ul>
<p class="noindent">The reader may be surprised to see the number of epochs—that is, the number of iterations—in the Bias-Variance Trade-off list above. In most iterative algorithms, the more iterations the better. But empirically, analysts have found that having too many iterations in NNs may result in overfitting.</p>
<h3 class="h2" id="ch11lev5">11.5 Activation Functions</h3>
<p class="noindent">If we were to simply input and output linear functions at each layer, we would have linear functions of linear functions of linear functions . . . , which would still be a linear function after all that combining. To be able to model nonlinear relations, we instead place <em class="calibre13">activation functions</em>, <em class="calibre13">a</em>(<em class="calibre13">t</em>), at the output of each layer.<span epub:type="pagebreak" id="page_190"/></p>
<p class="indent">Over the years, there has been some debate as to good choices for the activation function. In principle, any nonlinear function should work, but issues do arise, especially concerning the all-important convergence problem.</p>
<p class="indent">Consider once again <a href="ch06.xhtml#ch06fig02" class="calibre12">Figure 6-2</a>. The minimum around 2.2 comes at a rather sharp dip (in calculus terms, a large second derivative). But what if the curve were to look like that in <a href="ch11.xhtml#ch11fig02" class="calibre12">Figure 11-2</a>? There is a rather shallow trough near the minimum, extending, say, between −4 and 4. Here even a larger learning rate might have us spending many iterations with almost no progress. This is the <em class="calibre13">vanishing gradient problem</em>. And if the curve is very sharp near the minimum, we may have an <em class="calibre13">exploding gradient problem</em>, which can wreak havoc with even very small learning rates.</p>
<div class="image"><img alt="Image" id="ch11fig02" src="../images/ch11fig02.jpg" class="calibre98"/></div>
<p class="figcap"><em class="calibre13">Figure 11-2: Shallow minimum region</em></p>
<p class="indent">The choice of activation function plays a big role in these things. There is a multiplicative effect across layers. (Again, for those who know calculus, this is the Chain Rule in action.) And quantities in the interval (−1,1) become smaller and smaller when multiplied together, so that multiplicative effect results in smaller and smaller numbers, hence a vanishing gradient. If the gradient is large at each layer, we may develop an exploding gradient.</p>
<p class="indent">After years of trial and error, the popular choice among NN users today is the <em class="calibre13">Rectified Linear Unit (ReLU)</em>: <em class="calibre13">f</em>(<em class="calibre13">x</em>) is 0 for <em class="calibre13">x</em> &lt; 0 but is equal to <em class="calibre13">x</em> for <em class="calibre13">x</em> ≥ 0.</p>
<h3 class="h2" id="ch11lev6">11.6 Regularization</h3>
<p class="noindent">As noted, NNs have a tendency to overfit, with many having thousands of weights and some even millions. Remember, the weights are essentially linear regression coefficients, so the total number of weights is effectively the new value of <em class="calibre13">p</em> (that is, our number of features). We must find some way to reduce that value.<span epub:type="pagebreak" id="page_191"/></p>
<h4 class="h3" id="ch11lev6sec1"><em class="calibre22"><strong class="calibre3">11.6.1 L1 and L2 Regularization</strong></em></h4>
<p class="noindent">Since NNs (typically) minimize a sum of squares, we can apply a penalty term to reduce the size of the solution, just as in the cases of ridge regression and the LASSO. Recall also that in the LASSO, with the <em class="calibre13">ℓ</em><sub class="calibre27">1</sub> penalty, this tends to produce a sparse solution, with most coefficients being 0s.</p>
<p class="indent">Well, that is exactly what we want here. We fear we have too many weights, and we hope that applying an <em class="calibre13">ℓ</em><sub class="calibre27">1</sub> penalty will render most of them nil.</p>
<p class="indent">However, that may not happen with NNs due to the use of nonlinear activation functions. The problem is that the contours in <a href="ch09.xhtml#ch09fig03" class="calibre12">Figure 9-3</a> are no longer ellipses, and thus the “first contact point” will not likely be at a corner of the diamond.</p>
<p class="indent">Nevertheless, <em class="calibre13">ℓ</em><sub class="calibre27">1</sub> will still shrink the weights, as will <em class="calibre13">ℓ</em><sub class="calibre27">2</sub>, so we should achieve dimension reduction in some sense.</p>
<h4 class="h3" id="ch11lev6sec2"><em class="calibre22"><strong class="calibre3">11.6.2 Regularization by Dropout</strong></em></h4>
<p class="noindent">If a weight is 0, then in a picture of the network, such as <a href="ch11.xhtml#ch11fig01" class="calibre12">Figure 11-1</a>, the corresponding link is removed. So, if the goal is to remove some links, why not simply remove some links directly? Or better yet, remove entire nodes. That is exactly what <em class="calibre13">dropout</em> does.</p>
<p class="indent">For instance, if our dropout rate is 0.2, we randomly (and temporarily) choose 20 percent of the links from the given layer and remove them. There are further details that we will not list here, but this is the essence of the method.</p>
<h3 class="h2" id="ch11lev7">11.7 Example: Fall Detection Data</h3>
<p class="noindent">Let’s revisit the dataset analyzed in <a href="ch08.xhtml#ch08lev9sec4" class="calibre12">Section 8.9.4</a>. We’ll do a grid search for a good hyperparameter combination.</p>
<p class="indent">Recall that the <code>qeFT()</code> argument <code>pars</code> defines the grid in that it specifies the range of values we wish to explore.</p>
<pre class="calibre16">&gt; <span class="codestrong">pars &lt;- list(hidden=c('5,5','25,25','100,100','100,0.2,100,0.2',</span>
   <span class="codestrong">'100,0.5,100,0.5','250,0.5,250,0.5'),</span>
   <span class="codestrong">learnRate=c(0.0001,0.0005,0.001,0.005))</span>
&gt; <span class="codestrong">ftout &lt;- qeFT(fd,'ACTIVITY','qeNeural',pars=pars,nTst=250,nXval=25)</span></pre>
<p class="noindent">So, we are varying the number of neurons per layer (5, 100, 250) and the dropout rate (none, 0.2, 0.5). We could also have varied <code>nEpoch</code> and even the activation functions. Note, too, that we could have tried having different numbers of neurons in different layers.</p>
<p class="indent"><span epub:type="pagebreak" id="page_192"/>Here are the results:</p>
<pre class="calibre16">&gt; <span class="codestrong">ftout$outdf</span>
            hidden learnRate meanAcc        CI    bonfCI
1          100,100     5e-03 0.53256 0.5437322 0.5519608
2  100,0.2,100,0.2     5e-03 0.55896 0.5669100 0.5727654
3  250,0.5,250,0.5     1e-03 0.59480 0.6029615 0.6089727
4  250,0.5,250,0.5     5e-03 0.59696 0.6055761 0.6119221
5          100,100     1e-03 0.60040 0.6106513 0.6182015
6  100,0.2,100,0.2     1e-03 0.60048 0.6074992 0.6126690
7  250,0.5,250,0.5     5e-04 0.60928 0.6154121 0.6199285
8  100,0.5,100,0.5     5e-03 0.60952 0.6176062 0.6235618
9            25,25     5e-03 0.61344 0.6228619 0.6298013
10         100,100     5e-04 0.61744 0.6253014 0.6310915
11 100,0.5,100,0.5     1e-03 0.62120 0.6288043 0.6344051
12 100,0.2,100,0.2     5e-04 0.63056 0.6395884 0.6462380
13 100,0.5,100,0.5     5e-04 0.64048 0.6502662 0.6574739
14           25,25     1e-03 0.64664 0.6539690 0.6593669
15 250,0.5,250,0.5     1e-04 0.65368 0.6603284 0.6652252
16         100,100     1e-04 0.66168 0.6700386 0.6761949
17           25,25     5e-04 0.66528 0.6740087 0.6804375
18 100,0.2,100,0.2     1e-04 0.67240 0.6814274 0.6880762
19             5,5     5e-03 0.68504 0.6927206 0.6983776
20 100,0.5,100,0.5     1e-04 0.69240 0.6989253 0.7037314
21             5,5     1e-03 0.69696 0.7049328 0.7108050
22             5,5     5e-04 0.70368 0.7099837 0.7146265
23           25,25     1e-04 0.70608 0.7143002 0.7203546
24             5,5     1e-04 0.72544 0.7366268 0.7448662</pre>
<p class="indent">The first thing to notice is how much smaller the smallest value is than the largest. In fact, the latter is actually about the same as the base accuracy:</p>
<pre class="calibre16">&gt; <span class="codestrong">qeNeural(fd,'ACTIVITY')$baseAcc</span>  # any qe* function could be called
[1] 0.7182421</pre>
<p class="noindent">Without the features, we would have an error rate of 72 percent.</p>
<p class="indent">So, exploring the use of different values of the hyperparameter really paid off here.</p>
<p class="indent">But still, interesting patterns emerge here, notably the effect of the learning rate. The smaller values tended to do poorly. Remember, if our learning rate is too small, not only might it slow down convergence, but it also may leave us stuck at a local minimum.</p>
<p class="indent">Finally, note that in this case, a smaller value for the dropout rate seemed to produce better results.<span epub:type="pagebreak" id="page_193"/></p>
<h3 class="h2" id="ch11lev8">11.8 Pitfall: Convergence Problems</h3>
<p class="noindent">As noted, it is often a challenge to configure NN analysis so that proper convergence to a good solution is attained. In some cases, one might even encounter the <em class="calibre13">broken clock problem</em>—that is, the network predicts the same value no matter what the inputs are.</p>
<p class="indent">Or, one might encounter output like this:</p>
<pre class="calibre16">Epoch 27/30
618/618 [==============================] -
1s 2ms/step - loss: nan - accuracy: 0.7571 - val _loss: nan - val_accuracy: 0.7520</pre>
<p class="noindent">Here <code>nan</code> stands for “not a number.” That ominous-sounding message may mean the code attempted to divide by 0, which may be due to the vanishing gradient problem.</p>
<p class="indent">The following describes a few tricks we can try, typically specified via one or more hyperparameters.</p>
<p class="indent">In some cases, convergence problems may be solved by scaling the data, either using the R <code>scale()</code> function or by mapping to [0,1]. It is recommended that one routinely scale one’s data; in <code>qeNeural()</code>, scaling is actually hardwired into the software.</p>
<p class="indent">Here are some values to tweak:</p>
<p class="block"><strong class="calibre5">Learning rate</strong>   Discussed in <a href="ch06.xhtml#ch06lev3sec8" class="calibre12">Section 6.3.8</a>.</p>
<p class="block"><strong class="calibre5">Activation function</strong>   Try changing to one with a steeper/shallower slope. For example, the function <em class="calibre13">a</em>(<em class="calibre13">t</em>) = 1/(1 + exp (−2<em class="calibre13">t</em>)) is steeper around <em class="calibre13">t</em> = 0 than the ordinary logistic function.</p>
<p class="block"><strong class="calibre5">Early stopping</strong>   In most algorithms, the more iterations the better, but in NNs, many issues depart from conventional wisdom. Running the algorithm for too long may result in convergence to a poor solution. This leads to the notion of <em class="calibre13">early stopping</em>, of which there are many variants.</p>
<p class="block"><strong class="calibre5">Momentum</strong>   The rough idea here is that “We’re on a roll,” with the last few epochs producing winning moves in the right direction, reducing validation error each time. So, instead of calculating the next step size individually, why not combine the last few step sizes? The next step size will be set to a weighted average of the last few, with heavier weight on the more recent ones. (This hyperparameter is not available in <code>qeNeural()</code> but may be accessed through the <code>keras</code> package directly.)</p>
<p class="noindent">Note that regression applications, as opposed to classification, may be especially prone to convergence problems, since <em class="calibre13">Y</em> is unbounded.<span epub:type="pagebreak" id="page_194"/></p>
<h3 class="h2" id="ch11lev9">11.9 Close Relation to Polynomial Regression</h3>
<p class="noindent">In <a href="ch08.xhtml#ch08lev11" class="calibre12">Section 8.11</a>, we introduced polynomial regression, a linear model in which the features are in polynomial form. So, for instance, instead of just having people’s heights and ages as features, in a quadratic model we now would also have the squares of heights and ages, as well as a cross-product term, height × age.</p>
<p class="indent">Polynomials popped up again in SVM, with polynomial kernels. We might have, for instance, not just height and age but also the squares of heights and ages, as well the height × age term. And we noted that even the use of the radial basis function, a nonpolynomial kernel, is approximately a polynomial due to Taylor series expansion.</p>
<p class="indent">It turns out that NNs essentially do polynomial regression as well. To see this, let’s look again at <a href="ch11.xhtml#ch11fig01" class="calibre12">Figure 11-1</a>. Suppose we take as our activation function the squaring function <em class="calibre13">t</em><sup class="calibre11">2</sup> . That is not a common choice at all, but we’ll start with that and then extend the argument.</p>
<p class="indent">So, in the hidden layer in <a href="ch11.xhtml#ch11fig01" class="calibre12">Figure 11-1</a>, a circle forms a linear combination of the inputs and then outputs the square of the linear combination. That means the outputs of the hidden layer are second-degree polynomials in the inputs. If we were to have a second hidden layer, its outputs would be fourth-degree polynomials.</p>
<p class="indent">What if our activation function itself were to be a polynomial? Then again, each successive layer would give us higher and higher degree polynomials in the inputs. Since NNs minimize the sum of squared prediction errors, just as in the linear model, you can see that the minimizing solution will be that of polynomial regression.</p>
<p class="indent">And what about the popular activation functions? One is the <em class="calibre13">hyperbolic tangent</em>, <em class="calibre13">tanh</em>(<em class="calibre13">t</em>), whose graph looks similar to the logistic function. But it too has a Taylor series expansion, so what we are doing is approximately polynomial regression.</p>
<p class="indent">ReLU does not have a Taylor series expansion, but we can form a polynomial approximation there too.</p>
<p class="indent">In that case, why not just use polynomial regression in the first place? Why NNs? One answer is that it just would be computationally infeasible for large- <em class="calibre13">p</em> data, where we could have a very large number of polynomial terms in calling <code>lm()</code> or <code>glm()</code>. This would cause memory issues. (It’s less of a problem for NNs because they find the least squares solutions iteratively. This may cause convergence problems but at least uses less memory.) The kernel trick is very helpful here, and there is even the <em class="calibre13">kernel ridge regression</em> method that applies this to linear ridge models, but it turns out that this too is infeasible for large-<em class="calibre13">n</em> cases.</p>
<p class="indent">NNs have their own computational issues, as noted, but through trying many combinations of hyperparameters, we may still have a good outcome. Also, if we manage to find a good NN fit on some classes of problems, sometimes we can tweak it to find a good NN fit on some related class (<em class="calibre13">transfer learning</em>).<span epub:type="pagebreak" id="page_195"/></p>
<h3 class="h2" id="ch11lev10">11.10 Bias vs. Variance in Neural Networks</h3>
<p class="noindent">We often refer to the number of hidden layers as the <em class="calibre13">depth</em> of a network and the number of units per layer as the <em class="calibre13">width</em>. The larger the product of these two (actually depth times the square of the width), the more weights or parameters the network has. As discussed in <a href="ch08.xhtml#ch08lev10sec1" class="calibre12">Section 8.10.1</a>, the more parameters a model has, the more variance increases, even though bias is reduced.</p>
<p class="indent">This can be seen as well in light of the polynomial regression connection to NNs described in the previous section. Roughly speaking, the larger the number of hidden layers in an NN, the higher the degree of a polynomial regression approximation. And the higher the degree of a polynomial regression model, the smaller the bias but the larger the variance.</p>
<p class="indent">So, NNs are not immune to the Bias-Variance Trade-off. This must be kept in mind when designing an NN’s architecture.</p>
<h3 class="h2" id="ch11lev11">11.11 Discussion</h3>
<p class="noindent">NNs have played a major role in the “ML revolution” of recent years, with notable success in certain types of applications. But they can incur huge computational costs, in some cases having run times measured in hours or even days, and can have vexing convergence problems.</p>
<p class="indent">In addition, folklore in the ML community suggests that NNs are not especially effective with <em class="calibre13">tabular data</em>, meaning the type stored in data frames—that is, every dataset we have seen so far in this book. The reader may wish to reserve NNs for usage in applications such as image recognition and natural language processing, which will be covered in the next two chapters.<span epub:type="pagebreak" id="page_196"/></p>
</div></body></html>