<html><head></head><body>
<div id="sbo-rt-content" class="calibre1"><h2 class="h1" id="ch02"><span epub:type="pagebreak" id="page_31" class="calibre2"/><strong class="calibre3"><span class="big">2</span><br class="calibre18"/>CLASSIFICATION MODELS</strong></h2>
<div class="imagec"><img alt="Image" src="../images/common.jpg" class="calibre14"/></div>
<p class="noindent">The last chapter briefly introduced <em class="calibre13">classification applications</em>, in which we predict dummy or categorical variables. These differ from the <em class="calibre13">numeric applications</em> we’ve analyzed, such as predicting the number of bike riders, which is a numeric entity. For instance, in a marketing application, we might wish to predict whether a customer will purchase a certain product. In that case, we’d represent the “Y” outcome with a dummy variable, using 1 for buying the item and 0 for not buying it. There are two <em class="calibre13">classes</em> here: Buy and Not Buy.</p>
<p class="indent">We discussed an example of categorical <em class="calibre13">Y</em> in the “Some Terminology” box in <a href="ch01.xhtml#ch01lev1" class="calibre12">Section 1.1</a>. In that example, a physician has divided patients into three classes—that is, three categories—depending on their spinal condition: normal (NO), disk hernia (DH), and spondylolisthesis (SL). Here <em class="calibre13">Y</em> is the class, or category, for the given patient. Thus <em class="calibre13">Y</em> is a categorical variable with three categories. If <em class="calibre13">Y</em> is coded as an R <em class="calibre13">factor</em>, which is usually the case, then the factor will have three levels. On the other hand, we might code <em class="calibre13">Y</em> as a vector of dummy variables, with one for each class.</p>
<p class="indent"><span epub:type="pagebreak" id="page_32"/>We will analyze this vertebrae data in detail in <a href="ch02.xhtml#ch02lev3sec1" class="calibre12">Section 2.3.1</a>, stating where to obtain it and so on. But let’s do a sneak preview to illustrate the previously described notions of class, or category.</p>
<pre class="calibre16">&gt; <span class="codestrong">vert &lt;- read.table('column_3C.dat',header=FALSE,stringsAsFactors=TRUE)</span>
&gt; <span class="codestrong">head(vert)</span>
     V1    V2    V3    V4     V5    V6 V7
1 63.03 22.55 39.61 40.48  98.67 -0.25 DH
2 39.06 10.06 25.02 29.00 114.41  4.56 DH
3 68.83 22.22 50.09 46.61 105.99 -3.53 DH
4 69.30 24.65 44.31 44.64 101.87 11.21 DH
5 49.71  9.65 28.32 40.06 108.17  7.92 DH
6 40.25 13.92 25.12 26.33 130.33  2.23 DH
&gt; <span class="codestrong">table(vert$V7)</span>

 DH  NO  SL
 60 100 150
&gt; <span class="codestrong">class(vert$V7)</span>
[1] "factor"
&gt; <span class="codestrong">levels(vert$V7)</span>
[1] "DH" "NO" "SL"</pre>
<p class="noindent">The vertebral condition is in the last column. We see that in this dataset, there were 60 patients of class DH and so on.</p>
<p class="indent">This chapter goes into more detail regarding classification applications, beginning with a brief discussion of a conceptual issue, the notion of a <em class="calibre13">regression function</em>, and then getting right to the data analysis. We’ll bring in some new datasets, again analyzing them using <code>qeKNN()</code> but showing some special issues that arise in classification contexts.</p>
<h3 class="h2" id="ch02lev1">2.1 Classification Is a Special Case of Regression</h3>
<p class="noindent">Classification applications are quite common in ML. In fact, they probably form the majority of ML applications. How does the regression function <em class="calibre13">r</em>(<em class="calibre13">t</em>) (see <a href="ch01.xhtml#ch01lev6" class="calibre12">Section 1.6</a>) play out in such contexts?</p>
<p class="indent">Recall that the regression function relates mean <em class="calibre13">Y</em> to <em class="calibre13">X</em>. If we are predicting weight from height and age, then <em class="calibre13">r</em>(71, 25) means the mean weight of all people of height 71 inches and age 25. But how does this work if <em class="calibre13">Y</em> is a dummy variable?</p>
<p class="indent">In classification settings, the regression function, a conditional mean, becomes a conditional probability. To see why, consider a classification application in which the outcome <em class="calibre13">Y</em> is represented by a dummy variable coded 1 or 0, such as the marketing example at the start of this chapter. After collecting our <em class="calibre13">k</em>-nearest neighbors, we average their 1s and 0s. Say, for example, that <em class="calibre13">k</em> = 8, and the outcomes for those 8 neighbors are 0,1,1,0,0,0,1,0. The average is then (0 + 1 + 1 + 0 + 0 + 0 + 1 + 0) / 8 = 3/8 = 0.375. Since this means that 3/8 of the outcomes were 1s, you can think of the average of 0-or-1 outcomes as the <em class="calibre13">probability</em> of a 1.</p>
<p class="indent"><span epub:type="pagebreak" id="page_33"/>In the marketing example, the regression function is the probability that a customer will buy a certain product, conditioned on the customer’s feature values, such as age, gender, income, and so on. We then guess either Buy or Not Buy according to whichever class has the larger probability. Since there are just two classes here, that’s equivalent to saying we guess Buy if and only if its class probability is greater than 0.5. (This strategy minimizes the overall probability of misclassification. However, other criteria are possible, which is a point we will return to later.)</p>
<p class="indent">The same is true in multiclass settings. Consider the medical application above. For a new patient whose vertebral status is to be predicted, ML would give the physician three probabilities—one for each class. As above, these probabilities come from averaging 1s and 0s in dummy variables, with one dummy for each class. The preliminary diagnosis would be the spinal class with the highest estimated probability.</p>
<p class="indent">In summary:</p>
<p class="block">The function <em class="calibre13">r</em>() defined for use in predicting numeric quantities applies to classification settings as well. In those settings, mean values reduce to probabilities, and we use those to predict class.</p>
<p class="noindent">This view is nice as a unifying concept.</p>
<div class="note">
<p class="notet"><strong class="calibre3"><span class="notes">NOTE</span></strong></p>
<p class="notep"><em class="calibre13">The reader may wonder why we use three dummy variables to classify patients’ spinal conditions. As noted in <a href="ch01.xhtml#ch01lev4" class="calibre12">Section 1.4</a>, just two should suffice here; however, certain other methods covered later on require more than two. For consistency, we’ll always take the approach of using as many dummies as classes. Note that this convention is for</em> Y<em class="calibre13">; categorical features in</em> X <em class="calibre13">will still usually have one fewer dummy than values that the feature can take on.</em></p>
</div>
<p class="indent">Therefore, classification problems are really special cases of regression, which is a point we’ll often return to in this book. However, the field uses some confusing terminology, which one must be aware of.</p>
<p class="indent">Previously, we distinguished between, on the one hand, <em class="calibre13">numerical</em> applications, say, predicting the <em class="calibre13">number</em> of bike riders, and, on the other hand, <em class="calibre13">classification</em> applications, such as the earlier marketing and medical examples where we are predicting a <em class="calibre13">class</em>.</p>
<p class="indent">However, it is customary in the ML field to call numeric applications <em class="calibre13">regression problems</em>. This, of course, is a source of confusion, since both numeric and classification applications involve the regression function! Sigh . . . As long as you are aware of this, it’s not a big issue, but in this book we will use the term <em class="calibre13">numeric-Y applications</em> for clarity.</p>
<h3 class="h2" id="ch02lev2">2.2 Example: The Telco Churn Dataset</h3>
<p class="noindent">For our first example of a classification model, we’ll take the Telco Customer Churn dataset. In marketing circles, the term <em class="calibre13">churn</em> refers to customers moving from one purveyor of a service to another. A service will then hope to identify customers who are likely “flight risks,” or those <span epub:type="pagebreak" id="page_34"/>with a substantial probability of leaving. So, we have two classes: Churn or No Churn (that is, Leave or Stay).</p>
<p class="indent">You can download and learn more about the dataset at <a href="https://www.kaggle.com/blastchar/telco-customer-churn" class="calibre12"><em class="calibre13">https://www.kaggle.com/blastchar/telco-customer-churn</em></a>. Let’s load it and take a look:</p>
<pre class="calibre16">&gt; <span class="codestrong">telco &lt;- read.csv('WA_Fn-UseC_-Telco-Customer-Churn.csv',header=TRUE)</span>
&gt; <span class="codestrong">head(telco)</span>
  customerID gender SeniorCitizen Partner Dependents tenure
1 7590-VHVEG Female             0     Yes         No      1
2 5575-GNVDE   Male             0      No         No     34
3 3668-QPYBK   Male             0      No         No      2
4 7795-CFOCW   Male             0      No         No     45
5 9237-HQITU Female             0      No         No      2
6 9305-CDSKC Female             0      No         No      8
  PhoneService    MultipleLines InternetService
1           No No phone service             DSL
2          Yes               No             DSL
3          Yes               No             DSL
4           No No phone service             DSL
5          Yes               No     Fiber optic
6          Yes              Yes     Fiber optic
...
&gt; <span class="codestrong">names(telco)</span>
 [1] "customerID"       "gender"
 [3] "SeniorCitizen"    "Partner"
 [5] "Dependents"       "tenure"
 [7] "PhoneService"     "MultipleLines"
 [9] "InternetService"  "OnlineSecurity"
[11] "OnlineBackup"     "DeviceProtection"
[13] "TechSupport"      "StreamingTV"
[15] "StreamingMovies"  "Contract"
[17] "PaperlessBilling" "PaymentMethod"
[19] "MonthlyCharges"   "TotalCharges"
[21] "Churn"</pre>
<p class="noindent">That last column is the response; <code>Yes</code> in the <code>Churn</code> column means yes, the customer bolted.</p>
<p class="indent">Let’s move on to data preparation, such as checking for NA values. This is a rather complex dataset, necessitating extra prep—a bonus for learners of data science like readers of this book!</p>
<h4 class="h3" id="ch02lev2sec1"><em class="calibre22"><strong class="calibre3">2.2.1 Pitfall: Factor Data Read as Non-factor</strong></em></h4>
<p class="noindent">Many features in the telco dataset are R factors (that is, nonnumeric quantities such as <code>gender</code> and <code>InternetService</code>). Most R ML packages, including the <code>qe*</code>-series functions, allow factors. But wait . . . they’re not factors after all. By default, <code>read.csv()</code> treats nonnumeric items as character strings:<span epub:type="pagebreak" id="page_35"/></p>
<pre class="calibre16">&gt; <span class="codestrong">class(telco$Churn)</span>
[1] "character"</pre>
<p class="noindent">Since our software expects R factors, we need to tell R to treat nonnumeric items as factors. (Actually, depending on your version of R, and your default settings, this may actually be your default value. If you are not sure, go ahead and set this in your call.)</p>
<pre class="calibre16">&gt; <span class="codestrong">telco &lt;- read.csv('WA_Fn-UseC_-Telco-Customer-Churn.csv',header=TRUE,</span>
   <span class="codestrong">stringsAsFactors=TRUE)</span>
&gt; <span class="codestrong">class(telco$Churn)</span>
[1] "factor"</pre>
<p class="indent">Failure to do this will result in character or numeric values, causing problems when we run the ML functions.</p>
<h4 class="h3" id="ch02lev2sec2"><em class="calibre22"><strong class="calibre3">2.2.2 Pitfall: Retaining Useless Features</strong></em></h4>
<p class="noindent">In some datasets, some columns have no predictive value and should be removed. The <code>customerID</code> feature is of no predictive value (though it might be if we had multiple data points for each customer), so we’ll delete the first column:</p>
<pre class="calibre16">&gt; <span class="codestrong">tc &lt;- telco[,-1]</span></pre>
<p class="noindent">Retaining useless features can lead to overfitting.</p>
<h4 class="h3" id="ch02lev2sec3"><em class="calibre22"><strong class="calibre3">2.2.3 Dealing with NA Values</strong></em></h4>
<p class="noindent">As noted in <a href="ch01.xhtml#ch01lev16" class="calibre12">Section 1.16</a>, many datasets contain NA values for missing (not available) data. Let’s see if this is the case here:</p>
<pre class="calibre16">&gt; <span class="codestrong">sum(is.na(tc))</span>
[1] 11</pre>
<p class="noindent">There are indeed 11 NA values present.</p>
<p class="indent">We’ll use listwise deletion here as a first-level analysis (see <a href="ch01.xhtml#ch01lev16" class="calibre12">Section 1.16</a>), but if we were to pursue the matter further, we may look more closely at the NA pattern. R actually has a <code>complete.cases()</code> function, which returns <code>TRUE</code> for the rows that are intact. Let’s delete the other cases:</p>
<pre class="calibre16">&gt; <span class="codestrong">ccIdxs &lt;- which(complete.cases(tc))</span>
&gt; <span class="codestrong">tc &lt;- tc[ccIdxs,]</span></pre>
<p class="noindent">If we don’t need to know which particular cases are excluded, we could simply run:</p>
<pre class="calibre16">&gt; <span class="codestrong">tc &lt;- na.exclude(tc)</span></pre>
<p class="noindent">How many cases are left?<span epub:type="pagebreak" id="page_36"/></p>
<pre class="calibre16">&gt; <span class="codestrong">nrow(tc)</span>
[1] 7032</pre>
<p class="noindent">It’s generally a good idea to check this.</p>
<p class="indent">Among other things, the number of remaining rows affects the size of the value we choose for the number of near neighbors <em class="calibre13">k</em> (see <a href="ch01.xhtml#ch01lev12sec4" class="calibre12">Section 1.12.4</a>).</p>
<h4 class="h3" id="ch02lev2sec4"><em class="calibre22"><strong class="calibre3">2.2.4 Applying the k-Nearest Neighbors Method</strong></em></h4>
<p class="noindent">Let’s see how to call the <code>qeKNN()</code> function in classification problems, say, by setting <em class="calibre13">k</em> to 75:</p>
<pre class="calibre16">&gt; <span class="codestrong">set.seed(9999)</span>
&gt; <span class="codestrong">knnout &lt;- qeKNN(tc,'Churn',75,yesYVal='Yes')</span></pre>
<p class="indent">As an example of prediction, say we have a new case to predict, such as a hypothetical customer like the one in row 8 of the data but who is male and a senior citizen. To set this up, we’ll copy row 8 of <code>tc</code> and make the stated changes in the <code>gender</code> and <code>SeniorCitizen</code> columns:</p>
<pre class="calibre16">&gt; <span class="codestrong">newCase &lt;- tc[8,]</span>
&gt; <span class="codestrong">newCase$gender &lt;- 'Male'</span>
&gt; <span class="codestrong">newCase$SeniorCitizen &lt;- 1</span></pre>
<p class="indent">Note, too, that since this revised row will be our “X,” we need to remove the “Y” portion—that is, remove the <code>Churn</code> column. We saw earlier that <code>Churn</code> is column 21, but remember, we removed column 1, so it’s now in column 20.</p>
<pre class="calibre16">&gt; <span class="codestrong">newCase &lt;- newCase[,-20]</span></pre>
<p class="indent">Alternatively, we could have used the <code>subset()</code> function in base R,</p>
<pre class="calibre16">&gt; <span class="codestrong">newCase &lt;- subset(newCase,select=-Churn)</span></pre>
<p class="noindent">the <code>data.table</code> package, or the tidyverse. As noted on <a href="ch01.xhtml#ch01lev5sec1" class="calibre12">page 11</a>, it is up to readers to choose whichever R constructs they feel most comfortable with; our focus in this book is on ML, with R playing only a supporting role.</p>
<p class="indent">Now we make the prediction for the new case:</p>
<pre class="calibre16">&gt; <span class="codestrong">predict(knnout,newCase)</span>
[,1]
[1,] 0.3066667</pre>
<p class="noindent">The class of interest to us is Churn, so we check the probability of Churn for this case, which is 0.32. Since it’s under 0.5, we guess No Churn.</p>
<p class="indent"><span epub:type="pagebreak" id="page_37"/>Calls to <code>qeKNN()</code> for classification problems are essentially the same as for numeric-outcome applications. The form of the output is slightly different.</p>
<p class="indent">But . . . how does <code>qeKNN()</code> know that this is a classification application rather than a numeric- <em class="calibre13">Y</em> problem? Our specified <em class="calibre13">Y</em> variable, <code>Churn</code>, is an R factor, signaling to <code>qeKNN()</code> that we are running a classification application.</p>
<p class="indent">Let’s check the classification accuracy.</p>
<pre class="calibre16">&gt; <span class="codestrong">knnout$testAcc</span>
[1] 0.2247511</pre>
<p class="noindent">We have a misclassification rate of about 22 percent, which is not too bad. Once again, however, I must emphasize that those numbers are subject to sampling variation, which we will discuss further in <a href="ch03.xhtml" class="calibre12">Chapter 3</a>.</p>
<h4 class="h3" id="ch02lev2sec5"><em class="calibre22"><strong class="calibre3">2.2.5 Pitfall: Overfitting Due to Features with Many Categories</strong></em></h4>
<p class="noindent">Suppose we had not removed the <code>customerID</code> column in our original data, <code>telco</code>. How many distinct IDs are there?</p>
<pre class="calibre16">&gt; <span class="codestrong">length(levels(telco$customerID))</span>
[1] 7043</pre>
<p class="noindent">The number of IDs is also the number of rows, as there’s one record per customer.</p>
<p class="indent">Recall that <code>qeKNN()</code>, like functions in many R packages, internally converts factors to dummy variables. If we had not removed this column, there would have been 7,042 columns in the internal version of <code>tc</code> just stemming from this ID column! Not only would the result be unwieldy, but the presence of all these columns would dilute the power of k-NN.</p>
<p class="indent">This latter phenomenon is known as <em class="calibre13">overfitting</em>. Using too many features will actually reduce accuracy in predicting future cases. <a href="ch03.xhtml" class="calibre12">Chapter 3</a> covers this in greater detail, but for now, one might loosely think of the data as being “shared” by too many features, with not much data available to each one. Note that overfitting is a concern both in classification and numeric-<em class="calibre13">Y</em> applications.</p>
<p class="indent">There also may be computational issues if we were to include the ID. Having 7,000 customer IDs would mean 7,000 dummy variables. That means the internal data matrix has over 7000 × 7000 entries—about 50 million. And at 8 bytes each, that means something like 0.4GB of RAM. We’ve got to remove this column.</p>
<p class="indent">Even with ML packages that directly accept factor data, one must keep an eye on what the package is doing—and what we are feeding into it. It’s good practice to watch for R factors with a large number of levels. They may appear useful, and may in fact be so. But they can also lead to overfitting and computational or memory problems.</p>
<h3 class="h2" id="ch02lev3"><span epub:type="pagebreak" id="page_38" class="calibre2"/>2.3 Example: Vertebrae Data</h3>
<p class="noindent">Consider another UCI dataset, Vertebral Column Dataset,<sup class="calibre11"><a id="ch2fn1b" class="calibre12"/><a href="footnote.xhtml#ch2fn1" class="calibre12">1</a></sup> which is on diseases of the vertebrae. It is described by the curator as a dataset that contains “values for six biomechanical features used to classify orthopaedic patients into 3 classes (normal, disk hernia, or spondilolysthesis [ <em class="calibre13">sic</em> ]).” They abbreviate the three classes as NO, DH, and SL.</p>
<p class="indent">This example is similar to the last one, but with three classes instead of two. Recall that in a two-class problem, we predict on the basis of whether the probability of the class of interest is greater than 0.5. In the telco example, that class was Churn, so we predict either Churn or No Churn, depending on whether the Churn probability is greater than 0.5. That probability turned out to be 0.32, so we predicted No Churn. But with three or more classes, none of the probabilities might be above 0.5; we simply choose the class with the largest probability.</p>
<h4 class="h3" id="ch02lev3sec1"><em class="calibre22"><strong class="calibre3">2.3.1 Analysis</strong></em></h4>
<p class="noindent">Let’s read in the data and, as usual, take a look around.</p>
<pre class="calibre16">&gt; <span class="codestrong">vert &lt;- read.table('column_3C.dat',header=FALSE,stringsAsFactors=TRUE)</span>
&gt; <span class="codestrong">head(vert)</span>
     V1    V2    V3    V4     V5    V6 V7
1 63.03 22.55 39.61 40.48  98.67 -0.25 DH
2 39.06 10.06 25.02 29.00 114.41  4.56 DH
3 68.83 22.22 50.09 46.61 105.99 -3.53 DH
4 69.30 24.65 44.31 44.64 101.87 11.21 DH
5 49.71  9.65 28.32 40.06 108.17  7.92 DH
6 40.25 13.92 25.12 26.33 130.33  2.23 DH
&gt; <span class="codestrong">nrow(vert)</span>
[1] 310</pre>
<p class="noindent">The patient status is column <code>V7</code>. We see, by the way, that the curator of the dataset decided to group the rows by patient class. That’s why the <code>qe*</code>-series functions randomly choose the holdout sets.</p>
<p class="indent">Let’s fit the model. The question of how to choose <em class="calibre13">k</em> is still open, but we need to take into account the size of our dataset. We only have 310 cases here, in contrast to the <em class="calibre13">n</em> = 7032 we had in the customer churn example. Recall from <a href="ch01.xhtml#ch01lev12sec4" class="calibre12">Section 1.12.4</a> that the larger our number of data points <em class="calibre13">n</em> is, the larger we can set the number of nearest neighbors <em class="calibre13">k</em>, so with this small dataset, let’s try a small value here, say, <em class="calibre13">k</em> = 5.</p>
<pre class="calibre16">&gt; <span class="codestrong">set.seed(9999)</span>
&gt; <span class="codestrong">kout &lt;- qeKNN(vert,'V7',5)</span></pre>
<p class="indent"><span epub:type="pagebreak" id="page_39"/>As an example of prediction, consider a patient similar to the first one in our data but with <code>V2</code> being 25 rather than 22.55. What would our predicted class be?</p>
<pre class="calibre16">&gt; <span class="codestrong">z &lt;- vert[1,-7]</span>  # exclude Y
&gt; <span class="codestrong">z$V2 &lt;- 25</span>
&gt; <span class="codestrong">predict(kout,z)</span>
$predClasses
[1] "dfr.DH"
<br class="calibre1"/>
$probs
     dfr.DH dfr.NO dfr.SL
[1,]    0.6    0.2    0.2</pre>
<p class="noindent">We’d predict the DH class, with an estimated probability of 0.6. Now let’s find the overall accuracy of our predictions using this model.</p>
<pre class="calibre16">&gt; <span class="codestrong">kout$testAcc</span>
[1] 0.1935484</pre>
<p class="noindent">We would have an error rate of about 19 percent. Note, though, that due to the small sample size (310), our predictions would, in this case, be quite susceptible to sampling variation.</p>
<h3 class="h2" id="ch02lev4">2.4 Pitfall: Error Rate Improves Only Slightly Using the Features</h3>
<p class="noindent">When working with any ML method, on any dataset, it’s important to check whether your predictions have a better chance of success than those based on random chance, without using the features. Recall our analysis along those lines in <a href="ch01.xhtml#ch01lev12sec2" class="calibre12">Section 1.12.2</a>. Let’s see an example in the classification realm.</p>
<p class="indent">Consider the telco dataset in <a href="ch02.xhtml#ch02lev2" class="calibre12">Section 2.2</a>. We found that in using 19 features to predict whether a customer will be loyal or not, we would err about 22 percent of the time. Is 22 percent good?</p>
<p class="indent">To answer that, consider what happens if we don’t have any feature to predict from. Then we would be forced to predict on the basis of what most customers do. What percentage of them bolt?</p>
<pre class="calibre16">&gt; <span class="codestrong">mean(telco$Churn == 'Yes')</span>
[1] 0.2653699</pre>
<p class="noindent">This says about 27 percent of the customers leave. We will often do computations of this form, so let’s review how it works. The expression</p>
<pre class="calibre16"><span class="codestrong">telco$Churn == 'Yes'</span></pre>
<p class="noindent">evaluates to a bunch of TRUEs and FALSEs. But in R, as in most computer languages, TRUE and FALSE are taken to be 1 and 0, respectively. So, we <span epub:type="pagebreak" id="page_40"/>are taking the mean of a bunch of 1s and 0s, giving us the proportion of 1s. That’s the proportion of <code>Yes</code> entries.</p>
<p class="indent">So, without customer information, we would simply predict everyone to stay—and we would be wrong 27 percent of the time. In other words, using customer information reduces our error rate from 27 to 22 percent—helpful, yes, but not dramatically so. Of course, we might do better with other values of <em class="calibre13">k</em>, and the reader is urged to try some, but it does put our analysis in perspective.</p>
<p class="indent">As you can see, a seemingly “good” error rate may be little or no better than random chance. Always remember to check the unconditional class probabilities—that is, the ones computed without <em class="calibre13">X</em>.</p>
<p class="indent">Let’s consider the example in <a href="ch02.xhtml#ch02lev3" class="calibre12">Section 2.3</a> in this regard. We achieved an error rate of about 26 percent. If we didn’t use the features, guessing instead the most prevalent class overall, would our error rate increase?</p>
<p class="indent">To that end, let’s see what proportion each class has, ignoring our six features. We can answer this question easily.</p>
<pre class="calibre16">&gt; <span class="codestrong">table(vert$V7) / nrow(vert)</span>
       DH        NO        SL
0.1935484 0.3225806 0.4838710</pre>
<p class="noindent">The call to <code>table()</code> gives us the category counts, so dividing by the total number of counts gives us the proportions.</p>
<p class="indent">If we did not use the features, we’d always guess the SL class, as it is the most common. We would then be wrong a proportion of 1 − 0.4838710 = 0.516129 of the time, which is much worse than the 26 percent error rate we attained using the features. Using the features greatly enhances our predictive ability in this case.</p>
<p class="indent">Actually, the <code>qe*</code>-series functions in <code>regtools</code> compute the featureless error rate for us. In the vertebrae example above:</p>
<pre class="calibre16">&gt; <span class="codestrong">kout$baseAcc</span>
[1] 0.5089606</pre>
<p class="noindent">Note that the result is a little different from the earlier figure of 0.516129. This is because the latter was computed on the full dataset, while this one was computed with holdout: the mean came from the training set while the <em class="calibre13">Y</em> values were from the holdout set.</p>
<p class="indent">As seen here, in classification settings, <code>baseAcc</code> will show the overall misclassification rate if one does not use the features. Of course, the same comparison—error rates using the features and not—is of interest in numeric <em class="calibre13">Y</em> settings. Therefore, if we were to ignore the features, our guess for a new <em class="calibre13">Y</em> would be the overall mean value of <em class="calibre13">Y</em> in the training set, which is analogous to our using the conditional mean if we do use the features. Without the features, the analog of MAPE is then the mean absolute difference between the overall mean <em class="calibre13">Y</em> and the actual <em class="calibre13">Y</em> in the test set. This is reported in <code>baseAcc</code>.</p>
<p class="indent"><span epub:type="pagebreak" id="page_41"/>As an example, consider again the bike ridership data from <a href="ch01.xhtml" class="calibre12">Chapter 1.</a></p>
<pre class="calibre16">&gt; <span class="codestrong">data(day1)</span>
&gt; <span class="codestrong">day1 &lt;- day1[,c(8,10:13,16)]</span>  # extract the desired features
&gt; <span class="codestrong">set.seed(9999)</span>
&gt; <span class="codestrong">knnout &lt;- qeKNN(day1,'tot',k=5)</span>  # fit the k-NN model
holdout set has  73 rows
&gt; <span class="codestrong">knnout$testAcc</span>
[1] 1203.644
&gt; <span class="codestrong">knnout$baseAcc</span>
[1] 1784.578</pre>
<p class="noindent">Here, using our chosen set of 5 predictors, MAPE was about 1,204, versus 1,785 using no predictors.</p>
<h3 class="h2" id="ch02lev5">2.5 The Confusion Matrix</h3>
<p class="noindent">In multiclass problems, the overall error rate is only the start of the story. We might also calculate the (unfortunately named) <em class="calibre13">confusion matrix</em>, which computes per-class error rates. Let’s see how this plays out in the vertebrae data.</p>
<p class="indent">The <code>qe*</code>-series functions include the confusion matrix in the return value.</p>
<pre class="calibre16">&gt; <span class="codestrong">kout$confusion</span>
      pred
actual DH NO SL
    DH  6  2  0
    NO  2  6  2
    SL  1  1 11</pre>
<p class="noindent">Of the 6 + 2 + 0 = 8 data points with actual class DH, 6 were correctly classified as DH, but 2 were misclassified as NO, though none were wrongly predicted as SL.</p>
<p class="indent">This type of analysis enables a more finely detailed assessment of our predictive power. It’s quite frequently used by MLers to identify potential areas of weakness of one’s model.</p>
<h3 class="h2" id="ch02lev6">2.6 Clearing the Confusion: Unbalanced Data</h3>
<p class="noindent">Here, we will discuss issues regarding <em class="calibre13">unbalanced data</em>, a common situation in classification problems that is much discussed in ML circles.</p>
<p class="indent">Recall that in our customer churn example earlier in this chapter, about 73 percent of the customers were “loyal,” while 27 percent moved to another telco. With the 7,032 cases in our data, those figures translate to 5,141 loyal cases and 1,901 cases of churn. The loyal cases outnumber the churn ones by a ratio of more than 2.5 to 1. Often, this ratio can be 100 to 1 or even more. Such a situation is termed <em class="calibre13">unbalanced</em>. (In this section, our discussion will mainly cover the two-class case, but multiclass cases are similar.)</p>
<p class="indent"><span epub:type="pagebreak" id="page_42"/>Many analysts recommend that if one’s dataset has unbalanced class sizes, one should modify the data to create equal class counts. Their reasoning is that application of ML methods to unbalanced data will result in almost all predictions being that we guess the class of a new data point to be the dominant class—for instance, we always guess No Churn rather than Churn in the telco data. That is not very informative!</p>
<p class="indent">Illustrations of the problem and offered remedies appear in numerous parts of the ML literature, ranging from web tutorials<sup class="calibre11"><a id="ch2fn2b" class="calibre12"/><a href="footnote.xhtml#ch2fn2" class="calibre12">2</a></sup> to major CRAN packages, such as <code>caret</code>, <code>parsnp</code>, and <code>mlr3</code>. In spite of warnings by statisticians,<sup class="calibre11"><a id="ch2fn3b" class="calibre12"/><a href="footnote.xhtml#ch2fn3" class="calibre12">3</a></sup> all of these sources recommend that you artificially equalize the class counts in your data, say, by discarding “excess” data from the dominant class.</p>
<p class="indent">While it is true that unbalanced data will result in always, or almost always, predicting the dominant class, remedying by artificially equalizing the class sizes is unwarranted and, in many cases, harmful. Clearly, discarding data is generally not a good idea; it will always weaken one’s analysis. Moreover, depending on the goals of the given application, it may actually be desirable to always guess the dominant class.</p>
<h4 class="h3" id="ch02lev6sec1"><em class="calibre22"><strong class="calibre3">2.6.1 Example: The Kaggle Appointments Dataset</strong></em></h4>
<p class="noindent">To illustrate how best to deal with unbalanced data, let’s look at a dataset from Kaggle,<sup class="calibre11"><a id="ch2fn4b" class="calibre12"/><a href="footnote.xhtml#ch2fn4" class="calibre12">4</a></sup> a firm with the curious business model of operating data science competitions. The goal is to use this dataset to predict whether a patient will fail to keep a doctor’s appointment; if the medical office can flag the patients at risk of not showing up, staff can make extra efforts to avoid the economic losses resulting from no-shows.</p>
<p class="indent">Read in the data:</p>
<pre class="calibre16">&gt; <span class="codestrong">ma &lt;- read.csv('KaggleV2-May-2016.csv',header=TRUE,</span>
   <span class="codestrong">stringsAsFactors=TRUE)</span>
&gt; <span class="codestrong">names(ma)</span>
 [1] "PatientId"      "AppointmentID"  "Gender"         "ScheduledDay"
 [5] "AppointmentDay" "Age"            "Neighbourhood"  "Scholarship"
 [9] "Hypertension"   "Diabetes"       "Alcoholism"     "Handicap"
[13] "SMS_received"   "No.show"
&gt; <span class="codestrong">nrow(ma)</span>
[1] 110527</pre>
<p class="indent">Should we remove the patient ID, as we did in the telco data, to avoid overfitting? We see that on average, each patient appears less than twice in the data, meaning there is not much data per patient:</p>
<pre class="calibre16">&gt; <span class="codestrong">length(unique(ma$PatientId))</span>
[1] 62299</pre>
<p class="indent"><span epub:type="pagebreak" id="page_43"/>So, yes, we probably should not include this feature. Using the same reasoning, it makes sense to remove the appointment ID, neighborhood, appointment day, and scheduled day variables:</p>
<pre class="calibre16">&gt; <span class="codestrong">ma &lt;- ma[,-c(1,2,4,5,7)]</span>
&gt; <span class="codestrong">names(ma)</span>
[1] "Gender"       "Age"           "Scholarship"  "Hypertension" "Diabetes"
[6] "Alcoholism"   "Handicap"      "SMS_received" "No.show"</pre>
<p class="indent">About 20 percent of cases are no-shows (counterintuitively, <code>Yes</code> here means “Yes, the patient didn’t show up”):</p>
<pre class="calibre16">&gt; <span class="codestrong">table(ma$No.show)</span>
   No   Yes
88208 22319</pre>
<p class="noindent">Yes, it is indeed unbalanced data.</p>
<p class="indent">Recall that the concern is that our predicted <em class="calibre13">Y</em>s will also be unbalanced—that is, most or all will predict the patient to show up. Let’s check this by inspecting the output of running <code>qeKNN()</code>. Here is the call:</p>
<pre class="calibre16">&gt; <span class="codestrong">kout &lt;- qeKNN(ma,'No.show',25)</span></pre>
<p class="indent">There is a lot of information in most <code>qe*</code>-series function return values. Here are the components of the <code>qeKNN()</code> function output:</p>
<pre class="calibre16">&gt; <span class="codestrong">names(kout)</span>
 [1] "whichClosest"   "regests"        "scaleX"         "classif"
 [5] "xminmax"        "mhdists"        "x"              "y"
 [9] "noPreds"        "leave1out"      "startAt1adjust" "classNames"
[13] "factorsInfo"    "trainRow1"      "holdoutPreds"   "testAcc"
[17] "baseAcc"        "confusion"      "holdIdxs"</pre>
<p class="noindent">That <code>holdoutPreds</code> component is actually the return value of <code>regtools::kNN()</code>, discussed in <a href="ch01.xhtml#ch01lev17" class="calibre12">Section 1.17</a>. (The R notation <code>p::e</code> means the entity <code>e</code> in the package <code>p</code>.) Let’s see what is in there:</p>
<pre class="calibre16">&gt; <span class="codestrong">names(kout$holdoutPreds)</span>
[1] "predClasses" "probs"</pre>
<p class="noindent">Checking the documentation, we find that <code>predClasses</code> is the vector of predicted <em class="calibre13">Y</em>s in the holdout set, which is just what we need. Let’s tabulate them:</p>
<pre class="calibre16">&gt; <span class="codestrong">predY &lt;- kout$holdoutPreds$predClasses</span>
&gt; <span class="codestrong">table(predY)</span>
predY
 No Yes
990  10</pre>
<p class="noindent"><span epub:type="pagebreak" id="page_44"/>First, how do we deal with <em class="calibre13">two</em> dollar signs (<code>$</code>) in this expression?</p>
<pre class="calibre16">kout$holdoutPreds$predClasses</pre>
<p class="noindent">Remember, the notation <code>u$v</code> means component <code>v</code> within the object <code>u</code>. So, <code>u$v$w</code> means the component <em class="calibre13">w</em> within the object <code>u$v</code> ! So yes, we have an R list within an R list here, which is common in the R world.</p>
<p class="indent">At any rate, we see that with our model here, we predict in the vast majority of cases that the patient will show up (again, Yes means a no-show), confirming the concern many people have about unbalanced data.</p>
<p class="indent">In fact, the predictions are even more unbalanced than the data itself; we saw about 20 percent of the <em class="calibre13">Y</em>s in the overall dataset were no-shows, while here that is true for only 1.3 percent of the predicted holdout <em class="calibre13">Y</em>s. Actually, this is to be expected: remember, if the model finds the probability of breaking the appointment to be greater than 0.5, the prediction will be that they don’t show up, even if they actually did.</p>
<p class="indent">This is exactly the problem cited by the sources mentioned earlier who recommend artificially balancing the data. They recommend altering the data in a manner that makes all classes represented equally in the data. They suggest one of the following remedies (or variants) to equalize the class sizes:</p>
<p class="noindent1"><strong class="calibre5">Downsample</strong></p>
<p class="block1">Replace the No cases in our data with 22,319 randomly chosen elements from the original 88,208. We then will have 22,319 Yes records and 22,139 No records, thus achieving balance.</p>
<p class="noindent1"><strong class="calibre5">Upsample</strong></p>
<p class="block1">Replace the Yes cases with 88,208 randomly selected elements from the original 22,319 (with replacement). We then will have 88,208 Yes records and 88,208 No records, thus achieving balance.</p>
<p class="noindent">One would apply one’s chosen ML method, say, k-NN, to the modified data, and then predict new cases to be no-shows according to whether the estimated conditional probability is larger than 0.5 or not.</p>
<h4 class="h3" id="ch02lev6sec2"><em class="calibre22"><strong class="calibre3">2.6.2 A Better Approach to Unbalanced Data</strong></em></h4>
<p class="noindent">Again, downsampling is undesirable; data is precious and shouldn’t be discarded. The other approach to balancing, upsampling, doesn’t make sense either—why would adding fully duplicate data help?</p>
<p class="indent">In addition, balancing assumes equal adverse impact from false negatives and false positives, which is unlikely in applications like the appointments data. One could set up formal utility values here for the relative costs of false negatives and false positives. But in many applications, we need more flexibility than what a fully mechanical algorithm gives us. For instance, consider credit card fraud. As noted in global accounting network PricewaterhouseCoopers’s publication <em class="calibre13">Fraud: A Guide to Its Prevention,</em> <span epub:type="pagebreak" id="page_45"/><em class="calibre13">Detection and Investigation</em>, “Every fraud incident is different, and reactive responses will vary depending on the facts that are unique to each case.”<sup class="calibre11"><a id="ch2fn5b" class="calibre12"/><a href="footnote.xhtml#ch2fn5" class="calibre12">5</a></sup></p>
<p class="indent">The easier and better solution is to simply have our ML algorithm flag the cases in which there is a substantial probability of fraud, say, above some specified threshold, then take it “by hand” from there. Once the algorithm has selected a set of possible instances of fraud, the (human) auditor will take into account that estimated probability—now worrying not only that it is larger than the threshold but also <em class="calibre13">how much larger</em>—as well as such factors as the amount of the charge, special characteristics not measured in the available features, and so on.</p>
<p class="indent">The auditor may not give priority, for instance, to a case in which the probability is above the threshold but in which the monetary value of the transaction is small. On the other hand, the auditor may give this transaction a closer look, even if the monetary value is small, if the probability is much higher than the threshold value.</p>
<p class="indent">Thus, the practical solution to the unbalanced-data “problem” is not to artificially resample the data but instead to identify individual cases of interest in the context of the application. This then means cases of sufficiently high probability to be of concern, again in the context of the given application.</p>
<p class="indent">Recall that the <code>probs</code> component of a <code>qe*</code>-series call in a classification application gives the estimated probabilities of the various classes—exactly what we need. For instance, in the missed-appointments dataset, we can check <code>probs</code> to find the probability a patient will be a no-show.</p>
<p class="indent">Note that <code>probs</code> has one row for each case to be predicted. Since the missed-appointments data has two classes, there will be two columns. We saw previously that the Yes class (that is, missed appointments) is listed second. Let’s take a look.</p>
<pre class="calibre16">&gt; <span class="codestrong">preds &lt;- predict(kout,ma[,-9]) # exclude "Y", column 9</span>
&gt; <span class="codestrong">table(preds$probs)</span>
    0  0.04  0.08  0.12  0.16   0.2  0.24  0.28
  604  6283  9959 16465 17713 18427 12641  8943
 0.32  0.36   0.4  0.44  0.48  0.52  0.56   0.6
 7081  5293  3306  2123  1038   298   154   121
 0.64  0.68  0.72
   27    25    26</pre>
<p class="noindent">There are quite a few patients who, despite being more likely than not to keep the appointment, still have a substantial risk of no-show. For instance, 18,427 people have a 0.2 estimated probability of failing to keep their <span epub:type="pagebreak" id="page_46"/>appointment. An additional 28,435 patients have more than a 25 percent chance of not showing up:</p>
<pre class="calibre16">&gt; <span class="codestrong">sum(preds$probs &gt; 0.75)</span>
[1] 80451</pre>
<p class="indent">The reasonable approach would be to decide on a threshold for no-show probability, then determine which patients fail to meet that threshold. With a threshold of 0.75, for instance, any patient whose probability of keeping the appointment is greater than that level might be given special attention. We would make extra phone calls to them, explain penalties for missed appointments, and so on.</p>
<p class="indent">If those calls prove too burdensome, we can increase the threshold. Or, if the calls turn out to be highly effective, we can reduce it. Either way, the point is that the power is now in the hands of the end user of the data, where it ought to be. Artificially balancing the data denies the user that power.</p>
<h3 class="h2" id="ch02lev7">2.7 Receiver Operating Characteristic and Area Under Curve</h3>
<p class="noindent">We have seen MAPE used as a measure of predictive power in numeric-<em class="calibre13">Y</em>  problems, while overall misclassification error (OME) is used in classification applications. Both are very popular, but in the classification case, there are other common measures, two of which we will discuss now.</p>
<h4 class="h3" id="ch02lev7sec1"><em class="calibre22"><strong class="calibre3">2.7.1 Details of ROC and AUC</strong></em></h4>
<p class="noindent">Many analysts use the <em class="calibre13">Area Under Curve (AUC)</em> value as an overall measure of predictive power in classification problems. The curve under consideration is the <em class="calibre13">Receiver Operating Characteristic (ROC)</em> curve.</p>
<p class="indent">To understand ROC, recall <a href="ch02.xhtml#ch02lev6sec2" class="calibre12">Section 2.6.2</a>, where we spoke of a threshold for class prediction. If the estimated class probability is on one side of the threshold, we predict Class 1, and on the other side, we predict Class 0. Threshold values can be anywhere from 0 to 1; we choose our value based on our goals in the particular application.</p>
<p class="indent">The ROC curve then explores various scenarios. How well would we predict if, say, we take the threshold to be 0.4? What about 0.7? And so on? The question “How well would we predict?” is addressed by two numbers: the <em class="calibre13">true positive rate (TPR)</em> and the <em class="calibre13">false positive rate (FPR)</em>.</p>
<p class="indent">TPR, also known as the <em class="calibre13">sensitivity</em>, is the probability that we guess Class 1, given that the class actually is Class 1. FPR, the <em class="calibre13">specificity</em>, is the probability that we guess Class 1, given that the true class is Class 0. Note that the threshold value determines FPR and TPR. The ROC curve then plots TPR against FPR as the threshold varies.</p>
<p class="indent">The AUC is then the total area under the ROC curve. It takes on values between 0 and 1. The higher the curve, the better, as it implies that for any fixed FPR, TPR is high. Thus, the closer AUC is to 1.0, the better the predictive ability.<span epub:type="pagebreak" id="page_47"/></p>
<h4 class="h3" id="ch02lev7sec2"><em class="calibre22"><strong class="calibre3">2.7.2 The qeROC() Function</strong></em></h4>
<p class="noindent">The <code>qeROC()</code> function wraps <code>roc()</code> in the <code>pROC</code> package. It performs ROC analysis on the output of a <code>qe*-</code> function, say, <code>qeKNN()</code>, using the call form:</p>
<pre class="calibre16">qeROC(dataIn, qeOut, yName, yLevelName)</pre>
<p class="noindent">Here, <code>dataIn</code> is the dataset on which the <code>qe*-</code> function had been called, <code>qeOut</code> is the output of that function, <code>yName</code> is the name of <em class="calibre13">Y</em> in <code>dataIn</code>, and <code>yLevelName</code> is the <em class="calibre13">Y</em> level (in the R factor sense) of interest. Note that the latter allows for the multiclass case.</p>
<p class="indent">As noted, <code>qeROC()</code> calls <code>pPROC::roc()</code>. The return value of the former consists of the return value of the latter. It may be useful to assign the return value to a variable for further use (see below), but if this is not done, the ROC curve is plotted and the AUC value is printed out.</p>
<p class="indent">Note that <code>qeROC()</code> operates on the holdout set. This is important for the same reasons that the <code>testAcc</code> output of the <code>qe*</code>-series functions use the holdout set (see <a href="ch01.xhtml#ch01lev12" class="calibre12">Section 1.12</a>).</p>
<h4 class="h3" id="ch02lev7sec3"><em class="calibre22"><strong class="calibre3">2.7.3 Example: Telco Churn Data</strong></em></h4>
<p class="noindent">Let’s see what the ROC curve has to say about the Telco Churn data, continuing our earlier k-NN analysis.</p>
<pre class="calibre16">&gt; <span class="codestrong">w &lt;- qeROC(tc,knnout,'No')</span>
...
&gt; <span class="codestrong">w$auc</span>
Area under the curve: 0.8165</pre>
<p class="noindent">The plot is shown in <a href="ch02.xhtml#ch02fig01" class="calibre12">Figure 2-1</a>. Let’s see how to interpret it.</p>
<div class="image"><img alt="Image" id="ch02fig01" src="../images/ch02fig01.jpg" class="calibre23"/></div>
<p class="figcap"><em class="calibre13">Figure 2-1: ROC, Telco Churn data</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_48"/>The 45-degree line is drawn to represent pure guessing, with our rate of prediction of a positive class being the same, regardless of whether we have a true positive or not. Again, the further the ROC curve is above this line, the better.</p>
<h4 class="h3" id="ch02lev7sec4"><em class="calibre22"><strong class="calibre3">2.7.4 Example: Vertebrae Data</strong></em></h4>
<p class="noindent">The <code>qeROC()</code> function can also be used in multiclass settings:</p>
<pre class="calibre16">&gt; <span class="codestrong">qeROC(vert,kout,'V7','DH')</span>
Area under the curve: 0.9181
&gt; <span class="codestrong">qeROC(vert,kout,'V7','NO')</span>
Area under the curve: 0.7985
&gt; <span class="codestrong">qeROC(vert,kout,'V7','SL')</span>
Area under the curve: 0.8706</pre>
<p class="indent">Since this is a small dataset, we opted for larger holdout (under the 10 percent default, the holdout would only have 31 cases). But even then, one must keep in mind that those AUC values are subject to considerable sample variation. Thus we must be cautious in concluding that we are less accurate in predicting the <code>'NO'</code> cases.</p>
<p class="indent">Anyway, what do these numbers mean in this multiclass context? Actually, they are the same ones one would get by running three individual ROC analyses (on the same holdout set); <code>qeROC()</code> is just a convenience function in this case, alleviating the user of the need to run <code>roc()</code> three times. And since no new computation is done (the class probabilities are scaled to sum to 1.0), this also saves the user computation time if the dataset is large.</p>
<h4 class="h3" id="ch02lev7sec5"><em class="calibre22"><strong class="calibre3">2.7.5 Pitfall: Overreliance on AUC</strong></em></h4>
<p class="noindent">AUC can be a useful number to complement OME, and many analysts regard it as an integral part of their ML toolkits. However, one should use it cautiously.</p>
<p class="indent">Mathematically, AUC is the average value of ROC over all possible threshold values. But recall, each threshold value corresponds implicitly to a relative utility. In a credit card fraud dataset, for instance, we may believe it is far worse to decide a transaction is legitimate when it is actually fraudulent, compared to vice versa. The problem, then, is that there are some threshold values that we would never even consider using for a particular application. Yet they are averaged into the AUC value, thus rendering the latter less meaningful.</p>
<h3 class="h2" id="ch02lev8"><span epub:type="pagebreak" id="page_49" class="calibre2"/>2.8 Conclusions</h3>
<p class="noindent">We now have a solid foundation in the two basic types of ML problems: numeric-<em class="calibre13">Y</em> and classification. Along the way, we’ve picked up some miscellaneous skills, such as removing useless features and dealing with NA values.</p>
<p class="indent">It’s now time to take a serious look at how to choose the value of <em class="calibre13">k</em> and, more generally, hyperparameters in all ML methods, which is a topic that we have been glossing over so far. We will look at this in detail in the next two chapters.<span epub:type="pagebreak" id="page_50"/></p>
</div></body></html>