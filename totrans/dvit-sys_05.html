<html><head></head><body>
<h2 class="h2" id="ch05"><span epub:type="pagebreak" id="page_231"/><span class="big">5</span><br/>WHAT VON NEUMANN KNEW: COMPUTER ARCHITECTURE</h2>&#13;
<div class="imagec"><img alt="image" src="../images/common.jpg"/></div>&#13;
<p class="noindents">The term <em>computer architecture</em> may refer to the entire hardware level of the computer. However, it is often used to refer to the design and implementation of the digital processor part of the computer hardware, and we focus on the computer processor architecture in this chapter.</p>&#13;
<p class="indent">The <em>central processing unit</em> (CPU, or processor) is the part of the computer that executes program instructions on program data. Program instructions and data are stored in the computer’s random access memory (RAM). A particular digital processor implements a specific <em>instruction set architecture</em> (ISA), which defines the set of instructions and their binary encoding, the set of CPU registers, and the effects of executing instructions on the state of the processor. There are many different ISAs, including SPARC, IA32, MIPS, ARM, ARC, PowerPC, and x86 (the latter including IA32 and x86-64). A <em>microarchitecture</em> defines the circuitry of an implementation of a specific ISA. Microarchitecture implementations of the same ISA can differ as long as they implement the ISA definition. For example, Intel and AMD produce different microprocessor implementations of IA32 ISA.</p>&#13;
<p class="indent">Some ISAs define a <em>reduced instruction set computer</em> (RISC), and others define a <em>complex instruction set computer</em> (CISC). RISC ISAs have a small set <span epub:type="pagebreak" id="page_232"/>of basic instructions that each execute quickly; each instruction executes in about a single processor clock cycle, and compilers combine sequences of several basic RISC instructions to implement higher-level functionality. In contrast, a CISC ISA’s instructions provide higher-level functionality than RISC instructions. CISC architectures also define a larger set of instructions than RISC, support more complicated addressing modes (ways to express the memory locations of program data), and support variable-length instructions. A single CISC instruction may perform a sequence of low-level functionality and may take several processor clock cycles to execute. This same functionality would require multiple instructions on a RISC architecture.</p>&#13;
<div class="g-box">&#13;
<p class="box-title">THE HISTORY OF RISC VERSUS CISC</p>&#13;
<p class="noindentt">In the early 1980s, researchers at Berkeley and Stanford universities developed RISC through the Berkeley RISC project and the Stanford MIPS project. David Paterson of Berkeley and John Hennessy of Stanford won the 2017 Turing Award<sup><a href="ch05.xhtml#fn5_1" id="rfn5_1">1</a></sup> (the highest award in computing) for their work developing RISC architectures.</p>&#13;
<p class="noindentt">At the time of its development, the RISC architecture was a radical departure from the commonly held view that ISAs needed to be increasingly complex to achieve high performance. “The RISC approach differed from the prevailing complex instruction set computer (CISC) computers of the time in that it required a small set of simple and general instructions (functions a computer must perform), requiring fewer transistors than complex instruction sets and reducing the amount of work a computer must perform.”<sup><a href="ch05.xhtml#fn5_2" id="rfn5_2">2</a></sup></p>&#13;
<p class="noindentt">CISC ISAs express programs in fewer instructions than RISC, often resulting in smaller program executables. On systems with small main memory, the size of the program executable is an important factor in the program’s performance, since a large executable leaves less RAM space available for other parts of a running program’s memory space. Microarchitectures based on CISC are also typically specialized to efficiently execute the CISC variable-length and higher- functionality instructions. Specialized circuitry for executing more complex instructions may result in more efficient execution of specific higher-level functionality, but at the cost of requiring more complexity for all instruction execution.</p>&#13;
<p class="noindentt">In comparing RISC to CISC, RISC programs contain more total instructions to execute, but each instruction executes much more efficiently than most CISC instructions, and RISC allows for simpler microarchitecture designs than CISC. CISC programs contain fewer instructions, and CISC microarchitectures are designed to execute more complicated instructions efficiently, but they require more complex microarchitecture designs and faster clock rates. In general, RISC processors result in more efficient design and better performance. As computer memory sizes have increased over time, the size of the program executable is less important to a program’s performance. CISC, however, has been the dominant ISA due in large part to it being implemented by and supported by industry.</p>&#13;
<p class="noindentt"><span epub:type="pagebreak" id="page_233"/>Today, CISC remains the dominant ISA for desktop and many server-class computers. For example, Intel’s x86 ISAs are CISC-based. RISC ISAs are more commonly seen in high-end servers (e.g., SPARC) and in mobile devices (e.g., ARM) due to their low power requirements. A particular microarchitecture implementation of a RISC or CISC ISA may incorporate both RISC and CISC design under the covers. For example, most CISC processors use microcode to encode some CISC instructions in a more RISC-like instruction set that the underlying processor executes, and some modern RISC instruction sets contain a few more complex instructions or addressing modes than the initial MIPS and Berkeley RISC instruction sets.</p>&#13;
</div>&#13;
<p class="indent">All modern processors, regardless of their ISA, adhere to the von Neumann architecture model. The general-purpose design of the von Neumann architecture allows it to execute any type of program. It uses a stored-program model, meaning that the program instructions reside in computer memory along with program data, and both are inputs to the processor.</p>&#13;
<p class="indent">This chapter introduces the von Neumann architecture and the ancestry and components that underpin modern computer architecture. We build an example digital processor (CPU) based on the von Neumann architecture model, design a CPU from digital circuits that are constructed from logic gate building blocks, and demonstrate how the CPU executes program instructions.</p>&#13;
<h3 class="h3" id="lev1_38">5.1 The Origin of Modern Computing Architectures</h3>&#13;
<p class="noindent">When tracing the ancestry of modern computing architecture, it is tempting to consider that modern computers are part of a linear chain of successive transmutations, with each machine simply an improvement of the one that previously existed. While this view of inherited improvements in computer design may hold true for certain classes of architecture (consider the iterative improvements of the iPhone X from the original iPhone), the root of the architectural tree is much less defined.</p>&#13;
<p class="indent">From the 1700s until the early 1900s, mathematicians served as the first <em>human</em> computers for calculations related to applications of science and engineering.<sup><a href="ch05.xhtml#fn5_3" id="rfn5_3">3</a></sup> The word “computer” originally referred to “one who computes.” Women mathematicians often served in the role of computer. In fact, the use of women as human computers was so pervasive that computational complexity was measured in “kilo-girls,” or the amount of work a thousand human computers could complete in one hour.<sup><a href="ch05.xhtml#fn5_4" id="rfn5_4">4</a></sup> Women were widely considered to be better at doing mathematical calculations than men, as they tended to be more methodical. Women were not allowed to hold the position of engineer. As such, they were relegated to more “menial” work, such as computing complex calculations.</p>&#13;
<p class="indent">The first general-purpose digital computer, the <em>Analytical Engine</em>, was designed by British mathematician Charles Babbage, who is credited by some as the father of the computer. The Analytical Engine was an extension of his original invention, the Difference Engine, a mechanical calculator that <span epub:type="pagebreak" id="page_234"/>was capable of calculating polynomial functions. Ada Lovelace, who perhaps should be known as the mother of computing, was the very first person to develop a computer program and the first to publish an algorithm that could be computed using Charles Babbage’s Analytical Engine. In her notes is included her recognition of the general-purpose nature of the Analytical Engine: “[t]he Analytical Engine has no pretensions whatever to originate anything. It can do whatever we know how to order it to perform.”<sup><a href="ch05.xhtml#fn5_5" id="rfn5_5">5</a></sup> However, unlike modern computers, the Analytical Engine was a mechanical device and was only partially built. Most of the designers of what became the direct forerunners to the modern computer were unaware of the work of Babbage and Lovelace when they developed their own machines.</p>&#13;
<p class="indent">Thus, it is perhaps more accurate to think about modern computer architecture rising out of a primordial soup of ideas and innovations that arose in the 1930s and 1940s. For example, in 1937, Claude Shannon, a student at MIT, wrote what would go on to be perhaps the most influential masters thesis of all time. Drawing upon the work of George Boole (the mathematician who developed Boolean algebra), Shannon showed that Boolean logic could be applied to circuits and could be used to develop electrical switches. This would lead to the development of the binary computing system, and much of future digital circuit design. While men would design many early electronic computers, women (who were not allowed to be engineers) became programming pioneers, leading the design and development of many early software innovations, such as programming languages, compilers, algorithms, and operating systems.</p>&#13;
<p class="indent">A comprehensive discussion of the rise of computer architecture is not possible in this book (see elsewhere<sup><a href="ch05.xhtml#fn5_6" id="rfn5_6">6</a>,<a href="ch05.xhtml#fn5_7" id="rfn5_7">7</a></sup> for details); however, we briefly enumerate several significant innovations that occurred in the 1930s and 1940s that were instrumental in the rise of modern computer architecture.</p>&#13;
<h4 class="h4" id="lev2_85">5.1.1 The Turing Machine</h4>&#13;
<p class="noindent">In 1937, British mathematician Alan Turing proposed<sup><a href="ch05.xhtml#fn5_8" id="rfn5_8">8</a></sup> the “Logical Computing Machine,” a theoretical computer. Turing used this machine to prove that there exists no solution to the decision problem (in German, the <em>Entscheidungsproblem</em>), posed by the mathematicians David Hilbert and Wilhelm Ackermann in 1928. The decision problem is an algorithm that takes a statement as input and determines whether the statement is universally valid. Turing proved that no such algorithm exists by showing that the <em>halting problem</em> (will machine <em>X</em> halt on input <em>y</em>?) was undecidable for Turing’s machine. As part of this proof, Turing described a universal machine that is capable of performing the tasks of any other computing machine. Alonzo Church, Turing’s dissertation advisor at Princeton University, was the first to refer to the <em>logical computing machine</em> as the <em>Turing machine</em>, and its universal form as the <em>universal Turing machine</em>.</p>&#13;
<p class="indent">Turing later returned to England and served his country as part of the code breaking unit in Bletchley Park during World War II. He was instrumental in the design and construction of the <em>Bombe</em>, an electromechanical <span epub:type="pagebreak" id="page_235"/>device that helped break the cipher produced by the Enigma machine, which was commonly used by Nazi Germany to protect sensitive communication during World War II.</p>&#13;
<p class="indent">After the war, Turing designed the <em>automatic computing engine</em> (ACE). The ACE was a stored-program computer, meaning that both the program instructions and its data are loaded into the computer memory and run by the general-purpose computer. His paper, published in 1946, is perhaps the most detailed description of such a computer.<sup><a href="ch05.xhtml#fn5_9" id="rfn5_9">9</a></sup></p>&#13;
<h4 class="h4" id="lev2_86">5.1.2 Early Electronic Computers</h4>&#13;
<p class="noindent">World War II accelerated much of the development of early computers. However, due to the classified nature of military operations in World War II, many of the details of innovations that occurred as a result of the frenetic activity during the war was not publicly acknowledged until years later. A good example of this is Colossus, a machine designed by British engineer Tommy Flowers to help break the Lorenz cipher, which was used by Nazi Germany to encode high-level intelligence communication. Some of Alan Turing’s work aided in its design. Built in 1943, Colossus is arguably the first programmable, digital, and fully electronic computer. However, it was a special-purpose computer, designed specifically for code breaking. The Women’s Royal Naval Service (WRNS, known as the “Wrens”) served as operators of Colossus. In spite of the <em>General Report on Tunny</em><sup><a href="ch05.xhtml#fn5_10" id="rfn5_10">10</a></sup> noting that several of the Wrens showed ability in cryptographic work, none of them were given the position of cryptographer, and instead were delegated to more menial Colossus operation tasks.<sup><a href="ch05.xhtml#fn5_11" id="rfn5_11">11</a>,<a href="ch05.xhtml#fn5_12" id="rfn5_12">12</a></sup></p>&#13;
<p class="indent">On the other side of the Atlantic, American scientists and engineers were hard at work creating computers of their own. Harvard professor Howard Aiken (who was also a Naval Commander in the US Navy Reserves) designed the Mark I, an electromechanical, general-purpose programmable computer. Built in 1944, it aided in the design of the atomic bomb. Aiken built his computer largely unaware of Turing’s work and was motivated by the goal of bringing Charles Babbage’s analytical engine to life.<sup><a href="ch05.xhtml#fn5_13" id="rfn5_13">13</a></sup> A key feature of the Mark I was that it was fully automatic and able to run for days without human intervention. This would be a foundational feature in future computer design.</p>&#13;
<p class="indent">Meanwhile, American engineers John Mauchly and Presper Eckert of the University of Pennsylvania designed and built the <em>Electronic Numerical Integrator and Computer</em> (ENIAC) in 1945. ENIAC is arguably the forerunner of modern computers. It was digital (though it used decimal rather than binary), fully electronic, programmable, and general purpose. While the original version of ENIAC did not have stored-program capabilities, this feature was built into it before the end of the decade. ENIAC was financed and built for the US Army’s Ballistic Research Laboratory and was designed primarily to calculate ballistic trajectories. Later, it would be used to aid in the design of the hydrogen bomb.</p>&#13;
<p class="indent">As men were drafted into the armed forces during World War II, women were hired to help in the war effort as human computers. With the arrival <span epub:type="pagebreak" id="page_236"/>of the first electronic computers, women became the first programmers, as programming was considered secretarial work. It should come as no surprise that many of the early innovations in programming, such as the first compiler, the notion of modularizing programs, debugging, and assembly language, are credited to women inventors. Grace Hopper, for example, developed the first high-level and machine-independent programming language (COBOL) and its compiler. Hopper was also a programmer for the Mark I and wrote the book that described its operation.</p>&#13;
<p class="indent">The ENIAC programmers were six women: Jean Jennings Bartik, Betty Snyder Holberton, Kay McNulty Mauchly, Frances Bilas Spence, Marlyn Wescoff Meltzer, and Ruth Lichterman Teitelbaum. Unlike the Wrens, the ENIAC women were given a great deal of autonomy in their task; given just the wiring diagrams of ENIAC, they were told to figure out how it worked and how to program it. In addition to their innovation in solving how to program (and debug) one of the world’s first electronic general-purpose computers, the ENIAC programmers also developed the idea of algorithmic flow charts, and developed important programming concepts such as subroutines and nesting. Like Grace Hopper, Jean Jennings Bartik and Betty Snyder Holberton would go on to have long careers in computing, and are some of the early computing pioneers. Unfortunately, the full extent of women’s contributions in early computing is not known. Unable to advance, many women left the field after World War II. We encourage readers to learn more about early women programmers.<sup><a href="ch05.xhtml#fn5_14" id="rfn5_14">14</a>,<a href="ch05.xhtml#fn5_15" id="rfn5_15">15</a>,<a href="ch05.xhtml#fn5_16" id="rfn5_16">16</a></sup></p>&#13;
<p class="indent">The British and the Americans were not the only ones interested in the potential of computers. In Germany, Konrad Zuse developed the first electromechanical general-purpose digital programmable computer, the Z3, which was completed in 1941. Zuse came up with his design independently of the work of Turing and others. Notably, Zuse’s design used binary (rather than decimal), the first computer of its kind to use the binary system. However, the Z3 was destroyed during aerial bombing of Berlin, and Zuse was unable to continue his work until 1950. His work largely went unrecognized until years later. He is widely considered the father of computing in Germany.</p>&#13;
<h4 class="h4" id="lev2_87">5.1.3 So What Did von Neumann Know?</h4>&#13;
<p class="noindent">From our discussion of the origin of modern computer architecture, it is apparent that in the 1930s and 1940s there were several innovations that led to the rise of the computer as we know it today. In 1945, John von Neumann published a paper, “First draft of a report on the EDVAC,”<sup><a href="ch05.xhtml#fn5_17" id="rfn5_17">17</a></sup> which describes an architecture on which modern computers are based. EDVAC was the successor of ENIAC. It differed from ENIAC in that it was a binary computer instead of decimal, and it was a stored-program computer. Today, this description of EDVAC’s architectural design is known as the von Neumann architecture.</p>&#13;
<p class="indent">The <em>von Neumann architecture</em> describes a general-purpose computer, one that is designed to run any program. It also uses a stored-program model, <span epub:type="pagebreak" id="page_237"/>meaning that program instructions and data are both loaded onto the computer to run. In the von Neumann model, there is no distinction between instructions and data; both are loaded into the computer’s internal memory, and program instructions are fetched from memory and executed by the computer’s functional units that execute program instructions on program data.</p>&#13;
<p class="indent">John von Neumann’s contributions weave in and out of several of the previous stories in computing. A Hungarian mathematician, he was a professor at both the Institute of Advanced Study and Princeton University, and he served as an early mentor to Alan Turing. Later, von Neumann became a research scientist on the Manhattan Project, which led him to Howard Aiken and the Mark I; he would later serve as a consultant on the ENIAC project, and correspond regularly with Eckert and Mauchly. His famous paper describing EDVAC came from his work on the Electronic Discrete Variable Automatic Computer (EDVAC), proposed to the US Army by Eckert and Mauchly, and built at the University of Pennsylvania. EDVAC included several architectural design innovations that form the foundation of almost all modern computers: it was general purpose, used the binary numeric system, had internal memory, and was fully electric. In large part because von Neumann was the sole author of the paper,<sup><a href="ch05.xhtml#fn5_18" id="rfn5_18">18</a></sup> the architectural design the paper describes is primarily credited to von Neumann and has become known as the von Neumann architecture. It should be noted that Turing described in great detail the design of a similar machine in 1946. However, since von Neumann’s paper was published before Turing’s, von Neumann received the chief credit for these innovations.</p>&#13;
<p class="indent">Regardless of who “really” invented the von Neumann architecture, von Neumann’s own contributions should not be diminished. He was a brilliant mathematician and scientist. His contributions to mathematics range from set theory to quantum mechanics and game theory. In computing, he is also regarded as the inventor of the <em>merge sort</em> algorithm. Walter Isaacson argued that one of von Neumann’s greatest strengths lay in his ability to collaborate widely and to intuitively see the importance of novel concepts.<sup><a href="ch05.xhtml#fn5_19" id="rfn5_19">19</a></sup> A lot of the early designers of the computer worked in isolation from one another. Isaacson argues that by witnessing the slowness of the Mark I computer, von Neumann was able to intuitively realize the value of a truly electronic computer, and the need to store and modify programs in memory. It could therefore be argued that von Neumann, even more than Eckert and Mauchly, grasped and fully appreciated the power of a fully electronic stored-program computer.</p>&#13;
<h3 class="h3" id="lev1_39">5.2 The von Neumann Architecture</h3>&#13;
<p class="noindent">The von Neumann architecture serves as the foundation for most modern computers. In this section, we briefly characterize the architecture’s major components.</p>&#13;
<p class="indent">The von Neumann architecture (depicted in <a href="ch05.xhtml#ch5fig1">Figure 5-1</a>) consists of five main components.</p>&#13;
<div class="number">&#13;
<p class="number"><span epub:type="pagebreak" id="page_238"/>1. The <em>processing unit</em> executes program instructions.</p>&#13;
<p class="number">2. The <em>control unit</em> drives program instruction execution on the processing unit. Together, the processing and control units make up the CPU.</p>&#13;
<p class="number">3. The <em>memory unit</em> stores program data and instructions.</p>&#13;
<p class="number">4. The <em>input unit(s)</em> load program data and instructions on the computer and initiate program execution.</p>&#13;
<p class="number">5. The <em>output unit(s)</em> store or receive program results.</p>&#13;
</div>&#13;
<p class="indent">Buses connect the units, and are used by the units to send control and data information to one another. A <em>bus</em> is a communication channel that transfers binary values between communication endpoints (the senders and receivers of the values). For example, a data bus that connects the memory unit and the CPU could be implemented as 32 parallel wires that together transfer a four-byte value, one bit transferred on each wire. Typically, architectures have separate buses for sending data, memory addresses, and control between units. The units use the control bus to send control signals that request or notify other units of actions, the address bus to send the memory address of a read or write request to the memory unit, and the data bus to transfer data between units.</p>&#13;
<div class="imagec" id="ch5fig1"><img alt="image" src="../images/05fig01.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-1: The von Neumann architecture consists of the processing, control, memory, input, and output units. The control and processing units make up the CPU, which contains the ALU, the general-purpose CPU registers, and some special-purpose registers (IR and PC). The units are connected by buses used for data transfer and communication between the units.</em></p>&#13;
<h4 class="h4" id="lev2_88">5.2.1 The CPU</h4>&#13;
<p class="noindent">The control and processing units together implement the CPU, which is the part of the computer that executes program instructions on program data.</p>&#13;
<h4 class="h4" id="lev2_89">5.2.2 The Processing Unit</h4>&#13;
<p class="noindent">The <em>processing unit</em> of the von Neumann machine consists of two parts. The first is the <em>arithmetic/logic unit</em> (ALU), which performs mathematical operations such as addition, subtraction, and logical or, to name a few. Modern ALUs typically perform a large set of arithmetic operations. The second part <span epub:type="pagebreak" id="page_239"/>of the processing unit is a set of registers. A <em>register</em> is a small, fast unit of storage used to hold program data and the instructions that are being executed by the ALU. Crucially, there is no distinction between instructions and data in the von Neumann architecture. For all intents and purposes, instructions <em>are</em> data. Each register is therefore capable of holding one data word.</p>&#13;
<h4 class="h4" id="lev2_90">5.2.3 The Control Unit</h4>&#13;
<p class="noindent">The <em>control unit</em> drives the execution of program instructions by loading them from memory and feeding instruction operands and operations through the processing unit. The control unit also includes some storage to keep track of execution state and to determine its next action to take: the <em>program counter</em> (PC) keeps the memory address of the next instruction to execute, and the <em>instruction register</em> (IR) stores the instruction, loaded from memory, that is currently being executed.</p>&#13;
<h4 class="h4" id="lev2_91">5.2.4 The Memory Unit</h4>&#13;
<p class="noindent">Internal memory is a key innovation of the von Neumann architecture. It provides program data storage that is close to the processing unit, significantly reducing the amount of time to perform calculations. The <em>memory unit</em> stores both program data and program instructions—storing program instructions is a key part of the stored-program model of the von Neumann architecture</p>&#13;
<p class="indent">The size of memory varies from system to system. However, a system’s ISA limits the range of addresses that it can express. In modern systems, the smallest addressable unit of memory is one byte (8 bits), and thus each address corresponds to a unique memory location for one byte of storage. As a result, 32-bit architectures typically support a maximum address space size of 2<sup>32</sup>, which corresponds to 4 gigabytes (GiB) of addressable memory.</p>&#13;
<p class="indent">The term <em>memory</em> sometimes refers to an entire hierarchy of storage in the system. It can include registers in the processing unit as well as secondary storage devices like hard disk drives (HDD) or solid-state drives (SSD). In <a href="ch11.xhtml#ch11">Chapter 11</a>, we discuss the memory hierarchy in detail. For now, we use the term “memory” interchangeably with internal <em>random access memory</em> (RAM)—memory that can be accessed by the central processing unit. RAM storage is random access because all RAM storage locations (addresses) can be accessed directly. It is useful to think of RAM as a linear array of addresses, where each address corresponds to one byte of memory.</p>&#13;
<div class="g-box">&#13;
<p class="box-title">WORD SIZES THROUGH HISTORY</p>&#13;
<p class="noindentt"><em>Word size</em>, which is defined by an ISA, is the number of bits of the standard data size that a processor handles as a single unit. The standard word size has fluctuated over the years. For EDVAC, the word size was proposed at 30 bits. In the 1950s, 36-bit word sizes were common. With the innovation of the <span epub:type="pagebreak" id="page_240"/>IBM 360 in the 1960s, word sizes became more or less standardized, and started to expand from 16 bits, to 32 bits, to today’s 64 bits. If you examine the Intel architecture in more detail, you may notice the remnants of some of these old decisions, as 32-bit and 64-bit architectures were added as extensions of the original 16-bit architecture.</p>&#13;
</div>&#13;
<h4 class="h4" id="lev2_92">5.2.5 The Input and Output (I/O) Units</h4>&#13;
<p class="noindent">While the control, processing, and memory units form the foundation of the computer, the input and output units enable it to interact with the outside world. In particular, they provide mechanisms for loading a program’s instructions and data into memory, storing its data outside of memory, and displaying its results to users.</p>&#13;
<p class="indent">The <em>input unit</em> consists of the set of devices that enable a user or program to get data from the outside world into the computer. The most common forms of input devices today are the keyboard and mouse. Cameras and microphones are other examples.</p>&#13;
<p class="indent">The <em>output unit</em> consists of the set of devices that relay results of computation from the computer back to the outside world or that store results outside internal memory. For example, the monitor is a common output device. Other output devices include speakers and haptics.</p>&#13;
<p class="indent">Some modern devices, such as the touchscreen, act as both input and output, enabling users to both input and receive data from a single unified device.</p>&#13;
<p class="indent">Solid-state and hard drives are another example of devices that act as both input and output devices. These storage devices act as input devices when they store program executable files that the operating system loads into computer memory to run, and they act as output devices when they store files to which program results are written.</p>&#13;
<h4 class="h4" id="lev2_93">5.2.6 The von Neumann Machine in Action: Executing a Program</h4>&#13;
<p class="noindent">The five units that make up the von Neumann architecture work together to implement a <em>fetch–decode–execute–store</em> cycle of actions that together execute program instructions. This cycle starts with a program’s first instruction, and is repeated until the program exits:</p>&#13;
<div class="number">&#13;
<p class="number">1. The control unit <em>fetches</em> the next instruction from memory. The control unit has a special register, the program counter (PC), that contains the address of the next instruction to fetch. It places that address on the <em>address bus</em> and places a <em>read</em> command on the <em>control bus</em> to the memory unit. The memory unit then reads the bytes stored at the specified address and sends them to the control unit on the <em>data bus</em>. The instruction register (IR) stores the bytes of the instruction received from the memory unit. The control unit also <span epub:type="pagebreak" id="page_241"/>increments the PC’s value to store the address of the new next instruction to fetch.</p>&#13;
<p class="number">2. The control unit <em>decodes</em> the instruction stored in the IR. It decodes the instruction bits that encode which operation to perform and the bits that encode where the operands are located. The instruction bits are decoded based on the ISA’s definition of the encoding of its instructions. The control unit also fetches the data operand values from their locations (from CPU registers, memory, or encoded in the instruction bits), as input to the processing unit.</p>&#13;
<p class="number">3. The processing unit <em>executes</em> the instruction. The ALU performs the instruction operation on instruction data operands.</p>&#13;
<p class="number">4. The control unit <em>stores</em> the result to memory. The result of the processing unit’s execution of the instruction is stored to memory. The control unit writes the result to memory by placing the result value on the <em>data bus</em>, placing the address of the storage location on the <em>address bus</em>, and placing a <em>write</em> command on the <em>control bus</em>. When received, the memory unit writes the value to memory at the specified address.</p>&#13;
</div>&#13;
<p class="indent">The input and output units are not directly involved in the execution of program instructions. Instead, they participate in the program’s execution by loading a program’s instructions and data and by storing or displaying the results of the program’s computation.</p>&#13;
<p class="indent"><a href="ch05.xhtml#ch5fig2">Figures 5-2</a> and <a href="ch05.xhtml#ch5fig3">5-3</a> show the four phases of instruction execution by the von Neumann architecture for an example addition instruction whose operands are stored in CPU registers. In the <em>fetch</em> phase, the control unit reads the instruction at the memory address stored in the PC (1234). It sends the address on the address bus, and a READ command on the control bus. The memory unit receives the request, reads the value at address 1234, and sends it to the control unit on the data bus. The control unit places the instruction bytes in the IR register and updates the PC with the address of the next instruction (1238 in this example). In the <em>decode</em> phase, the control unit feeds bits from the instruction that specify which operation to perform to the processing unit’s ALU, and uses instruction bits that specify which registers store operands to read operand values from the processing unit’s registers into the ALU (the operand values are 3 and 4 in this example). In the <em>execute</em> phase, the ALU part of the processing unit executes the operation on the operands to produce the result (3 + 4 is 7). Finally, in the <em>store</em> phase the control unit writes the result (7) from the processing unit to the memory unit. The memory address (5678) is sent on the address bus, a WRITE command is sent on the control bus, and the data value to store (7) is sent on the data bus. The memory unit receives this request and stores 7 at memory address 5678. In this example, we assume that the memory address to store the result is encoded in the instruction bits.</p>&#13;
<span epub:type="pagebreak" id="page_242"/>&#13;
<div class="imagec" id="ch5fig2"><img alt="image" src="../images/05fig02.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-2: The <em>fetch</em> and <em>decode</em> stages of execution of the von Neumann architecture for an example addition instruction. Operand, result, and memory addresses are shown as decimal values, and memory contents are shown as binary values.</em></p>&#13;
<div class="imagec" id="ch5fig3"><img alt="image" src="../images/05fig03.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-3: The <em>execute</em> and <em>store</em> stages of execution of the von Neumann architecture for an example addition instruction. Operand, result, and memory addresses are shown as decimal values, and memory contents are shown as binary values.</em></p>&#13;
<h3 class="h3" id="lev1_40">5.3 Logic Gates</h3>&#13;
<p class="noindent"><em>Logic gates</em> are the building blocks of the digital circuitry that implements arithmetic, control, and storage functionality in a digital computer. Designing <span epub:type="pagebreak" id="page_243"/>complicated digital circuits involves employing a high degree of abstraction: a designer creates simple circuits that implement basic functionality from a small set of basic logic gates; these simple circuits, abstracted from their implementation, are used as the building blocks for creating more complicated circuits (simple circuits are combined together to create new circuits with more complicated functionality); these more complicated circuits may be further abstracted and used as a building block for creating even more complicated functionality; and so on to build complete processing, storage, and control components of a processor.</p>&#13;
<div class="g-box">&#13;
<p class="box-title">TRANSISTORS</p>&#13;
<p class="noindentt">Logic gates are created from transistors that are etched into a semiconductor material (e.g., silicon chips). Transistors act as switches that control electrical flow through the chip. A transistor can switch its state between on or off (between a high or low voltage output). Its output state depends on its current state plus its input state (high or low voltage). Binary values are encoded with these high (1) and low (0) voltages, and logic gates are implemented by arrangements of a few transistors that perform switching actions on the inputs to produce the logic gate’s output. The number of transistors that can fit on an integrated circuit (a chip) is a rough measure of its power; with more transistors per chip, there are more building blocks to implement more functionality or storage.</p>&#13;
</div>&#13;
<h4 class="h4" id="lev2_94">5.3.1 Basic Logic Gates</h4>&#13;
<p class="noindent">At the lowest level, all circuits are built from linking logic gates together. Logic gates implement Boolean operations on Boolean operands (0 or 1). <em>AND</em>, <em>OR</em>, and <em>NOT</em> form a complete set of logic gates from which any circuit can be constructed. A logic gate has one (NOT) or two (AND and OR) binary input values and produces a binary output value that is the bitwise logical operation on its input. For example, an input value of 0 to a NOT gate outputs 1 (1 is NOT(0)). A <em>truth table</em> for a logical operation lists the operation’s value for each permutation of inputs. <a href="ch05.xhtml#ch5tab1">Table 5-1</a> shows the truth tables for the AND, OR, and NOT logic gates.</p>&#13;
<p class="tabcap" id="ch5tab1"><strong>Table 5-1:</strong> Truth Table for AND, OR, and NOT</p>&#13;
<table class="line">&#13;
<colgroup>&#13;
<col style="width:20%"/>&#13;
<col style="width:20%"/>&#13;
<col style="width:20%"/>&#13;
<col style="width:20%"/>&#13;
<col style="width:20%"/>&#13;
</colgroup>&#13;
<tr class="borderb">&#13;
<td style="vertical-align: top"><p class="tab-c"><strong>A</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab-c"><strong>B</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab-c"><strong>A AND B</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab-c"><strong>A OR B</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab-c"><strong>NOT A</strong></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
</tr>&#13;
</table>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_244"/><a href="ch05.xhtml#ch5fig4">Figure 5-4</a> shows how computer architects represent these gates in circuit drawings.</p>&#13;
<div class="imagec" id="ch5fig4"><img alt="image" src="../images/05fig04.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-4: The AND, OR, and NOT logic gates for single-bit inputs produce a single-bit output</em></p>&#13;
<p class="indent">A multibit version of a logic gate (for <em>M</em>-bit input and output) is a very simple circuit constructed using <em>M</em> one-bit logic gates. Individual bits of the <em>M</em>-bit input value are each input into a different one-bit gate that produces the corresponding output bit of the <em>M</em>-bit result. For example, <a href="ch05.xhtml#ch5fig5">Figure 5-5</a> shows a four-bit AND circuit built from four 1-bit AND gates.</p>&#13;
<div class="imagec" id="ch5fig5"><img alt="image" src="../images/05fig05.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-5: A four-bit AND circuit built from four 1-bit AND gates</em></p>&#13;
<p class="indent">This type of very simple circuit, one that just expands input and output bit width for a logic gate, is often referred to as an <em>M</em>-bit gate for a particular value of <em>M</em> specifying the input and output bit width (number of bits).</p>&#13;
<h4 class="h4" id="lev2_95">5.3.2 Other Logic Gates</h4>&#13;
<p class="noindent">Even though the set of logic gates consisting of AND, OR, and NOT is sufficient for implementing any circuit, there are other basic logic gates that are often used to construct digital circuits. These additional logic gates include NAND (the negation of A AND B), NOR (the negation of A OR B), and XOR (exclusive OR). Their truth tables are shown in <a href="ch05.xhtml#ch5tab2">Table 5-2</a>.</p>&#13;
<p class="tabcap" id="ch5tab2"><span epub:type="pagebreak" id="page_245"/><strong>Table 5-2:</strong> Truth Table for NAND, NOR, and XOR</p>&#13;
<table class="line">&#13;
<colgroup>&#13;
<col style="width:20%"/>&#13;
<col style="width:20%"/>&#13;
<col style="width:20%"/>&#13;
<col style="width:20%"/>&#13;
<col style="width:20%"/>&#13;
</colgroup>&#13;
<tr class="borderb">&#13;
<td style="vertical-align: top"><p class="tab-c"><strong>A</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab-c"><strong>B</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab-c"><strong>A NAND B</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab-c"><strong>A NOR B</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab-c"><strong>A XOR B</strong></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
</tr>&#13;
</table>&#13;
<p class="indent">The NAND, NOR, and XOR gates appear in circuit drawings, as shown in <a href="ch05.xhtml#ch5fig6">Figure 5-6</a>.</p>&#13;
<div class="imagec" id="ch5fig6"><img alt="image" src="../images/05fig06.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-6: The NAND, NOR, and XOR logic gates</em></p>&#13;
<p class="indent">The circle on the end of the NAND and NOR gates represents negation or NOT. For example, the NOR gate looks like an OR gate with a circle on the end, representing the fact that NOR is the negation of OR.</p>&#13;
<div class="g-box">&#13;
<p class="box-title">MINIMAL SUBSETS OF LOGIC GATES</p>&#13;
<p class="noindentt">NAND, NOR, and XOR are not necessary for building circuits, but they are additional gates added to the set {AND, OR, NOT} that are commonly used in circuit design. Of the larger set {AND, OR, NOT, NAND, NOR, XOR}, there exist other minimal subsets of logic gates that alone are sufficient for building any circuit (the subset {AND, OR, NOT} is not the only one, but it is the easiest set to understand). Because NAND, NOR, and XOR are not necessary, their functionality can be implemented by combining AND, OR, and NOT gates into circuits that implement NAND, NOR, and XOR functions. For example, NOR can be built using a NOT combined with an OR gate, <code>(A NOR B)</code> ≡ <code>NOT(A OR B)</code>), as shown in <a href="ch05.xhtml#ch5fig7">Figure 5-7</a>.</p>&#13;
<div class="imagec" id="ch5fig7"><img alt="image" src="../images/05fig07.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-7: The NOR gate can be implemented using an OR and a NOT gate. The inputs, A and B, are first fed through an OR gate, and the OR gate’s output is input to a NOT gate (NOR is the NOT of OR).</em></p>&#13;
<p class="noindent">Today’s integrated circuits chips are built using CMOS technology, which uses NAND as the basic building block of circuits on the chip. The NAND gate by itself makes up another minimal subset of complete logic gates.</p>&#13;
</div>&#13;
<h3 class="h3" id="lev1_41"><span epub:type="pagebreak" id="page_246"/>5.4 Circuits</h3>&#13;
<p class="noindent">Digital circuits implement core functionality of the architecture. They implement the <em>Instruction Set Architecture</em> (ISA) in hardware, and also implement storage and control functionality throughout the system. Designing digital circuits involves applying multiple levels of abstraction: circuits implementing complex functionality are built from smaller circuits that implement partial functionality, which are built from even simpler circuits, and so on down to the basic logic gate building blocks of all digital circuits. <a href="ch05.xhtml#ch5fig8">Figure 5-8</a> illustrates a circuit abstracted from its implementation. The circuit is represented as a <em>black box</em> labeled with its functionality or name and with only its input and output shown, hiding the details of its internal implementation.</p>&#13;
<div class="imagec" id="ch5fig8"><img alt="image" src="../images/05fig08.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-8: A circuit is implemented by linking together subcircuits and logic gates. Its functionality is abstracted from the details of its implementation and can be used as a building block for creating other circuits.</em></p>&#13;
<p class="indent">There are three main categories of circuit building blocks: arithmetic/logic, control, and storage circuits. A processor integrated circuit, for example, contains all three types of subcircuits: its register set uses storage circuits; its core functionality for implementing arithmetic and logic functions uses arithmetic and logic circuits; and control circuits are used throughout the processor to drive the execution of instructions and to control loading and storing values in its registers.</p>&#13;
<p class="indent">In this section, we discuss these three types of circuit, showing how to design a basic circuit from logic gates, and then how to build larger circuits from basic circuits and logic gates.</p>&#13;
<h4 class="h4" id="lev2_96">5.4.1 Arithmetic and Logic Circuits</h4>&#13;
<p class="noindent">Arithmetic and logic circuits implement the arithmetic and logic instructions of an ISA that together make up the <em>arithmetic logic unit</em> (ALU) of the processor. Arithmetic and logic circuits also implement parts of other functionality in the CPU. For example, arithmetic circuits are used to increment <span epub:type="pagebreak" id="page_247"/>the program counter (PC) as part of the first step of instruction execution, and they are used to calculate memory addresses by combining instruction operand bits and register values.</p>&#13;
<p class="indent">Circuit design often starts with implementing a 1-bit version of a simple circuit from logic gates. This 1-bit circuit is then used as a building block for implementing <em>M</em>-bit versions of the circuit. The steps for designing a 1-bit circuit from basic logic gates are:</p>&#13;
<div class="number">&#13;
<p class="number">1. Design the truth table for the circuit: determine the number of inputs and outputs, and add a table entry for every permutation of input bit(s) that specifies the value of the output bit(s).</p>&#13;
<p class="number">2. Using the truth table, write an expression for when each circuit output is 1 in terms of its input values combined with AND, OR, NOT.</p>&#13;
<p class="number">3. Translate the expression into a sequence of logic gates, where each gate gets its inputs from either an input to the circuit or from the output of a preceding logic gate.</p>&#13;
</div>&#13;
<p class="indent">We follow these steps to implement a single-bit <em>equals</em> circuit: bitwise equals (<code>A</code> <code>== B</code>) outputs 1 when the values of <code>A</code> and <code>B</code> are the same, and it outputs 0 otherwise.</p>&#13;
<p class="indent">First, design the truth table for the circuit:</p>&#13;
<p class="tabcap" id="ch5tab3"><strong>Table 5-3:</strong> Truth Table for a Simple Equality Circuit</p>&#13;
<table class="line">&#13;
<colgroup>&#13;
<col style="width:30%"/>&#13;
<col style="width:30%"/>&#13;
<col style="width:40%"/>&#13;
</colgroup>&#13;
<tr class="borderb">&#13;
<td style="vertical-align: top"><p class="tab-c"><strong>A</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab-c"><strong>B</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab-c"><strong>A == B Output</strong></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
</tr>&#13;
</table>&#13;
<p class="indent">Next, write expressions for when <code>A == B</code> is 1 in terms of <code>A</code> and <code>B</code> combined with AND, OR, and NOT. First, consider each row whose output is 1 separately, starting with the first row in the truth table:</p>&#13;
<table class="line">&#13;
<colgroup>&#13;
<col style="width:30%"/>&#13;
<col style="width:30%"/>&#13;
<col style="width:40%"/>&#13;
</colgroup>&#13;
<tr class="borderb">&#13;
<td style="vertical-align: top"><p class="tab-c"><strong>A</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab-c"><strong>B</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab-c"><strong>A == B</strong></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
</tr>&#13;
</table>&#13;
<p class="indent">For the input values in this row, construct a <em>conjunction</em> of expressions of its inputs that evaluate to 1. A conjunction combines subexpressions that evaluate to 0 or 1 with AND, and is itself 1 only when both of its subexpressions evaluate to 1. Start by expressing when each input evaluates to 1:</p>&#13;
<pre>NOT(A)    # is 1 when A is 0<br/>&#13;
NOT(B)    # is 1 when B is 0</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_248"/>Then, create their conjunction (combine them with AND) to yield an expression for when this row of the truth table evaluates to 1:</p>&#13;
<pre>NOT(A) AND NOT(B)    # is 1 when A and B are both 0</pre>&#13;
<p class="indent">We do the same thing for the last row in the truth table, whose output is also 1:</p>&#13;
<table class="line">&#13;
<colgroup>&#13;
<col style="width:30%"/>&#13;
<col style="width:30%"/>&#13;
<col style="width:40%"/>&#13;
</colgroup>&#13;
<tr class="borderb">&#13;
<td style="vertical-align: top"><p class="tab-c"><strong>A</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab-c"><strong>B</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab-c"><strong>A == B</strong></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
</tr>&#13;
</table>&#13;
<pre>A AND B   # is 1 when A and B are both 1</pre>&#13;
<p class="indent">Finally, create a <em>disjunction</em> (an OR) of each conjunction corresponding to a row in the truth table that evaluates to 1:</p>&#13;
<pre>(NOT(A) AND NOT(B)) OR (A AND B)  # is 1 when A and B are both 0 or both 1</pre>&#13;
<p class="indent">At this point we have an expression for <code>A == B</code> that can be translated to a circuit. At this step, circuit designers employ techniques to simplify the expression to create a minimal equivalent expression (one that corresponds to the fewest operators and/or shortest path length of gates through the circuit). Designers must take great care when minimizing a circuit design to ensure the equivalence of the translated expression. There are formal methods for circuit minimization that are beyond the scope of our coverage, but we will employ a few heuristics as we develop circuits.</p>&#13;
<p class="indent">For our example, we directly translate the preceding expression to a circuit. We may be tempted to replace (NOT(A) AND NOT(B)) with (A NAND B), but note that these two expressions <em>are not</em> equivalent: they do not evaluate the same for all permutations of A and B. For example, when A is 1 and B is 0, (A == B) is 0 and (A NAND B) is 1.</p>&#13;
<p class="indent">To translate the expression to a circuit, start from the innermost expression and work outward (the innermost will be the first gates, whose outputs will be inputs to subsequent gates). The first set of gates correspond to any negation of input values (NOT gates of inputs A and B). Next, for each conjunction, create parts of the circuit feeding input values into an AND gate. The AND gate outputs are then fed into OR gate(s) representing the disjunction. The resulting circuit is shown in <a href="ch05.xhtml#ch5fig9">Figure 5-9</a>.</p>&#13;
<div class="imagec" id="ch5fig9"><img alt="image" src="../images/05fig09.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-9: The one-bit equality circuit (A == B) constructed from AND, OR, and NOT logic gates</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_249"/>To verify the correctness of this circuit, simulate all possible permutations of input values A and B through the circuit and verify that the output of the circuit matches its corresponding row in the truth table for (A == B). For example, if A is 0 and B is 0, the two NOT gates negate their values before being fed through the top AND gate, so the input to this AND gate is (1, 1), resulting in an output of 1, which is the top input value to the OR gate. The values of A and B (0, 0) are fed directly though the bottom AND gate, resulting in output of 0 from the bottom AND gate, which is the lower input to the OR gate. The OR gate thus receives input values (1, 0) and outputs the value 1. So, when A and B are both 0, the circuit correctly outputs 1. <a href="ch05.xhtml#ch5fig10">Figure 5-10</a> illustrates this example.</p>&#13;
<div class="imagec" id="ch5fig10"><img alt="image" src="../images/05fig10.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-10: An example showing how the one-bit equality circuit computes (A == B). Starting with input values 0 for A and 0 for B, the values propagate through the gates making up the circuit to compute the correct output value of 1 for A == B.</em></p>&#13;
<p class="indent">Viewing the implementation of a one-bit equality circuit as a unit allows it to be abstracted from its implementation, and thus it can be more easily used as a building block for other circuits. We represent this abstraction of the one-bit equality circuit (shown in <a href="ch05.xhtml#ch5fig11">Figure 5-11</a>) as a box with its two inputs labeled <em>A</em> and <em>B</em> and its single output labeled <em>A == B</em>. The internal gates that implement the one-bit equality circuit are hidden in this abstracted view of the circuit.</p>&#13;
<div class="imagec" id="ch5fig11"><img alt="image" src="../images/05fig11.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-11: The one-bit equality circuit abstraction. This circuit can be used as a building block in other circuits.</em></p>&#13;
<p class="indent">Single-bit versions of NAND, NOR, and XOR circuits can be constructed similarly, using only AND, OR, and NOT gates, starting with their truth tables (<a href="ch05.xhtml#ch5tab4">Table 5-4</a>) and applying the same steps as the one-bit equality circuit.</p>&#13;
<span epub:type="pagebreak" id="page_250"/>&#13;
<p class="tabcap" id="ch5tab4"><strong>Table 5-4:</strong> Truth Table for the NAND, NOR, and XOR Circuits</p>&#13;
<table class="line">&#13;
<colgroup>&#13;
<col style="width:20%"/>&#13;
<col style="width:20%"/>&#13;
<col style="width:20%"/>&#13;
<col style="width:20%"/>&#13;
<col style="width:20%"/>&#13;
</colgroup>&#13;
<tr class="borderb">&#13;
<td style="vertical-align: top"><p class="tab-c"><strong>A</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab-c"><strong>B</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab-c"><strong>A NAND B</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab-c"><strong>A NOR B</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab-c"><strong>A XOR B</strong></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
</tr>&#13;
</table>&#13;
<p class="indent">Multibit versions of these circuits are constructed from multiple single-bit versions of the circuits in a similar way to how the four-bit AND gate was constructed from four 1-bit AND gates in “Basic Logic Gates” on <a href="ch05.xhtml#lev2_94">page 243</a>.</p>&#13;
<h4 class="h4" id="lev2_97">Arithmetic Circuits</h4>&#13;
<p class="noindent">Arithmetic circuits are constructed using exactly the same method as we used for constructing the logic circuits. For example, to construct a 1-bit adder circuit, start with the truth table for single-bit addition, which has two input values, A and B, and two output values, one for the SUM of A and B, and another output for overflow or CARRY OUT. <a href="ch05.xhtml#ch5tab5">Table 5-5</a> shows the resulting truth table for one-bit add.</p>&#13;
<p class="tabcap" id="ch5tab5"><strong>Table 5-5:</strong> Truth Table for a One-Bit Adder Circuit</p>&#13;
<table class="line">&#13;
<colgroup>&#13;
<col style="width:25%"/>&#13;
<col style="width:25%"/>&#13;
<col style="width:25%"/>&#13;
<col style="width:25%"/>&#13;
</colgroup>&#13;
<tr class="borderb">&#13;
<td style="vertical-align: top"><p class="tab-c"><strong>A</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab-c"><strong>B</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab-c"><strong>SUM</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab-c"><strong>CARRY OUT</strong></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
</tr>&#13;
</table>&#13;
<p class="indent">In the next step, for each output, SUM and CARRY OUT, create logical expressions of when the output value is 1. These expressions are expressed as disjunctions of per-row conjunctions of input values:</p>&#13;
<pre>SUM: (NOT(A) AND B) OR (A AND NOT(B))     # 1 when exactly one of A or B is 1<br/>&#13;
CARRY OUT:  A AND B                       # 1 when both A and B are 1</pre>&#13;
<p class="indent">The expression for CARRY OUT cannot be simplified. However, the expression for SUM is more complicated and can be simplified, leading to a simpler circuit design. The first thing to note is that the SUM output can also be expressed as (A XOR B). If we have an XOR gate or circuit, expressing SUM as (A XOR B) results in a simpler adder circuit design. If not, then the expression using AND, OR, and NOT is used and implemented using AND, OR, and NOT gates.</p>&#13;
<p class="indent">Let’s assume that we have an XOR gate that we can use for implementing the 1-bit adder circuit. The resulting circuit is shown in <a href="ch05.xhtml#ch5fig12">Figure 5-12</a>.</p>&#13;
<span epub:type="pagebreak" id="page_251"/>&#13;
<div class="imagec" id="ch5fig12"><img alt="image" src="../images/05fig12.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-12: The one-bit adder circuit has two inputs, A and B, and two outputs, SUM and CARRY OUT.</em></p>&#13;
<p class="indent">The one-bit adder circuit can be used as a building block for more complicated circuits. For example, we may want to create <em>N</em>-bit adder circuits for performing addition on values of different sizes (e.g. one-byte, two-byte, or four-byte adder circuits). However, creating an <em>N</em>-bit adder circuit from <em>N</em> one-bit adder circuits requires more care than creating an <em>N</em>-bit logic circuits from <em>N</em> 1-bit logic circuits.</p>&#13;
<p class="indent">When performing a multibit addition (or subtraction), individual bits are summed in order from the least significant bit to the most significant bit. As this bitwise addition proceeds, if the sum of the <em>i</em>th bits results in a carry out value of 1, then an additional 1 is added with the two (<em>i</em> + 1)st bits. In other words, the carry out of the <em>i</em>th bit adder circuit is an input value to the (<em>i</em> + 1)st bit adder circuit.</p>&#13;
<p class="indent">Thus, to implement a multibit adder circuit, we need a new one-bit adder circuit that has three inputs: A, B, and CARRY IN. To do this, follow the steps described earlier for creating a one-bit adder circuit, with three inputs (A, B, CARRY IN) and two outputs (SUM and CARRY OUT), starting with the truth table for all possible permutations of its three inputs. We leave the design of this circuit as an exercise for the reader, but we show its abstraction as a one-bit adder circuit in <a href="ch05.xhtml#ch5fig13">Figure 5-13</a>.</p>&#13;
<div class="imagec" id="ch5fig13"><img alt="image" src="../images/05fig13.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-13: The one-bit adder circuit with three inputs (A, B, and CARRY IN) and two outputs (SUM and CARRY OUT).</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_252"/>Using this version of a one-bit adder circuit as a building block, we can construct an <em>N</em>-bit adder circuit by feeding corresponding operand bits through individual one-bit adder circuits, feeding the CARRY OUT value from the <em>i</em>th one-bit adder circuit into the CARRY IN value of the (<em>i</em> + 1)st one-bit adder circuit. The one-bit adder circuit for the 0th bits receives a value of 0 for its CARRY IN from another part of the CPU circuitry that decodes the ADD instruction.</p>&#13;
<p class="indent">This type of <em>N</em>-bit adder circuit, built from <em>N</em> one-bit adder circuits, is called a <em>ripple carry adder</em>, shown in <a href="ch05.xhtml#ch5fig14">Figure 5-14</a>. The SUM result <em>ripples</em> or propagates through the circuit from the low-order to the high-order bits. Only after bit 0 of the SUM and CARRY OUT values are computed will bit 1 of the SUM and CARRY OUT be correctly computed. This is because the 1st bit’s CARRY IN gets its value from the 0th bit’s CARRY OUT, and so on for subsequent higher-order bits of the result.</p>&#13;
<div class="imagec" id="ch5fig14"><img alt="image" src="../images/05fig14.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-14: A four-bit ripple adder circuit created from four 1-bit adder circuits</em></p>&#13;
<p class="indent">Circuits for other arithmetic and logic functions are constructed in similar ways by combining circuits and logic gates. For example, a subtraction circuit that computes (A − B) can be built from adder and negation circuits that compute subtraction as (A + (−B)).</p>&#13;
<h4 class="h4" id="lev2_98">5.4.2 Control Circuits</h4>&#13;
<p class="noindent">Control circuits are used throughout a system. On the processor, they drive the execution of program instructions on program data. They also control loading and storing values to different levels of storage (between registers, cache, and RAM), and control hardware devices in the system. Just like arithmetic <span epub:type="pagebreak" id="page_253"/>and logic circuits, control circuits that implement complicated functionality are built by combining simpler circuits and logic gates.</p>&#13;
<p class="indent">A <em>multiplexer</em> (MUX) is an example of a control circuit that selects, or chooses, one of several values. The CPU may use a multiplexer circuit to select from which CPU register to read an instruction operand value.</p>&#13;
<p class="indent">An <em>N</em>-way multiplexer has a set of <em>N</em> input values and a single output value selected from one of its inputs. An additional input value, <em>Select</em> (S), encodes which of its <em>N</em> inputs is chosen for its output.</p>&#13;
<p class="indent">The most basic two-way MUX selects between two 1-bit inputs, A and B. The Select input for a two-way multiplexer is a single bit: if the S input is 1, it will select A for output; if it is 0 it will select B for output. The table that follows shows the truth table for a two-way one-bit multiplexer. The value of the selection bit (S) chooses either the value of A or B as the MUX output value.</p>&#13;
<table class="line">&#13;
<colgroup>&#13;
<col style="width:20%"/>&#13;
<col style="width:20%"/>&#13;
<col style="width:20%"/>&#13;
<col style="width:30%"/>&#13;
</colgroup>&#13;
<tr class="borderb">&#13;
<td style="vertical-align: top"><p class="tab-c"><strong>A</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab-c"><strong>B</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab-c"><strong>S</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab-c"><strong>Out</strong></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0 (B’s value)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1 (B’s value)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0 (B’s value)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1 (B’s value)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0 (A’s value)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0 (A’s value)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1 (A’s value)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1 (A’s value)</p></td>&#13;
</tr>&#13;
</table>&#13;
<p class="indent"><a href="ch05.xhtml#ch5fig15">Figure 5-15</a> shows the two-way multiplexer circuit for single-bit input.</p>&#13;
<div class="imagec" id="ch5fig15"><img alt="image" src="../images/05fig15.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-15: A two-way 1-bit multiplexer circuit. The value of the signal input (S) is used to pick which of its two inputs (A or B) will be the circuit’s output value: when S is 1, A is chosen; when S is 0, B is chosen.</em></p>&#13;
<p class="indent"><a href="ch05.xhtml#ch5fig16">Figure 5-16</a> shows how the multiplexer chooses A’s output with an S input value of 1. For example, suppose that the input values are 1 for A, 0 for B, and 1 for S. S is negated before being sent to the top AND gate with B (0 AND B), resulting in a 0 output value from the top AND gate. S feeds into the bottom AND gate with A, resulting in (1 AND A), which evaluates to the value of A being output from the bottom AND gate. The value of A (1 in our example) and 0 from the top AND gate feed as input to the OR gate, resulting in (0 OR A) being output. In other words, when S is 1, the MUX <span epub:type="pagebreak" id="page_254"/>chooses the value of A as its output (A’s value being 1 in our example). The value of B does not affect the final output of the MUX, because 0 will always be the output of the top AND gate when S is 1.</p>&#13;
<div class="imagec" id="ch5fig16"><img alt="image" src="../images/05fig16.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-16: A two-way 1-bit multiplexer circuit chooses (outputs) A when S is 1.</em></p>&#13;
<p class="indent"><a href="ch05.xhtml#ch5fig17">Figure 5-17</a> shows the path through the multiplexer when the S input value 0 chooses B’s output. If we consider the same input for A and B as the previous example, but change S to 0, then the negation of 0 is input to the top AND gate resulting in (1 AND B), or B’s value, output from the top AND gate. The input to the bottom AND gate is (0 AND A), resulting in 0 from the bottom AND gate. Thus, the input values to the OR gate are (B OR 0), which evaluates to B’s value as the MUX’s output (B’s value being 0 in our example).</p>&#13;
<div class="imagec" id="ch5fig17"><img alt="image" src="../images/05fig17.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-17: A two-way 1-bit multiplexer circuit chooses (outputs) B when S is 0.</em></p>&#13;
<p class="indent">A two-way 1-bit MUX circuit is a building block for constructing two-way <em>N</em>-bit MUX circuits. For example, <a href="ch05.xhtml#ch5fig18">Figure 5-18</a> shows a two-way four-bit MUX built from four 1-bit two-way MUX circuits.</p>&#13;
<span epub:type="pagebreak" id="page_255"/>&#13;
<div class="imagec" id="ch5fig18"><img alt="image" src="../images/05fig18.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-18: A two-way four-bit multiplexer circuit built from four two-way 1-bit multiplexer circuits. A single signal bit, S, chooses either A or B as output.</em></p>&#13;
<p class="indent">An <em>N</em>-way multiplexer chooses one of <em>N</em> inputs as output. It requires a slightly different MUX circuit than a two-way MUX, and needs log<sub>2</sub>(<em>N</em>) bits for its Select input. The additional selection bits are needed because with log<sub>2</sub>(<em>N</em>) bits, <em>N</em> distinct values can be encoded, one for selecting each of the <em>N</em> inputs. Each distinct permutation of the log<sub>2</sub>(<em>N</em>) Select bits is input with one of the <em>N</em> input values to an AND gate, resulting in exactly one MUX input value selected as the MUX output. <a href="ch05.xhtml#ch5fig19">Figure 5-19</a> shows an example of a one-bit four-way MUX circuit.</p>&#13;
<div class="imagec" id="ch5fig19"><img alt="image" src="../images/05fig19.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-19: A four-way multiplexer circuit has four inputs and two</em> (log<sub>2</sub>(4)) <em>select bits that encode which of the four inputs should be output by the multiplexer.</em></p>&#13;
<p class="indent">The four-way MUX circuit uses four three-input AND gates and one four-input OR gate. Multi-input versions of gates can be built by chaining together multiple two-input AND (and OR) gates. For example, a three-input AND gate is built from two two-input <span epub:type="pagebreak" id="page_256"/>AND gates, where the first AND gate takes two of the input values and the second AND gate takes the third input value and the output from the first AND gate: (x AND y AND z) is equivalent to ((x AND y) AND z).</p>&#13;
<p class="indent">To see how the four-way MUX circuit works, consider an S input value of 2 (0b10 in binary), as shown in <a href="ch05.xhtml#ch5fig20">Figure 5-20</a>. The top AND gate gets as input (NOT(S<sup>0</sup>) AND NOT(S<sup>1</sup>) AND A), or (1 AND 0 AND A), resulting in 0 output from the top AND gate. The second AND gate gets input values (0 AND 0 AND B), resulting in 0 output. The third AND gate gets input values (1 AND 1 AND C), resulting in the value of C output. The last AND gate gets (0 AND 1 AND D), resulting in 0 output. The OR gate has inputs (0 OR 0 OR C OR 0), resulting in the value of C output by the MUX (an S value of 2 chooses C).</p>&#13;
<div class="imagec" id="ch5fig20"><img alt="image" src="../images/05fig20.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-20: A four-way multiplexer circuit chooses C as output when the Select input, S, is 2 (0b10).</em></p>&#13;
<p class="indent">Demultiplexers and decoders are two other examples of control circuits. A <em>demultiplexer</em> (DMUX) is the inverse of a multiplexer. Whereas a multiplexer chooses one of <em>N</em> inputs, a demultiplexer chooses one of <em>N</em> outputs. A DMUX takes a single input value and a selection input, and has <em>N</em> outputs. Based on the value of S, it sends the input value to exactly one of its <em>N</em> outputs (the value of the input is routed on to one of <em>N</em> output lines). A DMUX circuit is often used to select one of <em>N</em> circuits to pass a value. A <em>decoder</em> circuit takes an encoded input and enables one of several outputs based on the input value. For example, a decoder circuit that has an <em>N</em>-bit input value uses that value to enable (to set to 1) exactly one of its 2<sup><em>N</em></sup> output lines (the one corresponding to the encoding of the <em>N</em>-bit value). <a href="ch05.xhtml#ch5fig21">Figure 5-21</a> shows an example of a two-way one-bit DMUX circuit, whose selection input value (s) chooses which of its two outputs gets the input value A. It also shows an example of a two-bit decoder circuit, whose input bits determine which of four outputs get set to 1. The truth tables for both circuits are also shown.</p>&#13;
<span epub:type="pagebreak" id="page_257"/>&#13;
<div class="imagec" id="ch5fig21"><img alt="image" src="../images/05fig21.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-21: A two-way one-bit demultiplexer and a two-bit decoder, along with their truth tables</em></p>&#13;
<h4 class="h4" id="lev2_99">5.4.3 Storage Circuits</h4>&#13;
<p class="noindent"><em>Storage circuits</em> are used to construct computer memory for storing binary values. The type of computer memory built from storage circuits is called <em>static RAM</em> (SRAM). It is used to build CPU register storage and on-chip cache memory. Systems typically use <em>dynamic RAM</em> (DRAM) for main memory (RAM) storage. The capacitor-based design of DRAM requires that it be periodically refreshed with the value it stores, hence the “dynamic” moniker. SRAM is circuit-based storage that does not need to have its values refreshed, thus it is referred to as static RAM. Circuit-based memory is faster but more expensive than capacitor-based memory. As a result, SRAM tends to be used for storage at the top of the memory hierarchy (CPU registers and on-chip cache memory), and DRAM for main memory (RAM) storage. In this chapter, we focus on circuit-based memory like SRAM.</p>&#13;
<p class="indent">To store a value, a circuit must contain a feedback loop so that the value is retained by the circuit. In other words, a storage circuit’s value depends on its input values and also its currently stored value. When the circuit stores a value, its currently stored value and its inputs together produce an output that matches the currently stored value (i.e., the circuit continues to store the same value). When a new value is written into a storage circuit, the circuit’s input values change momentarily to modify the behavior of the circuit, which results in a new value being written into and stored in the circuit. Once written, the circuit resumes a steady state of storing the newly written value until the next write to the circuit occurs.</p>&#13;
<h5 class="h5" id="lev3_49">RS Latch</h5>&#13;
<p class="noindent">A latch is a digital circuit that stores (or remembers) a one-bit value. One example is a <em>reset–set latch</em> (or RS latch). An RS latch has two input values, R and S, and one output value, Q, which is also the value stored in the latch. An RS latch may additionally output NOT(Q), the negation of the stored value. <a href="ch05.xhtml#ch5fig22">Figure 5-22</a> shows an RS latch circuit for storing a single bit.</p>&#13;
<span epub:type="pagebreak" id="page_258"/>&#13;
<div class="imagec" id="ch5fig22"><img alt="image" src="../images/05fig22.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-22: An RS latch circuit stores a one-bit value.</em></p>&#13;
<p class="indent">The first thing to note about the RS latch is the feedback loop from its outputs to its inputs: the output of the top NAND gate (Q) is input (a) to the bottom NAND gate, and the output of the bottom NAND gate (~Q) is input (b) to the top NAND gate. When inputs S and R are both 1, the RS latch stores the value Q. In other words, when S and R are both 1, the RS latch output value Q is stable. To see this behavior, consider <a href="ch05.xhtml#ch5fig23">Figure 5-23</a>; this shows an RS latch that stores the value 1 (Q is 1). When R and S are both 1, the feedback input value (a) to the bottom NAND gate is the value of Q, which is 1, so the output of the bottom NAND gate is 0 (1 NAND 1 is 0). The feedback input value (b) to the top NAND gate is the output of the bottom NAND gate, which is 0. The other input to the top NAND gate is 1, the value of S. The output of the top gate is 1 (1 NAND 0 is 1). Thus, when S and R are both 1, this circuit continuously stores the value of Q (1 in this example).</p>&#13;
<div class="imagec" id="ch5fig23"><img alt="image" src="../images/05fig23.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-23: An RS latch that stores a one-bit value. R and S are both 1 when the latch stores a value. The stored value is output Q.</em></p>&#13;
<p class="indent">To change the value stored in an RS latch, the value of exactly one of R or S is set to 0. When the latch stores the new value, R and S are set back to 1. Control circuitry around the RS latch ensures that R and S can never simultaneously be 0: at most one of them will have a value 0, and a value of 0 for one of R or S means that a value is being written into the RS latch. To store the value 0 in an RS latch, input R is set to 0 (and the value of S stays at 1). To store the value 1 in an RS latch, input S is set to 0 (and the value of R stays at 1). For example, assume that the RS latch currently stores 1. To write 0 into the latch, R’s value is set to 0. This means that the values 0 and 1 are input to the lower NAND gate which computes the result of (0 NAND 1), or is 1. This output value of 1 is also input b to the top NAND gate (shown in <a href="ch05.xhtml#ch5fig24">Figure 5-24</a> B). With a new b input value of 1 and the S input value 1, the upper NAND gate computes a new output value 0 for Q, which is also fed as <span epub:type="pagebreak" id="page_259"/>input a into the lower NAND gate (shown in <a href="ch05.xhtml#ch5fig24">Figure 5-24</a> C). With a’s value 0 and b’s value 1, the latch now stores 0. When R is eventually set back to 1 the RS latch continues to store the value 0 (shown in <a href="ch05.xhtml#ch5fig24">Figure 5-24</a> D).</p>&#13;
<div class="imagec" id="ch5fig24"><img alt="image" src="../images/05fig24.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-24: To write 0 to an RS latch, momentarily set R to 0.</em></p>&#13;
<h5 class="h5" id="lev3_50">Gated D Latch</h5>&#13;
<p class="noindent">A <em>gated D latch</em> adds circuitry to an RS latch to ensure that it never receives an input of 0 to both R and S simultaneously. <a href="ch05.xhtml#ch5fig25">Figure 5-25</a> shows the construction of a gated D latch.</p>&#13;
<div class="imagec" id="ch5fig25"><img alt="image" src="../images/05fig25.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-25: A gated D latch stores a one-bit value. Its first set of NAND gates control writes to the RS latch and ensure that the values of R and S are never both simultaneously 0.</em></p>&#13;
<p class="indent">The data input (D) to the gated D latch is the value to store into the circuit (either 0 or 1). The Write Enable (WE) input controls writing a value into the RS latch. When WE is 0, the output from both NAND gates is 1, resulting in R and S input values of 1 to the RS latch (the RS latch stores a value). The gated D latch writes the value of D into the RS latch only when <span epub:type="pagebreak" id="page_260"/>WE is 1. Because the data input (D) value is inverted before it is sent to the bottom NAND gate, the input of only one of the top or bottom NAND gates is 1. This means that when the WE bit is 1, exactly one of R or S is 0. For example, when D is 1 and WE is 1, the top NAND computes (1 NAND 1) and the bottom NAND gate computes (O NAND 1). As a result, the input to S from the top NAND gate is 0 and the input to R from the bottom NAND gate is 1, resulting in writing the value 1 into the RS latch. When the WE input is 0, both NAND gates output 1, keeping R and S at 1. In other words, when WE is 0, the value of D has no effect on the value stored in the RS latch; only when WE is 1 is the value of D written into the latch. To write another value into the gated D latch, set D to the value to store and WE to 0.</p>&#13;
<h5 class="h5" id="lev3_51">CPU Register</h5>&#13;
<p class="noindent">Multibit storage circuits are built by linking several one-bit storage circuits together. For example, combining 32 one-bit D latches together yields a 32-bit storage circuit that could be used as a 32-bit CPU register, as shown in <a href="ch05.xhtml#ch5fig26">Figure 5-26</a>. The register circuit has two input values: a 32-bit data value and a one-bit Write Enable signal. Internally, each one-bit D latch takes as its D input one bit of the register’s 32-bit <em>Data in</em> input, and each one-bit D latch takes the register’s WE input as its WE input. The register’s output is the 32-bit value stored across the 32 one-bit D latches that make up the register circuit.</p>&#13;
<div class="imagec" id="ch5fig26"><img alt="image" src="../images/05fig26.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-26: A CPU register is built from multiple gated D latches (32 of them for a 32-bit register). When its WE input is 1, the Data input is written into the register. Its Data output is the stored value.</em></p>&#13;
<h3 class="h3" id="lev1_42">5.5 Building a Processor: Putting It All Together</h3>&#13;
<p class="noindent">The <em>central processing unit</em> (CPU) implements the processing and control units of the von Neumann architecture, the parts that drive the execution of program instructions on program data (see <a href="ch05.xhtml#ch5fig27">Figure 5-27</a>).</p>&#13;
<span epub:type="pagebreak" id="page_261"/>&#13;
<div class="imagec" id="ch5fig27"><img alt="image" src="../images/05fig27.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-27: The CPU implements the processing and control unit parts of the von Neumann architecture.</em></p>&#13;
<p class="indent">The CPU is constructed from basic arithmetic/logic, storage, and control circuit building blocks. Its main functional components are the <em>arithmetic logic unit</em> (ALU), which performs arithmetic and logic operations; a set of general-purpose <em>registers</em> for storing program data; some control circuitry and special-purpose registers that are used in the implementation of instruction execution; and a <em>clock</em> that drives the circuitry of the CPU to execute program instructions.</p>&#13;
<p class="indent">In this section, we present the main parts of the CPU, including the ALU and register file, and show how they are combined to implement a CPU. In the next section, we discuss how the CPU executes program instructions and how the clock is used to drive the execution of program instructions.</p>&#13;
<h4 class="h4" id="lev2_100">5.5.1 The ALU</h4>&#13;
<p class="noindent">The ALU is a complex circuit that implements all arithmetic and logic operations on signed and unsigned integers. A separate floating-point unit performs arithmetic operations on floating-point values. The ALU takes integer operand values and an <em>opcode</em> value that specifies the operation to perform (e.g., addition). The ALU outputs the resulting value of performing the specified operation on the operand inputs and <em>condition code</em> values that encode information about the result of the operation. Common condition codes specify whether the ALU result is negative, zero, or if there is a carry-out bit from the operation. For example, given the C statement</p>&#13;
<pre>x = 6 + 8;</pre>&#13;
<p class="noindent">the CPU begins executing the addition by feeding the operand values (6 and 8) and the bits that encode an ADD operation to the ALU circuit. The ALU computes the result and outputs it along with condition codes to indicate that the result is nonnegative, is nonzero, and causes no carry-out. Each condition code is encoded in a single bit. A bit value of 1 indicates that the condition holds, and a bit value of 0 indicates that it does not hold for the ALU result. In our example, the bit pattern 000 specifies the set of three conditions associated with executing 6 + 8: the result is not negative (0), is not zero (0), and the carry-out value is zero (0).</p>&#13;
<p class="indent">Condition codes, set by the ALU as part of its execution of an operation, are sometimes used by subsequent instructions that choose an action based on a particular condition. For example, an ADD instruction can compute the (x + 8) part of the following <code>if</code> statement.</p>&#13;
<span epub:type="pagebreak" id="page_262"/>&#13;
<pre>if( (x + 8) != 0 ) {<br/>&#13;
    x++;<br/>&#13;
}</pre>&#13;
<p class="indent">The ALU’s execution of the ADD instruction sets condition codes based on the result of adding <code>(x + 8)</code>. A conditional jump instruction executed after the ADD instruction tests the condition code bits set by the ADD instruction and either jumps (skips over executing the instructions in the <code>if</code> body) or not based on their value. For example, if the ADD instruction sets the zero condition code to 0, the conditional jump instruction will not jump past the instructions associated with the <code>if</code> body (0 for the zero condition code means that the result of the ADD was not zero). If the zero condition code is 1, it will jump past the <code>if</code> body instructions. To implement a jump past a set of instructions, the CPU writes the memory address of the first instruction after the <code>if</code> body instructions into the <em>program counter</em> (PC), which contains the address of the next instruction to execute.</p>&#13;
<p class="indent">An ALU circuit combines several arithmetic and logic circuits (for implementing its set of operations) with a multiplexer circuit to pick the ALU’s output. Rather than trying to selectively activate only the arithmetic circuit associated with the specific operation, a simple ALU sends its operand input values to all of its internal arithmetic and logic circuits. The output from all of the ALU’s internal arithmetic and logic circuits are input to its multiplexer circuit, which chooses the ALU’s output. The opcode input to the ALU is used as the signal input to the multiplexer to select which arithmetic/logic operation to select as the ALU’s output. Condition code output is based on the MUX output combined with circuitry to test the output’s value to determine each condition code bit.</p>&#13;
<p class="indent"><a href="ch05.xhtml#ch5fig28">Figure 5-28</a> shows an example ALU circuit that performs four different operations (ADD, OR, AND, and EQUALS) on two 32-bit operands. It also produces a single condition code output that indicates whether the result of the operation is zero. Notice that the ALU directs the opcode to a multiplexer that selects which of the ALU’s four arithmetic results it outputs.</p>&#13;
<div class="imagec" id="ch5fig28"><img alt="image" src="../images/05fig28.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-28: A four-function ALU that performs ADD, OR, AND, and EQUALS on two 32-bit operands. It has one condition code output bit that specifies whether the result is 0.</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_263"/>The opcode input to the ALU comes from bits in the instruction that the CPU is executing. For example, the binary encoding for an ADD instruction might consist of four parts:</p>&#13;
<pre>  OPCODE BITS | OPERAND A SOURCE | OPERAND B SOURCE | RESULT DESTINATION</pre>&#13;
<p class="indent">Depending on the CPU architecture, operand source bits might encode a CPU register, the memory address storing the operand value, or literal operand values. For example, in an instruction to perform 6 + 8, the literal values 6 and 8 could be encoded directly into the operand specifier bits of the instruction.</p>&#13;
<p class="indent">For our ALU, the opcode requires two bits because the ALU supports four operations, and two bits can encode four distinct values (00, 01, 10, 11), one for each operation. In general, an ALU that performs <em>N</em> distinct operations needs log<sub>2</sub>(<em>N</em>) opcode bits to specify which operation result to output from the ALU.</p>&#13;
<p class="indent"><a href="ch05.xhtml#ch5fig29">Figure 5-29</a> shows an example of how the opcode and operand bits of an ADD instruction are used as input into our ALU.</p>&#13;
<div class="imagec" id="ch5fig29"><img alt="image" src="../images/05fig29.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-29: Opcode bits from an instruction are used by the ALU to choose which operation to output. In this example, different bits from an ADD instruction are fed into the ALU operand and opcode inputs to perform addition of 6 and 8.</em></p>&#13;
<h4 class="h4" id="lev2_101">5.5.2 The Register File</h4>&#13;
<p class="noindent">At the top of the memory hierarchy, the CPU’s set of general-purpose registers store temporary values. CPUs provide a very small number of registers, commonly 8–32 (e.g., the IA32 architecture provides 8, MIPS provides 16, and ARM provides 13). Instructions often get their operand values from, or store their results to, general-purpose registers. For example, an ADD instruction may be encoded as “add the value from Register 1 to the value from Register 2 and store the result in Register 3.”</p>&#13;
<p class="indent">The CPU’s set of general-purpose registers is organized into a <em>register file</em> circuit. A register file consists of a set of register circuits (see “CPU Register” on <a href="ch05.xhtml#lev3_51">page 260</a>) for storing data values and some control circuits (see “Control Circuits” on <a href="ch05.xhtml#lev2_98">page 252</a>) for controlling reads and writes to its registers. The circuit typically has a single data input line for the value to write into one of its registers, and two data output lines for simultaneously reading two values from its registers.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_264"/><a href="ch05.xhtml#ch5fig30">Figure 5-30</a> shows an example of a register file circuit with four registers. Its two output values (Data out<sub class="textsubscript"><strong>0</strong></sub> and Data out<sub class="textsubscript"><strong>1</strong></sub>) are controlled by two multiplexer circuits. Each of its read selection inputs (Sr<sub class="textsubscript"><strong>0</strong></sub> and Sr<sub class="textsubscript"><strong>1</strong></sub>) is fed into one of the MUXs to pick the register value for the corresponding output. The data input to the register file (the Data in line) is sent to every register circuit, and its write enable (WE) input is fed through a demultiplexer (DMUX) circuit first before being sent to each register circuit. A DMUX circuit takes one input value and chooses which of <em>N</em> outputs to send the value to, sending the remaining <em>N –</em> 1 outputs 0. The write selection input (S<sub class="textsubscript"><strong>w</strong></sub>) to the register file is sent to the DMUX circuit to choose the WE value’s destination register. When the register file’s WE input value is 0, no value is written into a register because each register’s WE bit also gets 0 (thus, Data in has no effect on the values stored in the registers). When the WE bit is 1, the DMUX outputs a WE bit value of 1 to only the register specified by the write selection input (S<sub class="textsubscript"><strong>w</strong></sub>), resulting in the Data in value being written to the selected register only.</p>&#13;
<div class="imagec" id="ch5fig30"><img alt="image" src="../images/05fig30.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-30: The register file: the set of CPU general-purpose registers used to store instruction operand and result values</em></p>&#13;
<h5 class="h5" id="lev3_52">Special-Purpose Registers</h5>&#13;
<p class="noindent">In addition to the set of general-purpose registers in the register file, a CPU contains special-purpose registers that store the address and content of instructions. The <em>program counter</em> (PC) stores the memory address of the next instruction to execute, and the <em>instruction register</em> (IR) stores the bits of the current instruction being executed by the CPU. The bits of the instruction stored in the IR are used as input into different parts of the CPU during the instruction’s execution. We discuss these registers in more detail in “The Processor’s Execution of Program Instructions” on <a href="ch05.xhtml#lev1_43">page 266</a>.</p>&#13;
<h4 class="h4" id="lev2_102">5.5.3 The CPU</h4>&#13;
<p class="noindent">With the ALU and register file circuits, we can build the main parts of the CPU, as shown in <a href="ch05.xhtml#ch5fig31">Figure 5-31</a>. Because instruction operands often come from values stored in general-purpose registers, the register file’s outputs <span epub:type="pagebreak" id="page_265"/>send data to the ALU’s inputs. Similarly, because instruction results are often stored in registers, the ALU’s result output is sent as input to the register file. The CPU has additional circuitry to move data between the ALU, register file, and other components (e.g., main memory).</p>&#13;
<div class="imagec" id="ch5fig31"><img alt="image" src="../images/05fig31.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-31: The ALU and register file make up the main parts of the CPU. The ALU performs operations, and the register file stores operand and result values. Additional special-purpose registers store instruction addresses (PC) and contents (IR). Note that instructions might retrieve operands from or store results to locations other than the register file (e.g., main memory).</em></p>&#13;
<p class="indent">These main parts of the CPU make up its <em>data path</em>. The data path consists of the parts of the CPU that perform arithmetic and logic operations (the ALU) and store data (registers), and the buses that connect these parts. The CPU also implements a <em>control path</em> that drives the execution of program instructions by the ALU on operands stored in the register file. Additionally, the control path issues commands to I/O devices and coordinates memory accesses as needed by instructions. For example, some instructions may get their operand values directly from (or store their results directly to) memory locations rather than general-purpose registers. In the next section, we focus our discussion of CPU instruction execution on instructions that get operand values and store results to the register file. The CPU requires additional control circuitry to read operand values or to write instruction results to other locations, but the main instruction execution steps behave the same regardless of the source and destination locations.</p>&#13;
<h3 class="h3" id="lev1_43"><span epub:type="pagebreak" id="page_266"/>5.6 The Processor’s Execution of Program Instructions</h3>&#13;
<p class="noindent">Instruction execution is performed in several stages. Different architectures implement different numbers of stages, but most implement the Fetch, Decode, Execute, and WriteBack phases of instruction execution in four or more discrete stages. In discussing instruction execution, we focus on these four stages of execution, and we use an ADD instruction as our example. Our ADD instruction example is encoded as shown in <a href="ch05.xhtml#ch5fig32">Figure 5-32</a>.</p>&#13;
<div class="imagec" id="ch5fig32"><img alt="image" src="../images/05fig32.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-32: An example instruction format for a three-register operation. The instruction is encoded in binary with subsets of its bits corresponding to encodings of different parts of the instruction: the operation (opcode), the two source registers (the operands), and the destination register for storing the result of the operation. The example shows the encoding of an ADD instruction in this format.</em></p>&#13;
<p class="indent">To execute an instruction, the CPU first <em>fetches</em> the next instruction from memory into a special-purpose register, the instruction register (IR). The memory address of the instruction to fetch is stored in another special-purpose register, the program counter (PC). The PC keeps track of the memory address of the next instruction to fetch and is incremented as part of executing the fetch stage so that it stores the value of the very next instruction’s memory address. For example, if all instructions are 32 bits long, the PC’s value is incremented by four (each byte, eight bits, has a unique address) to store the memory address of the instruction immediately following the one being fetched. Arithmetic circuits that are separate from the ALU increment the PC’s value. The PC’s value may also change during the WriteBack stage. For example, some instructions jump to specific addresses, such as those associated with the execution of loops, <code>if</code>–<code>else</code> blocks, or function calls. <a href="ch05.xhtml#ch5fig33">Figure 5-33</a> shows the fetch stage of execution.</p>&#13;
<span epub:type="pagebreak" id="page_267"/>&#13;
<div class="imagec" id="ch5fig33"><img alt="image" src="../images/05fig33.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-33: The fetch stage of instruction execution: the instruction at the memory address value stored in the PC register is read from memory and stored into the IR. The PC’s value is also incremented at the end of this stage (if instructions are four bytes, the next address is 1238; the actual instruction size varies by architecture and instruction type).</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_268"/>After fetching the instruction, the CPU <em>decodes</em> the instruction bits stored in the IR register into four parts: the high-order bits of an instruction encode the opcode, which specifies the operation to perform (e.g., ADD, SUB, OR . . . ), and the remaining bits are divided into three subsets that specify the two operand sources and the result destination. In our example, we use registers for both sources and the result destination. The opcode is sent on wires that are input to the ALU and the source bits are sent on wires that are inputs to the register file. The source bits are sent to the two read selection inputs (Sr<sub class="textsubscript">0</sub> and Sr<sub class="textsubscript">1</sub>) that specify which register values are read from the register file. The Decode stage is shown in <a href="ch05.xhtml#ch5fig34">Figure 5-34</a>.</p>&#13;
<div class="imagec" id="ch5fig34"><img alt="image" src="../images/05fig34.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-34: The Decode stage of instruction execution: separate the instruction bits in the IR into components and send them as input to the ALU and register file. The opcode bits in the IR are sent to the ALU selection input to choose which operation to perform. The two sets of operand bits in the IR are sent to the selection inputs of the register file to pick the registers from which to read the operand values. The destination bits in the IR are sent to the register file in the WriteBack stage. They specify the register to which to write the ALU result.</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_269"/>After the Decode stage determines the operation to perform and the operand sources, the ALU performs the operation in the next stage, the <em>Execution</em> stage. The ALU’s data inputs come from the two outputs of the register file, and its selection input comes from the opcode bits of the instruction. These inputs propagate through the ALU to produce a result that combines the operand values with the operation. In our example, the ALU outputs the result of adding the value stored in Reg1 to the value stored in Reg3, and outputs the condition code values associated with the result value. The Execution stage is shown in <a href="ch05.xhtml#ch5fig35">Figure 5-35</a>.</p>&#13;
<div class="imagec" id="ch5fig35"><img alt="image" src="../images/05fig35.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-35: The Execution stage of instruction execution: the ALU performs the specified operation (from the instruction opcode bits) on its input values (from the register file outputs).</em></p>&#13;
<p class="indent">In the <em>WriteBack</em> stage, the ALU result is stored in the destination register. The register file receives the ALU’s result output on its Data in input, the destination register (from instructions bits in the IR) on its write-select (S<sub class="textsubscript">w</sub>) input, and 1 on its WE input. For example, if the destination register is Reg0, then the bits encoding Reg0 in the IR are sent as the S<sub class="textsubscript">w</sub> input to the register file to pick the destination register. The output from the ALU is sent as the Data in input to the register file, and the WE bit is set to 1 to enable writing the ALU result into Reg0. The WriteBack stage is shown in <a href="ch05.xhtml#ch5fig36">Figure 5-36</a>.</p>&#13;
<span epub:type="pagebreak" id="page_270"/>&#13;
<div class="imagec" id="ch5fig36"><img alt="image" src="../images/05fig36.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-36: The WriteBack stage of instruction execution: the result of the execution stage (the output from the ALU) is written to the destination register in the register file. The ALU output is the register file’s Data in input, the destination bits of the instruction go to the register file’s write-selection input (S<sub class="textsubscript">w</sub>), and the WE input is set to 1 to enable writing the Data in value to the specified destination register.</em></p>&#13;
<h4 class="h4" id="lev2_103">5.6.1 Clock-Driven Execution</h4>&#13;
<p class="noindent">A clock drives the CPU’s execution of instructions, triggering the start of each stage. In other words, the clock is used by the CPU to determine when inputs to circuits associated with each stage are ready to be used by the circuit, and it controls when outputs from circuits represent valid results from one stage and can be used as inputs to other circuits executing the next stage.</p>&#13;
<p class="indent">A CPU clock measures discrete time as opposed to continuous time. In other words, there exists a time 0, followed by a time 1, followed by a time 2, and so on for each subsequent clock tick. A processor’s <em>clock cycle time</em> measures the time between each clock tick. A processor’s <em>clock speed</em> (or <em>clock rate</em>) is <code>1/(clock</code> <code>cycle time)</code>. It is typically measured in megahertz (MHz) or gigahertz (GHz). A 1-MHz clock rate has one million clock ticks per second, and 1GHz has one billion clock ticks per second. The clock rate is a measure of how fast the CPU can run, and is an estimate of the maximum number of instructions per second a CPU can execute. For example, on simple scalar processors like our example CPU, a 2-GHz processor might achieve a maximum instruction execution rate of two billion instructions per second (or two instructions every nanosecond).</p>&#13;
<p class="indent">Although increasing the clock rate on a single machine will improve its performance, clock rate alone is not a meaningful metric for comparing the performance of different processors. For example, some architectures (such as RISC) require fewer stages to execute instructions than others (such as <span epub:type="pagebreak" id="page_271"/>CISC). In architectures with fewer execution stages a slower clock may yield the same number of instructions completed per second as on another architecture with a faster clock rate but more execution stages. For a specific microprocessor, however, doubling its clock speed will roughly double its instruction execution speed.</p>&#13;
<div class="g-box">&#13;
<p class="box-title">CLOCK RATES AND PROCESSOR PERFORMANCE</p>&#13;
<p class="noindentt">Historically, increasing the clock rate (along with designing more complicated and powerful microarchitectures that a faster clock can drive) has been a very effective way for computer architects to improve processor performance. For example, in 1974, the Intel 8080 CPU ran at 2 MHz (a clock rate of two million cycles per second). The clock rate of the Intel Pentium Pro, introduced in 1995, was 150 MHz (150 million cycles per second), and the clock rate of the Intel Pentium 4, introduced in 2000, was 1.3 GHz or (1.3 <em>billion</em> cycles per second). Clock rates peaked in the mid to late 2000s with processors like the IBM z10, which had a clock rate of 4.4 GHz.</p>&#13;
<p class="noindentt">Today, however, CPU clock rates have reached their limit due to problems associated with handling heat dissipation of faster clocks. This limit is known as the <em>power wall</em>. The power wall resulted in the development of multicore processors starting in the mid 2000s. Multicore processors have multiple “simple” CPU cores per chip, each core driven by a clock whose rate has not increased from the previous-generation core. Multicore processor design is a way to improve CPU performance without having to increase the CPU clock rate.</p>&#13;
</div>&#13;
<h5 class="h5" id="lev3_53">The Clock Circuit</h5>&#13;
<p class="noindent">A clock circuit uses an oscillator circuit to generate a very precise and regular pulse pattern. Typically, a crystal oscillator generates the base frequency of the oscillator circuit, and the pulse pattern of the oscillator is used by the clock circuit to output a pattern of alternating high and low voltages that correspond to an alternating pattern of 1 and 0 binary values. <a href="ch05.xhtml#ch5fig37">Figure 5-37</a> shows an example clock circuit generating a regular output pattern of 1 and 0.</p>&#13;
<div class="imagec" id="ch5fig37"><img alt="image" src="../images/05fig37.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-37: The regular output pattern of 1 and 0 of a clock circuit. Each sequence of 1 and 0 makes up a clock cycle.</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_272"/>A <em>clock cycle</em> (or tick) is a 1 and 0 subsequence from the clock circuit pattern. The transition from a 1 to a 0 or a 0 to a 1 is called a <em>clock edge</em>. Clock edges trigger state changes in CPU circuits, driving the execution of instructions. The rising clock edge (the transition from 0 to 1 at the beginning of a new clock cycle) indicates a state in which input values are ready for a stage of instruction execution. For example, the rising edge transition signals that input values to the ALU circuit are ready. While the clock’s value is 1, these inputs propagate through the circuit until the output of the circuit is ready. This is called the <em>propagation delay</em> through the circuit. For example, while the clock signal is 1 the input values to the ALU propagate through the ALU operation circuits and then through the multiplexer to produce the correct output from the ALU for the operation combining the input values. On the falling edge (the transition from 1 to 0), the outputs of the stage are stable and ready to be propagated to the next location (shown as “output ready” in <a href="ch05.xhtml#ch5fig38">Figure 5-38</a>). For example, the output from the ALU is ready on the falling edge. For the duration of the clock value 0, the ALU’s output propagates to register file inputs. On the next clock cycle the rising edge indicates that the register file input value is ready to write into a register (shown as “new input” in <a href="ch05.xhtml#ch5fig38">Figure 5-38</a>).</p>&#13;
<div class="imagec" id="ch5fig38"><img alt="image" src="../images/05fig38.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-38: The rising edge of a new clock cycle triggers changes in the inputs to the circuits it controls. The falling edge triggers when the outputs are valid from the circuits it controls.</em></p>&#13;
<p class="indent">The length of the clock cycle (or the clock rate) is bounded by the longest propagation delay through any stage of instruction execution. The execution stage and propagation through the ALU is usually the longest stage. Thus, half of the clock cycle time must be no faster than the time it takes for the ALU input values to propagate through the slowest operation circuit to the ALU outputs (i.e., the outputs reflect the results of the operation on the inputs). For example, in our four-operation ALU (OR, ADD, AND, and EQUALS), the ripple carry adder circuit has the longest propagation delay and determines the minimum length of the clock cycle.</p>&#13;
<p class="indent">Because it takes one clock cycle to complete one stage of CPU instruction execution, a processor with a four-stage instruction execution sequence (Fetch, Decode, Execute, WriteBack; see <a href="ch05.xhtml#ch5fig39">Figure 5-39</a>) completes at most one instruction every four clock cycles.</p>&#13;
<span epub:type="pagebreak" id="page_273"/>&#13;
<div class="imagec" id="ch5fig39"><img alt="image" src="../images/05fig39.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-39: Four-stage instruction execution takes four clock cycles to complete.</em></p>&#13;
<p class="indent">If, for example, the clock rate is 1 GHz, one instruction takes four nanoseconds to complete (each of the four stages taking one nanosecond). With a 2-GHz clock rate, one instruction takes only two nanoseconds to complete.</p>&#13;
<p class="indent">Although clock rate is a factor in a processor’s performance, clock rate alone is not a meaningful measure of its performance. Instead, the average number of <em>cycles per instruction</em> (CPI) measured over a program’s full execution is a better measure of a CPU’s performance. Typically, a processor cannot maintain its maximum CPI for an entire program’s execution. A submaximum CPI is the result of many factors, including the execution of common program constructs that change control flow such as loops, <code>if</code>–<code>else</code> branching, and function calls. The average CPI for running a set of standard benchmark programs is used to compare different architectures. CPI is a more accurate measure of the CPU’s performance as it measures its speed executing a program versus a measure of one aspect of an individual instruction’s execution. See a computer architecture textbook<sup><a href="ch05.xhtml#fn5_20" id="rfn5_20">20</a></sup> for more details about processor performance and designing processors to improve their performance.</p>&#13;
<h4 class="h4" id="lev2_104">5.6.2 Putting It All Together: The CPU in a Full Computer</h4>&#13;
<p class="noindent">The data path (ALU, register file, and the buses that connect them) and the control path (instruction execution circuitry) make up the CPU. Together they implement the processing and control parts of the von Neumann architecture. Today’s processors are implemented as digital circuits etched into silicon chips. The processor chip also includes some fast on-chip cache memory (implemented with latch storage circuits), used to store copies of recently used program data and instructions close to the processor. See <a href="ch11.xhtml#ch11">Chapter 11</a> for more information about on-chip cache memory.</p>&#13;
<p class="indent"><a href="ch05.xhtml#ch5fig40">Figure 5-40</a> shows an example of a processor in the context of a complete modern computer, whose components together implement the von Neumann architecture.</p>&#13;
<span epub:type="pagebreak" id="page_274"/>&#13;
<div class="imagec" id="ch5fig40"><img alt="image" src="../images/05fig40.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-40: The CPU in a full modern computer. Buses connect the processor chip, main memor, and input and output devices.</em></p>&#13;
<h3 class="h3" id="lev1_44">5.7 Pipelining: Making the CPU Faster</h3>&#13;
<p class="noindent">Our four-stage CPU takes four cycles to execute one instruction: the first cycle is used to fetch the instruction from memory; the second to decode the instruction and read operands from the register file; the third for the ALU to execute the operation; and the fourth to write back the ALU result to a register in the register file. To execute a sequence of <em>N</em> instructions takes 4<em>N</em> clock cycles, as each is executed one at a time, in order, by the CPU.</p>&#13;
<div class="imagec" id="ch5fig41"><img alt="image" src="../images/05fig41.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-41: Executing three instructions takes 12 total cycles.</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_275"/><a href="ch05.xhtml#ch5fig41">Figure 5-41</a> shows three instructions taking a total of 12 cycles to execute, four cycles per instruction, resulting in a CPI of 4 (CPI is the average number of cycles to execute an instruction). However, the control circuitry of the CPU can be improved to achieve a better (lower) CPI value.</p>&#13;
<p class="indent">In considering the pattern of execution in which each instruction takes four cycles to execute, followed by the next instruction taking four cycles, and so on, the CPU circuitry associated with implementing each stage is only actively involved in instruction execution once every four cycles. For example, after the Fetch stage, the fetch circuitry in the CPU is not used to perform any useful action related to executing an instruction for the next three clock cycles. If, however, the fetch circuitry could continue to actively execute the Fetch parts of subsequent instructions in the next three cycles, the CPU could complete the execution of more than a single instruction every four cycles.</p>&#13;
<p class="indent">CPU <em>pipelining</em> is this idea of starting the execution of the next instruction before the current instruction has fully completed its execution. CPU pipelining executes instructions in order, but it allows the execution of a sequence of instructions to overlap. For example, in the first cycle, the first instruction enters its Fetch stage of execution. In the second cycle, the first instruction moves to its Decode stage, and the second instruction simultaneously enters its Fetch stage. In the third cycle, the first instruction moves to its Execution stage, the second instruction to its Decode stage, and the third instruction is fetched from memory. In the fourth cycle, the first instruction moves to its WriteBack stage and completes, the second instruction moves to its Execution stage, the third to its Decode, and the fourth instruction enters its Fetch stage. At this point, the CPU pipeline of instructions is full—every CPU stage is actively executing program instructions where each subsequent instruction is one stage behind its predecessor. When the pipeline is full, the CPU completes the execution of one instruction every clock cycle!</p>&#13;
<div class="imagec" id="ch5fig42"><img alt="image" src="../images/05fig42.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-42: Pipelining: overlapping instruction execution to achieve one instruction completed per cycle. The circle indicates that the CPU has reached the steady state of completing one instruction every cycle.</em></p>&#13;
<p class="indent"><a href="ch05.xhtml#ch5fig42">Figure 5-42</a> shows an example of pipelined instruction execution through our CPU. Starting with the fourth clock cycle the pipeline fills, meaning that the CPU completes the execution of one instruction every cycle, achieving <span epub:type="pagebreak" id="page_276"/>a CPI of 1 (shown in the circle in <a href="ch05.xhtml#ch5fig42">Figure 5-42</a>). Notice that the total number of cycles required to execute a single instruction (the instruction <em>latency</em>) has not decreased in pipelined execution—it still takes four cycles for each instruction to execute. Instead, pipelining increases instruction <em>throughput</em>, or the number of instructions that the CPU can execute in a given period of time, by overlapping the execution of sequential instructions in a staggered manner, through the different stages of the pipeline.</p>&#13;
<p class="indent">Since the 1970s, computer architects have used pipelining as a way to drastically improve the performance of microprocessors. However, pipelining comes at the cost of a more complicated CPU design than one that does not support pipelined execution. Additional storage and control circuitry is needed to support pipelining. For example, multiple instruction registers may be required to store the multiple instructions currently in the pipeline. This added complexity is almost always worth the large improvements in CPI that pipelining provides. As a result, most modern microprocessors implement pipelined execution.</p>&#13;
<p class="indent">The idea of pipelining is also used in other contexts in computer science to speed up execution, and the idea applies to many non-CS applications as well. Consider, for example, the task of doing multiple loads of laundry using a single washing machine. If completing one laundry consists of four steps (washing, drying, folding, and putting away clothes), then after washing the first load, the second load can go in the washing machine while the first load is in the dryer, overlapping the washing of individual laundry loads to speed up the total time it takes to wash four loads. Factory assembly lines are another example of pipelining.</p>&#13;
<p class="indent">In our discussion of how a CPU executes program instructions and CPU pipelining, we used a simple four-stage pipeline and an example ADD instruction. To execute instructions that load and store values between memory and registers, a five-stage pipeline is used. A five-stage pipeline includes a Memory stage for memory access: Fetch–Decode–Execute–Memory– WriteBack. Different processors may have fewer or more pipeline stages than a typical five-stage pipeline. For example, the initial ARM architecture had three stages (Fetch, Decode, and Execute, wherein the Execute stage performed both the ALU execution and the register file WriteBack functionality). More recent ARM architectures have more than five stages in their pipelines. The initial Intel Pentium architectures had a five-stage pipeline, but later architectures had significantly more pipeline stages. For example, the Intel Core i7 has a 14-stage pipeline.</p>&#13;
<h3 class="h3" id="lev1_45">5.8 Advanced Pipelined Instruction Considerations</h3>&#13;
<p class="noindent">Recall that pipelining improves the performance of a processor by overlapping the execution of multiple instructions. In our earlier discussion on pipelining, we described a simple four-stage pipeline with the basic stages of Fetch (F), Decode (D), Execute (E) and WriteBack (W). In our discussion that follows, we also consider a fifth stage, Memory (M), which represents <span epub:type="pagebreak" id="page_277"/>an access to data memory. Our five-stage pipeline therefore comprises the following stages:</p>&#13;
<ul>&#13;
<li class="noindent">Fetch (F): reads an instruction from memory (pointed to by the program counter).</li>&#13;
<li class="noindent">Decode (D): reads source registers and sets control logic.</li>&#13;
<li class="noindent">Execute (E): executes the instruction.</li>&#13;
<li class="noindent">Memory (M): reads from or writes to data memory.</li>&#13;
<li class="noindent">WriteBack (W): stores a result in a destination register.</li>&#13;
</ul>&#13;
<p class="indent">Recall that the compiler transforms lines of code into a series of machine code instructions for the CPU to execute. Assembly code is a human-readable version of machine code. The snippet below displays a series of made-up assembly instructions:</p>&#13;
<pre>MOV M[0x84], Reg1     # move value at memory address 0x84 to register Reg1<br/>&#13;
ADD 2, Reg1, Reg1     # add 2 to value in Reg1 and store result in Reg1<br/>&#13;
MOV 4, Reg2           # copy the value 4 to register Reg2<br/>&#13;
ADD Reg2, Reg2, Reg2  # compute Reg2 + Reg2, store result in Reg2<br/>&#13;
JMP L1&lt;0x14&gt;          # jump to executing code at L1 (code address 0x14)</pre>&#13;
<p class="indent">Don’t worry if you are having trouble parsing the snippet—we cover assembly in greater detail in <a href="ch07.xhtml#ch07">Chapter 7</a>. For now, it suffices to focus on the following set of facts:</p>&#13;
<ul>&#13;
<li class="noindent">Every ISA defines a set of instructions.</li>&#13;
<li class="noindent">Each instruction operates on one or more operands (that is, registers, memory, or constant values).</li>&#13;
<li class="noindent">Not all instructions require the same number of pipeline stages to execute.</li>&#13;
</ul>&#13;
<p class="indent">In our previous discussion, it was assumed that every instruction takes the same number of cycles to execute; however, this is usually not the case. For example, the first <code>MOV</code> instruction requires all five stages, as it requires the movement of data from memory to a register. In contrast, the next three instructions require only four stages (F, D, E, W) to execute given that the operations involve only registers, and not memory. The last instruction (<code>JMP</code>) is a type of <em>branch</em> or <em>conditional</em> instruction. Its purpose is to transfer the flow of control to another part of the code. Specifically, addresses in the code region of memory reference different <em>instructions</em> in an executable. Since the <code>JMP</code> instruction does not update a general-purpose register, the WriteBack stage is omitted, resulting in only three stages (F, D, E) being required. We cover conditional instructions in greater detail in “Conditional Control and Loops” on <a href="ch07.xhtml#lev1_54">page 310</a>.</p>&#13;
<p class="indent">A <em>pipeline stall</em> results when any instruction is forced to wait for another to finish executing before it can continue. Compilers and processors do whatever they can to avoid pipeline stalls in order to maximize performance.</p>&#13;
<h4 class="h4" id="lev2_105"><span epub:type="pagebreak" id="page_278"/>5.8.1 Pipelining Consideration: Data Hazards</h4>&#13;
<p class="noindent">A <em>data hazard</em> occurs when two instructions attempt to access common data in an instruction pipeline. As an example, consider the first pair of instructions from the previous code snippet:</p>&#13;
<pre>MOV M[0x84], Reg1     # move value at memory address 0x84 to register Reg1<br/>&#13;
ADD 2, Reg1, Reg1     # add 2 to value in Reg1 and store result in Reg1</pre>&#13;
<div class="imagec" id="ch5fig43"><img alt="image" src="../images/05fig43.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-43: An example of a pipeline hazard arising from two instructions simultaneously reaching the same pipeline stage</em></p>&#13;
<p class="indent">Recall that this <code>MOV</code> instruction requires five stages (as it involves an access to memory), whereas the <code>ADD</code> instruction requires only four. In this scenario, both instructions will attempt to write to register <code>Reg1</code> at the same time (see <a href="ch05.xhtml#ch5fig43">Figure 5-43</a>).</p>&#13;
<p class="indent">The processor prevents the aforementioned scenario by first forcing every instruction to take five pipeline stages to execute. For instructions that normally take fewer than five stages, the CPU adds a “no-operation” (<code>NOP</code>) instruction (also called a pipeline “bubble”) to substitute for that phase.</p>&#13;
<p class="indent">However, the problem is still not fully resolved. Since the goal of the second instruction is to add <code>2</code> to the value stored in register <code>Reg1</code>, the <code>MOV</code> instruction needs to finish <em>writing</em> to register <code>Reg1</code> before the <code>ADD</code> instruction can execute correctly. A similar problem exists in the next two instructions:</p>&#13;
<pre>MOV 4, Reg2           # copy the value 4 to register Reg2<br/>&#13;
ADD Reg2, Reg2, Reg2  # compute Reg2 + Reg2, store result in Reg2</pre>&#13;
<div class="imagec" id="ch5fig44"><img alt="image" src="../images/05fig44.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-44: The processor can reduce the damage caused by pipeline hazards by forwarding operands between instructions.</em></p>&#13;
<p class="indent">These two instructions load the value <code>4</code> into register <code>Reg2</code> and then multiply it by 2 (by adding to itself). Once again, bubbles are added to enforce that each instruction takes five pipeline stages. In this case, regardless of the bubbles, the second instruction’s execute phase occurs <em>before</em> the first instruction finishes writing the required value (<code>4</code>) to register <code>Reg2</code>.</p>&#13;
<p class="indent">Adding more bubbles is a suboptimal solution because it stalls the pipeline. Instead, processors employ a technique called <em>operand forwarding</em>, in <span epub:type="pagebreak" id="page_279"/>which the pipeline reads the result from the previous operation. Looking at <a href="ch05.xhtml#ch5fig44">Figure 5-44</a>, while the instruction <code>MOV 4, Reg2</code> executes, it forwards its results to the instruction <code>ADD Reg2, Reg2, Reg2</code>. So, while the <code>MOV</code> instruction is writing to register <code>Reg2</code>, the <code>ADD</code> instruction can use the updated value of <code>Reg2</code> that it received from the <code>MOV</code> instruction.</p>&#13;
<h4 class="h4" id="lev2_106">5.8.2 Pipelining Hazards: Control Hazards</h4>&#13;
<p class="noindent">The pipeline is optimized for instructions that occur one after another. Control changes in a program arising from conditionals such as <code>if</code> statements or loops can seriously affect the pipeline performance. Let’s take a look at a different example code snippet, first in C:</p>&#13;
<pre>int result = *x; // x holds an int<br/>&#13;
int temp = *y;   // y holds another int<br/>&#13;
<br/>&#13;
if (result &lt;= temp) {<br/>&#13;
   result = result - temp;<br/>&#13;
}<br/>&#13;
else {<br/>&#13;
   result = result + temp;<br/>&#13;
}<br/>&#13;
return result;</pre>&#13;
<p class="indent">This snippet simply reads integer data from two different pointers, compares the values, and then does different arithmetic based on the result. Here is how the preceding code snippet may translate into assembly instructions:</p>&#13;
<pre>  MOV M[0x84], Reg1     # move value at memory address 0x84 to register Reg1<br/>&#13;
  MOV M[0x88], Reg2     # move value at memory address 0x88 to register Reg2<br/>&#13;
  CMP Reg1, Reg2        # compare value in Reg1 to value in Reg2<br/>&#13;
  JLE L1&lt;0x14&gt;          # switch code execution to L1 if Reg1 less than Reg2<br/>&#13;
  ADD Reg1, Reg2, Reg1  # compute Reg1 + Reg2, store result in Reg1<br/>&#13;
  JMP L2&lt;0x20&gt;          # switch code execution to L2 (code address 0x20)<br/>&#13;
L1:<br/>&#13;
  SUB Reg1, Reg2, Reg1  # compute Reg1 - Reg2, store in Reg1<br/>&#13;
L2:<br/>&#13;
  RET                   # return from function</pre>&#13;
<p class="indent">This sequence of instructions loads data from memory into two separate registers, compares the values, and then does different arithmetic based on whether the value in the first register is less than the value in the second. The <code>if</code> statement is represented in this example with two instructions: the compare (<code>CMP</code>) instruction and a conditional jump less than (<code>JLE</code>) instruction. We cover conditional instructions in greater detail in “Conditional Control and Loops” on <a href="ch07.xhtml#lev1_54">page 310</a>; for now, it is sufficient to understand that the <code>CMP</code> instruction <em>compares</em> two registers, while the <code>JLE</code> instruction is a special type of branch instruction that switches code execution to another part of the <span epub:type="pagebreak" id="page_280"/>program <em>if and only if</em> the condition (i.e., less than or equal, in this case) is true.</p>&#13;
<p class="note"><strong><span class="black">Warning</span> DON’T GET OVERWHELMED BY THE DETAILS!</strong></p>&#13;
<p class="note-w">Looking at assembly for the first time can be understandably intimidating. If this is how you feel, try not to worry! We cover assembly in much greater detail in <a href="ch07.xhtml#ch07">Chapter 7</a>. The key takeaway is that code containing conditional statements translates to a series of assembly instructions just like any other code snippet. However, unlike other code snippets, conditional statements are <em>not</em> guaranteed to execute in a particular way. The uncertainty surrounding how a conditional statement executes has large ramifications for the pipeline.</p>&#13;
<div class="imagec" id="ch5fig45"><img alt="image" src="../images/05fig45.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-45: An example of a control hazard resulting from a conditional branch</em></p>&#13;
<p class="indent">A <em>control hazard</em> occurs when the pipeline encounters a branch (or conditional) instruction. When this happens, the pipeline has to “guess” whether the branch will be taken. If the branch is not taken, the process continues to execute the next instructions in sequence. Consider the example in <a href="ch05.xhtml#ch5fig45">Figure 5-45</a>. If the branch is taken, the next instruction that executes should be the <code>SUB</code> instruction. However, it is impossible to know whether the branch is taken until the <code>JLE</code> instruction finishes executing. At that point, the <code>ADD</code> and <code>JMP</code> instructions have already been loaded into the pipeline. If the branch <em>is</em> taken, these “junk” instructions in the pipeline need to be removed, or <em>flushed</em>, before the pipeline can be reloaded with new instructions. Flushing the pipeline is expensive.</p>&#13;
<p class="indent">There are a few options that hardware engineers can choose to implement to help the processor deal with control hazards:</p>&#13;
<ul>&#13;
<li class="noindent"><strong>Stall the pipeline</strong>: As a simple solution, whenever there is a branch, add lots of <code>NOP</code> bubbles and stall the pipeline until the processor is sure that the branch is taken. Although stalling the pipeline will fix the issue, it will also lead to a performance hit (see <a href="ch05.xhtml#ch5fig46">Figure 5-46</a>).</li>&#13;
<li class="noindent"><strong>Branch prediction</strong>: The most common solution is to use a <em>branch predictor</em>, which will predict which way a branch will go, based on previous executions. Modern branch predictors are really good and accurate. However, this approach has recently caused some security vulnerabilities (e.g. Spectre<sup><a href="ch05.xhtml#fn5_21" id="rfn5_21">21</a></sup>). <a href="ch05.xhtml#ch5fig46">Figure 5-46</a> depicts how a branch predictor may deal with the control hazard discussed.</li>&#13;
<li class="noindent"><strong>Eager execution</strong>: In eager execution, the CPU executes both sides of the branch and performs a conditional transfer of data rather <span epub:type="pagebreak" id="page_281"/>than control (implemented through the <code>cmov</code> and <code>csel</code> instructions in x86 and ARMv8-A, respectively). A conditional transfer of data enables the processor to continue execution without disrupting the pipeline. However, not all code is capable of taking advantage of eager execution, which can be dangerous in the case of pointer dereferences and side effects.</li>&#13;
</ul>&#13;
<div class="imagec" id="ch5fig46"><img alt="image" src="../images/05fig46.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-46: Potential solutions for handling control hazards</em></p>&#13;
<h3 class="h3" id="lev1_46">5.9 Looking Ahead: CPUs Today</h3>&#13;
<p class="noindent">CPU pipelining is one example of <em>instruction-level parallelism</em> (ILP), in which the CPU simultaneously executes multiple instructions in parallel. In a pipelined execution, the CPU simultaneously executes multiple instructions by overlapping their execution in the pipeline. A simple pipelined CPU can achieve a CPI of 1, completing the execution of one instruction every clock cycle. Modern microprocessors typically employ pipelining along with other ILP techniques and include multiple CPU cores to achieve processor CPI values of less than 1. For these microarchitectures, the average number of <em>instructions per cycle</em> (IPC) is the metric commonly used to describe their performance. A large IPC value indicates that a processor achieves a high sustained degree of simultaneous instruction execution.</p>&#13;
<p class="indent">Transistors are the building blocks of all circuitry on an integrated circuit (a chip). The processing and control units of modern CPUs are constructed from circuits, which are built from subcircuits and basic logic gates that are implemented with transistors. Transistors also implement the storage circuits used in CPU registers and in fast on-chip cache memory that stores copies of recently accessed data and instructions (we discuss cache memory in detail in <a href="ch11.xhtml#ch11">Chapter 11</a>.</p>&#13;
<p class="indent">The number of transistors that can fit on a chip is a rough measure of its performance. <em>Moore’s Law</em> is the observation, made by Gordon Moore in 1975, that the number of transistors per integrated circuit doubles about every two years.<sup><a href="ch05.xhtml#fn5_22" id="rfn5_22">22</a></sup> A doubling in the number of transistors per chip every two years means that computer architects can design a new chip with twice as much space for storage and computation circuitry, roughly doubling its power. Historically, computer architects used the extra transistors to design <span epub:type="pagebreak" id="page_282"/>more complex single processors using ILP techniques to improve overall performance.</p>&#13;
<h4 class="h4" id="lev2_107">5.9.1 Instruction-Level Parallelism</h4>&#13;
<p class="noindent">Instruction level parallelism (ILP) is a term for a set of design techniques used to support parallel execution of a single program’s instructions on a single processor. ILP techniques are transparent to the programmer, meaning that a programmer writes a sequential C program but the processor executes several of its instructions simultaneously, in parallel, on one or more execution units. Pipelining is one example of ILP, where a sequence of program instructions execute simultaneously, each in a different pipeline stage. A pipelined processor can execute one instruction per cycle (can achieve an IPC of 1). Other types of microprocessor ILP designs can execute more than a single instruction per clock cycle and achieve IPC values higher than 1.</p>&#13;
<p class="indent">A <em>vector processor</em> is an architecture that implements ILP through special vector instructions that take one-dimensional arrays (vectors) of data as their operands. Vector instructions are executed in parallel by a vector processor on multiple execution units, each unit performing an arithmetic operation on single elements of its vector operands. In the past, vector processors were often used in large parallel computers. The 1976 Cray-1 was the first vector processor–based supercomputer, and Cray continued to design its supercomputers with vector processors throughout the 1990s. However, eventually this design could not compete with other parallel supercomputer designs, and today vector processors appear primarily in accelerator devices such as graphics processing units (GPUs) that are particularly optimized for performing computation on image data stored in 1D arrays.</p>&#13;
<p class="indent"><em>Superscalar</em> is another example of an ILP processor design. A superscalar processor is a single processor with multiple execution units and multiple execution pipelines. A superscalar processor fetches a set of instructions from a sequential program’s instruction stream, and breaks them up into multiple independent streams of instructions that are executed in parallel by its execution units. A superscalar processor is an <em>out-of-order processor</em>, or one that executes instructions out of the order in which they appear in a sequential instruction stream. Out-of-order execution requires identifying sequences of instructions without dependencies that can safely execute in parallel. A superscalar processor contains functionality to dynamically create the multiple streams of independent instructions to feed through its multiple execution units. This functionality must perform dependency analysis to ensure the correct ordering of any instruction whose execution depends on the result of a previous instruction in these sequential streams. As an example, a superscalar processor with five pipelined execution units can execute five instructions from a sequential program in a single cycle (can achieve an IPC of 5). However, due to instruction dependencies, it is not always the case that a superscalar processor can keep all of its pipelines full.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_283"/><em>Very long instruction word</em> (VLIW) is another ILP microarchitecture design that is similar to superscalar. In VLIW architectures, however, the compiler is responsible for constructing the multiple independent instruction streams executed in parallel by the processor. A compiler for a VLIW architecture analyzes the program instructions to statically construct a VLIW instruction that consists of multiple instructions, one from each independent stream. VLIW leads to simpler processor design than superscalar because the VLIW processor does not need to perform dependency analysis to construct the multiple independent instruction streams as part of its execution of program instructions. Instead, a VLIW processor just needs added circuitry to fetch the next VLIW instruction and break it up into its multiple instructions that it feeds into each of its execution pipelines. However, by pushing dependency analysis to the compiler, VLIW architectures require specialized compilers to achieve good performance.</p>&#13;
<p class="indent">One problem with both superscalar and VLIW is that the degree of parallel performance is often significantly limited by the sequential application programs they execute. Dependencies between instructions in the program limit the ability to keep all of the pipelines full.</p>&#13;
<h4 class="h4" id="lev2_108">5.9.2 Multicore and Hardware Multithreading</h4>&#13;
<p class="noindent">By designing single processors that employed increasingly complicated ILP techniques and increasing the CPU clock speed to drive this increasingly complicated functionality, computer architects designed processors whose performance kept pace with Moore’s Law until the early 2000s. After this time, CPU clock speeds could no longer increase without greatly increasing a processor’s power consumption.<sup><a href="ch05.xhtml#fn5_23" id="rfn5_23">23</a></sup> This led to the current era of multicore and multithreaded microarchitectures, both of which require <em>explicit parallel programming</em> by a programmer to speed up the execution of a single program.</p>&#13;
<p class="indent"><em>Hardware multithreading</em> is a single-processor design that supports executing multiple hardware threads. A <em>thread</em> is an independent stream of execution. For example, two running programs each have their own thread of independent execution. These two programs’ threads of execution could then be scheduled by the operating system to run “at the same time” on a multithreaded processor. Hardware multithreading may be implemented by a processor alternating between executing instructions from each of its threads’ instruction streams each cycle. In this case, the instructions of different hardware threads are not all executed simultaneously each cycle. Instead, the processor is designed to quickly switch between executing instructions from different threads’ execution streams. This usually results in a speed-up of their execution as a whole as compared to their execution on a singly threaded processor.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_284"/>Multithreading can be implemented in hardware on either scalar- or superscalar-type microprocessors. At a minimum, the hardware needs to support fetching instructions from multiple separate instruction streams (one for each thread of execution), and have separate register sets for each thread’s execution stream. These architectures are <em>explicitly multithreaded</em><sup><a href="ch05.xhtml#fn5_24" id="rfn5_24">24</a></sup> because, unlike superscalar architectures, each of the execution streams is independently scheduled by the operating system to run a separate logical sequence of program instructions. The multiple execution streams could come from multiple sequential programs or from multiple software threads from a single multithreaded parallel program (we discuss multithreaded parallel programming in section 14.1).</p>&#13;
<p class="indent">Hardware multithreaded microarchitectures that are based on superscalar processors have multiple pipelines and multiple execution units, and thus they can execute instructions from several hardware threads simultaneously, in parallel, resulting in an IPC value greater than 1. Multithreaded architectures based on simple scalar processors implement <em>interleaved multithreading</em>. These microarchitectures typically share a pipeline and always share the processor’s single ALU (the CPU switches between executing different threads on the ALU). This type of multithreading cannot achieve IPC values greater than 1. Hardware threading supported by superscalar-based microarchitectures is often called <em>simultaneous multithreading</em> (SMT).<sup><a href="ch05.xhtml#fn5_25" id="rfn5_25">25</a></sup> Unfortunately, SMT is often used to refer to both types of hardware multithreading, and the term alone is not always sufficient to determine whether a multithreaded microarchitecture implements true simultaneous or interleaved multithreading.</p>&#13;
<p class="indent"><em>Multicore processors</em> contain multiple complete CPU cores. Like multithreaded processors, each core is independently scheduled by the OS. However, each core of a multicore processor is a full CPU core, one that contains its own complete and separate functionality to execute program instructions. A multicore processor contains replicas of these CPU cores with some additional hardware support for the cores to share cached data. Each core of a multicore processor could be scalar, superscalar, or hardware multithreaded. <a href="ch05.xhtml#ch5fig47">Figure 5-47</a> shows an example of a multicore computer.</p>&#13;
<span epub:type="pagebreak" id="page_285"/>&#13;
<div class="imagec" id="ch5fig47"><img alt="image" src="../images/05fig47.jpg"/></div>&#13;
<p class="figcap"><em>Figure 5-47: A computer with a multicore processor. The processor contains multiple complete CPU cores, each with its own private cache memory. The cores communicate with each and share a larger shared cached memory via on-chip buses.</em></p>&#13;
<p class="indent">Multicore microprocessor design is the primary way in which the performance of processor architectures can continue to keep pace with Moore’s Law without increasing the processor clock rate. A multicore computer can simultaneously run several sequential programs, the OS scheduling each core with a different program’s instruction stream. It can speed up execution of a single program if the program is written as an explicitly multithreaded (software-level threads) parallel program. For example, the OS can schedule the threads of an individual program to run simultaneously on individual cores of the multicore processor, speeding up the execution of the program compared to its execution of a sequential version of the same program. In <a href="ch14.xhtml#ch14">Chapter 14</a>, we discuss explicit multithreaded parallel programming for multicore and other types of parallel systems with shared main memory.</p>&#13;
<h4 class="h4" id="lev2_109"><span epub:type="pagebreak" id="page_286"/>5.9.3 Some Example Processors</h4>&#13;
<p class="noindent">Today, processors are built using a mix of ILP, hardware multithreading, and multicore technologies. In fact, it is difficult to find a processor that is not multicore. Desktop-class processors typically have two to eight cores, many of which also support a low level of per-core multithreading. For example, AMD Zen multicore processors<sup><a href="ch05.xhtml#fn5_26" id="rfn5_26">26</a></sup> and Intel’s hyperthreaded multicore Xeon and Core processors<sup><a href="ch05.xhtml#fn5_27" id="rfn5_27">27</a></sup> both support two hardware threads per core. Intel’s hyperthreaded cores implement interleaved multithreading. Thus, each of its cores can only achieve an IPC of 1, but with multiple CPU cores per chip, the processor can achieve higher IPC levels.</p>&#13;
<p class="indent">Processors designed for high-end systems, such as those used in servers and supercomputers, contain many cores, where each core has a high degree of multithreading. For example, Oracle’s SPARC M7 processor,<sup><a href="ch05.xhtml#fn5_28" id="rfn5_28">28</a></sup> used in high-end servers, has 32 cores. Each of its cores has eight hardware threads, two of which can execute simultaneously, resulting in a maximum IPC value of 64 for the processor. The two fastest supercomputers in the world (as of June 2019)<sup><a href="ch05.xhtml#fn5_29" id="rfn5_29">29</a></sup> use IBM’s Power 9 processors.<sup><a href="ch05.xhtml#fn5_30" id="rfn5_30">30</a></sup> Power 9 processors have up to 24 cores per chip, and each core supports up to eight-way simultaneous multithreading. A 24-core version of the Power 9 processor can achieve an IPC of 192.</p>&#13;
<h3 class="h3" id="lev1_47">5.10 Summary</h3>&#13;
<p class="noindent">In this chapter, we presented the computer’s architecture, focusing on its processor (CPU) design and implementation in order to understand how it runs a program. Today’s modern processors are based on the von Neumann architecture, which defines a stored-program, universal computer. The general-purpose design of the von Neumann architecture allows it to execute any type of program.</p>&#13;
<p class="indent">To understand how the CPU executes program instructions, we built an example CPU, starting with basic logic-gate building blocks to create circuits that together implement a digital processor. A digital processor’s functionality is built by combining control, storage, and arithmetic/logic circuits, and is run by a clock circuit that drives the Fetch, Decode, Execute, and WriteBack phases of its execution of program instructions.</p>&#13;
<p class="indent">All processor architectures implement an instruction set architecture (ISA) that defines the set of CPU instructions, the set of CPU registers, and the effects of executing instructions on the state of the processor. There are many different ISAs, and there are often different microprocessor implementations of a given ISA. Today’s microprocessors also use a variety of techniques to improve processor performance, including pipelined execution, instruction-level parallelism, and multicore design.</p>&#13;
<p class="indent">For more breadth and depth of coverage on computer architecture, we recommend reading a computer architecture textbook.<sup><a href="ch05.xhtml#fn5_31" id="rfn5_31">31</a></sup></p>&#13;
<h3 class="h3" id="lev1_48"><span epub:type="pagebreak" id="page_287"/>Notes</h3>&#13;
<p class="fnote"><a href="ch05.xhtml#rfn5_1" id="fn5_1">1.</a> “ACM A. M. Turing award winners,” <em><a href="https://amturing.acm.org/">https://amturing.acm.org/</a></em></p>&#13;
<p class="fnote"><a href="ch05.xhtml#rfn5_2" id="fn5_2">2.</a> “Pioneers of modern computer architecture receive ACM A. M. Turing award,” ACM Media Center Notice, March 2018, <em><a href="https://www.acm.org/media-center/2018/march/turing-award-2017">https://www.acm.org/media-center/2018/march/turing-award-2017</a></em></p>&#13;
<p class="fnote"><a href="ch05.xhtml#rfn5_3" id="fn5_3">3.</a> David Alan Grier, <em>When Computers Were Human</em>, Princeton University Press, 2005.</p>&#13;
<p class="fnote"><a href="ch05.xhtml#rfn5_4" id="fn5_4">4.</a> Megan Garber, “Computing power used to be measured in <em>kilo-girls</em>,” <em>The Atlantic</em>, October 16, 2013. <em><a href="https://www.theatlantic.com/technology/archive/2013/10/computing-power-used-to-be-measured-in-kilo-girls/280633/">https://www.theatlantic.com/technology/archive/2013/10/computing-power-used-to-be-measured-in-kilo-girls/280633/</a></em></p>&#13;
<p class="fnote"><a href="ch05.xhtml#rfn5_5" id="fn5_5">5.</a> Betty Alexandra Toole, <em>Ada, The Enchantress of Numbers</em>, Strawberry Press, 1998.</p>&#13;
<p class="fnote"><a href="ch05.xhtml#rfn5_6" id="fn5_6">6.</a> George Dyson, <em>Turing’s Cathedral: The Origins of the Digital Universe</em>, Pantheon, 2012.</p>&#13;
<p class="fnote"><a href="ch05.xhtml#rfn5_7" id="fn5_7">7.</a> Walter Isaacson, <em>The Innovators: How a Group of Inventors, Hackers, Genius and Geeks Created the Digital Revolution</em>, Simon and Schuster, 2014.</p>&#13;
<p class="fnote"><a href="ch05.xhtml#rfn5_8" id="fn5_8">8.</a> Alan M. Turing, “On computable numbers, with an application to the <em>Entscheidungsproblem</em>,” <em>Proceedings of the London Mathematical Society</em> 2(1), pp. 230–265, 1937.</p>&#13;
<p class="fnote"><a href="ch05.xhtml#rfn5_9" id="fn5_9">9.</a> Brian Carpenter and Robert Doran, “The other Turing machine,” <em>The Computer Journal</em> 20(3), pp. 269–279, 1977.</p>&#13;
<p class="fnote1"><a href="ch05.xhtml#rfn5_10" id="fn5_10">10.</a> James A. Reeds, Whitfield Diffie, and J. V. Field (eds), <em>Breaking Teleprinter Ciphers at Bletchley Park: General Report on Tunny with Emphasis on Statistical Methods (1945)</em>, Wiley, 2015.</p>&#13;
<p class="fnote1"><a href="ch05.xhtml#rfn5_11" id="fn5_11">11.</a> Jack Copeland et al., <em>Colossus: The Secrets of Bletchley Park’s Code-Breaking Computers</em>, OUP, 2010.</p>&#13;
<p class="fnote1"><a href="ch05.xhtml#rfn5_12" id="fn5_12">12.</a> Janet Abbate, <em>Recoding Gender</em>, MIT Press, 2012.</p>&#13;
<p class="fnote1"><a href="ch05.xhtml#rfn5_13" id="fn5_13">13.</a> Walter Isaacson, <em>The Innovators: How a Group of Inventors, Hackers, Genius and Geeks Created the Digital Revolution</em>, Simon and Schuster, 2014.</p>&#13;
<p class="fnote1"><a href="ch05.xhtml#rfn5_14" id="fn5_14">14.</a> Janet Abbate, <em>Recoding Gender</em>, MIT Press, 2012.</p>&#13;
<p class="fnote1"><a href="ch05.xhtml#rfn5_15" id="fn5_15">15.</a> LeAnn Erickson, <em>Top Secret Rosies: The Female Computers of World War II</em>, Public Broadcasting System, 2010.</p>&#13;
<p class="fnote1"><a href="ch05.xhtml#rfn5_16" id="fn5_16">16.</a> Kathy Kleiman, The Computers, <em><a href="http://eniacprogrammers.org/">http://eniacprogrammers.org/</a></em></p>&#13;
<p class="fnote1"><a href="ch05.xhtml#rfn5_17" id="fn5_17">17.</a> John von Neumann, “First draft of a report on the EDVAC (1945).” Reprinted in <em>IEEE Annals of the History of Computing</em> 4, pp. 27–75, 1993.</p>&#13;
<p class="fnote1"><a href="ch05.xhtml#rfn5_18" id="fn5_18">18.</a> John von Neumann, “First draft of a report on the EDVAC (1945).” Reprinted in <em>IEEE Annals of the History of Computing</em> 4, pp. 27–75, 1993.</p>&#13;
<p class="fnote1"><a href="ch05.xhtml#rfn5_19" id="fn5_19">19.</a> Walter Isaacson, <em>The Innovators: How a Group of Inventors, Hackers, Genius and Geeks Created the Digital Revolution</em>, Simon and Schuster, 2014.</p>&#13;
<p class="fnote1"><a href="ch05.xhtml#rfn5_20" id="fn5_20">20.</a> One suggestion is John Hennessy and David Patterson, <em>Computer Architecture: A Quantitative Approach</em>, Morgan Kaufmann, 2011.</p>&#13;
<p class="fnote1"><a href="ch05.xhtml#rfn5_21" id="fn5_21">21.</a> Peter Bright, “Google: Software is never going to be able to fix Spectre-type bugs,” <em>Ars Technica</em>, 2019.</p>&#13;
<p class="fnote1"><a href="ch05.xhtml#rfn5_22" id="fn5_22">22.</a> Moore first observed a doubling every year in 1965; he then updated this in 1975 to every &gt; 2 years, which became known as Moore’s Law. Moore’s Law held until around 2012 when improvements in transistor density began to slow. Moore predicted the end of Moore’s Law in the mid 2020s.</p>&#13;
<span epub:type="pagebreak" id="page_288"/>&#13;
<p class="fnote1"><a href="ch05.xhtml#rfn5_23" id="fn5_23">23.</a> Adrian McMenamin, “The end of Dennard scaling,” <em><a href="https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/">https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/</a></em></p>&#13;
<p class="fnote1"><a href="ch05.xhtml#rfn5_24" id="fn5_24">24.</a> T. Ungerer, B. Robic, and J. Silc, “A survey of processors with explicit multi-threading,” <em>ACM Computing Surveys</em> 35(1), pp. 29–63, 2003.</p>&#13;
<p class="fnote1"><a href="ch05.xhtml#rfn5_25" id="fn5_25">25.</a> T. Ungerer, B. Robic, and J. Silc, “A survey of processors with explicit multi-threading,” <em>ACM Computing Surveys</em> 35(1), pp. 29–63, 2003.</p>&#13;
<p class="fnote1"><a href="ch05.xhtml#rfn5_26" id="fn5_26">26.</a> <em><a href="https://www.amd.com/en/technologies/zen-core">https://www.amd.com/en/technologies/zen-core</a></em></p>&#13;
<p class="fnote1"><a href="ch05.xhtml#rfn5_27" id="fn5_27">27.</a> <em><a href="https://www.intel.com/content/www/us/en/architecture-and-technology/hyper-threading-technology.html">https://www.intel.com/content/www/us/en/architecture-and-technology/hyper-threading-technology.html</a></em></p>&#13;
<p class="fnote1"><a href="ch05.xhtml#rfn5_28" id="fn5_28">28.</a> <em><a href="https://web.archive.org/web/20190819165804/http://www.oracle.com/us/products/servers-storage/sparc-m7-processor-ds-2687041.pdf">https://web.archive.org/web/20190819165804/http://www.oracle.com/us/products/servers-storage/sparc-m7-processor-ds-2687041.pdf</a></em></p>&#13;
<p class="fnote1"><a href="ch05.xhtml#rfn5_29" id="fn5_29">29.</a> <em><a href="https://www.top500.org/lists/top500/">https://www.top500.org/lists/top500/</a></em></p>&#13;
<p class="fnote1"><a href="ch05.xhtml#rfn5_30" id="fn5_30">30.</a> <em><a href="https://www.ibm.com/it-infrastructure/power/power9">https://www.ibm.com/it-infrastructure/power/power9</a></em></p>&#13;
<p class="fnote1"><a href="ch05.xhtml#rfn5_31" id="fn5_31">31.</a> One suggestion is David A. Patterson and John L. Hennessy, <em>Computer Organization and Design: The Hardware and Software Interface</em>, Morgan Kaufmann, 2010.</p>&#13;
</body></html>