<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><h2 class="h2" id="ch27"><span epub:type="pagebreak" id="page_179"/><strong><span class="big">27</span><br/>PROPER METRICS</strong></h2>&#13;
<div class="image1"><img src="../images/common.jpg" alt="Image" width="252" height="252"/></div>&#13;
<p class="noindent">What are the three properties of a distance function that make it a <em>proper</em> metric?</p>&#13;
<p class="indent">Metrics are foundational to mathematics, computer science, and various other scientific domains. Understanding the fundamental properties that define a good distance function to measure distances or differences between points or datasets is important. For instance, when dealing with functions like loss functions in neural networks, understanding whether they behave like proper metrics can be instrumental in knowing how optimization algorithms will converge to a solution.</p>&#13;
<p class="indent">This chapter analyzes two commonly utilized loss functions, the mean squared error and the cross-entropy loss, to demonstrate whether they meet the criteria for proper metrics.</p>&#13;
<h3 class="h3" id="ch00lev136"><strong>The Criteria</strong></h3>&#13;
<p class="noindent">To illustrate the criteria of a proper metric, consider two vectors or points <strong>v</strong> and <strong>w</strong> and their distance <em>d</em>(<strong>v</strong>, <strong>w</strong>), as shown in <a href="ch27.xhtml#ch27fig1">Figure 27-1</a>.</p>&#13;
<div class="image"><img id="ch27fig1" src="../images/27fig01.jpg" alt="Image" width="600" height="312"/></div>&#13;
<p class="figcap"><em>Figure 27-1: The Euclidean distance between two 2D vectors</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_180"/>The criteria of a proper metric are the following:</p>&#13;
<ul>&#13;
<li class="noindent">The distance between two points is always non-negative, <em>d</em>(<strong>v</strong>, <strong>w</strong>) <em>≥</em> 0, and can be 0 only if the two points are identical, that is, <strong>v</strong> = <strong>w</strong>.</li>&#13;
<li class="noindent">The distance is symmetric; for instance, <em>d</em>(<strong>v</strong>, <strong>w</strong>) = <em>d</em>(<strong>w</strong>, <strong>v</strong>).</li>&#13;
<li class="noindent">The distance function satisfies the <em>triangle inequality</em> for any three points: <strong>v</strong>, <strong>w</strong>, <strong>x</strong>, meaning <em>d</em>(<strong>v</strong>, <strong>w</strong>) <em>≤ d</em>(<strong>v</strong>, <strong>x</strong>) + <em>d</em>(<strong>x</strong>, <strong>w</strong>).</li>&#13;
</ul>&#13;
<p class="indent">To better understand the triangle inequality, think of the points as vertices of a triangle. If we consider any triangle, the sum of two of the sides is always larger than the third side, as illustrated in <a href="ch27.xhtml#ch27fig2">Figure 27-2</a>.</p>&#13;
<div class="image"><img id="ch27fig2" src="../images/27fig02.jpg" alt="Image" width="419" height="206"/></div>&#13;
<p class="figcap"><em>Figure 27-2: Triangle inequality</em></p>&#13;
<p class="indent">Consider what would happen if the triangle inequality depicted in <a href="ch27.xhtml#ch27fig2">Figure 27-2</a> weren’t true. If the sum of the lengths of sides AB and BC was shorter than AC, then sides AB and BC would not meet to form a triangle; instead, they would fall short of each other. Thus, the fact that they meet and form a triangle demonstrates the triangle inequality.</p>&#13;
<h3 class="h3" id="ch00lev137"><strong>The Mean Squared Error</strong></h3>&#13;
<p class="noindent">The <em>mean squared error (MSE)</em> loss computes the squared Euclidean distance between a target variable <em>y</em> and a predicted target value <em>ŷ</em>:</p>&#13;
<div class="image1"><img src="../images/f0180-01.jpg" alt="Image" width="296" height="80"/></div>&#13;
<p class="indent">The index <em>i</em> denotes the <em>i</em>th data point in the dataset or sample. Is this loss function a proper metric?</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_181"/>For simplicity’s sake, we will consider the <em>squared error (SE)</em> loss between two data points (though the following insights also hold for the MSE). As shown in the following equation, the SE loss quantifies the squared difference between the predicted and actual values for a single data point, while the MSE loss averages these squared differences over all data points in a dataset:</p>&#13;
<div class="image1"><img src="../images/f0181-01.jpg" alt="Image" width="198" height="40"/></div>&#13;
<p class="indent">In this case, the SE satisfies the first part of the first criterion: the distance between two points is always non-negative. Since we are raising the difference to the power of 2, it cannot be negative.</p>&#13;
<p class="indent">How about the second criterion, that the distance can be 0 only if the two points are identical? Due to the subtraction in the SE, it is intuitive to see that it can be 0 only if the prediction matches the target variable, <em>y</em> = <em>ŷ</em>. As with the first criterion, we can use the square to confirm that SE satisfies the second criterion: we have (<em>y</em> – <em>ŷ</em>)<sup>2</sup> = (<em>ŷ</em> – <em>y</em>)<sup>2</sup>.</p>&#13;
<p class="indent">At first glance, it seems that the squared error loss also satisfies the third criterion, the triangle inequality. Intuitively, you can check this by choosing three arbitrary numbers, here 1, 2, 3:</p>&#13;
<ul>&#13;
<li class="noindent">(1 – 2)<sup>2</sup> <em>≤</em> (1 – 3)<sup>2</sup> + (2 – 3)<sup>2</sup></li>&#13;
<li class="noindent">(1 – 3)<sup>2</sup> <em>≤</em> (1 – 2)<sup>2</sup> + (2 – 3)<sup>2</sup></li>&#13;
<li class="noindent">(2 – 3)<sup>2</sup> <em>≤</em> (1 – 2)<sup>2</sup> + (1 – 3)<sup>2</sup></li>&#13;
</ul>&#13;
<p class="indent">However, there are values for which this is not true. For example, consider the values <em>a</em> = 0, <em>b</em> = 2, and <em>c</em> = 1. This gives us <em>d</em>(<em>a</em>, <em>b</em>) = 4, <em>d</em>(<em>a</em>, <em>c</em>) = 1, and <em>d</em>(<em>b</em>, <em>c</em>) = 1, such that we have the following scenario, which violates the triangle inequality:</p>&#13;
<ul>&#13;
<li class="noindent">(0 – 2)<sup>2</sup> ≰ (0 – 1)<sup>2</sup> + (2 – 1)<sup>2</sup></li>&#13;
<li class="noindent">(2 – 1)<sup>2</sup> <em>≤</em> (0 –1)<sup>2</sup> + (0 – 2)<sup>2</sup></li>&#13;
<li class="noindent">(0 – 1)<sup>2</sup> <em>≤</em> (0 –2)<sup>2</sup> + (1 – 2)<sup>2</sup></li>&#13;
</ul>&#13;
<p class="indent">Since it does not satisfy the triangle inequality via the example above, we conclude that the (mean) squared error loss is not a proper metric.</p>&#13;
<p class="indent">However, if we change the squared error into the <em>root-squared error</em></p>&#13;
<div class="image1"><img src="../images/f0181-02.jpg" alt="Image" width="104" height="35"/></div>&#13;
<p class="indent">the triangle inequality can be satisfied:</p>&#13;
<div class="image1"><img src="../images/f0181-03.jpg" alt="Image" width="405" height="50"/></div>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>You might be familiar with the</em> L<em><sub>2</sub> distance or Euclidean distance, which is known to satisfy the triangle inequality. These two distance metrics are equivalent to the root-squared error when considering two scalar values.</em><span epub:type="pagebreak" id="page_182"/></p>&#13;
</div>&#13;
<h3 class="h3" id="ch00lev138"><strong>The Cross-Entropy Loss</strong></h3>&#13;
<p class="noindent"><em>Cross entropy</em> is used to measure the distance between two probability distributions. In machine learning contexts, we use the discrete cross-entropy loss (CE) between class label <em>y</em> and the predicted probability <em>p</em> when we train logistic regression or neural network classifiers on a dataset consisting of <em>n</em> training examples:</p>&#13;
<div class="image1"><img src="../images/f0182-01.jpg" alt="Image" width="386" height="80"/></div>&#13;
<p class="indent">Is this loss function a proper metric? Again, for simplicity’s sake, we will look at the cross-entropy function (<em>H</em>) between only two data points:</p>&#13;
<div class="image1"><img src="../images/f0182-02.jpg" alt="Image" width="236" height="28"/></div>&#13;
<p class="indent">The cross-entropy loss satisfies one part of the first criterion: the distance is always non-negative because the probability score is a number in the range [0, 1]. Hence, log(<em>p</em>) ranges between –<em>∞</em> and 0. The important part is that the <em>H</em> function includes a negative sign. Hence, the cross entropy ranges between <em>∞</em> and 0 and thus satisfies one aspect of the first criterion shown above.</p>&#13;
<p class="indent">However, the cross-entropy loss is not 0 for two identical points. For example, <em>H</em>(0.9, 0.9) = –0.9 <em>×</em> log(0.9) = 0.095.</p>&#13;
<p class="indent">The second criterion shown above is also violated by the cross-entropy loss because the loss is not symmetric: –<em>y ×</em> log(<em>p</em>) ≠ –<em>p ×</em> log(<em>y</em>). Let’s illustrate this with a concrete, numeric example:</p>&#13;
<ul>&#13;
<li class="noindent">If <em>y</em> = 1 and <em>p</em> = 0.5, then –1 <em>×</em> log(0.5) = 0.693.</li>&#13;
<li class="noindent">If <em>y</em> = 0.5 and <em>p</em> = 1, then –0.5 <em>×</em> log(1) = 0.</li>&#13;
</ul>&#13;
<p class="indent">Finally, the cross-entropy loss does not satisfy the triangle inequality, <em>H</em>(<em>r</em>, <em>p</em>) <em>≥ H</em>(<em>r</em>, <em>q</em>) + <em>H</em>(<em>q</em>, <em>p</em>). Let’s illustrate this with an example as well. Suppose we choose <em>r</em> = 0.9, <em>p</em> = 0.5, and <em>q</em> = 0.4. We have:</p>&#13;
<ul>&#13;
<li class="noindent"><em>H</em>(0.9, 0.5) = 0.624</li>&#13;
<li class="noindent"><em>H</em>(0.9, 0.4) = 0.825</li>&#13;
<li class="noindent"><em>H</em>(0.4, 0.5) = 0.277</li>&#13;
</ul>&#13;
<p class="noindent">As you can see, 0.624 <em>≥</em> 0.825 + 0.277 does not hold here.</p>&#13;
<p class="indent">In conclusion, while the cross-entropy loss is a useful loss function for training neural networks via (stochastic) gradient descent, it is not a proper distance metric, as it does not satisfy any of the three criteria.<span epub:type="pagebreak" id="page_183"/></p>&#13;
<h3 class="h3" id="ch00lev139"><strong>Exercises</strong></h3>&#13;
<p class="number1"><strong>27-1.</strong> Suppose we consider using the mean absolute error (MAE) as an alternative to the root mean square error (RMSE) for measuring the performance of a machine learning model, where <img class="middle" src="../images/f0183-01.jpg" alt="Image" width="280" height="33"/> and <img class="middle" src="../images/f0183-02.jpg" alt="Image" width="310" height="52"/>. However, a colleague argues that the MAE is not a proper distance metric in metric space because it involves an absolute value, so we should use the RMSE instead. Is this argument correct?</p>&#13;
<p class="number1"><strong>27-2.</strong> Based on your answer to the previous question, would you say that the MAE is better or is worse than the RMSE?<span epub:type="pagebreak" id="page_184"/></p>&#13;
</div>
</div>
</body></html>