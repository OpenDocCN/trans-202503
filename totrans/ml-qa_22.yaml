- en: '**19'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**19'
- en: EVALUATING GENERATIVE LARGE LANGUAGE MODELS**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**评估生成型大型语言模型**'
- en: '![Image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/common.jpg)'
- en: What are the standard metrics for evaluating the quality of text generated by
    large language models, and why are these metrics useful?
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 用来评估大型语言模型生成文本质量的标准指标是什么？这些指标为何有用？
- en: Perplexity, BLEU, ROUGE, and BERTScore are some of the most common evaluation
    metrics used in natural language processing to assess the performance of LLMs
    across various tasks. Although there is ultimately no way around human quality
    judgments, human evaluations are tedious, expensive, hard to automate, and subjective.
    Hence, we develop metrics to provide objective summary scores to measure progress
    and compare different approaches.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 困惑度、BLEU、ROUGE和BERTScore是自然语言处理领域中常用的几种评估指标，用于评估LLM在各种任务中的表现。尽管最终无法避免人工质量评估，但人工评估既繁琐又昂贵，且难以自动化且带有主观性。因此，我们开发了这些指标，以提供客观的总结分数来衡量进展并比较不同的方法。
- en: This chapter discusses the difference between intrinsic and extrinsic performance
    metrics for evaluating LLMs, and then it dives deeper into popular metrics like
    BLEU, ROUGE, and BERTScore and provides simple hands-on examples for illustration
    purposes.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了评估大型语言模型（LLM）内在和外在性能指标的区别，接着深入探讨了像BLEU、ROUGE和BERTScore等流行指标，并提供了简单的实际操作示例以作说明。
- en: '**Evaluation Metrics for LLMs**'
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**LLM评估指标**'
- en: The *perplexity metric* is directly related to the loss function used for pretraining
    LLMs and is commonly used to evaluate text generation and text completion models.
    Essentially, it quantifies the average uncertainty of the model in predicting
    the next word in a given context—the lower the perplexity, the better.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*困惑度指标* 与用于预训练LLM的损失函数直接相关，并且通常用于评估文本生成和文本补全模型。它本质上量化了模型在给定上下文中预测下一个单词时的平均不确定性——困惑度越低，模型表现越好。'
- en: The *bilingual evaluation understudy (BLEU)* score is a widely used metric for
    evaluating the quality of machine-generated translations. It measures the overlap
    of n-grams between the machine-generated translation and a set of human-generated
    reference translations. A higher BLEU score indicates better performance, ranging
    from 0 (worst) to 1 (best).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*双语评估下游任务（BLEU）* 分数是评估机器生成翻译质量的广泛使用的指标。它衡量机器生成翻译和一组人工生成参考翻译之间的n-gram重叠程度。较高的BLEU分数表示更好的表现，范围从0（最差）到1（最好）。'
- en: The *recall-oriented understudy for gisting evaluation (ROUGE)* score is a metric
    primarily used for evaluating automatic summarization (and sometimes machine translation)
    models. It measures the overlap between the generated summary and reference summaries.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*召回导向下游任务评估（ROUGE）* 分数是主要用于评估自动摘要（有时也用于机器翻译）模型的指标。它衡量生成的摘要和参考摘要之间的重叠程度。'
- en: We can think of perplexity as an *intrinsic metric* and BLEU and ROUGE as *extrinsic
    metrics*. To illustrate the difference between the two types of metrics, think
    of optimizing the conventional cross entropy to train an image classifier. The
    cross entropy is a loss function we optimize during training, but our end goal
    is to maximize the classification accuracy. Since classification accuracy cannot
    be optimized directly during training, as it’s not differentiable, we minimize
    the surrogate loss function like the cross entropy. Minimizing the cross entropy
    loss more or less correlates with maximizing the classification accuracy.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以把困惑度视为*内在指标*，而把BLEU和ROUGE视为*外在指标*。为了说明这两种类型的指标之间的区别，可以想象优化传统的交叉熵来训练一个图像分类器。交叉熵是我们在训练过程中优化的损失函数，但我们的最终目标是最大化分类精度。由于分类精度无法在训练过程中直接优化，因为它不可微分，因此我们最小化像交叉熵这样的替代损失函数。最小化交叉熵损失或多或少与最大化分类精度相关。
- en: Perplexity is often used as an evaluation metric to compare the performance
    of different language models, but it is not the optimization target during training.
    BLEU and ROUGE are more related to classification accuracy, or rather precision
    and recall. In fact, BLEU is a precision-like score to evaluate the quality of
    a translated text, while ROUGE is a recall-like score to evaluate summarized texts.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 困惑度（Perplexity）通常作为评估指标，用于比较不同语言模型的表现，但它并不是训练过程中的优化目标。BLEU和ROUGE更与分类精度相关，或者说与精确度和召回率相关。实际上，BLEU是一种类似精确度的分数，用来评估翻译文本的质量，而ROUGE是一种类似召回率的分数，用来评估摘要文本。
- en: The following sections discuss the mechanics of these metrics in more detail.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 以下章节将更详细地讨论这些指标的机制。
- en: '***Perplexity***'
  id: totrans-13
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***困惑度（Perplexity）***'
- en: Perplexity is closely related to the cross entropy directly minimized during
    training, which is why we refer to perplexity as an *intrinsic metric*.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 困惑度与训练过程中直接最小化的交叉熵密切相关，这也是我们将困惑度称为*内在指标*的原因。
- en: Perplexity is defined as 2^(*H*(*p*, *q*)/*n*), where *H*(*p*, *q*) is the cross
    entropy between the true distribution of words *p* and the predicted distribution
    of words *q*, and *n* is the sentence length (the number of words or tokens) to
    normalize the score. As cross entropy decreases, perplexity decreases as well—the
    lower the perplexity, the better. While we typically compute the cross entropy
    using a natural logarithm, we calculate the cross entropy and perplexity with
    a base-2 logarithm for the intuitive relationship to hold. (However, whether we
    use a base-2 or natural logarithm is only a minor implementation detail.)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 困惑度被定义为2^(*H*(*p*, *q*)/*n*)，其中*H*(*p*, *q*)是词的真实分布*p*和预测分布*q*之间的交叉熵，*n*是句子长度（单词数或标记数），用于标准化得分。随着交叉熵的减少，困惑度也会减少——困惑度越低，效果越好。虽然我们通常使用自然对数计算交叉熵，但为了保持直观的关系，我们使用以2为底的对数来计算交叉熵和困惑度。（然而，无论使用以2为底的对数还是自然对数，都是一个较小的实现细节。）
- en: 'In practice, since the probability for each word in the target sentence is
    always 1, we compute the cross entropy as the logarithm of the probability scores
    returned by the model we want to evaluate. In other words, if we have the predicted
    probability score for each word in a sentence *s*, we can compute the perplexity
    directly as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，由于目标句子中每个单词的概率始终为1，我们将交叉熵计算为模型返回的概率分数的对数。换句话说，如果我们有每个单词的预测概率分数，我们可以直接计算困惑度，如下所示：
- en: '![Image](../images/f0129-01.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0129-01.jpg)'
- en: 'where *s* is the sentence or text we want to evaluate, such as “The quick brown
    fox jumps over the lazy dog,” *p*(*s*) is the probability scores returned by the
    model, and *n* is the number of words or tokens. For example, if the model returns
    the probability scores [0.99, 0.85, 0.89, 0.99, 0.99, 0.99, 0.99, 0.99], the perplexity
    is:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*s*是我们要评估的句子或文本，例如“The quick brown fox jumps over the lazy dog.”，*p*(*s*)是模型返回的概率分数，*n*是单词数或标记数。例如，如果模型返回的概率分数是[0.99,
    0.85, 0.89, 0.99, 0.99, 0.99, 0.99, 0.99]，则困惑度为：
- en: '![Image](../images/f0129-02.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0129-02.jpg)'
- en: If the sentence was “The fast black cat jumps over the lazy dog,” with probabilities
    [0.99, 0.65, 0.13, 0.05, 0.21, 0.99, 0.99, 0.99], the corresponding perplexity
    would be 2.419.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果句子是“The fast black cat jumps over the lazy dog.”，且概率为[0.99, 0.65, 0.13, 0.05,
    0.21, 0.99, 0.99, 0.99]，则相应的困惑度为2.419。
- en: You can find a code implementation and example of this calculation in the *supplementary/q19-evaluation-llms*
    subfolder at *[https://github.com/rasbt/MachineLearning-QandAI-book](https://github.com/rasbt/MachineLearning-QandAI-book)*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*supplementary/q19-evaluation-llms*子文件夹中的* [https://github.com/rasbt/MachineLearning-QandAI-book](https://github.com/rasbt/MachineLearning-QandAI-book)
    *找到这个计算的代码实现和示例。
- en: '***BLEU Score***'
  id: totrans-22
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***BLEU分数***'
- en: BLEU is the most popular and most widely used metric for evaluating translated
    texts. It’s used in almost all LLMs capable of translation, including popular
    tools such as OpenAI’s Whisper and GPT models.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: BLEU是最受欢迎和最广泛使用的翻译文本评估标准。几乎所有具备翻译能力的大型语言模型（LLM）都使用它，包括OpenAI的Whisper和GPT模型等流行工具。
- en: BLEU is a reference-based metric that compares the model output to human-generated
    references and was first developed to capture or automate the essence of human
    evaluation. In short, BLEU measures the lexical overlap between the model output
    and the human-generated references based on a precision score.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: BLEU是一种基于参考的度量标准，它将模型输出与人工生成的参考文本进行比较，最初开发的目的是捕捉或自动化人类评估的本质。简而言之，BLEU通过基于精度的得分衡量模型输出与人工生成参考文本之间的词汇重叠。
- en: In more detail, as a precision-based metric, BLEU counts how many words in the
    generated text (candidate text) occur in the reference text divided by the candidate
    text length (the number of words), where the reference text is a sample translation
    provided by a human, for example. This is commonly done for n-grams rather than
    individual words, but for simplicity, we will stick to words or 1-grams. (In practice,
    BLEU is often computed for 4-grams.)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 更详细地说，作为一种基于精度的度量标准，BLEU计算生成文本（候选文本）中有多少单词出现在参考文本中，并将其除以候选文本的长度（单词数），其中参考文本是由人工提供的样本文本翻译。例如，通常对n-grams（n元组）进行计算，而不是单个单词，但为了简便起见，我们将仅考虑单词或1-gram。（在实际应用中，BLEU通常是针对4-gram计算的。）
- en: '[Figure 19-1](ch19.xhtml#ch19fig1) demonstrates the BLEU score calculation,
    using the example of calculating the 1-gram BLEU score. The individual steps in
    [Figure 19-1](ch19.xhtml#ch19fig1) illustrate how we compute the 1-gram BLEU score
    based on its individual components, the weighted precision times a brevity penalty.
    You can also find a code implementation of this calculation in the *supplementary/q15-text-augment*
    subfolder at *[https://github.com/rasbt/MachineLearning-QandAI-book](https://github.com/rasbt/MachineLearning-QandAI-book)*.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[图19-1](ch19.xhtml#ch19fig1)展示了BLEU分数的计算，使用了计算1-gram BLEU分数的例子。[图19-1](ch19.xhtml#ch19fig1)中的每个步骤说明了我们如何基于BLEU分数的各个组成部分来计算1-gram
    BLEU分数，即加权精确度乘以简洁性惩罚。你还可以在*supplementary/q15-text-augment*子文件夹中找到这个计算的代码实现，链接地址是*[https://github.com/rasbt/MachineLearning-QandAI-book](https://github.com/rasbt/MachineLearning-QandAI-book)*。'
- en: '![Image](../images/19fig01.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/19fig01.jpg)'
- en: '*Figure 19-1: Calculating a 1-gram BLEU score*'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*图19-1：计算1-gram BLEU分数*'
- en: BLEU has several shortcomings, mostly owing to the fact that it measures string
    similarity, and similarity alone is not sufficient for capturing quality. For
    instance, sentences with similar words but different word orders might still score
    high, even though altering the word order can significantly change the meaning
    of a sentence and result in poor grammatical structure. Furthermore, since BLEU
    relies on exact string matches, it is sensitive to lexical variations and is incapable
    of identifying semantically similar translations that use synonyms or paraphrases.
    In other words, BLEU may assign lower scores to translations that are, in fact,
    accurate and meaningful.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: BLEU有几个缺点，主要是因为它衡量的是字符串相似性，而仅仅相似性不足以捕捉翻译质量。例如，句子中虽然单词相似但词序不同，可能仍然得分较高，尽管改变词序会显著改变句子的意义并导致语法结构差。此外，由于BLEU依赖于精确的字符串匹配，它对词汇变异非常敏感，无法识别使用同义词或释义的语义相似翻译。换句话说，BLEU可能会给那些实际上是准确和有意义的翻译分配较低的分数。
- en: The original BLEU paper found a high correlation with human evaluations, though
    this was disproven later.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的BLEU论文发现它与人工评估具有较高的相关性，尽管这一点后来被证明是错误的。
- en: Is BLEU flawed? Yes. Is it still useful? Also yes. BLEU is a helpful tool to
    measure or assess whether a model improves during training, as a proxy for fluency.
    However, it may not reliably give a correct assessment of the quality of the generated
    translations and is not well suited for detecting issues. In other words, it’s
    best used as a model selection tool, not a model evaluation tool.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: BLEU有缺陷吗？是的。它仍然有用吗？也是的。BLEU是一个有用的工具，可以用来衡量或评估模型在训练过程中是否有所改进，作为流利度的代理。然而，它可能不能可靠地评估生成翻译的质量，也不适合检测问题。换句话说，它最好作为模型选择工具，而不是模型评估工具。
- en: At the time of writing, the most popular alternatives to BLEU are METEOR and
    COMET (see the “[References](ch19.xhtml#ch00lev100)” section at the end of this
    chapter for more details).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在写作时，最流行的BLEU替代方法是METEOR和COMET（有关更多详细信息，请参见本章末尾的“[参考文献](ch19.xhtml#ch00lev100)”部分）。
- en: '***ROUGE Score***'
  id: totrans-33
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***ROUGE分数***'
- en: While BLEU is commonly used for translation tasks, ROUGE is a popular metric
    for scoring text summaries.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然BLEU通常用于翻译任务，但ROUGE是评分文本摘要的流行指标。
- en: There are many similarities between BLEU and ROUGE. The precision-based BLEU
    score checks how many words in the candidate translation occur in the reference
    translation. The ROUGE score also takes a flipped approach, checking how many
    words in the reference text appear in the generated text (here typically a summarization
    instead of a translation); this can be interpreted as a recall-based score.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: BLEU和ROUGE之间有许多相似之处。基于精确度的BLEU分数检查候选翻译中有多少个单词出现在参考翻译中。ROUGE分数也采取了相反的方式，检查参考文本中有多少个单词出现在生成的文本中（这里通常是摘要而非翻译）；这可以被解释为基于召回的分数。
- en: Modern implementations compute ROUGE as an F1 score that is the harmonic mean
    of recall (how many words in the reference occur in the candidate text) and precision
    (how many words in the candidate text occur in the reference text). For example,
    [Figure 19-2](ch19.xhtml#ch19fig2) shows a 1-gram ROUGE score computation (though
    in practice, ROUGE is often computed for bi-grams, that is, 2-grams).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现代的实现方式将ROUGE计算为一个F1分数，它是召回率（参考文本中有多少个单词出现在候选文本中）和精确率（候选文本中有多少个单词出现在参考文本中）的调和平均值。例如，[图19-2](ch19.xhtml#ch19fig2)展示了一个1-gram
    ROUGE分数的计算（尽管在实际操作中，ROUGE通常计算的是2-grams，即二元组）。
- en: '![Image](../images/19fig02.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/19fig02.jpg)'
- en: '*Figure 19-2: Computing ROUGE for 1-grams*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*图19-2：计算1-gram ROUGE*'
- en: 'There are other ROUGE variants beyond ROUGE-1 (the F1 score–based ROUGE score
    for 1-grams):'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 除了ROUGE-1（基于F1分数的1-gram ROUGE得分）之外，还有其他ROUGE变体：
- en: '**ROUGE-N**    Measures the overlap of n-grams between the candidate and reference
    summaries. For example, ROUGE-1 would look at the overlap of individual words
    (1-grams), while ROUGE-2 would consider the overlap of 2-grams (bigrams).'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**ROUGE-N**    衡量候选摘要和参考摘要之间的n-gram重叠。例如，ROUGE-1会查看单个单词的重叠（1-gram），而ROUGE-2会考虑2-gram（大二元组）的重叠。'
- en: '**ROUGE-L**    Measures the longest common subsequence (LCS) between the candidate
    and reference summaries. This metric captures the longest co-occurring in-order
    subsequence of words, which may have gaps in between them.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**ROUGE-L**    衡量候选摘要和参考摘要之间的最长公共子序列（LCS）。该度量捕捉单词的最长共现顺序子序列，它们之间可能有空隙。'
- en: '**ROUGE-S**    Measures the overlap of *skip-bigrams*, or word pairs with a
    flexible number of words in between them. It can be useful to capture the similarity
    between sentences with different word orderings.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**ROUGE-S**    衡量*跳跃大二元组*（skip-bigrams）的重叠，即在两个单词之间有灵活数量的单词的词对。它有助于捕捉具有不同词序的句子之间的相似性。'
- en: ROUGE shares similar weaknesses with BLEU. Like BLEU, ROUGE does not account
    for synonyms or paraphrases. It measures the n-gram overlap between the candidate
    and reference summaries, which can lead to lower scores for semantically similar
    but lexically different sentences. However, it’s still worth knowing about ROUGE
    since, according to a study, *all* papers introducing new summarization models
    at computational linguistics conferences in 2021 used it, and 69 percent of those
    papers used *only* ROUGE.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ROUGE与BLEU有相似的弱点。与BLEU一样，ROUGE并不考虑同义词或释义。它衡量候选摘要和参考摘要之间的n-gram重叠，这可能导致语义相似但词汇不同的句子得分较低。然而，仍然值得了解ROUGE，因为根据一项研究，2021年计算语言学会议上所有介绍新总结模型的论文都使用了ROUGE，其中69%的论文仅使用了ROUGE。
- en: '***BERTScore***'
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***BERTScore***'
- en: Another more recently developed extrinsic metric is BERTScore.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个最近开发的外部度量是BERTScore。
- en: For readers familiar with the inception score for generative vision models,
    BERTScore takes a similar approach, using embeddings from a pretrained model (for
    more on embeddings, see [Chapter 1](ch01.xhtml)). Here, BERT-Score measures the
    similarity between a candidate text and a reference text by leveraging the contextual
    embeddings produced by the BERT model (the encoder-style transformer discussed
    in [Chapter 17](ch17.xhtml)).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对于熟悉生成视觉模型的inception score的读者，BERTScore采取了类似的方法，使用预训练模型的嵌入（有关嵌入的更多信息，请参见[第1章](ch01.xhtml)）。在这里，BERTScore通过利用BERT模型生成的上下文嵌入，衡量候选文本和参考文本之间的相似度（该BERT模型是[第17章](ch17.xhtml)中讨论的编码器式转换器）。
- en: 'The steps to compute BERTScore are as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 计算BERTScore的步骤如下：
- en: Obtain the candidate text via the LLM you want to evaluate (PaLM, LLaMA, GPT,
    BLOOM, and so on).
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过你想要评估的LLM（如PaLM、LLaMA、GPT、BLOOM等）获取候选文本。
- en: Tokenize the candidate and reference texts into subwords, preferably using the
    same tokenizer used for training BERT.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将候选文本和参考文本进行词元化，最好使用与训练BERT时相同的分词器。
- en: Use a pretrained BERT model to create the embeddings for all tokens in the candidate
    and reference texts.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用预训练的BERT模型为候选文本和参考文本中的所有词元创建嵌入。
- en: Compare each token embedding in the candidate text to all token embeddings in
    the reference text, computing their cosine similarity.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将候选文本中每个词元的嵌入与参考文本中所有词元的嵌入进行比较，计算它们的余弦相似度。
- en: Align each token in the candidate text with the token in the reference text
    that has the highest cosine similarity.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将候选文本中的每个词元与参考文本中具有最高余弦相似度的词元对齐。
- en: Compute the final BERTScore by taking the average similarity scores of all tokens
    in the candidate text.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过计算候选文本中所有词元的平均相似度分数来计算最终的BERTScore。
- en: '[Figure 19-3](ch19.xhtml#ch19fig3) further illustrates these six steps. You
    can also find a computational example in the *subfolder/q15-text-augment* subfolder
    at *[https://github.com/rasbt/MachineLearning-QandAI-book](https://github.com/rasbt/MachineLearning-QandAI-book)*.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 19-3](ch19.xhtml#ch19fig3)进一步说明了这六个步骤。你也可以在*subfolder/q15-text-augment*子文件夹中找到一个计算示例，网址为*[https://github.com/rasbt/MachineLearning-QandAI-book](https://github.com/rasbt/MachineLearning-QandAI-book)*。'
- en: '![Image](../images/19fig03.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/19fig03.jpg)'
- en: '*Figure 19-3: Computing the BERTScore step by step*'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 19-3：逐步计算BERTScore*'
- en: BERTScore can be used for translations and summaries, and it captures the semantic
    similarity better than traditional metrics like BLEU and ROUGE. However, BERTScore
    is more robust in paraphrasing than BLEU and ROUGE and captures semantic similarity
    better due to its contextual embeddings. Also, it may be computationally more
    expensive than BLEU and ROUGE, as it requires using a pretrained BERT model for
    the evaluation. While BERTScore provides a useful automatic evaluation metric,
    it’s not perfect and should be used alongside other evaluation techniques, including
    human judgment.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: BERTScore可以用于翻译和总结，并且比传统度量如BLEU和ROUGE更好地捕捉语义相似度。然而，BERTScore在同义句生成方面比BLEU和ROUGE更强大，并且由于其上下文嵌入，它更好地捕捉语义相似性。此外，它可能比BLEU和ROUGE在计算上更昂贵，因为它需要使用预训练的BERT模型进行评估。尽管BERTScore提供了一个有用的自动评估指标，但它并不完美，应该与其他评估技术一起使用，包括人工判断。
- en: '**Surrogate Metrics**'
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**替代度量**'
- en: 'All metrics covered in this chapter are surrogates or proxies to evaluate how
    useful the model is in terms of measuring how well the model compares to human
    performance for accomplishing a goal. As mentioned earlier, the best way to evaluate
    LLMs is to assign human raters who judge the results. However, since this is often
    expensive and not easy to scale, we use the aforementioned metrics to estimate
    model performance. To quote from the InstructGPTpaper “Training Language Models
    to Follow Instructions with Human Feedback”: “Public NLP datasets are not reflective
    of how our language models are used . . . [They] are designed to capture tasks
    that are easy to evaluate with automatic metrics.”'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖的所有度量标准都是用于评估模型在衡量其与人类表现进行对比时的有用性的替代指标。如前所述，评估LLM的最佳方法是指定人工评分者来判断结果。然而，由于这通常成本较高且难以扩展，我们使用前述的度量标准来估算模型性能。引用《InstructGPT
    paper“用人类反馈训练语言模型遵循指令”》中的话：“公共NLP数据集并不能反映我们的语言模型是如何使用的... [它们] 旨在捕捉易于通过自动化度量评估的任务。”
- en: Besides perplexity, ROUGE, BLEU, and BERTScore, several other popular evaluation
    metrics are used to assess the predictive performance of LLMs.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 除了困惑度（perplexity）、ROUGE、BLEU和BERTScore之外，还使用了其他几种流行的评估度量来评估LLM的预测性能。
- en: '**Exercises**'
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**练习**'
- en: '**19-1.** In step 5 of [Figure 19-3](ch19.xhtml#ch19fig3), the cosine similarity
    between the two embeddings of “cat” is not 1.0, where 1.0 indicates a maximum
    cosine similarity. Why is that?'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**19-1.** 在[图19-3](ch19.xhtml#ch19fig3)的第5步中，“cat”两个嵌入之间的余弦相似度并不是1.0，其中1.0表示最大余弦相似度。为什么会这样？'
- en: '**19-2.** In practice, we might find that the BERTScore is not symmetric. This
    means that switching the candidate and reference sentences could result in different
    BERTScores for specific texts. How could we address this?'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**19-2.** 实际上，我们可能会发现BERTScore并不对称。这意味着交换候选句子和参考句子可能会导致某些文本的BERTScore不同。我们该如何解决这个问题？'
- en: '**References**'
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: 'The paper proposing the original BLEU method: Kishore Papineni et al., “BLEU:
    A Method for Automatic Evaluation of Machine Translation” (2002), *[https://aclanthology.org/P02-1040/](https://aclanthology.org/P02-1040/)*.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提出原始BLEU方法的论文：Kishore Papineni等，“BLEU：一种自动评估机器翻译的方法”（2002），* [https://aclanthology.org/P02-1040/](https://aclanthology.org/P02-1040/)*。
- en: 'A follow-up study disproving BLEU’s high correlation with human evaluations:
    Chris Callison-Burch, Miles Osborne, and Philipp Koehn, “Re-Evaluating the Role
    of BLEU in Machine Translation Research” (2006), *[https://aclanthology.org/E06-1032/](https://aclanthology.org/E06-1032/)*.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一项否定BLEU与人工评估高度相关性的后续研究：Chris Callison-Burch，Miles Osborne和Philipp Koehn，“重新评估BLEU在机器翻译研究中的作用”（2006），*
    [https://aclanthology.org/E06-1032/](https://aclanthology.org/E06-1032/)*。
- en: 'The shortcomings of BLEU, based on 37 studies published over 20 years: Benjamin
    Marie, “12 Critical Flaws of BLEU” (2022), *[https://medium.com/@bnjmn_marie/12-critical-flaws-of-bleu-1d790ccbe1b1](https://medium.com/@bnjmn_marie/12-critical-flaws-of-bleu-1d790ccbe1b1)*.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BLEU的缺点，基于20年来发布的37项研究：Benjamin Marie，"12个BLEU的关键缺陷"（2022），* [https://medium.com/@bnjmn_marie/12-critical-flaws-of-bleu-1d790ccbe1b1](https://medium.com/@bnjmn_marie/12-critical-flaws-of-bleu-1d790ccbe1b1)*。
- en: 'The paper proposing the original ROUGE method: Chin-Yew Lin, “ROUGE: A Package
    for Automatic Evaluation of Summaries” (2004), *[https://aclanthology.org/W04-1013/](https://aclanthology.org/W04-1013/)*.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提出原始ROUGE方法的论文：Chin-Yew Lin，“ROUGE：一种自动评估摘要的工具包”（2004），* [https://aclanthology.org/W04-1013/](https://aclanthology.org/W04-1013/)*。
- en: 'A survey on the usage of ROUGE in conference papers: Sebastian Gehrmann, Elizabeth
    Clark, and Thibault Sellam, “Repairing the Cracked Foundation: A Survey of Obstacles
    in Evaluation Practices for Generated Text” (2022), *[https://arxiv.org/abs/2202.06935](https://arxiv.org/abs/2202.06935)*.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于 ROUGE 在会议论文中的使用调查：Sebastian Gehrmann、Elizabeth Clark 和 Thibault Sellam，“修复裂缝：生成文本评估实践中的障碍调查”（2022），*[https://arxiv.org/abs/2202.06935](https://arxiv.org/abs/2202.06935)*。
- en: 'BERTScore, an evaluation metric based on a large language model: Tianyi Zhang
    et al., “BERTScore: Evaluating Text Generation with BERT” (2019), *[https://arxiv.org/abs/1904.09675](https://arxiv.org/abs/1904.09675)*.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERTScore，一种基于大型语言模型的评估指标：Tianyi Zhang 等人，“BERTScore：使用 BERT 评估文本生成”（2019），*[https://arxiv.org/abs/1904.09675](https://arxiv.org/abs/1904.09675)*。
- en: 'A comprehensive survey on evaluation metrics for large language models: Asli
    Celikyilmaz, Elizabeth Clark, and Jianfeng Gao, “Evaluation of Text Generation:
    A Survey” (2021), *[https://arxiv.org/abs/2006.14799](https://arxiv.org/abs/2006.14799)*.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于大型语言模型评估指标的综合调研：Asli Celikyilmaz、Elizabeth Clark 和 Jianfeng Gao，“文本生成评估：一项调查”（2021），*[https://arxiv.org/abs/2006.14799](https://arxiv.org/abs/2006.14799)*。
- en: 'METEOR is a machine translation metric that improves upon BLEU by using advanced
    matching techniques and aiming for better correlation with human judgment at the
    sentence level: Satanjeev Banerjee and Alon Lavie, “METEOR: An Automatic Metric
    for MT Evaluation with Improved Correlation with Human Judgments” (2005), *[https://aclanthology.org/W05-0909/](https://aclanthology.org/W05-0909/)*.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: METEOR 是一种改进了 BLEU 的机器翻译指标，采用先进的匹配技术，旨在与人类判断在句子层面上实现更好的相关性：Satanjeev Banerjee
    和 Alon Lavie，“METEOR：一种具有更好与人类判断相关性的自动机器翻译评估指标”（2005），*[https://aclanthology.org/W05-0909/](https://aclanthology.org/W05-0909/)*。
- en: 'COMET is a neural framework that sets new standards for correlating machine
    translation quality with human judgments, using cross-lingual pretrained models
    and multiple types of evaluation: Ricardo Rei et al., “COMET: A Neural Framework
    for MT Evaluation” (2020), *[https://arxiv.org/abs/2009.09025](https://arxiv.org/abs/2009.09025)*.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'COMET 是一个神经网络框架，采用跨语言预训练模型和多种评估方式，为机器翻译质量与人类判断之间的相关性设定了新的标准：Ricardo Rei 等人，“COMET:
    用于机器翻译评估的神经网络框架”（2020），*[https://arxiv.org/abs/2009.09025](https://arxiv.org/abs/2009.09025)*。'
- en: 'The InstructGPT paper: Long Ouyang et al., “Training Language Models to Follow
    Instructions with Human Feedback” (2022), *[https://arxiv.org/abs/2203.02155](https://arxiv.org/abs/2203.02155)*.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: InstructGPT 论文：Long Ouyang 等人，“训练语言模型以遵循指令并获得人类反馈”（2022），*[https://arxiv.org/abs/2203.02155](https://arxiv.org/abs/2203.02155)*。
