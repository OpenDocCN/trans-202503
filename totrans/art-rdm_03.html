<html><head></head><body>
<h2 class="h2" id="ch03"><span epub:type="pagebreak" id="page_73"/><strong><span class="big">3</span><br/>SIMULATE THE REAL WORLD</strong></h2>
<div class="image1"><img alt="Image" src="../images/common.jpg"/></div>
<p class="noindenta"><em>Computer simulations</em> are programs that use randomness to simulate real-world events and processes. More specifically, computer simulations manipulate <em>models</em>, programmatic stand-ins for the real world.</p>
<p class="indent">We’ll begin this chapter by defining what a model is. Then we’ll get our feet wet with two straightforward simulation examples: estimating <em>π</em> by throwing darts and gathering people together in a room to estimate the probability that at least two of them share a birthday. Once we’ve done that, we’ll wade in further to explore Darwinian evolution via simulation, capturing essential characteristics of natural selection and genetic drift.</p>
<h3 class="h3" id="ch00lev1_18"><strong>Introduction to Models</strong></h3>
<p class="noindent">We can define a model in many ways, but I like this definition from Daniel L. Hartl in <em>A Primer of Population Genetics and Genomics</em> (Oxford University Press, 2020):</p>
<div class="bq">
<p class="noindent">A model is an intentional simplification of a complex situation designed to eliminate extraneous detail in order to focus on the essentials.</p>
</div>
<p class="noindent"><span epub:type="pagebreak" id="page_74"/>Think of a model as an approximation of something that we’re interested in exploring or characterizing. There are no requirements for what that something is or for how we model it.</p>
<p class="indent">In this chapter, a model is a piece of code that attempts to capture the essential character of a real-world process, like what happens to the probability of shared birthdays as more and more people gather in a room, or how natural selection and genetic drift affect the genomes of a population. Simulation lets us control the experimental world while allowing random behavior, to understand what has happened or might happen, especially as critical parameters (environmental factors) are varied.</p>
<p class="indent">Consider the following statement, which is attributed to British statistician George Box:</p>
<p class="center">All models are wrong, but some are useful.</p>
<p class="noindent">Unless particularly trivial, all models are wrong in some way, especially those of the real world. If the model is well conceived and well implemented, it might lead to valuable conclusions about the modeled process. The word <em>process</em> implies a sequence of events, that is, time. Many models simulate processes unfolding in time; for example, we’ll explore fundamental evolutionary processes acting at a population level over time.</p>
<p class="indent">A good model captures enough of the thing being modeled to generate conclusions worthy of confidence tempered with reality. Blind faith in a model’s output isn’t recommended. At best, a model falls into the “trust, but verify” category—a good rule of thumb for all scientific claims.</p>
<p class="indent">Let’s slide into simulation by throwing darts to estimate <em>π</em>.</p>
<h3 class="h3" id="ch00lev1_19"><strong>Estimate Pi</strong></h3>
<p class="noindent">We’ll generate an estimate of <em>π</em>, the ratio between the circumference of a circle and its diameter, by throwing darts at a board. Doing this in real life would be time consuming, so we’ll simulate the process instead; that is, we’ll make a model.</p>
<h4 class="h4" id="ch00lev2_25"><em><strong>Using a Dartboard</strong></em></h4>
<p class="noindent">First, let’s learn how throwing darts at a board tells us about the value of <em>π</em>. For that, we need a diagram (<a href="ch03.xhtml#ch03fig01">Figure 3-1</a>).</p>
<div class="image"><img alt="Image" id="ch03fig01" src="../images/03fig01.jpg"/></div>
<p class="figcap"><span epub:type="pagebreak" id="page_75"/><em>Figure 3-1: Simulating dart throws</em></p>
<p class="indent"><a href="ch03.xhtml#ch03fig01">Figure 3-1</a> shows a square with a circle inside it. The circle’s diameter isn’t marked explicitly, but we’ll say it’s 2, meaning the radius is 1. The diameter also matches the length of the sides of the square; therefore, the areas of the square and circle are</p>
<div class="image1"><img alt="Image" src="../images/f0075-01.jpg"/></div>
<p class="noindent">implying:</p>
<div class="image1"><img alt="Image" src="../images/f0075-02.jpg"/></div>
<p class="noindent">We calculate <em>π</em> by dividing the area of the circle by the area of the square and multiplying that by 4.</p>
<p class="indent">If we throw many darts, or pick many random points, they’ll eventually cover the circle and the square. We can use the number of darts that land inside each shape as a proxy for the areas. We now have an algorithm: throw darts and count the number that land inside the circle (<em>N</em>) and inside the square (<em>M</em>), then divide <em>N</em> by <em>M</em> and multiply by 4 to get an estimate of <em>π</em>.</p>
<p class="indent"><span epub:type="pagebreak" id="page_76"/>The previous figure’s example points are all in the first quadrant, which works well for our estimate because the ratio of the area of the square to the circle is the same as the ratio of the portion of each shape in the first quadrant. Specifically, the first quadrant is 1/4 the size of the full shapes, so the areas are divided by 4. But both the circle and square areas are divided by 4, meaning their ratio remains the same, <em>π</em>/4. This means we need only to throw darts that land in the first quadrant.</p>
<p class="indent">Now that we understand how to estimate the respective areas and <em>π</em> by throwing darts, how should we actually “throw” them? The answer lies in the previous comment about the first quadrant.</p>
<p class="indent">Here’s the algorithm:</p>
<ol>
<li class="noindent">Randomly pick two numbers in [0, 1) and call them <em>x</em> and <em>y</em>. These become the point where the dart lands, (<em>x</em>, <em>y</em>).</li>
<li class="noindent">Increment <em>M</em>, the counter for the number of points in the square.</li>
<li class="noindent">If <em>x</em><sup>2</sup> + <em>y</em><sup>2</sup> ≤ 1, increment <em>N</em>, the number of points inside the circle.</li>
<li class="noindent">Repeat steps 1 through 3 for as many darts as desired.</li>
<li class="noindent">Return (4<em>N</em>)/<em>M</em> as the estimate of <em>π</em>.</li>
</ol>
<p class="indent">We pick points in [0, 1) so that the points are all in the first quadrant and land inside the square. Therefore, if we throw <em>n</em> darts, <em>M</em> = <em>n</em>, and we need only ask if the same points are also inside the circle.</p>
<p class="indent">In step 2 we ask a question about the circle, with <em>x</em><sup>2</sup> + <em>y</em><sup>2</sup> ≤ 1 coming from the Pythagorean theorem: <em>a</em><sup>2</sup> + <em>b</em><sup>2</sup> = <em>c</em><sup>2</sup>, where <em>c</em> is the side opposite the right angle. Here the triangle sides are <em>x</em> and <em>y</em>, meaning the radius (<em>r</em> = 1) is the hypotenuse, <em>x</em><sup>2</sup> + <em>y</em><sup>2</sup>. Any point forming a hypotenuse less than <em>r</em> = 1 is inside the circle.</p>
<p class="indent">Before we proceed, we should ask whether this is a fair model of the process of throwing darts, and whether we’ve made any unfair assumptions. After all, a model attempts to mimic <em>what’s most important</em> about a process. We’re using two uniformly selected random numbers in [0, 1) to represent the location where a dart might land, and we’ve made only one assumption: that <em>all</em> darts land in the first quadrant. Limiting the random values to [0, 1) eliminates out-of-range darts, so we count every dart throw as landing in at least the square.</p>
<p class="indent">With these questions answered, we’re ready to test.</p>
<h4 class="h4" id="ch00lev2_26"><em><strong>Simulating Random Darts</strong></em></h4>
<p class="noindent">The code we want is in <em>sim_pi.py</em>. To run it, supply the number of simulated darts and the desired randomness source. For example:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 sim_pi.py 10000 pcg64</span>
pi = 3.16120000</pre>
<p class="noindent">This throws 10,000 darts using PCG64. The result is <em>π ≈</em> 3.1612. The correct value rounded to four digits is 3.1416, so we’re in the ballpark. The estimate <span epub:type="pagebreak" id="page_77"/>uses only four decimal places because we’re approximating <em>π</em> with a fraction that has a denominator of 10,000. Ten more runs gives:</p>
<p class="noindentt">3.1496, 3.1188, 3.1468, 3.1700, 3.1292, 3.1372, 3.0916, 3.1608, 3.1236, 3.1140</p>
<p class="noindentt">Combining all 11 runs gives <em>π ≈</em> 3.1366, which is about 0.16 percent off from the four-digit value.</p>
<p class="indent">Let’s increase the number of darts:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 sim_pi.py 1_000_000 pcg64</span>
pi = 3.14157600</pre>
<p class="noindent">That’s more like it. The correct value to six places is 3.141593.</p>
<p class="indent">Let’s go for broke—this should nail it:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 sim_pi.py 100_000_000 pcg64</span>
pi = 3.14180732</pre>
<p class="noindent">Odd. We threw 100 times as many darts as the previous run, but the result wasn’t as good. Nothing’s wrong with our approach; that’s how the random cookie crumbles. A second run using PCG64 and 100 million darts returned 3.14160636, which is better. Still, it raises the question: Why such variation? That’s the nature of a random generator, and it serves as a reminder to repeat simulations multiple times to convince ourselves that they’re producing reasonable output and to get numerous estimates.</p>
<p class="indent">Individual runs using the other randomness sources supported by the <span class="literal">RE</span> class gave:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 sim_pi.py 100_000_000 mt19937</span>
pi = 3.14151340
&gt; <span class="codestrong1">python3 sim_pi.py 100_000_000 urandom</span>
pi = 3.14148696
&gt; <span class="codestrong1">python3 sim_pi.py 100_000_000 rdrand</span>
pi = 3.14139680
&gt; <span class="codestrong1">python3 sim_pi.py 100_000_000 minstd</span>
pi = 3.14156084
&gt; <span class="codestrong1">python3 sim_pi.py 100_000_000 RandomDotOrg.bin</span>
pi = 3.14161204</pre>
<p class="noindent">The last run uses <em>RandomDotOrg.bin</em>, a 510MB file of random data from <em><a href="http://random.org">random.org</a></em>. All randomness sources produce reasonable estimates of <em>π</em> but are still not satisfying. Why aren’t they closer to the actual value of 3.14159265 . . . ?</p>
<h4 class="h4" id="ch00lev2_27"><em><strong>Understanding the RE Class Output</strong></em></h4>
<p class="noindent">Let’s reconsider what we want the random throwing of darts to simulate. We’re looking to compare <em>areas</em>, so we want the darts to cover the areas as evenly as possible.</p>
<p class="indent"><span epub:type="pagebreak" id="page_78"/><a href="ch03.xhtml#ch03fig02">Figure 3-2</a>, which is a duplicate of <a href="ch01.xhtml#ch01fig05">Figure 1-5</a>, shows us what is happening. The middle plot in the figure shows the placement of points when using a random generator. There are gaps and places where the points are concentrated. It’s a reasonable coverage of the area, but not a uniformly dense one.</p>
<div class="image"><img alt="Image" id="ch03fig02" src="../images/03fig02.jpg"/></div>
<p class="figcap"><em>Figure 3-2: Bad quasirandom (left), pseudorandom (middle), and good quasirandom sequences (right)</em></p>
<p class="indent">The right plot in <a href="ch03.xhtml#ch03fig02">Figure 3-2</a>, from a pair of quasirandom sequences, is much more uniform over the area. Let’s try using that instead. The code we want for this case is in <em>sim_pi_quasi.py</em>:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 sim_pi_quasi.py 10000 2 3</span>
pi = 3.14480000
&gt; <span class="codestrong1">python3 sim_pi_quasi.py 100000 2 3</span>
pi = 3.14208000
&gt; <span class="codestrong1">python3 sim_pi_quasi.py 1_000_000 2 3</span>
pi = 3.14157200</pre>
<p class="indent">The first argument is the number of darts; the other two are the bases for the quasirandom sequence. To cover a 2D plane, we need two different bases, here 2 and 3. As the number of darts increases, so does the quality of the estimate. With 1 million darts, it has already matched the first run of <em>sim_pi.py</em> with PCG64. There’s no randomness here; every run with the same number of darts and the same bases results in the same output. Also, because we’re generating the quasirandom sequence in pure Python, the runtime increases dramatically with the number of darts. For example, this run</p>
<pre class="pre">&gt; <span class="codestrong1">python3 sim_pi_quasi.py 10_000_000 2 3</span>
pi = 3.14159680</pre>
<p class="noindent">is correct to five decimals but took 12 minutes to run on my reference Intel i7 system. Asking for 100 million samples produces <em>π ≈</em> 3.14159184 after a two-and-a-half-hour run.</p>
<p class="indent">Let’s focus momentarily on the performance of each pseudorandom generator supported by the <span class="literal">RE</span> class. The file <em>sim_pi_test.py</em> estimates <em>π</em> 50 times each for PCG64, MT19937, MINSTD, <span class="literal">urandom</span>, and <span class="literal">RDRAND</span> using 2 million simulated dart throws. The result is the box plot in <a href="ch03.xhtml#ch03fig03">Figure 3-3</a>.</p>
<div class="image"><img alt="Image" id="ch03fig03" src="../images/03fig03.jpg"/></div>
<p class="figcap"><span epub:type="pagebreak" id="page_79"/><em>Figure 3-3: A box plot showing the distribution of π estimates by randomness source</em></p>
<p class="indent">A <em>box plot</em> is a diagram summarizing a set of data; in this case, the 50 estimates of <em>π</em>, that is, the 50 separate runs of <em>sim_pi.py</em> for each pseudorandom generator. Each generator’s output produces a box with a horizontal bar across it. The bar represents the median value, or the 50th percentile. Half the estimates were below this value and half above. The box’s lower and upper limits are the 25th and 75th percentiles, respectively. So, 75 percent of the estimates were below the upper part of the box and the remaining 25 percent were above it.</p>
<p class="indent">The <em>whiskers</em>, called fliers by Matplotlib, extend beyond the box. The height of the box, the difference between the 75th percentile and the 25th, is known as the <em>interquartile range (IQR)</em>. The whiskers are the box quartiles plus or minus 1.5 times the IQR. Any data values outside the whiskers are candidates for <em>outliers</em>, values that are atypical when compared to the rest of the data. Outliers might be errors or the exciting thing we’re looking for; context is everything.</p>
<p class="indent">The five boxes in <a href="ch03.xhtml#ch03fig03">Figure 3-3</a> are statistically identical. There are two potential outliers for MINSTD, but another run of <em>sim_pi_test.py</em> generates a new plot with a different set of boxes and potential outliers, even from <span class="literal">RDRAND</span>, which is as close to a true source of randomness as we can get for repeated sampling. Random processes sometimes produce strange output; there’s no meaning attached to it. This phenomenon is partly why detecting true cancer clusters can be tricky.</p>
<h4 class="h4" id="ch00lev2_28"><em><strong>Implementing the Darts Model</strong></em></h4>
<p class="noindent">Our dart-throwing simulation works. Now, let’s review the code to understand how:</p>
<pre class="pre">import sys
from RE import *
<span epub:type="pagebreak" id="page_80"/>

def Simulate(N, rng):
    v = rng.random(2*N)
    x = v[::2]
    y = v[1::2]
    d = x*x + y*y
    inside = len(np.where(d &lt;= 1.0)[0])
    return 4.0*inside/N

N = int(sys.argv[1])
kind = sys.argv[2]
rng = RE(kind=kind)
pi = Simulate(N, rng)
print("pi = %0.8f" % pi)</pre>
<p class="indent">The code at the bottom parses the command line to get the number of darts to throw (<span class="literal">N</span>) and the type of randomness source to use (<span class="literal">kind</span>). A generator is created (<span class="literal">rng</span>) and passed, along with the number of darts, to <span class="literal">Simulate</span>, which returns an estimate of <em>π</em> that is then printed.</p>
<p class="indent">All the action is in <span class="literal">Simulate</span>. We need <span class="literal">N</span> points, the locations where our darts landed. We can either use <span class="literal">rng</span> twice—first to get <em>x</em>-coordinates and then again to get <em>y</em>-coordinates—or generate twice as many points as we need and partition them in pairs. I chose the latter. Therefore, <span class="literal">v</span> contains 2<em>N</em> values. The first and then every other point become <span class="literal">x</span>, while the second and every other after that become <span class="literal">y</span>.</p>
<p class="indent">By design, all the points are inside the square. We need only to decide which are also inside the circle. For that, we need to know if <em>x</em><sup>2</sup> + <em>y</em><sup>2</sup> <em>≤ r</em><sup>2</sup> for radius <em>r</em> = 1. To this end, we set <span class="literal">d</span> to <em>x</em><sup>2</sup> + <em>y</em><sup>2</sup> and use NumPy’s <span class="literal">where</span> to find the indices that are less than or equal to 1. The count of those indices tells us how many points are <span class="literal">inside</span> the circle. Finally, the function returns the estimate of <em>π</em> as four times the number inside divided by the number of darts thrown.</p>
<p class="indent">Our dart simulation is complete. Now, let’s simulate a party to see how many people we need in a room to have a better-than-50 percent chance that at least two of them share a birthday.</p>
<h3 class="h3" id="ch00lev1_20"><strong>Birthday Paradox</strong></h3>
<p class="noindent">How many people need to be in a room for the probability of at least two of them sharing a birthday to be above 50 percent? There’s a mathematical way to determine this probability, but if we don’t know the math, we can find it by extensive experimentation: we can throw many parties, with differing numbers of people invited, and at each one figure out if at least two of them share a birthday. While this approach will work, it’ll be terribly slow and expensive.</p>
<h4 class="h4" id="ch00lev2_29"><span epub:type="pagebreak" id="page_81"/><em><strong>Simulating 100,000 Parties</strong></em></h4>
<p class="noindent">Assuming we aren’t willing to write a grant proposal for a million dollars to conduct this experiment with actual people, to say nothing of gaining review board approval and informed consent from thousands of people, is there any other way to approach the problem? You guessed it: simulation.</p>
<p class="indent">Every person has a birthday, so we’ll simulate the number of people in the room and assign each a randomly selected birthday. Then we’ll look at each possible pair and ask if they have the same birthday. There are 365 days in a year, ignoring leap years, so we’ll represent birthdays by picking a day of the year as a proxy for an actual birthday. In other words, each simulated person is assigned an integer in [0, 364]. If any two have the same number, they share a birthday.</p>
<p class="indent">We want the probability of a match for a given number of people, meaning one simulation isn’t sufficient. We need many, many simulations for a fixed number of people in the room. The number of times there’s a match divided by the number of simulations converges to the probability we seek.</p>
<p class="indent">Here’s our algorithm:</p>
<ol>
<li class="noindent">Fix the number of people in the room (<em>K</em>).</li>
<li class="noindent">Assign each of the <em>K</em> people a birthday ([0, 364]).</li>
<li class="noindent">Check each possible pair. If they share a birthday, increment <em>M</em>.</li>
<li class="noindent">Repeat from step 2 <em>N</em> times.</li>
<li class="noindent">The estimated probability for <em>K</em> people in a room is <em>M</em>/<em>N</em>.</li>
</ol>
<p class="indent">We’ll vary <em>K</em> from 2 to 50. <em>N</em> should be a large number, like <em>N</em> = 100,000 to simulate 100,000 parties with <em>K</em> people. We can always make <em>N</em> larger and try again, as varying simulation parameters and observing what happens is part of what makes simulations worthwhile. If things blow up when we make a subtle change, we might have a bug in our code or, worse yet, a logic flaw in our design.</p>
<h4 class="h4" id="ch00lev2_30"><em><strong>Testing the Birthday Model</strong></em></h4>
<p class="noindent">The code we need is in <em>birthday.py</em>. Let’s run it to understand its output before walking through it. For example, here’s the output for a run asking for the probability of at least one match in a room with 11 people:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 birthday.py 11 minstd</span>
11 people in the room, probability of at least 1 match = 0.140430</pre>
<p class="indent">We’re told that the probability of at least one birthday match for a room of 11 people is about 14 percent. The second argument is the randomness source to use, here MINSTD. Feel free to experiment with other sources.</p>
<p class="indent">If we add a third argument, we can store the output and bring it into Python to understand what it means:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 birthday.py 11 minstd 11.npy</span>
11 people in the room, probability of at least 1 match = 0.142610
<span epub:type="pagebreak" id="page_82"/>
&gt; <span class="codestrong1">python3</span>
&gt;&gt;&gt; <span class="codestrong1">import numpy as np</span>
&gt;&gt;&gt; <span class="codestrong1">d = np.load("11.npy")</span>
&gt;&gt;&gt; <span class="codestrong1">d</span>
array([85739, 13462,   663,   125,    10,     0,     0,     1])</pre>
<p class="indent">The first line runs the code a second time. Notice that the probability changes slightly; random selection of birthdays will produce varying results that eventually converge to a mean after many repetitions of the simulation. We’ll experiment with this fact in a bit.</p>
<p class="indent">Next, I ran Python (and ignored the startup message) before importing NumPy and the output file, <em>11.npy</em>. The <span class="literal">d</span> array contains a histogram of the number of times that many birthday matches were found for 100,000 simulations where the index into <span class="literal">d</span> is the number. <a href="ch03.xhtml#ch03tab01">Table 3-1</a> shows the number of matches and how often they appeared.</p>
<p class="tabcap" id="ch03tab01"><strong>Table 3-1:</strong> Number of Matches and Frequency of Appearance</p>
<table class="table-h">
<colgroup>
<col style="width:35%"/>
<col style="width:35%"/>
<col style="width:30%"/>
</colgroup>
<thead>
<tr>
<th class="tab_th"><strong>Matches</strong></th>
<th class="tab_th"><strong>Count</strong></th>
<th class="tab_th"><strong>Percent</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="bg1">0</td>
<td class="bg1">85,739</td>
<td class="bg1">85.739</td>
</tr>
<tr>
<td class="bg">1</td>
<td class="bg">13,462</td>
<td class="bg">13.462</td>
</tr>
<tr>
<td class="bg1">2</td>
<td class="bg1">663</td>
<td class="bg1">0.663</td>
</tr>
<tr>
<td class="bg">3</td>
<td class="bg">125</td>
<td class="bg">0.125</td>
</tr>
<tr>
<td class="bg1">4</td>
<td class="bg1">10</td>
<td class="bg1">0.010</td>
</tr>
<tr>
<td class="bg">5</td>
<td class="bg">0</td>
<td class="bg">0.000</td>
</tr>
<tr>
<td class="bg1">6</td>
<td class="bg1">0</td>
<td class="bg1">0.000</td>
</tr>
<tr>
<td class="bg">7</td>
<td class="bg">1</td>
<td class="bg">0.001</td>
</tr>
</tbody>
</table>
<p class="indent">In 85.7 percent of the cases, when 11 people were in the room no two shared a birthday. Likewise, 13.5 percent of the cases resulted in a single pair sharing a birthday. Finally, in one run out of 100,000, seven pairs of people shared a birthday. This is the nature of randomness: sometimes remarkable things happen.</p>
<p class="indent">Next, I ran <em>birthday.py</em> five times, once for each of the randomness sources built into <span class="literal">RE</span>, and always with 23 people in the room. The average probability returned was 0.507478 or 50.7 percent. This is the first number of people to return a probability greater than 50 percent; therefore, to answer the question at the beginning of this section, we need 23 people in a room, on average, to have a greater than 50 percent chance that at least two of them share a birthday.</p>
<p class="indent"><span epub:type="pagebreak" id="page_83"/>Let’s try to visualize what’s happening here (<a href="ch03.xhtml#ch03fig04">Figure 3-4</a>).</p>
<div class="image"><img alt="Image" id="ch03fig04" src="../images/03fig04.jpg"/></div>
<p class="figcap"><em>Figure 3-4: The probability of at least one match as a function of number of people (left) and the histogram of matches by people in the room (right)</em></p>
<p class="indent">The left-hand side of <a href="ch03.xhtml#ch03fig04">Figure 3-4</a> shows the probability of one or more matches as a function of the number of people in the room. The vertical line is 23 people, and the dashed horizontal line is 50 percent. As claimed, 23 people is the minimum number needed to exceed 50 percent.</p>
<p class="indent">The right side of <a href="ch03.xhtml#ch03fig04">Figure 3-4</a> presents three histograms showing the fraction of runs returning the indicated number of matches. The bars are offset to prevent overlapping, but the leftmost bar is on the actual number of matches. When there are only 10 people in the room, the probability of no match is high and more than one match is essentially zero. For 23 people, one match is relatively common, two less so, and three pairs happen about 3 percent of the time. With 40 people, we’re past the 23-person transition, so it’s more likely than not to have multiple matches.</p>
<h4 class="h4" id="ch00lev2_31"><em><strong>Implementing the Birthday Model</strong></em></h4>
<p class="noindent">Let’s take a walk through <em>birthday.py</em>, shown in <a href="ch03.xhtml#ch03list01">Listing 3-1</a>.</p>
<pre class="pre">import sys
import numpy as np
from RE import *

def Simulate(rng, M):
    matches = []
    for n in range(100_000):
        match = 0
        bdays = rng.random(M)
        for i in range(M-1):
            for j in range(i+1,M):
                if (bdays[i] == bdays[j]):
                    match += 1
        matches.append(match)
<span epub:type="pagebreak" id="page_84"/>
    matches = np.array(matches)
    return np.bincount(matches)

people = int(sys.argv[1])
rng = RE(kind=sys.argv[2], low=0, high=365, mode="int")
matches = Simulate(rng, people)
prob = matches[1:].sum() / matches.sum()
print("%d people in the room, probability of at least 1 match = %0.6f" % (people, prob))
if (len(sys.argv) == 4):
    np.save(sys.argv[3], matches)</pre>
<p class="list" id="ch03list01"><em>Listing 3-1: Simulate checking birthdays for multiple people in a room</em></p>
<p class="indent">As with <em>sim_pi.py</em>, all the action is in <span class="literal">Simulate</span>. The code at the bottom of <a href="ch03.xhtml#ch03list01">Listing 3-1</a> parses the command line to get the number of people in the room along with the randomness source, one of those supported by <span class="literal">RE</span> or a filename, and, if specified, the name of an output file (a NumPy array). Note that <span class="literal">rng</span> is configured to return integers in [0, 365).</p>
<p class="indent">The randomness source (<span class="literal">rng</span>) and the number of people in the room are passed to <span class="literal">Simulate</span>. The return value is a histogram of the number of times that many matches occurred in the fixed 100,000 simulations (<span class="literal">matches</span>). The first element of <span class="literal">matches</span> is the number of times there were no matches, so the sum of all remaining elements divided by the sum of all elements is the probability of one or more matches (<span class="literal">prob</span>). The code then displays the probability and writes the histogram to disk if requested.</p>
<p class="indent">In <span class="literal">Simulate</span>, <span class="literal">M</span> is the number of people in the room, fixed for all 100,000 simulations. <span class="literal">Matches</span> will hold the outcome of each simulation, or the number of matches found.</p>
<p class="indent">The first <span class="literal">for</span> loop covers the simulations. For each simulation, a random set of birthdays is selected (<span class="literal">bdays</span>), one for each of the <span class="literal">M</span> persons in the room. Then, the double loop over <span class="literal">i</span> and <span class="literal">j</span> compares the <span class="literal">i</span>th person’s birthday with all others, counting each <span class="literal">match</span>. The loop limits for <span class="literal">i</span> and <span class="literal">j</span> avoid double-counting; if the <span class="literal">i</span>th person’s birthday matches the <span class="literal">j</span>th person’s, then the <span class="literal">j</span>th person’s will match the <span class="literal">i</span>th person’s, which has already been counted. When all pairs of people have been compared, the count in <span class="literal">match</span> is appended to <span class="literal">matches</span>.</p>
<p class="indent">Finally, after all 100,000 simulations, the list <span class="literal">matches</span> is turned into a NumPy array and passed to <span class="literal">np.bincount</span> to count the occurrences of each number of matches.</p>
<p class="indent">Is <em>birthday.py</em> a fair simulation? Does it do what we expect? As Hartl says, does it “eliminate extraneous detail in order to focus on the essentials”? The essential task here is to pick birthdays fairly once the number of people in the room is fixed. We assumed that birthdays are uniformly distributed throughout the year—a reasonable assumption.</p>
<p class="indent">The simulations we’ve discussed so far are warm-ups. Let’s kick things up a notch and explore what is, surely, the most important process in the world, at least to the myriad of lifeforms on this planet: evolution.</p>
<h3 class="h3" id="ch00lev1_21"><span epub:type="pagebreak" id="page_85"/><strong>Simulating Evolution</strong></h3>
<p class="noindent">The evolution of organisms is a complex process affected by genetic and environmental factors. In this section, we’ll explore two factors: natural selection and genetic drift.</p>
<p class="indent"><em>Natural selection</em>, described by Darwin in the 19th century, is often characterized as “survival of the fittest.” It posits that in an environment, organisms whose <em>genotype</em> (genetic code) leads to an increased likelihood of survival and reproduction are those more likely to pass their genes on to succeeding generations. In this way, over time, the characteristics of the organism are modified, often eventually leading to organisms that can no longer breed with each other—that is, new species.</p>
<p class="indent">While natural selection relates to improved likelihood of survival and reproduction, <em>genetic drift</em> is an effect caused by environmental changes that isolate a small population of organisms from the larger population. In genetic drift, the subpopulation of organisms present during the time of isolation will have a different mix of genes that can cause rapid changes in the overall gene pool, often leading to new species.</p>
<p class="indent">We want to simulate the essential components of natural selection and genetic drift. Let’s begin with the former; once we simulate natural selection, simulating genetic drift becomes that much clearer.</p>
<h4 class="h4" id="ch00lev2_32"><em><strong>Natural Selection</strong></em></h4>
<p class="noindent">Here are the requirements for simulating natural selection:</p>
<ol>
<li class="noindent">We need a population of organisms, each consisting of a collection of genes. An organism’s genes determine its fitness to the environment.</li>
<li class="noindent">We need an environment and some way to characterize it in terms of how well organisms are adapted to it. Additionally, we need a measure of fitness for this environment.</li>
<li class="noindent">We need to simulate natural selection’s two most important tools: breeding between organisms (<em>crossover</em>) and random <em>mutation</em>. This simulation must be affected by an organism’s level of fitness to the environment.</li>
<li class="noindent">We need to step from generation to generation so we can monitor the population over time.</li>
<li class="noindent">Finally, we need to easily visualize the characteristics of the population as it evolves across generations.</li>
</ol>
<p class="noindent">Let’s work through each statement.</p>
<h5 class="h5"><strong>The Organisms</strong></h5>
<p class="noindent">Our organisms have six genes in their genomes, each with 16 possible variants or <em>alleles</em>. Therefore, an organism is a vector of six elements, each [0, 15]. These numbers will become clear as we proceed.</p>
<h5 class="h5"><span epub:type="pagebreak" id="page_86"/><strong>The Environment</strong></h5>
<p class="noindent">We’ll define our environment by a set of genes that correspond to the “ideal” organism, the one best suited to the environment. In nature, most organisms are well suited to their environment; if they weren’t, they’d quickly go extinct. However, in the spirit of simulation, we’ll pick a set of genes to be the “best” and use them as a proxy for the environment.</p>
<p class="indent">We’ll use the distance between the environment’s gene vector and an organism’s gene vector as a measure of the organism’s fitness. The smaller this distance, the fitter the organism. While there are many possible definitions of “distance” when discussing vectors (points), we’ll use the <em>Euclidean distance</em>: the straight line distance between two points. We’ll imagine each gene vector to be the coordinates of a point in a six-dimensional space.</p>
<p class="indent">If the environment’s gene vector is <strong>e</strong> = (<em>e</em><sub>0</sub>, <em>e</em><sub>1</sub>, <em>e</em><sub>2</sub>, <em>e</em><sub>3</sub>, <em>e</em><sub>4</sub>, <em>e</em><sub>5</sub>) and the organism’s is <strong>x</strong> = (<em>x</em><sub>0</sub>, <em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, <em>x</em><sub>3</sub>, <em>x</em><sub>4</sub>, <em>x</em><sub>5</sub>), then the Euclidean distance between them is:</p>
<div class="image1"><img alt="Image" id="ch03equ1" src="../images/f0086-01.jpg"/></div>
<p class="noindent">In other words, it’s the square root of the sum of the squares of the differences, coordinate by coordinate. Here, each coordinate is an integer in [0, 15] to represent the selected allele for that gene. In addition, we’ll define a minimum distance to interpret as “good enough.”</p>
<h5 class="h5"><strong>Crossover and Mutation</strong></h5>
<p class="noindent">Sexual reproduction is a brilliant method for mixing genes and creating diversity in the gene pool. Our organisms will breed by crossover, which picks a position along the genome and copies all the genes of the first parent up to that point, followed by all the remaining genes from the second parent. The new combination becomes the genetic code of the offspring. Finally, we’ll apply random mutation by picking a gene and randomly changing its value. <a href="ch03.xhtml#ch03fig05">Figure 3-5</a> illustrates the crossover and mutation process.</p>
<div class="image"><img alt="Image" id="ch03fig05" src="../images/03fig05.jpg"/></div>
<p class="figcap"><span epub:type="pagebreak" id="page_87"/><em>Figure 3-5: Crossover and mutation producing a new offspring organism (frog image in the public domain, courtesy of Wikimedia Commons)</em></p>
<p class="indent">Our organisms aren’t frogs, but you get the idea. Two organisms create an offspring using the first two genes of one and the final four of the other. Then, mutation changes one of the genes from 10 to 2.</p>
<p class="indent">To simulate fitness influencing reproduction, we’ll bias the selection of organisms such that those with smaller fitness values are more likely to breed. We’ll do this with a <em>beta distribution</em>, which is included with NumPy. A beta distribution uses two parameters to affect the shape reflecting the overall histogram of samples. If both parameters, <em>a</em> and <em>b</em>, are equal to 1, the beta distribution mimics a uniform distribution. If the <em>b</em> parameter is increased slightly, the distribution is modified, making it more likely that values closer to zero will be selected.</p>
<p class="indent"><span epub:type="pagebreak" id="page_88"/>Therefore, when breeding the next generation, we’ll select population members with indices closer to zero. We’ll sort the population by fitness, with fitter organisms nearer to the top of the 2D array of organisms, in which each row is an organism and each column a gene.</p>
<p class="indent">The net result is that fitter organisms are more likely to breed. Therefore, generation by generation, we expect the entire population to inch closer to the ideal fitness for the environment.</p>
<h5 class="h5"><strong>The Population from Generation to Generation</strong></h5>
<p class="noindent">I previously alluded to keeping the population in a 2D array. We’ll fix the population size at 384 organisms; why will become apparent in time. Therefore, a population of organisms becomes a 2D array of 384 rows and 6 columns. Each generation will breed another 384 organisms. Put another way, our organisms are seasonal; they live a season (time step) and die after spawning the next generation. Population geneticists often use such a discrete model.</p>
<p class="indent">Therefore, a simulation implements each of these steps:</p>
<ol>
<li class="noindent">Select the initial population at random.</li>
<li class="noindent">Select an environment at random.</li>
<li class="noindent">For <em>N</em> generations, calculate the per-organism fitness, sort the population by fitness, and breed each member of the population using crossover and mutation.</li>
<li class="noindent">Create output based on the sequence of populations.</li>
</ol>
<h5 class="h5"><strong>Visualization</strong></h5>
<p class="noindent">Every generation produces a population of 384 organisms, each with 6 genes represented by one of 16 alleles. Now we’ll learn why the population is always 384 organisms with 6 genes. We want to produce as output an image where each row of the image shows the population, one organism per pixel, along with the environment. Therefore, the output image will have 384 columns plus additional columns to show the environment. Each pixel gets its color from the genetic code of the corresponding organism with genes mapping to 4 bits of a 24-bit RGB color value, as in <a href="ch03.xhtml#ch03fig06">Figure 3-6</a>.</p>
<div class="image"><img alt="Image" id="ch03fig06" src="../images/03fig06.jpg"/></div>
<p class="figcap"><em>Figure 3-6: Mapping genes to RGB colors</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_89"/>In the figure, the gene vector becomes a forest green pixel. As the generations evolve, we expect the population to move closer to the color of the environment. Of course, random mutation will have some say in the matter, as well as fitness; we’ll experiment with both.</p>
<p class="indent">Let’s begin with a static environment.</p>
<h4 class="h4" id="ch00lev2_33"><em><strong>Static World</strong></em></h4>
<p class="noindent">We’ll dive into code after our experiments. To run an experiment with a static environment, use <em>darwin_static.py</em>:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 darwin_static.py 500 60 0.01 4 minstd darwin_static.png 73939133</span></pre>
<p class="noindent">There are several parameters, common to most of our experiments:</p>
<div class="bqparan">
<p class="noindentin"><span class="codestrong">500</span>   Number of generations (rows)</p>
<p class="noindentin"><span class="codestrong">60</span>   Fitness bias, [0, 1000]</p>
<p class="noindentin"><span class="codestrong">0.01</span>   Mutation probability</p>
<p class="noindentin"><span class="codestrong">4</span>   “Good enough” threshold</p>
<p class="noindentin"><span class="codestrong">minstd</span>   Generator name, or filename</p>
<p class="noindentin"><span class="codestrong">darwin_static.png</span>   Output image name</p>
<p class="noindentin"><span class="codestrong">73939133</span>   Seed value (optional)</p>
</div>
<p class="indent">While the simulation runs, you’ll see the average fitness per generation. As the generations evolve, the fitness decreases until it hovers around the “good enough” value. When the simulation ends, then take a look at <em>darwin_static.png</em>. Color is required, but the image begins something like <a href="ch03.xhtml#ch03fig07">Figure 3-7</a>.</p>
<div class="image"><img alt="Image" id="ch03fig07" src="../images/03fig07.jpg"/></div>
<p class="figcap"><em>Figure 3-7: Visualizing a static world</em></p>
<p class="noindent">Even with the seed specified, there will be variation between runs because we’re using the NumPy beta distribution function, and it doesn’t pay attention to our seed value.</p>
<p class="indent">Read the image from top to bottom. The top row is the initial, randomly generated population of 384 organisms. Each subsequent row is another <span epub:type="pagebreak" id="page_90"/>generation bred from the one above, each time sorted by fitness so that fitter organisms are closer to the left edge. The stripe on the far left is the environment, represented by the ideal genome’s color.</p>
<p class="indent">As you follow down the rows of the image, the population becomes more like the ideal environment. However, it never collapses to match the environment exactly. Three command line arguments affect how quickly and consistently the population matches the environment: the fitness bias, the mutation probability, and the “good enough” threshold. Let’s understand each.</p>
<h5 class="h5"><strong>The Fitness Bias</strong></h5>
<p class="noindent">The fitness bias is an integer in the range 0 to 1,000. As we’ll learn in the code, this value is divided by 1,000 and added to the second beta distribution parameter. The purpose is to increase the fitness of organisms with genomes that are better suited to the environment. If the fitness bias is 0, there’s no reproductive benefit to being better suited to the environment. As the bias increases, the reproductive benefit increases to cause the population to approach the environment’s ideal more rapidly.</p>
<p class="indent">As an example, run <em>darwin_static.py</em> a second time and change only the fitness bias from 60 to 600. The population should approach the environment’s ideal in only a few generations. Change the fitness bias to 0 and run again. What do you notice now? The population isn’t able to improve because a fitness bias of 0 means no reproductive benefit based on an organism’s genome. If you make the fitness bias 15, you might need about 1,500 generations, but you should see the population eventually adapt to the environment. Even a small reproductive advantage matters over the long haul.</p>
<h5 class="h5"><strong>The Mutation Probability</strong></h5>
<p class="noindent">Now, set the fitness bias to 60 and adjust the mutation rate, expressed as a probability. For example, a mutation rate of 0.01 gives each newly bred organism a 1 percent chance of undergoing random mutation. A 1 percent mutation rate is exceptionally high compared to living animals, but we need to see the effects we’re after without millions of generations.</p>
<p class="indent">Change the mutation rate of <em>darwin_static.py</em> to 0; this means each new generation will be created by crossover only. Run a few times and look at the output. What do you notice? The population fitness should hit 4 (a distance of 4 from the ideal genome) and remain there indefinitely. It can’t do anything else because the genomes are already “ideal,” so there’s nothing left to change; pick any two for crossover, and, regardless of the crossover point, the offspring’s genome will still be identical to the parents’.</p>
<p class="indent">Let’s see how sensitive the population is to mutation. Alter the mutation rate from 0.01 to 0.1 (10 percent) and run a few more times. Notice that the population adapts to the environment, but never completely. Indeed, as you look down the rows of the output image, you’ll likely see regions where many members of the population were adapted, but a new mutation appeared and quickly altered the balance so that the population had to adapt again in the following generations.</p>
<p class="indent"><span epub:type="pagebreak" id="page_91"/>My runs that used a mutation rate of 0.1 generally ended with the population mean fitness in the 7.5 to 8.5 range, much higher than the 4 found by no mutation. If you change the mutation rate to 0.2 or even 0.8, the population should have a harder time adapting to the environment because mutations continually push the population away from the ideal. Changing the mutation to a lower rate, say 0.005, leads the population to adapt well, but over time (that is, moving down the rows of the output image), you’ll see that small groups of mutants appear, then adapt, then appear again with another mutation. In the output image, these groups appear as splashes of color on the right side of the image—the least fit organisms with the lowest probability of breeding.</p>
<h5 class="h5"><strong>The “Good Enough” Threshold</strong></h5>
<p class="noindent">The final command line argument is the mysterious “good enough” value, the minimum distance between the environment’s ideal genome and an organism’s. When calculating the population’s fitness, any distance less than this value is set to this value. Experiment by changing the “good enough” value while holding the fitness bias and mutation rate fixed (for example, at 60 and 0.01, respectively). The higher the “good enough” value, the coarser the population’s adaptation to the environment. Set it to 0 and the population will collapse to the ideal, quickly if the fitness bias is larger; if the mutation rate is 0, the population will stay there.</p>
<p class="indent">I recommend experimenting with <em>darwin_static.py</em> until you develop an intuitive feel for how changes to the fitness bias, mutation rate, and “good enough” value affect the outcome. Try to predict what you expect to see in the output image ahead of time. When using simulations, it’s vital to vary parameters, especially by pushing them to their limits. Not only does this help with understanding the processes the simulation is attempting to capture, it serves as a sanity check on the simulation itself, as a way to perhaps uncover weaknesses or errors making the results less valid.</p>
<p class="indent">In the real world, environments are not static, at least not for timeframes over which evolution typically acts (though rapid evolution is possible). Let’s add a new feature to the simulation, allowing the environment to change slowly with time to understand how such change affects the population.</p>
<h4 class="h4" id="ch00lev2_34"><em><strong>Gradually Changing World</strong></em></h4>
<p class="noindent">The code in <em>darwin_slow.py</em> is almost identical to the code of the previous section, but it introduces a new feature: the environment will change, slightly, at an interval specified on the command line. For example:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 darwin_slow.py 500 60 0.01 4 0.01 mt19937 darwin_slow.png 66</span></pre>
<p class="noindent">The new parameter is the 0.01 before the pseudorandom generator (<span class="literal">mt19937</span>). It says to slightly modify the environment on each generation with a probability of 1 percent. The <em>darwin_slow.py</em> example leads to an output image where the environment changes four times. The output image is similar to <span epub:type="pagebreak" id="page_92"/>the static case, but each environment transition is marked with a black line on the left. For example, the first two transitions appear as in <a href="ch03.xhtml#ch03fig08">Figure 3-8</a>.</p>
<div class="image"><img alt="Image" id="ch03fig08" src="../images/03fig08.jpg"/></div>
<p class="figcap"><em>Figure 3-8: The environment in transition</em></p>
<p class="indent">Detail will be visible only in the full-color image; see <em>darwin_slow.png</em> on the book’s GitHub repository. The initial, random population is adapting to the environment when the first transition occurs. The population then quickly adapts to the new environment when the environment changes again.</p>
<p class="indent">If you look at the full <em>darwin_slow.png</em> image—hopefully using an image view allowing full resolution horizontally—you’ll notice that after the second change to the environment, the population adapts quite well, but it takes several generations. The visual effect is to smear the colors to the right, where the less fit organisms are listed. I recommend running the simulation several times without the fixed seed of 66 to observe the overall effect with different colors. Then, explore how modifications to the fitness bias and mutation rate play out as the smooth changes to the environment happen. To get you started, observe what happens on multiple runs with the parameters listed in <a href="ch03.xhtml#ch03tab02">Table 3-2</a>.</p>
<p class="tabcap" id="ch03tab02"><strong>Table 3-2:</strong> Parameters to Try</p>
<table class="table-h">
<colgroup>
<col style="width:30%"/>
<col style="width:30%"/>
<col style="width:40%"/>
</colgroup>
<thead>
<tr>
<th class="tab_th"><strong>Fitness bias</strong></th>
<th class="tab_th"><strong>Mutation rate</strong></th>
<th class="tab_th"><strong>Environment probability</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="bg1">600</td>
<td class="bg1">0.01</td>
<td class="bg1">0.300</td>
</tr>
<tr>
<td class="bg">60</td>
<td class="bg">0.01</td>
<td class="bg">0.300</td>
</tr>
<tr>
<td class="bg1">1</td>
<td class="bg1">0.01</td>
<td class="bg1">0.005</td>
</tr>
</tbody>
</table>
<p class="indent">The first two parameter sets illustrate the effect of a rapidly changing environment with both strong and not so strong advantages to fitter organisms. The final set of parameters uses the weakest of reproductive advantages, but couples it with an almost static environment.</p>
<p class="indent">Slowly varying environments give organisms time to adapt. However, throughout the Earth’s long history, not all environmental changes were slow; some were quite sudden, even happening overnight. Let’s simulate a catastrophe.</p>
<h4 class="h4" id="ch00lev2_35"><span epub:type="pagebreak" id="page_93"/><em><strong>Catastrophic World</strong></em></h4>
<p class="noindent">One fine day, some 66 million years ago, the lifeforms of Earth were minding their own business when a giant asteroid rudely interrupted and, as a consequence, ended the 100-million-year-plus reign of the nonavian dinosaurs. Bad news for them; good news for us. A catastrophe happened, and life responded and appeared quite different after the impact. The same thing happened about 252 million years ago during The Great Dying when life nearly perished. The world after the extinction looked very different from the world before.</p>
<p class="indent">We’ve simulated gradual environmental change; now, let’s give the simulation a hard knock and see what happens. We need <em>darwin_catastrophic.py</em>. Give it a go:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 darwin_catastrophic.py 500 60 0.01 4 0.01 pcg64 darwin_catastrophic.png 12345</span></pre>
<p class="indent">The parameters are identical to those of <em>darwin_slow.py</em>, the difference being that whenever the environment should change, it does so by picking an entirely new ideal environment genome. The transitions are stark. The image generated is similar to that of <em>darwin_slow.py</em>, but without the horizontal black line to mark the transitions. In this case, the transitions are, generally, quite obvious. To see what I mean, run the code, or look at <em>darwin_catastrophic.png</em> on the book’s GitHub page.</p>
<p class="indent">Experiment with the simulation parameters to explore the consequences. For example, change the number of generations to 2,000 and look at the output image in its entirety by zooming out. The population’s delayed response to each environmental catastrophe is plain to see.</p>
<p class="indent">Our final simulation introduces genetic drift. How do suddenly split populations fare in adapting to new environments?</p>
<h4 class="h4" id="ch00lev2_36"><em><strong>Genetic Drift</strong></em></h4>
<p class="noindent">A <em>population bottleneck</em> happens when a population experiences a sudden reduction in size. One kind of population bottleneck is the <em>founder effect</em>, which occurs when a small population splits from a larger population. The random mix of alleles in the new, smaller population might dramatically affect the long-term survival and evolution of the organisms. The code in <em>darwin_drift.py</em> simulates genetic drift due to the founding of a new, smaller population.</p>
<p class="indent">The code is similar to the previous examples, but with a twist. First, a larger population (all 384 organisms) evolves for generations trying to adapt to its environment. Then, a specified fraction of the whole population “splits off” to become a new population. The two populations now evolve separately. Imagine a colony of organisms stranded on an island and no longer able to breed with their mainland cousins. To make things interesting, after the split, a catastrophe happens, so we can observe how well (or poorly) the two populations cope with the sudden change in environment.</p>
<p class="noindent"><span epub:type="pagebreak" id="page_94"/>For example:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 darwin_drift.py 500 60 0.01 4 0.2 pcg64 darwin_drift 1337</span></pre>
<p class="indent">The 0.2 before <span class="literal">pcg64</span> now refers to the fraction of the population that will split off to form the new population; that is, 20 percent of the population, randomly selected, breaks off to create the new population, leaving the other 80 percent as the larger population.</p>
<p class="indent">Unlike our other simulations, <em>darwin_drift.py</em> expects a base filename (<span class="literal">darwin_drift</span>) instead of a complete filename. The code outputs an image, <em>darwin_drift.png</em>, along with a plot of the mean population fitness by generation, <em>darwin_drift_plot.png</em>. As before, there are 500 generations using a fitness bias of 60 and a mutation probability of 1 percent.</p>
<p class="indent">So what does our image look like? Yet again, I suggest you review the color image from the book’s GitHub repository, but part of the output is in <a href="ch03.xhtml#ch03fig09">Figure 3-9</a>.</p>
<div class="image"><img alt="Image" id="ch03fig09" src="../images/03fig09.jpg"/></div>
<p class="figcap"><em>Figure 3-9: Visualizing genetic drift</em></p>
<p class="noindent">This snippet of the larger image shows part of the simulation after the split. The smaller population is on the left, with the larger on the right. Also, if you examine the environment, there is a sudden catastrophe about one-sixth of the way down. The two populations respond differently to the disaster. This is particularly visible in the color version of the image.</p>
<p class="indent">Before the catastrophe, both populations were reasonably well adapted to their environment. However, the smaller population cannot recover after the catastrophe or successfully adjust to the new environment. The larger population does eventually adapt. The color version of the image clearly shows that the smaller population sometimes produces generations where organisms are adapting to the new environment, but they never last long. This effect mirrors reality; small populations are often very fragile and easily harmed by rapid environmental change because they lack the genetic diversity to adapt in time.</p>
<p class="indent"><span epub:type="pagebreak" id="page_95"/><a href="ch03.xhtml#ch03fig10">Figure 3-10</a> tracks mean population fitness as a function of generation.</p>
<div class="image"><img alt="Image" id="ch03fig10" src="../images/03fig10.jpg"/></div>
<p class="figcap"><em>Figure 3-10: Mean population fitness by generation before and after the catastrophe</em></p>
<p class="indent">The founder effect event happens around generation 145, where two lines appear. The thicker line follows the smaller population. At first, both populations are relatively fit to the environment, which hasn’t yet changed, though it could be argued that there is more variation in fitness in the smaller population.</p>
<p class="indent">The catastrophe occurs around generation 318. Immediately afterward, both populations’ fitness scores increase dramatically. Remember, a lower fitness score is better. Subsequent generations begin to adapt to the new environment, but not at the same rate. The larger population, still 80 percent of the original size, adapts to the new environment over time; however, the smaller population fails to do so, at least for the 500 generations simulated. For most runs of <em>darwin_drift.py</em>, the smaller population fails to adapt to the new environment as well as the larger population. Sometimes the reverse happens. We’ll talk about this effect in the discussion.</p>
<p class="indent">What happens if the population splits in half (0.5)? Or, what if the new population is a tiny fraction, say 5 or 10 percent? It’s probably easiest to experiment in these cases by fixing the fitness bias and mutation rate. Then, fix the population fraction and vary those parameters.</p>
<p class="indent">In <a href="ch03.xhtml#ch03fig09">Figure 3-9</a>, when the population splits, the members of the new, smaller population are selected at random. Therefore, they typically have an uneven representation of the genotypes in the larger population due to chance. That difference might mean that uncommon genotypes now have an opportunity to become more common.</p>
<p class="indent"><span epub:type="pagebreak" id="page_96"/>This effect is illustrated by the code in <em>drift.py</em>. First, a “population” of 10,000 digits, [0, 9], are selected. Then 20 subpopulations are constructed by choosing 50 members of the larger population at random. Finally, the mean of the larger population is displayed along with the mean of 20 subpopulations. If the mix of digits is the same in each, then the means will be quite close to each other. They are not:</p>
<pre class="pre">Population mean = 4.562900
Sub-population means:
  3.60, 4.42, 4.52, 4.82, 4.36, 5.40, 4.72, 4.54, 4.24, 4.28,
  4.76, 4.66, 4.98, 4.90, 4.50, 4.50, 5.10, 4.44, 4.30, 4.62</pre>
<p class="noindent">The subpopulation means range from a low of 3.60 to a high of 5.40. Uniformly selected digits should give a population mean of 4.5, which is close to the larger population mean. The subpopulations, due to random chance, represent very different collections of digits (genomes). Now we understand why population bottlenecks lead to genetic drift.</p>
<h4 class="h4" id="ch00lev2_37"><em><strong>Testing the Simulations</strong></em></h4>
<p class="noindent">We used four different sets of code for the previous simulations. Rather than detail each, thereby committing the literary equivalent of “death by PowerPoint,” let’s walk through one of them here and present snippets from the others as needed.</p>
<p class="indent">Look at <a href="ch03.xhtml#ch03list02">Listing 3-2</a>, which contains the critical parts of <em>darwin_static.py</em>.</p>
<pre class="pre"><span class="ent">❶</span> ngen = int(sys.argv[1])
   advantage = int(sys.argv[2])
   mutation = float(sys.argv[3])
   good = float(sys.argv[4])
   kind = sys.argv[5]
   oname = sys.argv[6]
   if (len(sys.argv) == 8):
       seed = int(sys.argv[7])
       rng = RE(kind=kind, seed=seed)
   else:
       rng = RE(kind=kind)
 
<span class="ent">❷</span> npop = 384
   pop = np.zeros((npop, 6))
   for i in range(npop):
    <span class="ent">❸</span> pop[i,:] = (16*rng.random(6)).astype("uint8")
   environment = (16*rng.random(6)).astype("uint8")
   hpop = np.zeros((ngen,npop,6))
   henv = np.zeros((ngen,6))

<span class="ent">❹</span> for g in range(ngen):
    <span class="ent">❺</span> fitness = np.zeros(npop)
       for i in range(npop):<span epub:type="pagebreak" id="page_97"/>
           d = np.sqrt(((pop[i]-environment)**2).sum())
           if (d &lt; good):
               d = good
           fitness[i] = d
    
    <span class="ent">❻</span> idx = np.argsort(fitness)
       pop = pop[idx]
       fitness = fitness[idx]

    <span class="ent">❼</span> hpop[g,:,:] = pop
       henv[g,:] = environment
       print("%6d: fitness = %0.8f" % (g, fitness.mean()))

    <span class="ent">❽</span> nxt = []
       for i in range(npop):
           nxt.append(Mate(pop,fitness,advantage))
       pop = np.array(nxt)</pre>
<p class="list" id="ch03list02"><em>Listing 3-2: Simulating a static environment</em></p>
<p class="indent">I’ve excluded comments and code related to generating the output image. Please review the file itself if you’re curious about how those parts work. I recommend reading through at least <span class="literal">MakeRGB</span> to understand the mapping from genes to RGB color values.</p>
<p class="indent">The code falls naturally into three parts: parsing the command line <span class="ent">➊</span>, setting up the simulation <span class="ent">➋</span>, and running the simulation <span class="ent">➍</span>. In the first part, the randomness engine (<span class="literal">rng</span>) is configured, with or without a seed, to return floats in [0, 1). The engine is used for different things, so it’s better to use only the basic range and adjust the bounds and data type as needed.</p>
<p class="indent">Then the initial population (<span class="literal">pop</span>) of 384 organisms (<span class="literal">npop</span>) is created <span class="ent">➋</span>. Each organism is given a randomly generated genome <span class="ent">➌</span>. The <span class="literal">environment</span> is similarly defined. The final two variables, <span class="literal">hpop</span> and <span class="literal">henv</span>, track the evolution of the population and, for other variants of the code, the environment. Note the use of a 3D array imagined here as a vector of 2D arrays, each holding the population for that generation. The output image is produced using both <span class="literal">hpop</span> and <span class="literal">henv</span>.</p>
<p class="indent">The simulation is now ready; the initial population and environment have been defined. The main loop <span class="ent">➍</span> evaluates each generation. The loop body has four paragraphs: calculate the per-organism fitness <span class="ent">➎</span>, sort the population by fitness <span class="ent">➏</span>, keep a copy of the population for image generation <span class="ent">➐</span>, and, finally, breed the next generation <span class="ent">➑</span>.</p>
<p class="indent">Let’s go through each of those steps. To calculate fitness, we subtract each organism’s genome from that of the environment and square and sum the result across all genes before applying the square root. This is the NumPy version of <a href="ch03.xhtml#ch03equ1">Equation 3.1</a>. Fitness in hand, <span class="literal">idx</span> sorts both the population and the fitness vector so that fitter organisms are closer to the beginning of the population <span class="ent">➏</span>. Then <span class="literal">hpop</span> stores the sorted population and environment for <span epub:type="pagebreak" id="page_98"/>the output image <span class="ent">➐</span>. The mean population fitness is also printed. As the population evolves, this mean value should decrease, depending on the mutation rate and the fitness bias.</p>
<p class="indent">The last thing to do for this generation is replace it <span class="ent">➑</span>. Repeated calls to <span class="literal">Mate</span> breed a new population of <span class="literal">npop</span> organisms from the existing population as shown in <a href="ch03.xhtml#ch03list03">Listing 3-3</a>.</p>
<pre class="pre">def Mate(pop, fitness, advantage):
    a = advantage / 1000
    i = int(len(pop)*np.random.beta(1,1+a))
    j = i
    while (j == i):
        j = int(len(pop)*np.random.beta(1,1+a))
    c = int(6*rng.random())
    org = np.hstack((pop[i][:c], pop[j][c:]))
    
    if (rng.random() &lt; mutation):
        c = int(6*rng.random())
        org[c] = int(16*rng.random())
    return org</pre>
<p class="list" id="ch03list03"><em>Listing 3-3: Producing the next generation</em></p>
<p class="noindent">Here, <span class="literal">Mate</span> is given the current population and associated fitness (both sorted), along with the fitness bias (<span class="literal">advantage</span>). As mentioned, the fitness bias is divided by 1,000.</p>
<p class="indent">The function needs to select two distinct organisms, indices <span class="literal">i</span> and <span class="literal">j</span>, and then produce a new organism by crossover. A call to NumPy’s <span class="literal">beta</span> function returns a value in [0, 1), which, when scaled by the size of the population, will return an integer in [0, 383]. The <span class="literal">while</span> loop runs until a distinct second organism is selected (<span class="literal">j</span>).</p>
<p class="indent"><a href="ch03.xhtml#ch03fig11">Figure 3-11</a> shows beta distributions for different fitness bias values.</p>
<div class="image"><img alt="Image" id="ch03fig11" src="../images/03fig11.jpg"/></div>
<p class="figcap"><em>Figure 3-11: Beta distribution as altered by fitness bias value. Fit organisms are more likely to breed if the left portion of the distribution is higher than the right.</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_99"/>If the bias is 0, the beta distribution acts as a uniform distribution. There are 100 bins in the plot, so each will appear about 1 percent of the time (solid line). A relatively weak fitness bias of 60 favors small values, or fitter organisms, while strongly rejecting the least fit. Similarly, a bias of 900 selects most fit to least fit linearly.</p>
<p class="indent">Crossover selects a gene position, [0, 5], and constructs the offspring (<span class="literal">org</span>) by keeping the first <span class="literal">c</span> genes of one parent and adding in the remaining genes from the second. Then, if a random value is less than the global <span class="literal">mutation</span> threshold, a randomly selected gene is given a new value, [0, 15]. Finally, the function returns the new organism’s genome.</p>
<p class="indent">To recap: configure, then loop over generations evaluating the population’s current fitness before using that information to breed the next generation. Once the dust settles, generate the output image.</p>
<p class="indent">The file <em>darwin_slow.py</em>, which changes the environment as the population evolves, is nearly identical to <em>darwin_static.py</em>. The loop has one additional code paragraph after breeding the next generation:</p>
<pre class="pre">if (rng.random() &lt; eprob):
    offset = 2*rng.random(6)-1
    environment = environment + offset
    environment = np.maximum(0,np.minimum(15,environment))
    environment = (environment + 0.5).astype("uint8")</pre>
<p class="indent">Here, <span class="literal">eprob</span> is the probability of the environment changing, read from the command line. If the environment is to change, we add an <span class="literal">offset</span> vector to alter the ideal genome by ±1 for each gene. Compare this with the catastrophic environmental change from <em>darwin_catastrophic.py</em>:</p>
<pre class="pre">if (rng.random() &lt; eprob):
    environment = (16*rng.random(6)).astype("uint8")</pre>
<p class="indent">The final program, <em>darwin_drift.py</em>, is structurally similar to <em>darwin_catastrophic.py</em>, but after a set number of generations, the population splits into two. After this split, the environment is altered catastrophically. Although bookkeeping is involved, conceptually nothing new is happening.</p>
<h3 class="h3" id="ch00lev1_22"><strong>Exercises</strong></h3>
<p class="noindent">Use the following exercises as a springboard to expand your appreciation for the power of simulation. When working through them, entertain any “what if” questions that pop into your head:</p>
<ul>
<li class="noindent">Alter <em>sim_pi.py</em> to make two calls to <span class="literal">rng</span>, first to get the <em>x</em>-coordinates, then to get the <em>y</em>-coordinates. Do you notice any difference? Did you expect to?</li>
<li class="noindent">Modify <em>birthday.py</em> to use <em>N</em> = 1,000,000 or <em>N</em> = 10,000. Is there a noticeable difference?</li>
<li class="noindent"><span epub:type="pagebreak" id="page_100"/>We assumed that birthdays are uniformly distributed throughout the year. This isn’t strictly true, at least for Western countries. For those countries, September birthdays are more common. What happens to the true probability of two randomly selected people sharing a birthday in that case? Does it increase or decrease? You may wish to explore the file <em>birthday_true.py</em>, which uses data from the United Kingdom.</li>
<li class="noindent">How many people need to be at the party to have a 99 percent probability of at least one birthday match?</li>
<li class="noindent">Our evolution simulations assumed that all members of a generation bred and then died to produce a new generation that was the same size as the last. What happens if the least fit 10 percent die and do not reproduce while the fittest 2 percent breed twice?</li>
</ul>
<p class="noindentt">Bonus points:</p>
<p class="noindentt">In his 1889 book, <em>Calcul des probabilités</em>, Joseph Bertrand outlined three approaches for calculating the probability that a randomly selected chord of a circle is longer than the side of an equilateral triangle inscribed in the circle. The files <em>bertrand0.py</em>, <em>bertrand1.py</em>, and <em>bertrand2.py</em> implement simulations corresponding to the three approaches:</p>
<div class="bqparan">
<p class="noindentin"><em><strong>bertrand0.py</strong></em>   Use the chord defined by two randomly selected points on the circumference of the circle.</p>
<p class="noindentin"><em><strong>bertrand1.py</strong></em>   Use the chord perpendicular to a randomly selected point along a randomly selected radius of the circle.</p>
<p class="noindentin"><em><strong>bertrand2.py</strong></em>   Use a randomly selected point inside the circle as the midpoint of the chord.</p>
</div>
<p class="indent">Run all three approaches to select random chords of the circle. For example</p>
<pre class="pre">&gt; <span class="codestrong1">python3 bertrand0.py 500 b0.png mt19937 359</span>
Probability is approximately 154/500 = 0.3080000</pre>
<p class="noindent">produces the estimated probability along with a plot showing the selected chords, as shown in <a href="ch03.xhtml#ch03fig12">Figure 3-12</a>.</p>
<div class="image"><img alt="Image" id="ch03fig12" src="../images/03fig12.jpg"/></div>
<p class="figcap"><span epub:type="pagebreak" id="page_101"/><em>Figure 3-12: Selected chords when choosing points on the circumference of the circle</em></p>
<p class="indent">What is the estimated probability for each approach? Which one is correct? This is known as <em>Bertrand’s paradox</em>, and it serves as a cautionary tale to be careful when defining what we want to simulate and how we go about it. Review the code to see how the chords are selected.</p>
<h3 class="h3" id="ch00lev1_23"><strong>Summary</strong></h3>
<p class="noindent">In this chapter, we began an introductory exploration of models and simulations; we’ll continue to encounter various models throughout the book.</p>
<p class="indent">We started slowly, with simulations estimating the value of <em>π</em> by throwing darts and the number of people in a room, on average, necessary to have a better than 50 percent chance of at least two sharing a birthday. We then constructed a model to simulate two essential aspects of biological evolution: natural selection and genetic drift. We learned that even an incomplete model can be a useful tool that offers helpful insights.</p>
<p class="indent">Our simulations captured some essence of important evolutionary mechanisms, like natural selection and genetic drift, but a huge part of reality was missing: death and extinction. For example, many small populations kept evolving when they ought to have gone extinct. Extinction is natural; virtually every species that has ever lived is extinct (though there’s no reason for us to hurry the process along). Adding death and extinction would create a level of complexity to the simulation that doesn’t fit with what we can accomplish in this book. Regardless, the simulations of this section are practical and illustrative as far as they go. All analogies fail at some point—that doesn’t make them useless.</p>
<p class="indent">The next chapter continues our investigation of useful randomness by diving into the world of optimization. Can randomness be put to work in service of locating the best of something?</p>
<div class="note">
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>
<p class="notep"><em>There is no formal resolution to Bertrand’s paradox. Many people, including me, feel that p = 1/2 is the most reasonable answer.</em><span epub:type="pagebreak" id="page_102"/></p>
</div>
</body></html>