- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Inspecting and Modifying Data
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/chapterart.png)'
  prefs: []
  type: TYPE_IMG
- en: If I were to propose a toast to a newly minted class of data analysts, I’d raise
    my glass and say, “May your data arrive perfectly structured and free of errors!”
    In reality, you’ll sometimes receive data in such a sorry state that it’s hard
    to analyze without modifying it. This is called *dirty data*, a general label
    for data with errors, missing values, or poor organization that makes standard
    queries ineffective. In this chapter, you’ll use SQL to clean a set of dirty data
    and perform other useful maintenance tasks to make data workable.
  prefs: []
  type: TYPE_NORMAL
- en: Dirty data can have multiple origins. Converting data from one file type to
    another or giving a column the wrong data type can cause information to be lost.
    People also can be careless when inputting or editing data, leaving behind typos
    and spelling inconsistencies. Whatever the cause may be, dirty data is the bane
    of the data analyst.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll learn how to examine data to assess its quality and how to modify data
    and tables to make analysis easier. But the techniques you’ll learn will be useful
    for more than just cleaning data. The ability to make changes to data and tables
    gives you options for updating or adding new information to your database as it
    becomes available, elevating your database from a static collection to a living
    record.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin by importing our data.
  prefs: []
  type: TYPE_NORMAL
- en: Importing Data on Meat, Poultry, and Egg Producers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this example, we’ll use a directory of US meat, poultry, and egg producers.
    The Food Safety and Inspection Service (FSIS), an agency within the US Department
    of Agriculture, compiles and updates this database regularly. The FSIS is responsible
    for inspecting animals and food at more than 6,000 meat processing plants, slaughterhouses,
    farms, and the like. If inspectors find a problem, such as bacterial contamination
    or mislabeled food, the agency can issue a recall. Anyone interested in agriculture
    business, food supply chain, or outbreaks of foodborne illnesses will find the
    directory useful. Read more about the agency on its site at [https://www.fsis.usda.gov/](https://www.fsis.usda.gov/).
  prefs: []
  type: TYPE_NORMAL
- en: The data we’ll use comes from [https://www.data.gov/](https://www.data.gov/),
    a website run by the US federal government that catalogs thousands of datasets
    from various federal agencies ([https://catalog.data.gov/dataset/fsis-meat-poultry-and-egg-inspection-directory-by-establishment-name/](https://catalog.data.gov/dataset/fsis-meat-poultry-and-egg-inspection-directory-by-establishment-name/)).
    I’ve converted the Excel file posted on the site to CSV format, and you’ll find
    a link to the file *MPI_Directory_by_Establishment_Name.csv* along with other
    resources for this book at [https://nostarch.com/practical-sql-2nd-edition/](https://nostarch.com/practical-sql-2nd-edition/).
  prefs: []
  type: TYPE_NORMAL
- en: To import the file into PostgreSQL, use the code in [Listing 10-1](#listing10-1)
    to create a table called `meat_poultry_egg_establishments` and use `COPY` to add
    the CSV file to the table. As in previous examples, use pgAdmin to connect to
    your `analysis` database, and then open the Query Tool to run the code. Remember
    to change the path in the `COPY` statement to reflect the location of your CSV
    file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 10-1: Importing the FSIS Meat, Poultry, and Egg Inspection Directory'
  prefs: []
  type: TYPE_NORMAL
- en: The table has 10 columns. We add a natural primary key constraint to the `establishment_number`
    column 1, which will hold unique values that identify each establishment. Most
    of the remaining columns relate to the company’s name and location. You’ll use
    the `activities` column 2, which describes activities at the company, in the “Try
    It Yourself” exercise at the end of this chapter. We set most columns to `text`.
    In PostgreSQL, `text` is a varying length data type that affords us up to 1GB
    of data (see Chapter 4). The column `dbas` contains strings of more than 1,000
    characters in its rows, so we’re prepared to handle that. We import the CSV file
    3 and then create an index on the `company` column 4 to speed up searches for
    particular companies.
  prefs: []
  type: TYPE_NORMAL
- en: 'For practice, let’s use the `count()` aggregate function introduced in Chapter
    9 to check how many rows are in the `meat_poultry_egg_establishments` table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The result should show 6,287 rows. Now let’s find out what the data contains
    and determine whether we can glean useful information from it as is, or if we
    need to modify it in some way.
  prefs: []
  type: TYPE_NORMAL
- en: Interviewing the Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Interviewing data is my favorite part of analysis. We interview a dataset to
    discover its details—what it holds, what questions it can answer, and how suitable
    it is for our purposes—the same way a job interview reveals whether a candidate
    has the skills required.
  prefs: []
  type: TYPE_NORMAL
- en: The aggregate queries from Chapter 9 are a useful interviewing tool because
    they often expose the limitations of a dataset or raise questions you may want
    to ask before drawing conclusions and assuming the validity of your findings.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the `meat_poultry_egg_establishments` table’s rows describe food
    producers. At first glance, we might assume that each company in each row operates
    at a distinct address. But it’s never safe to assume in data analysis, so let’s
    check using the code in [Listing 10-2](#listing10-2).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 10-2: Finding multiple companies at the same address'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we group companies by unique combinations of the `company`, `street`,
    `city`, and `st` columns. Then we use `count(*)`, which returns the number of
    rows for each combination of those columns and gives it the alias `address_count`.
    Using the `HAVING` clause introduced in Chapter 9, we filter the results to show
    only cases where more than one row has the same combination of values. This should
    return all duplicate addresses for a company.
  prefs: []
  type: TYPE_NORMAL
- en: 'The query returns 23 rows, which means there are close to two dozen cases where
    the same company is listed multiple times at the same address:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This is not necessarily a problem. There may be valid reasons for a company
    to appear multiple times at the same address. For example, two types of processing
    plants could exist with the same name. On the other hand, we may have found data
    entry errors. Either way, it’s a wise practice to eliminate concerns about the
    validity of a dataset before relying on it, and this result should prompt us to
    investigate individual cases before we draw conclusions. However, this dataset
    has other issues that we need to look at before we can get meaningful information
    from it. Let’s work through a few examples.
  prefs: []
  type: TYPE_NORMAL
- en: Checking for Missing Values
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Next, we’ll check whether we have values from all states and whether any rows
    are missing a state code by asking a basic question: How many meat, poultry, and
    egg processing companies are there in each state? We’ll use the aggregate function
    `count()` along with `GROUP BY` to determine this, as shown in [Listing 10-3](#listing10-3).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 10-3: Grouping and counting states'
  prefs: []
  type: TYPE_NORMAL
- en: 'The query is a simple count that tallies the number of times each state postal
    code (`st`) appears in the table. Your result should include 57 rows, grouped
    by the state postal code in the column `st`. Why more than the 50 US states? Because
    the data includes Puerto Rico and other unincorporated US territories, such as
    Guam and American Samoa. Alaska (`AK`) is at the top of the results with a count
    of `17` establishments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: However, the row at the bottom of the list has a `NULL` value in the `st` column
    and a `3` in `st_count`. That means three rows have a `NULL` in `st`. To see the
    details of those facilities, let’s query those rows.
  prefs: []
  type: TYPE_NORMAL
- en: In [Listing 10-4](#listing10-4), we add a `WHERE` clause with the `st` column
    and the `IS NULL` keywords to find which rows are missing a state code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 10-4: Using `IS NULL` to find missing values in the `st` column'
  prefs: []
  type: TYPE_NORMAL
- en: 'This query returns three rows that don’t have a value in the `st` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: That’s a problem, because any counts that include the `st` column will be incorrect,
    such as the number of establishments per state. When you spot an error such as
    this, it’s worth making a quick visual check of the original file you downloaded.
    Unless you’re working with files in the gigabyte range, you can usually open a
    CSV file in one of the text editors I noted in Chapter 1 and search for the row.
    If you’re working with larger files, you might be able to examine the source data
    using utilities such as `grep` (on Linux and macOS) or `findstr` (on Windows).
    In this case, a visual check of the file from [https://www.data.gov/](https://www.data.gov/)
    confirms that, indeed, there was no state listed in those rows in the file, so
    the error is organic to the data, not one introduced during import.
  prefs: []
  type: TYPE_NORMAL
- en: In our interview of the data so far, we’ve discovered that we’ll need to add
    missing values to the `st` column to clean up this table. Let’s look at what other
    issues exist in our dataset and make a list of cleanup tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Checking for Inconsistent Data Values
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Inconsistent data is another factor that can hamper our analysis. We can check
    for inconsistently entered data within a column by using `GROUP BY` with `count()`.
    When you scan the unduplicated values in the results, you might be able to spot
    variations in the spelling of names or other attributes.
  prefs: []
  type: TYPE_NORMAL
- en: For example, many of the 6,200 companies in our table are multiple locations
    owned by just a few multinational food corporations, such as Cargill or Tyson
    Foods. To find out how many locations each company owns, we count the values in
    the `company` column. Let’s see what happens when we do, using the query in [Listing
    10-5](#listing10-5).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 10-5: Using `GROUP BY` and `count()` to find inconsistent company names'
  prefs: []
  type: TYPE_NORMAL
- en: 'Scrolling through the results reveals a number of cases in which a company’s
    name is spelled in several different ways. For example, notice the entries for
    the Armour-Eckrich brand:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: At least four different spellings are shown for seven establishments that are
    likely owned by the same company. If we later perform any aggregation by company,
    it would help to standardize the names so all the items counted or summed are
    grouped properly. Let’s add that to our list of items to fix.
  prefs: []
  type: TYPE_NORMAL
- en: Checking for Malformed Values Using length()
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It’s a good idea to check for unexpected values in a column that should be consistently
    formatted. For example, each entry in the `zip` column in the `meat_poultry_egg_establishments
    table should be formatted in the style of US ZIP codes with five digits. However,
    that’s not what is in our dataset.`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
